<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 14]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.IR](#cs.IR) [Total: 25]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LG](#cs.LG) [Total: 79]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Engineered Simultaneity: The Physical Impossibility of Consolidated Price Discovery Across Spacelike-Separated Exchanges](https://arxiv.org/abs/2602.22350)
*Paul Borrill*

Main category: cs.DC

TL;DR: 本文介绍了工程同步性的概念，并以美国国家最佳买卖报价（NBBO）为例，说明了其价值依赖于参考系的问题。由于交易所之间存在物理距离，导致光传播时间差产生不可避免的时间窗口，在此期间无法定义独立于参考系的价格顺序。高频交易公司利用这一时间窗口获取信息不对称优势，从而每年从其他市场参与者中提取约50亿美元。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示金融系统中因地理位置分散而产生的同步性问题，以及由此引发的信息不对称现象对市场的影响。

Method: 通过分析SEC的NMS规则611下的NBBO机制，结合相对论原理探讨不同参考系下价格同步性的变化；同时考察高频交易者如何利用直接数据流与综合证券信息处理器之间的延迟差异获利。

Result: 证明了NBBO的价值取决于定义“当前”价格的参考系；发现高频交易公司能够利用微秒级的时间差来获得相对于使用较慢数据源的投资者的优势。

Conclusion: 结论指出，将‘同时性’的概念应用于没有独立于参考系意义的领域是一种类别错误，这种做法造成了显著的信息不对称，并且对市场产生了重大经济影响。

Abstract: We introduce the concept of engineered simultaneity: a system design that (1) requires comparing events at spacelike-separated locations, (2) implements this comparison via an implicit simultaneity convention, and (3) represents the result as objective rather than conventional. The United States National Best Bid and Offer (NBBO), mandated by SEC Regulation NMS Rule 611, is shown to be an instance of engineered simultaneity. We prove that the NBBO is frame-dependent: its value depends on the reference frame in which "current" prices are defined. Since the exchanges that generate quote data are separated by distances of 43-1,180 km, light-travel times of 143-3,940 microseconds create unavoidable windows during which no frame-independent price ordering exists. High-frequency trading firms exploit this window by accessing exchange data via direct feeds (latency ~tens of microseconds) while the consolidated Securities Information Processor operates at ~1,128 microseconds -- a ratio exceeding 50:1. We demonstrate that this constitutes a category mistake in the sense of Ryle: the NBBO applies the concept of "simultaneity" in a domain where it has no frame-independent meaning. The resulting information asymmetry extracts approximately $5 billion annually from other market participants.

</details>


### [2] [DIAL: Decentralized I/O AutoTuning via Learned Client-side Local Metrics for Parallel File System](https://arxiv.org/abs/2602.22392)
*Md Hasanur Rashid,Xinyi Li,Youbiao He,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: 本文提出了一种名为DIAL的新方法，通过学习客户端本地指标来实现去中心化的I/O自动调优，从而在不依赖全局运行时度量的情况下改善了并行文件系统的I/O性能。


<details>
  <summary>Details</summary>
Motivation: 现有的并行文件系统（PFS）客户端I/O行为自动调优方法过度依赖大量全局运行时度量和应用I/O模式的准确建模，这导致开销巨大，限制了在实际系统中实现细粒度、动态调优的能力。

Method: DIAL采取了一种去中心化的方法，将每个I/O客户端视为独立单元，并仅使用其可观察到的本地指标进行配置调整。借助机器学习模型的帮助，DIAL使多个可调节单元能够做出独立但集体的决策，及时响应全局存储系统中的变化，以提高应用程序的整体I/O性能。

Result: DIAL展示了在减少对全局指标依赖的同时，依然能够有效提升并行文件系统内应用程序I/O性能的可能性。

Conclusion: 通过采用去中心化的方式结合机器学习技术，DIAL为并行文件系统的I/O自动调优提供了一个新的视角，它能够基于客户端局部信息做出优化决策，进而促进整体系统性能的提升。

Abstract: Enabling efficient, high-performance data access in parallel file systems (PFS) is critical for today's high-performance computing systems. PFS client-side I/O heavily impacts the final I/O performance delivered to individual applications and the entire system. Autotuning the key client-side I/O behaviors has been extensively studied and shows promising results. However, existing work has heavily relied on extensive number of global runtime metrics to monitor and accurate modeling of applications' I/O patterns. Such heavy overheads significantly limit the ability to enable fine-grained, dynamic tuning in practical systems. In this study, we propose DIAL (Decentralized I/O AutoTuning via Learned Client-side Local Metrics) which takes a drastically different approach. Instead of trying to extract the global I/O patterns of applications, DIAL takes a decentralized approach, treating each I/O client as an independent unit and tuning configurations using only its locally observable metrics. With the help of machine learning models, DIAL enables multiple tunable units to make independent but collective decisions, reacting to what is happening in the global storage systems in a timely manner and achieving better I/O performance globally for the application.

</details>


### [3] [AdapTBF: Decentralized Bandwidth Control via Adaptive Token Borrowing for HPC Storage](https://arxiv.org/abs/2602.22409)
*Md Hasanur Rashid,Dong Dai*

Main category: cs.DC

TL;DR: 本文提出了一种名为AdapTBF的自适应带宽控制方法，旨在提高每个应用程序性能和整体存储效率的同时确保公平性。通过在现代并行文件系统中引入自适应借用与借出机制，AdapTBF能够有效管理I/O带宽，在极端条件下也能保持高存储利用率。


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算（HPC）应用共享全局存储系统的设计可能导致某些应用消耗过多存储带宽，从而影响其他大型作业的执行效率。虽然基于令牌桶过滤器（TBF）的比例限制方法尝试解决这一问题，但严格的带宽限制往往导致整体I/O效率降低。因此，需要一种新的I/O控制策略来最大化单个应用性能及整体存储系统的使用效率，同时保证不同规模作业之间的公平性。

Method: 研究者们提出了AdapTBF方案，该方案基于现有的TBF机制但在现代并行文件系统如Lustre中加入了自适应借用和借出带宽控制的新特性。AdapTBF允许根据实际情况动态调整各应用可使用的带宽量，以应对突发性I/O需求而不浪费服务器容量。

Result: 实验结果表明，相较于传统的TBF方法，AdapTBF能够在维持高水平存储利用效率的同时更有效地管理I/O带宽资源。即使是在极具挑战性的场景下，它也显示出了良好的性能表现。

Conclusion: AdapTBF提供了一种改进的I/O带宽控制方式，对于提升HPC环境中各类应用的性能以及整个存储系统的运行效率具有重要意义。此外，它还为未来开发更加灵活高效的资源共享机制提供了新思路。

Abstract: Modern high-performance computing (HPC) applications run on compute resources but share global storage systems. This design can cause problems when applications consume a disproportionate amount of storage bandwidth relative to their allocated compute resources. For example, an application running on a single compute node can issue many small, random writes and consume excessive I/O bandwidth from a storage server. This can hinder larger jobs that write to the same storage server and are allocated many compute nodes, resulting in significant resource waste.
  A straightforward solution is to limit each application's I/O bandwidth on storage servers in proportion to its allocated compute resources. This approach has been implemented in parallel file systems using Token Bucket Filter (TBF). However, strict proportional limits often reduce overall I/O efficiency because HPC applications generate short, bursty I/O. Limiting bandwidth can waste server capacity when applications are idle or prevent applications from temporarily using higher bandwidth during bursty phases.
  We argue that I/O control should maximize per-application performance and overall storage efficiency while ensuring fairness (e.g., preventing small jobs from blocking large-scale ones). We propose AdapTBF, which builds on TBF in modern parallel file systems (e.g., Lustre) and introduces a decentralized bandwidth control approach using adaptive borrowing and lending. We detail the algorithm, implement AdapTBF in Lustre, and evaluate it using synthetic workloads modeled after real-world scenarios. Results show that AdapTBF manages I/O bandwidth effectively while maintaining high storage utilization, even under extreme conditions.

</details>


### [4] [CARAT: Client-Side Adaptive RPC and Cache Co-Tuning for Parallel File Systems](https://arxiv.org/abs/2602.22423)
*Md Hasanur Rashid,Nathan R. Tallent,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: 本文提出了一种名为CARAT的机器学习引导框架，用于在线调整并行文件系统的客户端RPC和缓存参数。该框架能够根据本地可观察到的指标独立地做出智能调整决策，以响应应用程序I/O行为和系统状态的实时变化。实验结果表明，与默认或静态配置相比，CARAT可以实现高达3倍的性能提升，并且具有广泛的部署潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的自动调优框架虽然可以根据应用程序的I/O模式调整PFS参数，但缺乏可扩展性、适应性和在线操作的能力。为了克服这些限制，作者们专注于开发一个可扩展的在线调优解决方案。

Method: 开发了一个名为CARAT的新框架，它利用机器学习指导来共同调整PFS的客户端RPC及缓存参数。CARAT的独特之处在于仅依赖于本地可观察到的度量标准，让每个客户端能够独立地作出智能调整决定，同时对应用程序I/O行为及系统状态的变化做出反应。

Result: 通过使用Lustre进行原型设计并广泛测试后发现，在动态I/O模式、实际HPC工作负载以及多客户端部署情况下，CARAT相较于默认或静态配置最高可达到3倍的性能改进。

Conclusion: CARAT证明了其在提高并行文件系统性能方面的有效性与通用性，由于具备良好的可扩展性和轻量化特性，认为CARAT有潜力被广泛应用于现有的PFS中，从而为各种数据密集型应用带来益处。

Abstract: Tuning parallel file system in High-Performance Computing (HPC) systems remains challenging due to the complex I/O paths, diverse I/O patterns, and dynamic system conditions. While existing autotuning frameworks have shown promising results in tuning PFS parameters based on applications' I/O patterns, they lack scalability, adaptivity, and the ability to operate online. In this work, focusing on scalable online tuning, we present CARAT, an ML-guided framework to co-tune client-side RPC and caching parameters of PFS, leveraging only locally observable metrics. Unlike global or pattern-dependent approaches, CARAT enables each client to make independent and intelligent tuning decisions online, responding to real-time changes in both application I/O behaviors and system states. We then prototyped CARAT using Lustre and evaluated it extensively across dynamic I/O patterns, real-world HPC workloads, and multi-client deployments. The results demonstrated that CARAT can achieve up to 3x performance improvement over the default or static configurations, validating the effectiveness and generality of our approach. Due to its scalability and lightweight, we believe CARAT has the potential to be widely deployed into existing PFS and benefit various data-intensive applications.

</details>


### [5] [GetBatch: Distributed Multi-Object Retrieval for ML Data Loading](https://arxiv.org/abs/2602.22434)
*Alex Aizman,Abhishek Gaikwad,Piotr Żelasko*

Main category: cs.DC

TL;DR: 本文提出了一种新的对象存储API GetBatch，用于优化机器学习训练流程中的数据批量获取问题。GetBatch通过将独立的GET操作替换为单个确定性、容错的流式执行来提高吞吐量并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 在机器学习训练过程中，从分布式存储集群中获取大量样本时，单独发出数千个GET请求会导致每个请求的开销，这往往超过了实际的数据传输时间。为了解决这个问题，作者提出了一个新的解决方案。

Method: 提出名为GetBatch的新对象存储API，它将批量检索提升为首要级别的存储操作，取代了传统的独立GET操作方式，并支持单一的确定性和具备容错能力的流处理执行模式。

Result: GetBatch对于小对象实现了高达15倍的吞吐量改进；在一个实际生产环境下的训练工作负载测试中，与单独使用GET请求相比，P95批次检索延迟降低了2倍，而P99每对象尾部延迟则减少了3.7倍。

Conclusion: GetBatch有效解决了传统方法中由于过多GET请求导致的效率低下问题，显著提高了数据获取的速度和稳定性，对依赖大规模数据集进行训练的机器学习应用具有重要意义。

Abstract: Machine learning training pipelines consume data in batches. A single training step may require thousands of samples drawn from shards distributed across a storage cluster. Issuing thousands of individual GET requests incurs per-request overhead that often dominates data transfer time. To solve this problem, we introduce GetBatch - a new object store API that elevates batch retrieval to a first-class storage operation, replacing independent GET operations with a single deterministic, fault-tolerant streaming execution. GetBatch achieves up to 15x throughput improvement for small objects and, in a production training workload, reduces P95 batch retrieval latency by 2x and P99 per-object tail latency by 3.7x compared to individual GET requests.

</details>


### [6] [veScale-FSDP: Flexible and High-Performance FSDP at Scale](https://arxiv.org/abs/2602.22437)
*Zezhou Wang,Youjie Li,Zhiqi Lin,Jiacheng Yang,Cong Xie,Guanyu Feng,Zheng Zhong,Ziyue Huang,Hongyu Zhu,Zhi Zhang,Yanghua Peng,Xin Liu*

Main category: cs.DC

TL;DR: 介绍了一种新的FSDP系统veScale-FSDP，通过结合灵活的分片格式RaggedShard与结构感知规划算法来提高大规模模型训练时的灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的FSDP系统在处理结构感知训练方法（如块级量化训练）及非元素级优化器时遇到困难，并且在通信和内存效率方面存在不足，限制了其扩展到数万GPU的能力。

Method: 提出veScale-FSDP系统，该系统利用一种名为RaggedShard的灵活分片格式加上结构感知计划算法，旨在为FSDP提供所需的数据高效放置，支持块级量化和非元素级优化器。

Result: 实验结果显示，相比现有FSDP系统，veScale-FSDP能够实现5~66%的吞吐量提升以及16~30%的内存使用降低，并且能够有效扩展至数万GPU。

Conclusion: veScale-FSDP通过引入创新的设计克服了传统FSDP系统的局限性，在支持复杂模型训练的同时显著提升了性能和可扩展性。

Abstract: Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.

</details>


### [7] [Fault-tolerant Reduce and Allreduce operations based on correction](https://arxiv.org/abs/2602.22445)
*Martin Kuettler,Hermann Haertig*

Main category: cs.DC

TL;DR: 本文提出了一种新的Reduce算法，该算法在基于树的阶段之前加入了一个类似校正的通信阶段，使得算法能够容忍一定数量的过程失败。基于此，进一步结合Broadcast和Reduce来实现Allreduce功能。


<details>
  <summary>Details</summary>
Motivation: 先前的工作已经提出了基于一些信息传播算法（如闲话或基于树的通信）实现广播的方法，并随后进行校正。本研究旨在将类似的想法应用于Reduce操作中，通过引入一个类似于校正的通信阶段来提高对过程失败的容忍度。

Method: 本研究首先为Reduce设计了一种新方法，其中包含一个模仿校正机制的通信步骤，然后是一个基于树结构的处理步骤。通过对这种新Reduce方法的语义定义及证明，展示了其有效性。接着，研究者将这一新型Reduce与已有的广播技术相结合，以构建Allreduce功能。

Result: 开发出了一种对于过程失败具有一定容错能力的Reduce算法，并且成功地将Broadcast与改进后的Reduce结合起来形成了Allreduce。

Conclusion: 通过引入一种新的、具有容错性的Reduce算法，并将其与Broadcast结合形成Allreduce，本研究为分布式计算提供了一种更为可靠的数据聚合方式。

Abstract: Implementations of Broadcast based on some information dissemination algorithm -- e.g., gossip or tree-based communication -- followed by a correction algorithm has been proposed previously. This work describes an approach to apply a similar idea to Reduce. In it, a correction-like communication phase precedes a tree-based phase. This provides a Reduce algorithm which is tolerant to a number of failed processes. Semantics of the resulting algorithm are provided and proven.
  Based on these results, Broadcast and Reduce are combined to provide Allreduce.

</details>


### [8] [FuxiShuffle: An Adaptive and Resilient Shuffle Service for Distributed Data Processing on Alibaba Cloud](https://arxiv.org/abs/2602.22580)
*Yuhao Lin,Zhipeng Tang,Jiayan Tong,Junqing Xiao,Bin Lu,Yuhang Li,Chao Li,Zhiguo Zhang,Junhua Wang,Hao Luo,James Cheng,Chuang Hu,Jiawei Jiang,Xiao Yan*

Main category: cs.DC

TL;DR: FuxiShuffle, a data shuffle service for large-scale production environments, improves adaptability and failure resilience through dynamic mode selection, progress-aware scheduling, and an efficient failover mechanism, leading to reduced job completion time and resource consumption.


<details>
  <summary>Details</summary>
Motivation: The existing systems for improving shuffle efficiency in distributed data processing are not adaptable to the highly dynamic job characteristics and cluster resource conditions. Furthermore, their fault tolerance mechanisms are passive and inefficient. FuxiShuffle was developed to address these limitations by providing a more adaptable and resilient solution for ultra-large production environments such as Alibaba Cloud MaxCompute platform.

Method: FuxiShuffle dynamically selects the appropriate shuffle mode based on runtime information, uses progress-aware scheduling for downstream workers, and determines the best backup strategy for each shuffle data chunk. It also implements multi-replica failover, careful memory management, and incremental recovery to ensure efficient failure resilience without losing computation progress.

Result: Experiments demonstrated that FuxiShuffle outperforms baseline systems by significantly reducing both end-to-end job completion times and overall resource usage. Additionally, micro experiments validated the effectiveness of FuxiShuffle's design in enhancing adaptability and failure resilience.

Conclusion: FuxiShuffle successfully addresses the limitations of current shuffle systems by introducing a general data shuffle service with improved adaptability and failure resilience, making it well-suited for ultra-large production environments where dynamic job characteristics and resource conditions are prevalent.

Abstract: Shuffle exchanges intermediate results between upstream and downstream operators in distributed data processing and is usually the bottleneck due to factors such as small random I/Os and network contention. Several systems have been designed to improve shuffle efficiency, but from our experiences of running ultra-large clusters at Alibaba Cloud MaxCompute platform, we observe that they can not adapt to highly dynamic job characteristics and cluster resource conditions, and their fault tolerance mechanisms are passive and inefficient when failures are inevitable. To tackle their limitations, we design and implement FuxiShuffle as a general data shuffle service for the ultra-large production environment of MaxCompute, featuring good adaptability and efficient failure resilience. Specifically, to achieve good adaptability, FuxiShuffle dynamically selects the shuffle mode based on runtime information, conducts progress-aware scheduling for the downstream workers, and automatically determines the most suitable backup strategy for each shuffle data chunk. To make failure resilience efficient, FuxiShuffle actively ensures data availability with multi-replica failover, prevents memory overflow with careful memory management, and employs an incremental recovery mechanism that does not lose computation progress. Our experiments show that, compared to baseline systems, FuxiShuffle significantly reduces not only end-to-end job completion time but also aggregate resource consumption. Micro experiments suggest that our designs are effective in improving adaptability and failure resilience.

</details>


### [9] [FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving](https://arxiv.org/abs/2602.22593)
*Shouwei Gao,Junqi Yin,Feiyi Wang,Wenqian Dong*

Main category: cs.DC

TL;DR: Flying Serving是一个基于vLLM的系统，支持在线DP-TP切换而无需重启引擎工作器。它通过虚拟化状态来实现重新配置，从而避免了数据移动，并在多种服务场景下显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的服务栈通常在部署时就固定了并行配置，这使得应对突发流量、优先级请求或长上下文请求变得困难且反应迟缓。为了解决这个问题，需要一种能够灵活适应变化的服务系统。

Method: Flying Serving通过以下方式实现了灵活的并行配置转换：(i)零拷贝模型权重管理器，按需提供TP分片视图；(ii)KV缓存适配器，在DP/TP布局之间保持请求KV状态；(iii)预先初始化的通信池，用于摊销集体设置成本；(iv)无死锁调度器，即使在执行偏差下也能协调安全过渡。

Result: 通过对三种流行的大型语言模型及实际服务场景测试表明，Flying Serving在高负载下最高可提升4.79倍性能，在低负载下则为3.47倍，同时还能支持延迟和内存驱动的请求。

Conclusion: Flying Serving通过引入创新机制解决了传统服务栈在面对非稳定流量和混合请求需求时的局限性，能够在不中断服务的情况下实现实时并行模式切换，极大地提升了系统的灵活性与性能。

Abstract: Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\times$ under high load and $3.47\times$ under low load while supporting latency- and memory-driven requests.

</details>


### [10] [Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study](https://arxiv.org/abs/2602.22760)
*Philipp Wiesner,Soeren Becker,Brett Cornick,Dominik Scheinert,Alexander Acker,Odej Kao*

Main category: cs.DC

TL;DR: 该技术报告介绍了一个系统，该系统在可再生能源过剩期间跨地理分布的GPU集群进行大规模语言模型的全参数训练，通过弹性切换本地单点训练和联邦多点同步来利用原本会被削减的清洁能源。初步结果显示，这种感知削减的调度方法保持了训练质量的同时，将运营排放量减少到了单点基线的5-12%。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLM）的训练需要大量的计算能力和能源。与此同时，可再生能源源经常产生超过电网吸收能力的电量，导致削减现象发生——即故意减少那些本可能被浪费掉的清洁发电量。这些时期提供了一个机会：如果能够将训练与削减窗口期对齐，那么就可以使用既清洁又便宜的电力来进行预训练。

Method: 研究者们提出了一套系统，能够在区域削减窗口期内跨地理分布的GPU集群执行全参数的大规模语言模型训练，并且根据站点可用性灵活地在本地单站点训练与联邦多站点同步之间切换。实验中使用了Flower联邦学习框架来训练一个包含5.61亿参数的transformer模型，其中削减时间段基于实际的边际碳强度数据得出。

Result: 初步结果表明，通过感知削减的调度策略不仅保持了良好的训练效果，而且相比于传统的单站点训练方式，还成功地将操作过程中产生的温室气体排放量降至原水平的5%-12%左右。

Conclusion: 这项工作展示了一种新的方法，即通过与可再生能源过剩时间相匹配来安排大型语言模型训练任务，从而实现更环保、成本更低的AI模型开发过程。

Abstract: Training large language models (LLMs) requires substantial compute and energy. At the same time, renewable energy sources regularly produce more electricity than the grid can absorb, leading to curtailment, the deliberate reduction of clean generation that would otherwise go to waste. These periods represent an opportunity: if training is aligned with curtailment windows, LLMs can be pretrained using electricity that is both clean and cheap. This technical report presents a system that performs full-parameter LLM training across geo-distributed GPU clusters during regional curtailment windows, elastically switching between local single-site training and federated multi-site synchronization as sites become available or unavailable. Our prototype trains a 561M-parameter transformer model across three clusters using the Flower federated learning framework, with curtailment periods derived from real-world marginal carbon intensity traces. Preliminary results show that curtailment-aware scheduling preserves training quality while reducing operational emissions to 5-12% of single-site baselines.

</details>


### [11] [An Artificial Intelligence Framework for Joint Structural-Temporal Load Forecasting in Cloud Native Platforms](https://arxiv.org/abs/2602.22780)
*Qingyuan Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种面向微服务拓扑的结构化时间联合负载预测框架，通过构建基于服务级别观察的邻域聚合和全局概要视图，形成实例、服务和集群级别的分层负载表示。该框架引入轻量级结构先验到注意力计算中，增强了调用依赖性的表达，同时采用多目标回归策略共同优化服务级别和集群级别预测，从而提高跨粒度稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对云原生环境中复杂的微服务调用关系、多尺度叠加的负载波动以及显著的跨服务影响问题，需要一种能够有效捕捉负载传播与积累，并且能够一致地建模局部突发情况和整体趋势的方法。

Method: 提出了一种面向微服务拓扑结构的时间演变服务调用图与多元负载序列相结合的框架。该方法通过构建基于服务层面观测值的邻域聚合视图及全局汇总视图来形成跨越实例、服务和集群三个层级的负载表示；统一序列编码器用于建模多尺度历史上下文；在注意力机制中引入轻量级结构先验以增强对调用依赖性的捕捉。

Result: 实验结果表明了多层次融合和结构注入的有效性，并明确了它们的有效配置范围。此外，还进行了单因素敏感性分析，系统地考察了时间窗口长度、编码深度以及正则化强度等因素的影响。

Conclusion: 所提出的框架为云环境中的容量评估、资源编排和运行时态势感知提供了一个可重用的建模范式和实现路径。

Abstract: This study targets cloud native environments where microservice invocation relations are complex, load fluctuations are multi-scale and superimposed, and cross-service impacts are significant. We propose a structured temporal joint load prediction framework oriented to microservice topology. The method represents the system as a coupled entity of a time-evolving service invocation graph and multivariate load sequences. It constructs neighborhood-aggregated and global summarized views based on service level observations. This forms layered load representations across instance, service, and cluster levels. A unified sequence encoder models multi-scale historical context. To strengthen the expression of invocation dependencies, the framework introduces a lightweight structural prior into attention computation. This enables more effective capture of load propagation and accumulation along invocation chains, while maintaining consistent modeling of local bursts and overall trends. The training objective adopts a multi-objective regression strategy that jointly optimizes service level and cluster level predictions to improve cross-granularity stability. We further conduct single-factor sensitivity analyses on key structural and training hyperparameters. We systematically examine the effects of time window length, encoding depth, and regularization strength. The results support the necessity of multi-granularity fusion and structural injection and clarify their effective configuration ranges. Overall, the framework provides a reusable modeling paradigm and implementation path for capacity assessment, resource orchestration, and runtime situational understanding in cloud environments.

</details>


### [12] [Workload Buoyancy: Keeping Apps Afloat by Identifying Shared Resource Bottlenecks](https://arxiv.org/abs/2602.22852)
*Oliver Larsson,Thijs Metsch,Cristian Klein,Erik Elmroth*

Main category: cs.DC

TL;DR: 本文提出了一种新的抽象概念——浮力，用于表征多租户系统中的工作负载性能。通过结合应用级指标和系统级的共享资源争用见解，浮力提供了一个全面的性能动态视图，平均而言，在指示瓶颈方面比传统启发式方法好19.3%。


<details>
  <summary>Details</summary>
Motivation: 当前多租户、硬件异构计算环境给有效的工作负载编排带来了显著挑战。简单的基于CPU利用率或应用程序级别指标的性能评估启发法往往不足以捕捉由资源共享竞争和噪声邻居效应引起的复杂性能动态。

Method: 提出了“浮力”这一新概念，它不仅整合了应用级别的度量标准，还考虑到了系统层面对于共享资源争夺的理解，从而为理解性能动态提供了全方位视角。

Result: 实验表明，“浮力”能够比传统的方法更准确地识别出性能瓶颈，平均改善率达到19.3%。此外，还展示了如何使用‘浮力’作为传统性能指标的直接替代品，以提高可观察性，并做出更加明智的调度与优化决策。

Conclusion: 浮力作为一种新型的工作负载性能特征化手段，在多租户系统中具有重要的应用价值，能够促进资源感知及应用程序感知的编排策略发展。

Abstract: Modern multi-tenant, hardware-heterogeneous computing environments pose significant challenges for effective workload orchestration. Simple heuristics for assessing workload performance, such as CPU utilization or application-level metrics, are often insufficient to capture the complex performance dynamics arising from resource contention and noisy-neighbor effects. In such environments, performance bottlenecks may emerge in any shared system resource, leading to unexpected and difficult-to-diagnose degradation.
  This paper introduces buoyancy, a novel abstraction for characterizing workload performance in multi-tenant systems. Unlike traditional approaches, buoyancy integrates application-level metrics with system-level insights of shared resource contention to provide a holistic view of performance dynamics. By explicitly capturing bottlenecks and headroom across multiple resources, buoyancy facilitates resource-aware and application-aware orchestration in a manner that is intuitive, extensible, and generalizable across heterogeneous platforms. We evaluate buoyancy using representative multi-tenant workloads to illustrate its ability to expose performance-limiting resource interactions. Buoyancy provides a 19.3% better indication of bottlenecks compared to traditional heuristics on average. We additionally show how buoyancy can act as a drop-in replacement for conventional performance metrics, enabling improved observability and more informed scheduling and optimization decisions.

</details>


### [13] [A Simple Distributed Deterministic Planar Separator](https://arxiv.org/abs/2602.22916)
*Yaseen Abd-Elhaleem,Michal Dory,Oren Weimann*

Main category: cs.DC

TL;DR: 本文提出了一种更简单的确定性分隔符算法，具有近似最优的$\tilde O(D)$轮复杂度。通过让每个顶点简单地将其权重转移到它所在的任意一个面上，实现了对平面图上经典问题如单源最短路径、最大流、有向全局最小割和可达性的分布式算法的去随机化。


<details>
  <summary>Details</summary>
Motivation: 在分布式环境中，许多优化问题的解决轮数复杂度有一个普遍的下界$D$（图的直径）。因此，大小为$O(D)$的分隔符是优选的。然而，现有的$O(D)$大小分隔符的分布式算法是随机化的，这使得使用它的算法也必须是随机化的。开发一个确定性的算法成为了一个有趣且未解决的问题。

Method: 作者提出了一种新的方法来转移顶点到图的面的权重：每个顶点直接将其权重转移到它所在的一个任意面上。这种方法比之前的工作中所使用的复杂或随机的方法要简单得多。

Result: 新提出的确定性分隔符算法能够在$\tilde O(D)$轮内完成，与已有的最优结果相匹配。该算法不仅简化了过程，还保持了高效的轮数复杂度。

Conclusion: 这项工作展示了一种大大简化的确定性分隔符算法，能够有效应用于平面图上的多种经典问题，并有助于这些领域内已有算法的去随机化。

Abstract: A balanced separator of a graph $G$ is a set of vertices whose removal disconnects the graph into connected components that are a constant factor smaller than $G$. Lipton and Tarjan [FOCS'77] famously proved that every planar graph admits a balanced separator of size $O(\sqrt{n})$, as well as a balanced separator of size $O(D)$ that is a simple path (where $D$ is $G$'s diameter). In the centralized setting, both separators can be found in linear time. In the distributed setting, $D$ is a universal lower bound for the round complexity of solving many optimization problems, so, separators of size $O(D)$ are preferable.
  It was not until [DISC'17] that a distributed algorithm was devised by Ghaffari and Parter to compute such an $O(D)$-size separator in $\tilde O(D)$ rounds, by adapting the Lipton-Tarjan algorithm to the distributed model. Since then, this algorithm was used in several distributed algorithms for planar graphs, e.g., [GP, DISC'17], [LP, STOC'19], [AEDPW, PODC'25]. However, the algorithm is randomized, deeming the algorithms that use it to be randomized as well. Obtaining a deterministic algorithm remained an interesting open question until [PODC'25], when a (complex) deterministic separator algorithm was given by Jauregui, Montealegre and Rapaport.
  We present a much simpler deterministic separator algorithm with the same (near-optimal) $\tilde O(D)$-round complexity. While previous works devised either complicated or randomized ways of transferring weights from vertices to faces of $G$, we show that a straightforward way also works: Each vertex simply transfers its weight to one arbitrary face it lies on. That's it!
  We note that a deterministic separator algorithm directly derandomizes the state-of-the-art distributed algorithms for classical problems on planar graphs such as single-source shortest-paths, maximum-flow, directed global min-cut, and reachability.

</details>


### [14] [LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure](https://arxiv.org/abs/2602.23036)
*Jaehong Cho,Hyunmin Choi,Guseul Heo,Jongse Park*

Main category: cs.DC

TL;DR: 本文介绍了一种新的系统级模拟器LLMServingSim 2.0，旨在分析异构和分散的大语言模型（LLM）服务基础设施中的硬件-软件交互。该模拟器能够精确再现关键性能指标，并为下一代LLM服务基础设施的设计提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）服务架构向异构化和组件分离方向发展，现有模拟器难以同时对多样化硬件及分散处理技术进行统一建模。因此，理解这些架构下的硬件-软件运行时交互变得尤为重要且具有挑战性。

Method: 开发了LLMServingSim 2.0，这是一个集成化的系统级模拟器，它将服务决策与硬件行为结合到一个运行时循环中，支持通过基于配置文件的建模方式灵活地整合新兴加速器和内存系统，并捕捉动态服务行为及其系统级影响。

Result: LLMServingSim 2.0在真实部署环境中得到了验证，对于复杂配置也能保持约10分钟的仿真时间，平均误差仅为0.97%。这表明它可以准确再现关键的性能、内存和功耗指标。

Conclusion: LLMServingSim 2.0成功地提供了一个连接硬件创新和服务系统设计之间的桥梁，促进了针对下一代LLM服务基础设施的有效探索与协同设计。

Abstract: Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or software choices in isolation, but by their runtime interaction through scheduling, data movement, and interconnect behavior. However, understanding these interactions remains challenging, as existing simulators lack the ability to jointly model heterogeneous hardware and disaggregated serving techniques within a unified, runtime-driven framework.
  This paper presents LLMServingSim 2.0, a unified system-level simulator designed to make runtime-driven hardware-software interactions in heterogeneous and disaggregated LLM serving infrastructures explicit and analyzable. LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. The simulator supports extensible integration of emerging accelerators and memory systems through profile-based modeling, while capturing dynamic serving behavior and system-level effects. We validate LLMServingSim 2.0 against real deployments, showing that it reproduces key performance, memory, and power metrics with an average error of 0.97%, while maintaining simulation times of around 10 minutes even for complex configurations. These results demonstrate that LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [EmpiRE-Compass: A Neuro-Symbolic Dashboard for Sustainable and Dynamic Knowledge Exploration, Synthesis, and Reuse](https://arxiv.org/abs/2602.22276)
*Oliver Karras,Amirreza Alasti,Lena John,Sushant Aggarwal,Yücel Celik*

Main category: cs.SE

TL;DR: 本研究引入了EmpiRE-Compass，一个神经符号仪表盘，旨在通过研究知识图谱(RKGs)和大型语言模型(LLMs)的结合来提高文献综述(LRs)在软件工程(SE)和需求工程(RE)中的可持续性、透明度和可复用性。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物数量的增加，软件工程和需求工程领域的文献综述（LRs）数量也在快速增长。生成式人工智能虽然能够快速生成LRs，但往往牺牲了质量、严谨性和透明度。此外，这些研究通常不分享底层数据和工件，限制了复制与再利用的可能性。

Method: 研究人员开发了一个名为EmpiRE-Compass的神经符号仪表板，它采用模块化系统设计，并为策划和定制的能力问题提供工作流程。该工具基于两个需求工程案例构建，利用研究知识图谱(RKGs)语义结构化地组织底层数据，并通过大型语言模型(LLMs)促进轻松动态的数据访问、复制及再利用。

Result: EmpiRE-Compass提供了三个核心功能：(1)针对策划能力问题的探索性可视化分析；(2)针对自定义能力问题的神经符号综合；(3)所有查询、分析和结果均可公开获取的知识重用。此工具通过将RKGs和LLMs统一在一个神经符号仪表板中，降低了技术障碍，促进了透明度和可重复性，并支持协作、持续更新以及可重用的文献综述。

Conclusion: EmpiRE-Compass通过整合研究知识图谱和大型语言模型于一神经符号界面，为需求工程、软件工程及其他领域内的文献综述提供了更加可持续的方法论，从而减少了技术壁垒，提高了透明度与可再现性，并实现了协作、不断更新且可重复使用的文献综述过程。

Abstract: Software engineering (SE) and requirements engineering (RE) face a significant increase in secondary studies, particularly literature reviews (LRs), due to the ever-growing number of scientific publications. Generative artificial intelligence (GenAI) exacerbates this trend by producing LRs rapidly but often at the expense of quality, rigor, and transparency. At the same time, secondary studies often fail to share underlying data and artifacts, limiting replication and reuse. This paper introduces EmpiRE-Compass, a neuro-symbolic dashboard designed to lower barriers for accessing, replicating, and reusing LR data. Its overarching goal is to demonstrate how LRs can become more sustainable by semantically structuring their underlying data in research knowledge graphs (RKGs) and by leveraging large language models (LLMs) for easy and dynamic access, replication, and reuse. Building on two RE use cases, we developed EmpiRE-Compass with a modular system design and workflows for curated and custom competency questions. The dashboard is freely available online, accompanied by a demonstration video. To manage operational costs, a limit of 25 requests per IP address per day applies to the default LLM (GPT-4o mini). All source code and documentation are released as an open-source project to foster reuse, adoption, and extension. EmpiRE-Compass provides three core capabilities: (1) Exploratory visual analytics for curated competency questions; (2) Neuro-symbolic synthesis for custom competency questions; and (3) Reusable knowledge with all queries, analyses, and results openly available. By unifying RKGs and LLMs in a neuro-symbolic dashboard, EmpiRE-Compass advances sustainable LRs in RE, SE, and beyond. It lowers technical barriers, fosters transparency and reproducibility, and enables collaborative, continuously updated, and reusable LRs

</details>


### [16] [The Ethos of the PEERfect REVIEWer: Scientific Care and Collegial Welfare](https://arxiv.org/abs/2602.22292)
*Oliver Karras*

Main category: cs.SE

TL;DR: 本文通过作者十年的学术经验，提出了PEERfect REVIEWer的理念，强调科学严谨性和同僚福祉在同行评审中的重要性，并提供了16条实用建议来指导审稿人如何在保持高标准的同时以建设性、支持性、尊重及时的方式进行同行评审。


<details>
  <summary>Details</summary>
Motivation: 同行评审是学术界的重要组成部分，但往往缺乏对所有参与者（包括作者、共同审稿人、研讨会或会议组织者以及期刊编辑）的支持和鼓励所必需的同情心。为了解决这一问题，本文提出了一种新的同行评审理念，旨在促进共同进步与福祉。

Method: 基于作者在学术界的十年经历，结合与同事的专业交流、文献考量以及对自己撰写和收到的评审意见的反思，发展出一套包含16条具体建议的指南，用以引导审稿人在实践中实现科学关怀与同僚福祉两大核心价值。

Result: 形成了名为PEERfect REVIEWer的同行评审理念及其配套指南，该理念及指南帮助审稿人维护高水平的科学标准，并以一种积极支持、相互尊重且及时反馈的方式开展同行评审工作。

Conclusion: 通过将科学关怀与同僚福祉置于同行评审的核心位置，这份经验报告不仅重申了科学严谨性的重要性，同时也呼吁更多地关注同情心的作用。它邀请审稿人们重新思考自己的角色，不仅仅作为质量把关者，而是成为每位参与者的学术旅程中的合作伙伴。

Abstract: Peer review remains a cornerstone in academia, yet it frequently falls short in fostering joint progress and well-being. While peer review primarily emphasizes scientific rigor, it often lacks the empathy essential for supporting and encouraging all peers involved. In this experience report, I aim to highlight that peer review is a practice that demands both scientific care for quality and collegial welfare for the joint progress and well-being of all peers involved, including authors, co-reviewers, workshop or conference organizers, and journal editors. Drawing on my ten years of experience in academia, I propose the ethos of the PEERfect REVIEWer, grounded in the two core values: Scientific care and collegial welfare. Through reflection shaped by professional exchanges with colleagues, consideration of literature, and an examination of both self-authored and received reviews, I formulated an accompanying guideline with 16 practical recommendations to guide reviewers in their actions to achieve these two values. The ethos of the PEERfect REVIEWer and its accompanying guideline help reviewers in upholding high scientific standards and conducting peer review in a constructive, supportive, respectful, and timely manner. They demonstrate that scientific rigor and empathy are complementary forces that promote impactful peer review practice. By placing scientific care and collegial welfare at the core of peer review, this experience report reaffirms the importance of scientific rigor while also advocating for greater attention to empathy. It invites reviewers to reconsider their role not merely as gatekeepers but as partners in the academic journey of each peer involved. The PEERfect REVIEWer is both a caretaker of quality and a steward of joint progress and well-being - as truly impactful peer review practice requires scientific rigor and empathy in equal measure.

</details>


### [17] [EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization](https://arxiv.org/abs/2602.22368)
*Jiahao Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: 本文提出EyeLayer，一种轻量级注意力增强模块，通过整合人类阅读代码时的眼动模式来提高基于大语言模型的代码摘要生成效果。实验显示，在不同模型上，EyeLayer相较于强基准方法在标准度量上均有显著提升，最高提升了13.17%的BLEU-4分数。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在代码摘要任务中取得了显著进展，但能否进一步利用人类在理解代码方面的专业知识来指导和增强这些模型仍是一个开放问题。

Method: 提出了EyeLayer，这是一个轻量级的注意力增强模块，它通过多模态高斯混合模型模拟人类读代码时的注意力分布，并根据学习到的参数(μ_i, σ_i^2)重新分配token嵌入，以此方式将开发者关注点的信息融入到LLM中。

Result: EyeLayer在包括LLaMA-3.2、Qwen3以及CodeBERT在内的多种规模和架构的模型上进行了评估，结果表明它在标准度量如BLEU-4上相比强大的微调基线有了一致性的改进，最高提升了13.17%。

Conclusion: 研究结果证明了人类注视模式编码了能够补充LLMs语义焦点的注意力信号，并且这些信号可以有效地跨不同的模型进行迁移，从而改善代码摘要的效果。

Abstract: Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understanding further guide and enhance these models? We propose EyeLayer, a lightweight attention-augmentation module that incorporates human eye-gaze patterns, as a proxy of human expertise, into LLM-based code summarization. EyeLayer models human attention during code reading via a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters (μ_i, σ_i^2) that capture where and how intensively developers focus. This design enables learning generalizable attention priors from eye-tracking data and incorporating them into LLMs seamlessly, without disturbing existing representations. We evaluate EyeLayer across diverse model families (i.e., LLaMA-3.2, Qwen3, and CodeBERT) covering different scales and architectures. EyeLayer consistently outperforms strong fine-tuning baselines across standard metrics, achieving gains of up to 13.17% on BLEU-4. These results demonstrate that human gaze patterns encode complementary attention signals that enhance the semantic focus of LLMs and transfer effectively across diverse models for code summarization.

</details>


### [18] [Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents](https://arxiv.org/abs/2602.22402)
*Cosmo Santoni*

Main category: cs.SE

TL;DR: 该论文提出了一种名为上下文记忆虚拟化（CMV）的系统，它将大型语言模型在长时间推理任务中积累的理解视为版本控制状态。通过借鉴操作系统虚拟内存的概念，CMV使用有向无环图来表示会话历史，并定义了快照、分支和修剪等操作，以促进独立并行会话间的上下文重用。同时，文章介绍了一种三遍结构无损修剪算法，可以在保留用户消息和助手回复原样的同时显著减少令牌数量。通过对76个实际编码会话的研究表明，即使在提示缓存的情况下，这种修剪方法仍然具有经济效益，特别是在混合工具使用的会话中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型参与更加复杂的推理任务，在对话窗口内积累了大量的状态信息——包括架构映射、权衡决策以及代码库惯例等。当会话达到上下文限制而不得不进行损失性压缩时，这些累积的理解就会丢失。为了解决这一问题，研究者们提出了上下文记忆虚拟化（Contextual Memory Virtualisation, CMV）系统。

Method: CMV系统采用了类似于操作系统虚拟内存的技术，将会话历史建模为一个有向无环图（DAG），并且定义了一系列正式的操作如快照、分支与修剪等，使得不同并行会话间能够共享上下文信息。此外，还开发了一个三阶段的结构无损修剪算法，能够在完全保留所有用户消息及助手回应的同时大幅度降低所需令牌数。

Result: 实验结果表明，在处理包含大量额外信息（例如原始工具输出、Base64格式图片和元数据）的会话时，所提出的修剪算法可平均减少20%的令牌数，最高可达86%。基于对76次真实世界中的编程会话分析发现，即使考虑到提示缓存的影响，此方法依然能保持经济可行性，尤其是在涉及多种工具交互的场景下效果最为明显，平均节省率达到39%，通常在10轮对话后即可实现收支平衡。

Conclusion: 上下文记忆虚拟化(CMV)提供了一种有效的方法来管理和优化大型语言模型在持续推理过程中产生的状态信息，通过引入版本控制概念和高效的修剪策略，不仅能够延长单一会话的有效时间长度，还能跨多个并行会话重用上下文信息，从而提高整体效率。

Abstract: As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Contextual Memory Virtualisation (CMV), a system that treats accumulated LLM understanding as version-controlled state. Borrowing from operating system virtual memory, CMV models session history as a Directed Acyclic Graph (DAG) with formally defined snapshot, branch, and trim primitives that enable context reuse across independent parallel sessions. We introduce a three-pass structurally lossless trimming algorithm that preserves every user message and assistant response verbatim while reducing token counts by a mean of 20% and up to 86% for sessions with significant overhead by stripping mechanical bloat such as raw tool outputs, base64 images, and metadata. A single-user case-study evaluation across 76 real-world coding sessions demonstrates that trimming remains economically viable under prompt caching, with the strongest gains in mixed tool-use sessions, which average 39% reduction and reach break-even within 10 turns. A reference implementation is available at https://github.com/CosmoNaught/claude-code-cmv.

</details>


### [19] [XMENTOR: A Rank-Aware Aggregation Approach for Human-Centered Explainable AI in Just-in-Time Software Defect Prediction](https://arxiv.org/abs/2602.22403)
*Saumendu Roy,Banani Roy,Chanchal Roy,Richard Bassey*

Main category: cs.SE

TL;DR: 本文提出了一种名为XMENTOR的人性化、排名感知的聚合方法，作为VS Code插件实现，旨在解决机器学习缺陷预测模型解释不一致的问题。通过自适应阈值、排名和符号一致性以及回退策略来统一多个事后解释，提供清晰而不压倒用户的单一连贯视图。用户研究表明，该方法有助于减少困惑，并为日常调试和缺陷审查任务提供了更强的支持。


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的缺陷预测模型能够提高软件质量，但其不透明的推理机制给开发者带来了信任问题。虽然有如LIME、SHAP等可解释人工智能(XAI)方法试图提供透明度，但当它们一起使用时往往会产生相互矛盾的解释，增加了开发者的困惑与认知负担。为了解决这一可用性挑战，需要一种新的方法来整合这些解释，以提升开发者对模型的信任及使用体验。

Method: 提出了XMENTOR，这是一种以人为本的、考虑排名因素的事后解释聚合方法，并将其作为Visual Studio Code插件实现。XMENTOR通过运用自适应阈值调整、排名与符号一致性检查以及备用策略等手段，将多种事后解释合并成一个清晰且易于理解的整体视图，从而在不增加用户认知负担的前提下提升了信息的可读性和一致性。

Result: 用户研究显示，接近90%的参与者更偏好聚合后的解释方式，认为这种方式减少了他们面对不同解释时产生的困惑感，同时对于日常工作中遇到的调试和代码缺陷复查任务也给予了更好的支持。

Conclusion: 本研究表明，通过结合不同的解释方法并将之融入到开发人员的工作流程中，可以显著增强机器学习模型的可解释性、实用性和可信度。

Abstract: Machine learning (ML)-based defect prediction models can improve software quality. However, their opaque reasoning creates an HCI challenge because developers struggle to trust models they cannot interpret. Explainable AI (XAI) methods such as LIME, SHAP, and BreakDown aim to provide transparency, but when used together, they often produce conflicting explanations that increase confusion, frustration, and cognitive load. To address this usability challenge, we introduce XMENTOR, a human-centered, rank-aware aggregation method implemented as a VS Code plugin. XMENTOR unifies multiple post-hoc explanations into a single, coherent view by applying adaptive thresholding, rank and sign agreement, and fallback strategies to preserve clarity without overwhelming users. In a user study, nearly 90% of the participants preferred aggregated explanations, citing reduced confusion and stronger support for daily tasks of debugging and review of defects. Our findings show how combining explanations and embedding them into developer workflows can enhance interpretability, usability, and trust.

</details>


### [20] [Automating the Detection of Requirement Dependencies Using Large Language Models](https://arxiv.org/abs/2602.22456)
*Ikram Darif,Feifei Niu,Manel Abdellatif,Lionel C. Briand,Ramesh S.,Arun Adiththan*

Main category: cs.SE

TL;DR: 本文提出了一种基于大型语言模型的方法LEREDD，用于自动检测需求间的依赖关系。通过结合检索增强生成和上下文学习技术，LEREDD能够直接从自然语言需求中识别多种类型的依赖性。实验结果显示，该方法在准确性和F1分数上均优于现有基线，并特别擅长于细粒度依赖类型检测。此外，还提供了一个包含813对需求的标注数据集以支持可复现性和未来研究。


<details>
  <summary>Details</summary>
Motivation: 需求之间存在各种类型的依赖关系，这些依赖关系对于软件开发过程中的决策和活动至关重要。然而，在现代软件系统中，由于需求的数量庞大且复杂，加上自然语言描述的需求含糊不清并频繁变化，使得识别这些依赖关系变得非常困难。这导致了需求依赖检测经常被忽视或手动进行。而大型语言模型（LLM）在自然语言处理方面表现出色，为解决这一问题提供了新的途径。

Method: 提出了名为LEREDD的新方法，它利用检索增强生成（RAG）和上下文内学习（ICL）来自动检测需求之间的依赖关系。该方法旨在直接从自然语言形式的需求文本中识别出不同类型的依赖关系。

Result: 实验评估表明，与两种最先进的基线相比，LEREDD在区分相关和不相关需求时提供了高度准确的结果，达到0.93的准确率和0.84的F1分数；特别是对于非依赖情况下的F1得分为0.96。对于特定类型的依赖检测，如‘需要’依赖类型，LEREDD相较于基线平均提高了94.87%到105.41%的F1得分。

Conclusion: 研究表明，基于大型语言模型的LEREDD方法在自动检测需求依赖方面表现出色，特别是在精细粒度依赖类型的识别上超越了现有的解决方案。同时，公开提供的带有注释的数据集将有助于进一步的研究和发展。

Abstract: Requirements are inherently interconnected through various types of dependencies. Identifying these dependencies is essential, as they underpin critical decisions and influence a range of activities throughout software development. However, this task is challenging, particularly in modern software systems, given the high volume of complex, coupled requirements. These challenges are further exacerbated by the ambiguity of Natural Language (NL) requirements and their constant change. Consequently, requirement dependency detection is often overlooked or performed manually. Large Language Models (LLMs) exhibit strong capabilities in NL processing, presenting a promising avenue for requirement-related tasks. While they have shown to enhance various requirements engineering tasks, their effectiveness in identifying requirement dependencies remains unexplored. In this paper, we introduce LEREDD, an LLM-based approach for automated detection of requirement dependencies that leverages Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL). It is designed to identify diverse dependency types directly from NL requirements. We empirically evaluate LEREDD against two state-of-the-art baselines. The results show that LEREDD provides highly accurate classification of dependent and non-dependent requirements, achieving an accuracy of 0.93, and an F1 score of 0.84, with the latter averaging 0.96 for non-dependent cases. LEREDD outperforms zero-shot LLMs and baselines, particularly in detecting fine-grained dependency types, where it yields average relative gains of 94.87% and 105.41% in F1 scores for the Requires dependency over the baselines. We also provide an annotated dataset of requirement dependencies encompassing 813 requirement pairs across three distinct systems to support reproducibility and future research.

</details>


### [21] [RandSet: Randomized Corpus Reduction for Fuzzing Seed Scheduling](https://arxiv.org/abs/2602.22729)
*Yuchong Xie,Kaikai Zhang,Yu Liu,Rundong Yang,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.SE

TL;DR: RandSet proposes a new randomized corpus reduction method for fuzzers, which enhances seed diversity, improves code coverage, and finds more bugs with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of seed explosion in fuzzing, where managing a large corpus leads to inefficiencies in seed scheduling. Current solutions like cull_queue, AFL-Cmin, and MinSet either compromise on seed diversity or incur significant runtime overhead, necessitating a new approach.

Method: The method involves formulating the corpus reduction as a set cover problem and computing a randomized subset that covers all features of the entire corpus. Seeds are then scheduled from this smaller, randomized subset to mitigate seed explosion, ensuring both diversity and efficiency.

Result: Results show RandSet achieves significantly more diverse seed selection than other reduction techniques, with average subset ratios of 4.03% and 5.99% on standalone and FuzzBench programs. There's a 16.58% coverage gain on standalone programs and up to 3.57% on FuzzBench in AFL++, and it triggers up to 7 more ground-truth bugs on Magma, with an overhead of just 1.17%-3.93%.

Conclusion: RandSet, a novel randomized corpus reduction technique, effectively mitigates seed explosion in fuzzing by reducing the corpus size while maintaining diverse seed selection and with minimal overhead. It outperforms other reduction techniques in terms of coverage, bug-finding capabilities, and introduces only a small overhead.

Abstract: Seed explosion is a fundamental problem in fuzzing seed scheduling, where a fuzzer maintains a huge corpus and fails to choose promising seeds. Existing works focus on seed prioritization but still suffer from seed explosion since corpus size remains huge. We tackle this from a new perspective: corpus reduction, i.e., computing a seed corpus subset. However, corpus reduction could lead to poor seed diversity and large runtime overhead. Prior techniques like cull_queue, AFL-Cmin, and MinSet suffer from poor diversity or prohibitive overhead, making them unsuitable for high-frequency seed scheduling.
  We propose RandSet, a novel randomized corpus reduction technique that reduces corpus size and yields diverse seed selection simultaneously with minimal overhead. Our key insight is introducing randomness into corpus reduction to enjoy two benefits of a randomized algorithm: randomized output (diverse seed selection) and low runtime cost. Specifically, we formulate corpus reduction as a set cover problem and compute a randomized subset covering all features of the entire corpus. We then schedule seeds from this small, randomized subset rather than the entire corpus, effectively mitigating seed explosion.
  We implement RandSet on three popular fuzzers: AFL++, LibAFL, and Centipede, and evaluate it on standalone programs, FuzzBench, and Magma. Results show RandSet achieves significantly more diverse seed selection than other reduction techniques, with average subset ratios of 4.03% and 5.99% on standalone and FuzzBench programs. RandSet achieves a 16.58% coverage gain on standalone programs and up to 3.57% on FuzzBench in AFL++, triggers up to 7 more ground-truth bugs than the state-of-the-art on Magma, while introducing only 1.17%-3.93% overhead.

</details>


### [22] [Productivity and Collaboration in Hybrid Agile Teams: An Interview Study](https://arxiv.org/abs/2602.22835)
*Elisabeth Mo,Jefferson Seide Molléri,Asle Fagerstrøm*

Main category: cs.SE

TL;DR: 本研究通过与三个挪威敏捷团队的九次访谈，探讨了混合工作环境对生产力和协作的影响。研究发现，混合工作减少了非正式互动、造成了参与不均，并增加了对数字工具的依赖。敏捷仪式成为了协调的关键，而信任、沟通和工具支持则调节着团队效能。


<details>
  <summary>Details</summary>
Motivation: 由于疫情后混合工作成为常态，改变了敏捷团队的价值交付、合作及适应方式，本研究旨在探索混合设置如何影响这些团队的生产力和协作。

Method: 采用了定性研究方法，通过对三个挪威敏捷团队进行九次深入访谈来收集数据。

Result: 研究表明，在混合工作模式下，非正式互动减少、成员参与度不均衡且更加依赖数字工具。此外，敏捷实践如日常站会等变成了重要的同步点；团队效能受到信任水平、沟通质量以及技术支持程度的影响。

Conclusion: 混合环境下的敏捷工作是一个正在发展的领域，需要定制化的结构来促进包容性、团队凝聚力和可持续的表现。

Abstract: Hybrid work has become a reality post-pandemic, transforming how Agile teams deliver value, collaborate, and adapt. This study investigate how hybrid settings influence productivity and collaboration through nine interviews with three Norwegian Agile teams. Our findings show that hybrid work reduces informal interaction, creates uneven participation, and increases reliance on digital tools. Agile ceremonies became alignment anchors, while trust, communication, and tool support mediate team effectiveness. Hybrid Agile work is an evolving field that requires tailored structures to support inclusion, team cohesion, and sustainable performance.

</details>


### [23] [CL4SE: A Context Learning Benchmark For Software Engineering Tasks](https://arxiv.org/abs/2602.23047)
*Haichuan Hu,Ye Shang,Guoqing Xie,Congqing He,Quanjun Zhang*

Main category: cs.SE

TL;DR: 本文提出了CL4SE，一个面向软件工程的上下文学习基准，包括四种类型的上下文和对应的任务，并通过大规模数据集评估了五种主流大语言模型的表现，结果显示上下文学习显著提高了各任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究缺乏对软件工程特定上下文类型的系统分类以及专门的基准来量化不同上下文在核心软件工程工作流程中的异质效应。为了解决这个问题，作者们开发了一个全面的基准测试平台CL4SE，旨在探索不同类型的上下文如何影响大型语言模型处理软件工程任务的能力。

Method: 文章定义了一种细粒度的、包含四种软件工程导向上下文类型的分类体系（可解释示例、项目特定上下文、程序决策上下文、正面与负面上下文），每种类型都映射到一个代表性任务上。基于此，从超过30个开源项目中构建了高质量的数据集，包含13,000多个样本，并使用九项指标对五种主流的大规模语言模型进行了评估。

Result: 实验结果表明，上下文学习使所有任务的平均性能提升了24.7%。具体而言，程序上下文将代码审查性能提高了最多33%，混合正负上下文改善了补丁评估准确率高达30%，项目特定上下文增加了代码摘要BLEU分数14.78%，而可解释示例则增强了代码生成PASS@1比率5.72%。

Conclusion: CL4SE建立了首个标准化的软件工程上下文学习评价框架，提供了关于任务特定上下文设计的实际经验见解，并发布了一个大规模数据集以促进该领域的可重复性研究。

Abstract: Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive & negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.

</details>


### [24] [LLM-Powered Silent Bug Fuzzing in Deep Learning Libraries via Versatile and Controlled Bug Transfer](https://arxiv.org/abs/2602.23065)
*Kunpeng Zhang,Dongwei Xiao,Daoyuan Wu,Jiali Zhao,Yuanyi Lin,Tongtong Xu,Shaohua Wang,Shuai Wang*

Main category: cs.SE

TL;DR: 本文提出了一种利用大型语言模型从历史漏洞报告中提取上下文感知的漏洞模式，以进行深度学习库静默漏洞模糊测试的方法。该方法通过功能嵌入匹配语义相关的API，并合成带有定制预言的测试用例，从而实现对静默漏洞的主动检测。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模糊测试技术在检测崩溃方面取得了进展，但它们难以检测静默漏洞，因为缺乏有效的测试程序和相应的预言。

Method: 通过使用大型语言模型从历史问题中提取上下文感知的漏洞模式，基于功能的嵌入来匹配语义相关的应用程序编程接口（API），并合成带有定制预言的测试案例。为确保上下文感知漏洞转移的可靠性，还引入了一个由大型语言模型驱动的自我验证模块，系统地评估每个转移漏洞实例的有效性。

Result: 开发了名为TransFuzz的工具，并在三个主流深度学习库：PyTorch、TensorFlow和MindSpore上进行了评估。TransFuzz成功发现了79个先前未知的漏洞（其中12个被确认为常见漏洞和暴露（CVEs）），覆盖10种类型的漏洞。

Conclusion: 所提出的方法证明了其在迁移深度学习库漏洞发现能力方面的有效性和通用性。

Abstract: Deep learning (DL) libraries are widely used in critical applications, where even subtle silent bugs can lead to serious consequences. While existing DL fuzzing techniques have made progress in detecting crashes, they inherently struggle to detect silent bugs due to the lack of effective test programs and corresponding oracles.
  Building on the observation that historical bug reports contain rich, underutilized information about silent bugs, we leverage large language models (LLMs) to perform versatile yet controlled bug transfer for silent bug fuzzing. Specifically, our approach uses LLMs to extract context-aware bug patterns from historical issues, match semantically related Application Programming Interfaces (APIs) using functionality-based embeddings, and synthesize test cases with customized oracles. This enables proactive detection of silent bugs by transferring high-risk contexts and oracle designs from known buggy APIs to functionally similar target APIs. To ensure the reliability of our context-aware bug transfer, we introduce an LLM-powered self-validation module that systematically evaluates the validity of each transferred bug instance. We implement this methodology in a tool named TransFuzz and evaluate it on three mainstream DL libraries: PyTorch, TensorFlow, and MindSpore. TransFuzz successfully discovers 79 previously unknown bugs (12 confirmed as Common Vulnerabilities and Exposures (CVEs)) in 10 bug types, demonstrating its effectiveness and generalizability in migrating DL library bug discovery capabilities.

</details>


### [25] [Utilizing LLMs for Industrial Process Automation](https://arxiv.org/abs/2602.23331)
*Salim Fares*

Main category: cs.SE

TL;DR: 该研究旨在将大型语言模型（LLMs）应用于工业过程自动化领域，特别是针对专有上下文中使用的高度专业化编程语言，通过解决实际编程任务来加速制造系统的开发周期。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来许多出版物探讨了如何在软件工程中使用大型语言模型（LLMs），但这些工作主要集中在像Python这样的通用编程语言上。对于工业过程自动化领域内使用高度专业化且通常仅限于专有环境的语言来说，LLMs的应用价值尚未得到充分探索。因此，这项研究的目标是填补这一空白，探索如何在工业开发过程中有效利用和整合LLMs，以提高生产效率。

Method: 本摘要未详细说明具体方法。根据研究目标推测，可能涉及的方法包括但不限于：选择合适的LLM作为基础模型；收集或创建特定于工业过程自动化领域的训练数据集；调整或微调选定的LLM以适应专业编程语言的特点；设计实验来评估调整后模型在处理真实世界编程任务时的表现。

Result: 本摘要没有提供具体的结果。预计研究结果将展示经过调整/微调后的LLM能否有效地生成用于工业应用（如机器人手臂移动程序）的代码，并对开发流程产生积极影响。

Conclusion: 虽然摘要中没有明确提到结论部分，但从研究目的来看，预期结论可能会强调LLMs在促进工业过程自动化软件开发方面的作用，以及其对于提高此类系统开发效率的潜力。

Abstract: A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [26] [Enriching Taxonomies Using Large Language Models](https://arxiv.org/abs/2602.22213)
*Zeinab Ghamlouch,Mehwish Alam*

Main category: cs.IR

TL;DR: Taxoria是一个利用大型语言模型来丰富现有分类法的管道，通过提出候选节点、验证和整合来增强分类法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的分类法在信息结构化和分类中起着关键作用，但它们通常覆盖范围有限，存在过时或模糊的节点，影响了知识检索的效果。

Method: Taxoria采用现有分类法作为种子，通过提示大型语言模型（LLMs）建议增补节点，然后对这些候选节点进行验证以减少幻觉并确保语义相关性，最后将它们整合进原始分类法中。

Result: 结果产生了一个经过扩展的分类法，并且提供了来源追踪以及最终合并分类法的可视化展示，以便进一步分析。

Conclusion: Taxoria为改善分类法的质量提供了一种新方法，能够提高其覆盖范围和准确性，从而更有效地支持知识检索任务。

Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichment pipeline that leverages Large Language Models (LLMs) to enhance a given taxonomy. Unlike approaches that extract internal LLM taxonomies, Taxoria uses an existing taxonomy as a seed and prompts an LLM to propose candidate nodes for enrichment. These candidates are then validated to mitigate hallucinations and ensure semantic relevance before integration. The final output includes an enriched taxonomy with provenance tracking and visualization of the final merged taxonomy for analysis.

</details>


### [27] [Adaptive Prefiltering for High-Dimensional Similarity Search: A Frequency-Aware Approach](https://arxiv.org/abs/2602.22214)
*Teodor-Ioan Calin*

Main category: cs.IR

TL;DR: 本研究提出了一种自适应预过滤框架，该框架利用查询频率模式和聚类一致性指标动态分配计算资源。通过将查询空间按照Zipfian分布划分成不同的频率层级，并基于历史访问模式及局部密度特性分配差异化的搜索策略，该方法在保持亚毫秒级延迟的同时减少了20.4%的距离计算量，同时为未见过的查询提供了一致性基础的回退策略。


<details>
  <summary>Details</summary>
Motivation: 传统的高维相似性搜索策略未能充分利用现实世界中查询分布的异质性特点，导致了资源分配上的不效率。为了提高检索系统的性能，需要开发一种能够根据查询频率模式和数据集内部结构动态调整搜索策略的方法。

Method: 研究者们设计了一个自适应预过滤框架，它首先依据Zipfian分布将查询空间分割成多个频率层；然后，根据不同层次的历史访问记录以及局部区域的数据密度特征来制定个性化的搜索计划。此外，该方案还引入了轻量级的频率追踪机制与基于一致性的备用策略以处理新出现的查询请求。

Result: 实验结果表明，在ImageNet-1k数据集上使用CLIP嵌入进行测试时，相比于静态nprobe选择方法，所提出的频率感知预算分配方案能够在保证相同召回率的前提下减少20.4%的距离计算次数，并且在GPU加速的FAISS索引上保持亚毫秒级别的响应时间。

Conclusion: 这项工作展示了一种有效的自适应预过滤技术，能够显著降低大规模高维相似性搜索任务中的计算成本，同时不影响系统性能。通过结合查询频率分析与局部数据结构信息，该方法不仅提高了现有检索系统的效率，也为未来的研究开辟了新的方向。

Abstract: High-dimensional similarity search underpins modern retrieval systems, yet uniform search strategies fail to exploit the heterogeneous nature of real-world query distributions. We present an adaptive prefiltering framework that leverages query frequency patterns and cluster coherence metrics to dynamically allocate computational budgets. Our approach partitions the query space into frequency tiers following Zipfian distributions and assigns differentiated search policies based on historical access patterns and local density characteristics. Experiments on ImageNet-1k using CLIP embeddings demonstrate that frequency-aware budget allocation achieves equivalent recall with 20.4% fewer distance computations compared to static nprobe selection, while maintaining sub-millisecond latency on GPU-accelerated FAISS indices. The framework introduces minimal overhead through lightweight frequency tracking and provides graceful degradation for unseen queries through coherence-based fallback policies.

</details>


### [28] [Retrieval-Augmented Generation Assistant for Anatomical Pathology Laboratories](https://arxiv.org/abs/2602.22216)
*Diogo Pires,Yuriy Perezhohin,Mauro Castelli*

Main category: cs.IR

TL;DR: 本研究提出并评估了一种针对解剖病理学实验室定制的检索增强生成（RAG）助手，旨在为技术人员提供基于上下文的答案以回答与协议相关的问题。通过调整分块策略、检索方法和嵌入模型，发现递归分块和混合检索提供了最强的基础性能。采用生物医学特定的嵌入模型进一步提高了答案的相关性、忠实度以及上下文召回率。


<details>
  <summary>Details</summary>
Motivation: 解剖病理学中准确高效地获取实验室协议至关重要，因为高达70%的医疗决策依赖于实验室诊断。然而，静态文档如打印手册或PDF通常过时、碎片化且难以搜索，这可能导致工作流程错误和诊断延迟。

Method: 研究人员整理了来自葡萄牙一家医疗机构的99个AP协议，并构建了323个问答对进行系统评估。进行了十项实验，变化因素包括分块策略、检索方法及嵌入模型。使用RAGAS框架（忠实度、答案相关性、上下文回忆）以及top-k检索指标来评估性能。

Result: 结果表明，递归分块和混合检索方法提供了最佳的基础性能。结合生物医学专用嵌入模型（MedEmbed）后，答案相关性(0.74)、忠实度(0.70)以及上下文回忆(0.77)均有所提升。Top-k分析显示，仅检索一个最高排名的片段(k=1)能够最大化效率与准确性。

Conclusion: 这些发现强调了在医疗保健领域部署RAG系统的关键设计考虑因素，并展示了它们将静态文档转变为动态可靠的知识助手的潜力，从而提高实验室工作流效率和支持患者安全。

Abstract: Accurate and efficient access to laboratory protocols is essential in Anatomical Pathology (AP), where up to 70% of medical decisions depend on laboratory diagnoses. However, static documentation such as printed manuals or PDFs is often outdated, fragmented, and difficult to search, creating risks of workflow errors and diagnostic delays. This study proposes and evaluates a Retrieval-Augmented Generation (RAG) assistant tailored to AP laboratories, designed to provide technicians with context-grounded answers to protocol-related queries. We curated a novel corpus of 99 AP protocols from a Portuguese healthcare institution and constructed 323 question-answer pairs for systematic evaluation. Ten experiments were conducted, varying chunking strategies, retrieval methods, and embedding models. Performance was assessed using the RAGAS framework (faithfulness, answer relevance, context recall) alongside top-k retrieval metrics. Results show that recursive chunking and hybrid retrieval delivered the strongest baseline performance. Incorporating a biomedical-specific embedding model (MedEmbed) further improved answer relevance (0.74), faithfulness (0.70), and context recall (0.77), showing the importance of domain-specialised embeddings. Top-k analysis revealed that retrieving a single top-ranked chunk (k=1) maximized efficiency and accuracy, reflecting the modular structure of AP protocols. These findings highlight critical design considerations for deploying RAG systems in healthcare and demonstrate their potential to transform static documentation into dynamic, reliable knowledge assistants, thus improving laboratory workflow efficiency and supporting patient safety.

</details>


### [29] [RAGdb: A Zero-Dependency, Embeddable Architecture for Multimodal Retrieval-Augmented Generation on the Edge](https://arxiv.org/abs/2602.22217)
*Ahmed Bin Khalid*

Main category: cs.IR

TL;DR: 本文提出了一种名为RAGdb的新架构，该架构将多模式自动摄入、基于ONNX的提取和混合向量检索整合到一个可移植的SQLite容器中。通过使用确定性的混合评分函数（HSF），它消除了查询时对GPU推理的需求。实验表明，RAGdb在实体检索上达到了100%的召回率，并且与传统的基于Docker的RAG堆栈相比，在增量更新期间具有更高的效率以及显著减少的磁盘占用。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成(RAG)架构已经演变成一个复杂的分布式堆栈，这对边缘计算、隔离环境及隐私受限应用构成了重大障碍。为了解决这个问题并降低入门门槛，作者提出了RAGdb。

Method: RAGdb采用了一种单体架构设计，集成了自动化多模态数据摄取、基于ONNX的特征提取和一种结合了次线性TF-IDF向量化与精确子串提升技术的混合评分函数(HSF)，所有这些功能都封装在一个易于携带的SQLite数据库文件内。

Result: 实验结果证明，RAGdb在普通消费级笔记本电脑上的表现优异，实现了100%的Recall@1指标用于实体检索任务；同时，相较于冷启动，其在增量更新时表现出31.6倍的效率提升；此外，对比典型的基于Docker的RAG解决方案，RAGdb极大地减少了99.5%左右的磁盘空间需求。

Conclusion: RAGdb作为一种创新的单一文件知识容器，为去中心化、本地优先的人工智能提供了可行的技术基础，特别是在那些重视数据主权的应用场景下。

Abstract: Retrieval-Augmented Generation (RAG) has established itself as the standard paradigm for grounding Large Language Models (LLMs) in domain-specific, up-to-date data. However, the prevailing architecture for RAG has evolved into a complex, distributed stack requiring cloud-hosted vector databases, heavy deep learning frameworks (e.g., PyTorch, CUDA), and high-latency embedding inference servers. This ``infrastructure bloat'' creates a significant barrier to entry for edge computing, air-gapped environments, and privacy-constrained applications where data sovereignty is paramount.
  This paper introduces RAGdb, a novel monolithic architecture that consolidates automated multimodal ingestion, ONNX-based extraction, and hybrid vector retrieval into a single, portable SQLite container. We propose a deterministic Hybrid Scoring Function (HSF) that combines sublinear TF-IDF vectorization with exact substring boosting, eliminating the need for GPU inference at query time. Experimental evaluation on an Intel i7-1165G7 consumer laptop demonstrates that RAGdb achieves 100\% Recall@1 for entity retrieval and an ingestion efficiency gain of 31.6x during incremental updates compared to cold starts. Furthermore, the system reduces disk footprint by approximately 99.5\% compared to standard Docker-based RAG stacks, establishing the ``Single-File Knowledge Container'' as a viable primitive for decentralized, local-first AI.
  Keywords: Edge AI, Retrieval-Augmented Generation, Vector Search, Green AI, Serverless Architecture, Knowledge Graphs, Efficient Computing.

</details>


### [30] [Comparative Analysis of Neural Retriever-Reranker Pipelines for Retrieval-Augmented Generation over Knowledge Graphs in E-commerce Applications](https://arxiv.org/abs/2602.22219)
*Teri Rumble,Zbyněk Gazdík,Javad Zarrin,Jagdeep Ahluwalia*

Main category: cs.IR

TL;DR: 本研究设计并比较了多种检索-重排序管道，用于电子商务环境中的知识图谱自然语言查询。实验结果显示，在STaRK半结构化知识库上优化后的RAG管道配置比已发布的基准高出20.4%的Hit@1和14.5%的平均倒数排名（MRR）。


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成（RAG）在处理非结构化文本时表现出色，但在应用于结构化知识图谱时面临挑战，如跨连接图的检索扩展以及在响应生成过程中保持上下文关系。解决这些挑战对于开发特定领域的助手至关重要。

Method: 使用STaRK半结构化知识库（一个生产规模的电子商务数据集），研究人员评估了为语言查询优化的多个RAG管道配置。

Result: 实验结果表明，与已发布的基准相比，实现了20.4%更高的Hit@1和14.5%更高的平均倒数排名（MRR）。

Conclusion: 这项工作建立了一个实用框架，用于将领域特定的半结构化知识库集成到生成系统中，并为准备投入生产的RAG系统的部署提供了可操作见解。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed Natural Language Processing (NLP), enabling complex information retrieval and generation tasks. Retrieval-Augmented Generation (RAG) has emerged as a key innovation, enhancing factual accuracy and contextual grounding by integrating external knowledge sources with generative models. Although RAG demonstrates strong performance on unstructured text, its application to structured knowledge graphs presents challenges: scaling retrieval across connected graphs and preserving contextual relationships during response generation. Cross-encoders refine retrieval precision, yet their integration with structured data remains underexplored. Addressing these challenges is crucial for developing domain-specific assistants that operate in production environments. This study presents the design and comparative evaluation of multiple Retriever-Reranker pipelines for knowledge graph natural language queries in e-Commerce contexts. Using the STaRK Semi-structured Knowledge Base (SKB), a production-scale e-Commerce dataset, we evaluate multiple RAG pipeline configurations optimized for language queries. Experimental results demonstrate substantial improvements over published benchmarks, achieving 20.4% higher Hit@1 and 14.5% higher Mean Reciprocal Rank (MRR). These findings establish a practical framework for integrating domain-specific SKBs into generative systems. Our contributions provide actionable insights for the deployment of production-ready RAG systems, with implications that extend beyond e-Commerce to other domains that require information retrieval from structured knowledge bases.

</details>


### [31] [What Makes an Ideal Quote? Recommending "Unexpected yet Rational" Quotations via Novelty](https://arxiv.org/abs/2602.22220)
*Bowei Zhang,Jin Xiao,Guanglei Yue,Qianyu He,Yanghua Xiao,Deqing Yang,Jiaqing Liang*

Main category: cs.IR

TL;DR: 本文提出了一种基于新颖性的引语推荐框架NovelQR，旨在选择在上下文中具有新颖性但语义连贯的引语。通过实验表明，该系统推荐的引语比其他基线方法更合适、更新颖、更具吸引力。


<details>
  <summary>Details</summary>
Motivation: 现有的引语推荐系统主要优化表面层次的主题相关性，忽视了使引语令人难忘的深层语义和美学属性。研究发现人们偏好那些在特定情境下“出乎意料却又合理”的引语，并且现有模型难以完全理解引语的深层含义。

Method: 受陌生化理论启发，作者们将引语推荐形式化为选取上下文新颖但语义一致的引语任务，并开发了名为NovelQR的新颖性驱动引语推荐框架来实现这一目标。

Result: 实验结果表明，在覆盖不同现实领域中的双语数据集上，人类评委认为该系统推荐的引语比其他基线方法更恰当、更新颖、更吸引人，并且在新颖性估计方面与现有方法相当或超越。

Conclusion: 本文介绍的方法能够有效提高引语推荐的质量，特别是在新颖性和语义一致性方面。

Abstract: Quotation recommendation aims to enrich writing by suggesting quotes that complement a given context, yet existing systems mostly optimize surface-level topical relevance and ignore the deeper semantic and aesthetic properties that make quotations memorable. We start from two empirical observations. First, a systematic user study shows that people consistently prefer quotations that are ``unexpected yet rational'' in context, identifying novelty as a key desideratum. Second, we find that strong existing models struggle to fully understand the deep meanings of quotations. Inspired by defamiliarization theory, we therefore formalize quote recommendation as choosing contextually novel but semantically coherent quotations. We operationalize this objective with NovelQR, a novelty-driven quotation recommendation framework. A generative label agent first interprets each quotation and its surrounding context into multi-dimensional deep-meaning labels, enabling label-enhanced retrieval. A token-level novelty estimator then reranks candidates while mitigating auto-regressive continuation bias. Experiments on bilingual datasets spanning diverse real-world domains show that our system recommends quotations that human judges rate as more appropriate, more novel, and more engaging than other baselines, while matching or surpassing existing methods in novelty estimation.

</details>


### [32] [MoDora: Tree-Based Semi-Structured Document Analysis System](https://arxiv.org/abs/2602.23061)
*Bangrui Xu,Qihang Yao,Zirui Tang,Xuanhe Zhou,Yeye He,Shihan Yu,Qianqian Xu,Bin Wang,Guoliang Li,Conghui He,Fan Wu*

Main category: cs.IR

TL;DR: 提出了MoDora系统，通过局部对齐聚合策略、组件相关树（CCTree）和基于问题类型的检索策略来解决半结构化文档中的自然语言问答挑战。实验表明，MoDora在准确性方面比基线方法高出5.97%至61.07%。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以支持半结构化文档上的自然语言问答，因为OCR提取的元素通常缺乏原始语义环境，现有方法不能有效表示文档内的层次结构或保留布局特定的区别，并且回答问题时需要从文档不同区域或页面中检索并关联相关信息。

Method: 首先采用局部对齐聚合策略将OCR解析的元素转换为具有布局意识的组件，并进行类型特定的信息提取；其次设计了组件相关树(CCTree)以层次组织组件，明确建模组件间关系及布局差异；最后提出了一种基于问题类型的检索策略，支持基于布局的网格分区定位检索以及LLM指导下的语义检索修剪。

Result: 实验结果显示，MoDora在准确度上相比基线提升了5.97%-61.07%。

Conclusion: MoDora有效地解决了半结构化文档中存在的三大技术难题，显著提高了此类文档上自然语言问答任务的表现。

Abstract: Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.
  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.

</details>


### [33] [TWICE: An LLM Agent Framework for Simulating Personalized User Tweeting Behavior with Long-term Temporal Features](https://arxiv.org/abs/2602.22222)
*Bingrui Jin,Kunyao Lan,Mengyue Wu*

Main category: cs.IR

TL;DR: 提出了一个基于大语言模型的框架TWICE，该框架利用社交媒体数据的长期时态和个人化特征来模拟个性化用户发推行为，并能捕捉长期时间特性。实验结果表明，此框架通过有效结合时间动态性提高了个性化用户模拟的效果，为长期行为跟踪提供了有力解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的用户模拟器主要关注集体行为或交互系统，在需要建模时间特征的任务上表现不佳。研究旨在解决这一局限性，提出一种能够模拟个性化用户发推行为同时捕捉到长期时间特性的新方法。

Method: 开发了名为TWICE的框架，它基于大型语言模型（LLM），整合了个性化用户画像、事件驱动的记忆模块以及个性化风格重写的工作流程，用以模拟具有长期时间特征的个性化用户发推行为。

Result: 通过全面评估，特别是在分析发推风格和基于事件的行为变化方面，实验结果显示提出的框架在融合时间动态方面有效地提升了个性化用户模拟的质量。

Conclusion: TWICE框架成功地解决了现有用户模拟器在处理需要建模时间特征任务上的不足，为实现更准确的个性化用户行为模拟及长时间行为追踪提供了一种有效的手段。

Abstract: User simulators are often used to generate large amounts of data for various tasks such as generation, training, and evaluation. However, existing approaches concentrate on collective behaviors or interactive systems, struggling with tasks that require modeling temporal characteristics. To address this limitation, we propose TWICE, an LLM-based framework that leverages the long-term temporal and personalized features of social media data. This framework integrates personalized user profiling, an event-driven memory module, and a workflow for personalized style rewriting, enabling simulation of personalized user tweeting behavior while capturing long-term temporal characteristics. In addition, we conduct a comprehensive evaluation with a focus on analyzing tweeting style and event-based changes in behavior. Experiment results demonstrate that our framework improves personalized user simulation by effectively incorporating temporal dynamics, providing a robust solution for long-term behavior tracking.

</details>


### [34] [SQaLe: A Large Text-to-SQL Corpus Grounded in Real Schemas](https://arxiv.org/abs/2602.22223)
*Cornelius Wolff,Daniel Gomm,Madelon Hulsebos*

Main category: cs.IR

TL;DR: 本文介绍了一个大规模半合成的文本到SQL数据集SQaLe，该数据集基于从真实世界模式集合SchemaPile扩展而来的135,875个关系数据库模式构建而成。通过结合模式采样、问题合成和SQL构造的方法，生成了517,676个高质量的问题-模式-查询三元组。SQaLe数据集捕捉到了现实中的模式大小变化、多样的查询模式以及自然语言模糊性，同时保持执行有效性。


<details>
  <summary>Details</summary>
Motivation: 开发通用化文本到SQL模型的关键瓶颈在于缺乏具有足够模式和查询复杂度、领域覆盖范围及任务多样性的大规模数据集。

Method: 研究者们提出了一种原则性的生成流程，该流程结合了模式采样、问题合成与SQL构造技术，以创建名为SQaLe的大规模半合成文本到SQL数据集。

Result: 最终创建了包含517,676个高质量(问题, 模式, 查询)三元组的数据集SQaLe，该数据集在现实性方面超过了现有的基准测试和其他数据集。

Conclusion: SQaLe数据集为文本到SQL的研究提供了迄今为止最贴近实际的大规模数据支持，并促进了数据规模化与模型泛化的研究愿景。

Abstract: Advances in large language models have accelerated progress in text-to-SQL, methods for converting natural language queries into valid SQL queries. A key bottleneck for developing generalizable text-to-SQL models is the lack of large-scale datasets with sufficient schema and query complexity, domain coverage, and task diversity. We introduce SQaLe: a large-scale semi-synthetic text-to-SQL dataset built on 135,875 relational database schemas expanded from a collection of real-world schemas, SchemaPile. We establish a principled generation pipeline which combines schema sampling, question synthesis, and SQL construction, and produce 517,676 high-quality (question, schema, query) triples. The SQaLe dataset captures realistic schema size variability, diverse query patterns, and natural language ambiguity while maintaining execution validity. We provide an analysis of its contents and characteristics, and find that SQaLe introduces the most realistic large-scale text-to-SQL dataset to date in comparison with existing benchmarks and datasets. We discuss how SQaLe enables our vision for data scaling and model generalization in text-to-SQL research. The dataset is accessible at: https://huggingface.co/datasets/trl-lab/SQaLe-text-to-SQL-dataset.

</details>


### [35] [SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG](https://arxiv.org/abs/2602.22225)
*Xuechen Zhang,Koustava Goswami,Samet Oymak,Jiasi Chen,Nedim Lipka*

Main category: cs.IR

TL;DR: 提出了一种名为SmartChunk检索的查询自适应框架，用于长文档问答。该框架通过预测每个查询的最佳分块抽象级别，并使用轻量级压缩模块生成高级分块嵌入，从而在保持准确性和效率的同时避免了固定策略的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法受限于静态分块和平面检索，导致检索质量高度依赖于分块大小、容易引入无关或误导性信息，并且难以扩展到大型语料库。

Method: 开发了SmartChunk检索框架，包括一个能够预测每个查询最佳分块抽象级别的规划器和一个产生高级分块嵌入而无需重复摘要的轻量级压缩模块。此外，还介绍了一种新的强化学习方案STITCH来提高准确性和泛化能力。

Result: SmartChunk在五个QA基准测试及一个域外数据集上表现优于最先进的RAG基线模型，同时降低了成本。进一步分析显示其在更大规模语料库中具有良好的可扩展性，并在域外数据集上持续获得收益。

Conclusion: SmartChunk作为一种通用的自适应检索框架，在处理不同类型的文档和查询风格时展现出更高的效率与准确性，为长文档问答提供了有效的解决方案。

Abstract: Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, predetermined, fixed-size chunks, embeddings are retrieved uniformly, and generation relies on whatever chunks are returned. This design brings challenges, as retrieval quality is highly sensitive to chunk size, often introduces noise from irrelevant or misleading chunks, and scales poorly to large corpora. We present SmartChunk retrieval, a query-adaptive framework for efficient and robust long-document question answering (QA). SmartChunk uses (i) a planner that predicts the optimal chunk abstraction level for each query, and (ii) a lightweight compression module that produces high-level chunk embeddings without repeated summarization. By adapting retrieval granularity on the fly, SmartChunk balances accuracy with efficiency and avoids the drawbacks of fixed strategies. Notably, our planner can reason about chunk abstractions through a novel reinforcement learning scheme, STITCH, which boosts accuracy and generalization. To reflect real-world applications, where users face diverse document types and query styles, we evaluate SmartChunk on five QA benchmarks plus one out-of-domain dataset. Across these evaluations, SmartChunk outperforms state-of-the-art RAG baselines, while reducing cost. Further analysis demonstrates strong scalability with larger corpora and consistent gains on out-of-domain datasets, highlighting its effectiveness as a general framework for adaptive retrieval.

</details>


### [36] [SEGB: Self-Evolved Generative Bidding with Local Autoregressive Diffusion](https://arxiv.org/abs/2602.22226)
*Yulong Gao,Wan Jiang,Mingzhe Cao,Xuepu Wang,Zeyu Pan,Haonan Yang,Ye Liu,Xin Yang*

Main category: cs.IR

TL;DR: 提出了一种名为Self-Evolved Generative Bidding (SEGB)的框架，通过合成短期未来状态来指导每次出价，并进行价值导向的策略优化。实验表明，SEGB在AuctionNet基准测试和大规模A/B测试中表现优于现有方法，在线部署时目标成本提高了10.19%。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式自动出价策略缺乏对动态市场的前瞻性，且通常依赖于模拟器或外部专家来进行后训练改进。为了解决这些问题，提出了一个全新的自我进化生成出价框架。

Method: 该研究引入了Self-Evolved Generative Bidding (SEGB)框架，它首先合成可能的短期未来状态以引导每个出价决策，然后执行基于价值的策略精炼迭代发现更优策略而无需任何外部干预。

Result: 实验结果表明，SEGB在AuctionNet基准以及大规模A/B测试中均显著优于最先进基线模型。在线上大规模部署期间，SEGB实现了目标成本增加10.19%，证明了其高级规划与进化范式的有效性。

Conclusion: 本研究提出的SEGB框架能够有效提升在线广告中的自动化出价效率，不仅克服了传统方法对于外部输入的依赖性问题，还在实际应用中展示了明显的性能优势。

Abstract: In the realm of online advertising, automated bidding has become a pivotal tool, enabling advertisers to efficiently capture impression opportunities in real-time. Recently, generative auto-bidding has shown significant promise, offering innovative solutions for effective ad optimization. However, existing offline-trained generative policies lack the near-term foresight required for dynamic markets and usually depend on simulators or external experts for post-training improvement. To overcome these critical limitations, we propose Self-Evolved Generative Bidding (SEGB), a framework that plans proactively and refines itself entirely offline. SEGB first synthesizes plausible short-horizon future states to guide each bid, providing the agent with crucial, dynamic foresight. Crucially, it then performs value-guided policy refinement to iteratively discover superior strategies without any external intervention. This self-contained approach uniquely enables robust policy improvement from static data alone. Experiments on the AuctionNet benchmark and a large-scale A/B test validate our approach, demonstrating that SEGB significantly outperforms state-of-the-art baselines. In a large-scale online deployment, it delivered substantial business value, achieving a +10.19% increase in target cost, proving the effectiveness of our advanced planning and evolution paradigm.

</details>


### [37] [RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval](https://arxiv.org/abs/2602.22278)
*Dawei Su,Dongsheng Wang*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架RetLLM，用于在无需训练和数据的情况下查询多模态大语言模型（MLLMs）进行多模态信息检索（MMIR）。通过将MMIR表述为相似度分数生成任务，并采用粗略到精细的流程直接预测检索分数。实验表明，RetLLM在多个基准上优于微调模型，证明了MLLMs在简单可扩展框架下具备强大的多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管最近基于对比微调框架整合多模态大语言模型知识的方法提高了多模态信息检索（MMIR）的表现，但这些方法存在预训练不一致的问题且需要大量数据集。因此，研究者们旨在开发一种既不需要额外训练也不依赖于大规模数据集的新框架，以利用MLLMs的内在多模态推理能力来改进MMIR。

Method: RetLLM框架首先将MMIR任务定义为一个生成相似度分数的任务，并促使MLLMs直接预测检索分数。该过程分为两个阶段：第一阶段是粗略筛选，使用top-k过滤策略构建高质量候选池；第二阶段则是精细化评分，同时输入查询和候选给MLLMs来预测最终的检索得分。此外，引入了一个视觉增强模块帮助MLLMs重新捕捉被忽视的视觉信息。

Result: 广泛的实验结果表明，与经过微调的模型相比，RetLLM在多个MMIR基准测试中表现更优。消融研究表明了每个组件的有效性。

Conclusion: 本研究表明，在没有进行任何训练的情况下，MLLMs能够通过简单的、可扩展的框架实现强大的MMIR性能，这凸显了它们固有的多模态推理能力。

Abstract: Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM

</details>


### [38] [TFPS: A Temporal Filtration-enhanced Positive Sample Set Construction Method for Implicit Collaborative Filtering](https://arxiv.org/abs/2602.22521)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 提出了一种新的基于时间过滤增强的方法（TFPS），用于构建高质量的正样本集，通过时间衰减模型和分层策略来改进推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的负采样策略在优化过程中忽略了对正样本的探索，且去噪推荐方法忽视了时间信息；现有工作在模型聚合期间整合了序列信息但忽略了时间间隔信息，导致难以准确捕捉用户的当前偏好。

Method: 设计了一个基于交互时间间隔的时间衰减模型，并将原始图转换为加权用户-项目二分图；根据预定义的过滤操作对加权用户-项目二分图进行分层；设计了一种层级增强策略，为分层子图构建高质量的正样本集。

Result: 理论分析表明TFPS能够提高Recall@k和NDCG@k的表现，在三个真实世界数据集上的广泛实验验证了所提方法的有效性。此外，TFPS还可以与各种隐式CF推荐者或负采样方法结合使用以提升其表现。

Conclusion: 本研究提出的TFPS方法通过考虑时间因素并采用分层策略有效提高了正样本质量，进而提升了推荐系统的效果。

Abstract: The negative sampling strategy can effectively train collaborative filtering (CF) recommendation models based on implicit feedback by constructing positive and negative samples. However, existing methods primarily optimize the negative sampling process while neglecting the exploration of positive samples. Some denoising recommendation methods can be applied to denoise positive samples within negative sampling strategies, but they ignore temporal information. Existing work integrates sequential information during model aggregation but neglects time interval information, hindering accurate capture of users' current preferences. To address this problem, from a data perspective, we propose a novel temporal filtration-enhanced approach to construct a high-quality positive sample set. First, we design a time decay model based on interaction time intervals, transforming the original graph into a weighted user-item bipartite graph. Then, based on predefined filtering operations, the weighted user-item bipartite graph is layered. Finally, we design a layer-enhancement strategy to construct a high-quality positive sample set for the layered subgraphs. We provide theoretical insights into why TFPS can improve Recall@k and NDCG@k, and extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed method. Additionally, TFPS can be integrated with various implicit CF recommenders or negative sampling methods to enhance its performance.

</details>


### [39] [Generative Agents Navigating Digital Libraries](https://arxiv.org/abs/2602.22529)
*Saber Zerhoudi,Michael Granitzer*

Main category: cs.IR

TL;DR: 本文介绍了一种名为Agent4DL的用户搜索行为模拟器，专为数字图书馆环境设计。它能够生成逼真的用户档案和动态搜索会话，模仿实际的搜索策略。通过与真实用户数据比较验证了其准确性，并且在生成多样化、情境感知的用户行为方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 长期以来，由于隐私问题，公开可用的关于用户搜索模式的数据集稀缺，这一直是数字图书馆研究中的一个挑战。大型语言模型的发展为此提供了新的解决方案。

Method: 提出了Agent4DL，一种专门为数字图书馆环境设计的用户搜索行为模拟器。该模拟器能够创建真实的用户配置文件及动态搜索过程，精确地模仿了查询、点击以及停止等具体到每个用户的行为特征。

Result: 通过与真实用户数据对比验证了Agent4DL的有效性。此外，与其他现有的用户搜索模拟工具（如SimIIR 2.0）相比，Agent4DL在生成更加多样性和上下文敏感性的用户行为方面表现更优。

Conclusion: Agent4DL作为一款创新的用户搜索行为模拟器，在数字图书馆环境中展现了良好的性能，特别是在提高模拟行为的多样性和情境适应性方面。

Abstract: In the rapidly evolving field of digital libraries, the development of large language models (LLMs) has opened up new possibilities for simulating user behavior. This innovation addresses the longstanding challenge in digital library research: the scarcity of publicly available datasets on user search patterns due to privacy concerns. In this context, we introduce Agent4DL, a user search behavior simulator specifically designed for digital library environments. Agent4DL generates realistic user profiles and dynamic search sessions that closely mimic actual search strategies, including querying, clicking, and stopping behaviors tailored to specific user profiles. Our simulator's accuracy in replicating real user interactions has been validated through comparisons with real user data. Notably, Agent4DL demonstrates competitive performance compared to existing user search simulators such as SimIIR 2.0, particularly in its ability to generate more diverse and context-aware user behaviors.

</details>


### [40] [Towards Dynamic Dense Retrieval with Routing Strategy](https://arxiv.org/abs/2602.22547)
*Zhan Su,Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Bingbing Wen,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 本文提出了一种新的密集检索方法，称为动态密集检索（DDR），它使用前缀调整作为特定领域的模块，并通过动态路由策略灵活地适应不同领域。在六个零样本下游任务上的广泛评估表明，该方法仅利用2%的训练参数即可超越现有的密集检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有的密集检索（DR）范式在应用于新任务时需要针对特定任务进行微调，但这种方法存在两个显著局限性：当训练数据集有限时，很难将DR适应到新领域；旧的DR模型过时后，通常会被从头开始训练的新模型取代，这在需要频繁更新模型的情况下成本过高。

Method: 提出了动态密集检索（DDR）这一新型密集检索方法，该方法采用“前缀调整”作为一种专门针对特定领域的模块。这些模块可以通过一种动态路由策略组合起来，从而实现检索部分的高度灵活的领域适应。

Result: 在六个零样本下游任务上进行了广泛的评估，结果显示该方法能够仅用2%的训练参数就超过传统的密集检索方法的表现。

Conclusion: 动态密集检索(DDR)为实现更加灵活的密集检索提供了新的途径，并且对于将密集检索应用于各种任务而言是一个有希望的发展方向。

Abstract: The \textit{de facto} paradigm for applying dense retrieval (DR) to new tasks involves fine-tuning a pre-trained model for a specific task. However, this paradigm has two significant limitations: (1) It is difficult adapt the DR to a new domain if the training dataset is limited.
  (2) Old DR models are simply replaced by newer models that are trained from scratch when the former are no longer up to date. Especially for scenarios where the model needs to be updated frequently, this paradigm is prohibitively expensive. To address these challenges, we propose a novel dense retrieval approach, termed \textit{dynamic dense retrieval} (DDR). DDR uses \textit{prefix tuning} as a \textit{module} specialized for a specific domain. These modules can then be compositional combined with a dynamic routing strategy, enabling highly flexible domain adaptation in the retrieval part. Extensive evaluation on six zero-shot downstream tasks demonstrates that this approach can surpass DR while utilizing only 2\% of the training parameters, paving the way to achieve more flexible dense retrieval in IR. We see it as a promising future direction for applying dense retrieval to various tasks.

</details>


### [41] [Where Relevance Emerges: A Layer-Wise Study of Internal Attention for Zero-Shot Re-Ranking](https://arxiv.org/abs/2602.22591)
*Haodong Chen,Shengyao Zhuang,Zheng Yao,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 本文研究了基于大型语言模型的零样本文档重排序方法，提出了一种选择性ICR策略，通过直接提取内部注意力信号并减少推理延迟30%-50%，在不影响效果的情况下优化了现有ICR方法。实验表明，该方法甚至能让小型模型超越当前基于生成的方法，并重新定义了效率与效果之间的边界。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型（LLMs）的零样本文档重排序方法主要依赖于生成评分或输出logits，这导致了推理延迟和结果一致性的问题。尽管最近提出了In-Context Re-ranking (ICR)作为解决这些问题的一种O(1)复杂度的替代方案，但其尚未充分探索不同层间贡献及其跨架构的一致性。此外，对于不同排名框架下内部注意力机制与传统生成及似然机制之间缺乏统一的研究比较。

Method: 本文对生成、似然以及内部注意力机制在多个排名框架下进行了正交评估，并发现了一个普遍存在的“钟形曲线”分布现象，即相关信号在transformer各层中的分布情况。基于这一发现，作者提出了Selective-ICR策略，它能够显著降低推理延迟而不牺牲性能。

Result: 实验结果显示，在需要大量推理的BRIGHT基准测试中，精确捕捉高质量上下文内注意力信号从根本上减少了对模型扩展和强化学习的需求：一个8亿参数的零样本模型达到了与14亿参数经强化学习训练过的重排序器相当的表现；同时，即使是一个6千万参数的模型也优于最先进的基于生成方法的效果。

Conclusion: 这项研究重新定义了基于LLM的重排序任务中效率与效果之间的界限，揭示了内部信号在处理复杂推理排名任务时所具有的潜在价值。

Abstract: Zero-shot document re-ranking with Large Language Models (LLMs) has evolved from Pointwise methods to Listwise and Setwise approaches that optimize computational efficiency. Despite their success, these methods predominantly rely on generative scoring or output logits, which face bottlenecks in inference latency and result consistency. In-Context Re-ranking (ICR) has recently been proposed as an $O(1)$ alternative method. ICR extracts internal attention signals directly, avoiding the overhead of text generation. However, existing ICR methods simply aggregate signals across all layers; layer-wise contributions and their consistency across architectures have been left unexplored. Furthermore, no unified study has compared internal attention with traditional generative and likelihood-based mechanisms across diverse ranking frameworks under consistent conditions.
  In this paper, we conduct an orthogonal evaluation of generation, likelihood, and internal attention mechanisms across multiple ranking frameworks. We further identify a universal "bell-curve" distribution of relevance signals across transformer layers, which motivates the proposed Selective-ICR strategy that reduces inference latency by 30%-50% without compromising effectiveness. Finally, evaluation on the reasoning-intensive BRIGHT benchmark shows that precisely capturing high-quality in-context attention signals fundamentally reduces the need for model scaling and reinforcement learning: a zero-shot 8B model matches the performance of 14B reinforcement-learned re-rankers, while even a 0.6B model outperforms state-of-the-art generation-based approaches. These findings redefine the efficiency-effectiveness frontier for LLM-based re-ranking and highlight the latent potential of internal signals for complex reasoning ranking tasks. Our code and results are publicly available at https://github.com/ielab/Selective-ICR.

</details>


### [42] [Fine-grained Semantics Integration for Large Language Model-based Recommendation](https://arxiv.org/abs/2602.22632)
*Jiawen Feng,Xiaoyu Kong,Leheng Sheng,Bin Wu,Chao Yi,Feifang Yang,Xiang-Rong Sheng,Han Zhu,Xiang Wang,Jiancan Wu,Xiangnan He*

Main category: cs.IR

TL;DR: 该论文提出了一种名为TS-Rec的方法，通过语义感知的嵌入初始化和标记级语义对齐来解决基于大型语言模型（LLMs）的推荐系统中的两个关键挑战：语义无意义初始化和粗粒度对齐。实验表明，TS-Rec在所有标准指标上都优于传统和生成基线。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的基于LLM的推荐系统已经显示出了性能提升，但在建模SID空间时仍然面临着两个基本问题：1) 语义无意义初始化：SID令牌被随机初始化，切断了SID空间与预训练语言空间之间的语义联系；2) 粗粒度对齐：当前的对齐任务主要集中在项目级别优化，而忽略了SID序列中单个令牌的语义。

Method: 提出了TS-Rec方法，它包含两个关键部分：(1) 语义感知嵌入初始化(SA-Init)，通过使用教师模型提取的关键字的预训练嵌入进行平均池化来初始化SID令牌嵌入；(2) 标记级语义对齐(TS-Align)，将SID序列内的单个令牌与相应项目簇的共享语义对齐。

Result: 广泛的实验证明，在两个真实世界基准测试中，TS-Rec在所有标准指标上一致地优于传统和生成基线。结果表明，整合细粒度语义信息可以显著提高基于LLM的生成式推荐系统的性能。

Conclusion: 通过引入TS-Rec以解决基于LLM的推荐系统中存在的SID空间建模难题，研究成功提升了推荐效果，证明了细粒度语义信息整合的重要性。

Abstract: Recent advances in Large Language Models (LLMs) have shifted in recommendation systems from the discriminative paradigm to the LLM-based generative paradigm, where the recommender autoregressively generates sequences of semantic identifiers (SIDs) for target items conditioned on historical interaction. While prevalent LLM-based recommenders have demonstrated performance gains by aligning pretrained LLMs between the language space and the SID space, modeling the SID space still faces two fundamental challenges: (1) Semantically Meaningless Initialization: SID tokens are randomly initialized, severing the semantic linkage between the SID space and the pretrained language space at start point, and (2) Coarse-grained Alignment: existing SFT-based alignment tasks primarily focus on item-level optimization, while overlooking the semantics of individual tokens within SID sequences.To address these challenges, we propose TS-Rec, which can integrate Token-level Semantics into LLM-based Recommenders. Specifically, TS-Rec comprises two key components: (1) Semantic-Aware embedding Initialization (SA-Init), which initializes SID token embeddings by applying mean pooling to the pretrained embeddings of keywords extracted by a teacher model; and (2) Token-level Semantic Alignment (TS-Align), which aligns individual tokens within the SID sequence with the shared semantics of the corresponding item clusters. Extensive experiments on two real-world benchmarks demonstrate that TS-Rec consistently outperforms traditional and generative baselines across all standard metrics. The results demonstrate that integrating fine-grained semantic information significantly enhances the performance of LLM-based generative recommenders.

</details>


### [43] [Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators](https://arxiv.org/abs/2602.22647)
*Zhengyang Su,Isay Katsman,Yueqi Wang,Ruining He,Lukasz Heldt,Raghunandan Keshavan,Shao-Chuan Wang,Xinyang Yi,Mingyan Gao,Onkar Dalal,Lichan Hong,Ed Chi,Ningren Han*

Main category: cs.IR

TL;DR: 本文提出了一种名为STATIC的技术，这是一种专为基于LLM的生成检索在TPU/GPU上高效执行而设计的约束解码方法。通过将前缀树转换为静态压缩稀疏行矩阵，极大地提高了硬件加速器上的效率。实验表明，该技术在大规模工业视频推荐平台上的应用不仅显著影响了产品指标，而且保持了极低的延迟开销，并且在学术基准测试中也显示了对冷启动性能的改善。


<details>
  <summary>Details</summary>
Motivation: 针对现有自回归解码无法直接支持基于业务逻辑限制输出空间的问题，以及当前利用前缀树进行约束解码的方法在硬件加速器（如TPU/GPU）上存在严重延迟问题，提出了一个更高效、可扩展的解决方案。

Method: 开发了STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding)，一种高效的约束解码技术。它通过将前缀树扁平化为静态压缩稀疏行(CSR)矩阵，使得不规则的树遍历转变为完全矢量化的稀疏矩阵操作，从而在硬件加速器上实现了巨大的效率提升。

Result: 当部署于服务数十亿用户的大型工业视频推荐平台时，STATIC相比CPU实现的速度提升了948倍，比硬件加速的二分查找基线快了47至1033倍。此外，在广泛的实用配置下，STATIC的运行时开销依然非常低。同时，在学术基准测试中，STATIC还能够大幅提高生成检索的冷启动性能。

Conclusion: STATIC是首个能够在生产规模下实现严格约束生成检索部署的技术方案，其在保证几乎无额外延迟的前提下，显著提高了推荐系统的性能和效率。

Abstract: Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.

</details>


### [44] [Generative Recommendation for Large-Scale Advertising](https://arxiv.org/abs/2602.22732)
*Ben Xue,Dan Liu,Lixiang Wang,Mingjie Sun,Peng Wang,Pengfei Zhang,Shaoyun Shi,Tianyu Xu,Yunhao Sha,Zhiqiang Liu,Bo Kong,Bo Wang,Hang Yang,Jieting Xue,Junhao Wang,Shengyu Wang,Shuping Hui,Wencai Ye,Xiao Lin,Yongzhi Li,Yuhang Chen,Zhihui Yin,Quan Chen,Shiyang Wen,Wenjin Wu,Han Li,Guorui Zhou,Changcheng Li,Peng Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种面向生产的生成式推荐系统GR4AD，该系统在架构、学习和服务方面进行了共同设计，以适应大规模广告场景。通过引入UA-SID、LazyAR解码器以及VSL和RSPO等技术手段，在保持效果的同时降低了推理成本，并且实现了在线更新。实验显示相较于现有基于DLRM的方法，GR4AD能够提升多达4.2%的广告收入。


<details>
  <summary>Details</summary>
Motivation: 针对大规模广告系统中实时生成推荐的需求，当前的大语言模型训练和服务方法并不完全适用。因此，需要一种新的解决方案来提高模型的能力同时控制服务成本。

Method: 提出了名为GR4AD（Generative Recommendation for ADdvertising）的生成式推荐系统，其中包括：
- UA-SID（统一广告语义ID），用于捕捉复杂的业务信息。
- LazyAR，一种懒惰自回归解码器，旨在减少短文本多候选生成时的层间依赖性，从而降低推理开销。
- VSL（价值感知监督学习）与RSPO（排名引导的softmax偏好优化算法），后者是一种列表级强化学习算法，根据列表级别的指标优化基于价值的奖励。
- 动态束搜索服务策略，根据生成级别和在线负载调整束宽度以控制计算量。

Result: 大型在线A/B测试表明，与现有的基于DLRM的堆栈相比，GR4AD最多可提高4.2%的广告收入。此外，无论是在模型扩展还是推理时间扩展方面都取得了持续收益。

Conclusion: GR4AD作为一个专为广告设计的生产导向型生成式推荐系统，已经在拥有超过4亿用户的快手广告系统中全面部署，并实现了高吞吐量的实时服务。

Abstract: Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.

</details>


### [45] [PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA](https://arxiv.org/abs/2602.22903)
*Yunpeng Hong,Chenyang Bu,Jie Zhang,Yi He,Di Wu,Xindong Wu*

Main category: cs.IR

TL;DR: 本文提出了一种名为PSQE（伪种子质量增强）的方法，通过多模态信息和聚类重采样来提高伪种子的精度和平衡量，从而改善无监督多模态实体对齐的效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态实体对齐中由于缺乏标注种子对而转向无监督范式时遇到的问题，特别是多模态信息引入导致伪种子在知识图谱中的覆盖不均衡问题。

Method: 提出了PSQE方法，该方法利用多模态信息与聚类-重采样技术来提升伪种子的质量及其在图中的分布平衡性。

Result: 实验结果验证了理论分析，并表明PSQE作为即插即用模块能够显著提高基线模型的表现。

Conclusion: 通过改进伪种子的质量和平衡性，可以有效提升基于对比学习的多模态实体对齐模型性能，尤其是在处理稀疏区域内的实体时。

Abstract: Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporation of multimodal information often results in imbalanced coverage of pseudo-seeds within the knowledge graph. To overcome this, we propose PSQE (Pseudo-Seed Quality Enhancement) to improve the precision and graph coverage balance of pseudo seeds via multimodal information and clustering-resampling. Theoretical analysis reveals the impact of pseudo seeds on existing contrastive learning-based MMEA models. In particular, pseudo seeds can influence the attraction and the repulsion terms in contrastive learning at once, whereas imbalanced graph coverage causes models to prioritize high-density regions, thereby weakening their learning capability for entities in sparse regions. Experimental results validate our theoretical findings and show that PSQE as a plug-and-play module can improve the performance of baselines by considerable margins.

</details>


### [46] [SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress](https://arxiv.org/abs/2602.22913)
*Yang Yu,Lei Kou,Huaikuan Yi,Bin Chen,Yayu Cao,Lei Shen,Chao Zhang,Bing Wang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 本文提出了SIGMA，一种基于语义和指令驱动的多任务生成式推荐系统，旨在解决现有推荐方法难以快速适应变化趋势和满足多样化推荐需求的问题。通过统一潜在空间、混合项目标记化方法、大规模多任务微调数据集以及自适应概率融合机制，SIGMA能够更准确地处理各种推荐任务。离线实验和在线A/B测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，生成式推荐正在逐渐改变推荐系统的模式。但现有的大多数方法仍然局限于基于交互的下一项预测范式内，无法迅速适应不断演变的趋势或在现实场景中满足多样化的推荐任务及特定业务需求。

Method: 1. 通过一个既能捕捉语义又能反映协作关系的统一潜在空间来锚定一般语义中的项目实体。
2. 开发了一种混合项目标记化方法以实现精确建模和高效生成。
3. 构建了一个大规模多任务监督微调（SFT）数据集，使SIGMA能够通过遵循指示来完成各种推荐需求。
4. 设计了一个结合了适应性概率融合机制的三步项目生成过程，根据特定任务要求调整输出分布，从而提高推荐的准确性和多样性。

Result: 广泛的离线实验与在线A/B测试表明，SIGMA在处理不同类型的推荐任务时表现出色，并且能够有效提升推荐结果的质量。

Conclusion: SIGMA为应对快速变化的趋势并满足实际应用中多样化的推荐需求提供了一种创新解决方案。它通过引入基于语义和指令驱动的方法，在保持推荐准确性的同时增强了系统的灵活性和适用范围。

Abstract: With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.

</details>


### [47] [Sequential Regression for Continuous Value Prediction using Residual Quantization](https://arxiv.org/abs/2602.23012)
*Runpeng Cui,Zhipeng Sun,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 提出了一种基于残差量化（RQ）的序列学习框架，用于连续值预测任务，如用户观看时长、电商交易GMV等。该方法通过递归地从粗到细预测有序量化码来表示目标连续值，并引入了一个表征学习目标以优化预测准确性。在多个公共基准测试和大规模在线实验中，该方法表现出优于现有技术的效果，并具有强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法依赖于严格的参数分布假设，在处理复杂且长尾的数据分布时性能受限。这些方法要么过于简化无法准确建模现实世界的复杂性，要么过于复杂导致可扩展性和泛化能力不佳。为了解决工业级推荐系统中的连续值预测挑战，提出了新的解决方案。

Method: 开发了一种基于残差量化(RQ)的序列学习框架，其中目标连续值被表示为一系列有序量化码之和，这些量化码按照从粗到细的顺序递归预测，同时减少量化误差。此外，还引入了对齐RQ码嵌入空间与目标值序数结构的表征学习目标，进一步提高了预测精度。

Result: 在LTV和观看时间预测的公开基准测试以及一个针对短视频推荐平台GMV预测的大规模在线实验中，所提出的方法均表现出色，超越了当前最先进的方法，并且在多样化的连续值预测任务上展示了强大的泛行能力。

Conclusion: 本研究提出的基于RQ的序列学习框架有效解决了连续值预测问题，特别是在面对复杂及长尾数据分布时表现优异。它不仅提高了预测准确性，也展现了良好的跨任务泛化性能。

Abstract: Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.
  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.

</details>


### [48] [MaRI: Accelerating Ranking Model Inference via Structural Re-parameterization in Large Scale Recommendation System](https://arxiv.org/abs/2602.23105)
*Yusheng Huang,Pengbo Xu,Shen Wang,Changxin Lao,Jiangxia Cao,Shuang Wen,Shuang Yang,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 本文提出了一种名为MaRI的矩阵重新参数化推理框架，旨在通过优化特征融合矩阵乘法来加速排名模型的推理过程，同时不会损失任何准确性。


<details>
  <summary>Details</summary>
Motivation: 当前用于加速排名模型（如粗排和细排模型）的方法通常会导致准确性的显著下降。本文注意到在特征融合矩阵乘法中用户侧计算存在冗余，并希望通过结构重新参数化来减少这种冗余，从而实现无损加速。

Method: 提出了MaRI框架，利用结构重新参数化的理念针对特征融合矩阵乘法中的用户侧计算冗余问题进行优化。该方法作为现有技术的一种补充手段，专注于在不牺牲准确性的情况下加快排名模型的推理速度。

Result: MaRI成功地在不引起任何准确性损失的前提下，实现了对排名模型推理过程的有效加速。

Conclusion: MaRI提供了一种新颖且有效的途径来解决大规模推荐系统中排名模型加速与保持高精度之间的矛盾，为未来的研究开辟了新的方向。

Abstract: Ranking models, i.e., coarse-ranking and fine-ranking models, serve as core components in large-scale recommendation systems, responsible for scoring massive item candidates based on user preferences. To meet the stringent latency requirements of online serving, structural lightweighting or knowledge distillation techniques are commonly employed for ranking model acceleration. However, these approaches typically lead to a non-negligible drop in accuracy. Notably, the angle of lossless acceleration by optimizing feature fusion matrix multiplication, particularly through structural reparameterization, remains underexplored. In this paper, we propose MaRI, a novel Matrix Re-parameterized Inference framework, which serves as a complementary approach to existing techniques while accelerating ranking model inference without any accuracy loss. MaRI is motivated by the observation that user-side computation is redundant in feature fusion matrix multiplication, and we therefore adopt the philosophy of structural reparameterization to alleviate such redundancy.

</details>


### [49] [From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2602.23132)
*Ruochen Yang,Xiaodong Li,Jiawei Sheng,Jiangxia Cao,Xinkui Lin,Shen Wang,Shuang Yang,Zhaojie Liu,Tingwen Liu*

Main category: cs.IR

TL;DR: 提出了一种基于扩散模型的框架FatsMB，通过从行为无关到行为特定在潜在空间中引导偏好生成，以实现多样且准确的多行为序列推荐。


<details>
  <summary>Details</summary>
Motivation: 当前的多行为序列推荐方法忽略了用户决策背后的潜在偏好，并且由于项目和行为之间的不对称确定性，基于偏好评分的区分范式无法有效捕捉从低熵行为到高熵项目的不确定性，导致推荐效率低下且多样性不足。

Method: 设计了一个多行为自动编码器（MBAE）来构建统一的用户潜在偏好空间，促进了跨行为间的交互与协作；采用了行为感知RoPE（BaRoPE）进行多信息融合；并在潜在空间内执行针对目标行为的具体偏好转移，同时引入了多条件引导层归一化（MCGLN）来进行去噪处理。

Result: 在真实世界数据集上的广泛实验表明，该模型相比现有方法具有更高的有效性。

Conclusion: 通过提出的FatsMB框架，在多行为序列推荐任务上取得了显著的效果提升，为用户提供更高效、更多样化的推荐体验。

Abstract: Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \textbf{FatsMB}, a framework based diffusion model that guides preference generation \textit{\textbf{F}rom Behavior-\textbf{A}gnostic \textbf{T}o Behavior-\textbf{S}pecific} in latent spaces, enabling diverse and accurate \textit{\textbf{M}ulti-\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.

</details>


### [50] [Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments](https://arxiv.org/abs/2602.23234)
*Evangelia Christakopoulou,Vivekkumar Patel,Hemanth Velaga,Sandip Gaikwad*

Main category: cs.IR

TL;DR: 该论文通过系统评估LLM配置，发现一个专门的、微调过的模型在提供高度相关的标签方面显著优于一个更大的预训练模型。利用这个最优模型生成数百万文本相关性标签，以克服数据稀缺问题。将这些文本相关性标签加入到生产排序器中后，离线NDCG在行为相关性和文本相关性上都有所提高。全球A/B测试表明转化率有统计学意义的+0.24%增长，尤其是在尾部查询中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 面对专家提供的文本相关性标签相对稀缺而行为相关性标签丰富的挑战，研究旨在通过增强文本相关性来最大化搜索系统的相关性。

Method: 首先系统地评估了不同的LLM（大语言模型）配置，确定了一个特别调整过的模型比一个更大规模的预训练模型更有效于生成高质量的相关性标签；然后使用此模型生成大量的文本相关性标签，并将其整合进现有的排序算法中。

Result: 实验结果表明，在加入新的文本相关性标签之后，无论是离线评价指标NDCG还是实际应用中的转化率都得到了显著提升，特别是在处理那些缺乏足够行为数据支持的长尾查询时效果尤为明显。

Conclusion: 通过引入由优化后的LLM生成的大规模文本相关性标签，能够有效改善搜索结果的质量，尤其在处理长尾查询方面具有显著优势，从而为用户提供更好的搜索体验。

Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [51] [Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads](https://arxiv.org/abs/2602.22299)
*Kunpeng Zhang,Poppy Zhang,Shawndra Hill,Amel Awadelkarim*

Main category: cs.MM

TL;DR: 本研究提出了一种基于多模态大语言模型（MLLMs）的框架，用于分析视频广告的'吸引期'（前三秒），通过两种帧采样策略和BERTopic技术进行高级抽象，结合音频属性等特征对实际数据进行了验证，展示了该方法在优化视频广告策略方面的实用性和预测能力。


<details>
  <summary>Details</summary>
Motivation: 视频广告是品牌与消费者互动的重要媒介，但其关键的'吸引期'（即前三秒）如何有效吸引观众注意力并影响参与度尚未得到充分探索。传统的分析方法难以捕捉到视频内容中视觉、听觉和文本元素之间微妙的相互作用，需要更先进的框架来进行全面评估。

Method: 研究采用了一种基于变换器的多模态大语言模型（MLLMs）来分析视频广告的吸引期。实验测试了均匀随机采样和关键帧选择两种帧采样策略，以确保声学特征提取既平衡又具代表性。使用最先进的MLLMs处理吸引期视频，生成描述性分析，并利用BERTopic技术提炼成连贯的主题。此外，该框架还整合了音频属性及广告定位信息等特征，丰富了进一步分析的特征集。

Result: 通过对社交媒体平台上的大规模真实世界数据进行实证验证，证明了所提框架的有效性，揭示了吸引期特性与关键性能指标如每投资转化率之间的相关性。结果突出了该方法在视频广告策略优化中的实用价值和预测能力。

Conclusion: 这项研究通过提供一种可扩展的方法论来理解并增强视频广告的初始时刻，在视频广告分析领域取得了进展，为优化视频广告策略提供了有价值的见解。

Abstract: Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagement metrics. Analyzing this brief window is challenging due to the multimodal nature of video content, which blends visual, auditory, and textual elements. Traditional methods often miss the nuanced interplay of these components, requiring advanced frameworks for thorough evaluation.
  This study presents a framework using transformer-based multimodal large language models (MLLMs) to analyze the hooking period of video ads. It tests two frame sampling strategies, uniform random sampling and key frame selection, to ensure balanced and representative acoustic feature extraction, capturing the full range of design elements. The hooking video is processed by state-of-the-art MLLMs to generate descriptive analyses of the ad's initial impact, which are distilled into coherent topics using BERTopic for high-level abstraction. The framework also integrates features such as audio attributes and aggregated ad targeting information, enriching the feature set for further analysis.
  Empirical validation on large-scale real-world data from social media platforms demonstrates the efficacy of our framework, revealing correlations between hooking period features and key performance metrics like conversion per investment. The results highlight the practical applicability and predictive power of the approach, offering valuable insights for optimizing video ad strategies. This study advances video ad analysis by providing a scalable methodology for understanding and enhancing the initial moments of video advertisements.

</details>


### [52] [MViR: Multi-View Visual-Semantic Representation for Fake News Detection](https://arxiv.org/abs/2602.22944)
*Haochen Liang,Xinqi Su,Jun Wang,Chaomeng Chen,Zitong Yu*

Main category: cs.MM

TL;DR: 提出了一种多视角视觉-语义表示(MViR)框架来检测虚假新闻，通过捕捉和融合多视角的视觉-语义特征以提高检测准确性。实验表明MViR在基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理虚假新闻时往往忽视了新闻中图像与文本间不同视角的视觉-语义方面，因此需要一种能有效结合这些多视角信息的方法来改善检测效果。

Method: 开发了一个名为MViR的框架，它包括使用金字塔膨胀卷积来获取多视角视觉-语义特征的模块、一个多视角特征融合模块以及多个聚合器用于提取有助于检测的多视角语义线索。

Result: 在标准数据集上的实验结果显示，所提出的MViR框架相比其他方法具有更好的性能。

Conclusion: MViR框架通过有效地整合多视角视觉-语义信息，在虚假新闻检测领域展现出了显著的优势。

Abstract: With the rise of online social networks, detecting fake news accurately is essential for a healthy online environment. While existing methods have advanced multimodal fake news detection, they often neglect the multi-view visual-semantic aspects of news, such as different text perspectives of the same image. To address this, we propose a Multi-View Visual-Semantic Representation (MViR) framework. Our approach includes a Multi-View Representation module using pyramid dilated convolution to capture multi-view visual-semantic features, a Multi-View Feature Fusion module to integrate these features with text, and multiple aggregators to extract multi-view semantic cues for detection. Experiments on benchmark datasets demonstrate the superiority of MViR. The source code of FedCoop is available at https://github.com/FlowerinZDF/FakeNews-MVIR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: 本文提出了一种名为AOT（对抗对手训练）的自博弈框架，通过创建自己的训练数据来增强多模态大语言模型(MLLM)的鲁棒性。同时，还引入了大规模对抗数据集AOT-SFT以提升MLLM在视觉复杂场景中的感知鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型(MLLMs)能力强大，但在面对视觉上复杂的场景时表现出感知脆弱性。这种弱点源于对有限训练数据集的依赖，这些数据集扩展成本高昂且限制了模型鲁棒性的上限。

Method: 作者们首先提出了AOT-SFT，一个用于促进MLLM鲁棒性的大型对抗数据集。基于此，他们设计了AOT（对抗对手训练），一种自博弈框架，其中图像编辑攻击者和防御者MLLM之间形成共进化过程。攻击者生成一系列多样化的图像操作作为挑战，迫使防御者适应并改进其性能。

Result: 广泛的实验表明，AOT能够显著提高防御者的感知鲁棒性，并减少幻觉现象的发生，为训练更可靠的MLLM提供了一个可扩展的范例。

Conclusion: 通过AOT-SFT数据集与AOT框架的应用，本研究为解决MLLM在复杂视觉场景下的脆弱性问题开辟了新路径，证明了所提方法在提升模型鲁棒性和可靠性方面的有效性。

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [54] [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)
*Alex Morehead,Miruna Cretu,Antonia Panescu,Rishabh Anand,Maurice Weiler,Tynan Perez,Samuel Blau,Steven Farrell,Wahid Bhimji,Anubhav Jain,Hrushikesh Sahasrabuddhe,Pietro Lio,Tommi Jaakkola,Rafael Gomez-Bombarelli,Rex Ying,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 本文介绍了Zatom-1，一个统一了3D分子和材料的生成与预测学习的基础模型。它通过多模态流匹配目标训练而成，同时建模离散原子类型和连续3D几何结构，从而支持可扩展预训练并加速采样过程。实验表明，该模型在生成和预测基准上表现优秀，并且能够显著减少生成推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有的AI方法大多针对单一领域（分子或材料）和单一任务（生成或预测），这限制了表示共享和迁移能力的发展。为了克服这一局限性，研究者们开发了Zatom-1模型来统一3D化学建模中的生成和预测任务。

Method: Zatom-1是一个基于Transformer架构的模型，采用了多模态流匹配目标进行训练，可以同时处理离散的原子类型信息以及连续的三维几何形状数据。此外，还利用联合生成预训练作为下游多任务属性、能量及力预测的基础初始化步骤。

Result: 实验结果显示，Zatom-1不仅在生成和预测任务上达到了与专门基线相当甚至更好的性能，而且将生成推理所需时间减少了超过一个数量级。值得注意的是，从材料到分子性质预测的跨化学领域正向转移也得到了验证。

Conclusion: Zatom-1模型成功地为3D分子和材料提供了统一的生成与预测学习框架，展示了良好的泛化能力和效率提升潜力。

Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>


### [55] [Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function](https://arxiv.org/abs/2602.22255)
*Ahmed Nebli,Hadi Saadatdoorabi,Kevin Yam*

Main category: cs.LG

TL;DR: 本文介绍了一个序列建模框架，其中潜在状态是在一个有限维希尔伯特空间下由学习得到的时间依赖哈密顿量演化的复值波函数。该框架利用量子干涉来处理竞争假设，并通过Born规则提取标记概率。主要理论贡献是证明了这种读出方式的表示优势：对于一类消歧任务，复数幺正模型能够以N维状态精确解决，而实数正交模型则需要Ω(N^2)的状态维度。


<details>
  <summary>Details</summary>
Motivation: 作者提出了一种新的序列建模方法，旨在利用量子力学中的概念（如量子干涉和幺正变换）来改进传统递归架构在处理竞争性假设时的表现。相比依靠门控机制的传统模型，新框架能更有效地抑制冲突解释并增强兼容解释。

Method: 引入一种基于复值波函数序列建模的新框架，该波函数在一个有限维希尔伯特空间中根据学习到的时间相关哈密顿量演化。使用Cayley (Crank--Nicolson)离散化确保每一步的状态范数被严格保持。通过Born规则从波函数中提取标记概率。

Result: 研究表明，对于特定类型的消歧任务，所提出的复数幺正模型可以以较低的维度实现与高维实数正交模型相同的性能。此外，还推导出了潜变量概率质量的连续方程，提供了追踪不同维度间信息流的方法。

Conclusion: 该研究展示了如何将量子力学的概念应用于序列建模问题，并证明了这样做可以在某些情况下提供表示能力上的显著优势。

Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, our framework utilizes quantum interference: the Hamiltonian steers the phases of complex amplitudes so that conflicting interpretations cancel while compatible ones reinforce. The dynamics are strictly unitary, ensuring that the state norm is preserved exactly at every time step via a Cayley (Crank--Nicolson) discretization. Token probabilities are extracted using the Born rule, a quadratic measurement operator that couples magnitudes and relative phases. Our primary theoretical contribution is a separation theorem characterizing the representational advantage of this readout: we define a family of disambiguation tasks that a complex unitary model of dimension $N$ solves exactly, but which requires a state dimension of $Ω(N^2)$ for any real-valued orthogonal model equipped with a standard affine-softmax readout. This quadratic gap arises because the Born rule implicitly lifts the $N$-dimensional state into the space of rank-one Hermitian matrices, accessing pairwise phase correlations that are inaccessible to linear projections. Finally, we derive a continuity equation for the latent probability mass, yielding conserved pairwise currents that serve as a built-in diagnostic for tracing information flow between dimensions.

</details>


### [56] [Code World Models for Parameter Control in Evolutionary Algorithms](https://arxiv.org/abs/2602.22260)
*Camilo Chacón Sartori,Guillem Rodríguez Corominas*

Main category: cs.LG

TL;DR: 本文扩展了代码世界模型(CWMs)，使其能够预测随机组合优化中的环境动态。通过给定次优轨迹，LLM可以合成优化器动态的模拟器，并据此选择每一步的变异强度k。实验结果表明，在多种测试环境下，CWM-greedy方法表现优异，不仅在性能上接近理论最优策略，而且在样本效率、成功率和泛化能力方面超越了其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索大型语言模型（LLM）是否能够学习并理解优化器的行为模式，并利用这种知识来控制优化过程。特别是针对随机组合优化问题，试图通过LLM生成的程序来模拟优化器的动力学特性，从而实现对优化过程的有效干预与改进。

Method: 采用的方法是基于Code World Models (CWMs)框架，该框架允许LLM生成Python程序用以预测环境动态。研究者将这一概念从确定性游戏领域拓展到了随机组合优化领域。通过对(1+1)-RLS_k算法产生的次优路径进行分析，让LLM构建出一个关于优化器行为的仿真器；然后通过这个仿真器上的贪婪规划来决定每一步的突变强度k值。

Result: 结果显示，在\lo{} 和 \onemax{} 上，CWM-greedy 方法的表现与理论上最优策略相差不超过6%；对于具有欺骗性的山谷特征导致所有自适应基线失败的情况(\jump{$_k$})，CWM-greedy 达到了100%的成功率；在NK-Landscape上，当提示中包含经验转移统计数据时，CWM-greedy 在十五个独立生成实例上的表现优于所有基准；此外，CWM 还在样本效率、成功率及泛化能力方面优于DQN。

Conclusion: 结论指出，使用LLM合成的代码世界模型能够有效地模拟并控制随机组合优化过程中优化器的行为，且在多个标准测试问题上展示了比现有方法更好的性能，特别是在处理未知或复杂环境中展现出了显著优势。

Abstract: Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.

</details>


### [57] [Sustainable LLM Inference using Context-Aware Model Switching](https://arxiv.org/abs/2602.22261)
*Yuvarani,Akashdeep Singh,Zahra Fathanah,Salsabila Harlen,Syeikha Syafura Al-Zahra binti Zahari,Hema Subramaniam*

Main category: cs.LG

TL;DR: 提出了一种基于上下文感知的模型切换方法，以根据查询复杂度动态选择合适的语言模型，从而减少能源消耗并保持响应质量。实验结果表明，与始终使用最大模型相比，该方法可将能耗降低67.5%，同时对于简单查询的响应时间提高了约68%。


<details>
  <summary>Details</summary>
Motivation: 当前AI应用中大型语言模型的能源消耗问题日益严重，现有的统一推理策略导致了大量不必要的能源浪费。为了解决这一问题，并朝着更加节能和可持续的人工智能系统发展，提出了新的方法。

Method: 采用了一种名为“面向能效的语言模型切换”的方法，结合缓存重复查询、基于规则的复杂度评分、机器学习分类来捕捉语义意图以及一个随着时间从交互模式中学习的用户自适应组件。

Result: 实验评估显示，提出的模型切换方法相较于总是使用最大的模型能够降低高达67.5%的能量消耗，在维持93.6%响应质量的同时，对简单查询的响应时间也显著提高了大约68%。

Conclusion: 研究证明了通过动态选择适合任务复杂性的语言模型，可以在不牺牲响应质量的前提下大幅提高能源效率，为实现更节能且可持续发展的AI系统提供了可行且可扩展的方法路径。

Abstract: Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.

</details>


### [58] [Entropy-Controlled Flow Matching](https://arxiv.org/abs/2602.22265)
*Chika Maduabuchi*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法——熵控制流匹配（ECFM），通过在连续方程路径上施加全局熵率预算约束，以直接控制轨迹的信息几何。该方法在Wasserstein空间中是凸优化问题，并且当λ趋于0时收敛于经典最优传输。此外，还提供了模式覆盖和密度下限的保证，并构造了无约束流匹配的近似最优反例。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉生成器在实证表现上很强，但标准的流匹配目标并不直接控制轨迹的信息几何，导致可能出现低熵瓶颈，短暂耗尽语义模式。为了解决这个问题，提出了熵控制流匹配（ECFM）方法。

Method: ECFM是在连续方程路径上的一个受约束变分原理，强制执行全局熵率预算d/dt H(μ_t) >= -λ。该方法在Wasserstein空间中表现为凸优化问题，并拥有KKT/庞特里亚金系统。此外，它还具有等价于薛定谔桥的随机控制表示，并带有显式的熵乘数。

Result: 研究显示，在纯运输状态下，ECFM能够恢复熵OT测地线，并且随着λ趋向于0时Γ-收敛到经典OT。此外，本研究还获得了关于模式覆盖和密度下限的证书式保证以及利普希茨稳定性，并为无约束流匹配构建了接近最优的崩溃反例。

Conclusion: 熵控制流匹配（ECFM）提供了一种有效的方法来改善视觉生成器中的信息几何控制问题，确保了更高的模式多样性和稳定性。

Abstract: Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.

</details>


### [59] [Data-Driven Supervision of a Thermal-Hydraulic Process Towards a Physics-Based Digital Twin](https://arxiv.org/abs/2602.22267)
*Osimone Imhogiemhe,Yoann Jus,Hubert Lejeune,Saïd Moussaoui*

Main category: cs.LG

TL;DR: 本文开发了一个数字孪生系统，用于热力液压过程监控中的故障检测和诊断。该系统结合了数值模拟与机器学习方法，并在特定测试场景下验证了其有效性，展示了良好的参数变化定位及更新准确性。


<details>
  <summary>Details</summary>
Motivation: 实时监控生产过程是多个行业面临的共同挑战，旨在确保安全、持续生产和维持高效率。随着物理系统仿真工具和数据驱动的机器学习模型的进步，设计高效的系统监控数值工具成为可能。本文旨在利用数字孪生概念开发一个专注于故障检测和诊断的系统。

Method: 基于系统的数值模拟以及机器学习方法，提出了不同的模块来专门处理过程参数变化检测及其在线估计。

Result: 所提出的故障检测和诊断算法在一个特定测试场景下得到了验证，该场景中系统参数发生了单次变化。数值结果表明，在参数变化定位及其值更新方面具有良好的准确性。

Conclusion: 本研究成功地为热力液压过程监控开发了一个数字孪生系统，能够有效地进行故障检测和诊断。通过结合数值模拟和机器学习技术，该系统在处理过程参数变化检测及在线估计方面展现出了显著的优势。

Abstract: The real-time supervision of production processes is a common challenge across several industries. It targets process component monitoring and its predictive maintenance in order to ensure safety, uninterrupted production and maintain high efficiency level. The rise of advanced tools for the simulation of physical systems in addition to data-driven machine learning models offers the possibility to design numerical tools dedicated to efficient system monitoring. In that respect, the digital twin concept presents an adequate framework that proffers solution to these challenges. The main purpose of this paper is to develop such a digital twin dedicated to fault detection and diagnosis in the context of a thermal-hydraulic process supervision. Based on a numerical simulation of the system, in addition to machine learning methods, we propose different modules dedicated to process parameter change detection and their on-line estimation. The proposed fault detection and diagnosis algorithm is validated on a specific test scenario, with single one-off parameter change occurrences in the system. The numerical results show good accuracy in terms of parameter variation localization and the update of their values.

</details>


### [60] [AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2602.22268)
*Changhai Zhou,Shiyang Zhang,Yuhua Zhou,Qian Qiao,Jun Gao,Cheng Jin,Kaizhou Qin,Weizhong Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为AutoQRA的联合优化框架，用于在混合量化微调过程中同时优化每一层的比特宽度和LoRA秩配置。通过两阶段优化过程解决了大规模离散搜索空间和频繁微调迭代带来的高评估成本问题，实验表明该方法能在保持与全精度微调相近性能的同时，内存占用量与统一4位方法相当。


<details>
  <summary>Details</summary>
Motivation: 当前的顺序处理流程（先量化再进行参数高效微调）未能充分利用量化比特宽度和LoRA秩之间的复杂交互作用，导致即使是在量化误差很低的情况下也未必能获得良好的微调效果。不同比特宽度和秩配置在同一内存预算下会导致显著不同的结果。

Method: 开发了AutoQRA框架，采用全局多保真进化搜索初始化候选配置，并通过信任区域贝叶斯优化进一步精细化搜索最佳配置。整个过程分为两个阶段：1) 利用特定算子及性能模型对候选配置进行筛选；2) 在给定内存预算条件下，局部优化有潜力的搜索区域来识别最优解。

Result: AutoQRA能够在训练时主动补偿特定层中的量化噪声，且其实验结果显示其表现接近全精度微调，但内存占用却与统一使用4比特量化的方法相似。

Conclusion: 通过AutoQRA框架可以有效解决在GPU内存受限条件下量化与微调之间存在的矛盾，实现更优的资源利用效率和模型性能。

Abstract: Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.

</details>


### [61] [CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning](https://arxiv.org/abs/2602.22269)
*Arnab Nath,Harsh Kasyap*

Main category: cs.LG

TL;DR: 本文提出了一种名为CQSA（Clustered Quantum Secure Aggregation）的框架，旨在解决量子辅助联邦学习中使用全局GHZ状态时遇到的挑战，如大规模客户端下保真度下降及拜占庭客户端检测问题。通过局部量子聚合和统计分析，CQSA在保持模型收敛性的同时实现了更高的状态保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的量子安全聚合协议依赖于所有参与客户端共享的一个全局GHZ状态，这导致了随着客户端数量增加，大规模GHZ状态的保真度快速恶化的问题，并且全局聚合方式也难以识别出恶意客户端。为了解决这些问题，提出了一个新的解决方案。

Method: 研究者们开发了一个称为CQSA的新方法，该方法将客户端随机分组为小集群，每个集群利用高保真度、低量子比特数的GHZ状态执行本地量子聚合。服务器随后使用余弦相似度和欧几里得距离等统计度量来分析集群级别的聚合结果，以发现潜在的恶意贡献。

Result: 理论分析与模拟实验表明，在去极化噪声条件下，CQSA能够确保模型稳定收敛，并且相比基于全局GHZ状态的方法，它达到了更好的状态保真度。

Conclusion: CQSA作为一种模块化的聚合框架，成功地解决了物理限制以及联邦学习中对拜占庭鲁棒性的需求之间的矛盾，为量子辅助联邦学习提供了一个新的方向。

Abstract: Federated Learning (FL) enables collaborative model training without sharing raw data. However, shared local model updates remain vulnerable to inference and poisoning attacks. Secure aggregation schemes have been proposed to mitigate these attacks. In this work, we aim to understand how these techniques are implemented in quantum-assisted FL. Quantum Secure Aggregation (QSA) has been proposed, offering information-theoretic privacy by encoding client updates into the global phase of multipartite entangled states. Existing QSA protocols, however, rely on a single global Greenberger-Horne-Zeilinger (GHZ) state shared among all participating clients. This design poses fundamental challenges: fidelity of large-scale GHZ states deteriorates rapidly with the increasing number of clients; and (ii) the global aggregation prevents the detection of Byzantine clients. We propose Clustered Quantum Secure Aggregation (CQSA), a modular aggregation framework that reconciles the physical constraints of near-term quantum hardware along with the need for Byzantine-robustness in FL. CQSA randomly partitions the clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server analyzes statistical relationships between cluster-level aggregates employing common statistical measures such as cosine similarity and Euclidean distance to identify malicious contributions. Through theoretical analysis and simulations under depolarizing noise, we demonstrate that CQSA ensures stable model convergence, achieves superior state fidelity over global QSA.

</details>


### [62] [Prior Knowledge-enhanced Spatio-temporal Epidemic Forecasting](https://arxiv.org/abs/2602.22270)
*Sijie Ruan,Jinyu Li,Jia Wei,Zenghao Xu,Jie Bao,Junshi Xu,Junyang Qiu,Hanning Yuan,Xiaoxiao Wang,Shuliang Wang*

Main category: cs.LG

TL;DR: 提出了一种新的时空先验感知流行病预测器(STOEP)，通过整合隐式时空先验和显式专家先验来改善流行病信号的敏感性、空间关系的建模以及参数估计的稳定性。STOEP在实际COVID-19和流感数据集上的表现优于基准方法，并已被部署在中国某省级CDC中使用。


<details>
  <summary>Details</summary>
Motivation: 当前流行病预测方法存在对弱流行病信号不敏感、空间关系过于简化及参数估计不稳定的问题，为此提出了STOEP框架以解决这些问题。

Method: STOEP框架包含三个关键部分：（1）基于案例的邻接学习(CAL)，利用历史感染模式动态调整基于流动性的区域依赖；（2）空间信息参数估计(SPE)，采用可学习的空间先验增强弱流行病信号；（3）基于过滤器的机制预测(FMF)，运用专家指导下的自适应阈值策略规范流行病参数。

Result: 通过对真实世界中的COVID-19和流感数据集进行广泛实验表明，STOEP比最佳基线方法在RMSE上提高了11.1%。此外，该系统已在中国某省级CDC部署应用。

Conclusion: STOEP提供了一种有效的解决方案，能够更好地处理流行病预测中的挑战，包括提高对弱信号的敏感度、更准确地建模空间关系以及稳定参数估计，在实际应用场景中表现出色。

Abstract: Spatio-temporal epidemic forecasting is critical for public health management, yet existing methods often struggle with insensitivity to weak epidemic signals, over-simplified spatial relations, and unstable parameter estimation. To address these challenges, we propose the Spatio-Temporal priOr-aware Epidemic Predictor (STOEP), a novel hybrid framework that integrates implicit spatio-temporal priors and explicit expert priors. STOEP consists of three key components: (1) Case-aware Adjacency Learning (CAL), which dynamically adjusts mobility-based regional dependencies using historical infection patterns; (2) Space-informed Parameter Estimating (SPE), which employs learnable spatial priors to amplify weak epidemic signals; and (3) Filter-based Mechanistic Forecasting (FMF), which uses an expert-guided adaptive thresholding strategy to regularize epidemic parameters. Extensive experiments on real-world COVID-19 and influenza datasets demonstrate that STOEP outperforms the best baseline by 11.1% in RMSE. The system has been deployed at one provincial CDC in China to facilitate downstream applications.

</details>


### [63] [Support Tokens, Stability Margins, and a New Foundation for Robust LLMs](https://arxiv.org/abs/2602.22271)
*Deepak Agarwal,Dhyey Dharmendrakumar Mavani,Suyash Gupta,Karthik Sethuraman,Tejas Dharamsi*

Main category: cs.LG

TL;DR: 本文将因果自注意力转换器重新解释在一个概率框架内，揭示了自注意力参数上的屏障约束现象，并提出了一种贝叶斯框架及MAP估计目标，通过向标准的交叉熵损失添加平滑对数屏障惩罚来训练模型，从而提供更鲁棒的模型而不牺牲样本外准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索自注意力机制背后的深层结构和理论见解，以及如何通过引入概率框架来改进大型语言模型（LLM）的训练过程，使其更加稳健。

Method: 采用概率PCA的概念对因果自注意力转换器进行了重新诠释；基于发现的屏障约束现象分析了token空间内的几何结构；提出了一个处理序列建模的贝叶斯框架；通过修改标准LLM训练中的损失函数（加入平滑对数屏障惩罚项）来实现所提方法。

Result: 研究表明，在不牺牲样本外准确性的前提下，该方法能够训练出更为鲁棒的模型；同时，也为理解LLM解码动态提供了理论依据。

Conclusion: 通过对因果自注意力转换器的概率性重新解读，不仅加深了对其工作原理的理解，还开发了一种简单有效的技术以提高模型的鲁棒性。

Abstract: Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.
  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.

</details>


### [64] [Tackling Privacy Heterogeneity in Differentially Private Federated Learning](https://arxiv.org/abs/2602.22633)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: 本文针对联邦学习中隐私预算异质性的问题，提出了一种隐私感知的客户端选择策略。通过建立一个理论基础来量化隐私异质性对训练误差的影响，并将该策略形式化为一个凸优化问题，以自适应调整选择概率从而最小化训练误差。实验表明，在异质隐私预算下，与现有基线相比，该方法在CIFAR-10上的测试准确率提高了高达10%。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私联邦学习（DP-FL）方法通常假设所有客户端共享统一的隐私预算，这在现实场景中并不成立，因为实际中的隐私需求差异很大。这种隐私异质性导致了传统依赖数据量的客户端选择策略无法有效区分提供高质量更新的客户端和由于严格隐私限制而引入大量噪声的客户端。

Method: 作者首先进行了收敛分析，量化了隐私异质性对训练误差的影响；基于此分析，提出了一个隐私感知的客户端选择策略，并将其表述成一个凸优化问题，旨在通过自适应地调节选择概率来最小化训练误差。

Result: 广泛的实验结果证明，所提出的方法在处理具有不同隐私预算的客户端时表现出色，特别是在CIFAR-10数据集上，相较于其他基准方法，测试准确率最高可提高10%。

Conclusion: 研究强调了在进行实用且有效的联邦学习时考虑隐私异质性的重要性，并通过实验证明了提出的隐私感知客户端选择策略能够显著提升模型性能。

Abstract: Differentially private federated learning (DP-FL) enables clients to collaboratively train machine learning models while preserving the privacy of their local data. However, most existing DP-FL approaches assume that all clients share a uniform privacy budget, an assumption that does not hold in real-world scenarios where privacy requirements vary widely. This privacy heterogeneity poses a significant challenge: conventional client selection strategies, which typically rely on data quantity, cannot distinguish between clients providing high-quality updates and those introducing substantial noise due to strict privacy constraints. To address this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We establish a theoretical foundation by deriving a convergence analysis that quantifies the impact of privacy heterogeneity on training error. Building on this analysis, we propose a privacy-aware client selection strategy, formulated as a convex optimization problem, that adaptively adjusts selection probabilities to minimize training error. Extensive experiments on benchmark datasets demonstrate that our approach achieves up to a 10% improvement in test accuracy on CIFAR-10 compared to existing baselines under heterogeneous privacy budgets. These results highlight the importance of incorporating privacy heterogeneity into client selection for practical and effective federated learning.

</details>


### [65] [X-REFINE: XAI-based RElevance input-Filtering and archItecture fiNe-tuning for channel Estimation](https://arxiv.org/abs/2602.22277)
*Abdul Karim Gizzini,Yahia Medjahdi*

Main category: cs.LG

TL;DR: 提出了一种基于XAI的框架X-REFINE，用于联合输入过滤和架构微调，通过分解基础的、符号稳定的LRP epsilon规则反向传播预测，从而为子载波和隐藏神经元提供高分辨率的相关性评分。模拟结果表明X-REFINE在不同场景下实现了优越的可解释性-性能-复杂度权衡，在显著降低计算复杂度的同时保持了强大的误码率(BER)表现。


<details>
  <summary>Details</summary>
Motivation: 当前6G无线通信中采用的人工智能原生架构虽然重要，但深度学习模型因黑盒性质及高复杂度限制了其实际应用。尽管基于扰动的XAI解决方案提供了输入过滤，但它们往往忽视了内部结构优化的问题。

Method: 提出了X-REFINE框架，该框架利用分解式、符号稳定的LRP epsilon规则来反向传播预测，并为子载波和隐藏神经元生成高分辨率的相关性分数。这一过程允许对最忠实的模型组件进行全面优化。

Result: 仿真结果显示，X-REFINE在各种情况下都能达到更好的可解释性-性能-复杂度平衡，大幅减少了计算复杂度同时维持了良好的误码率(BER)表现。

Conclusion: X-REFINE作为一个基于XAI的方法，成功地解决了6G无线通信领域内深度学习模型面临的挑战，即提高了模型的可解释性和效率而不牺牲性能。

Abstract: AI-native architectures are vital for 6G wireless communications. The black-box nature and high complexity of deep learning models employed in critical applications, such as channel estimation, limit their practical deployment. While perturbation-based XAI solutions offer input filtering, they often neglect internal structural optimization. We propose X-REFINE, an XAI-based framework for joint input-filtering and architecture fine-tuning. By utilizing a decomposition-based, sign-stabilized LRP epsilon rule, X-REFINE backpropagates predictions to derive high-resolution relevance scores for both subcarriers and hidden neurons. This enables a holistic optimization that identifies the most faithful model components. Simulation results demonstrate that X-REFINE achieves a superior interpretability-performance-complexity trade-off, significantly reducing computational complexity while maintaining robust bit error rate (BER) performance across different scenarios.

</details>


### [66] [Integrating Machine Learning Ensembles and Large Language Models for Heart Disease Prediction Using Voting Fusion](https://arxiv.org/abs/2602.22280)
*Md. Tahsin Amin,Tanim Ahmmod,Zannatul Ferdus,Talukder Naemul Hasan Naem,Ehsanul Ferdous,Arpita Bhattacharjee,Ishmam Ahmed Solaiman,Nahiyan Bin Noor*

Main category: cs.LG

TL;DR: 本研究将传统机器学习模型与大型语言模型(LLM)相结合，用于预测心血管疾病。结果表明，混合ML集成和LLM推理在Gemini 2.5 Flash下达到了最佳效果（96.62%的准确率，0.97 AUC），展示了当LLM与ML模型结合而非单独使用时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要早期识别、精准风险分类和可靠的决策支持技术。尽管机器学习(ML)算法，特别是像随机森林、XGBoost、LightGBM和CatBoost这样的集成方法，在处理复杂的非线性患者数据方面表现出色且经常优于逻辑回归，但大型语言模型(LLM)的出现提供了新的零样本和少样本推理能力。

Method: 本研究采用了一个包含1,190名患者记录的合并数据集来预测心血管疾病，并通过OpenRouter API比较了传统机器学习模型与开源大型语言模型的表现。最终，提出了一种ML集成与LLM推理的混合融合方法。

Result: 结果显示，ML集成取得了最高的性能（95.78%的准确率，ROC-AUC 0.96），而LLM在零样本（78.9%）和少样本（72.6%）设置中表现一般。提出的混合方法增强了不确定情况下的强度，显示了在结构化表格预测案例中ML集成被认为是最佳选择，但可以与混合ML-LLM系统集成以提供轻微提升并为更可靠的临床决策支持工具铺平道路。

Conclusion: 混合ML集成与LLM推理的方法在Gemini 2.5 Flash下达到了最佳的结果（96.62%的准确率，0.97 AUC），证明了当LLM与ML模型结合使用时比单独使用LLM更为有效。此外，这为开发更加可靠的心血管疾病临床决策支持工具开辟了新途径。

Abstract: Cardiovascular disease is the primary cause of death globally, necessitating early identification, precise risk classification, and dependable decision-support technologies. The advent of large language models (LLMs) provides new zero-shot and few-shot reasoning capabilities, even though machine learning (ML) algorithms, especially ensemble approaches like Random Forest, XGBoost, LightGBM, and CatBoost, are excellent at modeling complex, non-linear patient data and routinely beat logistic regression. This research predicts cardiovascular disease using a merged dataset of 1,190 patient records, comparing traditional machine learning models (95.78% accuracy, ROC-AUC 0.96) with open-source large language models via OpenRouter APIs. Finally, a hybrid fusion of the ML ensemble and LLM reasoning under Gemini 2.5 Flash achieved the best results (96.62% accuracy, 0.97 AUC), showing that LLMs (78.9 % accuracy) work best when combined with ML models rather than used alone. Results show that ML ensembles achieved the highest performance (95.78% accuracy, ROC-AUC 0.96), while LLMs performed moderately in zero-shot (78.9%) and slightly better in few-shot (72.6%) settings. The proposed hybrid method enhanced the strength in uncertain situations, illustrating that ensemble ML is considered the best structured tabular prediction case, but it can be integrated with hybrid ML-LLM systems to provide a minor increase and open the way to more reliable clinical decision-support tools.

</details>


### [67] [Early Risk Stratification of Dosing Errors in Clinical Trials Using Machine Learning](https://arxiv.org/abs/2602.22285)
*Félicien Hêche,Sohrab Ferdowsi,Anthony Yazdani,Sara Sansaloni-Pastor,Douglas Teodoro*

Main category: cs.LG

TL;DR: 本研究开发了一种基于机器学习的框架，用于在临床试验启动前根据其出现高剂量错误率的可能性进行早期风险分层。通过结合结构化特征和文本数据，所构建的模型能够有效地对临床试验进行风险分类，并且概率校准对于将模型输出转化为可靠且可解释的风险类别至关重要。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一个基于机器学习的框架，能够在临床试验（CTs）开始之前就依据它们表现出高剂量错误率的可能性来进行早期的风险分层。这样可以帮助支持临床研究中的主动性和基于风险的质量管理。

Method: 研究人员从ClinicalTrials.gov收集了42,112个CT的数据集，包括结构化、半结构化的试验数据以及非结构化的协议相关自由文本数据。CT被分配了二元标签以表示是否具有高剂量错误率，这些信息是从不良事件报告、MedDRA术语及Wilson置信区间中获得的。接着评估了几种模型的表现：一个是基于结构化特征训练的XGBoost模型；另一个是使用文本数据的ClinicalModernBERT模型；还有就是简单地融合两种模态的晚融合模型。最后，为了实现可解释的试验级别风险分层，应用了事后概率校准方法。

Result: 晚融合模型达到了最高的AUC-ROC值(0.862)。除了良好的区分能力外，经过校准后的输出还使得CT能够被稳健地分成预定义的风险类别。被标记为具有过高剂量错误率的试验比例随着预测风险组别的增加而单调上升，并与相应的预测概率范围相一致。

Conclusion: 这项研究引入了一个可重复且可扩展的机器学习框架，用于对有高剂量错误率风险的CTs进行早期、试验级别的风险分层，从而支持临床研究中的前瞻性、基于风险的质量管理。

Abstract: Objective: The objective of this study is to develop a machine learning (ML)-based framework for early risk stratification of clinical trials (CTs) according to their likelihood of exhibiting a high rate of dosing errors, using information available prior to trial initiation. Materials and Methods: We constructed a dataset from ClinicalTrials.gov comprising 42,112 CTs. Structured, semi-structured trial data, and unstructured protocol-related free-text data were extracted. CTs were assigned binary labels indicating elevated dosing error rate, derived from adverse event reports, MedDRA terminology, and Wilson confidence intervals. We evaluated an XGBoost model trained on structured features, a ClinicalModernBERT model using textual data, and a simple late-fusion model combining both modalities. Post-hoc probability calibration was applied to enable interpretable, trial-level risk stratification. Results: The late-fusion model achieved the highest AUC-ROC (0.862). Beyond discrimination, calibrated outputs enabled robust stratification of CTs into predefined risk categories. The proportion of trials labeled as having an excessively high dosing error rate increased monotonically across higher predicted risk groups and aligned with the corresponding predicted probability ranges. Discussion: These findings indicate that dosing error risk can be anticipated at the trial level using pre-initiation information. Probability calibration was essential for translating model outputs into reliable and interpretable risk categories, while simple multimodal integration yielded performance gains without requiring complex architectures. Conclusion: This study introduces a reproducible and scalable ML framework for early, trial-level risk stratification of CTs at risk of high dosing error rates, supporting proactive, risk-based quality management in clinical research.

</details>


### [68] [OmniZip: Learning a Unified and Lightweight Lossless Compressor for Multi-Modal Data](https://arxiv.org/abs/2602.22286)
*Yan Zhao,Zhengxue Cheng,Junxuan Zhang,Dajiang Zhou,Qunshan Gu,Qi Wang,Li Song*

Main category: cs.LG

TL;DR: OmniZip是一个轻量级的多模态无损压缩器，适用于图像、文本、语音等多种数据类型。通过统一的分词器、多模态路由上下文学习机制和前馈设计，OmniZip在多个数据集上实现了比gzip等现有压缩工具更高的压缩效率，并支持在资源受限设备上的近实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的无损压缩算法大多针对单一模态设计，在处理多模态数据时会导致冗余部署。设计一个既能处理多种类型数据又保持高效简洁的统一压缩解决方案是很有必要的。

Method: OmniZip采用了一个轻量化的基础架构，并结合了三个关键组成部分：能够将不同格式的数据转换为令牌的统一化分词器、允许灵活进行多模态上下文建模的模态路由上下文学习机制以及进一步增强模型非线性表示能力的模态路由前馈设计。此外，还利用重新参数化训练策略来增加模型容量。

Result: OmniZip在CLIC-M、TouchandGo、enwik9、LibriSpeech和WikiSQL等多个数据集上的表现优于或与最先进的压缩器相当，其压缩效率相比gzip分别提高了42%、57%、62%、42%和53%。此外，它能够在MacBook CPU和iPhone NPU上实现约1MB/s的近实时推理速度。

Conclusion: OmniZip提供了一种新颖且高效的多模态无损压缩方案，不仅减少了传统方法中因需针对每种数据类型单独部署压缩器而导致的冗余问题，而且还在保证高压缩率的同时保持了较低的计算成本，使其非常适合应用于边缘设备。

Abstract: Lossless compression is essential for efficient data storage and transmission. Although learning-based lossless compressors achieve strong results, most of them are designed for a single modality, leading to redundant compressor deployments in multi-modal settings. Designing a unified multi-modal compressor is critical yet challenging, as different data types vary largely in format, dimension, and statistics. Multi-modal large language models offer a promising resolution but remain too complex for practical use. Thus, we propose \textbf{OmniZip}, \textbf{a unified and lightweight lossless compressor for multi-modal data (like image, text, speech, tactile, database, and gene sequence)}. Built on a lightweight backbone, OmniZip incorporates three key components to enable efficient multi-modal lossless compression: a modality-unified tokenizer that reversibly transforms diverse data into tokens, a modality-routing context learning mechanism that enables flexible multi-modal context modeling, and a modality-routing feedforward design that further enhances the model's nonlinear representation flexibility. A reparameterization training strategy is used to enhance model capacity. OmniZip outperforms or matches other state-of-the-art compressors on multiple modalities, achieving 42\%, 57\%, 62\% and 42\%, 53\% higher compression efficiency than gzip on CLIC-M, TouchandGo, enwik9, LibriSpeech, and WikiSQL datasets, respectively. It also supports near real-time inference on resource-constrained edge devices, reaching about 1MB/s on MacBook CPUs and iPhone NPUs. Our code is released at https://github.com/adminasmi/OmniZip-CVPR2026.

</details>


### [69] [Reliable XAI Explanations in Sudden Cardiac Death Prediction for Chagas Cardiomyopathy](https://arxiv.org/abs/2602.22288)
*Vinícius P. Chagas,Luiz H. T. Viana,Mac M. da S. Carlos,João P. V. Madeiro,Roberto C. Pedrosa,Thiago Alves Rocha,Carlos H. L. Cavalcante*

Main category: cs.LG

TL;DR: 本研究通过应用一种具有正确性保证的逻辑解释方法，解决了查加斯心肌病（CC）中突发心脏死亡（SCD）预测的问题。该方法应用于一个准确率和召回率超过95%的人工智能分类器上，展示了强大的预测性能和100%的解释准确性。与最先进的启发式方法相比，它表现出更好的一致性和鲁棒性，从而增强了临床信任，并促进了AI驱动工具在实践中的整合及大规模部署，特别是在需要这些工具的流行区域。


<details>
  <summary>Details</summary>
Motivation: 由于查加斯心肌病（CC）中突发心脏死亡（SCD）难以预测，尤其是在未被归类为高风险的患者中，尽管人工智能和机器学习模型提高了风险分层的准确性，但它们因缺乏透明度而受到限制，被视为没有清晰决策过程的“黑盒子”。此外，一些方法提供的启发式解释没有正确性的保障，导致了决策过程中的错误。

Method: 研究者们采用了一种带有正确性保证的基于逻辑的可解释性方法来解决CC中SCD预测的问题。这种方法被应用于一个准确率和召回率都超过95%的人工智能分类器上，以评估其预测性能和解释的准确性。

Result: 实验结果显示，所提出的可解释性方法不仅保持了高水平的预测表现，还达到了100%的解释准确性。相较于现有最先进方法，新方法展示出更优的一致性和鲁棒性。

Conclusion: 这种基于逻辑且带有正确性保证的可解释性方法成功地提升了对CC中SCD预测的信任度，有利于AI技术在医疗领域的实际应用推广，特别是在那些急需这类技术的地方。

Abstract: Sudden cardiac death (SCD) is unpredictable, and its prediction in Chagas cardiomyopathy (CC) remains a significant challenge, especially in patients not classified as high risk. While AI and machine learning models improve risk stratification, their adoption is hindered by a lack of transparency, as they are often perceived as \textit{black boxes} with unclear decision-making processes. Some approaches apply heuristic explanations without correctness guarantees, leading to mistakes in the decision-making process. To address this, we apply a logic-based explainability method with correctness guarantees to the problem of SCD prediction in CC. This explainability method, applied to an AI classifier with over 95\% accuracy and recall, demonstrated strong predictive performance and 100\% explanation fidelity. When compared to state-of-the-art heuristic methods, it showed superior consistency and robustness. This approach enhances clinical trust, facilitates the integration of AI-driven tools into practice, and promotes large-scale deployment, particularly in endemic regions where it is most needed.

</details>


### [70] [Manifold of Failure: Behavioral Attraction Basins in Language Models](https://arxiv.org/abs/2602.22291)
*Sarthak Munshi,Manish Bhatt,Vineeth Sai Narajala,Idan Habler,AmmarnAl-Kahfah,Ken Huang,Blake Gatto*

Main category: cs.LG

TL;DR: 本文提出了一种系统地绘制大型语言模型（LLMs）失败流形的框架，通过使用MAP-Elites方法来揭示这些失败区域的连续拓扑结构，并引入了对齐偏差作为质量度量。研究发现不同模型具有不同的拓扑特征，从而为理解AI安全提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 为了全面理解AI的安全性，作者认为不仅要关注将对抗样本投影回自然数据流形以恢复安全性，还应该刻画不安全区域本身。

Method: 采用MAP-Elites算法处理质量多样性问题，旨在照亮这些失败区域（称为行为吸引盆地）的连续拓扑。引入对齐偏差作为衡量标准，指导搜索向模型行为与其预期对齐偏离最大的方向发展。

Result: 在三个LLM上进行实验：Llama-3-8B、GPT-OSS-20B和GPT-5-Mini。结果显示，MAP-Elites达到了高达63%的行为覆盖率，发现了多达370个独特的漏洞生态位，并揭示了显著不同的模型特定拓扑特征。

Conclusion: 该方法能够生成每个模型安全景观的可解释全局地图，这是现有任何攻击方法都无法提供的，它改变了从寻找离散故障到理解其底层结构的范式。

Abstract: While prior work has focused on projecting adversarial examples back onto the manifold of natural data to restore safety, we argue that a comprehensive understanding of AI safety requires characterizing the unsafe regions themselves. This paper introduces a framework for systematically mapping the Manifold of Failure in Large Language Models (LLMs). We reframe the search for vulnerabilities as a quality diversity problem, using MAP-Elites to illuminate the continuous topology of these failure regions, which we term behavioral attraction basins. Our quality metric, Alignment Deviation, guides the search towards areas where the model's behavior diverges most from its intended alignment. Across three LLMs: Llama-3-8B, GPT-OSS-20B, and GPT-5-Mini, we show that MAP-Elites achieves up to 63% behavioral coverage, discovers up to 370 distinct vulnerability niches, and reveals dramatically different model-specific topological signatures: Llama-3-8B exhibits a near-universal vulnerability plateau (mean Alignment Deviation 0.93), GPT-OSS-20B shows a fragmented landscape with spatially concentrated basins (mean 0.73), and GPT-5-Mini demonstrates strong robustness with a ceiling at 0.50. Our approach produces interpretable, global maps of each model's safety landscape that no existing attack method (GCG, PAIR, or TAP) can provide, shifting the paradigm from finding discrete failures to understanding their underlying structure.

</details>


### [71] [Global River Forecasting with a Topology-Informed AI Foundation Model](https://arxiv.org/abs/2602.22293)
*Hancheng Ren,Gang Zhao,Shuo Wang,Louise Slater,Dai Yamazaki,Shu Liu,Jingfang Fan,Shibo Cui,Ziming Yu,Shengyu Kang,Depeng Zuo,Dingzhi Peng,Zongxue Xu,Bo Pang*

Main category: cs.LG

TL;DR: GraphRiverCast (GRC) is a topology-informed AI model for simulating river hydrodynamics globally, capable of operating without historical data and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of data scarcity in hydrology, which often limits forecasting to isolated predictions, and to achieve systemic simulation of river networks with reduced reliance on observational data.

Method: Development and application of GraphRiverCast (GRC), an AI foundation model that incorporates topological information to simulate multivariate river hydrodynamics. GRC can operate in a 'ColdStart' mode, making predictions without historical data, and leverages physics-aligned neural operator architecture for cross-scale adaptive simulation.

Result: GRC demonstrated robust performance as a standalone simulator, achieving a Nash-Sutcliffe Efficiency (NSE) of around 0.82 in 7-day global pseudo-hindcasts. It also showed superiority over physics-based and locally-trained AI models when adapted through pre-training and fine-tuning, extending its advantage from gauged reaches to entire river networks.

Conclusion: The introduction of GRC represents a significant step towards more effective and scalable simulation of river hydrodynamics, emphasizing the importance of incorporating topological information and utilizing a physics-based pre-training approach.

Abstract: River systems operate as inherently interconnected continuous networks, meaning river hydrodynamic simulation ought to be a systemic process. However, widespread hydrology data scarcity often restricts data-driven forecasting to isolated predictions. To achieve systemic simulation and reduce reliance on river observations, we present GraphRiverCast (GRC), a topology-informed AI foundation model designed to simulate multivariate river hydrodynamics in global river systems. GRC is capable of operating in a "ColdStart" mode, generating predictions without relying on historical river states for initialization. In 7-day global pseudo-hindcasts, GRC-ColdStart functions as a robust standalone simulator, achieving a Nash-Sutcliffe Efficiency (NSE) of approximately 0.82 without exhibiting the significant error accumulation typical of autoregressive paradigms. Ablation studies reveal that topological encoding serves as indispensable structural information in the absence of historical states, explicitly guiding hydraulic connectivity and network-scale mass redistribution to reconstruct flow dynamics. Furthermore, when adapted locally via a pre-training and fine-tuning strategy, GRC consistently outperforms physics-based and locally-trained AI baselines. Crucially, this superiority extends from gauged reaches to full river networks, underscoring the necessity of topology encoding and physics-based pre-training. Built on a physics-aligned neural operator architecture, GRC enables rapid and cross-scale adaptive simulation, establishing a collaborative paradigm bridging global hydrodynamic knowledge with local hydrological reality.

</details>


### [72] [When Should a Model Change Its Mind? An Energy-Based Theory and Regularizer for Concept Drift in Electrocardiogram (ECG) Signals](https://arxiv.org/abs/2602.22294)
*Timothy Oladunni,Blessing Ojeme,Kyndal Maclin,Clyde Baidoo*

Main category: cs.LG

TL;DR: 研究提出了一种基于能量的框架PECT，用于处理动态生理信号中的概念稳定性问题，并通过ECRL方法实现，实验显示在多种模型上提高了扰动准确率并减少了表示漂移。


<details>
  <summary>Details</summary>
Motivation: 现有的概念漂移框架难以区分无害的信号变化和真正的概念漂移，导致深度学习模型在面对生理信号中自然波动时产生不稳定预测。

Method: 提出了生理能量守恒理论(PECT)及其实现方法——能量约束表示学习(ECRL)，旨在通过限制与信号能量变化不成比例的潜在表示移动来维持概念稳定性。

Result: 最强三模态混合模型（1D+2D+Transformer）下，正常精度略有下降（96.0%降至94.1%），但扰动数据的准确性显著提高（从72.6%提升至85.5%），同时融合表示漂移减少了超过45%。

Conclusion: PECT作为能量漂移定律，在连续生理信号的概念稳定性管理中展现出有效性，为处理虚拟漂移提供了一个新的视角。

Abstract: Models operating on dynamic physiologic signals must distinguish benign, label-preserving variability from true concept change. Existing concept-drift frameworks are largely distributional and provide no principled guidance on how much a model's internal representation may move when the underlying signal undergoes physiologically plausible fluctuations in energy. As a result, deep models often misinterpret harmless changes in amplitude, rate, or morphology as concept drift, yielding unstable predictions, particularly in multimodal fusion settings.
  This study introduces Physiologic Energy Conservation Theory (PECT), an energy-based framework for concept stability in dynamic signals. PECT posits that under virtual drift, normalized latent displacement should scale proportionally with normalized signal energy change, while persistent violations of this proportionality indicate real concept drift. We operationalize this principle through Energy-Constrained Representation Learning (ECRL), a lightweight regularizer that penalizes energy-inconsistent latent movement without modifying encoder architectures or adding inference-time cost.
  Although PECT is formulated for dynamic signals in general, we instantiate and evaluate it on multimodal ECG across seven unimodal and hybrid models. Experiments show that in the strongest trimodal hybrid (1D+2D+Transformer), clean accuracy is largely preserved (96.0% to 94.1%), while perturbed accuracy improves substantially (72.6% to 85.5%) and fused representation drift decreases by over 45%. Similar trends are observed across all architectures, providing empirical evidence that PECT functions as an energy-drift law governing concept stability in continuous physiologic signals.

</details>


### [73] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 本文提出了一种基于离线逆强化学习的方法，用于机械故障检测。通过使用对抗逆强化学习训练一个能够区分正常和异常状态的判别器，该方法能从健康运行序列中直接学习奖励动态，从而无需手动设计奖励函数或依赖故障标签。在三个基准数据集上的评估表明，该模型可以为正常样本分配较低的异常分数，为故障样本分配较高的异常分数，实现了早期且可靠的故障检测。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的机械故障检测方法未能充分利用其序列决策的优势，通常将故障检测视为简单的猜测游戏（上下文强盗问题）。为了克服这一限制，并更好地利用强化学习处理时序数据的能力，提出了新的方法论。

Method: 研究者们把机械故障检测建模成一个离线逆强化学习问题，采用对抗逆强化学习技术来训练一个判别器。该判别器能够识别出正常操作与策略生成转换之间的差异，由此产生的奖励作为异常评分，反映偏离正常行为的程度。

Result: 在HUMS2023、IMS以及XJTU-SY这三个运行至失效的基准数据集上进行了测试，结果表明所提模型能够有效地给正常样本打低分而给故障样本打高分，证明了该方法在早期发现及稳健性方面具有优越性能。

Conclusion: 通过将强化学习的顺序推理能力与机械故障检测的时间结构相匹配，本研究为工业环境中基于数据驱动的诊断提供了新的方向。

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [74] [A 1/R Law for Kurtosis Contrast in Balanced Mixtures](https://arxiv.org/abs/2602.22334)
*Yuda Bi,Wenjun Xiao,Linhao Bai,Vince D Calhoun*

Main category: cs.LG

TL;DR: 本文探讨了基于峰度的独立成分分析在宽广、平衡混合中的局限性，并提出了一个尖锐的冗余定律。此外，还介绍了通过净化方法恢复与R无关对比度的策略。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决基于峰度的ICA在处理宽泛且均衡混合信号时效果不佳的问题。

Method: 通过证明一个尖锐的冗余定律来定义标准化投影的有效宽度与总体过量峰度之间的关系，并提出了一种称为“净化”的方法来选择少量符号一致的源以恢复对比度。

Result: 研究表明，在给定条件下要超越特定估计尺度需要满足一定的条件；同时，净化方法能够成功恢复与R无关的对比度。实验结果验证了预测的衰减情况、交叉点以及对比度恢复的有效性。

Conclusion: 对于宽泛和平衡混合的情况，基于峰度的ICA表现会减弱，但通过采用净化技术可以选择出关键信号源从而改善这一状况。

Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|κ(y)|=O(κ_{\max}/R_{\mathrm{eff}})$, yielding the order-tight $O(c_bκ_{\max}/R)$ under balance (typically $c_b=O(\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\sqrt{T})$ estimation scale requires $R\lesssim κ_{\max}\sqrt{T}$. We also show that \emph{purification} -- selecting $m\!\ll\!R$ sign-consistent sources -- restores $R$-independent contrast $Ω(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\sqrt{T}$ crossover, and contrast recovery.

</details>


### [75] [Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization](https://arxiv.org/abs/2602.22387)
*Yixuan Li,Archer Y. Yang,Yue Li*

Main category: cs.LG

TL;DR: 该论文提出了一种背景对比非负矩阵分解方法(\model)，旨在通过共同分解目标数据集和匹配的背景，利用对比目标抑制背景表达结构，从而提取富集的目标潜在主题。这种方法可以在高维数据中有效地分离出特定条件下的生物信号变化，并且在模拟和多种生物数据集中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在高维数据中，感兴趣的生物信号经常被跨条件下共享的主要变异所掩盖，这些变异可能来自基线生物结构或技术效应，这会妨碍标准降维方法解析特定条件下的结构。现有的背景校正方法要么无法扩展到高维度，要么缺乏可解释性。因此，需要一种新的方法来解决这些问题。

Method: 本文引入了背景对比非负矩阵分解（\model），它通过共同分解目标数据集和匹配背景，采用共享非负基底，在一个对比目标下抑制背景表达结构，以提取富集的目标潜在主题。该模型能够产生直接在特征级别上可解释的非负成分，并明确地隔离出目标特异性变异。此外，\model 通过高效的乘法更新算法学习，该算法基于矩阵乘法，非常适合GPU硬件加速，并且可以通过类似深度学习方法的小批量训练来处理大数据。

Result: 在模拟实验及多样的生物数据集测试中，\model 方法能够揭示传统方法无法发现的信号，包括抑郁症患者死后脑部单细胞RNA测序中的疾病相关程序、小鼠中与基因型相关的蛋白质表达模式、白血病治疗特定转录变化以及癌细胞系中的TP53依赖药物反应等。

Conclusion: 背景对比非负矩阵分解(\model)为从高维数据中分离出条件特异性生物信号提供了一个有效的新途径。其不仅能够处理大规模数据集，而且提供了良好的可解释性，对于生物医学研究具有重要价值。

Abstract: Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.

</details>


### [76] [Predicting Multi-Drug Resistance in Bacterial Isolates Through Performance Comparison and LIME-based Interpretation of Classification Models](https://arxiv.org/abs/2602.22400)
*Santanam Wishal,Riad Sahara*

Main category: cs.LG

TL;DR: 该研究提出了一种可解释的机器学习框架，用于通过临床特征和抗生素敏感性模式预测细菌分离株中的多重耐药性（MDR）。评估了五种分类模型，集成模型特别是XGBoost和LightGBM在所有指标上表现出优越的预测能力。使用LIME方法生成实例级解释，以解决临床透明度问题。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性的上升，特别是多重耐药性（MDR），给临床决策带来了严峻挑战，因为治疗选择有限且常规敏感性测试耗时较长。本研究旨在开发一种新的方法来早期识别MDR并提供可解释的结果，以支持基于机器学习的临床决策辅助。

Method: 研究中评估了五个分类模型：逻辑回归、随机森林、AdaBoost、XGBoost和LightGBM。这些模型在一个包含9,714个隔离菌株的数据集上进行了训练，其中抗性编码到抗生素家族级别，以捕捉符合MDR定义的跨类抗性模式。采用准确率、F1分数、AUC-ROC和马修斯相关系数等指标进行性能评估，并使用LIME方法为每个案例生成解释。

Result: 实验结果显示，集成模型尤其是XGBoost和LightGBM在所有评价指标上展示了更优的预测性能。LIME分析表明喹诺酮类、复方新诺明、多粘菌素、氨基糖苷类及呋喃妥因的耐药性是预测MDR最强的影响因素，这与已知的生物学机制相一致。

Conclusion: 结合高性能模型与局部可解释性不仅提高了预测准确性，还提供了对抗生素管理具有行动指导意义的信息。这种框架有助于更早地识别出MDR情况，并增加了对机器学习辅助临床决策系统的信任度。

Abstract: The rise of Antimicrobial Resistance, particularly Multi-Drug Resistance (MDR), presents a critical challenge for clinical decision-making due to limited treatment options and delays in conventional susceptibility testing. This study proposes an interpretable machine learning framework to predict MDR in bacterial isolates using clinical features and antibiotic susceptibility patterns. Five classification models were evaluated, including Logistic Regression, Random Forest, AdaBoost, XGBoost, and LightGBM. The models were trained on a curated dataset of 9,714 isolates, with resistance encoded at the antibiotic family level to capture cross-class resistance patterns consistent with MDR definitions. Performance assessment included accuracy, F1-score, AUC-ROC, and Matthews Correlation Coefficient. Ensemble models, particularly XGBoost and LightGBM, demonstrated superior predictive capability across all metrics. To address the clinical transparency gap, Local Interpretable Model-agnostic Explanations (LIME) was applied to generate instance-level explanations. LIME identified resistance to quinolones, Co-trimoxazole, Colistin, aminoglycosides, and Furanes as the strongest contributors to MDR predictions, aligning with known biological mechanisms. The results show that combining high-performing models with local interpretability provides both accuracy and actionable insights for antimicrobial stewardship. This framework supports earlier MDR identification and enhances trust in machine learning-assisted clinical decision support.

</details>


### [77] [MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion](https://arxiv.org/abs/2602.22405)
*Syed Omer Shah,Mohammed Maqsood Ahmed,Danish Mohiuddin Mohammed,Shahnawaz Alam,Mohd Vahaj ur Rahman*

Main category: cs.LG

TL;DR: MolFM-Lite is a multi-modal model for molecular property prediction, integrating 1D, 2D, and 3D molecular representations through cross-attention fusion and using Feature-wise Linear Modulation. It introduces a conformer ensemble attention mechanism and a cross-modal fusion layer, improving AUC by 7-11% over single-modality models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy of molecular property prediction by incorporating multiple molecular representations (1D, 2D, and 3D) and accounting for the dynamic nature of molecular geometry, rather than relying on a single static representation as most existing machine learning models do.

Method: The method involves the development of MolFM-Lite, which uses a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors, and a cross-modal fusion layer where each modality can attend to others. The model also conditions predictions on experimental context via FiLM. Pre-training on a large dataset with specific objectives helps in effective weight initialization.

Result: Results show that tri-modal fusion provides 7-11% AUC improvement over single-modality baselines, and the use of conformer ensembles adds approximately 2% AUC improvement over single-conformer variants. The effectiveness of each architectural component was confirmed through comprehensive ablation studies across four datasets.

Conclusion: MolFM-Lite demonstrates the benefits of combining multiple molecular representations and conditioning on experimental context for improved molecular property prediction. The release of code, trained models, and data splits supports reproducibility and further research.

Abstract: Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.

</details>


### [78] [A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection](https://arxiv.org/abs/2602.22412)
*Ruiqi Zhou,Donghao Zhu,Houcai Shen*

Main category: cs.LG

TL;DR: 本文提出了一种基于学习的混合框架，该框架自适应地结合即时匹配和延迟匹配，在动态环境中灵活调整匹配策略，从而减少等待时间和市场拥堵，同时仅牺牲有限的匹配效率。


<details>
  <summary>Details</summary>
Motivation: 在肾脏交换、货运交换等匹配市场中，延迟匹配已被证明可以提高整体市场效率。然而，延迟匹配带来的好处对参与者的逗留时间和离开行为高度敏感，并且可能会带来显著的成本，包括更长的等待时间和增加的市场拥堵。这些相互竞争的影响使得固定匹配政策在动态环境中缺乏灵活性。

Method: 本文提出了一种基于学习的混合框架，该框架能够自适应地结合即时匹配与延迟匹配。此框架通过持续收集用户随时间离开的数据，利用回归方法估计潜在的离开分布，并基于一个决定阈值来判断是否应在后续时间段内延迟匹配，这个阈值决定了系统对于匹配效率损失的容忍度。

Result: 提出的框架能够在只牺牲有限匹配效率的前提下大幅减少等待时间和拥堵问题。通过动态调整其匹配策略，混合框架使系统性能可以在纯贪婪和纯耐心策略之间灵活插值，为静态匹配机制提供了一种稳健且适应性强的替代方案。

Conclusion: 综上所述，这种基于学习的混合框架通过自适应地结合即时匹配与延迟匹配的方式，有效地平衡了等待时间、市场拥堵与匹配效率之间的关系，为匹配市场的管理提供了更加灵活高效的方法。

Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.

</details>


### [79] [Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428)
*Daniel Geyfman,Felix Draxler,Jan Groeneveld,Hyunsoo Lee,Theofanis Karaletsos,Stephan Mandt*

Main category: cs.LG

TL;DR: 本文探讨了测试时引导机制在预训练扩散模型中的应用，指出现有方法侧重于最大化奖励而非从真实贝叶斯后验分布中采样，并提出了能够校准采样的新估计器，在贝叶斯推理任务和黑洞图像重建上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的测试时引导方法主要关注于最大化奖励，而忽视了从真实的贝叶斯后验分布中进行采样，导致推断结果失准。研究旨在解决这一问题，提出更准确地反映后验分布的新方法。

Method: 首先分析了现有测试时引导方法为何无法正确恢复后验分布的原因，随后设计并提出了一种新的、一致性的估计器，该估计器支持从贝叶斯后验分布中进行校准后的采样。

Result: 所提出的方法在一系列贝叶斯推理任务上显著优于先前的技术，并且在黑洞图像重建方面达到了最先进的水平。

Conclusion: 通过引入一种允许从贝叶斯后验分布中进行校准采样的新型估计器，本研究解决了现有测试时引导方法中存在的关键问题，从而提高了模型输出与实际需求之间的匹配度。

Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

</details>


### [80] [From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review](https://arxiv.org/abs/2602.22438)
*Uttamasha Anjally Oyshi,Susan Gauch*

Main category: cs.LG

TL;DR: 该研究提出了Fair-PaperRec，一种具有可微公平损失的多层感知器(MLP)，用于在双盲评审后重新排序论文。通过合成数据集和实际会议数据测试表明，在增加代表性不足群体参与度的同时，能够保持甚至提高学术质量。


<details>
  <summary>Details</summary>
Motivation: 尽管经常采用双盲评审，但与作者人口统计特征相关的系统性偏见仍然使代表性不足的群体处于不利地位。研究者假设，如果训练一个带有明确公平正则化的审稿推荐系统，则可以在不降低质量的情况下提高包容性。

Method: 研究者引入了Fair-PaperRec，这是一种具有交叉属性（如种族、国家）上可微公平损失的多层感知器(MLP)，旨在双盲评审后对论文进行重排序。首先使用覆盖高、中、接近公平偏见级别的合成数据集来验证假设；随后，在来自ACM SIGCHI, DIS, 和 IUI的真实会议数据上进一步测试了该方法。

Result: 在合成数据集上的多次随机运行显示，随着公平权重的增加，宏观/微观多样性得到加强，同时保持效用大致稳定。而在真实世界场景下，适当调整后的Fair-PaperRec配置能使代表性不足群体的参与度最高增加42.03%，整体效用相对于历史选择最多改变3.16%。

Conclusion: 从合成实验到现实应用的结果表明，公平正则化既可作为促进平等的机制，也能温和地充当质量调节器，特别是在高度偏见的情况下。通过先在受控条件下分析公平参数的行为，再在真实提交上验证它们，Fair-PaperRec为评审后论文选择提供了一个实用且注重公平性的框架，不仅保留而且在某些情况下还能增强所测量的学术质量。

Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.

</details>


### [81] [ECHO: Encoding Communities via High-order Operators](https://arxiv.org/abs/2602.22446)
*Emilio Ferrara*

Main category: cs.LG

TL;DR: ECHO, a new scalable and self-supervised architecture, overcomes the limitations of GNNs in community detection by integrating topological and semantic features, achieving high accuracy and efficiency on large-scale networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations faced by traditional topological algorithms and Graph Neural Networks (GNNs) in community detection within attributed networks. Topological methods ignore semantic features, while GNNs face computational bottlenecks due to feature over-smoothing and memory constraints.

Method: ECHO (Encoding Communities via High order Operators) is introduced as a solution. It reframes community detection as an adaptive, multi-scale diffusion process. The method incorporates a Topology Aware Router for analyzing structural heuristics and routing graphs through the optimal inductive bias, along with a memory sharded full batch contrastive objective and a chunked O(N · K) similarity extraction method to bypass memory bottlenecks.

Result: ECHO successfully addresses the Semantic Wall and Systems Wall, demonstrating scale-invariant accuracy on synthetic LFR benchmarks up to 1 million nodes. On real-world social networks with over 1.6 million nodes, it achieves clustering at a rate of more than 2,800 nodes per second, matching the speed of highly optimized topological baselines.

Conclusion: ECHO represents a significant advancement in community detection, offering a scalable and efficient solution that integrates both topological and semantic information, thereby overcoming the classical resolution limit and computational challenges associated with large-scale network analysis.

Abstract: Community detection in attributed networks faces a fundamental divide: topological algorithms ignore semantic features, while Graph Neural Networks (GNNs) encounter devastating computational bottlenecks. Specifically, GNNs suffer from a Semantic Wall of feature over smoothing in dense or heterophilic networks, and a Systems Wall driven by the O(N^2) memory constraints of pairwise clustering. To dismantle these barriers, we introduce ECHO (Encoding Communities via High order Operators), a scalable, self supervised architecture that reframes community detection as an adaptive, multi scale diffusion process. ECHO features a Topology Aware Router that automatically analyzes structural heuristics sparsity, density, and assortativity to route graphs through the optimal inductive bias, preventing heterophilic poisoning while ensuring semantic densification. Coupled with a memory sharded full batch contrastive objective and a novel chunked O(N \cdot K) similarity extraction method, ECHO completely bypasses traditional O(N^2) memory bottlenecks without sacrificing the mathematical precision of global gradients. Extensive evaluations demonstrate that this topology feature synergy consistently overcomes the classical resolution limit. On synthetic LFR benchmarks scaled up to 1 million nodes, ECHO achieves scale invariant accuracy despite severe topological noise. Furthermore, on massive real world social networks with over 1.6 million nodes and 30 million edges, it completes clustering in mere minutes with throughputs exceeding 2,800 nodes per second matching the speed of highly optimized purely topological baselines. The implementation utilizes a unified framework that automatically engages memory sharded optimization to support adoption across varying hardware constraints. GitHub Repository: https://github.com/emilioferrara/ECHO-GNN

</details>


### [82] [Beyond performance-wise Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.22470)
*Balazs Pejo*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习中客户端对模型可信度（包括可靠性、弹性和公平性）的贡献，发现现有的基于性能的评估方法不足以全面评估和公平奖励分配。


<details>
  <summary>Details</summary>
Motivation: 联邦学习提供了一个隐私友好的协作学习框架，但其成功取决于参与者的贡献。现有客户端评估方法主要关注模型性能，如准确率或损失，这仅代表机器学习模型整体效用的一个维度。本文旨在探讨一个关键但被忽视的问题：客户端对模型可信度的贡献，具体指模型的可靠性（对噪声数据的容忍度）、弹性（对抗样本的抵抗力）以及公平性（通过人口统计学平等衡量）。

Method: 为了量化这些多方面的贡献，作者采用了Shapley值的最新近似方法，这是一种有原则的价值归属方法。

Result: 结果表明，没有一个客户端在所有维度上都表现出色，并且这些维度彼此之间很大程度上是独立的。

Conclusion: 这揭示了当前评估方案的一个关键缺陷：没有任何单一指标能够充分地进行综合评估和公平奖励分配。

Abstract: Federated learning offers a privacy-friendly collaborative learning framework, yet its success, like any joint venture, hinges on the contributions of its participants. Existing client evaluation methods predominantly focus on model performance, such as accuracy or loss, which represents only one dimension of a machine learning model's overall utility. In contrast, this work investigates the critical, yet overlooked, issue of client contributions towards a model's trustworthiness -- specifically, its reliability (tolerance to noisy data), resilience (resistance to adversarial examples), and fairness (measured via demographic parity). To quantify these multifaceted contributions, we employ the state-of-the-art approximation of the Shapley value, a principled method for value attribution. Our results reveal that no single client excels across all dimensions, which are largely independent from each other, highlighting a critical flaw in current evaluation scheme: no single metric is adequate for comprehensive evaluation and equitable rewarding allocation.

</details>


### [83] [Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns](https://arxiv.org/abs/2602.22479)
*Afshin Khadangi*

Main category: cs.LG

TL;DR: 本文提出了一种名为TRC^2的新架构，旨在解决语言模型在持续学习中遇到的灾难性遗忘问题。通过结合皮层柱上的稀疏丘脑路由以及快速修正路径等机制，TRC^2能够在保持先前习得行为的同时迅速适应新数据流，从而改善了稳定性与可塑性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在面对非稳定数据时，标准训练和微调流程容易出现灾难性遗忘的问题。尽管有一些方法可以提高模型的稳定性，但它们往往会导致延迟增加、内存占用变大或者计算密集度提升等问题，尤其是在处理长上下文时表现不佳。因此，需要一种新的架构来改进连续学习中的稳定性-可塑性权衡。

Method: 研究者们提出了TRC^2（Thalamically Routed Cortical Columns），这是一种仅解码器后端设计，它从架构层面着手解决持续学习挑战。TRC^2整合了皮质柱上稀疏的丘脑路由技术，加上调节、预测、记忆及反馈机制，并且引入了一条快速纠正路径以支持快速调整而不破坏较慢参数的稳定性。该架构具有稀疏性和块并行处理能力，使得训练和推理过程更加高效。

Result: 实验结果表明，在语言建模和持续学习基准测试中，TRC^2能够以相似的计算量改善稳定性-可塑性之间的平衡，允许在数据流中快速适应同时保留先前学到的行为。

Conclusion: TRC^2提供了一种有效的方法来增强语言模型在持续学习任务中的性能，特别是对于那些要求模型既能快速适应新信息又能记住旧知识的应用场景来说，TRC^2展现出了巨大潜力。

Abstract: Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC$^{2}$ (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC$^{2}$ combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC$^{2}$ improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.

</details>


### [84] [Reinforcement-aware Knowledge Distillation for LLM Reasoning](https://arxiv.org/abs/2602.22495)
*Zhaoyang Zhang,Shuli Jiang,Yantao Shen,Yuting Zhang,Dhananjay Ram,Shuo Yang,Zhuowen Tu,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: 该论文提出了一种新的知识蒸馏方法RLAD（Reinforcement Learning-aware Distillation），通过在强化学习过程中进行选择性模仿来解决现有方法中的分布不匹配和目标干扰问题。核心组件TRRD利用PPO/GRPO样式的似然比目标替代了传统的KL正则化，实现了基于优势感知的信任区域约束蒸馏，从而更好地平衡探索、开发与模仿。实验结果表明，RLAD在多种逻辑推理和数学基准测试中均优于离线蒸馏、标准GRPO以及基于KL的在线教师-学生知识蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法大多针对监督微调设计，在与强化学习结合时面临分布不匹配和目标冲突的问题：教师模型的指导可能与学生模型当前的rollout分布不符，而KL正则化器可能会与奖励最大化的目标产生竞争，并需要仔细调整损失权重。为了解决这些问题，作者提出了RLAD方法。

Method: 本文提出的方法名为RLAD (Reinforcement Learning-aware Distillation)，其核心组成部分是Trust Region Ratio Distillation (TRRD)。TRRD将传统的教师-学生KL正则化替换成了一个类似于PPO/GRPO风格的似然比目标函数，该目标函数基于教师-旧策略混合体构建，能够在学生rollouts上实现基于优势感知且受信任区域限制的知识蒸馏。

Result: 实验结果表明，在不同的逻辑推理和数学基准测试中，RLAD相较于离线蒸馏、标准GRPO以及基于KL的在线教师-学生知识蒸馏方法表现出更好的性能。

Conclusion: RLAD提供了一种有效的方法来减少大型语言模型的推理成本，同时保持甚至提高它们在复杂任务上的表现。这种方法通过在强化学习过程中智能地整合教师模型的知识，解决了传统知识蒸馏技术面临的挑战。

Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.

</details>


### [85] [Sharp Convergence Rates for Masked Diffusion Models](https://arxiv.org/abs/2602.22505)
*Yuchen Liang,Zhiheng Tan,Ness Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文针对离散扩散模型中的采样器进行了理论分析，特别是对Euler方法和首次命中采样器（FHS）进行了总变差（TV）距离上的直接分析，改进了参数依赖关系，并且放松了得分估计的假设条件。此外，还提供了Euler采样器的收敛下界以及FHS采样器的误差紧致性证明。


<details>
  <summary>Details</summary>
Motivation: 尽管Euler方法和最近提出的首次命中采样器在实际应用中表现出色，但它们的理论基础仍然不足。现有分析通常基于Kullback-Leibler (KL) 散度，这往往导致参数依赖性较宽松，并且对得分估计有着较强的假设要求。因此，有必要发展更直接有效的分析方法来理解这些采样器的行为。

Method: 通过采用基于总变差（TV）距离的方法，文章首先对Euler方法进行了分析，该分析不仅放宽了对得分估计的假设，而且改善了参数之间的依赖关系。同时，对于首次命中采样器（FHS），提出了一种解耦路径分析法，证明其除了由得分估计引起的误差外不再引入额外的抽样误差。

Result: 研究结果表明，Euler方法可以在不需任何替代初始化的情况下建立收敛保证；同时也给出了Euler采样器关于数据维度$d$和目标精度$\varepsilon$的紧致收敛下界。对于FHS采样器，研究表明其误差仅来自于得分估计，并且这一结论是紧致的。

Conclusion: 本研究通过对Euler方法和FHS采样器进行基于TV距离的新颖分析，为理解这些采样技术提供了更加坚实的理论基础。特别是对于FHS采样器而言，这种分析方法揭示了其性能优越的原因，并可能为未来的研究提供新的视角。

Abstract: Discrete diffusion models have achieved strong empirical performance in text and other symbolic domains, with masked (absorbing-rate) variants emerging as competitive alternatives to autoregressive models. Among existing samplers, the Euler method remains the standard choice in many applications, and more recently, the First-Hitting Sampler (FHS) has shown considerable promise for masked diffusion models. Despite their practical success, the theoretical understanding of these samplers remains limited. Existing analyses are conducted in Kullback-Leibler (KL) divergence, which often yields loose parameter dependencies and requires strong assumptions on score estimation. Moreover, these guarantees do not cover recently developed high-performance sampler of FHS. In this work, we first develop a direct total-variation (TV) based analysis for the Euler method that overcomes these limitations. Our results relax assumptions on score estimation, improve parameter dependencies, and establish convergence guarantees without requiring any surrogate initialization. Also for this setting, we provide the first convergence lower bound for the Euler sampler, establishing tightness with respect to both the data dimension $d$ and the target accuracy $\varepsilon$. Finally, we analyze the FHS sampler and show that it incurs no sampling error beyond that induced by score estimation, which we show to be tight with a matching lower error bound. Overall, our analysis introduces a direct TV-based error decomposition along the CTMC trajectory and a decoupling-based path-wise analysis for FHS, which may be of independent interest.

</details>


### [86] [Space Syntax-guided Post-training for Residential Floor Plan Generation](https://arxiv.org/abs/2602.22507)
*Zhuoyang Jiang,Dongqing Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种空间句法引导的后训练方法（SSPT），通过非可微预言机将空间句法知识注入到平面图生成中，从而优化了公共空间的主导性和功能层次。此外，还引入了一个名为SSPT-Bench (Eval-8) 的基准测试集来评估模型性能。实验结果表明，与基于分布拟合的基线相比，该方法在公共空间的主导性和功能层次恢复方面有所改进，特别是使用PPO强化学习策略时，计算效率更高且方差更小。


<details>
  <summary>Details</summary>
Motivation: 预训练的住宅平面图生成模型通常被优化以适应大规模数据分布，但这可能会忽略一些重要的建筑先验知识，比如家庭公共空间（如客厅和门厅）的配置优势和连通性。为了解决这个问题，并将这些关键的建筑设计原则融入到生成模型中，提出了空间句法引导的后训练方法。

Method: 文章介绍的方法包括两个主要组成部分：一是Space Syntax-guided Post-training (SSPT)，它通过一个非可微预言机将空间句法知识整合进平面图生成过程中；二是SSPT-Bench (Eval-8)，一个用于模型评估的基准测试集。SSPT实现采用两种策略：(i) 通过空间句法过滤和扩散微调进行迭代再训练；(ii) 使用带有空间句法奖励的PPO算法进行强化学习。

Result: 实验结果显示，相对于单纯依赖于数据分布拟合的传统方法，所提出的两种策略均能显著改善生成平面图中公共空间的主导地位及清晰的功能层级结构。特别是采用PPO算法时，在保持较高计算效率的同时也降低了结果的变异性。

Conclusion: 研究证明了空间句法引导的后训练方法可以有效地将建筑学理论融入数据驱动的平面图生成过程之中，为提高住宅平面图设计质量提供了一条可行之路。同时，该方法具有良好的扩展性，能够与其他生成模型框架相结合。

Abstract: Pre-trained generative models for residential floor plans are typically optimized to fit large-scale data distributions, which can under-emphasize critical architectural priors such as the configurational dominance and connectivity of domestic public spaces (e.g., living rooms and foyers). This paper proposes Space Syntax-guided Post-training (SSPT), a post-training paradigm that explicitly injects space syntax knowledge into floor plan generation via a non-differentiable oracle. The oracle converts RPLAN-style layouts into rectangle-space graphs through greedy maximal-rectangle decomposition and door-mediated adjacency construction, and then computes integration-based measurements to quantify public space dominance and functional hierarchy.
  To enable consistent evaluation and diagnosis, we further introduce SSPT-Bench (Eval-8), an out-of-distribution benchmark that post-trains models using conditions capped at $\leq 7$ rooms while evaluating on 8-room programs, together with a unified metric suite for dominance, stability, and profile alignment. SSPT is instantiated with two strategies: (i) iterative retraining via space-syntax filtering and diffusion fine-tuning, and (ii) reinforcement learning via PPO with space-syntax rewards. Experiments show that both strategies improve public-space dominance and restore clearer functional hierarchy compared to distribution-fitted baselines, while PPO achieves stronger gains with substantially higher compute efficiency and reduced variance. SSPT provides a scalable pathway for integrating architectural theory into data-driven plan generation and is compatible with other generative backbones given a post-hoc evaluation oracle.

</details>


### [87] [TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series](https://arxiv.org/abs/2602.22520)
*Xiannan Huang,Shen Fang,Shuhan Qiu,Chengcheng Yu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: TEFL (Temporal Error Feedback Learning) 是一种新的时间序列预测学习框架，通过在训练和评估过程中整合历史预测残差来提高预测准确性。实验表明，TEFL能够在多种真实数据集上显著降低平均绝对误差（MAE），特别是在面对突发变化和分布偏移时表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代深度预测模型通常仅针对点预测损失进行优化，并未充分利用滚动预测产生的历史残差信息，这些信息可能包含持续偏差、未建模模式或演变动态等有价值的内容。

Method: 提出了一种名为TEFL（Temporal Error Feedback Learning）的学习框架，该框架将历史残差显式地纳入到预测流程中。为了解决多步预测设置中的实际应用问题，TEFL解决了三个关键挑战：选择可观察的多步残差、通过轻量级低秩适配器集成残差以保持效率并防止过拟合，以及设计了一个两阶段训练过程来共同优化基础预测器和错误模块。

Result: 在10个真实世界数据集和5种骨干架构上的广泛实验表明，TEFL能够一致地提高预测精度，平均减少5-10%的MAE。此外，在面临突然变化和分布转移的情况下，TEFL展示了强大的鲁棒性，错误减少超过10%，最高达到19.5%。

Conclusion: 通过直接将基于残差的反馈嵌入到学习过程中，TEFL为现代深度预测系统提供了一种简单、通用且有效的改进方案。

Abstract: Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.

</details>


### [88] [Predicting Tennis Serve directions with Machine Learning](https://arxiv.org/abs/2602.22527)
*Ying Zhu,Ruthuparna Naikar*

Main category: cs.LG

TL;DR: 本研究开发了一种机器学习方法来预测职业网球运动员的第一发球方向，揭示了顶级选手可能在发球决策中使用混合策略模型，并且疲劳可能是选择发球方向的一个因素。此外，研究表明对于回球者来说，情境信息可能比先前认为的更重要。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过理解球员的发球决策来帮助分析职业网球比赛中服务器与回球者之间的心理博弈。

Method: 采用机器学习方法结合特征工程，用于预测职业网球选手的第一发球方向。

Result: 该方法对男性选手的平均预测准确率为49%，对女性选手为44%。

Conclusion: 结果表明顶尖职业选手在发球时可能采取混合策略模型行事，并指出疲劳可能是影响发球方向选择的一个因素；同时强调了对于回球者而言，情境信息的重要性可能超出以往的认知。

Abstract: Serves, especially first serves, are very important in professional tennis. Servers choose their serve directions strategically to maximize their winning chances while trying to be unpredictable. On the other hand, returners try to predict serve directions to make good returns. The mind game between servers and returners is an important part of decision-making in professional tennis matches. To help understand the players' serve decisions, we have developed a machine learning method for predicting professional tennis players' first serve directions. Through feature engineering, our method achieves an average prediction accuracy of around 49\% for male players and 44\% for female players. Our analysis provides some evidence that top professional players use a mixed-strategy model in serving decisions and that fatigue might be a factor in choosing serve directions. Our analysis also suggests that contextual information is perhaps more important for returners' anticipatory reactions than previously thought.

</details>


### [89] [Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization](https://arxiv.org/abs/2602.22536)
*Jichao Zhang,Ran Miao,Limin Li*

Main category: cs.LG

TL;DR: 本文提出了持久非负矩阵分解(pNMF)，这是一种参数化的NMF问题系列，能够生成一系列跨分辨率一致的嵌入表示，通过利用持久同调识别出在哪个尺度上连接性结构经历质变，并引入了一系列图拉普拉斯算子来形成一个耦合的NMF公式，具有尺度级几何正则化和显式的跨尺度一致性约束。


<details>
  <summary>Details</summary>
Motivation: 现有的基于NMF的方法本质上是单尺度的，无法捕捉跨分辨率的连接结构演变。

Method: 提出了一种新的方法——持久非负矩阵分解（pNMF），它是一种根据尺度参数化的NMF问题系列。该方法利用持久同调理论确定了在哪个尺度上底层连接经历了质的变化，并基于这些尺度诱导出一系列图拉普拉斯算子，进而形成了具有尺度级几何正则化和显式跨尺度一致性约束的耦合NMF公式。此外，还为解决由此产生的模型开发了一个保证收敛的顺序交替优化算法。

Result: 数值实验表明，所提方法在合成数据集以及单细胞RNA测序数据集上对于多尺度低秩嵌入的有效性得到了验证。

Conclusion: 本研究提出的pNMF方法成功地解决了现有NMF技术中未能有效处理跨分辨率数据的问题，为多尺度数据分析提供了有力工具。

Abstract: Matrix factorization techniques, especially Nonnegative Matrix Factorization (NMF), have been widely used for dimensionality reduction and interpretable data representation. However, existing NMF-based methods are inherently single-scale and fail to capture the evolution of connectivity structures across resolutions. In this work, we propose persistent nonnegative matrix factorization (pNMF), a scale-parameterized family of NMF problems, that produces a sequence of persistence-aligned embeddings rather than a single one. By leveraging persistent homology, we identify a canonical minimal sufficient scale set at which the underlying connectivity undergoes qualitative changes. These canonical scales induce a sequence of graph Laplacians, leading to a coupled NMF formulation with scale-wise geometric regularization and explicit cross-scale consistency constraint. We analyze the structural properties of the embeddings along the scale parameter and establish bounds on their increments between consecutive scales. The resulting model defines a nontrivial solution path across scales, rather than a single factorization, which poses new computational challenges. We develop a sequential alternating optimization algorithm with guaranteed convergence. Numerical experiments on synthetic and single-cell RNA sequencing datasets demonstrate the effectiveness of the proposed approach in multi-scale low-rank embeddings.

</details>


### [90] [LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation](https://arxiv.org/abs/2602.22537)
*Shouwei Gao,Xu Zheng,Dongsheng Luo,Sheng Di,Wenqian Dong*

Main category: cs.LG

TL;DR: 本文介绍了一个名为LUMOS的端到端框架，它基于L0正则化学习，能够统一特征选择和模型剪枝，以简化科学机器学习（SciML）模型的设计。通过在13个不同的SciML工作负载上进行评估，展示了LUMOS的有效性和通用性，平均减少了71.45%的参数并提升了6.4倍的推理速度。


<details>
  <summary>Details</summary>
Motivation: 设计有效的科学机器学习模型通常需要大量的先验知识与手动调优，特别是在决定使用哪些输入特征以及模型规模方面。为了降低对人工调整的依赖同时保持预测准确性，提出了LUMOS框架。

Method: LUMOS是一个基于L0正则化学习的端到端框架，利用半随机门控和重新参数化技术，在训练过程中动态选择信息量大的特征并对冗余参数进行剪枝。

Result: 实验显示，LUMOS在13种不同的SciML模型上实现了平均71.45%的参数减少，并且平均加快了6.4倍的推理速度。此外，还验证了LUMOS在多达八块GPU上的分布式数据并行训练中的可扩展性。

Conclusion: LUMOS提供了一种有效的方法来简化SciML模型的设计流程，通过自动化的特征选择和模型剪枝降低了对专家知识的需求，同时保证了模型的准确性和效率。

Abstract: The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of

</details>


### [91] [Relatron: Automating Relational Machine Learning over Relational Databases](https://arxiv.org/abs/2602.22552)
*Zhikai Chen,Han Xie,Jian Zhang,Jiliang Tang,Xiang Song,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 研究了关系深度学习（RDL）和深度特征合成（DFS）在关系数据库预测建模中的表现，提出了一个名为Relatron的任务嵌入式元选择器，能够根据任务特性选择合适的模型，并通过轻量级损失景观指标优化模型选择。实验表明，Relatron相比强基准线提高了高达18.5%的性能，同时成本仅为基于Fisher信息方法的十分之一。


<details>
  <summary>Details</summary>
Motivation: 关系数据库(RDB)上的预测建模支持多种应用，但同时捕捉跨表依赖性和复杂特征交互具有挑战性。尽管关系深度学习(RDL)方法通过消息传递自动化特征工程表现出色，但与经典方法如深度特征合成(DFS)相比，其相对优势及有效架构设计原则仍不明确。

Method: 本研究统一了RDL和DFS的设计空间，并对不同RDB任务进行了以架构为中心的搜索。分析结果得出了三个关键发现：(1) RDL的表现并不总是优于DFS，性能高度依赖于具体任务；(2) 没有一种架构能在所有任务中占优，强调了需要根据任务来选择模型的重要性；(3) 验证准确率不是选择架构的可靠指南。基于这些发现，研究人员创建了一个模型性能库，并提出了一种名为Relatron的任务嵌入式元选择器，该选择器可以根据任务信号来决定使用RDL还是DFS，并缩小家族内搜索范围。此外，还利用轻量级损失景观度量来避免脆弱的检查点，偏好更平坦的最优解。

Result: 实验结果显示，Relatron解决了“更多调整，更差表现”的问题，在联合超参数-架构优化中相较于强大的基线方法实现了最高达18.5%的性能提升，而且成本仅为采用Fisher信息方法的十分之一。

Conclusion: 本研究表明，对于关系数据库上的预测建模而言，没有一种通用的最佳方法适合所有情况；相反，根据特定任务的特点选择适当的方法是非常重要的。Relatron提供了一种有效的手段来指导这种选择过程，同时通过优化搜索策略显著降低了成本。

Abstract: Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the "more tuning, worse performance" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.

</details>


### [92] [Multilingual Safety Alignment Via Sparse Weight Editing](https://arxiv.org/abs/2602.22554)
*Jiaming Liang,Zhaoxin Wang,Handing Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏权重编辑的无需训练的对齐框架，旨在解决低资源语言在大型语言模型中安全机制不足的问题。通过约束线性变换将低资源语言中的有害表示映射到高资源语言的安全子空间，并保持了通用实用性。实验表明，该方法显著降低了低资源语言的攻击成功率，同时几乎不影响一般推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对于不同语言的安全性能存在明显差异，特别是低资源语言容易绕过为高资源语言（如英语）设计的安全防护措施。现有的解决方案要么计算成本高昂，要么依赖于稀缺的多语言安全数据。因此，研究者们提出了一种新的、无需额外训练的方法来改善这一状况。

Method: 提出的方法基于稀疏权重编辑，发现安全功能主要集中在一小部分‘安全神经元’上。通过将跨语言对齐问题形式化为一种受约束的线性转换，研究者们找到了一个封闭形式解，能够最优地把低资源语言中的有害表达映射到高资源语言更加稳固的安全子空间内，同时利用零空间投影限制保证了整体效用不受影响。

Result: 通过对8种语言和多个模型家族（包括Llama-3, Qwen-2.5等）进行广泛测试后，结果表明所提出的方法能显著降低低资源语言环境下的攻击成功率(ASR)，而对一般的逻辑推理能力几乎没有负面影响。这些改进都是通过一次高效的数据处理实现的。

Conclusion: 研究表明，通过采用稀疏权重编辑技术可以有效提升低资源语言在大型语言模型中的安全性，同时保持良好的通用性能。这为解决多语言环境下存在的安全差异提供了一个新思路。

Abstract: Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.

</details>


### [93] [Autoregressive Visual Decoding from EEG Signals](https://arxiv.org/abs/2602.22555)
*Sicheng Dai,Hongwang Xiao,Shan Yu,Qiwei Ye*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级高效的从EEG信号解码视觉信息的框架AVDE，通过预训练模型和对比学习对齐EEG与图像表示，并采用自回归生成框架预测更细尺度的标记以实现连贯的图像生成。实验表明，AVDE在图像检索和重建任务中优于现有方法，同时参数使用量仅为10%。


<details>
  <summary>Details</summary>
Motivation: 当前基于EEG信号解码视觉信息的方法面临模态差距大、计算开销高及多阶段适应过程复杂等问题，限制了其在实际脑机接口应用中的实用性。

Method: 首先利用预训练的EEG模型LaBraM并通过对比学习微调来对齐EEG与图像表示；其次采用基于'下一尺度预测'策略的自回归生成框架：图像被编码为多尺度标记图，然后训练一个转换器从作为最粗略表示的EEG嵌入开始自回归地预测更细尺度的标记。

Result: 在两个数据集上的实验显示，AVDE在图像检索和重建任务上都超过了之前最先进的方法，且只使用了10%的参数。此外，中间输出的可视化表明AVDE的生成过程反映了人类视觉感知的层次性。

Conclusion: 研究结果强调了自回归模型作为高效且可解释工具在实用BCI应用中的潜力。

Abstract: Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely on complex adaptation processes involving multiple stages, making it hard to maintain consistency and manage compounding errors. Furthermore, the computational overhead imposed by large-scale diffusion models limit their practicality in real-world brain-computer interface (BCI) applications. In this work, we present AVDE, a lightweight and efficient framework for visual decoding from EEG signals. First, we leverage LaBraM, a pre-trained EEG model, and fine-tune it via contrastive learning to align EEG and image representations. Second, we adopt an autoregressive generative framework based on a "next-scale prediction" strategy: images are encoded into multi-scale token maps using a pre-trained VQ-VAE, and a transformer is trained to autoregressively predict finer-scale tokens starting from EEG embeddings as the coarsest representation. This design enables coherent generation while preserving a direct connection between the input EEG signals and the reconstructed images. Experiments on two datasets show that AVDE outperforms previous state-of-the-art methods in both image retrieval and reconstruction tasks, while using only 10% of the parameters. In addition, visualization of intermediate outputs shows that the generative process of AVDE reflects the hierarchical nature of human visual perception. These results highlight the potential of autoregressive models as efficient and interpretable tools for practical BCI applications.

</details>


### [94] [Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation](https://arxiv.org/abs/2602.22556)
*Zihang Xu,Haozhi Xie,Ziqi Miao,Wuxuan Gong,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 提出了一种两阶段框架，通过混合微调和自适应强化学习来稳定大型推理模型中的自适应思考过程，从而改善了准确性-效率的权衡问题，并在不同难度的问题和分布外任务上展示了稳健性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）对于低复杂度查询表现出过度思考的行为，现有的解决方法在准确性和效率之间的平衡不稳定，且对异质性推理行为的鲁棒性较差。

Method: 该研究提出了一个两阶段框架：首先采用混合微调使模型暴露于思考与不思考两种行为模式下，建立良好的初始化条件；然后进行带有正确性保持优势塑造(CPAS)的自适应强化学习以及长度感知梯度调节(LAGR)，以避免抑制正确的长链推理并稳定优化。

Result: 实验结果表明，相比强大的基线，在Qwen2.5-1.5B 和 7B上实现了高达+3.7/+3.6的准确率提升，同时减少了40.6%/43.9%的生成令牌数。进一步分析显示了方法在不同难度问题及分布外任务上的鲁棒性和泛化能力。

Conclusion: 所提出的两阶段框架有效解决了大型推理模型中因过度思考而导致的准确性-效率权衡问题，提高了模型处理不同类型问题时的表现。

Abstract: Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.

</details>


### [95] [Operationalizing Fairness: Post-Hoc Threshold Optimization Under Hard Resource Limits](https://arxiv.org/abs/2602.22560)
*Moirangthem Tiken Singh,Amit Kalita,Sapam Jitu Singh*

Main category: cs.LG

TL;DR: 本文提出了一种后处理、模型无关的阈值优化框架，该框架在严格的资源限制下同时平衡了安全性、效率和公平性，并强制执行单一的全局决策阈值。实验结果表明，在资源受限的情况下，该框架能够保持较高的风险识别率，而标准的无约束公平启发式方法则几乎无效。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署机器学习需要在预测安全性和算法公平性之间找到平衡点。然而，现有的公平干预措施往往假设资源不受限制，并使用违反反歧视法规的群体特定决策阈值。因此，有必要开发一种既符合法律要求又能有效平衡各种伦理目标的方法。

Method: 引入了一个后处理、模型无关的阈值优化框架，它能够在严格且硬性的能力约束条件下共同平衡安全性、效率与平等。为了确保法律合规性，该框架实施了一个单一的全球决策阈值。通过参数化道德损失函数结合有界决策规则来防止干预量超过可用资源。

Result: 主要结果显示，在测试配置中超过80%的情况下，容量限制决定了最终部署的阈值；在25%的严格容量限制下，所提出的框架成功维持了较高的风险识别（召回率范围为0.409至0.702），而标准无约束公平启发式方法几乎失去效用。

Conclusion: 理论上的公平目标必须明确从属于操作能力限制才能保持部署状态。通过将预测评分与政策评估分开并严格限制干预率，该框架为利益相关者提供了一种实用且合法的机制以导航资源受限环境下的不可避免的伦理权衡。

Abstract: The deployment of machine learning in high-stakes domains requires a balance between predictive safety and algorithmic fairness. However, existing fairness interventions often as- sume unconstrained resources and employ group-specific decision thresholds that violate anti- discrimination regulations. We introduce a post-hoc, model-agnostic threshold optimization framework that jointly balances safety, efficiency, and equity under strict and hard capacity constraints. To ensure legal compliance, the framework enforces a single, global decision thresh- old. We formulated a parameterized ethical loss function coupled with a bounded decision rule that mathematically prevents intervention volumes from exceeding the available resources. An- alytically, we prove the key properties of the deployed threshold, including local monotonicity with respect to ethical weighting and the formal identification of critical capacity regimes. We conducted extensive experimental evaluations on diverse high-stakes datasets. The principal re- sults demonstrate that capacity constraints dominate ethical priorities; the strict resource limit determines the final deployed threshold in over 80% of the tested configurations. Furthermore, under a restrictive 25% capacity limit, the proposed framework successfully maintains high risk identification (recall ranging from 0.409 to 0.702), whereas standard unconstrained fairness heuristics collapse to a near-zero utility. We conclude that theoretical fairness objectives must be explicitly subordinated to operational capacity limits to remain in deployment. By decou- pling predictive scoring from policy evaluation and strictly bounding intervention rates, this framework provides a practical and legally compliant mechanism for stakeholders to navigate unavoidable ethical trade-offs in resource-constrained environments.

</details>


### [96] [S2O: Early Stopping for Sparse Attention via Online Permutation](https://arxiv.org/abs/2602.22575)
*Yu Zhang,Songwei Liu,Chenqian Yan,Sheng Lin,Beichen Ning,Fangmin Chen,Xing Wang*

Main category: cs.LG

TL;DR: S2O通过在线置换和早期停止策略提高了注意力机制的稀疏性上限，显著降低了计算密度并加速了推理过程，同时保持了端到端的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于注意力机制随着序列长度呈二次增长，极大限制了长上下文推理。现有的块级稀疏化方法虽然可以减少延迟，但粗粒度的块结构限制了进一步提高稀疏性的可能性。

Method: S2O采用了一种基于在线置换的方法来实现稀疏注意力的提前停止。该方法受到内存系统中虚拟到物理地址映射的启发，重新审视并分解了FlashAttention执行流程，允许非连续token加载而非按原始顺序加载连续跨度。此外，S2O引入了一个依据重要性从高到低处理的提前停止规则，当当前块得分低于阈值时即终止计算以跳过贡献较小的部分。

Result: 在Llama-3.1-8B模型下使用128K上下文时，S2O能够在匹配稀疏度条件下将单操作MSE降低3.82倍，并且在匹配MSE条件下减少了3.31倍的预填充计算密度；同时，它保持了端到端的准确性，并实现了7.51倍的注意力加速以及3.81倍的整体加速。

Conclusion: S2O有效地提升了实际应用中的稀疏性上限，减少了计算量并加速了模型推理速度，同时确保了最终结果的质量不受影响。

Abstract: Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We present S2O, which performs early stopping for sparse attention via online permutation. Inspired by virtual-to-physical address mapping in memory systems, S2O revisits and factorizes FlashAttention execution, enabling inference to load non-contiguous tokens rather than a contiguous span in the original order. Motivated by fine-grained structures in attention heatmaps, we transform explicit permutation into an online, index-guided, discrete loading policy; with extremely lightweight preprocessing and index-remapping overhead, it concentrates importance on a small set of high-priority blocks. Building on this importance-guided online permutation for loading, S2O further introduces an early-stopping rule: computation proceeds from high to low importance; once the current block score falls below a threshold, S2O terminates early and skips the remaining low-contribution blocks, thereby increasing effective sparsity and reducing computation under a controlled error budget.
  As a result, S2O substantially raises the practical sparsity ceiling. On Llama-3.1-8B under a 128K context, S2O reduces single-operator MSE by 3.82$\times$ at matched sparsity, and reduces prefill compute density by 3.31$\times$ at matched MSE; meanwhile, it preserves end-to-end accuracy and achieves 7.51$\times$ attention and 3.81$\times$ end-to-end speedups.

</details>


### [97] [IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck](https://arxiv.org/abs/2602.22581)
*Tian Bian,Yifan Niu,Chaohao Yuan,Chengzhi Piao,Bingzhe Wu,Long-Kai Huang,Yu Rong,Tingyang Xu,Hong Cheng,Jia Li*

Main category: cs.LG

TL;DR: 提出了一种基于信息瓶颈原则的端到端方法IBCircuit，用于全面发现语言模型中的计算子图（电路），无需为不同任务设计特定的激活破坏方案。在间接对象识别和大于任务中，IBCircuit相比最近的相关工作能更准确地识别关键节点和边组件。


<details>
  <summary>Details</summary>
Motivation: 现有的电路发现研究忽略了这些电路的整体性质，并且需要为不同的任务设计特定的激活破坏方案，这种方法不够准确且效率低下。

Method: 提出了一个基于信息瓶颈原则的名为IBCircuit的优化框架，该框架能够以整体的方式发现电路，并适用于任何给定的任务而不需要繁琐的激活破坏设计。

Result: 在间接对象识别(IOI)和大于任务中，IBCircuit相较于近期相关工作，在关键节点组成和边组成方面识别出更为忠实和精简的电路。

Conclusion: IBCircuit提供了一种有效的方法来综合识别对解决特定任务至关重要的计算子图或电路，表明了其在提高电路发现准确性方面的潜力。

Abstract: Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.

</details>


### [98] [TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion](https://arxiv.org/abs/2602.22586)
*Donghong Cai,Jiarui Feng,Yanbo Wang,Da Zheng,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为TabDLM的框架，用于通过联合数值-语言扩散模型生成自由形式的表格数据。该方法结合了掩码扩散和连续扩散过程来处理文本、分类及数值特征，并通过双向注意力机制捕捉跨模态交互。实验表明，与基于扩散模型和大语言模型的方法相比，TabDML在多种基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中的表格数据集越来越包含开放性文本字段（例如评论或临床笔记）以及结构化的数字和分类属性，因此如何有效生成这种异构表格成为了一个挑战。现有方法要么难以扩展到开放文本导致文本质量下降，要么在处理精确或范围广泛的数值时遇到困难。

Method: 提出了TabDLM，一种利用掩码扩散语言模型(MDLMs)构建的联合数值-语言扩散模型框架。它通过掩码扩散处理文本和分类特征，同时通过学习特定的数字令牌嵌入以连续扩散过程处理数值特征；接着利用双向注意力机制在一个模型内捕捉不同模态间的相互作用。

Result: 广泛的实验证明，在多样化基准测试上，与强大的基于扩散模型和大语言模型的基线相比，TabDLM展现出了有效性。

Conclusion: 本研究开发的TabDLM为解决包含自由格式文本字段的复杂表格数据生成问题提供了新的思路，证明了其相对于现有技术的优势。

Abstract: Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.

</details>


### [99] [pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training](https://arxiv.org/abs/2602.22592)
*Wenzheng Zhang,Bingzheng Liu,Yang Hu,Xiaoying Bai,Wentao Zhang,Bin Cui*

Main category: cs.LG

TL;DR: 本文提出了一种名为pQuant的方法，通过将线性层拆分为1比特主分支和高精度分支来解决参数民主化效应问题，从而在极低比特量化的情况下提高大型语言模型的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前从零开始的量化感知训练方法虽然能够构建具有极低比特权重（低于2比特）的高效大语言模型，但存在准确度不高、难以扩展的问题。研究者发现了其中的关键瓶颈是参数民主化效应，即所有参数的敏感性变得均等，严重限制了表达能力。

Method: 提出了一种名为pQuant的新方法，该方法通过将线性层分解为两个专门的分支——一个用于高效计算的主导1比特分支和一个专注于保留最敏感参数的小型高精度分支——来解决这个问题。此外，通过定制化的特征缩放，指导模型将敏感参数分配给高精度分支，并进一步扩展这些分支成为多个稀疏激活的专业模块，以实现高效的容量扩展。

Result: 广泛的实验表明，pQuant在极端低比特量化方面达到了最先进的性能。

Conclusion: pQuant提供了一个有效的解决方案，解决了现有低比特量化方法中遇到的准确性与可扩展性挑战，为边缘部署提供了显著优势。

Abstract: Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.

</details>


### [100] [Transformers converge to invariant algorithmic cores](https://arxiv.org/abs/2602.22600)
*Joshua S. Schiffman*

Main category: cs.LG

TL;DR: 本文通过提取算法核心，揭示了不同训练运行和规模下保持不变的低维不变量，表明变压器计算是围绕紧凑、共享的算法结构组织的。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展示了复杂的能力，但理解它们内部的工作机制仍然是一个核心挑战。训练选择的是行为而非电路，因此许多权重配置可以实现相同的功能。本研究旨在区分哪些内部结构反映了计算，哪些仅仅是特定训练过程的偶然结果。

Method: 本研究开发了一种方法来提取算法核心：即对任务性能来说必要且充分的紧凑子空间。通过对独立训练的变压器进行分析，发现尽管学习到了不同的权重，但最终会收敛到相同的算法核心上。

Result: 研究发现，马尔可夫链变压器在几乎正交的子空间中嵌入3D核心，但恢复出完全相同的转换谱；模块化加法变压器在理解时发现了紧凑的循环算子，随后膨胀，为记忆到泛化的转变提供了预测模型；GPT-2语言模型通过单轴控制主谓一致，当该轴被翻转时，在整个生成过程中跨尺度地反转语法数。这些结果显示了跨越训练运行和规模而持续存在的低维不变量。

Conclusion: 研究表明，变压器计算是围绕紧凑、共享的算法结构组织起来的。对于机械解释性而言，专注于这些不变量——即计算的本质——而不是特定于实现的细节可能是有益的。

Abstract: Large language models exhibit sophisticated capabilities, yet understanding how they work internally remains a central challenge. A fundamental obstacle is that training selects for behavior, not circuitry, so many weight configurations can implement the same function. Which internal structures reflect the computation, and which are accidents of a particular training run? This work extracts algorithmic cores: compact subspaces necessary and sufficient for task performance. Independently trained transformers learn different weights but converge to the same cores. Markov-chain transformers embed 3D cores in nearly orthogonal subspaces yet recover identical transition spectra. Modular-addition transformers discover compact cyclic operators at grokking that later inflate, yielding a predictive model of the memorization-to-generalization transition. GPT-2 language models govern subject-verb agreement through a single axis that, when flipped, inverts grammatical number throughout generation across scales. These results reveal low-dimensional invariants that persist across training runs and scales, suggesting that transformer computations are organized around compact, shared algorithmic structures. Mechanistic interpretability could benefit from targeting such invariants -- the computational essence -- rather than implementation-specific details.

</details>


### [101] [$φ$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models](https://arxiv.org/abs/2602.22601)
*Thanh-Dat Truong,Huu-Thien Tran,Jackson Cothren,Bhiksha Raj,Khoa Luu*

Main category: cs.LG

TL;DR: 本文提出了一种新的公平直接偏好优化（FaiDPO或$φ$-DPO）框架，旨在解决大型多模态模型在持续学习过程中由于数据不平衡导致的公平性问题。通过调整学习与成对偏好信号的一致性来缓解灾难性遗忘，并引入新的$φ$-DPO损失函数以处理分布偏差。理论分析和实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对大型多模态模型在持续学习中遇到的数据不平衡引起的学习偏差及性能不佳的问题，特别是考虑到当前研究主要集中在解决灾难性遗忘而忽视了因数据不平衡造成的不公平现象。

Method: 开发了一个基于直接偏好优化(DPO)的新持续学习范式，随后定义了一种新的$φ$-DPO损失函数来特别处理数据不平衡带来的挑战。此外，还为现有基准创建了成对偏好注释，以便于进行基于$φ$-DPO的持续学习实验。

Result: 广泛的实验和消融研究表明，所提出的$φ$-DPO方法在多个基准测试中达到了最先进的性能，优于之前的大型多模态模型持续学习方法。

Conclusion: FaiDPO($φ$-DPO)框架不仅能够有效减轻大型多模态模型中的灾难性遗忘现象，同时也能很好地应对由数据不平衡引发的公平性问题，为未来的研究提供了新方向。

Abstract: Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $φ$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $φ$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $φ$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $φ$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.

</details>


### [102] [DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion](https://arxiv.org/abs/2602.22610)
*Tao Huang,Jiayang Meng,Xu Yang,Chen Hou,Hong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种针对条件扩散变换器的DP-aware AdaLN-Zero机制，通过限制条件引起的增益而不改变DP-SGD机制来解决在差分隐私随机梯度下降下由条件驱动的重尾梯度导致的问题。实验表明，在匹配的隐私设置下，装备了DP-aware AdaLN-Zero的DP-SGD在插值/补全和预测方面有所改进，并且在实际电力数据集及两个公开ETT基准测试中表现优于普通的DP-SGD方法。


<details>
  <summary>Details</summary>
Motivation: 条件注入使扩散模型能够生成具有上下文感知能力的输出，这对许多时间序列任务至关重要。然而，异构条件上下文（如观察历史、缺失模式或异常协变量）可能会引发每个样本的重尾梯度。在差分隐私随机梯度下降(DP-SGD)下，这些罕见的条件驱动重尾梯度会不成比例地触发全局裁剪，从而导致以异常值为主的更新、更大的裁剪偏差以及在固定隐私预算下的性能下降。因此，需要一种新的方法来解决这一问题。

Method: 提出了DP-aware AdaLN-Zero，这是一种针对条件扩散变换器的即插即用敏感性感知条件机制，它能够在不修改DP-SGD机制的情况下限制条件诱导的增益。该方法通过对条件表示幅度和AdaLN调制参数进行联合约束，利用有界重新参数化来抑制梯度裁剪前的极端梯度尾事件及其噪声注入。

Result: 实验证明，装备了DP-aware AdaLN-Zero的DP-SGD在给定相同隐私设定条件下改善了插值/补全与预测任务的表现。此外，在真实世界电力数据集及两套公共ETT基准上，相较于传统DP-SGD方法，新方法均显示出一致性的提升。进一步的梯度诊断分析将这些改进归因于特定于条件的尾部重塑以及减少的裁剪失真，同时保持了非私有训练中的表达力。

Conclusion: 结果表明，敏感性感知条件机制可以显著提高私有条件下扩散训练的效果，而不会牺牲标准性能。

Abstract: Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.

</details>


### [103] [Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD](https://arxiv.org/abs/2602.22611)
*Jiayang Meng,Tao Huang,Chen Hou,Guolong Zheng,Hong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为LM-DP-SGD的新方法，通过根据每层的成员推断攻击（MIA）风险自适应地分配隐私保护，从而在相同的隐私预算下减少峰值IR级别的MIA风险同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 在嵌入式接口设置中，预训练模型被查询以获得中间表示（IRs），这些IRs的分布特性可能会泄露训练集成员信号，导致成员推断攻击（MIAs）。尽管差分隐私随机梯度下降（DP-SGD）可以缓解这种泄漏，但现有实现采用每个样本的梯度裁剪和统一的、不分层的噪声乘数，忽略了各层之间MIA脆弱性的差异。

Method: 本文引入了Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD) 方法，该方法根据各层的MIA风险比例自适应地分配隐私保护。具体来说，LM-DP-SGD使用公共数据集训练一个影子模型，从其训练/测试分割中提取逐层IR，并拟合特定于层的MIA对手，利用它们的攻击错误率作为MIA风险估计。然后，利用MIAs的跨数据集可转移性，这些估计值用于重新调整每一层对全局裁剪梯度的贡献，在固定噪声幅度下提供适当的层次保护。此外，还建立了关于LM-DP-SGD隐私性和收敛性的理论保证。

Result: 广泛的实验表明，在相同的隐私预算下，与传统方法相比，LM-DP-SGD能够降低峰值IR级别的MIA风险，同时保持模型的实用性，从而提供了更优的隐私-实用性权衡。

Conclusion: 通过针对不同层的MIA风险自适应地应用隐私保护措施，LM-DP-SGD能够在不牺牲过多模型性能的前提下有效增强对抗成员推断攻击的能力，为深度学习模型的隐私保护提出了新的方向。

Abstract: In Embedding-as-an-Interface (EaaI) settings, pre-trained models are queried for Intermediate Representations (IRs). The distributional properties of IRs can leak training-set membership signals, enabling Membership Inference Attacks (MIAs) whose strength varies across layers. Although Differentially Private Stochastic Gradient Descent (DP-SGD) mitigates such leakage, existing implementations employ per-example gradient clipping and a uniform, layer-agnostic noise multiplier, ignoring heterogeneous layer-wise MIA vulnerability. This paper introduces Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD), which adaptively allocates privacy protection across layers in proportion to their MIA risk. Specifically, LM-DP-SGD trains a shadow model on a public shadow dataset, extracts per-layer IRs from its train/test splits, and fits layer-specific MIA adversaries, using their attack error rates as MIA-risk estimates. Leveraging the cross-dataset transferability of MIAs, these estimates are then used to reweight each layer's contribution to the globally clipped gradient during private training, providing layer-appropriate protection under a fixed noise magnitude. We further establish theoretical guarantees on both privacy and convergence of LM-DP-SGD. Extensive experiments show that, under the same privacy budget, LM-DP-SGD reduces the peak IR-level MIA risk while preserving utility, yielding a superior privacy-utility trade-off.

</details>


### [104] [Semantic Tube Prediction: Beating LLM Data Efficiency with JEPA](https://arxiv.org/abs/2602.22617)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.LG

TL;DR: 本研究提出了一种新的假设，即语义测地线假设，并基于此开发了语义管预测（STP）任务，作为JEPA风格的正则化器。实验表明，STP可以在NL-RX-SYNTH数据集上使用少16倍的数据量达到与基线相当的准确性，直接挑战了Chinchilla式缩放法则对数据效率的要求。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型遵循一致的缩放规律，但这些规律更多是描述性的而非规定性的，它们描述的是典型的训练情况而不是最优训练。很少有研究成功挑战由这些规律暗示的数据效率界限，这正是本文的主要关注点。

Method: 提出了语义测地线假设，认为token序列在平滑的语义流形上追踪测地线，因此局部呈现线性。基于这一原则，研究者们设计了一个名为语义管预测（STP）的新任务，这是一种JEPA风格的正则化方法，它将隐藏状态轨迹限制在测地线的一个管道区域内。

Result: 实验证明，通过应用STP，LLMs能够在NL-RX-SYNTH数据集上仅用1/16的数据量就达到了与基线相同的准确率，这表明通过采用基于原理的几何先验可以超越单纯依赖规模扩展的方法。

Conclusion: 研究表明，通过引入基于几何学原理的约束条件，如语义管预测任务，可以显著提高大型语言模型的数据效率，进而挑战现有的关于模型训练所需数据量的缩放定律。

Abstract: Large Language Models (LLMs) obey consistent scaling laws -- empirical power-law fits that predict how loss decreases with compute, data, and parameters. While predictive, these laws are descriptive rather than prescriptive: they characterize typical training, not optimal training. Surprisingly few works have successfully challenged the data-efficiency bounds implied by these laws -- which is our primary focus. To that end, we introduce the Geodesic Hypothesis, positing that token sequences trace geodesics on a smooth semantic manifold and are therefore locally linear. Building on this principle, we propose a novel Semantic Tube Prediction (STP) task, a JEPA-style regularizer that confines hidden-state trajectories to a tubular neighborhood of the geodesic. STP generalizes JEPA to language without requiring explicit multi-view augmentations. We show this constraint improves signal-to-noise ratio, and consequently preserves diversity by preventing trajectory collisions during inference. Empirically, STP allows LLMs to match baseline accuracy with 16$\times$ less training data on the NL-RX-SYNTH dataset, directly violating the data term of Chinchilla-style scaling laws and demonstrating that principled geometric priors can surpass brute-force scaling. Code is available at https://github.com/galilai-group/llm-jepa#stp.

</details>


### [105] [ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL](https://arxiv.org/abs/2602.22623)
*Xingyu Lu,Jinpeng Wang,YiFan Zhang,Shijie Ma,Xiao Hu,Tianke Zhang,Haonan fan,Kaiyu Jiang,Changyi Liu,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Chun Yuan*

Main category: cs.LG

TL;DR: 提出了ContextRL框架，通过提供完整参考解决方案作为上下文增强可识别性，并采用多轮采样策略提高可达性。实验表明该方法显著提高了知识发现效率，减少了奖励欺骗现象。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有强化学习方法在处理感知和推理任务时遇到的瓶颈问题，特别是在于模型的可识别性和可达性方面。

Method: 提出了一种新的框架ContextRL，它利用上下文增强来解决上述问题。具体来说，通过给奖励模型提供完整的参考解决方案以增强其对高质量推理过程的辨识能力；同时引入了多轮采样机制，让奖励模型能够为失败尝试生成错误报告，从而指导策略从先前全负样本中恢复正确响应。

Result: 在11个感知与推理基准测试上进行了实验验证，结果表明ContextRL可以显著提升知识发现效率。特别是使得Qwen3-VL-8B模型达到了与更大规模（32B）模型相媲美的性能表现，大幅超越了标准RLVR基线的同时有效缓解了奖励欺骗问题。

Conclusion: 研究深入分析了上下文信息对于提高奖励模型准确性和减少奖励欺骗的重要性，为未来RLVR研究提供了宝贵见解。

Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to "recover" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.

</details>


### [106] [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642)
*Qin-Wen Luo,Sheng Ren,Xiang Chen,Rui Liu,Jun Fang,Naiqiang Tan,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于难度感知的强化学习方法（CEEH），旨在解决现有压缩方法在缩短推理路径时导致熵崩溃的问题。通过动态评估实例难度，对较难问题保持探索空间多样性，同时对简单问题进行积极压缩，并引入了基于历史最短正确响应的动态最优长度惩罚机制来稳定奖励信号。实验结果表明，该方法在六个推理基准测试中均能有效减少响应长度且保持与基础模型相当的准确性，相较于仅优化长度的方法提高了Pass@k指标。


<details>
  <summary>Details</summary>
Motivation: 现有的压缩方法在尝试缩短大型语言模型的推理步骤以降低计算成本和延迟时，往往会牺牲模型的推理能力。特别是当直接优化更短的推理轨迹时，会导致熵崩溃，从而过早地缩小探索空间，抑制了对复杂问题的有效推理路径的发现。

Method: 提出了一个名为CEEH（Compress responses for Easy questions and Explore Hard ones）的方法，它是一种基于难度感知的强化学习高效推理方法。该方法能够动态评估每个实例的难度，对于难题使用选择性熵正则化来保持搜索空间的多样性，而对于容易的问题则允许更加激进的压缩。此外，还引入了一个基于历史上最短正确响应的动态最优长度惩罚机制，用以对抗由熵引起的长度膨胀并稳定奖励信号。

Result: 在六个推理基准上的实验显示，CEEH方法能够在保持与基线模型相近准确率的同时显著减少响应长度，并且相比于单纯追求缩短长度的方法，在Pass@k指标上有所提升。

Conclusion: 通过采用CEEH策略，可以在不明显损害推理性能的前提下有效降低大型语言模型处理复杂推理任务时产生的计算成本和延迟问题，为实际部署提供了新的解决方案。

Abstract: Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.

</details>


### [107] [MUG: Meta-path-aware Universal Heterogeneous Graph Pre-Training](https://arxiv.org/abs/2602.22645)
*Lianze Shan,Jitao Zhao,Dongxiao He,Yongqi Huang,Zhiyong Feng,Weixiong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的元路径感知的通用异构图预训练方法（MUG），旨在解决异构图中由于类型多样性和元路径变化带来的统一表示空间构建和跨数据集学习模式迁移的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的通用图预训练研究主要集中在同质图上，对于具有更高结构和语义复杂性的异构图尚未充分探索。鉴于异构图中存在的多样性类型及其特定于数据集的语义阻碍了统一表示空间的建立，加上不同数据集中元路径的数量与含义差异导致编码聚合模式难以跨数据集应用的问题，提出了需要一种新的方法来克服这些障碍。

Method: 为了解决上述问题，MUG引入了一个输入统一模块，将每个异构图中的多种节点和关系类型信息整合成一个统一的表示，并通过维度感知编码器将其投影到共享空间，从而实现不同模式图之间的对齐。此外，MUG还训练了一个共享编码器来捕捉跨多样化元路径视图的一致结构模式，而不是依赖于特定于数据集的聚合策略，同时采用全局目标函数以促进区分性并减少特定于数据集的偏见。

Result: 广泛的实验表明，MUG在一些真实数据集上是有效的。

Conclusion: MUG方法通过引入创新的输入统一模块以及跨元路径一致结构模式捕捉机制，在处理异构图时展现出显著优势，为未来研究提供了新的方向。

Abstract: Universal graph pre-training has emerged as a key paradigm in graph representation learning, offering a promising way to train encoders to learn transferable representations from unlabeled graphs and to effectively generalize across a wide range of downstream tasks. However, recent explorations in universal graph pre-training primarily focus on homogeneous graphs and it remains unexplored for heterogeneous graphs, which exhibit greater structural and semantic complexity. This heterogeneity makes it highly challenging to train a universal encoder for diverse heterogeneous graphs: (i) the diverse types with dataset-specific semantics hinder the construction of a unified representation space; (ii) the number and semantics of meta-paths vary across datasets, making encoding and aggregation patterns learned from one dataset difficult to apply to others. To address these challenges, we propose a novel Meta-path-aware Universal heterogeneous Graph pre-training (MUG) approach. Specifically, for challenge (i), MUG introduces a input unification module that integrates information from multiple node and relation types within each heterogeneous graph into a unified representation.This representation is then projected into a shared space by a dimension-aware encoder, enabling alignment across graphs with diverse schemas.Furthermore, for challenge (ii), MUG trains a shared encoder to capture consistent structural patterns across diverse meta-path views rather than relying on dataset-specific aggregation strategies, while a global objective encourages discriminability and reduces dataset-specific biases. Extensive experiments demonstrate the effectiveness of MUG on some real datasets.

</details>


### [108] [LEDA: Latent Semantic Distribution Alignment for Multi-domain Graph Pre-training](https://arxiv.org/abs/2602.22660)
*Lianze Shan,Jitao Zhao,Dongxiao He,Siqi Liu,Jiaxu Cui,Weixiong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的Latent sEmantic Distribution Alignment (LEDA) 模型，用于通用图预训练，以解决现有方法在从通用图中学习有效知识时面临的挑战。通过引入维度投影单元和变分语义推理模块，该模型能够自适应地将多样化的领域特征对齐到共享的语义空间，并且在跨域场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前，大型通用模型（如GPT和DeepSeek）的发展激发了人们希望在图预训练中引入通用性，以利用图表示来跨越不同领域学习丰富且可泛化的知识，从而提高各种下游应用的表现。然而，现有的大多数方法在从通用图中学习有效知识方面遇到了困难，主要问题在于过于简单的数据对齐以及有限的训练指导。简单数据对齐的问题源于使用一种直接统一的方法处理高度多样化的图数据，这不能正确对齐语义并误导了预训练模型；而有限训练指导的问题则体现在不加区分地将在特定领域内有效的预训练范式应用于跨领域场景上。虽然这种方法对于增强单个数据空间内的判别性表征有效，但在从多个图中捕捉有效知识方面表现不佳。

Method: 为了解决上述挑战，提出了名为Latent sEmantic Distribution Alignment (LEDA) 的新模型，专为通用图预训练设计。首先，通过引入一个维度投影单元来自适应地将多样化领域的特性映射至共享语义空间中，同时尽量减少信息丢失。此外，还设计了一个变分语义推断模块来获取共享的潜在分布，然后利用这种分布指导领域投影，确保跨领域的语义一致性和学习。

Result: LEDA模型在广泛的图类型和下游任务上都展现了强大的性能。尤其值得注意的是，在少量样本的跨域设置下，它相对于领域内基线以及其他先进的通用预训练模型有显著优势。

Conclusion: 提出的LEDA模型成功解决了现有方法在跨域图预训练中存在的关键问题，通过创新性的维度投影与变分语义推理机制，实现了高效的知识转移与泛化能力提升。

Abstract: Recent advances in generic large models, such as GPT and DeepSeek, have motivated the introduction of universality to graph pre-training, aiming to learn rich and generalizable knowledge across diverse domains using graph representations to improve performance in various downstream applications. However, most existing methods face challenges in learning effective knowledge from generic graphs, primarily due to simplistic data alignment and limited training guidance. The issue of simplistic data alignment arises from the use of a straightforward unification for highly diverse graph data, which fails to align semantics and misleads pre-training models. The problem with limited training guidance lies in the arbitrary application of in-domain pre-training paradigms to cross-domain scenarios. While it is effective in enhancing discriminative representation in one data space, it struggles to capture effective knowledge from many graphs. To address these challenges, we propose a novel Latent sEmantic Distribution Alignment (LEDA) model for universal graph pre-training. Specifically, we first introduce a dimension projection unit to adaptively align diverse domain features into a shared semantic space with minimal information loss. Furthermore, we design a variational semantic inference module to obtain the shared latent distribution. The distribution is then adopted to guide the domain projection, aligning it with shared semantics across domains and ensuring cross-domain semantic learning. LEDA exhibits strong performance across a broad range of graphs and downstream tasks. Remarkably, in few-shot cross-domain settings, it significantly outperforms in-domain baselines and advanced universal pre-training models.

</details>


### [109] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 本研究提出了一种两组件框架，用于预测抗菌药物耐药性（AMR）趋势，并支持基于证据的政策决策。通过比较六种模型在WHO GLASS数据上的表现，XGBoost展现了最佳性能，且特征重要性分析显示前一年的耐药率是主要预测因素。此外，研究还实现了一个结合了WHO政策文件和本地部署语言模型的检索增强生成管道，以产生有来源依据、限制幻觉的政策答案。


<details>
  <summary>Details</summary>
Motivation: 鉴于抗菌药物耐药性（AMR）日益成为全球危机，预计到2050年每年将导致1000万人死亡，而利用机器学习从WHO全球抗菌药物耐药性和使用监测系统(GLASS)的数据中预测群体层面耐药性趋势的研究却很少。因此，有必要开发一种有效的框架来预测AMR趋势并支持政策决策。

Method: 本研究采用了一种两组件框架，首先对六个模型（朴素法、线性回归、岭回归、XGBoost、LightGBM和LSTM）进行基准测试，这些模型应用于来自六个WHO区域（2021-2023年）的5,909个WHO GLASS观测值上。其次，实现了检索增强生成(RAG)管道，该管道结合了WHO政策文档的ChromaDB向量存储与本地部署的Phi-3 Mini语言模型。

Result: XGBoost在所有评估过的模型中表现最好，其测试MAE为7.07%，R方为0.854，比朴素基线提高了83.1%。特征重要性分析表明，前一年的耐药率为最主要的预测因子（重要性占比50.5%），不同区域的MAE范围从欧洲地区的4.16%至东南亚地区的10.14%不等。RAG管道能够生成具有来源依据、减少幻觉的政策建议。

Conclusion: 研究表明，XGBoost模型对于预测抗菌药物耐药性趋势非常有效，且通过整合机器学习与自然语言处理技术，可以为政策制定者提供基于证据的支持。这为未来如何更有效地利用现有数据资源来应对抗菌药物耐药性问题提供了新的思路。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


### [110] [Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement](https://arxiv.org/abs/2602.22681)
*Shuchen Zhu,Rizhen Hu,Mingze Wang,Mou Sun,Xue Wang,Kun Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: 本文提出了一种名为LITE的优化策略，通过在平坦方向上应用更大的Hessian阻尼系数和学习率来加速大型语言模型的预训练。实验表明，LITE能够显著加快多种架构、参数规模、数据集以及学习率调度下的Muon和SOAP算法的训练速度。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型预训练所需的巨大计算资源问题，现有基于矩阵的优化器虽然利用了精细的曲率信息以超越AdamW，但其更新方式倾向于各向同性——在平坦方向上相对保守而在尖锐方向上可能过于激进。为解决这一局限性，研究旨在开发一种更高效的优化方法，特别是在高度各向异性的优化场景中。

Method: 首先建立了一个统一的黎曼常微分方程(ODE)框架，揭示了常见的自适应算法如何协同工作：预条件子诱导出一个减轻病态条件的黎曼几何，而动量则作为促进收敛的黎曼阻尼项。基于这些见解，提出了LITE，这是一种通过沿平坦轨迹应用更大Hessian阻尼系数和学习率来增强训练动态的广义加速策略。

Result: 广泛的实验证明，LITE能够显著地加速Muon和SOAP在不同架构（密集型、MoE）、参数规模（130M-1.3B）、数据集（C4, Pile）及学习率计划（余弦、升温-稳定-衰减）下的表现。理论分析也证实了LITE有助于在各向异性景观中沿着平坦方向更快地收敛。

Conclusion: LITE提供了一种原则性的方法来有效提高大型语言模型预训练的效率，并且已被证明可以在多种情况下显著加速当前最先进的优化器。

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

</details>


### [111] [Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning](https://arxiv.org/abs/2602.22703)
*Hao Yu,Shuning Jia,Guanghao Li,Wenhao Jiang,Chun Yuan*

Main category: cs.LG

TL;DR: 本文提出了GeoPerceive基准和GeoDPO框架，旨在提升视觉-语言模型的几何感知能力。实验结果表明，与监督微调相比，GeoDPO在领域内、跨领域数据集及下游推理任务上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉-语言模型因对基本图表元素感知有限而在几何推理方面遇到的问题，作者们开发了一种新的方法来增强这些模型对于几何图形的理解能力。

Method: 引入了GeoPerceive作为包含图表实例及其领域特定语言（DSL）表示形式的基准，并设计了一个高效的数据自动生成流程。基于此，提出了GeoDPO，一个利用翻译指导的强化学习框架，通过训练NL-to-DSL翻译器来连接自然语言与DSL，从而计算出细粒度的奖励信号。

Result: 实验评估显示，在领域内数据上GeoDPO相较于监督微调提升了26.5%，跨领域数据上提高了8.0%，而在下游推理任务上的增长更是达到了39.0%。这证明了GeoDPO不仅能够有效改善模型性能，还拥有更强的泛化能力。

Conclusion: 研究结果表明，GeoDPO相比传统的监督微调方法，在提高视觉-语言模型的几何感知能力和泛化性方面具有明显优势。

Abstract: Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.

</details>


### [112] [Multi-agent imitation learning with function approximation: Linear Markov games and beyond](https://arxiv.org/abs/2602.22810)
*Luca Viano,Till Freihaut,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 本文首次对具有线性特征的马尔可夫博弈中的多智能体模仿学习进行了理论分析，提出了一种在特征层面而非状态-动作层面定义的集中系数，并提供了一个计算效率高的交互式多智能体模仿学习算法，该算法仅依赖于特征映射的维度。基于这些发现，提出了一个深度多智能体模仿学习交互算法，在井字游戏和四子棋等游戏中明显优于行为克隆（BC）。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过利用给定特征来简化多智能体模仿学习中所需的假设条件，特别是试图找到一种方法来减少或消除对于所有策略偏差集中系数的需求，从而提高算法效率及适用范围。

Method: 首先通过对线性马尔可夫博弈进行理论分析，引入了基于特征级别的集中系数概念；接着开发出首个针对此类环境设计的高效互动型多智能体模仿学习算法；最后，基于上述理论成果，构建了一个深度学习版本的互动算法。

Result: 结果表明所提出的特征级集中系数可以在信息丰富的特征下显著小于传统的状态-行动级同类指标。此外，新开发的算法展示出其样本复杂度仅仅与特征映射的维度相关联。实践上，提出的深度多智能体模仿学习算法在几个经典博弈任务上表现超越了行为克隆方法。

Conclusion: 这项工作为理解和改进多智能体系统中的模仿学习提供了新的视角，特别是证明了通过利用环境结构可以有效降低问题难度并提升解决方案的有效性和效率。

Abstract: In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level "all policy deviation concentrability coefficient" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.

</details>


### [113] [Moral Preferences of LLMs Under Directed Contextual Influence](https://arxiv.org/abs/2602.22831)
*Phil Blandfort,Tushar Karayil,Urja Pawar,Robert Graham,Alex McKenzie,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 研究发现，即使在表面上相关的情况下，情境影响也经常显著改变大型语言模型（LLMs）在道德决策中的选择。基线偏好不能很好地预测方向性可操控性，因为模型可能在基准上显得中立，但在影响下表现出系统性的可操控性不对称。此外，影响可能会适得其反：模型可能明确声称中立或贬低情境线索，但它们的选择仍然会变化，有时甚至朝相反方向变化。推理过程减少了平均敏感度，却放大了有偏见的少量示例的影响。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型（LLMs）的道德标准通常使用无上下文提示，假设偏好是稳定的。然而，在实际应用中，提示往往包含了用户请求、社会规范等上下文信号，这些信号可能会影响决策。本研究旨在探索定向的情境影响如何重塑类似电车难题这样的道德分类设置下的决策。

Method: 研究人员开发了一个针对电车难题式道德分类场景下定向情境影响的初步评估框架。对于每个人口统计因素，他们应用匹配的方向反转情境影响，这些影响仅在于它们所偏好的群体不同，从而能够系统地测量方向响应。

Result: 研究结果显示，(i) 即便是在仅有表面关联的情况下，情境影响也常常大幅改变了决定；(ii) 基础偏好作为预测方向性操纵能力的效果不佳，因为模型虽然看起来在基础状态下保持中立，但在受到外界影响时却显示出系统性的操纵不对称性；(iii) 影响可能会产生反作用：模型或许会明确声明中立立场或者低估情境暗示的重要性，但其选择依旧会发生变动，有时甚至是朝着完全相反的方向；以及(iv) 推理活动减少了平均敏感度，但增强了带有偏见的小样本案例的效果。

Conclusion: 基于上述发现，建议扩展道德评估，引入受控且方向翻转的情境操作来更好地描述模型行为。

Abstract: Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.

</details>


### [114] [MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction](https://arxiv.org/abs/2602.22850)
*Yi He,Yina Cao,Jixiu Zhai,Di Wang,Junxiao Kong,Tianchi Lu*

Main category: cs.LG

TL;DR: 本文提出了一种高性能模型MEDNA-DFM以及基于机制的信号纯化算法，用于准确识别DNA甲基化模式。该模型在不同物种间表现出色，并且其泛化能力依赖于保守的内在基序而非系统发育关系。此外，通过对果蝇6mA案例的研究提出了序列-结构协同假设，并通过计算突变验证了这一假设的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在二分类任务如DNA甲基化识别中表现出色，但其“黑箱”特性限制了对生物学机制的理解。为了解决这个问题并提高对表观遗传调控的理解，需要开发既高效又能提供生物洞见的新方法。

Method: 开发了一个名为MEDNA-DFF的高性能模型及配套的机制启发式信号纯化算法；利用这些工具来识别保守的甲基化模式，并分析模型泛化性能背后的原因；通过案例研究和计算机模拟验证提出的生物假设。

Result: MEDNA-DFM能够有效捕捉保守的甲基化模式，在多种物种间实现稳健区分；模型的泛化能力主要由保守基序（例如GC含量）驱动；所开发的算法提取出的基序比先前研究更可靠；通过Drosophila 6mA案例研究支持了‘序列-结构协同’假说，并通过模拟实验进一步证实去除任一或两个关键元素会显著降低模型识别能力。

Conclusion: 本研究不仅提供了一种强大的DNA甲基化预测工具，还展示了可解释性深度学习如何促进方法创新和生物假设生成。

Abstract: Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its "black-box" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a "sequence-structure synergy" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.

</details>


### [115] [Fair feature attribution for multi-output prediction: a Shapley-based perspective](https://arxiv.org/abs/2602.22882)
*Umberto Biccari,Alain Ibáñez de Opakua,José María Mato,Óscar Millet,Roberto Morales,Enrique Zuazua*

Main category: cs.LG

TL;DR: 本文在Shapley框架内对多输出预测器的特征归因进行了公理化描述，通过将经典Shapley公理扩展到向量值合作博弈，证明了任何满足效率、对称性、虚拟玩家和可加性的归因规则必须独立地分解为各个输出分量。这表明所有基于Shapley的解释都必须遵循这一组件式的结构，并且指出了多输出学习中公平一致性解释的确切范围。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决SHAP解释是否需要独立计算每个输出坐标的问题，以及这种做法背后的理论必要性。

Method: 作者通过将经典的Shapley公理扩展至向量值的合作博弈来建立一个刚性定理，该定理表明任何满足特定条件（效率、对称性、虚拟玩家和可加性）的归因规则必须按输出分量逐个分解。

Result: 研究结果表明，任何联合输出的归因规则必须至少放松经典Shapley公理中的一个。此外，实验还展示了多输出模型在训练和部署时可以节省计算资源，同时保持与Shapley公理所要求的组件式结构一致的SHAP解释。

Conclusion: 本文揭示了基于Shapley的解释方法中一个先前未被形式化的结构性约束，明确了多输出学习中能够保持公平一致性的解释范围。

Abstract: In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity must necessarily decompose component-wise across outputs. Consequently, any joint-output attribution rule must relax at least one of the classical Shapley axioms. This result identifies a previously unformalized structural constraint in Shapley-based interpretability, clarifying the precise scope of fairness-consistent explanations in multi-output learning. Numerical experiments on a biomedical benchmark illustrate that multi-output models can yield computational savings in training and deployment, while producing SHAP explanations that remain fully consistent with the component-wise structure imposed by the Shapley axioms.

</details>


### [116] [NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion](https://arxiv.org/abs/2602.22911)
*Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法NoRA（非线性秩适应），通过引入SiLU门控和结构化dropout来解决LoRA在复杂推理任务中遇到的'线性天花板'问题。实验表明，NoRA以更低的秩实现了优于LoRA的表现，并且激活了奇异值谱中的休眠尾部，防止了线性方法中观察到的秩塌陷现象。


<details>
  <summary>Details</summary>
Motivation: 低秩适应(LoRA)在参数高效微调领域占据主导地位，但在处理复杂的推理任务时遇到了一个关键障碍——即随着秩的增加其性能提升逐渐减少，这是因为内在的线性约束限制了它的表现。

Method: 提出了NoRA（非线性秩适应）方法，该方法是一种权重级并行适配器，通过注入SiLU门控和结构性dropout机制来诱导流形扩展。

Result: 在SlimOrca基准测试中，当秩为64时NoRA（PPL 3.89）的表现超过了秩为512时的LoRA（PPL 3.90）。此外，在数学推理任务上，NoRA达到了1.97的困惑度，显著超越了LoRA饱和点的2.07。

Conclusion: NoRA通过激活奇异值谱中的休眠部分有效地解决了线性方法中存在的秩崩溃问题，显示出了比传统LoRA更好的光谱效率。

Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.

</details>


### [117] [Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements](https://arxiv.org/abs/2602.23035)
*Viraj Patel,Marko Grujic,Philipp Aigner,Theodor Abart,Marcus Granegger,Deblina Bhattacharjee,Katharine Fraser*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理学的潜在关系框架，用于将心脏涡流建模为图中的交互节点。该模型结合了神经关系推理架构与受物理启发的相互作用能量和生死动态，从而生成一个对疾病严重程度和干预水平敏感的潜在图。通过对主动脉缩窄的计算流体动力学模拟应用此方法，发现随着主动脉半径变窄，涡流之间的相互作用变得更加强烈且频繁，导致更高的图熵，并与缩窄严重程度单调相关。此外，该方法扩展到不同水平左心室辅助装置支持下的超声数据集，再次证明了该潜在图表示能够捕捉到连贯涡流结构的减弱，显示了跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的成像和计算方法无法捕捉心脏血流模式中连贯流动特征的基本关系结构，而这些模式包含了关于疾病严重程度和临床干预的重要信息。因此，需要一种新的方法来更好地理解和分析这些复杂的关系结构。

Method: 作者提出了一个结合神经关系推理架构、物理启发式交互能量及生死动态的物理学信息潜在关系框架，用于将心脏涡流建模为图中的交互节点。首先，该方法被应用于主动脉缩窄的计算流体动力学模拟；随后，又进一步扩展至不同水平左心室辅助装置支持条件下的左心室超声数据集。

Result: 通过这种方法，研究人员发现当主动脉半径减小时，涡流间的相互作用变得更加频繁且强烈，进而导致图熵增加，这与缩窄严重程度呈正相关（R²=0.78, Spearman |ρ|=0.96）。此外，在处理超声数据时，潜在图同样能够捕捉到连贯涡流结构的变化，表明了该技术具有良好的跨模态泛化性能。

Conclusion: 研究结果表明，通过使用所提出的潜在交互图及其熵值作为标记，可以有效评估心脏疾病的严重程度及治疗干预的效果。

Abstract: Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.

</details>


### [118] [RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection](https://arxiv.org/abs/2602.23060)
*Xin Wang,Burcu Ozek,Aruna Mohan,Amirhossein Ravari,Or Zilbershot,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 该论文提出了一种新的心电图分析方法RhythmBERT，它将心电图视为一种语言范式，通过自编码器将P、QRS和T段编码为符号令牌，并结合连续嵌入来保留细粒度形态。这种方法在未标记的心电图记录上进行预训练，仅使用单导联就能达到与强12导联基线相当或更优的表现，尤其对于临床挑战性案例如细微ST-T异常和心肌梗塞的诊断。


<details>
  <summary>Details</summary>
Motivation: 当前的自我监督学习方法在处理心电图时通常将其视为通用时间序列，忽略了生理语义和节奏层面的结构；对比方法使用的增强方式会扭曲形态，而生成方法采用固定窗口分割则会导致心脏周期错位。因此需要开发一种能够考虑心电图生理对齐特性和结构的新方法。

Method: RhythmBERT是一种生成式心电图语言模型，它将心电图当作语言范式处理，通过基于自编码器的潜在表示将P、QRS及T段转换成符号令牌。同时，它还利用互补的连续嵌入保持了波形的细节特征，从而实现对波形结构和节律的统一理解。该模型在约80万份未标注的心电图记录上进行了掩码预测目标预训练。

Result: 评估结果显示，尽管只使用单一导联数据，RhythmBERT在常见病症如房颤以及临床上具有挑战性的病例例如细微ST-T变化和心肌梗塞检测方面，均达到了与强大的12导联基线模型相媲美甚至更好的性能。

Conclusion: 研究表明，将心电图视为结构化语言的方法提供了一个可扩展且生理对齐的途径，有助于推进心脏疾病分析领域的发展。

Abstract: Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.

</details>


### [119] [PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training](https://arxiv.org/abs/2602.23111)
*Yanyi Li,Yimu Zhang,Cong Fang*

Main category: cs.LG

TL;DR: 提出了一种名为PRAC的新方法，通过将激活分解为主成分子空间和随机子空间来压缩大型语言模型的激活数据，这种方法在保证性能几乎不受影响的情况下实现了高达36%的内存减少。


<details>
  <summary>Details</summary>
Motivation: 现有的压缩方法未能充分利用激活数据的频谱结构，导致收敛速度慢或压缩效果有限。随着大批量训练中激活逐渐成为主要的内存瓶颈问题，需要一种新的方法来有效解决这个问题。

Method: 提出了PRAC（Principal-Random Subspace for LLM Activation Compression），该方法创新性地利用奇异值分解(SVD)捕捉激活的主要子空间以保留主要信息，并从正交补集中采样一个随机子空间来近似尾部。通过引入精确的比例因子，证明了PRAC能够在特定条件下提供最小方差的无偏梯度估计器。

Result: 广泛的实验表明，无论是预训练还是微调任务，PRAC都能实现高达36%的整体内存减少，同时对性能的影响可以忽略不计且计算成本极低。

Conclusion: PRAC为大规模语言模型训练中的激活压缩提供了有效的解决方案，不仅显著减少了内存需求，而且保持了良好的算法收敛性和模型性能。

Abstract: Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.

</details>


### [120] [Learning Physical Operators using Neural Operators](https://arxiv.org/abs/2602.23113)
*Vignesh Gopakumar,Ander Gray,Dan Giles,Lorenzo Zanisi,Matt J. Kusner,Timo Betcke,Stanislas Pamela,Marc Peter Deisenroth*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的训练框架，通过算子分裂方法分解PDE，并使用神经算子学习非线性物理算子，同时用固定有限差分卷积逼近线性算子。该方法构建了一个模块化的专家混合架构，能够通过显式编码底层算子结构来推广到新的物理领域，并且可以通过标准ODE解算器实现连续时间预测。在不可压缩和可压缩Navier-Stokes方程上的实验表明，该方法具有更好的收敛性和对未知物理现象的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子虽然在解决偏微分方程（PDEs）方面表现出色，但难以超越训练分布范围进行泛化，并且通常受限于固定的时域离散化。为了解决这些限制，提出了一个基于物理信息的训练框架。

Method: 采用算子分裂方法分解PDEs，训练不同的神经算子以学习各个非线性物理算子，并利用固定有限差分卷积近似线性算子。将此建模任务表述为神经常微分方程（ODE），其中所学得的算子构成右端项，从而允许通过标准ODE求解器进行连续时间预测，并隐式地强制执行PDE约束。

Result: 在不可压缩和可压缩Navier-Stokes方程上进行了验证，证明了该方法相较于传统方法，在收敛性和面对未见物理场景时的性能表现更优。此外，该方法保持了参数效率，支持超出训练范围的时间外推，并提供了可以针对已知物理特性进行验证的可解释组件。

Conclusion: 通过引入基于物理信息的训练框架以及采用算子分裂方法，本研究成功解决了现有神经算子模型在泛化能力和时域灵活性方面的局限性。提出的方法不仅提高了处理特定类型PDEs的能力，还增强了模型对于新物理情况的适应性。

Abstract: Neural operators have emerged as promising surrogate models for solving partial differential equations (PDEs), but struggle to generalise beyond training distributions and are often constrained to a fixed temporal discretisation. This work introduces a physics-informed training framework that addresses these limitations by decomposing PDEs using operator splitting methods, training separate neural operators to learn individual non-linear physical operators while approximating linear operators with fixed finite-difference convolutions. This modular mixture-of-experts architecture enables generalisation to novel physical regimes by explicitly encoding the underlying operator structure. We formulate the modelling task as a neural ordinary differential equation (ODE) where these learned operators constitute the right-hand side, enabling continuous-in-time predictions through standard ODE solvers and implicitly enforcing PDE constraints. Demonstrated on incompressible and compressible Navier-Stokes equations, our approach achieves better convergence and superior performance when generalising to unseen physics. The method remains parameter-efficient, enabling temporal extrapolation beyond training horizons, and provides interpretable components whose behaviour can be verified against known physics.

</details>


### [121] [Regularized Online RLHF with Generalized Bilinear Preferences](https://arxiv.org/abs/2602.23116)
*Junghyun Lee,Minju Hong,Kwang-Sung Jun,Chulhee Yun,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文研究了具有通用偏好的上下文在线RLHF问题，旨在识别纳什均衡。通过采用广义双线性偏好模型(GBPM)，能够捕捉通过低秩、斜对称矩阵表示的潜在非传递性偏好。作者证明了贪婪策略的对偶间隙受估计误差平方的限制，并基于此及特征多样性假设，提出了两种简单算法：贪心采样和探索-然后-承诺，分别实现了多项式对数遗憾和平方根形式的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决更广泛条件下的在线强化学习与人类反馈（RLHF）问题，特别是当偏好关系可能表现出非传递性质时。此前的研究主要集中在逆向KL正则化上，而这项工作则扩展到了任何强凸正则化器的应用场景中。

Method: 方法上，采用了广义双线性偏好模型来建模可能存在的非传递性偏好，并利用强凸性和该模型的斜对称特性证明了贪婪策略的对偶间隙与估计误差的关系。接着，在此基础上提出两种算法——贪心采样和探索-然后-承诺，前者在多项式对数级别上达到无$e^{O(η)}$项的遗憾边界；后者通过利用低秩结构达到了统计效率保证。

Result: 结果表明，对于高维度情况下的在线RLHF问题，所提出的两种算法均能有效减少遗憾：贪心采样算法实现了$\tilde{O}(ηd^4 (\log T)^2)$级别的遗憾，而探索-然后-承诺算法则进一步将遗憾降低至$\tilde{O}(\sqrt{ηr T})$。

Conclusion: 结论是，通过引入广义双线性偏好模型并开发出两种新颖算法，本研究为处理复杂偏好关系下的在线强化学习提供了新的理论基础和技术手段，特别是在高维空间中展现出了良好的性能。

Abstract: We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\tilde{O}(ηd^4 (\log T)^2)$. (2) Explore-Then-Commit achieves $\mathrm{poly}(d)$-free regret $\tilde{O}(\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.

</details>


### [122] [DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding](https://arxiv.org/abs/2602.23135)
*Tyler Bonnet,Marek Rei*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的架构DyGnROLE，它通过分离源节点和目标节点的表示来处理动态图中不对称的行为模式和时间动态。通过引入自监督预训练目标TCLP，在不需要标注数据的情况下学习角色特定的表示。实验表明DyGnROLE在未来的边分类任务上显著优于现有的多种基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态图通常是定向的，其中源节点和目的节点表现出不对称的行为模式和时间动态特性。然而，现有的动态图架构大多依赖于共享参数来处理源节点和目的节点，缺乏系统性的角色感知建模。

Method: 提出了DyGnROLE（Dynamic Graph Node-Role-Oriented Latent Encoding），一种基于Transformer的架构，明确区分了源节点和目的节点的表示。该模型利用独立的嵌入词汇表和角色语义位置编码捕捉每个角色独特的结构和时间背景。为了提高这些专门嵌入在标签较少情况下的有效性，作者引入了一个自监督预训练目标——Temporal Contrastive Link Prediction (TCLP)。

Result: 评估显示，在未来边分类方面，DyGnROLE明显优于一系列最先进的基线模型。这证明了角色感知建模是动态图学习的有效策略。

Conclusion: DyGnROLE通过为源节点和目的节点提供独立的表示方法，并结合自监督预训练技术，能够在动态图分析领域实现更佳的表现，尤其是在低标签环境下。

Abstract: Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systematic role-aware modeling. We propose DyGnROLE (Dynamic Graph Node-Role-Oriented Latent Encoding), a transformer-based architecture that explicitly disentangles source and destination representations. By using separate embedding vocabularies and role-semantic positional encodings, the model captures the distinct structural and temporal contexts unique to each role. Critical to the effectiveness of these specialized embeddings in low-label regimes is a self-supervised pretraining objective we introduce: Temporal Contrastive Link Prediction (TCLP). The pretraining uses the full unlabeled interaction history to encode informative structural biases, enabling the model to learn role-specific representations without requiring annotated data. Evaluation on future edge classification demonstrates that DyGnROLE substantially outperforms a diverse set of state-of-the-art baselines, establishing role-aware modeling as an effective strategy for dynamic graph learning.

</details>


### [123] [Prediction of Diffusion Coefficients in Mixtures with Tensor Completion](https://arxiv.org/abs/2602.23142)
*Zeno Romero,Kerstin Münnemann,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 本文提出了一种混合张量补全方法（TCM），用于预测二元混合物在无限稀释条件下的温度依赖扩散系数。该方法基于Tucker分解，并结合了实验数据与半经验SEGWE模型的先验知识，在268K到378K范围内实现了高精度预测。此外，通过主动学习策略扩展实验数据库进一步提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 预测混合物中的扩散系数对于许多应用至关重要，但实验数据稀缺。机器学习方法为解决这一问题提供了可能，尤其是矩阵补全方法（MCMs）在预测热物理性质方面表现良好。然而，MCMs仅限于单一温度预测且高度依赖高质量实验数据。本研究旨在开发一种能够跨越更宽温度范围并提高预测准确性的新方法。

Method: 采用一种基于Tucker分解的混合张量补全方法（TCM）。该方法联合利用了不同温度下（298K、313K和333K）二元系统无限稀释条件下扩散系数的实验数据进行训练，并将半经验SEGWE模型的预测作为贝叶斯训练框架内的先验知识。TCM能够在268K至378K之间线性外推。为了增加实验数据集，使用了主动学习策略来指导脉冲场梯度核磁共振测量以获取新的扩散数据。

Result: 相比于现有的模型，TCM在整个研究温度区间内显著提高了预测扩散系数的准确性。特别是当整合了额外通过主动学习获得的新实验结果后，TCM的预测性能得到了实质性的提升。

Conclusion: 结合数据高效的机器学习方法与自适应实验设计可以有效推进运输性质的预测建模工作。所提出的TCM不仅克服了传统MCMs对单点温度预测的限制，而且通过主动学习策略增强了其预测能力。

Abstract: Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.

</details>


### [124] [Partial recovery of meter-scale surface weather](https://arxiv.org/abs/2602.23146)
*Jonathan Giezendanner,Qidong Yang,Eric Schmitt,Anirban Chandra,Daniel Salles Civitarese,Johannes Jakubik,Jeremy Vila,Detlef Hohl,Campbell Watson,Sherrie Wang*

Main category: cs.LG

TL;DR: 通过结合粗略的大气状态、稀疏的地表站测量和高分辨率地球观测数据，研究者能够以10米分辨率推断出美国大陆近地表风、温度和湿度的连续空间场。相较于ERA5，这些推断出的场减少了风误差29%，温度和露点误差6%，同时解释了更多固定时间步长下的空间方差。该研究展示了在大陆尺度上进行米级分辨率推理的计算可行方法，并揭示了之前未被发现的地球系统组成部分。


<details>
  <summary>Details</summary>
Motivation: 当前天气分析和预报中缺乏由于地表覆盖和地形导致的数十到数百米范围内的近地表大气条件的显著差异。本研究旨在探讨这种米级尺度的变化是否反映了不可简化为混沌动力学的一部分，还是可以从地表特征和大尺度大气强迫中预测出来的一个组分。

Method: 通过将粗糙的大气状态与稀疏的地表站点测量及高分辨率地球观测数据相结合，研究人员能够在美国大陆范围内以10米分辨率推断出近地表风、温度以及湿度的空间连续分布情况。

Result: 相对于ERA5，所推导出的气象参数（如风速、温度和露点）具有更高的准确性，具体表现为风速误差降低了29%，而温度与露点误差则各自下降了6%；此外，在固定的时间点上，这种方法还能够解释更多的空间变化量。

Conclusion: 研究表明，通过将粗略的动力模型与静态精细尺度特征相结合，可以揭示地球系统中以前未解决的部分。此方法不仅扩展了天气建模的可能性边界，也展示了一种在大陆尺度上实现米级分辨率推论的计算上可行的方法。

Abstract: Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.

</details>


### [125] [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)
*Gal Kesten-Pomeranz,Yaniv Nikankin,Anja Reusch,Tomer Tsaban,Ora Schueler-Furman,Yonatan Belinkov*

Main category: cs.LG

TL;DR: 本研究探讨了蛋白质语言模型（PLM）如何识别蛋白质序列中的重复片段，无论是精确重复还是近似重复。研究揭示了PLM通过结合基于语言的模式匹配与专门的生物知识来完成这一生物学任务的机制，为研究更复杂的进化过程奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列中存在大量的重复段落，这些重复对于蛋白质结构和功能至关重要。尽管已有算法用于识别这类重复，但最近的研究表明，蛋白质语言模型(PLMs)也能通过遮罩标记预测来识别重复。本文旨在阐明PLMs内部识别重复的具体机制。

Method: 研究人员首先分析了PLMs在检测精确重复与近似重复时的行为差异，并发现处理近似重复的机制实际上涵盖了处理精确重复的方法。接着，他们描述了这一机制包含两个主要阶段：1. 利用通用位置注意力头及编码氨基酸相似性的神经元等生物特化组件构建特征表示；2. 诱导头关注重复片段间的对齐令牌，促进正确答案的选择。

Result: 研究结果表明，PLMs能够通过结合基于语言的模式匹配以及特定的生物知识来解决识别蛋白质序列中重复的问题，这不仅加深了我们对PLM工作原理的理解，也为利用PLM研究更复杂进化过程提供了可能。

Conclusion: 该研究表明，蛋白质语言模型通过整合语言模式匹配与专业生物信息，在识别蛋白质序列内重复片段方面表现出色。这种能力为未来使用此类模型探索更加复杂的进化现象奠定了坚实的基础。

Abstract: Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>


### [126] [Physics Informed Viscous Value Representations](https://arxiv.org/abs/2602.23280)
*Hrishikesh Viswanath,Juanwu Lu,S. Talha Bukhari,Damon Conover,Ziran Wang,Aniket Bera*

Main category: cs.LG

TL;DR: 提出了一种基于哈密顿-雅可比-贝尔曼方程粘性解的物理信息正则化方法，用于离线目标条件强化学习中改进价值估计，通过费曼-卡茨定理实现蒙特卡洛估计，提高了几何一致性和在高维复杂任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习由于状态-动作空间覆盖率有限，导致准确的价值估计具有挑战性。现有基于物理信息的方法尝试通过一阶偏微分方程（如Eikonal方程）定义的正则化来施加物理和几何约束，但这些方法在复杂高维环境中可能不适用。

Method: 提出一种新的物理信息正则化方法，源自哈密顿-雅可比-贝尔曼方程的粘性解；利用费曼-卡茨定理将PDE解重新表述为期望值，从而允许使用蒙特卡洛估计来避免高阶梯度中的数值不稳定问题。

Result: 实验表明该方法提高了几何一致性，并且广泛适用于导航以及高维复杂的操作任务。

Conclusion: 本研究提出的物理信息正则化方法能够有效提升离线目标条件强化学习的价值估计准确性，并在处理高维复杂环境时表现出良好的性能。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.

</details>


### [127] [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)
*Ilya Balabin,Thomas M. Kaiser*

Main category: cs.LG

TL;DR: 该系列论文旨在通过结合化学理论、生物学理论、概率论和因果关系，解决自然科学中机器学习模型的因果缺陷。第一部分提出了现象的基础因果结构形式框架，并引入了焦点的新概念，即机器学习算法在大数据集中聚焦于潜在机制的能力。还提供了关于Akt抑制剂家族的初步证明。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习技术在处理大型数据集方面取得了显著进展，但自然科学研究中的机器学习预测器通常被视为黑箱使用，缺乏对数据集因果结构的详细考虑。为了纠正这一点，需要一个坚实的统一理论来指导。

Method: 通过整合化学理论、生物学理论、概率论以及因果关系的概念，提出了一种新的数学框架——推断力学，用于建模自然界的机制而无需依赖还原论工具。

Result: 定义了焦点这一新概念，展示了机器学习算法如何能够从大数据集中识别出隐藏的基本机制，并且以Akt抑制剂为例子给出了初步证据。

Conclusion: 本文作为系列研究的第一部分，为化学生物学中的现象提供了一个基础性的因果结构框架，并通过引入焦点概念扩展到了机器学习领域，为后续两部分的研究奠定了基础。

Abstract: Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>


### [128] [A Proper Scoring Rule for Virtual Staining](https://arxiv.org/abs/2602.23305)
*Samuel Tonks,Steve Hood,Ryan Musso,Ceridwen Hopely,Steve Titus,Minh Doan,Iain Styles,Alexander Krull*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息增益(IG)的细胞级评估框架，用于直接评估预测后验分布。通过使用IG和其他度量标准对扩散和GAN模型进行评估，展示了IG能揭示其他度量无法显示的重大性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前生成虚拟染色（VS）模型的评估协议仅检查数据集上边际分布的准确性，而非预测后验分布。因此需要一种可以直接评价每个细胞预测后验的方法。

Method: 引入了信息增益（IG），这是一种严格合适评分规则，并且具有坚实的理论基础，允许解释性以及跨模型和特征比较结果。

Result: 研究中使用了IG及其他度量标准对一个广泛的HTS数据集上的扩散及基于GAN的模型进行了评估，结果显示IG能够揭示其他度量标准所不能发现的重要性能差异。

Conclusion: 信息增益为评估生成虚拟染色模型提供了新的视角，有助于更准确地衡量不同模型在高通量筛选中的表现。

Abstract: Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.

</details>


### [129] [Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336)
*Camilo Gomez,Pengyang Wang,Liansheng Tang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的可微分近似方法，称为Soft-Binary-Argmax，用于解决零一损失函数在梯度优化中的不可微分问题。通过约束优化框架构建平滑的、顺序保持的投影到n,k维超单纯形上，并且可以有效地计算其Jacobian并将其集成到二元和多类学习系统中。实验证明，这种方法通过对输出logits施加几何一致性约束，在大批量训练下显著提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的零一损失被认为是分类性能的黄金标准，但由于其非可微性而无法与基于梯度的优化兼容。本文旨在开发一种新的可微分近似技术来克服这一局限性，从而允许更丰富的归纳偏置并与任务特定目标更紧密地对齐。

Method: 研究者们提出了一个名为Soft-Binary-Argmax的新算子，它通过约束优化框架创建了一个光滑的、保持顺序的投影到n,k维超单纯形上。此外，他们还展示了如何有效计算该算子的雅可比矩阵，并将其整合进二元及多元学习体系中。

Result: 实验结果表明，本方法能够通过向输出logits添加几何一致性约束，在大规模批次训练情况下显著改善模型的泛化能力，从而缩小了通常在大规模批次训练中观察到的表现差距。

Conclusion: 本文介绍了一种新颖的方法，即Soft-Binary-Argmax，为零一损失提供了一个可微分的近似，这有助于提高在大量数据批次训练下的机器学习模型的泛化能力。

Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

</details>


### [130] [Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341)
*Alkis Kalavasis,Anay Mehrotra,Manolis Zampetakis,Felix Zhou,Ziyu Zhu*

Main category: cs.LG

TL;DR: 本文解决了在凸划分下高斯均值估计的两个基本问题：（1）何时均值在凸划分下是可识别的；（2）在可识别性和凸划分的前提下，是否可能实现计算效率高的估计。


<details>
  <summary>Details</summary>
Motivation: 粗数据产生于学习者只能观察到样本的部分信息时，即只知道样本所属集合而非其确切值。这种情况在测量四舍五入、传感器限制以及经济系统滞后中自然发生。研究聚焦于从粗数据中进行高斯均值估计的问题，特别是在每个真实样本x来自具有单位协方差的d维高斯分布但仅通过包含x的一个分区集合来揭示的情况下。当粗样本所含信息量“低”时，无法从观测样本中唯一恢复均值（即问题不可识别）。先前的研究已经证明，在未知均值可识别且分区仅由凸集组成时，可以进行样本高效的均值估计。然而，如果缺乏凸性，则均值估计变为NP难问题。尽管如此，关于均值在何种条件下于凸划分下可识别，以及在这种情况下是否能实现计算上有效的估计这两个基础问题仍待解决。

Method: 该文没有提供具体的方法细节，但从摘要中可以推断出，作者们可能使用了理论分析方法来探索在给定凸划分条件下高斯均值估计的可能性边界，并提出了新算法或改进现有技术以克服之前遇到的挑战，特别是针对那些在凸性假设下能够保证有效性的场景。

Result: 文章解决了上述提到的两个开放问题：一是确定了在凸划分条件下高斯均值何时可被识别；二是证实了在满足可识别性和凸划分条件的前提下，确实存在计算上可行的估计方法。

Conclusion: 通过这项工作，研究人员不仅明确了在特定条件下（即凸划分）高斯均值估计的可行性，还为开发更高效、更准确的数据处理策略奠定了基础，特别是在面对由于各种原因导致只能获得不完整或‘粗糙’数据的情况时。

Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

</details>


### [131] [SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353)
*Simon Roschmann,Paul Krzakala,Sonia Mazelet,Quentin Bouniot,Zeynep Akata*

Main category: cs.LG

TL;DR: 本研究提出了一种名为SOTAlign的两阶段框架，该框架能够利用少量配对数据和大量未配对数据来有效对齐预训练的单模态编码器。通过首先使用线性教师从有限的配对数据中恢复粗略共享几何结构，然后经由基于最优传输的散度在未配对样本上细化对齐，SOTAlign能够学习跨数据集和编码器对的强大联合嵌入，并且明显优于现有的监督与半监督基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究人员旨在探索是否可以通过大幅减少监督来实现有意义的模型对齐，而无需依赖对比损失和数百万配对样本。为此，他们引入了一种半监督设置，在这种设置下，使用少量图像-文本配对加上大量的未配对数据来对齐预训练的单模态编码器。

Method: 提出了SOTAlign，一种两阶段框架。第一阶段，通过一个线性教师从有限的配对数据中恢复粗略的共享几何；第二阶段，在未配对样本上通过基于最优传输的散度进一步细化对齐过程，以传递关系结构而不过度约束目标空间。

Result: SOTAlign能够有效地利用未配对的图像和文本数据，学习到跨不同数据集及编码器对的鲁棒联合嵌入，并且在性能上显著超越了全监督和半监督基准方法。

Conclusion: 这项研究表明，通过采用SOTAlign这样的方法，可以在很大程度上减少所需监督的同时达成有效的模型对齐，为未来的研究开辟了新的方向。

Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [132] [Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA](https://arxiv.org/abs/2602.22721)
*Fengyu Li,Junhao Zhu,Kaishi Song,Lu Chen,Zhongming Yao,Tianyi Li,Christian S. Jensen*

Main category: cs.DB

TL;DR: 提出了Operation-R1框架，通过新型强化学习训练轻量级大语言模型，在单次推理中生成高质量的数据准备管道以解决表格问答问题。相比多步准备基线方法，该框架在两个基准数据集上实现了平均绝对准确率的显著提升，同时大幅减少了表格压缩率和成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于操作员中心的解决方案虽然在表格问答任务中表现出色，但需要多次调用大型语言模型，导致延迟高且计算成本大。

Method: 开发了Operation-R1框架，采用一种新的强化学习变体来训练轻量级LLM（如Qwen-4B/1.7B），使其能够一次性生成用于TQA的高效数据准备流程。为支持此训练过程，引入了自监督奖励机制、差异感知组重采样技术以及两种互补机制——操作合并与自适应回滚，以增强生成流程的鲁棒性。

Result: 实验结果表明，在相同的大语言模型基础上，相较于多步骤准备基线，Operation-R1在两个基准测试集上分别获得了9.55和6.08个百分点的平均绝对精度增益，并且实现了79%的表压缩率及2.2倍的成本降低。

Conclusion: Operation-R1通过减少对大型语言模型的依赖次数并优化数据准备流程，不仅提高了表格问答任务中的性能，还显著降低了执行时间和成本开销。

Abstract: Table Question Answering (TQA) aims to answer natural language questions over structured tables. Large Language Models (LLMs) enable promising solutions to this problem, with operator-centric solutions that generate table manipulation pipelines in a multi-step manner offering state-of-the-art performance. However, these solutions rely on multiple LLM calls, resulting in prohibitive latencies and computational costs.
  We propose Operation-R1, the first framework that trains lightweight LLMs (e.g., Qwen-4B/1.7B) via a novel variant of reinforcement learning with verifiable rewards to produce high-quality data-preparation pipelines for TQA in a single inference step. To train such an LLM, we first introduce a self-supervised rewarding mechanism to automatically obtain fine-grained pipeline-wise supervision signals for LLM training. We also propose variance-aware group resampling to mitigate training instability. To further enhance robustness of pipeline generation, we develop two complementary mechanisms: operation merge, which filters spurious operations through multi-candidate consensus, and adaptive rollback, which offers runtime protection against information loss in data transformation. Experiments on two benchmark datasets show that, with the same LLM backbone, Operation-R1 achieves average absolute accuracy gains of 9.55 and 6.08 percentage points over multi-step preparation baselines, with 79\% table compression and a 2.2$\times$ reduction in monetary cost.

</details>


### [133] [Optimizing SSD-Resident Graph Indexing for High-Throughput Vector Search](https://arxiv.org/abs/2602.22805)
*Weichen Zhao,Yuncheng Lu,Yao Tian,Hao Zhang,Jiehui Li,Minghao Zhao,Yakun Li,Weining Qian*

Main category: cs.DB

TL;DR: VeloANN, a new out-of-memory ANNS system, optimizes storage and CPU utilization through a locality-aware data layout, coroutine-based asynchronous runtime, and record-level buffer pool, achieving up to 5.8x throughput and 3.25x latency reduction compared to state-of-the-art disk-based systems.


<details>
  <summary>Details</summary>
Motivation: The existing out-of-memory ANNS systems suffer from CPU underutilization and read amplification due to limited access locality during graph traversal. VeloANN aims to solve these issues to improve search efficiency when working with large vector indexes stored on SSDs.

Method: VeloANN introduces a locality-aware data layout that uses hierarchical compression and an affinity-based placement scheme to co-locate related vectors within the same page, thus reducing fragmentation and over-fetching. It also employs a record-level buffer pool to keep hot records in memory, minimizing excessive page swapping. Furthermore, it adopts a coroutine-based asynchronous runtime for efficient task scheduling and incorporates asynchronous prefetching alongside a beam-aware search strategy to optimize the use of cached data.

Result: Experiments demonstrate that VeloANN can achieve up to 5.8 times higher throughput and reduce latency by up to 3.25 times compared to leading disk-based ANNS systems. Additionally, it reaches 0.92 times the throughput of in-memory systems while using only 10% of their memory footprint.

Conclusion: VeloANN significantly enhances the performance of out-of-memory ANNS, making it a promising solution for applications requiring efficient similarity search over large-scale datasets, especially when main memory is a limiting factor.

Abstract: Graph-based approximate nearest neighbor search (ANNS) methods (e.g., HNSW) have become the de facto state of the art for their high precision and low latency. To scale beyond main memory, recent out-of-memory ANNS systems leverage SSDs to store large vector indexes. However, they still suffer from severe CPU underutilization and read amplification (i.e., storage stalls) caused by limited access locality during graph traversal. We present VeloANN, which mitigates storage stalls through a locality-aware data layout and a coroutine-based asynchronous runtime. VeloANN utilizes hierarchical compression and affinity-based data placement scheme to co-locate related vectors within the same page, effectively reducing fragmentation and over-fetching. We further design a record-level buffer pool, where each record groups the neighbors of a vector; by persistently retaining hot records in memory, it eliminates excessive page swapping under constrained memory budgets. To minimize CPU scheduling overheads during disk I/O interruptions, VeloANN employs a coroutine-based asynchronous runtime for lightweight task scheduling. On top of this, it incorporates asynchronous prefetching and a beam-aware search strategy to prioritize cached data, ultimately improving overall search efficiency. Extensive experiments show that VeloANN outperforms state-of-the-art disk-based ANN systems by up to 5.8x in throughput and 3.25x in latency reduction, while achieving 0.92x the throughput of in-memory systems using only 10% of their memory footprint.

</details>


### [134] [Workload-Aware Incremental Reclustering in Cloud Data Warehouses](https://arxiv.org/abs/2602.23289)
*Yipeng Liu,Renfei Zhou,Jiaqi Yan,Haunchen Zhang*

Main category: cs.DB

TL;DR: 本文提出了一种新的工作负载感知算法WAIR，用于识别并对查询范围边界上的微分区进行重新聚类，以提高数据修剪效率。该方法在保证查询性能接近最优的同时，显著降低了重新聚类的成本，并通过标准基准测试和真实工作负载验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动聚类方法缺乏在连续数据摄入和不断变化的工作负载下的动态云环境中所需的灵活性。为了提高大数据表中数据修剪的有效性，需要一种更加灵活高效的方法来维护数据聚类。

Method: 提出了边界微分区的概念，并开发了WAIR算法，该算法能够识别出对修剪效率最为关键的边界微分区并仅对这些分区进行重新聚类。WAIR的设计目标是在保持近似全排序表布局查询性能的前提下，将重聚类成本控制在一个理论上界以内。

Result: 实验结果表明，与现有解决方案相比，WAIR不仅提高了查询性能，还减少了总体成本。这通过在标准基准（TPC-H, DSB）以及实际工作负载中的原型重聚类服务实现进行了评估证实。

Conclusion: 通过引入WAIR算法，可以有效地增强云端数据仓库中的数据修剪效率，同时减少因频繁重聚类带来的额外开销，为处理大规模、动态变化的数据集提供了一个有前景的方案。

Abstract: Modern cloud data warehouses store data in micro-partitions and rely on metadata (e.g., zonemaps) for efficient data pruning during query processing. Maintaining data clustering in a large-scale table is crucial for effective data pruning. Existing automatic clustering approaches lack the flexibility required in dynamic cloud environments with continuous data ingestion and evolving workloads. This paper advocates a clean separation between reclustering policy and clustering-key selection. We introduce the concept of boundary micro-partitions that sit on the boundary of query ranges. We then present WAIR, a workload-aware algorithm to identify and recluster only boundary micro-partitions most critical for pruning efficiency. WAIR achieves near-optimal (with respect to fully sorted table layouts) query performance but incurs significantly lower reclustering cost with a theoretical upper bound. We further implement the algorithm into a prototype reclustering service and evaluate on standard benchmarks (TPC-H, DSB) and a real-world workload. Results show that WAIR improves query performance and reduces the overall cost compared to existing solutions.

</details>


### [135] [AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search](https://arxiv.org/abs/2602.23342)
*Weijian Chen,Haotian Liu,Yangshen Deng,Long Xiang,Liang Huang,Gezi Li,Bo Tang*

Main category: cs.DB

TL;DR: 提出了一种名为AlayaLaser的新型高效磁盘图索引系统，通过优化计算瓶颈而非仅关注I/O减少来提高大规模高维向量相似性搜索性能。实验表明，AlayaLaser不仅超越了现有的基于磁盘图的索引系统，甚至在某些情况下表现优于内存中的索引系统。


<details>
  <summary>Details</summary>
Motivation: 随着向量数据维度（例如数百或数千）的增加，当前基于磁盘图的近似最近邻搜索系统的性能实际上受到计算能力的限制，而不是I/O操作。这意味着存在一个重要的优化机会：现有系统普遍专注于减少I/O，而忽略了可以大幅改进性能的计算开销问题。

Method: 首先使用适应性的roofline模型对现有基于磁盘图的索引系统进行了性能分析；然后设计了一个新的磁盘数据布局以利用现代CPU上的SIMD指令有效缓解计算瓶颈；接着开发了一系列优化技术（如基于度数的节点缓存、基于集群的入口点选择和早期调度策略）进一步提升AlayaLaser的表现。

Result: 广泛的实验证明，在多种大规模高维向量数据集上，AlayaLaser不仅优于其他基于磁盘图的索引方法，还能与甚至超过一些内存索引系统的性能。

Conclusion: AlayaLaser作为一种针对高维数据特别优化的新方法，在处理大规模高维向量相似性搜索时表现出色，展示了其在实际应用中的巨大潜力。

Abstract: On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space.
  In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-based node cache, cluster-based entry point selection, and early dispatch strategy) to further improve the performance of AlayaLaser. We last conduct extensive experimental studies on a wide range of large-scale high-dimensional vector datasets to verify the superiority of AlayaLaser. Specifically, AlayaLaser not only surpasses existing on-disk graph-based index systems but also matches or even exceeds the performance of in-memory index systems.

</details>
