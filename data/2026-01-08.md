<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 46]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [The Pneuma Project: Reifying Information Needs as Relational Schemas to Automate Discovery, Guide Preparation, and Align Data with Intent](https://arxiv.org/abs/2601.03618)
*Muhammad Imam Luthfi Balaka,Raul Castro Fernandez*

Main category: cs.DB

TL;DR: Pneuma-Seeker, a system powered by language models, aims to assist users in discovering and preparing data by iteratively refining their information needs. It integrates advanced techniques like retrieval-augmented generation and agentic frameworks to support semi-automatic workflows, thereby helping to clarify user intent, guide the discovery process, and create documents that meet specific needs. Additionally, it serves as a documentation layer for preserving organizational knowledge.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges faced in data discovery and preparation, particularly when user intent is not clearly defined or changes over time. The goal is to develop a more efficient and effective way to help users articulate and fulfill their information needs through a language model-powered platform.

Method: The Pneuma Project introduces Pneuma-Seeker, which employs a combination of context specialization, a conductor-style planner, and a convergence mechanism based on shared state. This system leverages recent advancements in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation methods to support an iterative, language-guided workflow for data discovery and document creation.

Result: Through LLM-based user simulations, the evaluation demonstrates that Pneuma-Seeker can successfully surface latent user intents, guide the data discovery process, and produce documents that are fit for purpose. Moreover, it contributes to the organization's memory by acting as an emergent documentation layer, capturing and preserving institutional knowledge.

Conclusion: Pneuma-Seeker presents a novel approach to overcoming the bottlenecks in data management, especially concerning vague or evolving user intents. By integrating cutting-edge technologies and methodologies, it not only facilitates the articulation and fulfillment of information needs but also supports the preservation of organizational knowledge, marking a significant step forward in the field of data discovery and preparation.

Abstract: Data discovery and preparation remain persistent bottlenecks in the data management lifecycle, especially when user intent is vague, evolving, or difficult to operationalize. The Pneuma Project introduces Pneuma-Seeker, a system that helps users articulate and fulfill information needs through iterative interaction with a language model-powered platform. The system reifies the user's evolving information need as a relational data model and incrementally converges toward a usable document aligned with that intent. To achieve this, the system combines three architectural ideas: context specialization to reduce LLM burden across subtasks, a conductor-style planner to assemble dynamic execution plans, and a convergence mechanism based on shared state. The system integrates recent advances in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation to support semi-automatic, language-guided workflows. We evaluate the system through LLM-based user simulations and show that it helps surface latent intent, guide discovery, and produce fit-for-purpose documents. It also acts as an emergent documentation layer, capturing institutional knowledge and supporting organizational memory.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [2] [Enhancing Retrieval-Augmented Generation with Two-Stage Retrieval: FlashRank Reranking and Query Expansion](https://arxiv.org/abs/2601.03258)
*Sherine George*

Main category: cs.IR

TL;DR: 本文提出了一种两阶段检索管道，结合了基于大语言模型（LLM）的查询扩展和FlashRank重排序器，以在有限的令牌预算下选择最优证据子集，从而提高回答准确性、忠实性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）框架虽然增强了事实性和领域适应性，但面临一个关键瓶颈：如何在检索召回率与大型语言模型上下文限制之间找到平衡点。检索过少可能遗漏重要信息，而过多则可能导致提示窗口过载，降低相关性并增加成本。

Method: 提出了一种两阶段检索流程，包括使用LLM驱动的查询扩展来提高候选召回率，以及引入FlashRank作为快速边际效用重新排名工具，该工具能够在给定令牌预算约束下动态选取最佳证据子集。FlashRank将文档实用性建模为相关性、新颖性、简洁性和交叉编码证据的一个加权组合。

Result: 通过上述方法，系统能够有效提升答案的准确性、忠实度以及计算效率。

Conclusion: 本文介绍的方法提供了一个通用解决方案，用于改善检索增强生成过程中面临的挑战，即在保证检索内容广泛覆盖的同时，也考虑到了与大型语言模型交互时的成本效益问题。

Abstract: Retrieval-Augmented Generation (RAG) couples a retriever with a large language model (LLM) to ground generated responses in external evidence. While this framework enhances factuality and domain adaptability, it faces a key bottleneck: balancing retrieval recall with limited LLM context. Retrieving too few passages risks missing critical context, while retrieving too many overwhelms the prompt window, diluting relevance and increasing cost.
  We propose a two-stage retrieval pipeline that integrates LLM-driven query expansion to improve candidate recall and FlashRank, a fast marginal-utility reranker that dynamically selects an optimal subset of evidence under a token budget. FlashRank models document utility as a weighted combination of relevance, novelty, brevity, and cross-encoder evidence. Together, these modules form a generalizable solution that increases answer accuracy, faithfulness, and computational efficiency.

</details>


### [3] [LLMDiRec: LLM-Enhanced Intent Diffusion for Sequential Recommendation](https://arxiv.org/abs/2601.03259)
*Bo-Chian Chen,Manel Slokom*

Main category: cs.IR

TL;DR: 本研究提出了一种新的方法LLMDiRec，该方法通过将大型语言模型（LLMs）整合到意图感知的扩散模型中，以解决现有序列推荐模型在捕捉用户行为背后的丰富语义意图方面的不足。实验结果表明，该方法在五个公开数据集上优于现有最先进算法，特别是在捕捉复杂用户意图和提高长尾项目的推荐性能方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的序列推荐模型，即使是先进的基于扩散的方法，在捕捉用户行为背后的丰富语义意图方面也常常遇到困难，特别是对于新用户或长尾项目而言。这种局限性源于它们依赖于缺乏语义基础的身份标识嵌入。

Method: 研究者们提出了LLMDiRec，一种新颖的方法，它通过将大型语言模型（LLMs）与一个意图感知的扩散模型相结合来弥补这一差距。该方法利用动态融合机制和多任务目标对来自ID嵌入的合作信号以及来自LLMs的丰富语义表示进行结合。

Result: 通过对五个公开数据集的广泛实验，证明了LLMDiRec比最先进的算法表现更好，特别是在捕捉复杂的用户意图和增强针对长尾物品的推荐效果方面显示出显著改进。

Conclusion: 通过将大型语言模型集成到意图感知扩散模型中，LLMDiRec能够有效地改善推荐系统对于复杂用户意图的理解，并且特别地提高了长尾商品的推荐质量。

Abstract: Existing sequential recommendation models, even advanced diffusion-based approaches, often struggle to capture the rich semantic intent underlying user behavior, especially for new users or long-tail items. This limitation stems from their reliance on ID-based embeddings, which lack semantic grounding. We introduce LLMDiRec, a new approach that addresses this gap by integrating Large Language Models (LLMs) into an intent-aware diffusion model. Our approach combines collaborative signals from ID embeddings with rich semantic representations from LLMs, using a dynamic fusion mechanism and a multi-task objective to align both views. We run extensive experiments on five public datasets. We run extensive experiments on five public datasets. We demonstrate that \modelname outperforms state-of-the-art algorithms, with particularly strong improvements in capturing complex user intents and enhancing recommendation performance for long-tail items.

</details>


### [4] [Roles of MLLMs in Visually Rich Document Retrieval for RAG: A Survey](https://arxiv.org/abs/2601.03262)
*Xiantao Zhang*

Main category: cs.IR

TL;DR: 本文探讨了多模态大语言模型（MLLMs）在视觉丰富文档（VRDs）检索增强生成中的应用，归纳了三种角色：模态统一标注者、多模态嵌入器和端到端表示器，并比较了它们在检索粒度、信息保真度、延迟与索引大小及重排序和基础兼容性方面的表现。同时指出了关键权衡点，为每种角色的应用提供了实用建议，并提出了未来研究方向如自适应检索单元、模型规模缩减以及评估方法的发展。


<details>
  <summary>Details</summary>
Motivation: 视觉丰富文档（VRDs）因其布局依赖语义、脆弱的OCR识别及证据分散于复杂图表和结构化表格中，给检索增强生成带来了挑战。因此，有必要探索如何利用多模态大语言模型（MLLMs）来解决这些问题，使VRD检索更加实用。

Method: 通过文献综述的方式，将现有研究分为三个主要角色：1. 模态统一标注者 2. 多模态嵌入器 3. 端到端表示器。并基于检索粒度、信息准确性、延迟时间与索引大小等因素对比分析了这三类角色的特点。

Result: 总结了不同角色在处理VRD时的优势与劣势，讨论了各自适用场景下的选择指南，并对未来研究方向提出了展望。

Conclusion: 多模态大语言模型在提升视觉丰富文档检索效率方面展现了巨大潜力，但同时也存在一些亟待解决的问题。明确不同类型解决方案的特点有助于更好地指导实际应用；而针对自适应检索单位开发、减少模型体积以及建立更有效的评估机制将是该领域未来的重要发展方向。

Abstract: Visually rich documents (VRDs) challenge retrieval-augmented generation (RAG) with layout-dependent semantics, brittle OCR, and evidence spread across complex figures and structured tables. This survey examines how Multimodal Large Language Models (MLLMs) are being used to make VRD retrieval practical for RAG. We organize the literature into three roles: Modality-Unifying Captioners, Multimodal Embedders, and End-to-End Representers. We compare these roles along retrieval granularity, information fidelity, latency and index size, and compatibility with reranking and grounding. We also outline key trade-offs and offer some practical guidance on when to favor each role. Finally, we identify promising directions for future research, including adaptive retrieval units, model size reduction, and the development of evaluation methods.

</details>


### [5] [Efficient Sequential Recommendation for Long Term User Interest Via Personalization](https://arxiv.org/abs/2601.03479)
*Qiang Zhang,Hanchao Yu,Ivan Ji,Chen Yuan,Yi Zhang,Chihuang Liu,Xiaolong Wang,Christopher E. Lambert,Ren Chen,Chen Kovacs,Xinzhu Bei,Renqin Cai,Rui Li,Lizhu Zhang,Xiangjun Fan,Qunshu Zhang,Benyu Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种新的序列推荐方法，通过将用户长交互历史压缩为可学习的token，并与最近的交互结合来生成推荐，从而提高了效率并保持了高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管序列模型在推荐系统中的规模法则已被验证，但在其计算能力在实际应用中因transformer模型非线性（二次）增长的本质而显示出低效。为了提高序列模型的效率，作者引入了这种新方法。

Method: 该方法将长用户交互历史压缩成可学习的tokens，并与最近的交互相结合以生成推荐。此方法可以应用于现有的基于transformer的推荐模型，如HSTU和HLLM。

Result: 广泛的实验表明，该方法能够显著降低计算成本，同时保持较高的推荐准确率。

Conclusion: 所提出的方法展示了其在多种顺序模型上的通用性和有效性，为改善推荐系统的效率提供了新的解决方案。

Abstract: Recent years have witnessed success of sequential modeling, generative recommender, and large language model for recommendation. Though the scaling law has been validated for sequential models, it showed inefficiency in computational capacity when considering real-world applications like recommendation, due to the non-linear(quadratic) increasing nature of the transformer model. To improve the efficiency of the sequential model, we introduced a novel approach to sequential recommendation that leverages personalization techniques to enhance efficiency and performance. Our method compresses long user interaction histories into learnable tokens, which are then combined with recent interactions to generate recommendations. This approach significantly reduces computational costs while maintaining high recommendation accuracy. Our method could be applied to existing transformer based recommendation models, e.g., HSTU and HLLM. Extensive experiments on multiple sequential models demonstrate its versatility and effectiveness. Source code is available at \href{https://github.com/facebookresearch/PerSRec}{https://github.com/facebookresearch/PerSRec}.

</details>


### [6] [STELLA: Self-Reflective Terminology-Aware Framework for Building an Aerospace Information Retrieval Benchmark](https://arxiv.org/abs/2601.03496)
*Bongmin Kim*

Main category: cs.IR

TL;DR: 本文提出了STELLA框架，用于构建航空航天领域的信息检索基准。该基准通过系统化流程从NASA技术报告服务器文档中构建，并生成两种类型的查询：术语一致查询（TCQ）和术语不可知查询（TAQ），以分别评估词汇匹配与语义匹配能力。实验表明大型基于解码器的嵌入模型在语义理解上表现最强，而像BM25这样的词汇匹配方法在需要精确词汇匹配的技术领域仍然非常有竞争力。


<details>
  <summary>Details</summary>
Motivation: 航空航天行业的任务高度依赖于大量技术文档的搜索与重用，但目前缺少能够反映该领域术语及查询意图特征的公开信息检索基准。

Method: 提出STELLA框架，并据此构建了一个特定于航空航天领域的信息检索评估集——STELLA基准。该框架包括文档布局检测、段落切分、术语词典构建、合成查询生成以及跨语言扩展等步骤。此外，结合密度链（CoD）和自我反思方法改进了查询生成的质量，并实现了反映真实用户查询习惯的混合跨语言扩展。

Result: 七种嵌入模型在STELLA基准上的评估显示，大型基于解码器的嵌入模型展示了最强的语义理解能力；然而，在对准确词汇匹配至关重要的领域内，诸如BM25之类的词汇匹配方法依然保持极高的竞争力。

Conclusion: STELLA基准为航空航天领域内嵌入模型性能的可靠评估与提升提供了可重复的基础。

Abstract: Tasks in the aerospace industry heavily rely on searching and reusing large volumes of technical documents, yet there is no public information retrieval (IR) benchmark that reflects the terminology- and query-intent characteristics of this domain. To address this gap, this paper proposes the STELLA (Self-Reflective TErminoLogy-Aware Framework for BuiLding an Aerospace Information Retrieval Benchmark) framework. Using this framework, we introduce the STELLA benchmark, an aerospace-specific IR evaluation set constructed from NASA Technical Reports Server (NTRS) documents via a systematic pipeline that comprises document layout detection, passage chunking, terminology dictionary construction, synthetic query generation, and cross-lingual extension. The framework generates two types of queries: the Terminology Concordant Query (TCQ), which includes the terminology verbatim to evaluate lexical matching, and the Terminology Agnostic Query (TAQ), which utilizes the terminology's description to assess semantic matching. This enables a disentangled evaluation of the lexical and semantic matching capabilities of embedding models. In addition, we combine Chain-of-Density (CoD) and the Self-Reflection method with query generation to improve quality and implement a hybrid cross-lingual extension that reflects real user querying practices. Evaluation of seven embedding models on the STELLA benchmark shows that large decoder-based embedding models exhibit the strongest semantic understanding, while lexical matching methods such as BM25 remain highly competitive in domains where exact lexical matching technical term is crucial. The STELLA benchmark provides a reproducible foundation for reliable performance evaluation and improvement of embedding models in aerospace-domain IR tasks. The STELLA benchmark can be found in https://huggingface.co/datasets/telepix/STELLA.

</details>


### [7] [Shielded RecRL: Explanation Generation for Recommender Systems without Ranking Degradation](https://arxiv.org/abs/2601.03608)
*Ansh Tiwari,Ayush Chauhan*

Main category: cs.IR

TL;DR: 本文提出了一种名为Shielded RecRL的强化学习方法，旨在为推荐系统生成个性化解释而不影响系统的原始排名性能。通过结合长度、内容相关性和连贯性的复合奖励信号，并使用近端策略优化（PPO）对大型语言模型进行微调，仅调整其0.4%的参数。实验结果显示，在亚马逊图书数据集上，Shielded RecRL相对点击率提高了22.5%，同时保持了推荐项目的排名行为几乎不变。


<details>
  <summary>Details</summary>
Motivation: 传统的基于RLHF的推荐方法直接优化项目排名，可能忽略了提供有用的解释给用户的重要性。Shielded RecRL的目标是在不牺牲推荐系统原有排序性能的前提下，利用强化学习来生成个性化的解释，从而增强用户体验。

Method: 采用两塔架构，保持推荐系统的排名模型不变的同时让一个语言模型学会产生有用的解释。设计了一个结合了解释长度、内容相关性以及连贯性的综合奖励信号，并应用带KL散度约束的近端策略优化(PPO)技术，通过LoRA适配器仅对大型语言模型的0.4%参数进行微调。

Result: 在包含约5万次交互的亚马逊图书数据集（奇幻和浪漫类）上的实验表明，Shielded RecRL能够使相对点击率(CTR)提高22.5%（比基线高出1.225倍），同时几乎不影响推荐项的排名表现。广泛的消融研究证实了梯度屏蔽策略和奖励设计有效地平衡了解释质量与策略漂移。

Conclusion: Shielded RecRL通过提供丰富且个性化的解释增强了推荐面向用户的方面，而不会降低核心推荐准确性。

Abstract: We introduce Shielded RecRL, a reinforcement learning approach to generate personalized explanations for recommender systems without sacrificing the system's original ranking performance. Unlike prior RLHF-based recommender methods that directly optimize item rankings, our two-tower architecture keeps the recommender's ranking model intact while a language model learns to produce helpful explanations. We design a composite reward signal combining explanation length, content relevance, and coherence, and apply proximal policy optimization (PPO) with a KL-divergence constraint to fine-tune a large language model with only 0.4% of its parameters trainable via LoRA adapters. In experiments on an Amazon Books dataset (approximately 50K interactions in the fantasy and romance genres), Shielded RecRL improved the relative click-through rate (CTR) by 22.5% (1.225x over baseline) while keeping the recommender's item-ranking behavior virtually unchanged. An extensive ablation study confirms that our gradient shielding strategy and reward design effectively balance explanation quality and policy drift. Our results demonstrate that Shielded RecRL enhances user-facing aspects of recommendations through rich, personalized explanations without degrading core recommendation accuracy.

</details>


### [8] [Perception-Aware Bias Detection for Query Suggestions](https://arxiv.org/abs/2601.03730)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 本文扩展了Bonart等人开发的针对人物相关搜索查询建议中的偏见检测流程，引入了感知意识度量以更好地检测系统性主题偏见。分析结果表明，该增强流程能够更有效地识别出用户可能察觉到的偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管网页搜索中的偏见一直是研究的重点，但对于查询建议中偏见的关注却很少。随着对这一问题认识的提高，自动偏见检测方法的需求也在增加。然而，由于查询建议的稀疏性和缺乏上下文元数据，使得它们成为偏见检测中的难题。此外，这些建议往往被用户短暂且潜意识地接受。

Method: 通过向现有偏见检测流程（由Bonart等提出）中添加感知意识指标来改进其能力，特别是针对与人相关的搜索查询建议中存在的系统性主题偏见。

Result: 采用改进后的流程进行的分析证实了这种方法的有效性，并且由于使用了感知意识偏见检测指标，可以假设管道产生的发现反映了用户实际上会注意到的偏见。

Conclusion: 本研究表明，通过结合感知意识度量，能够有效提升人物相关搜索查询建议中系统性主题偏见的检测效果。

Abstract: Bias in web search has been in the spotlight of bias detection research for quite a while. At the same time, little attention has been paid to query suggestions in this regard. Awareness of the problem of biased query suggestions has been raised. Likewise, there is a rising need for automatic bias detection approaches. This paper adds on the bias detection pipeline for bias detection in query suggestions of person-related search developed by Bonart et al. \cite{Bonart_2019a}. The sparseness and lack of contextual metadata of query suggestions make them a difficult subject for bias detection. Furthermore, query suggestions are perceived very briefly and subliminally. To overcome these issues, perception-aware metrics are introduced. Consequently, the enhanced pipeline is able to better detect systematic topical bias in search engine query suggestions for person-related searches. The results of an analysis performed with the developed pipeline confirm this assumption. Due to the perception-aware bias detection metrics, findings produced by the pipeline can be assumed to reflect bias that users would discern.

</details>


### [9] [Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning](https://arxiv.org/abs/2601.03748)
*Dario Maio,Stefano Rizzi*

Main category: cs.IR

TL;DR: 本文提出了一种结合语义聚类和多维分区的策略，以改进大规模检索增强生成(RAG)系统的检索效率和可解释性。通过引入维度事实模型(DFM)作为概念框架来指导RAG语料库的多维分区设计，旨在为现代RAG架构提供一种原则性的、可解释且可控的检索策略。


<details>
  <summary>Details</summary>
Motivation: 现有的工业规模检索平台虽然在效率上有效，但缺乏对语料库划分的概念性理由。这些机制依赖于基于相似性的底层组织方式，而没有充分利用时间、组织背景等概念维度来进行更合理的分区。

Method: 提出了一个名为维度事实模型(DFM)的概念框架，用于指导RAG语料库中多维分区的设计。该框架允许围绕事实、维度、层次结构及粒度进行有原则地推理，并支持分层路由与控制回退策略，使得即使在元数据不完整的情况下也能保证检索过程的稳健性。

Result: 通过采用DFM框架，能够将搜索过程从‘黑箱’式的相似度匹配转变为一个可管理和确定的工作流程，从而提高检索操作的透明度与可控性。

Conclusion: 本文旨在填补OLAP风格的多维建模与现代RAG架构之间的空白，促进对大规模下有原则、可解释及可控检索策略的研究。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.

</details>


### [10] [Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation](https://arxiv.org/abs/2601.03903)
*Yuhan Yang,Jie Zou,Guojia An,Jiwei Wei,Yang Yang,Heng Tao Shen*

Main category: cs.IR

TL;DR: 提出了一种基于扩散的潜在邻居生成模型DiffSBR，用于会话推荐系统中，通过增强会话表示来提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于显式观察到的会话数据，忽略了潜在的相关邻居，因此未能充分利用邻居会话在推荐中的潜力。

Method: DiffSBR利用检索增强扩散和自增强扩散两个模块生成高质量的潜在邻居。检索增强扩散模块使用检索到的邻居作为指导信号来约束并重建潜在邻居的分布；而自增强扩散模块则通过对比学习将当前会话的多模态信号注入，以明确引导潜在邻居的生成。

Result: 四个公开数据集上的广泛实验表明，DiffSBR能够生成有效的潜在邻居，并相对于最新的基准提高了推荐性能。

Conclusion: DiffSBR模型通过引入潜在邻居增强了会话表示，从而有效缓解了数据稀疏性问题并提升了基于会话的推荐系统的性能。

Abstract: Session-based recommendation aims to predict the next item that anonymous users may be interested in, based on their current session interactions. Recent studies have demonstrated that retrieving neighbor sessions to augment the current session can effectively alleviate the data sparsity issue and improve recommendation performance. However, existing methods typically rely on explicitly observed session data, neglecting latent neighbors - not directly observed but potentially relevant within the interest space - thereby failing to fully exploit the potential of neighbor sessions in recommendation. To address the above limitation, we propose a novel model of diffusion-based latent neighbor generation for session-based recommendation, named DiffSBR. Specifically, DiffSBR leverages two diffusion modules, including retrieval-augmented diffusion and self-augmented diffusion, to generate high-quality latent neighbors. In the retrieval-augmented diffusion module, we leverage retrieved neighbors as guiding signals to constrain and reconstruct the distribution of latent neighbors. Meanwhile, we adopt a training strategy that enables the retriever to learn from the feedback provided by the generator. In the self-augmented diffusion module, we explicitly guide the generation of latent neighbors by injecting the current session's multi-modal signals through contrastive learning. After obtaining the generated latent neighbors, we utilize them to enhance session representations for improving session-based recommendation. Extensive experiments on four public datasets show that DiffSBR generates effective latent neighbors and improves recommendation performance against state-of-the-art baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Lightweight Transformer Architectures for Edge Devices in Real-Time Applications](https://arxiv.org/abs/2601.03290)
*Hema Hariharan Samson*

Main category: cs.LG

TL;DR: 该论文全面调查了专为边缘设备设计的轻量级Transformer架构，涵盖了模型压缩、量化、剪枝和知识蒸馏等技术，并对MobileBERT、TinyBERT等变体进行了性能基准测试。实验结果表明，现代轻量级Transformer在减少模型大小和推理延迟的同时，能够保持较高的准确率。此外，文中还分析了不同优化策略的效果，并提出了一个实用的6步部署流程。


<details>
  <summary>Details</summary>
Motivation: 将基于Transformer的模型部署到资源受限的边缘设备上是实现实时人工智能应用的关键挑战。为了克服这一挑战，研究人员致力于开发轻量级Transformer架构并探索各种优化技术。

Method: 通过系统地回顾包括MobileBERT、TinyBERT在内的多种轻量级Transformer变体，并提供它们在GLUE、SQuAD等标准数据集上的性能基准；同时，研究当前行业采用模式以及主流硬件平台（如NVIDIA Jetson）和部署框架（如TensorFlow Lite）。

Result: 研究表明，现代轻量级Transformers可以在保持全模型75-96%精度的同时，使模型大小减少4-10倍，推理延迟降低3-9倍，使得这些模型能够在功率消耗仅为2-5W的设备上运行。

Conclusion: 稀疏注意力机制、混合精度量化(INT8/FP16)以及硬件感知神经架构搜索被认定为最有效的优化策略。文章还指出了实现最佳硬件利用率的理想参数范围，针对不同类型模型的最佳量化点，以及跨边缘平台的能量效率概况。

Abstract: The deployment of transformer-based models on resource-constrained edge devices represents a critical challenge in enabling real-time artificial intelligence applications. This comprehensive survey examines lightweight transformer architectures specifically designed for edge deployment, analyzing recent advances in model compression, quantization, pruning, and knowledge distillation techniques. We systematically review prominent lightweight variants including MobileBERT, TinyBERT, DistilBERT, EfficientFormer, EdgeFormer, and MobileViT, providing detailed performance benchmarks on standard datasets such as GLUE, SQuAD, ImageNet-1K, and COCO. Our analysis encompasses current industry adoption patterns across major hardware platforms (NVIDIA Jetson, Qualcomm Snapdragon, Apple Neural Engine, ARM architectures), deployment frameworks (TensorFlow Lite, ONNX Runtime, PyTorch Mobile, CoreML), and optimization strategies. Experimental results demonstrate that modern lightweight transformers can achieve 75-96% of full-model accuracy while reducing model size by 4-10x and inference latency by 3-9x, enabling deployment on devices with as little as 2-5W power consumption. We identify sparse attention mechanisms, mixed-precision quantization (INT8/FP16), and hardware-aware neural architecture search as the most effective optimization strategies. Novel findings include memory-bandwidth bottleneck analysis revealing 15-40M parameter models achieve optimal hardware utilization (60-75% efficiency), quantization sweet spots for different model types, and comprehensive energy efficiency profiling across edge platforms. We establish real-time performance boundaries and provide a practical 6-step deployment pipeline achieving 8-12x size reduction with less than 2% accuracy degradation.

</details>


### [12] [Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning](https://arxiv.org/abs/2601.03320)
*Yu Luo,Shuo Han,Yihan Hu,Dong Li,Jianye Hao*

Main category: cs.LG

TL;DR: 本文提出了一种新的策略优化框架$R^2VPO$，通过约束策略比率的方差来稳定策略更新并保留有价值轨迹的梯度信号。该方法在微调最先进的大型语言模型时表现出色，与基于强剪切的基线相比，平均相对增益高达17%，同时达到收敛所需的rollouts减少了约50%。


<details>
  <summary>Details</summary>
Motivation: 当前的在线强化学习方法，特别是PPO和GRPO，在微调大型语言模型（LLMs）方面表现突出。然而，这些方法中的策略比率裁剪虽然能稳定训练过程，但会无差别地截断来自高回报但高分歧动作的梯度，抑制了复杂推理中罕见但极具信息量的“顿悟时刻”。此外，一旦数据稍微过时，硬性裁剪就会使其变得不可用，导致严重的样本效率低下问题。

Method: 本文重新审视了策略优化中的信任区域目标，并证明明确约束策略比率的方差可以提供一个原则性的、平滑的硬性裁剪替代方案。基于这一见解，提出了$R^2VPO$（比率-方差正则化策略优化），这是一个新颖的原对偶框架，支持稳定的在线学习，并通过动态重加权旧样本来实现有原则的离线数据重用而不是丢弃它们。

Result: $R^2VPO$ 在微调包括DeepSeek-Distill-Qwen-1.5B 和 openPangu-Embedded系列(1B 和 7B)在内的最先进LLMs上进行了广泛评估，结果表明$R^2VPO$始终实现了更优的渐近性能，相较于强大的基于裁剪的基线，平均相对增益最高可达17%，同时大约需要少50%的rollouts来达到收敛。

Conclusion: 研究发现确立了比率方差控制作为提高RL基础LLM对齐中稳定性和数据效率的一个有前途的方向。

Abstract: On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence actions, suppressing rare but highly informative "eureka moments" in complex reasoning. Moreover, once data becomes slightly stale, hard clipping renders it unusable, leading to severe sample inefficiency. In this work, we revisit the trust-region objective in policy optimization and show that explicitly constraining the \emph{variance (second central moment) of the policy ratio} provides a principled and smooth relaxation of hard clipping. This distributional constraint stabilizes policy updates while preserving gradient signals from valuable trajectories. Building on this insight, we propose $R^2VPO$ (Ratio-Variance Regularized Policy Optimization), a novel primal-dual framework that supports stable on-policy learning and enables principled off-policy data reuse by dynamically reweighting stale samples rather than discarding them. We extensively evaluate $R^2VPO$ on fine-tuning state-of-the-art LLMs, including DeepSeek-Distill-Qwen-1.5B and the openPangu-Embedded series (1B and 7B), across challenging mathematical reasoning benchmarks. Experimental results show that $R^2VPO$ consistently achieves superior asymptotic performance, with average relative gains of up to 17% over strong clipping-based baselines, while requiring approximately 50% fewer rollouts to reach convergence. These findings establish ratio-variance control as a promising direction for improving both stability and data efficiency in RL-based LLM alignment.

</details>


### [13] [Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting](https://arxiv.org/abs/2601.03321)
*Kun Zhao,Siyuan Dai,Pan Wang,Jifeng Song,Hui Ji,Chenghua Lin,Liang Zhan,Haoteng Tang*

Main category: cs.LG

TL;DR: 本文提出了一种新的"Reason-then-Summarize"架构，通过Group Relative Policy Optimization进行优化，旨在解决多模态大型语言模型在放射学报告生成中存在的问题，如事实性幻觉和视觉证据与语言输出不一致的问题。实验表明，该方法不仅提高了临床效率指标，在减少幻觉方面也优于强大的监督基线。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在放射学报告生成上展现出强大潜力，但其临床应用受到架构异质性和事实性幻觉普遍存在的限制。标准的监督微调往往不能很好地将语言输出与视觉证据对齐，而现有的强化学习方法要么计算成本过高，要么探索能力有限。

Method: 作者首先系统地评估了适用于医学影像的最佳视觉编码器和LLM骨干配置；接着提出了一个名为"Reason-then-Summarize"的新架构，并通过Group Relative Policy Optimization (GRPO) 进行优化。该框架将生成过程分为两个部分：一个是用于详细发现的思考模块，另一个是用于结构化疾病标签的回答模块。此外，还使用了一个多维度复合奖励函数来明确惩罚生成叙述与最终诊断之间的逻辑差异。

Result: 在MIMIC-CXR基准测试上的广泛实验表明，所提出的方法在临床有效性指标上达到了最先进的性能，并且与强监督基线相比显著减少了幻觉现象。

Conclusion: 这项工作为自洽的放射学报告生成提供了一个全面的框架，不仅解决了现有方法的一些关键局限性，还在提高报告准确度和减少错误信息方面取得了实质性进展。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel "Reason-then-Summarize" architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.

</details>


### [14] [Extreme-value forest fire prediction A study of the Loss Function in an Ordinality Scheme](https://arxiv.org/abs/2601.03327)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 本研究提出了一个针对法国运营决策的野火严重程度等级预测的序数分类框架，通过比较不同的损失函数设计，发现序数感知目标特别是加权Kappa损失（WKLoss）在极端严重性类别上表现最佳。未来工作将侧重于整合季节动态和不确定性信息以提高极端事件预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 鉴于野火在空间和严重程度上的高度不平衡特性，直接预测罕见但关键的高严重度火灾事件变得尤为困难。本研究旨在通过引入与法国运营决策相一致的序数分类框架来改进这一挑战。

Method: 研究了损失函数设计对神经模型预测罕见高严重度火灾事件能力的影响，对比了标准交叉熵与几种序数感知目标，包括从截断离散广义帕累托分布导出的概率TDeGPD损失，并通过多种架构及真实运营数据进行了广泛基准测试。

Result: 序数监督显著提高了模型性能，特别是在最极端严重性类别上，使用加权Kappa损失(WKLoss)达到了最优的整体结果，尽管对于极为罕见的事件，由于数据集中代表性的极低，表现仍然受限。

Conclusion: 研究强调了在野火预报系统中结合严重程度排序、数据不平衡考虑以及季节性风险的重要性。未来的工作方向将集中在通过纳入季节动态和不确定性信息来进一步提高极端事件预测的可靠性。

Abstract: Wildfires are highly imbalanced natural hazards in both space and severity, making the prediction of extreme events particularly challenging. In this work, we introduce the first ordinal classification framework for forecasting wildfire severity levels directly aligned with operational decision-making in France. Our study investigates the influence of loss-function design on the ability of neural models to predict rare yet critical high-severity fire occurrences. We compare standard cross-entropy with several ordinal-aware objectives, including the proposed probabilistic TDeGPD loss derived from a truncated discrete exponentiated Generalized Pareto Distribution. Through extensive benchmarking over multiple architectures and real operational data, we show that ordinal supervision substantially improves model performance over conventional approaches. In particular, the Weighted Kappa Loss (WKLoss) achieves the best overall results, with more than +0.1 IoU gain on the most extreme severity classes while maintaining competitive calibration quality. However, performance remains limited for the rarest events due to their extremely low representation in the dataset. These findings highlight the importance of integrating both severity ordering, data imbalance considerations, and seasonality risk into wildfire forecasting systems. Future work will focus on incorporating seasonal dynamics and uncertainty information into training to further improve the reliability of extreme-event prediction.

</details>


### [15] [Local Gradient Regulation Stabilizes Federated Learning under Client Heterogeneity](https://arxiv.org/abs/2601.03584)
*Ping Luo,Jiahuan Wang,Ziqing Wen,Tao Sun,Dongsheng Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于群智能启发的梯度重新聚合方法（ECGR），通过调节局部梯度动态来稳定异构联邦学习系统，并在理论分析和实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在实际部署中因统计异质性面临稳定性挑战，客户端异质性主要通过扭曲客户端优化过程中的局部梯度动力学，导致系统漂移并阻碍全局收敛。

Method: 受到群智能的启发，研究者们提出了探索-收敛梯度重聚合（ECGR）方法，旨在平衡对齐良好与错位的梯度成分，以保留有用更新同时抑制不稳定效应。该方法从客户端角度出发，无需增加额外通信开销即可调节局部梯度贡献。

Result: 通过理论分析及包括LC25000医学影像数据集在内的广泛实验表明，对于存在数据分布异质性的场景，调节局部梯度动态能够一致地稳定最先进的联邦学习方法。

Conclusion: 研究表明，通过ECGR方法调控局部梯度动力学可以有效地解决由客户端异质性引起的联邦学习稳定性问题，为处理现实世界中的非均匀数据分布提供了新的解决方案。

Abstract: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, yet its stability is fundamentally challenged by statistical heterogeneity in realistic deployments. Here, we show that client heterogeneity destabilizes FL primarily by distorting local gradient dynamics during client-side optimization, causing systematic drift that accumulates across communication rounds and impedes global convergence. This observation highlights local gradients as a key regulatory lever for stabilizing heterogeneous FL systems. Building on this insight, we develop a general client-side perspective that regulates local gradient contributions without incurring additional communication overhead. Inspired by swarm intelligence, we instantiate this perspective through Exploratory--Convergent Gradient Re-aggregation (ECGR), which balances well-aligned and misaligned gradient components to preserve informative updates while suppressing destabilizing effects. Theoretical analysis and extensive experiments, including evaluations on the LC25000 medical imaging dataset, demonstrate that regulating local gradient dynamics consistently stabilizes federated learning across state-of-the-art methods under heterogeneous data distributions.

</details>


### [16] [LUT-KAN: Segment-wise LUT Quantization for Fast KAN Inference](https://arxiv.org/abs/2601.03332)
*Oleksandr Kuznetsov*

Main category: cs.LG

TL;DR: This paper presents LUT-KAN, a method for quantizing and compiling Kolmogorov--Arnold Networks (KAN) using lookup tables, which reduces inference time on CPUs while preserving model accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high computational cost associated with spline evaluations in Kolmogorov--Arnold Networks (KAN) during inference on CPUs, as well as the difficulty in applying standard quantization techniques due to the nature of the computations involved.

Method: The paper introduces LUT-KAN, a segment-wise lookup-table (LUT) compilation and quantization method for PyKAN-style KAN layers. It converts each edge function into a per-segment LUT with affine int8/uint8 quantization and linear interpolation. The authors propose an 'honest baseline' methodology to evaluate the speed gains from LUT-KAN over B-spline evaluation under the same backend optimization conditions.

Result: Experiments show that LUT-KAN can reduce steady-state CPU inference latency by 12x under NumPy and 10x under Numba backends, while maintaining classification quality with an F1 drop below 0.0002. The memory overhead is about 10x at L=64, and the method provides a robust solution with controlled sweeps over LUT resolution and quantization schemes.

Conclusion: LUT-KAN is an effective method for compiling and quantizing KAN layers, which preserves classification quality while significantly reducing CPU inference latency. The trade-off is a higher memory overhead, but this is offset by the benefits in terms of speed and reproducibility.

Abstract: Kolmogorov--Arnold Networks (KAN) replace scalar weights by learnable univariate functions, often implemented with B-splines. This design can be accurate and interpretable, but it makes inference expensive on CPU because each layer requires many spline evaluations. Standard quantization toolchains are also hard to apply because the main computation is not a matrix multiply but repeated spline basis evaluation. This paper introduces LUT-KAN, a segment-wise lookup-table (LUT) compilation and quantization method for PyKAN-style KAN layers. LUT-KAN converts each edge function into a per-segment LUT with affine int8/uint8 quantization and linear interpolation. The method provides an explicit and reproducible inference contract, including boundary conventions and out-of-bounds (OOB) policies. We propose an ``honest baseline'' methodology for speed evaluation: B-spline evaluation and LUT evaluation are compared under the same backend optimization (NumPy vs NumPy and Numba vs Numba), which separates representation gains from vectorization and JIT effects. Experiments include controlled sweeps over LUT resolution L in 16, 32, 64, 128 and two quantization schemes (symmetric int8 and asymmetric uint8). We report accuracy, speed, and memory metrics with mean and standard deviation across multiple seeds. A two-by-two OOB robustness matrix evaluates behavior under different boundary modes and OOB policies. In a case study, we compile a trained KAN model for DoS attack detection (CICIDS2017 pipeline) into LUT artifacts. The compiled model preserves classification quality (F1 drop below 0.0002) while reducing steady-state CPU inference latency by 12x under NumPy and 10x under Numba backends (honest baseline). The memory overhead is approximately 10x at L=64. All code and artifacts are publicly available with fixed release tags for reproducibility.

</details>


### [17] [ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification](https://arxiv.org/abs/2601.03600)
*Xiao Lin,Philip Li,Zhichen Zeng,Tingwei Li,Tianxin Wei,Xuying Ning,Gaotang Li,Yuzhong Chen,Hanghang Tong*

Main category: cs.LG

TL;DR: 本文提出了一种基于放大的越狱检测器ALERT，旨在解决在零样本条件下大型语言模型对越狱攻击的脆弱性问题。通过逐步放大良性与越狱提示之间的内部特征差异，ALERT能够有效识别安全相关的层、模块以及标记，并在三个安全基准测试中展示了显著优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有丰富的安全对齐策略，但大型语言模型（LLMs）仍极易受到越狱攻击的影响，这些攻击绕过了安全保障措施并带来了严重的安全隐患。现有的检测方法主要依赖于训练数据中存在的越狱模板来识别越狱状态。然而，很少有研究关注更为现实且具有挑战性的零样本越狱检测场景，在这种情况下，训练过程中没有可用的越狱模板。这种设置更准确地反映了新攻击不断出现和演变的真实世界情况。

Method: 为了解决这一挑战，作者们提出了一种分层、模块化及令牌级别的放大框架，该框架能够逐渐增加良性与越狱提示之间内部特征差异。基于此框架，他们发现了与安全性相关的层，确定了固有编码零样本区分信号的具体模块，并定位到了信息量丰富的安全令牌。进一步地，基于这些发现提出了ALERT（Amplification-based Jailbreak Detector），这是一种高效的零样本越狱检测器，它在放大的表示上引入了两个独立而又互补的分类器。

Result: 广泛的实验表明，在三个安全基准测试中，无论对于哪种数据集或攻击策略，ALERT始终能保持非常强的零样本检测性能。具体来说：(i) 在所有数据集和攻击策略下，ALERT的表现都稳定位于前两名之内；(ii) 与第二优秀的基线相比，ALERT在平均准确率和F1分数上至少高出10%，有时甚至高达40%。

Conclusion: 本研究开发出了一种名为ALERT的有效零样本越狱检测解决方案，该方案不仅能够在没有任何先验知识的情况下成功识别潜在的越狱尝试，而且其性能显著超越当前最先进的方法。

Abstract: Despite rich safety alignment strategies, large language models (LLMs) remain highly susceptible to jailbreak attacks, which compromise safety guardrails and pose serious security risks. Existing detection methods mainly detect jailbreak status relying on jailbreak templates present in the training data. However, few studies address the more realistic and challenging zero-shot jailbreak detection setting, where no jailbreak templates are available during training. This setting better reflects real-world scenarios where new attacks continually emerge and evolve. To address this challenge, we propose a layer-wise, module-wise, and token-wise amplification framework that progressively magnifies internal feature discrepancies between benign and jailbreak prompts. We uncover safety-relevant layers, identify specific modules that inherently encode zero-shot discriminative signals, and localize informative safety tokens. Building upon these insights, we introduce ALERT (Amplification-based Jailbreak Detector), an efficient and effective zero-shot jailbreak detector that introduces two independent yet complementary classifiers on amplified representations. Extensive experiments on three safety benchmarks demonstrate that ALERT achieves consistently strong zero-shot detection performance. Specifically, (i) across all datasets and attack strategies, ALERT reliably ranks among the top two methods, and (ii) it outperforms the second-best baseline by at least 10% in average Accuracy and F1-score, and sometimes by up to 40%.

</details>


### [18] [Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs](https://arxiv.org/abs/2601.03793)
*Sethupathy Parameswaran,Suresh Sundaram,Yuan Fang*

Main category: cs.LG

TL;DR: 提出了一种新的零样本提示调优（ZPT）框架，通过利用通用双模态条件生成器来解决文本属性图中的零样本节点分类问题。


<details>
  <summary>Details</summary>
Motivation: 在文本属性图中进行零样本节点分类是一个重要挑战，特别是由于缺乏标记数据。

Method: 首先预训练一个图-语言模型以捕捉图结构和每个节点的关联文本描述；接着训练一个条件生成模型学习图和文本模态中节点的联合分布，从而基于类名生成合成样本；使用这些合成节点和文本嵌入执行连续提示调优。

Result: 在多个基准数据集上进行了广泛的实验，表明该框架优于现有的最先进基线，并通过消融研究验证了双模态生成器的贡献。

Conclusion: 本论文提出的ZPT框架为文本属性图上的零样本节点分类提供了一个有效的方法。

Abstract: Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: https://github.com/Sethup123/ZPT.

</details>


### [19] [Weather-Aware Transformer for Real-Time Route Optimization in Drone-as-a-Service Operations](https://arxiv.org/abs/2601.03376)
*Kamal Mohamed,Lillian Wassim,Ali Hamdi,Khaled Shaban*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，通过天气感知的深度学习模型来加速无人机服务中的路线预测。实验结果表明，与传统算法相比，该方法在保持路线优化性能的同时显著提高了计算速度，特别是在动态环境条件下，基于transformer的架构表现更优。


<details>
  <summary>Details</summary>
Motivation: 经典路径规划算法如A*和Dijkstra虽然能提供最优解，但其计算复杂度限制了它们在动态环境下的实时应用能力。为了解决这一局限性，并考虑到气象条件对无人机操作的影响，提出了一个结合天气启发式的机器学习解决方案。

Method: 通过对从经典算法模拟生成的合成数据集上训练机器学习及深度学习模型来实现。采用基于transformer和注意力机制的架构，利用天气启发式信息预测最佳下一个节点选择；注意力机制能够动态地给包括风模式、风向和温度在内的环境因素赋予权重，以改善不利天气条件下的路由决策。

Result: 实验结果显示，所提出的天气感知模型相比于传统算法，在保持路线优化性能的同时实现了显著的计算加速。特别是基于transformer的架构显示出对动态环境约束更强的适应性。

Conclusion: 所提出的框架支持大规模无人机服务操作中实现实时、响应天气变化的路线优化，代表了自主无人机系统效率与安全性方面的重大进步。

Abstract: This paper presents a novel framework to accelerate route prediction in Drone-as-a-Service operations through weather-aware deep learning models. While classical path-planning algorithms, such as A* and Dijkstra, provide optimal solutions, their computational complexity limits real-time applicability in dynamic environments. We address this limitation by training machine learning and deep learning models on synthetic datasets generated from classical algorithm simulations. Our approach incorporates transformer-based and attention-based architectures that utilize weather heuristics to predict optimal next-node selections while accounting for meteorological conditions affecting drone operations. The attention mechanisms dynamically weight environmental factors including wind patterns, wind bearing, and temperature to enhance routing decisions under adverse weather conditions. Experimental results demonstrate that our weather-aware models achieve significant computational speedup over traditional algorithms while maintaining route optimization performance, with transformer-based architectures showing superior adaptation to dynamic environmental constraints. The proposed framework enables real-time, weather-responsive route optimization for large-scale DaaS operations, representing a substantial advancement in the efficiency and safety of autonomous drone systems.

</details>


### [20] [SIGMA: Scalable Spectral Insights for LLM Collapse](https://arxiv.org/abs/2601.03385)
*Yi Gu,Lingyou Pang,Xiangkun Ye,Tianyu Wang,Jianyu Lin,Carey E. Priebe,Alexander Aue*

Main category: cs.LG

TL;DR: 本文提出了一种名为SIGMA的新框架，用于通过嵌入Gram矩阵的谱视角来衡量模型崩溃。SIGMA不仅为跟踪表示空间的收缩提供了数学基础度量，而且其随机公式化还使得在大规模基础模型中估计这些界限成为可能，从而有效地捕捉到退化状态的转变，并提供理论见解和实用工具来监控递归训练管道的健康状况。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据快速应用于大型语言模型（LLMs）的训练，出现了一个技术挑战——“模型崩溃”，即通过模型生成内容进行递归训练导致分布方差和表示质量收缩的过程。尽管这种现象越来越明显，但在高维空间中量化和预测其发生的严格方法仍然难以捉摸。

Method: 引入了SIGMA（Spectral Inequalities for Gram Matrix Analysis），这是一种统一框架，通过嵌入Gram矩阵的谱视角来基准测试模型崩溃。通过推导并利用矩阵谱的确定性和随机界限，SIGMA提供了一种数学上基于的度量标准来追踪表示空间的收缩。特别地，我们的随机公式化能够实现对这些界限的大规模估计，使得该框架适用于全特征分解不可行的大规模基础模型。

Result: 研究表明，SIGMA能够有效捕捉向退化状态过渡的过程，既提供了关于崩溃机制的理论洞察，又为监控递归训练流程的健康状况提供了一个实用且可扩展的工具。

Conclusion: SIGMA作为一种新的分析工具，对于理解和管理大型语言模型中的模型崩溃问题具有重要意义。它不仅有助于深入理解模型崩溃背后的机制，而且还提供了一种实用的方法来监测和预防实际应用中可能出现的问题。

Abstract: The rapid adoption of synthetic data for training Large Language Models (LLMs) has introduced the technical challenge of "model collapse"-a degenerative process where recursive training on model-generated content leads to a contraction of distributional variance and representational quality. While the phenomenology of collapse is increasingly evident, rigorous methods to quantify and predict its onset in high-dimensional spaces remain elusive. In this paper, we introduce SIGMA (Spectral Inequalities for Gram Matrix Analysis), a unified framework that benchmarks model collapse through the spectral lens of the embedding Gram matrix. By deriving and utilizing deterministic and stochastic bounds on the matrix's spectrum, SIGMA provides a mathematically grounded metric to track the contraction of the representation space. Crucially, our stochastic formulation enables scalable estimation of these bounds, making the framework applicable to large-scale foundation models where full eigendecomposition is intractable. We demonstrate that SIGMA effectively captures the transition towards degenerate states, offering both theoretical insights into the mechanics of collapse and a practical, scalable tool for monitoring the health of recursive training pipelines.

</details>


### [21] [Inferring Clinically Relevant Molecular Subtypes of Pancreatic Cancer from Routine Histopathology Using Deep Learning](https://arxiv.org/abs/2601.03410)
*Abdul Rehman Akbar,Alejandro Levya,Ashwini Esnakula,Elshad Hasanov,Anne Noonan,Upender Manne,Vaibhav Sahai,Lingbin Meng,Susan Tsai,Anil Parwani,Wei Chen,Ashish Manne,Muhammad Khalid Khan Niazi*

Main category: cs.LG

TL;DR: PanSubNet, a deep learning model, accurately predicts molecular subtypes of pancreatic ductal adenocarcinoma (PDAC) from H&E-stained whole-slide images, offering a cost-effective and rapid solution for clinical use. It demonstrates high performance in both internal and external validations, maintaining or even improving prognostic stratification compared to RNA-seq methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current molecular subtyping methods for PDAC, such as high cost, long turnaround time, and significant tissue requirements, by developing an interpretable deep learning framework that can predict therapy-relevant molecular subtypes directly from standard H&E-stained whole-slide images (WSIs).

Method: A deep learning framework named PanSubNet was developed using data from 1,055 patients across two multi-institutional cohorts. The model integrates dual-scale architecture, combining cellular-level morphology with tissue-level architecture, and utilizes attention mechanisms for multi-scale representation learning. It was trained and validated using paired histology and RNA-seq data, with ground-truth labels derived from a refined Moffitt 50-gene signature.

Result: PanSubNet achieved a mean AUC of 88.5% on internal validation within the PANCAN cohort through five-fold cross-validation, showing balanced sensitivity and specificity. On the independent TCGA cohort, it demonstrated robust generalizability with an AUC of 84.0%. The model also preserved and, in cases of metastatic disease, enhanced prognostic stratification when compared to RNA-seq based labels. Furthermore, the predictions were consistent with established transcriptomic programs, differentiation markers, and DNA damage repair signatures.

Conclusion: PanSubNet provides a clinically deployable and interpretable tool for genetic subtyping of PDAC, enabling rapid and cost-effective molecular stratification from routine H&E-stained slides. This approach supports integration into digital pathology workflows and advances precision oncology for PDAC, with ongoing efforts to validate and assess its real-world performance.

Abstract: Molecular subtyping of PDAC into basal-like and classical has established prognostic and predictive value. However, its use in clinical practice is limited by cost, turnaround time, and tissue requirements, thereby restricting its application in the management of PDAC. We introduce PanSubNet, an interpretable deep learning framework that predicts therapy-relevant molecular subtypes directly from standard H&E-stained WSIs. PanSubNet was developed using data from 1,055 patients across two multi-institutional cohorts (PANCAN, n=846; TCGA, n=209) with paired histology and RNA-seq data. Ground-truth labels were derived using the validated Moffitt 50-gene signature refined by GATA6 expression. The model employs dual-scale architecture that fuses cellular-level morphology with tissue-level architecture, leveraging attention mechanisms for multi-scale representation learning and transparent feature attribution. On internal validation within PANCAN using five-fold cross-validation, PanSubNet achieved mean AUC of 88.5% with balanced sensitivity and specificity. External validation on the independent TCGA cohort without fine-tuning demonstrated robust generalizability (AUC 84.0%). PanSubNet preserved and, in metastatic disease, strengthened prognostic stratification compared to RNA-seq based labels. Prediction uncertainty linked to intermediate transcriptional states, not classification noise. Model predictions are aligned with established transcriptomic programs, differentiation markers, and DNA damage repair signatures. By enabling rapid, cost-effective molecular stratification from routine H&E-stained slides, PanSubNet offers a clinically deployable and interpretable tool for genetic subtyping. We are gathering data from two institutions to validate and assess real-world performance, supporting integration into digital pathology workflows and advancing precision oncology for PDAC.

</details>


### [22] [Spectral Archaeology: The Causal Topology of Model Evolution](https://arxiv.org/abs/2601.03424)
*Valentin Noël*

Main category: cs.LG

TL;DR: 本研究介绍了一种无需训练的机制性探针，使用注意力图谱来揭示模型内部的工作机制。通过计算代数连通性、平滑度和频谱熵等指标，研究者发现了一些标准评估中未注意到的不连续性问题，并提出了被动触发连通性崩溃（PTCC）的概念，解释了特定课程转换过程中模型在处理非典型结构时出现的问题。此外，还识别出了四种重复处理策略，并指出可以通过激活引导部分恢复连通性。研究最后提出，主要拓扑结构更受标记化密度而非语言身份的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于现有的行为基准仅能告诉我们模型做了什么，但无法解释其工作原理。因此，引入一种基于注意力图谱的新方法来深入理解模型内部机制，特别是不同层之间的相互作用以及它们如何影响模型性能。

Method: 采用了一种无训练需求的机制性探针技术，将每个模型层视为一个令牌图，并对其计算代数连通性(λ2)、平滑度和频谱熵。通过对12个模型及10种语言进行分析，得出了稳定的“频谱指纹”。

Result: 研究发现了当模型经历特定类型的课程转换（如从代码到聊天）时，在处理非典型英语句法结构上会出现连通性显著下降的现象；确定了这种现象与课程设计有关，并且伴随着形式路由能力增强而风格灵活性减弱的权衡；识别出能够跨世系完美鉴别的四种重复处理策略；并指出可以通过激活导向部分恢复连通性，大约可以恢复38%的信息流损失。

Conclusion: 注意力图谱提供了一种实用工具，可用于审计和验证训练方案的有效性。此外，研究表明主导的拓扑状态更多地受到标记化密度而非具体语言种类的影响，这暗示着健康的几何形态可能系统性地跨越不同的书写系统而变化。

Abstract: Behavioral benchmarks tell us \textit{what} a model does, but not \textit{how}. We introduce a training-free mechanistic probe using attention-graph spectra. Treating each layer as a token graph, we compute algebraic connectivity ($λ_2$), smoothness, and spectral entropy. Across 12 models and 10 languages, these measures yield stable ``spectral fingerprints'' that expose discontinuities missed by standard evaluation.
  We report four results. (1) Models undergoing specific curriculum transitions (e.g., code-to-chat) show an English-only, syntax-triggered connectivity failure on non-canonical constructions, reaching $Δλ_2 \approx -0.76$. We term this scar \textit{Passive-Triggered Connectivity Collapse} (PTCC). Analysis of the Phi lineage reveals that PTCC appears and resolves across developmental stages, implicating brittle curriculum shifts rather than synthetic data per se. (2) PTCC reflects a specialization trade-off: strengthened formal routing at the expense of stylistic flexibility. (3) We identify four recurrent processing strategies; simple frozen-threshold rules enable perfect forensic identification across lineages. (4) Mechanistically, PTCC localizes to a sparse Layer 2 ``compensatory patch'' of heads that fails under syntactic stress; activation steering can partially restore connectivity, recovering $\approx 38\%$ of lost information flow.
  Finally, dominant topological regimes track tokenization density more than language identity, suggesting ``healthy'' geometry varies systematically across scripts. Overall, attention-graph spectra provide a practical tool for auditing and training-regime verification.

</details>


### [23] [The Illusion of Specialization: Unveiling the Domain-Invariant "Standing Committee" in Mixture-of-Experts Models](https://arxiv.org/abs/2601.03425)
*Yan Wang,Yitao Xu,Nanhan Shen,Jinyan Su,Jimin Huang,Zining Zhu*

Main category: cs.LG

TL;DR: 本研究通过引入COMMITTEEAUDIT框架，揭示了专家混合模型中存在一个领域不变的常务委员会现象，即少数专家处理大部分任务，而边缘专家则处理特定领域的知识。这表明模型内部的专门化程度远低于先前假设，并且当前的训练目标可能与模型自然优化路径相悖，影响了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 研究人员质疑专家混合模型是否真的通过稀疏路由实现了领域专业化。为了解这一问题，他们开发了一个名为COMMITTEEAUDIT的新框架来分析专家组而非单个专家层面的路由行为。

Method: 提出了COMMITTEEAUDIT框架，该框架能够以专家组而非个体专家的角度分析路由行为。研究选取了三个代表性模型以及MMLU基准测试进行实验。

Result: 研究发现，在不同领域、层次和路由预算下，存在一个领域不变的‘常务委员会’，由一组少量但固定的专家组成，它们占据了大部分的路由量。进一步定性分析显示，这些常务委员会主要负责推理结构和语法，而外围专家则处理特定领域的知识。

Conclusion: 研究表明，专家混合模型中存在强烈的集中计算倾向，这表明模型内的专业化程度远不如人们普遍认为的那样广泛。此外，当前旨在平衡负载等训练目标可能与模型自然优化方向相抵触，从而限制了训练效率和最终性能。

Abstract: Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.

</details>


### [24] [VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding](https://arxiv.org/abs/2601.03434)
*Zibo Liu,Muyang Li,Zhe Jiang,Shigang Chen*

Main category: cs.LG

TL;DR: 本文介绍了VNU-Bench，这是首个针对新闻领域多源跨视频理解的基准测试。它设计了一系列新的问题类型，以从不同角度测试模型对多源多媒体新闻的理解能力，并通过一种新颖的人机结合QA生成过程来解决大规模数据集构建中的可扩展性和质量控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有的新闻视频理解基准主要集中在单源、视频内部推理上，而现实世界的新闻消费本质上是多源的：同一事件由不同的媒体以互补的细节、独特的叙述选择，甚至有时是随时间发展的冲突声明进行报道。因此，强大的新闻理解需要模型能够比较不同来源的观点，跨来源对齐多媒体证据，并综合多源信息。

Method: 为了填补这一空白，研究人员引入了VNU-Bench，一个专为多源跨视频理解设计的新基准。该研究还设计了一种新式的问题类型，专门用于测试模型在多种不同角度下理解多源多媒体新闻的能力。此外，他们提出了一种创新性的混合人-机问答生成流程，旨在解决大规模跨源新闻理解数据集建设中遇到的规模性与质量控制难题。

Result: 创建的数据集包括429个新闻组、1,405个视频以及2,501个高质量问题。对闭源和开源的多模态模型进行了全面评估后发现，VNU-Bench对于当前的多模态大型语言模型来说是一个相当大的挑战。

Conclusion: VNU-Bench作为首个针对新闻领域内多源跨视频理解任务设计的基准测试，不仅提供了一个新的平台来评估现有模型处理复杂新闻内容的能力，而且也揭示了当前技术在面对实际应用场景时所面临的困难。

Abstract: News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.

</details>


### [25] [Soft Contextualized Encoder For User Defined Text Classification](https://arxiv.org/abs/2601.03450)
*Charu Maheshwari,Vyas Raina*

Main category: cs.LG

TL;DR: 本文提出了一种软上下文化编码器架构，用于用户自定义文本分类（UDTC），该架构能够将每个候选标签与标签集和输入查询的静态软提示表示进行上下文化。通过在多样化的多源数据集上训练，模型能够有效地泛化到对完全未见过的主题集进行零样本分类。


<details>
  <summary>Details</summary>
Motivation: 用户自定义文本分类（UDTC）面临将输入文本分类至用户指定且先前未曾见过类别的挑战，这种情况在企业分析、内容审核及领域特定信息检索等现实世界应用中频繁出现。

Method: 提出了一种针对UDTC的软上下文化编码器架构，该架构能够根据标签集合以及输入查询的静态软提示表示来为每个候选标签创建上下文环境。

Result: 通过对不同来源的数据集进行训练，使得模型能够在完全未见过的主题集上实现有效的零样本分类。此外，在多个未见过的UDTC基准测试中，该模型表现出了最先进水平的表现，始终优于或与基线相匹配。

Conclusion: 所提出的软上下文化编码器架构对于解决用户自定义文本分类问题非常有效，特别是在处理之前未见过的类别时，能够展现出卓越的性能。

Abstract: User-Defined Text Classification (UDTC) considers the challenge of classifying input text to user-specified, previously unseen classes, a setting that arises frequently in real-world applications such as enterprise analytics, content moderation, and domain-specific information retrieval. We propose a soft-contextualized encoder architecture for UDTC which contextualizes each candidate label with the label set and a static soft prompt representation of the input query. Training on diverse, multi-source datasets enables the model to generalize effectively to zero-shot classification over entirely unseen topic sets drawn from arbitrary domains. We evaluate the proposed architecture both on held-out in-distribution test data and on multiple unseen UDTC benchmarks. Across datasets, the model achieves state-of-the-art performance, consistently outperforming or matching the baselines.

</details>


### [26] [Hybrid Approach for Driver Behavior Analysis with Machine Learning, Feature Optimization, and Explainable AI](https://arxiv.org/abs/2601.03477)
*Mehedi Hasan Shuvo,Md. Raihan Tapader,Nur Mohammad Tamjid,Sajjadul Islam,Ahnaf Atef Choudhury,Jia Uddin*

Main category: cs.LG

TL;DR: 本文提出了一种混合方法来分析驾驶员行为，使用了来自Kaggle的数据集，并通过预处理技术如标签编码、随机过采样和标准缩放后测试了13种机器学习算法。随机森林分类器达到了95%的准确率。通过应用LIME技术识别出对准确性影响最大的前十个特征，并重新训练模型后，随机森林分类器的准确率略有下降至94.2%，表明可以在不牺牲性能的情况下提高模型效率。


<details>
  <summary>Details</summary>
Motivation: 鉴于之前的研究在利用机器学习和深度学习技术分析驾驶行为时存在特征优化不足的问题，导致高性能与可解释性之间的妥协。本研究旨在填补这一空白，提出一种新的方法以同时提高预测能力和模型解释力。

Method: 采用了一个包含12,857行和18列的数据集进行研究；首先对数据进行了预处理（包括标签编码、随机过采样和标准化）；接着测试了13种不同的机器学习算法；最后，通过XAI中的LIME技术确定了对准确性贡献最大的前十个正向及负向特征，并基于此重新训练模型。

Result: 随机森林分类器在初次测试中获得了95%的准确率；使用LIME技术识别关键特征并重训模型后，该分类器的准确率略微降至94.2%。

Conclusion: 提出的混合模型不仅能够保持较高的预测精度，同时也增强了模型对于驾驶行为过程的解释能力，从而为投资于此类解决方案提供了有力支持。

Abstract: Progressive driver behavior analytics is crucial for improving road safety and mitigating the issues caused by aggressive or inattentive driving. Previous studies have employed machine learning and deep learning techniques, which often result in low feature optimization, thereby compromising both high performance and interpretability. To fill these voids, this paper proposes a hybrid approach to driver behavior analysis that uses a 12,857-row and 18-column data set taken from Kaggle. After applying preprocessing techniques such as label encoding, random oversampling, and standard scaling, 13 machine learning algorithms were tested. The Random Forest Classifier achieved an accuracy of 95%. After deploying the LIME technique in XAI, the top 10 features with the most significant positive and negative influence on accuracy were identified, and the same algorithms were retrained. The accuracy of the Random Forest Classifier decreased slightly to 94.2%, confirming that the efficiency of the model can be improved without sacrificing performance. This hybrid model can provide a return on investment in terms of the predictive power and explainability of the driver behavior process.

</details>


### [27] [From Bits to Chips: An LLM-based Hardware-Aware Quantization Agent for Streamlined Deployment of LLMs](https://arxiv.org/abs/2601.03484)
*Kaiyuan Deng,Hangyu Zheng,Minghai Qing,Kunxiong Zhu,Gen Li,Yang Xiao,Lan Emily Zhang,Linke Guo,Bo Hui,Yanzhi Wang,Geng Yuan,Gagan Agrawal,Wei Niu,Xiaolong Ma*

Main category: cs.LG

TL;DR: 本文介绍了一种名为HAQA的自动化框架，该框架利用大型语言模型来简化量化和部署过程，通过有效调整超参数和硬件配置，同时提高部署质量和易用性。实验结果表明，与未经优化的Llama模型相比，使用HAQA可以实现高达2.3倍的推理速度提升、更高的吞吐量以及更佳的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多非专业用户开始对部署大型语言模型感兴趣，但受限于特定硬件资源限制，如何在保持高精度的同时满足硬件要求成为一大难题。尽管模型量化技术有助于缓解内存和计算瓶颈，但调优及部署量化模型所带来的额外复杂性反而加剧了这些挑战，使得整个过程对大多数用户来说不够友好。

Method: 提出了一种名为Hardware-Aware Quantization Agent (HAQA)的自动化框架，该框架利用大型语言模型（LLMs）来简化从量化到部署的全过程。HAQA能够有效地进行超参数调整，并针对不同硬件平台自动寻找最优设置，从而减少大量手动工作并展示出更好的适应性。

Result: 实验结果显示，在Llama上应用HAQA后，与未经优化的模型相比，实现了最高达2.3倍的推理加速，同时还提高了吞吐量和准确性。此外，HAQA能够在多种硬件平台上实施自适应量化策略，即使是最优设置看似违反直觉时也能自动找到，极大地减少了所需的手动工作量。

Conclusion: HAQA提供了一个自动化解决方案，旨在解决大型语言模型部署过程中遇到的性能与易用性之间的矛盾。它不仅能够显著加快推理速度、增加吞吐量和提高准确性，而且还能减轻用户在调整超参数和选择合适硬件配置方面的负担。

Abstract: Deploying models, especially large language models (LLMs), is becoming increasingly attractive to a broader user base, including those without specialized expertise. However, due to the resource constraints of certain hardware, maintaining high accuracy with larger model while meeting the hardware requirements remains a significant challenge. Model quantization technique helps mitigate memory and compute bottlenecks, yet the added complexities of tuning and deploying quantized models further exacerbates these challenges, making the process unfriendly to most of the users. We introduce the Hardware-Aware Quantization Agent (HAQA), an automated framework that leverages LLMs to streamline the entire quantization and deployment process by enabling efficient hyperparameter tuning and hardware configuration, thereby simultaneously improving deployment quality and ease of use for a broad range of users. Our results demonstrate up to a 2.3x speedup in inference, along with increased throughput and improved accuracy compared to unoptimized models on Llama. Additionally, HAQA is designed to implement adaptive quantization strategies across diverse hardware platforms, as it automatically finds optimal settings even when they appear counterintuitive, thereby reducing extensive manual effort and demonstrating superior adaptability. Code will be released.

</details>


### [28] [VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation](https://arxiv.org/abs/2601.03525)
*Longwen Wang,Xuan'er Wu,Xiaohui Hu,Yirui Liu,Yuankai Fan,Kaidong Yu,Qizhen Weng,Wei Xi,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架VeRPO，用于代码生成，该框架通过可验证的执行反馈合成鲁棒且密集的奖励。实验表明，VeRPO在多种基准测试中均优于基于结果和基于奖励模型的方法，提高了性能同时几乎没有时间成本和GPU内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励设计方法在强化学习（RL）中的代码生成领域存在局限性，特别是主流的通过/失败结果奖励虽然保证了功能正确性，但其稀疏性限制了性能提升；而最近研究的外部奖励模型虽尝试提供更丰富连续的奖励，却面临奖励不一致及计算成本过高的问题。

Method: 提出了VeRPO（Verifiable Dense Reward Policy Optimization），一种新型RL框架，它根据训练期间的执行统计数据动态估计每个单元测试难度权重，并基于通过的单元测试权重之和构建密集奖励。此外，VeRPO还结合全局执行结果以增强部分成功与端到端功能性正确性之间的一致性，从而仅依赖于可验证的执行反馈建立了一个鲁棒且密集的奖励范式。

Result: 广泛的实验显示，在不同基准和设置下，VeRPO相较于基于结果驱动和基于奖励模型的基线方法展现出一致性的优越表现，特别是在pass@1指标上实现了高达8.83%的增长，同时几乎没有任何额外的时间成本(<0.02%)以及零GPU内存占用。

Conclusion: VeRPO作为一种创新的RL框架，为代码生成任务提供了有效的解决方案，通过引入基于可验证执行反馈的密集奖励机制，不仅改善了模型训练效率，而且在保持或提高代码质量的同时大幅减少了计算资源需求。

Abstract: Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \textbf{VeRPO} (\textbf{V}erifiable D\textbf{e}nse \textbf{R}eward \textbf{P}olicy \textbf{O}ptimization), a novel RL framework for code generation that synthesizes \textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\% gain in pass@1 with negligible time cost (< 0.02\%) and zero GPU memory overhead.

</details>


### [29] [Green's-Function Spherical Neural Operators for Biological Heterogeneity](https://arxiv.org/abs/2601.03561)
*Hao Tang,Hao Chen,Hao Li,Chao Li*

Main category: cs.LG

TL;DR: 本文提出了一种可设计的格林函数框架（DGF）和基于此框架的格林函数球面神经算子（GSNO），该算子结合了等变解、不变解和各向异性解三种解决方案，以适应具有干扰变化性和各向异性的现实世界异构系统，同时保持谱效率。在多个数据集上的评估表明了GSNO的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的球面深度学习方法在保持强烈的球面几何归纳偏置与建模现实世界异质性之间面临挑战。为了解决这个问题，同时保留球面几何特性，研究提出了新的方法。

Method: 首先引入了一个可设计的格林函数框架（DGF），提供了新的球面算子解策略：在旋转群下设计系统的格林函数。基于DGF，为了模拟生物异质性，提出了格林函数球面神经算子（GSNO），融合了三种算子解：(1) 从等变格林函数得出的等变解用于一致性建模；(2) 从不变格林函数得出的不变解用于消除不必要的异质性，例如一致背景场；(3) 从各向异性格林函数得出的各向异性解用于建模有优先方向的系统，特别是纤维。

Result: 结果表明，所得到的模型GSNO能够适应具有干扰变化性和各向异性的现实世界异构系统，同时保持谱效率。通过在球形MNIST、浅水方程、扩散MRI纤维预测、皮层划分以及分子结构建模上的评估展示了GSNO的优越性。

Conclusion: 通过引入DGF和GSNO，本研究成功地解决了在维持球面几何特性的同时处理现实世界异质性的难题，为解决相关问题提供了一种有效的新途径。

Abstract: Spherical deep learning has been widely applied to a broad range of real-world problems. Existing approaches often face challenges in balancing strong spherical geometric inductive biases with the need to model real-world heterogeneity. To solve this while retaining spherical geometry, we first introduce a designable Green's function framework (DGF) to provide new spherical operator solution strategy: Design systematic Green's functions under rotational group. Based on DGF, to model biological heterogeneity, we propose Green's-Function Spherical Neural Operator (GSNO) fusing 3 operator solutions: (1) Equivariant Solution derived from Equivariant Green's Function for symmetry-consistent modeling; (2) Invariant Solution derived from Invariant Green's Function to eliminate nuisance heterogeneity, e.g., consistent background field; (3) Anisotropic Solution derived from Anisotropic Green's Function to model anisotropic systems, especially fibers with preferred direction. Therefore, the resulting model, GSNO can adapt to real-world heterogeneous systems with nuisance variability and anisotropy while retaining spectral efficiency. Evaluations on spherical MNIST, Shallow Water Equation, diffusion MRI fiber prediction, cortical parcellation and molecule structure modeling demonstrate the superiority of GSNO.

</details>


### [30] [A Proposed Paradigm for Imputing Missing Multi-Sensor Data in the Healthcare Domain](https://arxiv.org/abs/2601.03565)
*Vaibhav Gupta,Florian Grensing,Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 本文探讨了通过可穿戴传感器持续健康监测以早期预测血糖事件的挑战，特别是信号噪声和数据缺失问题。研究分析了现有数据集的局限性，并提出了一种系统性的方法论，即根据不同特征特性和缺失间隔时长定制插补策略，以有效处理时间序列数据中的长时间缺失。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病如糖尿病的管理面临重大挑战，尤其是低血糖等并发症的风险需要及时发现与干预。虽然利用可穿戴传感器进行持续健康监测为早期预测血糖事件提供了可能，但多传感器数据的有效使用受到诸如信号噪声及频繁的数据缺失等问题的阻碍。

Method: 本研究首先考察了现有数据集中存在的限制，并强调了对于低血糖预测至关重要的关键特征的时间特性。接着，对包括机器学习和深度学习在内的最新研究所采用的各种插补技术进行了全面分析。基于此，提出了一个根据特定特征性质及其缺失时段长度来调整插补策略的系统框架。

Result: 研究表明，针对不同类型特征以及不同长度的数据缺失区间采取专门设计的插补方法能够更有效地应对数据中固有的异质性时间模式。

Conclusion: 文章最后指出，研究个体特征的时间动态变化并实施多种特征特定插补技术是解决这一问题的关键。

Abstract: Chronic diseases such as diabetes pose significant management challenges, particularly due to the risk of complications like hypoglycemia, which require timely detection and intervention. Continuous health monitoring through wearable sensors offers a promising solution for early prediction of glycemic events. However, effective use of multisensor data is hindered by issues such as signal noise and frequent missing values. This study examines the limitations of existing datasets and emphasizes the temporal characteristics of key features relevant to hypoglycemia prediction. A comprehensive analysis of imputation techniques is conducted, focusing on those employed in state-of-the-art studies. Furthermore, imputation methods derived from machine learning and deep learning applications in other healthcare contexts are evaluated for their potential to address longer gaps in time-series data. Based on this analysis, a systematic paradigm is proposed, wherein imputation strategies are tailored to the nature of specific features and the duration of missing intervals. The review concludes by emphasizing the importance of investigating the temporal dynamics of individual features and the implementation of multiple, feature-specific imputation techniques to effectively address heterogeneous temporal patterns inherent in the data.

</details>


### [31] [Local Intrinsic Dimensionality of Ground Motion Data for Early Detection of Complex Catastrophic Slope Failure](https://arxiv.org/abs/2601.03569)
*Yuansan Liu,Antoinette Tordesillas,James Bailey*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法stLID，通过结合空间和时间信息来改进现有的基于局部固有维度(LID)的技术，以更准确地检测滑坡灾害。该方法包括三个主要改进：运动学增强、空间融合以及时间建模，并在实验中展示了比现有方法更好的故障检测精度和提前预警时间。


<details>
  <summary>Details</summary>
Motivation: 现有的基于表面位移数据分析的方法通常无法同时捕捉到数据中的空间相关性和时间动态性，这对早期且准确地识别潜在滑坡区域至关重要。为了弥补这一不足，研究者们专注于地面监测的滑坡现象，并开发了一个能够联合利用时空信息的新方法。

Method: 本研究基于已有的sLID技术进行了扩展，提出了三项关键改进：1）通过将速度纳入sLID计算中加强了对短期时间依赖性和变形率关系的捕捉；2）运用贝叶斯估计法整合邻近地区的sLID值，从而将空间关联性嵌入到了LID评分里；3）引入了tLID这一时间变体，从时间序列数据中学习长期动态变化规律，为位移行为提供了稳健的时间表示。最终，所有这些元素被集成进一个统一框架——时空LID(stLID)，用于识别在任一或两方面都异常的样本。

Result: 广泛的实验证明，相较于现有方法，stLID在故障检测精度及提前量方面均表现出色。

Conclusion: 通过结合空间与时间信息并加以改进后的stLID方法，在复杂滑坡事件尤其是发生在同一斜坡不同区域的连续性失效问题上展现了显著优势，为有效减轻地质灾害风险提供了一种强有力的工具。

Abstract: Local Intrinsic Dimensionality (LID) has shown strong potential for identifying anomalies and outliers in high-dimensional data across a wide range of real-world applications, including landslide failure detection in granular media. Early and accurate identification of failure zones in landslide-prone areas is crucial for effective geohazard mitigation. While existing approaches typically rely on surface displacement data analyzed through statistical or machine learning techniques, they often fall short in capturing both the spatial correlations and temporal dynamics that are inherent in such data. To address this gap, we focus on ground-monitored landslides and introduce a novel approach that jointly incorporates spatial and temporal information, enabling the detection of complex landslides and including multiple successive failures occurring in distinct areas of the same slope. To be specific, our method builds upon an existing LID-based technique, known as sLID. We extend its capabilities in three key ways. (1) Kinematic enhancement: we incorporate velocity into the sLID computation to better capture short-term temporal dependencies and deformation rate relationships. (2) Spatial fusion: we apply Bayesian estimation to aggregate sLID values across spatial neighborhoods, effectively embedding spatial correlations into the LID scores. (3) Temporal modeling: we introduce a temporal variant, tLID, that learns long-term dynamics from time series data, providing a robust temporal representation of displacement behavior. Finally, we integrate both components into a unified framework, referred to as spatiotemporal LID (stLID), to identify samples that are anomalous in either or both dimensions. Extensive experiments show that stLID consistently outperforms existing methods in failure detection precision and lead-time.

</details>


### [32] [Variational Inference, Entropy, and Orthogonality: A Unified Theory of Mixture-of-Experts](https://arxiv.org/abs/2601.03577)
*Ye Su,Yong Liu*

Main category: cs.LG

TL;DR: 本文首次构建了一个统一的理论框架，从贝叶斯和信息论的角度解释了Mixture-of-Experts (MoE) 模型中的Top-k路由及辅助负载均衡机制，并证明了在专家表示具有高互相关性时存在一个'一致性障碍'。通过施加几何正交性可以缩小NP难全局最优解与多项式时间贪婪近似之间的差距，确认了几何正交性正则化是大规模模型最优工程松弛的有效方法。


<details>
  <summary>Details</summary>
Motivation: 尽管Mixture-of-Experts (MoE) 模型允许大型语言模型高效扩展，但其核心机制如Top-k路由和辅助负载均衡仍缺乏坚实的理论基础。本文旨在为这些实践提供一个统一且严谨的理论框架，以支持它们的设计并指导未来的研究。

Method: 从贝叶斯视角出发，将Top-k路由和负载均衡视为最佳稀疏后验估计和先验正则化；同时，从信息论角度将其视作减少路由模糊性和最大化信道容量的手段。此外，研究还指出了路由问题本质上是一个NP难的稀疏子集选择问题，并严格证明了当专家表示之间高度相干时，贪婪策略无法找到最优解。

Result: 研究发现了“一致性障碍”，即当专家间特征呈现高度相关时，传统贪婪算法无法达到最优解。进一步地，通过在专家特征空间中强制执行几何正交性，可以在一定程度上缓解这个问题，使得实际应用中的近似解更加接近于理论上的最优解。

Conclusion: 本研究不仅提供了关于MoE模型设计的重要理论依据和技术保障，同时也揭示了几何正交性正则化对于提升此类模型性能的关键作用，为理解和开发新的MoE架构奠定了坚实的基础。

Abstract: Mixture-of-Experts models enable large language models to scale efficiently, as they only activate a subset of experts for each input. Their core mechanisms, Top-k routing and auxiliary load balancing, remain heuristic, however, lacking a cohesive theoretical underpinning to support them. To this end, we build the first unified theoretical framework that rigorously derives these practices as optimal sparse posterior approximation and prior regularization from a Bayesian perspective, while simultaneously framing them as mechanisms to minimize routing ambiguity and maximize channel capacity from an information-theoretic perspective. We also pinpoint the inherent combinatorial hardness of routing, defining it as the NP-hard sparse subset selection problem. We rigorously prove the existence of a "Coherence Barrier"; when expert representations exhibit high mutual coherence, greedy routing strategies theoretically fail to recover the optimal expert subset. Importantly, we formally verify that imposing geometric orthogonality in the expert feature space is sufficient to narrow the divide between the NP-hard global optimum and polynomial-time greedy approximation. Our comparative analyses confirm orthogonality regularization as the optimal engineering relaxation for large-scale models. Our work offers essential theoretical support and technical assurance for a deeper understanding and novel designs of MoE.

</details>


### [33] [A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data](https://arxiv.org/abs/2601.03603)
*Kaidong Feng,Zhu Sun,Roy Ka-Wei Lee,Xun Jiang,Yin-Leng Theng,Yi Ding*

Main category: cs.LG

TL;DR: 本文通过使用College Experience Sensing (CES)数据集，对传统机器学习(ML)、深度学习(DL)和大型语言模型(LLM)在心理健康预测中的表现进行了全面的基准测试研究。结果表明，DL模型，特别是Transformer，在总体性能上表现最佳，而LLM虽然在上下文推理方面表现出色但在时间建模上较弱。个性化策略能够显著改善严重心理状态的预测。


<details>
  <summary>Details</summary>
Motivation: 智能手机感知提供了一种非侵入且可扩展的方式来追踪与心理健康相关的日常行为变化，如睡眠、移动性和手机使用的变化，这些往往先于压力、焦虑或抑郁的症状出现。尽管大多数先前的研究集中在响应现有状况的检测上，但心理健康预测使我们能够通过即时适应性干预来提供主动支持。

Method: 本研究利用了迄今为止最大规模的大学生心理健康纵向数据集——College Experience Sensing (CES)数据集，比较了传统机器学习(ML)、深度学习(DL)以及大型语言模型(LLM)方法在心理健康预测上的效果。评价涵盖了不同时间窗口、特征粒度、个性化策略以及类别不平衡处理方法。

Result: 结果显示，深度学习模型，尤其是Transformer架构（宏F1=0.58），在整体表现上最优；而大型语言模型则在情境推理方面显示出优势，但在时间序列建模上相对较弱。此外，采用个性化策略对于严重心理状态的预测有显著改进作用。

Conclusion: 通过对不同建模方法如何随时间解释手机感应行为数据的揭示，这项工作为下一代适应性强、以人为本的心理健康技术的发展奠定了基础，从而推动了研究和实际福祉的进步。

Abstract: Smartphone sensing offers an unobtrusive and scalable way to track daily behaviors linked to mental health, capturing changes in sleep, mobility, and phone use that often precede symptoms of stress, anxiety, or depression. While most prior studies focus on detection that responds to existing conditions, forecasting mental health enables proactive support through Just-in-Time Adaptive Interventions. In this paper, we present the first comprehensive benchmarking study comparing traditional machine learning (ML), deep learning (DL), and large language model (LLM) approaches for mental health forecasting using the College Experience Sensing (CES) dataset, the most extensive longitudinal dataset of college student mental health to date. We systematically evaluate models across temporal windows, feature granularities, personalization strategies, and class imbalance handling. Our results show that DL models, particularly Transformer (Macro-F1 = 0.58), achieve the best overall performance, while LLMs show strength in contextual reasoning but weaker temporal modeling. Personalization substantially improves forecasts of severe mental health states. By revealing how different modeling approaches interpret phone sensing behavioral data over time, this work lays the groundwork for next-generation, adaptive, and human-centered mental health technologies that can advance both research and real-world well-being.

</details>


### [34] [Policy-Guided Search on Tree-of-Thoughts for Efficient Problem Solving with Bounded Language Model Queries](https://arxiv.org/abs/2601.03606)
*Sumedh Pendurkar,Guni Sharon*

Main category: cs.LG

TL;DR: 该研究通过将语言模型的概率作为启发式来指导搜索，减少了思维评估的数量，并采用Levin树搜索（LTS）算法在有限的计算预算下有效提高了语言模型解决任务的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决在有限计算预算的情况下提高语言模型解决问题任务性能的问题，尤其是考虑到与语言模型推理相关的显著计算成本。

Method: 利用语言模型分配给思维的概率作为启发式，在Tree-of-Thoughts (ToT)框架内引导搜索，减少思维评估数量；并调整了Levin树搜索(LTS)算法以适应ToT框架，利用语言模型作为策略来有效地指导树探索。

Result: 实验结果表明，在固定的语言模型查询预算下，LTS在三个领域（Blocksworld, PrOntoQA, Array Sorting）和四个不同的语言模型中，相比基线搜索算法能够实现相当或更高的准确率。此外，还分析了LTS对于语言模型最终softmax层常用的温度值敏感性的理论结果。

Conclusion: LTS在ToT上的应用被证明是有效的，特别是在实现成本效益和时间效率问题解决方面，使其非常适合于延迟关键型和资源受限的应用。

Abstract: Recent studies explored integrating state-space search algorithms with Language Models (LM) to perform look-ahead on the token generation process, the ''Tree-of-Thoughts'' (ToT), generated by LMs, thereby improving performance on problem-solving tasks. However, the affiliated search algorithms often overlook the significant computational costs associated with LM inference, particularly in scenarios with constrained computational budgets. Consequently, we address the problem of improving LM performance on problem-solving tasks under limited computational budgets. We demonstrate how the probabilities assigned to thoughts by LMs can serve as a heuristic to guide search within the ToT framework, thereby reducing the number of thought evaluations. Building on this insight, we adapt a heuristic search algorithm, Levin Tree Search (LTS), to the ToT framework, which leverages LMs as policies to guide the tree exploration efficiently. We extend the theoretical results of LTS by showing that, for ToT (a pruned tree), LTS guarantees a bound on the number of states expanded, and consequently, on the number of thoughts generated. Additionally, we analyze the sensitivity of this bound to the temperature values commonly used in the final softmax layer of the LM. Empirical evaluation under a fixed LM query budget demonstrates that LTS consistently achieves comparable or higher accuracy than baseline search algorithms within the ToT framework, across three domains (Blocksworld, PrOntoQA, Array Sorting) and four distinct LMs. These findings highlight the efficacy of LTS on ToT, particularly in enabling cost-effective and time-efficient problem-solving, making it well-suited for latency-critical and resource-constrained applications.

</details>


### [35] [Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias](https://arxiv.org/abs/2601.03612)
*Joonwon Seo*

Main category: cs.LG

TL;DR: 本文介绍了一种通过结构归纳偏置解决复调音乐生成中"缺失的中间部分"问题的新方法，使用贝多芬钢琴奏鸣曲作为案例研究，提出了智能嵌入架构，减少了参数数量，并通过信息论、Rademacher复杂度和范畴论提供了严格的数学证明。实证结果表明验证损失减少，并通过SVD分析和专家听觉研究得到了确认。


<details>
  <summary>Details</summary>
Motivation: 为了解决复调音乐生成中的“缺失的中间部分”问题，作者引入了一种新的方法，即通过结构归纳偏置来改善模型表现。选择贝多芬的钢琴奏鸣曲作为研究对象，旨在探索音高与手部属性之间的独立性，并提出更有效的架构以减少参数量同时保持甚至提高模型性能。

Method: 采用归一化互信息（NMI）量化音高与手部属性间的独立性；设计并实现名为Smart Embedding的新架构，其能够显著降低模型参数量；运用信息论、Rademacher复杂度及范畴论等数学工具对所提方案进行理论验证。

Result: 实验结果表明，新提出的Smart Embedding架构使得参数量减少了48.30%，并且在信息论框架下仅带来可忽略的信息损失(上限为0.153比特)；从Rademacher复杂度角度来看，该方法相比传统方法拥有28.09%更紧致的一般化界限；此外，通过SVD分析及一项包含53位参与者的专家听觉测试进一步证实了模型的有效性，验证损失降低了9.47%。

Conclusion: 这项工作不仅提供了一个结合理论与实践的研究框架，用于改进基于深度学习的AI音乐生成技术，而且通过一系列严格的数学论证展示了如何利用结构归纳偏置来增强模型稳定性和泛化能力，从而为这一领域带来了经过验证的新见解。

Abstract: This monograph introduces a novel approach to polyphonic music generation by addressing the "Missing Middle" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.

</details>


### [36] [ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning](https://arxiv.org/abs/2601.03646)
*Zhengyi Kwan,Zhang Wei,Aik Beng Ng,Zhengkui Wang,Simon See*

Main category: cs.LG

TL;DR: 提出了一种基于结构化表示学习和聚合的强化学习调度器ReLA，它能够有效缩短制造系统中的作业调度时间，并在各种规模的问题实例上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的作业调度解决方案在处理大规模问题时存在运行时间长或调度质量不足的问题。

Method: 设计了一个名为ReLA的强化学习调度器，通过自注意力机制、卷积以及跨注意力机制来学习来自作业操作和机器等调度实体的多样化表示，并采用多尺度架构对这些表示进行聚合以支持决策制定过程。

Result: 实验显示，在小、中、大型作业实例上，ReLA在大多数测试设置下的完成时间最短。对于非大型实例，相比最新的基线方法，ReLA将最优性差距降低了13.0%；而对于大规模实例，则将该差距减少了78.6%，平均最优性差距分别降低到了7.3%和2.1%。

Conclusion: ReLA所学得的表示与聚合为强化学习调度提供了强大的决策支持，有助于实现快速作业完成及现实世界应用中的决策制定。

Abstract: Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA's learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.

</details>


### [37] [In Search of Grandmother Cells: Tracing Interpretable Neurons in Tabular Representations](https://arxiv.org/abs/2601.03657)
*Ricardo Knauer,Erik Rodner*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的方法来量化神经元对于单个概念的显著性和选择性，并通过分析发现，在基础模型中确实存在一些对高级概念显示出适度且统计上显著的显著性和选择性的神经元。


<details>
  <summary>Details</summary>
Motivation: 研究者们希望了解基础模型中的决策过程是否可以被解释，特别是是否存在类似‘祖母细胞’的神经元，即那些仅对单一概念有反应并且天然可解释的神经元。

Method: 作者提出了两种新的信息论度量方法，用于评估神经元针对特定概念的显著性和选择性。然后，他们将这些度量应用于TabPFN（一种表格型基础模型）的表示上，并通过简单的搜索来识别最显著和最具选择性的神经元-概念配对。

Result: 研究表明，在像TabPFN这样的模型中，某些神经元确实表现出对高阶概念的中等程度且具有统计学意义的显著性和选择性。这意味着，无需依赖更复杂的解释技术，有时也能识别出可解释的神经元。

Conclusion: 该研究为理解基础模型内部如何自然地出现可解释的神经元提供了初步证据，表明即使不采用复杂的技术手段，也有可能找到这些易于理解的神经元。

Abstract: Foundation models are powerful yet often opaque in their decision-making. A topic of continued interest in both neuroscience and artificial intelligence is whether some neurons behave like grandmother cells, i.e., neurons that are inherently interpretable because they exclusively respond to single concepts. In this work, we propose two information-theoretic measures that quantify the neuronal saliency and selectivity for single concepts. We apply these metrics to the representations of TabPFN, a tabular foundation model, and perform a simple search across neuron-concept pairs to find the most salient and selective pair. Our analysis provides the first evidence that some neurons in such models show moderate, statistically significant saliency and selectivity for high-level concepts. These findings suggest that interpretable neurons can emerge naturally and that they can, in some cases, be identified without resorting to more complex interpretability techniques.

</details>


### [38] [AMIR-GRPO: Inducing Implicit Preference Signals into GRPO](https://arxiv.org/abs/2601.03661)
*Amir Hossein Yari,Fajri Koto*

Main category: cs.LG

TL;DR: 本文提出了一种名为AMIR-GRPO的新方法，通过直接从组内奖励排名构建隐式的对比正则化器来增强GRPO，无需额外注释。该机制加强了对低奖励轨迹的抑制，减弱了响应级别的长度偏差，并将每次回放组转化为更密集的监督约束集。在多个数学推理基准测试中，AMIR-GRPO始终优于强大的GRPO基线，并且能够提供比标准GRPO解决的实例子集更广泛的覆盖增益。


<details>
  <summary>Details</summary>
Motivation: 当前用于大型语言模型（LLMs）复杂推理任务调整的主要范式是强化学习，其中群体相对策略优化（GRPO）被广泛应用于大规模后训练。然而，在重推理环境中，GRPO面临结构上的限制：序列级优势归一化引入了系统性的长度偏差、低质量轨迹的惩罚被稀释以及标量目标抛弃了嵌入在组内奖励排名中的丰富成对偏好信息。因此，从昂贵的回放中获得的宝贵监督未能得到充分利用。

Method: 为了解决这些问题，研究者们提出了AMIR-GRPO方法，它通过直接利用组内奖励排名创建一个类似DPO风格的隐式对比正则化器来增强GRPO，而不需要任何附加注解。这种做法旨在放大对低回报轨迹的压制效果，减少回应层面的长度偏见问题，并把每一次回放小组转变为一组更加紧密的监管约束条件。

Result: 实验结果表明，在多种数学推理基准上，AMIR-GRPO不仅稳定地超越了强大的GRPO基线，还使得正确与错误推理链之间的区分更加清晰，同时对于那些仅依靠传统GRPO无法解决的问题实例也提供了更为广泛的覆盖改进。

Conclusion: 总之，AMIR-GRPO作为一种改进型的GRPO方法，在处理复杂的推理任务时展现出了显著的优势，尤其是在提高监督效率和扩大解决问题范围方面。

Abstract: Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized.
  We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.

</details>


### [39] [Stochastic Voronoi Ensembles for Anomaly Detection](https://arxiv.org/abs/2601.03664)
*Yang Cao*

Main category: cs.LG

TL;DR: 提出了一种名为SVEAD的新型异常检测方法，通过构建随机Voronoi图集合并根据局部尺度加权归一化单元相对距离来评分点。该方法在45个数据集上的实验表明优于12种最先进方法，并实现了线性时间复杂度和常数空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法难以处理具有不同局部密度的数据集：基于距离的方法可能会遗漏局部异常值，而基于密度的方法需要仔细选择参数且具有二次时间复杂度。

Method: SVEAD (Stochastic Voronoi Ensembles Anomaly Detector) 构建了随机Voronoi图集合，并通过局部尺度加权的归一化单元相对距离来为点打分。

Result: SVEAD 在45个数据集上进行了测试，结果表明它优于12种最新的异常检测方法。此外，SVEAD达到了线性时间复杂度和常数空间复杂度。

Conclusion: SVEAD提供了一种有效检测局部异常值的新方法，同时保持了高效的时间和空间复杂度。

Abstract: Anomaly detection aims to identify data instances that deviate significantly from majority of data, which has been widely used in fraud detection, network security, and industrial quality control. Existing methods struggle with datasets exhibiting varying local densities: distance-based methods miss local anomalies, while density-based approaches require careful parameter selection and incur quadratic time complexity. We observe that local anomalies, though indistinguishable under global analysis, become conspicuous when the data space is decomposed into restricted regions and each region is examined independently. Leveraging this geometric insight, we propose SVEAD (Stochastic Voronoi Ensembles Anomaly Detector), which constructs ensemble random Voronoi diagrams and scores points by normalized cell-relative distances weighted by local scale. The proposed method achieves linear time complexity and constant space complexity. Experiments on 45 datasets demonstrate that SVEAD outperforms 12 state-of-the-art approaches.

</details>


### [40] [A Pre-trained Reaction Embedding Descriptor Capturing Bond Transformation Patterns](https://arxiv.org/abs/2601.03689)
*Weiqi Liu,Fenglei Cao,Yuan Qi,Li-Cheng Xu*

Main category: cs.LG

TL;DR: 本文介绍了一种新的反应级描述符RXNEmb，它基于预训练模型RXNGraphormer，能够区分真实反应和虚构反应，并学习内在的键形成与断裂模式。通过数据驱动的方法对USPTO-50k数据集进行重新聚类，使得分类更加直接地反映基于键变化的相似性。此外，通过对注意力权重的分析，该模型还提供了化学关键位点的机制见解。


<details>
  <summary>Details</summary>
Motivation: 随着数据驱动反应预测模型的兴起，有效的反应描述符对于连接现实世界化学与数字表示至关重要。然而，通用的、针对反应级别的描述符仍然稀缺。

Method: 研究引入了RXNEmb，这是一种新的反应级描述符，来源于一个预先训练好的模型RXNGraphormer。该模型旨在区分具有错误键变化的虚构反应与真实反应，从而学习到内在的键形成与断裂模式。

Result: 通过使用RXNEmb对USPTO-50k数据集进行数据驱动的重新聚类，结果表明这种分类方法比基于规则的类别更能直接反映出基于键变化的相似性。结合降维技术后，RXNEmb使我们能够可视化反应空间的多样性。同时，注意力权重分析显示模型关注于化学上的关键位点，为理解提供机理洞察。

Conclusion: RXNEmb作为一款强大的、可解释的工具，在反应指纹识别和分析中发挥作用，为采用更以数据为中心的方法进行反应分析和发现铺平道路。

Abstract: With the rise of data-driven reaction prediction models, effective reaction descriptors are crucial for bridging the gap between real-world chemistry and digital representations. However, general-purpose, reaction-wise descriptors remain scarce. This study introduces RXNEmb, a novel reaction-level descriptor derived from RXNGraphormer, a model pre-trained to distinguish real reactions from fictitious ones with erroneous bond changes, thereby learning intrinsic bond formation and cleavage patterns. We demonstrate its utility by data-driven re-clustering of the USPTO-50k dataset, yielding a classification that more directly reflects bond-change similarities than rule-based categories. Combined with dimensionality reduction, RXNEmb enables visualization of reaction space diversity. Furthermore, attention weight analysis reveals the model's focus on chemically critical sites, providing mechanistic insight. RXNEmb serves as a powerful, interpretable tool for reaction fingerprinting and analysis, paving the way for more data-centric approaches in reaction analysis and discovery.

</details>


### [41] [Inference Attacks Against Graph Generative Diffusion Models](https://arxiv.org/abs/2601.03701)
*Xiuling Wang,Xin Huang,Guibo Luo,Jianliang Xu*

Main category: cs.LG

TL;DR: 本文探讨了图生成扩散模型中的信息泄露问题，通过三种类型的黑盒推理攻击（图重建攻击、属性推断攻击和成员推断攻击）来揭示这些模型可能存在的隐私风险，并提出了两种防御机制以减轻这些攻击，同时保持模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着图生成扩散模型在处理复杂图结构方面的应用越来越广泛，其潜在的隐私风险尚未得到充分研究。本文旨在通过设计并实施几种针对此类模型的推理攻击，揭示模型中存在的信息泄漏问题。

Method: 1. 设计了一种图重建攻击，能够从生成的图中重构出与训练图结构相似的图。
2. 提出了一个属性推断攻击方法，用于从生成图中推断训练图的属性，如平均图密度和密度分布。
3. 开发了两种成员推断攻击技术，用来判断给定的图是否存在于训练集中。
4. 对比基线方法，在三种不同类型的图生成扩散模型及六个真实世界图数据集上进行了广泛的实验。
5. 提出了两种新的防御机制，以缓解上述推理攻击带来的威胁，同时优化了防御强度与目标模型效用之间的平衡。

Result: 实验结果表明，所提出的攻击手段在有效性方面显著优于基线方法。此外，新提出的防御策略不仅有效抵御了攻击，还在保持模型性能的同时提供了更好的安全性。

Conclusion: 本研究表明，即使是最先进的图生成扩散模型也面临着严重的隐私泄露风险。通过引入专门设计的攻击方案以及相应的防御措施，可以为未来该领域的研究提供有价值的参考。

Abstract: Graph generative diffusion models have recently emerged as a powerful paradigm for generating complex graph structures, effectively capturing intricate dependencies and relationships within graph data. However, the privacy risks associated with these models remain largely unexplored. In this paper, we investigate information leakage in such models through three types of black-box inference attacks. First, we design a graph reconstruction attack, which can reconstruct graphs structurally similar to those training graphs from the generated graphs. Second, we propose a property inference attack to infer the properties of the training graphs, such as the average graph density and the distribution of densities, from the generated graphs. Third, we develop two membership inference attacks to determine whether a given graph is present in the training set. Extensive experiments on three different types of graph generative diffusion models and six real-world graphs demonstrate the effectiveness of these attacks, significantly outperforming the baseline approaches. Finally, we propose two defense mechanisms that mitigate these inference attacks and achieve a better trade-off between defense strength and target model utility than existing methods. Our code is available at https://zenodo.org/records/17946102.

</details>


### [42] [TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL](https://arxiv.org/abs/2601.03703)
*Lang Cao,Hui Ruan,Yongqian Li,Peng Chao,Wu Ning,Haonan Song,Renhong Chen,Yitong Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法TreeAdv，用于改进基于组的强化学习中的优势分配问题。通过构建基于熵驱动采样方法的树结构，TreeAdv能够更有效地探索和分配优势，从而在数学推理基准测试中超越了GRPO和GSPO的表现，并且使用了更少的生成令牌。


<details>
  <summary>Details</summary>
Motivation: 标准的Group Relative Policy Optimization（GRPO）方法将每个rollout轨迹视为独立的平面序列，给所有token指派单一序列级别的优势值，这导致了样本效率低下以及偏好冗长、重复的思维链而没有提高逻辑深度的问题。

Method: 提出了Tree-Structured Advantage Redistribution for Group-Based RL (TreeAdv) 方法，该方法显式地利用了组rollouts的树状结构来进行探索和优势分配。首先，根据一种熵驱动的抽样方法建立一组树（即森林），其中每棵树在高不确定性决策处分支，同时跨rollouts共享低不确定性token。接着，通过重新分配完整rollouts（所有叶节点）的优势来聚合内部树段的token级优势。

Result: 在10个数学推理基准测试上，TreeAdv持续优于GRPO和GSPO，同时在相同的监督、数据和解码预算下使用了显著更少的生成token。

Conclusion: TreeAdv提供了一个有效的方法来解决基于组的强化学习任务中的样本效率低下和长度偏差问题，通过引入树状结构来改善探索与优势分配过程。

Abstract: Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.

</details>


### [43] [The Geometry of the Pivot: A Note on Lazy Pivoted Cholesky and Farthest Point Sampling](https://arxiv.org/abs/2601.03706)
*Gil Shabat*

Main category: cs.LG

TL;DR: 本文揭示了Pivoted Cholesky分解在核方法中的几何直觉，表明其关键选择步骤等价于使用核度量的最远点采样，并且Cholesky因子构建是隐式的Gram-Schmidt正交化。同时提供了简洁的推导和简约的Python实现。


<details>
  <summary>Details</summary>
Motivation: 尽管Pivoted Cholesky分解在数值线性代数中的代数性质被广泛记录，但在核方法上下文中它的几何直观性往往不明显。本研究旨在阐明该算法在再生核希尔伯特空间（RKHS）内的几何解释。

Method: 通过展示关键选择步骤与基于核度量的最远点采样（FPS）数学上的等价性，以及说明Cholesky因子构造过程实际上执行了一种隐式的Gram-Schmidt正交化，从而为理解Pivoted Cholesky分解提供了一个新的视角。

Result: 研究成功地将Pivoted Cholesky分解的关键概念与其在RKHS中的几何意义联系起来，并通过简洁的数学推导和Python代码实现了理论与实践之间的桥梁搭建。

Conclusion: 这项工作不仅加深了对Pivoted Cholesky分解如何在大规模核矩阵低秩近似中发挥作用的理解，还通过引入几何直觉简化了这一过程的认知难度，促进了从理论到实际应用的转化。

Abstract: Low-rank approximations of large kernel matrices are ubiquitous in machine learning, particularly for scaling Gaussian Processes to massive datasets. The Pivoted Cholesky decomposition is a standard tool for this task, offering a computationally efficient, greedy low-rank approximation. While its algebraic properties are well-documented in numerical linear algebra, its geometric intuition within the context of kernel methods often remains obscure. In this note, we elucidate the geometric interpretation of the algorithm within the Reproducing Kernel Hilbert Space (RKHS). We demonstrate that the pivotal selection step is mathematically equivalent to Farthest Point Sampling (FPS) using the kernel metric, and that the Cholesky factor construction is an implicit Gram-Schmidt orthogonalization. We provide a concise derivation and a minimalist Python implementation to bridge the gap between theory and practice.

</details>


### [44] [ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization](https://arxiv.org/abs/2601.03723)
*Shijie Zhang,Kevin Zhang,Zheyuan Gu,Xiang Guo,Rujun Guo,Shaoyu Liu,Guanjun Jiang,Xiaozhao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法Elastic Trust Regions (ETR)，用于解决现有Group Relative Policy Optimization (GRPO)算法在处理异质性信号时存在的局限性。通过微观和宏观两个层面的弹性调整，ETR能够更好地适应优势幅度和方差的变化，从而提高学习效率并保持探索能力。实验表明，ETR在AIME和MATH基准测试中优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 当前主导强化学习领域的方法——组相对策略优化（GRPO）假设所有样本具有相同的、静态的信任区域约束，这与结果驱动学习中信号质量（即优势大小和方差）存在显著差异的事实不相符。这种固定约束无法充分利用高质量信号，并且不能有效抑制噪音，导致熵快速下降。为了解决这一问题，提出了Elastic Trust Regions (ETR) 方法。

Method: ETR采用双层弹性机制来构建信号感知的优化环境：在微观层面，根据优势大小调整裁剪边界以加快从高置信度路径的学习；在宏观层面，则利用组内方差隐式地为处于最佳学习区的任务分配更大的更新预算。

Result: 通过在AIME和MATH基准上的广泛实验验证，ETR不仅持续超越了GRPO的表现，实现了更高的准确性，而且有效地缓解了策略熵的退化，保证了持续的探索能力。

Conclusion: ETR作为一种动态调节信任区域的新方法，在处理异构信号方面表现出色，能够更有效地促进学习过程中的探索与利用平衡，是改进现有GRPO算法的有效途径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \textbf{E}lastic \textbf{T}rust \textbf{R}egions (\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.

</details>


### [45] [EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning](https://arxiv.org/abs/2601.03725)
*Jing-Cheng Pang,Liu Sun,Chang Zhou,Xian Tang,Haichuan Ma,Kun Jiang,Jianlong Wang,Kai Zhang,Sijie Wu,Haoran Cai,Chenwei Wu,Xubin Li,Xin Chen*

Main category: cs.LG

TL;DR: 提出了一种新的框架EDCO，通过动态课程编排和推理熵来优化领域特定的大语言模型微调过程。实验表明，EDCO在不同领域的模型（Qwen3-4B和Llama3.2-3B）上表现优于传统方法，并且有效减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型微调方法依赖于静态课程设计，缺乏对模型训练过程中变化需求的适应性。为解决这一问题，提出了基于推理熵和动态课程编排的新框架EDCO。

Method: EDCO框架包括三个核心部分：高效的熵估计器、基于熵的课程生成器以及LLM训练器。其中，熵估计器使用前缀令牌近似全序列熵；课程生成器选择具有最高推理熵的数据点；训练器则基于选定的课程优化模型。

Result: 在通信、医学和法律领域的综合实验中，EDCO对于Qwen3-4B和Llama3.2-3B模型在监督学习和强化学习设置下均表现出色，同时提出的高效熵估计方法相较于传统方法节省了83.5%的计算时间。

Conclusion: EDCO提供了一种有效的解决方案，以提高领域特定大语言模型微调过程中的学习效率和性能，同时显著降低了所需计算资源。

Abstract: Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.

</details>


### [46] [Probabilistic Transformers for Joint Modeling of Global Weather Dynamics and Decision-Centric Variables](https://arxiv.org/abs/2601.03753)
*Paulius Rauba,Viktor Cikojevic,Fran Bartolic,Sam Levang,Ty Dickinson,Chase Dwelle*

Main category: cs.LG

TL;DR: 本文提出了一种名为GEM-2的概率变换器，它能够同时学习全球大气动力学以及用户直接操作的一系列变量。这种轻量且计算效率高的模型在直接预测性能上超过了现有的数值天气预报模型，并且在经济价值度量方面达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 由于决策依赖于天气预报中的特定功能变量（如极值、累积和阈值超过）而不是状态变量本身，这导致用户需要通过后处理来估计这些目标，这种方法可能不是最优的并且可能引入结构性偏差。核心问题是，当前模型没有直接学习到这些对决策至关重要的分布特征。

Method: 研究者开发了GEM-2，这是一种概率变换器，旨在同时掌握全球大气动态及与用户直接相关的变量。该模型具有较轻的参数量（约2.75亿个参数），并且训练速度比现有最先进技术快20至100倍。GEM-2使用CRPS目标进行训练。

Result: 实验结果显示，GEM-2不仅在直接预测性能上超越了运行中的数值天气预测模型，还能与依靠昂贵多步骤扩散过程或需要定制化多阶段微调策略的机器学习模型竞争。此外，在决策理论评估下，GEM-2展示了最先进的经济价值指标，以及在S2S和季节性时间尺度上稳定收敛至气候学特性。

Conclusion: GEM-2提供了一个有效解决高利益相关领域内天气预报问题的新方法，通过直接学习对用户决策有用的变量，提高了预测准确性和实用性。

Abstract: Weather forecasts sit upstream of high-stakes decisions in domains such as grid operations, aviation, agriculture, and emergency response. Yet forecast users often face a difficult trade-off. Many decision-relevant targets are functionals of the atmospheric state variables, such as extrema, accumulations, and threshold exceedances, rather than state variables themselves. As a result, users must estimate these targets via post-processing, which can be suboptimal and can introduce structural bias. The core issue is that decisions depend on distributions over these functionals that the model is not trained to learn directly.
  In this work, we introduce GEM-2, a probabilistic transformer that jointly learns global atmospheric dynamics alongside a suite of variables that users directly act upon. Using this training recipe, we show that a lightweight (~275M params) and computationally efficient (~20-100x training speedup relative to state-of-the-art) transformer trained on the CRPS objective can directly outperform operational numerical weather prediction (NWP) models and be competitive with ML models that rely on expensive multi-step diffusion processes or require bespoke multi-stage fine-tuning strategies. We further demonstrate state-of-the-art economic value metrics under decision-theoretic evaluation, stable convergence to climatology at S2S and seasonal timescales, and a surprising insensitivity to many commonly assumed architectural and training design choices.

</details>


### [47] [Detecting Semantic Backdoors in a Mystery Shopping Scenario](https://arxiv.org/abs/2601.03805)
*Arpad Berta,Gabor Danner,Istvan Hegedus,Mark Jelasity*

Main category: cs.LG

TL;DR: 提出了一种检测分类模型中语义后门的方法，通过创建参考模型池并校准模型距离阈值来识别干净的模型。该方法在对抗训练请求下最可靠，使用逆向生成的输入样本来测量模型间的距离，能够有效地区分干净和中毒的模型。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏清晰可辨识的模式，语义后门比基于触发模式的后门更难以检测。研究动机来源于消费者保护场景，其中监管机构测试机器学习服务提供商是否在其提供的模型中植入了后门。

Method: 在假设已知清洁训练数据集及模型训练流程的前提下，通过构建一个由少量干净和中毒模型组成的参考模型池，并且在校准模型距离阈值的基础上开发一种新的方法来检测语义后门。实验分析了几种计算模型间距离的方法，并测试了服务商采取适应性攻击以逃避检测的情况。

Result: 实验结果表明，在要求服务商提供对抗训练的情况下，利用特别生成的用来最大化与干净样本之间距离的输入样本来衡量模型距离的方法最为可靠。这种方法经常能够完全区分出干净和被污染的模型，并且证明优于当前最先进的后门检测器。

Conclusion: 本文提出的方法为检测语义后门提供了有效的解决方案，尤其是在服务商可能采取措施试图规避检测的情形下依然表现出色。

Abstract: Detecting semantic backdoors in classification models--where some classes can be activated by certain natural, but out-of-distribution inputs--is an important problem that has received relatively little attention. Semantic backdoors are significantly harder to detect than backdoors that are based on trigger patterns due to the lack of such clearly identifiable patterns. We tackle this problem under the assumption that the clean training dataset and the training recipe of the model are both known. These assumptions are motivated by a consumer protection scenario, in which the responsible authority performs mystery shopping to test a machine learning service provider. In this scenario, the authority uses the provider's resources and tools to train a model on a given dataset and tests whether the provider included a backdoor. In our proposed approach, the authority creates a reference model pool by training a small number of clean and poisoned models using trusted infrastructure, and calibrates a model distance threshold to identify clean models. We propose and experimentally analyze a number of approaches to compute model distances and we also test a scenario where the provider performs an adaptive attack to avoid detection. The most reliable method is based on requesting adversarial training from the provider. The model distance is best measured using a set of input samples generated by inverting the models in such a way as to maximize the distance from clean samples. With these settings, our method can often completely separate clean and poisoned models, and it proves to be superior to state-of-the-art backdoor detectors as well.

</details>


### [48] [Feature-Aware One-Shot Federated Learning via Hierarchical Token Sequences](https://arxiv.org/abs/2601.03882)
*Shudong Liu,Hanwen Zhang,Xiuling Wang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: 本文提出了一种名为FALCON的框架，通过特征感知的分层令牌序列生成和知识蒸馏来提高一次性联邦学习（OSFL）在非IID图像数据上的有效性。实验表明，FALxon在多样化的非IID场景中优于最佳OSFL基线平均准确率9.58%。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数一次性联邦学习方法难以在医学影像等现实领域实现稳健性能，或在处理非独立同分布数据时效率低下。为了解决这些限制，提出了FALCON框架。

Method: FALCON框架包括：1) 每个客户端使用预训练视觉编码器将图像压缩成捕捉多尺度语义的分层令牌序列；2) 使用多尺度自回归变压器生成器建模令牌序列分布并生成合成序列；3) 客户端上传合成序列及基于真实令牌序列训练的本地分类器至服务器；4) 服务器通过知识蒸馏减少对精确分布建模的依赖来进行全局训练。

Result: 在医疗和自然图像数据集上进行的实验验证了FALCON在多种非IID情况下的有效性，并且其平均准确性比最佳OSFL基线高出9.58%。

Conclusion: FALCON通过引入特征感知的层次化令牌序列生成与知识提炼技术，有效提升了针对非IID图像数据的一次性联邦学习(OSFL)的表现，解决了现有方法在处理实际领域如医疗影像数据时存在的问题。

Abstract: One-shot federated learning (OSFL) reduces the communication cost and privacy risks of iterative federated learning by constructing a global model with a single round of communication. However, most existing methods struggle to achieve robust performance on real-world domains such as medical imaging, or are inefficient when handling non-IID (Independent and Identically Distributed) data. To address these limitations, we introduce FALCON, a framework that enhances the effectiveness of OSFL over non-IID image data. The core idea of FALCON is to leverage the feature-aware hierarchical token sequences generation and knowledge distillation into OSFL. First, each client leverages a pretrained visual encoder with hierarchical scale encoding to compress images into hierarchical token sequences, which capture multi-scale semantics. Second, a multi-scale autoregressive transformer generator is used to model the distribution of these token sequences and generate the synthetic sequences. Third, clients upload the synthetic sequences along with the local classifier trained on the real token sequences to the server. Finally, the server incorporates knowledge distillation into global training to reduce reliance on precise distribution modeling. Experiments on medical and natural image datasets validate the effectiveness of FALCON in diverse non-IID scenarios, outperforming the best OSFL baselines by 9.58% in average accuracy.

</details>


### [49] [Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training](https://arxiv.org/abs/2601.03895)
*Chi Liu,Xin Chen*

Main category: cs.LG

TL;DR: 本文提出了一种改进的GRPO算法，称为ABC-GRPO，通过调整剪辑机制提高了灵活性和泛化能力，在数学推理任务上表现优于标准GRPO，并保持了较高的熵以维持模型探索能力。


<details>
  <summary>Details</summary>
Motivation: 研究者发现现有的GRPO算法在某些情况下由于其剪辑机制存在不足，希望通过适当的修改来提高算法的灵活性和泛化性能。

Method: 提出了Adaptive-Boundary-Clipping GRPO (ABC-GRPO)，这是一种对原始GRPO框架进行非对称性和适应性改进的方法。

Result: 实验表明，与标准GRPO相比，ABC-GRPO在使用Qwen3 LLMs完成数学推理任务时表现出更好的性能，并在整个训练过程中保持了更高的熵，有助于防止过早收敛。

Conclusion: 通过引入ABC-GRPO，研究不仅提升了GRPO算法的表现，还增强了其在复杂任务中的适用性。此外，公开了实现代码以促进可重复性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.

</details>


### [50] [FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning](https://arxiv.org/abs/2601.03938)
*Yujie Feng,Hao Wang,Jian Li,Xu Chu,Zhaolu Kang,Yiran Liu,Yasha Wang,Philip S. Yu,Xiao-Ming Wu*

Main category: cs.LG

TL;DR: 本文提出了一种新的持续学习框架FOREVER，该框架基于遗忘曲线理论调整记忆回放时间表，并通过与模型内部演变相匹配而非固定训练步骤来确定何时以及如何进行回放。实验表明，这种方法能够有效减轻大型语言模型中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆回放方法通常依赖于固定的、基于步数的启发式策略，这可能与模型实际的学习进度不一致。受人类遗忘曲线启发，研究者们希望开发出一种更符合模型自身演化规律的记忆回放机制，以更好地解决连续学习过程中出现的灾难性遗忘现象。

Method: FOREVER是一种新颖的持续学习框架，它根据优化器更新幅度定义‘模型时间’，并利用基于遗忘曲线理论的重播调度程序来决定何时执行重播操作。此外，还引入了强度感知正则化机制来自适应地控制重播方式。

Result: 在三个不同的持续学习基准测试及从0.6B到13B参数规模的不同模型上进行的广泛实验表明，FOREVER能一致地缓解灾难性遗忘问题。

Conclusion: 通过采用基于遗忘曲线理论设计的记忆回放策略，FOREVER为大型语言模型提供了一个有效的解决方案，用以应对连续学习任务中面临的挑战，特别是关于如何避免灾难性遗忘的问题。

Abstract: Continual learning (CL) for large language models (LLMs) aims to enable sequential knowledge acquisition without catastrophic forgetting. Memory replay methods are widely used for their practicality and effectiveness, but most rely on fixed, step-based heuristics that often misalign with the model's actual learning progress, since identical training steps can result in varying degrees of parameter change. Motivated by recent findings that LLM forgetting mirrors the Ebbinghaus human forgetting curve, we propose FOREVER (FORgEtting curVe-inspired mEmory Replay), a novel CL framework that aligns replay schedules with a model-centric notion of time. FOREVER defines model time using the magnitude of optimizer updates, allowing forgetting curve-inspired replay intervals to align with the model's internal evolution rather than raw training steps. Building on this approach, FOREVER incorporates a forgetting curve-based replay scheduler to determine when to replay and an intensity-aware regularization mechanism to adaptively control how to replay. Extensive experiments on three CL benchmarks and models ranging from 0.6B to 13B parameters demonstrate that FOREVER consistently mitigates catastrophic forgetting.

</details>


### [51] [Stage-specific cancer survival prediction enriched by explainable machine learning](https://arxiv.org/abs/2601.03977)
*Parisa Poorhasani,Bogdan Iancu*

Main category: cs.LG

TL;DR: 研究使用SEER数据集创建并验证了可解释的机器学习模型，以预测结直肠癌、胃癌和肝癌的阶段特异性癌症存活率。通过SHAP和LIME等可解释性技术，揭示了在不同癌症阶段和类型中某些人口统计学和临床变量对生存的不同影响，为个性化治疗计划提供了透明度和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 传统生存预测模型通常基于所有疾病阶段的综合样本来训练和评估，可能导致性能高估并忽略阶段特有的差异。本研究旨在通过构建阶段特异性模型来提供新的见解，识别每个癌症阶段最重要的因素，并支持个性化治疗规划。

Method: 利用SEER数据集开发了针对结直肠癌、胃癌和肝癌的阶段特异性癌症存活预测模型。应用了解释性技术如SHapley Additive exPlanations (SHAP) 和 Local Interpretable Model-agnostic Explanations (LIME)，以揭示关键特征与癌症阶段之间的相互作用。

Result: 研究表明特定的人口统计学和临床变量在不同癌症阶段和类型中对生存的影响存在显著差异。通过采用可解释性的方法，能够发现这些重要的特征-癌症阶段互动，从而增加模型的透明度和临床实用性。

Conclusion: 专注于阶段特异性模型的研究不仅提高了癌症存活预测的准确性，而且通过强调每个阶段的关键因素增加了透明度和潜在的临床相关性，有助于促进个性化的治疗计划制定。

Abstract: Despite the fact that cancer survivability rates vary greatly between stages, traditional survival prediction models have frequently been trained and assessed using examples from all combined phases of the disease. This method may result in an overestimation of performance and ignore the stage-specific variations. Using the SEER dataset, we created and verified explainable machine learning (ML) models to predict stage-specific cancer survivability in colorectal, stomach, and liver cancers. ML-based cancer survival analysis has been a long-standing topic in the literature; however, studies involving the explainability and transparency of ML survivability models are limited. Our use of explainability techniques, including SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), enabled us to illustrate significant feature-cancer stage interactions that would have remained hidden in traditional black-box models. We identified how certain demographic and clinical variables influenced survival differently across cancer stages and types. These insights provide not only transparency but also clinical relevance, supporting personalized treatment planning. By focusing on stage-specific models, this study provides new insights into the most important factors at each stage of cancer, offering transparency and potential clinical relevance to support personalized treatment planning.

</details>


### [52] [Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing](https://arxiv.org/abs/2601.04051)
*Viktor Martinek,Roland Herzog*

Main category: cs.LG

TL;DR: 本文提出了一种新的符号回归方法，该方法能够处理多个类别变量，并引入了参数共享的不同层次。通过合成数据示例和实际天体物理数据集的应用，展示了这种方法在减少所需参数数量、提高问题理解度方面的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的符号回归方法在处理相似现象时仅使用单一表达式配以不同参数集，但这些方法要么只允许特定于类别值的参数（非共享），要么引入与类别值无关的参数（共享）。本文旨在通过考虑多个类别变量并引入参数共享的不同层次来扩展现有工作，从而减少参数数量并揭示更多关于问题的信息。

Method: 本文提出的方法包括处理两个类别变量，并在这两个类别之间引入不同程度的参数共享。此外，还通过一个仅用于拟合的合成数据示例测试了该设置在减少数据需求和迁移学习方面的能力。最后，在一个真实的天体物理学数据集上应用所提方法，进一步验证其有效性。

Result: 实验结果表明，对于合成数据示例，新方法能够在减少数据需求的同时支持有效的迁移学习。而在真实世界的天体物理数据集上，与之前的研究相比，虽然只考虑了一个类别变量，但本研究实现了相似的拟合质量，同时需要显著更少的独立参数，并从中提取到了有关问题的额外信息。

Conclusion: 本文介绍的新方法为符号回归领域提供了处理多类别变量的有效途径，通过引入参数共享的不同层级，不仅减少了模型所需的参数量，还增强了对问题背景的理解。这为科学发现中利用符号回归技术提供了一种更为灵活且强大的工具。

Abstract: Symbolic Regression aims to find symbolic expressions that describe datasets. Due to better interpretability, it is a machine learning paradigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only "non-shared" (category-value-specific) parameters, and others also incorporate "shared" (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parameter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i.e., parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Using a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic regression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one categorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem.

</details>


### [53] [LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis](https://arxiv.org/abs/2601.04054)
*Yayati Jadhav,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本研究提出了一种自回归扩散框架，通过将机械连杆表示为顺序构建的图来解决设计中的非线性运动-配置关系问题。该方法结合了因果变换器和去噪扩散概率模型(DDPM)，能够自适应地生成和修正节点，从而实现复杂机械系统的设计。


<details>
  <summary>Details</summary>
Motivation: 由于连续节点放置、离散拓扑配置和非线性运动学约束之间复杂的耦合关系，设计出能够达到目标末端执行器轨迹的机械连杆是一个根本性的挑战。传统的优化与启发式方法在面对这一问题时往往因计算量过大而变得不可行。

Method: 引入了一个自回归扩散框架，利用连杆装配的二元性质，将机制表示为依次构建的图，其中节点对应于关节，边对应于刚性连接。此方法结合了因果变换器与去噪扩散概率模型（DDPM），两者均以通过变压器编码器编码的目标轨迹为条件。因果变换器按节点顺序预测离散拓扑结构，而DDPM则细化每个节点的空间坐标及其与之前生成节点之间的边连接性。

Result: 所提出的基于图的数据驱动方法超越了传统优化方法，支持可扩展逆向设计，并适用于具有任意节点数的机制。成功合成包含多达20个节点的连杆系统，并展示了对N节点架构的可扩展性。

Conclusion: 这项工作推进了自回归图生成方法论和计算运动学综合技术的发展，为复杂机械系统的可扩展逆向设计建立了新的范例。

Abstract: Designing mechanical linkages to achieve target end-effector trajectories presents a fundamental challenge due to the intricate coupling between continuous node placements, discrete topological configurations, and nonlinear kinematic constraints. The highly nonlinear motion-to-configuration relationship means small perturbations in joint positions drastically alter trajectories, while the combinatorially expanding design space renders conventional optimization and heuristic methods computationally intractable. We introduce an autoregressive diffusion framework that exploits the dyadic nature of linkage assembly by representing mechanisms as sequentially constructed graphs, where nodes correspond to joints and edges to rigid links. Our approach combines a causal transformer with a Denoising Diffusion Probabilistic Model (DDPM), both conditioned on target trajectories encoded via a transformer encoder. The causal transformer autoregressively predicts discrete topology node-by-node, while the DDPM refines each node's spatial coordinates and edge connectivity to previously generated nodes. This sequential generation enables adaptive trial-and-error synthesis where problematic nodes exhibiting kinematic locking or collisions can be selectively regenerated, allowing autonomous correction of degenerate configurations during design. Our graph-based, data-driven methodology surpasses traditional optimization approaches, enabling scalable inverse design that generalizes to mechanisms with arbitrary node counts. We demonstrate successful synthesis of linkage systems containing up to 20 nodes with extensibility to N-node architectures. This work advances autoregressive graph generation methodologies and computational kinematic synthesis, establishing new paradigms for scalable inverse design of complex mechanical systems.

</details>


### [54] [Minimum distance classification for nonlinear dynamical systems](https://arxiv.org/abs/2601.04058)
*Dominique Martinez*

Main category: cs.LG

TL;DR: 提出了一种名为Dynafit的基于核的方法，用于学习训练轨迹与底层动力学之间的距离度量，并根据学习到的度量将新观测分类到具有最相似动力学的类别中。该方法通过近似Koopman算子来线性化特征空间中的动力学，并且可以通过核技巧在不考虑特征空间维度的情况下计算距离度量。


<details>
  <summary>Details</summary>
Motivation: 为了解决由非线性动态生成的轨迹数据分类问题，其中每个类别对应一个独特的动态系统。

Method: 提出了Dynafit，这是一种基于核的方法，用来学习训练轨迹和底层动力学之间的一个距离度量。学习算法近似了Koopman算子，该算子在一个可能无限维的特征空间内全局线性化动力学。利用机器学习中常见的核技巧，在特征空间内独立于其维度计算距离度量。

Result: Dynafit可以应用于涉及非线性动力系统和传感器的各种分类任务。有效性通过三个示例得到了说明：使用逻辑斯蒂映射进行混沌检测、手写动力学识别以及视觉动态纹理识别。

Conclusion: Dynafit提供了一种有效的方法来对由不同动态系统产生的非线性轨迹数据进行分类，它能够在线性和非线性特征空间中工作，并且当有可用的关于动态的部分知识时，核函数还可以被定制以包含这些信息。

Abstract: We address the problem of classifying trajectory data generated by some nonlinear dynamics, where each class corresponds to a distinct dynamical system. We propose Dynafit, a kernel-based method for learning a distance metric between training trajectories and the underlying dynamics. New observations are assigned to the class with the most similar dynamics according to the learned metric. The learning algorithm approximates the Koopman operator which globally linearizes the dynamics in a (potentially infinite) feature space associated with a kernel function. The distance metric is computed in feature space independently of its dimensionality by using the kernel trick common in machine learning. We also show that the kernel function can be tailored to incorporate partial knowledge of the dynamics when available. Dynafit is applicable to various classification tasks involving nonlinear dynamical systems and sensors. We illustrate its effectiveness on three examples: chaos detection with the logistic map, recognition of handwritten dynamics and of visual dynamic textures.

</details>


### [55] [Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models](https://arxiv.org/abs/2601.04110)
*Magnus Bühler,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 本文提出了一种名为CausalMixFT的方法，通过使用从目标数据集拟合的结构因果模型（SCMs）生成结构一致的合成样本，来增强表格基础模型在数据稀缺情况下的微调鲁棒性和下游性能。实验结果表明，该方法不仅提高了分类任务的中位数归一化ROC-AUC得分，还缩小了验证-测试性能相关性差距，从而改善了基于验证的早停策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的情况下对表格基础模型进行微调是一个挑战，因为基于更加稀缺的验证数据来进行早停往往无法准确反映模型的真实泛化能力。为了解决这个问题，并提高微调过程中的稳定性和最终模型的表现，作者提出了CausalMixFT方法。

Method: CausalMixFT利用结构性因果模型（SCMs）从目标数据集中学习并生成保持特征依赖性的合成样本，以此增加训练时的数据多样性。这些合成样本与真实数据相结合，用于增强微调阶段的数据量和质量。

Result: 通过对TabArena上的33个分类数据集以及超过2300次微调运行的评估显示，CausalMixFT能够将中位数归一化ROC-AUC分数从标准微调方法的0.10提升至0.12，并且相较于其他纯统计生成方法具有明显优势。此外，它还将验证-测试性能相关性差距从0.67降至0.30。

Conclusion: 研究结果证明，在数据扩增过程中加入因果结构是一种有效且有原则的方法，可以提高低数据环境下表格基础模型微调的稳定性与表现。

Abstract: Fine-tuning tabular foundation models (TFMs) under data scarcity is challenging, as early stopping on even scarcer validation data often fails to capture true generalization performance. We propose CausalMixFT, a method that enhances fine-tuning robustness and downstream performance by generating structurally consistent synthetic samples using Structural Causal Models (SCMs) fitted on the target dataset. This approach augments limited real data with causally informed synthetic examples, preserving feature dependencies while expanding training diversity. Evaluated across 33 classification datasets from TabArena and over 2300 fine-tuning runs, our CausalMixFT method consistently improves median normalized ROC-AUC from 0.10 (standard fine-tuning) to 0.12, outperforming purely statistical generators such as CTGAN (-0.01), TabEBM (-0.04), and TableAugment (-0.09). Moreover, it narrows the median validation-test performance correlation gap from 0.67 to 0.30, enabling more reliable validation-based early stopping, a key step toward improving fine-tuning stability under data scarcity. These results demonstrate that incorporating causal structure into data augmentation provides an effective and principled route to fine-tuning tabular foundation models in low-data regimes.

</details>


### [56] [Clinical Data Goes MEDS? Let's OWL make sense of it](https://arxiv.org/abs/2601.04164)
*Alberto Marfoglia,Jong Ho Jhee,Adrien Coulet*

Main category: cs.LG

TL;DR: The paper introduces MEDS-OWL, an OWL ontology, and meds2rdf, a Python library, to integrate the Medical Event Data Standard (MEDS) with the Semantic Web ecosystem, enabling better interoperability and FAIR-aligned data transformation for event-based clinical data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of standardized and semantically explicit representation in healthcare data, which limits interoperability and reproducibility. The aim is to bridge the gap between the Medical Event Data Standard (MEDS) and the Semantic Web ecosystem to enhance data integration, transformation, and analysis capabilities.

Method: The authors developed MEDS-OWL, a lightweight OWL ontology, to provide formal concepts and relations for representing MEDS datasets as RDF graphs. They also created meds2rdf, a Python conversion library, to transform MEDS events into RDF graphs that conform to the ontology. The approach was demonstrated using a synthetic dataset related to patient care pathways for ruptured intracranial aneurysms, and the resulting graph was validated using SHACL constraints.

Result: The first release of MEDS-OWL includes 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Together with the meds2rdf library, it facilitates the transformation of clinical data into FAIR-aligned datasets, provenance-aware publishing, and improved interoperability. The work establishes a robust foundation for further graph-based analytics on event-based clinical data.

Conclusion: By integrating MEDS with the Semantic Web through MEDS-OWL and meds2rdf, the paper contributes a reusable semantic layer for event-based clinical data, enhancing interoperability, data transformation, and setting a strong basis for advanced analytics. This solution addresses the limitations of current healthcare data standards and paves the way for more effective and reproducible machine learning workflows.

Abstract: The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [57] [Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication](https://arxiv.org/abs/2601.03390)
*Daniel Qian,Xiyu Hao,Jinkun Geng,Yuncheng Yao,Aurojit Panda,Jinyang Li,Anirudh Sivaraman*

Main category: cs.DC

TL;DR: Aspen, a leaderless BFT protocol, achieves near-optimal latency of 2Δ+ε by using a best-effort sequencing layer to handle contention, and it outperforms previous protocols in terms of commit latency and throughput under wide-area distributed settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a Byzantine Fault Tolerant (BFT) protocol that can provide low latency for user-facing applications such as payments. The challenge is to maintain this low latency even in the presence of contention, which can cause replicas to diverge and trigger recovery procedures in other protocols.

Method: Aspen introduces a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates to manage contention without requiring a leader. It requires n = 3f + 2p + 1 replicas to tolerate up to f Byzantine nodes, with an additional 2p nodes allowing the fast path to continue even if p replicas diverge due to unpredictable network delays. When optimistic conditions fail, Aspen reverts to a PBFT-style protocol to ensure safety and liveness under partial synchrony.

Result: In a wide-area distributed setting, Aspen was able to commit requests in less than 75 ms, improving upon previous protocols by 1.2 to 3.3 times, while supporting 19,000 requests per second.

Conclusion: Aspen demonstrates that it is possible to achieve a near-optimal commit latency of 2Δ+ε in a leaderless BFT protocol, even in the presence of contention. This makes it suitable for use in permissioned blockchains where low latency and high throughput are critical, such as in payment systems.

Abstract: As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2Δ$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.
  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2Δ+ \varepsilon$, where $\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\times$ improvement compared to previous protocols, while supporting 19,000 requests per second.

</details>


### [58] [Majorum: Ebb-and-Flow Consensus with Dynamic Quorums](https://arxiv.org/abs/2601.03862)
*Francesco D'Amato,Roberto Saltini,Thanh-Hai Tran,Yann Vonlanthen,Luca Zanolini*

Main category: cs.DC

TL;DR: Majorum is an ebb-and-flow construction for consensus protocols that ensures dynamic availability and can finalize blocks in as few as three slots under optimal conditions, using a quorum-based approach.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to address the limitation of dynamically available consensus protocols, which cannot provide strong safety guarantees during network partitions or extended asynchrony. The goal is to create a protocol that remains live even when honest participants go offline and rejoin, while still ensuring safety and liveness.

Method: The method introduced in the paper is Majorum, an ebb-and-flow construction that combines a dynamically available protocol (based on TOB-SVD, a quorum-based protocol) with a partially synchronous finality protocol. This combination allows for block finalization in as few as three slots and requires only a single voting phase per slot when conditions are favorable.

Result: Under optimistic conditions, Majorum can finalize blocks in as few as three slots, with each slot finalizing the next block extending the previously finalized one, and it requires only a single voting phase per slot.

Conclusion: Majorum presents a novel ebb-and-flow construction that enhances the dynamic availability of consensus protocols, offering fast finality under optimal conditions while maintaining the robustness needed during network instability.

Abstract: Dynamic availability is the ability of a consensus protocol to remain live despite honest participants going offline and later rejoining. A well-known limitation is that dynamically available protocols, on their own, cannot provide strong safety guarantees during network partitions or extended asynchrony. Ebb-and-flow protocols [SP21] address this by combining a dynamically available protocol with a partially synchronous finality protocol that irrevocably finalizes a prefix.
  We present Majorum, an ebb-and-flow construction whose dynamically available component builds on a quorum-based protocol (TOB-SVD). Under optimistic conditions, Majorum finalizes blocks in as few as three slots while requiring only a single voting phase per slot. In particular, when conditions remain favourable, each slot finalizes the next block extending the previously finalized one.

</details>


### [59] [A Scheduling Framework for Efficient MoE Inference on Edge GPU-NDP Systems](https://arxiv.org/abs/2601.03992)
*Qi Wu,Chao Fang,Jiayuan Chen,Ye Lin,Yueqi Zhang,Yichuan Bai,Yuan Du,Li Du*

Main category: cs.DC

TL;DR: 本文提出了一种高效的推理框架，通过利用张量并行性、负载均衡感知调度算法以及无数据集预取策略来解决Mixture-of-Experts (MoE) 模型在边缘GPU-NDP系统部署时面临的三个关键挑战，从而显著提升了资源受限环境下的MoE推理效率。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE)模型虽然有助于边缘部署，但其大的内存占用需要具有近数据处理(NDP)能力的GPU系统支持。然而，在这种基于边缘的GPU-NDP系统上部署MoE模型面临着三个主要问题：1）由于非均匀专家选择和平行性导致的NDP单元间严重负载不平衡；2）NDP单元内专家计算期间GPU利用率不足；3）由于不可预测的专家激活模式所需的数据预分析过多。为了解决这些问题，提出了本研究。

Method: 论文提出的解决方案包括一个高效推理框架中的三项关键优化措施：1）探索了MoE推理中未被充分开发的张量并行性，以针对边缘低批处理场景同时跨多个NDP单元分割和计算大型专家参数；2）一种负载平衡意识调度算法，旨在通过在NDP单元与GPU之间分配专家计算任务来最大化资源利用率；3）一种无需依赖特定数据集的预取策略，用于主动加载经常访问的专家以最小化激活延迟。

Result: 实验结果显示，相比现有最先进方法，所提框架能使GPU-NDP系统平均达到2.41倍、最高可达2.56倍的端到端延迟加速，极大地提高了资源受限环境下MoE推理的效率。

Conclusion: 通过实施针对MoE模型在边缘设备上运行时遇到的具体挑战而设计的一系列创新性优化措施，该研究成功地展示了如何有效克服这些障碍，并实现更优性能。

Abstract: Mixture-of-Experts (MoE) models facilitate edge deployment by decoupling model capacity from active computation, yet their large memory footprint drives the need for GPU systems with near-data processing (NDP) capabilities that offload experts to dedicated processing units. However, deploying MoE models on such edge-based GPU-NDP systems faces three critical challenges: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation within NDP units, and 3) extensive data pre-profiling necessitated by unpredictable expert activation patterns for pre-fetching. To address these challenges, this paper proposes an efficient inference framework featuring three key optimizations. First, the underexplored tensor parallelism in MoE inference is exploited to partition and compute large expert parameters across multiple NDP units simultaneously towards edge low-batch scenarios. Second, a load-balancing-aware scheduling algorithm distributes expert computations across NDP units and GPU to maximize resource utilization. Third, a dataset-free pre-fetching strategy proactively loads frequently accessed experts to minimize activation delays. Experimental results show that our framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches, significantly enhancing MoE inference efficiency in resource-constrained environments.

</details>


### [60] [Hummingbird: SLO-Oriented GPU Preemption at Microsecond-scale](https://arxiv.org/abs/2601.04071)
*Tiancheng Hu,Chenxi Wang,Ting Cao,Jin Qin,Lei Chen,Xinyu Xiao,Junhao Hu,Hongliang Tian,Shoumeng Yan,Huimin Cui,Quan Chen,Tao Xie*

Main category: cs.DC

TL;DR: 本文提出了Hummingbird，一种面向SLO的GPU调度系统，它通过在闭源GPU上实现微秒级抢占并有效利用空闲GPU时间片来解决现有GPU共享技术面临的挑战。实验表明，Hummingbird能够显著提高高优先级任务的SLO达成率，并且在与低优先级任务共存时几乎不降低其性能，同时还能大幅提升低优先级任务的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前的GPU共享方法，如空间和时间共享，在试图提高利用率的同时难以兼顾服务级别目标（SLO）的遵守和效率最大化，特别是在闭源GPU缺乏细粒度任务调度支持的情况下。

Method: 开发了名为Hummingbird的新系统，该系统能够在闭源GPU上实现微秒级别的抢占，并能有效地收集闲置的GPU时间片段。

Result: 评估显示，与最先进的空间和时间共享方法相比，Hummingbird将高优先级任务的SLO达成率分别提高了9.7倍和3.5倍。相比于独占执行，当高优先级任务与低优先级任务一同运行于Hummingbird时，其SLO达成率仅下降不到1%。此外，低优先级任务的吞吐量比最新的时间共享方法高出2.4倍。

Conclusion: Hummingbird在确保SLO的同时极大地提高了GPU使用效率。

Abstract: Existing GPU-sharing techniques, including spatial and temporal sharing, aim to improve utilization but face challenges in simultaneously ensuring SLO adherence and maximizing efficiency due to the lack of fine-grained task scheduling on closed-source GPUs. This paper presents Hummingbird, an SLO-oriented GPU scheduling system that overcomes these challenges by enabling microsecond-scale preemption on closed-source GPUs while effectively harvesting idle GPU time slices. Comprehensive evaluations across diverse GPU architectures reveal that Hummingbird improves the SLO attainment of high-priority tasks by 9.7x and 3.5x compared to the state-of-the-art spatial and temporal-sharing approaches. When compared to executing exclusively, the SLO attainment of the high-priority task, collocating with low-priority tasks on Hummingbird, only drops by less than 1%. Meanwhile, the throughput of the low-priority task outperforms the state-of-the-art temporal-sharing approaches by 2.4x. Hummingbird demonstrates significant effectiveness in ensuring the SLO while enhancing GPU utilization.

</details>


### [61] [Failure-Resilient and Carbon-Efficient Deployment of Microservices over the Cloud-Edge Continuum](https://arxiv.org/abs/2601.04123)
*Francisco Ponce,Simone Gazza,Andrea D'Iapico,Roberto Amadini,Antonio Brogi,Stefano Forti,Saverio Giallorenzo,Pierluigi Plebani,Davide Usai,Monica Vitali,Gianluigi Zavattaro,Jacopo Soldani*

Main category: cs.DC

TL;DR: 本文介绍了一种名为FREEDA的工具链，它能够自动化地在云-边缘连续体上部署具有故障弹性和低碳效率的微服务应用程序。通过适应不断变化的操作条件、资源可用性和可持续性限制，FREEDA能够在保持MSA质量和服务连续性的同时减少碳排放。实验结果表明FREEDA能够通过迁移服务、调整规格选择或重新平衡工作负载来自主地重新配置部署，成功实现了韧性、效率和环境影响之间的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 在异构和动态的云-边缘基础设施上部署基于微服务的应用程序时，需要权衡诸如故障恢复能力、性能和环境可持续性等相互冲突的目标。为了解决这一挑战，提出了FREEDA工具链，旨在自动实现故障恢复能力强且低碳的部署。

Method: FREEDA工具链持续调整部署配置以响应操作条件的变化、资源可得性以及可持续性的约束条件。该方法包括根据需要迁移服务、调整服务实例规模（即“flavour selections”）或者重新分配工作负载。此外，还开发了一个实验套件，利用多种模拟和仿真场景来验证工具链应对现实世界挑战的有效性。

Result: 实验结果显示，FREEDA能够有效地自主重构部署策略，通过服务迁移、调整服务实例大小或重新平衡工作量等方式，成功达成了韧性、效率与环境影响三者间的最优平衡。

Conclusion: FREEDA工具链展示了其在面对资源耗尽、节点故障及碳强度波动等实际情况时，仍能有效保证微服务应用的质量与服务连续性，并同时降低了碳足迹。这表明FREEDA是实现高效、可靠且环保的云-边缘计算解决方案的一个有力工具。

Abstract: Deploying microservice-based applications (MSAs) on heterogeneous and dynamic Cloud-Edge infrastructures requires balancing conflicting objectives, such as failure resilience, performance, and environmental sustainability. In this article, we introduce the FREEDA toolchain, designed to automate the failure-resilient and carbon-efficient deployment of MSAs over the Cloud-Edge Continuum.
  The FREEDA toolchain continuously adapts deployment configurations to changing operational conditions, resource availability, and sustainability constraints, aiming to maintain the MSA quality and service continuity while reducing carbon emissions. We also introduce an experimental suite using diverse simulated and emulated scenarios to validate the effectiveness of the toolchain against real-world challenges, including resource exhaustion, node failures, and carbon intensity fluctuations. The results demonstrate FREEDA's capability to autonomously reconfigure deployments by migrating services, adjusting flavour selections, or rebalancing workloads, successfully achieving an optimal balance among resilience, efficiency, and environmental impact.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [62] [An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS](https://arxiv.org/abs/2601.03430)
*Mohamed Ouf,Shayan Noei,Zeph Van Iterson,Mariam Guizani,Ying Zou*

Main category: cs.SE

TL;DR: 研究对比了1,039个GitHub仓库中的422个OSS4SG项目和617个传统开源软件项目的社区结构、贡献者参与度和编码实践，发现OSS4SG项目拥有更稳定和"粘性"的社区，全年保持一致的参与度，并且依赖核心贡献者进行代码质量和问题解决；而传统开源项目则表现出较高的贡献者流动性和季节性的参与度波动，在问题解决上更多依赖于非核心贡献者。


<details>
  <summary>Details</summary>
Motivation: 理解面向社会公益的开源软件(OSS4SG)项目中社区动态与贡献者模式对于确保这些项目的可持续发展及长期影响至关重要。然而，尽管对常规开源软件已有大量研究，但关于OSS4SG使命驱动性质如何影响其开发实践的知识仍然有限。

Method: 通过大规模实证研究，分析了包括422个OSS4SG项目和617个常规开源软件项目在内的1,039个GitHub仓库，以比较它们在社区结构、贡献者参与度以及编码实践上的差异。

Result: OSS4SG项目形成了显著更加稳定和'粘性'(63.4%)的社区，而常规开源软件项目则更具'吸引力'(75.4%)，吸引着高流动性贡献者。此外，OSS4SG项目整年都显示出一致的活跃度，相比之下，常规开源软件社区活动存在季节性变化。在解决问题方面，OSS4SG项目严重依赖核心贡献者来保证代码质量及处理问题，而常规开源软件项目则主要依靠临时贡献者解决问题，核心贡献者则专注于维护代码质量。

Conclusion: 该研究表明，OSS4SG项目在构建持久稳定的开发者社区方面表现优于传统开源软件项目，并且在维持持续参与度和利用核心成员解决关键任务方面也做得更好。这表明OSS4SG特有的使命导向可能对其开发流程产生积极影响。

Abstract: Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and "sticky" (63.4%) communities, whereas conventional OSS projects are more "magnetic" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality.

</details>


### [63] [CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models](https://arxiv.org/abs/2601.03432)
*Danny Brahman,Mohammad Mahoor*

Main category: cs.SE

TL;DR: 本文提出了一种新的基准测试方法CodeEval，用于全面评估大型语言模型在Python编程24个不同方面的代码生成能力。该数据集覆盖了初、中、高三个熟练程度，并包括基于类和基于函数的问题类型。此外，还开发了一个开源执行框架RunCodeEval，以方便研究人员使用CodeEval进行评估，从而指导提高LLM的编程能力。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型的评价主要集中在常识推理、语言理解和逻辑推理能力上，但在代码生成能力的评估方面存在不足。现有基准数据集无法准确指出模型的具体优缺点，阻碍了针对性改进模型推理合成代码的能力。

Method: 作者们引入了CodeEval，这是一个多维度的基准数据集，旨在严格评估大型语言模型在Python编程中的24个不同方面。此数据集涵盖了从初学者到高级的不同熟练水平，并且包含了详细的题目说明与全面的测试套件。为了促进广泛采用，研究者同时开发了RunCodeEval，一个开源执行框架，它为研究者提供即用型的CodeEval评估流水线。

Result: 通过RunCodeEval框架，研究者可以轻松获取关于模型在复杂度级别、问题类型以及编程类别上的强项和弱项的详细见解。这有助于实现有针对性的评估，并指导大型语言模型编程能力的提升。

Conclusion: 新提出的CodeEval及其配套工具RunCodeEval为评估及改进大型语言模型在Python编程方面的表现提供了有力支持，推动了相关领域的进步。

Abstract: Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies.

</details>


### [64] [Bootstrapping Code Translation with Weighted Multilanguage Exploration](https://arxiv.org/abs/2601.03512)
*Yuhan Wu,Huan Zhang,Wei Cheng,Chen Shen,Jingyue Yang,Wei Hu*

Main category: cs.SE

TL;DR: 提出了一种名为BootTrans的方法，通过利用测试套件的功能不变性和跨语言可移植性来解决代码翻译中的数据稀缺和平行数据优化不平衡问题。实验结果表明该方法在多个翻译方向上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 代码翻译面临的主要挑战是平行数据的缺乏以及处理不同语言对时的优化不平衡。

Method: BootTrans方法通过使用测试套件作为通用验证预言，并引入了包含种子池和探索池的双池架构以逐步扩展训练数据。此外，设计了一种语言感知权重机制，根据相关语言间的相对表现动态优先考虑更难的翻译方向。

Result: 在HumanEval-X和TransCoder-Test基准上的广泛实验显示，在所有翻译方向上相比基线大语言模型有显著改进。消融研究验证了引导和加权组件的有效性。

Conclusion: BootTrans提供了一种有效的方法来克服代码翻译中遇到的数据稀缺和平行数据优化不平衡的问题，通过创新地利用测试套件与特定设计的语言感知机制展示了其优越性能。

Abstract: Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components.

</details>


### [65] [Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day](https://arxiv.org/abs/2601.03513)
*Yi Wang,Zhenting Huang,Zhaohan Ding,Ruoxue Liao,Yuan Huang,Xinzijian Liu,Jiajun Xie,Siheng Chen,Linfeng Zhang*

Main category: cs.SE

TL;DR: Deploy-Master是一个用于大规模工具发现、构建规范推断、基于执行的验证和发布的全流程工作流，它能够将异构开源库转化为可运行的容器化功能，并在一天内为50,112个科学工具创建了可重复使用的运行时环境。此外，它还提供了关于部署过程中的吞吐量、成本概况、失败面和规格不确定性等信息，这些信息只有在大规模下才可见。


<details>
  <summary>Details</summary>
Motivation: 现有的开源科学软件虽然丰富，但大多数工具编译困难、配置复杂且难以复用，这限制了科研工作的可重复性、大规模评估以及科学工具与现代AI-for-Science (AI4S) 和自主工作流的实际整合。

Method: 提出了一种名为Deploy-Master的工作流，旨在实现大规模工具发现、构建规范推断、基于执行的验证及发布。该流程首先从超过50万个公共仓库中筛选出符合条件的可执行工具候选者，然后将这些异构开源仓库转换为基于实际执行而非文档声明的可运行、容器化的功能。

Result: 在一天时间内，尝试构建了52,550个工具，并成功为其中50,112个科学工具建立了可复制的运行环境。每个成功的工具都通过最小可执行命令进行验证，并注册到SciencePedia以供搜索和重用。

Conclusion: 除了提供可运行的工具外，研究还报告了涉及5万个工具规模的部署追踪情况，揭示了仅在大规模操作下才会显现的吞吐量、成本特征、失败模式和规范不确定性等问题。这些结果解释了为什么科学软件难以运作，并提倡共享、可观测的执行基础作为可扩展AI4S和自主科学的基础。

Abstract: Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.
  We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.
  Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science.

</details>


### [66] [On the Robustness of Fairness Practices: A Causal Framework for Systematic Evaluation](https://arxiv.org/abs/2601.03621)
*Verya Monjezi,Ashish Kumar,Ashutosh Trivedi,Gang Tan,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 本文探讨了机器学习算法在关键决策中的公平性问题，特别是针对社会经济应用中可能对边缘化群体造成的不公平影响。文章评估了当前推荐的公平性实践的有效性和可靠性，特别是在标签错误、数据缺失或分布变化情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习算法越来越多地被用于金融、刑事司法和自动驾驶等社会经济领域的关键决策制定，它们有可能基于其数据驱动与模式寻找的本质，导致机会、利益、资源或信息在不同人口群体之间不均衡分配的问题，进而潜在地伤害到边缘化社区。为了应对这些公平性顾虑，软件工程及机器学习界已努力制定了创建公平机器学习软件的最佳实践。但是，负责开发数据驱动系统的软件专业人士能否可靠地依赖这些建议？以及当面临错误标签、数据缺失或分布变化时，这些实践能多好地泛化？这些问题构成了本文的核心主题。

Method: 该论文采用了理论分析与实证研究相结合的方法，通过构建实验场景来模拟现实世界中可能出现的数据质量问题（如标签错误、数据缺失）和环境变化（如分布漂移），以此测试现有公平性干预措施的有效性。

Result: 研究发现，在理想条件下提出的公平性改进措施在面对实际部署中常见的数据挑战时表现不佳。特别地，当存在标签噪声、缺失值或者训练与测试数据间存在显著差异时，这些方法往往无法达到预期的公平性目标。

Conclusion: 尽管当前为促进机器学习系统公平性而提出的一系列最佳实践具有重要价值，但它们在处理真实世界中遇到的各种复杂情况方面仍存在局限性。因此，未来的研究需要更加关注如何使公平性策略更具鲁棒性和适应性，以更好地服务于所有用户群体。

Abstract: Machine learning (ML) algorithms are increasingly deployed to make critical decisions in socioeconomic applications such as finance, criminal justice, and autonomous driving. However, due to their data-driven and pattern-seeking nature, ML algorithms may develop decision logic that disproportionately distributes opportunities, benefits, resources, or information among different population groups, potentially harming marginalized communities. In response to such fairness concerns, the software engineering and ML communities have made significant efforts to establish the best practices for creating fair ML software. These include fairness interventions for training ML models, such as including sensitive features, selecting non-sensitive attributes, and applying bias mitigators. But how reliably can software professionals tasked with developing data-driven systems depend on these recommendations? And how well do these practices generalize in the presence of faulty labels, missing data, or distribution shifts? These questions form the core theme of this paper.

</details>


### [67] [Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test](https://arxiv.org/abs/2601.03640)
*Mohd Ariful Haque,Kishor Datta Gupta,Mohammad Ashiqur Rahman,Roy George*

Main category: cs.SE

TL;DR: 本文介绍了一个旨在测试大语言模型在代码生成任务中数据转录准确性的基准，通过要求模型将高精度十进制常量转换为Python代码并执行简单聚合计算来实现。该基准强调数据完整性，并且作为现有代码生成评估的补充压力测试。


<details>
  <summary>Details</summary>
Motivation: 实际软件任务中需要精确地将给定的数据转录到代码里，比如加密常数、协议测试向量等。由于这些任务对操作敏感，小的疏漏或更改可能会导致程序虽然语法正确但存在潜在问题。因此，本文提出了一个最小化的转录到代码的基准，以隔离这种可靠性问题。

Method: 论文提出了一种方法，即给定一组高精度的十进制常量，要求模型生成包含这些常量并且能够进行简单聚合计算的Python代码。介绍了提示变体、基于字符串精确匹配的评估协议以及用于描述状态跟踪和长期生成失败的分析框架。

Result: 通过这个紧凑的压力测试，可以更好地理解大型语言模型在处理需要高度数据完整性的代码生成任务时的表现。这有助于识别模型在状态保持和长距离生成方面存在的问题。

Conclusion: 此基准测试作为一个紧凑的压力测试工具，补充了现有的侧重于算法推理而非数据完整性的代码生成评估方式，强调了对于保证数据准确无误转录的重要性。

Abstract: Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning.

</details>


### [68] [Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study](https://arxiv.org/abs/2601.03780)
*Md Ahasanuzzaman,Bram Adams,Emad Fallahzadeh,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper examines the representativeness of code generation benchmarks for Large Language Models (LLMs) by analyzing the coverage of Knowledge Units (KUs) in comparison to real-world Python projects. A novel prompt-based LLM framework was introduced to rebalance KU distributions in benchmarks, resulting in a more realistic assessment of LLMs' coding abilities. The augmented benchmarks revealed a notable decrease in LLM performance, highlighting the limitations of previous evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the concern that current benchmarks for assessing LLMs' code generation abilities may not accurately reflect the diversity of programming concepts encountered in actual software development. This discrepancy could lead to incomplete or misleading evaluations of LLMs' true capabilities, necessitating a deeper investigation into the representativeness of these benchmarks and the development of methods to improve them.

Method: The researchers conducted an empirical study analyzing the coverage of Knowledge Units (KUs) in two widely used Python benchmarks (HumanEval and MBPP) and compared them with 30 real-world Python projects. To address the identified misalignment, they proposed a prompt-based LLM framework designed to generate KU-based tasks, which were then used to augment the existing benchmarks. This process improved both the KU coverage and the alignment of the benchmarks with real-world usage patterns.

Result: Results showed that each benchmark covered approximately half of the 20 identified KUs, while real-world projects utilized all KUs with a relatively balanced distribution. By employing the newly proposed framework, 440 new tasks were generated, significantly enhancing the benchmarks' KU coverage and achieving over a 60% improvement in distributional alignment. Subsequent LLM evaluations on these enhanced benchmarks demonstrated consistent and statistically significant declines in performance, suggesting that earlier assessments had overestimated the models' proficiency.

Conclusion: The study concludes that existing benchmarks for evaluating LLMs' code generation capabilities, such as HumanEval and MBPP, cover only a limited range of Knowledge Units (KUs) compared to real-world Python projects. The introduction of a prompt-based LLM framework to synthesize KU-based tasks improves the representativeness of these benchmarks, leading to a more realistic evaluation of LLMs. Evaluations on the augmented benchmarks show a significant drop in LLMs' performance, indicating that previous benchmarks overestimated their capabilities. These findings offer guidance for creating more accurate and comprehensive evaluations of LLMs' code generation skills.

Abstract: Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined.
  To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions.
  To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities.

</details>


### [69] [Once Upon a Team: Investigating Bias in LLM-Driven Software Team Composition and Task Allocation](https://arxiv.org/abs/2601.03857)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Amleto Di Salle,Patrizio Pelliccione,Gemma Catolino,Fabio Palomba*

Main category: cs.SE

TL;DR: 研究发现大型语言模型在软件工程任务如团队组成和任务分配中存在系统性偏见，导致技术与领导角色的不均衡分配，强调了需要注重公平性的评估。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被越来越多地用于提高生产力和支持软件工程任务，在处理诸如团队组成和任务分配等社会敏感决策时，它们引发了关于公平性的担忧。尽管已有研究表明LLMs可能再现刻板印象，但这些分析仍处于探索阶段，并且通常单独考察敏感属性。本研究旨在通过分析候选人国籍和代词的综合效应，调查LLMs是否在团队组成和任务分配上表现出偏见。

Method: 使用三种不同的大型语言模型以及3,000次模拟决策来探究国籍与代词对团队选择概率及任务分配的影响。

Result: 结果显示，即使考虑了专业相关因素后，人口统计学特征显著影响了入选几率和任务分配方式；此外，任务分布还反映了刻板印象，即技术和领导职位在不同群体之间分配不均。

Conclusion: 结论指出，大型语言模型加剧了软件工程环境中基于人口统计特征的不公平现象，这突出了实施公平意识评估的重要性。

Abstract: LLMs are increasingly used to boost productivity and support software engineering tasks. However, when applied to socially sensitive decisions such as team composition and task allocation, they raise concerns of fairness. Prior studies have revealed that LLMs may reproduce stereotypes; however, these analyses remain exploratory and examine sensitive attributes in isolation. This study investigates whether LLMs exhibit bias in team composition and task assignment by analyzing the combined effects of candidates' country and pronouns. Using three LLMs and 3,000 simulated decisions, we find systematic disparities: demographic attributes significantly shaped both selection likelihood and task allocation, even when accounting for expertise-related factors. Task distributions further reflected stereotypes, with technical and leadership roles unevenly assigned across groups. Our findings indicate that LLMs exacerbate demographic inequities in software engineering contexts, underscoring the need for fairness-aware assessment.

</details>


### [70] [Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design](https://arxiv.org/abs/2601.03878)
*Giovanni Rosa,David Moreno-Lumbreras,Gregorio Robles,Jesús M. González-Barahona*

Main category: cs.SE

TL;DR: 本研究设计了一个使用CURRANTE（一个Visual Studio Code扩展）的实证研究，该工具通过三个阶段（规范、测试和功能）来指导开发者利用大语言模型辅助代码生成。研究旨在分析人类在规范和测试细化中的干预如何影响LLM生成代码的质量和动态。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地融入软件开发工作流程中，它们在结构化、规范驱动过程中的行为仍不被充分理解。因此，需要进一步探索人机协作模式下这些模型的表现及其对代码质量的影响。

Method: 采用CURRANTE工具，这是一个支持人在环路工作流的Visual Studio Code扩展，用于协助基于大语言模型的代码生成。参与者将解决来自LiveCodeBench数据集的中等难度问题，并记录详细的交互日志、有效性指标（如通过率）、效率指标（如首次通过时间）以及迭代行为。

Result: 研究结果预期能够揭示人类参与规范定义与测试用例优化过程中对大语言模型产出代码质量及开发动态的具体影响。

Conclusion: 通过对人类干预作用于规格说明与测试精炼过程的研究，本论文希望能为下一代融合了人类推理与模型驱动代码生成技术的开发环境设计提供实证依据。

Abstract: Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.

</details>


### [71] [Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures](https://arxiv.org/abs/2601.03988)
*Nicolas Lacroix,Mireille Blay-Fornarino,Sébastien Mosser,Frederic Precioso*

Main category: cs.SE

TL;DR: 本文评估了小型语言模型(SLMs)在理解和分类代码方面的能力，以克服现有方法在从源代码中提取机器学习(ML)流水线阶段时遇到的挑战。研究通过一系列统计测试（如Cochran's Q检验、McNemar's检验和Pearson's卡方检验）来比较不同SLMs的表现，并分析了这些模型如何增进我们对数据科学实践的理解。


<details>
  <summary>Details</summary>
Motivation: 现有的从源代码中抽取机器学习管道结构阶段的方法存在局限性，包括非可扩展的手动标记或不充分支持领域多样性的机器学习分类器。为了解决这些问题，需要更加灵活可靠的方法。因此，文章旨在探索小型语言模型是否能够利用其代码理解和分类能力来应对上述挑战，并进一步推动我们对数据科学实践的理解。

Method: 研究采用了一种验证性研究设计，基于两项针对当前技术局限性具有相关性的参考工作。首先，使用Cochran's Q检验比较了几种不同的小型语言模型。然后，选取表现最佳的模型，通过两次独立的McNemar's检验与参考研究进行对比评估。此外，还进行了额外的Cochran's Q检验，以分析分类定义变化对性能的影响。最后，运用Pearson's卡方检验来进行拟合优度分析，将本研究关于数据科学实践的认识与先前研究结果相比较。

Result: 虽然摘要中没有直接提供具体的结果数值，但可以推测出研究通过一系列统计测试得到了不同小型语言模型之间以及与传统方法相比的性能差异。这可能包括哪些模型在特定任务上表现更好，以及分类定义的变化如何影响模型性能等发现。

Conclusion: 尽管摘要未明确给出结论，但从研究目标来看，这项工作的主要贡献在于证明了小型语言模型在理解代码及促进数据科学实践理解方面的潜力。同时，它也揭示了不同因素（例如使用的模型类型、分类体系的设计）如何影响最终结果。

Abstract: Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.
  Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.
  Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.

</details>


### [72] [Smells Depend on the Context: An Interview Study of Issue Tracking Problems and Smells in Practice](https://arxiv.org/abs/2601.04124)
*Lloyd Montgomery,Clara Lüders,Christian Rahe,Walid Maalej*

Main category: cs.SE

TL;DR: 本研究通过深入访谈26位来自不同组织和行业的有经验的软件工程从业者，探讨了他们在问题跟踪系统(ITSs)中遇到的常见问题及文献中讨论的31种潜在问题实践（称为ITS smells）的相关性。结果发现了14个常见的问题，包括问题可发现性、僵尸问题、工作流膨胀以及缺乏工作流执行等，并指出许多ITS smells要么不出现要么不是问题，强调了解决方案需考虑上下文因素如ITS配置、工作流阶段和团队规模。


<details>
  <summary>Details</summary>
Motivation: 尽管研究人员已经广泛分析了ITS数据来自动化或协助特定活动，例如问题分配、重复检测或优先级预测，但对于开发人员在使用ITS时面临的具体挑战的研究仍然很少。特别是对于软件工程团队在ITS中遇到的问题以及某些做法和变通方法（比如留空'优先级'字段）何时被视为有问题知之甚少。为了填补这一空白，本研究旨在探索这些问题及其背后的原因。

Method: 采用了深度访谈的方法，与来自不同组织和行业背景下的26名资深软件工程实践者进行了交流。通过对访谈记录应用主题分析法，识别出参与者们普遍遇到的问题以及他们对文献中提到的31种潜在问题实践的看法。

Result: 研究确定了14个常见的问题点，其中包括：问题查找困难、僵尸问题的存在、过度复杂的工作流程设计以及缺乏有效执行工作流程的现象。此外还发现，很多之前文献中提及的ITS smells实际上并不构成问题或者根本不会发生。研究建议，解决这些问题需要考虑到诸如ITS系统配置、当前所处的工作流阶段及团队规模等因素的影响。

Conclusion: 问题跟踪系统中的问题和不良实践高度依赖于具体的上下文环境。提出了可能的工具解决方案以配置、监控并可视化展示这些不良实践，从而帮助应对上述挑战。

Abstract: Issue Tracking Systems (ITSs) enable software developers and managers to collect and resolve issues collaboratively. While researchers have extensively analysed ITS data to automate or assist specific activities such as issue assignments, duplicate detection, or priority prediction, developer studies on ITSs remain rare. Particularly, little is known about the challenges Software Engineering (SE) teams encounter in ITSs and when certain practices and workarounds (such as leaving issue fields like "priority" empty) are considered problematic. To fill this gap, we conducted an in-depth interview study with 26 experienced SE practitioners from different organisations and industries. We asked them about general problems encountered, as well as the relevance of 31 ITS smells (aka potentially problematic practices) discussed in the literature. By applying Thematic Analysis to the interview notes, we identified 14 common problems including issue findability, zombie issues, workflow bloat, and lack of workflow enforcement. Participants also stated that many of the ITS smells do not occur or are not problematic. Our results suggest that ITS problems and smells are highly dependent on context factors such as ITS configuration, workflow stage, and team size. We also discuss potential tooling solutions to configure, monitor, and visualise ITS smells to cope with these challenges.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [73] [Transforming Video Subjective Testing with Training, Engagement, and Real-Time Feedback](https://arxiv.org/abs/2601.04184)
*Kumar Rahul,Sriram Sethuraman,Andrew Segall,Yixu Chen*

Main category: cs.MM

TL;DR: 本文提出了一种新的框架，通过改进评分者培训、实时注意力打分机制以及高效的成对比较流程来提高主观视频质量评估的准确性和可靠性。实验结果表明，这种三阶段方法（包括培训、测验和带反馈的测试）能显著提高数据质量，并有助于训练更好的客观视频质量度量标准。


<details>
  <summary>Details</summary>
Motivation: 传统的主观视频质量评估方法在捕捉细微感知差异和保证用户输入可靠性方面存在局限性。为了解决这些问题，作者提出了一个综合框架，旨在优化评分者的培训过程、增强其在评估过程中的注意力，并通过更有效的成对比较方式减少所需比较次数，从而提高评估的质量与效率。

Method: 该研究设计了一个包含自动化培训测验、实时注意力评分机制以及基于链式的高效成对比较程序在内的集成框架。参与者首先完成关于视频质量指标的学习并通过自动测验；接着，在测试过程中利用“黄金”视频对监控并强化评分者的注意力集中度，对于注意力分散的情况给予惩罚；最后采用高效的成对比较方法获得以Just-Objectionable-Differences (JOD)单位表示的质量得分。

Result: 实验对比了三个组别（无培训、有培训但无反馈、有培训且有反馈）共80名参与者的数据，结果显示培训测验显著提高了黄金单位准确性并降低了平局率，而实时反馈进一步提升了数据质量，产生了最单调的质量评级。新方法尤其在R-Q曲线的高质量部分减少了非单调情况，这通常对应于普通观众偏好的稍微压缩但不那么颗粒感的内容。

Conclusion: 提出的三阶段方法（培训、测验及带有反馈的测试）能够有效改善主观视频质量评估的数据质量，特别是在处理高清晰度视频时表现优异。此外，这种方法还有助于开发更加精准的客观视频质量度量标准。

Abstract: Subjective video quality assessment is crucial for optimizing streaming and compression, yet traditional protocols face limitations in capturing nuanced perceptual differences and ensuring reliable user input. We propose an integrated framework that enhances rater training, enforces attention through real-time scoring, and streamlines pairwise comparisons to recover quality scores with fewer comparisons. Participants first undergo an automated training quiz to learn key video quality indicators (e.g., compression artifacts) and verify their readiness. During the test, a real-time attention scoring mechanism, using "golden" video pairs, monitors and reinforces rater focus by applying penalties for lapses. An efficient chain-based pairwise comparison procedure is then employed, yielding quality scores in Just-Objectionable-Differences (JOD) units. Experiments comparing three groups (no training, training without feedback, and training with feedback) with 80 participants demonstrate that training-quiz significantly improves data quality in terms of golden unit accuracy and reduces tie rate, while real-time feedback further improves data quality and yields the most monotonic quality ratings. The new training, quiz, testing with feedback, 3-phase approach can significantly reduce the non-monotonic cases on the high quality part of the R-Q curve where normal viewer typically prefer the slightly compressed less-grainy content and help train a better objective video quality metric.

</details>
