<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.IR](#cs.IR) [Total: 16]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code](https://arxiv.org/abs/2602.11209)
*Ziyi Yang,Kalit Inani,Keshav Kabra,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.SE

TL;DR: 提出了一种新的混合测试框架SAFuzz，利用LLM引导的自适应模糊测试来更有效地检测算法漏洞。相比现有方法，SAFuzz在提高漏洞识别精度的同时减少了时间成本，并且与现有的单元测试生成方法结合使用时能进一步提高错误检测率。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码助手加速了软件开发过程，当前的测试框架难以跟上由此产生的大量AI生成代码的步伐。传统的模糊测试技术往往均匀分配资源，缺乏对算法漏洞模式的语义理解，导致资源使用效率低下以及遗漏潜在漏洞。

Method: 设计并实现了一个名为SAFuzz的混合测试框架，该框架结合了基于提示的行为多样化、带有问题特定预言机的捕获生成器以及一个基于大型语言模型（LLM）的预测器，以实现自适应资源分配和动态提前停止。

Result: 在CSES算法问题上的评估显示，相比于最先进方法GreenFuzz, SAFuzz将漏洞区分精度从77.9%提升到了85.7%，同时时间成本降低了1.71倍。此外，当与现有单元测试生成方法相结合时，错误检测召回率从67.3%增加到79.5%。

Conclusion: 通过引入SAFuzz，研究者们展示了一种有效增强AI生成代码安全性分析能力的新途径，其不仅提高了发现算法中潜在安全问题的能力，还优化了整体测试流程的效率。

Abstract: While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.

</details>


### [2] [Patient Digital Twins for Chronic Care: Technical Hurdles, Lessons Learned, and the Road Ahead](https://arxiv.org/abs/2602.11223)
*Micheal P. Papazoglou,Bernd J. Krämer,Mira Raheem,Amal Elgammal*

Main category: cs.SE

TL;DR: 本研究探讨了患者医疗数字孪生（PMDT）作为慢性病管理的一种新范式，通过综合临床、基因组、生活方式及生活质量数据实现。基于QUALITOP肿瘤学研究和分布式AI平台的初步实施表明，尽管存在标准化对接、隐私治理等挑战，但PMDT在个性化医疗预测分析方面展现出巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 当前健康系统对于慢性疾病管理存在碎片化和反应性的问题，而患者医疗数字孪生（PMDT）提供了一种新的方法论，旨在通过整合各类患者相关数据来改善这一状况。

Method: 研究采用了基于本体论建模的方法以及联邦分析试点项目，结合了QUALITOP肿瘤学案例研究与一个分布式人工智能平台的应用实例来进行探索。

Result: 结果显示出利用PMDT进行自动化推理和预测分析的技术优势，并且指出了在遵循HL7 FHIR和OMOP标准的同时，确保隐私保护措施到位的重要性。此外，还强调了扩大联邦查询规模和开发用户友好型临床界面的需求。

Conclusion: 通过反思这些实践经验，研究人员为软件工程师提供了可操作的见解，并指出领域特定语言（DSLs）和模型驱动工程等技术在未来发展中扮演着重要角色，以推动PMDT成为可信且适应性强的慢性护理生态系统。

Abstract: Chronic diseases constitute the principal burden of morbidity, mortality, and healthcare costs worldwide, yet current health systems remain fragmented and predominantly reactive. Patient Medical Digital Twins (PMDTs) offer a paradigm shift: holistic, continuously updated digital counterparts of patients that integrate clinical, genomic, lifestyle, and quality-of-life data. We report early implementations of PMDTs via ontology-driven modeling and federated analytics pilots. Insights from the QUALITOP oncology study and a distributed AI platform confirm both feasibility and challenges: aligning with HL7 FHIR and OMOP standards, embedding privacy governance, scaling federated queries, and designing intuitive clinician interfaces. We also highlight technical gains, such as automated reasoning over multimodal blueprints and predictive analytics for patient outcomes. By reflecting on these experiences, we outline actionable insights for software engineers and identify opportunities, such as DSLs and model-driven engineering, to advance PMDTs toward trustworthy, adaptive chronic care ecosystems.

</details>


### [3] [Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data](https://arxiv.org/abs/2602.11411)
*Yang Liu,Armstrong Foundjem,Xingfang Wu,Heng Li,Foutse Khomh*

Main category: cs.SE

TL;DR: 该研究通过使用扰动数据集对大型语言模型进行微调，以提高其在编码相关任务中的鲁棒性。结果表明，虽然这会轻微降低模型性能，但能显著增强其对抗输入扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了确保大型语言模型在处理多样化输入时能够抵抗潜在漏洞，避免产生错误或不安全的代码输出，本研究旨在探索通过微调扰动数据集来提升这些模型对于对抗性输入的鲁棒性。

Method: 研究者系统地评估了通过字符级、词级和句子级扰动的数据集对LLMs进行微调的效果，并将结果与基准模型及未扰动数据集上微调过的模型进行了比较。

Result: 实验发现，利用扰动数据集微调LLMs可以显著提高模型的鲁棒性（RD通常下降约4%-6%），特别是对于那些鲁棒性较弱的模型。不过，这样做通常会导致模型性能略有下降（pass@1通常下降约1%-3%），尽管有时也能观察到性能有所提升。

Conclusion: 用扰动数据对LLMs进行微调可以在一定程度上牺牲性能的前提下有效提高模型的鲁棒性，突出了在编码应用中平衡LLMs鲁棒性和性能的重要性。

Abstract: Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.
  Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.
  Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.
  Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\% - 6\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\% - 3\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.
  Conclusion \& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.

</details>


### [4] [A Grounded Theory of Debugging in Professional Software Engineering Practice](https://arxiv.org/abs/2602.11435)
*Haolin Li,Michael Coblenz*

Main category: cs.SE

TL;DR: 本研究通过定性研究方法，观察了专业开发者和直播编码者在实际代码库中处理调试任务的过程。研究提出了一种基于迭代诊断过程的调试理论，强调了开发者如何根据代码库上下文、复杂性和熟悉度调整其导航和执行策略，并利用外部资源来构建系统的心理模型。这一理论为工具设计和软件工程教育提供了关于调试实践的人为中心视角。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究记录了调试策略和工具使用情况，但对于有经验的开发人员如何在大型真实世界的代码库中推理错误，尚缺乏理论解释。为了填补这一空白，该研究旨在探索专业开发者在调试过程中如何思考和行动。

Method: 采用扎根理论方法进行了一项定性研究，观察了7名职业开发者及5名职业直播编码者在其个人代码库上完成17个调试任务的情况。

Result: 提出了一个将调试视为结构化、迭代式诊断过程的理论，在此过程中程序员更新系统的心智模型以指导信息收集。开发者通过交替使用导航与执行策略，采用前向和后向追踪推理模式，并根据代码库背景、复杂度和个人熟悉程度调整这些方法。此外，他们还会搜集外部资源作为代码证据的补充，而他们的经验使他们能够系统地构建心智模型。

Conclusion: 贡献了一个关于专业调试的扎根理论，揭示了实践中以人为本的维度，这对工具设计和软件工程教育具有重要意义。

Abstract: Debugging is a central yet complex activity in software engineering. Prior studies have documented debugging strategies and tool usage, but little theory explains how experienced developers reason about bugs in large, real-world codebases. We conducted a qualitative study using a grounded theory approach. We observed seven professional developers and five professional live-coding streamers working on 17 debugging tasks in their own codebases, capturing diverse contexts of debugging. We theorize debugging as a structured, iterative diagnostic process in which programmers update a mental model of the system to guide information gathering. Developers gather information by alternating between navigation and execution strategies, employing forward and backward tracing modes of reasoning and adapting these approaches according to codebase context, complexity, and familiarity. Developers also gather external resources to complement code-based evidence, with their experience enabling them to systematically construct a mental model. We contribute a grounded theory of professional debugging that surfaces the human-centered dimensions of the practice, with implications for tool design and software engineering education.

</details>


### [5] [Search-Based Quantum Program Testing via Commuting Pauli String](https://arxiv.org/abs/2602.11487)
*Asmar Muqeet,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: 本文提出了一种基于搜索的量子程序测试方法SB-QOPS，通过可交换泡利串重新定义了测试用例，并引入了一个以测量为中心的预言机。该方法提高了测试预算的利用效率，并增加了发现细微故障的可能性。大规模实证评估显示，在真实量子计算机和模拟器上，对于多达29个量子位的量子电路，SB-QOPS在三种不同量子计算平台上的故障检测得分为100%。


<details>
  <summary>Details</summary>
Motivation: 现有的量子软件测试方法依赖于简单的测试输入和统计预言机、昂贵的程序规范以及在实际量子计算机上有限的验证。为了应对这些挑战，作者提出了SB-QOPS。

Method: SB-QOPS扩展了之前提出的QOPS方法，将测试用例重新定义为泡利串形式，并引入了一个利用它们交换属性的测量中心预言机。此外，通过系统地探索期望值为基础的适应度函数所定义的搜索空间来提高测试预算使用率并增加揭示潜在错误的机会。

Result: 实验结果表明，SB-QOPS显著优于QOPS，在IBM、IQM和Quantinuum三个不同的量子计算平台上，对最多包含29个量子比特的电路实现了100%的故障检测率。

Conclusion: SB-QOPS为量子程序提供了一种有效的测试方法，能够减少对完整程序规范的需求同时提高测试效率与质量。

Abstract: Quantum software testing is important for reliable quantum software engineering. Despite recent advances, existing quantum software testing approaches rely on simple test inputs and statistical oracles, costly program specifications, and limited validation on real quantum computers. To address these challenges, we propose SB-QOPS, a search-based quantum program testing approach via commuting Pauli strings. SB-QOPS, as a direct extension to a previously proposed QOPS approach, redefines test cases in terms of Pauli strings and introduces a measurement-centric oracle that exploits their commutation properties, enabling effective testing of quantum programs while reducing the need for full program specifications. By systematically exploring the search space through an expectation-value-based fitness function, SB-QOPS improves test budget utilization and increases the likelihood of uncovering subtle faults. We conduct a large-scale empirical evaluation on quantum circuits of up to 29 qubits on real quantum computers and emulators. We assess three search strategies: Genetic Algorithm, Hill Climbing, and the (1+1) Evolutionary Algorithm, and evaluate SB-QOPS under both simulated and real noisy conditions. Experiments span three quantum computing platforms: IBM, IQM, and Quantinuum. Results show that SB-QOPS significantly outperforms QOPS, achieving a fault-detection score of 100% for circuits up to 29 qubits, and demonstrating portability across quantum platforms.

</details>


### [6] [Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond](https://arxiv.org/abs/2602.11671)
*Minh Le-Anh,Huyen Nguyen,Khanh An Tran,Nam Le Hai,Linh Ngo Van,Nghi D. Q. Bui,Bach Le*

Main category: cs.SE

TL;DR: Hydra, a novel repository-level code generation framework, improves upon existing Retrieval-Augmented Generation (RAG) methods by incorporating a structure-aware indexing strategy and a dependency-aware retriever, leading to state-of-the-art performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to enhance the effectiveness of large language models for code (CodeLLMs) in repository-level settings, where they face challenges due to cross-file dependencies and structural context. The authors aim to address the limitations of current RAG approaches that often fail to maintain the coherence between code units and miss out on functionally relevant dependencies.

Method: The method introduced in this paper is called Hydra, which is a repository-level code generation framework. It consists of three main components: 1) A structure-aware indexing strategy that maps repositories into hierarchical trees, thereby preserving the code's structure and dependencies. 2) A lightweight dependency-aware retriever (DAR) designed to identify and fetch true dependencies needed by a target function. 3) A hybrid retrieval mechanism that integrates DAR with similarity-based retrieval to offer both necessary building blocks and practical usage examples.

Result: Experiments conducted on the DevEval and RepoExec benchmarks, which require the implementation of functions from real-world repositories with complex contexts, demonstrate that Hydra achieves state-of-the-art performance. Specifically, it surpasses the strongest baseline by more than 5% in Pass@1 accuracy and enables smaller CodeLLMs to perform as well as or better than much larger models using conventional retrievers.

Conclusion: In conclusion, the paper presents Hydra, an innovative approach to repository-level code generation that significantly outperforms existing methods. By focusing on the structural and dependency aspects of code, Hydra not only improves the accuracy of generated code but also allows for more efficient use of computational resources through the successful application of smaller models.

Abstract: Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.

</details>


### [7] [WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements](https://arxiv.org/abs/2602.11724)
*Xiwen Teoh,Yun Lin,Duc-Minh Nguyen,Ruofei Ren,Wenjie Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: This paper introduces WebTestPilot, an LLM-based agent for end-to-end web testing that addresses challenges of implicit and probabilistic inference by symbolizing GUI elements and translating natural language specifications into steps with inferred conditions. It achieves a 99% task completion rate, 96% precision, and 96% recall in bug detection, outperforming the best baseline.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the technical challenges faced by visual language model (VLM) agents in end-to-end web testing, specifically the implicit oracle inference challenge and the probabilistic inference challenge, which can lead to uncertainty in distinguishing between hallucinations and real application bugs. The goal is to create a more reliable and accurate testing tool that can handle these issues effectively.

Method: WebTestPilot uses a symbolization layer to detect and represent critical GUI elements as symbols and translates natural language requirements into a sequence of steps, each equipped with pre- and post-conditions over the symbols. This approach allows for the validation of implicit requirements and the capturing of data, temporal, and causal dependencies. Additionally, a benchmark of bug-injected web applications was created to evaluate the performance of the NL-to-E2E testing approach.

Result: WebTestPilot achieved a 99% task completion rate, 96% precision, and 96% recall in bug detection, significantly outperforming the best baseline with +70% in precision and +27% in recall. The system demonstrated generalizability across various natural language inputs and model scales.

Conclusion: WebTestPilot demonstrates significant improvements in the reliability and accuracy of end-to-end web testing using natural language requirements, by overcoming the challenges associated with implicit and probabilistic inferences. The results suggest that it is a robust solution for automating web testing while maintaining high standards of precision and recall.

Abstract: Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.
  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.

</details>


### [8] [Leveraging Language Models to Discover Evidence-Based Actions for OSS Sustainability](https://arxiv.org/abs/2602.11746)
*Nafiz Imtiaz Khan,Vladimir Filkov*

Main category: cs.SE

TL;DR: 本研究通过使用大语言模型(LLMs)作为软件工程(SE)文献中的证据挖掘工具，设计了一种RAG管道和双层提示策略，以从829篇ICSE和FSE论文中提取研究行动项(ReACTs)，即简洁且与具体开源软件(OSS)实践相关的建议。最终获得了1,312条符合严格质量标准的ReACTs，并被组织成可以与项目信号工具（如APEX）相连接的面向实践类别，为OSS项目朝向可持续性发展提供基于证据的指导。


<details>
  <summary>Details</summary>
Motivation: 虽然现有工作已经能够准确预测开源软件(OSS)项目的可持续性，但这些模型往往缺乏直接可操作性，因为它们的特点通常是高层次的社会技术信号。同时，数十年的经验软件工程研究积累了大量关于改善项目健康的实际做法的证据，但这些证据尚未得到充分利用。

Method: 研究人员开发了一个RAG管道和两层提示策略来从软件工程文献中提取研究行动项(ReACTs)。第一层涉及系统地探索开放式LLM及提示技术，选择最佳组合来从论文中得出候选ReACTs；第二层则通过后续提示过滤错误信息、提取影响和证据，并评估合理性和准确性。

Result: 该方法生成了总共1,922个ReACTs，其中1,312个满足严格的品质要求，并被归类整理成与特定OSS实践相关联的面向实践类别。这些类别能够与像APEX这样的工具所提供的项目信号相连接。

Conclusion: 本研究提出的方法为将分散的研究成果转化为结构化、基于证据的行动指南提供了可重复且可扩展的方式，旨在帮助开源软件项目实现可持续性。

Abstract: When successful, Open Source Software (OSS) projects create enormous value, but most never reach a sustainable state. Recent work has produced accurate models that forecast OSS sustainability, yet these models rarely tell maintainers what to do: their features are often high-level socio-technical signals that are not directly actionable. Decades of empirical software engineering research have accumulated a large but underused body of evidence on concrete practices that improve project health.
  We close this gap by using LLMs as evidence miners over the SE literature. We design a RAG-pipeline and a two-layer prompting strategy that extract researched actionables (ReACTs): concise, evidence-linked recommendations mapping to specific OSS practices. In the first layer, we systematically explore open LLMs and prompting techniques, selecting the best-performing combination to derive candidate ReACTs from 829 ICSE and FSE papers. In the second layer, we apply follow-up prompting to filter hallucinations, extract impact and evidence, and assess soundness and precision.
  Our pipeline yields 1,922 ReACTs, of which 1,312 pass strict quality criteria and are organized into practice-oriented categories connectable to project signals from tools like APEX. The result is a reproducible, scalable approach turning scattered research findings into structured, evidence-based actions guiding OSS projects toward sustainability.

</details>


### [9] [Verifiable Provenance of Software Artifacts with Zero-Knowledge Compilation](https://arxiv.org/abs/2602.11887)
*Javier Ron,Martin Monperrus*

Main category: cs.SE

TL;DR: 本文提出了一种基于零知识虚拟机(zkVM)的可验证源代码来源的新方法。通过在zkVM中执行编译器，系统生成编译输出和一个加密证明，确保编译过程使用了指定的源代码和编译器。实验结果表明该方法能够有效防止多种攻击方式，并适用于实际软件。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证源代码来源技术如可重复构建需要复杂的工具链匹配与环境重现，实施起来颇具挑战性。为了克服这一难题并提供更强的安全保障，作者提出了利用零知识虚拟机进行编译的新方案。

Method: 通过将编译过程置于零知识虚拟机内部执行，从而同时产生编译后的二进制文件及一个加密证明来证实编译确实依据给定的源代码和编译器完成。研究团队采用RISC Zero zkVM与ChibiCC C编译器进行了概念验证实现。

Result: 对200个合成程序以及来自OpenSSL和libsodium项目的共52个源文件进行了评估。所有针对编译器替换、源码篡改、输出更改和重放攻击的恶意测试均被成功阻止。

Conclusion: 基于zkVM的编译方法不仅为真实世界软件提供了可行的解决方案，而且在抵御各种安全威胁方面展现了显著优势。

Abstract: Verifying that a compiled binary originates from its claimed source code is a fundamental security requirement, called source code provenance. Achieving verifiable source code provenance in practice remains challenging. The most popular technique, called reproducible builds, requires difficult matching and reexecution of build toolchains and environments. We propose a novel approach to verifiable provenance based on compiling software with zero-knowledge virtual machines (zkVMs). By executing a compiler within a zkVM, our system produces both the compiled output and a cryptographic proof attesting that the compilation was performed on the claimed source code with the claimed compiler. We implement a proof-of-concept implementation using the RISC Zero zkVM and the ChibiCC C compiler, and evaluate it on 200 synthetic programs as well as 31 OpenSSL and 21 libsodium source files. Our results show that zk-compilation is applicable to real-world software and provides strong security guarantees: all adversarial tests targeting compiler substitution, source tampering, output manipulation, and replay attacks are successfully blocked.

</details>


### [10] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation](https://arxiv.org/abs/2602.11904)
*Weixing Zhang,Bowen Jiang,Yuhong Fu,Anne Koziolek,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（如Claude Sonnet 4.5和GPT-5.2）在文本DSLs的语法与实例共同进化中的潜力，发现它们在小规模案例中表现良好，但在处理较大规模实例时性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着软件语言为了添加新功能等原因而不断演化，原本符合这些语法的文本实例可能会变得过时。虽然模型驱动工程提供了许多技术来使模型与元模型变更同步演变，但这些方法并不适用于文本DSL，并且可能丢失诸如布局和注释等对人类重要的信息。

Method: 使用Claude Sonnet 4.5和GPT-5.2两种大型语言模型，在十个案例语言上进行了十次运行测试，评估了正确性和人类相关信息保存情况。

Result: 小型案例中表现强劲（对于需要修改少于20行的实例，准确率和召回率均≥94%），但随着规模增大性能下降：Claude在40行时保持85%召回率，而GPT无法处理最大实例。响应时间随实例大小显著增加，且语法演化复杂度及删除粒度比变更类型更影响性能。

Conclusion: 这项研究表明基于大型语言模型的共演进在特定情况下是有效的，但也揭示了当前存在的局限性。

Abstract: Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.

</details>


### [11] [Improving Code Generation via Small Language Model-as-a-judge](https://arxiv.org/abs/2602.11911)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 本研究训练了多个最新的小型语言模型（SLMs）作为代码正确性评判者，并评估了它们区分正确与错误实现的能力。结果表明，即使不使用基于执行的信息，现代SLMs的表现也优于RankEF；当用作代码排序器时，这些模型比RankEF获得了更高的性能增益，并且能够以更低的成本与大5到25倍的大型语言模型（LLMs）竞争。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自动化代码生成方面表现出色，但对于不太常见或领域特定的语言来说，其表现可能不佳，促使企业开发内部代码生成器。虽然开源模型可以为此目的进行训练，但只有具有数十亿参数的大型语言模型才能与商业工具相匹敌，这需要昂贵的训练和部署成本。最近的研究提出了一种方法，通过生成多个候选解决方案并利用另一个小型模型选择最有可能正确的那个来支持代码生成。Sun等人提出的RankEF模型就是一个例子，但它没有评估T5排名器的分类准确性，即它将正确实现误判为不正确或反之的概率，这引发了关于语言模型作为其他任务（如自动代码审查）中的代码正确性评判者的可靠性的问题。此外，他们的实验涉及的是相对较旧的模型，使得不清楚这种方法是否仍然有助于公司以较低成本训练出与大规模语言模型性能相当的自己的代码生成器。

Method: 研究人员首先训练了数个最新的小型语言模型作为代码正确性评判者。接着，他们评估了这些模型在区分正确与错误代码实现上的能力。最后，研究还探索了当这些模型被用作代码排序器时，相比于先前的工作（如RankEF）以及更大的语言模型，在性能方面的提升情况。

Result: 结果显示，现代的小型语言模型即使不依赖于基于执行的信息也能超越RankEF的表现。当这些模型应用于代码排序时，不仅实现了比RankEF更高的性能增长，而且还能以远低于成本的方式与规模大得多的大型语言模型竞争。

Conclusion: 这项研究表明，通过适当训练，较新的小型语言模型可以在代码生成任务中发挥重要作用，尤其是对于那些希望以较低成本获得接近顶级性能的企业而言。这为开发高效的代码生成解决方案提供了新的视角。

Abstract: Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.

</details>


### [12] [Studying Quality Improvements Recommended via Manual and Automated Code Review](https://arxiv.org/abs/2602.11925)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 本研究对比了人类评审员和基于深度学习的代码审查工具（以ChatGPT-4为代表）在建议代码质量改进方面的异同。结果显示，虽然ChatGPT推荐的代码变更数量是人类评审员的大约2.4倍，但它仅能识别出人类指出的质量问题中的10%。然而，ChatGPT额外提供的评论中有约40%指向了有意义的质量问题。这表明，当前基于DL的代码审查可以作为人类手动审查之外的一个补充质量检查手段，但尚不能完全替代人类审查或节省审查时间。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索基于深度学习的方法在自动化代码审查中能否像人类评审员一样有效地推荐代码质量改进措施。

Method: 通过一个基于挖掘的研究方法收集并手动检查了739条由人类评审员提出的关于240个PR（Pull Request）的改进建议，并对这些建议进行了分类。随后使用ChatGPT-4对相同的PR进行代码审查，并将其推荐的质量改进与人类评审员的建议进行了比较。

Result: 结果表明，尽管ChatGPT平均推荐的代码变更数量比人类多约2.4倍，但它只能发现人类报告的质量问题中的10%。不过，ChatGPT生成的额外评论中大约有40%确实指出了有价值的质量问题。

Conclusion: 结论是基于深度学习的代码审查技术目前可作为人类手动审查过程的一个补充质量检查手段，但还不足以代替人类评审员的角色，也不应被视为减少代码审查时间的方式。

Abstract: Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.

</details>


### [13] [An Empirical Study of the Imbalance Issue in Software Vulnerability Detection](https://arxiv.org/abs/2602.12038)
*Yuejun Guo,Qiang Hu,Qiang Tang,Yves Le Traon*

Main category: cs.SE

TL;DR: This paper investigates the impact of data imbalance on the performance of deep learning models used for detecting software vulnerabilities, finding that while certain techniques help with specific metrics, none addresses all issues, and suggests areas for further research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why deep learning (DL) based vulnerability detection shows variable performance across different datasets and to explore if the imbalance issue, where vulnerable code instances are far fewer than non-vulnerable ones, is a key factor behind this variability.

Method: A comprehensive empirical study was conducted using nine open-source datasets and two state-of-the-art deep learning models to validate the conjecture that the imbalance issue is central to the variability of model performance in vulnerability detection. The study also evaluated how existing solutions for imbalance issues perform in this context.

Result: The results show that the imbalance issue indeed plays a critical role in the performance of DL models for vulnerability detection. Additionally, it was found that Focal loss improves precision, mean false error and class-balanced loss improve recall, and random over-sampling enhances the F1-measure, but no solution performs well across all metrics.

Conclusion: The study confirms that the imbalance issue is a core problem in deep learning-based vulnerability detection, and different existing solutions for imbalance perform variably across datasets and evaluation metrics. No single solution excels in all metrics, and external influences on these solutions are explored to provide insights for developing new solutions.

Abstract: Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.

</details>


### [14] [ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair](https://arxiv.org/abs/2602.12058)
*Zhiyong Chen,Jialun Cao,Chang Xu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: ModelWisdom 是一个交互式环境，通过可视化和大型语言模型来提高 TLA+ 模型检查的可解释性和可操作性。它提供了模型可视化、图优化、模型摘要和模型修复等功能，以将原始模型检查器输出转化为交互式、可解释的工作流程，从而改善理解并减少调试工作量。


<details>
  <summary>Details</summary>
Motivation: 尽管TLA+中的模型检查提供了强大的正确性保证，但从业者在解释反例、理解大规模状态转换图以及修复故障模型方面仍然面临重大挑战。这些困难源于原始模型检查器输出的有限解释性以及追踪违规行为回源规范所需的大量手动工作。

Method: ModelWisdom 提供了以下功能：(i) 模型可视化，包括用颜色突出显示违规情况、从转换到TLA+代码的点击链接以及违规状态与损坏属性之间的映射；(ii) 图形优化，包括基于树的结构化和节点/边缘折叠以管理大型模型；(iii) 模型摘要，通过大型语言模型（LLMs）总结并解释子图，并执行预处理和部分解释；(iv) 模型修复，提取错误信息并支持迭代调试。

Result: ModelWisdom 能够将原始模型检查器输出转化为更加互动且易于理解的工作流程，这有助于改善对非平凡TLA+规范的理解，并减少了调试所需的工作量。

Conclusion: ModelWisdom 通过提供一系列工具和服务显著提高了TLA+模型检查过程中的用户体验，特别是对于那些需要处理复杂模型及相应反例的情况。

Abstract: Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.

</details>


### [15] [Performance Antipatterns: Angel or Devil for Power Consumption?](https://arxiv.org/abs/2602.12079)
*Alessandro Aneggi,Vincenzo Stoico,Andrea Janes*

Main category: cs.SE

TL;DR: 本研究实证调查了已知的性能反模式是否也对微服务架构的能耗产生负面影响。通过实施十种反模式并进行受控负载测试，结果表明虽然所有反模式都降低了性能，但只有部分与电力消耗显著增加有关。


<details>
  <summary>Details</summary>
Motivation: 尽管已知性能反模式会降低基于微服务系统的响应性，但对于它们如何影响能源消耗的研究仍较少。这项工作的目的是探讨Smith和Williams定义的一些广泛研究的性能反模式是否同样对电力使用产生不利影响。

Method: 研究人员将十个不同的性能反模式实现为独立的微服务，并在受控负载条件下对其进行了评估。对于每一种反模式，他们收集了关于性能、CPU和DRAM功耗以及资源利用率的数据，每个反模式重复实验30次。

Result: 结果显示，尽管所有的性能反模式如预期那样恶化了系统性能，但是只有一部分显示出响应时间增长与功率消耗上升之间存在统计学上的显著关系。特别地，一些反模式达到了CPU饱和点，此时即便响应时间继续增长，功率吸收也不会再增加；而其他几种反模式（例如不必要的处理、坡道）则显示出能量-性能耦合现象，表明其效率低下。

Conclusion: 研究表明，虽然所有被注入的性能反模式都会导致响应时间延长，但只有少数同时表现为明显的能源反模式。在某些情况下，随着额外延迟主要转化为更长的执行时间而不是更高的瞬时功耗，CPU功率水平几乎保持不变。该研究为识别既是性能又是能源反模式的情况提供了系统性的基础，并为设计更加节能的微服务架构提供了可操作的见解。

Abstract: Performance antipatterns are known to degrade the responsiveness of microservice-based systems, but their impact on energy consumption remains largely unexplored. This paper empirically investigates whether widely studied performance antipatterns defined by Smith and Williams also negatively influence power usage. We implement ten antipatterns as isolated microservices and evaluate them under controlled load conditions, collecting synchronized measurements of performance, CPU and DRAM power consumption, and resource utilization across 30 repeated runs per antipattern. The results show that while all antipatterns degrade performance as expected, only a subset exhibit a statistically significant relationship between response time and increased power consumption. Specifically, several antipatterns reach CPU saturation, capping power draw regardless of rising response time, whereas others (\eg Unnecessary Processing, The Ramp) demonstrate energy-performance coupling indicative of inefficiency. Our results show that, while all injected performance antipatterns increase response time as expected, only a subset also behaves as clear energy antipatterns, with several cases reaching a nearly constant CPU power level where additional slowdowns mainly translate into longer execution time rather than higher instantaneous power consumption. The study provides a systematic foundation for identifying performance antipatterns that also behave as energy antipatterns and offers actionable insights for designing more energy-efficient microservices architectures.

</details>


### [16] [PPTAM$η$: Energy Aware CI/CD Pipeline for Container Based Applications](https://arxiv.org/abs/2602.12081)
*Alessandro Aneggi,Xiaozhou Li,Andrea Janes*

Main category: cs.SE

TL;DR: 本文介绍了一种名为PPTAMη的自动化流水线，它将功率和能量测量集成到GitLab CI中，用于容器化的API系统，使得开发者能够看到每次提交的能量消耗情况，支持测试工程师进行版本比较，并且让研究人员可以进行趋势分析。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的研究表明设计模式、代码异味以及重构会影响能源效率，但CI/CD流水线很少衡量能源消耗。为了改善这一状况，作者们开发了PPTAMη，旨在提高微服务架构下的能源可见性。

Method: PPTAMη是一个自动化的流程，它通过协调负载生成、容器监控与硬件功耗探针来收集每次提交时可比较的度量指标。该流程基于GitLab CI构建，适用于容器化API系统。

Result: 通过对一个JWT认证API在四个不同提交上的评估，PPTAMη成功地收集到了性能和能耗指标，并对架构、测量方法学及验证过程进行了总结。

Conclusion: PPTAMη为开发人员提供了观察每个版本能量消耗的能力，帮助测试工程师对比不同版本之间的差异，并且便于研究者进行长期的趋势分析，从而促进了更加节能的服务开发实践。

Abstract: Modern container-based microservices evolve through rapid deployment cycles, but CI/CD pipelines still rarely measure energy consumption, even though prior work shows that design patterns, code smells and refactorings affect energy efficiency. We present PPTAM$η$, an automated pipeline that integrates power and energy measurement into GitLab CI for containerised API systems, coordinating load generation, container monitoring and hardware power probes to collect comparable metrics at each commit. The pipeline makes energy visible to developers, supports version comparison for test engineers and enables trend analysis for researchers. We evaluate PPTAM$η$ on a JWT-authenticated API across four commits, collecting performance and energy metrics and summarising the architecture, measurement methodology and validation.

</details>


### [17] [Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting](https://arxiv.org/abs/2602.12256)
*Alex Chudic,Gül Çalıklı*

Main category: cs.SE

TL;DR: 本文探讨了通过少量示例提示（few-shot prompting）生成单元测试的效果，特别是不同来源的测试工件（如人工编写、SBST或LLM生成的）对大语言模型生成单元测试质量的影响。研究发现，使用人工编写的例子作为提示可以产生最佳覆盖率和正确性，并且基于问题描述和代码相似性的综合选择方法最有效。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在零样本学习方面已有广泛研究，但在单元测试生成中的少量示例学习潜力仍待探索。考虑到软件仓库中混合存在人工编写的测试、传统工具生成的测试以及LLM生成的测试，本研究旨在探究不同来源的测试工件如何影响LLM通过少量示例提示生成单元测试的质量及其对现有测试套件改进的贡献。

Method: 实验采用了HumanEval和ClassEval数据集，利用集成到GitHub Copilot中的GPT-4o进行。评估不仅涵盖了正确性和覆盖率，还考察了可读性、认知复杂度及维护性等指标。此外，研究还评估了几种基于检索的方法来选取相关示例用于少量示例提示。

Result: 结果显示，当使用人工编写的示例时，生成的单元测试具有最高的覆盖率和正确性；而基于问题描述与代码之间组合相似性的方法，在挑选示例以形成高效少量示例提示方面表现最为出色。

Conclusion: 这项研究表明，通过精心挑选的例子进行少量示例提示，LLMs能够生成高质量的单元测试，这不仅提高了测试套件的有效性，也增强了人类与AI共同开发环境下的代码理解和维护能力。

Abstract: Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Automated Optimization Modeling via a Localizable Error-Driven Perspective](https://arxiv.org/abs/2602.11164)
*Weiting Liu,Han Wu,Yufei Kuang,Xiongwei Han,Tao Zhong,Jianfeng Feng,Wenlian Lu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的错误驱动学习框架MIND，通过局部化错误驱动的视角来自定义整个模型训练过程，从数据合成到后训练阶段。这种方法针对现有自动化优化建模方法中稀疏性问题和难以解决的问题奖励稀疏性的限制，实验表明MIND在六个基准测试上始终优于现有的所有自动优化建模方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLMs）的自动化优化建模虽然在辅助复杂人类决策方面显示出了潜力，但其效能受限于高质量训练数据的稀缺性和不足利用。此外，现有方法面临两个基本局限：特定错误问题的稀疏性以及与难题相关的稀疏奖励。这些局限导致了领域特定后训练中的次优表现。

Method: 提出了名为MIND（通过局部化错误驱动视角实现自动化优化建模）的新框架，该框架基于观察到优化建模中错误传播的独特可定位模式，并且引入了动态监督微调策略优化(DFPO)来处理难问题。

Result: 在六个基准上的实验结果显示，MIND持续优于所有最先进的自动化优化建模方法。

Conclusion: MIND作为一种创新的错误驱动学习框架，能够有效克服现有自动化优化建模方法中的关键局限，特别是在处理具有挑战性的优化问题时表现出色。

Abstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\textbf{m}ated opt\textbf{i}mization modeli\textbf{n}g via a localizable error-\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \textbf{D}ynamic Supervised \textbf{F}ine-Tuning \textbf{P}olicy \textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.

</details>


### [19] [KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models](https://arxiv.org/abs/2602.11184)
*Zukang Xu,Zhixiong Zhao,Xing Hu,Zhixuan Chen,Dawei Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的向量量化框架KBVQ-MoE，用于改善基于MoE的大规模语言模型的极低比特量化问题。通过输入驱动的冗余消除和偏置校正输出稳定化技术，该方法显著提高了模型在资源受限环境下的部署效率，同时保持了较高的准确性。


<details>
  <summary>Details</summary>
Motivation: Mixture of Experts (MoE) 模型虽然在提高性能的同时保持了计算效率，但由于其巨大的参数大小和内存需求，在资源受限环境下部署面临挑战。直接将向量量化(VQ)应用于MoE通常会导致性能大幅下降，主要原因包括专家间存在冗余表示以及由专家聚合导致的累积输出偏差被放大。

Method: 提出了名为KBVQ-MoE的新框架来解决上述问题。该框架结合了两种技术：1）利用Karhunen-Loeve变换(KLT)引导的奇异值分解(SVD)进行输入驱动的冗余消除，提取主要权重分量并在专家之间共享；2）只对专家特定（非冗余）表示应用向量量化，并通过通道级仿射补偿来校正量化输出。

Result: 实验结果表明，KBVQ-MoE相比现有量化方法能够更好地保持模型准确性。例如，在Qwen1.5-MoE-A2.7B模型上使用3比特量化达到了67.99的平均准确率，几乎与FP16基线的68.07相同。

Conclusion: KBVQ-MoE提供了一种有效的方法来减少MoE模型的参数大小和内存需求，同时保持高精度，为边缘设备和其他资源受限平台上的高效部署开辟了道路。

Abstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.

</details>


### [20] [Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy](https://arxiv.org/abs/2602.11185)
*Zhendong Huang,Hengjie Cao,Fang Dong,Ruijun Huang,Mengyi Chen,Yifeng Yang,Xin Zhang,Anrui Chen,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Li Shang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Spectra的优化器，它能够抑制主导的低秩尖峰子空间而不放大对噪声敏感的频谱尾部。实验表明，与AdamW相比，Spectra在训练LLaMA3 8B时达到相同目标损失的速度快了30%，每步端到端开销减少了0.7%，优化器状态内存减少了49.25%，平均下游准确率提高了1.62%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）训练中的梯度信号高度各向异性：重复的语言结构将能量集中在少数主导频谱方向上，而上下文特定信息则分布在一个长尾中。这种尖峰-尾部分离在整个训练过程中持续存在，其中尖峰仅占据约1.5%的方向却主导了优化器统计，这限制了尾部的学习。

Method: 受上述分析启发，研究者们提出了Spectra，这是一种能够识别尖峰的优化器，旨在抑制主导的低秩尖峰子空间而不增加对噪声敏感的频谱尾部的影响。Spectra通过缓存和预热启动的幂迭代追踪尖峰子空间，并以极小的额外开销和显著减少的优化器状态内存来应用低秩频谱塑形。

Result: 在使用50B tokens训练LLaMA3 8B时，Spectra比AdamW更快地达到相同的目标损失（快30%），同时每步的端到端开销降低了0.7%，优化器状态内存减少了49.25%，并且平均下游准确性提高了1.62%。与Muon相比，Spectra在优化器处理时间上快了5.1倍，实现了更低的最终损失，并且平均准确性提高了0.66%。

Conclusion: Spectra作为一种新的优化方法，在加速训练、降低资源消耗以及提高模型性能方面显示出显著优势，特别是在处理具有强烈各向异性梯度的大规模语言模型时。

Abstract: Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spike occupying only about 1.5% of directions yet dominating optimizer statistics. This dominance suppresses tail learning by contracting tail updates through second moment normalization and tightening the globally stable learning rate bound. Motivated by this analysis, we propose Spectra, a spike aware optimizer that suppresses the dominant low rank spike subspace without amplifying the noise sensitive spectral tail. Spectra tracks the spike subspace via cached, warm started power iteration and applies low rank spectral shaping with negligible overhead and substantially reduced optimizer state memory. On LLaMA3 8B trained on 50B tokens, Spectra reaches the same target loss 30% faster than AdamW, reduces per step end to end overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves average downstream accuracy by 1.62%. Compared to Muon, Spectra is 5.1x faster in optimizer processing time, achieves a lower final loss, and improves average accuracy by 0.66%.

</details>


### [21] [MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models](https://arxiv.org/abs/2602.11192)
*Arian Raje,Anupam Nayak,Gauri Joshi*

Main category: cs.LG

TL;DR: 本文提出MELINOE方法，通过微调MoE模型以更偏好激活较少的专家，从而减少GPU和CPU之间的数据传输开销，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 尽管混合专家(MoE)模型能够有效降低每个token激活参数的数量，但由于其庞大的总体参数量及模型大小，在资源受限环境下使用受到限制。现有解决方案尝试将部分专家迁移到CPU内存中，仅在需要时才加载到GPU，但这种方法带来了显著的I/O延迟问题。

Method: MELINOE是一种新方法，通过对MoE模型进行微调来使其更加倾向于为每个序列激活较少数量的专家。通过在GPU内存中缓存这些优选的专家，可以减少专家更换频率以及CPU-GPU之间的数据传输开销。

Result: 与高效的基线相比，MELINOE提高了1.2到3倍的吞吐量；与依赖大量数据传输的基线相比，最高可提升至14.7倍的吞吐量。同时，该方法还保持了下游任务中的模型性能，甚至有所改进。

Conclusion: MELINOE提供了一种可靠的途径来增强MoE模型在推理过程中的效率，特别是在资源受限环境中。

Abstract: Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained settings as all of the parameters must still be loaded into GPU memory. Prior works aim to address this memory bottleneck by offloading certain experts into CPU memory and porting them to GPU memory only when they are activated. In practice, these methods suffer from the significant I/O latency incurred by expert transfer. We present MELINOE, a method that fine-tunes an MoE model to more strongly prefer activating a smaller number of experts per sequence. Caching these preferred experts in GPU memory reduces expert churn and CPU-GPU transfer overhead. MELINOE increases throughput by $1.2-3\times$ over efficient baselines and up to $14.7\times$ over transfer-heavy baselines while retaining or even improving the performance of the model on a downstream task, making it a reliable method for improving MoE inference efficiency.

</details>


### [22] [Predicting the post-wildfire mudflow onset using machine learning models on multi-parameter experimental data](https://arxiv.org/abs/2602.11194)
*Mahta Movasat,Ingrid Tomac*

Main category: cs.LG

TL;DR: 本研究利用多种机器学习算法（如多元线性回归、逻辑回归等）分析了实验室条件下模拟的降雨对不同土壤斜坡的影响，以预测和分类泥流的发生。研究发现细沙在低强度长时间降雨下特别容易受到侵蚀，且高强度降雨的前10分钟对于泥流的产生至关重要。这些结果表明机器学习技术在野火后灾害评估及应急响应规划中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于野火频发导致的地表或近地表土壤疏水性增加，进而引发灾难性的泥石流现象变得越来越危险。为了理解由关键参数共同作用驱动的泥石流发生的时间和条件，包括变化的降雨强度、坡度梯度、水分入渗值和颗粒大小等因素，进行这项研究显得尤为重要。

Method: 通过使用雨量设备在倾斜的沟槽中对各种土壤进行实验，模仿野外条件，并应用多个机器学习(ML)算法：多元线性回归(MLR)、逻辑回归(LR)、支持向量分类器(SVC)、K-均值聚类以及主成分分析(PCA)，来预测总排放量、侵蚀情况并对失败结果进行分类。

Result: 多元线性回归能够有效预测总排放量，但在预测粗砂侵蚀方面不够准确；逻辑回归和支持向量分类器在分类失败结果上达到了良好的准确性，并得到了聚类和降维的支持。敏感性分析显示，细沙在低强度、长持续时间的降雨下极易受到侵蚀。此外，研究还表明高强度降雨最初的10分钟对于排放量和失败最为关键。

Conclusion: 研究结果强调了机器学习方法在理解和预测野火后泥石流行为方面的价值，特别是在灾害评估和紧急响应计划制定方面展示了巨大潜力。

Abstract: Post-wildfire mudflows are increasingly hazardous due to the prevalence of wildfires, including those on the wildland-urban interface. Upon burning, soil on the surface or immediately beneath becomes hydrophobic, a phenomenon that occurs predominantly on sand-based hillslopes. Rainwater and eroded soil blanket the downslope, leading to catastrophic debris flows. Soil hydrophobicity enhances erosion, resulting in post-wildfire debris flows that differ from natural mudflows in intensity, duration, and destructiveness. Thus, it is crucial to understand the timing and conditions of debris-flow onset, driven by the coupled effects of critical parameters: varying rain intensities (RI), slope gradients, water-entry values, and grain sizes (D50). Machine Learning (ML) techniques have become increasingly valuable in geotechnical engineering due to their ability to model complex systems without predefined assumptions. This study applies multiple ML algorithms: multiple linear regression (MLR), logistic regression (LR), support vector classifier (SVC), K-means clustering, and principal component analysis (PCA) to predict and classify outcomes from laboratory experiments that model field conditions using a rain device on various soils in sloped flumes. While MLR effectively predicted total discharge, erosion predictions were less accurate, especially for coarse sand. LR and SVC achieved good accuracy in classifying failure outcomes, supported by clustering and dimensionality reduction. Sensitivity analysis revealed that fine sand is highly susceptible to erosion, particularly under low-intensity, long-duration rainfall. Results also show that the first 10 minutes of high-intensity rain are most critical for discharge and failure. These findings highlight the potential of ML for post-wildfire hazard assessment and emergency response planning.

</details>


### [23] [AM-FM: A Foundation Model for Ambient Intelligence Through WiFi](https://arxiv.org/abs/2602.11200)
*Guozhen Zhu,Yuqian Hu,Sakila Jayaweera,Weihang Gao,Wei-Hsiang Wang,Jiaxuan Zhang,Beibei Wang,Chenshu Wu,K. J. Ray Liu*

Main category: cs.LG

TL;DR: 本文提出了AM-FM，首个通过WiFi实现环境智能和感知的基础模型。该模型基于从全球部署的20种商用设备类型收集的920万个未标记CSI样本进行预训练，并在九个下游任务中展示了强大的跨任务性能和改进的数据效率。


<details>
  <summary>Details</summary>
Motivation: 尽管WiFi基础设施为数十亿物联网设备提供了无处不在、始终在线且保护隐私的能力，但无线感知通常依赖于需要大量标记数据的任务特定模型，这限制了实际部署。因此，作者们希望开发一种能够利用现有无线基础设施实现可扩展环境智能的基础模型。

Method: AM-FM是基于对比学习、掩码重建以及针对无线信号定制的物理信息目标，在来自全球部署的20种商用设备类型的920万未标记CSI样本上进行预训练的。这些样本是在439天内收集的。

Result: AM-FM在涵盖九个下游任务的公开基准测试中表现出了强大的跨任务性能，同时提高了数据效率。

Conclusion: 这项研究表明，通过使用基础模型如AM-FM，可以利用现有的无线基础设施来实现可扩展的环境智能。

Abstract: Ambient intelligence, continuously understanding human presence, activity, and physiology in physical spaces, is fundamental to smart environments, health monitoring, and human-computer interaction. WiFi infrastructure provides a ubiquitous, always-on, privacy-preserving substrate for this capability across billions of IoT devices. Yet this potential remains largely untapped, as wireless sensing has typically relied on task-specific models that require substantial labeled data and limit practical deployment. We present AM-FM, the first foundation model for ambient intelligence and sensing through WiFi. AM-FM is pre-trained on 9.2 million unlabeled Channel State Information (CSI) samples collected over 439 days from 20 commercial device types deployed worldwide, learning general-purpose representations via contrastive learning, masked reconstruction, and physics-informed objectives tailored to wireless signals. Evaluated on public benchmarks spanning nine downstream tasks, AM-FM shows strong cross-task performance with improved data efficiency, demonstrating that foundation models can enable scalable ambient intelligence using existing wireless infrastructure.

</details>


### [24] [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776)
*Cláudio Correia,Alberto E. A. Ferreira,Lucas Martins,Miguel P. Bento,Sofia Guerreiro,Ricardo Ribeiro Pereira,Ana Sofia Gomes,Jacopo Bono,Hugo Ferreira,Pedro Bizarro*

Main category: cs.LG

TL;DR: 该论文提出了一种名为MUSE的模型服务框架，旨在通过将模型分数与客户决策边界分离来实现无缝模型更新。MUSE针对多租户环境设计，通过动态意图路由和两级分数转换优化基础设施复用，确保模型输出映射到一个稳定的参考分布。在Feedzai的大规模部署中，MUSE处理了大量事件，并大幅减少了模型上线时间，从而提高了对抗攻击的适应性，节省了数百万美元的欺诈损失和运营成本。


<details>
  <summary>Details</summary>
Motivation: 在二分类系统中，决策阈值用于将模型评分转化为具体行动。选择合适的阈值不仅依赖于基础模型评分的具体分布，还取决于每个使用该模型客户的特定业务决策。然而，重新训练模型不可避免地会改变评分分布，使现有阈值失效。在多租户“评分即服务”环境中，由于决策界限位于客户端管理的基础架构内，这导致了一个严重瓶颈：重新校准需要协调数百个客户之间的阈值更新，消耗了大量人力资源并导致模型停滞。

Method: 研究者们提出了MUSE（Model Update with Seamless Experience），一种模型服务框架，它通过将模型评分与客户端决策边界解耦来支持无缝模型更新。MUSE利用动态意图导向路由共享模型，并结合两层评分转换机制，将模型输出映射到一个稳定、参照的分布上。

Result: MUSE已在Feedzai大规模部署，每秒处理超过一千个事件，在过去12个月里处理了超过550亿个事件，覆盖数十个租户，同时保持高可用性和低延迟承诺。通过将模型前置时间从几周减少到几分钟，MUSE增强了模型对变化攻击的抵御能力，从而避免了数百万美元的欺诈损失及运营成本。

Conclusion: MUSE提供了一种有效的方法来解决多租户环境下模型更新时遇到的问题，包括但不限于因重新训练引起的评分分布偏移以及跨多个客户端协调阈值更新所需的人力资源。其成功应用证明了MUSE能够显著提高效率、降低成本，并增强面对不断演变威胁时的安全防护水平。

Abstract: In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.

</details>


### [25] [Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders](https://arxiv.org/abs/2602.11204)
*Zhuxin Lei,Ziyuan Yang,Yi Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为ZePAD的新方法，用于防御下游任务无关的对抗性示例（DAEs），同时保持对良性数据的表现。该方法通过双分支结构增强了模型的鲁棒性，并且只需要一次微调即可适用于多种下游任务。实验表明，ZePAD在提高对抗鲁棒性的同时也显著提升了良性样本上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练编码器容易受到下游任务无关对抗性示例的影响，而现有防御手段多依赖于特定任务的对抗性微调，这不仅限制了通用性还可能导致灾难性遗忘和良性性能下降。因此，需要一种更通用的方法来增强模型对抗此类攻击的能力，同时不牺牲其在正常数据上的表现。

Method: ZePAD采用双分支设计：一个Multi-Pattern Adversarial Enhancement Branch使用两个对抗性微调后的编码器加强抵抗能力；另一个Benign Memory Preservation Branch则专注于本地数据以确保对抗鲁棒性的提升不会损害到良性样本处理能力。这种方法能够增加特征多样性，使得单次对抗性微调足以应对跨不同下游任务时遇到的DAE挑战。

Result: 广泛的实验证明了ZePAD的有效性，在11种自监督学习方法和6个数据集上进行了测试。结果显示，与基线相比，ZePAD在某些情况下可以将良性性能提高29.20%，同时将对抗鲁棒性提高73.86%。

Conclusion: ZePAD提供了一个有效解决下游任务无关对抗性示例问题的方案，它能够在不牺牲良性性能的前提下显著增强模型的对抗鲁棒性。此外，通过引入特征多样性，ZePAD允许单一调整即服务于多种下游应用，展示了强大的泛化能力。

Abstract: The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downstream models. While several defense methods have been explored recently, they rely primarily on task-specific adversarial fine-tuning, which inevitably limits generalizability and causes catastrophic forgetting and deteriorates benign performance. Different with previous works, we propose a more rigorous defense goal that requires only a single tuning for diverse downstream tasks to defend against DAEs and preserve benign performance. To achieve this defense goal, we introduce Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD), which is inspired by the inherent sensitivity of neural networks to data characteristics. Specifically, ZePAD is a dual-branch structure, which consists of a Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch) that uses two adversarially fine-tuned encoders to strengthen adversarial resistance. The Benign Memory Preservation Branch (BMP-Branch) is trained on local data to ensure adversarial robustness does not compromise benign performance. Surprisingly, we find that ZePAD can directly detect DAEs by evaluating branch confidence, without introducing any adversarial exsample identification task during training. Notably, by enriching feature diversity, our method enables a single adversarial fine-tuning to defend against DAEs across downstream tasks, thereby achieving persistent robustness. Extensive experiments on 11 SSL methods and 6 datasets validate its effectiveness. In certain cases, it achieves a 29.20% improvement in benign performance and a 73.86% gain in adversarial robustness, highlighting its zero-sacrifice property.

</details>


### [26] [Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems](https://arxiv.org/abs/2602.11208)
*Xin Ju,Nok Hei,Fung,Yuyan Zhang,Carl Jacquemyn,Matthew Jackson,Randolph Settgast,Sally M. Benson,Gege Wen*

Main category: cs.LG

TL;DR: 提出了一种新的神经算子——自适应物理变换器（APT），用于解决地下系统模拟中的计算难题，能够处理高分辨率的局部异质特征和长距离物理影响。APT在规则和不规则网格上均表现出色，并且首次直接从自适应网格细化模拟中学习，展示了跨数据集学习的能力。


<details>
  <summary>Details</summary>
Motivation: 地球地下是现代社会的重要基石，提供诸如碳氢化合物、地热和矿物等重要能源资源，同时也是二氧化碳封存的主要储库。然而，由于地质异质性、对高分辨率的需求以及不同传播时间尺度物理过程的紧密耦合，这些系统的全物理数值模拟计算成本极高。

Method: 开发了自适应物理变换器（APT），这是一种与几何、网格和物理无关的神经算子，它通过图编码器提取高分辨率局部异质特征，并利用全局注意力机制来解决长距离物理效应问题。

Result: APT在规则和不规则网格上的地下任务中优于现有最先进架构，具有强大的超分辨率能力。此外，APT是首个可以直接从自适应网格细化模拟中学习的架构，并且展示出了跨数据集学习的能力。

Conclusion: APT为大规模地下基础模型的发展提供了强大且可扩展的基础，解决了当前地下系统模拟面临的计算挑战。

Abstract: The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.

</details>


### [27] [Towards Compressive and Scalable Recurrent Memory](https://arxiv.org/abs/2602.11212)
*Yunchong Song,Jushi Kai,Liming Lu,Kaixi Qiu,Zhouhan Lin*

Main category: cs.LG

TL;DR: 本文提出了一种新的记忆架构——弹性记忆（Elastic Memory），基于HiPPO框架进行在线函数逼近，旨在解决Transformer在处理长上下文时面临的二次瓶颈问题。通过将历史序列视为连续信号的样本，并采用最优在线压缩方法将其编码成固定大小的记忆状态，该方法能够在保持理论原则的同时实现实际可扩展性。实验表明，在长上下文数据集上，Elastic Memory不仅在内存效率上优于基线模型，而且随着模型规模的扩大仍能保持性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在尝试通过引入循环记忆来扩展当前窗口以外的上下文时，通常面临理论原则与实际可扩展性之间的基本权衡。为了解决这个问题，作者提出了一个基于HiPPO框架的新记忆架构——弹性记忆（Elastic Memory）。

Method: Elastic Memory视历史序列为来自连续信号的样本，利用最优在线压缩技术将其编码进固定大小的记忆状态中。此外，它还提出了一种灵活的多项式采样机制，用于从压缩状态重建历史摘要。

Result: 在三个领域的长上下文（32k+）数据集上，Elastic Memory的表现始终优于基线。当参数数量相同时，其内存使用量仅为Memorizing Transformer的1/16，并且即使Melodi拥有额外30%的参数量，Elastic Memory依然表现更佳。随着模型规模的增加，Elastic Memory不仅保持了相对于所有基线的优势，而且速度远超四倍规模下的Melodi。

Conclusion: Elastic Memory提供了一个有效解决方案，解决了Transformer处理长文本时遇到的问题，同时确保了良好的理论基础和出色的实践性能。

Abstract: Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent approaches introduce recurrent memory to extend context beyond the current window, yet these often face a fundamental trade-off between theoretical principles and practical scalability. To address this, we introduce Elastic Memory, a novel memory architecture grounded in the HiPPO framework for online function approximation. Elastic Memory treats historical sequence as samples from continuous signals, applying optimal online compression to encode them into a fixed-size memory state. For retrieval, we propose a flexible \textit{polynomial sampling} mechanism that reconstructs a history summary from this compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at 4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost performance.

</details>


### [28] [Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning](https://arxiv.org/abs/2602.11215)
*Lintao Wang,Zhuqiang Lu,Yilin Zhu,Kun Hu,Zhenfei Yin,Shixiang Tang,Zhiyong Wang,Wanli Ouyang,Xinzhu Ma*

Main category: cs.LG

TL;DR: 本研究首次系统地探讨了跨学科大型语言模型（LLM）的微调，通过构建一个涵盖五个学科的数据集来分析全微调、LoRA、LoRA-MoE以及LoRA组合的学习模式。研究揭示了多学科学习相比单一学科训练更加多变，并提炼出四个一致的经验法则：平衡后求多样、合并后对齐、优化后扩展、共享后特化。这些发现为开发具有泛化能力的科学LLM提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 尽管跨领域知识协同有望提高大型语言模型在科学研究中的泛化能力和适用范围，但对于这类模型在多学科背景下的学习动态了解有限。这项工作的动机在于填补这一空白，探索如何有效利用跨学科数据进行模型微调。

Method: 研究人员构建了一个包含五个不同学科领域的语料库，并基于此对多种微调策略（包括但不限于全量微调、LoRA及其变体）进行了对比实验。通过对这些方法在处理跨学科任务时表现的观察与分析，试图找出最有效的多学科训练方案。

Result: 研究表明，多学科学习过程比单学科更复杂且变化更大；同时总结出了四条经验法则，即先确保均衡再增加多样性、先整合信息再调整一致性、先优化设计再考虑规模扩大、先促进资源共享再鼓励专业化发展。这些规律为实现更好的跨学科适应性指明了方向。

Conclusion: 该研究不仅揭示了多学科背景下大型语言模型训练的独特挑战，还提出了具体的解决方案和指导原则，旨在推动能够跨越多个科学领域的通用型语言模型的发展。

Abstract: While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domain knowledge synergy. In this work, we present the first systematic study of multi-disciplinary LLM fine-tuning, constructing a five-discipline corpus and analyzing learning patterns of full fine-tuning, LoRA, LoRA-MoE, and LoRA compositions. Particularly, our study shows that multi-disciplinary learning is substantially more variable than single-discipline training and distills four consistent empirical laws: (1) Balance-then-Diversity: low-resource disciplines degrade performance unless mitigated via diversity-aware upsampling; (2) Merge-then-Align: restoring instruction-following ability is critical for cross-discipline synergy; (3) Optimize-then-Scale: parameter scaling offers limited gains without prior design optimization; and (4) Share-then-Specialize: asymmetric LoRA-MoE yields robust gains with minimal trainable parameters via shared low-rank projection. Together, these laws form a practical recipe for principled multi-discipline fine-tuning and provide actionable guidance for developing generalizable scientific LLMs.

</details>


### [29] [Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators](https://arxiv.org/abs/2602.11216)
*Panagiotis Antoniadis,Beatrice Pavesi,Simon Olsson,Ole Winther*

Main category: cs.LG

TL;DR: 该研究通过引入辅助信息源，如蛋白质语言模型嵌入，提高了可转移隐式转移算子（TITO）在分子动力学中的数据效率和泛化能力。提出的PLaTITO方法在非分布蛋白质系统的平衡采样基准上达到了最先进的性能，并探索了结构嵌入、温度等额外条件信号对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学由于生成独立样本所需的高计算成本而受到限制。虽然生成式分子动力学（GenMD）能够有效采样，但其跨分子系统的迁移性通常有限。研究旨在通过结合辅助信息源来提高用于分子动力学的可转移隐式转移算子的数据效率和泛化能力。

Method: 研究者们提出了一种称为PLaTITO的方法，该方法利用粗粒度TITO模型以及蛋白质语言模型（pLM）嵌入来改进分子动力学模拟。此外，还探讨了诸如结构嵌入、温度及大型语言模型衍生的嵌入等附加条件信号对于模型表现的影响。

Result: 结果表明，粗粒度TITO模型相比Boltzmann Emulators具有更高的数据效率；同时加入pLM嵌入进一步增强了模型在外部分布情况下的泛化能力。PLaTITO在包括快速折叠蛋白在内的非分布蛋白质系统上的均衡采样基准测试中取得了最佳成绩。

Conclusion: 通过整合辅助信息资源，特别是蛋白质语言模型嵌入，可以显著提升可转移隐式转移算子在分子动力学中的数据效率与泛化性能。这为未来开发更高效且适应性强的分子模拟工具提供了新的思路。

Abstract: Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.

</details>


### [30] [The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning](https://arxiv.org/abs/2602.11217)
*Simin Fan,Dimitris Paparas,Natasha Noy,Binbin Xiong,Noveen Sachdeva,Berivan Isik*

Main category: cs.LG

TL;DR: 该研究探讨了语言模型能力如何从预训练转移到监督微调（SFT），通过一系列相关性协议应用于不同数据混合和模型规模的准确性和置信度指标。实验揭示了跨能力类别、基准测试和规模的转移可靠性存在显著差异，这些发现为基准选择、数据管理和高效模型开发提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型的能力如何从预训练阶段转移到监督微调阶段对于有效的模型开发和数据管理至关重要。

Method: 通过将一系列相关性协议应用到跨越多种数据混合与模型规模的准确性及置信度衡量标准上，来解决四个核心问题：预训练期间建立的准确性和置信度排名在SFT后保持的程度；哪些基准作为稳健的跨阶段预测器是可靠的或不可靠的；随着模型规模的变化，转移动态如何变化；以及模型置信度与准确性的对齐程度，作为校准质量的一种度量，在不同训练阶段间是否也保持一致。

Result: 实验显示，跨能力领域、基准测试和规模的迁移可靠性差异巨大——准确性和置信度展现出独特且有时相反的扩展动态。

Conclusion: 这些发现揭示了预训练决策与下游结果之间复杂的相互作用，并为基准选取、数据整理及高效的模型开发提供了可操作的指南。

Abstract: Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.

</details>


### [31] [Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty](https://arxiv.org/abs/2602.11219)
*Tanmoy Mukherjee,Marius Kloft,Pierre Marquis,Zied Bouraoui*

Main category: cs.LG

TL;DR: 提出了一种基于信度集的方法来区分预测不确定性中的认知不确定性和偶然不确定性，通过两个独立的目标和不重叠的梯度路径训练模型，从而在构造上实现分离，而不是事后分解。这种方法显著降低了两种不确定性之间的相关性，并提高了不确定性与预测误差及数据真实模糊性的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法从相同的预测分布中估计认知不确定性和偶然不确定性，导致这些估计值通常高度相关，使得预测范围的变化同时影响两个组成部分，混淆了它们的含义。为了提高决策的可靠性，需要一种能够更准确地区分这两种类型的不确定性的方法。

Method: 采用了一种信度集公式，其中不确定性表示为一组预测分布；认知不确定性和偶然不确定性对应于不同的几何属性：集合的大小与元素内的噪声。具体来说，开发了一个变分信度概念瓶颈模型，该模型包含两个互不相交的不确定性头，通过不同的目标函数和非重叠的梯度路径进行训练。

Result: 所提出的方法在多标注基准测试中将认知不确定性和偶然不确定性之间的相关性降低了超过一个数量级，同时改善了认知不确定性与预测错误之间以及偶然不确定性与实际数据模糊性之间的一致性。

Conclusion: 研究展示了一种新的框架，可以有效分离预测不确定性中的认知成分和偶然成分，提供更加清晰可靠的决策支持。

Abstract: Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically strongly correlated, so changes in predictive spread simultaneously affect both components and blur their semantics. We propose a credal-set formulation in which uncertainty is represented as a set of predictive distributions, so that epistemic and aleatoric uncertainty correspond to distinct geometric properties: the size of the set versus the noise within its elements. We instantiate this idea in a Variational Credal Concept Bottleneck Model with two disjoint uncertainty heads trained by disjoint objectives and non-overlapping gradient paths, yielding separation by construction rather than post hoc decomposition. Across multi-annotator benchmarks, our approach reduces the correlation between epistemic and aleatoric uncertainty by over an order of magnitude compared to standard methods, while improving the alignment of epistemic uncertainty with prediction error and aleatoric uncertainty with ground-truth ambiguity.

</details>


### [32] [Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT](https://arxiv.org/abs/2602.11220)
*Jiacheng Wang,Ping Jian,Zhen Yang,Zirong Chen,Keren Liao,Zhongbin Guo*

Main category: cs.LG

TL;DR: 为了解决下游任务数据与模型先前训练分布存在显著差异时监督微调（SFT）可能导致的灾难性遗忘问题，本文提出了一种基于强化学习的数据重写策略。该策略通过优化以奖励反馈为导向的重写分布，在保持多样性和任务一致性的同时，更好地匹配模型的问答式生成分布。实验结果显示，这种方法在减少非下游基准测试中平均12.34%的遗忘同时，达到了与标准SFT相当的下游性能提升。


<details>
  <summary>Details</summary>
Motivation: 当用于特定下游任务时，大型语言模型经常需要经过监督微调（SFT）。如果下游数据与模型预训练阶段所用的数据分布存在较大差异，则可能引发灾难性遗忘的问题。现有解决方法如数据重写虽能缓解这一问题，但通常依赖于固定模板且难以保证与模型自然生成风格的一致性，导致多样性损失。

Method: 本文将数据重写视为一种策略学习问题，并引入了基于强化学习的方法来优化重写策略。该方法旨在提高重写后数据与模型QA式生成分布之间的匹配度，同时保持足够的多样性。通过结合自动可评估但难以端到端优化的目标，使用强化学习技术调整重写策略，以最大化综合考虑了分布对齐、多样性和任务一致性的奖励函数。

Result: 实验结果表明，所提出的方法不仅能够有效减少因分布偏移引起的灾难性遗忘现象，在非下游任务上的遗忘减少了约12.34%，而且在目标下游任务上的表现也与传统的SFT方法相当。

Conclusion: 本研究介绍了一种新的基于强化学习的数据重写框架，它有助于改善大型语言模型适应下游任务时遇到的挑战，特别是在处理与原始训练数据具有显著不同分布的新数据集时。

Abstract: Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .

</details>


### [33] [Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks](https://arxiv.org/abs/2602.11234)
*Ankita Paul,Wenyi Wang*

Main category: cs.LG

TL;DR: 提出了一种名为TopoGBM的学习框架，通过3D卷积自编码器并结合拓扑正则化来捕捉胶质母细胞瘤(GBM)的异质性和结构多样性，提高了跨机构MRI数据的预测准确性，并在多个队列中展示了优于基线模型的表现。


<details>
  <summary>Details</summary>
Motivation: 准确地使用深度学习对胶质母细胞瘤（GBM）进行预后受到空间和结构高度异质性以及不同机构间MRI采集协议不一致性的挑战。传统的方法往往无法充分捕捉肿瘤的多尺度形态多样性，导致了针对特定扫描仪的伪影和较差的跨站点预后效果。

Method: 开发了TopoGBM，这是一种利用3D卷积自编码器并通过拓扑正则化保持肿瘤复杂非欧几里得不变量的学习框架，在压缩潜在空间内保留了肿瘤的结构特征。通过强制执行这些拓扑先验，该方法能够明确建模高变异结构签名，这在侵袭性GBM中尤为常见。

Result: 在不同队列（UPENN, UCSF, RHUH）上进行了评估，并且在TCGA上的外部验证表明，TopoGBM达到了更好的性能(C-index测试0.67，验证0.58)，超过了在领域迁移下表现下降的基线模型。机制解释性分析显示，重建残差高度集中在病理异质区域，而肿瘤及健康组织误差显著较低。此外，基于遮挡的归因定位大约50%的预后信号到肿瘤及其多样化的周围微环境。

Conclusion: 研究结果表明，通过结合拓扑先验知识，可以学习到忠实于形态学的嵌入表示，既捕捉到了肿瘤异质性也保持了跨机构间的鲁棒性。

Abstract: Accurate prognosis for Glioblastoma (GBM) using deep learning (DL) is hindered by extreme spatial and structural heterogeneity. Moreover, inconsistent MRI acquisition protocols across institutions hinder generalizability of models. Conventional transformer and DL pipelines often fail to capture the multi-scale morphological diversity such as fragmented necrotic cores, infiltrating margins, and disjoint enhancing components leading to scanner-specific artifacts and poor cross-site prognosis. We propose TopoGBM, a learning framework designed to capture heterogeneity-preserved, scanner-robust representations from multi-parametric 3D MRI. Central to our approach is a 3D convolutional autoencoder regularized by a topological regularization that preserves the complex, non-Euclidean invariants of the tumor's manifold within a compressed latent space. By enforcing these topological priors, TopoGBM explicitly models the high-variance structural signatures characteristic of aggressive GBM. Evaluated across heterogeneous cohorts (UPENN, UCSF, RHUH) and external validation on TCGA, TopoGBM achieves better performance (C-index 0.67 test, 0.58 validation), outperforming baselines that degrade under domain shift. Mechanistic interpretability analysis reveals that reconstruction residuals are highly localized to pathologically heterogeneous zones, with tumor-restricted and healthy tissue error significantly low (Test: 0.03, Validation: 0.09). Furthermore, occlusion-based attribution localizes approximately 50% of the prognostic signal to the tumor and the diverse peritumoral microenvironment advocating clinical reliability of the unsupervised learning method. Our findings demonstrate that incorporating topological priors enables the learning of morphology-faithful embeddings that capture tumor heterogeneity while maintaining cross-institutional robustness.

</details>


### [34] [AI-Driven Clinical Decision Support System for Enhanced Diabetes Diagnosis and Management](https://arxiv.org/abs/2602.11237)
*Mujeeb Ur Rehman,Imran Rehan,Sohail Khalid*

Main category: cs.LG

TL;DR: 本研究开发并测试了一种结合专家见解和机器学习技术的人工智能临床决策支持系统（AI-CDSS），用于2型糖尿病的诊断。该系统在预测糖尿病、糖尿病前期以及无糖尿病状态方面展现了极高的准确性，尤其在非内分泌科专家环境中具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 鉴于2型糖尿病识别对初级保健医生来说是一个挑战，研究旨在通过开发一种专门针对2型糖尿病诊断的AI-CDSS来提高诊断准确性。

Method: 采用混合方法，结合专家知识与机器学习技术开发了AI-CDSS，并使用包含1298名患者的数据集进行训练（n=650）和测试（n=648）。此外，还进行了涉及105名患者的临床试点研究以比较AI-CDSS与非内分泌科专家之间的诊断准确性。

Result: AI-CDSS在预测糖尿病、糖尿病前期及无糖尿病状况时表现出非常高的准确率，分别为99.8%、99.3%、99.2% 和 98.8%。与内分泌学专家相比，在测试数据集上达到了98.8%的一致性；而在试点研究中，与糖尿病专家相比AI-CDSS的一致率为98.5%，远高于非内分泌科专家的85%。

Conclusion: 结果表明，AI-CDSS能够作为准确识别2型糖尿病的有效工具，尤其是在缺乏糖尿病专科医生的情况下。

Abstract: Identifying type 2 diabetes mellitus can be challenging, particularly for primary care physicians. Clinical decision support systems incorporating artificial intelligence (AI-CDSS) can assist medical professionals in diagnosing type 2 diabetes with high accuracy. This study aimed to assess an AI-CDSS specifically developed for the diagnosis of type 2 diabetes by employing a hybrid approach that integrates expert-driven insights with machine learning techniques. The AI-CDSS was developed (training dataset: n = 650) and tested (test dataset: n = 648) using a dataset of 1298 patients with and without type 2 diabetes. To generate predictions, the algorithm utilized key features such as body mass index, plasma fasting glucose, and hemoglobin A1C. Furthermore, a clinical pilot study involving 105 patients was conducted to assess the diagnostic accuracy of the system in comparison to non-endocrinology specialists. The AI-CDSS showed a high degree of accuracy, with 99.8% accuracy in predicting diabetes, 99.3% in predicting prediabetes, 99.2% in identifying at-risk individuals, and 98.8% in predicting no diabetes. The test dataset revealed a 98.8% agreement between endocrinology specialists and the AI-CDSS. Type 2 diabetes was identified in 45% of 105 individuals in the pilot study. Compared with diabetes specialists, the AI-CDSS scored a 98.5% concordance rate, greatly exceeding that of nonendocrinology specialists, who had an 85% agreement rate. These findings indicate that the AI-CDSS has the potential to be a useful tool for accurately identifying type 2 diabetes, especially in situations in which diabetes specialists are not readily available.

</details>


### [35] [How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?](https://arxiv.org/abs/2602.11246)
*Nikhil Garg,Jon Kleinberg,Kenny Peng*

Main category: cs.LG

TL;DR: 本文提出了一种数学框架来研究语言模型中间层是否以线性方式存储特征，并将这一假设分为线性表示和线性访问两个方面。通过理论分析，作者为线性压缩感知问题建立了几乎匹配的上下界，证明了在一定条件下神经元可以存储指数数量的特征，从而支持了'叠加假设'。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨语言模型中所谓的线性表示假设（LRH），即语言模型的中间层是以线性形式存储特征的。该研究试图解答需要多少个神经元才能同时实现对特定数量特征的线性表示与线性访问的问题。

Method: 研究采用数学分析方法，特别地，利用了经典压缩感知领域的结果作为基础，并针对线性解码要求提出了新的理论边界。上界证明依赖于标准随机矩阵构造技术；下界则借助Alon (2003) 关于近单位矩阵的秩界限以及Turán定理的应用。

Result: 研究得出了线性压缩感知所需的神经元数目d的近似上下界：下界为Ωε(k^2/log k * log(m/k))，而上界为Oε(k^2log m)。这些结果揭示了线性可访问性比单纯线性表示更强的事实，并且表明根据LRH，神经元确实能够存储大量的特征。

Conclusion: 本研究表明，对于k-稀疏输入，在允许非线性解码算法的情况下，所需神经元数目远少于当额外要求线性解码时的情况。此外，研究还讨论了其结果如何影响特征表示的几何结构，并扩展至考虑带激活函数及偏置项的解码器情形。

Abstract: We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are linearly embedded in neuron activations) and linear accessibility (features can be linearly decoded). We then ask: How many neurons $d$ suffice to both linearly represent and linearly access $m$ features? Classical results in compressed sensing imply that for $k$-sparse inputs, $d = O(k\log (m/k))$ suffices if we allow non-linear decoding algorithms (Candes and Tao, 2006; Candes et al., 2006; Donoho, 2006). However, the additional requirement of linear decoding takes the problem out of the classical compressed sensing, into linear compressed sensing.
  Our main theoretical result establishes nearly-matching upper and lower bounds for linear compressed sensing. We prove that $d = Ω_ε(\frac{k^2}{\log k}\log (m/k))$ is required while $d = O_ε(k^2\log m)$ suffices. The lower bound establishes a quantitative gap between classical and linear compressed setting, illustrating how linear accessibility is a meaningfully stronger hypothesis than linear representation alone. The upper bound confirms that neurons can store an exponential number of features under the LRH, giving theoretical evidence for the "superposition hypothesis" (Elhage et al., 2022).
  The upper bound proof uses standard random constructions of matrices with approximately orthogonal columns. The lower bound proof uses rank bounds for near-identity matrices (Alon, 2003) together with Turán's theorem (bounding the number of edges in clique-free graphs). We also show how our results do and do not constrain the geometry of feature representations and extend our results to allow decoders with an activation function and bias.

</details>


### [36] [HiFloat4 Format for Language Model Inference](https://arxiv.org/abs/2602.11287)
*Yuanyong Luo,Jing Huang,Yu Cheng,Ziwei Yu,Kaihua Zhang,Kehong Hong,Xinda Ma,Xin Wang,Anping Tong,Guipeng Hu,Yun Xu,Mehran Taghian,Peng Wu,Guanglin Li,Yunke Peng,Tianchi Hu,Minqi Chen,Michael Bi Mi,Hu Liu,Xiping Zhou,Junsong Wang,Qiang Lin,Heng Liao*

Main category: cs.LG

TL;DR: 本文介绍了一种专为深度学习设计的块浮点数据格式HiFloat4（HiF4），每个HiF4单元包含64个4位元素和32位共享缩放元数据，平均每个值使用4.5位。该格式通过三级缩放层次结构提高了表示空间利用率，并且在几个语言模型上的推理实验表明，与最先进的NVFP4格式相比，HiF4在多种模型和不同下游任务中实现了更高的平均准确性。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的、更高效的数据表示格式，以提高深度学习应用中的硬件效率和计算精度。

Method: 提出了一种名为HiFloat4的新数据格式，它将多个4位数值与共享的缩放信息打包在一起，同时采用三级缩放机制来优化动态范围的表现。

Result: 实验结果表明，在多个语言模型及不同的下游任务上，HiF4格式相较于现有最佳方案NVFP4提供了更好的平均准确率。

Conclusion: HiF4作为一种新颖的数据格式，不仅减少了硬件面积和功耗需求，而且在保持甚至提高计算精度方面表现优异，适用于深度学习领域。

Abstract: This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.

</details>


### [37] [Efficient Analysis of the Distilled Neural Tangent Kernel](https://arxiv.org/abs/2602.11320)
*Jamie Mahowald,Brian Bell,Alex Ho,Michael Geyer*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，即蒸馏神经切线核（DNTK），通过结合针对NTK调整的数据集蒸馏和最先进的投影方法来减少NTK计算的复杂性，同时保持内核结构和预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经切线核(NTK)方法在计算上受到需要跨多个数据点评估大型雅可比矩阵的限制。现有的方法主要通过投射和草图化雅可比矩阵来降低这种成本。本文旨在通过使用针对NTK调整的数据集蒸馏来压缩数据维度本身，从而进一步减少NTK的计算需求。

Method: 本文首先展示了可以通过数据集蒸馏诱导输入数据所跨越的神经切线空间，从而显著减少了所需的雅可比矩阵计算量。然后，作者指出每个类别的NTK矩阵具有低的有效秩，并且这种性质在数据降维过程中得以保留。基于这些观察，提出了蒸馏神经切线核(DNTK)，它将针对NTK调优的数据集蒸馏与最先进的投影技术相结合，极大地降低了NTK的计算复杂度。

Result: 研究表明，通过该方法可以在保持内核结构和预测性能的同时，将NTK的计算复杂度降低多达五个数量级。

Conclusion: 本文介绍的方法，即蒸馏神经切线核(DNTK)，为减少神经切线核方法中的计算负担提供了一个有效途径，同时能够保持良好的预测表现。

Abstract: Neural tangent kernel (NTK) methods are computationally limited by the need to evaluate large Jacobians across many data points. Existing approaches reduce this cost primarily through projecting and sketching the Jacobian. We show that NTK computation can also be reduced by compressing the data dimension itself using NTK-tuned dataset distillation. We demonstrate that the neural tangent space spanned by the input data can be induced by dataset distillation, yielding a 20-100$\times$ reduction in required Jacobian calculations. We further show that per-class NTK matrices have low effective rank that is preserved by this reduction. Building on these insights, we propose the distilled neural tangent kernel (DNTK), which combines NTK-tuned dataset distillation with state-of-the-art projection methods to reduce up NTK computational complexity by up to five orders of magnitude while preserving kernel structure and predictive performance.

</details>


### [38] [Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence](https://arxiv.org/abs/2602.11322)
*Jason Dury*

Main category: cs.LG

TL;DR: 提出了一种名为PAM（Predictive Associative Memory）的新型架构，该架构通过JEPA风格的预测器学习连续体验流中的时间共现关系，以导航嵌入空间的关联结构。实验表明，PAM在合成基准测试中能够有效识别经验性关联，其性能优于基于相似度的方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经系统中的记忆方法主要依赖于基于相似性的检索，但这种方法忽略了生物记忆的一个基本特性：通过时间共现进行关联。研究旨在开发一种新的记忆架构，它能更好地模仿生物记忆的时间关联机制。

Method: 提出了PAM架构，包含一个JEPA风格的预测器，用于学习连续体验流内的时间共现关系。此外，还引入了Inward JEPA来处理存储的经验以及Outward JEPA来处理传入的感觉数据。评估时将PAM视为联想回忆系统，专注于已体验过的关联而非对未见过的关联的一般化能力。

Result: 在合成基准测试上，PAM的表现非常出色，对于真实时间关联的记忆准确率达到了97%；跨边界召回率为0.421，而余弦相似度得分为零；区分共同经历和从未共同经历的状态时，AUC为0.916（余弦相似度为0.789）。即使是在房间间配对这样的限制条件下，PAM依旧表现优异。

Conclusion: PAM提供了一种新的方式来模拟生物记忆中的时间关联特征，并且在特定任务上的表现超越了传统的基于相似性的方法。这表明，通过学习时间共现模式可以有效地增强记忆系统的性能。

Abstract: Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through temporal co-occurrence. We propose Predictive Associative Memory (PAM), an architecture in which a JEPA-style predictor, trained on temporal co-occurrence within a continuous experience stream, learns to navigate the associative structure of an embedding space. We introduce an Inward JEPA that operates over stored experience (predicting associatively reachable past states) as the complement to the standard Outward JEPA that operates over incoming sensory data (predicting future states). We evaluate PAM as an associative recall system -- testing faithfulness of recall for experienced associations -- rather than as a retrieval system evaluated on generalisation to unseen associations. On a synthetic benchmark, the predictor's top retrieval is a true temporal associate 97% of the time (Association Precision@1 = 0.970); it achieves cross-boundary Recall@20 = 0.421 where cosine similarity scores zero; and it separates experienced-together from never-experienced-together states with a discrimination AUC of 0.916 (cosine: 0.789). Even restricted to cross-room pairs where embedding similarity is uninformative, the predictor achieves AUC = 0.849 (cosine: 0.503, chance). A temporal shuffle control confirms the signal is genuine temporal co-occurrence structure, not embedding geometry: shuffling collapses cross-boundary recall by 90%, replicated across training seeds. All results are stable across seeds (SD < 0.006) and query selections (SD $\leq$ 0.012).

</details>


### [39] [Structured Hybrid Mechanistic Models for Robust Estimation of Time-Dependent Intervention Outcomes](https://arxiv.org/abs/2602.11350)
*Tomer Meir,Ori Linial,Danny Eytan,Uri Shalit*

Main category: cs.LG

TL;DR: 本文提出了一种混合机制-数据驱动的方法，用于估计时间依赖性干预的结果。该方法将动态系统的转移算子分解为参数和非参数组件，并进一步区分与干预相关的和不相关的动态。实验结果表明，该混合方法在处理分布外(OOD)情况时优于纯数据驱动和机制模型。


<details>
  <summary>Details</summary>
Motivation: 在医学中，对生理调节（如液体给药下的心血管系统）和药代动力学等进行干预效果的估计对于优化结果至关重要。然而，数据驱动模型虽然能捕捉复杂的动态，但在分布外场景中常常表现不佳；而机制模型虽然通常比较稳健，但可能过于简化。因此，作者提出了一个结合两者优势的新方法。

Method: 该方法通过将动态系统的转移算子分解为参数化和非参数化成分来实现，同时区分出与干预相关及无关的动力学过程。此外，针对那些未知机制参数的情况，文中介绍了一个两阶段程序：首先基于模拟数据预训练编码器，然后从观察数据中学习修正信息。

Result: 研究结果显示，在两种具有不完全机制知识的情况下——周期摆动和丙泊酚推注——所提出的混合方法比纯数据驱动或机制方法表现更好，特别是在面对分布外的数据时。

Conclusion: 这项工作强调了混合机制-数据驱动模型在复杂现实世界动态系统中的鲁棒干预优化潜力。

Abstract: Estimating intervention effects in dynamical systems is crucial for outcome optimization. In medicine, such interventions arise in physiological regulation (e.g., cardiovascular system under fluid administration) and pharmacokinetics, among others. Propofol administration is an anesthetic intervention, where the challenge is to estimate the optimal dose required to achieve a target brain concentration for anesthesia, given patient characteristics, while avoiding under- or over-dosing. The pharmacokinetic state is characterized by drug concentrations across tissues, and its dynamics are governed by prior states, patient covariates, drug clearance, and drug administration. While data-driven models can capture complex dynamics, they often fail in out-of-distribution (OOD) regimes. Mechanistic models on the other hand are typically robust, but might be oversimplified. We propose a hybrid mechanistic-data-driven approach to estimate time-dependent intervention outcomes. Our approach decomposes the dynamical system's transition operator into parametric and nonparametric components, further distinguishing between intervention-related and unrelated dynamics. This structure leverages mechanistic anchors while learning residual patterns from data. For scenarios where mechanistic parameters are unknown, we introduce a two-stage procedure: first, pre-training an encoder on simulated data, and subsequently learning corrections from observed data. Two regimes with incomplete mechanistic knowledge are considered: periodic pendulum and Propofol bolus injections. Results demonstrate that our hybrid approach outperforms purely data-driven and mechanistic approaches, particularly OOD. This work highlights the potential of hybrid mechanistic-data-driven models for robust intervention optimization in complex, real-world dynamical systems.

</details>


### [40] [Toward Adaptive Non-Intrusive Reduced-Order Models: Design and Challenges](https://arxiv.org/abs/2602.11378)
*Amirpasha Hedayat,Alberto Padovan,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 本文提出并研究了三种自适应非侵入式降阶模型（ROM）方法，旨在在线更新潜在子空间和简化动力学。这些方法在超出初始训练范围时仍能保持有效。


<details>
  <summary>Details</summary>
Motivation: 传统的基于投影的降阶模型作为静态替代品使用时，在系统离开训练流形后其实用性受到限制。为了克服这一局限性，作者提出了能够在线更新潜在子空间和简化动力学的自适应非侵入式ROM方法。

Method: 基于静态非侵入式ROM的想法，特别是操作推断(OpInf)和最近引入的非侵入式轨迹优化ROM(NiTROM)，作者提出了三种形式：自适应OpInf（序列基础/操作重新拟合）、自适应NiTROM（编码器/解码器与多项式动力学的联合黎曼优化），以及一种将NiTROM初始化为OpInf更新的混合方法。

Result: 对于瞬态扰动的顶盖驱动腔流问题，当预测超出训练范围时，静态Galerkin/OpInf/NiTROM会出现漂移或不稳定现象。相比之下，自适应OpInf能够以较低成本有效地抑制振幅漂移；自适应NiTROM在频繁更新下几乎可以实现精确的能量跟踪，但对初始化和优化深度敏感；而混合方法在机制变化及最小离线数据条件下最为可靠，产生物理上一致的场和有界能量。

Conclusion: 研究强调了构建自我纠正、非侵入式的ROM的重要性，这些模型即使在动态发展远超初始流形的情况下也能保持有效性。此外，还指出对于ROM的预测声明需要考虑成本意识和透明度，明确区分训练/适应/部署阶段，并清晰报告在线预算和全阶模型查询情况。

Abstract: Projection-based Reduced Order Models (ROMs) are often deployed as static surrogates, which limits their practical utility once a system leaves the training manifold. We formalize and study adaptive non-intrusive ROMs that update both the latent subspace and the reduced dynamics online. Building on ideas from static non-intrusive ROMs, specifically, Operator Inference (OpInf) and the recently-introduced Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM), we propose three formulations: Adaptive OpInf (sequential basis/operator refits), Adaptive NiTROM (joint Riemannian optimization of encoder/decoder and polynomial dynamics), and a hybrid that initializes NiTROM with an OpInf update. We describe the online data window, adaptation window, and computational budget, and analyze cost scaling. On a transiently perturbed lid-driven cavity flow, static Galerkin/OpInf/NiTROM drift or destabilize when forecasting beyond training. In contrast, Adaptive OpInf robustly suppresses amplitude drift with modest cost; Adaptive NiTROM is shown to attain near-exact energy tracking under frequent updates but is sensitive to its initialization and optimization depth; the hybrid is most reliable under regime changes and minimal offline data, yielding physically coherent fields and bounded energy. We argue that predictive claims for ROMs must be cost-aware and transparent, with clear separation of training/adaptation/deployment regimes and explicit reporting of online budgets and full-order model queries. This work provides a practical template for building self-correcting, non-intrusive ROMs that remain effective as the dynamics evolve well beyond the initial manifold.

</details>


### [41] [Provably Efficient Algorithms for S- and Non-Rectangular Robust MDPs with General Parameterization](https://arxiv.org/abs/2602.11387)
*Anirudh Satheesh,Ziyi Chen,Furong Huang,Heng Huang*

Main category: cs.LG

TL;DR: 本文研究了在s-矩形和非矩形不确定集下具有一般策略参数化的鲁棒马尔可夫决策过程（RMDPs）。通过将平均奖励RMDPs简化为熵正则化折扣鲁棒MDPs，解决了强对偶性问题，并使得均衡计算变得可行。对于无限时域梯度估计，提出了一种多级蒙特卡洛梯度估计器，其样本复杂度为$\tilde{\mathcal{O}}(ε^{-2})$。基于此，针对s-矩形不确定性设计了一个投影梯度下降算法，对于非矩形不确定性设计了一个Frank-Wolfe算法。这是首个为超越$(s, a)$-矩形性的RMDPs提供样本复杂度保证的工作，在平均奖励设置中提供了首例此类保证，并改进了折扣鲁棒MDPs的现有界限。


<details>
  <summary>Details</summary>
Motivation: 先前关于鲁棒马尔可夫决策过程的研究主要局限于表格型策略，这导致要么缺乏样本复杂度保证，要么计算成本高昂。因此，需要一种方法来处理具有更一般策略参数化的鲁棒MDP问题，特别是在无限状态空间的情况下。

Method: 作者们通过将平均奖励RMDPs转化为熵正则化折扣鲁棒MDPs来恢复强对偶性并允许进行易于管理的平衡计算。此外，他们还提出了新的Lipschitz和平滑性质以适用于广泛使用的策略参数化形式。为了应对无限时域下的梯度估计挑战，引入了一种多级蒙特卡洛梯度估计技术。基于这些发现，开发了专门针对s-矩形不确定性情况的投影梯度下降算法以及处理非矩形不确定性的Frank-Wolfe算法。

Result: 研究表明，所提出的多级蒙特卡洛梯度估计器相较于之前的工作，在样本复杂度上提高了$\mathcal{O}(ε^{-2})$倍。此外，针对s-矩形不确定性和非矩形不确定性分别设计的算法也显著提升了性能：s-矩形不确定性下达到$\mathcal{O}(ε^{-5})$、非矩形不确定性下达到$\mathcal{O}(ε^{-4})$折扣奖励与$\mathcal{O}(ε^{-10.5})$平均奖励。

Conclusion: 本研究首次为超出$(s, a)$-矩形性的鲁棒马尔可夫决策过程提供样本复杂度保证，并且在平均奖励设定下取得了突破性进展。同时，它也为折扣鲁棒MDPs带来了改进的边界条件。

Abstract: We study robust Markov decision processes (RMDPs) with general policy parameterization under s-rectangular and non-rectangular uncertainty sets. Prior work is largely limited to tabular policies, and hence either lacks sample complexity guarantees or incurs high computational cost. Our method reduces the average reward RMDPs to entropy-regularized discounted robust MDPs, restoring strong duality and enabling tractable equilibrium computation. We prove novel Lipschitz and Lipschitz-smoothness properties for general policy parameterizations that extends to infinite state spaces. To address infinite-horizon gradient estimation, we introduce a multilevel Monte Carlo gradient estimator with $\tilde{\mathcal{O}}(ε^{-2})$ sample complexity, a factor of $\mathcal{O}(ε^{-2})$ improvement over prior work. Building on this, we design a projected gradient descent algorithm for s-rectangular uncertainty ($\mathcal{O}(ε^{-5})$) and a Frank--Wolfe algorithm for non-rectangular uncertainty ($\mathcal{O}(ε^{-4})$ discounted, $\mathcal{O}(ε^{-10.5})$ average reward), significantly improving prior results in both the discounted setting and average reward setting. Our work is the first one to provide sample complexity guarantees for RMDPs with general policy parameterization beyond $(s, a)$-rectangularity. It also provides the first such guarantees in the average reward setting and improves existing bounds for discounted robust MDPs.

</details>


### [42] [Sparse Semantic Dimension as a Generalization Certificate for LLMs](https://arxiv.org/abs/2602.11388)
*Dibyanayan Bandyopadhyay,Asif Ekbal*

Main category: cs.LG

TL;DR: 该论文提出了一种新的复杂性度量——稀疏语义维度（SSD），用来解释大型语言模型（LLM）为何能够有效泛化。通过分析模型内部表示的几何结构，研究者发现模型参数虽然庞大，但激活状态实际上位于一个低维且稀疏的流形上。利用稀疏自编码器训练得到的特征词汇表，作者们展示了模型的泛化能力更多地依赖于字典的稀疏性而非总参数数量。实验结果表明，更大的模型如Gemma-2B相较于GPT-2 Small，在识别活跃流形时需要更少的校准样本，显示出更好的压缩性和独特的语义结构。此外，此框架还可用作安全监控工具，通过检测异常输入导致的特征激增来指示知识不确定性。


<details>
  <summary>Details</summary>
Motivation: 标准统计学习理论预测，由于大型语言模型（LLM）的参数数量远超训练标记数，这些模型应该会过拟合。然而实践中，它们却能稳健地泛化。本文旨在探索这种现象背后的机制，并提出一种基于模型内部表示几何结构的新理论来解释这一问题。

Method: 引入了稀疏语义维度（SSD）作为复杂性度量指标，它是从稀疏自编码器（SAE）训练过程中获得的活跃特征词汇中导出的。研究者将LLM和SAE视为固定不变的oracle，利用这个框架来评估模型泛化能力与特征字典稀疏性之间的关系，而非单纯考虑总参数量。

Result: 实验证明了所提出的框架对于现实样本规模下的GPT-2 Small和Gemma-2B模型提供了非空洞的证书。值得注意的是，尽管Gemma-2B比GPT-2大了一个数量级，但它在确定其活跃流形时所需校准样本显著减少，这表明更大规模的模型学习到了更加可压缩且独特的语义结构。此外，当输入超出分布范围时，该框架还能作为可靠的安全监测手段，通过检测到的‘特征爆炸’现象来反映模型的知识不确定性。

Conclusion: 本研究表明，大型语言模型的有效泛化能力可能与其内部表示的稀疏性和几何结构有关，而不仅仅是参数数量。通过引入稀疏语义维度这一新概念，我们不仅能够更好地理解模型泛化的原因，而且还开发了一种潜在的安全监控方法。

Abstract: Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive "feature sharpness" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable "feature explosion" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.

</details>


### [43] [General and Efficient Steering of Unconditional Diffusion](https://arxiv.org/abs/2602.11395)
*Qingsong Wang,Mikhail Belkin,Yusu Wang*

Main category: cs.LG

TL;DR: 提出了一种无需在推理过程中使用梯度指导即可高效引导无条件扩散模型的方法，通过噪声对齐和可转移概念向量实现了快速可控的生成。


<details>
  <summary>Details</summary>
Motivation: 为了解决引导无条件扩散模型需要重新训练或每步计算梯度导致的大量计算开销问题。

Method: 利用噪声对齐和可转移概念向量两个观察结果，结合递归特征机（RFM）来识别概念方向，实现高效的条件控制。

Result: 在CIFAR-10、ImageNet和CelebA上的实验表明该方法比基于梯度的指导具有更高的准确性和质量，并且显著提高了推理速度。

Conclusion: 提供了一种新的有效方式来指导无条件扩散模型，能够在不牺牲图像质量和多样性的情况下提高生成效率。

Abstract: Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusion {without gradient guidance during inference}, enabling fast controllable generation. Our approach is built on two observations about diffusion model structure: Noise Alignment: even in early, highly corrupted stages, coarse semantic steering is possible using a lightweight, offline-computed guidance signal, avoiding any per-step or per-sample gradients. Transferable concept vectors: a concept direction in activation space once learned transfers across both {timesteps} and {samples}; the same fixed steering vector learned near low noise level remains effective when injected at intermediate noise levels for every generation trajectory, providing refined conditional control with efficiency. Such concept directions can be efficiently and reliably identified via Recursive Feature Machine (RFM), a light-weight backpropagation-free feature learning method. Experiments on CIFAR-10, ImageNet, and CelebA demonstrate improved accuracy/quality over gradient-based guidance, while achieving significant inference speedups.

</details>


### [44] [Can We Really Learn One Representation to Optimize All Rewards?](https://arxiv.org/abs/2602.11399)
*Chongyi Zheng,Royina Karegoudra Jayanth,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本研究揭示了前向后向（FB）表示学习的训练目标和学习行为，提出了一种简化的无监督预训练方法，称为一步前向后向表示学习（one-step FB），该方法能够执行策略改进步骤，并在多个连续控制领域中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习领域开始利用大型模型作为解决下游任务的基础，社区对于解决强化学习问题的最佳基础形式存在争议。先前的工作尝试通过一种无监督表示学习过程来实现对任意奖励的最优控制，而无需进一步微调。然而，这种被称为前向后向（FB）表示学习的方法其训练目标及学习机制仍然不够清晰。

Method: 本文通过对FB表示学习的存在条件、优化目标及其实际收敛情况进行分析，将其与排序匹配、拟合Q评估以及收缩映射等概念相联系。基于此分析，研究人员提出了一种简化版的无监督预训练方法——一步前向后向表示学习(one-step FB)，该方法专注于进行一次策略提升而非直接支持最优控制。

Result: 实验结果表明，在教学设置以及10个基于状态和图像的连续控制领域内，one-step FB相比原方法能够达到小5个数量级的误差，并且平均提升了24%的零样本性能表现。

Conclusion: 通过澄清FB表示学习背后的原理并提出简化版本one-step FB，这项工作为强化学习提供了一个新的视角和实用工具，有助于提高未见任务上模型的表现。

Abstract: As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.

</details>


### [45] [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)
*David Pardoe,Neil Daftary,Miro Furtado,Aditya Aiyer,Yu Wang,Liuqing Li,Tao Song,Lars Hertel,Young Jin Yun,Senthil Radhakrishnan,Zhiwei Wang,Tommy Li,Khai Tran,Ananth Nagarajan,Ali Naqvi,Yue Zhang,Renpeng Fang,Avi Romascanu,Arjun Kulothungun,Deepak Kumar,Praneeth Boda,Fedor Borisyuk,Ruoyan Wang*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的解码器仅变压器模型CADET，用于广告点击率预测，并在LinkedIn上部署。通过引入多种创新机制，如基于上下文条件的解码架构、自门控注意力机制等，实现了比现有生产模型更高的CTR提升。


<details>
  <summary>Details</summary>
Motivation: 尽管基于转换器的生成推荐系统在内容推荐方面取得了有希望的结果，但将这些架构应用于广告点击率（CTR）预测仍然存在独特挑战，比如处理评分后的上下文信号、保持离线-在线一致性以及扩展至工业工作负载等问题。

Method: 研究者们提出了CADET（Context-Conditioned Ads Decoder-Only Transformer），一种专为广告CTR预测设计的端到端解码器仅变压器模型。该方法包括了几个关键创新点：1) 一个能够明确建模诸如广告位置这类评分后信号的基于上下文条件的解码架构；2) 一种可以在表示和交互层面上自适应调节信息流的自门控注意机制；3) 一种基于时间戳的旋转位置嵌入变体，能够捕捉从秒到月的时间尺度上的时间关系；4) 防止模型学习不可用会话事件依赖性的会话屏蔽策略；5) 包括张量打包、序列分块及定制Flash Attention内核在内的生产工程技术，以实现大规模高效训练和服务。

Result: 在线A/B测试中，CADET相比现有的生产基准模型LiRank（DCNv2和顺序编码器的混合集成）实现了11.04%的CTR提升。该系统已在LinkedIn的广告平台上成功部署，服务于主页赞助更新的主要流量。

Conclusion: CADET模型通过引入一系列创新性改进，在解决广告CTR预测面临的特定挑战方面表现出了显著优势，为工业级应用提供了有效解决方案。

Abstract: Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.

</details>


### [46] [Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification](https://arxiv.org/abs/2602.11448)
*Nghia Nguyen,Tianjiao Ding,René Vidal*

Main category: cs.LG

TL;DR: 本研究提出了一种新的框架HCEP，该框架在视觉-语言模型的潜在空间中诱导概念嵌入的层次结构，并利用分层稀疏编码来恢复图像中存在的概念。实验结果表明，HCEP在概念精度和召回率方面优于基线方法，同时保持了竞争性的分类准确度。此外，在样本数量有限的情况下，HCEP达到了更高的分类准确度和概念恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于稀疏概念恢复的方法忽略了概念的层次结构，可能导致预测正确但解释与层次结构不符的问题。因此，需要一种能够考虑概念间层级关系的新方法来改进图像分类模型的可解释性。

Method: 提出了Hierarchical Concept Embedding & Pursuit (HCEP)框架，它在潜在空间中诱导出概念嵌入的层次结构，并使用分层稀疏编码技术来从图像中恢复相关概念。此方法假设给定图像正确的概念构成了层次中的根路径，并据此推导出了识别这些概念所需满足的理想条件。

Result: 通过实验证明了分层稀疏编码能可靠地恢复层次化的概念嵌入，而普通的稀疏编码则无法做到这一点。HCEP不仅在概念精度和召回上超越了基准方法，而且在样本量较少时也能达到更优的分类准确性和概念恢复表现。

Conclusion: 将层次结构引入稀疏编码可以产生更加可靠且易于理解的图像分类模型。HCEP证明了通过结合概念间的层次关系，可以在不影响分类性能的同时提高模型解释的质量。

Abstract: Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.

</details>


### [47] [Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning](https://arxiv.org/abs/2602.11465)
*Jared Levy,Aarti Lalwani,Elijah Wyckoff,Kenneth J. Loh,Sara P. Gombatto,Rose Yu,Emilia Farcas*

Main category: cs.LG

TL;DR: 本文提出了一种名为MT-AIM的深度学习分类流程，通过生成合成数据和特征增强来解决基于Motion Tape（一种低成本、便携式的织物可穿戴传感器）数据集样本量小和噪声问题，从而准确分类下背部运动。


<details>
  <summary>Details</summary>
Motivation: 背痛是一个普遍的问题，影响着大量人群，而某些下背部的动作往往会加重这种疼痛。为了帮助临床医生开具合适的物理治疗方案，评估这些动作至关重要。然而，在诊所之外远程监控患者的动作却十分困难。虽然高保真度的动作捕捉传感器可用于分类不同的动作，但它们成本高昂且在日常生活环境中使用不便。为此，一种新的基于织物的可穿戴传感器Motion Tape（MT）应运而生，它具有低成本和便携性的优点。不过，由于传感器新颖性和稳定性变化导致的数据集规模较小以及固有的噪音问题依然存在。

Method: 研究者提出了Motion-Tape Augmentation Inference Model (MT-AIM)，这是一个专为MT数据训练设计的深度学习分类流程。针对MT数据集中样本量有限及存在的噪声问题，MT-AIM利用条件生成模型来产生指定动作所需的合成MT数据，并预测关节运动学作为额外特征。

Result: 结合了合成数据生成与特征增强的方法后，MT-AIM能够在分类下背部运动方面达到最先进水平的准确性。

Conclusion: MT-AIM通过克服小规模数据集和噪声带来的挑战，成功地提高了基于Motion Tape传感器数据对下背部运动进行分类的准确性，促进了生理感知与运动分析之间的联系。

Abstract: Back pain is a pervasive issue affecting a significant portion of the population, often worsened by certain movements of the lower back. Assessing these movements is important for helping clinicians prescribe appropriate physical therapy. However, it can be difficult to monitor patients' movements remotely outside the clinic. High-fidelity data from motion capture sensors can be used to classify different movements, but these sensors are costly and impractical for use in free-living environments. Motion Tape (MT), a new fabric-based wearable sensor, addresses these issues by being low cost and portable. Despite these advantages, novelty and variability in sensor stability make the MT dataset small scale and inherent to noise. In this work, we propose the Motion-Tape Augmentation Inference Model (MT-AIM), a deep learning classification pipeline trained on MT data. In order to address the challenges of limited sample size and noise present within the MT dataset, MT-AIM leverages conditional generative models to generate synthetic MT data of a desired movement, as well as predicting joint kinematics as additional features. This combination of synthetic data generation and feature augmentation enables MT-AIM to achieve state-of-the-art accuracy in classifying lower back movements, bridging the gap between physiological sensing and movement analysis.

</details>


### [48] [External Division of Two Bregman Proximity Operators for Poisson Inverse Problems](https://arxiv.org/abs/2602.11482)
*Kazuki Haishima,Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，用于从被泊松噪声破坏的线性模型中恢复稀疏向量。通过引入基于两个Bregman邻近算子外部除法定义的新算子来促进稀疏解，并减少由经典ℓ1-范数正则化引起的估计偏差。实验表明，该方法比传统方法具有更稳定的收敛行为，并在合成数据和图像恢复问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决当线性模型受到泊松噪声影响时，如何有效地恢复稀疏向量的问题。特别地，它尝试改进由于使用经典ℓ1-范数正则化而导致的估计偏差问题。

Method: 提出的方法包括两个主要部分：一是定义了一个新算子，该算子通过两个Bregman邻近算子的外部除法得到，以促进稀疏解并减少估计偏差；二是将这个新算子嵌入到已有的NoLips算法中，替换标准的Bregman邻近算子。此外，还提供了对该外部除法算子几何结构的解释，帮助理解其在泊松逆问题原空间与对偶空间中的作用。

Result: 数值测试表明，所提出的方法相比于基于Kullback-Leibler (KL) 的传统方法表现出更加稳定的收敛特性，并且在合成数据集以及实际图像恢复任务上获得了显著优于现有技术的表现。

Conclusion: 通过引入新的基于Bregman邻近算子的外部除法算子，本文为从泊松噪声污染的数据中恢复稀疏向量提供了一个有效的方法。此方法不仅减少了估计偏差，而且在多种应用场景下均显示出优越性能。

Abstract: This paper presents a novel method for recovering sparse vectors from linear models corrupted by Poisson noise. The contribution is twofold. First, an operator defined via the external division of two Bregman proximity operators is introduced to promote sparse solutions while mitigating the estimation bias induced by classical $\ell_1$-norm regularization. This operator is then embedded into the already established NoLips algorithm, replacing the standard Bregman proximity operator in a plug-and-play manner. Second, the geometric structure of the proposed external-division operator is elucidated through two complementary reformulations, which provide clear interpretations in terms of the primal and dual spaces of the Poisson inverse problem. Numerical tests show that the proposed method exhibits more stable convergence behavior than conventional Kullback-Leibler (KL)-based approaches and achieves significantly superior performance on synthetic data and an image restoration problem.

</details>


### [49] [A Generic Framework for Fair Consensus Clustering in Streams](https://arxiv.org/abs/2602.11500)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文提出了首个在流模型中处理公平共识聚类的常数因子算法，该算法仅需存储对数数量的输入。此外，还引入了一个新的算法框架，该框架不仅提高了近似保证，而且对于任何可以有效计算近似最近公平聚类的公平性定义都是适用的。


<details>
  <summary>Details</summary>
Motivation: 先前的工作虽然在比例公平下引入了公平变体并获得了恒定因子逼近，但其离线方法需要存储所有输入的聚类，这对大多数大规模应用来说成本过高。因此，本文旨在研究有限内存条件下顺序到达的输入聚类的公平共识聚类问题。

Method: 提出了一种新通用算法框架，将最接近的公平聚类与聚类拟合相结合，并设计了首个能够处理流模型中的数据且只需存储少量输入的常数因子算法。

Result: 所提出的算法能够在流式设置中处理公平共识聚类问题，同时只存储对数数量的输入。此框架还能应用于更广泛的k-中位数共识聚类问题，并且对任何能有效计算近似最近公平聚类的公平性定义均适用。

Conclusion: 本研究为公平共识聚类提供了一种创新的方法，特别适用于大规模应用及流数据处理场景，同时保持了良好的近似度和公平性。

Abstract: Consensus clustering seeks to combine multiple clusterings of the same dataset, potentially derived by considering various non-sensitive attributes by different agents in a multi-agent environment, into a single partitioning that best reflects the overall structure of the underlying dataset. Recent work by Chakraborty et al, introduced a fair variant under proportionate fairness and obtained a constant-factor approximation by naively selecting the best closest fair input clustering; however, their offline approach requires storing all input clusterings, which is prohibitively expensive for most large-scale applications.
  In this paper, we initiate the study of fair consensus clustering in the streaming model, where input clusterings arrive sequentially and memory is limited. We design the first constant-factor algorithm that processes the stream while storing only a logarithmic number of inputs. En route, we introduce a new generic algorithmic framework that integrates closest fair clustering with cluster fitting, yielding improved approximation guarantees not only in the streaming setting but also when revisited offline. Furthermore, the framework is fairness-agnostic: it applies to any fairness definition for which an approximately close fair clustering can be computed efficiently. Finally, we extend our methods to the more general k-median consensus clustering problem.

</details>


### [50] [Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice](https://arxiv.org/abs/2602.11505)
*Jiangkai Xiong,Kalyan Talluri,Hanzhao Wang*

Main category: cs.LG

TL;DR: 本文针对企业难以直接观察消费者是否选择竞争对手或完全不购买的问题，提出了一种新的方法来校准外部选项的概率预测。通过这种方法，即使在仅有交易数据的情况下，也能从购买数据中得到有效的无购买估计。研究还探讨了在不同校准条件下如何改善无购买事件的估计，并分析了这些改进对后续决策（如商品组合优化）的影响。


<details>
  <summary>Details</summary>
Motivation: 企业通常无法直接观察到消费者是否选择了竞争对手的产品、决定不购买或是彻底考虑了企业的报价。这种缺失的外部选项信息使得市场大小和偏好估计变得困难，尤其是在只有交易数据被记录时。现有的方法往往依赖于辅助市场份额、汇总数据或跨市场数据。然而，当一个黑盒辅助预测器提供的外部选项概率可能因为训练环境与实际应用环境的不同而存在偏差时，如何利用这样的预测进行有效校准成为了研究的重点。

Method: 该研究首先在logit空间下的仿射失准情况下，提出了一种简单的回归方法来识别外部选项效用参数，并使用焦点环境中的仅购买数据一致地恢复无购买概率，而无需为无购买事件收集新标签。其次，在较弱的近似单调条件下，提出了基于排名的校准方法，并推导出有限样本误差界限，明确区分辅助预测器质量和第一阶段观测内集选择上的效用学习错误。此外，研究还将估计误差转化为下游决策质量，特别是对于商品组合优化而言，量化了校准准确性如何影响收入表现。

Result: 研究结果表明，所提出的方法能够在无购买事件估计以及随后的商品组合决策方面带来显著改进。此外，文中还讨论了通过结合多个辅助预测器来进行稳健聚合扩展的可能性。

Conclusion: 本文开发了一套能够将不完美的预测转变为统计上有效的无购买估计的方法论体系，这不仅有助于提高无购买行为预测的准确性，同时也为企业基于更准确的市场需求理解做出更好的商业决策提供了支持。

Abstract: Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.

</details>


### [51] [Unifying Stable Optimization and Reference Regularization in RLHF](https://arxiv.org/abs/2602.11523)
*Li He,Qiang Qu,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种统一的正则化方法，旨在平衡防止奖励黑客和保持策略更新稳定这两个目标，通过加权监督微调损失实现更好的权衡，从而在对齐性能和稳定性方面优于现有的RLHF和在线偏好学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前的解决方案分别采用独立的正则化策略来解决奖励黑客和稳定优化问题，但同时针对预训练模型$π_0$和当前策略$π_t$进行正则化时存在的隐含权衡尚未得到充分探索。

Method: 引入了一个统一的正则化方法，明确地平衡了防止奖励黑客与维持稳定策略更新的目标。该方法提出了一个具有优越权衡效果的加权监督微调损失函数。

Result: 广泛的实验表明，本方法在不同的基准测试中始终优于RLHF和在线偏好学习方法，实现了更好的对齐性能和稳定性。

Conclusion: 新提出的统一正则化方法不仅简化了实施复杂性，而且在增强对齐性能的同时保证了稳定性，为未来的研究提供了新的方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \textbf{reward hacking} and \textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.

</details>


### [52] [PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models](https://arxiv.org/abs/2602.11530)
*Eunyeong Cho,Jehyeon Bang,Ranggi Hwang,Minsoo Rhu*

Main category: cs.LG

TL;DR: PASCAL, a phase-aware scheduling algorithm, optimizes the serving of reasoning-based LLMs by prioritizing the reasoning phase to reduce Time-To-First-Token (TTFT) and using preemption and token pacing during the answering phase to ensure Quality-of-Experience (QoE). It effectively reduces tail TTFT by up to 72% while maintaining service level objectives for the answering phase.


<details>
  <summary>Details</summary>
Motivation: 随着基于推理的大型语言模型（LLM）利用Chain-of-Thought（CoT）推理的出现，为服务端带来了新的挑战，因为它们延长了推理阶段从而延迟了用户可见的输出，并增加了首次输出令牌的时间（Time-To-First-Token, TTFT）。现有的LLM服务框架无法区分推理和回答阶段，在GPU内存限制下导致性能下降。

Method: 研究者提出了PASCAL，这是一种阶段感知调度算法，通过优先处理推理阶段来减少TTFT，同时在回答阶段使用受控抢占和令牌节流来保持体验质量（QoE）。该层次结构调度器结合实例级放置与实例内执行，并允许在阶段边界处动态迁移以平衡负载并减少干扰。

Result: 在使用DeepSeek-R1-Distill-Qwen-32B的基准测试中，PASCAL最多减少了72%的尾部TTFT，同时保持了回答阶段的服务水平目标达成率。

Conclusion: 阶段感知调度对于基于推理的LLM部署至关重要，PASCAL通过优化推理和服务阶段显著提高了性能。

Abstract: The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.

</details>


### [53] [AltTS: A Dual-Path Framework with Alternating Optimization for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.11533)
*Zhihang Yuan,Zhiyuan Liu,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 本文提出了一种名为ALTTS的双路径框架，该框架将自回归(AR)和交叉关系(CR)建模显式解耦，通过交替优化来隔离梯度噪声并减少跨块干扰，从而在多变量时间序列预测中特别是在长期预测上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列预测涉及到两个定性不同的因素：（i）系列内部稳定的自回归(AR)动态；（ii）间歇性的跨维度交互作用，在长预测范围内可能会变得虚假。试图用单一模型同时捕捉这两种效应会导致优化冲突，影响训练稳定性和长期预测准确性。

Method: 提出了ALTTS，一个明确分离自回归和交叉关系(CR)建模的双路径框架。其中，AR路径使用线性预测器实现，而CR路径则采用配备了交叉关系自我注意机制(CRSA)的Transformer。这两条分支通过交替优化协调，以隔离梯度噪声并减少跨块干扰。

Result: 广泛的实验表明，ALTTS在多个基准测试中一致优于先前的方法，尤其是在长期预测方面表现尤为突出。

Conclusion: 精心设计的优化策略，而不是越来越复杂的架构，可以成为推动多变量时间序列预测进展的关键因素。

Abstract: Multivariate time series forecasting involves two qualitatively distinct factors: (i) stable within-series autoregressive (AR) dynamics, and (ii) intermittent cross-dimension interactions that can become spurious over long horizons. We argue that fitting a single model to capture both effects creates an optimization conflict: the high-variance updates needed for cross-dimension modeling can corrupt the gradients that support autoregression, resulting in brittle training and degraded long-horizon accuracy. To address this, we propose ALTTS, a dual-path framework that explicitly decouples autoregression and cross-relation (CR) modeling. In ALTTS, the AR path is instantiated with a linear predictor, while the CR path uses a Transformer equipped with Cross-Relation Self-Attention (CRSA); the two branches are coordinated via alternating optimization to isolate gradient noise and reduce cross-block interference. Extensive experiments on multiple benchmarks show that ALTTS consistently outperforms prior methods, with the most pronounced improvements on long-horizon forecasting. Overall, our results suggest that carefully designed optimization strategies, rather than ever more complex architectures, can be a key driver of progress in multivariate time series forecasting.

</details>


### [54] [Krause Synchronization Transformers](https://arxiv.org/abs/2602.11534)
*Jingkun Liu,Yisong Yue,Max Welling,Yue Song*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制——Krause Attention，它通过基于距离的局部和选择性稀疏交互来替代传统的全局聚合方式，从而促进结构化的局部同步而非全局混合。这种机制不仅有助于缓解注意力集中问题，还能将运行时间复杂度从序列长度的平方降低到线性级别。实验表明，该方法在视觉、自回归生成及大型语言模型上均表现出一致的优势，并显著减少了计算量。


<details>
  <summary>Details</summary>
Motivation: 作者注意到，Transformer中的自注意力机制依赖于全局归一化的softmax权重，导致所有token在每一层都竞争影响力。这种跨深度组成的互动模式会导致强烈的同步动力学，倾向于收敛到一个主导模式，这与表示崩溃和注意力沉陷现象有关。为了解决这些问题，提出了Krause Attention机制。

Method: Krause Attention受到有界信心共识动力学启发，用基于距离的局部化且选择性稀疏的交互代替了基于相似性的全局聚合。这样的设计促进了结构化的局部同步而不是全局混合，并且自然地调节了注意力集中度，减轻了注意力沉陷问题。此外，限制在局部邻域内的交互还将运行时复杂度从序列长度的二次方减少到了线性级别。

Result: 实验涵盖了视觉（ViT在CIFAR/ImageNet上的表现）、自回归生成（MNIST/CIFAR-10）以及大型语言模型（Llama/Qwen），结果表明Krause Attention能够带来一致的性能提升，同时大幅度减少了所需的计算量。

Conclusion: Krause Attention作为一种有效的归纳偏置，对于注意力机制来说是可扩展且高效的，它通过引入有界信心动态有效地解决了传统自注意力机制中存在的问题。

Abstract: Self-attention in Transformers relies on globally normalized softmax weights, causing all tokens to compete for influence at every layer. When composed across depth, this interaction pattern induces strong synchronization dynamics that favor convergence toward a dominant mode, a behavior associated with representation collapse and attention sink phenomena. We introduce Krause Attention, a principled attention mechanism inspired by bounded-confidence consensus dynamics. Krause Attention replaces similarity-based global aggregation with distance-based, localized, and selectively sparse interactions, promoting structured local synchronization instead of global mixing. We relate this behavior to recent theory modeling Transformer dynamics as interacting particle systems, and show how bounded-confidence interactions naturally moderate attention concentration and alleviate attention sinks. Restricting interactions to local neighborhoods also reduces runtime complexity from quadratic to linear in sequence length. Experiments across vision (ViT on CIFAR/ImageNet), autoregressive generation (MNIST/CIFAR-10), and large language models (Llama/Qwen) demonstrate consistent gains with substantially reduced computation, highlighting bounded-confidence dynamics as a scalable and effective inductive bias for attention.

</details>


### [55] [Native Reasoning Models: Training Language Models to Reason on Unverifiable Data](https://arxiv.org/abs/2602.11549)
*Yuanfu Wang,Zhixuan Liu,Xiangtian Li,Chaochao Lu,Chao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的训练框架NRT（原生推理训练），该框架通过仅使用标准问答对让模型生成自己的推理路径，从而摆脱了对高质量人工标注数据和外部验证器的依赖。实验表明，NRT在复杂推理领域表现优异，并且能够有效避免策略崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型训练范式依赖于高质量的人工标注推理数据和外部验证器，这不仅增加了数据收集成本，还可能嵌入人类的认知偏见，并将强化学习的应用范围限制在数学、编码等客观可评估领域内，无法覆盖大量不可验证的任务。

Method: NRT (Native Reasoning Training)框架将推理过程视为一个潜在变量来处理，采用统一的训练目标，将推理建模为优化问题，内在奖励那些增加模型产生正确答案可能性的路径。这一方法允许研究人员分析先前方法中存在的固有失败模式，如政策崩溃，并系统地设计更加稳健的奖励聚合函数，形成一种自我强化反馈循环，使模型学会以解决自身不确定性的思考方式。

Result: 基于Llama和Mistral模型家族的实证评估显示，NRT在无验证器方法中达到了最先进的性能，显著优于标准监督微调基线以及其他无验证器强化学习方法。特别是在复杂的推理任务上，NRT表现出色，并显示出对抗策略崩溃的高度鲁棒性。

Conclusion: NRT提供了一条通用且可扩展的道路，用于构建更强大且应用范围更广的推理系统。

Abstract: The prevailing paradigm for training large reasoning models--combining Supervised Fine-Tuning (SFT) with Reinforcement Learning with Verifiable Rewards (RLVR)--is fundamentally constrained by its reliance on high-quality, human-annotated reasoning data and external verifiers. This dependency incurs significant data-collection costs, risks embedding human cognitive biases, and confines the reinforcement learning stage to objectively assessable domains like mathematics and coding, leaving a wide range of unverifiable tasks beyond its scope. To overcome these limitations, we introduce NRT (Native Reasoning Training), a novel framework that cultivates complex reasoning by having the model generate its own reasoning traces using only standard question-answer pairs, thereby obviating the need for expert-written demonstrations. NRT reframes the training problem by treating the reasoning process as a latent variable. It employs a unified training objective that models reasoning as an optimization problem, intrinsically rewarding paths that increase the model's likelihood of producing the ground-truth answer. This unified perspective allows us to analyze intrinsic failure modes of prior methods, such as policy collapse, and systematically design more robust reward aggregation functions, creating a self-reinforcing feedback loop where the model learns to think in ways that resolve its own uncertainty. Empirical evaluation on Llama and Mistral model families demonstrates that NRT achieves state-of-the-art performance among verifier-free methods, significantly outperforming standard SFT baselines and prior verifier-free RL methods. Our approach yields particularly strong performance gains in complex reasoning domains and exhibits high robustness to policy collapse, offering a general, scalable path toward building more powerful and broadly applicable reasoning systems.

</details>


### [56] [TS-Memory: Plug-and-Play Memory for Time Series Foundation Models](https://arxiv.org/abs/2602.11550)
*Sisuo Lyu,Siru Zhong,Tiegang Chen,Weilin Ruan,Qingxiang Liu,Taiqiang Lv,Qingsong Wen,Raymond Chi-Wing Wong,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了一种名为TS-Memory的轻量级记忆适配器，通过两阶段训练过程将检索引起的分布校正蒸馏到适配器中，从而在不增加推理延迟的情况下改进了时间序列基础模型（TSFMs）在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应下游领域时面临参数调整导致灾难性遗忘和非参数检索造成高推理延迟的问题。

Method: Parametric Memory Distillation，具体实现为TS-Memory，包括构建一个离线、防泄漏kNN教师以合成置信度感知分位数目标，以及通过置信度门控监督将此校正蒸馏到轻量级记忆适配器中。

Result: 实验表明，TS-Memory在不同TSFMs和基准测试上一致地提高了点预测和概率预测的表现，并且效率与冻结的基础模型相当。

Conclusion: TS-Memory提供了一种有效的方法来提高TSFMs在面对分布偏移时对下游领域的适应能力，同时保持高效。

Abstract: Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires costly multi-domain maintenance, while Non-Parametric Retrieval improves forecasts but incurs high inference latency due to datastore search. We propose Parametric Memory Distillation and implement it as TS-Memory, a lightweight memory adapter that augments frozen TSFMs. TS-Memory is trained in two stages. First, we construct an offline, leakage-safe kNN teacher that synthesizes confidence-aware quantile targets from retrieved futures. Second, we distill this retrieval-induced distributional correction into a lightweight memory adapter via confidence-gated supervision. During inference, TS-Memory fuses memory and backbone predictions with constant-time overhead, enabling retrieval-free deployment. Experiments across diverse TSFMs and benchmarks demonstrate consistent improvements in both point and probabilistic forecasting over representative adaptation methods, with efficiency comparable to the frozen backbone.

</details>


### [57] [The Implicit Bias of Steepest Descent with Mini-batch Stochastic Gradient](https://arxiv.org/abs/2602.11557)
*Jichu Li,Xuan Tang,Difan Zou*

Main category: cs.LG

TL;DR: 该研究探讨了在多分类任务中，小批量随机最速下降法的隐式偏差如何受到批量大小、动量和方差减少的影响。研究发现，在没有动量的情况下，只有大批次才能收敛；而动量可以实现小批次收敛但会减慢收敛速度。此外，方差减少能够在任何批次大小下恢复完整的批次隐式偏差，尽管收敛速度较慢。通过统一分析，本研究为理解随机优化何时与全批量行为一致提供了清晰视角，并为进一步探索随机梯度最速下降算法的训练行为铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解不同范数诱导几何下解释的一系列广泛使用的优化方法（如SignSGD和Muon）在多分类问题中的隐式偏差特性，特别是批量大小、动量以及方差减少对最终最大边界行为及收敛速率的影响。

Method: 采用理论分析手段，研究了在没有动量情况下仅大批次能导致收敛的现象，同时也探讨了动量如何允许小批次收敛但以牺牲部分收敛速度为代价的过程。此外，还证明了方差减少技术可以在任何批次规模下恢复完全批次处理的隐式偏差，虽然这样做会降低收敛速率。

Result: 研究表明，无动量时需要较大的批次才能保证收敛性；引入动量后，即使使用较小的批次也能实现收敛，但收敛速度有所减慢。方差减少策略能够对于任意大小的批次重现全批次处理下的隐式偏差，不过这同样伴随着收敛速率的下降。特别地，当批次大小为1且不使用动量时，观察到了一种本质上不同的收敛偏移。

Conclusion: 整体而言，这项工作的综合分析揭示了随机优化方法在何种条件下能够与全批量处理表现出一致的行为特征，并为更深入地探究随机梯度最速下降算法的训练动态奠定了基础。

Abstract: A variety of widely used optimization methods like SignSGD and Muon can be interpreted as instances of steepest descent under different norm-induced geometries. In this work, we study the implicit bias of mini-batch stochastic steepest descent in multi-class classification, characterizing how batch size, momentum, and variance reduction shape the limiting max-margin behavior and convergence rates under general entry-wise and Schatten-$p$ norms. We show that without momentum, convergence only occurs with large batches, yielding a batch-dependent margin gap but the full-batch convergence rate. In contrast, momentum enables small-batch convergence through a batch-momentum trade-off, though it slows convergence. This approach provides fully explicit, dimension-free rates that improve upon prior results. Moreover, we prove that variance reduction can recover the exact full-batch implicit bias for any batch size, albeit at a slower convergence rate. Finally, we further investigate the batch-size-one steepest descent without momentum, and reveal its convergence to a fundamentally different bias via a concrete data example, which reveals a key limitation of purely stochastic updates. Overall, our unified analysis clarifies when stochastic optimization aligns with full-batch behavior, and paves the way for perform deeper explorations of the training behavior of stochastic gradient steepest descent algorithms.

</details>


### [58] [Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal](https://arxiv.org/abs/2602.11558)
*Fanqi Shen,Enhong Yang,Jiahe Li,Junru Hong,Xiaoran Pan,Zhizhang Yuan,Meng Li,Yang Yang*

Main category: cs.LG

TL;DR: 本文介绍了脑基础模型（BFMs）在神经科学中的应用，并提出了一个名为Brain4FMs的开放评估平台，该平台整合了15个代表性的BFMs和18个公开数据集，以标准化比较和分析预训练数据、自监督学习策略及架构对泛化性和下游性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着脑基础模型（BFMs）在临床诊断和前沿神经科学研究中的快速发展，领域内缺乏统一的方法论理解和标准化评估框架。为解决这一问题，文章旨在通过建立一个开放的评估平台来填补空白。

Method: 文章首先从模型视角出发，将BFMs归类于自监督学习（SSL）分类体系下；其次从数据集视角，总结了常见的下游任务并整理了临床与以人为中心的神经技术应用中具有代表性的公开数据集。基于此整合工作，开发了Brain4FMs平台，提供了即插即用接口，集成了15个BFMs和18个公开数据集。

Result: Brain4FMs平台支持标准化比较不同预训练数据、SSL策略以及架构对于模型泛化能力及下游任务表现的影响，有助于指导更准确且可迁移的BFM的发展。

Conclusion: 通过Brain4FMs平台的推出，研究者们能够更好地理解BFMs的设计选择如何影响其性能，促进了该领域朝向更加标准化和高效的方向发展。

Abstract: Brain Foundation Models (BFMs) are transforming neuroscience by enabling scalable and transferable learning from neural signals, advancing both clinical diagnostics and cutting-edge neuroscience exploration. Their emergence is powered by large-scale clinical recordings, particularly electroencephalography (EEG) and intracranial EEG, which provide rich temporal and spatial representations of brain dynamics. However, despite their rapid proliferation, the field lacks a unified understanding of existing methodologies and a standardized evaluation framework. To fill this gap, we map the benchmark design space along two axes: (i) from the model perspective, we organize BFMs under a self-supervised learning (SSL) taxonomy; and (ii) from the dataset perspective, we summarize common downstream tasks and curate representative public datasets across clinical and human-centric neurotechnology applications. Building on this consolidation, we introduce Brain4FMs, an open evaluation platform with plug-and-play interfaces that integrates 15 representative BFMs and 18 public datasets. It enables standardized comparisons and analysis of how pretraining data, SSL strategies, and architectures affect generalization and downstream performance, guiding more accurate and transferable BFMs. The code is available at https://anonymous.4open.science/r/Brain4FMs-85B8.

</details>


### [59] [Gradient Compression May Hurt Generalization: A Remedy by Synthetic Data Guided Sharpness Aware Minimization](https://arxiv.org/abs/2602.11584)
*Yujie Gu,Richeng Jin,Zhaoyang Zhang,Huaiyu Dai*

Main category: cs.LG

TL;DR: 本文发现联邦学习中的梯度压缩会导致损失景观变得更尖锐，特别是在非IID数据分布下，这可能影响泛化能力。为此，提出了FedSynSAM算法，通过利用全局模型轨迹来构建合成数据，从而准确估计全局扰动，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究指出，在联邦学习中使用梯度压缩虽然可以显著提高通信效率，但会使得损失景观变得更为尖锐，尤其是在面对非独立同分布的数据时，这可能会损害模型的泛化能力。而直接在联邦学习中应用Sharpness Aware Minimization (SAM)方法由于数据异质性问题面临全球扰动估计不准的问题。

Method: 提出了一种名为FedSynSAM的新方法，该方法通过利用全局模型轨迹生成合成数据，进而促进对全局扰动的准确估计。

Result: 理论分析上确立了所提算法的收敛性，并通过大量实验证明了FedSynSAM的有效性。

Conclusion: FedSynSAM为解决联邦学习中梯度压缩导致的损失景观尖锐化问题提供了一个有效的解决方案，特别是对于处理非IID数据情况下的挑战。

Abstract: It is commonly believed that gradient compression in federated learning (FL) enjoys significant improvement in communication efficiency with negligible performance degradation. In this paper, we find that gradient compression induces sharper loss landscapes in federated learning, particularly under non-IID data distributions, which suggests hindered generalization capability. The recently emerging Sharpness Aware Minimization (SAM) effectively searches for a flat minima by incorporating a gradient ascent step (i.e., perturbing the model with gradients) before the celebrated stochastic gradient descent. Nonetheless, the direct application of SAM in FL suffers from inaccurate estimation of the global perturbation due to data heterogeneity. Existing approaches propose to utilize the model update from the previous communication round as a rough estimate. However, its effectiveness is hindered when model update compression is incorporated. In this paper, we propose FedSynSAM, which leverages the global model trajectory to construct synthetic data and facilitates an accurate estimation of the global perturbation. The convergence of the proposed algorithm is established, and extensive experiments are conducted to validate its effectiveness.

</details>


### [60] [SkillRater: Untangling Capabilities in Multimodal Data](https://arxiv.org/abs/2602.11615)
*Naveen Sahi,Jeremy Dohmann,Armen Aghajanyan,Akshat Shrivastava*

Main category: cs.LG

TL;DR: 本文提出了一种名为SkillRater的新框架，它将数据质量评估分解为多个维度，每个维度对应模型需要获得的一种能力。通过这种多维度的方法，在视觉语言模型上的实验表明，相比未过滤的基线和单一评分方法，SkillRater在视觉理解、OCR以及STEM推理等不同能力上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的数据管理方法通常仅给样本分配一个质量分数，这种方法存在局限性：当训练需要多种不同的能力时，单一的评分机制无法同时最大化所有这些能力所需的有用信号。因此，作者认为质量应该是多维的概念，每个维度都对应着模型必须掌握的一种能力。

Method: 引入了SkillRater框架，该框架将数据筛选分解为专门的评价者——每种能力一个，每个都是通过元学习在不相交的验证目标上训练得到的——并通过一种渐进的选择规则来综合他们的评分：在每个训练阶段，如果任何一个评价者的评分高于随时间逐渐提高的阈值，则保留该样本，这样既保持了早期的多样性又在后期集中于高价值样本。

Result: 在20亿参数规模下，与未经筛选的基础模型相比，SkillRater在视觉理解方面提高了5.63%，OCR方面提高了2.00%，STEM推理方面提高了3.53%。所学得的评价信号几乎是正交的，这证实了分解确实捕捉到了真正独立的质量维度，解释了为什么它优于未经过滤的训练和单一学习过的过滤方法。

Conclusion: SkillRater提供了一种有效的方法来根据模型所需的不同能力对数据进行多维度的质量评估，从而提高模型性能。实验证明了这种方法的有效性和优越性。

Abstract: Data curation methods typically assign samples a single quality score. We argue this scalar framing is fundamentally limited: when training requires multiple distinct capabilities, a monolithic scorer cannot maximize useful signals for all of them simultaneously. Quality is better understood as multidimensional, with each dimension corresponding to a capability the model must acquire. We introduce SkillRater, a framework that decomposes data filtering into specialized raters - one per capability, each trained via meta-learning on a disjoint validation objective - and composes their scores through a progressive selection rule: at each training stage, a sample is retained if any rater ranks it above a threshold that tightens over time, preserving diversity early while concentrating on high-value samples late. We validate this approach on vision language models, decomposing quality into three capability dimensions: visual understanding, OCR, and STEM reasoning. At 2B parameters, SkillRater improves over unfiltered baselines by 5.63% on visual understanding, 2.00% on OCR, and 3.53% on STEM on held out benchmarks. The learned rater signals are near orthogonal, confirming that the decomposition captures genuinely independent quality dimensions and explaining why it outperforms both unfiltered training and monolithic learned filtering.

</details>


### [61] [How Well Do Large-Scale Chemical Language Models Transfer to Downstream Tasks?](https://arxiv.org/abs/2602.11618)
*Tatsuya Sagawa,Ryosuke Kojima*

Main category: cs.LG

TL;DR: 本研究探讨了在化学领域中，增加训练资源（如模型大小、数据集规模和计算能力）是否能同时提高预训练损失和下游任务性能。结果表明，尽管预训练损失随着训练资源的增加而持续下降，但下游任务的表现仅显示出有限的改进。此外，基于Hessian或损失景观的替代指标也无法准确估计下游表现。研究还发现，在某些情况下，即使预训练指标有所改善，下游性能也可能达到饱和甚至退化。


<details>
  <summary>Details</summary>
Motivation: 尽管普遍认为增加训练资源可以提升预训练效果及下游任务性能，但在化学领域这一假设尚未得到系统性验证。本研究旨在通过调整训练资源并测量跨多种分子属性预测任务中的迁移性能来检验这一观点。

Method: 研究者们通过改变化学语言模型的训练资源，包括模型大小、数据集大小以及训练算力，并对不同分子属性预测任务进行迁移学习性能评估。除了观察预训练损失的变化外，还探索了使用Hessian矩阵或损失地形图作为替代度量的可能性。

Result: 研究发现，随着训练资源的增长，预训练损失确实呈现下降趋势；然而，对于下游任务来说，其性能改善却十分有限。而且，尝试利用Hessian矩阵或者损失地形图等其他度量方式来预测下游任务表现也未能成功。更重要的是，当预训练指标继续优化时，某些条件下下游任务表现反而停滞不前甚至恶化。

Conclusion: 这项研究表明，仅仅依赖于预训练阶段的评价可能不足以反映模型在实际应用场景下的表现。因此，强调需要开发能够更直接地考虑下游任务特性的模型选择与评估策略。

Abstract: Chemical Language Models (CLMs) pre-trained on large scale molecular data are widely used for molecular property prediction. However, the common belief that increasing training resources such as model size, dataset size, and training compute improves both pretraining loss and downstream task performance has not been systematically validated in the chemical domain. In this work, we evaluate this assumption by pretraining CLMs while scaling training resources and measuring transfer performance across diverse molecular property prediction (MPP) tasks. We find that while pretraining loss consistently decreases with increased training resources, downstream task performance shows limited improvement. Moreover, alternative metrics based on the Hessian or loss landscape also fail to estimate downstream performance in CLMs. We further identify conditions under which downstream performance saturates or degrades despite continued improvements in pretraining metrics, and analyze the underlying task dependent failure modes through parameter space visualizations. These results expose a gap between pretraining based evaluation and downstream performance, and emphasize the need for model selection and evaluation strategies that explicitly account for downstream task characteristics.

</details>


### [62] [TreeGrad-Ranker: Feature Ranking via $O(L)$-Time Gradients for Decision Trees](https://arxiv.org/abs/2602.11623)
*Weida Li,Yaoliang Yu,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文重新审视了使用概率值（包括Shapley值和Banzhaf值）对决策树的局部预测值进行特征排序的方法，并提出了TreeGrad算法来直接优化联合目标，从而产生更优的特征排名。


<details>
  <summary>Details</summary>
Motivation: 研究者们希望改进用于解释决策树局部预测值的特征排序方法。尽管传统上使用Shapley和Banzhaf值等概率值来进行特征重要性排序，但这些方法在同时优化插入和删除指标时表现不佳。

Method: 提出了一种新的方法TreeGrad，它能够计算联合目标的多线性扩展梯度，进而直接优化这个目标。基于TreeGrad开发了两个工具：TreeGrad-Ranker用于生成特征排名，而TreeGrad-Shap则用于计算Beta Shapley值。

Result: 实验证明，TreeGrad-Ranker在插入和删除指标上都显著优于现有方法。此外，TreeGrad-Shap相比Linear TreeShap在数值稳定性方面也有极大提升。

Conclusion: 新提出的TreeGrad及其衍生方法不仅在理论上提供了更好的框架来解决特征排序问题，而且通过实验也证明了其优越性。

Abstract: We revisit the use of probabilistic values, which include the well-known Shapley and Banzhaf values, to rank features for explaining the local predicted values of decision trees. The quality of feature rankings is typically assessed with the insertion and deletion metrics. Empirically, we observe that co-optimizing these two metrics is closely related to a joint optimization that selects a subset of features to maximize the local predicted value while minimizing it for the complement. However, we theoretically show that probabilistic values are generally unreliable for solving this joint optimization. Therefore, we explore deriving feature rankings by directly optimizing the joint objective. As the backbone, we propose TreeGrad, which computes the gradients of the multilinear extension of the joint objective in $O(L)$ time for decision trees with $L$ leaves; these gradients include weighted Banzhaf values. Building upon TreeGrad, we introduce TreeGrad-Ranker, which aggregates the gradients while optimizing the joint objective to produce feature rankings, and TreeGrad-Shap, a numerically stable algorithm for computing Beta Shapley values with integral parameters. In particular, the feature scores computed by TreeGrad-Ranker satisfy all the axioms uniquely characterizing probabilistic values, except for linearity, which itself leads to the established unreliability. Empirically, we demonstrate that the numerical error of Linear TreeShap can be up to $10^{15}$ times larger than that of TreeGrad-Shap when computing the Shapley value. As a by-product, we also develop TreeProb, which generalizes Linear TreeShap to support all probabilistic values. In our experiments, TreeGrad-Ranker performs significantly better on both insertion and deletion metrics. Our code is available at https://github.com/watml/TreeGrad.

</details>


### [63] [GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks](https://arxiv.org/abs/2602.11629)
*Dongxiao He,Wenxuan Sun,Yongqi Huang,Jitao Zhao,Di Jin*

Main category: cs.LG

TL;DR: 本文探讨了图提示学习（GPL）在跨领域场景下的有效性，并提出了一种双分支GPL方法GP2F，该方法通过保留预训练知识和轻量级适配器进行任务特定调整，从而在跨领域小样本节点和图分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管GPL在从预训练目标到下游任务的适应过程中表现出色，特别是在跨领域场景下，但其在领域迁移情况下仍能保持有效性的原因尚未被探索。研究者观察到，在跨领域设置中，GPL方法与全微调（FT）和线性探测（LP）两种简单基线相比具有竞争力，这促使他们深入理解提示机制背后的原因。

Method: 本文首先进行了理论分析，证明结合使用两个互补分支（即保留预训练知识的分支和用于任务特定适应的分支）可以比单独使用任一分支获得更小的估计误差。基于这一发现，提出了GP2F方法，它包含一个冻结分支来保留预训练的知识，以及一个带有轻量级适配器的适应分支来进行任务特定调整。此外，通过对比损失和拓扑一致性损失实现了在拓扑约束下的自适应融合。

Result: 实验结果表明，在跨领域的少量样本节点分类和图分类任务上，所提出的方法GP2F相较于现有的方法表现更好。

Conclusion: 本研究表明，通过将预训练知识与任务特定适应相结合的方式，可以有效地提高GPL在跨领域场景中的性能。提出的GP2F方法不仅为理解GPL提供了一个新的视角，也为实际应用提供了一种有效的解决方案。

Abstract: Graph Prompt Learning (GPL) has recently emerged as a promising paradigm for downstream adaptation of pre-trained graph models, mitigating the misalignment between pre-training objectives and downstream tasks. Recently, the focus of GPL has shifted from in-domain to cross-domain scenarios, which is closer to the real world applications, where the pre-training source and downstream target often differ substantially in data distribution. However, why GPLs remain effective under such domain shifts is still unexplored. Empirically, we observe that representative GPL methods are competitive with two simple baselines in cross-domain settings: full fine-tuning (FT) and linear probing (LP), motivating us to explore a deeper understanding of the prompting mechanism. We provide a theoretical analysis demonstrating that jointly leveraging these two complementary branches yields a smaller estimation error than using either branch alone, formally proving that cross-domain GPL benefits from the integration between pre-trained knowledge and task-specific adaptation. Based on this insight, we propose GP2F, a dual-branch GPL method that explicitly instantiates the two extremes: (1) a frozen branch that retains pre-trained knowledge, and (2) an adapted branch with lightweight adapters for task-specific adaptation. We then perform adaptive fusion under topology constraints via a contrastive loss and a topology-consistent loss. Extensive experiments on cross-domain few-shot node and graph classification demonstrate that our method outperforms existing methods.

</details>


### [64] [TIP: Resisting Gradient Inversion via Targeted Interpretable Perturbation in Federated Learning](https://arxiv.org/abs/2602.11633)
*Jianhua Wang,Yinlin Su*

Main category: cs.LG

TL;DR: 本文提出了一种新的防御框架TIP，通过结合模型可解释性与频域分析来对抗联邦学习中的梯度反转攻击。该方法选择性地对高频分量注入扰动，从而破坏图像重建所需的细节信息，同时保持了模型的准确性。实验表明，TIP在隐私-实用性权衡及可解释性方面优于现有的基于差分隐私的防御措施。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然可以实现在保留数据本地性的前提下进行协作训练，但其梯度交换机制使其容易受到梯度反转攻击（GIA），攻击者能够以高保真度重构私人训练数据。现有的防御手段如差分隐私（DP）通常通过对所有参数无差别地加入噪声来实现，但这严重降低了模型的有效性和收敛稳定性。为解决这些问题，提出了TIP。

Method: TIP框架整合了模型可解释性与频率域分析技术。首先使用Grad-CAM量化通道敏感度，动态识别编码主要语义特征的关键卷积通道；接着将这些选定的核转换到频率域，并有选择性地向高频谱部分注入校准后的扰动。这样的设计旨在通过干扰高频成分来破坏用于图像重建的精细细节，同时保持对模型准确度至关重要的低频信息。

Result: 在基准数据集上的广泛测试显示，TIP能使针对最先进GIA的重建图片变得视觉上无法辨认，与此同时维持与非私有基线相当的整体模型精度，在隐私保护与实用价值之间的平衡以及可解释性方面显著优于现有基于DP的防御策略。

Conclusion: TIP作为一种创新的防御方案，成功解决了联邦学习中因梯度反转攻击而引发的数据隐私泄露问题，它不仅提高了抵抗此类攻击的能力，还保证了较高的模型性能和更好的隐私-实用性权衡。

Abstract: Federated Learning (FL) facilitates collaborative model training while preserving data locality; however, the exchange of gradients renders the system vulnerable to Gradient Inversion Attacks (GIAs), allowing adversaries to reconstruct private training data with high fidelity. Existing defenses, such as Differential Privacy (DP), typically employ indiscriminate noise injection across all parameters, which severely degrades model utility and convergence stability. To address those limitation, we proposes Targeted Interpretable Perturbation (TIP), a novel defense framework that integrates model interpretability with frequency domain analysis. Unlike conventional methods that treat parameters uniformly, TIP introduces a dual-targeting strategy. First, leveraging Gradient-weighted Class Activation Mapping (Grad-CAM) to quantify channel sensitivity, we dynamically identify critical convolution channels that encode primary semantic features. Second, we transform these selected kernels into the frequency domain via the Discrete Fourier Transform and selectively inject calibrated perturbations into the high-frequency spectrum. By selectively perturbing high-frequency components, TIP effectively destroys the fine-grained details necessary for image reconstruction while preserving the low-frequency information crucial for model accuracy. Extensive experiments on benchmark datasets demonstrate that TIP renders reconstructed images visually unrecognizable against state-of-the-art GIAs, while maintaining global model accuracy comparable to non-private baselines, significantly outperforming existing DP-based defenses in the privacy-utility trade-off and interpretability. Code is available in https://github.com/2766733506/asldkfjssdf_arxiv

</details>


### [65] [UMAP Is Spectral Clustering on the Fuzzy Nearest-Neighbor Graph](https://arxiv.org/abs/2602.11662)
*Yang Yang*

Main category: cs.LG

TL;DR: 本文证明了UMAP算法实际上是在模糊k最近邻图上执行谱聚类，并通过三个步骤建立了UMAP与对比学习以及谱聚类之间的等价关系，为UMAP的行为提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 尽管UMAP是一种广泛使用的非线性降维和数据可视化算法，但其与经典谱方法之间的精确关系一直未被正式定义。

Method: 1. 证明UMAP的随机优化带负采样是相似度图上的对比学习目标；2. 引用HaoChen等人的研究结果，说明相似度图上的对比学习等同于谱聚类；3. 验证UMAP的谱初始化计算了该谱问题的确切线性解。

Result: 对于高斯核来说，这种等价关系是完全成立的，而对于UMAP默认的柯西型核，则作为一阶近似成立。

Conclusion: 这项工作统一了UMAP、对比学习和谱聚类到一个框架下，为UMAP的行为提供了一些经验观察背后的理论基础。

Abstract: UMAP (Uniform Manifold Approximation and Projection) is among the most widely used algorithms for non linear dimensionality reduction and data visualisation. Despite its popularity, and despite being presented through the lens of algebraic topology, the exact relationship between UMAP and classical spectral methods has remained informal. In this work, we prove that UMAP performs spectral clustering on the fuzzy k nearest neighbour graph. Our proof proceeds in three steps: (1) we show that UMAP's stochastic optimisation with negative sampling is a contrastive learning objective on the similarity graph; (2) we invoke the result of HaoChen et al. [8], establishing that contrastive learning on a similarity graph is equivalent to spectral clustering; and (3) we verify that UMAP's spectral initialisation computes the exact linear solution to this spectral problem. The equivalence is exact for Gaussian kernels, and holds as a first order approximation for UMAP's default Cauchy type kernel. Our result unifies UMAP, contrastive learning, and spectral clustering under a single framework, and provides theoretical grounding for several empirical observations about UMAP's behaviour.

</details>


### [66] [Fully First-Order Algorithms for Online Bilevel Optimization](https://arxiv.org/abs/2602.11665)
*Tingkai Jia,Cheng Chen*

Main category: cs.LG

TL;DR: 提出了一种全一阶在线双层优化算法，通过将原问题重新表述为带不等式约束的单层在线问题并构造一系列拉格朗日函数来避免使用Hessian-vector产品，从而降低了计算成本。此外，还开发了一种改进版本，采用自适应内迭代方案进一步提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的在线双层优化(OBO)算法主要基于超梯度下降法，这种方法需要访问Hessian-vector乘积(HVP)oracle，可能会导致较高的计算成本。研究旨在通过消除隐式微分产生的对HVP的需求来降低这种开销。

Method: 通过将原始OBO问题重构成一个具有不等式约束的单层在线问题，并构建了一系列拉格朗日函数的方法。提出了一种完全基于一阶信息的OBO算法，并提供理论保证其达到$O(1 + V_T + H_{2,T})$遗憾界。另外，发展了一个改进版算法，该算法引入了自适应内部迭代机制，去除了对于内部最优解漂移变化的依赖性，并达到了$O(\sqrt{T} + V_T)$的遗憾界。

Result: 所提出的算法在理论上被证明可以达到$O(1 + V_T + H_{2,T})$的遗憾界；而改进后的算法则能实现$O(\sqrt{T} + V_T)$的遗憾界，在$V_{T}\ge O(\sqrt{T})$时显示出优势。

Conclusion: 本文介绍的方法有效地解决了非凸-强凸在线双层优化问题中高计算成本的问题，通过避免使用二阶梯度信息（即Hessian-vector乘积）达到了良好的性能表现。特别是当内层最优解的变化不大于特定阈值时，改进后的算法能够进一步提高效率。

Abstract: In this work, we study non-convex-strongly-convex online bilevel optimization (OBO). Existing OBO algorithms are mainly based on hypergradient descent, which requires access to a Hessian-vector product (HVP) oracle and potentially incurs high computational costs. By reformulating the original OBO problem as a single-level online problem with inequality constraints and constructing a sequence of Lagrangian function, we eliminate the need for HVPs arising from implicit differentiation. Specifically, we propose a fully first-order algorithm for OBO, and provide theoretical guarantees showing that it achieves regret of $O(1 + V_T + H_{2,T})$. Furthermore, we develop an improved variant with an adaptive inner-iteration scheme, which removes the dependence on the drift variation of the inner-level optimal solution and achieves regret of $O(\sqrt{T} + V_T)$. This regret have the advatange when $V_{T}\ge O(\sqrt{T})$.

</details>


### [67] [Explainable Machine-Learning based Detection of Knee Injuries in Runners](https://arxiv.org/abs/2602.11668)
*David Fuentes-Jiménez,Sara García-de-Villa,David Casillas-Pérez,Pablo Floría,Francisco-Manuel Melgarejo-Meseguer*

Main category: cs.LG

TL;DR: 该研究使用光学运动捕捉系统分析了839份跑步机记录，以识别与膝盖受伤相关的步态模式。通过结合时间序列和点值数据，深度学习模型（尤其是CNN）在识别髌股疼痛综合症(PFPS)和髂胫带综合症(ITBS)方面表现出优越性能，准确率分别达到77.9%、73.8%，对于合并伤害类别则为71.43%。


<details>
  <summary>Details</summary>
Motivation: 跑步时膝关节受伤的高发性，特别是髌股疼痛综合症(PFPS)和髂胫带综合症(ITBS)，需要通过精确的系统来捕捉并分析时间动态数据，从而识别与这些损伤相关的步态模式，以改善临床决策。

Method: 研究利用光学运动捕捉系统对包含健康跑者及受伤跑者的839份跑步机录像进行分析，重点放在支撑阶段，采用关节和节段角度的时间序列及离散点值。实验中考察了从传统基于点的度量到全支撑阶段时间序列以及混合表示的不同特征空间，并测试了包括经典算法（K-最近邻、高斯过程、决策树）和深度学习架构（CNNs、LSTMs）在内的多种模型。

Result: 结果表明，将时间序列与点值相结合能够显著提高检测效果。深度学习模型的表现优于经典模型，其中CNN在PFPS、ITBS以及合并伤害类别的分类上达到了最高的准确率：分别为77.9%、73.8%和71.43%。

Conclusion: 本研究表明，结合先进的机器学习技术的运动捕捉系统在识别与膝伤相关的跑步模式方面具有很大的潜力。

Abstract: Running is a widely practiced activity but shows a high incidence of knee injuries, especially Patellofemoral Pain Syndrome (PFPS) and Iliotibial Band Syndrome (ITBS). Identifying gait patterns linked to these injuries can improve clinical decision-making, which requires precise systems capable of capturing and analyzing temporal kinematic data.
  This study uses optical motion capture systems to enhance detection of injury-related running patterns. We analyze a public dataset of 839 treadmill recordings from healthy and injured runners to evaluate how effectively these systems capture dynamic parameters relevant to injury classification. The focus is on the stance phase, using joint and segment angle time series and discrete point values.
  Three classification tasks are addressed: healthy vs. injured, healthy vs. PFPS, and healthy vs. ITBS. We examine different feature spaces, from traditional point-based metrics to full stance-phase time series and hybrid representations. Multiple models are tested, including classical algorithms (K-Nearest Neighbors, Gaussian Processes, Decision Trees) and deep learning architectures (CNNs, LSTMs).
  Performance is evaluated with accuracy, precision, recall, and F1-score. Explainability tools such as Shapley values, saliency maps, and Grad-CAM are used to interpret model behavior. Results show that combining time series with point values substantially improves detection. Deep learning models outperform classical ones, with CNNs achieving the highest accuracy: 77.9% for PFPS, 73.8% for ITBS, and 71.43% for the combined injury class.
  These findings highlight the potential of motion capture systems coupled with advanced machine learning to identify knee injury-related running patterns.

</details>


### [68] [DRACO: a Cross-Domain Benchmark for Deep Research Accuracy, Completeness, and Objectivity](https://arxiv.org/abs/2602.11685)
*Joey Zhong,Hao Zhang,Clare Southern,Jeremy Yang,Thomas Wang,Kate Jung,Shu Zhang,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: 介绍了DRACO，一个用于评估复杂深度研究任务的基准测试集，涵盖10个领域和40个国家的信息源。该数据集基于匿名的真实世界使用模式构建，任务经过筛选和增强以确保其匿名性、开放性和复杂度，并且可以客观评估。评估维度包括事实准确性、分析广度与深度、呈现质量以及引用质量。


<details>
  <summary>Details</summary>
Motivation: 为了创建一个能够代表真实世界中复杂深度研究用例广泛范围的基准测试集，同时确保这些案例是匿名处理、开放性问题并且足够复杂，以便于客观评价。

Method: 从大规模深度研究系统中的匿名实际使用模式抽取任务样本，然后对这些样本进行过滤和增补，保证它们既匿名又具有开放性和复杂性，并且可以被客观地评价。每项任务都根据特定的任务评分标准在四个维度上进行打分：事实准确性、分析的广度与深度（包含完整性）、展示质量（包含客观性）以及引用质量。

Result: 开发了一个名为DRACO的新基准测试集，它包含了跨越10个不同领域和来自40个国家信息源的任务。这个公开可用的数据集旨在帮助研究人员更好地理解和改进复杂的深度研究任务。

Conclusion: 通过提供这样一个综合性的基准测试集DRACO，研究人员现在有了一个强有力的新工具来衡量和提高他们在执行复杂深度研究任务时的表现。

Abstract: We present DRACO (Deep Research Accuracy, Completeness, and Objectivity), a benchmark of complex deep research tasks. These tasks, which span 10 domains and draw on information sources from 40 countries, originate from anonymized real-world usage patterns within a large-scale deep research system. Tasks are sampled from a de-identified dataset of Perplexity Deep Research requests, then filtered and augmented to ensure that the tasks are anonymized, open-ended and complex, objectively evaluable, and representative of the broad scope of real-world deep research use cases. Outputs are graded against task-specific rubrics along four dimensions: factual accuracy (accuracy), breadth and depth of analysis (including completeness), presentation quality (including objectivity), and citation quality. DRACO is publicly available at https://hf.co/datasets/perplexity-ai/draco.

</details>


### [69] [ANML: Attribution-Native Machine Learning with Guaranteed Robustness](https://arxiv.org/abs/2602.11690)
*Oliver Zahn,Matt Beton,Simran Chana*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器学习框架ANML，该框架通过四个质量因素对训练样本进行加权处理。实验表明，ANML在多个数据集上比仅基于梯度的方法减少了33-72%的错误率，并且即使在面对复杂的联合攻击时也能够保持不低于最佳基线的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的人工智能系统在使用专家数据进行训练时，没有区分不同质量的数据样本的重要性。为了解决这一问题，作者引入了考虑数据来源和贡献者信誉等因素的新方法。

Method: 提出了归因原生机器学习(ANML)框架，它根据四个质量因素——基于梯度的一致性(q)、验证状态(v)、贡献者声誉(r)以及时间相关性(T)来给训练样本分配权重。此外还设计了一个两阶段自适应门控机制以确保ANML不会低于任何可用的最佳基线表现。

Result: ANML在五个不同的数据集上实现了相较于仅依赖梯度信号基准高达33-72%的误差减少。高质量数据占20%的情况下，其性能优于100%均匀加权数据47%。当单独样本检测失败时，贡献者级别的归因提供了比样本级别方法高出1.3至5.3倍的改进。

Conclusion: ANML提供了一种有效利用高质量数据的方法，同时提高了模型性能并支持下游归属分析。该方法不仅提升了模型准确性，而且在面对复杂攻击时表现出鲁棒性。

Abstract: Frontier AI systems increasingly train on specialized expert data, from clinical records to proprietary research to curated datasets, yet current training pipelines treat all samples identically. A Nobel laureate's contribution receives the same weight as an unverified submission. We introduce ANML (Attribution-Native Machine Learning), a framework that weights training samples by four quality factors: gradient-based consistency (q), verification status (v), contributor reputation (r), and temporal relevance (T). By combining what the model observes (gradient signals) with what the system knows about data provenance (external signals), ANML produces per-contributor quality weights that simultaneously improve model performance and enable downstream attribution. Across 5 datasets (178-32,561 samples), ANML achieves 33-72% error reduction over gradient-only baselines. Quality-weighted training is data-efficient: 20% high-quality data outperforms 100% uniformly weighted data by 47%. A Two-Stage Adaptive gating mechanism guarantees that ANML never underperforms the best available baseline, including under strategic joint attacks combining credential faking with gradient alignment. When per-sample detection fails against subtle corruption, contributor-level attribution provides 1.3-5.3x greater improvement than sample-level methods, with the advantage growing as corruption becomes harder to detect.

</details>


### [70] [SpiralFormer: Looped Transformers Can Learn Hierarchical Dependencies via Multi-Resolution Recursion](https://arxiv.org/abs/2602.11698)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,You Wu,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为SpiralFormer的循环Transformer模型，它采用多分辨率递归调度来执行迭代。这种方法使得模型能够学习跨不同尺度的层次依赖关系，并在参数和计算效率方面优于其他循环和非循环基线模型。


<details>
  <summary>Details</summary>
Motivation: 早期的循环Transformer通常在相同计算量下表现不如非循环基线模型。尽管最近的研究引入了更有效的递归机制以减少这种差距，但现有架构仍然以固定的全令牌分辨率运行，忽视了在压缩潜在表示上进行计算的可能效率。

Method: 提出了SpiralFormer，这是一种基于多分辨率递归调度执行的循环Transformer。通过这种方式，模型能够在不同的尺度上诱导出迭代级别的功能专业化，从而学习到层次依赖性。

Result: 实验证明，SpiralFormer在从160M到1.4B的不同模型规模上，都比循环与非循环基线模型展现出了更好的参数及计算效率。这表明序列分辨率可能是扩展递归架构的一个潜在方向。

Conclusion: SpiralFormer通过引入多分辨率递归机制，在提高参数和计算效率的同时，也展示了如何利用递归结构来更好地捕捉数据中的层次信息，为递归架构的发展提供了新的视角。

Abstract: Recursive (looped) Transformers decouple computational depth from parameter depth by repeatedly applying shared layers, providing an explicit architectural primitive for iterative refinement and latent reasoning. However, early looped Transformers often underperform non-recursive baselines of equal compute. While recent literature has introduced more effective recursion mechanisms to mitigate this gap, existing architectures still operate at a fixed, full-token resolution, neglecting the potential efficiency of computing over compressed latent representations. In this paper, we propose SpiralFormer, a looped Transformer that executes recurrence under a multi-resolution recursion schedule. We provide probing evidence that multi-resolution recursion enables the model to learn hierarchical dependencies by inducing iteration-wise functional specialization across different scales. Empirically, SpiralFormer achieves better parameter and compute efficiency than both looped and non-looped baselines across model scales from 160M to 1.4B, establishing sequence resolution as a potential axis for scaling recursive architectures.

</details>


### [71] [TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction](https://arxiv.org/abs/2602.11700)
*Yongyao Wang,Ziqi Miao,Lu Yang,Haonan Jia,Wenting Yan,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 提出了一种名为TabSieve的新框架，该框架通过首先选择少量信息行作为证据，然后基于所选证据预测缺失目标，从而改进了表格预测任务。此外还开发了相应的训练数据集TabSieve-SFT-40K和强化学习方法TAB-GRPO来优化证据选择与预测准确性。实验表明，TabSieve在分类和回归任务上均优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有的表格模型通常进行实例级推理，并且基于大语言模型的提示往往不够稳定。这些模型不能一致地利用相关行，而嘈杂的上下文会降低性能。为了解决这个问题，研究者提出了一个明确且可审计的证据使用框架。

Method: 设计了一个名为TabSieve的选择-预测框架，它首先从给定的表中选取一小部分信息量大的行作为证据，接着基于选出的证据来预测查询行中的缺失目标值。为了支持这一功能，研究人员构建了高质量的数据集TabSieve-SFT-40K，并引入了TAB-GRPO这一强化学习算法，后者能够同时优化证据选择和预测准确性的奖励机制，并通过动态任务优势平衡来稳定混合回归和分类训练过程。

Result: 在包含75个分类表和52个回归表的保留测试集上的实验显示，与次优基线相比，TabSieve在所有样本预算条件下都能持续提高表现，在分类任务上平均提高了2.92%，在回归任务上平均提升了4.45%。进一步分析表明，TabSieve更集中关注于选定的证据上，这有助于提高对噪声环境的鲁棒性。

Conclusion: TabSieve提供了一种新的方式来改善表格预测任务的表现，特别是通过更好地利用表内少数示例作为证据。该框架不仅提高了预测准确性，还增强了模型对于噪音数据的抵抗力。

Abstract: Tabular prediction can benefit from in-table rows as few-shot evidence, yet existing tabular models typically perform instance-wise inference and LLM-based prompting is often brittle. Models do not consistently leverage relevant rows, and noisy context can degrade performance. To address this challenge, we propose TabSieve, a select-then-predict framework that makes evidence usage explicit and auditable. Given a table and a query row, TabSieve first selects a small set of informative rows as evidence and then predicts the missing target conditioned on the selected evidence. To enable this capability, we construct TabSieve-SFT-40K by synthesizing high-quality reasoning trajectories from 331 real tables using a strong teacher model with strict filtering. Furthermore, we introduce TAB-GRPO, a reinforcement learning recipe that jointly optimizes evidence selection and prediction correctness with separate rewards, and stabilizes mixed regression and classification training via dynamic task-advantage balancing. Experiments on a held-out benchmark of 75 classification and 52 regression tables show that TabSieve consistently improves performance across shot budgets, with average gains of 2.92% on classification and 4.45% on regression over the second-best baseline. Further analysis indicates that TabSieve concentrates more attention on the selected evidence, which improves robustness to noisy context.

</details>


### [72] [Potential-energy gating for robust state estimation in bistable stochastic systems](https://arxiv.org/abs/2602.11712)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 本文提出了一种基于势能门控的方法，用于双阱随机动力学系统中的鲁棒状态估计。通过将观测噪声协方差与已知或假设的势能函数局部值联系起来，该方法能够根据状态接近势能极小值的程度来调整对观测的信任度。实验表明，此方法在合成基准测试和实际应用中都显著优于标准扩展卡尔曼滤波器等传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯滤波器在处理所有状态空间区域时采用统一的方式，而约束滤波器则通过对状态施加硬性边界来限制状态，这些方法在处理具有复杂势能景观的系统时表现不佳。因此，作者提出了基于势能门控的方法，旨在通过考虑物理机制来改进状态估计的准确性与鲁棒性。

Method: 提出的势能门控方法通过调节观测噪声协方差实现，具体而言是依据当前状态下的势能值动态调整对观测的信任程度。该技术被集成到几种不同类型的卡尔曼滤波器以及粒子滤波器中，并仅需引入两个额外超参数即可实施。

Result: 在Ginzburg-Landau双阱过程的合成基准测试中，即使存在10%的异常值污染情况下，相较于标准扩展卡尔曼滤波器，所提方法实现了57-80%的RMSE（均方根误差）降低。此外，在应用于NGRIP delta-18O冰芯记录的数据分析时，也展示了其在估计非对称参数γ及解释过滤器性能改进方面的能力。

Conclusion: 研究表明，基于势能门控的状态估计方法不仅能够有效提高面对异常值时的鲁棒性和准确性，而且对于潜在参数错误指定的情况也表现出良好的容忍度。这表明该方法在处理复杂动力学系统中的状态估计问题上具有广泛的应用前景。

Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.

</details>


### [73] [DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels](https://arxiv.org/abs/2602.11715)
*Haolei Bai,Lingcheng Kong,Xueyi Chen,Jianmian Wang,Zhiqiang Tao,Huan Wang*

Main category: cs.LG

TL;DR: 本文提出了一种双阶段精选强化学习框架（BiC-RL）和一个针对CUDA内核生成优化的数据集CuKe，用于训练名为DICE的一系列扩散大型语言模型。实验表明，DICE在CUDA内核生成方面显著优于同类规模的自回归和扩散大模型。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散型大型语言模型（dLLMs）在代码生成方面具有潜力，但它们在CUDA内核生成上的应用仍面临挑战，主要是因为领域高度专业化以及高质量训练数据严重缺乏。

Method: 开发了CuKe，一个专为高性能CUDA内核而优化的增强监督微调数据集，并提出了包含CUDA内核填充阶段和端到端CUDA内核生成阶段的双阶段精选强化学习框架（BiC-RL）。基于此训练框架，研究者们推出了DICE，一系列专门设计用来生成CUDA内核的扩散大型语言模型。

Result: 通过KernelBench平台进行的广泛测试显示，DICE的表现明显优于同等规模下的其他自回归及扩散型大模型，在CUDA内核生成领域树立了新的标杆。

Conclusion: 本研究表明，通过精心设计的数据集与训练框架，可以有效提升扩散型大型语言模型在特定任务如CUDA内核生成上的性能。

Abstract: Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.

</details>


### [74] [Dopamine: Brain Modes, Not Brains](https://arxiv.org/abs/2602.11726)
*Shervin Ghasemlou*

Main category: cs.LG

TL;DR: 本文提出了一种新的参数高效微调方法，通过学习每个神经元的阈值和增益来选择并重新缩放现有的计算，而不是重写基础权重。实验表明，该方法在MNIST及其旋转版本上有效，同时提供了更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的参数高效微调（PEFT）方法虽然有效，但难以从机制上解释，并且不直接显示哪些内部计算被重用或绕过。受神经调节启发，作者探索了另一种适应方式——改变模式，即选择和调整现有计算而非修改底层权重。

Method: 提出了\methodname{}，一种简单的激活空间PEFT技术，冻结基础权重并学习每个神经元的阈值和增益。训练时，平滑门控决定神经元激活是否参与；推理时，门控可以硬化以产生显式的条件计算和神经元级归因。

Result: 在MNIST与其45度旋转版本上的实验中，\methodname{}相比冻结基线提高了旋转精度，每层仅使用几百个可训练参数，并表现出部分激活稀疏性。与\lora{}相比，\methodname{}牺牲了一些准确性，但大幅减少了可训练参数数量，并提供了一个更易理解的“哪些神经元激发”机制。

Conclusion: 尽管\methodname{}在某些情况下可能由于冻结的基础缺乏目标模式所需特征而表达力有限，但它为参数高效微调提供了一种新颖且更加可解释的方法。

Abstract: Parameter-efficient fine-tuning (PEFT) methods such as \lora{} adapt large pretrained models by adding small weight-space updates. While effective, weight deltas are hard to interpret mechanistically, and they do not directly expose \emph{which} internal computations are reused versus bypassed for a new task. We explore an alternative view inspired by neuromodulation: adaptation as a change in \emph{mode} -- selecting and rescaling existing computations -- rather than rewriting the underlying weights. We propose \methodname{}, a simple activation-space PEFT technique that freezes base weights and learns per-neuron \emph{thresholds} and \emph{gains}. During training, a smooth gate decides whether a neuron's activation participates; at inference the gate can be hardened to yield explicit conditional computation and neuron-level attributions.
  As a proof of concept, we study ``mode specialization'' on MNIST (0$^\circ$) versus rotated MNIST (45$^\circ$). We pretrain a small MLP on a 50/50 mixture (foundation), freeze its weights, and then specialize to the rotated mode using \methodname{}. Across seeds, \methodname{} improves rotated accuracy over the frozen baseline while using only a few hundred trainable parameters per layer, and exhibits partial activation sparsity (a minority of units strongly active). Compared to \lora{}, \methodname{} trades some accuracy for substantially fewer trainable parameters and a more interpretable ``which-neurons-fire'' mechanism. We discuss limitations, including reduced expressivity when the frozen base lacks features needed for the target mode.

</details>


### [75] [U-Former ODE: Fast Probabilistic Forecasting of Irregular Time Series](https://arxiv.org/abs/2602.11738)
*Ilya Kuleshov,Alexander Marusov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 本文提出了一种新的架构UFO（U-Former ODE），它结合了U-Nets的并行多尺度特征提取、Transformers的强大全局建模以及Neural CDEs的连续时间动态特性，以提高对不规则采样时间序列进行概率预测的能力。实验显示UFO在五个标准基准测试中比十种最新的神经基线模型具有更高的预测准确性，并且推理速度比传统的Neural CDEs快15倍。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健和金融等领域，对不规则采样时间序列进行概率预测非常重要但同时也是一个巨大的挑战。现有的基于神经控制微分方程的方法虽然能够很好地模拟连续动态过程，但由于其计算本质上是顺序的，这限制了它们的可扩展性并且难以获得全局上下文信息。

Method: 研究者们提出了UFO（U-Former ODE）这一新颖架构，该架构将U-Nets的可并行化多尺度特征抽取能力、Transformers强大的全局建模能力与神经控制微分方程(Neural CDEs)的连续时间动力学相结合。通过构建一个完全因果关系且可并行化的模型，UFO能够在保持对局部时间动态敏感的同时实现全局感受野。

Result: 广泛的实验表明，在涵盖常规和非常规采样的五项标准基准测试中，UFO在预测准确性方面始终优于十个最先进的神经基线模型。此外，与传统Neural CDEs相比，UFO提供了高达15倍的更快推理速度，并且在长序列及高度多变量序列上表现出一致的强劲性能。

Conclusion: UFO作为一种创新的时间序列预测模型，不仅提高了预测精度，还显著提升了处理效率，为解决不规则采样时间序列的概率预测问题提供了一个有效的解决方案。

Abstract: Probabilistic forecasting of irregularly sampled time series is crucial in domains such as healthcare and finance, yet it remains a formidable challenge. Existing Neural Controlled Differential Equation (Neural CDE) approaches, while effective at modelling continuous dynamics, suffer from slow, inherently sequential computation, which restricts scalability and limits access to global context. We introduce UFO (U-Former ODE), a novel architecture that seamlessly integrates the parallelizable, multiscale feature extraction of U-Nets, the powerful global modelling of Transformers, and the continuous-time dynamics of Neural CDEs. By constructing a fully causal, parallelizable model, UFO achieves a global receptive field while retaining strong sensitivity to local temporal dynamics. Extensive experiments on five standard benchmarks -- covering both regularly and irregularly sampled time series -- demonstrate that UFO consistently outperforms ten state-of-the-art neural baselines in predictive accuracy. Moreover, UFO delivers up to 15$\times$ faster inference compared to conventional Neural CDEs, with consistently strong performance on long and highly multivariate sequences.

</details>


### [76] [Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning](https://arxiv.org/abs/2602.11779)
*Haoran Dang,Cuiling Lan,Hai Wan,Xibin Zhao,Yan Lu*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架TAMPO，通过将温度控制视为可学习的元策略来优化大型语言模型在强化学习中的表现。TAMPO采用双层循环过程，内循环中使用元策略选定的温度更新策略，外循环则根据轨迹优势调整温度分布。实验表明，TAMPO在五个数学推理基准测试上优于使用固定或启发式温度设定的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前，在训练过程中使用的静态或基于启发式的温度调度方案无法适应强化学习不断变化的需求，限制了策略改进的空间。因此，需要一种能够动态调整探索与利用之间平衡的新方法。

Method: 提出了Temperature Adaptive Meta Policy Optimization (TAMPO)框架，该框架把温度控制当作一个可以学习的元策略。TAMPO包括两个主要部分：内循环和外循环。内循环里，按照元策略选择的温度值更新LLM策略；外循环，则依据高优势轨迹的可能性奖励特定温度值，从而调整候选温度值的概率分布。

Result: 实验结果表明，在五个数学推理任务上，TAMPO的表现超过了使用固定或启发式温度设置的方法。

Conclusion: TAMPO提供了一种有效的方法来学习如何为大型语言模型在强化学习环境中自适应地调节温度参数，以促进更有效的探索与策略提升。

Abstract: Temperature is a crucial hyperparameter in large language models (LLMs), controlling the trade-off between exploration and exploitation during text generation. High temperatures encourage diverse but noisy outputs, while low temperatures produce focused outputs but may cause premature convergence. Yet static or heuristic temperature schedules fail to adapt to the dynamic demands of reinforcement learning (RL) throughout training, often limiting policy improvement. We propose Temperature Adaptive Meta Policy Optimization (TAMPO), a new framework that recasts temperature control as a learnable meta-policy. TAMPO operates through a hierarchical two-loop process. In the inner loop, the LLM policy is updated (e.g., using GRPO) with trajectories sampled at the temperature selected by the meta-policy. In the outer loop, meta-policy updates the distribution over candidate temperatures by rewarding those that maximize the likelihood of high-advantage trajectories. This trajectory-guided, reward-driven mechanism enables online adaptation without additional rollouts, directly aligning exploration with policy improvement. On five mathematical reasoning benchmarks, TAMPO outperforms baselines using fixed or heuristic temperatures, establishing temperature as an effective learnable meta-policy for adaptive exploration in LLM reinforcement learning. Accepted at ICLR 2026.

</details>


### [77] [Safe Fairness Guarantees Without Demographics in Classification: Spectral Uncertainty Set Perspective](https://arxiv.org/abs/2602.11785)
*Ainhize Barrainkua,Santiago Mazuelas,Novi Quadrianto,Jose A. Lozano*

Main category: cs.LG

TL;DR: 本文提出了一种名为SPECTRE的新方法，通过调整简单傅里叶特征映射的频谱并限制最坏情况分布与经验分布之间的偏差程度来解决在没有人口统计信息的情况下实现公平性的问题。实验表明，即使与那些可以访问人口统计群体信息的方法相比，SPECTRE也能提供最高的平均公平保证值以及最小的四分位距。


<details>
  <summary>Details</summary>
Motivation: 随着自动化分类系统的普及，人们越来越担心它们可能强化和放大现有的社会偏见。尽管已有许多方法被提出来提高分类器的公平性保证，但大多数现有干预措施假定能够获取所有实例的群体信息，这在实践中很少满足。此外，在没有人口统计信息的情况下追求公平性时，通常采用鲁棒优化技术，而这些技术的有效性受到所选不确定性集合的影响，并且往往过度强调异常值或过于悲观的情况，从而损害了整体性能和公平性。

Method: 为了解决上述问题，研究者们提出了SPECTRE，这是一种极小极大公平法，它通过调节一个简单的傅里叶特征映射的频谱，并约束最坏情况下分布偏离经验分布的程度。

Result: 通过对美国社区调查数据集（涉及20个州）进行广泛的实验，SPECTRE提供了最高的平均公平性保证值及最小的四分位距，甚至比那些能访问到人口统计群体信息的方法表现还要好。此外，研究还提供了理论分析，得出了个体群体和总体最坏情况误差的可计算边界，并描述了导致这些极端表现的最坏情况分布的特点。

Conclusion: SPECTRE作为一种新的无须依赖人口统计信息即可提升机器学习模型公平性的方法，在保持高公平性的同时也维持了较好的性能稳定性。

Abstract: As automated classification systems become increasingly prevalent, concerns have emerged over their potential to reinforce and amplify existing societal biases. In the light of this issue, many methods have been proposed to enhance the fairness guarantees of classifiers. Most of the existing interventions assume access to group information for all instances, a requirement rarely met in practice. Fairness without access to demographic information has often been approached through robust optimization techniques,which target worst-case outcomes over a set of plausible distributions known as the uncertainty set. However, their effectiveness is strongly influenced by the chosen uncertainty set. In fact, existing approaches often overemphasize outliers or overly pessimistic scenarios, compromising both overall performance and fairness. To overcome these limitations, we introduce SPECTRE, a minimax-fair method that adjusts the spectrum of a simple Fourier feature mapping and constrains the extent to which the worst-case distribution can deviate from the empirical distribution. We perform extensive experiments on the American Community Survey datasets involving 20 states. The safeness of SPECTRE comes as it provides the highest average values on fairness guarantees together with the smallest interquartile range in comparison to state-of-the-art approaches, even compared to those with access to demographic group information. In addition, we provide a theoretical analysis that derives computable bounds on the worst-case error for both individual groups and the overall population, as well as characterizes the worst-case distributions responsible for these extremal performances

</details>


### [78] [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786)
*Keita Broadwater*

Main category: cs.LG

TL;DR: 本文提出了一种名为加速提示压力测试（APST）的评估框架，专门用于评估大型语言模型在重复推理相同或几乎相同的提示时的安全性和一致性。通过这种方法，可以发现潜在的失败模式，并使用伯努利和二项式模型量化每次推理的失败概率，从而补充现有基准测试在持续使用场景下的风险评估。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型（LLM）基准测试主要通过跨多种任务的广度导向评估来衡量安全风险。但在实际部署中，更多遇到的是由于对相同或非常相似的提示进行重复推理而导致的操作失败，而非广泛的任务泛化能力问题。特别是在高风险环境中，响应的一致性和安全性尤为重要。因此，需要一种新的评估方法来更好地理解这些模型在持续使用条件下的表现。

Method: 引入了加速提示压力测试（APST），这是一种受到可靠性工程启发的深度导向评估框架。该方法通过对控制操作条件下（如解码温度）相同提示的反复采样，揭示包括幻觉、拒绝不一致以及不安全完成在内的潜在故障模式。APST将故障视为独立推理事件随机结果的一部分，并采用伯努利和二项式模型来正式定义安全故障，以估计每次推理的失败概率。

Result: 当将APST应用于多个基于AIR-BENCH衍生的安全提示进行评测的指令调优LLM时，研究发现具有类似基准对齐分数的模型，在重复采样尤其是随着温度增加的情况下，表现出显著不同的实际失败率。这表明浅层单样本评估可能掩盖了在持续使用情况下有意义的可靠性差异。

Conclusion: APST为评估LLM在重复推理情况下的安全性和可靠性提供了实用框架，填补了现有基准与面向部署的风险评估之间的空白。

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

</details>


### [79] [From Path Signatures to Sequential Modeling: Incremental Signature Contributions for Offline RL](https://arxiv.org/abs/2602.11805)
*Ziyi Zhao,Qingchuan Li,Yuxuan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种增量签名贡献（ISC）方法，将路径签名分解为张量代数空间中的时间有序序列，从而保留了签名的代数结构和表达能力，并使内部时间演变显式化。基于此表示，作者引入了ISC-Transformer (ISCT)，这是一种离线强化学习模型，在标准Transformer架构中集成了ISC而无需进一步的架构修改。实验结果表明，ISC方法为时间敏感控制任务提供了理论基础且实用有效的路径处理替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统的路径签名方法虽然能够提供非参数化的路径表示，但它们将时间结构折叠成一个全局对象，这限制了其在需要逐步反应的决策问题中的适用性。为了克服这一局限性，研究者提出了增量签名贡献(ISC)方法，旨在保持签名表达力的同时，让签名的内部时间演化变得明确。

Method: 研究者设计了增量签名贡献(ISC)方法，它将截断路径签名分解为按照时间顺序排列的一系列元素，这些元素对应于最后路径增量所引起的增量贡献。此外，他们还基于ISC方法构建了一个名为ISC-Transformer (ISCT)的离线强化学习模型，该模型将ISC直接集成到了标准的Transformer架构中，没有进行额外的架构调整。

Result: 通过在HalfCheetah、Walker2d、Hopper以及Maze2d等环境下的测试，包括那些具有延迟奖励和降级数据集的情况，结果显示ISC方法对于时间敏感型控制任务既具备坚实的理论基础又表现出实际的有效性。

Conclusion: 提出的增量签名贡献（ISC）方法及其应用到的ISC-Transformer模型成功地解决了传统路径签名方法在时间敏感控制任务中的局限性，为这类任务提供了一种新的、有效的解决方案。

Abstract: Path signatures embed trajectories into tensor algebra and constitute a universal, non-parametric representation of paths; however, in the standard form, they collapse temporal structure into a single global object, which limits their suitability for decision-making problems that require step-wise reactivity. We propose the Incremental Signature Contribution (ISC) method, which decomposes truncated path signatures into a temporally ordered sequence of elements in the tensor-algebra space, corresponding to incremental contributions induced by last path increments. This reconstruction preserves the algebraic structure and expressivity of signatures, while making their internal temporal evolution explicit, enabling processing signature-based representations via sequential modeling approaches. In contrast to full signatures, ISC is inherently sensitive to instantaneous trajectory updates, which is critical for sensitive and stability-requiring control dynamics. Building on this representation, we introduce ISC-Transformer (ISCT), an offline reinforcement learning model that integrates ISC into a standard Transformer architecture without further architectural modification. We evaluate ISCT on HalfCheetah, Walker2d, Hopper, and Maze2d, including settings with delayed rewards and downgraded datasets. The results demonstrate that ISC method provides a theoretically grounded and practically effective alternative to path processing for temporally sensitive control tasks.

</details>


### [80] [CAAL: Confidence-Aware Active Learning for Heteroscedastic Atmospheric Regression](https://arxiv.org/abs/2602.11825)
*Fei Jiang,Jiyang Xia,Junjie Yu,Mingfei Sun,Hugh Coe,David Topping,Dantong Liu,Zhenhui Jessie Li,Zhonghua Zheng*

Main category: cs.LG

TL;DR: 提出了一种基于置信度的主动学习框架（CAAL），用于在异方差设置中高效稳健地选择样本，以估计大气颗粒物难以测量的性质。实验表明CAAL优于标准的主动学习基线，并为高成本的大气颗粒物性质数据库的有效扩展提供了一个实用且通用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 量化空气污染对健康和气候的影响依赖于如毒性、吸湿性等关键的大气颗粒物属性。然而，这些属性通常需要复杂的观测技术或昂贵的粒子解析数值模拟，限制了标记数据的可用性。因此，研究旨在从常规可获得的观测数据中估计这些难以测量的颗粒物属性。

Method: 提出了一个名为CAAL的置信度感知主动学习框架，该框架包括两个组件：一个解耦的不确定性感知训练目标，它分别优化预测均值和噪声水平以稳定不确定性估计；以及一个置信度感知获取函数，它使用预测的偶然不确定性作为可靠性信号来动态加权认知不确定性。

Result: 通过粒子解析数值模拟和实际大气观测的实验证明，与标准主动学习基线相比，CAAL能够持续表现出更优性能。

Conclusion: 提出的CAAL框架不仅为在异方差条件下进行有效而稳健的样本选择提供了可能，而且也给高成本的大气颗粒物属性数据库的扩充带来了实用且广泛的解决方法。

Abstract: Quantifying the impacts of air pollution on health and climate relies on key atmospheric particle properties such as toxicity and hygroscopicity. However, these properties typically require complex observational techniques or expensive particle-resolved numerical simulations, limiting the availability of labeled data. We therefore estimate these hard-to-measure particle properties from routinely available observations (e.g., air pollutant concentrations and meteorological conditions). Because routine observations only indirectly reflect particle composition and structure, the mapping from routine observations to particle properties is noisy and input-dependent, yielding a heteroscedastic regression setting. With a limited and costly labeling budget, the central challenge is to select which samples to measure or simulate. While active learning is a natural approach, most acquisition strategies rely on predictive uncertainty. Under heteroscedastic noise, this signal conflates reducible epistemic uncertainty with irreducible aleatoric uncertainty, causing limited budgets to be wasted in noise-dominated regions. To address this challenge, we propose a confidence-aware active learning framework (CAAL) for efficient and robust sample selection in heteroscedastic settings. CAAL consists of two components: a decoupled uncertainty-aware training objective that separately optimises the predictive mean and noise level to stabilise uncertainty estimation, and a confidence-aware acquisition function that dynamically weights epistemic uncertainty using predicted aleatoric uncertainty as a reliability signal. Experiments on particle-resolved numerical simulations and real atmospheric observations show that CAAL consistently outperforms standard AL baselines. The proposed framework provides a practical and general solution for the efficient expansion of high-cost atmospheric particle property databases.

</details>


### [81] [A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production](https://arxiv.org/abs/2602.11861)
*Sümeyye Meryem Taşyürek,Enis Mücahid İskender,Hacer Yalim Keles*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于变分自编码器(VAE)的框架A$^{2}$V-SLP，用于学习手语生成中解缠的发音器官级别的潜在分布。该框架通过非自回归Transformer预测潜在均值和对数方差，并结合了词义注意机制来加强语言输入与动作之间的对齐。实验结果表明，该方法在完全不依赖词义的情况下，实现了最新的反向翻译性能和提高了动作的真实感。


<details>
  <summary>Details</summary>
Motivation: 作者旨在改进现有的手语生成模型，特别是针对现有模型中存在的确定性潜在嵌入导致的问题（如潜在崩溃）。通过引入一种新的变分框架，他们希望能够在保持发音器官级别表示的同时提高模型对手语序列生成的质量及真实度。

Method: 提出了A$^{2}$V-SLP，一个考虑对齐的变分框架，利用VAE从真实的手语姿势序列中提取特定发音器官的平均值和方差向量作为分布监督信息训练非自回归Transformer。给定文本嵌入后，Transformer可以预测潜在的均值和对数方差；然后，在解码阶段通过随机采样由VAE解码器重建最终的手语姿势序列。此外，还集成了一个词义注意力机制以增强语言输入与具体动作间的对齐效果。

Result: 实验结果显示，相比确定性的潜在回归方法，所提出的A$^{2}$V-SLP框架不仅达到了最先进的反向翻译表现，而且在完全不需要依赖词义的情况下也提升了运动的真实感。

Conclusion: 通过采用分布式的潜在建模方式而非确定性的嵌入，A$^{2}$V-SLP能够更好地维持发音器官级别的表示，避免潜在崩溃问题，从而在手语生成任务上取得了显著的进步。

Abstract: Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.

</details>


### [82] [In-Context Function Learning in Large Language Models](https://arxiv.org/abs/2602.11863)
*Elif Akata,Konstantinos Voudouris,Vincent Fortuin,Eric Schulz*

Main category: cs.LG

TL;DR: 本研究通过高斯过程（GP）的视角探讨了大型语言模型（LLM）在推理时从少量示例中学习的现象。实验显示，随着演示数量增加，LLM的学习曲线接近GP下界，并且其归纳偏好更倾向于较不平滑的GP核。此外，研究还发现强化学习和监督微调可以有效调整这些归纳偏好，使其更适合从更平滑的GP核采样的函数学习任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解大型语言模型如何在仅有少数示例的情况下进行上下文学习，以及它们的行为与高斯过程学习者有多相似。

Method: 构建了控制实验，其中模型观察到从已知GP先验抽取的多变量标量值函数样本序列。评估了预测误差与演示次数的关系，并与两个基准进行了比较：一个基于经验的GP回归学习者作为可达到误差的下界；另一个是1-最近邻规则的预期误差，作为数据驱动的上界。

Result: 不同规模模型的学习曲线受函数生成核的影响很大，并且随着演示数量的增加逐渐逼近GP下界。LLM的预测最有可能发生在不太平滑的GP核条件下。同时，后训练方法如强化学习和监督微调能够有效地改变这些归纳偏见，提高对于从平滑核GP采样函数的学习效率。

Conclusion: 整体而言，该框架量化了LLM像GP学习者一样行事的程度，并提供了工具来指导其归纳偏见以适应连续函数学习任务。

Abstract: Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.

</details>


### [83] [Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.11882)
*Suraj Ranganath,Anish Patnaik,Vaishak Menon*

Main category: cs.LG

TL;DR: 研究了在严格的精度预算下，低比特规划行为主要受总比特宽度还是模块间比特分配的影响。通过DINO-WM在Wall规划任务中的实验，发现8比特和6比特设置接近FP16表现，3比特设置性能崩溃，而4比特设置对分配敏感。这些结果支持了基于模块和预算的量化策略作为高效空间推理的研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索在有限精度预算下，高效的空问推理模型如何保持可靠性，并确定低比特规划行为是由总的比特宽度决定还是由模块间的比特分配决定。

Method: 使用DINO-WM模型在Wall规划任务上执行配对目标混合位评估，测试了不同变体（统一、混合、不对称及层间）在两个规划器预算下的表现。

Result: 观察到一个持续的三阶段模式：8比特和6比特设定与FP16非常接近；3比特设定则完全失效；4比特设定对于分配十分敏感。特别是在过渡区域，保持编码器精度可以提高相对于均匀量化的规划效果。

Conclusion: 本研究表明，在某些特定的比特设定范围内，比特分配方式对规划效率有显著影响，这表明需要进一步研究基于模块和预算意识的量化策略来促进高效的空间推理。

Abstract: Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.

</details>


### [84] [Universal Diffusion-Based Probabilistic Downscaling](https://arxiv.org/abs/2602.11893)
*Roberto Molinaro,Niall Siegenheim,Henry Martin,Mark Frey,Niels Poulsen,Philipp Seitz,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的通用降尺度框架，可以将确定性的低分辨率天气预报提升为概率性的高分辨率预测，无需针对特定模型进行微调。该方法对提高空间分辨率和不确定性表示有显著效果。


<details>
  <summary>Details</summary>
Motivation: 为了提高天气预报的空间分辨率和不确定性表示，同时不依赖于特定的上游天气模型，提出了一个不需要任何模型特定微调即可实现从低分辨率到高分辨率天气预测转换的框架。

Method: 开发了一个条件扩散模型，通过配对粗分辨率输入（约25公里分辨率）与高分辨率区域再分析目标（约5公里分辨率）进行训练，并以完全零样本方式应用于来自异构上游天气模型的确定性预报上。

Result: 在不同的人工智能基础和数值天气预报系统中，降尺度后的预报集合平均值始终优于每个模型自身的原始确定性预报，在概率技能方面也有显著改进，特别是在CRPS测量下。

Conclusion: 基于扩散的降尺度提供了一种可扩展、模型无关的概率接口，能够增强操作性天气预报流程中的空间分辨率和不确定性表示。

Abstract: We introduce a universal diffusion-based downscaling framework that lifts deterministic low-resolution weather forecasts into probabilistic high-resolution predictions without any model-specific fine-tuning. A single conditional diffusion model is trained on paired coarse-resolution inputs (~25 km resolution) and high-resolution regional reanalysis targets (~5 km resolution), and is applied in a fully zero-shot manner to deterministic forecasts from heterogeneous upstream weather models. Focusing on near-surface variables, we evaluate probabilistic forecasts against independent in situ station observations over lead times up to 90 h. Across a diverse set of AI-based and numerical weather prediction (NWP) systems, the ensemble mean of the downscaled forecasts consistently improves upon each model's own raw deterministic forecast, and substantially larger gains are observed in probabilistic skill as measured by CRPS. These results demonstrate that diffusion-based downscaling provides a scalable, model-agnostic probabilistic interface for enhancing spatial resolution and uncertainty representation in operational weather forecasting pipelines.

</details>


### [85] [Mitigating Mismatch within Reference-based Preference Optimization](https://arxiv.org/abs/2602.11902)
*Suqin Yuan,Xingrui Yu,Jiyang Zheng,Lei Feng,Dadong Wang,Ivor Tsang,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hybrid-DPO (HyPO)的方法，通过条件性地应用参考策略来解决直接偏好优化(DPO)中存在的过早满足问题。HyPO在保持DPO目标形式和计算成本的同时加强了对悲观样本的学习信号，从而提高了配对胜率等指标。


<details>
  <summary>Details</summary>
Motivation: 直接偏好优化（DPO）虽然已成为大型语言模型离线偏好校准的事实标准，但其依赖于参考策略的做法存在一个问题：对于悲观样本对，即使策略仍有误，只要政策边际超过参考边际，DPO就会提前减少梯度更新。这种现象被称为“过早满足”，导致训练-推理不匹配的问题。

Method: 为了解决上述问题，研究者提出了Hybrid-DPO (HyPO)，一种针对DPO的简单修改方案。HyPO根据参考策略的态度有条件地工作：当参考策略是乐观或中立时，它表现得像DPO；而当参考策略悲观时，则将\(Δ_θ-Δ_{\mathrm{ref}}\)替换为\(Δ_θ-\max\{0,Δ_{\mathrm{ref}}\}\)，从而视为参考策略为中立。

Result: 实验结果表明，在偏好校准任务上，HyPO能够提高与推理一致的评价指标，并且达到了更高的两两比较获胜率。这意味着通过条件性地去偏参考信号而非完全舍弃之，可以直接增强偏好校准的效果。

Conclusion: HyPO通过改进DPO中的参考策略使用方式，在保持原方法优点的同时有效缓解了过早满足问题，进而提升了偏好校准任务的表现。

Abstract: Direct Preference Optimization (DPO) has become the de facto standard for offline preference alignment of large language models, but its reliance on a reference policy introduces a critical tension. DPO weighs each update relative to a reference, which stabilizes the training by regularizing the updates within a trusted region. This reliance becomes problematic for pessimistic pairs, where the reference model prefers the rejected response. For these pairs, DPO prematurely attenuates the gradient as soon as the policy margin ($Δ_θ$) merely beats the reference margin ($Δ_{\mathrm{ref}}$) even if the policy is still wrong ($Δ_θ<0$). We name this failure premature satisfaction, which is a concrete form of the training-inference mismatch. Reference-free objectives remove this mismatch by optimizing the absolute margin, but at the cost of discarding the stabilizing signal of the reference. We mitigate this tension with Hybrid-DPO (HyPO), a drop-in modification to DPO that applies reference conditionally: HyPO behaves exactly like DPO when the reference is optimistic or neutral, and it treats the reference as neutral when it is pessimistic by replacing $Δ_θ-Δ_{\mathrm{ref}}$ with $Δ_θ-\max\{0,Δ_{\mathrm{ref}}\}$. This one-line change strictly strengthens per-example learning signals on pessimistic pairs while preserving DPO's objective form and computational cost. By conditionally debiasing the pessimistic reference signal, HyPO mitigates premature satisfaction; empirically, across preference alignment, HyPO improves inference-aligned metrics and achieves higher pairwise win rates. Our results provide evidence that direct preference alignment could be enhanced by conditionally debiasing the reference signal, rather than discarding it.

</details>


### [86] [Learning Conditional Averages](https://arxiv.org/abs/2602.11920)
*Marco Bressan,Nataly Brukhim,Nicolo Cesa-Bianchi,Emmanuel Esposito,Yishay Mansour,Shay Moran,Maximilian Thiessen*

Main category: cs.LG

TL;DR: 本文在PAC框架下引入了学习条件平均值的问题，目标是预测每个实例在其邻域内的平均标签。文章给出了当条件平均值可学习时的完整特征，并提供了样本复杂度边界。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决在PAC框架下的条件平均值学习问题，这一问题比传统的PAC学习更具一般性，并且可以应用于解释性、公平性和推荐系统等领域中出现的学习任务。

Method: 作者通过引入两个新的组合参数，对概念类和邻域系统进行了分析，进而为条件平均值学习提供了一个完整的可学习性表征。这两个参数与相关邻域图的独立数密切相关。

Result: 研究结果表明，当这两个新提出的组合参数同时有限时，条件平均值是可以被学习的，并且给出了紧致到对数因子的样本复杂度界限。

Conclusion: 本文扩展了PAC学习的概念，以适应更广泛的应用场景，包括但不限于解释性、公平性以及推荐系统，并且对条件平均值学习的可能性给出了严格的理论基础。

Abstract: We introduce the problem of learning conditional averages in the PAC framework. The learner receives a sample labeled by an unknown target concept from a known concept class, as in standard PAC learning. However, instead of learning the target concept itself, the goal is to predict, for each instance, the average label over its neighborhood -- an arbitrary subset of points that contains the instance. In the degenerate case where all neighborhoods are singletons, the problem reduces exactly to classic PAC learning. More generally, it extends PAC learning to a setting that captures learning tasks arising in several domains, including explainability, fairness, and recommendation systems. Our main contribution is a complete characterization of when conditional averages are learnable, together with sample complexity bounds that are tight up to logarithmic factors. The characterization hinges on the joint finiteness of two novel combinatorial parameters, which depend on both the concept class and the neighborhood system, and are closely related to the independence number of the associated neighborhood graph.

</details>


### [87] [Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration](https://arxiv.org/abs/2602.11937)
*Akhiad Bercovich,Nir Ailon,Vladimir Anisimov,Tomer Asida,Nave Assaf,Mohammad Dabbah,Ido Galil,Amnon Geifman,Yonatan Geifman,Izhak Golan,Roi Koren,Itay Levy,Zach Moshe,Pavlo Molchanov,Najeeb Nabwani,Mostofa Patwari,Omri Puny,Tomer Ronen,Itamar Schen,Elad Segal,Ido Shahaf,Oren Tropp,Ran Zilberstein,Ran El-Yaniv*

Main category: cs.LG

TL;DR: 通过使用Puzzle框架对gpt-oss-120B进行后训练神经架构搜索，研究者们开发了gpt-oss-puzzle-88B模型。该模型采用了多种技术来优化推理效率同时保持准确性，包括异构MoE专家剪枝、窗口注意力替换全上下文注意力、FP8 KV缓存量化等。在不同的硬件配置下，gpt-oss-puzzle-88B实现了显著的吞吐量提升，并且在各种基准测试中保持或稍微超过了原始模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了提高基于长推理路径的大规模语言模型（LLM）的服务效率而不牺牲答案质量，研究者们寻求通过架构优化减少推理成本。

Method: 采用名为Puzzle的后训练神经架构搜索（NAS）框架对gpt-oss-120B进行优化，生成了gpt-oss-puzzle-88B。此过程中结合了异构混合专家（MoE）剪枝、选择性地用窗口注意力代替全上下文注意力、以及使用校准比例的FP8 KV缓存量化等技术。此外，还运用了后训练强化学习来恢复精度。

Result: 新模型gpt-oss-puzzle-88B相比原版，在不同硬件配置下都表现出显著的吞吐量提升，例如在单个NVIDIA H100 GPU上吞吐量提升了2.82倍。更重要的是，它能够在保持甚至略微提高推理质量的同时大幅度降低推理成本。

Conclusion: 研究表明，通过应用Puzzle框架及一系列优化技术可以有效减少大规模语言模型的推理成本而不损害其性能，这为实际部署提供了更高效的选择。

Abstract: Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.

</details>


### [88] [Temporally Unified Adversarial Perturbations for Time Series Forecasting](https://arxiv.org/abs/2602.11940)
*Ruixian Su,Yukun Bao,Xinze Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的对抗扰动生成方法，即时间统一的对抗扰动（TUAPs）和一种基于时间戳梯度累积的方法（TGAM），以解决时间序列预测领域中对抗攻击忽视时间一致性的问题。实验表明，该方法在白盒和黑盒迁移攻击场景下均优于基线方法，并且即使没有TUAP限制也表现出优秀的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列预测领域的对抗攻击方法通常忽略了数据中的时间一致性，导致相同时间戳上的扰动值在重叠样本间出现分歧和矛盾，使得对抗攻击在实际数据操作中不切实际。

Method: 提出了Temporally Unified Adversarial Perturbations (TUAPs) 和 Timestamp-wise Gradient Accumulation Method (TGAM)。其中，TUAPs通过强制执行时间统一约束来确保所有重叠样本中每个时间戳的扰动一致；而TGAM则提供了一种模块化、高效的方法，通过聚合来自重叠样本的局部梯度信息有效地生成TUAPs。

Result: 在三个基准数据集上对四种代表性最先进模型进行了全面实验，结果表明所提出的方法无论是在白盒还是黑盒迁移攻击场景下都显著优于基线方法。此外，即使不受TUAP约束，该方法也显示出优越的迁移攻击性能。

Conclusion: 本研究提出的TUAPs与TGAM相结合的方法有效解决了时间序列预测领域内对抗攻击中存在的时序一致性问题，不仅提高了对抗攻击的效果，还增强了其在真实世界应用中的实用性。

Abstract: While deep learning models have achieved remarkable success in time series forecasting, their vulnerability to adversarial examples remains a critical security concern. However, existing attack methods in the forecasting field typically ignore the temporal consistency inherent in time series data, leading to divergent and contradictory perturbation values for the same timestamp across overlapping samples. This temporally inconsistent perturbations problem renders adversarial attacks impractical for real-world data manipulation. To address this, we introduce Temporally Unified Adversarial Perturbations (TUAPs), which enforce a temporal unification constraint to ensure identical perturbations for each timestamp across all overlapping samples. Moreover, we propose a novel Timestamp-wise Gradient Accumulation Method (TGAM) that provides a modular and efficient approach to effectively generate TUAPs by aggregating local gradient information from overlapping samples. By integrating TGAM with momentum-based attack algorithms, we ensure strict temporal consistency while fully utilizing series-level gradient information to explore the adversarial perturbation space. Comprehensive experiments on three benchmark datasets and four representative state-of-the-art models demonstrate that our proposed method significantly outperforms baselines in both white-box and black-box transfer attack scenarios under TUAP constraints. Moreover, our method also exhibits superior transfer attack performance even without TUAP constraints, demonstrating its effectiveness and superiority in generating adversarial perturbations for time series forecasting models.

</details>


### [89] [Using predictive multiplicity to measure individual performance within the AI Act](https://arxiv.org/abs/2602.11944)
*Karolin Frohnapfel,Mara Seyfert,Sebastian Bordt,Ulrike von Luxburg,Kristof Meding*

Main category: cs.LG

TL;DR: 本文探讨了在构建决策支持AI系统时遇到的预测多重性问题，并提出应如何根据欧盟AI法案的规定来评估和报告这一现象。提出了使用个体冲突比率和δ-模糊度作为工具量化模型间对于个体案例的分歧，以及建议让部署者能够获得关于预测多重性的信息以判断系统输出对特定个体是否足够可靠。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决AI系统中预测多重性带来的不确定性问题，尤其是在涉及直接影响人类决策的情况下，这种不确定性可能与欧盟AI法案中要求提供高风险AI系统性能报告的规定相冲突。

Method: 通过法律分析论证将预测多重性信息纳入有助于符合欧盟AI法案准确性规定；提出用个体冲突比率和δ-模糊度等工具量化模型间对个别案例的分歧；基于计算洞察提供了易于实施的规则来评估实践中预测多重性。

Result: 发现考虑预测多重性可以帮助AI系统的提供者更好地遵守欧盟AI法案中的准确性条款；提出的具体方法如个体冲突比率和δ-模糊度可有效识别受到不同模型预测影响较大的个体；为模型提供者提供了一套简单实用的评估指南。

Conclusion: 认为向AI系统的部署者提供有关预测多重性的信息是必要的，这有助于他们评估针对特定个人的系统输出是否足够可靠，从而确保遵循欧盟AI法案的要求。

Abstract: When building AI systems for decision support, one often encounters the phenomenon of predictive multiplicity: a single best model does not exist; instead, one can construct many models with similar overall accuracy that differ in their predictions for individual cases. Especially when decisions have a direct impact on humans, this can be highly unsatisfactory. For a person subject to high disagreement between models, one could as well have chosen a different model of similar overall accuracy that would have decided the person's case differently. We argue that this arbitrariness conflicts with the EU AI Act, which requires providers of high-risk AI systems to report performance not only at the dataset level but also for specific persons. The goal of this paper is to put predictive multiplicity in context with the EU AI Act's provisions on accuracy and to subsequently derive concrete suggestions on how to evaluate and report predictive multiplicity in practice. Specifically: (1) We argue that incorporating information about predictive multiplicity can serve compliance with the EU AI Act's accuracy provisions for providers. (2) Based on this legal analysis, we suggest individual conflict ratios and $δ$-ambiguity as tools to quantify the disagreement between models on individual cases and to help detect individuals subject to conflicting predictions. (3) Based on computational insights, we derive easy-to-implement rules on how model providers could evaluate predictive multiplicity in practice. (4) Ultimately, we suggest that information about predictive multiplicity should be made available to deployers under the AI Act, enabling them to judge whether system outputs for specific individuals are reliable enough for their use case.

</details>


### [90] [Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios](https://arxiv.org/abs/2602.11945)
*Hongliang Zhang,Jiguo Yu,Guijuan Wang,Wenshuo Ma,Tianqing He,Baobao Chai,Chunqiang Hu*

Main category: cs.LG

TL;DR: 本文提出了一种基于历史训练信息的性能增强模型对比联邦学习框架PMFL，通过在节点侧引入新颖的模型对比项并在服务器侧根据每个节点的累积参与次数自适应调整其聚合权重，从而提高了异构数据分布下的联邦学习性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统通常部署在异构场景中，节点间的数据分布和参与频率不同，这会削弱联邦学习的性能。为了解决这个问题，作者提出了PMFL框架来提高模型更新的一致性和全局目标的准确性。

Method: 1. 在节点端，通过整合历史本地模型到节点优化目标中设计了一个新的模型对比项，以捕捉稳定的对比点，从而改善异构数据分布下模型更新的一致性。
2. 在服务端，利用每个节点的累积参与计数来自适应地调整其聚合权重，以此修正由于节点参与频率不同所引起的全局目标偏差。
3. 更新后的全局模型融合了历史全局模型，减少了相邻轮次之间性能波动。

Result: 广泛的实验表明，在异构场景下，与现有的联邦学习方法相比，PMFL能够实现更优的表现。

Conclusion: 通过引入基于历史训练信息的模型对比策略以及动态调整节点聚合权重的方法，PMFL有效解决了联邦学习中因数据分布不均及参与频率差异导致的问题，显著提升了联邦学习系统的整体性能。

Abstract: Federated Learning (FL) enables multiple nodes to collaboratively train a model without sharing raw data. However, FL systems are usually deployed in heterogeneous scenarios, where nodes differ in both data distributions and participation frequencies, which undermines the FL performance. To tackle the above issue, this paper proposes PMFL, a performance-enhanced model-contrastive federated learning framework using historical training information. Specifically, on the node side, we design a novel model-contrastive term into the node optimization objective by incorporating historical local models to capture stable contrastive points, thereby improving the consistency of model updates in heterogeneous data distributions.
  On the server side, we utilize the cumulative participation count of each node to adaptively adjust its aggregation weight, thereby correcting the bias in the global objective caused by different node participation frequencies. Furthermore, the updated global model incorporates historical global models to reduce its fluctuations in performance between adjacent rounds. Extensive experiments demonstrate that PMFL achieves superior performance compared with existing FL methods in heterogeneous scenarios.

</details>


### [91] [RAM-Net: Expressive Linear Attention with Selectively Addressable Memory](https://arxiv.org/abs/2602.11958)
*Kaicheng Xiao,Haotian Li,Liran Dong,Guoliang Xing*

Main category: cs.LG

TL;DR: 本文提出了一种名为RAM-Net的新架构，它通过将输入映射到高维稀疏向量作为显式地址来选择性地访问大容量内存状态，从而在保持线性模型的内存效率的同时提升了全注意力机制的表现力。实验表明，RAM-Net在细粒度长距离检索任务上优于现有技术，并且在标准语言建模和零样本常识推理基准测试中也表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的线性注意力架构虽然提供了高效的推理能力，但将无界的历史压缩到固定大小的记忆中会限制表达能力和造成信息损失。为了解决这个问题，作者们开发了RAM-Net，旨在结合全注意力机制的表现力与线性模型的内存效率优势。

Method: RAM-Net的核心是将输入转换成高维稀疏向量，这些向量充当访问巨大记忆状态的明确地址。这样的设计允许状态大小以指数级增长而无需额外参数，减少了信号干扰并提高了检索准确性。此外，固有的稀疏性保证了极高的计算效率，因为状态更新仅限于少数几个条目。

Result: 广泛的实验证明，RAM-Net不仅在细致的长程检索任务上持续超越最先进基准，在标准的语言建模以及零次常识推理基准上也表现得很有竞争力。这证实了RAM-Net能够以显著减少的计算开销捕捉复杂的依赖关系。

Conclusion: RAM-Net作为一种新型架构，成功地在不牺牲内存效率的前提下增强了模型处理复杂依赖的能力。它提供了一种有效的方法来解决线性注意机制的信息丢失问题，并且在多种NLP任务中展示了其优越性能。

Abstract: While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.

</details>


### [92] [Manifold-Aware Temporal Domain Generalization for Large Language Models](https://arxiv.org/abs/2602.11965)
*Yiheng Yao,Zekun Cai,Xinyuan Song,Hiroki Hill Kobayashi,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为MaT-LoRA的新方法，通过在低秩适应子空间内约束时间更新至共享的低维流形，并通过结构化的时间核心建模其演变过程，从而解决了大型语言模型（LLMs）中全参数空间下的时间域泛化（TDG）计算不可行的问题。实验表明该方法不仅显著降低了时间建模复杂度，还保持了表达能力，并在合成数据集和真实世界数据集中展现了优越的时间泛化性能及实用性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大型语言模型（LLMs）部署时面临着随时间不断变化的数据分布偏移问题。虽然时间域泛化（TDG）试图解决这种结构化演变，但现有方法需要在全参数空间下进行模型调整，这使得对于现代LLMs来说变得计算上不可行。因此，研究旨在寻找一种更高效的方法来处理这个问题。

Method: 论文引入了一种基于参数高效微调的时间域泛化（TDG）几何重构方法。通过证明模型演变背后的低维时间结构可以通过参数高效的重新参数化得以保留，使得无需在原始参数空间中也能实现时间建模。基于这一原理，提出了Manifold-aware Temporal LoRA (MaT-LoRA)，它将时间更新限制在一个共享的低维流形内，并通过一个结构化的时间核心来模拟其演变。

Result: 广泛的实验表明，无论是针对合成数据还是真实世界中的科学文档、新闻出版商和评论评分等数据集，MaT-LoRA都能达到更好的时间泛化表现，同时对LLMs而言具有实际可扩展性。

Conclusion: MaT-LoRA作为一种新的方法，能够有效减少时间建模的复杂度，同时保持足够的表达力，为解决大型语言模型面对的时间分布偏移问题提供了一个可行且高效的解决方案。

Abstract: Temporal distribution shifts are pervasive in real-world deployments of Large Language Models (LLMs), where data evolves continuously over time. While Temporal Domain Generalization (TDG) seeks to model such structured evolution, existing approaches characterize model adaptation in the full parameter space. This formulation becomes computationally infeasible for modern LLMs. This paper introduces a geometric reformulation of TDG under parameter-efficient fine-tuning. We establish that the low-dimensional temporal structure underlying model evolution can be preserved under parameter-efficient reparameterization, enabling temporal modeling without operating in the ambient parameter space. Building on this principle, we propose Manifold-aware Temporal LoRA (MaT-LoRA), which constrains temporal updates to a shared low-dimensional manifold within a low-rank adaptation subspace, and models its evolution through a structured temporal core. This reparameterization dramatically reduces temporal modeling complexity while retaining expressive power. Extensive experiments on synthetic and real-world datasets, including scientific documents, news publishers, and review ratings, demonstrate that MaT-LoRA achieves superior temporal generalization performance with practical scalability for LLMs.

</details>


### [93] [Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret](https://arxiv.org/abs/2602.11995)
*Yifei Jin,Xin Zheng,Lei Guo*

Main category: cs.LG

TL;DR: 本文研究了动量最小均方（MLMS）算法在时变随机线性系统中的应用，通过理论分析和实验验证了其在非平稳环境下的快速适应性和鲁棒跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模数据处理场景中，数据通常以由复杂系统生成的顺序流形式到达，这些系统表现出漂移分布和时变系统参数。这种非平稳性挑战了传统的独立同分布样本假设，需要能够实时更新而无需昂贵再训练的算法。

Method: 本文采用了动量最小均方（MLMS）算法作为自适应识别工具，并对其在不同实际条件下的时变随机线性系统的跟踪性能和遗憾界进行了理论推导。与经典LMS算法相比，MLMS由于引入了动量而增加了一个动态状态，导致了更复杂的稳定性分析问题。

Result: 合成数据流和真实世界数据流上的实验表明，MLMS实现了快速适应和鲁棒跟踪，特别是在非平稳设置下，这与我们的理论结果一致。

Conclusion: MLMS算法展示出在现代流处理和在线学习应用中的潜力，特别是在处理非平稳数据流方面。

Abstract: In large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.

</details>


### [94] [FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client](https://arxiv.org/abs/2602.12014)
*Gongxi Zhu,Hanlin Gu,Lixin Fan,Qiang Yang,Yuxing Han*

Main category: cs.LG

TL;DR: 提出了一种名为FedGRPO的新框架，用于联邦基础模型（FedFMs）中通过客户端数据增强服务器端大模型性能的问题。该方法通过构建轻量级信心图来选择合适的客户端，并将问题及其解决方案打包成候选策略发送给选定的专家客户端，仅收集结果奖励信号，从而减少了隐私风险和通信开销，同时在不同任务上实现了较高的准确性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 当前联邦基础模型(FedFMs)中存在的知识转移方法要么需要昂贵的本地训练，要么导致高通信成本并带来不可避免的隐私风险。研究旨在开发一种新的方法以克服这些问题。

Method: FedGRPO框架分为两个模块：第一个模块基于辅助数据建立轻量级的信心图来进行能力为基础的专家选择；第二个模块采用“组相对”概念从Group Relative Policy Optimization (GRPO)框架中，将每个问题与解决方案原理一起打包为候选策略，然后分发给被选中的专家客户端，并通过联邦组相对损失函数仅聚合得到的标量奖励信号。

Result: 实验证明，在多样化的领域任务上，FedGRPO相比传统FedFMs基线在下游准确性以及通信效率方面表现更优。

Conclusion: FedGRPO提供了一种有效的方法来提高联邦学习设置下大型基础模型的表现，同时减少隐私泄露的风险和通信成本。

Abstract: One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or incur high communication costs and introduce unavoidable privacy risks. We reformulate this problem as a reinforcement learning style evaluation process and propose FedGRPO, a privacy preserving framework comprising two modules. The first module performs competence-based expert selection by building a lightweight confidence graph from auxiliary data to identify the most suitable clients for each question. The second module leverages the "Group Relative" concept from the Group Relative Policy Optimization (GRPO) framework by packaging each question together with its solution rationale into candidate policies, dispatching these policies to a selected subset of expert clients, and aggregating solely the resulting scalar reward signals via a federated group-relative loss function. By exchanging reward values instead of data or model updates, FedGRPO reduces privacy risk and communication overhead while enabling parallel evaluation across heterogeneous devices. Empirical results on diverse domain tasks demonstrate that FedGRPO achieves superior downstream accuracy and communication efficiency compared to conventional FedFMs baselines.

</details>


### [95] [Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling](https://arxiv.org/abs/2602.12045)
*Jed A. Duersch,Elohan Veillon,Astrid Klipfel,Adlane Sayede,Zied Bouraoui*

Main category: cs.LG

TL;DR: 提出了一种基于倒易空间的晶体材料生成方法，通过傅里叶变换表示晶体结构，并使用变压器变分自编码器和潜在扩散模型来生成新的晶体单元。


<details>
  <summary>Details</summary>
Motivation: 发现新的晶体材料需要能够处理周期性边界条件、晶体学对称性和物理约束的生成模型，同时要能够扩展到大且结构多样的单元格。

Method: 采用一种基于倒易空间的生成管道，该管道通过物种分辨单位细胞密度的截断傅里叶变换来表示晶体，而不是直接建模原子坐标。此表示法是固有周期性的，允许简单的空间群对称性代数操作，并自然支持生成过程中可变的原子多重性。

Result: 使用每空间维度仅九个傅里叶基函数的方法，可以重建包含多达108个每个化学物种原子的单元格。在LeMaterial基准上评估了重构和潜在扩散，并与基于坐标的基线在小单元格体系（每单位晶胞$\leq 16$个原子）中进行了无条件生成比较。

Conclusion: 所提出的方法为生成具有复杂结构的新晶体材料提供了一个有效途径，尤其适用于大而结构多样化的单位晶胞，同时克服了基于粒子方法的一些限制。

Abstract: The discovery of new crystalline materials calls for generative models that handle periodic boundary conditions, crystallographic symmetries, and physical constraints, while scaling to large and structurally diverse unit cells. We propose a reciprocal-space generative pipeline that represents crystals through a truncated Fourier transform of the species-resolved unit-cell density, rather than modeling atomic coordinates directly. This representation is periodicity-native, admits simple algebraic actions of space-group symmetries, and naturally supports variable atomic multiplicities during generation, addressing a common limitation of particle-based approaches. Using only nine Fourier basis functions per spatial dimension, our approach reconstructs unit cells containing up to 108 atoms per chemical species. We instantiate this pipeline with a transformer variational autoencoder over complex-valued Fourier coefficients, and a latent diffusion model that generates in the compressed latent space. We evaluate reconstruction and latent diffusion on the LeMaterial benchmark and compare unconditional generation against coordinate-based baselines in the small-cell regime ($\leq 16$ atoms per unit cell).

</details>


### [96] [Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards](https://arxiv.org/abs/2602.12049)
*Ryo Mikasa,Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.LG

TL;DR: 本研究提出了一种在线强化学习方法，通过在超级计算机上执行大型语言模型生成的代码，并直接将测量到的运行时性能（GFLOPS）作为奖励反馈给模型。同时，引入了分阶段质量多样性算法来逐步改变每个问题允许的优化技术，使模型能够从多角度学习代码优化。实验结果表明，结合运行时性能反馈与分阶段优化的强化学习可以提高LLM在高性能计算代码生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展示了强大的代码生成能力，但生成代码的运行时性能并不总是得到保证，在高性能计算领域内鲜有尝试使用运行时性能作为奖励训练这些模型。

Method: 采用在线强化学习方法，该方法在超级计算机上执行由大型语言模型生成的代码，并以实际测量的运行时性能（GFLOPS）为奖励信号；引入了一个名为Staged Quality-Diversity (SQD) 的新算法，它能够根据具体问题逐步调整所允许使用的优化技术；构建了一个连接GPU训练集群和CPU基准测试集群的分布式系统，并利用Group Relative Policy Optimization (GRPO) 对Qwen2.5 Coder 14B模型进行双精度矩阵乘法任务的训练。

Result: 通过两个实验表明，结合运行时性能反馈与分阶段优化策略的强化学习方法确实能够增强大型语言模型在高性能计算场景下的代码生成能力。

Conclusion: 研究表明，通过将运行时性能作为奖励信号并结合分阶段优化策略来进行强化学习训练，可以有效提升大型语言模型在高性能计算领域的代码生成表现。

Abstract: Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.

</details>


### [97] [PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories](https://arxiv.org/abs/2602.12080)
*Hyunsung Kim,Kunhee Lee,Sangwoo Seo,Sang-Ki Ko,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文提出了PathCRF框架，利用球员跟踪数据来检测足球比赛中的持球事件。通过将球员轨迹建模为完全连接的动态图，并采用条件随机场（CRF）确保逻辑一致性，该方法能够准确地识别诸如控球或传球等事件，从而显著减少了手动事件标注的需求。


<details>
  <summary>Details</summary>
Motivation: 尽管AI领域取得了进步，但足球赛事的数据收集仍然严重依赖人工标注。现有研究虽然探索了使用球员和球的轨迹进行自动事件检测，但由于高昂的基础建设和运营成本，球的追踪也难以大规模应用。这导致全面的数据收集主要限于顶级比赛，限制了数据分析在这一领域的广泛应用。

Method: 提出了一种名为PathCRF的框架，它仅使用球员跟踪数据来检测足球比赛中的持球事件。该方法将球员轨迹建模为一个全连接的动态图，并且定义事件检测问题为在每个时间步选择一个代表当前控球状态的边。为了保证结果边序列的逻辑一致性，采用了条件随机场(CRF)禁止连续边之间的不可能转换。发射分数和转移分数都是从基于集合注意力机制骨干架构生成的边嵌入中动态计算得出。在推理过程中，通过Viterbi解码获得最可能的边序列，并且当相邻时间步之间选定的边发生变化时就检测到像控球或传球这样的事件。

Result: 实验表明，PathCRF能够产生准确且逻辑一致的控球路径，使得可靠的下游分析成为可能，同时大大降低了对人工事件标注的需求。

Conclusion: PathCRF框架为足球比赛中持球事件的自动化检测提供了一个有效的解决方案，有助于推动该领域内更加广泛的数据驱动分析应用。

Abstract: Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.

</details>


### [98] [Empirical Gaussian Processes](https://arxiv.org/abs/2602.12082)
*Jihao Andreas Lin,Sebastian Ament,Louis C. Tiao,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 本文提出了Empirical GPs，一种从历史观测数据中估计均值和协方差函数的方法，以克服传统高斯过程回归模型中核函数选择的局限性。该方法理论上收敛于与真实数据生成过程KL散度最小的GP，并在学习曲线外推和时间序列预测基准测试上表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯过程（GPs）模型在实践中有效性受限于手工挑选的核函数，这一过程不仅需要专业知识，而且对假设空间施加了较强的限制，导致适应数据的能力有限。

Method: 提出了一种称为Empirical GPs的新框架，通过从历史观察语料库中经验地估计均值和协方差函数来构建灵活的数据驱动GP先验，而非依赖于标准参数化核。此外，还为从独立数据集中学习GP先验的问题制定了似然估计法，并推导了一个具有封闭形式更新的期望最大化算法，支持处理跨数据集的异质观测位置。

Result: Empirical GPs能够在学习曲线外推和时间序列预测基准测试上达到有竞争力的表现。理论分析表明，所提出的模型能够收敛到与实际数据产生过程最接近（在KL散度意义上）的GP。

Conclusion: Empirical GPs提供了一种新的途径，通过直接从数据中学习来构建更加灵活且适应性强的GP先验，从而克服了传统GP模型的一些局限性。

Abstract: Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.

</details>


### [99] [On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and Partial Coverage](https://arxiv.org/abs/2602.12107)
*Haolin Liu,Braham Snyder,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 本文研究了在$Q^\star$近似和部分覆盖下的离线强化学习，通过引入一个新的决策-估计分解框架来表征给定$Q^\star$函数类的内在复杂性，并且在部分覆盖的情况下为软$Q$-学习提供了首个$\epsilon^{-2}$样本复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 探讨$Q^\star$可实现性和贝尔曼完备性是否足以保证在部分覆盖下进行样本高效的离线强化学习，同时针对这一问题提出了新的理论贡献和技术改进。

Method: 提出了一种新的决策-估计分解框架，该框架可以与多种$Q^\star$估计程序结合使用；开发了一个新颖的二阶性能差异引理以改善样本复杂度的结果。

Result: 得出了信息论下界，证明了仅靠$Q^\star$可实现性和贝尔曼完备性不足以实现样本高效的学习；对软$Q$-学习给出了第一个$\epsilon^{-2}$样本复杂度结果；去除了当$Q^\star$的价值差距未知时额外在线交互的需求；首次分析了非表格情况下的CQL。

Conclusion: 本文的工作不仅回答了关于离线RL中$Q^\star$可实现性和贝尔曼完备性的开放问题，而且提供了一系列新的理论见解和技术工具，促进了我们对离线强化学习挑战的理解。

Abstract: We study offline reinforcement learning under $Q^\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: "Are $Q^\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?"
  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\star$ estimation procedures, modularizing and generalizing existing approaches.
  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $ε^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $ε^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\star$-realizability and Bellman completeness beyond the tabular case.

</details>


### [100] [Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning](https://arxiv.org/abs/2602.12123)
*Xubin Wang,Weijia Jia*

Main category: cs.LG

TL;DR: 提出了一种轻量级的监督元学习方法Meta-Sel，用于意图分类中的演示选择问题。该方法通过训练一个基于两个低成本元特征（TF-IDF余弦相似度和长度兼容性比率）的校准逻辑回归模型来实现。在推理时，Meta-Sel能够对整个候选池进行一次性向量化评分，并返回前k个演示示例，无需微调模型、在线探索或额外的大语言模型调用。此外，本文还进行了广泛的实证研究，比较了12种不同方法在四个意图数据集和五个开源大语言模型上的表现。结果显示，Meta-Sel在这些基准测试中始终表现出色，尤其对于较小的模型特别有效。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习中，演示选择是一个实际瓶颈：当提示预算有限时，所包含的少数示例会显著影响准确性，但同时选择过程必须足够高效以应对大规模候选池。

Method: Meta-Sel是一种轻量级的监督元学习方法，它从标记训练数据中学习快速且可解释的（候选，查询）对打分函数。通过从训练集中采样对并使用类别一致性作为监督构建元数据集，然后在一个基于TF-IDF余弦相似性和长度兼容性比率的低成本元特征上训练一个校准逻辑回归器。

Result: Meta-Sel能够在不需要模型微调、在线探索或额外的大语言模型调用的情况下，对整个候选池执行一次向量化评分，并返回最佳k个示例。这种方法提供了确定性的排名，并通过可解释的功能权重使选择机制易于审核。广泛的经验研究表明，在四个意图数据集和五个开源大型语言模型上的十二种方法对比中，Meta-Sel表现持续位于前列，尤其是在小规模模型中效果尤为突出。

Conclusion: Meta-Sel作为一种高效、低成本的演示选择方案，在提升意图分类任务性能方面展现了巨大潜力，特别是在资源受限情况下为小型模型提供高质量的选择支持。

Abstract: Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.
  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.
  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.

</details>


### [101] [Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation](https://arxiv.org/abs/2602.12125)
*Wenkai Yang,Weijie Liu,Ruobing Xie,Kai Yang,Saiyong Yang,Yankai Lin*

Main category: cs.LG

TL;DR: 本文扩展了在线策略蒸馏(OPD)框架，提出了广义在线策略蒸馏(G-OPD)，通过引入灵活的参考模型和奖励缩放因子来控制奖励项与KL正则化的相对权重。实验表明，当奖励缩放因子大于1时(ExOPD)，学生模型在多种任务中表现优于标准OPD，并且在强到弱的知识蒸馏设置下，使用教师模型在强化学习前的基础模型作为参考可以进一步提高性能。


<details>
  <summary>Details</summary>
Motivation: 在线策略蒸馏(OPD)已被证明能够有效提升学生模型的表现，但其理论基础及其相对于其他方法的优势尚未得到充分探讨。此外，如何更有效地利用教师模型的知识以进一步优化学生模型也是一个值得探索的问题。

Method: 首先从理论上将OPD定义为一种具有特定约束条件的密集KL约束强化学习。基于此理解，作者提出了一种新的框架G-OPD，它允许调整奖励函数与KL正则化之间的平衡，并支持任意选择参考模型。

Result: 实验结果表明，在数学推理和代码生成任务上，采用奖励外推(即奖励缩放因子大于1)的方法(ExOPD)能显著优于传统OPD方法。同时，在强至弱模型转换场景下，若选取教师模型未经强化学习调整的基础版本作为参考，则可获得更佳的知识转移效果。

Conclusion: 本研究不仅加深了对OPD机制的理解，还提供了一种新方法G-OPD用于改进知识蒸馏过程。特别是发现适当的奖励缩放及参考模型选择对于增强学生模型至关重要。

Abstract: On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.

</details>


### [102] [Oscillators Are All You Need: Irregular Time Series Modelling via Damped Harmonic Oscillators with Closed-Form Solutions](https://arxiv.org/abs/2602.12139)
*Yashas Shende,Aritra Das,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: Researchers developed a new method that combines the strengths of Transformers and Neural ODEs for handling irregular time series, by using a linear damped harmonic oscillator analogy with a known closed-form solution, which improves computational efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of Transformers in dealing with irregular time series and the computational inefficiency of Neural ODEs, by introducing an approach that leverages a closed-form solution based on a linear damped harmonic oscillator analogy.

Method: The method involves modeling keys and values as damped, driven oscillators and expanding the query in a sinusoidal basis. The use of a closed-form solution for the oscillator-based system allows for efficient computation while maintaining the expressivity and universal approximation property of continuous-time attention mechanisms.

Result: The result is a scalable and computationally efficient method that achieves state-of-the-art performance on benchmarks for irregular time series, significantly outpacing existing methods in terms of speed.

Conclusion: The conclusion is that the proposed oscillator-based parameterization not only resolves the computational bottleneck associated with numerical ODE solvers but also provides theoretical guarantees and superior performance for irregular time series modeling.

Abstract: Transformers excel at time series modelling through attention mechanisms that capture long-term temporal patterns. However, they assume uniform time intervals and therefore struggle with irregular time series. Neural Ordinary Differential Equations (NODEs) effectively handle irregular time series by modelling hidden states as continuously evolving trajectories. ContiFormers arxiv:2402.10635 combine NODEs with Transformers, but inherit the computational bottleneck of the former by using heavy numerical solvers. This bottleneck can be removed by using a closed-form solution for the given dynamical system - but this is known to be intractable in general! We obviate this by replacing NODEs with a novel linear damped harmonic oscillator analogy - which has a known closed-form solution. We model keys and values as damped, driven oscillators and expand the query in a sinusoidal basis up to a suitable number of modes. This analogy naturally captures the query-key coupling that is fundamental to any transformer architecture by modelling attention as a resonance phenomenon. Our closed-form solution eliminates the computational overhead of numerical ODE solvers while preserving expressivity. We prove that this oscillator-based parameterisation maintains the universal approximation property of continuous-time attention; specifically, any discrete attention matrix realisable by ContiFormer's continuous keys can be approximated arbitrarily well by our fixed oscillator modes. Our approach delivers both theoretical guarantees and scalability, achieving state-of-the-art performance on irregular time series benchmarks while being orders of magnitude faster.

</details>


### [103] [It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks](https://arxiv.org/abs/2602.12147)
*Zhongzheng Qiao,Sheng Pan,Anni Wang,Viktoriya Zhukova,Yong Liu,Xudong Jiang,Qingsong Wen,Mingsheng Long,Ming Jin,Chenghao Liu*

Main category: cs.LG

TL;DR: 提出了TIME，一个针对时间序列基础模型（TSFMs）的新一代任务中心基准，包括50个新数据集和98个预测任务，旨在解决现有基准在数据组成、数据完整性、任务表述和分析视角方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列基础模型（TSFMs）评价基准存在几个关键问题：数据来源受限且多为旧有资源重复使用；数据质量保证不足；任务设定与实际应用场景脱节；以及分析角度过于僵化，难以揭示通用见解。

Method: 开发了TIME，一个新的以任务为中心的评估基准，包含50个新的数据集及98个预测任务，专为严格的零样本TSFM评估而设计。通过结合大型语言模型与人类专家知识建立了一个严格的人机协同基准构建流程来确保高数据完整性和重新定义任务设定。此外还提出了一种新颖的模式级别评估视角，利用结构化时间序列特征来描述内在的时间属性。

Result: 对12个代表性TSFMs进行了评估，并建立了一个多层次排行榜以便深入分析和可视化检查。

Conclusion: TIME作为一个下一代任务中心基准，通过引入新鲜的数据集和改进的任务设定方式，解决了当前时间序列基础模型评测中存在的多个问题，为理解不同模式下模型能力提供了更通用的见解。

Abstract: Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.

</details>


### [104] [Amortized Molecular Optimization via Group Relative Policy Optimization](https://arxiv.org/abs/2602.12162)
*Muhammad bin Javaid,Hasham Hussain,Ashima Khanna,Berke Kisin,Jonathan Pirnay,Alexander Mitsos,Dominik G. Grimm,Martin Grohe*

Main category: cs.LG

TL;DR: 介绍了一种新的分子设计方法GRXForm，通过预训练的Graph Transformer模型和Group Relative Policy Optimization（GRPO）技术优化分子结构，以解决现有方法在处理不同起始结构时遇到的高方差问题。该方法能够无需推理时间的oracle调用或精炼过程就能在多目标优化中取得与领先实例优化器相竞争的成绩。


<details>
  <summary>Details</summary>
Motivation: 当前分子设计领域的方法在面对不同的起始结构时表现出较大的性能差异，这主要归因于各自起始结构异质性的难度导致的高方差问题。为了解决这个问题，并且提高对未见过结构的泛化能力，提出了新的解决方案。

Method: 提出了一种名为GRXForm的新方法，它基于预训练的Graph Transformer模型工作，通过逐步添加原子和键来优化分子。此外，引入了Group Relative Policy Optimization (GRPO) 技术用于目标导向的微调，通过相对于起始结构标准化奖励来减少方差。

Result: 实验证明，GRXForm能够在不依赖于推理时的oracle调用或额外精炼步骤的情况下，对分布外的分子骨架进行有效泛化，并在多目标优化任务上达到了与顶级实例优化器相当的表现。

Conclusion: GRXForm提供了一个有效的途径来解决分子设计中的高方差挑战，证明了其在多种起始结构下保持良好性能的能力，为未来研究开辟了新方向。

Abstract: Molecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as "Instance Optimizers'', expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.

</details>


### [105] [How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics](https://arxiv.org/abs/2602.12180)
*Yurong Chen,Yu He,Michael I. Jordan,Fan Yao*

Main category: cs.LG

TL;DR: 该论文研究了在偏好对齐框架中采样和参考策略选择的影响，展示了适当的实例依赖性采样可以提供更强的排序保证，并且分析了迭代对齐动态可能产生的持续振荡或熵崩溃现象。通过实验证实了这些发现。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的使大型语言模型与人类偏好保持一致的方法是有效的，但关于采样和参考策略选择对其效果的影响，在理论上仍不清晰。

Method: 通过广泛使用的偏好对齐框架——身份偏好优化（Identity Preference Optimization），研究了采样和参考策略选择的效果。还分析了一个迭代对齐过程，其中学到的策略会反馈到未来的采样和参考策略中。

Result: 证明了适当的实例依赖性采样能够提供更强的排序保证；而有偏的在线策略采样在结构化偏好下可能会导致过度集中。对于某些参数选择，所研究的动力学过程可能会表现出持续振荡或熵崩溃的现象，并且确定了能确保稳定性的条件。

Conclusion: 本研究为理解偏好对齐方法中的关键因素提供了理论见解，这些发现也适用于直接偏好优化等更广泛的偏好对齐方法。实验结果支持了上述理论分析。

Abstract: Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.

</details>


### [106] [WaveFormer: Wavelet Embedding Transformer for Biomedical Signals](https://arxiv.org/abs/2602.12189)
*Habib Irani,Bikram De,Vangelis Metsis*

Main category: cs.LG

TL;DR: 提出了一种名为WaveFormer的新型变压器架构，通过在嵌入构建和位置编码两个关键阶段集成小波分解来改善生物医学信号分类。该方法在处理长序列、复杂时间动态和多尺度频率模式方面表现出色，并在八个不同数据集上展示了竞争力。


<details>
  <summary>Details</summary>
Motivation: 生物医学信号分类面临挑战，包括长序列、复杂的时间动态以及多尺度频率模式，这些特点使得标准的Transformer架构难以有效捕捉。因此，需要一种新的架构来更好地处理这类信号。

Method: WaveFormer是一种改进的Transformer架构，在嵌入构造和位置编码两个重要环节融入了小波分解技术。具体来说，通过多通道离散小波变换（DWT）提取频率特征形成包含时域与频域信息的令牌；同时采用单通道DWT分析适应信号特定时间结构的位置编码。

Result: 通过对涵盖人类活动识别及脑信号分析的八个不同数据集进行评估，其中序列长度从50到3000个时间步不等，信道数量从1至144变化，实验结果表明WaveFormer通过全面的频率感知处理实现了竞争性的表现。

Conclusion: WaveFormer提供了一个将频域知识整合进基于Transformer的时间序列分类中的原则性框架，为处理具有复杂时间动态和多尺度频率特性的生物医学信号提供了有效的解决方案。

Abstract: Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.

</details>


### [107] [Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction](https://arxiv.org/abs/2602.12204)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 研究发现GPT-2模型中88%的注意力操作是多余的，基于此提出了CRAM机制，该机制能够减少训练过程中对注意力的需求，并在SRCD基准测试上实现了显著的效果提升。


<details>
  <summary>Details</summary>
Motivation: 研究人员观察到现有混合架构中的注意力使用模式要么均匀应用要么形成静态稀疏模式，这两种方式都忽视了随着重复模式变得熟悉，对注意力的需求应该随时间减少这一点。通过分析GPT-2模型，他们进一步发现大量的注意力操作实际上是在处理已经可以从模型隐藏状态预测出来的信息，这表明存在改进的空间。

Method: 介绍了一种名为CRAM（基于整合的自适应记忆路由）的新方法，灵感来源于生物的记忆巩固过程。CRAM旨在逐渐将情景检索提炼为参数化的语义记忆，从而随着时间推移减少对外部注意力资源的需求。与以前的方法相比，CRAM能够在训练过程中实现注意力使用的逐步下降。

Result: 实验结果表明，在大约3000步之后，CRAM能够通过一个明显的阶段转变达到37.8倍的注意力减少。此外，在提出的SRCD基准测试中，CRAM仅以1.6%的注意力计算量就达到了100%的检索准确率（相比之下，基线方法需要68%）。更重要的是，经过学习的巩固动态与人类从情节记忆到语义记忆转换曲线定量匹配良好。

Conclusion: CRAM提供了一种有效降低注意力需求同时保持甚至提高性能的新途径。它不仅证明了自己在特定任务上的优越性，还展示了良好的泛化能力，能够在未经重新训练的情况下将已巩固的模式转移到新任务上并减少约48-52%的注意力需求。

Abstract: Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \textbf{88\%} of attention operations retrieve information already predictable from the model's hidden state, and this redundancy does \emph{not} decrease during training. Motivated by this observation, we introduce \textbf{\ours{}} (\textbf{C}onsolidation-based \textbf{R}outing for \textbf{A}daptive \textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \ours{} exhibits \emph{decreasing attention utilization} over training, achieving a \textbf{37.8$\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \emph{impossible} without consolidation: any static routing scheme requires $Ω(f \cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \ours{} achieves \textbf{100\% retrieval accuracy} at 1.6\% attention compute (vs.\ 68\% for baselines), and consolidated patterns transfer to unseen tasks with \textbf{48--52\%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($γ= 0.43$ vs.\ $γ_{\text{human}} \approx 0.4$--$0.5$). Code and benchmarks are available at [anonymized].

</details>


### [108] [The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics](https://arxiv.org/abs/2602.12218)
*Christian Internò,Jumpei Yamaguchi,Loren Amdahl-Culleton,Markus Olhofer,David Klindt,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种非侵入性的评估协议PhyIP，用于测试物理量是否可以从冻结的表示中线性解码。研究发现在流体动力学和轨道力学中，当自监督学习（SSL）达到低错误率时，潜在结构变得线性可访问。与基于适应的评估相比，PhyIP在OOD测试中能更好地恢复内部能量和牛顿反平方比例关系。


<details>
  <summary>Details</summary>
Motivation: 现有的神经模型是否将物理定律作为世界模型内化，而不是利用统计捷径，这一点仍然难以确定，尤其是在分布外（OOD）变化的情况下。标准评估通常通过下游适应（例如微调或高容量探针）来测试潜在能力，但这种干预可以改变正在测量的表示，从而混淆了自监督学习期间学到的内容。

Method: 提出了一种称为PhyIP的非侵入性评估协议，该方法测试物理量是否能够从固定的表示中被线性解码，并且受到线性表示假设的启发。

Result: 在流体动力学和轨道力学领域，当自监督学习达到较低误差时，潜变量结构变得可通过线性方式获取。PhyIP能够在OOD测试中恢复出内部能量和牛顿逆平方律关系（如ρ>0.90）。相比之下，基于调整的方法可能会破坏这种结构（如ρ≈0.05）。

Conclusion: 基于适应的评估可能会掩盖潜在结构，而低容量探针则提供了对物理世界模型更准确的评估。

Abstract: Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.

</details>


### [109] [Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222)
*Miaosen Zhang,Yishan Liu,Shuxia Lin,Xu Yang,Qi Dai,Chong Luo,Weihao Jiang,Peng Hou,Anxiang Zeng,Xin Geng,Baining Guo*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，通过引入分布判别理论（DDT）和两种互补技术——分布内微调（IDFT）与提示解码，来解决监督微调（SFT）在泛化能力上不如强化学习（RL）的问题。该框架保持了SFT的效率，同时实现了与离线RL算法相当的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调虽然计算效率高，但其泛化能力通常不如强化学习。本文旨在通过开发一种新型框架缩小这一差距，使得即使在无法使用强化学习的情况下也能获得良好的泛化表现。

Method: 首先提出了分布判别理论(DDT)，用于解释并量化数据与模型诱导分布之间的对齐情况；基于DDT，进一步发展了两种互补的技术：一种是在损失层面上增强SFT泛化能力的分布内微调(IDFT)方法，另一种是能在数据层面上重新调整训练语料库以匹配模型分布的提示解码技术。

Result: 大量实验表明，所提出的框架不仅能够达到与知名离线RL算法（如DPO和SimPO）相媲美的泛化性能，还保留了SFT流程的高效性。

Conclusion: 该研究为那些不适合采用强化学习的领域提供了一个实用的选择方案，并展示了如何有效提升监督微调方法的泛化能力。

Abstract: Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT

</details>


### [110] [Categorical Flow Maps](https://arxiv.org/abs/2602.12233)
*Daan Roos,Oscar Davis,Floor Eijkelboom,Michael Bronstein,Max Welling,İsmail İlkan Ceylan,Luca Ambrogioni,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 本文提出了一种名为Categorical Flow Maps的流匹配方法，通过自蒸馏加速类别数据的少量步骤生成。该方法基于最近的流匹配变分公式和扩散及流模型中加速推理的趋势，定义了一个朝向单纯形的流映射，从而自然地约束了模型预测。此外，由于轨迹是连续的，因此可以使用现有的蒸馏技术进行训练，并且在测试时可以直接重用现有的指导和重新加权技术来引导采样达到下游目标。实验结果表明，在图像、分子图和文本上实现了领先的少量步骤生成效果，即使是在单步生成中也表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了加速类别数据的少量步骤生成，同时保持或提高生成质量，作者提出了Categorical Flow Maps方法。此方法旨在解决现有生成模型在快速生成高质量样本方面的不足，特别是当应用到如图像、分子图以及文本等复杂数据类型时。

Method: Categorical Flow Maps是一种基于流匹配的新方法，它利用了最近提出的流匹配变分公式，并结合了单纯形作为目标空间以确保概率质量被正确转移至预测终点。此外，借助于连续而非离散的轨迹表示，使得该方法能够兼容现有的知识蒸馏技术和引入了一种新的基于终点一致性的训练目标。

Result: 实验结果显示，Categorical Flow Maps在不同类型的类别数据（包括图像、分子图和文本）上均取得了最先进的少量步骤生成性能，尤其值得注意的是，在单步生成任务中也表现出了强大的能力。

Conclusion: Categorical Flow Maps提供了一种有效的手段来加速类别数据的生成过程，同时保持较高的生成质量。这种方法不仅适用于多种类型的数据集，而且还可以很容易地与现有的指导策略相结合，为实现特定目的而调整生成过程。

Abstract: We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.

</details>


### [111] [Olmix: A Framework for Data Mixing Throughout LM Development](https://arxiv.org/abs/2602.12237)
*Mayee F. Chen,Tyler Murray,David Heineman,Matt Jordan,Hannaneh Hajishirzi,Christopher Ré,Luca Soldaini,Kyle Lo*

Main category: cs.LG

TL;DR: 本文提出了一种名为Olmix的框架，旨在解决数据混合方法在实际语言模型开发中遇到的两个主要问题：一是对开发混合方法的配置空间理解不足；二是现有工作大多假设领域集固定不变，而实际开发过程中领域集会不断变化。Olmix通过全面的经验研究确定了哪些设计选择可以构成有效的混合方法，并引入了混合重用机制来高效地处理领域集更新后混合比例的重新计算。实验表明，在模拟真实世界LM开发过程中的五次领域集更新序列上，该方法相比完全重新计算节省了74%的计算资源，并且在下游任务上的表现提高了11.6%。


<details>
  <summary>Details</summary>
Motivation: 当前的数据混合方法在应用于实际的语言模型开发时存在局限性，包括对于开发混合方法所需考虑的设计选项缺乏充分的理解或共识，以及这些方法通常基于固定的领域集合来进行，这与现实中领域集合随时间变化的情况不符。为了解决这些问题并提高数据混合效率，提出了Olmix框架。

Method: Olmix框架通过对不同设计方案进行全面的经验性研究来探索有效混合方法的设计选择，并引入了‘混合重用’机制。这种机制允许在领域集发生变化时仅重新计算受影响领域的比率，而不是每次都从头开始整个过程，从而显著减少了所需的计算量。

Result: 实验结果表明，在模拟真实场景下的连续五次领域集变更过程中，使用Olmix框架的混合重用技术能够达到与每次完整重新计算相媲美的性能水平，同时将计算成本降低了74%。此外，在下游任务的表现上也观察到了11.6%的改进。

Conclusion: Olmix框架提供了一种更加灵活和高效的解决方案，以应对实际语言模型开发过程中遇到的数据混合挑战。它不仅有助于更好地理解和优化混合方法的设计，而且通过其创新性的混合重用机制，在保证性能的同时大大减少了计算需求。

Abstract: Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.

</details>


### [112] [Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces](https://arxiv.org/abs/2602.12245)
*Anthony Kobanda,Waris Radji*

Main category: cs.LG

TL;DR: 本文探讨了联合嵌入预测架构（JEPAs）与准度量强化学习（QRL）之间的联系，通过内在能量函数的概念，表明在目标到达控制中，最优成本到函数具有内在形式；反之，训练用于建模内在能量的JEPAs属于QRL所针对的准度量价值类别。此外，还讨论了对称有限能量在单向可达性上的结构不匹配问题，提出了当方向性重要时采用不对称（准度量）能量的理由。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于建立联合嵌入预测架构（JEPAs）和准度量强化学习（QRL）之间的桥梁，探索它们如何通过内在能量函数相互关联，并理解为何在特定情况下需要考虑能量或距离度量的非对称性。

Method: 本文通过定义一类JEPA能量函数——内在（最小作用）能量，作为两个状态之间可接受轨迹上累积局部努力的下确界，来连接JEPAs与QRL的观点。基于温和的闭合性和可加性假设，证明任何内在能量都是一个准度量。

Result: 研究表明，在目标达到控制中，最佳成本到函数恰好采取了这种内在形式。相反地，为建模内在能量而训练的JEPAs落入了QRL所瞄准的准度量价值类别。同时指出了对称有限能量对于单向可达性的结构不匹配，从而支持了当方向性起关键作用时采用不对称（准度量）能量的重要性。

Conclusion: 结论是，通过引入内在能量的概念，能够有效链接JEPAs与QRL领域内的研究工作，强调了在处理具有方向性特征的问题时采用准度量而非度量的重要性。

Abstract: Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [113] [Filtered Approximate Nearest Neighbor Search in Vector Databases: System Design and Performance Analysis](https://arxiv.org/abs/2602.11443)
*Abylay Amanbayev,Brian Tsan,Tri Dang,Florin Rusu*

Main category: cs.DB

TL;DR: 本文系统地分类了过滤策略，并评估了它们在FAISS、Milvus和pgvector中的集成效果，通过引入新的关系型数据集MoReVec及提出Global-Local Selectivity (GLS)相关性度量标准来量化过滤器与查询向量间的关系。研究发现引擎内的算法调整往往能超越原始索引性能，并据此为混合搜索负载下的索引类型选择和查询优化器配置提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成(RAG)应用对结合语义检索与元数据约束的需求增加，对于矢量数据库中通用过滤策略表现的理解仍显不足。本文旨在填补这一空白，通过系统化分析不同过滤策略在主流矢量数据库中的集成情况及其性能影响。

Method: 本文首先对过滤策略进行了系统化的分类，接着在FAISS、Milvus以及pgvector三个平台中评估这些策略的集成效果。为此，还特别构建了一个名为MoReVec的新关系型数据集用于基准测试，并提出了Global-Local Selectivity(GLS)指标以衡量过滤器与查询向量之间的关联程度。

Result: 实验结果显示，引擎内部的算法适应性经常能够超越单纯依赖索引的表现。具体来说：1. Milvus通过混合近似/精确执行实现了更高的召回稳定性；2. pgvector的成本基础查询优化器倾向于选择次优执行计划，在某些情况下即使顺序扫描可达到完美召回率且延迟相当；3. 对于低选择性查询，基于分区的索引（如IVFFlat）比基于图的索引（如HNSW）表现更好。

Conclusion: 本研究不仅揭示了不同类型索引在处理带有过滤条件的最近邻搜索任务时的行为差异，同时也强调了根据特定应用场景合理选择索引类型及调优查询优化器的重要性。此外，扩展了ANN-Benchmarks支持过滤向量搜索功能，并公开发布了该工具，以促进领域内进一步的研究与发展。

Abstract: Retrieval-Augmented Generation (RAG) applications increasingly rely on Filtered Approximate Nearest Neighbor Search (FANNS) to combine semantic retrieval with metadata constraints. While algorithmic innovations for FANNS have been proposed, there remains a lack of understanding regarding how generic filtering strategies perform within Vector Databases. In this work, we systematize the taxonomy of filtering strategies and evaluate their integration into FAISS, Milvus, and pgvector. To provide a robust benchmarking framework, we introduce a new relational dataset, \textit{MoReVec}, consisting of two tables, featuring 768-dimensional text embeddings and a rich schema of metadata attributes. We further propose the \textit{Global-Local Selectivity (GLS)} correlation metric to quantify the relationship between filters and query vectors.
  Our experiments reveal that algorithmic adaptations within the engine often override raw index performance. Specifically, we find that: (1) \textit{Milvus} achieves superior recall stability through hybrid approximate/exact execution; (2) \textit{pgvector}'s cost-based query optimizer frequently selects suboptimal execution plans, favoring approximate index scans even when exact sequential scans would yield perfect recall at comparable latency; and (3) partition-based indexes (IVFFlat) outperform graph-based indexes (HNSW) for low-selectivity queries. To facilitate this analysis, we extend the widely-used \textit{ANN-Benchmarks} to support filtered vector search and make it available online. Finally, we synthesize our findings into a set of practical guidelines for selecting index types and configuring query optimizers for hybrid search workloads.

</details>


### [114] [Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases](https://arxiv.org/abs/2602.11573)
*Wenyang Zhou,Jiadong Xie,Yingfan Liu,Zhihao Yin,Jeffrey Xu Yu,Hui Li,Zhangqian Mu,Xiaotian Qiao,Jiangtao Cui*

Main category: cs.DB

TL;DR: 本论文提出了一种名为FastPGT的高效框架，用于调整邻近图（PG）构建参数。通过同时构建多个PG，FastPGT加速了参数估计过程，减少了重复计算，并且在不牺牲调优质量的情况下，相比现有最佳方法VDTuner实现了最高2.37倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: k-近似最近邻搜索(k-ANNS)在高维向量空间中是一个跨领域的基本问题，随着向量数据库和检索增强生成的发展受到了越来越多的关注。基于邻近图(PG)的方法是当前最先进的解决方案之一，但其性能受到构造参数的影响很大。然而，目前还没有一种方法能够针对这一参数调整的过程进行考虑和优化。

Method: 本文介绍了FastPGT，这是一种用于调整PG构造参数的高效框架。FastPGT通过同时构建多个PG来加速参数估计，从而减少重复计算。此外，研究者们还改进了现有的最优调参模型，使其能够一次性推荐多个参数，并利用我们同时构建多个PG的方法对这些参数进行有效评估。

Result: 通过对真实数据集的广泛实验表明，FastPGT与现有最先进方法VDTuner相比，实现了高达2.37倍的速度提升，而且没有降低调参的质量。

Conclusion: FastPGT为解决邻近图(PG)构建过程中参数调整成本高的问题提供了一个有效的方案，通过同时构建并评估多个PG显著提高了参数调优效率。

Abstract: k-approximate nearest neighbor search (k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process.In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend multiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality.

</details>


### [115] [Towards a theory of Façade-X data access: satisfiability of SPARQL basic graph patterns](https://arxiv.org/abs/2602.11756)
*Luigi Asprino,Enrico Daga*

Main category: cs.DB

TL;DR: This paper consolidates Façade-X, a method for direct access to various data formats through a specialized RDF meta-model, and studies the satisfiability of SPARQL basic graph patterns. It provides an algorithm for deciding satisfiability and demonstrates practical feasibility via experiments, paving the way for improved query execution strategies and more efficient data integration for knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and effectiveness of data integration for knowledge graphs by addressing the limitations of Façade-X in handling SPARQL queries, specifically focusing on the satisfiability of basic graph patterns over Façade-X graphs, which are representations of non-graph data sources such as CSV, XML, or relational databases.

Method: The authors consolidate the Façade-X approach, formalize the concept of satisfiability for basic graph patterns within this framework, and develop an algorithm to determine if a given SPARQL query is satisfiable when applied to Façade-X data sources. They also conduct extensive experiments using a proof-of-concept implementation to test the practicality of their theory with real-world queries.

Result: The results show that the proposed algorithm can effectively decide the satisfiability of basic graph patterns on Façade-X data sources, and the experimental evaluation confirms the practical feasibility of the approach. This work sets the foundation for further research into query execution strategies for Façade-X and supports the development of more efficient data integration systems for knowledge graphs.

Conclusion: The conclusion suggests that the study successfully addresses the satisfiability of SPARQL queries over Façade-X data, providing a solid theoretical and practical basis for future improvements in data integration for knowledge graphs, and indicating potential for optimizing query execution and system performance.

Abstract: Data integration is the primary use case for knowledge graphs. However, integrated data are not typically graphs but come in different formats, for example, CSV, XML, or a relational database. Façade-X is a recently proposed method for providing direct access to an open-ended set of data formats. The method includes a meta-model that specialises RDF to fit general data structures. This model allows to express SPARQL queries targeting data sources with those structures. Previous work formalised Façade-X and demonstrated how it can theoretically represent any format expressible with a context-free grammar, as well as the relational model. A reference implementation, SPARQL Anything, demonstrates the feasibility of the approach in practice. It is noteworthy that Façade-X utilises a fraction of RDF, and, consequently, not all SPARQL queries yield a solution (i.e. are satisfiable) when evaluated over a Façade-X graph. In this article, we consolidate Façade-X, and we study the satisfiability of basic graph patterns. The theory is accompanied by an algorithm for deciding the satisfiability of basic graph patterns on Façade-X data sources. Furthermore, we provide extensive experiments with a proof-of-concept implementation, demonstrating practical feasibility, including with real-world queries. Our results pave the way for studying query execution strategies for Façade-X data access with SPARQL and supporting developers to build more efficient data integration systems for knowledge graphs.

</details>


### [116] [Data-Driven Trajectory Imputation for Vessel Mobility Analysis](https://arxiv.org/abs/2602.11890)
*Giannis Spiliopoulos,Alexandros Troupiotis-Kapeliaris,Kostas Patroumpas,Nikolaos Liapis,Dimitrios Skoutas,Dimitris Zissis,Nikos Bikakis*

Main category: cs.DB

TL;DR: 本文提出了一种名为HABIT的轻量级、可配置的基于H3聚合的船只轨迹填补框架，用于解决AIS数据中因覆盖限制或故意中断导致的轨迹空白问题。通过从历史AIS数据中提取、分析和索引运动模式，该方法在保持准确性的同时，在延迟方面优于基线方法，并且考虑到了船只特性和其运动模式。


<details>
  <summary>Details</summary>
Motivation: 由于覆盖限制或有意图的传输中断，AIS数据中存在大量船只轨迹空白，这会显著降低数据质量，导致分析不准确或不完整。现有最先进的填补方法主要针对车辆轨迹设计，但船只（特别是航行中的船只）的运动模式与之大不相同，例如平滑转弯、靠近港口操作或在恶劣天气条件下航行等特性未被充分考虑。

Method: 提出了HABIT框架，这是一种基于H3聚合的轻量级、可配置的船只轨迹填补方案。该框架通过从历史AIS数据中抽取、分析并索引运动模式来填补缺失的轨迹段落。

Result: 实证研究表明，在不同时间段、密度以及船只类型下，HABIT生成的海上轨迹填补结果不仅在准确性上可以与基准方法相媲美，而且在延迟表现上更优，同时还能考虑到船只特征及其运动模式。

Conclusion: HABIT提供了一种有效的方法来填补AIS数据中缺失的轨迹部分，通过利用历史数据中的移动模式进行预测。这种方法在保证了高精度的同时还提高了处理速度，并且能够适应不同类型船只的不同运动特点。

Abstract: Modeling vessel activity at sea is critical for a wide range of applications, including route planning, transportation logistics, maritime safety, and environmental monitoring. Over the past two decades, the Automatic Identification System (AIS) has enabled real-time monitoring of hundreds of thousands of vessels, generating huge amounts of data daily. One major challenge in using AIS data is the presence of large gaps in vessel trajectories, often caused by coverage limitations or intentional transmission interruptions. These gaps can significantly degrade data quality, resulting in inaccurate or incomplete analysis. State-of-the-art imputation approaches have mainly been devised to tackle gaps in vehicle trajectories, even when the underlying road network is not considered. But the motion patterns of sailing vessels differ substantially, e.g., smooth turns, maneuvering near ports, or navigating in adverse weather conditions. In this application paper, we propose HABIT, a lightweight, configurable H3 Aggregation-Based Imputation framework for vessel Trajectories. This data-driven framework provides a valuable means to impute missing trajectory segments by extracting, analyzing, and indexing motion patterns from historical AIS data. Our empirical study over AIS data across various timeframes, densities, and vessel types reveals that HABIT produces maritime trajectory imputations performing comparably to baseline methods in terms of accuracy, while performing better in terms of latency while accounting for vessel characteristics and their motion patterns.

</details>


### [117] [Designing and Comparing RPQ Semantics](https://arxiv.org/abs/2602.11949)
*Victor Marsault,Antoine Meyer*

Main category: cs.DB

TL;DR: 本文提出了一种框架，用于根据其他标准对正则路径查询（RPQ）语义进行分类和比较。通过将RPQ语义形式化为从数据库和查询映射到有限行走集的数学函数，定义了若干可能的属性，并展示了某些属性是相互排斥或无法满足的。此外，还给出了几个新的RPQ语义作为示例，以期对未来图数据库查询语言中新的语义设计提供灵感。


<details>
  <summary>Details</summary>
Motivation: 现代属性图数据库查询语言如Cypher、PGQL、GSQL以及标准GQL受到正则路径查询(RPQs)的形式主义启发。为了明确输出路径，它们偏离了经典的同态语义。然而，由于RPQs可能会匹配无限多的路径，向用户展示结果变得困难。现有语言使用特定标准来选择所有匹配中的一个有限子集。尽管有关于这些语义的研究，但主要集中在评估效率上。本文试图更好地理解、选择并设计RPQ语义。

Method: 文章提出了一个框架，用于根据其他标准来分类和比较RPQ语义。作者形式化了多个潜在属性，这些属性与被视为将数据库和查询映射到有限行走集合的数学函数的RPQ语义研究相关。

Result: 结果显示一些属性是互相排斥的或者根本无法满足。同时，文中也介绍了几种新的RPQ语义作为例子。

Conclusion: 通过本研究提出的框架及新RPQ语义示例，可以为未来图数据库查询语言中新的语义设计提供参考。

Abstract: Modern property graph database query languages such as Cypher, PGQL, GSQL, and the standard GQL draw inspiration from the formalism of regular path queries (RPQs). In order to output walks explicitly, they depart from the classical and well-studied homomorphism semantics. However, it then becomes difficult to present results to users because RPQs may match infinitely many walks. The aforementioned languages use ad-hoc criteria to select a finite subset of those matches. For instance, Cypher uses trail semantics, discarding walks with repeated edges; PGQL and GSQL use shortest walk semantics, retaining only the walks of minimal length among all matched walks; and GQL allows users to choose from several semantics. Even though there is academic research on these semantics, it focuses almost exclusively on evaluation efficiency.
  In an attempt to better understand, choose and design RPQ semantics, we present a framework to categorize and compare them according to other criteria. We formalize several possible properties, pertaining to the study of RPQ semantics seen as mathematical functions mapping a database and a query to a finite set of walks. We show that some properties are mutually exclusive, or cannot be met. We also give several new RPQ semantics as examples. Some of them may provide ideas for the design of new semantics for future graph database query languages.

</details>


### [118] [DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning](https://arxiv.org/abs/2602.12064)
*Yafeng Nan,Haifeng Sun,Zirui Zhuang,Qi Qi,Guojun Chu,Jianxin Liao,Dan Pei,Jingyu Wang*

Main category: cs.DB

TL;DR: 提出了一种名为DIVER的新系统，该系统通过动态交互式值链接自动进行证据推理，从而在没有专家帮助的情况下实现稳健的Text-to-SQL转换。实验表明，DIVER显著提高了各种Text-to-SQL模型的鲁棒性，并且在处理大规模、动态数据库值时特别有效。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的Text-to-SQL模型虽然准确性高，但严重依赖于专家编写的证据来澄清模式和值链接，这在现实世界中缺乏专家支持的情况下会导致性能大幅下降。为了解决这个问题，提出了一个能够在无专家辅助下自动生成高质量证据并完成准确SQL生成的新方法。

Method: 开发了一个名为DIVER的系统，它利用一套兼容工具箱来探索数据库，并通过结构化的工作空间（CoTF，思维与事实链）基于探测结果反思并选择新工具进行新一轮探测。此过程迭代执行，以识别现有方法未能发现的模式和值链接。

Result: 广泛的实验显示，DIER系统能将不同Text-to-SQL模型的执行准确率提高最多10.82%，有效效率得分提升至16.09%；同时，在面对大规模及动态变化的数据库值时，其动态交互式值链接功能显著增强了现有系统的鲁棒性和链接准确性。

Conclusion: DIVER系统提供了一种无需专家干预即可增强Text-to-SQL任务鲁棒性的解决方案，特别是对于那些需要处理复杂或不断变化的数据集的应用而言尤为重要。

Abstract: In the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [119] [Real Life Is Uncertain. Consensus Should Be Too!](https://arxiv.org/abs/2602.11362)
*Reginald Frank,Soujanya Ponnapalli,Octavio Lomeli,Neil Giridharan,Marcos K Aguilera,Natacha Crooks*

Main category: cs.DC

TL;DR: 本文提出了一种概率性故障模型，用以替代传统的f-threshold故障模型，旨在通过利用单个机器的故障曲线来绕过多数法定交集等传统瓶颈，从而构建更可靠、高效、成本效益更高且可持续的系统。


<details>
  <summary>Details</summary>
Motivation: 作者认为当前共识协议所依赖的f-threshold故障模型过于简化了现实世界中的情况，并限制了优化成本或性能的机会。因此，他们提议采用一种能够捕捉实践中观察到的复杂而细微的故障特性的概率性故障模型。

Method: 提出使用概率性共识协议，这些协议可以明确地利用每个机器的具体'故障曲线'，并探索避免传统瓶颈如大多数法定交集的新方法。

Result: 该方法有望使基于此新型故障模型设计的分布式系统变得更加可靠、效率更高、更具成本效益和可持续性。

Conclusion: 转向概率性故障模型为开发下一代更加灵活、适应性强以及在成本与性能之间取得更好平衡的分布式系统提供了新的方向。

Abstract: Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.

</details>


### [120] [LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training](https://arxiv.org/abs/2602.11686)
*Xinyi Liu,Yujie Wang,Fangcheng Fu,Xuefeng Xiao,Huixia Li,Jiashi Li,Bin Cui*

Main category: cs.DC

TL;DR: 本文提出了一种新的MoE训练框架LAER-MoE，通过全切分专家并行（FSEP）和精细调度通信操作来优化负载均衡，并在A100集群上的实验显示相比现有最先进系统最高可达1.69倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前Mixture-of-Experts (MoE)模型的专家并行训练中存在显著的负载不平衡问题，部分过载专家成为训练瓶颈。

Method: 提出了LAER-MoE框架，该框架基于一种新颖的并行模式——全切分专家并行（Fully Sharded Expert Parallel, FSEP），实现对每个专家参数按设备数量进行完全划分，并通过All-to-All通信恢复部分专家。同时，进行了细粒度的通信操作调度以减少通信开销，并开发了负载平衡规划器来制定专家重新布局策略及令牌路由方案。

Result: 实验结果表明，在A100集群上，该系统与当前最先进的训练系统相比可达到最高1.69倍的速度提升。

Conclusion: LAER-MoE为解决MoE模型训练中的负载不平衡问题提供了有效方案，通过创新的并行机制和优化策略实现了显著的性能改进。

Abstract: Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.
  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.

</details>


### [121] [Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions](https://arxiv.org/abs/2602.11741)
*Bo Guan*

Main category: cs.DC

TL;DR: 本文提出了一种在生产环境中使用的分布式速率限制系统架构，使用Redis内存缓存数据库及其有序集数据结构来实现高效率和低延迟的键值对数据集操作，并保持精度。文章主要贡献在于量化了所选滚动窗口算法与令牌桶和固定窗口算法相比，在准确性和内存成本之间的权衡。此外，还介绍了服务器端Lua脚本如何通过将清理、计数和插入捆绑成单个原子操作来消除并发环境中的竞争条件，以及三层架构的设计，用于管理限流规则的存储和更新。


<details>
  <summary>Details</summary>
Motivation: 设计一个同时具备准确性、可用性和可扩展性的速率限制器是分布式系统中的一个基本挑战，主要是因为在算法精度、可用性、一致性和分区容错之间存在权衡。

Method: 采用Redis内存缓存数据库及其中的Sorted Set数据结构，利用其O(log(N))时间复杂度的操作提供高效且低延时的数据处理能力；通过服务端Lua脚本来确保清理、计数和插入等操作以原子形式执行，从而避免并发环境下的竞态条件；提出了一个三层架构用于管理和更新限流规则。

Result: 实现了能够满足生产级要求的分布式速率限制系统架构，该架构不仅支持高效的键值对数据处理还能够在保证精度的同时减少内存开销；通过Lua脚本有效解决了并发场景下的竞态问题；并且通过对CAP定理中AP（可用性和分区容忍性）的选择作为工程实践中的一种实际折衷方案。

Conclusion: 该研究成功地设计并实现了一个既精确又具有高度可用性和可扩展性的分布式速率限制系统，适用于生产环境。它通过选择合适的算法和工具有效地平衡了准确性与资源消耗之间的关系。

Abstract: Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.

</details>


### [122] [An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization](https://arxiv.org/abs/2602.11998)
*Ramakant kumar*

Main category: cs.DC

TL;DR: 本文提出了一种基于拍卖的机制AUC-RAC，用于在物联网设备环境中有效卸载多个本地服务器之间的计算任务。该方法利用Docker集群的概念，通过基于拍卖的竞价过程优化了多个系统间计算任务的分配，从而提高了物联网设备的任务卸载和计算密集型服务。


<details>
  <summary>Details</summary>
Motivation: 在基于云的容器化中为物联网执行任务时，资源管理和成本优化仍然具有挑战性。为了更高效地进行计算密集型任务的卸载与处理，需要一种新的解决方案来改善这一状况。

Method: 提出了AUC-RAC，一种基于拍卖机制的方法，以实现物联网设备上下文中多个本地服务器之间计算任务的有效卸载。该方案结合了Docker集群（由管理节点MN和工作节点WNs组成）以及Docker容器化技术，通过让各节点参与基于拍卖的竞标过程来优化任务分配。

Result: 实验分析表明，通过促进本地服务器间的协作，所提方法能够为物联网设备提供改进的任务卸载及计算密集型服务。

Conclusion: AUC-RAC作为一种新颖的方法，在提高物联网环境下计算任务卸载效率方面展现了潜力，特别是在资源充足性的考量下优化了任务在多系统中的分配。

Abstract: Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.

</details>


### [123] [Contention Resolution, With and Without a Global Clock](https://arxiv.org/abs/2602.12070)
*Zixi Cai,Kuowen Chen,Shengquan Du,Tsvi Kopelowitz,Seth Pettie,Ben Plosk*

Main category: cs.DC

TL;DR: The study introduces a novel Contention Resolution protocol and explores the complexity differences when assuming a global clock, revealing a fundamental trade-off between expected and high-probability latency measures.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to explore the impact of having a global clock on the Contention Resolution problem, which has traditionally been studied under the assumption that parties only have access to their local time since wake-up. The authors aim to show how the presence of a global clock can enrich the problem and lead to new algorithmic techniques.

Method: The researchers designed a new Contention Resolution protocol with a specific latency bound. They also analyzed the complexity gap between randomized protocols in GlobalClock and LocalClock settings, and compared the expected latency and high-probability latency for memoryless protocols.

Result: The primary results include a new Contention Resolution protocol with a guaranteed latency, a demonstrated complexity gap between different types of protocols, and a proof that it is impossible to create a protocol that is optimal under both In-Expectation and With-High-Probability metrics.

Conclusion: The paper concludes by demonstrating that it is impossible to design a Contention Resolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics, and even more, it is not possible to simultaneously achieve an In-Expectation latency of o(n log^2 n / (log log n)^2) and a With-High-Probability latency of n log^O(1) n.

Abstract: In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\left(\left(n\log\log n\log^{(3)} n\log^{(4)} n\cdots \log^{(\log^* n)} n\right)\cdot 2^{\log^* n}\right) \le n(\log\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $Θ(n \log n/\log\log n)$ whereas the With-High-Probability latency is $Θ(n\log^2 n/\log\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\log^2 n/(\log\log n)^2)$ and With-High-Probability latency $n\log^{O(1)} n$ simultaneously.

</details>


### [124] [OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration](https://arxiv.org/abs/2602.12151)
*Youhe Jiang,Fangcheng Fu,Taiyi Wang,Guoliang He,Eiko Yoneki*

Main category: cs.DC

TL;DR: OServe是一个针对大型语言模型服务系统，通过引入一种新的工作负载感知调度算法和有效的工作负载自适应切换方法来解决工作负载在空间和时间上的异质性问题。实验表明，与最先进的服务系统相比，OServe可以将性能提高至多2倍（平均1.5倍）。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型服务系统通常假设工作负载在空间上是均匀的，在时间上是稳定的，并采用同构静态模型部署。然而实际上，工作负载在空间和时间上都表现出显著的异质性，这导致了性能不理想。为了解决这一问题，提出了OServe系统。

Method: OServe提出了一种新的工作负载感知调度算法，该算法根据实时工作负载特性优化异构模型部署。此外，OServe还介绍了一种有效的工作负载自适应切换方法，能够根据预测的工作负载变化迁移模型部署。

Result: 基于真实追踪数据的实验显示，相较于当前最先进服务系统，OServe能够实现最高达2倍、平均1.5倍的性能提升。

Conclusion: 通过引入工作负载感知调度算法及自适应切换机制，OServe成功解决了大型语言模型服务过程中面临的时空异质性挑战，显著提升了系统性能。

Abstract: Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\times$ (average: 1.5$\times$) compared to state-of-the-art serving systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [125] [MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan](https://arxiv.org/abs/2602.11235)
*Xin Song,Zhilin Guan,Ruidong Han,Binghao Tang,Tianwen Chen,Bing Li,Zihao Li,Han Zhang,Fei Jiang,Chaolin Xie,Chi Ma,Chunyang Jiang,Chunzhen Jing,Dengxuan Li,Fengyi Li,Lei Yu,Mengyao Sun,Pu Wang,Qing Wang,Rui Fan,Shangyu Chen,Shifeng Du,Siyuan Bai,Wei Lin,Wentao Zhu,Zhou Han,Zhuo Chen,Zikang Xu*

Main category: cs.IR

TL;DR: 本文提出了一种基于Transformer的推荐系统框架MTFM，旨在解决现有跨域和多场景方法在资源需求和输入对齐上的局限性。通过将跨域数据转换为异构token、用户级样本聚合以及引入分组查询注意力机制等手段，在不依赖预对齐的情况下高效捕获多场景知识，并通过系统级优化进一步提高了训练和推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的跨域（CDR）和多场景（MSR）推荐系统方法往往需要大量的资源并要求严格的输入对齐，这限制了它们的可扩展性。为了克服这些挑战，文章提出了一个新框架。

Method: 1. MTFM使用一种无需预先对齐的方法，将跨域数据转化为异构token。
2. 采用多场景用户级样本聚合技术来减少实例总数，从而提高训练效率。
3. 引入Grouped-Query Attention及定制化的Hybrid Target Attention以降低内存消耗与计算复杂度。
4. 实施多项系统级优化措施，如内核融合和消除CPU-GPU阻塞，进一步提升训练与推理速度。

Result: 离线和在线实验均验证了MTFM的有效性，表明通过增加模型容量及利用更多的多场景训练数据能够显著提高性能表现。

Conclusion: MTFM提供了一种有效且高效的解决方案来处理工业推荐系统中的跨域和多场景问题，它不仅减少了资源需求还提高了处理效率，同时保持了良好的性能。

Abstract: Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data.

</details>


### [126] [From Noise to Order: Learning to Rank via Denoising Diffusion](https://arxiv.org/abs/2602.11453)
*Sajad Ebrahimi,Bhaskar Mitra,Negar Arabzadeh,Ye Yuan,Haolun Wu,Fattane Zarrinkalam,Ebrahim Bagheri*

Main category: cs.IR

TL;DR: 本文提出了一种基于去噪扩散的深度生成方法DiffusionRank，用于信息检索中的学习排序（LTR），该方法能够建模特征向量和相关性标签之间的完整联合分布。实验结果显示，与传统的判别式方法相比，DiffusionRank模型表现出显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统上，学习排序（LTR）方法在信息检索中主要依赖于判别式机器学习方法，这些方法仅能根据查询-文档对的某些特征表示来建模文档与查询的相关概率。作者假设，在生成式设置下能够解释整个数据分布的候选解决方案会产生更稳健的排序模型。

Method: 提出了DiffusionRank，这是一种扩展了TabDiff（一种现有的针对表格数据集的基于去噪扩散的生成模型）的方法，用以创建经典判别式逐点和成对LTR目标的生成等价物。

Result: 实证结果表明，DiffusionRank模型相对于其判别式对应版本显示出了显著提升。

Conclusion: 本研究指出，在利用深度生成模型方法（如扩散模型）进行信息检索的学习排序方面存在丰富的未来研究探索空间。

Abstract: In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.

</details>


### [127] [KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance](https://arxiv.org/abs/2602.11518)
*Yupeng Li,Ben Chen,Mingyue Cheng,Zhiding Liu,Xuxin Zhang,Chenyi Lei,Wenwu Ou*

Main category: cs.IR

TL;DR: 本文介绍了一个名为KuaiSearch的电商搜索数据集，该数据集基于快手平台真实用户搜索互动构建，覆盖冷启动用户和长尾商品，并系统地涵盖了召回、排序和相关性判断三个关键搜索阶段。


<details>
  <summary>Details</summary>
Motivation: 电子商务搜索在连接用户需求与大量商品库存方面起着重要作用，但面临着查询模糊、商品文本语义弱及用户偏好多样等挑战。虽然大型语言模型（LLMs）的进步为解决这些问题提供了新机会，但现有数据集存在许多局限性，如查询构造过于简化、排除了冷启动用户和长尾商品等问题，限制了基于LLM的电子商务搜索研究。

Method: 通过构建并发布KuaiSearch数据集来应对上述挑战，该数据集基于快手平台的真实用户搜索交互，保留了真实的用户查询和自然语言形式的商品文本，同时涵盖了搜索流程中的召回、排序以及相关性判断三个关键阶段。

Result: 对KuaiSearch进行了多角度综合分析，并在多个代表性搜索任务上建立了基准实验。实验结果表明，KuaiSearch为实际电子商务搜索研究提供了一个有价值的基石。

Conclusion: KuaiSearch作为目前可用的最大电子商务搜索数据集之一，解决了现有数据集的一些主要问题，为推动基于大型语言模型的电子商务搜索技术发展奠定了坚实的基础。

Abstract: E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.

</details>


### [128] [LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling](https://arxiv.org/abs/2602.11562)
*Tianhe Lin,Ziwei Xiong,Baoyuan Ou,Yingjie Qin,Lai Xu,Xiaocheng Zhong,Yao Hu,Zhiyong Wang,Tao Zhou,Yubin Xu,Di Wu*

Main category: cs.IR

TL;DR: LASER, developed by Xiaohongshu, optimizes both system and algorithmic efficiency to model ultra-long user behavior sequences for recommendation systems, overcoming high I/O latency and computational complexity. It employs SeqVault for efficient data retrieval and Segmented Target Attention (STA) with a Global Stacked Target Attention (GSTA) module for effective sequence compression, achieving notable performance improvements in both offline evaluations and large-scale online A/B testing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the 'Latency Wall' in real-time industrial environments when modeling ultra-long user behavior sequences, which is crucial for capturing evolving and lifelong user interests in recommendation systems. The primary challenges are the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms.

Method: The method involves a full-stack optimization framework called LASER, featuring two main innovations: 1) System Efficiency - SeqVault, a unified schema-aware serving infrastructure that uses a hybrid DRAM-SSD indexing strategy to reduce retrieval latency and CPU usage; 2) Algorithmic Efficiency - Segmented Target Attention (STA) mechanism, which utilizes a sigmoid-based gating strategy to filter out irrelevant items, followed by a lightweight Global Stacked Target Attention (GSTA) module to refine these segments and capture cross-segment dependencies without high computational costs.

Result: Extensive offline evaluations show that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.

Conclusion: LASER, through its innovative approach to both system and algorithmic efficiency, successfully addresses the key challenges in deploying models for ultra-long user behavior sequences in real-time industrial settings, leading to improved performance and substantial business benefits.

Abstract: Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict "Latency Wall", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.

</details>


### [129] [Analytical Search](https://arxiv.org/abs/2602.11581)
*Yiteng Tu,Shuo Miao,Weihang Su,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 本文提出了分析搜索作为一种新兴的搜索范式，旨在满足多样化的分析信息需求。通过构建一个集成理解查询、召回导向检索、融合时考虑推理以及自适应验证的统一系统框架，分析搜索能够更好地支持具有高度问责要求的分析查询任务。


<details>
  <summary>Details</summary>
Motivation: 现有的信息检索范式难以满足大规模语料库中端到端解决分析性问题的需求，如趋势分析和因果影响评估等跨领域任务。它们要么侧重于查找信息而非解决问题，要么将所有内容视为简单的问答处理，缺乏对推理过程、证据使用及可验证性的充分控制。因此，需要一种新的搜索方式来支持这些有多种用途概念且高问责要求的分析查询。

Method: 提出了一种称为“分析搜索”的新搜索范式，它将搜索重新定义为以证据为基础、面向过程的分析工作流，并明确建模分析意图、收集证据进行融合，以及通过结构化多步骤推断得出可验证结论。此外，还展示了一个统一的系统框架，该框架集成了查询理解、召回导向检索、注重推理的融合与自适应验证等功能。

Result: 文章成功地定位了分析搜索相对于现有范式的独特之处，并提出了一个有助于开发下一代搜索引擎的综合系统架构，这些引擎能够更有效地支持用户的分析信息需求。

Conclusion: 强调了分析搜索在概念上的重要性和实践价值，并呼吁进一步研究努力，以推动支持分析信息需求的新一代搜索引擎的发展。

Abstract: Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.
  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.

</details>


### [130] [Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation](https://arxiv.org/abs/2602.11605)
*Yixiao Chen,Yuan Wang,Yue Liu,Qiyao Wang,Ke Cheng,Xin Xu,Juntong Yan,Shuojin Yang,Menghao Guo,Jun Zhang,Huan Yu,Jie Jiang*

Main category: cs.IR

TL;DR: Rec2PM框架通过将长用户交互历史压缩为紧凑的偏好记忆令牌来解决生成式推荐模型在处理长期序列时遇到的计算成本高和随机交互噪声累积的问题。它采用自参照教师强制策略，支持并行化训练和迭代更新，并且以令牌嵌入形式表示记忆提高了存储效率。实验表明，相比全序列模型，Rec2PM减少了推理延迟和内存占用，同时保持了较高的准确性。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型（GenRec）通常通过全注意力机制建模用户行为，但当扩展到终身序列时会受到过高的计算成本以及来自随机交互的噪音积累的影响。为了克服这些挑战，研究者们开发了新的方法。

Method: 提出了Rec2PM框架，该框架能够将长时间的用户交互记录压缩成简短的偏好记忆令牌。与传统的递归方法不同，Rec2PM使用了一种新颖的自参照教师强迫策略：利用历史的整体视图生成参考记忆，作为并行递归更新的监督目标。这使得可以在保持推理过程中迭代更新能力的同时进行完全并行训练。此外，通过将记忆表示为令牌嵌入而不是大量的KV缓存，Rec2PM实现了极高的存储效率。

Result: 大规模基准测试上的实验表明，与全序列模型相比，Rec2PM显著降低了推理延迟和内存占用量，同时达到了更高的准确性。分析还显示，偏好记忆功能作为一个去噪信息瓶颈，能够有效地过滤掉交互中的噪音，捕捉到稳定的长期兴趣。

Conclusion: Rec2PM提供了一个有效的解决方案，用于处理生成式推荐系统中长期用户序列带来的挑战，包括计算效率低下和数据噪声问题。通过引入偏好记忆令牌及自参照学习机制，不仅提高了模型训练与推断的速度和效率，也增强了模型对用户长期兴趣的理解准确度。

Abstract: Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.

</details>


### [131] [Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts](https://arxiv.org/abs/2602.11622)
*Haiyang Jiang,Tong Chen,Xinyi Gao,Guansong Pang,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 提出了一种新的MoE框架EvoFG，通过进化特征生成方案和记忆增强路由器来解决零样本图异常检测中的路由挑战，实验表明该方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的单个GNN方法不足以表达多样化的异常机制，而混合专家架构（MoE）虽然提供了整合不同GNN专家的可能性，但其在零样本图异常检测中的有效性受到分布偏移的严重限制，导致关键的路由问题。

Method: 设计了一个名为EvoFG的新MoE框架，包括一个基于LLM的生成器和Shapley值引导评估的进化特征生成方案，以及一个具有不变学习目标的记忆增强路由器，以应对跨图节点语义差异大和异常图分布不一致的问题。

Result: 在六个基准测试上进行了广泛的实验，结果显示EvoFG持续优于最新的基线方法，在零样本图异常检测中表现出强大且稳定的性能。

Conclusion: 本研究提出的EvoFG框架有效地解决了零样本图异常检测中的两个关键路由难题，并通过实验证明了其相对于当前最优方法的优势。

Abstract: Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.

</details>


### [132] [IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation](https://arxiv.org/abs/2602.11664)
*Huimin Yan,Longfei Xu,Junjie Sun,Zheng Liu,Wei Luo,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: This paper introduces IntTravel, a large-scale public dataset for integrated travel recommendation, and proposes an end-to-end, decoder-only generative framework to improve multi-task recommendation. The solution has been deployed on Amap, significantly increasing the CTR.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current research in next POI recommendation, which only focuses on 'where to go' but neglects other important aspects such as departure time, travel mode, and situational requirements along the journey. Additionally, the limited scale of existing datasets hinders accurate performance evaluation.

Method: The method involves creating IntTravel, a large-scale public dataset that includes 4.1 billion interactions from 163 million users with 7.3 million POIs. On top of this, an end-to-end, decoder-only generative framework for multi-task recommendation is introduced, incorporating information preservation, selection, and factorization to balance task collaboration with specialized differentiation.

Result: The results show substantial performance gains from the proposed framework, with state-of-the-art performance across both the IntTravel dataset and an additional non-travel benchmark. When deployed on Amap, it led to a 1.09% increase in CTR.

Conclusion: The conclusion is that by introducing a comprehensive dataset and a generative framework, the research effectively addresses the gaps in next POI recommendation, leading to improved user experience and higher CTR when implemented in real-world applications.

Abstract: Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: "when to depart", "how to travel", "where to go", and "what needs arise via the route". However, current research is limited by fragmented datasets that focus merely on next POI recommendation ("where to go"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.

</details>


### [133] [EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation](https://arxiv.org/abs/2602.11680)
*Yihang Li,Zhuo Liu,Wei Wei*

Main category: cs.IR

TL;DR: 本文提出了一种名为EpicCBR的多视图对比学习框架，用于冷启动捆绑推荐。该框架通过精确挖掘和利用项目关系来构建用户画像，并通过历史捆绑信息和用户偏好来表征新捆绑的特点。实验表明，EpicCBR在冷启动场景下显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的捆绑推荐模型主要依赖于观察到的用户-捆绑互动，这限制了对不断出现的新捆绑的探索。当前方法通常将每个捆绑视为独立实例，而忽视了充分利用热门项目上的用户-项目(UI)和捆绑-项目(BI)关系。

Method: 提出了一个名为EpicCBR的多视图对比学习框架，旨在解决冷启动捆绑推荐问题。该框架精准地挖掘并利用商品间的关系以构建用户画像，同时引入了一种基于受欢迎程度的方法，通过历史捆绑信息及用户偏好来刻画新捆绑的特征。此外，还引入了一个能够整合多种不同情境的多视图图对比学习框架，确保模型的泛化能力。

Result: 在三个流行的基准测试上进行了广泛的实验，结果表明EpicCBR相比最先进方法有显著的优势（最高可达387%），尤其是在冷启动场景中表现出了优越性。

Conclusion: EpicCBR作为一种新的冷启动捆绑推荐方法，在处理新兴捆绑推荐方面展现了巨大的潜力，为未来研究提供了新的方向。

Abstract: Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.

</details>


### [134] [Uncertainty-aware Generative Recommendation](https://arxiv.org/abs/2602.11719)
*Chenxiao Fan,Chongming Gao,Yaxin Gong,Haoyan Liu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: 本文提出了一种新的推荐方法——不确定性感知生成推荐(UGR)，该方法通过引入不确定性的概念来优化推荐过程，从而提高推荐性能并稳定训练。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化方法通常依赖于二元结果的正确性，忽视了模型内在的生成信心、样本学习难度的变化以及缺乏明确的信心表达，导致训练动态不稳定和决策风险无法量化。

Method: 提出了不确定性感知生成推荐(UGR)框架，该框架结合了三个机制：基于不确定性的加权奖励以惩罚自信错误；难度感知优化动态防止过早收敛；以及显式信心对齐使模型具备信心表达能力。

Result: 大量实验表明，UGR不仅在推荐性能上优于传统方法，而且从根本上稳定了训练过程，避免了标准方法中常见的性能下降问题。此外，所学得的信心还能支持可靠的下游风险感知应用。

Conclusion: 通过利用不确定性作为自适应优化的关键信号，UGR提供了一个统一的框架来改善生成式推荐系统的稳定性与效能。

Abstract: Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.

</details>


### [135] [ULTRA:Urdu Language Transformer-based Recommendation Architecture](https://arxiv.org/abs/2602.11836)
*Alishbah Bashir,Fatima Qaiser,Ijaz Hussain*

Main category: cs.IR

TL;DR: 本文提出了一种名为ULTRA的语义推荐框架，旨在解决乌尔都语这种低资源语言在个性化新闻检索中的内容推荐问题。通过采用双嵌入架构和基于查询长度的路由机制，该系统能够根据用户查询类型动态选择合适的语义处理管道，从而提高推荐的相关性和适应性。实验结果表明，与单一管道基线相比，该方法在不同类型的查询上都能显著提高推荐准确性，精度提高了90%以上。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为一种低资源语言，在个性化的新闻检索领域缺乏有效的语义内容推荐系统。现有的方法主要依赖于词汇匹配或与语言无关的技术，这些技术难以捕捉语义意图，并且在应对不同长度的查询和信息需求时表现不佳。

Method: 提出了ULTRA（乌尔都语语言转换器基础推荐架构），这是一种自适应语义推荐框架。它引入了具有查询长度感知路由机制的双嵌入架构，能够区分短小、意图明确的查询和较长、富含上下文信息的查询。基于一个阈值驱动的决策过程，用户查询被导向专门优化过的语义处理流程，以适应标题/头条级别或全文/文档级别的表示。

Result: 通过对大规模乌尔都语文本库进行广泛实验，证明所提出的架构能够在多种查询类型下持续提升推荐相关性。结果显示，与单一处理流程基准相比，推荐准确度提高了超过90%。

Conclusion: 研究发现确立了ULTRA作为低资源语言环境下一种稳健且可泛化的文本推荐架构的地位，为这类环境下的语义检索系统设计提供了实用见解。

Abstract: Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.

</details>


### [136] [Improving Neural Retrieval with Attribution-Guided Query Rewriting](https://arxiv.org/abs/2602.11841)
*Moncef Garouani,Josiane Mothe*

Main category: cs.IR

TL;DR: 提出了一种基于属性指导的查询重写方法，通过计算检索器中的基于梯度的标记属性来指导大型语言模型（LLM）进行查询重写，以澄清含糊或误导性的查询部分同时保持原意。在BEIR集合上的评估表明，这种方法能够显著提高检索效果，特别是在处理隐含或模糊信息需求时。


<details>
  <summary>Details</summary>
Motivation: 现有的神经检索器虽然有效但容易受到不明确或模棱两可查询的影响，导致即使存在相关文档也会发生错误排名。而当前解决此脆弱性的方法要么是让大型语言模型独立地重写查询而不考虑检索反馈，要么是使用解释性方法识别误导性词语但仅用于事后分析。

Method: 本文提出的方法首先针对每个查询从检索器中计算出基于梯度的词级属性分数；然后将这些分数作为软指引，在一个结构化的提示中引导大型语言模型对查询中较弱或具有误导性的部分进行澄清，同时保持原始意图不变。

Result: 实验结果表明，所提出的重写方法相比强大的基线模型在检索效果上取得了持续改进，尤其是在处理隐含或模糊的信息需求时获得了更大的收益。

Conclusion: 通过结合检索器反馈和大型语言模型的能力，该研究成功开发了一种新的查询重写策略，能够有效地解决因查询表述不清而导致的检索问题，并在多个数据集上验证了其有效性。

Abstract: Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.

</details>


### [137] [IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval](https://arxiv.org/abs/2602.11941)
*Benjamin Clavié,Atoof Shakir,Jonah Turner,Sean Lee,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 本文介绍了IncompeBench，一个用于评估音乐检索性能的新基准。它包含1,574个高质量音乐片段、500个多样化查询和超过125,000个单独的相关性判断，并通过多阶段流程创建了这些注释以确保高一致性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量的基准来评估音乐检索性能。

Method: 通过一个多阶段管道创建注释，开发了一个名为IncompeBench的新基准，该基准包括大量许可授权的高质量音乐片段、多样化的查询以及大量的个体相关性判断。

Result: 创建了一个新的公开可用的数据集IncompeBench，专门用于更准确地评价音乐信息检索的表现。

Conclusion: IncompeBench为音乐信息检索领域提供了一个强有力的评估工具，有助于推动该领域的进一步发展。

Abstract: Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.

</details>


### [138] [Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems](https://arxiv.org/abs/2602.12041)
*Heng Yu,Xiangjun Zhou,Jie Xia,Heng Zhao,Anxin Wu,Yu Zhao,Dongying Kong*

Main category: cs.IR

TL;DR: 本文提出了一种名为MLCC的特征交互架构，通过层次压缩和动态组合来组织特征交叉，能够有效捕捉高阶特征依赖性同时保持良好的计算复杂度。此外，还介绍了MC-MLCC，一种多通道扩展方法，将特征交互分解为并行子空间，使得在提高表达能力的同时显著减少了参数增长。实验表明所提出的模型相比强基线DLRM风格模型，在AUC上最高可提升0.52，同时减少高达26倍的模型参数与FLOPs。在线A/B测试进一步验证了该方法在实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代工业推荐系统主要基于深度学习推荐模型构建，其中交互骨干对于预测性能和系统效率至关重要。然而，现有的交互模块往往难以同时实现强大的交互能力、高效的计算效率以及良好的可扩展性，导致在严格的生产限制下扩大模型规模时投资回报率有限。

Method: 提出了MLCC，一种结构化的特征交互架构，通过层次压缩和动态组成来组织特征交叉；引入了MC-MLCC，一个多重通道扩展版本，它将特征交互分解成并行子空间，允许有效地水平扩展，并改善了表示容量，同时极大地减少了参数增长。

Result: 在三个公开基准测试和一个大规模工业数据集上的广泛实验显示，所提模型相对于强大的DLRM风格基线模型最多可以提高0.52 AUC，同时在类似性能下最多减少26倍的模型参数和FLOPs。全面的扩展分析展示了在嵌入维度、头部数量及通道数方面的稳定且可预测的扩展行为，基于通道的扩展比传统的嵌入膨胀实现了显著更好的效率。最后，在现实世界广告平台上的在线A/B测试验证了我们方法的实际效果。

Conclusion: 本研究开发的新模型（MLCC及其多通道版本MC-MLCC）不仅提高了点击率和转化率预测任务的表现，而且还在保证性能的同时大幅度降低了模型大小和计算需求，使其更适合于资源受限环境下的部署。

Abstract: Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.

</details>


### [139] [SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization](https://arxiv.org/abs/2602.12187)
*Sunghwan Kim,Wooseok Jeong,Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.IR

TL;DR: A new environment, SAGEO Arena, is introduced for the evaluation and optimization of Search-Augmented Generative Engine Optimization (SAGEO), addressing the limitations of current benchmarks by incorporating a full generative search pipeline over a large-scale, structurally rich web document corpus. The findings highlight the impracticality of existing approaches under realistic conditions and the importance of structural information in improving performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of a comprehensive evaluation environment for Search-Augmented Generative Engine Optimization (SAGEO) that can provide end-to-end visibility into optimization strategies, as well as the absence of consideration for the structural information of web documents in existing benchmarks, which are critical for the practical performance of SAGE systems.

Method: The method involves the creation of SAGEO Arena, an environment that integrates a complete generative search pipeline including retrieval, reranking, and generation stages, using a large-scale corpus of web documents with rich structural details. This setup allows for the joint targeting of SEO and GEO, enabling a more realistic assessment of SAGEO techniques.

Result: Results show that without proper adaptation, current SAGEO methods do not perform well in a full pipeline context and often have negative impacts on retrieval and reranking. The inclusion of structural information, such as schema markup, is found to be beneficial in enhancing the effectiveness of SAGEO, suggesting the need for stage-specific optimizations.

Conclusion: SAGEO Arena provides a much-needed platform for evaluating and optimizing SAGEO in a setting that closely mirrors real-world conditions, revealing the necessity of considering both the generative and retrieval aspects of SAGE systems, and the value of utilizing structural elements within web documents.

Abstract: Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.

</details>


### [140] [AttentionRetriever: Attention Layers are Secretly Long Document Retrievers](https://arxiv.org/abs/2602.12278)
*David Jiahao Fu,Lam Thanh Do,Jiayu Li,Kevin Chen-Chuan Chang*

Main category: cs.IR

TL;DR: 本文提出了一种新的长文档检索模型AttentionRetriever，该模型利用注意力机制和基于实体的检索来构建上下文感知的嵌入，并确定检索范围。实验表明，AttentionRetriever在长文档检索数据集上显著优于现有检索模型，同时保持了与密集检索模型相同的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）技术虽然被广泛用于帮助大型语言模型处理涉及长文档的任务，但这些检索模型并未专门针对长文档检索设计，未能很好地解决诸如上下文感知、因果依赖性以及检索范围等关键挑战。

Method: 提出了AttentionRetriever，这是一种新颖的长文档检索模型，它结合了注意力机制和基于实体的检索方法，以创建能够理解上下文的文档嵌入，并明确检索时需要覆盖的范围。

Result: 通过广泛的实验验证，AttentionRetriever不仅在多个长文档检索数据集上的表现明显优于当前其他检索模型，而且其运行效率也达到了与密集型检索模型相当的水平。

Conclusion: 研究表明，采用AttentionRetriever可以有效改善长文档检索任务的表现，同时维持高效运作，为处理长文档提供了更优的选择。

Abstract: Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.

</details>
