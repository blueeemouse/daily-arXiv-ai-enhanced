<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 63]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Monte Carlo Tree Search with Reasoning Path Refinement for Small Language Models in Conversational Text-to-NoSQL](https://arxiv.org/abs/2602.12574)
*Xubang Xiong,Raymond Chi-Wing Wong,Yuanfeng Song*

Main category: cs.DB

TL;DR: 本文提出了一个名为Stage-MCTS的框架，用于解决对话式文本到NoSQL的任务，通过将查询生成视为搜索问题来增强小型语言模型（SLM）的NoSQL特定推理能力。此外，还构建了一个跨领域的数据集CoNoSQL，包含超过2000个对话和150个数据库。实验显示该方法相比现有最先进模型在执行值匹配准确率上提高了高达7.93%。


<details>
  <summary>Details</summary>
Motivation: 现有的针对NoSQL数据库查询的研究主要集中在单轮交互上，忽略了现实世界中查询的对话性质。为了填补这一空白，研究者引入了对话式文本到NoSQL任务，旨在基于自然语言问题、NoSQL数据库以及对话历史自动生成NoSQL查询，从而降低用户使用难度。

Method: 提出了一种称为Stage-MCTS的新框架，它利用蒙特卡洛树搜索(MCTS)结合规则基础奖励产生逐步推理数据，并通过渐进式监督微调(SFT)与自我训练策略加强小型语言模型(SLMs)处理NoSQL相关推理的能力。

Result: 实验结果表明，所提方法相较于当前最先进的大型推理模型，在执行值匹配(EVM)准确性方面提升了最多7.93%。

Conclusion: 本研究表明，通过采用Stage-MCTS框架及相应策略，可以有效提高从自然语言转换为NoSQL查询的准确性，特别是对于涉及多轮对话场景的情况。

Abstract: NoSQL databases have been widely adopted in big data analytics, geospatial applications, and healthcare services, due to their flexibility and scalability. However, querying NoSQL databases requires specialized technical expertise, creating a high barrier for users. While recent studies have explored text-to-NoSQL problem, they primarily focus on single-turn interactions, ignoring the conversational nature of real-world queries. To bridge this gap, we introduce the Conversational Text-to-NoSQL task, which generates NoSQL queries given a natural language question, a NoSQL database, and the dialogue history. To address this task, we propose Stage-MCTS, a framework that endows small language models (SLMs) with NoSQL-specific reasoning capabilities by formulating query generation as a search problem. The framework employs Monte Carlo Tree Search (MCTS) guided by a rule-based reward to produce stepwise reasoning data, followed by progressive supervised fine-tuning (SFT) and self-training strategies. We further construct CoNoSQL, a cross-domain dataset with over 2,000 dialogues and 150 databases, to support evaluation. Experiments demonstrate that our approach outperforms state-of-the-art large reasoning models, improving execution value match (EVM) accuracy by up to 7.93%.

</details>


### [2] [Implicit Representation of Structural Constraints in ER-to-Relational Transformation: An Analysis of Cardinality Preservation](https://arxiv.org/abs/2602.12856)
*Dhammika Pieris*

Main category: cs.DB

TL;DR: 研究了概念模式中的结构约束在转换为逻辑模式后被表示的程度，发现一对一和一对多关系的最小参与约束以及最大参与约束在多数情况下无法被明确捕捉，而多对多关系仅能表明最大基数超过一，但不能保持确切值。


<details>
  <summary>Details</summary>
Motivation: 探讨实体-关系（ER）模型转换为关系数据库模式（RDS）时，概念模式中指定的结构约束是否能在逻辑模式中准确表达。

Method: 使用包含二元关系类型的实体-关系模型，并将其转化为关系数据库模式，通过经典转换框架下定义的主键(PK)和外键(FK)约束来分析转换结果。采用具有可变结构约束值的一般化ER模型来评估最终的关系数据库模式结构。

Result: 对于一对一和一对多关系类型，关系数据库模式不能明确地捕捉到最小参与约束，并且除了有限情况外，也无法编码精确的最大参与限制；对于多对多关系类型，模式只表明最大基数大于1，但无法保存具体的数值。

Conclusion: 标准的ER到关系型转换存在一定的表现限制，这对模式设计和约束执行有重要影响。

Abstract: This study examines the extent to which structural constraints specified in conceptual schemas are represented after transformation to logical schemas. Focusing on the conceptual-to-logical mapping, an Entity-Relationship (ER) model containing binary relationship types is transformed into a Relational Database Schema (RDS). The analysis is conducted under the classical transformation framework in which the logical schema is defined solely by primary key (PK) and foreign key (FK) constraints. Using generalised ER models with variable structural constraint values, the resulting RDS structures are evaluated to determine whether minimum and maximum participation constraints are represented unambiguously. The findings show that, for one-to-one and one-to-many relationships, RDSs do not unambiguously capture minimum participation constraints and do not encode exact maximum participation beyond limited cases. For many-to-many relationships, the schema indicates only that maximum cardinalities exceed one, without preserving exact values. These results clarify the representational limits of standard ER-to-relational transformations and have implications for schema design and constraint enforcement.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [3] [An Industrial-Scale Sequential Recommender for LinkedIn Feed Ranking](https://arxiv.org/abs/2602.12354)
*Lars Hertel,Gaurav Srivastava,Syed Ali Naqvi,Satyam Kumar,Yue Zhang,Borja Ocejo,Benjamin Zelditch,Adrian Englhardt,Hailing Cheng,Andy Hu,Antonio Alonso,Daming Li,Siddharth Dangi,Chen Zhu,Mingzhou Zhou,Wanning Li,Tao Huang,Fedor Borisyuk,Ganesh Parameswaran,Birjodh Singh Tiwana,Sriram Sankar,Qing Lan,Julie Choi,Souvik Ghosh*

Main category: cs.IR

TL;DR: 介绍了LinkedIn Feed的新推荐系统Feed-SR，基于Transformer的序列排名模型，相比之前的模型提高了用户参与度，并且在在线A/B测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了提高LinkedIn Feed的相关内容发现、建立联系和知识分享的能力，同时满足严格的生产环境要求，开发了新的推荐系统Feed-SR来替代现有的DCNv2为基础的排序器。

Method: 采用基于Transformer的序列排名模型作为核心架构，结合特定的建模选择、训练技术和服务优化措施，以适应LinkedIn的大规模部署需求。

Result: 与现有生产模型相比，在线A/B测试显示Feed-SR显著提升了成员参与度（时间花费增加2.10%）。此外，相对于其他序列及大语言模型基础的排名架构，Feed-SR提供了最佳的在线指标与生产效率组合。

Conclusion: Feed-SR成功地作为LinkedIn Feed的主要体验上线，不仅提高了用户互动率还展示了优秀的实际应用效果。

Abstract: LinkedIn Feed enables professionals worldwide to discover relevant content, build connections, and share knowledge at scale. We present Feed Sequential Recommender (Feed-SR), a transformer-based sequential ranking model for LinkedIn Feed that replaces a DCNv2-based ranker and meets strict production constraints. We detail the modeling choices, training techniques, and serving optimizations that enable deployment at LinkedIn scale. Feed-SR is currently the primary member experience on LinkedIn's Feed and shows significant improvements in member engagement (+2.10% time spent) in online A/B tests compared to the existing production model. We also describe our deployment experience with alternative sequential and LLM-based ranking architectures and why Feed-SR provided the best combination of online metrics and production efficiency.

</details>


### [4] [Visual RAG Toolkit: Scaling Multi-Vector Visual Retrieval with Training-Free Pooling and Multi-Stage Search](https://arxiv.org/abs/2602.12510)
*Ara Yeroyan*

Main category: cs.IR

TL;DR: 本文介绍了Visual RAG Toolkit，一个用于扩展多向量视觉检索的实用系统，通过无需训练的模型感知池化和多阶段检索来实现。该方法减少了每页存储的向量数量，从而大幅度减少向量间比较次数，并且在保持检索质量的同时显著提高了处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的多向量视觉检索器虽然准确度高，但随着每页产生数千个向量，其索引和搜索成本变得非常高。这促使作者开发一种新的解决方案以提高效率而不牺牲太多准确性。

Method: 受Matryoshka Embeddings启发，提出了一种静态空间池化技术，包括轻量级滑动窗口平均变体，用于从patch embeddings生成紧凑的tile级别和全局表示，加速候选生成过程。之后利用全多向量嵌入进行精确的最大相似度重排。

Result: 实验表明，在有限的ViDoRe v2基准数据集上，两阶段检索通常可以保持NDCG和Recall@5/10指标几乎没有下降，同时大幅提升了吞吐量（约4倍QPS）。此外，工具包还提供了强大的预处理功能以及可重复的评估流程。

Conclusion: 通过强调在常用截止点处的效率（例如k<=10），Visual RAG Toolkit降低了硬件门槛，使最先进的视觉检索技术变得更加实用可行。

Abstract: Multi-vector visual retrievers (e.g., ColPali-style late interaction models) deliver strong accuracy, but scale poorly because each page yields thousands of vectors, making indexing and search increasingly expensive. We present Visual RAG Toolkit, a practical system for scaling visual multi-vector retrieval with training-free, model-aware pooling and multi-stage retrieval. Motivated by Matryoshka Embeddings, our method performs static spatial pooling - including a lightweight sliding-window averaging variant - over patch embeddings to produce compact tile-level and global representations for fast candidate generation, followed by exact MaxSim reranking using full multi-vector embeddings.
  Our design yields a quadratic reduction in vector-to-vector comparisons by reducing stored vectors per page from thousands to dozens, notably without requiring post-training, adapters, or distillation. Across experiments with interaction-style models such as ColPali and ColSmol-500M, we observe that over the limited ViDoRe v2 benchmark corpus 2-stage retrieval typically preserves NDCG and Recall @ 5/10 with minimal degradation, while substantially improving throughput (approximately 4x QPS); with sensitivity mainly at very large k. The toolkit additionally provides robust preprocessing - high resolution PDF to image conversion, optional margin/empty-region cropping and token hygiene (indexing only visual tokens) - and a reproducible evaluation pipeline, enabling rapid exploration of two-, three-, and cascaded retrieval variants. By emphasizing efficiency at common cutoffs (e.g., k <= 10), the toolkit lowers hardware barriers and makes state-of-the-art visual retrieval more accessible in practice.

</details>


### [5] [DiffuRank: Effective Document Reranking with Diffusion Language Models](https://arxiv.org/abs/2602.12528)
*Qi Liu,Kun Ai,Jiaxin Mao,Yanzhao Zhang,Mingxin Li,Dingkun Long,Pengjun Xie,Fengbin Zhu,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 该论文提出了一种基于扩散语言模型（dLLMs）的文档重排序框架DiffuRank，通过三种不同的重排序策略探索了dLLMs在效率和灵活性方面的优势。实验表明，dLLMs在零样本和微调场景下均能与类似规模的自回归语言模型相媲美甚至超越。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLMs）的文档重排序方法主要依赖于自回归生成，这限制了它们的效率和灵活性。本文旨在通过引入扩散语言模型（dLLMs），克服现有方法存在的解码延迟高、错误传播以及难以修正等问题。

Method: 作者提出了DiffuRank框架，利用dLLMs进行文档重排序，并研究了三种策略：1) 点式方法，使用dLLMs估计每个查询-文档对的相关性；2) 基于logit的列表式方法，促使dLLMs同时评估多个文档的相关性并直接从模型logits中得出排名列表；3) 基于排列的列表式方法，调整dLLMs的标准解码过程以适应重排序任务。

Result: 实验结果表明，在多个基准测试上，无论是零样本还是经过微调后，dLLMs的表现都能与相似规模的自回归LLMs相匹敌，甚至在某些情况下超过了后者。

Conclusion: 研究表明，扩散语言模型为文档重排序提供了一个有前景且具有竞争力的选择，相较于传统的自回归架构，它展现了更高的效率和灵活性。

Abstract: Recent advances in large language models (LLMs) have inspired new paradigms for document reranking. While this paradigm better exploits the reasoning and contextual understanding capabilities of LLMs, most existing LLM-based rerankers rely on autoregressive generation, which limits their efficiency and flexibility. In particular, token-by-token decoding incurs high latency, while the fixed left-to-right generation order causes early prediction errors to propagate and is difficult to revise. To address these limitations, we explore the use of diffusion language models (dLLMs) for document reranking and propose DiffuRank, a reranking framework built upon dLLMs. Unlike autoregressive models, dLLMs support more flexible decoding and generation processes that are not constrained to a left-to-right order, and enable parallel decoding, which may lead to improved efficiency and controllability. Specifically, we investigate three reranking strategies based on dLLMs: (1) a pointwise approach that uses dLLMs to estimate the relevance of each query-document pair; (2) a logit-based listwise approach that prompts dLLMs to jointly assess the relevance of multiple documents and derives ranking lists directly from model logits; and (3) a permutation-based listwise approach that adapts the canonical decoding process of dLLMs to the reranking tasks. For each approach, we design corresponding training methods to fully exploit the advantages of dLLMs. We evaluate both zero-shot and fine-tuned reranking performance on multiple benchmarks. Experimental results show that dLLMs achieve performance comparable to, and in some cases exceeding, that of autoregressive LLMs with similar model sizes. These findings demonstrate the promise of diffusion-based language models as a compelling alternative to autoregressive architectures for document reranking.

</details>


### [6] [Reasoning to Rank: An End-to-End Solution for Exploiting Large Language Models for Recommendation](https://arxiv.org/abs/2602.12530)
*Kehan Zheng,Deyao Hong,Qian Li,Jun Zhang,Huan Yu,Jie Jiang,Hongning Wang*

Main category: cs.IR

TL;DR: 本文提出了一种名为"推理到排名"的端到端训练框架，该框架将推荐效用优化内化为大型语言模型中的逐步推理学习过程。通过用户-项目级别的推理以及强化学习来训练大型语言模型，实验结果显示了相对于传统方法和基于LLM的解决方案的一致性改进。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统需要超越基于模式评分的深度推理以适应用户不断变化的偏好并提供符合其意图的排序结果。尽管最近开始尝试利用大型语言模型（LLMs）进行推荐，但如何有效地优化这些模型以提高推荐质量仍是一个未充分探索的问题。

Method: 作者们提出了一个称为“Reasoning to Rank”的端到端训练框架，它在大型语言模型中实现推荐效用优化与逐步推理学习相结合。为了克服LLM推理时的位置偏差问题，并直接优化推理过程，本框架采取了用户-项目级别的推理方式，并使用了强化学习技术来进行整个模型的端到端训练。

Result: 通过对三个亚马逊数据集及一个大规模工业数据集进行测试，结果表明提出的方案相较于现有的强大常规方法和基于LLM的方法，在性能上取得了持续性的提升。

Conclusion: 广泛深入的分析验证了所提框架关键组成部分的重要性，并为未来在此领域的工作发展提供了启示。

Abstract: Recommender systems are tasked to infer users' evolving preferences and rank items aligned with their intents, which calls for in-depth reasoning beyond pattern-based scoring. Recent efforts start to leverage large language models (LLMs) for recommendation, but how to effectively optimize the model for improved recommendation utility is still under explored. In this work, we propose Reasoning to Rank, an end-to-end training framework that internalizes recommendation utility optimization into the learning of step-by-step reasoning in LLMs. To avoid position bias in LLM reasoning and enable direct optimization of the reasoning process, our framework performs reasoning at the user-item level and employs reinforcement learning for end-to-end training of the LLM. Experiments on three Amazon datasets and a large-scale industrial dataset showed consistent gains over strong conventional and LLM-based solutions. Extensive in-depth analyses validate the necessity of the key components in the proposed framework and shed lights on the future developments of this line of work.

</details>


### [7] [CAPTS: Channel-Aware, Preference-Aligned Trigger Selection for Multi-Channel Item-to-Item Retrieval](https://arxiv.org/abs/2602.12564)
*Xiaoyou Zhou,Yuqi Liu,Zhao Liu,Xiao Lv,Bo Chen,Ruiming Tang,Guorui Zhou*

Main category: cs.IR

TL;DR: 本文提出了一种名为CAPTS的框架，旨在解决多渠道触发选择中的价值归属偏差和渠道间协调问题。通过引入价值归属模块(VAM)和渠道自适应触发路由(CATR)模块，CAPTS能够在快手国际短视频平台上提高多渠道召回率，并在线上平均设备使用时长方面实现了0.351%的增长。


<details>
  <summary>Details</summary>
Motivation: 当前工业级推荐系统在采用U2I2I管道进行候选生成时面临两个主要挑战：一是触发器的价值评估基于直接反馈而非其作为检索种子的下游效用；二是各渠道独立选择触发器导致跨渠道重叠增加。为了解决这些问题，提出了CAPTS框架。

Method: CAPTS框架包括两个关键组成部分：价值归属模块（VAM）与渠道自适应触发路由（CATR）。VAM通过对每个触发器所引出项目在各个I2I渠道上的后续参与度给予信用来提供前瞻性的监督；而CATR则负责协调触发器到渠道的分配，以最大化多渠道检索的整体价值。

Result: 广泛的离线实验及快手国际版大规模在线A/B测试表明，CAPTS能够持续提升多渠道回忆性能，并在线上实现人均设备使用时长+0.351%的增长。

Conclusion: Channel-Aware, Preference-Aligned Trigger Selection (CAPTS) 提供了一个统一且灵活的方法来处理多渠道触发选择问题，通过优化触发器的选择过程显著提高了推荐系统的效率和用户参与度。

Abstract: Large-scale industrial recommender systems commonly adopt multi-channel retrieval for candidate generation, combining direct user-to-item (U2I) retrieval with two-hop user-to-item-to-item (U2I2I) pipelines. In U2I2I, the system selects a small set of historical interactions as triggers to seed downstream item-to-item (I2I) retrieval across multiple channels. In production, triggers are often selected using rule-based policies or learned scorers and tuned in a channel-by-channel manner. However, these practices face two persistent challenges: biased value attribution that values triggers by on-trigger feedback rather than their downstream utility as retrieval seeds, and uncoordinated multi-channel routing where channels select triggers independently under a shared quota, increasing cross-channel overlap. To address these challenges, we propose Channel-Aware, Preference-Aligned Trigger Selection (CAPTS), a unified and flexible framework that treats multi-channel trigger selection as a learnable routing problem. CAPTS introduces a Value Attribution Module (VAM) that provides look-ahead supervision by crediting each trigger with the subsequent engagement generated by items retrieved from it on each I2I channel, and a Channel-Adaptive Trigger Routing (CATR) module that coordinates trigger-to-channel assignment to maximize the overall value of multi-channel retrieval. Extensive offline experiments and large-scale online A/B tests on Kwai, Kuaishou's international short-video platform, show that CAPTS consistently improves multi-channel recall offline and delivers a +0.351% lift in average time spent per device online.

</details>


### [8] [RQ-GMM: Residual Quantized Gaussian Mixture Model for Multimodal Semantic Discretization in CTR Prediction](https://arxiv.org/abs/2602.12593)
*Ziye Tong,Jiahao Liu,Weimin Zhang,Hongji Ruan,Derick Tang,Zhanpeng Zeng,Qinsong Zeng,Peng Zhang,Tun Lu,Ning Gu*

Main category: cs.IR

TL;DR: 提出了一种新的方法RQ-GMM，用于改进多模态内容在点击率(CTR)预测中的应用。通过结合高斯混合模型与残差量化技术，该方法提升了代码本利用率和重建精度，在大规模短视频平台上的实验表明其相较于强基线有显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 直接将预训练模型得到的连续型嵌入整合到CTR模型中会导致次优结果，因为它们有着不一致的优化目标和收敛速度。尽管将嵌入离散化为语义ID再输入CTR模型是一个更有效的解决方案，但现有方法存在代码本利用不足、重建准确性和语义区分度低的问题。

Method: 提出了RQ-GMM（剩余量化高斯混合模型），它采用概率建模来更好地捕捉多模态嵌入空间的统计结构。通过结合高斯混合模型与残差量化技术，RQ-GMM实现了更好的代码本利用率和重建准确性。

Result: 在公开数据集上进行的实验以及在一个服务数亿用户的大型短视频平台上进行的在线A/B测试显示，RQ-GMM相比强大的基准线在广告商价值上提高了1.502%。

Conclusion: RQ-GMM提供了一个有效的方法来改善多模态内容在CTR预测中的表现，通过增强代码本利用率和重建精度解决了现有方法存在的问题，并已在实际应用场景中部署使用。

Abstract: Multimodal content is crucial for click-through rate (CTR) prediction. However, directly incorporating continuous embeddings from pre-trained models into CTR models yields suboptimal results due to misaligned optimization objectives and convergence speed inconsistency during joint training. Discretizing embeddings into semantic IDs before feeding them into CTR models offers a more effective solution, yet existing methods suffer from limited codebook utilization, reconstruction accuracy, and semantic discriminability. We propose RQ-GMM (Residual Quantized Gaussian Mixture Model), which introduces probabilistic modeling to better capture the statistical structure of multimodal embedding spaces. Through Gaussian Mixture Models combined with residual quantization, RQ-GMM achieves superior codebook utilization and reconstruction accuracy. Experiments on public datasets and online A/B tests on a large-scale short-video platform serving hundreds of millions of users demonstrate substantial improvements: RQ-GMM yields a 1.502% gain in Advertiser Value over strong baselines. The method has been fully deployed, serving daily recommendations for hundreds of millions of users.

</details>


### [9] [Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback](https://arxiv.org/abs/2602.12612)
*Sein Kim,Sangwu Park,Hongseok Kang,Wonjoong Kim,Jimin Seo,Yeonjun In,Kanghoon Yoon,Chanyoung Park*

Main category: cs.IR

TL;DR: 提出了一种新的框架Self-EvolveRec，通过结合用户模拟器和模型诊断工具来提供定性和定量的反馈，从而指导推荐系统设计的自动优化。该框架在推荐性能和用户满意度方面均优于现有的NAS和基于LLM的代码进化方法。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统自动化设计方法如神经架构搜索（NAS）受限于由人为先验定义的固定搜索空间，限制了创新到预定义的操作上。虽然最近的基于大语言模型（LLM）驱动的代码进化框架将搜索目标从固定的搜索空间转移到开放式程序空间，但它们主要依赖于标量指标（例如NDCG、命中率），这些指标无法为模型故障提供定性见解或改进的方向性指导。

Method: 提出了Self-EvolveRec框架，通过集成用户模拟器进行定性批评和模型诊断工具来进行定量内部验证，建立了一个方向性反馈循环。此外，还引入了一种诊断工具-模型共同进化策略，以确保随着推荐架构的发展评价标准能够动态适应。

Result: 广泛的实验表明，Self-EvolveRec在推荐性能和用户满意度方面显著优于最新的NAS和基于LLM驱动的代码进化基线。

Conclusion: Self-EvolveRec代表了一种有效的途径，通过结合定性与定量反馈机制来克服传统方法中固有的局限性，并促进推荐系统的持续改进。

Abstract: Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.

</details>


### [10] [Training Dense Retrievers with Multiple Positive Passages](https://arxiv.org/abs/2602.12727)
*Benben Wang,Minghao Tang,Hengran Zhang,Jiafeng Guo,Keping Bi*

Main category: cs.IR

TL;DR: 本文系统研究了多正例优化目标在检索器训练中的应用，通过统一的对比学习框架分析了Joint Likelihood、Summed Marginal Likelihood和Log-Sum-Exp Pairwise损失函数的特点，并在不同数据集上进行了广泛的评估。结果表明Log-Sum-Exp Pairwise方法在各种情况下都表现出色，而Joint Likelihood和Summed Marginal Likelihood对正例质量非常敏感。此外，随机抽样策略也被发现是一个可靠的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前知识密集型系统依赖于高效的检索器来设定下游模块的性能上限，但检索器训练受限于稀疏的单一正例标注导致的问题，如假阴性噪声和次优监督。尽管大型语言模型（LLMs）能够大规模收集全面的多正例相关标签，但对于如何将这些密集信号有效地整合进训练中仍缺乏明确策略。

Method: 本研究提出了一种系统的方法来探讨检索器训练中的多正例优化目标。通过一个共享的对比学习框架，统一了包括联合似然(JointLH)、总边际似然(SumMargLH)以及Log-Sum-Exp成对(LSEPair)损失在内的代表性目标函数，并对其独特的梯度行为进行了理论分析。

Result: 实证研究表明，在自然问题、MS MARCO及BEIR基准测试中，LSEPair方法展现出一致的鲁棒性和性能优势；相比之下，JointLH与SumMargLH对正例的质量极为敏感。同时，随机采样(Rand1LH)被证实为一种稳定有效的基线策略。

Conclusion: 通过结合理论见解与实验结果，该研究为利用密集型LLM增强监督以提高检索器效能提供了实用的设计原则。

Abstract: Modern knowledge-intensive systems, such as retrieval-augmented generation (RAG), rely on effective retrievers to establish the performance ceiling for downstream modules. However, retriever training has been bottlenecked by sparse, single-positive annotations, which lead to false-negative noise and suboptimal supervision. While the advent of large language models (LLMs) makes it feasible to collect comprehensive multi-positive relevance labels at scale, the optimal strategy for incorporating these dense signals into training remains poorly understood. In this paper, we present a systematic study of multi-positive optimization objectives for retriever training. We unify representative objectives, including Joint Likelihood (JointLH), Summed Marginal Likelihood (SumMargLH), and Log-Sum-Exp Pairwise (LSEPair) loss, under a shared contrastive learning framework. Our theoretical analysis characterizes their distinct gradient behaviors, revealing how each allocates probability mass across positive document sets. Empirically, we conduct extensive evaluations on Natural Questions, MS MARCO, and the BEIR benchmark across two realistic regimes: homogeneous LLM-annotated data and heterogeneous mixtures of human and LLM labels. Our results show that LSEPair consistently achieves superior robustness and performance across settings, while JointLH and SumMargLH exhibit high sensitivity to the quality of positives. Furthermore, we find that the simple strategy of random sampling (Rand1LH) serves as a reliable baseline. By aligning theoretical insights with empirical findings, we provide practical design principles for leveraging dense, LLM-augmented supervision to enhance retriever effectiveness.

</details>


### [11] [SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise](https://arxiv.org/abs/2602.12783)
*Yuejie Li,Ke Yang,Yueying Hua,Berlin Chen,Jianhao Nie,Yueping He,Caixin Kang*

Main category: cs.IR

TL;DR: 本文介绍了SQuTR，一个用于评估语音查询检索系统在复杂声学扰动下的鲁棒性的基准。该基准包括一个大规模数据集和统一的评估协议，通过合成来自200位真实说话人的语音并混合17种现实世界环境噪声来测试系统的性能。实验结果显示，随着噪声增加，检索性能下降，并且不同系统之间的性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有的评估数据集通常仅限于简单查询和受限噪声条件，不足以评估语音查询检索系统在复杂声学干扰下的鲁棒性。为了解决这一限制，作者提出了SQuTR，旨在提供一个可重现的测试平台以促进关于语音到文本检索鲁棒性的未来研究。

Method: SQuTR聚合了来自六个常用英文和中文文本检索数据集的37,317个唯一查询，覆盖多个领域和多样化的查询类型。使用200位真实说话人的声音特征合成语音，并在控制信噪比水平下混合17种类别的现实世界环境噪声，从而实现从安静到高噪声条件下可重复的鲁棒性评估。

Result: 实验结果表明，随着噪声水平的提高，检索性能下降，且不同系统之间存在显著差异。即使是大规模检索模型，在极端噪声条件下也表现不佳，这表明鲁棒性仍然是一个重要瓶颈。

Conclusion: SQuTR为基准测试和诊断分析提供了可重复使用的测试平台，并促进了针对语音至文本检索中鲁棒性的未来研究。

Abstract: Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.

</details>


### [12] [WISE: A Multimodal Search Engine for Visual Scenes, Audio, Objects, Faces, Speech, and Metadata](https://arxiv.org/abs/2602.12819)
*Prasanna Sridhar,Horace Lee,David M. S. Pinto,Andrew Zisserman,Abhishek Dutta*

Main category: cs.IR

TL;DR: 介绍了WISE，一个开源的音视频搜索引擎，它将多种多模态检索能力整合到单一实用工具中，支持自然语言和反向图像查询、基于人脸的搜索、音频事件检索、自动转录语音搜索及用户提供的元数据过滤，并能够高效处理大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 开发一种面向非机器学习专家用户的综合型音视频搜索引擎，通过集成多种多模态检索功能，使得用户能够轻松地在大型多媒体资料库中进行复杂的跨模态查询。

Method: 利用矢量搜索技术构建了名为WISE的系统，该系统具备模块化架构，支持自然语言与反向图像查询、人脸识别、声音事件的文字或音频文件检索、语音转写文本搜索以及自定义元数据筛选等功能。

Result: WISE系统成功实现了对数百万张图片或数千小时视频内容的有效检索，并且其设计允许本地部署以保护隐私或敏感信息，同时适用于不同的实际应用场景。

Conclusion: WISE作为一个开源项目为用户提供了一个强大的多媒体搜索解决方案，能够在不需要深厚机器学习知识的前提下，实现跨越多种媒体类型的复杂查询。

Abstract: In this paper, we present WISE, an open-source audiovisual search engine which integrates a range of multimodal retrieval capabilities into a single, practical tool accessible to users without machine learning expertise. WISE supports natural-language and reverse-image queries at both the scene level (e.g. empty street) and object level (e.g. horse) across images and videos; face-based search for specific individuals; audio retrieval of acoustic events using text (e.g. wood creak) or an audio file; search over automatically transcribed speech; and filtering by user-provided metadata. Rich insights can be obtained by combining queries across modalities -- for example, retrieving German trains from a historical archive by applying the object query "train" and the metadata query "Germany", or searching for a face in a place. By employing vector search techniques, WISE can scale to support efficient retrieval over millions of images or thousands of hours of video. Its modular architecture facilitates the integration of new models. WISE can be deployed locally for private or sensitive collections, and has been applied to various real-world use cases. Our code is open-source and available at https://gitlab.com/vgg/wise/wise.

</details>


### [13] [JARVIS: An Evidence-Grounded Retrieval System for Interpretable Deceptive Reviews Adjudication](https://arxiv.org/abs/2602.12941)
*Nan Lu,Leyang Li,Yurong Hu,Rui Lin,Shaoyi Xu*

Main category: cs.IR

TL;DR: 本文提出了一种名为JARVIS的新框架，通过增强检索和证据图结构来识别欺骗性评论。该方法不仅提高了检测的准确性（精度从0.953提高到0.988，召回率从0.830提高到0.901），而且在实际生产环境中显著减少了人工审核时间并提升了模型分析结果采纳率至96.4%。


<details>
  <summary>Details</summary>
Motivation: 面对电子商务生态系统中旨在人为操纵产品感知质量的虚假评论问题，现有的基于评论级别和图形的检测方法存在泛化不足与缺乏可解释性的局限。为解决这些问题，提出了新的解决方案。

Method: 提出了JARVIS框架，该框架利用混合密集-稀疏多模态检索技术寻找语义相似的证据，通过共享实体扩展关系信号，并构建异构证据图。之后，大型语言模型基于收集到的证据进行判断，生成具有解释性的风险评估报告。

Result: 离线实验显示，JARVIS在构造的评论数据集上性能有所提升，精度从0.953上升到了0.988，召回率则由0.830增加到了0.901。在生产环境中使用时，召回量增加了27%，同时人工检查所需时间减少了75%。此外，模型产生的分析结果被接受的比例达到了96.4%。

Conclusion: JARVIS框架有效解决了当前虚假评论检测中存在的泛化能力和可解释性问题，显著改善了电子商务平台上虚假评论治理的效果。

Abstract: Deceptive reviews, refer to fabricated feedback designed to artificially manipulate the perceived quality of products. Within modern e-commerce ecosystems, these reviews remain a critical governance challenge. Despite advances in review-level and graph-based detection methods, two pivotal limitations remain: inadequate generalization and lack of interpretability. To address these challenges, we propose JARVIS, a framework providing Judgment via Augmented Retrieval and eVIdence graph Structures. Starting from the review to be evaluated, it retrieves semantically similar evidence via hybrid dense-sparse multimodal retrieval, expands relational signals through shared entities, and constructs a heterogeneous evidence graph. Large language model then performs evidence-grounded adjudication to produce interpretable risk assessments. Offline experiments demonstrate that JARVIS enhances performance on our constructed review dataset, achieving a precision increase from 0.953 to 0.988 and a recall boost from 0.830 to 0.901. In the production environment, our framework achieves a 27% increase in the recall volume and reduces manual inspection time by 75%. Furthermore, the adoption rate of the model-generated analysis reaches 96.4%.

</details>


### [14] [RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems](https://arxiv.org/abs/2602.12968)
*Junhua Liu,Yang Jihao,Cheng Chang,Kunrong LI,Bin Fu,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 本文提出了一种名为RGAlign-Rec的闭环对齐框架，该框架结合了基于大语言模型（LLM）的语义推理器和查询增强（QE）排名模型，以解决电子商务聊天机器人中用户特征与知识库意图之间的语义鸿沟及通用LLM输出与任务特定排名效用之间目标不一致的问题。实验结果表明，该方法在多个指标上取得了显著改进，包括GAUC、错误率降低以及召回率等。


<details>
  <summary>Details</summary>
Motivation: 主动意图预测是现代电子商务聊天机器人的一项关键能力，它通过从行为和上下文信号中预判用户需求来实现“零查询”推荐。然而，现有的工业系统面临两个基本挑战：1) 离散用户特征与聊天机器人知识库中的语义意图之间的语义差距；2) 通用目的的大语言模型输出与特定于任务的排名实用性之间存在目标错位。为了解决这些问题...

Method: 提出了RGAlign-Rec，这是一个将基于大型语言模型（LLM）的语义推理器与查询增强型（Query-Enhanced, QE）排名模型相结合的闭环对齐框架。此外，还介绍了一种称为排名引导对齐（Ranking-Guided Alignment, RGA）的多阶段训练范式，利用下游排名信号作为反馈来优化LLM的潜在推理过程。

Result: 在Shopee提供的大规模工业数据集上进行了广泛的实验，结果显示RGAlign-Rec在GAUC上实现了0.12%的增长，导致错误率相对减少了3.52%，并且Recall@3提高了0.56%。在线A/B测试进一步验证了我们框架的累积效果：查询增强模型(QE-Rec)最初使点击率(CTR)提高了0.98%，随后的排名指导对齐阶段贡献了额外的0.13%增长。

Conclusion: 研究结果表明，排名意识对齐能够有效地同步语义推理与排名目标，在现实世界的主动推荐系统中显著提高了预测准确性和服务质量。

Abstract: Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling "zero-query" recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot's Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM's latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3. Online A/B testing further validates the cumulative effectiveness of our framework: the Query-Enhanced model (QE-Rec) initially yields a 0.98% improvement in CTR, while the subsequent Ranking-Guided Alignment stage contributes an additional 0.13% gain. These results indicate that ranking-aware alignment effectively synchronizes semantic reasoning with ranking objectives, significantly enhancing both prediction accuracy and service quality in real-world proactive recommendation systems.

</details>


### [15] [Awakening Dormant Users: Generative Recommendation with Counterfactual Functional Role Reasoning](https://arxiv.org/abs/2602.13134)
*Huishi Luo,Shuokai Li,Hanchen Yang,Zhongbo Sun,Haojie Ding,Boheng Zhang,Zijia Cai,Renliang Qian,Fan Yang,Tingting Gao,Chenyi Lei,Wenwu Ou,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 提出了一种名为RoleGen的新框架，通过结合转化轨迹推理器和生成行为骨干来激活沉睡用户，从而推动大规模电商平台的GMV增长。该方法在线上A/B测试中实现了Recall@1提高6.2%以及订单量增加7.3%的成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常基于单步估计物品的内在价值，忽略了物品在用户转化路径中的工具性作用。为了更好地理解和预测用户的潜在意图并驱动其后续决策，需要一种新的方法来有效激活那些活跃但转化率低的用户。

Method: 设计了RoleGen框架，它融合了一个基于LLM（大型语言模型）的转化轨迹推理器与一个生成行为骨干。推理器明确建模了物品的功能角色以重建意图演变过程，并使用反事实推断模拟多种转化路径；这些候选物品被整合进生成骨干，整个系统通过协作式的“推理-执行-反馈-反思”闭环策略进行优化。

Result: 通过对快手电商平台的广泛离线实验及线上A/B测试表明，RoleGen能够实现Recall@1指标提升6.2%，在线订单量增长7.3%。

Conclusion: RoleGen提供了一种有效的手段来激活电商平台上处于休眠状态的用户群体，有助于促进平台GMV的增长。

Abstract: Awakening dormant users, who remain engaged but exhibit low conversion, is a pivotal driver for incremental GMV growth in large-scale e-commerce platforms. However, existing approaches often yield suboptimal results since they typically rely on single-step estimation of an item's intrinsic value (e.g., immediate click probability). This mechanism overlooks the instrumental effect of items, where specific interactions act as triggers to shape latent intent and drive subsequent decisions along a conversion trajectory. To bridge this gap, we propose RoleGen, a novel framework that synergizes a Conversion Trajectory Reasoner with a Generative Behavioral Backbone. Specifically, the LLM-based Reasoner explicitly models the context-dependent Functional Role of items to reconstruct intent evolution. It further employs counterfactual inference to simulate diverse conversion paths, effectively mitigating interest collapse. These reasoned candidate items are integrated into the generative backbone, which is optimized via a collaborative "Reasoning-Execution-Feedback-Reflection" closed-loop strategy to ensure grounded execution. Extensive offline experiments and online A/B testing on the Kuaishou e-commerce platform demonstrate that RoleGen achieves a 6.2% gain in Recall@1 and a 7.3% increase in online order volume, confirming its effectiveness in activating the dormant user base.

</details>


### [16] [Asynchronous Verified Semantic Caching for Tiered LLM Architectures](https://arxiv.org/abs/2602.13165)
*Asmit Kumar Singh,Haozhe Wang,Laxmi Naga Santosh Attaluri,Tak Chiam,Weihua Zhu*

Main category: cs.IR

TL;DR: 介绍了Krites，一种异步的、由大型语言模型评判的缓存策略，旨在不改变服务决策的情况下扩展静态缓存覆盖范围。通过在关键路径上与标准静态阈值策略行为一致，并在最接近的静态响应略低于静态阈值时异步调用大型语言模型进行判断，以验证该静态响应是否适用于新提示。被批准的匹配项会被提升到动态缓存中，从而随着时间推移增加可重用策划静态答案的请求比例。


<details>
  <summary>Details</summary>
Motivation: 当前生产环境中使用的分层静态-动态缓存设计通常受到单一嵌入相似性阈值限制，这导致了保守阈值错过安全复用机会和激进阈值可能提供语义不正确响应之间的艰难权衡。

Method: 提出了一种名为Krites的新缓存策略，它利用大型语言模型作为裁判来异步评估那些稍微低于静态阈值的缓存条目是否可以接受。如果被LLM批准，则这些条目将被添加到动态缓存中供将来使用。

Result: 基于对话和搜索工作负载的跟踪驱动模拟表明，相比经过调整的基线，Krites能够将直接静态命中加上验证后推广的请求比例提高至多3.9倍，而且不会增加关键路径延迟。

Conclusion: Krites提供了一种有效的方法来扩大高质量静态回答的覆盖范围，同时保持服务质量和性能不受影响。

Abstract: Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [SHAPR: A Solo Human-Centred and AI-Assisted Practice Framework for Research Software Development](https://arxiv.org/abs/2602.12443)
*Ka Ching Chan*

Main category: cs.SE

TL;DR: 本文提出了SHAPR框架，作为对行动设计研究(ADR)的补充，旨在为单独进行、AI辅助的研究软件开发提供日常实践指导。通过明确角色、制品、反思性实践和轻量级治理机制，支持保持人类责任性和学习在AI辅助开发中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能正在重塑开发实践，在为研究者提供强大帮助的同时也引入了新的挑战，如问责制、反思和方法论严谨性问题。虽然行动设计研究(ADR)为研究和构建社会技术制品提供了坚实的理论基础，但对于如何将其原则具体应用于独立、AI辅助的研究软件开发实践中，却缺乏足够的指导。

Method: 文章提出了一种名为SHAPR（单人、以人为中心、AI辅助实践）的框架，该框架将ADR的高层次原则转化为适合现代研究环境的具体行动指南。通过对SHAPR内部一致性、与ADR原则的一致性以及其对于单独研究人员实践适用性的反思分析来进行形式化评估。

Result: SHAPR框架能够支持ADR建立-干预-评估周期的实施，并通过明确所需的角色、工件、反思实践及轻量级治理机制来维持人类的责任感和学习过程于AI辅助开发中。

Conclusion: 通过将研究软件开发、人机协作与反思性学习联系起来，本研究表明SHAPR不仅有助于知识生产，还可能成为HDR研究人员培训的支持工具。

Abstract: Research software has become a central vehicle for inquiry and learning in many Higher Degree Research (HDR) contexts, where solo researchers increasingly develop software-based artefacts as part of their research methodology. At the same time, generative artificial intelligence is reshaping development practice, offering powerful forms of assistance while introducing new challenges for accountability, reflection, and methodological rigour. Although Action Design Research (ADR) provides a well-established foundation for studying and constructing socio-technical artefacts, it offers limited guidance on how its principles can be operationalised in the day-to-day practice of solo, AI-assisted research software development. This paper proposes the SHAPR framework (Solo, Human-centred, AI-assisted PRactice) as a practice-level operational framework that complements ADR by translating its high-level principles into actionable guidance for contemporary research contexts. SHAPR supports the enactment of ADR Building-Intervention-Evaluation cycles by making explicit the roles, artefacts, reflective practices, and lightweight governance mechanisms required to sustain human accountability and learning in AI-assisted development. The contribution of the paper is conceptual: SHAPR itself is treated as the primary design artefact and unit of analysis and is evaluated formatively through reflective analysis of its internal coherence, alignment with ADR principles, and applicability to solo research practice. By explicitly linking research software development, Human-AI collaboration, and reflective learning, this study contributes to broader discussions on how SHAPR can support both knowledge production and HDR researcher training.

</details>


### [18] [FuncDroid: Towards Inter-Functional Flows for Comprehensive Mobile App GUI Testing](https://arxiv.org/abs/2602.12834)
*Jinlong He,Changwei Xia,Binru Huang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种基于功能流图(FFG)的GUI测试方法，旨在提高跨功能交互中的深度bug检测能力。通过结合长期和短期视图指导的测试过程，该方法能够自适应地优化功能边界，并系统地探索不同触发条件下的跨功能流程。实验表明，所开发的工具FuncDroid在覆盖率和bug检测数量上均优于现有技术，特别是在商业应用中发现了新的非崩溃功能性bug。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用程序(app)功能变得越来越复杂且迭代速度加快，确保高可靠性面临重大挑战。虽然面向功能的GUI测试受到了越来越多的研究关注，但现有的方法大多忽略了功能之间的交互作用，这使得它们在发现隐藏于跨功能行为深处的bug方面效果不佳。

Method: 首先设计了一个名为功能流图(FFG)的行为模型，明确捕捉了app的功能单元及其跨功能交互。基于FFG，进一步介绍了一种以跨功能流为导向的GUI测试方法，其目标是精确建模和深度bug检测。此方法通过一个长短期视角指导的测试过程来实现，可以自适应地细化功能边界并系统性地探索多种触发条件下跨功能流。

Result: 实验结果表明，与最先进基线相比，FuncDroid在覆盖率（+28%）和bug检测数量（+107%）方面显著优于前者。此外，FuncDroid还在商业应用中成功发现了18个先前未知的非崩溃功能性bug。

Conclusion: 提出的基于FFG的GUI测试方法及其实现工具FuncDroid有效提高了跨功能交互中深度bug的检测能力，对于提高移动应用可靠性和用户体验具有重要意义。

Abstract: As mobile application (app) functionalities grow increasingly complex and their iterations accelerate, ensuring high reliability presents significant challenges. While functionality-oriented GUI testing has attracted growing research attention, existing approaches largely overlook interactions across functionalities, making them ineffective at uncovering deep bugs hidden in inter-functional behaviors. To fill this gap, we first design a Functional Flow Graph (FFG), a behavioral model that explicitly captures an app's functional units and their inter-functional interactions. Based on the FFG, we further introduce an inter-functional-flow-oriented GUI testing approach with the dual goals of precise model construction and deep bug detection. This approach is realized through a long-short-term-view-guided testing process. By combining two complementary test-generation views, it can adaptively refine functional boundaries and systematically explore inter-functional flows under diverse triggering conditions. We implement our approach in a tool called FuncDroid, and evaluate it on two benchmarks: (1) a widely-used open-source benchmark with 50 reproducible crash bugs and (2) a diverse set of 52 popular commercial apps. Experimental results demonstrate that FuncDroid significantly outperforms state-of-the-art baselines in both coverage (+28%) and bug detection number (+107%). Moreover, FuncDroid successfully uncovers 18 previously unknown non-crash functional bugs in commercial apps, confirming its practical effectiveness.

</details>


### [19] [The Influence of Code Smells in Efferent Neighbors on Class Stability](https://arxiv.org/abs/2602.12950)
*Zushuai Zhang,Elliott Wen,Ewan Tempero*

Main category: cs.SE

TL;DR: 该研究探讨了类的出向邻居中存在代码异味是否会影响其稳定性，同时考虑了代码异味相互关联和相互作用的因素。通过分析100个顶级GitHub项目的提交历史记录，检测代码异味和静态依赖关系，并将这些因素建模为类稳定性的预测因子。


<details>
  <summary>Details</summary>
Motivation: 理解导致代码不稳定性的原因对于有效的软件维护至关重要。虽然通常认为代码异味会损害可维护性，但大多数先前的研究仅关注被修改类内部的异味。实际上，类的变化可能由其出向邻居（即它所依赖的类）的修改引起，即使该类本身是干净的。此外，代码异味很少单独出现，它们往往在类内或通过静态依赖连接的类间共同出现，形成代码异味互相关联现象，这可能导致代码异味互动，进一步加剧可维护性问题。然而，关于代码异味互相关联和互动对代码质量的影响仍需更多探索。

Method: 研究者们从100个顶级星级GitHub项目中挖掘了一年的提交历史，检测代码异味与静态依赖，确定代码异味之间的关联及互动，并将这些因素作为类稳定性的预测因素进行建模。

Result: 研究表明，一个类的出向邻居中存在的代码异味确实可以影响到该类的稳定性，尤其是当考虑到代码异味间的相互联系与相互作用时。

Conclusion: 为了提高软件系统的可维护性，不仅需要关注单个类中的代码异味，还需要重视那些可能引发连锁反应的、具有不良设计特征的依赖类。此外，代码异味之间的相互关联和相互作用也应被纳入考量范围，以更全面地评估和改善软件质量。

Abstract: Understanding what drives code instability is essential for effective software maintenance, as unstable classes require larger or more frequent edits and increase the risk of unintended side effects. Although code smells are widely believed to harm maintainability, most prior stability studies examine only the smells within the class being modified. In practice, however, classes can change because their efferent neighbors (i.e., the classes they depend on) are modified due to ripple effects that propagate along static dependencies, even if the class itself is clean. Such ripple effects may be more severe when the efferent neighbor exhibits code smells. In addition, code smells rarely occur alone. They often appear together within a class or across classes connected by static dependencies, a phenomenon known as code smell interrelation. Such interrelation can lead to code smell interaction, where smells are directly connected through static dependencies and may further compound maintainability issues. However, the effect of code smell interrelation and interaction on code quality remains largely underexplored. Therefore, this study investigates whether the presence of code smells in a class's efferent neighbors affects its stability, considering the factor of code smell interrelation and interaction. To achieve this, we mine one year of commit history from 100 top-starred GitHub projects, detect code smells and static dependencies, determine code smell interrelation and interaction, and model these factors as predictors of class stability.

</details>


### [20] [Analysis of Asset Administration Shell-based Negotiation Processes for Scaling Applications](https://arxiv.org/abs/2602.13029)
*David Dietrich,Armin Lechler,Alexander Verl*

Main category: cs.SE

TL;DR: 本文研究了主动资产管理壳(AAS)在资产数量增加情况下的协商效率，通过引入场景和评估标准对现有架构进行了扩展性测试。实验结果揭示了基于AAS的协商机制在性能限制、通信开销及适应性方面的表现，为AAS的发展和标准化提供了参考。


<details>
  <summary>Details</summary>
Motivation: 当前AAS标准化工作的重点在于子模型及其安全性上，而对于其主动行为的研究仍处于概念阶段。先前的研究仅考察了少量资产情况下的主动AAS架构示例，因此对于工业环境中的可扩展性存在疑问。本研究旨在分析主动AAS在应用规模增长时的表现，以促进该技术的进一步发展与标准化。

Method: 提出了一个情景设置以及评价指标来探讨主动AAS应用于更大规模时的效果。基于现有的主动AAS架构开发了一个可扩展实现，并据此执行了一系列不同资产数量条件下的实验。

Result: 实验结果显示了随着资产数量增加，基于AAS的协商机制所面临的性能局限、额外通信负担以及调整能力等方面的挑战。这些发现有助于更好地理解如何优化此机制以适应更广泛的工业应用场景。

Conclusion: 通过分析主动AAS在不同规模应用中的表现，本研究表明尽管存在一些性能和通信上的限制，但通过对现有问题的认识和改进，可以推动AAS技术的进步及其在实际工业环境中的采纳。

Abstract: The proactive Asset Administration Shell (AAS) enables bidirectional communication between assets. It uses the Language for I4.0 Components in VDI/VDE 2193 to facilitate negotiations, such as allocating products to available production resources. This paper investigates the efficiency of the negotiation, based on criteria, such as message load, for applications with a scaling number of assets. Currently, the focus of AAS standardization is on submodels and their security to enable interoperable data access. Their proactive behavior remains conceptual and is still a subject of scientific research. Existing studies examine proactive AAS architecture examples with a limited number of assets, raising questions about their scalability in industrial environments. To analyze proactive AAS for scaling applications, a scenario and evaluation criteria are introduced. A scalable implementation is developed using current architectures for proactive AAS, upon which experiments are conducted with a varying number of assets. The results reveal the performance limitations, communication overhead, and adaptability of the AAS-based negotiation mechanism scaling. This information can improve the further development and standardization of the AAS.

</details>


### [21] [Automated Testing of Task-based Chatbots: How Far Are We?](https://arxiv.org/abs/2602.13072)
*Diego Clerissi,Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本研究通过评估GitHub上精选的任务型聊天机器人的最新聊天机器人测试技术的有效性，来调查这些技术的局限性，如生成的测试场景过于简单和实现的预言机较弱。


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人的流行，有效地评估其质量变得至关重要。然而，传统的测试技术无法系统地覆盖聊天机器人的对话空间，尽管为聊天机器人专门设计的新方法已经出现，但它们仍然存在局限性，例如生成的测试场景过于简单以及预言机功能不够强大。

Method: 进行了一项验证性研究，选择了一系列基于任务的、使用最流行的商业和开源平台开发的聊天机器人，对最先进的聊天机器人测试技术的有效性进行了评估。

Result: 研究揭示了当前聊天机器人测试技术在处理复杂对话场景和提供可靠预言机判断方面存在的局限性。

Conclusion: 虽然针对聊天机器人的测试技术有所进步，但仍需进一步改进以更好地适应复杂对话场景并提高预言机的质量。

Abstract: Task-based chatbots are software, typically embedded in real-world applications, that assist users in completing tasks through a conversational interface. As chatbots are gaining popularity, effectively assessing their quality has become crucial. Whereas traditional testing techniques fail to systematically exercise the conversational space of chatbots, several approaches specifically targeting chatbots have emerged from both industry and research. Although these techniques have shown advancements over the years, they still exhibit limitations, such as simplicity of the generated test scenarios and weakness in implemented oracles. In this paper, we conduct a confirmatory study to investigate such limitations by evaluating the effectiveness of state-of-the-art chatbot testing techniques on a curated selection of task-based chatbots from GitHub, developed using the most popular commercial and open-source platforms.

</details>


### [22] [Source Code Hotspots: A Diagnostic Method for Quality Issues](https://arxiv.org/abs/2602.13170)
*Saleha Muzammil,Mughees Ur Rehman,Zoe Kotti,Diomidis Spinellis*

Main category: cs.SE

TL;DR: 研究分析了91个活跃GitHub项目的版本历史，找出了15种常见的代码热点模式，并指出自动化账号生成的编辑占所有热点编辑的74%，建议通过重构指南和持续集成检查来改善软件质量。


<details>
  <summary>Details</summary>
Motivation: 软件源代码中经常存在“热点”：这些代码片段比项目其他部分变更频繁得多，集中了维护活动。理解这些热点出现的原因有助于提高软件的质量。

Method: 通过挖掘91个正在演进且活跃开发中的GitHub仓库的完整版本历史，识别出15种重复出现的行级热点模式，并分析其背后的原因。

Result: 发现了15种常见热点模式，其中最普遍的是固定版本更新（26%）、长行更改（17%）以及格式化乒乓（9%）。值得注意的是，自动化账户产生了所有热点编辑中的74%，表明机器人活动是变化历史中一个主要但很大程度上可避免的噪音来源。

Conclusion: 通过将每种模式映射到具体的重构指南和持续集成检查，本研究为实践者提供了减少热点并系统性提升软件在配置性、稳定性和可变性方面质量的实际步骤。

Abstract: Software source code often harbours "hotspots": small portions of the code that change far more often than the rest of the project and thus concentrate maintenance activity. We mine the complete version histories of 91 evolving, actively developed GitHub repositories and identify 15 recurring line-level hotspot patterns that explain why these hotspots emerge. The three most prevalent patterns are Pinned Version Bump (26%), revealing brittle release practices; Long Line Change (17%), signalling deficient layout; and Formatting Ping-Pong (9%), indicating missing or inconsistent style automation. Surprisingly, automated accounts generate 74% of all hotspot edits, suggesting that bot activity is a dominant but largely avoidable source of noise in change histories. By mapping each pattern to concrete refactoring guidelines and continuous integration checks, our taxonomy equips practitioners with actionable steps to curb hotspots and systematically improve software quality in terms of configurability, stability, and changeability.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [23] [Distance-based certification for leader election in meshed graphs and local recognition of their subclasses](https://arxiv.org/abs/2602.12894)
*Jérémie Chalopin,Victor Chepoi,Maria Kokkou*

Main category: cs.DC

TL;DR: 本文提出了一个用于匿名网格图中领导选举的2-局部证明标记方案，其标签集为{0,1,2}。此外，还提供了3-局部证明标记方案来识别这些网格图子类，使用大小为O(log D)（D是图的直径）的标签。通过局部验证每个顶点v被标记为其到任意根s的距离d(s,v)，并利用此距离验证来确保图的三角-正方形复合体是单连通的，进而基于不同的局部到全局特征设计了针对不同网格图子类别的证明标记方案。为了实现具有常数大小标签的领导选举证明标记方案，展示了可以局部检查每个v是否按照某指定为首领的根s被标记为d(s,v) mod 3。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一种有效的、具有小标签空间的证明标记方案，以解决在匿名网格图中的领导选举问题，并且能够识别多种重要的网格图子类别。

Method: 方法包括：1. 设计一个2-局部证明标记方案，使得每个顶点都能根据与特定根节点的距离来标记；2. 利用距离验证保证图的三角-正方形复合体是单连通的，以此为基础来识别不同类型的网格图；3. 提出了一种3-局部证明标记方案，允许使用对数级别大小的标签来区分不同的网格图子类；4. 开发了一种特殊的证明标记方案，它能通过局部检查顶点是否被正确地标记为它们到指定首领根节点距离模3的结果，从而实现领导选举。

Result: 结果表明，所提出的方案能够在保持标签大小为常数的情况下成功地应用于领导选举问题，并且能够有效地区分和识别多种网格图子类别。

Conclusion: 结论是，本研究提供的证明标记方案不仅解决了匿名网格图中的领导选举问题，而且对于识别广泛存在的网格图子类别也十分有效，这为该领域内的进一步研究奠定了基础。

Abstract: In this paper, we present a 2-local proof labeling scheme with labels in $\{ 0,1,2\}$ for leader election in anonymous meshed graphs. Meshed graphs form a general class of graphs defined by a distance condition. They comprise several important classes of graphs, which have long been the subject of intensive studies in metric graph theory, geometric group theory, and discrete mathematics: median graphs, bridged graphs, chordal graphs, Helly graphs, dual polar graphs, modular, weakly modular graphs, and basis graphs of matroids. We also provide 3-local proof labeling schemes to recognize these subclasses of meshed graphs using labels of size $O(\log D)$ (where $D$ is the diameter of the graph).
  To establish these results, we show that in meshed graphs, we can verify locally that every vertex $v$ is labeled by its distance $d(s,v)$ to an arbitrary root $s$. To design proof labeling schemes to recognize the subclasses of meshed graphs mentioned above, we use this distance verification to ensure that the triangle-square complex of the graph is simply connected and we then rely on existing local-to-global characterizations for the different classes we consider.
  To get a proof-labeling scheme for leader election with labels of constant size, we then show that we can check locally if every $v$ is labeled by $d(s,v) \pmod{3}$ for some root $s$ that we designate as the leader.

</details>


### [24] [Classification of Local Optimization Problems in Directed Cycles](https://arxiv.org/abs/2602.13046)
*Thomas Boudier,Fabian Kuhn,Augusto Modanese,Ronja Stimpert,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文对有向循环中的局部优化问题在确定性和随机性LOCAL模型下的分布式计算复杂度进行了全面分类，并能够自动确定给定问题的复杂度类别，同时有效合成渐近最优的分布式算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于对有向循环中不同类型的局部优化问题（如最小-和、最大-和、最小-最大或最大-最小等）给出一个完整的分布式计算复杂度分类，从而为各种常见的分布式任务提供理论基础。

Method: 通过分析局部优化问题Π以及常数逼近比α，在确定性和随机性的LOCAL模型下，对于有向循环中的α-逼近问题，研究了其具有四种可能的复杂度类型之一。并且提出了一种高效的集中式序列元算法来自动确定复杂度类别并合成渐近最优的分布式算法。

Result: 对于任何局部优化问题Π及任意常数逼近比α，在有向循环中找到Π的α-逼近解的复杂度被归类为四种类别之一：1. 确定性LOCAL模型下O(1)轮，随机化LOCAL模型下O(1)轮；2. 确定性LOCAL模型下Θ(log* n)轮，随机化LOCAL模型下O(1)轮；3. 两种模型下均为Θ(log* n)轮；4. 两种模型下均为Θ(n)轮。此外，还能有效地确定这些复杂度类别，并合成最优算法。

Conclusion: 这项工作为有向循环中的局部优化问题提供了全面的分布式计算复杂度分类，并且开发出一种方法来自动识别给定问题的复杂度类别和构造相应的最优化算法，这是对先前仅针对局部搜索问题结果的重要扩展。

Abstract: We present a complete classification of the distributed computational complexity of local optimization problems in directed cycles for both the deterministic and the randomized LOCAL model. We show that for any local optimization problem $Π$ (that can be of the form min-sum, max-sum, min-max, or max-min, for any local cost or utility function over some finite alphabet), and for any \emph{constant} approximation ratio $α$, the task of finding an $α$-approximation of $Π$ in directed cycles has one of the following complexities:
  1. $O(1)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,
  2. $Θ(\log^* n)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,
  3. $Θ(\log^* n)$ rounds in deterministic LOCAL, $Θ(\log^* n)$ rounds in randomized LOCAL,
  4. $Θ(n)$ rounds in deterministic LOCAL, $Θ(n)$ rounds in randomized LOCAL.
  Moreover, for any given $Π$ and $α$, we can determine the complexity class automatically, with an efficient (centralized, sequential) meta-algorithm, and we can also efficiently synthesize an asymptotically optimal distributed algorithm.
  Before this work, similar results were only known for local search problems (e.g., locally checkable labeling problems). The family of local optimization problems is a strict generalization of local search problems, and it contains numerous commonly studied distributed tasks, such as the problems of finding approximations of the maximum independent set, minimum vertex cover, minimum dominating set, and minimum vertex coloring.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Abstractive Red-Teaming of Language Model Character](https://arxiv.org/abs/2602.12318)
*Nate Rahn,Allison Qi,Avery Griffin,Jonathan Michala,Henry Sleight,Erik Jones*

Main category: cs.LG

TL;DR: 本文提出了一种抽象红队方法，用于识别可能在部署时导致语言模型违反角色规范的自然语言查询类别。通过两种算法搜索这些类别，并在多个模型上验证了其有效性，结果表明该方法能有效发现潜在的问题领域，有助于提高语言模型的角色一致性。


<details>
  <summary>Details</summary>
Motivation: 为了确保语言模型助手遵循特定的角色规范，在大规模部署过程中减少不合规行为的发生，研究者希望通过远低于实际部署所需计算量的方法来提前识别可能导致违规行为的查询类型。

Method: 引入了抽象红队概念，寻找能够频繁引发违规行为的自然语言查询类别。提出了两种基于奖励模型的有效类别搜索算法：一种是针对类别生成器LLM进行强化学习；另一种则是利用强大的LLM从高分查询中迭代合成类别。

Result: 实验结果显示，所提出的算法在12项原则的角色规范和7个目标模型上均优于基线方法，并且生成了一些有趣的查询类别示例。比如，当要求Llama-3.1-8B-Instruct预测未来时，它会回答AI将统治人类；而向GPT-4.1-Mini询问监狱生存必需品时，则会热情推荐非法武器。

Conclusion: 本研究表明，通过使用抽象红队技术可以在语言模型部署前有效地对其角色一致性进行审计，从而帮助开发者更好地理解和改善模型的行为表现。

Abstract: We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. "The query is in Chinese. The query asks about family roles," that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries. Across a 12-principle character specification and 7 target models, we find that our algorithms consistently outperform baselines, and generate qualitatively interesting categories; for example, queries which ask Llama-3.1-8B-Instruct to predict the future lead to responses saying that AI will dominate humanity, and queries that ask GPT-4.1-Mini for essential prison survival items lead to enthusiastic recommendation of illegal weapons. Overall, we believe our results represent an important step towards realistic pre-deployment auditing of language model character.

</details>


### [26] [The Appeal and Reality of Recycling LoRAs with Adaptive Merging](https://arxiv.org/abs/2602.12323)
*Haokun Liu,Gyung Hyun Je,Marco Ciccone,Zhenlin Xu,Prasanth YSS,Colin Raffel*

Main category: cs.LG

TL;DR: 本研究探索了从Hugging Face Hub等模型库中回收利用LoRA模块的方法，发现自适应融合方法虽然可以提高基础模型的性能，但与使用相同数据训练新的LoRA相比优势有限。同时，研究指出，所选用于融合的具体LoRA的重要性不大，随机初始化参数值的LoRA也能达到相似效果，暗示这种做法可能主要通过某种正则化效应起作用而非实现跨任务间的正面迁移。


<details>
  <summary>Details</summary>
Motivation: 鉴于针对开放预训练模型微调的LoRA模块广泛可用，研究者们对能够自适应地合并这些LoRA以进一步提升表现的方法产生了兴趣。尽管已有方法在某些场景下展示了改进，但尚未有人尝试重用来自如Hugging Face Hub这样的平台上的‘野生’LoRA。本研究旨在填补这一空白，考察从大量用户贡献的LoRA中回收利用的可能性及其效果。

Method: 研究选取了近1000个基于Llama 3.1 8B-Instruct语言模型训练而成的用户贡献LoRA作为回收池，并通过一系列实验对比了多种自适应与非自适应的LoRA合并方法，其中包括一种通过广泛搜索方法论设计空间而新开发的技术。

Result: 结果显示，虽然自适应融合方法确实能相对基础模型有所改进，但相较于直接使用同样数据来调整系数的新LoRA训练来说，其带来的额外好处较为有限。更重要的是，无论选择哪组LoRA进行合并，甚至当参数被随机初始化时，性能表现都相差无几。这表明，采用回收LoRA的自适应融合可能主要是通过类似正则化的效果发挥作用。

Conclusion: 研究结论认为，自适应地从回收的LoRA中进行合并主要通过类似于正则化的作用提高了性能，而不是因为实现了积极的任务间转移。只有当池中有高度相关的LoRA时，才观察到了积极的转移效应。

Abstract: The widespread availability of fine-tuned LoRA modules for open pre-trained models has led to an interest in methods that can adaptively merge LoRAs to improve performance. These methods typically include some way of selecting LoRAs from a pool and tune merging coefficients based on a task-specific dataset. While adaptive merging methods have demonstrated improvements in some settings, no past work has attempted to recycle LoRAs found "in the wild" on model repositories like the Hugging Face Hub. To address this gap, we consider recycling from a pool of nearly 1,000 user-contributed LoRAs trained from the Llama 3.1 8B-Instruct language model. Our empirical study includes a range of adaptive and non-adaptive merging methods in addition to a new method designed via a wide search over the methodological design space. We demonstrate that adaptive merging methods can improve performance over the base model but provide limited benefit over training a new LoRA on the same data used to set merging coefficients. We additionally find not only that the specific choice of LoRAs to merge has little importance, but that using LoRAs with randomly initialized parameter values yields similar performance. This raises the possibility that adaptive merging from recycled LoRAs primarily works via some kind of regularization effect, rather than by enabling positive cross-task transfer. To better understand why past work has proven successful, we confirm that positive transfer is indeed possible when there are highly relevant LoRAs in the pool. We release the model checkpoints and code online.

</details>


### [27] [Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis](https://arxiv.org/abs/2602.12373)
*Yijun Ma,Zehong Wang,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Nitesh Chawla,Yanfang Ye*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Policy4OOD的知识引导时空世界模型，用于预测阿片类药物的未来结果、进行反事实分析以及优化政策干预。通过整合政策知识图谱、州级空间依赖性和社会经济时间序列数据，Policy4OOD能够提高预测准确性，并为公共卫生决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 阿片类药物危机是美国最严重的公共卫生危机之一，但评估政策干预措施的效果非常困难，因为在一个动态系统中多种政策相互作用，针对一个风险路径可能无意中放大另一个。作者认为有效的阿片类药物政策评估需要具备三个能力：在当前政策下预测未来结果、关于过去不同决定的反事实推理及对候选干预措施的优化，并提议通过建立世界模型来统一这些能力。

Method: 本文介绍了Policy4OOD，一种结合了政策知识图谱、州级空间依赖性及社会经济学时间序列的数据的世界模型，它利用条件转换器预测未来的阿片类药物相关结果。训练完成后，此模型可以作为模拟器使用：预测只需要一次正向传递；反事实分析则是在历史序列中替换不同的政策编码；而政策优化则是通过对学习到的模拟器执行蒙特卡洛树搜索来实现。为了支撑这一框架，研究者构建了一个包含阿片类药物死亡率、社会经济指标和结构化政策编码在内的州级月度数据集（2019-2024年）。

Result: 实验表明，考虑空间依赖性和结构化的政策知识显著提高了预测精度，验证了每个架构组件的有效性以及世界建模对于数据驱动公共健康决策支持的潜力。

Conclusion: Policy4OOD 提供了一种新的方法来处理阿片类药物危机相关的复杂政策评估问题，通过整合多方面的信息，包括政策知识、地理和社会经济因素，从而更准确地预测未来趋势并优化政策建议。

Abstract: The opioid epidemic remains one of the most severe public health crises in the United States, yet evaluating policy interventions before implementation is difficult: multiple policies interact within a dynamic system where targeting one risk pathway may inadvertently amplify another. We argue that effective opioid policy evaluation requires three capabilities -- forecasting future outcomes under current policies, counterfactual reasoning about alternative past decisions, and optimization over candidate interventions -- and propose to unify them through world modeling. We introduce Policy4OOD, a knowledge-guided spatio-temporal world model that addresses three core challenges: what policies prescribe, where effects manifest, and when effects unfold.Policy4OOD jointly encodes policy knowledge graphs, state-level spatial dependencies, and socioeconomic time series into a policy-conditioned Transformer that forecasts future opioid outcomes.Once trained, the world model serves as a simulator: forecasting requires only a forward pass, counterfactual analysis substitutes alternative policy encodings in the historical sequence, and policy optimization employs Monte Carlo Tree Search over the learned simulator. To support this framework, we construct a state-level monthly dataset (2019--2024) integrating opioid mortality, socioeconomic indicators, and structured policy encodings. Experiments demonstrate that spatial dependencies and structured policy knowledge significantly improve forecasting accuracy, validating each architectural component and the potential of world modeling for data-driven public health decision support.

</details>


### [28] [High-dimensional Level Set Estimation with Trust Regions and Double Acquisition Functions](https://arxiv.org/abs/2602.12391)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta*

Main category: cs.LG

TL;DR: 提出了一种名为TRLSE的新算法，旨在解决高维空间中的水平集估计问题。该算法通过全局和局部两个层次的获取函数来识别并细化阈值边界附近的区域。理论分析与实验证明了TRLSE在样本效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，水平集估计（LSE）是一个基本问题，特别是在主动学习场景下，当初始数据有限时，需要迭代地获取信息点以构建准确的分类器。随着维度增加，搜索体积呈指数增长，这使得在高维空间中解决问题变得困难。

Method: TRLSE算法采用双重获取函数策略，在全局层面和局部层面上同时工作，以识别并细化靠近阈值边界的区域。

Result: 通过广泛的评估表明，在多个合成及真实世界的LSE问题上，TRLSE相较于现有方法展现出更优秀的样本效率。此外，还提供了关于TRLSE准确性方面的理论分析。

Conclusion: TRLSE为处理高维空间内的水平集估计提供了一个有效的方法，通过其独特的双重获取函数设计，不仅提高了样本效率，而且在理论与实践两方面都显示出了优越性。

Abstract: Level set estimation (LSE) classifies whether an unknown function's value exceeds a specified threshold for given inputs, a fundamental problem in many real-world applications. In active learning settings with limited initial data, we aim to iteratively acquire informative points to construct an accurate classifier for this task. In high-dimensional spaces, this becomes challenging where the search volume grows exponentially with increasing dimensionality. We propose TRLSE, an algorithm for high-dimensional LSE, which identifies and refines regions near the threshold boundary with dual acquisition functions operating at both global and local levels. We provide a theoretical analysis of TRLSE's accuracy and show its superior sample efficiency against existing methods through extensive evaluations on multiple synthetic and real-world LSE problems.

</details>


### [29] [Synthetic Interaction Data for Scalable Personalization in Large Language Models](https://arxiv.org/abs/2602.12394)
*Yuchen Ma,Yue Huang,Wenjie Wang,Xiaonan Luo,Xiangliang Zhang,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文介绍了一种名为PersonaGym的高质量合成数据生成框架，用于克服个性化提示优化的数据限制。基于此，发布了PersonaAtlas数据集，并提出了Personalized Prompt Optimization (PPOpt)框架，该框架能够在不修改已部署语言模型的前提下根据用户交互历史优化提示，展示了在任务性能、个性化质量和对噪声及稀疏偏好信号鲁棒性方面的显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有针对大型语言模型（LLMs）的提示优化方法主要集中在任务级别，而忽视了用户的特定偏好和个人潜在约束。这一差距主要是由于缺乏能够大规模捕捉个性化用户-LLM互动的高质量隐私敏感数据以及缺乏个人偏好的稳健奖励信号。

Method: 开发了一个高保真合成数据生成框架PersonaGym来模拟真实的偏好行为和语义感知噪声，生成个性化的多轮互动轨迹；基于此创建了PersonaAtlas数据集；提出了一种可扩展且与模型无关的Personalized Prompt Optimization (PPOpt)框架，通过推理明确的用户档案并据此条件重写提示，结合冷启动监督先验与结果驱动的多目标强化学习来进行训练。

Result: 实验结果显示，在任务表现、个性化质量以及对噪声和稀疏偏好信号的鲁棒性方面，相比最先进基线有持续改进。

Conclusion: 通过引入PersonaGym框架、发布PersonaAtlas数据集及提出PPOpt框架，为解决个性化提示优化中存在的数据局限性提供了有效途径，同时证明了所提方法在提升任务执行效果和个人化体验方面的有效性。

Abstract: Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.

</details>


### [30] [AstRL: Analog and Mixed-Signal Circuit Synthesis with Deep Reinforcement Learning](https://arxiv.org/abs/2602.12402)
*Felicia B. Guo,Ken T. Ho,Andrei Vladimirescu,Borivoje Nikolic*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的模拟和混合信号集成电路设计方法(AstRL)，通过将电路设计视为图生成问题，实现了针对用户指定目标直接优化的电路合成。实验结果表明，在三个实际设计任务中，与最先进基线相比，该方法在传统设计指标上有了显著改进，所有生成的设计都是结构正确的，并且超过90%的设计展示了所需的功能性。


<details>
  <summary>Details</summary>
Motivation: 尽管模拟和混合信号（AMS）集成电路（ICs）在现代计算和通信系统中占据核心地位，但其自动化进展有限。主要挑战在于开发一种能够适用于各种不同、受限且不可微分的电路设计空间的通用优化方法。

Method: 作者们将电路设计作为图生成问题来处理，并引入了一种由深度强化学习驱动的新颖AMS综合方法——AstRL。这种方法基于策略梯度方法，在一个嵌入了仿真器的环境中生成直接针对用户指定目标优化的电路，并在训练过程中提供真实反馈。通过行为克隆及基于判别器的相似性奖励机制，首次展示了一个符合专家标准的一般化电路生成范式。

Result: 对于三个现实世界中的设计任务，所提方法在传统设计指标方面相对于现有最先进的基准显示出显著改善；所有生成的设计均为结构正确，并且超过90%的设计具备所需功能。

Conclusion: 本研究提出的基于深度强化学习的AMS电路设计方法(AstRL)为实现高度表达性和细粒度拓扑生成提供了可能，同时保证了结构一致性和有效性。

Abstract: Analog and mixed-signal (AMS) integrated circuits (ICs) lie at the core of modern computing and communications systems. However, despite the continued rise in design complexity, advances in AMS automation remain limited. This reflects the central challenge in developing a generalized optimization method applicable across diverse circuit design spaces, many of which are distinct, constrained, and non-differentiable. To address this, our work casts circuit design as a graph generation problem and introduces a novel method of AMS synthesis driven by deep reinforcement learning (AstRL). Based on a policy-gradient approach, AstRL generates circuits directly optimized for user-specified targets within a simulator-embedded environment that provides ground-truth feedback during training. Through behavioral-cloning and discriminator-based similarity rewards, our method demonstrates, for the first time, an expert-aligned paradigm for generalized circuit generation validated in simulation. Importantly, the proposed approach operates at the level of individual transistors, enabling highly expressive, fine-grained topology generation. Strong inductive biases encoded in the action space and environment further drive structurally consistent and valid generation. Experimental results for three realistic design tasks illustrate substantial improvements in conventional design metrics over state-of-the-art baselines, with 100% of generated designs being structurally correct and over 90% demonstrating required functionality.

</details>


### [31] [Soft Contamination Means Benchmarks Test Shallow Generalization](https://arxiv.org/abs/2602.12413)
*Ari Spiesberger,Juan J. Vazquez,Nicky Pochinkov,Tomáš Gavenčiak,Peli Grietzer,Gavin Leech,Nandi Schoots*

Main category: cs.LG

TL;DR: 研究发现，如果大型语言模型（LLM）的训练数据中混入了基准测试数据，会导致对分布外（OOD）泛化性能的估计出现偏差。传统的去污染过滤器无法检测语义重复项。通过嵌入Olmo3训练语料库进行的研究表明：1) 污染依然普遍存在；2) 包含基准数据的语义重复项确实能提高基准表现；3) 当在基准数据点的重复项上微调时，来自同一基准但真正保留的数据点上的表现也有所提升。这说明最近基准得分的增长可能既反映了真实能力的改进，也反映了随着训练语料库增长而积累的测试数据的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨当大型语言模型（LLM）的训练数据受到基准测试数据污染时，如何影响其对外部未见数据（即分布外数据，OOD）的真实泛化能力评估，并指出现有去污染方法未能有效识别语义上相同或近似相同的句子问题。

Method: 本研究采用了一种新的方式来检测训练数据中的‘软污染’现象，即通过将Olmo3训练语料库嵌入到向量空间中，从而能够识别出那些虽然在字符串层面不完全相同但在语义上高度相似的数据点。

Result: 研究结果表明，在多个实验中均发现了广泛存在的污染情况，特别是对于CodeForces和ZebraLogic等特定任务，存在大量的语义重复项。此外，研究还显示，利用包含这些语义重复项的数据进行训练可以提高相关基准测试的表现；而且，即使是在与原始基准数据点有显著差异的新数据点上，经过调整后的模型也能表现出更好的性能。

Conclusion: 研究结论指出，近期观察到的基准测试成绩提升可能是由于训练数据集中积累了更多有效的测试数据以及实际能力进步两方面因素共同作用的结果。这意味着当前基于某些基准测试所报告的进步可能部分归因于训练过程中无意间引入了类似测试集的数据。

Abstract: If LLM training data is polluted with benchmark test data, then benchmark performance gives biased estimates of out-of-distribution (OOD) generalization. Typical decontamination filters use n-gram matching which fail to detect semantic duplicates: sentences with equivalent (or near-equivalent) content that are not close in string space. We study this soft contamination of training data by semantic duplicates. Among other experiments, we embed the Olmo3 training corpus and find that: 1) contamination remains widespread, e.g. we find semantic duplicates for 78% of CodeForces and exact duplicates for 50% of ZebraLogic problems; 2) including semantic duplicates of benchmark data in training does improve benchmark performance; and 3) when finetuning on duplicates of benchmark datapoints, performance also improves on truly-held-out datapoints from the same benchmark. We argue that recent benchmark gains are thus confounded: the prevalence of soft contamination means gains reflect both genuine capability improvements and the accumulation of test data and effective test data in growing training corpora.

</details>


### [32] [Stabilizing Native Low-Rank LLM Pretraining](https://arxiv.org/abs/2602.12429)
*Paul Janson,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出了一种名为Spectron的新方法，通过动态限制权重更新的谱范数来解决低秩因子化训练大型语言模型时的不稳定性和损失峰值问题。这种方法允许完全使用低秩因子化权重从头开始训练模型，而不需要先前方法所需的全秩辅助指导。此外，还为原生低秩变换器建立了计算最优的缩放定律，展示了可预测的幂律行为和相对于密集模型改进的推理效率。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型取得了显著的成功，但其参数数量的增长带来了巨大的计算和内存挑战。虽然低秩因子化提供了一种减少训练和推理成本的方法，但目前社区内缺乏一种稳定的方法能够仅使用低秩权重从零开始训练模型，并且性能与全秩模型相匹配。

Method: 作者引入了Spectral renormalization with orthogonalization（Spectron）技术，该技术根据当前因素的谱范数动态地约束结果权重更新，以应对由于权重矩阵更新中最大奇异值无控制增长所导致的训练不稳定性及损失峰值问题。

Result: 实验表明，使用Spectron可以实现稳定、端到端的因子化训练过程，几乎没有任何额外开销。此外，研究还为采用原生低秩结构的变压器确立了计算最优化的比例法则，显示了相对于传统密集型模型更加高效和具有可预测性的功率律行为。

Conclusion: 本研究表明，通过适当的技术手段如Spectron，可以有效地利用低秩因子化技术训练大规模语言模型，不仅解决了训练过程中的稳定性问题，而且在保持模型性能的同时降低了计算需求。

Abstract: Foundation models have achieved remarkable success, yet their growing parameter counts pose significant computational and memory challenges. Low-rank factorization offers a promising route to reduce training and inference costs, but the community lacks a stable recipe for training models from scratch using exclusively low-rank weights while matching the performance of the dense model. We demonstrate that Large Language Models (LLMs) can be trained from scratch using exclusively low-rank factorized weights for all non-embedding matrices without auxiliary "full-rank" guidance required by prior methods. While native low-rank training often suffers from instability and loss spikes, we identify uncontrolled growth in the spectral norm (largest singular value) of the weight matrix update as the dominant factor. To address this, we introduce Spectron: Spectral renormalization with orthogonalization, which dynamically bounds the resultant weight updates based on the current spectral norms of the factors. Our method enables stable, end-to-end factorized training with negligible overhead. Finally, we establish compute-optimal scaling laws for natively low-rank transformers, demonstrating predictable power-law behavior and improved inference efficiency relative to dense models.

</details>


### [33] [Computationally sufficient statistics for Ising models](https://arxiv.org/abs/2602.12449)
*Abhijith Jayakumar,Shreya Shukla,Marc Vuffray,Andrey Y. Lokhov,Sidhant Misra*

Main category: cs.LG

TL;DR: 本文探讨了在仅能观测到有限统计信息的情况下，如何通过计算与观察之间的权衡来学习Gibbs分布的问题。以Ising模型为例，展示了通过观测至多O(γ)阶的统计量可以重建模型参数，并且讨论了当模型结构的先验信息可用时，即使观测能力更有限也能有效解决学习问题。


<details>
  <summary>Details</summary>
Motivation: 对于许多物理系统来说，期望观测到完整的样本配置是不现实的。因此，在只能访问有限统计集的情况下寻找计算上高效的解决方案是非常重要的。

Method: 采用Ising模型作为范例，研究了在给定的观察限制下，利用有限数量的统计数据（最高达O(γ)阶）来重构模型参数的方法。

Result: 证明了通过观测至多O(γ)阶的统计量，可以成功地重建具有特定ℓ1宽度γ的模型参数；此外，在拥有模型结构的部分先验知识情况下，即使观察力更加受限也能够有效地解决问题。

Conclusion: 本文提出了一种新的方法，可以在只使用有限统计信息的情况下学习Gibbs分布，这对于实际应用中的物理系统尤其有用。

Abstract: Learning Gibbs distributions using only sufficient statistics has long been recognized as a computationally hard problem. On the other hand, computationally efficient algorithms for learning Gibbs distributions rely on access to full sample configurations generated from the model. For many systems of interest that arise in physical contexts, expecting a full sample to be observed is not practical, and hence it is important to look for computationally efficient methods that solve the learning problem with access to only a limited set of statistics. We examine the trade-offs between the power of computation and observation within this scenario, employing the Ising model as a paradigmatic example. We demonstrate that it is feasible to reconstruct the model parameters for a model with $\ell_1$ width $γ$ by observing statistics up to an order of $O(γ)$. This approach allows us to infer the model's structure and also learn its couplings and magnetic fields. We also discuss a setting where prior information about structure of the model is available and show that the learning problem can be solved efficiently with even more limited observational power.

</details>


### [34] [Regularized Meta-Learning for Improved Generalization](https://arxiv.org/abs/2602.12469)
*Noor Islam S. Mohammad,Md Muntaqim Meherab*

Main category: cs.LG

TL;DR: 本文提出了一种正则化元学习框架，通过结合冗余感知投影、统计元特征增强和交叉验证正则化元模型来解决深度集成方法中的冗余、不稳定加权及过拟合问题。在Playground Series S6E1基准测试中，该框架表现出优于简单平均和传统Ridge堆叠的效果，并且运行时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 深度集成方法虽然经常提高预测性能，但它们存在三个实际限制：基础模型之间的冗余增加了计算成本并降低了条件性，多重共线性下的权重不稳定，以及元学习管道中的过拟合。为了解决这些问题，提出了一个正则化的元学习框架。

Method: 该框架通过四阶段流水线解决了上述挑战，包括冗余感知投影、统计元特征增强和基于交叉验证的正则化元模型（Ridge, Lasso, 和 ElasticNet）。采用多指标去重策略利用相关性和MSE阈值去除近似共线性的预测因子，同时保持预测多样性。设计了工程集成统计量和交互项来恢复原始预测列无法获得的高阶结构。最后，通过逆RMSE混合阶段减轻正则器选择的方差。

Result: 在Playground Series S6E1基准测试（10万个样本，72个基础模型）上，所提出的框架达到了8.582的折外RMSE，优于简单平均（8.894）和传统的Ridge堆叠（8.627），同时与贪婪爬山法（8.603）相当，但运行时间快了4倍。条件分析显示，在冗余投影后矩阵的有效条件数减少了53.7%。全面消融实验表明，去重、统计元特征和元集成混合均对该框架表现有持续贡献。

Conclusion: 这些结果表明，正则化元学习作为一种稳定且部署效率高的堆叠策略，在高维集成系统中具有显著优势。

Abstract: Deep ensemble methods often improve predictive performance, yet they suffer from three practical limitations: redundancy among base models that inflates computational cost and degrades conditioning, unstable weighting under multicollinearity, and overfitting in meta-learning pipelines. We propose a regularized meta-learning framework that addresses these challenges through a four-stage pipeline combining redundancy-aware projection, statistical meta-feature augmentation, and cross-validated regularized meta-models (Ridge, Lasso, and ElasticNet). Our multi-metric de-duplication strategy removes near-collinear predictors using correlation and MSE thresholds ($τ_{\text{corr}}=0.95$), reducing the effective condition number of the meta-design matrix while preserving predictive diversity. Engineered ensemble statistics and interaction terms recover higher-order structure unavailable to raw prediction columns. A final inverse-RMSE blending stage mitigates regularizer-selection variance. On the Playground Series S6E1 benchmark (100K samples, 72 base models), the proposed framework achieves an out-of-fold RMSE of 8.582, improving over simple averaging (8.894) and conventional Ridge stacking (8.627), while matching greedy hill climbing (8.603) with substantially lower runtime (4 times faster). Conditioning analysis shows a 53.7\% reduction in effective matrix condition number after redundancy projection. Comprehensive ablations demonstrate consistent contributions from de-duplication, statistical meta-features, and meta-ensemble blending. These results position regularized meta-learning as a stable and deployment-efficient stacking strategy for high-dimensional ensemble systems.

</details>


### [35] [Designing RNAs with Language Models](https://arxiv.org/abs/2602.12470)
*Milan Gautam,Ning Dai,Tianshuo Zhou,Bowen Xie,David Mathews,Liang Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件序列生成的RNA设计方法，通过自回归语言模型将目标结构直接映射到序列，并结合监督学习和强化学习优化最终指标。实验表明，该方法在关键指标上优于现有最先进系统，同时速度提高了1.7倍。


<details>
  <summary>Details</summary>
Motivation: RNA设计任务是寻找能够折叠成目标二级结构的序列，对生物学和生物医学有着广泛的影响，但由于序列空间巨大且存在指数级的竞争性折叠，计算上具有挑战性。传统的方法将其视为一个优化问题，依赖于特定实例的启发式或基于约束的搜索。为了克服这些问题，本文提出了一个新的框架，即把RNA设计看作是条件序列生成的问题。

Method: 本文引入了一个可重用的神经近似器，具体实现为一个自回归语言模型（LM），用于将目标结构直接映射到序列。首先，在由随机诱导产生的结构-序列对上以监督方式训练模型；然后利用强化学习（RL）来优化端到端度量。此外，还提出了一种选择小部分样本进行RL的方法，这大大提高了RL的效率和质量。

Result: 在四个数据集上的实验结果表明，所提出的方法在Boltzmann概率等关键指标上超过了最先进的系统，同时处理速度提升了1.7倍。

Conclusion: 本文介绍了一种新的RNA设计方法，通过条件语言模型生成的方式提供了比针对每个实例优化的传统方法更可扩展、任务无关的选择。这种方法不仅在性能上有所提升，还在效率上有显著改进。

Abstract: RNA design, the task of finding a sequence that folds into a target secondary structure, has broad biological and biomedical impact but remains computationally challenging due to the exponentially large sequence space and exponentially many competing folds. Traditional approaches treat it as an optimization problem, relying on per-instance heuristics or constraint-based search. We instead reframe RNA design as conditional sequence generation and introduce a reusable neural approximator, instantiated as an autoregressive language model (LM), that maps target structures directly to sequences. We first train our model in a supervised setting on random-induced structure-sequence pairs, and then use reinforcement learning (RL) to optimize end-to-end metrics. We also propose methods to select a small subset for RL that greatly improves RL efficiency and quality. Across four datasets, our approach outperforms state-of-the-art systems on key metrics such as Boltzmann probability while being 1.7x faster, establishing conditional LM generation as a scalable, task-agnostic alternative to per-instance optimization for RNA design. Our code and data are available at https://github.com/KuNyaa/RNA-Design-LM.

</details>


### [36] [Tight Bounds for Logistic Regression with Large Stepsize Gradient Descent in Low Dimension](https://arxiv.org/abs/2602.12471)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: 本研究对二维数据下梯度下降法训练线性模型进行二分类问题进行了更精细的分析，证明了当学习率足够大时，梯度下降可以在一定迭代次数后找到具有特定损失上界的点，并且给出了从不稳定阶段到稳定阶段所需时间的上下界。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于优化具有可分离数据的二分类问题中使用梯度下降最小化logistic损失函数的过程。先前的研究表明，通过选择较大的步长可以获得加速的收敛速度，尽管会导致损失函数的非单调性。本文旨在对二维情况下的梯度下降提供一个更紧致的分析。

Method: 通过对GD在与最大间隔分类器正交的子空间中的振荡动力学进行细致分析，作者们展示了当学习率足够大时，梯度下降能够在达到某一迭代次数后，找到损失小于某个特定值的解。此外，他们还为GD从不稳定状态（非单调损失）转变为稳定状态（单调损失）所需的时间提供了上下界估计。

Result: 研究表明，在给定的学习率和足够的迭代次数条件下，梯度下降能够找到一个小于$\mathcal{O}(1/(ηT))$损失的解。同时，对于从非单调到单调转变所需时间$τ$，也给出了相匹配的上下界。

Conclusion: 该工作为二维数据集上的梯度下降算法提供了更精确的理解，特别是在处理非单调损失行为方面。它不仅改进了已有结果，而且通过紧密的界限分析进一步阐明了算法的行为。

Abstract: We consider the optimization problem of minimizing the logistic loss with gradient descent to train a linear model for binary classification with separable data. With a budget of $T$ iterations, it was recently shown that an accelerated $1/T^2$ rate is possible by choosing a large step size $η= Θ(γ^2 T)$ (where $γ$ is the dataset's margin) despite the resulting non-monotonicity of the loss. In this paper, we provide a tighter analysis of gradient descent for this problem when the data is two-dimensional: we show that GD with a sufficiently large learning rate $η$ finds a point with loss smaller than $\mathcal{O}(1/(ηT))$, as long as $T \geq Ω(n/γ+ 1/γ^2)$, where $n$ is the dataset size. Our improved rate comes from a tighter bound on the time $τ$ that it takes for GD to transition from unstable (non-monotonic loss) to stable (monotonic loss), via a fine-grained analysis of the oscillatory dynamics of GD in the subspace orthogonal to the max-margin classifier. We also provide a lower bound of $τ$ matching our upper bound up to logarithmic factors, showing that our analysis is tight.

</details>


### [37] [On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs](https://arxiv.org/abs/2602.12506)
*Rosie Zhao,Anshul Shah,Xiaoyu Zhu,Xinke Deng,Zhongyu Jiang,Yang Yang,Joerg Liebelt,Arnab Mondal*

Main category: cs.LG

TL;DR: 尽管RL微调可以提高视觉语言模型在基准测试中的准确性，但它也可能导致对文本线索的过度依赖、视觉基础薄弱以及产生幻觉等问题。简单的文本扰动会显著降低模型的鲁棒性和置信度。研究还揭示了准确性与忠实性之间的权衡问题，并建议在训练和评估过程中同时强调正确性、鲁棒性和推理的忠实性。


<details>
  <summary>Details</summary>
Motivation: 增强大型语言模型（LLMs）在推理密集型任务上的表现激发了将强化学习（RL）微调技术扩展到视觉语言模型（VLMs）。然而，尽管RL微调后的VLMs在视觉推理基准上有所改进，但它们仍然容易受到弱视觉基础、产生幻觉及过度依赖文本提示的影响。

Method: 通过引入误导性的字幕或错误的思维链（CoT）轨迹等受控文本扰动来测试模型的反应，并使用基于熵的度量来展示这些扰动如何重塑模型不确定性。此外，分析了RL微调动态，揭示了准确性和忠实性之间的权衡关系，并探讨了对抗性增强与忠实性感知奖励相结合的方法对于改善模型鲁棒性和一致性的作用。

Result: 简单文本扰动会导致模型鲁棒性和置信度大幅下降；RL微调虽然提高了基准准确性，但也可能侵蚀伴随的CoT及其对上下文变化的鲁棒性；对抗性增强有助于提高鲁棒性，但单独使用并不能防止忠实性漂移；结合忠实性感知奖励可以在一定程度上恢复答案与推理之间的一致性，但在与增强方法配对时，训练存在陷入捷径策略的风险，鲁棒性难以实现。

Conclusion: 仅关注准确性的评估存在局限性，需要开发同时强调正确性、鲁棒性和视觉基础推理忠实性的训练和评估协议。

Abstract: Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.

</details>


### [38] [Bench-MFG: A Benchmark Suite for Learning in Stationary Mean Field Games](https://arxiv.org/abs/2602.12517)
*Lorenzo Magnino,Jiacheng Shen,Matthieu Geist,Olivier Pietquin,Mathieu Laurière*

Main category: cs.LG

TL;DR: 本文提出了一套针对平均场博弈（MFGs）的全面基准测试套件Bench-MFG，旨在解决当前多智能体系统算法评估标准缺失的问题。通过引入问题分类、提供典型环境及随机MFG实例生成方法，并对多种学习算法进行基准测试，以期为未来实验比较提供标准化指导。


<details>
  <summary>Details</summary>
Motivation: 目前在平均场博弈与强化学习交叉领域内缺乏一个标准化的评估协议，导致研究人员依赖于定制化且通常过于简单的环境来测试他们的方法。这种碎片化现象使得评价新兴方法的鲁棒性、泛化能力和失效模式变得非常困难。

Method: 作者提出了一个名为Bench-MFG的基准测试套件，专注于离散时间和空间下的静态设置。他们还介绍了一个从无交互游戏到动力学耦合游戏的问题类别分类法，并为此类别的每个部分提供了原型环境。此外，他们开发了MF-Garnets方法用于生成随机MFG实例，以便进行严格的统计测试。

Result: 研究团队对不同环境下的各种学习算法进行了基准测试，其中包括一种新的黑盒方法（MF-PSO），该方法旨在最小化可利用性。基于广泛的实证结果，文章提出了若干指南以期标准化未来的实验对比工作。

Conclusion: 通过推出Bench-MFG及其相关工具集，本研究填补了MFGs领域中对于系统性评估手段的需求空白，促进了该领域内研究成果之间更加公平和有意义的比较。

Abstract: The intersection of Mean Field Games (MFGs) and Reinforcement Learning (RL) has fostered a growing family of algorithms designed to solve large-scale multi-agent systems. However, the field currently lacks a standardized evaluation protocol, forcing researchers to rely on bespoke, isolated, and often simplistic environments. This fragmentation makes it difficult to assess the robustness, generalization, and failure modes of emerging methods. To address this gap, we propose a comprehensive benchmark suite for MFGs (Bench-MFG), focusing on the discrete-time, discrete-space, stationary setting for the sake of clarity. We introduce a taxonomy of problem classes, ranging from no-interaction and monotone games to potential and dynamics-coupled games, and provide prototypical environments for each. Furthermore, we propose MF-Garnets, a method for generating random MFG instances to facilitate rigorous statistical testing. We benchmark a variety of learning algorithms across these environments, including a novel black-box approach (MF-PSO) for exploitability minimization. Based on our extensive empirical results, we propose guidelines to standardize future experimental comparisons. Code available at \href{https://github.com/lorenzomagnino/Bench-MFG}{https://github.com/lorenzomagnino/Bench-MFG}.

</details>


### [39] [Constraint-Rectified Training for Efficient Chain-of-Thought](https://arxiv.org/abs/2602.12526)
*Qinhang Wu,Sen Lin,Ming Zhang,Yingbin Liang,Ness B. Shroff*

Main category: cs.LG

TL;DR: 本文提出了一种基于约束优化的后训练框架CRT，旨在通过最小化推理长度和仅在性能低于参考时校正准确性来有效减少冗余推理，从而在保持答案质量的同时降低令牌使用量。


<details>
  <summary>Details</summary>
Motivation: 尽管长推理轨迹可以提高答案质量和解锁自我纠正等能力，但它们也带来了高昂的推理成本并经常引入冗余步骤（即过度思考）。现有寻求高效推理策略的方法可能面临准确度严重下降以及对超参数过于敏感的问题。

Method: 作者们介绍了一种名为CRT(Constraint-Rectified Training)的原则性后训练框架，该框架基于参照保护下的约束优化，能够为高效推理提供更加稳定且可解释性强的形式。此外，还扩展了两阶段训练方案：首先发现最短可靠的推理模式，然后在学习到的长度预算下细化准确性，防止冗长CoT的再次出现。

Result: 综合评估表明，该框架一致地减少了令牌使用量，同时保持了稳健可靠的答案质量水平。进一步分析显示，CRT不仅通过缩短响应来提高推理效率，而且还能减少内部语言冗余，并自然产生一系列涵盖不同解释长度范围的中间检查点，使得无需重新训练即可对推理冗长度进行细粒度控制。

Conclusion: 本研究提出的CRT方法成功解决了现有方法中遇到的准确度下降及对超参数敏感等问题，为实现高效且高质量的推理提供了新的解决方案。

Abstract: Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), especially when combined with reinforcement learning (RL) based post-training methods. While longer reasoning traces can improve answer quality and unlock abilities such as self-correction, they also incur high inference costs and often introduce redundant steps, known as overthinking. Recent research seeks to develop efficient reasoning strategies that balance reasoning length and accuracy, either through length-aware reward design or prompt-based calibration. However, these heuristic-based approaches may suffer from severe accuracy drop and be very sensitive to hyperparameters. To address these problems, we introduce CRT (Constraint-Rectified Training), a principled post-training framework based on reference-guarded constrained optimization, yielding a more stable and interpretable formulation for efficient reasoning. CRT alternates between minimizing reasoning length and rectifying accuracy only when performance falls below the reference, enabling stable and effective pruning of redundant reasoning. We further extend CRT with a two-stage training scheme that first discovers the shortest reliable reasoning patterns and then refines accuracy under a learnt length budget, preventing the re-emergence of verbose CoT. Our comprehensive evaluation shows that this framework consistently reduces token usage while maintaining answer quality at a robust and reliable level. Further analysis reveals that CRT improves reasoning efficiency not only by shortening responses but also by reducing internal language redundancy, leading to a new evaluation metric. Moreover, CRT-based training naturally yields a sequence of intermediate checkpoints that span a spectrum of explanation lengths while preserving correctness, enabling fine-grained control over reasoning verbosity without retraining.

</details>


### [40] [Analytical Results for Two Exponential Family Distributions in Hierarchical Dirichlet Processes](https://arxiv.org/abs/2602.12527)
*Naiqi Li*

Main category: cs.LG

TL;DR: 本文扩展了分层狄利克雷过程（HDP）的应用范围，超出了狄利克雷-多项式设定，并为采用分层贝叶斯非参数模型的研究者提供了实用的解析结果。特别地，研究了指数族中的泊松分布和正态分布，在HDP框架下导出了相应的伽马-泊松以及正态-伽马-正态共轭对的显式闭式表达。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的HDP应用主要集中在狄利克雷-多项式共轭结构上，但该框架本身更为通用，原则上可以容纳广泛的共轭先验-似然对。为了扩展HDP的应用范围并探索指数族内重要成员在HDP框架下的分析结果，特别是对于泊松分布和正态分布，本研究旨在提供更广泛的实用解析解。

Method: 通过数学推导与证明的方法，作者们在HDP构造下针对泊松分布和正态分布分别导出了对应的Gamma-Poisson及Normal-Gamma-Normal共轭对的明确闭式表达式。

Result: 成功获得了HDP框架下Gamma-Poisson与Normal-Gamma-Normal共轭对的具体闭式解，这些结果清晰展示了底层数学结构，并展示了如何系统地利用共轭性于分层非参数模型中。

Conclusion: 这项工作不仅扩大了HDP的应用范围，还为使用分层贝叶斯非参数方法的研究人员提供了具体的、可操作的分析成果。

Abstract: The Hierarchical Dirichlet Process (HDP) provides a flexible Bayesian nonparametric framework for modeling grouped data with a shared yet unbounded collection of mixture components. While existing applications of the HDP predominantly focus on the Dirichlet-multinomial conjugate structure, the framework itself is considerably more general and, in principle, accommodates a broad class of conjugate prior-likelihood pairs. In particular, exponential family distributions offer a unified and analytically tractable modeling paradigm that encompasses many commonly used distributions. In this paper, we investigate analytic results for two important members of the exponential family within the HDP framework: the Poisson distribution and the normal distribution. We derive explicit closed-form expressions for the corresponding Gamma-Poisson and Normal-Gamma-Normal conjugate pairs under the hierarchical Dirichlet process construction. Detailed derivations and proofs are provided to clarify the underlying mathematical structure and to demonstrate how conjugacy can be systematically exploited in hierarchical nonparametric models. Our work extends the applicability of the HDP beyond the Dirichlet-multinomial setting and furnishes practical analytic results for researchers employing hierarchical Bayesian nonparametrics.

</details>


### [41] [Flow-Factory: A Unified Framework for Reinforcement Learning in Flow-Matching Models](https://arxiv.org/abs/2602.12529)
*Bowen Ping,Chengyou Jia,Minnan Luo,Hangwei Qian,Ivor Tsang*

Main category: cs.LG

TL;DR: Flow-Factory is a unified, modular framework that simplifies the integration of new algorithms and models for reinforcement learning, supporting multiple reward systems and offering production-ready features.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges faced by practitioners in reinforcement learning, such as fragmented codebases, model-specific implementations, and engineering complexity. The authors aim to provide a solution that decouples algorithms, models, and rewards, allowing for more seamless integration and rapid prototyping of new innovations.

Method: The method introduced in this paper is called Flow-Factory, which adopts a modular, registry-based architecture. This design allows for the easy addition of new algorithms, architectures, and support across different models including GRPO, DiffusionNFT, and AWM, as well as compatibility with Flux, Qwen-Image, and WAN video models. It also provides memory optimization, flexible multi-reward training, and distributed training capabilities.

Result: The result of implementing Flow-Factory is a reduction in implementation overhead, enabling researchers to prototype and scale their innovations more efficiently. It supports various reinforcement learning approaches and provides a streamlined, production-ready environment for both research and practical applications.

Conclusion: In conclusion, Flow-Factory offers a comprehensive solution for integrating and experimenting with reinforcement learning algorithms and models. By providing a unified and modular framework, it addresses key challenges in the field, thereby facilitating faster innovation and easier scaling of new ideas.

Abstract: Reinforcement learning has emerged as a promising paradigm for aligning diffusion and flow-matching models with human preferences, yet practitioners face fragmented codebases, model-specific implementations, and engineering complexity. We introduce Flow-Factory, a unified framework that decouples algorithms, models, and rewards through through a modular, registry-based architecture. This design enables seamless integration of new algorithms and architectures, as demonstrated by our support for GRPO, DiffusionNFT, and AWM across Flux, Qwen-Image, and WAN video models. By minimizing implementation overhead, Flow-Factory empowers researchers to rapidly prototype and scale future innovations with ease. Flow-Factory provides production-ready memory optimization, flexible multi-reward training, and seamless distributed training support. The codebase is available at https://github.com/X-GenGroup/Flow-Factory.

</details>


### [42] [AMPS: Adaptive Modality Preference Steering via Functional Entropy](https://arxiv.org/abs/2602.12533)
*Zihan Huang,Xintong Li,Rohan Surana,Tong Yu,Rui Wang,Julian McAuley,Jingbo Shang,Junda Wu*

Main category: cs.LG

TL;DR: 本文提出了一种实例感知的诊断度量和可学习模块来控制多模态大型语言模型（MLLMs）中的模态偏好问题，实验结果表明该方法在调整模态偏好方面优于传统方法，同时保持了较低的生成错误率。


<details>
  <summary>Details</summary>
Motivation: 针对多模态大型语言模型中存在的显著模态偏好问题，现有解决方案通常采用统一的调节强度，但这种方法要么因为调节过强而损害标准推理增加错误率，要么因为调节太弱而效果不佳。而且，由于不同多模态实例对调节敏感度差异大，单一全局强度难以校准。因此，作者们旨在开发一种能够最小化对推理干扰的同时解决上述限制的新方法。

Method: 研究者们引入了一种实例感知的诊断指标，用于量化每个模态的信息贡献，并揭示样本特定的对调节敏感性。基于这些见解，他们提出了一种缩放策略，减少对敏感样本的调节，并设计了一个可学习模块来推断缩放模式，从而实现对模态偏好的实例级控制。

Result: 实验结果显示，提出的实例感知调节方法在调节多模态偏好方面优于传统的调节方式，能够在有效调整模态偏好的同时维持较低的生成错误率。

Conclusion: 通过引入新的实例感知度量及相应控制机制，本研究为改进多模态大型语言模型中模态偏好问题提供了有效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) often exhibit significant modality preference, which is a tendency to favor one modality over another. Depending on the input, they may over-rely on linguistic priors relative to visual evidence, or conversely over-attend to visually salient but facts in textual contexts. Prior work has applied a uniform steering intensity to adjust the modality preference of MLLMs. However, strong steering can impair standard inference and increase error rates, whereas weak steering is often ineffective. In addition, because steering sensitivity varies substantially across multimodal instances, a single global strength is difficult to calibrate. To address this limitation with minimal disruption to inference, we introduce an instance-aware diagnostic metric that quantifies each modality's information contribution and reveals sample-specific susceptibility to steering. Building on these insights, we propose a scaling strategy that reduces steering for sensitive samples and a learnable module that infers scaling patterns, enabling instance-aware control of modality preference. Experimental results show that our instance-aware steering outperforms conventional steering in modulating modality preference, achieving effective adjustment while keeping generation error rates low.

</details>


### [43] [Exploring Accurate and Transparent Domain Adaptation in Predictive Healthcare via Concept-Grounded Orthogonal Inference](https://arxiv.org/abs/2602.12542)
*Pengfei Hu,Chang Lu,Feifan Liu,Yue Ning*

Main category: cs.LG

TL;DR: 提出了一种名为ExtraCare的方法，用于分解患者表示为不变和协变组件，以提高临床事件预测模型的透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在电子健康记录上的临床事件预测中经常遇到性能下降的问题，尤其是在不同数据分布下部署时。尽管领域适应方法可以缓解这种问题，但其“黑匣子”特性限制了在需要透明度来建立信任和安全的临床实践中的广泛应用。

Method: 通过监督这两个组件并在训练过程中强制它们的正交性，我们的模型保留了标签信息，同时暴露了特定于域的变化，从而比大多数特征对齐模型提供更准确的预测。更重要的是，它通过将稀疏潜在维度映射到医学概念并量化它们通过定向消融贡献来提供人类可理解的解释。

Result: ExtraCare在两个真实世界的EHR数据集上进行了评估，跨越多种领域划分设置，展示了优越的表现以及增强的透明度，这从广泛的案例研究中得到证实。

Conclusion: 这项工作不仅提高了基于EHR的临床事件预测模型的准确性，还通过提供清晰的人类可理解解释增强了模型的透明度，促进了该技术在实际医疗场景中的应用。

Abstract: Deep learning models for clinical event prediction on electronic health records (EHR) often suffer performance degradation when deployed under different data distributions. While domain adaptation (DA) methods can mitigate such shifts, its "black-box" nature prevents widespread adoption in clinical practice where transparency is essential for trust and safety. We propose ExtraCare to decompose patient representations into invariant and covariant components. By supervising these two components and enforcing their orthogonality during training, our model preserves label information while exposing domain-specific variation at the same time for more accurate predictions than most feature alignment models. More importantly, it offers human-understandable explanations by mapping sparse latent dimensions to medical concepts and quantifying their contributions via targeted ablations. ExtraCare is evaluated on two real-world EHR datasets across multiple domain partition settings, demonstrating superior performance along with enhanced transparency, as evidenced by its accurate predictions and explanations from extensive case studies.

</details>


### [44] [SD-MoE: Spectral Decomposition for Effective Expert Specialization](https://arxiv.org/abs/2602.12556)
*Ruijun Huang,Fang Dong,Xin Zhang,Hengjie Cao,Zhendong Huang,Anrui Chen,Jixian Zhou,Mengyi Chen,Yifeng Yang,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Chun Zhang,Li Shang*

Main category: cs.LG

TL;DR: 本文从参数和梯度空间的谱角度分析了Mixture-of-Experts (MoE)架构中专家专业化失败的原因，并提出了一种新的方法Spectral-Decoupled MoE (SD-MoE)，该方法在光谱空间中同时分解参数和梯度，从而提高了下游任务的表现，促进了有效的专家专业化，且几乎不增加额外的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的Mixture-of-Experts (MoE)架构旨在通过条件计算来实现大型语言模型中的专家专业化，但在实践中经常遇到问题：一些专家变得功能相似，而另一些则实际上成为了共享专家，这限制了模型的有效容量和性能。研究者们观察到这种现象与人类语料库中普遍存在的低秩结构有关，并试图解决这一问题以提高模型表现。

Method: 作者采用了一种基于谱视角的方法来分析参数和梯度空间，揭示了专家之间在主要谱成分上高度重叠以及主导梯度子空间强对齐的现象。基于这些发现，他们提出了Spectral-Decoupled MoE (SD-MoE)方案，该方案能够在谱空间内同时解耦参数和梯度。

Result: 实验表明，SD-MoE能够显著提升下游任务上的性能，同时有效促进专家间的区分度，且无需大量额外计算资源。此外，这种方法可以轻松集成进包括Qwen和DeepSeek在内的多种现有MoE框架中。

Conclusion: 通过引入Spectral-Decoupled MoE (SD-MoE)方法，研究不仅解决了传统Mixture-of-Experts架构中存在的专家同质化问题，还为改进大规模语言模型提供了新思路。

Abstract: Mixture-of-Experts (MoE) architectures scale Large Language Models via expert specialization induced by conditional computation. In practice, however, expert specialization often fails: some experts become functionally similar, while others functioning as de facto shared experts, limiting the effective capacity and model performance. In this work, we analysis from a spectral perspective on parameter and gradient spaces, uncover that (1) experts share highly overlapping dominant spectral components in their parameters, (2) dominant gradient subspaces are strongly aligned across experts, driven by ubiquitous low-rank structure in human corpus, and (3) gating mechanisms preferentially route inputs along these dominant directions, further limiting specialization. To address this, we propose Spectral-Decoupled MoE (SD-MoE), which decomposes both parameter and gradient in the spectral space. SD-MoE improves performance across downstream tasks, enables effective expert specialization, incurring minimal additional computation, and can be seamlessly integrated into a wide range of existing MoE architectures, including Qwen and DeepSeek.

</details>


### [45] [Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling](https://arxiv.org/abs/2602.12567)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为FO-RI-FedAvg的新方法，通过自适应粗糙度感知近端正则化和非整数阶局部优化两种互补的客户端机制来提高联邦学习在连接电动汽车上的稳定性。实验表明，该方法相比其他联邦学习基线方法，在减少客户端参与的情况下仍能实现更高的准确性和更稳定的收敛性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在连接的电动汽车（BEVs）上面临严重的不稳定性问题，主要由于间歇性的连接、随时间变化的客户端参与以及由不同操作条件引起的显著客户端间差异。传统FedAvg及许多先进方法在这种实际约束下可能会遭受过度漂移和收敛性能下降的问题。

Method: 提出了Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg)方法，作为FedAvg的一个轻量级模块化扩展。该方法通过两种互补的客户端侧机制来提升稳定性：(i) 自适应粗糙度感知近端正则化，根据局部损失景观的粗糙程度动态调整向全局模型拉力；(ii) 非整数阶局部优化，利用短期记忆平滑冲突更新方向。

Result: 在两个真实世界BEV能量预测数据集VED及其扩展版本eVED上进行实验，结果显示FO-RI-FedAvg相比强大的联邦学习基线方法能够达到更高的精度和更加稳定的收敛性，尤其是在客户端参与度降低的情况下表现更为突出。

Conclusion: FO-RI-FedAvg提供了一个有效解决联邦学习中因连接不稳定等因素导致性能下降问题的方案，通过引入新颖的客户端侧机制提高了算法对于恶劣环境下的鲁棒性和有效性。

Abstract: Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.

</details>


### [46] [VI-CuRL: Stabilizing Verifier-Independent RL Reasoning via Confidence-Guided Variance Reduction](https://arxiv.org/abs/2602.12579)
*Xin-Qiang Cai,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种不依赖外部验证器的课程强化学习框架VI-CuRL，通过优先考虑高置信度样本有效管理偏差-方差权衡，减少动作和问题方差。理论分析证明了估计量的渐近无偏性，并在六个具有挑战性的基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的基于可验证奖励的强化学习（RLVR）虽然能够增强大型语言模型（LLMs）的推理能力，但其对外部验证器的依赖限制了其扩展性。研究发现，RLVR主要通过激发潜在能力来起作用，这促使了无需验证器算法的发展。然而，在这种设置下，标准方法如群体相对策略优化面临着破坏性梯度方差的问题，常常导致训练崩溃。

Method: 为了解决这一问题，本文引入了Verifier-Independent Curriculum Reinforcement Learning (VI-CuRL)框架，该框架利用模型内在的信心水平构建独立于外部验证器的学习课程。通过优先处理高信心样本，VI-CuRL有效地管理了偏差-方差之间的平衡，特别针对降低行动和问题的方差进行了优化。

Result: 提供了严格的理论分析，证明所提出的估计量保证了渐近无偏性。实验结果表明，VI-CuRL促进了训练过程中的稳定性，并且无论是否使用验证器，在六个具有挑战性的基准上持续优于不需要验证器的基础方法。

Conclusion: VI-CuRL作为一种创新的方法，不仅解决了传统RLVR方法中存在的关键挑战，还展示了在提高大规模语言模型推理能力方面的重要潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a dominant paradigm for enhancing Large Language Models (LLMs) reasoning, yet its reliance on external verifiers limits its scalability. Recent findings suggest that RLVR primarily functions by eliciting latent capabilities, motivating the development of verifier-free algorithms. However, in such settings, standard methods like Group Relative Policy Optimization face a critical challenge: destructive gradient variance that often leads to training collapse. To address this issue, we introduceVerifier-Independent Curriculum Reinforcement Learning (VI-CuRL), a framework that leverages the model's intrinsic confidence to construct a curriculum independent from external verifiers. By prioritizing high-confidence samples, VI-CuRL effectively manages the bias-variance trade-off, specifically targeting the reduction of action and problem variance. We provide a rigorous theoretical analysis, proving that our estimator guarantees asymptotic unbiasedness. Empirically, VI-CuRL promotes stability and consistently outperforms verifier-independent baselines across six challenging benchmarks with/without verifiers.

</details>


### [47] [Multi-Head Attention as a Source of Catastrophic Forgetting in MoE Transformers](https://arxiv.org/abs/2602.12587)
*Anrui Chen,Ruijun Huang,Xin Zhang,Fang Dong,Hengjie Cao,Zhendong Huang,Yifeng Yang,Mengyi Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Tun Lu,Fan Yang,Li Shang*

Main category: cs.LG

TL;DR: 本文探讨了Mixture-of-Experts (MoE)架构在持续学习中遇到的问题，特别是由于多头注意力机制导致的预路由瓶颈问题。作者提出了一种新的方法MH-MoE来解决这个问题，通过增加路由粒度和减少组合碰撞来有效缓解遗忘现象。


<details>
  <summary>Details</summary>
Motivation: 尽管Mixture-of-Experts (MoE)架构因其稀疏路由特性被认为适合持续学习，但实际应用中仍然存在显著的知识遗忘问题。研究发现，这是由于多头注意力机制将来自不同头的独特信号合并为单一输入给路由器，从而限制了路由器对单独通道而非特征组合进行操作的能力。

Method: 提出了MH-MoE模型，该模型通过对子表示执行基于头部的路由以提高路由粒度并减少组合碰撞。研究还定义了一个名为\(N_{eff}\)的有效组合数来量化这种碰撞效应，并且发现较高的\(N_{eff}\)与持续训练后旧任务损失增加更大相关联。

Result: 实验结果表明，在TRACE数据集上使用Qwen3-0.6B/8B时，相比LoRAMoE基线（11.2%），MH-MoE能够显著降低回溯权重转移(BWT)，达到4.5%，证明了其在缓解遗忘方面的有效性。

Conclusion: MH-MoE通过改进路由策略成功地解决了由多头注意力机制引起的预路由瓶颈问题，从而提高了MoE Transformer在持续学习场景下的性能，特别是在减轻遗忘方面展示了优越性。

Abstract: Mixture-of-Experts (MoE) architectures are often considered a natural fit for continual learning because sparse routing should localize updates and reduce interference, yet MoE Transformers still forget substantially even with sparse, well-balanced expert utilization. We attribute this gap to a pre-routing bottleneck: multi-head attention concatenates head-specific signals into a single post-attention router input, forcing routing to act on co-occurring feature compositions rather than separable head channels. We show that this router input simultaneously encodes multiple separately decodable semantic and structural factors with uneven head support, and that different feature compositions induce weakly aligned parameter-gradient directions; as a result, routing maps many distinct compositions to the same route. We quantify this collision effect via a route-wise effective composition number $N_{eff}$ and find that higher $N_{eff}$ is associated with larger old-task loss increases after continual training. Motivated by these findings, we propose MH-MoE, which performs head-wise routing over sub-representations to increase routing granularity and reduce composition collisions. On TRACE with Qwen3-0.6B/8B, MH-MoE effectively mitigates forgetting, reducing BWT on Qwen3-0.6B from 11.2% (LoRAMoE) to 4.5%.

</details>


### [48] [Vehicle behaviour estimation for abnormal event detection using distributed fiber optic sensing](https://arxiv.org/abs/2602.12591)
*Hemant Prasad,Daisuke Ikefuji,Shin Tominaga,Hitoshi Sakurai,Manabu Otani*

Main category: cs.LG

TL;DR: 本文提出了一种通过跟踪单个车辆路径和检测道路上的车道变换来识别单车道异常的方法，利用聚类技术估计车辆位置并拟合路径，通过监测参考车辆振动谱心的变化来检测车道变换。实际交通数据评估表明，该方法在检测代表异常存在的车道变换事件上准确率为80%。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式光纤传感系统虽然能有效检测到交通拥堵情况，但对于导致这些拥堵的单车道异常情况的检测仍存在挑战。因此，本研究旨在开发一种新的方法，通过监控车辆变道行为来早期发现可能引起交通阻塞的单车道问题。

Method: 本文采用的方法包括：首先，使用聚类技术估计所有时间点上车辆的位置，并据此拟合出行驶路径；其次，通过追踪高速公路上一辆参照车辆的振动光谱中心变化来监测车道变换。

Result: 通过对真实交通数据进行测试，结果显示所提出的方法对于检测表示存在异常状况的车道变换事件具有80%的准确性。

Conclusion: 本研究提出的一种基于车辆路径跟踪与车道变换检测相结合的方法，为利用现有光纤基础设施实现对可能导致交通拥堵的单车道异常情况进行有效监测提供了新的途径。

Abstract: The distributed fiber-optic sensing (DFOS) system is a cost-effective wide-area traffic monitoring technology that utilizes existing fiber infrastructure to effectively detect traffic congestions. However, detecting single-lane abnormalities, that lead to congestions, is still a challenge. These single-lane abnormalities can be detected by monitoring lane change behaviour of vehicles, performed to avoid congestion along the monitoring section of a road. This paper presents a method to detect single-lane abnormalities by tracking individual vehicle paths and detecting vehicle lane changes along a section of a road. We propose a method to estimate the vehicle position at all time instances and fit a path using clustering techniques. We detect vehicle lane change by monitoring any change in spectral centroid of vehicle vibrations by tracking a reference vehicle along a highway. The evaluation of our proposed method with real traffic data showed 80% accuracy for lane change detection events that represent presence of abnormalities.

</details>


### [49] [HyperMLP: An Integrated Perspective for Sequence Modeling](https://arxiv.org/abs/2602.12601)
*Jiecheng Lu,Shihao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的视角来看待自注意力机制，将其视为一个动态的两层MLP，其权重由上下文历史实例化。基于这种观点，引入了HyperMLP和HyperGLU模型，它们能够在特征空间和序列空间中学习动态混合，并且在相同的参数预算下，这些模型一致优于强softmax-attention基线。


<details>
  <summary>Details</summary>
Motivation: 文章动机在于提供一种更简单、更统一的观点来理解自注意力机制，即把自回归注意头看作是一个从上下文历史中实例化的动态两层多层感知器(MLP)。这样的视角允许标准的MLP激活函数如ReLU或GLU自然地实现对依赖于上下文的记忆池而非概率分布的输入条件选择。

Method: 通过将自注意力解释为动态生成的两层MLP，作者们提出了HyperMLP与HyperGLU，这两种方法能够根据输入条件在特征空间及序列空间中学习动态混合。特别地，采用反向偏移（滞后）布局以使时间混合符合自回归语义。

Result: 研究表明，在相同的参数预算条件下，HyperMLP和HyperGLU的表现始终优于强大的softmax-attention基线。此外，还提供了关于该结构表达能力和影响的理论表征。

Conclusion: 本研究提供了一个新颖而统一的方式来理解自注意力机制，并通过提出的HyperMLP和HyperGLU证明了这种方法的有效性，表明它们可以作为现有自注意力模型的一种改进方案。

Abstract: Self-attention is often viewed as probabilistic query-key lookup, motivating designs that preserve normalized attention scores and fixed positional semantics. We advocate a simpler and more unified perspective: an autoregressive attention head can be viewed as a dynamic two-layer MLP whose weights are instantiated from the context history. From this view, attention scores form an ever-growing hidden representation, and standard MLP activations such as ReLU or GLU naturally implement input-conditioned selection over a context-dependent memory pool rather than a probability distribution. Based on this formulation, we introduce HyperMLP and HyperGLU, which learn dynamic mixing in both feature space and sequence space, using a reverse-offset (lag) layout to align temporal mixing with autoregressive semantics. We provide theoretical characterizations of the expressivity and implications of this structure, and empirically show that HyperMLP/HyperGLU consistently outperform strong softmax-attention baselines under matched parameter budgets.

</details>


### [50] [Block-Sample MAC-Bayes Generalization Bounds](https://arxiv.org/abs/2602.12605)
*Matthias Frey,Jingge Zhu,Michael C. Gastpar*

Main category: cs.LG

TL;DR: 本文提出了一种新的MAC-Bayes界限（平均近似正确），与传统的PAC-Bayes界限相比，新界限仅依赖于训练数据的子集，并且在适当选择块大小的情况下提供了更紧致的界限。此外，文章还探讨了是否可能建立高概率版本的MAC-Bayes界限，并给出了否定的答案。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进现有PAC-Bayes和MAC-Bayes界限的紧致性问题，通过引入一种只依赖于训练数据子集的新MAC-Bayes界限来实现这一点。

Method: 作者开发了一系列基于块样本的MAC-Bayes界限，这些界限可以看作是已知PAC-Bayes界限期望形式的一种泛化。此外，通过数值例子展示了即使传统PAC-Bayes界限失效时，所提出的MAC-Bayes界限对于适当的块大小选择也能提供有限的界限值。

Result: 结果表明，提出的MAC-Bayes界限能够显著改善传统PAC-Bayes及MAC-Bayes界限的紧致度。而且，当MAC-Bayes界限以O(n^-1/2)的速度减小时，通常不可能建立一个比O(1/log n)更快消失率的PAC-Bayes界限。

Conclusion: 结论是新提出的MAC-Bayes界限不仅在理论上有所创新，在实际应用中也具有潜力，但同时也指出了一些限制，比如很难同时满足特定条件下PAC-Bayes界限的快速收敛性和对允许误差概率的对数依赖性。

Abstract: We present a family of novel block-sample MAC-Bayes bounds (mean approximately correct). While PAC-Bayes bounds (probably approximately correct) typically give bounds for the generalization error that hold with high probability, MAC-Bayes bounds have a similar form but bound the expected generalization error instead. The family of bounds we propose can be understood as a generalization of an expectation version of known PAC-Bayes bounds. Compared to standard PAC-Bayes bounds, the new bounds contain divergence terms that only depend on subsets (or \emph{blocks}) of the training data. The proposed MAC-Bayes bounds hold the promise of significantly improving upon the tightness of traditional PAC-Bayes and MAC-Bayes bounds. This is illustrated with a simple numerical example in which the original PAC-Bayes bound is vacuous regardless of the choice of prior, while the proposed family of bounds are finite for appropriate choices of the block size. We also explore the question whether high-probability versions of our MAC-Bayes bounds (i.e., PAC-Bayes bounds of a similar form) are possible. We answer this question in the negative with an example that shows that in general, it is not possible to establish a PAC-Bayes bound which (a) vanishes with a rate faster than $\mathcal{O}(1/\log n)$ whenever the proposed MAC-Bayes bound vanishes with rate $\mathcal{O}(n^{-1/2})$ and (b) exhibits a logarithmic dependence on the permitted error probability.

</details>


### [51] [RelBench v2: A Large-Scale Benchmark and Repository for Relational Data](https://arxiv.org/abs/2602.12606)
*Justin Gu,Rishabh Ranjan,Charilaos Kanatsoulis,Haiming Tang,Martin Jurkovic,Valter Hudovernik,Mark Znidar,Pranshu Chaturvedi,Parth Shroff,Fengyu Li,Jure Leskovec*

Main category: cs.LG

TL;DR: 本文介绍了RelBench v2，一个用于关系深度学习(RDL)的基准测试扩展版本。它增加了四个大规模的关系数据集，并引入了新的自动完成任务类型，还整合了外部基准和评估框架，实验结果表明RDL模型在多种任务中优于单表基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着关系深度学习（RDL）的发展，特别是朝向更大的模型和关系基础模型发展时，需要可扩展且现实的基准来支持系统的评估与进步。

Method: 通过增加涵盖学术出版、企业资源规划、消费者平台及临床记录的大规模关系数据集，以及引入要求模型直接在关系表内推断缺失属性值同时考虑时间约束的新类预测目标——自动完成任务，进一步扩展了RelBench至其第二版。此外，还将外部基准和评估框架集成到RelBench v2中。

Result: 实验结果显示，RDL模型在自动完成、预测及推荐任务上持续优于仅使用单一表格的传统基线方法。

Conclusion: 研究强调了明确建模关系结构的重要性，并展示了RelBench v2作为评估RDL系统进展的有效工具。

Abstract: Relational deep learning (RDL) has emerged as a powerful paradigm for learning directly on relational databases by modeling entities and their relationships across multiple interconnected tables. As this paradigm evolves toward larger models and relational foundation models, scalable and realistic benchmarks are essential for enabling systematic evaluation and progress. In this paper, we introduce RelBench v2, a major expansion of the RelBench benchmark for RDL. RelBench v2 adds four large-scale relational datasets spanning scholarly publications, enterprise resource planning, consumer platforms, and clinical records, increasing the benchmark to 11 datasets comprising over 22 million rows across 29 tables. We further introduce autocomplete tasks, a new class of predictive objectives that require models to infer missing attribute values directly within relational tables while respecting temporal constraints, expanding beyond traditional forecasting tasks constructed via SQL queries. In addition, RelBench v2 expands beyond its native datasets by integrating external benchmarks and evaluation frameworks: we translate event streams from the Temporal Graph Benchmark into relational schemas for unified relational-temporal evaluation, interface with ReDeLEx to provide uniform access to 70+ real-world databases suitable for pretraining, and incorporate 4DBInfer datasets and tasks to broaden multi-table prediction coverage. Experimental results demonstrate that RDL models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks, highlighting the importance of modeling relational structure explicitly.

</details>


### [52] [Coden: Efficient Temporal Graph Neural Networks for Continuous Prediction](https://arxiv.org/abs/2602.12613)
*Zulun Zhu,Siqiang Luo*

Main category: cs.LG

TL;DR: 本文介绍了一种新的TGNN模型——Coden，旨在解决动态图上连续预测的问题。Coden能够在保持预测准确性的同时克服现有TGNN在计算复杂度上的瓶颈，并且在五个动态数据集上的评估显示，Coden在效率和效果上均超越了现有的性能基准。


<details>
  <summary>Details</summary>
Motivation: 当前的TGNN主要针对给定时间跨度的一次性预测设计，而很多实际应用需要的是频繁随时间进行的连续预测。直接将现有TGNN应用于连续预测场景会导致显著的计算开销或预测质量下降，尤其是在处理大规模图时。

Method: 提出了一种名为Coden的新型TGNN模型，专门设计用于动态图上的高效有效学习。该模型创新性地解决了现有TGNN中的关键复杂度瓶颈问题，同时保持了可比的预测准确性。此外，文章还提供了理论分析来证明Coden的有效性和效率，并澄清了它与基于RNN及注意力机制模型之间的对偶关系。

Result: 通过在五个动态数据集上进行的实验表明，Coden在效率和效果两方面都超过了现有的性能标准，确立了其作为演变图环境中连续预测的优越解决方案的地位。

Conclusion: Coden是一种为连续预测任务优化设计的TGNN模型，不仅能够有效地处理大规模动态图上的连续预测问题，而且在效率和准确性之间取得了良好的平衡。

Abstract: Temporal Graph Neural Networks (TGNNs) are pivotal in processing dynamic graphs. However, existing TGNNs primarily target one-time predictions for a given temporal span, whereas many practical applications require continuous predictions, that predictions are issued frequently over time. Directly adapting existing TGNNs to continuous-prediction scenarios introduces either significant computational overhead or prediction quality issues especially for large graphs. This paper revisits the challenge of { continuous predictions} in TGNNs, and introduces {\sc Coden}, a TGNN model designed for efficient and effective learning on dynamic graphs. {\sc Coden} innovatively overcomes the key complexity bottleneck in existing TGNNs while preserving comparable predictive accuracy. Moreover, we further provide theoretical analyses that substantiate the effectiveness and efficiency of {\sc Coden}, and clarify its duality relationship with both RNN-based and attention-based models. Our evaluations across five dynamic datasets show that {\sc Coden} surpasses existing performance benchmarks in both efficiency and effectiveness, establishing it as a superior solution for continuous prediction in evolving graph environments.

</details>


### [53] [Formalizing the Sampling Design Space of Diffusion-Based Generative Models via Adaptive Solvers and Wasserstein-Bounded Timesteps](https://arxiv.org/abs/2602.12624)
*Sangwoo Jo,Sungjoon Choi*

Main category: cs.LG

TL;DR: 提出了一种新的框架SDM，通过几何视角重新审视了扩散模型中的求解器选择和调度问题，实现了在不同噪声阶段自适应地使用低阶和高阶求解器，并通过Wasserstein有界优化框架系统地推导出自适应时间步长，从而保证采样过程忠实于基础连续动力学。该方法在多个标准基准上达到了最先进的性能，同时减少了函数评估次数。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在各个领域取得了显著的成绩，但高昂的采样成本限制了它们的实际应用。现有研究主要集中在训练目标或单个求解器上，而对整个采样设计，特别是求解器的选择与调度，依然依赖静态启发式方法。

Method: 提出了SDM框架，从几何角度出发，使数值求解器与扩散轨迹的内在属性相匹配；通过分析ODE动态特性，证明了早期高噪声阶段可以使用高效的低阶求解器，而随着非线性程度增加，可逐步引入高阶求解器；此外，还引入了一个Wasserstein有界的优化框架来正式化调度过程，系统地推导出自适应时间步长，明确约束局部离散化误差。

Result: SDM在标准基准测试中表现出色，包括CIFAR-10上的FID得分为1.93、FFHQ为2.41以及AFHQv2为1.98，且与现有采样器相比，所需的功能评估次数减少。

Conclusion: SDM提供了一个原则性的框架，通过智能选择和调度求解器提高了扩散模型的效率和准确性，无需额外训练或架构调整即可实现最佳性能。

Abstract: Diffusion-based generative models have achieved remarkable performance across various domains, yet their practical deployment is often limited by high sampling costs. While prior work focuses on training objectives or individual solvers, the holistic design of sampling, specifically solver selection and scheduling, remains dominated by static heuristics. In this work, we revisit this challenge through a geometric lens, proposing SDM, a principled framework that aligns the numerical solver with the intrinsic properties of the diffusion trajectory. By analyzing the ODE dynamics, we show that efficient low-order solvers suffice in early high-noise stages while higher-order solvers can be progressively deployed to handle the increasing non-linearity of later stages. Furthermore, we formalize the scheduling by introducing a Wasserstein-bounded optimization framework. This method systematically derives adaptive timesteps that explicitly bound the local discretization error, ensuring the sampling process remains faithful to the underlying continuous dynamics. Without requiring additional training or architectural modifications, SDM achieves state-of-the-art performance across standard benchmarks, including an FID of 1.93 on CIFAR-10, 2.41 on FFHQ, and 1.98 on AFHQv2, with a reduced number of function evaluations compared to existing samplers. Our code is available at https://github.com/aiimaginglab/sdm.

</details>


### [54] [Uncovering spatial tissue domains and cell types in spatial omics through cross-scale profiling of cellular and genomic interactions](https://arxiv.org/abs/2602.12651)
*Rui Yan,Xiaohan Xing,Xun Wang,Zixia Zhou,Md Tauhidul Islam,Lei Xing*

Main category: cs.LG

TL;DR: 本文介绍了一种名为CellScape的深度学习框架，用于解决空间转录组学数据噪音大、结构复杂的问题，能够有效捕捉细胞间空间交互与基因组内在关系之间的相互作用，从而改进了空间域分割并支持跨多种转录组数据集的空间细胞分析。


<details>
  <summary>Details</summary>
Motivation: 由于空间转录组学数据固有的噪声性、大量性和结构性复杂性，现有的计算方法难以有效地捕捉到空间交互与基因组内在关系之间的联系，限制了我们识别关键生物模式的能力。因此，需要一种新的方法来克服这些限制。

Method: 提出了一种名为CellScape的深度学习框架，该框架旨在同时建模组织空间中的细胞相互作用以及细胞间的基因组关系，生成将空间信号与潜在基因调控机制无缝集成的全面表示。

Result: 通过使用CellScape，研究人员能够发现具有生物学信息价值的模式，这不仅改善了空间域分割，还促进了对不同转录组数据集进行详尽的空间细胞分析。

Conclusion: CellScape提供了一个准确且多功能的框架，适用于深入分析和解释空间转录组学数据，为理解细胞在特定微环境下的身份和功能提供了强有力的支持。

Abstract: Cellular identity and function are linked to both their intrinsic genomic makeup and extrinsic spatial context within the tissue microenvironment. Spatial transcriptomics (ST) offers an unprecedented opportunity to study this, providing in situ gene expression profiles at single-cell resolution and illuminating the spatial and functional organization of cells within tissues. However, a significant hurdle remains: ST data is inherently noisy, large, and structurally complex. This complexity makes it intractable for existing computational methods to effectively capture the interplay between spatial interactions and intrinsic genomic relationships, thus limiting our ability to discern critical biological patterns. Here, we present CellScape, a deep learning framework designed to overcome these limitations for high-performance ST data analysis and pattern discovery. CellScape jointly models cellular interactions in tissue space and genomic relationships among cells, producing comprehensive representations that seamlessly integrate spatial signals with underlying gene regulatory mechanisms. This technique uncovers biologically informative patterns that improve spatial domain segmentation and supports comprehensive spatial cellular analyses across diverse transcriptomics datasets, offering an accurate and versatile framework for deep analysis and interpretation of ST data.w

</details>


### [55] [SLA2: Sparse-Linear Attention with Learnable Routing and QAT](https://arxiv.org/abs/2602.12675)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Kaiwen Zheng,Youhe Jiang,Ion Stoica,Jianfei Chen,Jun Zhu,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: 本文提出了一种改进的稀疏-线性注意力机制SLA2，通过引入可学习路由器、更直接的稀疏-线性注意力组合方式以及低比特注意力设计，实现了在视频扩散模型中达到97%的注意力稀疏度，并提供了18.6倍的注意力加速，同时保持了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏-线性注意力(SLA)机制依赖于一种基于注意力权重大小的启发式分割方法来分配计算给稀疏或线性分支，这可能不是最优选择；此外，对SLA中的注意力误差进行正式分析后发现，它与直接分解为稀疏和线性注意力之间存在不匹配。

Method: 提出了SLA2，包含三个主要组成部分：(I)一个可学习路由器，用于动态选择每个注意力计算应使用稀疏还是线性注意力；(II)更加忠实且直接地实现稀疏-线性注意力的方法，利用可学习比率结合稀疏与线性注意力分支；(III)稀疏+低比特注意力设计方案，通过量化感知微调引入低比特注意力以减少量化误差。

Result: 实验表明，在视频扩散模型上，SLA2能够实现97%的注意力稀疏度，并提供18.6倍的注意力加速，同时维持生成质量不变。

Conclusion: SLA2通过引入创新的设计显著提高了稀疏-线性注意力机制的效率与性能，为视频生成等任务提供了新的解决方案。

Abstract: Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.

</details>


### [56] [Flow Matching from Viewpoint of Proximal Operators](https://arxiv.org/abs/2602.12683)
*Kenji Fukumizu,Wei Huang,Han Bao,Shuntuo Xu,Nisha Chandramoothy*

Main category: cs.LG

TL;DR: 本文重新表述了OT-CFM模型，通过扩展的Brenier势函数给出了一个精确的邻近形式表达，并且不需要假设目标分布具有密度。此外还讨论了当批次大小增加时迷你批次OT-CFM收敛到总体形式的情况，并证明了对于以流形为支撑的目标，OT-CFM在时间重缩放后展现出终端正常双曲性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进现有的OT-CFM模型，使其能够处理不具有密度的目标分布，并探索其在更广泛条件下的理论性质和实际应用潜力。

Method: 通过对OT-CFM进行重新表述，引入了一个扩展的Brenier势函数来构建模型的精确邻近形式表达。同时，基于凸势函数的第二epi-导数，分析了OT-CFM针对流形支持目标时的动力学特性。

Result: 成功地将OT-CFM模型重新表述为无需假设目标分布具有密度的形式，并发现对于流形支持的目标，该模型表现出特定的动力学行为：在数据流形法线方向上呈指数收缩，而在切线方向上保持中立。

Conclusion: 这项工作不仅拓展了OT-CFM的应用范围，而且对其动力学特性的深入理解有助于未来在相关领域中的进一步研究与发展。

Abstract: We reformulate Optimal Transport Conditional Flow Matching (OT-CFM), a class of dynamical generative models, showing that it admits an exact proximal formulation via an extended Brenier potential, without assuming that the target distribution has a density. In particular, the mapping to recover the target point is exactly given by a proximal operator, which yields an explicit proximal expression of the vector field. We also discuss the convergence of minibatch OT-CFM to the population formulation as the batch size increases. Finally, using second epi-derivatives of convex potentials, we prove that, for manifold-supported targets, OT-CFM is terminally normally hyperbolic: after time rescaling, the dynamics contracts exponentially in directions normal to the data manifold while remaining neutral along tangential directions.

</details>


### [57] [Trust the uncertain teacher: distilling dark knowledge via calibrated uncertainty](https://arxiv.org/abs/2602.12687)
*Jeonghyun Kim,SooKyung Kim,Richeng Xuan,Hyunsoo Cho*

Main category: cs.LG

TL;DR: 本文提出了一种名为Calibrated Uncertainty Distillation (CUD)的新框架，旨在解决传统知识蒸馏中教师模型过度自信的问题。通过直接调整教师模型的预测分布，CUD平衡了准确性和校准度，使学生模型在容易和困难的例子上都能受益。实验表明，采用CUD的学生模型不仅更准确，而且在校准度和处理长尾输入方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统的基于交叉熵训练的教师模型在知识蒸馏过程中往往无法保留细微的概率模式（即“暗知识”），导致其分布变得过于尖锐、自信，实际上却脆弱且对表示层面的知识转移帮助有限。这种过度自信特别影响高基数任务中的性能，并减少了分布偏移下的鲁棒性。

Method: 提出了Calibrated Uncertainty Distillation (CUD)框架，该方法从分布的角度重新审视了知识蒸馏过程。CUD鼓励教师模型在信息量大的地方揭示不确定性，并引导学生模型从经过校准而非单纯增强确定性的目标中学习。

Result: 跨多个基准测试的结果显示，使用CUD方法的学生模型比传统方法更加准确，在分布偏移下有更好的校准度，并且对于模糊及长尾输入也更为可靠。

Conclusion: 通过引入CUD框架，研究有效地解决了传统知识蒸馏中存在的问题，即教师模型过度自信而导致的学生模型性能不佳。此方法提高了学生模型的准确性、校准度以及面对未知或罕见情况时的表现。

Abstract: The core of knowledge distillation lies in transferring the teacher's rich 'dark knowledge'-subtle probabilistic patterns that reveal how classes are related and the distribution of uncertainties. While this idea is well established, teachers trained with conventional cross-entropy often fail to preserve such signals. Their distributions collapse into sharp, overconfident peaks that appear decisive but are in fact brittle, offering little beyond the hard label or subtly hindering representation-level transfer. This overconfidence is especially problematic in high-cardinality tasks, where the nuances among many plausible classes matter most for guiding a compact student. Moreover, such brittle targets reduce robustness under distribution shift, leaving students vulnerable to miscalibration in real-world conditions. To address this limitation, we revisit distillation from a distributional perspective and propose Calibrated Uncertainty Distillation (CUD), a framework designed to make dark knowledge more faithfully accessible. Instead of uncritically adopting the teacher's overconfidence, CUD encourages teachers to reveal uncertainty where it is informative and guides students to learn from targets that are calibrated rather than sharpened certainty. By directly shaping the teacher's predictive distribution before transfer, our approach balances accuracy and calibration, allowing students to benefit from both confident signals on easy cases and structured uncertainty on hard ones. Across diverse benchmarks, CUD yields students that are not only more accurate, but also more calibrated under shift and more reliable on ambiguous, long-tail inputs.

</details>


### [58] [Leverage-Weighted Conformal Prediction](https://arxiv.org/abs/2602.12693)
*Shreyas Fadnavis*

Main category: cs.LG

TL;DR: 提出了一种基于统计杠杆的加权一致性预测方法（LWCP），该方法无需训练辅助模型，通过设计矩阵的几何结构实现自适应性，同时保持了有限样本边际有效性和渐近最优条件覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应方法需要训练辅助模型，而传统的分割一致性预测虽然提供了具有有限样本边缘覆盖范围的无分布预测区间，但在低方差区域过度覆盖，在高方差区域覆盖不足。

Method: 引入了Leverage-Weighted Conformal Prediction (LWCP) 方法，通过一个关于统计杠杆（即帽子矩阵对角线）的函数来加权非一致性分数，从而从设计矩阵的几何结构而不是通过拟合辅助模型来获得自适应性。

Result: 证明了LWCP对于任何权重函数都保持了有限样本的边际有效性；当异方差性通过杠杆因素时，几乎不需要宽度成本就能达到渐进最优条件覆盖率；在高斯假设下恢复经典预测区间的形态和宽度，同时保留了无分布保证。此外，还证实随机杠杆近似可以精确保持覆盖范围，并且控制宽度扰动，而普通的一致性预测存在一个持续的、与样本大小无关的条件覆盖率差距，LWCP消除了这个差距。

Conclusion: 实验结果表明，LWCP在合成数据和真实数据上均能显著减少不同设置下的条件覆盖率差异，而且除了选择权重函数外不需要额外的超参数，计算开销也极小。

Abstract: Split conformal prediction provides distribution-free prediction intervals with finite-sample marginal coverage, but produces constant-width intervals that overcover in low-variance regions and undercover in high-variance regions. Existing adaptive methods require training auxiliary models. We propose Leverage-Weighted Conformal Prediction (LWCP), which weights nonconformity scores by a function of the statistical leverage -- the diagonal of the hat matrix -- deriving adaptivity from the geometry of the design matrix rather than from auxiliary model fitting. We prove that LWCP preserves finite-sample marginal validity for any weight function; achieves asymptotically optimal conditional coverage at essentially no width cost when heteroscedasticity factors through leverage; and recovers the form and width of classical prediction intervals under Gaussian assumptions while retaining distribution-free guarantees. We further establish that randomized leverage approximations preserve coverage exactly with controlled width perturbation, and that vanilla CP suffers a persistent, sample-size-independent conditional coverage gap that LWCP eliminates. The method requires no hyperparameters beyond the choice of weight function and adds negligible computational overhead to vanilla CP. Experiments on synthetic and real data confirm the theoretical predictions, demonstrating substantial reductions in conditional coverage disparity across settings.

</details>


### [59] [SWING: Unlocking Implicit Graph Representations for Graph Random Features](https://arxiv.org/abs/2602.12703)
*Alessandro Manenti,Avinava Dubey,Arijit Sehanobish,Cesare Alippi,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出了一种新的算法SWING，用于处理由隐式表示给出的图上的图随机特征计算。该方法利用连续空间中的游走而非节点间的游走，并通过Gumbel-softmax采样机制和线性化核函数实现高效近似计算。


<details>
  <summary>Details</summary>
Motivation: 针对由隐式表示给出的图（i-graphs）进行图随机特征计算时面临的挑战，如ε-邻域图在机器学习中的应用，提出了一种新型算法来提高此类计算的效率与准确性。

Method: 开发了名为SWING的新算法，该算法基于连续空间中图的嵌入执行游走操作，并采用定制化的Gumbel-softmax采样机制结合随机特征及重要性采样技术以实现对原始组合计算的有效近似。

Result: 理论分析表明SWING能够准确且高效地逼近原始组合计算；实验结果展示了SWING在不同类型的i-graphs上均表现出色。

Conclusion: SWING为处理隐式定义图上的图随机特征提供了加速器友好型解决方案，无需具体化输入图即可完成计算任务。

Abstract: We propose SWING: Space Walks for Implicit Network Graphs, a new class of algorithms for computations involving Graph Random Features on graphs given by implicit representations (i-graphs), where edge-weights are defined as bi-variate functions of feature vectors in the corresponding nodes. Those classes of graphs include several prominent examples, such as: $ε$-neighborhood graphs, used on regular basis in machine learning. Rather than conducting walks on graphs' nodes, those methods rely on walks in continuous spaces, in which those graphs are embedded. To accurately and efficiently approximate original combinatorial calculations, SWING applies customized Gumbel-softmax sampling mechanism with linearized kernels, obtained via random features coupled with importance sampling techniques. This algorithm is of its own interest. SWING relies on the deep connection between implicitly defined graphs and Fourier analysis, presented in this paper. SWING is accelerator-friendly and does not require input graph materialization. We provide detailed analysis of SWING and complement it with thorough experiments on different classes of i-graphs.

</details>


### [60] [Mixture of Predefined Experts: Maximizing Data Usage on Vertical Federated Learning](https://arxiv.org/abs/2602.12708)
*Jon Irureta,Gorka Azkune,Jon Imaz,Aizea Lojo,Javier Fernandez-Marques*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Split-MoPE的新框架，该框架结合了分裂学习与一种特殊的预定义专家混合（MoPE）架构，以解决垂直联邦学习中样本对齐不全的问题。通过使用预训练的编码器针对目标数据领域进行处理，Split-MoPE在单轮通信中达到了最先进的性能，并且对于恶意或噪声参与者具有内在的鲁棒性，同时提供每个样本的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前垂直联邦学习(VFL)方案通常假定所有参与者之间完全样本对齐，但在实际应用中这种假设很少成立。为了克服这一局限，提出了新的方法来最大化利用可用数据，即使在没有完全样本重叠的情况下也能有效运作。

Method: 引入了Split-MoPE框架，它将分裂学习技术与特定设计的Mixture of Predefined Experts (MoPE)相结合。与传统的Mixture of Experts不同，MoPE采用预设的专家来处理特定的数据排列，从而无需动态学习路由。此外，通过利用为特定数据域预先训练好的编码器，Split-MoPE能够在一次通信轮次内达到最佳性能。

Result: 实验结果表明，在视觉(CIFAR-10/100)和表格(Breast Cancer Wisconsin)数据集上，相较于现有最先进系统如LASER和Vertical SplitNN，Split-MoPE表现出了优越的性能，特别是在高数据缺失率等挑战性条件下。

Conclusion: Split-MoPE不仅解决了垂直联邦学习中存在的样本不对齐问题，而且在减少通信开销的同时提高了模型性能。此外，它还提供了对个体贡献度量的能力，增强了系统的透明度与安全性。

Abstract: Vertical Federated Learning (VFL) has emerged as a critical paradigm for collaborative model training in privacy-sensitive domains such as finance and healthcare. However, most existing VFL frameworks rely on the idealized assumption of full sample alignment across participants, a premise that rarely holds in real-world scenarios. To bridge this gap, this work introduces Split-MoPE, a novel framework that integrates Split Learning with a specialized Mixture of Predefined Experts (MoPE) architecture. Unlike standard Mixture of Experts (MoE), where routing is learned dynamically, MoPE uses predefined experts to process specific data alignments, effectively maximizing data usage during both training and inference without requiring full sample overlap. By leveraging pretrained encoders for target data domains, Split-MoPE achieves state-of-the-art performance in a single communication round, significantly reducing the communication footprint compared to multi-round end-to-end training. Furthermore, unlike existing proposals that address sample misalignment, this novel architecture provides inherent robustness against malicious or noisy participants and offers per-sample interpretability by quantifying each collaborator's contribution to each prediction. Extensive evaluations on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets demonstrate that Split-MoPE consistently outperforms state-of-the-art systems such as LASER and Vertical SplitNN, particularly in challenging scenarios with high data missingness.

</details>


### [61] [Adaptive Structured Pruning of Convolutional Neural Networks for Time Series Classification](https://arxiv.org/abs/2602.12744)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: 提出了一种名为动态结构化剪枝（DSP）的全自动框架，用于时间序列分类模型的结构化剪枝，以解决资源受限设备上的计算瓶颈问题。在128个UCR数据集上验证了该方法的有效性，平均压缩率分别为LITETime架构的58%和InceptionTime架构的75%，同时保持了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习的时间序列分类模型虽然预测性能强，但其高计算量和内存需求限制了在资源受限设备上的部署。现有的结构化剪枝方法通常依赖于手动调整超参数如剪枝比率，这限制了跨数据集的可扩展性和泛化能力。

Method: 提出了动态结构化剪枝（DSP），一种完全自动化的、针对基于卷积的时间序列分类模型的结构化剪枝框架。通过在训练过程中引入实例级稀疏损失来诱导通道级稀疏性，并随后进行全局激活分析以识别并去除冗余滤波器，无需预定义任何剪枝比率。

Result: 通过对128个UCR数据集采用两种不同最先进架构：LITETime和InceptionTime进行验证，结果显示对于LITETime架构平均压缩率达到58%，而对于InceptionTime架构则达到了75%的压缩率，同时保持了分类精度。冗余性分析证实了DSP能够生成紧凑且信息丰富的表示形式。

Conclusion: 本研究提出的动态结构化剪枝方法为实现可扩展和高效的时间序列分类模型部署提供了一个实用途径。

Abstract: Deep learning models for Time Series Classification (TSC) have achieved strong predictive performance but their high computational and memory requirements often limit deployment on resource-constrained devices. While structured pruning can address these issues by removing redundant filters, existing methods typically rely on manually tuned hyperparameters such as pruning ratios which limit scalability and generalization across datasets. In this work, we propose Dynamic Structured Pruning (DSP), a fully automatic, structured pruning framework for convolution-based TSC models. DSP introduces an instance-wise sparsity loss during training to induce channel-level sparsity, followed by a global activation analysis to identify and prune redundant filters without needing any predefined pruning ratio. This work tackles computational bottlenecks of deep TSC models for deployment on resource-constrained devices. We validate DSP on 128 UCR datasets using two different deep state-of-the-art architectures: LITETime and InceptionTime. Our approach achieves an average compression of 58% for LITETime and 75% for InceptionTime architectures while maintaining classification accuracy. Redundancy analyses confirm that DSP produces compact and informative representations, offering a practical path for scalable and efficient deep TSC deployment.

</details>


### [62] [Hierarchical Successor Representation for Robust Transfer](https://arxiv.org/abs/2602.12753)
*Changmin Yu,Máté Lengyel*

Main category: cs.LG

TL;DR: 本文提出了分层后继表示（HSR），通过在预测表示的构建中引入时间抽象，克服了经典后继表示（SR）对策略依赖和拓扑复杂环境中的光谱扩散问题。HSR结合非负矩阵分解(NMF)生成稀疏、低秩的状态表示，提高了多隔间环境中新任务转移的样本效率，并揭示了可解释的拓扑结构，有效地连接了无模型优化与基于模型的灵活性。此外，HSR的时间扩展预测结构也支持高效探索，适用于大规模程序生成环境。


<details>
  <summary>Details</summary>
Motivation: 经典后继表示（SR）虽然能够将预测动力学与奖励解耦，允许快速泛化不同的奖励配置，但其存在对策略的依赖性以及在拓扑复杂环境中因光谱扩散导致特征密集且重叠的问题。这些问题限制了SR在动态变化的任务需求及环境下的适用性和可扩展性。

Method: 提出了一种新的方法——分层后继表示（HSR），它通过在构建预测表示时引入时间抽象来解决上述局限性。HSR学习到的状态特征对于由任务引起的策略变化具有鲁棒性。进一步地，通过应用非负矩阵分解（NMF）技术于HSR上，研究人员获得了有助于高效转移到新任务中的稀疏、低秩状态表示。

Result: 实验结果表明，HSR-NMF不仅能够发现可解释的拓扑结构，还提供了一个不依赖于特定策略的层次图，很好地平衡了无模型优化与基于模型规划之间的灵活性。此外，研究还展示了HSR的时间扩展预测结构如何被用来促进在大范围程序生成环境中的有效探索。

Conclusion: 通过引入HSR及其与NMF结合使用的方法，本研究为处理策略依赖性和拓扑复杂性带来的挑战提供了新的解决方案，同时促进了更高效的探索机制的发展。这标志着向开发更加灵活和强大的强化学习算法迈出了重要一步。

Abstract: The successor representation (SR) provides a powerful framework for decoupling predictive dynamics from rewards, enabling rapid generalisation across reward configurations. However, the classical SR is limited by its inherent policy dependence: policies change due to ongoing learning, environmental non-stationarities, and changes in task demands, making established predictive representations obsolete. Furthermore, in topologically complex environments, SRs suffer from spectral diffusion, leading to dense and overlapping features that scale poorly. Here we propose the Hierarchical Successor Representation (HSR) for overcoming these limitations. By incorporating temporal abstractions into the construction of predictive representations, HSR learns stable state features which are robust to task-induced policy changes. Applying non-negative matrix factorisation (NMF) to the HSR yields a sparse, low-rank state representation that facilitates highly sample-efficient transfer to novel tasks in multi-compartmental environments. Further analysis reveals that HSR-NMF discovers interpretable topological structures, providing a policy-agnostic hierarchical map that effectively bridges model-free optimality and model-based flexibility. Beyond providing a useful basis for task-transfer, we show that HSR's temporally extended predictive structure can also be leveraged to drive efficient exploration, effectively scaling to large, procedurally generated environments.

</details>


### [63] [Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs](https://arxiv.org/abs/2602.12756)
*Xingyu Zhang,Hanyun Du,Zeen Song,Jianqi Zhang,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 本文提出了一种新的闭环框架F-LLM（反馈驱动的大型语言模型），通过可学习的残差估计器和反馈控制器来主动稳定轨迹，从而显著减少了时间序列预测中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的时间序列预测方法通常采用简单的自回归生成策略，这种方式在推理过程中以开环方式进行，导致了不可避免的错误累积（暴露偏差）。早期的小偏差会随着时间推移演变成显著的轨迹漂移。

Method: 受到控制理论启发，作者们提出了F-LLM，一种新型的闭环架构。该方法不仅包括一个用于估计残差的学习模块（观察者），还引入了一个反馈控制器来积极地稳定预测轨迹。此外，研究还提供了一个理论保证，即如果基础模型满足局部利普希茨约束，则所提出的闭环机制可以确保误差被均匀地限制在一个范围内。

Result: 广泛的实验表明，与标准方法相比，F-LLM能够显著减少误差传播，并且在多个时间序列基准测试中表现出色。

Conclusion: F-LLM作为一种创新的方法论，通过结合控制理论的概念解决了现有大型语言模型在进行长时间序列预测时面临的主要挑战之一——误差累积问题，为提高预测准确性提供了新思路。

Abstract: Large Language Models (LLMs) have recently shown exceptional potential in time series forecasting, leveraging their inherent sequential reasoning capabilities to model complex temporal dynamics. However, existing approaches typically employ a naive autoregressive generation strategy. We identify a critical theoretical flaw in this paradigm: during inference, the model operates in an open-loop manner, consuming its own generated outputs recursively. This leads to inevitable error accumulation (exposure bias), where minor early deviations cascade into significant trajectory drift over long horizons. In this paper, we reformulate autoregressive forecasting through the lens of control theory, proposing \textbf{F-LLM} (Feedback-driven LLM), a novel closed-loop framework. Unlike standard methods that passively propagate errors, F-LLM actively stabilizes the trajectory via a learnable residual estimator (Observer) and a feedback controller. Furthermore, we provide a theoretical guarantee that our closed-loop mechanism ensures uniformly bounded error, provided the base model satisfies a local Lipschitz constraint. Extensive experiments demonstrate that F-LLM significantly mitigates error propagation, achieving good performance on time series benchmarks.

</details>


### [64] [GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories](https://arxiv.org/abs/2602.12828)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: 提出了一种名为GRAIL的框架，该框架利用结构化的几何表示和结构感知检索来处理纵向电子健康记录（EHRs），以预测患者的下一次就诊事件。通过结合确定性编码系统层次结构与数据驱动的时间关联性，GRAIL在双曲空间中嵌入统一的临床图，并将每次访问总结为一个概率中心事件，从而减少稀疏观察带来的噪声。实验表明，GRAIL能够提高多种类型下一次就诊预测的准确性，并生成更符合层次结构的预测结果。


<details>
  <summary>Details</summary>
Motivation: 针对使用纵向电子健康记录(EHRs)预测未来临床事件时面临的挑战，如稀疏多类型的临床事件、层次化的医学词汇表以及大型语言模型(LLMs)在处理长篇结构化历史记录时易出现虚构现象的问题，研究者们旨在开发一种更为有效的解决方案，特别是针对下一次就诊事件预测。

Method: 提出了GRAIL框架，它采用结构化的几何表示方法和结构感知检索技术来建模纵向EHRs。首先，通过整合基于编码系统的确定性层次结构及跨事件类型的基于数据的时间关联性来构建统一的临床图；接着，在双曲空间内对该图进行嵌入，并为每次就诊创建一个代表性的概率中心事件，以此方式降低由稀疏观察引起的数据噪声。在推理阶段，GRAIL可以检索出一套与层次性和时间进展一致的未来可能发生的临床事件集，并可选择性地使用LLM作为约束条件下的重排序器来进一步优化结果。

Result: 在MIMIC-IV数据集上的实验显示，对于多类型下一次就诊事件预测任务，GRAIL框架能够持续提升预测性能，并且相比其他方法而言，其预测结果更加符合医学术语间的层次关系。

Conclusion: 研究表明，GRAIL作为一种新颖的方法论，在处理EHRs以实现对未来临床事件尤其是下一次就诊情况的有效预测方面展现了显著优势。它不仅提高了预测精度，还确保了预测结果的一致性和合理性，为临床决策支持提供了有力工具。

Abstract: Predicting future clinical events from longitudinal electronic health records (EHRs) is challenging due to sparse multi-type clinical events, hierarchical medical vocabularies, and the tendency of large language models (LLMs) to hallucinate when reasoning over long structured histories. We study next-visit event prediction, which aims to forecast a patient's upcoming clinical events based on prior visits. We propose GRAIL, a framework that models longitudinal EHRs using structured geometric representations and structure-aware retrieval. GRAIL constructs a unified clinical graph by combining deterministic coding-system hierarchies with data-driven temporal associations across event types, embeds this graph in hyperbolic space, and summarizes each visit as a probabilistic Central Event that denoises sparse observations. At inference time, GRAIL retrieves a structured set of clinically plausible future events aligned with hierarchical and temporal progression, and optionally refines their ranking using an LLM as a constrained inference-time reranker. Experiments on MIMIC-IV show that GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.

</details>


### [65] [FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching](https://arxiv.org/abs/2602.12829)
*Lei Lv,Yunfei Li,Yu Luo,Fuchun Sun,Xiao Ma*

Main category: cs.LG

TL;DR: 本文提出了Field Least-Energy Actor-Critic (FLAC)，一种无需考虑似然性的框架，通过惩罚速度场的动能来调节策略随机性，从而解决了迭代生成策略在最大熵强化学习中遇到的问题。实验表明，FLAC在高维基准测试中表现出色或与强基线相当，同时避免了显式的密度估计。


<details>
  <summary>Details</summary>
Motivation: 迭代生成策略（如扩散模型和流匹配）为连续控制提供了更优的表现力，但它们的动作对数密度不可直接访问，这给最大熵强化学习带来了挑战。为了克服这一问题，并保持策略表达能力的同时简化计算过程，作者们提出了新的方法。

Method: 提出了一种名为Field Least-Energy Actor-Critic (FLAC)的新框架，该框架不依赖于似然度，而是通过惩罚速度场的动能来管理策略的随机性。此方法将策略优化视为相对于高熵参考过程（例如均匀分布）的广义薛定谔桥问题。基于这种方法论，研究者还推导出了一种能量正则化的策略迭代方案以及一种可以通过拉格朗日对偶机制自动调整动能的实际离策略算法。

Result: 实验结果表明，在面对高维度基准时，FLAC能够达到优于或至少可与强力基线相媲美的表现，而且在整个过程中不需要进行显式的动作密度估计。

Conclusion: 通过引入FLAC框架，本研究不仅成功解决了使用迭代生成策略时面临的最大熵强化学习难题，而且还提供了一个既能有效控制策略随机性又能简化计算复杂度的新途径。

Abstract: Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.

</details>


### [66] [X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting](https://arxiv.org/abs/2602.12869)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: 本文提出了一种名为X-VORTEX的时空对比学习框架，该框架能够从未标记的LiDAR点云序列中学习到物理感知表示，解决了传感器稀疏性和时间变化涡流动力学两大核心挑战。通过使用真实世界超过一百万次LiDAR扫描的数据集进行评估，X-VORTEX在仅需1%监督基线所需标记数据的情况下，实现了更优的涡流中心定位，并支持精确轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要将每次扫描视为独立的、完全监督的分割问题，忽略了时间结构并且无法扩展至实践中收集的大规模未标注档案。鉴于此，开发一种能够利用未标注数据并考虑到时间连续性的新方法变得必要。

Method: X-VORTEX采用增强重叠理论为基础的时空对比学习策略，通过结合弱扰动序列与经由时间子采样和空间遮罩生成的强增强对应部分来构建配对输入，促使模型跨越缺失帧和局部观测对表示进行对齐。架构上，时间分布几何编码器用于提取每扫特征，而序列聚合器则建模跨可变长度序列演变中的涡流状态。

Result: X-VORTEX在实际应用中表现出色，在需要远少于监督方法标签量的情况下（仅为1%），就达到了更好的涡流中心定位效果，并且所学到的表征还支持了准确的轨迹预测。

Conclusion: 本研究展示了一种新的解决飞机尾涡追踪难题的方法——X-VORTEX，它不仅有效地处理了传感器稀疏性及时变涡流动力学带来的挑战，而且显著减少了对于标注数据的需求，同时保持甚至提高了涡流跟踪及预测性能。

Abstract: Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.

</details>


### [67] [Transporting Task Vectors across Different Architectures without Training](https://arxiv.org/abs/2602.12952)
*Filippo Rinaldi,Aniello Panariello,Giacomo Salici,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: 本文提出了一种名为Theseus的无训练方法，用于在不同宽度的异构模型之间传输任务特定更新。通过将任务更新表征为对中间表示的功能效应，并使用正交Procrustes分析对齐表示空间后，可以稳定地解决任务向量传输问题，从而无需额外训练或反向传播即可实现跨架构的任务更新转移。


<details>
  <summary>Details</summary>
Motivation: 适应大型预训练模型到下游任务时产生的任务特定参数更新对于每个模型变体来说都是昂贵的重新学习成本。虽然已有研究表明可以在具有相同架构的模型间转移这种更新，但在不同宽度的模型之间转移这些更新仍较少被探索。

Method: 介绍了一种名为Theseus的方法，它不直接匹配参数，而是通过任务更新在中间表示上引起的功能效应来描述。任务向量的传输被形式化为一个基于观察激活的功能匹配问题，在通过正交Procrustes分析对齐表示空间之后，该问题允许有一个保持更新几何结构的稳定闭式解。

Result: 在视觉和语言模型上的评估表明，相比于强大的基线，Theseus能够在不同的模型宽度下带来一致性的改进，而且不需要额外的训练或反向传播。

Conclusion: 当任务身份是功能定义而非参数定义时，任务更新可以有意义地跨架构进行转移。

Abstract: Adapting large pre-trained models to downstream tasks often produces task-specific parameter updates that are expensive to relearn for every model variant. While recent work has shown that such updates can be transferred between models with identical architectures, transferring them across models of different widths remains largely unexplored. In this work, we introduce Theseus, a training-free method for transporting task-specific updates across heterogeneous models. Rather than matching parameters directly, we characterize a task update by the functional effect it induces on intermediate representations. We formalize task-vector transport as a functional matching problem on observed activations and show that, after aligning representation spaces via orthogonal Procrustes analysis, it admits a stable closed-form solution that preserves the geometry of the update. We evaluate Theseus on vision and language models across different widths, showing consistent improvements over strong baselines without additional training or backpropagation. Our results show that task updates can be meaningfully transferred across architectures when task identity is defined functionally rather than parametrically.

</details>


### [68] [Extending confidence calibration to generalised measures of variation](https://arxiv.org/abs/2602.12975)
*Andrew Thompson,Vivek Desai*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量标准——变差校准误差（VCE），用于评估机器学习分类器的校准情况。VCE扩展了已知的预期校准误差（ECE）方法，能够对概率分布的变化进行更全面的考量，并且通过合成预测数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的预期校准误差（ECE）主要用于评估最大概率或置信度的校准情况，但未能充分考虑整个概率分布的特点。因此，作者旨在开发一种新的度量标准来解决这个问题，即能够评估任何变化度量的校准情况。

Method: 引入了变差校准误差（VCE）作为新的度量标准，它基于现有ECE的概念之上进一步发展而来，允许对于描述概率分布变化的其他指标（如香农熵）也进行有效的校准评估。

Result: 通过设计上完全校准的合成预测数据实例展示了VCE的有效性；随着样本数量增加，VCE趋向于零，表明其具有理想的属性。同时，与文献中提出的另一种基于熵的校准度量相比，VCE表现出了更好的性能。

Conclusion: 变差校准误差（VCE）为评估机器学习分类器的概率分布校准提供了新的视角和工具，尤其在需要考虑整体分布特性而非仅关注最高置信度时显示出优越性。

Abstract: We propose the Variation Calibration Error (VCE) metric for assessing the calibration of machine learning classifiers. The metric can be viewed as an extension of the well-known Expected Calibration Error (ECE) which assesses the calibration of the maximum probability or confidence. Other ways of measuring the variation of a probability distribution exist which have the advantage of taking into account the full probability distribution, for example the Shannon entropy. We show how the ECE approach can be extended from assessing confidence calibration to assessing the calibration of any metric of variation. We present numerical examples upon synthetic predictions which are perfectly calibrated by design, demonstrating that, in this scenario, the VCE has the desired property of approaching zero as the number of data samples increases, in contrast to another entropy-based calibration metric (the UCE) which has been proposed in the literature.

</details>


### [69] [Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling](https://arxiv.org/abs/2602.12976)
*Jin Li,Kleanthis Malialis,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法VAE++ESDD，该方法通过增量学习和双层集成（包括变分自编码器(VAE)集成用于异常预测以及概念漂移检测器集成）来解决非平稳环境中大量未标记流数据的异常检测问题。实验结果表明，该方法在处理具有极低异常率和多种漂移特性的实际与合成数据集时，明显优于现有基线和其他最先进方法。


<details>
  <summary>Details</summary>
Motivation: 在数字时代背景下，各领域产生的大量流数据通常是未标记的，这使得识别事件特别是异常变得困难；尤其是在非平稳环境下，由于概念漂移的存在，模型性能会随着时间推移而下降。为了解决这些问题，提出了新的方法VAE++ESDD。

Method: VAE++ESDD采用增量学习技术，并结合了两种集成策略：一种是基于变分自编码器(VAEs)的集成，用于异常预测；另一种则是由多个统计基础的概念漂移检测机制组成的集成。

Result: 通过使用具有严重或极端低异常率及不同漂移特征的真实世界和合成数据集进行广泛实验研究后发现，所提方法VAE++ESDD显著优于强大的基准方法以及其他最先进的技术。

Conclusion: 本研究表明，VAE++ESDD是一种有效的解决方案，能够应对非平稳环境下的大规模未标记流数据中异常检测挑战，特别是在存在概念漂移的情况下。

Abstract: In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.

</details>


### [70] [Uncertainty in Federated Granger Causality: From Origins to Systemic Consequences](https://arxiv.org/abs/2602.13004)
*Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: 本文首次提出了在联邦Granger因果框架中严格量化不确定性及其传播的方法论，通过区分数据噪声和模型变异性，并推导出封闭形式的递归式来建模不确定性随客户端-服务器交互而演变的过程。实证评估显示，明确表征不确定性显著提高了联邦因果推理的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦Granger因果算法仅提供因果关系的确定性点估计，忽略了不确定性。为了解决这一问题，本文旨在开发一种方法，以在遵守数据主权约束的同时，能够在分布式基础设施应用中准确地量化不确定性及其传播。

Method: 文章首先系统地分类了不确定性的来源，区分了数据噪声（aleatoric）与模型变异性（epistemic）的影响。接着，作者们推导出了描述不确定性如何通过客户端-服务器互动进化的封闭形式递归公式，并识别出四个新的交叉协方差组成部分，这些成分将数据不确定性与跨联邦架构的模型参数不确定性联系起来。此外，还定义了这些不确定性递归公式的严格收敛条件，并获得了服务器及客户端模型参数的具体稳态方差。

Result: 研究结果表明，稳态方差仅依赖于客户端的数据统计特性，从而消除了对初始认知先验的依赖并增强了鲁棒性。通过对合成基准测试以及真实世界工业数据集进行的经验评估进一步证明，明确刻画不确定性可以极大地提高联邦因果推理结果的可靠性与可解释性。

Conclusion: 本研究建立了一种新颖的方法论，用于在联邦Granger因果框架内严格量化不确定性及其传播过程。这种方法不仅有助于更好地理解不确定性如何影响联邦学习环境下的因果发现，而且也为改进现有联邦GC算法提供了理论基础。

Abstract: Granger Causality (GC) provides a rigorous framework for learning causal structures from time-series data. Recent federated variants of GC have targeted distributed infrastructure applications (e.g., smart grids) with distributed clients that generate high-dimensional data bound by data-sovereignty constraints. However, Federated GC algorithms only yield deterministic point estimates of causality and neglect uncertainty. This paper establishes the first methodology for rigorously quantifying uncertainty and its propagation within federated GC frameworks. We systematically classify sources of uncertainty, explicitly differentiating aleatoric (data noise) from epistemic (model variability) effects. We derive closed-form recursions that model the evolution of uncertainty through client-server interactions and identify four novel cross-covariance components that couple data uncertainties with model parameter uncertainties across the federated architecture. We also define rigorous convergence conditions for these uncertainty recursions and obtain explicit steady-state variances for both server and client model parameters. Our convergence analysis demonstrates that steady-state variances depend exclusively on client data statistics, thus eliminating dependence on initial epistemic priors and enhancing robustness. Empirical evaluations on synthetic benchmarks and real-world industrial datasets demonstrate that explicitly characterizing uncertainty significantly improves the reliability and interpretability of federated causal inference.

</details>


### [71] [Machine Learning-Based Classification of Jhana Advanced Concentrative Absorption Meditation (ACAM-J) using 7T fMRI](https://arxiv.org/abs/2602.13008)
*Puneet Kumar,Winson F. Z. Yang,Alakhsimar Singh,Xiaobai Li,Matthew D. Sacchet*

Main category: cs.LG

TL;DR: 本研究使用功能磁共振成像(fMRI)导出的区域同质性(ReHo)图，结合机器学习方法来区分高级冥想状态（ACAM-J）和非冥想状态。通过分析20名高级冥想者的数据，并利用一位高级实践者的密集单一案例数据进行验证，发现前额叶和前扣带回区域对模型决策贡献最大。集成模型在区分ACAM-J与对照条件时达到了66.82%的准确率，表明机器学习技术在分类高级冥想状态方面的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索高级冥想状态（ACAM-J）相关的神经关联对于理解意识和福祉至关重要。本研究旨在评估是否可以利用fMRI衍生的区域同质性(ReHo)图通过机器学习方法来有效地区分ACAM-J和其他非冥想状态。

Method: 研究团队收集了20位高级冥想者的群体层面fMRI数据用于训练分类器，并从一位高级实践者那里获得了执行ACAM-J及控制任务期间的密集单个案数据以检验泛化能力。基于预定义的大脑感兴趣区计算ReHo图并提取特征，然后采用分层交叉验证训练多个机器学习分类器，用以判断ReHo模式能否区分ACAM-J与非冥想状态。

Result: 集成模型在区分ACAM-J与对照条件下实现了66.82% (p < 0.05) 的准确度。特征重要性分析显示，前额叶和前扣带回区域对模型决策贡献最大，这与这些区域已知参与注意力调节和元认知过程相一致。此外，Cohen's kappa值反映了一定程度的一致性，支持了使用机器学习区分ACAM-J与非冥想状态的可能性。

Conclusion: 研究结果表明，使用机器学习技术基于fMRI衍生的ReHo图来区分高级冥想状态是可行的。这一发现为未来关于神经调节及高级冥想机制模型的研究提供了依据。

Abstract: Jhana advanced concentration absorption meditation (ACAM-J) is related to profound changes in consciousness and cognitive processing, making the study of their neural correlates vital for insights into consciousness and well-being. This study evaluates whether functional MRI-derived regional homogeneity (ReHo) can be used to classify ACAM-J using machine-learning approaches. We collected group-level fMRI data from 20 advanced meditators to train the classifiers, and intensive single-case data from an advanced practitioner performing ACAM-J and control tasks to evaluate generalization. ReHo maps were computed, and features were extracted from predefined brain regions of interest. We trained multiple machine learning classifiers using stratified cross-validation to evaluate whether ReHo patterns distinguish ACAM-J from non-meditative states. Ensemble models achieved 66.82% (p < 0.05) accuracy in distinguishing ACAM-J from control conditions. Feature-importance analysis indicated that prefrontal and anterior cingulate areas contributed most to model decisions, aligning with established involvement of these regions in attentional regulation and metacognitive processes. Moreover, moderate agreement reflected in Cohen's kappa supports the feasibility of using machine learning to distinguish ACAM-J from non-meditative states. These findings advocate machine-learning's feasibility in classifying advanced meditation states, future research on neuromodulation and mechanistic models of advanced meditation.

</details>


### [72] [Probabilistic Wind Power Forecasting with Tree-Based Machine Learning and Weather Ensembles](https://arxiv.org/abs/2602.13010)
*Max Bruninx,Diederik van Binsbergen,Timothy Verstraeten,Ann Nowé,Jan Helsen*

Main category: cs.LG

TL;DR: 本文介绍了一种通过使用天气预报集合的梯度提升树来获取风力发电日前概率预测的方法。对比了三种最先进的概率预测方法，结果表明机器学习方法相比传统方法能显著提高预测准确性，其中条件扩散模型表现最佳。此外，使用天气预报集合可以进一步提高点预测精度。


<details>
  <summary>Details</summary>
Motivation: 为了继续促进可再生能源（如风力发电）与电力网的整合，准确的生产预测是必要的。

Method: 采用基于梯度提升树的方法，并结合一系列天气预报来进行风力发电的概率性日前预测。比较分析了三种先进的概率预测技术：一致性分位数回归、自然梯度提升和条件扩散模型。所有这些方法都可以与基于树的机器学习相结合。

Result: 实验结果显示，相比于功率曲线法和校准后的尾流模型，机器学习方法能够将平均绝对误差分别降低最多53%和33%。在三种概率预测方法中，条件扩散模型被发现对于风力发电量的概率性和点估计提供了最好的总体表现。此外，研究还指出利用一组天气预报可以将点预测精度提高至多23%。

Conclusion: 机器学习方法特别是条件扩散模型，在风力发电日前预测方面表现出色，优于传统的工程方法。同时，使用天气预报集合有助于进一步增强预测精度。

Abstract: Accurate production forecasts are essential to continue facilitating the integration of renewable energy sources into the power grid. This paper illustrates how to obtain probabilistic day-ahead forecasts of wind power generation via gradient boosting trees using an ensemble of weather forecasts. To this end, we perform a comparative analysis across three state-of-the-art probabilistic prediction methods-conformalised quantile regression, natural gradient boosting and conditional diffusion models-all of which can be combined with tree-based machine learning. The methods are validated using four years of data for all wind farms present within the Belgian offshore zone. Additionally, the point forecasts are benchmarked against deterministic engineering methods, using either the power curve or an advanced approach incorporating a calibrated analytical wake model. The experimental results show that the machine learning methods improve the mean absolute error by up to 53% and 33% compared to the power curve and the calibrated wake model. Considering the three probabilistic prediction methods, the conditional diffusion model is found to yield the best overall probabilistic and point estimate of wind power generation. Moreover, the findings suggest that the use of an ensemble of weather forecasts can improve point forecast accuracy by up to 23%.

</details>


### [73] [Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery](https://arxiv.org/abs/2602.13021)
*Jing Xiao,Xinhai Chen,Jiaming Peng,Qinglin Wang,Menghan Jia,Zhiquan Lai,Guangping Yu,Dongsheng Li,Tiejun Li,Jie Liu*

Main category: cs.LG

TL;DR: 提出了一种名为PG-SR的先验引导符号回归框架，通过引入先验约束检查器和PACE机制，在理论和实验上都优于现有方法，能够有效避免伪方程陷阱。


<details>
  <summary>Details</summary>
Motivation: 现有的符号回归方法容易陷入伪方程陷阱，即生成的方程虽然拟合观测数据但违背科学原理，主要是因为这些方法主要依赖于经验风险最小化而缺乏确保科学一致性的显式约束。

Method: 构建了一个三阶段流程（预热、进化、精炼）的PG-SR框架，其中包含一个先验约束检查器来明确编码领域先验为可执行的约束程序，并在进化阶段使用了Prior Annealing Constrained Evaluation (PACE) 机制以逐步指导发现过程朝向科学一致性更强的方向发展。

Result: 理论证明显示，PG-SR减少了假设空间的Rademacher复杂度，从而提供了更紧致的一般化边界，并对伪方程形成了防护；实验结果表明，无论是在不同领域、面对不同质量的先验信息、噪声数据还是数据稀缺情况下，PG-SR的表现均优于当前最先进的基线模型。

Conclusion: PG-SR作为一种创新的符号回归解决方案，不仅增强了模型对于伪方程的抵抗力，还在多样化的实际应用中展现出了优越性能。

Abstract: Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.

</details>


### [74] [Resource-Efficient Gesture Recognition through Convexified Attention](https://arxiv.org/abs/2602.13030)
*Daniel Schwartz,Dario Salvucci,Yusuf Osmanlioglu,Richard Vallett,Genevieve Dion,Ali Shokoufandeh*

Main category: cs.LG

TL;DR: 本研究提出了一种凸优化注意力机制，专为可穿戴应用设计，通过非扩张单纯形投影和凸损失函数动态加权特征，同时保持凸性。在基于纺织物的电容传感器上实现时，该方法对点击手势和滑动手势均达到了100%的识别准确率，并且仅需120至360个参数，比传统方法减少了97%的参数量。此外，它还具有亚毫秒级的推理时间和极低的存储需求（小于7KB），使得直接在电子纺织品内实现手势界面成为可能，无需外部处理。


<details>
  <summary>Details</summary>
Motivation: 可穿戴电子纺织品接口需要具备手势识别能力，但面临着能耗、计算能力和形态上的严重限制，这使得传统的深度学习方法难以应用。虽然像MobileNet这样的轻量级架构提高了效率，但它们仍然需要数千个参数，这对纺织集成平台构成了挑战。

Method: 本文介绍了一种针对可穿戴应用设计的凸化注意力机制，能够动态地给特征赋予权重的同时保持凸性，通过使用非扩张单纯形投影和凸损失函数来实现。与采用非凸softmax操作的传统注意力机制不同，这种方法利用了到概率单纯形上的欧几里得投影结合多类合页损失，保证了全局收敛性。

Result: 在拥有四个连接点的基于织物的电容式传感器上实施时，对于点击手势和滑动手势的识别都达到了100.00%的准确度——这一结果在10折交叉验证及保留测试评估中保持一致——并且只需要120-360个参数，相比传统方法减少达97%。此外，该方法具有次毫秒级别的推理时间(290-296μs)以及极小的存储需求(<7KB)。

Conclusion: 这些结果表明，凸优化可以有效地支持电子纺织品接口中的设备上机器学习，从而直接在电子纺织品内部实现手势交互界面而不需要外部处理单元。然而，实际部署前仍需跨越多个用户、环境条件以及更复杂的手势词汇进行验证。

Abstract: Wearable e-textile interfaces require gesture recognition capabilities but face severe constraints in power consumption, computational capacity, and form factor that make traditional deep learning impractical. While lightweight architectures like MobileNet improve efficiency, they still demand thousands of parameters, limiting deployment on textile-integrated platforms. We introduce a convexified attention mechanism for wearable applications that dynamically weights features while preserving convexity through nonexpansive simplex projection and convex loss functions. Unlike conventional attention mechanisms using non-convex softmax operations, our approach employs Euclidean projection onto the probability simplex combined with multi-class hinge loss, ensuring global convergence guarantees. Implemented on a textile-based capacitive sensor with four connection points, our approach achieves 100.00\% accuracy on tap gestures and 100.00\% on swipe gestures -- consistent across 10-fold cross-validation and held-out test evaluation -- while requiring only 120--360 parameters, a 97\% reduction compared to conventional approaches. With sub-millisecond inference times (290--296$μ$s) and minimal storage requirements ($<$7KB), our method enables gesture interfaces directly within e-textiles without external processing. Our evaluation, conducted in controlled laboratory conditions with a single-user dataset, demonstrates feasibility for basic gesture interactions. Real-world deployment would require validation across multiple users, environmental conditions, and more complex gesture vocabularies. These results demonstrate how convex optimization can enable efficient on-device machine learning for textile interfaces.

</details>


### [75] [Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL](https://arxiv.org/abs/2602.13035)
*Yixiao Zhou,Yang Li,Dongzhou Cheng,Hehe Fan,Yu Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为内省大语言模型的层次化强化学习框架，该模型能够根据隐藏状态选择采样温度，并基于下游奖励联合优化温度和令牌策略。实验表明，学习到的温度策略优于固定和启发式基线，并且表现出与推理不确定性相一致的可解释探索行为。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于静态值或与任务级奖励脱钩的启发式调整来控制采样温度，而采样温度直接通过调节策略熵来控制探索-利用之间的权衡。为了克服这一局限性，研究者提出了一个能够让模型在生成过程中学习如何控制采样温度的新框架。

Method: 研究者设计了一个称为内省大语言模型（Introspective LLM）的层次化强化学习架构，在每个解码步骤中，该模型会基于其隐藏状态挑选一个温度，并从相应的分布中抽取下一个令牌。温度和令牌策略是根据下游奖励使用坐标上升方案共同优化的。

Result: 实验结果表明，在数学推理基准测试中，学习到的温度策略比固定的和启发式的基线表现更好，并显示出与推理不确定性相匹配的可理解的探索行为。

Conclusion: 通过让大语言模型学习如何控制其自身的采样温度，可以改善它们在需要探索和利用之间找到平衡的任务中的性能。内省大语言模型提供了一种新颖的方法来实现这一点，它不仅提高了性能，还带来了更符合直觉的行为模式。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.

</details>


### [76] [TCRL: Temporal-Coupled Adversarial Training for Robust Constrained Reinforcement Learning in Worst-Case Scenarios](https://arxiv.org/abs/2602.13040)
*Wentao Xu,Zhongming Yao,Weihao Li,Zhenghang Song,Yumeng Song,Tianyi Li,Yushuai Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的时序耦合对抗训练框架TCRL，用于在最坏情况下进行鲁棒约束强化学习。TCRL通过引入一种无需显式建模对手的最坏情况感知成本约束函数以及建立双重约束防御机制来应对时序耦合扰动攻击，并在多种CRL任务中表现出优于现有方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒约束强化学习方法主要集中在单步扰动和时间独立的对抗模型上，缺乏对时序耦合扰动明确建模的鲁棒性处理。为了克服这些挑战并提高系统在安全关键领域如自动驾驶、机器人学及电网管理中的应用性能，需要开发新方法。

Method: 提出了TCRL，一种新颖的时间耦合对抗训练框架，它包括：1) 引入一种能够在不需显式模拟敌手的情况下估计时序耦合扰动下安全成本的最坏情况感知成本约束函数；2) 在奖励上设置双重约束防御机制以对抗时序耦合对手同时保持奖励不可预测性。

Result: 实验结果表明，在面对时序耦合扰动攻击时，TCRL在各种CRL任务中展现出比现有方法更优的鲁棒性表现。

Conclusion: TCRL为解决时序耦合扰动下的鲁棒约束强化学习问题提供了一个有效解决方案，其在多个实验场景中证明了优越性。

Abstract: Constrained Reinforcement Learning (CRL) aims to optimize decision-making policies under constraint conditions, making it highly applicable to safety-critical domains such as autonomous driving, robotics, and power grid management. However, existing robust CRL approaches predominantly focus on single-step perturbations and temporally independent adversarial models, lacking explicit modeling of robustness against temporally coupled perturbations. To tackle these challenges, we propose TCRL, a novel temporal-coupled adversarial training framework for robust constrained reinforcement learning (TCRL) in worst-case scenarios. First, TCRL introduces a worst-case-perceived cost constraint function that estimates safety costs under temporally coupled perturbations without the need to explicitly model adversarial attackers. Second, TCRL establishes a dual-constraint defense mechanism on the reward to counter temporally coupled adversaries while maintaining reward unpredictability. Experimental results demonstrate that TCRL consistently outperforms existing methods in terms of robustness against temporally coupled perturbation attacks across a variety of CRL tasks.

</details>


### [77] [GPTZero: Robust Detection of LLM-Generated Texts](https://arxiv.org/abs/2602.13042)
*George Alexandru Adam,Alexander Cui,Edwin Thomas,Emily Napier,Nazar Shmatko,Jacob Schnell,Jacob Junqi Tian,Alekhya Dronavalli,Edward Tian,Dongwon Lee*

Main category: cs.LG

TL;DR: 本文介绍了一种名为GPTZero的先进AI检测解决方案，用于区分人类编写和AI生成的文本。它具有层次化的多任务架构，能够在多个领域实现高精度预测，并通过多层次自动化红队测试提高了对对抗性攻击和改写的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，辨别文本是由人类还是AI生成成为了一个新的挑战。这引发了诸如技能评估被削弱、低质量内容批量生产以及错误信息扩散等问题。为了解决这些问题，研究提出了GPTZero。

Method: 提出了一种层次化、多任务的架构来支持人类与AI文本之间的灵活分类；在多种领域上展示了顶尖级别的准确性；通过多层次自动红队技术增强对对抗性攻击及重述策略的抵抗力。

Result: GPTZero能够准确且可解释地检测出文本来源，并教育用户如何负责任地使用该工具，从而确保了文本评估过程中的公平性和透明度。

Conclusion: GPTZero作为一款先进的工业级AI检测解决方案，在识别由人或机器创作的文字方面表现出色，同时强调了其对于促进公平透明评价的重要性。

Abstract: While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.

</details>


### [78] [Geometric Manifold Rectification for Imbalanced Learning](https://arxiv.org/abs/2602.13045)
*Xubin Wang,Qing Li,Weijia Jia*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架GMR（几何流形修正），用于处理不平衡的结构化数据。该框架利用局部几何先验，通过几何置信度估计和非对称清洗来改善分类性能，在多个基准数据集上展示了与强大的采样基线相比具有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，当表格数据集受到噪声和重叠类边界的影响时，不平衡分类是一个巨大的挑战。从几何角度来看，主要困难在于多数类对少数类流形的拓扑侵入，这模糊了真实的决策边界。传统的欠采样技术如ENN使用对称清理规则和统一投票机制，无法捕捉局部流形结构，并且常常意外地移除有信息量的少数样本。

Method: 提出了GMR（Geometric Manifold Rectification）框架，它包括两个贡献：(1) 几何置信度估计，采用逆距离加权kNN投票结合自适应距离度量来捕捉局部可靠性；(2) 非对称清洗，对于多数样本采取严格措施，同时通过限制少数样本移除来保守地保护少数样本。

Result: 在多个基准数据集上的广泛实验表明，GMR与强大的采样基线相比具有竞争力。

Conclusion: GMR提供了一种有效的方法来解决不平衡数据分类问题，特别是当面对复杂的数据分布时，能够更好地保留少数类的信息并提高分类器的性能。

Abstract: Imbalanced classification presents a formidable challenge in machine learning, particularly when tabular datasets are plagued by noise and overlapping class boundaries. From a geometric perspective, the core difficulty lies in the topological intrusion of the majority class into the minority manifold, which obscures the true decision boundary. Traditional undersampling techniques, such as Edited Nearest Neighbours (ENN), typically employ symmetric cleaning rules and uniform voting, failing to capture the local manifold structure and often inadvertently removing informative minority samples. In this paper, we propose GMR (Geometric Manifold Rectification), a novel framework designed to robustly handle imbalanced structured data by exploiting local geometric priors. GMR makes two contributions: (1) Geometric confidence estimation that uses inverse-distance weighted kNN voting with an adaptive distance metric to capture local reliability; and (2) asymmetric cleaning that is strict on majority samples while conservatively protecting minority samples via a safe-guarding cap on minority removal. Extensive experiments on multiple benchmark datasets show that GMR is competitive with strong sampling baselines.

</details>


### [79] [Diverging Flows: Detecting Extrapolations in Conditional Generation](https://arxiv.org/abs/2602.13061)
*Constantinos Tsakonas,Serena Ivaldi,Jean-Baptiste Mouret*

Main category: cs.LG

TL;DR: 本文提出了一种名为Diverging Flows的新方法，该方法能够让单一模型同时执行条件生成和原生外推检测，通过结构性强制执行对非流形输入的低效传输。在合成流形、跨域风格转换以及天气温度预测上的评估表明，这种方法可以在不牺牲预测保真度或推理延迟的情况下有效检测外推情况。


<details>
  <summary>Details</summary>
Motivation: 流动匹配（FM）能够建模复杂的条件分布，使其成为诸如机器人技术和天气预报等预测任务中的最先进方法。然而，在安全关键环境中部署受到一个重要的外推风险的阻碍：由于平滑性偏差，即使对于非流形条件，流动模型也会产生看似合理的输出，导致无法与有效预测区分开来的无声失败。

Method: 提出了Diverging Flofs方法，它通过结构性地加强对外部输入的无效运输来实现单个模型同时进行条件生成和本征外推检测。

Result: 在合成流形、跨领域样式转移及气温预测上的测试显示，该方法能在保持预测准确性和推理速度的同时有效地识别出外推情况。

Conclusion: Diverging Flows为可靠的流动模型提供了一个稳健的解决方案，为医学、机器人学以及气候科学等领域的可靠部署铺平了道路。

Abstract: The ability of Flow Matching (FM) to model complex conditional distributions has established it as the state-of-the-art for prediction tasks (e.g., robotics, weather forecasting). However, deployment in safety-critical settings is hindered by a critical extrapolation hazard: driven by smoothness biases, flow models yield plausible outputs even for off-manifold conditions, resulting in silent failures indistinguishable from valid predictions. In this work, we introduce Diverging Flows, a novel approach that enables a single model to simultaneously perform conditional generation and native extrapolation detection by structurally enforcing inefficient transport for off-manifold inputs. We evaluate our method on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating that it achieves effective detection of extrapolations without compromising predictive fidelity or inference latency. These results establish Diverging Flows as a robust solution for trustworthy flow models, paving the way for reliable deployment in domains such as medicine, robotics, and climate science.

</details>


### [80] [Backdoor Attacks on Contrastive Continual Learning for IoT Systems](https://arxiv.org/abs/2602.13062)
*Alfous Tim,Kuniyilh Simi D*

Main category: cs.LG

TL;DR: 本文全面分析了物联网系统中对比持续学习（CCL）所面临的后门攻击问题，探讨了嵌入级攻击的目标、IoT部署特有的持久化机制，并开发了一个针对IoT的分层分类法。同时比较了不同学习范式下的漏洞，并在考虑IoT限制条件下评估了防御策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网系统对持续学习的依赖日益增加，以适应非稳态环境的变化，比如传感器漂移、用户行为变化、设备老化及对抗动态等。而将对比表示学习与增量适应相结合的方法虽然能够实现任务和领域间的鲁棒特征重用，但同时也引入了新的安全漏洞，特别是后门攻击的风险。

Method: 文章首先形式化了嵌入层攻击的目的，接着考察了IoT部署下特有的持久机制，并为IoT量身打造了一个层次化的分类体系。此外，还跨不同的学习范式比较了脆弱性，并且在考虑到IoT局限性的情况下评估了几种防御策略。

Result: 研究发现表明，尽管CCL对于提高自适应IoT智能非常有效，但如果安全保障措施不足，则可能提升长期存在的表示层威胁水平。

Conclusion: 通过本研究可以得出结论：虽然对比持续学习方法增强了物联网系统的适应性和智能化程度，但其也带来了不容忽视的安全隐患，特别是关于如何防止通过后门攻击植入持久恶意行为的问题需要进一步研究和解决。

Abstract: The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.

</details>


### [81] [Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13069)
*Juneyoung Park,Yuri Hong,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为Memory-efficient Structured Backpropagation (MeSP)的新方法，旨在解决移动设备上进行大型语言模型微调时面临的内存限制问题。通过利用LoRA的低秩结构重计算反向传播中的中间投影，MeSP在保持梯度数学一致性的前提下显著减少了内存使用量，使得以前由于内存限制而不可行的微调场景变得可能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在进行大型语言模型的设备端微调时，面临严重的内存限制问题，通常需要在高内存消耗的精确梯度与低内存但估计值嘈杂的方法之间做出权衡。

Method: 提出了Memory-efficient Structured Backpropagation (MeSP)，该方法通过手动推导反向传递过程，并利用LoRA的低秩结构来重新计算中间投影$h = xA$，从而无需存储此投影以节省内存。

Result: MeSP相比传统方法实现了平均49%的内存减少，在Qwen2.5-0.5B模型上将峰值内存从361MB降至136MB，同时保持了梯度的数学一致性。此外，分析还揭示了MeZO梯度估计与真实梯度之间几乎零相关性的问题。

Conclusion: MeSP提供了一种有效解决方案，能够在不牺牲梯度准确性的情况下大幅降低内存需求，为资源受限环境下的个性化模型微调开辟了新的可能性。

Abstract: On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \ll d_{in}$, eliminating the need to store it. MeSP achieves 49\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.

</details>


### [82] [Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic](https://arxiv.org/abs/2602.13071)
*Shuai Liu,Ning Cao,Yile Chen,Yue Jiang,Gao Cong*

Main category: cs.LG

TL;DR: 本文提出了一种名为MobTA的新方法，用于解决无目标城市移动数据的情况下生成移动轨迹的问题。该方法仅依赖源城市的移动数据和两个城市的公共巴士时刻表，并通过任务向量的算术运算实现从源城市到目标城市的参数转移。实验表明，MobTA的表现优于现有方法，接近使用目标城市真实移动数据微调模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的移动轨迹生成方法通常需要至少一部分来自目标城市的实际移动数据，这限制了它们在无法获取数据场景中的应用。为了解决这一问题，作者提出了基于公交条件下的零样本轨迹生成新设置，旨在不需要目标城市任何移动轨迹的情况下生成反映该城市出行模式的轨迹。

Method: 提出了MobTA方法，它将任务算术引入轨迹生成中。首先，MobTA利用源城市的移动数据及两地公开的公交车时间表建模从基于公交时间表的轨迹生成到移动轨迹生成之间的参数变化；然后，通过任务向量间的算术操作将这种转变应用于目标城市。此外，还对不同基础和指令调整的大语言模型下MobTA的稳定性进行了理论分析。

Result: 广泛的实验证明，即使没有目标城市的实际移动数据，MobTA也能够显著优于现有的轨迹生成方法，并且其性能接近于那些使用了目标城市移动轨迹进行微调后的模型。

Conclusion: MobTA提供了一个创新的方法来处理数据不可访问情景下的轨迹生成问题，通过引入任务算术成功地将源城市的移动模式转换为目标城市的轨迹预测，展现出强大的泛化能力与实用性。

Abstract: Mobility trajectory data provide essential support for smart city applications. However, such data are often difficult to obtain. Meanwhile, most existing trajectory generation methods implicitly assume that at least a subset of real mobility data from target city is available, which limits their applicability in data-inaccessible scenarios. In this work, we propose a new problem setting, called bus-conditioned zero-shot trajectory generation, where no mobility trajectories from a target city are accessible. The generation process relies solely on source city mobility data and publicly available bus timetables from both cities. Under this setting, we propose MobTA, the first approach to introduce task arithmetic into trajectory generation. MobTA models the parameter shift from bus-timetable-based trajectory generation to mobility trajectory generation in source city, and applies this shift to target city through arithmetic operations on task vectors. This enables trajectory generation that reflects target-city mobility patterns without requiring any real mobility data from it. Furthermore, we theoretically analyze MobTA's stability across base and instruction-tuned LLMs. Extensive experiments show that MobTA significantly outperforms existing methods, and achieves performance close to models finetuned using target city mobility trajectories.

</details>


### [83] [LCSB: Layer-Cyclic Selective Backpropagation for Memory-Efficient On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13073)
*Juneyoung Park,Eunbeen Yoon,Seongwan Kim. Jaeho Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为Layer-Cyclic Selective Backpropagation (LCSB)的新方法，通过每步仅对部分层计算梯度来提高大型语言模型在移动设备上的训练效率。LCSB相比Memory-efficient backpropagation (MeBP)能提供高达1.40倍的速度提升，并且质量下降不超过2%。此外，在4位量化设置下，LCSB还展现了更好的稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管Memory-efficient backpropagation (MeBP)已经能够让大型语言模型在内存小于1GB的移动设备上进行一阶微调，但其需要在每个步骤中通过所有Transformer层进行反向传播，这导致了显著的时间成本问题，其中权重解压缩就占到了32-42%的反向时间。

Method: 提出了Layer-Cyclic Selective Backpropagation (LCSB)，该方法每步只针对选定的一部分层计算梯度。利用残差连接保证了梯度流经身份路径，而AdamW动量则为未选中的层提供了隐式更新。LCSB被解释为LoRA参数空间上的块坐标下降法，从而为其收敛性提供了理论依据。

Result: LCSB实现了最高达1.40倍的速度提升，同时在五个模型和三个任务上的质量下降均低于2%。特别地，在4位量化环境中，对于完全发散于全反向传播下的30亿参数模型，LCSB能够使其平稳收敛，表明选择性梯度计算具有隐式的正则化效果。

Conclusion: LCSB作为一种新的反向传播方法，在不明显牺牲性能的情况下，显著提高了大型语言模型在资源受限环境如移动设备上的训练效率与稳定性。

Abstract: Memory-efficient backpropagation (MeBP) has enabled first-order fine-tuning of large language models (LLMs) on mobile devices with less than 1GB memory. However, MeBP requires backward computation through all transformer layers at every step, where weight decompression alone accounts for 32--42% of backward time. We propose Layer-Cyclic Selective Backpropagation (LCSB), which computes gradients for only a subset of layers per step. Our key insight is that residual connections guarantee gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. We interpret LCSB as Block Coordinate Descent on the LoRA parameter space, providing theoretical justification for convergence. LCSB achieves up to 1.40$\times$ speedup with less than 2\% quality degradation across five models and three tasks. Surprisingly, in 4-bit quantized settings, LCSB exhibits superior stability: a 3B model that completely diverges under full backpropagation converges smoothly with LCSB, suggesting an implicit regularization effect from selective gradient computation.

</details>


### [84] [Unified Multi-Domain Graph Pre-training for Homogeneous and Heterogeneous Graphs via Domain-Specific Expert Encoding](https://arxiv.org/abs/2602.13075)
*Chundong Liang,Yongqi Huang,Dongxiao He,Peiyuan Li,Yawen Li,Di Jin,Weixiong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种统一的多域图预训练方法GPH^2，能够同时处理同构和异构图，并通过领域特定专家编码来应对跨域分布差异，从而在下游任务中实现稳定的迁移学习。


<details>
  <summary>Details</summary>
Motivation: 当前大多数图预训练方法仅适用于同构或异构图之一，这限制了不同图类型间统一建模的能力。而现实应用中往往需要处理混合类型的图数据，且上游预训练与下游部署之间存在分布偏移。

Method: 提出了一个统一的多视图图构造方法，可以同时对同构和异构图进行编码；引入了针对特定领域的专家编码策略，每个专家独立地在一个单一图形上进行预训练以捕捉领域特有知识；设计了面向任务的专家融合策略，在下游任务中自适应地整合多个专家的知识。

Result: 广泛的实验表明，GPH^2能够在不同类型和领域的图之间实现稳定的知识迁移，性能显著优于现有的图预训练方法。

Conclusion: 通过结合同构与异构图的预训练以及采用领域特定专家编码策略，GPH^2为解决实际应用中的图数据多样性问题提供了一个有效的方法。

Abstract: Graph pre-training has achieved remarkable success in recent years, delivering transferable representations for downstream adaptation. However, most existing methods are designed for either homogeneous or heterogeneous graphs, thereby hindering unified graph modeling across diverse graph types. This separation contradicts real-world applications, where mixed homogeneous and heterogeneous graphs are ubiquitous, and distribution shifts between upstream pre-training and downstream deployment are common. In this paper, we empirically demonstrate that a balanced mixture of homogeneous and heterogeneous graph pre-training benefits downstream tasks and propose a unified multi-domain \textbf{G}raph \textbf{P}re-training method across \textbf{H}omogeneous and \textbf{H}eterogeneous graphs ($\mathbf{GPH^{2}}$). To address the lack of a unified encoder for homogeneous and heterogeneous graphs, we propose a Unified Multi-View Graph Construction that simultaneously encodes both without explicit graph-type-specific designs. To cope with the increased cross-domain distribution discrepancies arising from mixed graphs, we introduce domain-specific expert encoding. Each expert is independently pre-trained on a single graph to capture domain-specific knowledge, thereby shielding the pre-training encoder from the adverse effects of cross-domain discrepancies. For downstream tasks, we further design a Task-oriented Expert Fusion Strategy that adaptively integrates multiple experts based on their discriminative strengths. Extensive experiments on mixed graphs demonstrate that $\text{GPH}^{2}$ enables stable transfer across graph types and domains, significantly outperforming existing graph pre-training methods.

</details>


### [85] [EXCODER: EXplainable Classification Of DiscretE time series Representations](https://arxiv.org/abs/2602.13087)
*Yannik Hahn,Antonin Königsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: 本文探讨了通过将时间序列转换为离散潜在表示（如使用VQ-VAE和DVAE方法）来增强可解释性AI(XAI)技术的有效性，同时保持分类性能。提出了一种新的度量标准——相似子序列准确性(SSA)，用于量化XAI识别的重要子序列与训练数据中标签分布之间的一致性。研究结果表明，离散潜在表示不仅保留了分类所需的必要特征，还提供了更紧凑、可解释且计算效率更高的时间序列分析解释途径。


<details>
  <summary>Details</summary>
Motivation: 深度学习极大地提升了时间序列分类的性能，但这些模型缺乏可解释性仍是一个主要挑战。尽管可解释的人工智能(XAI)技术旨在使模型决策更加透明，但原始时间序列数据中的高维度性和噪声往往限制了它们的效果。

Method: 研究采用了向量量化变分自动编码器(VQ-VAE)和离散变分自动编码器(DVAE)等方法，将时间序列转换为离散潜在表示，以减少冗余并聚焦于最富信息量的模式上。此外，提出了相似子序列准确性(SSA)这一新指标，用以定量评估XAI识别出的关键子序列与训练数据中标记分布之间的一致性。

Result: 研究表明，对压缩后的表征应用XAI方法可以产生简洁而结构化的解释，这些解释在不牺牲分类表现的情况下保持了忠实度。离散潜在表征不仅保留了分类所需的基本特性，而且为时间序列分析提供了通向更加紧凑、易于理解及计算高效解释的道路。

Conclusion: 通过对时间序列进行离散化潜在表示转换，不仅可以保存分类所需的核心特征，还能显著提高基于XAI的时间序列模型解释的质量与效率。

Abstract: Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.

</details>


### [86] [R-Diverse: Mitigating Diversity Illusion in Self-Play LLM Training](https://arxiv.org/abs/2602.13103)
*Gengsheng Li,Jinghan He,Shijie Wang,Dan Zhang,Ruiqi Liu,Renrui Zhang,Zijun Yao,Junfeng Fang,Haiyun Guo,Jinqiao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为R-Diverse的方法，通过Memory-Augmented Penalty (MAP) 和 Skill-Aware Measurement (SAM) 两个创新来解决在自博弈过程中出现的多样性幻觉问题，从而持续提高大型语言模型（LLM）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自我对弈框架（如R-Zero）在训练过程中经常表现出非持续性改进的问题，即随着自我对弈的继续进行，初期获得的优势会逐渐消失。作者发现了一个关键失败模式——多样性幻觉，它表现为局部多样性和表面多样性两种形式，导致求解器看似接受了多样化训练但实际上却陷入重复模式。

Method: 为了解决上述问题，研究者提出了R-Diverse方法，其中包括两项主要创新：1. Memory-Augmented Penalty (MAP)，利用持久记忆库防止跨迭代循环；2. Skill-Aware Measurement (SAM)，根据所练习的推理技能而非问题表面变化来评估多样性。

Result: 实验结果表明，在10个数学和通用推理基准测试中，R-Diverse相比之前的方法能够在更多轮次中保持进步，并且表现更优。

Conclusion: R-Diverse通过引入新的机制有效地解决了自我对弈中的多样性幻觉问题，为提高LLM推理能力提供了一个有效途径。

Abstract: Self-play bootstraps LLM reasoning through an iterative Challenger-Solver loop: the Challenger is trained to generate questions that target the Solver's capabilities, and the Solver is optimized on the generated data to expand its reasoning skills. However, existing frameworks like R-Zero often exhibit non-sustained improvement, where early gains degrade as self-play continues. We identify a key failure mode, Diversity Illusion, where the Solver's training signals appear diverse yet collapse into recurring underlying patterns. It manifests as (1) Local Diversity Illusion, where diversity is enforced only within-batch, inducing cross-iteration mode cycling; and (2) Surface Diversity Illusion, where questions vary superficially but require near-identical reasoning skills. To mitigate them, we propose R-Diverse with two aligned innovations: Memory-Augmented Penalty (MAP), which uses a persistent memory bank to discourage recycling across iterations, and Skill-Aware Measurement (SAM), which evaluates diversity by the reasoning skills exercised rather than surface variation of questions. Across 10 math and general reasoning benchmarks, R-Diverse sustains gains over more iterations and consistently outperforms prior self-play methods. Code is available at https://github.com/Gengsheng-Li/R-Diverse.

</details>


### [87] [Quantization-Robust LLM Unlearning via Low-Rank Adaptation](https://arxiv.org/abs/2602.13151)
*João Vitor Boer Abitante,Joana Meneguzzo Pasquali,Luan Fonseca Garcia,Ewerton de Oliveira,Thomas da Silva Paula,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.LG

TL;DR: 本文提出了一种基于低秩适应(LoRA)的量化鲁棒性遗忘方法，通过冻结基础模型并将更新集中在可训练的适配器上，以在4比特量化后保持有效更新。实验结果表明，该方法在Llama-2-7B模型上提高了4比特量化的实用性，并显著减少了隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的去学习旨在从已训练模型中移除特定知识，但在实际部署时经常需要进行后训练量化(PTQ)来提高推理效率。然而，过度激进的低比特PTQ可能会掩盖或删除去学习的更新，导致量化后的模型恢复到去学习之前的行为。标准全参数微调通常引起的参数变化太小，无法在4比特量化后存活下来。

Method: 提出了一种利用低秩适应(LoRA)实现量化鲁棒性的去学习方法：冻结基础模型，并将去学习集中在可训练的适配器上，确保量化后的有效更新得以保留。

Result: 在使用MUSE数据集(BOOKS和NEWS)对Llama-2-7B模型进行评估时，LoRA方法提升了4比特量化的实用性，在某些情况下最多提高了7.93个百分点；同时，在NEWS数据集上对于GA+GDR也观察到了实用性的提升。此外，LoRA还能显著减少4比特PTQ条件下的隐私泄漏，例如对于BOOKS上的GA+KLR，PrivLeak指标从-25.68改进到了-5.86（更接近理想的0值），同时维持了强大的遗忘能力（VerMem和KnowMem接近于0）。

Conclusion: 使用LoRA进行机器去学习对于需要量化处理才能部署模型的情景是有益的，因为它不仅能够提高4比特量化后的实用性，还能显著降低隐私泄露的风险。

Abstract: Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.

</details>
