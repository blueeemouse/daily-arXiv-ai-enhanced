<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 45]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [CACTUSDB: Unlock Co-Optimization Opportunities for SQL and AI/ML Inferences](https://arxiv.org/abs/2602.23469)
*Lixi Zhou,Kanchan Chowdhury,Lulu Xie,Jaykumar Tandel,Hong Guan,Zhiwei Fan,Xinwei Fu,Jia Zou*

Main category: cs.DB

TL;DR: 本文介绍了一种新的数据库系统CactusDB，它基于Velox构建，并采用三层中间表示(IR)支持关系运算符、表达式运算符和ML函数，以及一种新颖的基于蒙特卡洛树搜索(MCTS)的优化器，以解决SQL与AI/ML模型推理查询结合执行时的协同优化问题。实验表明，该系统在处理推理工作负载时相比其他系统可达到最高441倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前数据库系统中缺乏对SQL与AI/ML模型推理查询结合执行的有效支持，导致性能不佳。为了克服现有技术无法同时支持所有协同优化方法的问题，需要开发一个能够统一不同数据和计算抽象的系统，并且设计出能有效探索指数级搜索空间的优化器。

Method: 提出并实现了一个名为CactusDB的新系统，该系统建立在高性能的UDF中心数据库引擎Velox之上。CactusDB引入了三层IR结构来灵活地优化任意子计算，并且设计了一种基于MCTS的优化器，通过查询嵌入技术使不同查询间可以共享和重用优化知识。

Result: 通过对12个代表性推理工作负载及2000个随机生成的推理查询进行评估，在如MovieLens和TPCx-AI等知名数据集上，CactusDB相对于其他系统实现了高达441倍的速度提升。

Conclusion: CactusDB提供了一种有效的解决方案，解决了将SQL与AI/ML模型推理相结合时面临的挑战，包括统一多种数据和计算抽象的能力以及利用基于MCTS的优化器高效探索大搜索空间的能力。其实验结果证明了其在提高推理查询性能方面的优越性。

Abstract: There is a growing demand for supporting inference queries that combine Structured Query Language (SQL) and Artificial Intelligence / Machine Learning (AI/ML) model inferences in database systems, to avoid data denormalization and transfer, facilitate management, and alleviate privacy concerns. Co-optimization techniques for executing inference queries in database systems without accuracy loss fall into four categories: (O1) Relational algebra optimization treating AI/ML models as black-box user-defined functions (UDFs); (O2) Factorized AI/ML inferences; (O3) Tensor-relational transformation; and (O4) General cross-optimization techniques. However, we found none of the existing database systems support all these techniques simultaneously, resulting in suboptimal performance. In this work, we identify two key challenges to address the above problem: (1) the difficulty of unifying all co-optimization techniques that involve disparate data and computation abstractions in one system; and (2) the lack of an optimizer that can effectively explore the exponential search space. To address these challenges, we present CactusDB, a novel system built atop Velox - a high-performance, UDF-centric database engine, open-sourced by Meta. CactusDB features a three-level Intermediate Representations (IR) that supports relational operators, expression operators, and ML functions to enable flexible optimization of arbitrary sub-computations. Additionally, we propose a novel Monte-Carlo Tree Search (MCTS)-based optimizer with query embedding, co-designed with our unique three-level IR, enabling shared and reusable optimization knowledge across different queries. Evaluation of 12 representative inference workloads and 2,000 randomly generated inference queries on well-known datasets, such as MovieLens and TPCx-AI, shows that CactusDB achieves up to 441 times speedup compared to alternative systems.

</details>


### [2] [GPU-Native Approximate Nearest Neighbor Search with IVF-RaBitQ: Fast Index Build and Search](https://arxiv.org/abs/2602.23999)
*Jifan Shi,Jianyang Gao,James Xia,Tamás Béla Fehér,Cheng Long*

Main category: cs.DB

TL;DR: 提出了一种新的GPU原生近似最近邻搜索解决方案IVF-RaBitQ，它结合了基于聚类的方法IVF和RaBitQ量化方法，旨在同时实现快速索引构建、高吞吐量搜索、高召回率和低存储需求。实验表明，在保证约0.95的召回率时，IVF-RaBitQ比最先进的基于图的方法CAGRA具有2.2倍的更高查询每秒(QPS)，并且平均构建索引速度提高了7.7倍。


<details>
  <summary>Details</summary>
Motivation: 为了在GPU上同时达到快速索引构建、高吞吐量搜索、高召回率以及低存储要求的目标，现有的基于图的方法虽然提供了较高的召回率和吞吐量但构建时间和存储成本较高；而基于聚类的方法虽然在构建效率上表现良好，但在需要高召回率时往往因大量查询操作而对内存带宽和计算能力造成压力。

Method: 研究者提出了IVF-RaBitQ (GPU)这一GPU原生ANNS解决方案，该方案将基于聚类的方法IVF与RaBitQ量化相结合。对于索引构建过程，开发了一个可扩展的GPU原生RaBitQ量化方法来支持大规模下的快速准确低位编码；而在搜索阶段，则设计了针对RaBitQ码的距离计算方案及融合搜索内核以提高吞吐量并保持高召回率。

Result: 通过将IVF-RaBitQ集成到NVIDIA cuVS库中并在多个数据集上进行测试后发现，该方法在召回率、吞吐量、索引构建时间和存储占用方面均表现出色。特别是当召回率约为0.95时，相比最新的基于图的方法CAGRA，IVF-RaBitQ实现了2.2倍更高的QPS，并且平均而言能够将索引构建速度加快7.7倍。

Conclusion: IVF-RaBitQ作为一项创新性的GPU原生ANNS解决方案，成功地在多种关键性能指标之间找到了平衡点，为现代检索和推荐工作负载提供了一种高效处理大规模高维向量的新途径。

Abstract: Approximate nearest neighbor search (ANNS) on GPUs is gaining increasing popularity for modern retrieval and recommendation workloads that operate over massive high-dimensional vectors. Graph-based indexes deliver high recall and throughput but incur heavy build-time and storage costs. In contrast, cluster-based methods build and scale efficiently yet often need many probes for high recall, straining memory bandwidth and compute. Aiming to simultaneously achieve fast index build, high-throughput search, high recall, and low storage requirement for GPUs, we present IVF-RaBitQ (GPU), a GPU-native ANNS solution that integrates the cluster-based method IVF with RaBitQ quantization into an efficient GPU index build/search pipeline. Specifically, for index build, we develop a scalable GPU-native RaBitQ quantization method that enables fast and accurate low-bit encoding at scale. For search, we develop GPU-native distance computation schemes for RaBitQ codes and a fused search kernel to achieve high throughput with high recall. With IVF-RaBitQ implemented and integrated into the NVIDIA cuVS Library, experiments on cuVS Bench across multiple datasets show that IVF-RaBitQ offers a strong performance frontier in recall, throughput, index build time, and storage footprint. For Recall approximately equal to 0.95, IVF-RaBitQ achieves 2.2x higher QPS than the state-of-the-art graph-based method CAGRA, while also constructing indices 7.7x faster on average. Compared to the cluster-based method IVF-PQ, IVF-RaBitQ delivers on average over 2.7x higher throughput while avoiding accessing the raw vectors for reranking.

</details>


### [3] [NSHEDB: Noise-Sensitive Homomorphic Encrypted Database Query Engine](https://arxiv.org/abs/2602.24271)
*Boram Jung,Yuliang Li,Hung-Wei Tseng*

Main category: cs.DB

TL;DR: NSHEDB, a secure query processing engine, utilizes word-level leveled homomorphic encryption (LHE) to reduce ciphertext expansion and avoid bootstrapping, enabling efficient equality, range, and aggregation operations on encrypted data with 20x-1370x speedup and 73x storage reduction over current HE-based systems.


<details>
  <summary>Details</summary>
Motivation: 尽管同态加密(HE)理论上能够直接对加密数据进行计算，从而提供强大的加密保证以实现安全和隐私保护的数据存储与查询执行，但其在数据库系统中的实际应用仍受到极大的限制，主要由于密文扩展、内存开销以及重置噪声水平以确保正确性的自举运算成本过高。

Method: NSHEDB采用基于BFV方案的字级分层HE（LHE）来最小化密文扩展并避免昂贵的自举过程。它引入了全新的技术用于纯同态计算下的等值、范围及聚合操作执行，无需在不同HE方案间转换密文或依赖可信硬件。此外，还整合了一个噪声感知查询规划器，旨在增加计算深度同时保持安全保证。

Result: 通过在真实世界数据库工作负载(TPC-H)上的实现与评估显示，相比现有基于HE的系统，NSHEDB达到了20倍至1370倍的速度提升和73倍的存储减少，并且在半诚实模型下维持了128位的安全性，没有密钥释放或信任组件。

Conclusion: 这项研究提出了一种新的系统架构层面解决方法——NSHEDB，有效解决了同态加密应用于数据库时面临的性能瓶颈问题，不仅显著提升了处理速度和减少了存储需求，同时也保证了高水平的数据安全性和隐私保护能力。

Abstract: Homomorphic encryption (HE) enables computations directly on encrypted data, offering strong cryptographic guarantees for secure and privacy-preserving data storage and query execution. However, despite its theoretical power, practical adoption of HE in database systems remains limited due to extreme cipher-text expansion, memory overhead, and the computational cost of bootstrapping, which resets noise levels for correctness.
  This paper presents NSHEDB, a secure query processing engine designed to address these challenges at the system architecture level. NSHEDB uses word-level leveled HE (LHE) based on the BFV scheme to minimize ciphertext expansion and avoid costly bootstrapping. It introduces novel techniques for executing equality, range, and aggregation operations using purely homomorphic computation, without transciphering between different HE schemes (e.g., CKKS/BFV/TFHE) or relying on trusted hardware. Additionally, it incorporates a noise-aware query planner to extend computation depth while preserving security guarantees.
  We implement and evaluate NSHEDB on real-world database workloads (TPC-H) and show that it achieves 20x-V1370x speedup and a 73x storage reduction compared to state-of-the-art HE-based systems, while upholding 128-bit security in a semi-honest model with no key release or trusted components.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [4] [Reason to Contrast: A Cascaded Multimodal Retrieval Framework](https://arxiv.org/abs/2602.23369)
*Xuanming Cui,Hong-You Chen,Hao Yu,Hao Yuan,Zihao Wang,Shlok Kumar Mishra,Hanchao Yu,Yonghuan Yang,Jun Xiao,Ser-Nam Lim,Jianpeng Cheng,Qi Guo,Xiangjun Fan*

Main category: cs.IR

TL;DR: 本文提出了TTE-v2，一种基于额外输入令牌预算而非模型或嵌入大小进行推理驱动性能扩展的混合多模态检索框架。通过增加重排序阶段来增强查询-候选交互，并提供细粒度监督以挖掘难负样本和过滤假负样本，形成一个反馈循环加强上游检索器。实验表明，TTE-v2在MMEB-V2基准测试中达到了新的最先进准确率75.7%。


<details>
  <summary>Details</summary>
Motivation: 传统多模态检索系统主要依赖于双编码器架构，其性能与嵌入维度紧密相关。最近的工作Think-Then-Embed (TTE)表明，在嵌入之前引入多模态推理以激发更多信息令牌可以进一步提高检索效果。为了进一步改进这种方法，作者提出了一种新的框架TTE-v2，旨在通过增加额外的推理步骤来进行重排序，从而实现更丰富的查询-候选互动。

Method: TTE-v2是一种结合了初始多模态检索和附加推理步骤用于重新排序的混合多模态检索框架。该方法不依靠扩大模型或嵌入尺寸而是利用额外的输入令牌预算来提升性能。重排序阶段还提供了对难负样本挖掘和假负样本过滤的细粒度监管，形成了一个能有效强化上游检索器的反馈循环。

Result: 实验结果表明，TTE-v2-7B在MMEB-V2基准上达到了75.7%的新最先进准确率，而TTE-v2-2B则能够匹敌甚至超越使用显著更大外部数据训练的主要7B模型。

Conclusion: 研究结果强调了按令牌扩展作为多模态检索另一种扩展范式所具有的潜力。

Abstract: Traditional multimodal retrieval systems rely primarily on bi-encoder architectures, where performance is closely tied to embedding dimensionality. Recent work, Think-Then-Embed (TTE), shows that incorporating multimodal reasoning to elicit additional informative tokens before embedding can further improve retrieval. In this paper, we extend this paradigm with TTE-v2, a hybrid multimodal retrieval framework that introduces reasoning-driven performance scaling based on additional input token budget rather than model or embedding size. Our approach augments the initial multimodal retrieval with additional reasoning steps for reranking, enabling more expressive query-candidate interactions at test time. The reranking stage further provides fine-grained supervision for hard negative mining and false negative filtering, creating a feedback loop that effectively strengthens the upstream retriever. This cascaded design delivers substantial test-time improvements based on intermediate reasoning token scaling. Experiments on the MMEB-V2 benchmark demonstrate that TTE-v2-7B achieves a new state-of-the-art accuracy of 75.7%, and that TTE-v2-2B matches or surpasses leading 7B models trained with significantly larger external data. Our results highlight the promise of token-wise scaling as an alternative scaling paradigm for multimodal retrieval.

</details>


### [5] [Democratizing GraphRAG: Linear, CPU-Only Graph Retrieval for Multi-Hop QA](https://arxiv.org/abs/2602.23372)
*Qizhi Wang*

Main category: cs.IR

TL;DR: 本文提出了一种名为SPRIG的仅使用CPU、线性时间复杂度且无需token的GraphRAG流程，该方法通过轻量级NER驱动的共现图替代了基于LLM的图构建，并利用个性化PageRank算法提高了多跳检索效率，同时保持了相近的召回率。


<details>
  <summary>Details</summary>
Motivation: 当前许多GraphRAG系统虽然能够改善多跳检索性能，但往往依赖于成本高昂的基于大型语言模型（LLM）的图构建以及需要大量GPU资源进行推理。为了克服这些限制，使得GraphRAG技术更加普及化，研究者们提出了一个更为高效且易于实现的新方案。

Method: 研究团队开发了一个被称为SPRIG的新框架，它完全基于CPU运行，具有线性时间复杂度，并且不需要任何token。SPRIG采用命名实体识别（NER）技术生成轻量级的共现图来代替复杂的基于LLM的图构造过程；接着，运用个性化PageRank算法完成信息检索任务。

Result: 实验结果显示，与现有方法相比，在不显著影响Recall@10的情况下，SPRIG能够将多跳检索性能提高28%。此外，研究还探讨了在何种情况下CPU友好的图形检索对增强多跳召回率特别有效，以及何时强大的词汇混合策略已经足够好。

Conclusion: 这项工作为无需高昂计算成本或GPU支持条件下推广GraphRAG提供了一条可行路径。通过引入一种创新性的非LLM依赖型图构建及检索机制——SPRIG，不仅降低了硬件要求，同时也保证了较高的检索质量。

Abstract: GraphRAG systems improve multi-hop retrieval by modeling structure, but many approaches rely on expensive LLM-based graph construction and GPU-heavy inference. We present SPRIG (Seeded Propagation for Retrieval In Graphs), a CPU-only, linear-time, token-free GraphRAG pipeline that replaces LLM graph building with lightweight NER-driven co-occurrence graphs and uses Personalized PageRank (PPR) for 28% with negligible Recall@10 changes. The results characterize when CPU-friendly graph retrieval helps multi-hop recall and when strong lexical hybrids (RRF) are sufficient, outlining a realistic path to democratizing GraphRAG without token costs or GPU requirements.

</details>


### [6] [Higress-RAG: A Holistic Optimization Framework for Enterprise Retrieval-Augmented Generation via Dual Hybrid Retrieval, Adaptive Routing, and CRAG](https://arxiv.org/abs/2602.23374)
*Weixi Lin*

Main category: cs.IR

TL;DR: 本文介绍了Higress RAG MCP服务器，这是一种旨在通过"全链路优化"策略解决从概念验证到生产级RAG系统过渡过程中遇到的主要挑战（检索精度低、生成阶段幻觉率高和实时应用延迟不可接受）的企业级架构。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型与企业知识管理系统集成面临的问题包括：复杂查询的检索精度低、生成阶段出现大量幻觉以及对于实时应用而言延迟过高。

Method: 论文提出了一种名为Higress RAG MCP Server的新架构，它基于Model Context Protocol并通过一系列创新技术如自适应路由、语义缓存、混合检索及纠正性RAG等来解决上述问题。此外还详细描述了几个关键技术实现，比如Higress-Native Splitter用于结构感知的数据摄入、Reciprocal Rank Fusion (RRF)用于合并密集型和稀疏型检索信号以及一个具有动态阈值设置的50毫秒延迟语义缓存机制。

Result: 实验评估表明，通过对整个检索生命周期进行优化——从前检索查询重写到后检索纠正评价——Higress RAG系统为企业AI部署提供了一个可扩展且抗幻觉的解决方案。

Conclusion: Higress RAG MCP服务器通过其独特设计有效解决了将大型语言模型整合进企业环境时面临的几大难题，为实现更高效准确的知识管理提供了新的途径。

Abstract: The integration of Large Language Models (LLMs) into enterprise knowledge management systems has been catalyzed by the Retrieval-Augmented Generation (RAG) paradigm, which augments parametric memory with non-parametric external data. However, the transition from proof-of-concept to production-grade RAG systems is hindered by three persistent challenges: low retrieval precision for complex queries, high rates of hallucination in the generation phase, and unacceptable latency for real-time applications. This paper presents a comprehensive analysis of the Higress RAG MCP Server, a novel, enterprise-centric architecture designed to resolve these bottlenecks through a "Full-Link Optimization" strategy. Built upon the Model Context Protocol (MCP), the system introduces a layered architecture that orchestrates a sophisticated pipeline of Adaptive Routing, Semantic Caching, Hybrid Retrieval, and Corrective RAG (CRAG). We detail the technical implementation of key innovations, including the Higress-Native Splitter for structure-aware data ingestion, the application of Reciprocal Rank Fusion (RRF) for merging dense and sparse retrieval signals, and a 50ms-latency Semantic Caching mechanism with dynamic thresholding. Experimental evaluations on domain-specific Higress technical documentation and blogs verify the system's architectural robustness. The results demonstrate that by optimizing the entire retrieval lifecycle - from pre-retrieval query rewriting to post-retrieval corrective evaluation - the Higress RAG system offers a scalable, hallucination-resistant solution for enterprise AI deployment.

</details>


### [7] [Unified Learning-to-Rank for Multi-Channel Retrieval in Large-Scale E-Commerce Search](https://arxiv.org/abs/2602.23530)
*Aditya Gaydhani,Guangyue Xu,Dhanush Kamath,Ankit Singh,Alex Li*

Main category: cs.IR

TL;DR: 本文提出了一种统一的排序模型，该模型可以从多个检索渠道合并和排序文档，并基于用户行为信号捕捉短期意图变化。实验表明，该方法比基于排名的融合方法提高了2.85%的用户转化率，同时满足生产延迟要求。


<details>
  <summary>Details</summary>
Motivation: 针对大规模电子商务搜索中如何在严格延迟限制下有效整合来自不同渠道的文档以优化业务KPI（如用户转化）的问题，现有的基于排名的融合方法由于使用固定的全局渠道权重且独立处理各渠道而无法考虑查询特定的渠道效用及跨渠道交互作用。

Method: 将多渠道融合重新定义为一个依赖于查询的学习排序问题，并提出了一个统一的排序模型，该模型能够学习从多个检索渠道合并并排序文档。此模型作为一个渠道感知的学习排序任务来共同优化点击、加入购物车和购买等操作，同时结合了渠道特定目标。此外，还加入了最近的用户行为信号来捕捉对提高多渠道排名转化至关重要的短期意图转变。

Result: 在线A/B测试显示，所提方法相较于基于排名的融合方法，在用户转化方面取得了+2.85%的提升。该模型达到了生产环境中对于延迟的要求，实现了95分位数下的延迟低于50毫秒，并已在Target.com上部署。

Conclusion: 通过引入一种新的渠道感知学习排序方法，研究成功地解决了多渠道融合中的挑战，显著提高了用户转化率，同时也保证了系统性能符合实际应用需求。

Abstract: Large-scale e-commerce search must surface a broad set of items from a vast catalog, ranging from bestselling products to new, trending, or seasonal items. Modern systems therefore rely on multiple specialized retrieval channels to surface products, each designed to satisfy a specific objective. A key challenge is how to effectively merge documents from these heterogeneous channels into a single ranked list under strict latency constraints while optimizing for business KPIs such as user conversion. Rank-based fusion methods such as Reciprocal Rank Fusion (RRF) and Weighted Interleaving rely on fixed global channel weights and treat channels independently, failing to account for query-specific channel utility and cross-channel interactions. We observe that multi-channel fusion can be reformulated as a query-dependent learning-to-rank problem over heterogeneous candidate sources. In this paper, we propose a unified ranking model that learns to merge and rank documents from multiple retrieval channels. We formulate the problem as a channel-aware learning-to-rank task that jointly optimizes clicks, add-to-carts, and purchases while incorporating channel-specific objectives. We further incorporate recent user behavioral signals to capture short-term intent shifts that are critical for improving conversion in multi-channel ranking. Our online A/B experiments show that the proposed approach outperforms rank-based fusion methods, leading to a +2.85\% improvement in user conversion. The model satisfies production latency requirements, achieving a p95 latency of under 50\,ms, and is deployed on Target.com.

</details>


### [8] [Synthetic Data Powers Product Retrieval for Long-tail Knowledge-Intensive Queries in E-commerce Search](https://arxiv.org/abs/2602.23620)
*Gui Ling,Weiyuan Li,Yue Jiang,Wenjun Peng,Xingxian Liu,Dongshuai Li,Fuyu Lv,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 本文提出了一种针对长尾、知识密集型查询的有效数据合成框架，通过将强大的离线查询重写模型的能力隐式地提炼到高效的在线检索系统中，从而改善了电子商务搜索中的产品检索问题。实验表明，仅通过将这种合成数据纳入检索模型训练中，就能显著提高用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有的产品检索系统在处理主流查询时已经进行了广泛的优化，但对于长尾查询尤其是那些需要特定领域知识推理的知识密集型查询仍存在挑战。这类查询具有多样的语言模式，往往缺乏明确的购买意图，并且由于可靠的行为日志不足而难以优化。

Method: 研究者们提出了一种专门用于处理包含长尾及知识密集型查询的数据合成框架。该方法利用大型语言模型的强大语言理解能力来训练一个多候选查询重写模型，同时使用多种奖励信号以捕捉其重写能力。通过精心策划的查询-产品对以及一个强大的离线检索管道实现这一目标。

Result: 实验结果显示，在没有采用任何额外技巧的情况下，仅仅将上述合成数据引入到检索模型训练中就能够带来显著改进。在线并行的人工评估结果也显示出用户搜索体验得到了明显提升。

Conclusion: 通过提出的高效数据合成框架能够有效解决电子商务搜索场景下长尾查询所带来的挑战，进而为用户提供更高质量的产品检索服务。

Abstract: Product retrieval is the backbone of e-commerce search: for each user query, it identifies a high-recall candidate set from billions of items, laying the foundation for high-quality ranking and user experience. Despite extensive optimization for mainstream queries, existing systems still struggle with long-tail queries, especially knowledge-intensive ones. These queries exhibit diverse linguistic patterns, often lack explicit purchase intent, and require domain-specific knowledge reasoning for accurate interpretation. They also suffer from a shortage of reliable behavioral logs, which makes such queries a persistent challenge for retrieval optimization. To address these issues, we propose an efficient data synthesis framework tailored to retrieval involving long-tail, knowledge-intensive queries. The key idea is to implicitly distill the capabilities of a powerful offline query-rewriting model into an efficient online retrieval system. Leveraging the strong language understanding of LLMs, we train a multi-candidate query rewriting model with multiple reward signals and capture its rewriting capability in well-curated query-product pairs through a powerful offline retrieval pipeline. This design mitigates distributional shift in rewritten queries, which might otherwise limit incremental recall or introduce irrelevant products. Experiments demonstrate that without any additional tricks, simply incorporating this synthetic data into retrieval model training leads to significant improvements. Online Side-By-Side (SBS) human evaluation results indicate a notable enhancement in user search experience.

</details>


### [9] [Learning to Reflect and Correct: Towards Better Decoding Trajectories for Large-Scale Generative Recommendation](https://arxiv.org/abs/2602.23639)
*Haibo Xing,Hao Deng,Lingyu Mu,Jinxin Hu,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 提出了一种名为GRC的新框架，用于改进生成式推荐系统中的解码过程。该框架通过引入生成-反思-修正流程来提高推荐质量，并使用基于GRPO的强化学习进行优化。实验表明，GRC在实际应用中能够显著提升广告收入，同时保持较低的延迟开销。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐模型通常采用单次解码方式而缺乏明确的精炼步骤，导致早期偏差累积并最终降低推荐质量。

Method: 提出了GRC框架，将标准解码扩展为生成-反思-修正的过程。具体来说，GRC引入了监督下的反思-修正模板，将解码过程分解为初始草稿生成、多粒度反思以及反思指导下的修正。此外，为了进一步探索GRC过程带来的更大范围的精炼空间，采用了基于GRPO的强化学习对整个GRC轨迹进行了优化。对于在线服务效率问题，提出了熵引导的反思调度策略（EGRS），在波束搜索过程中动态地为高不确定性解码轨迹分配更多的修正预算。

Result: 在真实数据集上的广泛实验表明，与六种最先进的基线方法相比，GRC最高可提升15.74%的表现。在线A/B测试也证明了其在大规模工业推荐场景中的实质性实用价值，实现了1.79%的广告收入增长，且仅伴有适度的延迟开销。

Conclusion: GRC框架提供了一种有效的方法来改善生成式推荐系统的推荐质量，不仅提高了性能指标，在实际部署时也展示了良好的商业价值。

Abstract: Generative Recommendation (GR) has become a promising paradigm for large-scale recommendation systems. However, existing GR models typically perform single-pass decoding without explicit refinement, causing early deviations to accumulate and ultimately degrade recommendation quality. To tackle this problem, we propose GRC, which is, to our knowledge, the first structured reflection-correction framework for GR that extends standard decoding into a Generation-Reflection-Correction (GRC) process. Concretely, GRC introduces a supervised reflection-correction template that decomposes the decoding process into initial draft generation, multi-granular reflection, and reflection-guided correction, thereby enabling structured reflection and correction in the semantic token space. To further explore the enlarged refinement space introduced by the GRC process, we optimize the entire GRC trajectory with GRPO-based reinforcement learning, under a carefully designed reward function with token-level and trajectory-level signals. For efficient online serving, we propose an Entropy-Guided Reflection Scheduling (EGRS) strategy that dynamically allocates more correction budget to high-uncertainty decoding trajectories during beam search. Extensive experiments on real-world datasets show that GRC consistently outperforms six state-of-the-art baselines by up to 15.74%, and online A/B tests demonstrate its substantial practical value in large-scale industrial recommendation, delivering a 1.79% lift in advertising revenue with only modest latency overhead.

</details>


### [10] [Geodesic Semantic Search: Learning Local Riemannian Metrics for Citation Graph Retrieval](https://arxiv.org/abs/2602.23665)
*Brandon Yee,Lucas Wang,Kundana Kommini,Krishna Sharma*

Main category: cs.IR

TL;DR: 提出了一种名为Geodesic Semantic Search (GSS)的检索系统，该系统通过学习引用图上的节点特定黎曼度量来实现几何感知语义搜索。与依赖固定欧几里得距离的标准嵌入式检索不同，GSS在每个节点上学习一个低秩度量张量，从而诱导出局部正半定度量。通过多源Dijkstra算法、最大边际相关性重排和路径一致性过滤来进行检索。在包含169,000篇论文的数据集上，GSS相比SPECTER+FAISS基准提高了23%的Recall@20，并提供了可解释的引用路径。


<details>
  <summary>Details</summary>
Motivation: 现有的基于嵌入的检索方法通常依赖于固定的欧几里得距离，这可能无法充分捕捉数据内在的复杂结构。GSS旨在通过引入一种新的方式——即在引用图上学习节点特定的黎曼度量——来改善这一点，以支持更精准且具有几何意识的语义搜索。

Method: GSS为每个节点学习一个低秩度量张量$\mL_i \in \R^{d \times r}$，进而诱导出局部正半定度量$\mG_i = \mL_i \mL_i^\top + \eps \mI$。这种参数化确保了度量的有效性同时保持模型的可行性。检索过程结合了基于学到的测地线距离的多源Dijkstra算法、最大边际相关性重排以及路径一致性过滤。此外，还提出了一种从粗到细的层次搜索策略，利用k-means池化减少计算成本。

Result: 在处理包含169,000篇论文的数据集时，GSS相对于SPECTER+FAISS基线在Recall@20指标上取得了23%的相对提升，并能够提供更加直观的引用路径。采用分层搜索策略后，在保持97%检索质量的同时，计算成本降低了4倍。

Conclusion: 研究表明，通过学习引用图中节点特定的黎曼度量来执行几何感知语义搜索是可行且有效的。GSS不仅提高了检索性能，还增强了结果的可解释性。提出的低秩度量近似方法也得到了理论分析的支持，并通过实验验证了其有效性。

Abstract: We present Geodesic Semantic Search (GSS), a retrieval system that learns node-specific Riemannian metrics on citation graphs to enable geometry-aware semantic search. Unlike standard embedding-based retrieval that relies on fixed Euclidean distances, \gss{} learns a low-rank metric tensor $\mL_i \in \R^{d \times r}$ at each node, inducing a local positive semi-definite metric $\mG_i = \mL_i \mL_i^\top + \eps \mI$. This parameterization guarantees valid metrics while keeping the model tractable. Retrieval proceeds via multi-source Dijkstra on the learned geodesic distances, followed by Maximal Marginal Relevance reranking and path coherence filtering. On citation prediction benchmarks with 169K papers, \gss{} achieves 23\% relative improvement in Recall@20 over SPECTER+FAISS baselines while providing interpretable citation paths. Our hierarchical coarse-to-fine search with k-means pooling reduces computational cost by 4$\times$ compared to flat geodesic search while maintaining 97\% retrieval quality. We provide theoretical analysis of when geodesic distances outperform direct similarity, characterize the approximation quality of low-rank metrics, and validate predictions empirically. Code and trained models are available at https://github.com/YCRG-Labs/geodesic-search.

</details>


### [11] [FuXi-Linear: Unleashing the Power of Linear Attention in Long-term Time-aware Sequential Recommendation](https://arxiv.org/abs/2602.23671)
*Yufei Ye,Wei Guo,Hao Wang,Luankang Zhang,Heng Chang,Hong Zhu,Yuyang Ye,Yong Liu,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: FuXi-Linear, a linear-complexity model, addresses the limitations of current recommendation systems by incorporating a Temporal Retention Channel and a Linear Positional Channel, which together improve handling of long sequences, reduce interference between temporal and semantic signals, and provide better positional information. The model demonstrates superior performance in both speed and recommendation quality on long sequences.


<details>
  <summary>Details</summary>
Motivation: Current recommendation systems are limited by their quadratic complexity attention mechanisms, which struggle with long user sequences and slow down inference. While linear attention offers a solution, it faces challenges such as neglecting temporal signals, insufficient positional information, and a focus on short sequences. This paper aims to address these issues with a new model called FuXi-Linear.

Method: The proposed method, FuXi-Linear, includes two key components: a Temporal Retention Channel that computes periodic attention weights using temporal data, avoiding crosstalk between temporal and semantic signals; and a Linear Positional Channel that integrates positional information through learnable kernels within linear complexity. These components work together to enable efficient processing of long sequences while maintaining or improving recommendation quality.

Result: FuXi-Linear is shown to outperform state-of-the-art models in recommendation quality for long sequences, achieving up to 10x speedup in the prefill stage and up to 21x speedup in the decode stage compared to competitive baselines. Additionally, the model exhibits power-law scaling properties at a thousand-length scale, indicating its robustness for very long sequences.

Conclusion: FuXi-Linear effectively tackles the shortcomings of existing recommendation systems by introducing a more sophisticated treatment of temporal and positional information, leading to significant improvements in both efficiency and effectiveness for long-sequence recommendations.

Abstract: Modern recommendation systems primarily rely on attention mechanisms with quadratic complexity, which limits their ability to handle long user sequences and slows down inference. While linear attention is a promising alternative, existing research faces three critical challenges: (1) temporal signals are often overlooked or integrated via naive coupling that causes mutual interference between temporal and semantic signals while neglecting behavioral periodicity; (2) insufficient positional information provided by existing linear frameworks; and (3) a primary focus on short sequences and shallow architectures. To address these issues, we propose FuXi-Linear, a linear-complexity model designed for efficient long-sequence recommendation. Our approach introduces two key components: (1) a Temporal Retention Channel that independently computes periodic attention weights using temporal data, preventing crosstalk between temporal and semantic signals; (2) a Linear Positional Channel that integrates positional information through learnable kernels within linear complexity. Moreover, we demonstrate that FuXi-Linear exhibits a robust power-law scaling property at a thousand-length scale, a characteristic largely unexplored in prior linear recommendation studies. Extensive experiments on sequences of several thousand tokens demonstrate that FuXi-Linear outperforms state-of-the-art models in recommendation quality, while achieving up to 10$\times$ speedup in the prefill stage and up to 21$\times$ speedup in the decode stage compared to competitive baselines. Our code has been released in a public repository https://github.com/USTC-StarTeam/fuxi-linear.

</details>


### [12] [Recommending Search Filters To Improve Conversions At Airbnb](https://arxiv.org/abs/2602.23717)
*Hao Li,Kedar Bellare,Siyu Yang,Sherry Chen,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 本文介绍了一种新的机器学习技术应用，用于推荐搜索过滤器以提高预订转化率。开发的过滤器推荐系统在Airbnb成功部署并通过在线A/B测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然搜索过滤器旨在促进在线市场的转化，但它们对推动转化的直接影响在现有文献中尚未得到充分探索。为了填补这一空白，本研究旨在通过推荐搜索过滤器来直接提高预订转化率。

Method: 提出了一种建模框架，该框架通过推荐中间工具（即搜索过滤器）来直接针对下层漏斗转化（预订）。利用此框架，从零开始设计并构建了Airbnb的过滤器推荐系统，并解决了冷启动和服务要求严格等挑战。

Result: 所开发的过滤器推荐系统已在Airbnb成功部署，支持多个用户界面，并通过在线A/B测试验证实现了预订转化率的增量提升。此外，消融研究进一步验证了方法及关键设计选择的有效性。

Conclusion: 本工作确保了搜索过滤器在Airbnb上服务于其最终目的——帮助客人找到并预订理想的住宿。

Abstract: Airbnb, a two-sided online marketplace connecting guests and hosts, offers a diverse and unique inventory of accommodations, experiences, and services. Search filters play an important role in helping guests navigate this variety by refining search results to align with their needs. Yet, while search filters are designed to facilitate conversions in online marketplaces, their direct impact on driving conversions remains underexplored in the existing literature.
  This paper bridges this gap by presenting a novel application of machine learning techniques to recommend search filters aimed at improving booking conversions. We introduce a modeling framework that directly targets lower-funnel conversions (bookings) by recommending intermediate tools, i.e. search filters. Leveraging the framework, we designed and built the filter recommendation system at Airbnb from the ground up, addressing challenges like cold start and stringent serving requirements.
  The filter recommendation system we developed has been successfully deployed at Airbnb, powering multiple user interfaces and driving incremental booking conversion lifts, as validated through online A/B testing. An ablation study further validates the effectiveness of our approach and key design choices. By focusing on conversion-oriented filter recommendations, our work ensures that search filters serve their ultimate purpose at Airbnb - helping guests find and book their ideal accommodations.

</details>


### [13] [UniFAR: A Unified Facet-Aware Retrieval Framework for Scientific Documents](https://arxiv.org/abs/2602.23766)
*Zheng Dou,Zhao Zhang,Deqing Wang,Yikun Ban,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 提出了一种名为UniFAR的统一检索框架，旨在解决文档中心模型与问题驱动检索之间的不匹配问题。通过自适应多粒度聚合、可学习方面锚点以及联合训练来同时支持文档-文档和问题-文档科学文献检索。实验结果表明，UniFAR在多个检索任务中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和检索增强生成（RAG）的发展，科学文献检索从基于文档间关系转向了以自然语言问题为导向的检索方式。这种转变导致了传统文档为中心的方法与新的问题驱动检索需求之间存在系统性错配，包括输入粒度、语义焦点及训练信号等方面的差异。

Method: 开发了UniFAR框架，该框架通过引入自适应多粒度聚合技术来处理不同长度文本间的差距；利用可学习的方面锚点使文档结构更好地与查询意图相匹配；并通过联合训练整合文档-文档和问题-文档两种监督形式。

Result: 实验评估显示，无论是在不同的检索任务还是基准模型上，UniFAR的表现都优于先前的技术方案。

Conclusion: UniFAR作为一种新颖且通用性强的解决方案，在促进科学文献检索向更有效的问题导向型模式转变方面展现出了显著的优势。

Abstract: Existing scientific document retrieval (SDR) methods primarily rely on document-centric representations learned from inter-document relationships for document-document (doc-doc) retrieval. However, the rise of LLMs and RAG has shifted SDR toward question-driven retrieval, where documents are retrieved in response to natural-language questions (q-doc). This change has led to systematic mismatches between document-centric models and question-driven retrieval, including (1) input granularity (long documents vs. short questions), (2) semantic focus (scientific discourse structure vs. specific question intent), and (3) training signals (citation-based similarity vs. question-oriented relevance). To this end, we propose UniFAR, a Unified Facet-Aware Retrieval framework to jointly support doc-doc and q-doc SDR within a single architecture. UniFAR reconciles granularity differences through adaptive multi-granularity aggregation, aligns document structure with question intent via learnable facet anchors, and unifies doc-doc and q-doc supervision through joint training. Experimental results show that UniFAR consistently outperforms prior methods across multiple retrieval tasks and base models, confirming its effectiveness and generality.

</details>


### [14] [RAD-DPO: Robust Adaptive Denoising Direct Preference Optimization for Generative Retrieval in E-commerce](https://arxiv.org/abs/2602.23964)
*Zhiguo Chen,Guohao Sun,Yiming Qiu,Xingzhi Yao,Mingming Li,Huimu Wang,Yangqi Zhang,Songlin Wang,Sulong Xu*

Main category: cs.IR

TL;DR: 本文提出了一种新的方法RAD-DPO，以解决直接偏好优化(DPO)在结构化语义ID(SIDs)应用中的三个主要问题：共享层次前缀的梯度冲突、对隐式反馈中噪声伪负样本的脆弱性以及多标签查询中有效候选之间的概率“挤压效应”。通过引入token级别的梯度分离、基于相似性的动态奖励权重调整及与全局SFT损失结合的多标签全局对比目标，该方法在大规模电子商务平台上的离线实验和在线A/B测试中显示出了显著的排名质量和训练效率改进。


<details>
  <summary>Details</summary>
Motivation: 生成式检索（GR）作为一种强大的范式，在电商搜索领域通过自回归解码语义ID（SIDs）来检索商品。然而，使GR适应复杂的用户偏好仍面临挑战。尽管直接偏好优化（DPO）提供了一个有效的对齐解决方案，但将其直接应用于结构化的SIDs时存在三方面的问题：一是惩罚共享的层级前缀导致梯度冲突；二是容易受到来自隐式反馈中的噪声伪负面样本影响；三是对于含有多项相关物品的多标签查询，加剧了有效候选者之间的概率‘挤压效应’。

Method: 为了解决上述问题，作者们提出了RAD-DPO方法，该方法包括以下关键组件：
- token级别的梯度分离，用来保护前缀结构不受损害。
- 基于相似性的动态奖励加权机制，旨在减轻标签噪音的影响。
- 一个与全局SFT损失相结合的多标签全局对比目标，专门设计用于明确扩大正面覆盖范围。

Result: 广泛的离线实验结果以及在一个大规模电商平台进行的在线A/B测试表明，所提方法RAD-DPO能够显著提高排序质量并增强训练效率。

Conclusion: 综上所述，RAD-DPO通过对现有DPO技术的改进，成功解决了其在处理结构化SID数据时遇到的关键难题，并且在实际应用场景下表现出了卓越性能，为生成式检索系统提供了更加高效可靠的解决方案。

Abstract: Generative Retrieval (GR) has emerged as a powerful paradigm in e-commerce search, retrieving items via autoregressive decoding of Semantic IDs (SIDs). However, aligning GR with complex user preferences remains challenging. While Direct Preference Optimization (DPO) offers an efficient alignment solution, its direct application to structured SIDs suffers from three limitations: (i) it penalizes shared hierarchical prefixes, causing gradient conflicts; (ii) it is vulnerable to noisy pseudo-negatives from implicit feedback; and (iii) in multi-label queries with multiple relevant items, it exacerbates a probability "squeezing effect" among valid candidates. To address these issues, we propose RAD-DPO, which introduces token-level gradient detachment to protect prefix structures, similarity-based dynamic reward weighting to mitigate label noise, and a multi-label global contrastive objective integrated with global SFT loss to explicitly expand positive coverage. Extensive offline experiments and online A/B testing on a large-scale e-commerce platform demonstrate significant improvements in ranking quality and training efficiency.

</details>


### [15] [Towards Efficient and Generalizable Retrieval: Adaptive Semantic Quantization and Residual Knowledge Transfer](https://arxiv.org/abs/2602.23978)
*Huimu Wang,Xingzhi Yao,Yiming Qiu,Qinghong Zhang,Haotian Wang,Yufan Cui,Songlin Wang,Sulong Xu,Mingming Li*

Main category: cs.IR

TL;DR: 提出了一种名为SA^2CRQ的新框架，通过动态分配编码长度和利用头部项目学习到的语义流形来加速尾部项目的表示学习，从而改善了基于语义ID的生成检索在处理头部和尾部项目时面临的挑战，特别是在冷启动检索场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于语义ID的生成检索方法存在一个持续的问题：头部项目容易遇到ID碰撞影响下游任务的表现，而数据稀疏的尾部项目（包括冷启动项目）则表现出有限的泛化能力。

Method: 提出了Anchored Curriculum with Sequential Adaptive Quantization (SA^2CRQ)框架，该框架包含两部分：1. Sequential Adaptive Residual Quantization (SARQ)，根据项目路径熵动态调整编码长度；2. Anchored Curriculum Residual Quantization (ACRQ)，使用从头部项目学到的固定语义流形来规范并加速尾部项目的表示学习过程。

Result: 在一个大规模工业搜索引擎及多个公开数据集上的实验结果显示，与现有基线相比，SA^2CRQ能够一致地提高性能，尤其在冷启动检索情景下效果显著。

Conclusion: SA^2CRQ框架通过引入创新的方法解决了基于语义ID的生成检索中存在的问题，为提高工业应用中的检索效率提供了一个有效的解决方案。

Abstract: While semantic ID-based generative retrieval enables efficient end-to-end modeling in industrial applications, these methods face a persistent trade-off: head items are susceptible to ID collisions that negatively impact downstream tasks, whereas data-sparse tail items, including cold-start items, exhibit limited generalization. To address this issue, we propose the Anchored Curriculum with Sequential Adaptive Quantization (SA^2CRQ) framework. The framework introduces Sequential Adaptive Residual Quantization (SARQ) to dynamically allocate code lengths based on item path entropy, assigning longer, discriminative IDs to head items and shorter, generalizable IDs to tail items. To mitigate data sparsity, the Anchored Curriculum Residual Quantization (ACRQ) component utilizes a frozen semantic manifold learned from head items to regularize and accelerate the representation learning of tail items. Experimental results from a large-scale industrial search system and multiple public datasets indicate that SA^2CRQ yields consistent improvements over existing baselines, particularly in cold-start retrieval scenarios.

</details>


### [16] [Robust Aggregation for Federated Sequential Recommendation with Sparse and Poisoned Data](https://arxiv.org/abs/2602.23982)
*Minh Hieu Nguyen*

Main category: cs.IR

TL;DR: 提出了一种针对稀疏和对抗条件下联邦序列推荐的鲁棒聚合框架，通过防御意识的聚合机制识别并降低不可靠客户端更新的影响，同时保持良性参与者的信息信号，并通过表示级约束和序列感知正则化来稳定用户和项目嵌入以及保持用户建模的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 联邦序列推荐虽然减少了隐私风险，但存在两个主要问题：一是单个客户端通常只贡献短且高度稀疏的交互序列，限制了学习到的用户表示的可靠性；二是联邦优化过程容易受到恶意或损坏的客户端更新的影响，其中中毒梯度可能会严重扭曲全局模型。这些问题在具有时间动态性的序列推荐中尤为严重。

Method: 提出了一种鲁棒聚合框架，该框架采用防御意识的聚合机制来识别并减少不可靠客户端更新的影响，同时保留来自稀疏但良性参与者的有用信号。此外，该框架引入了表示级约束以稳定用户和项目嵌入，并结合了序列感知正则化以在有限本地观察的情况下维持用户建模的时间一致性。

Result: 提出的框架能够有效地减轻恶意客户端对全局模型的负面影响，同时保持从稀疏数据中获取的有效信息。此外，它有助于维护用户和物品嵌入的稳定性，即使面对潜在的中毒或异常贡献时也能如此。还通过序列感知正则化增强了用户行为模式的时间连贯性。

Conclusion: 本研究为联邦序列推荐提供了一个新的鲁棒聚合解决方案，能够在稀疏性和对抗环境下提高模型训练的可靠性和有效性。

Abstract: Federated sequential recommendation distributes model training across user devices so that behavioural data remains local, reducing privacy risks. Yet, this setting introduces two intertwined difficulties. On the one hand, individual clients typically contribute only short and highly sparse interaction sequences, limiting the reliability of learned user representations. On the other hand, the federated optimisation process is vulnerable to malicious or corrupted client updates, where poisoned gradients can significantly distort the global model. These challenges are particularly severe in sequential recommendation, where temporal dynamics further complicate signal aggregation. To address this problem, we propose a robust aggregation framework tailored for federated sequential recommendation under sparse and adversarial conditions. Instead of relying on standard averaging, our method introduces a defence-aware aggregation mechanism that identifies and down-weights unreliable client updates while preserving informative signals from sparse but benign participants. The framework incorporates representation-level constraints to stabilise user and item embeddings, preventing poisoned or anomalous contributions from dominating the global parameter space. In addition, we integrate sequence-aware regularisation to maintain temporal coherence in user modelling despite limited local observations.

</details>


### [17] [Colour Contrast on the Web: A WCAG 2.1 Level AA Compliance Audit of Common Crawl's Top 500 Domains](https://arxiv.org/abs/2602.24067)
*Thom Vaughan,Pedro Ortiz Suarez*

Main category: cs.IR

TL;DR: 该研究通过分析Common Crawl的档案，对500个最常访问的网站进行了WCAG 2.1/2.2 AA级颜色对比度合规性的大规模自动化审计。结果显示40.9%的颜色配对未能达到正常文本所需的4.5:1对比度阈值，表明即使是高流量网站也普遍存在颜色对比度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估和提高互联网上主要网站的颜色对比度合规性，以确保这些网站对于所有用户（包括视觉障碍者）都是可访问的。通过利用公开的数据集进行静态分析，研究还旨在提供一个可重复的研究方法，同时减少对目标网站服务器的影响。

Method: 采用的方法是基于Common Crawl CC-MAIN-2026-08爬虫存档中的数据，而不是执行实时网页抓取，来分析选定网站的首页。研究特别关注了CSS文件中定义的颜色配对，并依据WCAG指南检查这些配对是否符合最低颜色对比度标准。

Result: 在分析了240个主页后，发现共有4,327种不同的前景/背景颜色组合，其中1,771种(占总数的40.9%)不符合正常文本要求的4.5:1对比度比率。各站点通过率的中位数为62.7%，只有20.4%的站点在其检测到的所有颜色组合上完全符合规定。

Conclusion: 结论指出，即使是在最受欢迎的网站上，颜色对比度问题仍然是一个普遍存在的可访问性障碍。此外，不同领域之间的合规程度存在显著差异，提示需要进一步努力来改善这一状况。

Abstract: We present a large-scale automated audit of WCAG 2.1/2.2 Level AA colour contrast compliance across the 500 most frequently crawled registered domains in Common Crawl's CC-MAIN-2026-08 February 2026 crawl archive. Rather than conducting a live crawl, all page content was sourced from Common Crawl's open WARC archives, ensuring reproducibility and eliminating any load on target web servers. Our static CSS analysis of 240 homepages identified 4,327 unique foreground/background colour pairings, of which 1,771 (40.9%) failed to meet the 4.5:1 contrast ratio threshold for normal text. The median per-site pass rate was 62.7%, with 20.4% of sites achieving full compliance across all detected colour pairings. These findings suggest that colour contrast remains a widespread accessibility barrier on the most prominent websites, with significant variation across domain categories.

</details>


### [18] [Recommendation Algorithms: A Comparative Study in Movie Domain](https://arxiv.org/abs/2602.24125)
*Rohit Chivukula,T. Jaya Lakshmi,Hemlata Sharma,C. H. S. N. P. Sairam Rallabandi*

Main category: cs.IR

TL;DR: 本文通过将推荐问题视为回归任务，利用从Netflix数据集中提取的新特征构建回归模型，并结合XGBoost、K-最近邻和基于矩阵分解的算法来改进电影推荐系统。基于矩阵分解的算法在RMSE指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 提高电子商务公司的收入，改善电影推荐系统的性能，为用户提供更准确的电影推荐。

Method: 使用Netflix挑战数据集，对用户评分行为和电影特性进行了探索性数据分析。然后从数据中提取了聚合、基于矩阵分解(MF)以及基于用户和电影相似性的多种特征。最后，采用XGBoost回归算法中的一个特征，同时利用Python的Surprise库中的K-最近邻(K-Nearest Neighbors, KNN)算法和MF算法进行推荐。

Result: 基于矩阵分解的推荐算法在根均方误差(RMSE)这一评价指标上提供了最佳的推荐结果。

Conclusion: 通过对Netflix数据集的深入分析与处理，基于矩阵分解的方法被证明是提高电影推荐系统准确性的有效手段。

Abstract: Intelligent recommendation systems have clearly increased the revenue of well-known e-commerce firms. Users receive product recommendations from recommendation systems. Cinematic recommendations are made to users by a movie recommendation system. There have been numerous approaches to the problem of recommendation in the literature. It is viewed as a regression task in this research. A regression model was built using novel properties extracted from the dataset and used as features in the model. For experimentation, the Netflix challenge dataset has been used. Video streaming service Netflix is a popular choice for many. Customers' prior viewing habits are taken into account when Netflix makes movie recommendations to them. An exploratory data analysis on the Netflix dataset was conducted to gain insights into user rating behaviour and movie characteristics. Various kinds of features, including aggregating, Matrix Factorization (MF) based, and user and movie similarity based, have been extracted in the subsequent stages. In addition to a feature in the XGBoost regression algorithm, the K-Nearest Neighbors and MF algorithms from Python's Surprise library are used for recommendations. Based on Root Mean Square Error (RMSE), MF-based algorithms have provided the best recommendations.

</details>


### [19] [Science Fiction and Fantasy in Wikipedia: Exploring Structural and Semantic Cues](https://arxiv.org/abs/2602.24229)
*Włodzimierz Lewoniewski,Milena Stróżyna,Izabela Czumałowska,Elżbieta Lewańska*

Main category: cs.IR

TL;DR: 本研究探讨了使用维基百科文章的结构和语义特征来识别与科幻和奇幻（SF/F）相关的内容，尽管这些类型的边界往往是模糊且重叠的。


<details>
  <summary>Details</summary>
Motivation: 由于科幻、奇幻及其混合体的文章类型界限模糊且常常重叠，在维基百科上辨识它们具有挑战性。然而，维基百科提供了文本之外的机器可读结构，如分类、内部链接以及对应的维基数据项声明。不过，这些信号反映了社区惯例，可能带有偏见或不完整。

Method: 这项研究通过检查维基百科文章的结构和语义特征，探索如何利用这些信息来识别与科幻和奇幻相关的条目。

Result: 该研究展示了如何利用维基百科提供的额外结构化信息来更好地理解和分类科幻及奇幻内容，尽管存在社区惯例导致的潜在偏差或信息不全问题。

Conclusion: 通过分析维基百科文章中的结构和语义特征，可以更有效地识别出与科幻和奇幻有关的内容，即使在面对类型界限模糊的情况下也是如此。

Abstract: Identifying which Wikipedia articles are related to science fiction, fantasy, or their hybrids is challenging because genre boundaries are porous and frequently overlap. Wikipedia nonetheless offers machine-readable structure beyond text, including categories, internal links (wikilinks), and statements if corresponding Wikidata items. However, each of these signals reflects community conventions and can be biased or incomplete. This study examines structural and semantic features of Wikipedia articles that can be used to identify content related to science fiction and fantasy (SF/F).

</details>


### [20] [Beyond the Click: A Framework for Inferring Cognitive Traces in Search](https://arxiv.org/abs/2602.24265)
*Saber Zerhoudi,Michael Granitzer*

Main category: cs.IR

TL;DR: 本文提出了一种从行为日志中推断认知痕迹的框架，利用信息觅食理论和人类专家判断构建多智能体系统，以提高模型在预测会话结果和用户挣扎恢复等任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的用户模拟器主要模仿用户的行为而没有理解背后的思维过程，导致无法了解用户的感受或困惑。为了解决这个问题，作者提出了一种新方法来从大规模交互日志中推断出用户的认知轨迹。

Method: 采用基于信息觅食理论（IFT）和人类专家意见的多智能体系统的方法，从用户的行为日志中推断认知轨迹。

Result: 所提出的认知轨迹能够改进模型在诸如预测会话结果和帮助用户克服困难等任务中的性能，并且作者还发布了一些公开数据集的注释以及一个开源工具，允许研究人员将该方法应用于自己的数据上。

Conclusion: 这项工作提供了构建更加人性化的用户模拟器所需的工具和数据，同时也为检索系统在以用户为中心的性能维度上的评估提供了可能。

Abstract: User simulators are essential for evaluating search systems, but they primarily copy user actions without understanding the underlying thought process. This gap exists since large-scale interaction logs record what users do, but not what they might be thinking or feeling, such as confusion or satisfaction. To solve this problem, we present a framework to infer cognitive traces from behavior logs. Our method uses a multi-agent system grounded in Information Foraging Theory (IFT) and human expert judgment. These traces improve model performance on tasks like forecasting session outcomes and user struggle recovery. We release a collection of annotations for several public datasets, including AOL and Stack Overflow, and an open-source tool that allows researchers to apply our method to their own data. This work provides the tools and data needed to build more human-like user simulators and to assess retrieval systems on user-oriented dimensions of performance.

</details>


### [21] [Resources for Automated Evaluation of Assistive RAG Systems that Help Readers with News Trustworthiness Assessment](https://arxiv.org/abs/2602.24277)
*Dake Zhang,Mark D. Smucker,Charles L. A. Clarke*

Main category: cs.IR

TL;DR: The TREC 2025 DRAGUN Track introduced tasks for developing systems to help readers assess news trustworthiness. The track featured two tasks: Question Generation and Report Generation, with an automated judging process (AutoJudge) that closely matched human-assessed rankings, enabling future research on RAG systems and their evaluation.


<details>
  <summary>Details</summary>
Motivation: 鉴于在线新闻中可靠报道与错误信息并存，读者难以评估新闻的可信度。为此，TREC 2025 DRAGUN 轨道为研究者提供了一个开发和评估辅助RAG系统的机会，旨在通过生成面向读者、有充分引用的报告来支持对新闻可信度的评估。

Method: 组织者设计了两个任务：（任务1）问题生成，产生10个排名调查问题；以及（任务2，主要任务）报告生成，基于MS MARCO V2.1分段语料库生成一篇250字的报告。此外，还创建了一个自动评判过程（AutoJudge），用于评估未参与原始评估的任务提交。

Result: AutoJudge在任务1和任务2上的评分与TREC人工评估相比显示出良好的一致性(Kendall's τ分别为0.678和0.872)，这表明该自动化评判过程能够有效地替代人工评估。

Conclusion: 这些资源不仅支持了辅助性新闻可信度评估RAG系统的评测，并且以人工评估作为基准，促进了关于改进自动化RAG评估的研究。

Abstract: Many readers today struggle to assess the trustworthiness of online news because reliable reporting coexists with misinformation. The TREC 2025 DRAGUN (Detection, Retrieval, and Augmented Generation for Understanding News) Track provided a venue for researchers to develop and evaluate assistive RAG systems that support readers' news trustworthiness assessment by producing reader-oriented, well-attributed reports. As the organizers of the DRAGUN track, we describe the resources that we have newly developed to allow for the reuse of the track's tasks. The track had two tasks: (Task 1) Question Generation, producing 10 ranked investigative questions; and (Task 2, the main task) Report Generation, producing a 250-word report grounded in the MS MARCO V2.1 Segmented Corpus. As part of the track's evaluation, we had TREC assessors create importance-weighted rubrics of questions with expected short answers for 30 different news articles. These rubrics represent the information that assessors believe is important for readers to assess an article's trustworthiness. The assessors then used their rubrics to manually judge the participating teams' submitted runs. To make these tasks and their rubrics reusable, we have created an automated process to judge runs not part of the original assessing. We show that our AutoJudge ranks existing runs well compared to the TREC human-assessed evaluation (Kendall's $τ= 0.678$ for Task 1 and $τ= 0.872$ for Task 2). These resources enable both the evaluation of RAG systems for assistive news trustworthiness assessment and, with the human evaluation as a benchmark, research on improving automated RAG evaluation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Peeling Off the Cocoon: Unveiling Suppressed Golden Seeds for Mutational Greybox Fuzzing](https://arxiv.org/abs/2602.23736)
*Ruixiang Qian,Chunrong Fang,Zengxu Chen,Youxin Fu,Zhenyu Chen*

Main category: cs.SE

TL;DR: PoCo技术旨在通过逐步移除障碍条件语句并进行更深入的种子选择来改进现代基于覆盖率的种子选择（CSS）技术。


<details>
  <summary>Details</summary>
Motivation: 为了提高现有的基于覆盖率的种子选择技术的效果，特别是解决由于障碍条件语句导致的选择深度不足的问题。

Method: 采用逐步移除障碍条件语句的方法，并结合更深层次的种子选择过程。

Result: 增强了现有CSS技术的能力，允许更深层次地探索程序路径。

Conclusion: PoCo为改善基于覆盖率的种子选择提供了一种有效的新方法，特别是在处理复杂程序逻辑时。

Abstract: PoCo is a technique that aims to enhance modern coverage-based seed selection (CSS) techniques (such as afl-cmin) by gradually removing obstacle conditional statements and conducting deeper seed selection.

</details>


### [23] [The Vocabulary of Flaky Tests in the Context of SAP HANA](https://arxiv.org/abs/2602.23957)
*Alexander Berndt,Zoltán Nochta,Thomas Bach*

Main category: cs.SE

TL;DR: 本研究在大型工业项目SAP HANA中评估了基于测试代码中的源代码标识符来识别不稳定测试及其根本原因的方法。通过复制Pinto等人的先前工作，并评估不同的特征提取技术（TF-IDF和TF-IDFC-RF）以及分类模型（CodeBERT和XGBoost），结果显示，虽然取得了较高的F1分数（最高达0.99），但这些结果对于开发者来说并不具有实际操作性，因为它们难以直接转化为可行动的建议。


<details>
  <summary>Details</summary>
Motivation: 自动化测试执行是获取软件项目质量信息的重要活动，然而所谓的不稳定测试对这一过程产生了负面影响。这类测试看似随机失败且与代码更改无关，因此不能提供清晰的质量信号。之前的工作提出了一种基于测试代码中的源代码标识符来识别不稳定测试的方法，但尚未在大规模工业环境中得到评估。

Method: 首先，在SAP HANA的背景下复制了Pinto等人之前的研究成果；其次，评估了不同的特征提取技术，包括TF-IDF和TF-IDFC-RF；最后，使用CodeBERT和XGBoost作为分类模型进行评测。为了确保比较的有效性，研究采用了来自先前工作的数据集及两个来自SAP HANA的数据集。

Result: 复制研究表明，在原始数据集和其中一个SAP HANA数据集上得到了相似的结果。原方法在原始数据集上获得了0.94的F1-Score，在SAP HANA数据集上为0.92；而新扩展方法则分别达到了0.96和0.99的F1-Score。依赖外部数据源被确定为SAP HANA环境下测试不稳定性的常见根源之一。

Conclusion: 尽管大型工业项目的词汇表可能与具体术语略有不同，但在诸如远程依赖这样的类别方面，与之前的实证发现相似。不过，即使拥有相当高的F1-Score值，无论是找到代表不稳定的源代码标识符还是进行黑盒预测，在实践中都因结果缺乏可操作性而限制了其应用价值。

Abstract: Background. Automated test execution is an important activity to gather information about the quality of a software project. So-called flaky tests, however, negatively affect this process. Such tests fail seemingly at random without changes to the code and thus do not provide a clear signal. Previous work proposed to identify flaky tests based on the source code identifiers in the test code. So far, these approaches have not been evaluated in a large-scale industrial setting. Aims. We evaluate approaches to identify flaky tests and their root causes based on source code identifiers in the test code in a large-scale industrial project. Method. First, we replicate previous work by Pinto et al. in the context of SAP HANA. Second, we assess different feature extraction techniques, namely TF-IDF and TF-IDFC-RF. Third, we evaluate CodeBERT and XGBoost as classification models. For a sound comparison, we utilize both the data set from previous work and two data sets from SAP HANA. Results. Our replication shows similar results on the original data set and on one of the SAP HANA data sets. While the original approach yielded an F1-Score of 0.94 on the original data set and 0.92 on the SAP HANA data set, our extensions achieve F1-Scores of 0.96 and 0.99, respectively. The reliance on external data sources is a common root cause for test flakiness in the context of SAP HANA. Conclusions. The vocabulary of a large industrial project seems to be slightly different with respect to the exact terms, but the categories for the terms, such as remote dependencies, are similar to previous empirical findings. However, even with rather large F1-Scores, both finding source code identifiers for flakiness and a black box prediction have limited use in practice as the results are not actionable for developers.

</details>


### [24] [Context-Aware Functional Test Generation via Business Logic Extraction and Adaptation](https://arxiv.org/abs/2602.24108)
*Yakun Zhang,Zihan Wang,Xinzhi Peng,Zihao Xie,Xiaodong Wang,Xutao Li,Dan Hao,Lu Zhang,Yunming Ye*

Main category: cs.SE

TL;DR: LogiDroid, a two-stage approach for generating functional test cases by extracting business logic and adapting it to the target mobile applications, significantly outperforms state-of-the-art methods in testing functional requirements, with improvements of over 48% and 55% on two different datasets.


<details>
  <summary>Details</summary>
Motivation: Functional testing is crucial for ensuring that mobile applications meet user requirements, but it faces challenges due to the difficulty in acquiring and reusing complex business logic from unstructured requirements and the semantic gap when applying this logic to various GUI environments. This hinders the automatic generation of effective test cases.

Method: The proposed method, LogiDroid, consists of two stages: Knowledge Retrieval and Fusion, where a dataset is used to retrieve relevant cases and extract business logic; and Context-Aware Test Generation, which analyzes the extracted business logic and the real-time GUI environment to generate functional test cases.

Result: Experiments on two widely-used datasets covering 28 real-world applications and 190 functional requirements show that LogiDroid successfully tested 40% of the functional requirements on the FrUITeR dataset (an improvement of over 48%) and 65% on the Lin dataset (an improvement of over 55%) compared to existing methods.

Conclusion: LogiDroid effectively addresses the challenges in functional testing for mobile applications by accurately understanding application semantics and leveraging domain expertise, leading to significant improvements in the success rate of testing functional requirements.

Abstract: Functional testing is essential for verifying that the business logic of mobile applications aligns with user requirements, serving as the primary methodology for quality assurance in software development. Despite its importance, functional testing remains heavily dependent on manual effort due to two core challenges. First, acquiring and reusing complex business logic from unstructured requirements remains difficult, which hinders the understanding of specific functionalities. Second, a significant semantic gap exists when adapting business logic to the diverse GUI environments, which hinders the generation of test cases for specific mobile applications. To address the preceding challenges, we propose LogiDroid, a two-stage approach that generates individual functional test cases by extracting business logic and adapting it to target applications. First, in the Knowledge Retrieval and Fusion stage, we construct a dataset to retrieve relevant cases and extract business logic for the target functionality. Second, in the Context-Aware Test Generation stage, LogiDroid jointly analyzes the extracted business logic and the real-time GUI environment to generate functional test cases. This design allows LogiDroid to accurately understand application semantics and use domain expertise to generate complete test cases with verification assertions. We assess the effectiveness of LogiDroid using two widely-used datasets that cover 28 real-world applications and 190 functional requirements. Experimental results show that LogiDroid successfully tested 40% of functional requirements on the FrUITeR dataset (an improvement of over 48% compared to the state-of-the-art approaches) and 65% on the Lin dataset (an improvement of over 55% compared to the state-of-the-art approaches). These results demonstrate the significant effectiveness of LogiDroid in functional test generation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [25] [QoSFlow: Ensuring Service Quality of Distributed Workflows Using Interpretable Sensitivity Models](https://arxiv.org/abs/2602.23598)
*Md Hasanur Rashid,Jesun Firoz,Nathan R. Tallent,Luanzheng Guo,Meng Tang,Dong Dai*

Main category: cs.DC

TL;DR: 本文提出了一种名为QoSFlow的性能建模方法，用于将工作流的执行配置空间划分为具有相似行为的区域，从而实现有效的QoS驱动调度。通过三个不同工作流的评估表明，QoSFlow提供的执行建议比最佳标准启发式方法高出27.38%。


<details>
  <summary>Details</summary>
Motivation: 随着分布式科学工作流的重要性日益增加，确保服务质量（QoS）约束变得至关重要，比如最小化时间或限制在资源子集内执行。然而，即使是在类似配置下，工作流行为也难以预测，这使得提供QoS保证变得困难。为了解决这一问题并有效处理QoS调度，提出了新的解决方案。

Method: 引入了QoSFlow，一种性能建模方法，它能够将工作流的执行配置空间分割成拥有相似行为特性的区域。每个区域根据给定的统计敏感度对具有可比执行时间的配置进行分组，从而允许通过分析推理而不是详尽测试来实现高效的QoS驱动调度。

Result: 通过对三个不同工作流的评估显示，QoSFlow给出的执行推荐比表现最好的标准启发式算法提高了27.38%。实证验证还证实，对于不同的QoS约束条件，QoSFlow推荐的配置始终与测量到的执行结果相匹配。

Conclusion: QoSFlow证明了其作为一种新方法的有效性，能够显著提高QoS驱动调度的表现，并且其推荐的配置能够在实际应用中准确地反映预期的行为特性。

Abstract: With the increasing importance of distributed scientific workflows, there is a critical need to ensure Quality of Service (QoS) constraints, such as minimizing time or limiting execution to resource subsets. However, the unpredictable nature of workflow behavior, even with similar configurations, makes it difficult to provide QoS guarantees. For effective reasoning about QoS scheduling, we introduce QoSFlow, a performance modeling method that partitions a workflow's execution configuration space into regions with similar behavior. Each region groups configurations with comparable execution times according to a given statistical sensitivity, enabling efficient QoS-driven scheduling through analytical reasoning rather than exhaustive testing. Evaluation on three diverse workflows shows that QoSFlow's execution recommendations outperform the best-performing standard heuristic by 27.38%. Empirical validation confirms that QoSFlow's recommended configurations consistently match measured execution outcomes across different QoS constraints.

</details>


### [26] [Green or Fast? Learning to Balance Cold Starts and Idle Carbon in Serverless Computing](https://arxiv.org/abs/2602.23935)
*Bowen Sun,Christos D. Antonopoulos,Evgenia Smirni,Bin Ren,Nikolaos Bellas,Spyros Lalis*

Main category: cs.DC

TL;DR: 提出了一种名为LACE-RL的管理框架，利用深度强化学习动态调整无服务器计算中函数实例的保持活动时间，以平衡服务延迟和碳排放。基于华为公共云跟踪数据的实验表明，该方法在减少冷启动次数和空闲时保持活动状态的碳排放方面优于静态策略和其他基准方法。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算简化了云部署过程，但带来了新的挑战，如如何有效管理服务延迟和降低碳排放。当前存在的问题包括减少冷启动延迟需要保留热函数实例，而减少碳排放则偏好回收闲置资源。此外，电网碳强度随时间和工作负载模式变化，使得固定的保持活动策略效率低下。

Method: 提出了LACE-RL框架，将无服务器容器保留问题定义为一个序列决策问题，并采用深度强化学习技术来动态调整保持活动的时间长度。此方法同时考虑了冷启动概率、特定于函数的延迟成本以及实时碳强度等因素。

Result: 通过华为公共云追踪数据进行评估的结果显示，与华为采用的静态策略相比，LACE-RL能够减少51.69%的冷启动次数及77.08%因空闲保持活动导致的碳排放。此外，在权衡延迟与碳排放方面，LACE-RL也展现出了优于现有启发式方法和单一目标基线方案的表现，接近理想性能。

Conclusion: 研究证明了LACE-RL能够在实际应用中有效地平衡无服务器环境下的服务延迟与碳排放问题，为未来云计算平台提供了一种可持续发展的管理方案。

Abstract: Serverless computing simplifies cloud deployment but introduces new challenges in managing service latency and carbon emissions. Reducing cold-start latency requires retaining warm function instances, while minimizing carbon emissions favors reclaiming idle resources. This balance is further complicated by time-varying grid carbon intensity and varying workload patterns, under which static keep-alive policies are inefficient. We present LACE-RL, a latency-aware and carbon-efficient management framework that formulates serverless pod retention as a sequential decision problem. LACE-RL uses deep reinforcement learning to dynamically tune keep-alive durations, jointly modeling cold-start probability, function-specific latency costs, and real-time carbon intensity. Using the Huawei Public Cloud Trace, we show that LACE-RL reduces cold starts by 51.69% and idle keep-alive carbon emissions by 77.08% compared to Huawei's static policy, while achieving better latency-carbon trade-offs than state-of-the-art heuristic and single-objective baselines, approaching Oracle performance.

</details>


### [27] [Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving](https://arxiv.org/abs/2602.24044)
*Ferran Agullo,Joan Oliveras,Chen Wang,Alberto Gutierrez-Torre,Olivier Tardieu,Alaa Youssef,Jordi Torres,Josep Ll. Berral*

Main category: cs.DC

TL;DR: 本文提出了一种数据驱动的流水线，用于计算给定工作负载下大型语言模型适配器的放置方案，以最少数量的GPU服务工作负载，同时避免请求饥饿和GPU内存错误。该流水线集成了数字孪生、机器学习模型以及贪婪放置算法三个组件，能够大幅提高GPU效率，并且可以适应其他优化目标如延迟最小化。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型适配器在分布式服务系统中引入了复杂的缓存和调度挑战，尽管已有研究主要集中在减少延迟上，但通过最大化吞吐量来提高资源效率仍待探索。

Method: 构建了一个包括特定于LLM-适配器服务的数字孪生、基于DT生成数据训练的简化机器学习模型以及利用基于ML性能估计的贪婪放置算法在内的数据驱动流水线。

Result: 实验结果表明，该流水线能显著提高GPU效率，减少了支持目标工作负载所需的GPU数量。此外，该方法还可以调整以适应其他目标，比如最小化延迟。

Conclusion: 提出的数据驱动流水线有效解决了大规模语言模型适配器在分布式环境中面临的资源效率问题，为未来的大规模LLM服务基础设施提供了多功能解决方案。

Abstract: Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [28] [MSVBench: Towards Human-Level Evaluation of Multi-Shot Video Generation](https://arxiv.org/abs/2602.23969)
*Haoyuan Shi,Yunxin Li,Nanhao Deng,Zhenran Xu,Xinyu Chen,Longyue Wang,Baotian Hu,Min Zhang*

Main category: cs.MM

TL;DR: 本文介绍了MSVBench，这是首个专为多镜头视频生成设计的基准测试，它结合了大型多模态模型的高级语义推理与领域特定专家模型的精细感知严谨性，以评估视频生成方法。实验表明现有模型主要作为视觉插值器而非真实世界模型，并且通过微调轻量级模型可以达到与商业模型相当的人类对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成评价方法无法满足复杂、多镜头叙事的需求，缺乏全面的故事资源和跨镜头度量来评估长篇连贯性和吸引力。

Method: 提出了MSVBench，一个包含分层脚本和参考图像的综合基准测试，用于多镜头视频生成。同时引入了一种混合评估框架，将大型多模态模型的高级语义推理能力与领域特定专家模型的细致感知严谨性相结合。

Result: 通过对20种不同范式的视频生成方法进行评估，发现当前模型虽然具有很强的视觉逼真度，但主要表现为视觉插值器而不是真正意义上的世界模型。此外，该基准测试与人类判断之间达到了94.4%的相关性。进一步地，通过在MSVBench提供的精炼推理轨迹上微调轻量级模型，可以获得与商业模型如Gemini-2.5-Flash相媲美的人类对齐表现。

Conclusion: MSVBench不仅填补了多镜头视频生成领域内综合性评估工具的空白，而且提供了一个可扩展的监督信号，有助于提高视频生成技术的整体水平。

Abstract: The evolution of video generation toward complex, multi-shot narratives has exposed a critical deficit in current evaluation methods. Existing benchmarks remain anchored to single-shot paradigms, lacking the comprehensive story assets and cross-shot metrics required to assess long-form coherence and appeal. To bridge this gap, we introduce MSVBench, the first comprehensive benchmark featuring hierarchical scripts and reference images tailored for Multi-Shot Video generation. We propose a hybrid evaluation framework that synergizes the high-level semantic reasoning of Large Multimodal Models (LMMs) with the fine-grained perceptual rigor of domain-specific expert models. Evaluating 20 video generation methods across diverse paradigms, we find that current models--despite strong visual fidelity--primarily behave as visual interpolators rather than true world models. We further validate the reliability of our benchmark by demonstrating a state-of-the-art Spearman's rank correlation of 94.4% with human judgments. Finally, MSVBench extends beyond evaluation by providing a scalable supervisory signal. Fine-tuning a lightweight model on its pipeline-refined reasoning traces yields human-aligned performance comparable to commercial models like Gemini-2.5-Flash.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Detoxifying LLMs via Representation Erasure-Based Preference Optimization](https://arxiv.org/abs/2602.23391)
*Nazanin Mohammadi Sepahvand,Eleni Triantafillou,Hugo Larochelle,Doina Precup,Daniel M. Roy,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 本文提出了一种名为REPO的新方法，通过将有害内容的表示收敛到无害内容来解决大型语言模型产生有毒输出的问题。这种方法能够实现深度且局部的编辑，同时保持模型的一般实用性，并对现有方法无法应对的复杂威胁表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施虽然减少了有害延续的可能性，但它们容易受到对抗性提示的影响，并且可以通过基于微调的重新学习攻击轻易被逆转。研究发现，这些对模型的修改是表面的，线性探测显示有害的“方向”仍然存在于表示中。

Method: 提出了基于表示擦除的偏好优化（REPO），将去毒化重构成一个令牌级别的偏好问题。利用一种新的目标和偏好数据，强制有毒延续的表示向其良性对应物靠拢。

Result: REPO实现了最前沿的鲁棒性，能够阻止包括重新学习攻击和增强型GCG越狱在内的复杂威胁，在这一点上，现有的基于表示和输出的方法失败了。

Conclusion: REPO不仅有效解决了大型语言模型生成有毒输出的问题，而且在面对复杂的攻击方式时也展现出了前所未有的鲁棒性，为安全部署这类模型提供了有力支持。

Abstract: Large language models (LLMs) trained on webscale data can produce toxic outputs, raising concerns for safe deployment. Prior defenses, based on applications of DPO, NPO, and similar algorithms, reduce the likelihood of harmful continuations, but not robustly so: they are vulnerable to adversarial prompting and easily undone by fine-tuning-based relearning attacks. Indeed, research has shown that these edits to the model are superficial: linear probing reveals that harmful "directions" remain present in representations. To address this, we propose Representation Erasure-based Preference Optimization (REPO), reformulating detoxification as a token-level preference problem. Using a novel objective with preference data, we force the representations of toxic continuations to converge toward their benign counterparts. Our mechanistic analysis reveals that this granular approach is critical: unlike baselines, REPO induces deep, localized edits to toxicity-encoding neurons while preserving general model utility. Exhaustive evaluations show that REPO achieves state-of-the-art robustness, stopping sophisticated threats-including relearning attacks and enhanced GCG jailbreaks-where existing representation- and output-based methods fail.

</details>


### [30] [U-CAN: Utility-Aware Contrastive Attenuation for Efficient Unlearning in Generative Recommendation](https://arxiv.org/abs/2602.23400)
*Zezheng Wu,Rui Wang,Xinghe Cheng,Yang Shao,Qing Yang,Jiapu Wang,Jingwei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为U-CAN的精准遗忘框架，该框架通过对比激活来量化风险，并对敏感数据进行选择性降级，同时保持推理电路的拓扑连通性。实验表明，U-CAN在隐私遗忘、效用保留和计算效率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统（GenRec）通常利用大型语言模型（LLMs）将个性化重新定义为指令驱动的序列生成任务。但是，在用户日志上微调时会无意中将敏感属性编码到模型参数中，引发了严重的隐私问题。现有的机器遗忘技术因多义困境而难以解决这一矛盾，导致在使用传统梯度或剪枝方法时出现灾难性的效用损失。

Method: 提出了Utility-aware Contrastive AttenuatioN (U-CAN)框架，一种基于低秩适配器的精准遗忘解决方案。U-CAN通过对激活情况进行对比来衡量风险，专注于那些对于遗忘集合高度敏感但被抑制在保留集合中的神经元。为了保护性能，引入了结合权重大小与保留集合激活范数的效用感知校准机制，给对保留性能贡献大的维度分配更高的效用分数。不同于二进制剪枝，U-CAN开发了自适应软衰减，采用可微衰减函数有选择地下调LoRA适配器上的高风险参数，抑制敏感检索路径并维持推理电路的拓扑连接性。

Result: 在两个公开数据集上进行的实验展示了U-CAN在七个指标上的优越表现，证明了其在实现强大隐私遗忘的同时还能有效保留模型效用，并且具备良好的计算效率。

Conclusion: U-CAN提供了一个有效的解决方案，能够在不显著损害模型性能的情况下处理由个人化推荐系统带来的隐私泄露问题。它不仅能够精确地删除敏感信息，同时也成功地维护了模型的核心功能不受影响。

Abstract: Generative Recommendation (GenRec) typically leverages Large Language Models (LLMs) to redefine personalization as an instruction-driven sequence generation task. However, fine-tuning on user logs inadvertently encodes sensitive attributes into model parameters, raising critical privacy concerns. Existing Machine Unlearning (MU) techniques struggle to navigate this tension due to the Polysemy Dilemma, where neurons superimpose sensitive data with general reasoning patterns, leading to catastrophic utility loss under traditional gradient or pruning methods. To address this, we propose Utility-aware Contrastive AttenuatioN (U-CAN), a precision unlearning framework that operates on low-rank adapters. U-CAN quantifies risk by contrasting activations and focuses on neurons with asymmetric responses that are highly sensitive to the forgetting set but suppressed on the retention set. To safeguard performance, we introduce a utility-aware calibration mechanism that combines weight magnitudes with retention-set activation norms, assigning higher utility scores to dimensions that contribute strongly to retention performance. Unlike binary pruning, which often fragments network structure, U-CAN develop adaptive soft attenuation with a differentiable decay function to selectively down-scale high-risk parameters on LoRA adapters, suppressing sensitive retrieval pathways and preserving the topological connectivity of reasoning circuits. Experiments on two public datasets across seven metrics demonstrate that U-CAN achieves strong privacy forgetting, utility retention, and computational efficiency.

</details>


### [31] [Long Range Frequency Tuning for QML](https://arxiv.org/abs/2602.23409)
*Michael Poppel,Jonas Stein,Sebastian Wölckert,Markus Baumann,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 本文探讨了量子机器学习模型中频率可调方法的局限性，并提出了一种基于三进制编码的网格初始化方法来克服这些限制，从而在合成目标和实际数据集上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决量子机器学习模型中频率可调方法的实际有效性问题，特别是当梯度优化无法将系数驱动到任意目标值时遇到的挑战。

Method: 通过系统实验揭示频率系数的有限可训练性后，提出了一种基于三进制编码的网格初始化方法，该方法生成密集整数频率谱，确保目标频率位于局部可达范围内。

Result: 对于包含三个偏移高频的合成目标，三进制网格初始化方法达到了0.9969的R^2分数，而频率可调基线仅为0.1841；在Flight Passengers真实世界数据集上，该方法相比频率可调初始化提高了22.8%的R^2分数（从0.7876提高到了0.9671）。

Conclusion: 提出的三进制网格初始化方法有效解决了频率可调方法中的频率可达性限制问题，显著提升了量子机器学习模型在模拟与实际应用场景下的表现。

Abstract: Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876).

</details>


### [32] [FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments](https://arxiv.org/abs/2602.23504)
*Anik Pramanik,Murat Kantarcioglu,Vincent Oria,Shantanu Sharma*

Main category: cs.LG

TL;DR: FedDAG是一种新的联邦学习框架，它通过结合数据和梯度信息来衡量客户端之间的相似性，并采用双编码器架构促进跨集群特征迁移，从而在不同基准测试中优于现有的聚类联邦学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前的聚类联邦学习方法仅依赖于数据相似性或梯度相似性来评估客户端相似性，这种方法不够全面；此外，这些方法限制了知识与表示共享给同一集群内的客户端，阻碍了模型从不同集群间的多样化客户端群体中获益。

Method: FedDAG提出了一种加权的、基于类别的相似性度量标准，该标准整合了数据与梯度信息，以提供更全面的相似性度量用于聚类过程。同时，FedDAG利用了一个双编码器架构，其中一个主编码器使用自己客户端的数据进行训练，而次级编码器则通过来自互补集群的梯度信息进行优化，以此实现跨集群的特征转移同时也保持了集群特定的专业化。

Result: 实验表明，在多样化的基准测试和数据异质性设置下，FedDAG在准确性方面始终优于最先进的聚类联邦学习基线方法。

Conclusion: FedDAG通过引入一种新的相似性度量方式及创新性的双编码器架构，有效解决了现有聚类联邦学习方法中存在的问题，为处理非同构客户数据提供了更优解。

Abstract: Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy.

</details>


### [33] [Brain-OF: An Omnifunctional Foundation Model for fMRI, EEG and MEG](https://arxiv.org/abs/2602.23410)
*Hanning Guo,Farah Abdellatif,Hanwen Bi,Andrei Galbenus,Jon. N. Shah,Abigail Morrison,Jürgen Dammers*

Main category: cs.LG

TL;DR: 本文提出了Brain-OF，首个能够处理fMRI、EEG和MEG数据的全功能脑基础模型，并通过引入任意分辨率神经信号采样器、DINT注意力机制与稀疏专家混合体以及掩码时频建模方法来实现跨模态的数据整合。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数脑基础模型仅限于单一功能模态，限制了它们利用跨成像技术互补的空间时间动态和集体数据规模的能力。为了解决这一局限性，研究者开发了一个能够在统一框架下处理单模态和多模态输入的全功能脑基础模型。

Method: 提出了一种名为Brain-OF的新模型，该模型联合预训练了fMRI、EEG和MEG等不同类型的脑信号；引入了Any-Resolution Neural Signal Sampler用于将不同的脑信号投影到一个共享语义空间中；采用了结合DINT注意力机制与Sparse Mixture of Experts的设计来管理语义偏移；还提出了Masked Temporal-Frequency Modeling作为双域预训练目标。

Result: Brain-OF在大约40个数据集上进行了预训练，并且在各种下游任务上表现出了优越的性能，证明了联合多模态集成和双域预训练的优势。

Conclusion: Brain-OF代表了朝向更强大、灵活的脑科学应用迈出的重要一步，它能够有效融合多种类型的脑活动数据，并通过创新性的架构设计克服了现有模型的一些关键限制。

Abstract: Brain foundation models have achieved remarkable advances across a wide range of neuroscience tasks. However, most existing models are limited to a single functional modality, restricting their ability to exploit complementary spatiotemporal dynamics and the collective data scale across imaging techniques. To address this limitation, we propose Brain-OF, the first omnifunctional brain foundation model jointly pretrained on fMRI, EEG and MEG, capable of handling both unimodal and multimodal inputs within a unified framework. To reconcile heterogeneous spatiotemporal resolutions, we introduce the Any-Resolution Neural Signal Sampler, which projects diverse brain signals into a shared semantic space.To further manage semantic shifts, the Brain-OF backbone integrates DINT attention with a Sparse Mixture of Experts, where shared experts capture modality-invariant representations and routed experts specialize in modality-specific semantics. Furthermore, we propose Masked Temporal-Frequency Modeling, a dual-domain pretraining objective that jointly reconstructs brain signals in both the time and frequency domains. Brain-OF is pretrained on a large-scale corpus comprising around 40 datasets and demonstrates superior performance across diverse downstream tasks, highlighting the benefits of joint multimodal integration and dual-domain pretraining.

</details>


### [34] [EvoX: Meta-Evolution for Automated Discovery](https://arxiv.org/abs/2602.23413)
*Shu Liu,Shubham Agarwal,Monishwaran Maheswaran,Mert Cemri,Zhifei Li,Qiuyang Mang,Ashwin Naren,Ethan Boneh,Audrey Cheng,Melissa Z. Pan,Alexander Du,Kurt Keutzer,Alexandros G. Dimakis,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.LG

TL;DR: 本文提出了一种自适应进化方法EvoX，它能够优化自身的进化过程，并在近200个真实世界的优化任务中表现优于现有的AI驱动的进化方法。


<details>
  <summary>Details</summary>
Motivation: 现有的结合大型语言模型驱动的优化与进化搜索的方法依赖于静态的搜索策略，这些策略无法很好地适应不同任务或同一任务中的变化。为了解决这一问题，研究者们开发了EvoX，一种可以自我优化其进化过程的方法。

Method: EvoX同时进化候选解决方案和用于生成这些方案的搜索策略，基于进展持续更新先前解的选择和变异方式。这样的设计允许系统在优化过程中动态地切换不同的搜索策略。

Result: 通过在接近200个实际优化任务上的测试，EvoX在大部分任务上超过了包括AlphaEvolve、OpenEvolve、GEPA及ShinkaEvolve在内的现有AI驱动进化方法的表现。

Conclusion: EvoX提供了一种更加灵活且有效的途径来执行AI驱动的进化搜索，对于需要不断调整搜索策略以应对复杂多变情况的任务尤其有用。

Abstract: Recent work such as AlphaEvolve has shown that combining LLM-driven optimization with evolutionary search can effectively improve programs, prompts, and algorithms across domains. In this paradigm, previously evaluated solutions are reused to guide the model toward new candidate solutions. Crucially, the effectiveness of this evolution process depends on the search strategy: how prior solutions are selected and varied to generate new candidates. However, most existing methods rely on fixed search strategies with predefined knobs (e.g., explore-exploit ratios) that remain static throughout execution. While effective in some settings, these approaches often fail to adapt across tasks, or even within the same task as the search space changes over time. We introduce EvoX, an adaptive evolution method that optimizes its own evolution process. EvoX jointly evolves candidate solutions and the search strategies used to generate them, continuously updating how prior solutions are selected and varied based on progress. This enables the system to dynamically shift between different search strategies during the optimization process. Across nearly 200 real-world optimization tasks, EvoX outperforms existing AI-driven evolutionary methods including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks.

</details>


### [35] [MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2602.23798)
*Tiantong Wang,Xinyu Yan,Tiantong Wu,Yurong Hao,Yong Jiang,Fei Huang,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 提出了一种名为MPU的隐私保护多扰动副本遗忘框架，通过在服务器端引入预处理和后处理模块，允许客户端在其私有的遗忘集上本地执行遗忘过程，同时保持与无噪声基线相当的遗忘性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在机器遗忘过程中面临的隐私困境，即严格的约束禁止共享服务器参数或客户端的遗忘集。

Method: 提出了一个算法无关的隐私保护多扰动副本遗忘框架MPU，该框架主要引入了两个服务器端模块：预处理用于生成随机化的副本，后处理用于通过调和去噪程序聚合更新以减轻扰动的影响。

Result: 实验表明，MPU能够实现与无噪声基线相媲美的遗忘性能，对于大多数算法而言，在10%噪声下平均退化远低于1%，甚至在1%噪声条件下对某些算法的表现优于无噪声基线。

Conclusion: MPU提供了一种有效的方法来解决机器遗忘中的隐私问题，同时保持了良好的遗忘效果。

Abstract: Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU.

</details>


### [36] [Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning](https://arxiv.org/abs/2602.23446)
*Alejandro Rodriguez Dominguez*

Main category: cs.LG

TL;DR: 本文提出了一种统一理论，解释了大规模语言模型由于人类监督渠道的局限性而产生的持续错误，并且证明了仅靠扩大规模无法消除这些错误。研究还表明，引入辅助的非人类信号可以增加有效的监督能力并减少或消除额外的错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于人类生成的数据和反馈进行训练，但它们表现出由标注噪声、主观偏好以及自然语言表达带宽有限引起的持续错误。作者认为这些限制反映了监督渠道的结构性质，而非模型规模或优化问题。

Method: 通过开发一个统一的理论框架，展示了当人类监督渠道不足以覆盖潜在评估目标时，它会成为一个信息减少的渠道，为任何受其支配的学习者带来严格正向的超额风险下限。这一理论在六个互补框架（算子理论、PAC-Bayes方法、信息论、因果推理、范畴论以及从人类反馈中学习强化学习的游戏理论分析）中得到了形式化。

Result: 实验结果表明，仅依靠人类监督存在持久性的性能下限，而足够信息量的辅助渠道能够显著减少或完全消除这种超额错误。

Conclusion: 该研究表明，仅靠扩大模型规模不能解决因人类监督不足导致的固有错误问题；通过结合使用非人类来源的信息（如检索、程序执行等工具），可以提高有效监督能力并降低错误率。

Abstract: Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error.

</details>


### [37] [Global Interpretability via Automated Preprocessing: A Framework Inspired by Psychiatric Questionnaires](https://arxiv.org/abs/2602.23459)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: 本文提出了一种名为REFINE的方法，通过将非线性处理限制在基线预处理模块中，并从这些稳定的基线项目学习到未来严重程度的线性映射，从而提高精神科问卷数据的预测准确性同时保持了预测关系的可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于精神科问卷对上下文高度敏感且往往只能弱预测后续症状的严重程度，导致预后关系难以学习。尽管灵活的非线性模型可以提高预测准确性，但它们有限的可解释性可能会削弱临床信任。因此，需要一种方法既能够提高预测准确性又保持良好的可解释性。

Method: 采用REFINE（利用冗余-随访信息的非线性增强）两阶段法，该方法首先通过基线预处理模块估计稳定项目值，然后从这些稳定化的基线项目学习到未来严重程度的线性映射。此方法将非线性能力集中在预处理阶段，而保持预后关系为透明的线性，从而通过系数矩阵实现全局可解释性。

Result: 实验表明，REFINE在精神科和非精神科纵向预测任务中优于其他可解释方法，同时保持了清晰的预后因素全局归因。

Conclusion: REFINE方法成功地提高了精神科问卷数据预测的准确性，同时保留了预测模型的可解释性，使得临床医生能够更好地理解并信任模型结果。

Abstract: Psychiatric questionnaires are highly context sensitive and often only weakly predict subsequent symptom severity, which makes the prognostic relationship difficult to learn. Although flexible nonlinear models can improve predictive accuracy, their limited interpretability can erode clinical trust. In fields such as imaging and omics, investigators commonly address visit- and instrument-specific artifacts by extracting stable signal through preprocessing and then fitting an interpretable linear model. We adopt the same strategy for questionnaire data by decoupling preprocessing from prediction: we restrict nonlinear capacity to a baseline preprocessing module that estimates stable item values, and then learn a linear mapping from these stabilized baseline items to future severity. We refer to this two-stage method as REFINE (Redundancy-Exploiting Follow-up-Informed Nonlinear Enhancement), which concentrates nonlinearity in preprocessing while keeping the prognostic relationship transparently linear and therefore globally interpretable through a coefficient matrix, rather than through post hoc local attributions. In experiments, REFINE outperforms other interpretable approaches while preserving clear global attribution of prognostic factors across psychiatric and non-psychiatric longitudinal prediction tasks.

</details>


### [38] [Uncertainty-aware Language Guidance for Concept Bottleneck Models](https://arxiv.org/abs/2602.23495)
*Yangyi Li,Mengdi Huai*

Main category: cs.LG

TL;DR: 提出了一种新的不确定性感知CBM方法，该方法能够量化大语言模型标注概念标签的不确定性，并将此不确定性纳入CBM训练过程中，以应对不同可靠性水平的大语言模型标注概念。


<details>
  <summary>Details</summary>
Motivation: 现有的概念瓶颈模型（CBMs）需要大量专家知识和劳动来注释人类可理解的概念，限制了其广泛应用；而利用大型语言模型（LLMs）构建概念瓶颈的工作则忽略了与LLMs注释概念相关的不确定性问题，缺乏有效机制来量化这些概念的不确定性，增加了由于LLMs产生幻觉导致错误的风险。此外，这些方法未能将与这些注释相关的不确定性融入到概念瓶颈模型的学习过程中。

Method: 提出了一种新型的不确定性感知CBM方法，该方法不仅严格地用有效且无分布假设的方式量化了由LLM注释的概念标签的不确定性，还通过将量化的概念不确定性纳入CBM训练程序中，以考虑到LLM注释概念间不同的可靠性程度。同时提供了对所提方法的理论分析。

Result: 在真实世界数据集上进行了广泛的实验验证，证明了所提出方法的有效性。

Conclusion: 新提出的不确定性感知CBM方法成功解决了现有技术中存在的关键局限性，即无法妥善处理由大型语言模型产生的概念注释不确定性问题，为提升CBM模型的鲁棒性和可靠性开辟了新途径。

Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first mapping input samples to high-level semantic concepts, followed by a combination of these concepts for the final classification. However, the annotation of human-understandable concepts requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. On the other hand, there are a few works that leverage the knowledge of large language models (LLMs) to construct concept bottlenecks. Nevertheless, they face two essential limitations: First, they overlook the uncertainty associated with the concepts annotated by LLMs and lack a valid mechanism to quantify uncertainty about the annotated concepts, increasing the risk of errors due to hallucinations from LLMs. Additionally, they fail to incorporate the uncertainty associated with these annotations into the learning process for concept bottleneck models. To address these limitations, we propose a novel uncertainty-aware CBM method, which not only rigorously quantifies the uncertainty of LLM-annotated concept labels with valid and distribution-free guarantees, but also incorporates quantified concept uncertainty into the CBM training procedure to account for varying levels of reliability across LLM-annotated concepts. We also provide the theoretical analysis for our proposed method. Extensive experiments on the real-world datasets validate the desired properties of our proposed methods.

</details>


### [39] [Sample Size Calculations for Developing Clinical Prediction Models: Overview and pmsims R package](https://arxiv.org/abs/2602.23507)
*Diana Shamsutdinova,Felix Zimmer,Oyebayo Ridwan Olaniran,Sarah Markham,Daniel Stahl,Gordon Forbes,Ewan Carr*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于模拟的方法，通过整合学习曲线、高斯过程优化和保证原则来确定达到目标性能所需的样本量，并开发了开源R包pmsims。该方法为临床预测模型的样本量估计提供了灵活、高效且可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 临床预测模型越来越多地用于医疗决策，但其开发所需的最小样本量仍然是一个关键且未解决的问题。现有方法在灵活性和准确性方面存在局限性，特别是对于复杂数据结构和机器学习模型而言。

Method: 研究者们回顾了当前预测建模中样本量估算的方法论，并引入了一个区分基于平均值标准与基于保证标准的概念框架。在此基础上，提出了一个新颖的基于模拟的方法，结合了学习曲线、高斯过程优化以及保证原则来识别能够以高概率达成目标表现的样本量。这一方法被实现在名为pmsims的开源R软件包中。

Result: 案例研究表明，不同方法、性能度量标准及建模策略下得到的样本量估计差异很大。与现有工具相比，pmsims提供了更加灵活、高效且易于理解的解决方案，能够适应各种模型类型和用户定义的度量标准，并明确考虑到了模型性能中的变异性。

Conclusion: 所提出的框架和软件通过结合灵活性与计算效率推进了临床预测建模领域内的样本量方法学发展。未来工作应将这些方法扩展到层次化和多模态数据上，纳入公平性和稳定性指标，并解决如缺失数据和复杂依赖结构等挑战。

Abstract: Background: Clinical prediction models are increasingly used to inform healthcare decisions, but determining the minimum sample size for their development remains a critical and unresolved challenge. Inadequate sample sizes can lead to overfitting, poor generalisability, and biased predictions. Existing approaches, such as heuristic rules, closed-form formulas, and simulation-based methods, vary in flexibility and accuracy, particularly for complex data structures and machine learning models. Methods: We review current methodologies for sample size estimation in prediction modelling and introduce a conceptual framework that distinguishes between mean-based and assurance-based criteria. Building on this, we propose a novel simulation-based approach that integrates learning curves, Gaussian Process optimisation, and assurance principles to identify sample sizes that achieve target performance with high probability. This approach is implemented in pmsims, an open-source, model-agnostic R package. Results: Through case studies, we demonstrate that sample size estimates vary substantially across methods, performance metrics, and modelling strategies. Compared to existing tools, pmsims provides flexible, efficient, and interpretable solutions that accommodate diverse models and user-defined metrics while explicitly accounting for variability in model performance. Conclusions: Our framework and software advance sample size methodology for clinical prediction modelling by combining flexibility with computational efficiency. Future work should extend these methods to hierarchical and multimodal data, incorporate fairness and stability metrics, and address challenges such as missing data and complex dependency structures.

</details>


### [40] [Neural Operators Can Discover Functional Clusters](https://arxiv.org/abs/2602.23528)
*Yicen Li,Jose Antonio Lara Benitez,Ruiyang Hong,Anastasis Kratsios,Paul David McNicholas,Maarten Valentijn de Hoop*

Main category: cs.LG

TL;DR: 该论文证明了基于样本的神经算子可以学习无限维再生核希尔伯特空间中任何有限类别的集合，并提出了一个由神经算子驱动的聚类管道用于函数数据，实验表明在经典方法失效的情况下，所提出的SNO能够恢复潜在的动力学结构。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于虽然神经算子（NOs）在回归任务中的应用越来越被理解，但在分类和其无监督类似物：聚类中的应用却知之甚少。本文旨在探索神经算子在处理无限维空间中的聚类问题时的有效性。

Method: 通过证明基于样本的神经算子能够在满足温和核采样假设下学习到非凸或不连通的类别集合，并且提出了一种适用于函数数据的、由预训练编码器和轻量级可训练头部组成的神经算子驱动的聚类流程。

Result: 实验结果展示了在多种合成ODE基准测试中，所提出的实用SNO能够在传统方法失败的情境下恢复出潜在的动力学结构，这与提出的普遍聚类理论是一致的。

Conclusion: 研究表明神经算子不仅理论上能够逼近任意闭合类别的集合，而且在实际应用中也表现出色，特别是在处理功能数据聚类问题上提供了一个有效的解决方案。

Abstract: Operator learning is reshaping scientific computing by amortizing inference across infinite families of problems. While neural operators (NOs) are increasingly well understood for regression, far less is known for classification and its unsupervised analogue: clustering. We prove that sample-based neural operators can learn any finite collection of classes in an infinite-dimensional reproducing kernel Hilbert space, even when the classes are neither convex nor connected, under mild kernel sampling assumptions. Our universal clustering theorem shows that any $K$ closed classes can be approximated to arbitrary precision by NO-parameterized classes in the upper Kuratowski topology on closed sets, a notion that can be interpreted as disallowing false-positive misclassifications.
  Building on this, we develop an NO-powered clustering pipeline for functional data and apply it to unlabeled families of ordinary differential equation (ODE) trajectories. Discretized trajectories are lifted by a fixed pre-trained encoder into a continuous feature map and mapped to soft assignments by a lightweight trainable head. Experiments on diverse synthetic ODE benchmarks show that the resulting practical SNO recovers latent dynamical structure in regimes where classical methods fail, providing evidence consistent with our universal clustering theory.

</details>


### [41] [Active Value Querying to Minimize Additive Error in Subadditive Set Function Learning](https://arxiv.org/abs/2602.23529)
*Martin Černý,David Sychrovský,Filip Úradník,Jakub Černý*

Main category: cs.LG

TL;DR: 本文研究了在加性误差下近似未知次可加集函数的问题，提出了最小化和最大化补全不同类别的缺失值集函数的方法，并通过实证展示了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于指定一个集函数通常需要为指数级数量的子集分配值，这在实践中往往是资源密集型的任务。简单的忽略某些值会导致模糊性，特别是在不完全集函数需要进一步优化时。受到关于使用确定性值查询无法近似次可加函数结果的启发，本研究探索了以加性误差方式近似未知次可加或其子类集函数的问题。

Method: 研究首先全面探讨了具有缺失值的不同类别集函数的最小和最大补全及其导致的距离；其次开发了方法来减少已知先验信息的集函数类别间的这种距离，通过在线和离线两种方式披露额外子集的价值来实现；最后，通过实际场景中的算法性能展示进行了实证分析。

Result: 研究发现了对于不同类别的集函数，其最小和最大补全间存在特定距离，并且通过选择性地揭示额外子集的价值可以有效地减小这个距离。此外，实证结果表明所提出的算法在现实应用中表现良好。

Conclusion: 本研究表明，通过适当策略选择性地增加集函数的信息，可以有效减少不完全集函数的最小和最大补全之间的距离，从而提高对未知次可加集函数的近似效果。

Abstract: Subadditive set functions play a pivotal role in computational economics (especially in combinatorial auctions), combinatorial optimization or artificial intelligence applications such as interpretable machine learning. However, specifying a set function requires assigning values to an exponentially large number of subsets in general, a task that is often resource-intensive in practice, particularly when the values derive from external sources such as retraining of machine learning models. A~simple omission of certain values introduces ambiguity that becomes even more significant when the incomplete set function has to be further optimized over. Motivated by the well-known result about inapproximability of subadditive functions using deterministic value queries with respect to a multiplicative error, we study a problem of approximating an unknown subadditive (or a subclass of thereof) set function with respect to an additive error -- i. e., we aim to efficiently close the distance between minimal and maximal completions. Our contributions are threefold: (i) a thorough exploration of minimal and maximal completions of different classes of set functions with missing values and an analysis of their resulting distance; (ii) the development of methods to minimize this distance over classes of set functions with a known prior, achieved by disclosing values of additional subsets in both offline and online manner; and (iii) empirical demonstrations of the algorithms' performance in practical scenarios.

</details>


### [42] [Dynamics of Learning under User Choice: Overspecialization and Peer-Model Probing](https://arxiv.org/abs/2602.23565)
*Adhyyan Narang,Sarah Dean,Lillian J Ratliff,Maryam Fazel*

Main category: cs.LG

TL;DR: 本文探讨了在多个平台从同一用户池获取数据的情况下，现有算法可能导致学习者几乎肯定会收敛到全局性能极差的模型的问题。为解决这一问题，提出了一种新的算法，允许学习者'探测'同行模型的预测，从而了解那些没有选择他们的用户。实验表明，在探查源足够有信息量时，该方法可以几乎肯定地收敛到一个具有有限全人口风险的稳定点。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用于经济相关领域的背景下，多个平台从相同的用户池中获取数据，每个用户选择最能满足他们需求的平台。先前的研究主要集中在学习者在其观察到的数据分布上的'局部'损失上。然而，存在这样的情况：即使存在低全人口损失的模型，使用现有算法的学习者也几乎肯定会收敛到全局性能极差的模型。这种现象通过一种称为过度专业化陷阱的反馈诱导机制发生。

Method: 受现代ML中知识蒸馏最近使用的启发，研究者提出了一种算法，让学习者能够'探测'同行模型的预测，这使他们能够了解未选择他们的用户。分析指出，当探查源足够有信息价值时（例如，已知的市场领导者或大多数具有良好全球表现的同行），此过程几乎肯定能收敛至一个全人群风险有限的静止点。

Result: 通过在MovieLens、Census和Amazon Sentiment数据集上的半合成实验验证了研究发现。结果显示，所提出的算法能够在探查源具备充分信息性的情况下，几乎肯定地收敛到一个具有限定全人口风险的稳态点。

Conclusion: 本文揭示了在多平台竞争环境下，现有机器学习算法可能陷入过度专业化陷阱的风险，并通过提出一种新颖的算法来克服这个问题，该算法利用其他模型作为探查源以改善自身对未服务用户的理解能力。

Abstract: In many economically relevant contexts where machine learning is deployed, multiple platforms obtain data from the same pool of users, each of whom selects the platform that best serves them. Prior work in this setting focuses exclusively on the "local" losses of learners on the distribution of data that they observe. We find that there exist instances where learners who use existing algorithms almost surely converge to models with arbitrarily poor global performance, even when models with low full-population loss exist. This happens through a feedback-induced mechanism, which we call the overspecialization trap: as learners optimize for users who already prefer them, they become less attractive to users outside this base, which further restricts the data they observe. Inspired by the recent use of knowledge distillation in modern ML, we propose an algorithm that allows learners to "probe" the predictions of peer models, enabling them to learn about users who do not select them. Our analysis characterizes when probing succeeds: this procedure converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative, e.g., a known market leader or a majority of peers with good global performance. We verify our findings with semi-synthetic experiments on the MovieLens, Census, and Amazon Sentiment datasets.

</details>


### [43] [SDMixer: Sparse Dual-Mixer for Time Series Forecasting](https://arxiv.org/abs/2602.23581)
*Xiang Ao*

Main category: cs.LG

TL;DR: 本文提出了一种双流稀疏Mixer预测框架，该框架能够从时域和频域序列中分别提取全局趋势和局部动态特征，并采用稀疏机制过滤无效信息，从而提高跨变量依赖建模的准确性。实验结果表明，此方法在多个真实场景数据集上达到了领先性能，证明了其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列预测在交通、能源和金融等领域得到广泛应用，但数据通常存在多尺度特性、弱相关性和噪声干扰等问题，这些问题限制了现有模型的预测性能。

Method: 提出了一种双流稀疏Mixer预测框架，旨在从时域和频域两方面捕捉序列中的全局趋势与局部动态特性，并通过稀疏机制去除不相关信息以增强跨变量间依赖关系建模的精度。

Result: 实验结果显示，在多种实际应用场景的数据集上，所提方法均表现出色，优于现有技术，验证了其高效及广泛的适用性。

Conclusion: 通过引入双流处理与时-频分析结合的方法论，以及利用稀疏策略优化信息筛选过程，本研究为解决多变量时间序列预测中存在的挑战提供了一个新的解决方案，具有较高的实用价值和发展潜力。

Abstract: Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer

</details>


### [44] [When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion](https://arxiv.org/abs/2602.23614)
*Kejing Yin,Haizhou Xu,Wenfang Yao,Chen Liu,Zijie Chen,Yui Haang Cheung,William K. Cheung,Jing Qin*

Main category: cs.LG

TL;DR: 本研究系统地评估了电子健康记录（EHR）和胸部X光片（CXR）之间的多模态融合在临床预测中的应用，探讨了多模态融合何时能改善预测、不同融合策略的比较、现有方法对缺失模态的鲁棒性以及多模态模型是否实现算法公平性。研究发现，在模态完整时多模态融合确实提高了性能，特别是在需要结合EHR与CXR信息的疾病上。然而，当面临实际中常见的数据缺失情况时，除非模型特别设计来处理不完整输入，否则多模态的优势会迅速减弱。此外，多模态融合本身并不自然提高公平性，子群体间的差异主要源于不同人口统计学组间敏感度的不平等。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习有望推进临床决策支持的发展，但在实践中尤其是在模态缺失和公平性约束条件下，多模态学习真正帮助的情况尚不清楚。这项工作的动机在于通过系统基准测试回答四个基本问题：多模态融合何时能改善临床预测；不同融合策略之间如何比较；现有方法对于缺失模态有多强的鲁棒性；以及多模态模型是否实现了算法上的公平性。

Method: 研究者们利用MIMIC-IV和MIMIC-CXR提供的标准化队列进行了EHR与CXR之间多模态融合的系统基准测试。他们考察了几种不同的融合策略，并分析了这些策略在模态完整及缺失条件下的表现。同时，还评估了多模态模型在不同人群亚组中的公平性表现。

Result: 结果表明，当所有模态都可用时，多模态融合确实可以提高某些疾病的预测性能，特别是那些需要从EHR和CXR中获取互补信息的疾病。但是，面对现实世界中经常遇到的数据缺失问题时，除非专门设计了能够应对这种情形的模型，否则多模态带来的好处将快速消失。另外，单纯依靠多模态融合并不能保证算法公平性的提升，跨人群亚组间的差异主要由不同群体间敏感度的不均衡引起。

Conclusion: 综上所述，这项工作为理解多模态学习在什么情况下有益、在哪些情况下可能失败及其背后原因提供了可操作指导，为开发既有效又可靠的临床多模态系统奠定了基础。此外，研究团队还发布了灵活的基准测试工具包，支持新模型和数据集的即插即用集成，以促进该领域内的进一步研究。

Abstract: Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench.

</details>


### [45] [On the Convergence of Single-Loop Stochastic Bilevel Optimization with Approximate Implicit Differentiation](https://arxiv.org/abs/2602.23633)
*Yubo Zhou,Luo Luo,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: 本文对单循环随机近似隐式微分（SSAID）算法进行了深入的收敛性分析，证明了该算法在保持单循环更新计算效率的同时，能够达到与目前最先进的多循环方法相同的最优收敛率，并首次明确地给出了κ依赖性的精细表征。


<details>
  <summary>Details</summary>
Motivation: 随机双层优化是元学习和超参数优化的基础框架。尽管单循环算法在实践中非常普遍，但它们在随机环境下的理论理解远不如多循环算法成熟。现有的分析往往产生次优的收敛速度或模糊了对下层条件数κ的关键依赖性。

Method: 本文采用了一种名为单循环随机近似隐式微分（SSAID）的方法，并对其进行了细致的收敛性分析。

Result: 研究表明，SSAID算法可以以$\mathcal{O}(κ^7 ε^{-2})$的复杂度达到ε-稳定点，这不仅与最先进的多循环方法的最佳$\mathcal{O}(ε^{-2})$速率相匹配，而且保持了单循环更新的计算效率；此外，还首次为基于随机AID的单循环方法提供了κ依赖性的明确、细粒度特征描述。

Conclusion: 这项工作表明，SSAID不仅仅是一种启发式方法，它还拥有一个严格的理论基础，其收敛保证可与主流多循环框架竞争。

Abstract: Stochastic Bilevel Optimization has emerged as a fundamental framework for meta-learning and hyperparameter optimization. Despite the practical prevalence of single-loop algorithms--which update lower and upper variables concurrently--their theoretical understanding, particularly in the stochastic regime, remains significantly underdeveloped compared to their multi-loop counterparts. Existing analyses often yield suboptimal convergence rates or obscure the critical dependence on the lower-level condition number $κ$, frequently burying it within generic Lipschitz constants. In this paper, we bridge this gap by providing a refined convergence analysis of the Single-loop Stochastic Approximate Implicit Differentiation (SSAID) algorithm. We prove that SSAID achieves an $ε$-stationary point with an oracle complexity of $\mathcal{O}(κ^7 ε^{-2})$. Our result is noteworthy in two aspects: (i) it matches the optimal $\mathcal{O}(ε^{-2})$ rate of state-of-the-art multi-loop methods (e.g., stocBiO) while maintaining the computational efficiency of a single-loop update; and (ii) it provides the first explicit, fine-grained characterization of the $κ$-dependence for stochastic AID-based single-loop methods. This work demonstrates that SSAID is not merely a heuristic approach, but admits a rigorous theoretical foundation with convergence guarantees competitive with mainstream multi-loop frameworks.

</details>


### [46] [FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation](https://arxiv.org/abs/2602.23636)
*Zhihao Ding,Jinming Li,Ze Lu,Jieming Shi*

Main category: cs.LG

TL;DR: 本文提出了一种名为FlexGuard的LLM调节器，它可以输出校准后的连续风险分数，并通过阈值调整支持特定严格度的决策。实验表明，与现有模型相比，FlexGuard在不同严格度下具有更高的调节准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的护栏模型大多将内容审核视为一个固定的二分类任务，这假设了有害性的定义是固定的。但实际上，不同平台之间以及随着时间推移，执行严格程度会有所不同。这种差异使得基于二分类的审核模型难以适应变化的需求。

Method: 首先构建了一个能够适应不同严格度要求的LLM审核基准测试工具FlexBench。接着提出了FlexGuard，一种基于LLM的调节器，它能输出反映风险严重程度的标准化连续评分，并允许通过设定阈值来适应特定的严格度需求。此外，还采用了风险对齐优化方法训练FlexGuard以提高得分-严重性的一致性，并提供了实用的阈值选择策略以便于部署时调整到目标严格度。

Result: 实验结果表明，在面对不同的严格度标准时，FlexGuard相较于其他现有调节器展现出了更高的准确性与更强的鲁棒性。

Conclusion: 通过引入FlexGuard，研究解决了当前固定二分类审核模型在应对多变的执行严格度上存在的问题，提供了一种更加灵活高效的内容安全解决方案。

Abstract: Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility.

</details>


### [47] [FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA](https://arxiv.org/abs/2602.23638)
*Haoran Zhang,Dongjun Kim,Seohyeon Cha,Haris Vikalo*

Main category: cs.LG

TL;DR: 提出了FedRot-LoRA，一种联邦LoRA框架，通过在聚合前对客户端更新进行正交变换来解决低秩因子化中的旋转不对齐问题，从而减少跨客户端子空间不匹配并提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦LoRA机制在实践中遇到了由于维持低秩所使用的因子平均与数学上正确的局部更新聚合之间的差异导致的显著聚合误差和不稳定训练的问题。主要原因是来自不同客户端的语义等价更新可以表示在不同的潜在子空间中，直接平均这些未对齐的因素会导致它们互相破坏性干扰，从而降低全局更新的质量。

Method: 提出FedRot-LoRA框架，在聚合前通过正交变换对齐客户端更新。这样既保持了语义更新，又减少了跨客户端子空间不匹配的情况，同时没有增加通信成本或限制模型表达力。

Result: 广泛的实验表明，无论是在自然语言理解还是生成任务上，FedRot-LoRA相较于现有联邦LoRA基线方法，在各种异质性和LoRA等级水平下均表现出色。此外，还提供了收敛性分析，展示了通过旋转对齐如何得到更紧致的聚合误差上界。

Conclusion: FedRot-LoRA通过引入旋转对齐解决了联邦学习中因低秩分解引起的旋转不变性问题，有效提高了模型训练的稳定性和性能。

Abstract: Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks.

</details>


### [48] [Selective Denoising Diffusion Model for Time Series Anomaly Detection](https://arxiv.org/abs/2602.23662)
*Kohei Obata,Zheng Chen,Yasuko Matsubara,Lingwei Zhu,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 提出了一种名为AnomalyFilter的新方法，通过在训练阶段对高斯噪声进行掩码并在不向实例添加噪声的情况下执行去噪过程，从而选择性地仅对时间序列中的异常部分进行去噪，同时保持正常部分不变。实验表明这种方法在五个数据集上实现了较低的正常部分重建误差，为时间序列异常检测提供了有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的时间序列异常检测方法依赖于条件策略来从白噪声中重建输入实例，这导致了在准确重建正常部分时遇到挑战，进而影响了检测性能。

Method: AnomalyFilter方法在训练阶段对高斯噪声进行掩蔽，并且在不向实例添加噪声的情况下执行去噪过程，这样可以构建一个只针对异常部分进行去噪的选择性过滤器。

Result: 广泛的实验结果表明，AnomalyFilter在五个不同数据集上对于正常部分能够达到显著低的重建误差，支持了其在异常检测中的有效性。

Conclusion: AnomalyFilter作为一种创新的方法，专注于设计特别适合时间序列异常检测（TSAD）的扩散模型噪声处理方式，显示出比传统扩散模型更好的性能。

Abstract: Time series anomaly detection (TSAD) has been an important area of research for decades, with reconstruction-based methods, mostly based on generative models, gaining popularity and demonstrating success. Diffusion models have recently attracted attention due to their advanced generative capabilities. Existing diffusion-based methods for TSAD rely on a conditional strategy, which reconstructs input instances from white noise with the aid of the conditioner. However, this poses challenges in accurately reconstructing the normal parts, resulting in suboptimal detection performance. In response, we propose a novel diffusion-based method, named AnomalyFilter, which acts as a selective filter that only denoises anomaly parts in the instance while retaining normal parts. To build such a filter, we mask Gaussian noise during the training phase and conduct the denoising process without adding noise to the instances. The synergy of the two simple components greatly enhances the performance of naive diffusion models. Extensive experiments on five datasets demonstrate that AnomalyFilter achieves notably low reconstruction error on normal parts, providing empirical support for its effectiveness in anomaly detection. AnomalyFilter represents a pioneering approach that focuses on the noise design of diffusion models specifically tailored for TSAD.

</details>


### [49] [Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training](https://arxiv.org/abs/2602.23696)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 该研究探讨了小型Transformer模型在训练过程中参数更新的几何特性，发现参数更新主要沿着一个主导漂移方向进行，并伴有横向的残余动态。通过使用未居中、行归一化的轨迹PCA技术，研究人员展示了单个方向早期即可捕捉到累积参数移动的一大比例，而剩余成分则编码了辅助探针性能中的振荡行为。瞬时梯度与这个主导方向几乎没有对齐，表明它是由累积优化器更新而非每批梯度结构引起的。比较AdamW与SGD变体在匹配损失水平下的表现揭示了轨迹几何学上的显著差异：AdamW发展出多维漂移结构，而SGD家族优化器产生的参数演变几乎共线且探针动力较弱。重加热选择性地扰乱横向成分，对主导漂移坐标的影响极小。这些发现表明，除了从损失值本身所观察到的现象外，优化器的选择还影响着学习轨迹的有效维度和结构。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在理解不同优化算法如何影响深度学习模型（特别是小型Transformer）训练过程中的参数更新路径及其几何特性。通过探索这些路径的内在结构，作者希望揭示优化器选择背后更深层次的学习机制以及它们如何影响模型训练的有效性和效率。

Method: 采用了未居中、行归一化的轨迹主成分分析（PCA）方法来研究参数更新的方向性和模式；对比了AdamW与几种SGD变体在类似损失水平下的训练轨迹特征；应用了重新加热技术以评估其对参数更新路径特定方面的影响。

Result: 研究发现，在训练初期，大部分参数变化可以由单一方向解释，这表明存在一个主导的漂移方向；剩余的变化反映了辅助任务性能中的周期性波动；AdamW优化器相较于SGD系列能够产生更加复杂的多维漂移结构；重新加热操作主要影响的是与主导漂移方向垂直的部分。

Conclusion: 优化器的选择不仅影响最终模型性能，也深刻塑造了整个训练过程中的参数更新路径及其几何特性。这意味着，为了获得最佳训练效果，需要考虑更多因素而不仅仅是关注损失函数的表现。

Abstract: We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone.

</details>


### [50] [Bridging Dynamics Gaps via Diffusion Schrödinger Bridge for Cross-Domain Reinforcement Learning](https://arxiv.org/abs/2602.23737)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出了一种新的跨域强化学习框架BDGxRL，通过Diffusion Schrödinger Bridge调整源域转移以匹配目标域动态，并引入奖励调节机制来确保与目标域动力学的一致性。实验表明，该方法在跨域基准测试中优于现有最先进方法，并且在动态转换下表现出很强的适应性。


<details>
  <summary>Details</summary>
Motivation: 跨域强化学习面临的主要挑战是在缺乏目标领域环境互动和奖励监督的情况下，直接学习策略变得困难。为了解决这一问题，研究者们开发了BDGxRL框架，旨在通过离线演示编码的目标域动态对齐源转移，从而实现无需访问目标环境或其奖励就能进行面向目标的策略学习。

Method: BDGxRL利用Diffusion Schrödinger Bridge技术将源域中的转换与目标域的动力学特性对齐，同时设计了一个基于状态转换估计奖励的机制，适用于经过DSB校正后的样本，保证奖励与目标域动力学之间的一致性。

Result: MuJoCo跨域基准测试显示，BDGxRL不仅超过了当前最先进的基线方法，而且在面对过渡动力学变化时也展现了强大的适应能力。

Conclusion: 本研究表明，通过BDGxRL框架能够在不同域间有效迁移策略，即便在没有目标环境交互或奖励信息的情况下也能取得良好的性能，为解决跨域强化学习问题提供了新思路。

Abstract: Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schrödinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts.

</details>


### [51] [OPTIAGENT: A Physics-Driven Agentic Framework for Automated Optical Design](https://arxiv.org/abs/2602.23761)
*Yuyu Geng,Lei Sun,Yao Gao,Xinxin Hu,Zhonghua Yi,Xiaolong Qian,Weijian Hu,Jian Bai,Kaiwei Wang*

Main category: cs.LG

TL;DR: 本文首次尝试将大型语言模型（LLMs）应用于光学设计领域，通过构建OptiDesignQA数据集和引入基于物理驱动的策略对齐方法DrGRPO，使得非专业用户也能成功开发出功能性透镜系统。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型拥有广泛的光学知识，但在设计透镜系统方面的能力仍然受到很大限制。本研究旨在利用这些模型的知识来缩小专业知识差距，让没有正式光学训练背景的用户也能够设计出有效的透镜系统。

Method: 创建了名为OptiDesignQA的数据集，该数据集包含了来自标准光学教科书的经典透镜系统以及由自动化设计算法生成的新配置；通过全系统合成与透镜完成的混合目标向LLM注入特定领域的光学专业知识；使用Group Relative Policy Optimization Done Right (DrGRPO) 结合光学词典奖励机制来调整模型，使之符合光学原理；最后，模型与专门的光学优化程序集成以进行端到端微调和精度提升。

Result: 实验结果表明，提出的方法相比传统基于优化的自动化设计算法和其他LLM同行，在性能上展现出优越性。

Conclusion: 这项工作展示了如何有效利用大型语言模型促进光学设计的进步，为不具备深厚光学背景的人士提供了新的设计工具。

Abstract: Optical design is the process of configuring optical elements to precisely manipulate light for high-fidelity imaging. It is inherently a highly non-convex optimization problem that relies heavily on human heuristic expertise and domain-specific knowledge. While Large Language Models (LLMs) possess extensive optical knowledge, their capabilities in leveraging the knowledge in designing lens system remain significantly constrained. This work represents the first attempt to employ LLMs in the field of optical design. We bridge the expertise gap by enabling users without formal optical training to successfully develop functional lens systems. Concretely, we curate a comprehensive dataset, named OptiDesignQA, which encompasses both classical lens systems sourced from standard optical textbooks and novel configurations generated by automated design algorithms for training and evaluation. Furthermore, we inject domain-specific optical expertise into the LLM through a hybrid objective of full-system synthesis and lens completion. To align the model with optical principles, we employ Group Relative Policy Optimization Done Right (DrGRPO) guided by Optical Lexicographic Reward for physics-driven policy alignment. This reward system incorporates structural format rewards, physical feasibility rewards, light-manipulation accuracy, and LLM-based heuristics. Finally, our model integrates with specialized optical optimization routines for end-to-end fine-tuning and precision refinement. We benchmark our proposed method against both traditional optimization-based automated design algorithms and LLM counterparts, and experimental results show the superiority of our method.

</details>


### [52] [MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning](https://arxiv.org/abs/2602.23770)
*Chenxing Lin,Xinhui Gao,Haipeng Zhang,Xinran Li,Haitao Wang,Songzhu Mei,Chenglu Wen,Weiquan Liu,Siqi Shen,Cheng Wang*

Main category: cs.LG

TL;DR: 提出了一种基于多尺度自回归生成的离线强化学习方法MAGE，它通过条件引导的多尺度自动编码器和多尺度变换器来学习和生成轨迹表示，从而在长时稀疏奖励任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式方法在处理具有稀疏奖励特征的长时间任务时存在困难。虽然分层生成方法试图通过将原始问题分解为较短时间范围内的子问题来缓解这一问题，但它们往往忽略了轨迹内在的多尺度时间结构，导致性能不佳。

Method: MAGE方法结合了条件引导的多尺度自动编码器与一个多尺度变换器，前者用于学习层次化的轨迹表示，后者则能够从粗到细的时间尺度上自回归地生成轨迹表示。此外，还采用了一个条件引导解码器来精确控制短期行为。

Result: 在五个离线RL基准测试以及与其他十五种基线算法对比实验中，MAGE展示了其成功整合多尺度轨迹建模与条件指导的能力，在长时间稀疏奖励场景下产生连贯且可控的轨迹。

Conclusion: MAGE提供了一种有效解决长时稀疏奖励任务挑战的新途径，通过引入多尺度轨迹建模技术显著提升了生成轨迹的质量和可控性。

Abstract: Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings.

</details>


### [53] [UPath: Universal Planner Across Topological Heterogeneity For Grid-Based Pathfinding](https://arxiv.org/abs/2602.23789)
*Aleksandr Ananikian,Daniil Drozdov,Konstantin Yakovlev*

Main category: cs.LG

TL;DR: 本文提出了一种通用的启发式预测器，该模型仅需训练一次即可泛化到所有未见任务上，显著减少了A*搜索算法所需的计算量，并且在与训练任务完全不同的任务上仍能提供接近最优解的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的方法大多假设训练和测试网格地图来自同一分布，对于不同分布的任务表现较差，限制了它们的实际应用。而实际中往往需要一个能够高效处理任何问题实例的通用求解器。

Method: 设计了一个通用的启发式预测器，该模型经过一次性训练后能够在广泛的未见任务谱系上进行泛化。

Result: 实验表明，所提方法最多可将A*的计算工作量减少至原来的1/2.2，同时在与训练时完全不同的任务上平均提供偏离最优成本3%以内的解决方案。

Conclusion: 本文提出的通用启发式预测器有效提升了A*搜索算法在不同任务上的性能，是首次由可学习求解器达到这一里程碑。

Abstract: The performance of search algorithms for grid-based pathfinding, e.g. A*, critically depends on the heuristic function that is used to focus the search. Recent studies have shown that informed heuristics that take the positions/shapes of the obstacles into account can be approximated with the deep neural networks. Unfortunately, the existing learning-based approaches mostly rely on the assumption that training and test grid maps are drawn from the same distribution (e.g., city maps, indoor maps, etc.) and perform poorly on out-of-distribution tasks. This naturally limits their application in practice when often a universal solver is needed that is capable of efficiently handling any problem instance. In this work, we close this gap by designing an universal heuristic predictor: a model trained once, but capable of generalizing across a full spectrum of unseen tasks. Our extensive empirical evaluation shows that the suggested approach halves the computational effort of A* by up to a factor of 2.2, while still providing solutions within 3% of the optimal cost on average altogether on the tasks that are completely different from the ones used for training $\unicode{x2013}$ a milestone reached for the first time by a learnable solver.

</details>


### [54] [GRAIL: Post-hoc Compensation by Linear Reconstruction for Compressed Networks](https://arxiv.org/abs/2602.23795)
*Wenwu Tang,Dong Wang,Lothar Thiele,Olga Saukh*

Main category: cs.LG

TL;DR: 本文提出了一种名为GRAIL的后处理块补偿方法，它能够在模型压缩后不进行微调的情况下恢复每个块的输入-输出行为。该方法通过使用小规模校准集来总结隐藏激活，并应用岭回归从减少的表示中线性重构原始隐藏表示。这种方法独立于选择器、数据感知（仅需几次前向传递而无需梯度或标签），并且在实际压缩情况下能够提高准确性或降低困惑度。


<details>
  <summary>Details</summary>
Motivation: 结构化的深度模型压缩方法虽然对硬件友好且能显著减少内存和推理成本，但在激进压缩下会导致准确率下降，需要后续微调，而这往往由于缺少标记数据或高昂的训练成本而不切实际。因此，作者提出了一个不需要微调就能恢复模型性能的方法。

Method: 提出的解决方案是名为GRAIL的后处理块补偿方法。该方法利用一个小校准集来总结隐藏层激活状态，并通过Gram矩阵应用岭回归技术从被压缩后的表征中重建原始隐藏层表征。此过程不依赖特定的选择器类型，也不需要梯度或标签信息。

Result: 实验表明，在ResNets、ViTs及解码器型大语言模型上，相较于无数据和有数据意识的剪枝或折叠基线方法，GRAIL在实际压缩比下能够一致地提升准确率或降低困惑度，同时具有可管理的开销且无需反向传播。

Conclusion: GRAIL提供了一种简单有效的零微调步骤，可以在模型压缩之后改善其性能，特别适用于那些因缺乏标记数据或高训练成本而难以进行微调的情况。

Abstract: Structured deep model compression methods are hardware-friendly and substantially reduce memory and inference costs. However, under aggressive compression, the resulting accuracy degradation often necessitates post-compression finetuning, which can be impractical due to missing labeled data or high training cost. We propose post-hoc blockwise compensation, called GRAIL, a simple zero-finetuning step applied after model compression that restores each block's input-output behavior using a small calibration set. The method summarizes hidden activations via a Gram matrix and applies ridge regression to linearly reconstruct the original hidden representation from the reduced one. The resulting reconstruction map is absorbed into the downstream projection weights, while the upstream layer is compressed. The approach is selector-agnostic (Magnitude, Wanda, Gram-based selection, or folding), data-aware (requiring only a few forward passes without gradients or labels), and recovers classic pruning or folding when the Gram matrix is near identity, indicating weak inter-channel correlations. Across ResNets, ViTs, and decoder-only LLMs, GRAIL consistently improves accuracy or perplexity over data-free and data-aware pruning or folding baselines in practical compression regimes, with manageable overhead and no backpropagation. The code is available at https://github.com/TWWinde/GRAIL.

</details>


### [55] [Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies](https://arxiv.org/abs/2602.23811)
*Xiang Li,Nan Jiang,Yuheng Zhang*

Main category: cs.LG

TL;DR: 本研究探讨了在一般函数逼近条件下离线强化学习（RL）的理论方面，扩展了现有算法以适用于大规模或连续动作空间，并解决了参数化策略中的上下文耦合难题，通过将镜像下降与自然策略梯度联系起来，揭示了离线RL和模仿学习之间惊人的统一。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习算法虽然在理论上建立了从离线数据中学习良好策略的基础，但它们通常只对有限且较小的动作空间有效，并且依赖于状态级镜像下降，无法很好地支持实践中常见的独立策略参数化方法。

Method: 研究人员通过将镜像下降法拓展到参数化策略上，识别并处理了其中的核心困难——上下文耦合问题，并展示了如何通过连接镜像下降与自然策略梯度来提供新的分析、保证以及算法见解。

Result: 该研究表明，通过解决参数化策略类中的上下文耦合问题，可以将理论保证扩展到更大或连续的动作空间，并发现离线强化学习与模仿学习之间的新联系。

Conclusion: 这项工作克服了现有离线RL算法的局限性，为在大规模或连续动作空间中使用参数化策略提供了理论基础和技术途径，同时揭示了离线RL与模仿学习之间的一个意外统一。

Abstract: We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning.

</details>


### [56] [Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective](https://arxiv.org/abs/2602.23816)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 本文提出了一种Safe Q Inverse Constrained Reinforcement Learning (SafeQIL)算法，该算法通过最大化演示轨迹的可能性来学习一个策略，同时平衡保守性和高奖励轨迹的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于找到一种策略，在受限的马尔可夫决策过程中（MDP），在已知奖励但未知约束和不可观测成本的情况下，能够安全地执行任务，并且能够在保持保守性的同时显著提高高回报轨迹的可能性。

Method: 提出了一个称为Safe Q Inverse Constrained Reinforcement Learning (SafeQIL) 的算法，它定义了状态-动作对的“承诺”概念，这个概念基于Q值，这不仅取决于特定任务的回报也考虑了状态的安全性评估。

Result: 通过将提出的SafeQIL算法与现有的逆向约束强化学习算法进行比较，证明了其在一系列具有挑战性的基准任务中的优势。

Conclusion: 本研究提供了一种新的方法来处理逆向强化学习问题，特别是在存在约束条件的情况下，通过结合奖励和安全性因素来寻找最有前途的策略。

Abstract: Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits.

</details>


### [57] [Inferring Chronic Treatment Onset from ePrescription Data: A Renewal Process Approach](https://arxiv.org/abs/2602.23824)
*Pavlin G. Poličar,Dalibor Stanimirović,Blaž Zupan*

Main category: cs.LG

TL;DR: 该论文提出了一种基于处方动态的慢性病治疗开始时间推断的概率框架，通过在大规模电子处方数据集上的应用，证明了该方法比简单的基于规则的方法更准确地估计疾病开始时间。


<details>
  <summary>Details</summary>
Motivation: 由于纵向电子健康记录(EHR)数据经常存在左删失问题，导致诊断记录不完整且不可靠，难以准确判断疾病的起始时间。相比之下，门诊处方形成了基于续药的轨迹，为疾病管理提供了连续信号。因此，研究旨在利用这种连续性来改进慢性病治疗开始时间的推断。

Method: 本文提出了一个概率框架，将处方动态建模为更新过程，并通过在基线泊松（随机开药）模式与特定模式威布尔（持续治疗）更新模型之间进行变点检测，来识别从偶发性治疗到持续性治疗之间的转变。

Result: 使用了一个包含240万个体的全国范围内电子处方数据集进行了验证，结果表明所提方法相比于简单基于规则的触发方式，在强左删失情况下能够显著减少不合理提前检测的情况发生，提供更加合理的时间估计。但不同疾病间检测性能存在差异，并且与处方密度密切相关。

Conclusion: 基于治疗的过程来推断慢性病治疗的起始时间是可行的，而且相比传统方法能提供更合理的估计。不过，这种方法的效果受到疾病类型以及处方频率等因素的影响。

Abstract: Longitudinal electronic health record (EHR) data are often left-censored, making diagnosis records incomplete and unreliable for determining disease onset. In contrast, outpatient prescriptions form renewal-based trajectories that provide a continuous signal of disease management. We propose a probabilistic framework to infer chronic treatment onset by modeling prescription dynamics as a renewal process and detecting transitions from sporadic to sustained therapy via change-point detection between a baseline Poisson (sporadic prescribing) regime and a regime-specific Weibull (sustained therapy) renewal model. Using a nationwide ePrescription dataset of 2.4 million individuals, we show that the approach yields more temporally plausible onset estimates than naive rule-based triggering, substantially reducing implausible early detections under strong left censoring. Detection performance varies across diseases and is strongly associated with prescription density, highlighting both the strengths and limits of treatment-based onset inference.

</details>


### [58] [FedNSAM:Consistency of Local and Global Flatness for Federated Learning](https://arxiv.org/abs/2602.23827)
*Junkang Liu,Fanhua Shang,Yuxuan Tian,Hongying Liu,Yuanyuan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的FedNSAM算法，通过引入全局Nesterov动量到局部更新中，来协调全局和平坦度的一致性，从而加速SAM算法。理论和实验证明了FedNSAM在收敛性和性能上的优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中多步本地更新和数据异质性通常会导致更尖锐的全局极小值，这会降低全局模型的性能。尽管现有的FL算法通过将锐度感知最小化(SAM)整合到本地训练中试图解决这个问题，但在高数据异质性的环境下，本地训练中的平坦度并不能保证全局模型同样平坦。因此，单纯减小客户端数据上局部损失面的锐度并不能有效提升SAM在FL中的效能，进而改善全局模型的泛化能力。

Method: 研究者定义了'平坦距离'的概念来解释上述现象，并基于此重新思考了FL中的SAM方法。通过理论分析'平坦距离'，提出了FedNSAM算法，该算法通过向局部更新过程中加入全局Nesterov动量，以促进全局与局部平坦度之间的一致性，从而加速SAM过程。FedNSAM利用全局Nesterov动量作为客户端全球扰动估计及外推的方向。

Result: 从理论上讲，研究证明了借助Nesterov外推法，FedNSAM相比FedSAM能够获得更紧的收敛界限。实验方面，在CNN和Transformer模型上进行了全面测试，验证了FedNSAM在性能和效率方面的优越表现。

Conclusion: 通过引入全局Nesterov动量至局部更新步骤，FedNSAM算法成功地提升了SAM在处理具有高度数据异质性的联邦学习场景下的有效性，不仅提高了模型的泛化能力还增强了训练效率。

Abstract: In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \textbf{flatness distance}, we propose a novel \textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM.

</details>


### [59] [ULW-SleepNet: An Ultra-Lightweight Network for Multimodal Sleep Stage Scoring](https://arxiv.org/abs/2602.23852)
*Zhaowen Wang,Dongdong Zhou,Qi Xu,Fengyu Cong,Mohammad Al-Sa'd,Jenni Raitoharju*

Main category: cs.LG

TL;DR: 本文提出了ULW-SleepNet，一种超轻量级多模态睡眠阶段评分框架，它能够有效整合多种生理信号的信息，并在减少计算负担的同时保持竞争性的准确度。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型虽然在自动睡眠阶段评分领域有所进展，但多数模型对计算资源要求高且仅适用于单通道脑电图（EEG），这限制了它们在处理多模态多导睡眠图(PSG)数据时的应用。为了解决这些问题，研究者开发了一种新的模型。

Method: 研究者提出了一种名为ULW-SleepNet的新型架构，该架构通过引入双流可分离卷积(DSSC)模块、深度可分离卷积、通道参数共享以及全局平均池化技术来降低计算开销同时保持较高精度。

Result: 在Sleep-EDF-20和Sleep-EDF-78数据集上评估时，ULW-SleepNet分别达到了86.9%和81.4%的准确率，仅有13.3K个参数和7.89M FLOPs。与最先进方法相比，该模型减少了高达98.6%的参数量，性能损失微乎其微。

Conclusion: ULW-SleepNet展示了强大的潜力，特别适合于穿戴式及物联网设备上的实时睡眠监测应用。

Abstract: Automatic sleep stage scoring is crucial for the diagnosis and treatment of sleep disorders. Although deep learning models have advanced the field, many existing models are computationally demanding and designed for single-channel electroencephalography (EEG), limiting their practicality for multimodal polysomnography (PSG) data. To overcome this, we propose ULW-SleepNet, an ultra-lightweight multimodal sleep stage scoring framework that efficiently integrates information from multiple physiological signals. ULW-SleepNet incorporates a novel Dual-Stream Separable Convolution (DSSC) Block, depthwise separable convolutions, channel-wise parameter sharing, and global average pooling to reduce computational overhead while maintaining competitive accuracy. Evaluated on the Sleep-EDF-20 and Sleep-EDF-78 datasets, ULW-SleepNet achieves accuracies of 86.9% and 81.4%, respectively, with only 13.3K parameters and 7.89M FLOPs. Compared to state-of-the-art methods, our model reduces parameters by up to 98.6% with only marginal performance loss, demonstrating its strong potential for real-time sleep monitoring on wearable and IoT devices. The source code for this study is publicly available at https://github.com/wzw999/ULW-SLEEPNET.

</details>


### [60] [A Theory of Random Graph Shift in Truncated-Spectrum vRKHS](https://arxiv.org/abs/2602.23880)
*Zhang Wan,Tingting Mu,Samuel Kaski*

Main category: cs.LG

TL;DR: 本文通过随机图生成的角度开发了一种在领域迁移下进行图分类的理论，其中假定同一类内的图共享相同的随机图模型（RGM），而领域迁移由RGM组件的变化引起。文章提出了一种假设RGM为数据生成过程的理论，并利用其与函数空间中假设复杂度的联系来进行细致的分析。基于向量值再生核希尔伯特空间（vRKHS）公式，推导出一个泛化边界，该边界的迁移惩罚可以分解为：(i) 领域差异项、(ii) 由可访问截断谱总结的谱几何项，以及(iii) 汇聚收敛和构建稳定性效应的幅度项。


<details>
  <summary>Details</summary>
Motivation: 现有的领域适应（DA）理论虽已很好地支持了处理图分布迁移的技术，但对于图样本本身作为结构化对象的信息探索较少。鉴于图的非欧几里得性质及针对图学习的专业架构进一步复杂化了对图分布迁移的细致分析，因此需要一种新的方法来更好地理解和处理这种迁移。

Method: 提出了一种将随机图模型（RGM）视为数据生成过程的新理论框架，并从函数空间角度探讨了它与假设复杂度之间的联系。研究者们还基于向量值再生核希尔伯特空间(vRKHS)建立了一个数学公式，用以推导出一个新的泛化界限，该界限能更细致地分析由于领域迁移导致的图分布变化。

Result: 所提出的理论成功地提供了对于领域迁移条件下图分类问题的深入理解。通过实验验证了新定义的各项指标（如领域差异、谱几何特性及振幅因素）的有效性。这些结果既适用于真实世界的数据集也适用于模拟环境。

Conclusion: 本研究提供了一种新的视角来解决领域迁移条件下的图分类挑战，通过引入随机图模型作为基础并结合先进的数学工具，能够更精确地衡量和解释不同类型的因素如何影响模型性能。

Abstract: This paper develops a theory of graph classification under domain shift through a random-graph generative lens, where we consider intra-class graphs sharing the same random graph model (RGM) and the domain shift induced by changes in RGM components. While classic domain adaptation (DA) theories have well-underpinned existing techniques to handle graph distribution shift, the information of graph samples, which are itself structured objects, is less explored. The non-Euclidean nature of graphs and specialized architectures for graph learning further complicate a fine-grained analysis of graph distribution shifts. In this paper, we propose a theory that assumes RGM as the data generative process, exploiting its connection to hypothesis complexity in function space perspective for such fine-grained analysis. Building on a vector-valued reproducing kernel Hilbert space (vRKHS) formulation, we derive a generalization bound whose shift penalty admits a factorization into (i) a domain discrepancy term, (ii) a spectral-geometry term summarized by the accessible truncated spectrum, and (iii) an amplitude term that aggregates convergence and construction-stability effects. We empirically verify the insights on these terms in both real data and simulations.

</details>


### [61] [LK Losses: Direct Acceptance Rate Optimization for Speculative Decoding](https://arxiv.org/abs/2602.23881)
*Alexander Samarin,Sergei Krutikov,Anton Shevtsov,Sergei Skvortsov,Filipp Fisin,Alexander Golubev*

Main category: cs.LG

TL;DR: 本文提出了一种名为LK损失的特殊训练目标，直接针对接受率进行优化，以解决在使用推测解码加速自回归大型语言模型推理时，小型草稿模型因容量有限而无法最大化接受率的问题。实验表明，与基于KL散度的标准训练相比，该方法在所有配置下都能提高接受率指标。


<details>
  <summary>Details</summary>
Motivation: 标准训练通过最小化Kullback-Leibler (KL) 散度来间接影响接受率，但这种方法对于容量有限的小型草稿模型来说，并不能保证接受率的最大化。因此，研究者们旨在找到一种可以直接提升接受率的新训练方法。

Method: 研究者们提出了LK损失函数，这是一种专门设计用来直接优化接受率的训练目标。LK损失函数易于实现且不引入额外计算负担，能够无缝集成到现有的推测器训练框架中。

Result: 通过对四种草稿架构和六个参数规模从8亿至685亿的目标模型进行全面测试，结果表明相比于传统的基于KL的方法，LK损失在各种设定下均能显著提高接受率指标，在一般、编码及数学领域上平均接受长度增加了8-10%。

Conclusion: LK损失为提高推测解码过程中的接受率提供了一个有效且实用性强的新方案，它不仅提升了性能表现，还具备良好的兼容性，可直接应用于当前存在的任何推测器训练环境中。

Abstract: Speculative decoding accelerates autoregressive large language model (LLM) inference by using a lightweight draft model to propose candidate tokens that are then verified in parallel by the target model. The speedup is significantly determined by the acceptance rate, yet standard training minimizes Kullback-Leibler (KL) divergence as a proxy objective. While KL divergence and acceptance rate share the same global optimum, small draft models, having limited capacity, typically converge to suboptimal solutions where minimizing KL does not guarantee maximizing acceptance rate. To address this issue, we propose LK losses, special training objectives that directly target acceptance rate. Comprehensive experiments across four draft architectures and six target models, ranging from 8B to 685B parameters, demonstrate consistent improvements in acceptance metrics across all configurations compared to the standard KL-based training. We evaluate our approach on general, coding and math domains and report gains of up to 8-10% in average acceptance length. LK losses are easy to implement, introduce no computational overhead and can be directly integrated into any existing speculator training framework, making them a compelling alternative to the existing draft training objectives.

</details>


### [62] [MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening](https://arxiv.org/abs/2602.23994)
*Vrushank Ahire,Yogesh Kumar,Anouck Girard,M. A. Ganaie*

Main category: cs.LG

TL;DR: 研究提出了一种名为MINT的多模态框架，通过将MRI的生物标志物结构转移到语音编码器中，实现了基于MRI知识的语音分析以筛查早期阿尔茨海默病。该方法在不需要实际进行神经影像检查的情况下，达到了与仅使用语音基线相当的表现，并且通过结合MRI信息进一步提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的发展过程中，轻度认知障碍（MCI）是从正常老化到痴呆的关键过渡阶段。虽然神经影像学如结构磁共振成像（MRI）提供了这一转变的生物标记物，但其高昂的成本和基础设施需求限制了大规模应用。而单独基于语音分析的方法虽提供了一个非侵入性的替代方案，但由于缺乏生物学基础，使得它在区分正常对照组与MCI时不够可靠。

Method: 提出了一个多阶段跨模态框架MINT，旨在训练期间将MRI中的生物标志物结构转移至语音编码器。首先利用1,228名受试者的数据训练一个MRI教师模型，定义出用于区分正常对照组与MCI的紧凑神经影像嵌入空间；然后通过残差投影头配合几何损失函数，使语音表示与此冻结的影像流形对齐，同时保持影像编码器的保真度；最后，在推理阶段直接应用从未接触过语音数据的冻结MRI分类器于已对齐的嵌入上。

Result: 实验结果表明，在ADNI-4数据集上测试时，对齐后的语音表现与仅使用语音基线的结果相当(AUC 0.720 vs 0.711)，并且无需在推理阶段使用任何影像资料。此外，多模态融合相比单独使用MRI有所改进(0.973 vs 0.958)。消融研究表明，dropout正则化和自监督预训练是关键的设计决策。

Conclusion: 这是首次展示从MRI到语音的知识迁移应用于早期阿尔茨海默病筛查的研究，为无需神经影像的大规模认知初步筛查建立了一条具有生物学基础的新途径。

Abstract: Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference.

</details>


### [63] [InfoNCE Induces Gaussian Distribution](https://arxiv.org/abs/2602.24012)
*Roy Betser,Eyal Gofer,Meir Yossef Levi,Guy Gilboa*

Main category: cs.LG

TL;DR: 该论文探讨了对比学习中的InfoNCE目标如何诱导表示向量呈现出高斯结构，并通过理论分析和实验证明了这一点。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解释为什么在对比学习中训练出的表示倾向于呈现高斯分布特性，提供这种现象背后的原理性理解。

Method: 方法包括在特定对齐和集中假设下展示高维表示投影接近多变量高斯分布，以及通过加入轻微正则化项促进低特征范数与高特征熵来达到类似渐近结果。

Result: 实验结果表明，在合成数据集及CIFAR-10上使用多种编码器架构时，所提出的模型一致地表现出高斯行为。

Conclusion: 结论是，该工作为对比学习中普遍观察到的高斯性提供了原则性的解释，并且得到的高斯模型有望支持对比学习领域的广泛应用。

Abstract: Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.

</details>


### [64] [RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models](https://arxiv.org/abs/2602.24040)
*Daniel Yang,Samuel Stante,Florian Redhardt,Lena Libon,Parnian Kassraie,Ido Hakimi,Barna Pásztor,Andreas Krause*

Main category: cs.LG

TL;DR: 本文提出了一种名为RewardUQ的统一框架，用于系统地评估奖励模型中的不确定性量化，并比较了不同方法在准确性和校准性方面的表现。实验结果表明，模型大小和初始化对性能的影响最大。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法依赖于点估计来确定奖励模型，但这种方法忽略了由于有限的人类反馈而产生的认知不确定性。考虑到这种不确定性可以减少通过不确定性引导的主动学习带来的人工标注成本，并减轻LLM后训练中的奖励过度优化问题。然而，目前还没有对不确定性感知奖励模型进行彻底的比较研究。

Method: 本文引入了一个称为RewardUQ的统一框架，用以系统地评估奖励模型中的不确定性量化。该框架不仅沿用了衡量准确性和校准性的标准指标，还提出了一个结合这两个维度的新排名策略，以便更简便地进行比较。

Result: 实验结果显示，模型大小与初始化对最终性能影响最大。此外，先前的很多工作如果采用不同的设计选择可能会取得更好的效果。为了促进新方法的发展与评估，并支持下游应用的部署，作者们开源了他们的框架作为一个Python包。

Conclusion: 通过引入RewardUQ框架，这项工作为理解和改进不确定性量化提供了新的视角，强调了适当考虑模型大小及初始化的重要性。同时，它也为未来的研究者提供了一个强大且易于使用的工具集。

Abstract: Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.

</details>


### [65] [pathsig: A GPU-Accelerated Library for Truncated and Projected Path Signatures](https://arxiv.org/abs/2602.24066)
*Tobias Nygaard*

Main category: cs.LG

TL;DR: 本文介绍了一个名为pathsig的PyTorch原生库，它能高效地计算路径签名，并且在GPU上实现了高吞吐量和几乎最小的峰值内存使用。与其它库相比，pathsig在截断签名计算方面达到了10-30倍的速度提升，在需要通过签名进行反向传播的训练中则达到了4-10倍的速度提升。此外，pathsig还支持用户自定义词集上的签名投影以及基于不均匀路径规律性的各向异性截断，有助于减少维度、冗余性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 虽然路径签名作为序列数据的一种丰富表示方法，在多种机器学习任务中表现出色并具有坚实的理论保证，但现有的库通常缺乏支持大规模基于梯度学习所需的可扩展性。为了解决这一问题，本研究旨在开发一个能够满足这种需求的新库。

Method: 引入了名为pathsig的PyTorch原生库，该库直接以词基为基础计算路径签名。通过利用CUDA内核并行更新前缀闭合词集上的签名系数，pathsig能够在保持接近最小峰值内存的同时实现高GPU吞吐量。此外，除了常规截断外，pathsig还支持将（无限维）签名投影到用户指定的词集上，并且支持由不均匀路径规律性激发的各向异性截断。

Result: 与其他库相比，pathsig在计算截断签名时速度提升了10-30倍；对于需要通过签名进行反向传播的训练过程，其速度也提高了4-10倍。同时，通过支持更紧凑的数据表示方式，pathsig还有助于减少维度、去除冗余并降低计算成本。

Conclusion: pathsig作为一个高效的路径签名计算库，不仅极大地提高了计算效率，而且提供了更加灵活的数据处理选项，从而在实际应用中展现出显著的优势。

Abstract: Path signatures provide a rich representation of sequential data, with strong theoretical guarantees and good performance in a variety of machine-learning tasks. While signatures have progressed from fixed feature extractors to trainable components of machine-learning models, existing libraries often lack the required scalability for large-scale, gradient-based learning. To address this gap, this paper introduces pathsig, a PyTorch-native library that computes path signatures directly in the word basis. By using CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, pathsig achieves high GPU throughput and near-minimal peak memory. Compared with other libraries, pathsig achieves 10-30x speedups for computation of truncated signatures and up to 4-10x speedups in training that require backpropagation through the signature. Beyond regular truncation, pathsig supports projections of the (infinite-dimensional) signature onto user-specified sets of words and anisotropic truncation motivated by inhomogeneous path regularity, enabling more compact representations that can reduce dimensionality, redundancy, and computational cost.

</details>


### [66] [Neural Diffusion Intensity Models for Point Process Data](https://arxiv.org/abs/2602.24083)
*Xinlong Du,Harsha Honnappa,Vinayak Rao*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经SDE的Cox过程变分框架，即神经扩散强度模型。通过显式的漂移修正保持了潜在强度的扩散结构，从而保证变分家族包含真实后验。实验表明该方法能够准确恢复潜在强度动态和后验路径，并且相比基于MCMC的方法具有显著的速度优势。


<details>
  <summary>Details</summary>
Motivation: Cox过程用于建模过分散点过程数据，但其非参数估计及后验推理通常难以处理，依赖于昂贵的MCMC方法。为了克服这些问题，提出了一个新的框架以提高效率并保持准确性。

Method: 引入了神经扩散强度模型（Neural Diffusion Intensity Models），一种由神经SDE驱动的Cox过程变分框架。利用过滤扩增理论证明了条件于点过程观察保留了潜在强度的扩散结构，并通过明确的漂移修正实现。设计了一个可摊销编码器架构，将不同长度的事件序列映射到后验强度路径上。

Result: 实验结果表明，所提出的方法能够在合成数据和真实世界数据集上准确地恢复潜在强度动态和后验路径，相较于基于MCMC的方法表现出数量级上的速度提升。

Conclusion: 神经扩散强度模型提供了一种有效替代传统MCMC方法的新途径，能够在保持较高精度的同时大幅度减少计算成本。

Abstract: Cox processes model overdispersed point process data via a latent stochastic intensity, but both nonparametric estimation of the intensity model and posterior inference over intensity paths are typically intractable, relying on expensive MCMC methods. We introduce Neural Diffusion Intensity Models, a variational framework for Cox processes driven by neural SDEs. Our key theoretical result, based on enlargement of filtrations, shows that conditioning on point process observations preserves the diffusion structure of the latent intensity with an explicit drift correction. This guarantees the variational family contains the true posterior, so that ELBO maximization coincides with maximum likelihood estimation under sufficient model capacity. We design an amortized encoder architecture that maps variable-length event sequences to posterior intensity paths by simulating the drift-corrected SDE, replacing repeated MCMC runs with a single forward pass. Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods.

</details>


### [67] [Learning with a Budget: Identifying the Best Arm with Resource Constraints](https://arxiv.org/abs/2602.24146)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文提出了一个名为SH-RR的算法，用于解决在资源限制条件下的最佳选择问题。该算法结合了对资源消耗的认知，并能够统一处理随机和确定性消费场景。


<details>
  <summary>Details</summary>
Motivation: 在很多应用中，评估不同选项的有效性伴随着不同的成本或资源使用。受这种异质性的驱动，作者研究了存在资源约束时如何识别最佳选项的问题。每次选择都会消耗一种或多种有限的资源。

Method: 提出了一种新的算法——Successive Halving with Resource Rationing (SH-RR)，它将资源意识分配整合到最佳选择识别的经典连续减半框架中。

Result: SH-RR算法不仅适用于随机消费环境也适用于确定性消费环境，并引入了一个新的有效消费度量标准来统一理论分析。

Conclusion: 通过引入SH-RR算法及新的有效消费衡量方式，为解决资源受限条件下的最佳选择识别问题提供了创新方法。

Abstract: In many applications, evaluating the effectiveness of different alternatives comes with varying costs or resource usage. Motivated by such heterogeneity, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem, where an agent seeks to identify the best alternative (aka arm) in the presence of resource constraints. Each arm pull consumes one or more types of limited resources. We make two key contributions. First, we propose the Successive Halving with Resource Rationing (SH-RR) algorithm, which integrates resource-aware allocation into the classical successive halving framework on best arm identification. The SH-RR algorithm unifies the theoretical analysis for both the stochastic and deterministic consumption settings, with a new \textit{effective consumption measure

</details>


### [68] [Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension](https://arxiv.org/abs/2602.24178)
*Adam R. Klivans,Konstantinos Stavropoulos,Arsen Vasilyan*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法来构建低度夹层多项式，对于一些基本函数类和边际分布，该方法在度数界限上取得了显著的改进。特别是对于高斯分布下的k个半空间函数，我们获得了度数为poly(k)的夹层多项式，相比之前的2^O(k)界限有了指数级的提升。此外，我们的方法还适用于低维且边界平滑的函数类。


<details>
  <summary>Details</summary>
Motivation: 先前的工作展示了低度夹层多项式近似器在学习设置中的强大能力，如具有分布偏移的学习、可测试学习以及污染学习。但现有的方法在处理某些函数类时存在度数界限过高的问题。本文旨在通过提供一种新的构造方法来解决这个问题，以获得更低度数的夹层多项式。

Method: 本文介绍的新方法主要利用目标函数边界的平滑性来直接构造夹层Lipschitz函数，并借助高维逼近理论的结果。此方法避免了使用先前最佳结果中所采用的FT-软化技术，从而对低维多项式阈值函数（PTFs）关于高斯分布的情况下，实现了双重指数级别的改进。

Result: 新方法成功地降低了多个重要函数类及特定边际分布条件下所需的多项式度数。特别地，在高斯分布下针对k个半空间函数的情况，所得夹层多项式的度数降低至poly(k)，较之前的研究成果有着显著改善。

Conclusion: 本研究提出的方法为构造低度夹层多项式提供了更有效途径，特别是在处理具有平滑边界的低维函数类时展现了优越性。这不仅简化了证明过程，而且在某些情况下带来了指数乃至双指数级别的改进。

Abstract: Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.
  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result.

</details>


### [69] [Multi-Objective Reinforcement Learning for Large-Scale Tote Allocation in Human-Robot Collaborative Fulfillment Centers](https://arxiv.org/abs/2602.24182)
*Sikata Sengupta,Guangyi Liu,Omer Gottesman,Joseph W Durham,Michael Kearns,Aaron Roth,Michael Caldara*

Main category: cs.LG

TL;DR: 本文提出了一种基于多目标强化学习的方法来优化基于容器的配送中心的整合过程，通过在零和博弈中运用最佳响应和无悔动态策略，解决了受限强化学习问题。该方法能够在现实仓库模拟中有效平衡多个目标，并学习到一个同时满足所有约束条件的单一策略。此外，还引入了一个理论框架处理误差抵消问题。


<details>
  <summary>Details</summary>
Motivation: 为了优化基于容器的配送中心中的整合流程，需要在处理速度、资源使用率以及空间利用率等竞争性目标之间做出权衡，同时遵守一系列实际操作限制。

Method: 将此问题定义为具有高维状态空间与动态系统行为的大规模多目标强化学习任务。基于最近在解决受限RL问题上取得的理论进展，特别是通过零和博弈中的最佳响应及无悔动态实现原则性的最小最大策略学习。

Result: 仿真结果显示所提方法能够有效地在不同目标间进行权衡，并且观察到即使理论上未保证也能学到一个单独策略以同时满足所有约束条件。对于误差抵消问题（时间平均解显示出振荡行为），提出的方法返回了一个迭代结果，其拉格朗日值接近游戏的最小最大值。

Conclusion: 这些成果展示了多目标强化学习在解决大规模工业系统中复杂而重要的决策问题上的潜力。

Abstract: Optimizing the consolidation process in container-based fulfillment centers requires trading off competing objectives such as processing speed, resource usage, and space utilization while adhering to a range of real-world operational constraints. This process involves moving items between containers via a combination of human and robotic workstations to free up space for inbound inventory and increase container utilization. We formulate this problem as a large-scale Multi-Objective Reinforcement Learning (MORL) task with high-dimensional state spaces and dynamic system behavior. Our method builds on recent theoretical advances in solving constrained RL problems via best-response and no-regret dynamics in zero-sum games, enabling principled minimax policy learning. Policy evaluation on realistic warehouse simulations shows that our approach effectively trades off objectives, and we empirically observe that it learns a single policy that simultaneously satisfies all constraints, even if this is not theoretically guaranteed. We further introduce a theoretical framework to handle the problem of error cancellation, where time-averaged solutions display oscillatory behavior. This method returns a single iterate whose Lagrangian value is close to the minimax value of the game. These results demonstrate the promise of MORL in solving complex, high-impact decision-making problems in large-scale industrial systems.

</details>


### [70] [Flow-Based Density Ratio Estimation for Intractable Distributions with Applications in Genomics](https://arxiv.org/abs/2602.24201)
*Egor Antipov,Alessandro Palma,Lorenzo Consoli,Stephan Günnemann,Andrea Dittadi,Fabian J. Theis*

Main category: cs.LG

TL;DR: 该论文提出了一种基于条件感知流匹配的方法来估计难以处理的数据分布对之间的密度比，这种方法可以有效减少计算成本，并在单细胞基因组数据分析中展示了其应用潜力。


<details>
  <summary>Details</summary>
Motivation: 估计成对难以处理的数据分布之间的密度比是概率建模中的一个核心问题。虽然正则化流等精确似然模型为密度比估计提供了一个有前景的方法，但基于流的直接评估方法由于需要分别为每个分布模拟昂贵的似然积分而计算成本高昂。

Method: 作者们利用了条件感知流匹配技术，推导出一种单一的动力学公式，用于沿生成轨迹跟踪密度比的变化。

Result: 研究显示，在封闭形式比率估计的模拟基准测试上表现出了竞争力，并且该方法支持单细胞基因omics数据分析中的多种任务，如通过比较不同实验条件下细胞状态的可能性来进行治疗效果评估和批次校正评价。

Conclusion: 本文提出的基于条件感知流匹配的新方法为解决数据分布间密度比估计问题提供了一个高效解决方案，并在实际应用场景中（特别是单细胞基因组学）展现了其价值。

Abstract: Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling, enabling principled comparisons of sample likelihoods under different data-generating processes across conditions and covariates. While exact-likelihood models such as normalizing flows offer a promising approach to density ratio estimation, naive flow-based evaluations are computationally expensive, as they require simulating costly likelihood integrals for each distribution separately. In this work, we leverage condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories. We demonstrate competitive performance on simulated benchmarks for closed-form ratio estimation, and show that our method supports versatile tasks in single-cell genomics data analysis, where likelihood-based comparisons of cellular states across experimental conditions enable treatment effect estimation and batch correction evaluation.

</details>


### [71] [The Stability of Online Algorithms in Performative Prediction](https://arxiv.org/abs/2602.24207)
*Gabriele Farina,Juan Carlos Perdomo*

Main category: cs.LG

TL;DR: 本文研究了算法预测在决策中的使用如何导致反馈循环，并证明了任何无悔算法在这种表现性设置中都会收敛到一个（混合）表现稳定均衡，揭示了如梯度下降等常见算法为何自然具有稳定性并防止失控反馈循环。


<details>
  <summary>Details</summary>
Motivation: 随着算法预测越来越多地用于决策，这些模型开始主动影响我们看到的数据分布，进而影响后续的再训练过程。这种现象被称作表现性预测。先前的研究对模型如何影响数据分布做出了较强的限制条件，而本文试图在不设此类假设的情况下探讨该问题。

Method: 本文采用了一种鞅论点，并允许随机化处理，从而避免了以往研究中对模型影响数据分布方式所作的强假设，绕过了寻找稳定模型的最新难度结果。

Result: 研究证明了，在表现性环境中部署的任何无悔算法最终都将收敛至一个（可能是混合策略下的）表现稳定均衡状态。此外，还阐明了像梯度下降这样的常用算法之所以能够自然稳定、防止出现失控反馈循环的原因。

Conclusion: 这项工作为在线优化与表现性之间未来的技术理念转移提供了可能，同时也为理解及设计更稳健的表现性预测系统奠定了基础。

Abstract: The use of algorithmic predictions in decision-making leads to a feedback loop where the models we deploy actively influence the data distributions we see, and later use to retrain on. This dynamic was formalized by Perdomo et al. 2020 in their work on performative prediction. Our main result is an unconditional reduction showing that any no-regret algorithm deployed in performative settings converges to a (mixed) performatively stable equilibrium: a solution in which models actively shape data distributions in ways that their own predictions look optimal in hindsight. Prior to our work, all positive results in this area made strong restrictions on how models influenced distributions. By using a martingale argument and allowing randomization, we avoid any such assumption and sidestep recent hardness results for finding stable models. Lastly, on a more conceptual note, our connection sheds light on why common algorithms, like gradient descent, are naturally stabilizing and prevent runaway feedback loops. We hope our work enables future technical transfer of ideas between online optimization and performativity.

</details>


### [72] [An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2602.24209)
*Mohsen Tajgardan,Atena Shiranzaei,Mahdi Rabbani,Reza Khoshkangini,Mahtab Jamali*

Main category: cs.LG

TL;DR: 本研究提出了一种高效的无监督联邦学习框架，通过利用两个不同物联网数据集的共享特征来提高异常检测性能，同时保持数据集特定特征。实验表明，该方法在异常检测准确性方面显著优于传统联邦学习方法。


<details>
  <summary>Details</summary>
Motivation: 针对物联网环境中由于设备能力、数据格式和通信限制的不同导致的数据异构性问题，以及这种异构性对维护全局模型性能和隐私构成的挑战，特别是对于基于物联网的异常检测场景中无监督联邦学习的应用。

Method: 提出了一个能够利用来自专注于异常检测与设备识别两大数据集之间共享特征的无监督联邦学习框架，并通过可解释AI技术如SHAP值来增强透明度和可解释性，确定影响局部模型决策的关键特征。

Result: 实验结果表明，在真实世界的物联网数据集上，所提出的方法在异常检测准确率方面明显优于传统的联邦学习方法。

Conclusion: 这项工作展示了通过使用互补数据集中的共享特征优化无监督联邦学习以实现在去中心化的物联网环境中取得更优异常检测结果的可能性。

Abstract: Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments.

</details>


### [73] [Adaptive Combinatorial Experimental Design: Pareto Optimality for Decision-Making and Inference](https://arxiv.org/abs/2602.24231)
*Hongrui Xie,Junyu Cao,Kan Xu*

Main category: cs.LG

TL;DR: 本文首次探讨了自适应组合实验设计，特别是在组合多臂老虎机（CMAB）中权衡遗憾最小化与统计功效的问题。通过定义帕累托最优性来形式化这一权衡，并为全老虎机反馈和半老虎机反馈两种情况分别提出了MixCombKL和MixCombUCB算法。研究结果表明，这两种算法都能达到帕累托最优，并且更丰富的反馈可以显著提高估计准确性，从而缩小可实现的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 在组合多臂老虎机问题中，为了同时达成遗憾最小化和对奖励差距进行准确推断这两个目标，需要解决探索与利用之间的权衡问题。本文旨在为此类问题提供一个原则性的框架。

Method: 作者引入了帕累托最优的概念来正式定义学习过程中的这种权衡，并根据不同的信息结构（即全老虎机反馈和半老虎机反馈），分别为这两种情况设计了名为MixCombKL和MixCombUCB的算法。

Result: 研究表明，所提出的两种算法都能够实现帕累托最优，在有限时间内保证了低遗憾值以及较小的手臂间差距估计误差。此外，发现更丰富的反馈类型能够显著提升估计精度，进而缩小了可达帕累托前沿。

Conclusion: 本研究为多目标决策制定中的自适应组合实验确立了一个有原则的框架，强调了在不同信息结构下有效平衡探索与利用的重要性。

Abstract: In this paper, we provide the first investigation into adaptive combinatorial experimental design, focusing on the trade-off between regret minimization and statistical power in combinatorial multi-armed bandits (CMAB). While minimizing regret requires repeated exploitation of high-reward arms, accurate inference on reward gaps requires sufficient exploration of suboptimal actions. We formalize this trade-off through the concept of Pareto optimality and establish equivalent conditions for Pareto-efficient learning in CMAB. We consider two relevant cases under different information structures, i.e., full-bandit feedback and semi-bandit feedback, and propose two algorithms MixCombKL and MixCombUCB respectively for these two cases. We provide theoretical guarantees showing that both algorithms are Pareto optimal, achieving finite-time guarantees on both regret and estimation error of arm gaps. Our results further reveal that richer feedback significantly tightens the attainable Pareto frontier, with the primary gains arising from improved estimation accuracy under our proposed methods. Taken together, these findings establish a principled framework for adaptive combinatorial experimentation in multi-objective decision-making.

</details>
