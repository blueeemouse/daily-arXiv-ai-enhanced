<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 88]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [StraTyper: Automated Semantic Type Discovery and Multi-Type Annotation for Dataset Collections](https://arxiv.org/abs/2602.04004)
*Christos Koutras,Juliana Freire*

Main category: cs.DB

TL;DR: StraTyper是一种新的方法，用于在数据集集合中发现列类型（CTD）和多类型注释（CMTA），通过系统地使用大语言模型（LLMs）来为特定数据集集合定制语义类型。它通过策略性的列聚类、受控的类型生成和迭代级联发现，在保持类型精度的同时最大化注释覆盖率，并减少了LLM的成本。实验表明，StraTyper能够准确地为数值型和非数值型数据发现类型，与商业LLM相比显著节省成本，并能有效地处理多类型列，同时改善下游任务如连接发现和模式匹配。


<details>
  <summary>Details</summary>
Motivation: 现有的列类型注解(CTA)方法需要用户指定一个封闭的语义类型集合，这限制了它们应用于特定领域数据集的能力，因为预定义标签往往缺乏足够的覆盖范围和特异性。此外，现实世界的数据集中经常包含属于多个语义类型的值，这违背了现有CTA方法的单一类型假设。虽然专有大语言模型（LLMs）在CTA上表现有效，但它们会产生高昂的费用并且对相似列产生不一致的结果，导致类型冗余，从而影响后续应用。

Method: 提出了StraTyper，一种用于数据集集合中列类型发现（CTD）和多类型注释（CMTA）的成本效益方法。该方法不需要预先定义语义标签，而是通过系统性地利用LLMs来发现适合当前数据集集合的类型。通过战略性列聚类、控制类型生成以及迭代级联发现，StraTyper能够在最小化LLM成本的同时平衡类型精确度和注释覆盖率。

Result: 实验证明，无论是手动还是借助LLM辅助评估，StraTyper都能为数值和非数值数据发现准确的类型，并且与商用LLM相比实现了大量成本节约。此外，它还能有效处理具有多重类型的列，并且其注释有助于提升包括连接发现和模式匹配在内的下游任务性能，优于仅基于LLM的基线方法。

Conclusion:  StraTyper提供了一种创新的方法来解决列类型注释中存在的问题，特别是在处理领域特定数据集时遇到的挑战。通过消除预定义语义标签的需求，采用策略性手段减少LLM成本，同时保证高精度和广泛覆盖，使得StraTyper不仅提高了效率还降低了开销。此外，它还展示了在改进下游应用程序方面优于传统方法的优势。

Abstract: Understanding dataset semantics is crucial for effective search, discovery, and integration pipelines. To this end, column type annotation (CTA) methods associate columns of tabular datasets with semantic types that accurately describe their contents, using pre-trained deep learning models or Large Language Models (LLMs). However, existing approaches require users to specify a closed set of semantic types either at training or inference time, hindering their application to domain-specific datasets where pre-defined labels often lack adequate coverage and specificity. Furthermore, real-world datasets frequently contain columns with values belonging to multiple semantic types, violating the single-type assumption of existing CTA methods. While proprietary LLMs have shown effectiveness for CTA, they incur high monetary costs and produce inconsistent outputs for similar columns, leading to type redundancy that negatively affects downstream applications. To address these challenges, we introduce StraTyper, a cost-effective method for column type discovery (CTD) and multi-type annotation (CMTA) in dataset collections. StraTyper eliminates the need for pre-defined semantic labels by systematically employing LLMs to discovery types tailored to the dataset collection at hand. Through strategic column clustering, controlled type generation, and iterative cascading discovery, StraTyper balances type precision with annotation coverage while minimizing LLM costs. Our experimental evaluation-both manual and LLM-assisted-on real-world benchmarks demonstrates that StraTyper discovers accurate types for both numerical and non-numerical data, achieves substantial cost savings compared to commercial LLMs, and effectively handles multi-typed columns. We further show that StraTyper's annotations improve downstream tasks, including join discovery and schema matching, outperforming LLM-only baselines.

</details>


### [2] [PluRel: Synthetic Data unlocks Scaling Laws for Relational Foundation Models](https://arxiv.org/abs/2602.04029)
*Vignesh Kothapalli,Rishabh Ranjan,Valter Hudovernik,Vijay Prakash Dwivedi,Johannes Hoffart,Carlos Guestrin,Jure Leskovec*

Main category: cs.DB

TL;DR: 介绍了PluRel框架，用于从零开始合成多表关系数据库。该框架通过建模模式、表间主外键连接以及表内特征分布来支持多样数据库的合成。研究表明，使用合成数据进行预训练可以提高模型在真实数据库上的泛化能力，并为继续在真实数据库上进行预训练提供了强大的基础模型。


<details>
  <summary>Details</summary>
Motivation: 由于隐私限制，需要用来训练关系基础模型（RFMs）的多样化关系数据库很少公开。尽管存在生成任意大小合成表格数据的方法，但要将模式结构和主-外键连接性纳入多表生成仍然具有挑战性。

Method: 提出了PluRel框架，该框架分步骤地(1)使用有向图对模式建模，(2)利用二分图表示表间的主-外键连接性，(3)通过条件因果机制模拟表中的特征分布。

Result: (1)观察到RFM预训练损失与合成数据库数量及总预训练令牌数之间存在幂律缩放关系；(2)增加合成数据库的数量能够改善模型对实际数据库的泛化性能；(3)基于合成数据的预训练可产生适用于后续在真实数据库上进一步预训练的强大基础模型。

Conclusion: 本研究提出的框架及其结果表明，采用合成数据扩展是一种很有前景的关系基础模型开发方法论。

Abstract: Relational Foundation Models (RFMs) facilitate data-driven decision-making by learning from complex multi-table databases. However, the diverse relational databases needed to train such models are rarely public due to privacy constraints. While there are methods to generate synthetic tabular data of arbitrary size, incorporating schema structure and primary--foreign key connectivity for multi-table generation remains challenging. Here we introduce PluRel, a framework to synthesize multi-tabular relational databases from scratch. In a step-by-step fashion, PluRel models (1) schemas with directed graphs, (2) inter-table primary-foreign key connectivity with bipartite graphs, and, (3) feature distributions in tables via conditional causal mechanisms. The design space across these stages supports the synthesis of a wide range of diverse databases, while being computationally lightweight. Using PluRel, we observe for the first time that (1) RFM pretraining loss exhibits power-law scaling with the number of synthetic databases and total pretraining tokens, (2) scaling the number of synthetic databases improves generalization to real databases, and (3) synthetic pretraining yields strong base models for continued pretraining on real databases. Overall, our framework and results position synthetic data scaling as a promising paradigm for RFMs.

</details>


### [3] [Piece of CAKE: Adaptive Execution Engines via Microsecond-Scale Learning](https://arxiv.org/abs/2602.04181)
*Zijie Zhao,Ryan Marcus*

Main category: cs.DB

TL;DR: 本文提出了一种名为CAKE的系统，该系统使用微秒级的情境多臂老虎机来学习为每个数据片段选择最优内核，从而在数据库操作中实现更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据库系统通常依赖于静态启发式或最坏情况下的优化默认值来选择这些内核，这往往错失了显著提高性能的机会。

Method: 提出了CAKE（反事实自适应内核执行）系统，通过利用反事实的低成本——选择性地运行多个内核以获得完整反馈，并将策略编译成低延迟后悔树，绕过了传统强化学习的高延迟问题。

Result: 实验表明，与最先进的静态启发式方法相比，CAKE可以将端到端的工作负载延迟降低至多2倍。

Conclusion: CAKE提供了一种新颖的方法来动态选择数据库操作的最佳物理实现，展示了在实际应用中减少工作负载延迟的巨大潜力。

Abstract: Low-level database operators often admit multiple physical implementations ("kernels") that are semantically equivalent but have vastly different performance characteristics depending on the input data distribution. Existing database systems typically rely on static heuristics or worst-case optimal defaults to select these kernels, often missing significant performance opportunities. In this work, we propose CAKE (Counterfactual Adaptive Kernel Execution), a system that learns to select the optimal kernel for each data "morsel" using a microsecond-scale contextual multi-armed bandit. CAKE circumvents the high latency of traditional reinforcement learning by exploiting the cheapness of counterfactuals -- selectively running multiple kernels to obtain full feedback -- and compiling policies into low-latency regret trees. Experimentally, we show that CAKE can reduce end-to-end workload latency by up to 2x compared to state-of-the-art static heuristics.

</details>


### [4] [LatentTune: Efficient Tuning of High Dimensional Database Parameters via Latent Representation Learning](https://arxiv.org/abs/2602.04190)
*Sein Kwon,Youngwan Jo,Seungyeon Choi,Jieun Lee,Huijun Jin,Sanghyun Park*

Main category: cs.DB

TL;DR: 提出了一种新的数据库参数调优方法LatentTune，通过数据增强策略、构建潜在空间以及整合外部度量信息来解决现有机器学习调优方法中的局限性。实验表明，该方法在MySQL和RocksDB上四个工作负载中优于基线模型，最高可达到1332%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于机器学习的数据库参数调优方法存在需要大量时间生成训练数据集、仅能优化部分参数而非整个配置空间、依赖相似工作负载的信息而不是直接利用目标工作负载信息等问题。

Method: 提出了LatentTune，它采用数据增强策略减少数据生成所需时间；构造一个潜在空间以压缩所有数据库参数的信息，从而能够优化整个配置空间；并且将外部度量信息集成到潜在空间中，以便根据实际目标工作负载进行精确调整。

Result: 实验结果表明，对于MySQL和RocksDB上的四个不同工作负载，LatentTune相较于基线模型表现出色，特别是对于RocksDB达到了高达1332%的性能改进，在MySQL上则实现了吞吐量增加11.82%，同时延迟减少了46.01%。

Conclusion: 通过引入创新的数据处理与优化技术，LatentTune成功克服了传统数据库参数调优方法中存在的几个主要问题，并且在多个基准测试中证明了其有效性和优越性。

Abstract: As data volumes continue to grow, optimizing database performance has become increasingly critical, making the implementation of effective tuning methods essential. Among various approaches, database parameter tuning has proven to be a highly effective means of enhancing performance. Recent studies have shown that machine learning techniques can successfully optimize database parameters, leading to significant performance improvements. However, existing methods still face several limitations. First, they require substantial time to generate large training datasets. Second, to cope with the challenges of highdimensional optimization, they typically optimize only a subset of parameters rather than the full configuration space. Third, they often rely on information from similar workloads instead of directly leveraging information from the target workload. To address these limitations, we propose LatentTune, a novel approach that differs fundamentally from traditional methods. To reduce the time required for data generation, LatentTune incorporates a data augmentation strategy. Furthermore, it constructs a latent space that compresses information from all database parameters, enabling the optimization of the full configuration space. In addition, LatentTune integrates external metric information into the latent space, allowing for precise tuning tailored to the actual target workload. Experimental results demonstrate that LatentTune outperforms baseline models across four workloads on MySQL and RocksDB, achieving up to 1332% improvement for RocksDB and 11.82% throughput gain with 46.01% latency reduction for MySQL.

</details>


### [5] [Identifying knowledge gaps in biodiversity data and their determinants at the regional level](https://arxiv.org/abs/2602.04314)
*Didier Alard,Anaïs Guéry*

Main category: cs.DB

TL;DR: 本研究分析了法国最大行政区域内的生物多样性数据库中的知识缺口，发现无脊椎动物的知识缺口比脊椎动物大。总体上，这些缺口更多地受到地点可达性变量的影响而非生态吸引力。农业压力对无脊椎动物的影响比脊椎动物更大。研究强调了通过地方资金和区域政治决策进行的生物多样性治理对于开放获取数据库中知识分布的影响，并建议重新定向生物多样性资金以减少偏差。


<details>
  <summary>Details</summary>
Motivation: 了解生物多样性开放访问数据库中空间和分类学知识缺口的决定因素，有助于指导此类数据的使用。研究旨在识别法国一个大区内的生物多样性数据库内知识缺口的成因。

Method: 采用完整性与无知分数两个指标评估8个分类群（5个脊椎动物类群和3个无脊椎动物类群）的知识缺口；分析整个区域以及三个前子区域层面的数据，以确定可能影响知识缺口的因素。

Result: 无脊椎动物显示出比脊椎动物更大的知识缺口。总体而言，这些缺口更多地由站点可达性相关变量决定，而非其生态吸引力。除了农业压力对无脊椎动物的影响显著大于脊椎动物外，所有组别共享相似的知识缺口决定因素。

Conclusion: 生物多样性治理通过地方资助和地区政治决策影响开放获取数据库中知识的分布。为减少偏差，建议将生物多样性资金重新导向至样本不足的分类群及未充分调查地区。当无法实现时，用户应利用知识缺口图纠正空间采样偏差，从而更准确理解物种分布情况。

Abstract: Biodiversity open-access databases are valuable resources in the structuring and accessibility of species occurrence data. By compiling different data sources, they reveal the uneven spatial distribution of knowledge, with areas or taxonomic groups better prospected than others. Understanding the determinants of spatial and taxonomic knowledge gaps helps in informing the use of open-access data. Here, we identified knowledge gaps' determinants within a French regional biodiversity database, in the largest administrative region in France. Knowledge gaps were assessed using two metrics, completeness and ignorance scores, for 8 taxonomic groups covering five vertebrates and three invertebrates groups. The data was analyzed for the entire region, but also at the level of the three former sub-regions, to identify the potential drivers that may account for knowledge gaps' determinants. Our findings show that invertebrates were characterized by higher knowledge gaps than vertebrates. Overall, knowledge gaps are influenced by variables related to sites' accessibility rather than ecological appeal across both metrics. All groups shared similar determinants of gaps, except for the impact of agricultural pressure which is found to be more significant for invertebrates than vertebrates. Ultimately, our study emphasizes the impact of biodiversity governance, through local funding and regional political decisions, on knowledge distribution in open-access databases. We recommend limiting these biases by redirecting biodiversity funding towards under-sampled taxonomic groups and under-prospected areas. When not possible, users of data extracted from these databases should correct for spatial-sampling biases (SSP) using knowledge gaps' maps in order to get a more accurate understanding of species occurrence.

</details>


### [6] [The Stretto Execution Engine for LLM-Augmented Data Systems](https://arxiv.org/abs/2602.04430)
*Gabriele Sanmartino,Matthias Urban,Paolo Papotti,Carsten Binnig*

Main category: cs.DB

TL;DR: 本文提出了Stretto，一种新的执行引擎，通过将查询规划视为约束优化问题并采用基于梯度的优化器来选择操作实现和分配错误预算，以在运行时-准确性权衡中高效地提供端到端查询保证。此外，Stretto还引入了一种新概念，即如何使用KV缓存来实现一系列不同的物理操作符，从而将稀疏的设计空间转换为密集的连续运行时-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM增强的数据系统虽然能够对结构化和非结构化数据进行语义查询，但在使用LLM驱动的操作符执行查询时会遇到基本的运行时间与准确性之间的权衡问题。因此，需要一个能够在保持查询质量的同时有效处理这种权衡的新解决方案。

Method: Stretto将查询规划定义为一个约束优化问题，并运用基于梯度的优化算法来共同决定操作符的具体实现方式及管道间的误差预算分配。同时，它提出利用KV缓存技术来创建多种不同类型的物理操作符，进而形成一个从稀疏设计向密集连续体转变的运行时-准确性折衷方案。

Result: 实验结果显示，Stretto不仅优于当前最先进的系统，而且始终能满足质量保证要求。

Conclusion: Stretto作为一款新型执行引擎，在处理LLM增强型数据系统中的运行时-准确性权衡问题上表现优异，能有效提供端到端查询保证。

Abstract: LLM-augmented data systems enable semantic querying over structured and unstructured data, but executing queries with LLM-powered operators introduces a fundamental runtime--accuracy trade-off. In this paper, we present Stretto, a new execution engine that provides end-to-end query guarantees while efficiently navigating this trade-off in a holistic manner. For this, Stretto formulates query planning as a constrained optimization problem and uses a gradient-based optimizer to jointly select operator implementations and allocate error budgets across pipelines. Moreover, to enable fine-grained execution choices, Stretto introduces a novel idea on how KV-caching can be used to realize a spectrum of different physical operators that transform a sparse design space into a dense continuum of runtime--accuracy trade-offs. Experiments show that Stretto outperforms state-of-the-art systems while consistently meeting quality guarantees.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [7] [Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval](https://arxiv.org/abs/2602.03992)
*Gabriel de Souza P. Moreira,Ronay Ak,Mengyao Xu,Oliver Holworthy,Benedikt Schifferer,Zhiding Yu,Yauhen Babakhin,Radek Osmulski,Jiarui Cai,Ryan Chesler,Bo Liu,Even Oldridge*

Main category: cs.IR

TL;DR: 本文介绍了Nemotron ColEmbed V2，一种基于预训练VLM的模型系列，旨在提高视觉文档检索的表现。这些模型在ViDoRe基准测试中达到了最先进的性能，特别是8B参数模型在ViDoRe V3排行榜上位列第一。文章还探讨了数据处理、训练及后训练阶段采用的关键技术以及如何平衡准确性和存储效率的方法。


<details>
  <summary>Details</summary>
Motivation: 随着对视觉文档检索需求的增长，开发能够有效利用视觉信息并简化索引流程的新模型变得尤为重要。

Method: 通过使用基于预训练视觉-语言模型（VLMs）的Nemotron ColEmbed V2系列模型，并结合集群采样、硬负样本挖掘、双向注意力机制、后期交互和模型融合等技术来优化模型性能。

Result: Nemotron ColEmbed V2系列中的8B参数模型在ViDoRe V3排行榜上取得了第一名的成绩，平均NDCG@10得分为63.42。此外，研究还展示了如何通过降低嵌入维度来平衡准确性与存储需求。

Conclusion: Nemotron ColEmbed V2系列模型为视觉文档检索领域提供了新的解决方案，不仅提高了检索性能，也探索了解决计算与存储工程挑战的有效方法。

Abstract: Retrieval-Augmented Generation (RAG) systems have been popular for generative applications, powering language models by injecting external knowledge. Companies have been trying to leverage their large catalog of documents (e.g. PDFs, presentation slides) in such RAG pipelines, whose first step is the retrieval component. Dense retrieval has been a popular approach, where embedding models are used to generate a dense representation of the user query that is closer to relevant content embeddings. More recently, VLM-based embedding models have become popular for visual document retrieval, as they preserve visual information and simplify the indexing pipeline compared to OCR text extraction.
  Motivated by the growing demand for visual document retrieval, we introduce Nemotron ColEmbed V2, a family of models that achieve state-of-the-art performance on the ViDoRe benchmarks. We release three variants - with 3B, 4B, and 8B parameters - based on pre-trained VLMs: NVIDIA Eagle 2 with Llama 3.2 3B backbone, Qwen3-VL-4B-Instruct and Qwen3-VL-8B-Instruct, respectively. The 8B model ranks first on the ViDoRe V3 leaderboard as of February 03, 2026, achieving an average NDCG@10 of 63.42.
  We describe the main techniques used across data processing, training, and post-training - such as cluster-based sampling, hard-negative mining, bidirectional attention, late interaction, and model merging - that helped us build our top-performing models. We also discuss compute and storage engineering challenges posed by the late interaction mechanism and present experiments on how to balance accuracy and storage with lower dimension embeddings.

</details>


### [8] [Following the TRAIL: Predicting and Explaining Tomorrow's Hits with a Fine-Tuned LLM](https://arxiv.org/abs/2602.04225)
*Yinan Zhang,Zhixi Chen,Jiazheng Jing,Zhiqi Shen*

Main category: cs.IR

TL;DR: 本文提出了一种名为TRAIL的微调大型语言模型，该模型可以同时预测短期项目流行度并生成自然语言解释。通过对比学习，TRAIL能够提供准确且可解释的流行度预测，并在实验中优于强大的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的推荐系统难以从大规模稀疏的用户-项目日志中提取用户偏好，且实时对每个用户的全目录进行排名耗时过长。此外，许多推荐系统只关注排名而忽略了提供解释，这可能有助于提高预测准确性并使推荐更具说服力。受近期工作启发，作者提出了一个能同时预测短期项目流行度和生成自然语言解释的新模型。

Method: 提出的方法称为TRAIL（趋势与解释集成学习者），它是一个经过微调的大规模语言模型，旨在联合预测短期项目流行度并生成忠实的自然语言解释。该方法采用对比学习技术，利用正负样本对齐其评分和解释与结构化趋势信号，从而产生准确且具有解释性的流行度预测。

Result: 广泛的实验证明了TRAIL不仅在性能上超越了强大的基线模型，而且还能生成连贯、有根据的解释。

Conclusion: TRAIL作为一个结合了短期项目流行度预测与自然语言解释生成能力的微调LLM，在推荐系统领域展现出巨大潜力，能够提供更准确且易于理解的推荐结果。

Abstract: Large Language Models (LLMs) have been widely applied across multiple domains for their broad knowledge and strong reasoning capabilities. However, applying them to recommendation systems is challenging since it is hard for LLMs to extract user preferences from large, sparse user-item logs, and real-time per-user ranking over the full catalog is too time-consuming to be practical. Moreover, many existing recommender systems focus solely on ranking items while overlooking explanations, which could help improve predictive accuracy and make recommendations more convincing to users. Inspired by recent works that achieve strong recommendation performance by forecasting near-term item popularity, we propose TRAIL (TRend and explAnation Integrated Learner). TRAIL is a fine-tuned LLM that jointly predicts short-term item popularity and generates faithful natural-language explanations. It employs contrastive learning with positive and negative pairs to align its scores and explanations with structured trend signals, yielding accurate and explainable popularity predictions. Extensive experiments show that TRAIL outperforms strong baselines and produces coherent, well-grounded explanations.

</details>


### [9] [LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval](https://arxiv.org/abs/2602.04263)
*Joohyung Yun,Doyup Lee,Wook-Shin Han*

Main category: cs.IR

TL;DR: 提出了一种名为LILaC的多模态检索框架，通过分层组件图和基于晚交互的子图检索方法来解决无关内容的影响和支持跨文档组件之间的多跳推理问题，在五个基准测试中达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态文档检索中的两个主要挑战：（1）减少由于固定、单一粒度检索单元引起的不相关内容的影响；（2）通过有效捕捉文档内及跨文档组件间的语义关系来支持多跳推理。

Method: 提出了LILaC，一个包含两大核心创新点的多模态检索框架。首先，引入了分层组件图，该图在粗细两层上明确表示多模态信息，以促进高效且准确的推理。其次，开发了一种基于晚交互的子图检索方法，该方法首先识别粗粒度节点以生成候选集，然后通过晚交互进行细粒度推理。

Result: 广泛的实验表明，LILaC在所有五个基准上都实现了最先进的检索性能，值得注意的是无需额外微调即可达到这一效果。

Conclusion: LILaC提供了一种新颖而有效的解决方案来应对多模态文档检索中的关键挑战，并且在多个标准评测中证明了其卓越性能。

Abstract: Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. An effective multimodal retriever needs to handle two main challenges: (1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and (2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. To address these challenges, we propose LILaC, a multimodal retrieval framework featuring two core innovations. First, we introduce a layered component graph, explicitly representing multimodal information at two layers - each representing coarse and fine granularity - facilitating efficient yet precise reasoning. Second, we develop a late-interaction-based subgraph retrieval method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction. Extensive experiments demonstrate that LILaC achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning. We make the artifacts publicly available at github.com/joohyung00/lilac.

</details>


### [10] [MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation](https://arxiv.org/abs/2602.04278)
*Lin Wang,Yang Zhang,Jingfan Chen,Xiaoyan Zhao,Fengbin Zhu,Qing Li,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 提出了一种名为MiniRec的数据选择框架，专为基于强化学习的大型语言模型推荐系统设计。该框架通过奖励信号评估样本可学习性，并通过与理想全局RL优化轨迹对齐来衡量样本代表性，同时确保多样性以减少冗余。结合从易到难的课程学习策略，MiniRec显著降低了训练成本，同时保持了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习（RL）的大规模语言模型（LLM）推荐系统面临效率挑战，全数据训练成本高昂。现有数据选择方法往往未能准确反映RL的学习动态，导致性能不佳。

Method: 提出了MiniRec，一种针对基于RL的LLM推荐系统的数据选择框架。该框架利用关键RL信号——奖励——来剔除过于简单或困难的样本，并通过与近似“理想”全局RL优化轨迹对齐来挑选具有代表性的样本。此外，还实施了多样化策略以降低冗余度。整个过程配合由简至难的课程学习策略进行。

Result: 广泛的实验表明，MiniRec能够有效减少训练成本的同时，大幅保留推荐系统的性能，强调了在基于RL的LLM推荐中进行奖励对齐和轨迹信息数据选择的重要性。

Conclusion: MiniRec提供了一个高效的数据选择方案，专门针对基于RL的LLM推荐系统设计，它不仅有助于降低成本，还能维持较高的推荐质量。

Abstract: The integration of reinforcement learning (RL) into large language models (LLMs) has opened new opportunities for recommender systems by eliciting reasoning and improving user preference modeling. However, RL-based LLM recommendation faces significant efficiency challenges, making full-data training costly. Existing data selection methods define sample value based on learnability or representativeness, yet their loss- or gradient-driven or dataset coverage-driven criteria often misalign with RL learning dynamics, resulting in suboptimal performance. To address this, we propose MiniRec, a data selection framework tailored for RL-based LLM recommendation. MiniRec evaluates sample learnability using key RL signals -- rewards -- pruning samples that are too easy (too high reward) or too difficult (consistently low reward). It assesses representativeness by aligning sample gradients with the approximated "ideal" global RL optimization trajectory, selecting samples that mainly drive model updates, and it also enforces diversity to reduce redundancy. Combined with a curriculum learning strategy from easy to hard samples, MiniRec significantly reduces training cost while largely preserving performance. Extensive experiments demonstrate MiniRec's effectiveness, highlighting the importance of reward-aligned, trajectory-informed data selection in RL-based LLM recommendation.

</details>


### [11] [SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.04451)
*Yi Sun,Jinyu Xu,Qing Xie,Jiachen Li,Yanchun Ma,Yongjian Liu*

Main category: cs.IR

TL;DR: 提出了SDR-CIR，一种基于链式思维推理的无训练语义去偏排序方法，用于解决组合图像检索（CIR）中的语义偏差问题。通过选择性链式思维减少视觉噪声，并采用锚定和去偏两步来缓解语义偏差，从而提高检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无训练零样本方法在执行组合图像检索时，由于模糊匹配的本质，容易产生相对于目标图像的语义偏差。

Method: 提出了一种名为SDR-CIR的方法，包括选择性链式思维指导多模态大语言模型提取与修改文本相关的视觉内容以减少原始视觉噪声；引入了包含锚定和去偏两个步骤的语义去偏排序方法，其中锚定步骤融合参考图像特征与目标描述特征强化有用语义并补充遗漏线索，而去偏步骤则明确建模参考图像对描述的视觉语义贡献，并将其作为惩罚项纳入相似度评分中。

Result: 实验结果表明，SDR-CIR在三个标准CIR基准测试中达到了单阶段方法中的最先进结果，同时保持了高效率。

Conclusion: SDR-CIR通过补充被忽略的线索同时抑制冗余信息，有效地减轻了语义偏差问题并提升了组合图像检索的表现。

Abstract: Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text. Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval. However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image. We propose SDR-CIR, a training-free Semantic Debias Ranking method based on CoT reasoning. First, Selective CoT guides the MLLM to extract visual content relevant to the modification text during image understanding, thereby reducing visual noise at the source. We then introduce a Semantic Debias Ranking with two steps, Anchor and Debias, to mitigate semantic bias. In the Anchor step, we fuse reference image features with target description features to reinforce useful semantics and supplement omitted cues. In the Debias step, we explicitly model the visual semantic contribution of the reference image to the description and incorporate it into the similarity score as a penalty term. By supplementing omitted cues while suppressing redundancy, SDR-CIR mitigates semantic bias and improves retrieval performance. Experiments on three standard CIR benchmarks show that SDR-CIR achieves state-of-the-art results among one-stage methods while maintaining high efficiency. The code is publicly available at https://github.com/suny105/SDR-CIR.

</details>


### [12] [DOS: Dual-Flow Orthogonal Semantic IDs for Recommendation in Meituan](https://arxiv.org/abs/2602.04460)
*Junwei Yin,Senjie Kou,Changhao Li,Shuli Wang,Xue Wei,Yinqiu Huang,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.IR

TL;DR: 提出了Dual-Flow Orthogonal Semantic IDs (DOS)方法来解决现有生成推荐系统中语义ID存在的问题，通过用户-项目双流框架和正交残差量化方案优化了语义空间的对齐与保存。该方法在美团应用中得到成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有的基于语义ID的生成推荐系统存在两大局限：一是缺乏上下文感知导致语义ID码本空间与生成空间之间出现差距，影响推荐质量；二是次优的量化方法加剧了大语言模型中的语义损失。

Method: 提出了一种名为Dual-Flow Orthogonal Semantic IDs (DOS)的新方法，该方法采用用户-项目双流架构利用协作信号调整语义ID码本空间与生成空间的一致性，并引入一种正交残差量化机制旋转语义空间以最大化保留语义信息。

Result: 广泛的离线实验和在线A/B测试表明了DOS的有效性。

Conclusion: DOS方法能够有效改善基于语义ID的推荐系统的性能，已被成功应用于美团移动应用，服务数亿用户。

Abstract: Semantic IDs serve as a key component in generative recommendation systems. They not only incorporate open-world knowledge from large language models (LLMs) but also compress the semantic space to reduce generation difficulty. However, existing methods suffer from two major limitations: (1) the lack of contextual awareness in generation tasks leads to a gap between the Semantic ID codebook space and the generation space, resulting in suboptimal recommendations; and (2) suboptimal quantization methods exacerbate semantic loss in LLMs. To address these issues, we propose Dual-Flow Orthogonal Semantic IDs (DOS) method. Specifically, DOS employs a user-item dual flow-framework that leverages collaborative signals to align the Semantic ID codebook space with the generation space. Furthermore, we introduce an orthogonal residual quantization scheme that rotates the semantic space to an appropriate orientation, thereby maximizing semantic preservation. Extensive offline experiments and online A/B testing demonstrate the effectiveness of DOS. The proposed method has been successfully deployed in Meituan's mobile application, serving hundreds of millions of users.

</details>


### [13] [VK-LSVD: A Large-Scale Industrial Dataset for Short-Video Recommendation](https://arxiv.org/abs/2602.04567)
*Aleksandr Poslavsky,Alexander D'yakonov,Yuriy Dorn,Andrey Zimovnov*

Main category: cs.IR

TL;DR: 本文介绍了一个名为VK-LSVD的大型短视频数据集，它包含了超过400亿次互动、1000万用户和近2000万个视频的信息。这个公开可用的数据集为研究序列推荐、冷启动场景以及下一代推荐系统提供了重要的基准资源。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏反映真实世界平台动态的大规模开放数据集，短视频推荐领域面临诸如从隐式反馈中建模快速变化的用户兴趣等独特挑战。为了填补这一空白，作者们引入了VK-LSVD数据集。

Method: 通过收集来自VK平台上的大量用户行为数据（包括但不限于内容嵌入、多样化的反馈信号及上下文元数据）构建了VK-LSVD数据集，并对其质量与多样性进行了分析。

Result: VK-LSVD成为迄今为止最大规模的公共工业级短视频数据集之一，涵盖了超过400亿次互动记录，涉及约1000万用户和接近2000万个视频。

Conclusion: VK-LSVD作为一个关键资源，在加速研究如序列推荐、冷启动问题以及发展新一代推荐系统方面具有重要意义，并且已经在VK RecSys Challenge 2025中发挥了核心作用。

Abstract: Short-video recommendation presents unique challenges, such as modeling rapid user interest shifts from implicit feedback, but progress is constrained by a lack of large-scale open datasets that reflect real-world platform dynamics. To bridge this gap, we introduce the VK Large Short-Video Dataset (VK-LSVD), the largest publicly available industrial dataset of its kind. VK-LSVD offers an unprecedented scale of over 40 billion interactions from 10 million users and almost 20 million videos over six months, alongside rich features including content embeddings, diverse feedback signals, and contextual metadata. Our analysis supports the dataset's quality and diversity. The dataset's immediate impact is confirmed by its central role in the live VK RecSys Challenge 2025. VK-LSVD provides a vital, open dataset to use in building realistic benchmarks to accelerate research in sequential recommendation, cold-start scenarios, and next-generation recommender systems.

</details>


### [14] [AIANO: Enhancing Information Retrieval with AI-Augmented Annotation](https://arxiv.org/abs/2602.04579)
*Sameh Khattab,Marie Bauer,Lukas Heine,Till Rostalski,Jens Kleesiek,Julian Friedrich*

Main category: cs.IR

TL;DR: Researchers developed AIANO, an AI-augmented annotation tool for creating information retrieval datasets, which nearly doubled the speed of dataset creation and improved ease of use and retrieval accuracy in a user study.


<details>
  <summary>Details</summary>
Motivation: The need for high-quality, curated information retrieval datasets has increased with the rise of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). However, current off-the-shelf annotation tools make the process complex and inefficient. The motivation is to streamline and improve the efficiency of the annotation process for such datasets.

Method: A specialized annotation tool named AIANO was developed, integrating human expertise with LLM assistance to simplify the annotation process. A within-subject user study was conducted with 15 participants, who used both a baseline tool and AIANO to create question-answering datasets, in order to compare the effectiveness and efficiency of each tool.

Result: AIANO significantly outperformed the baseline tool by nearly doubling the speed of annotation, while also being easier to use and improving the accuracy of information retrieval. The results highlight the benefits of using an AI-augmented approach for dataset creation in the field of information retrieval.

Conclusion: The AI-augmented annotation tool, AIANO, effectively accelerates and enhances the creation of information retrieval datasets, offering a more efficient and accurate solution compared to traditional methods, thus advancing the capabilities of annotation in domains that require extensive retrieval.

Abstract: The rise of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) has rapidly increased the need for high-quality, curated information retrieval datasets. These datasets, however, are currently created with off-the-shelf annotation tools that make the annotation process complex and inefficient. To streamline this process, we developed a specialized annotation tool - AIANO. By adopting an AI-augmented annotation workflow that tightly integrates human expertise with LLM assistance, AIANO enables annotators to leverage AI suggestions while retaining full control over annotation decisions. In a within-subject user study ($n = 15$), participants created question-answering datasets using both a baseline tool and AIANO. AIANO nearly doubled annotation speed compared to the baseline while being easier to use and improving retrieval accuracy. These results demonstrate that AIANO's AI-augmented approach accelerates and enhances dataset creation for information retrieval tasks, advancing annotation capabilities in retrieval-intensive domains.

</details>


### [15] [Multi-Source Retrieval and Reasoning for Legal Sentencing Prediction](https://arxiv.org/abs/2602.04690)
*Junjie Chen,Haitao Li,Qilei Zhang,Zhenghua Li,Ya Zhang,Quan Zhou,Cheng Luo,Yiqun Liu,Dongsheng Guo,Qingyao Ai*

Main category: cs.IR

TL;DR: 本文提出了一种结合多源检索和推理以及强化学习的框架$MSR^2$，用于改进法律判决预测中的量刑预测问题。实验表明，该方法提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 法律判决预测（LJP）在法律条文与指控预测上表现良好，但在量刑预测（LSP）仍面临挑战，因为需要细致的客观知识和灵活的主观推理。

Method: $MSR^2$框架通过整合基于大型语言模型（LLMs）的多源检索与推理，并利用强化学习指导中间主观推理步骤，旨在克服现有方法在量刑预测上的局限性。

Result: 通过两个真实世界数据集的实验验证了$MSR^2$能够同时提升量刑预测任务的准确率和可解释性。

Conclusion: 所提出的$MSR^2$框架展示了其在改善法律判决预测中量刑预测方面的潜力，为实际应用中的法律AI技术提供了新的方向。

Abstract: Legal judgment prediction (LJP) aims to predict judicial outcomes from case facts and typically includes law article, charge, and sentencing prediction. While recent methods perform well on the first two subtasks, legal sentencing prediction (LSP) remains difficult due to its need for fine-grained objective knowledge and flexible subjective reasoning. To address these limitations, we propose $MSR^2$, a framework that integrates multi-source retrieval and reasoning in LLMs with reinforcement learning. $MSR^2$ enables LLMs to perform multi-source retrieval based on reasoning needs and applies a process-level reward to guide intermediate subjective reasoning steps. Experiments on two real-world datasets show that $MSR^2$ improves both accuracy and interpretability in LSP, providing a promising step toward practical legal AI. Our code is available at https://anonymous.4open.science/r/MSR2-FC3B.

</details>


### [16] [Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention](https://arxiv.org/abs/2602.04711)
*Sagie Dekel,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: 本文提出了一种新的防御方法——稀疏文档注意力RAG（SDAG），以防止检索增强生成（RAG）模型受到语料库知识中毒攻击。SDAG通过阻止检索到的文档之间的交叉注意力，显著降低了攻击成功率，并且不需要微调或额外的架构更改。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）虽然有效但容易受到语料库知识中毒攻击，即攻击者通过向语料库注入误导性文档来操控LLM输出。标准因果注意力机制允许有害的跨文档交互作用，特别是在遭受攻击的情况下。

Method: 引入了名为Sparse Document Attention RAG (SDAG)的新防御方法，该方法基于块稀疏注意力机制，不允许检索文档间的交叉注意力。SDAG仅需在推理时对注意力掩码进行最小程度的修改，无需进一步微调或架构上的调整。

Result: 实证评估表明，在面对多种攻击策略时，SDAG方法在攻击成功率方面明显优于传统的因果注意力机制。此外，将SDAG与最先进的RAG防御方法结合使用时，其表现显著优于现有技术水平。

Conclusion: SDAG提供了一种有效对抗语料库知识中毒攻击的方法，为提高RAG系统安全性开辟了新途径，同时保持了实现简单和高效的特点。

Abstract: Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to steer an LLMs' output to an undesired response. We argue that the standard causal attention mechanism in LLMs enables harmful cross-document interactions, specifically in cases of attacks. Accordingly, we introduce a novel defense approach for RAG: Sparse Document Attention RAG (SDAG). This is a block-sparse attention mechanism that disallows cross-attention between retrieved documents. SDAG requires a minimal inference-time change to the attention mask; furthermore, no fine-tuning or additional architectural changes are needed. We present an empirical evaluation of LLM-based question answering (QA) with a variety of attack strategies on RAG. We show that our SDAG method substantially outperforms the standard causal attention mechanism in terms of attack success rate. We further demonstrate the clear merits of integrating SDAG with state-of-the-art RAG defense methods. Specifically, the integration results in performance that is statistically significantly better than the state-of-the-art.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems](https://arxiv.org/abs/2602.04120)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 本文提出了一种名为XaaS（Explainability-as-a-Service）的分布式架构，将可解释性作为系统服务处理而非模型特定功能。通过解耦推理与解释生成、引入分布式缓存机制及轻量级验证协议等创新，实现了在边缘设备上高效地请求、缓存和验证AI模型的解释，同时保持高解释质量并减少延迟38%。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释人工智能(XAI)取得了显著进展，但在边缘和物联网系统中的集成通常是临时且低效的。现有方法通常在模型推断的同时生成解释，导致冗余计算、高延迟以及跨异构边缘设备部署时的扩展性差。

Method: 提出了XaaS架构，主要包含三个创新点：1. 分布式解释缓存结合基于语义相似性的检索方法来大幅减少冗余计算；2. 轻量级验证协议确保缓存或新生成的解释的真实度；3. 自适应解释引擎根据设备能力和用户需求选择合适的解释方法。

Result: XaaS在制造质量控制、自动驾驶感知及医疗诊断这三个实际边缘AI应用场景中进行了性能评估，结果显示该架构能够降低38%的延迟，同时维持高质量的解释输出。

Conclusion: 这项工作通过实现透明和负责任的人工智能在大规模异构物联网系统中的部署，缩小了XAI研究与边缘实用性之间的差距。

Abstract: Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are "coupled" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.

</details>


### [18] [Training Data Efficiency in Multimodal Process Reward Models](https://arxiv.org/abs/2602.04145)
*Jinyuan Li,Chengsong Huang,Langlin Huang,Shaoyang Xu,Haolin Liu,Wenxuan Zhang,Jiaxin Huang*

Main category: cs.LG

TL;DR: 本文研究了MPRM训练的数据效率问题，提出了一种新的方法BIS来选择训练数据子集，以提高训练效率。实验结果表明，使用BIS方法选取的少量数据即可达到甚至超过全量数据训练的效果。


<details>
  <summary>Details</summary>
Motivation: 多模态过程奖励模型（MPRMs）对于MLLMs中的视觉推理至关重要，但其训练通常需要大规模蒙特卡洛注释语料库，导致训练成本高昂。本研究旨在通过提升数据利用效率来降低这种成本。

Method: 作者们首先通过初步实验发现随机子采样训练数据时MPRM训练迅速饱和，暗示现有MC注释语料库中存在大量冗余信息。基于此观察，他们提出了一个理论框架，并揭示了信息梯度更新取决于正面/负面步骤的标签混合程度及标签可靠性两个因素。受这些见解启发，提出了平衡信息分数（BIS），该方法在不增加额外开销的前提下，根据现有MC信号优先考虑混合程度和可靠性。

Result: 在VisualProcessBench上，使用InternVL2.5-8B和Qwen2.5-VL-7B两种基础模型进行测试的结果显示，BIS选择的数据子集能够与全数据表现相匹配甚至超越，在仅使用10%训练数据的情况下达到了全数据性能，相比随机子采样提高了相对4.1%。

Conclusion: 研究表明，通过采用BIS方法精心挑选训练样本可以显著提高MPRM训练的数据效率，减少对大规模标注数据的需求，同时保持或改善模型性能。

Abstract: Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.

</details>


### [19] [GOPO: Policy Optimization using Ranked Rewards](https://arxiv.org/abs/2602.03876)
*Kyuseong Choi,Dwaipayan Saha,Woojeong Kim,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 本文提出了一种新的策略优化方法——组序数策略优化（GOPO），该方法仅使用奖励的排名而忽略其绝对值，从而在非可验证奖励场景下提供更好的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有的从人类反馈中进行强化学习的方法依赖于绝对奖励大小进行策略优化，这在奖励不可验证的情况下（如摘要、指令跟随和聊天完成）会导致性能不佳。

Method: 引入了组序数策略优化（GOPO），一种基于奖励排名而非奖励数值的策略优化方法。

Result: 相比于组相对策略优化（GRPO），GOPO在非可验证奖励场景下表现出色，包括更高的训练/验证奖励轨迹、大多数中间训练步骤中的改进LLM-as-judge评估结果以及以显著较少的训练步骤达到相似质量的策略。

Conclusion: 研究证明了GOPO在一系列任务和模型尺寸上的一致性改进。

Abstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes.

</details>


### [20] [GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression](https://arxiv.org/abs/2602.03906)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为GeoIB的新方法，通过信息几何的视角重新审视了信息瓶颈问题。GeoIB直接控制信息压缩而无需估计互信息，并且在流行数据集上实现了比主流IB基线更好的预测准确性和压缩比率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统的信息瓶颈（IB）方法在深度学习中通常通过可处理的替代方法实现，如变分界限或神经互信息（MI）估计器，而不是直接控制互信息I(X;Z)本身。这种方法可能导致IB“压缩”只能间接控制并且优化过程脆弱。因此，作者希望通过新的方式来解决这个问题。

Method: 作者提出了一个名为GeoIB的方法，它基于信息几何学并避免了互信息估计。GeoIB通过两个互补项来控制信息压缩：(i) 在分布层面使用Fisher-Rao（FR）差异度量，这与KL散度匹配至二阶且对参数化不变；(ii) 从几何层面引入Jacobian-Frobenius（JF）项作为I(Z;X)的一个局部容量类型的上限，通过惩罚编码器的拉回体积扩展来实现。此外，还推导出一种与FR度量一致的自然梯度优化器，并证明标准加法自然梯度步骤与测地线更新在一级等效。

Result: 实验结果表明，在多个常用的数据集上，GeoIB相比主流的IB基线方法能够达到更优的预测准确性和压缩比率之间的平衡。同时，通过统一分布和几何正则化在一个瓶颈乘数下，GeoIB改进了不变性和优化稳定性。

Conclusion: GeoIB提供了一种新颖的信息瓶颈方法，通过结合分布级和几何级的正则化技术，实现了更直接的信息压缩控制及增强的优化稳定性。

Abstract: Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can make IB "compression" only indirectly controlled and optimization fragile.
  We revisit the IB problem through the lens of information geometry and propose a \textbf{Geo}metric \textbf{I}nformation \textbf{B}ottleneck (\textbf{GeoIB}) that dispenses with mutual information (MI) estimation. We show that I(X;Z) and I(Z;Y) admit exact projection forms as minimal Kullback-Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, GeoIB controls information compression with two complementary terms: (i) a distribution-level Fisher-Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian-Frobenius (JF) term that provides a local capacity-type upper bound on I(Z;X) by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the geodesic update. We conducted extensive experiments and observed that the GeoIB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. GeoIB improves invariance and optimization stability by unifying distributional and geometric regularization under a single bottleneck multiplier. The source code of GeoIB is released at "https://anonymous.4open.science/r/G-IB-0569".

</details>


### [21] [Causal Discovery for Cross-Sectional Data Based on Super-Structure and Divide-and-Conquer](https://arxiv.org/abs/2602.03914)
*Wenyu Wang,Yaping Wan*

Main category: cs.LG

TL;DR: 提出了一种新的轻量级框架，通过放宽对超级结构构建的严格要求，同时保持分而治之算法的好处，大大降低了条件独立性测试的成本而不牺牲准确性。实验表明该方法在减少CI测试数量的同时，达到了与PC和FCI相当或接近的结构准确性，并且在中国健康与养老追踪调查（CHARLS）数据集上的进一步验证确认了其实用适用性。


<details>
  <summary>Details</summary>
Motivation: 解决基于超级结构的分而治因果发现中的高计算成本问题，特别是在条件独立性测试昂贵且领域知识不可用的情况下。

Method: 采用宽松约束的超级结构与高效的图分割和合并策略相结合的方法。

Result: 实验证明了该方法在减少CI测试数量的同时，达到了与PC和FCCI相当或接近的结构准确性。此外，在实际应用中也证明了其有效性。

Conclusion: 即使在对初始超级结构做出最小假设的情况下，也能实现准确、可扩展的因果发现，为将分而治之方法应用于大规模、知识稀缺领域开辟了新途径。

Abstract: This paper tackles a critical bottleneck in Super-Structure-based divide-and-conquer causal discovery: the high computational cost of constructing accurate Super-Structures--particularly when conditional independence (CI) tests are expensive and domain knowledge is unavailable. We propose a novel, lightweight framework that relaxes the strict requirements on Super-Structure construction while preserving the algorithmic benefits of divide-and-conquer. By integrating weakly constrained Super-Structures with efficient graph partitioning and merging strategies, our approach substantially lowers CI test overhead without sacrificing accuracy. We instantiate the framework in a concrete causal discovery algorithm and rigorously evaluate its components on synthetic data. Comprehensive experiments on Gaussian Bayesian networks, including magic-NIAB, ECOLI70, and magic-IRRI, demonstrate that our method matches or closely approximates the structural accuracy of PC and FCI while drastically reducing the number of CI tests. Further validation on the real-world China Health and Retirement Longitudinal Study (CHARLS) dataset confirms its practical applicability. Our results establish that accurate, scalable causal discovery is achievable even under minimal assumptions about the initial Super-Structure, opening new avenues for applying divide-and-conquer methods to large-scale, knowledge-scarce domains such as biomedical and social science research.

</details>


### [22] [SpecMD: A Comprehensive Study On Speculative Expert Prefetching](https://arxiv.org/abs/2602.03921)
*Duc Hoang,Ajay Jaiswal,Mohammad Samragh,Minsik Cho*

Main category: cs.LG

TL;DR: 本文提出了一种名为SpecMD的框架，用于在各种硬件配置上对MoE缓存策略进行基准测试。通过实验发现，MoE专家访问不符合时间局部性假设（如LRU, LFU）。基于此观察，作者提出了Least-Stale这一新的淘汰策略，利用MoE可预测的专家访问模式来减少冲突丢失，相比LRU最高可达85倍。这使得在仅有5%或0.6GB VRAM缓存容量的情况下，实现了超过88%的命中率，并且对于OLMoE模型，首次令牌生成时间减少了34.7%。


<details>
  <summary>Details</summary>
Motivation: 尽管Mixture-of-Experts (MoE) 模型允许稀疏的专家激活，但要将这种稀疏性转化为实际性能提升，需要有效的专家缓存机制。之前的工作已经提出了以硬件为中心的缓存策略，但对于这些不同缓存策略之间以及它们与不同硬件规格之间的相互作用了解不足。

Method: 开发了名为SpecMD的标准框架，用于在不同的硬件配置下评估临时缓存策略。使用SpecMD进行了多项MoE缓存策略的全面基准测试，在控制条件下重现并扩展了先前的方法。基于观察到MoE专家访问并不符合时间局部性假设的事实，提出了一种新的淘汰策略——Least-Stale，该策略利用MoE可预见的专家访问模式来降低冲突缺失。

Result: 实验表明，与传统的LRU等策略相比，Least-Stale可以显著减少冲突丢失，最高可达85倍。在仅占用5%或0.6GB VRAM缓存容量时，实现了超过88%的命中率，并且对于OLMoE模型，Time-to-first-token (TTFT) 减少了34.7%。

Conclusion: 研究揭示了现有缓存策略可能不适用于MoE模型的特点，并通过引入Least-Stale策略展示了如何更有效地管理缓存资源以提高性能。

Abstract: Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric caching policies, but how these various caching policies interact with each other and different hardware specification remains poorly understood. To address this gap, we develop \textbf{SpecMD}, a standardized framework for benchmarking ad-hoc cache policies on various hardware configurations. Using SpecMD, we perform an exhaustive benchmarking of several MoE caching strategies, reproducing and extending prior approaches in controlled settings with realistic constraints. Our experiments reveal that MoE expert access is not consistent with temporal locality assumptions (e.g LRU, LFU). Motivated by this observation, we propose \textbf{Least-Stale}, a novel eviction policy that exploits MoE's predictable expert access patterns to reduce collision misses by up to $85\times$ over LRU. With such gains, we achieve over $88\%$ hit rates with up to $34.7\%$ Time-to-first-token (TTFT) reduction on OLMoE at only $5\%$ or $0.6GB$ of VRAM cache capacity.

</details>


### [23] [From Data to Behavior: Predicting Unintended Model Behaviors Before Training](https://arxiv.org/abs/2602.04735)
*Mengru Wang,Zhenqian Xu,Junfeng Fang,Yunzhi Yao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.LG

TL;DR: 研究介绍了一种名为Data2Behavior的新任务以及一种称为Manipulating Data Features (MDF)的方法，用于在训练前预测大型语言模型（LLMs）可能产生的意外行为。MDF通过数据的平均表示来揭示潜在偏见和安全风险，而无需更新任何参数，并且与微调相比，仅需约20%的GPU资源。实验结果表明，该方法能够有效预测并提供关于预训练漏洞的见解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）即使在没有明确提示或恶意内容的情况下，也可能从看似无害的训练数据中获取非预期的偏差。现有的方法难以在微调之前检测到这些风险，导致事后评估既昂贵又低效。为了解决这一挑战，提出了新的解决方案。

Method: 提出了一种名为Data2Behavior的新任务，旨在预测模型在训练之前的非预期行为。同时，开发了Manipulating Data Features (MDF)方法，它通过总结候选数据的平均表示并在基础模型的前向传递过程中注入这些信息，使得数据中的潜在统计信号可以影响模型激活，从而揭示潜在偏见和安全风险，整个过程不需要更新参数。

Result: MDF方法在Qwen3-14B、Qwen2.5-32B-Instruct及Gemma-3-12b-it等模型上的实验结果显示，能够在不消耗过多计算资源的前提下（仅使用约20%的GPU资源），可靠地预测出模型的非预期行为，并对预训练阶段存在的脆弱性提供了有价值的洞察。

Conclusion: 本研究表明，通过采用像MDF这样的轻量级方法，可以在不进行完整训练的情况下有效地预测LLMs可能出现的非预期行为，这对于提高模型的安全性和减少不必要的资源消耗具有重要意义。

Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.

</details>


### [24] [WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling](https://arxiv.org/abs/2602.03924)
*Michael Aich,Andreas Fürst,Florian Sestak,Carlos Ruiz-Gonzalez,Niklas Boers,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 研究人员开发了WIND，这是一种预训练的基础模型，能够处理各种天气和气候问题，无需针对特定任务进行微调。通过自我监督的视频重建目标进行预训练，并将不同领域的问题视为反问题解决，WIND能够执行概率预报、时空降尺度等任务，还能生成全球变暖情景下的极端天气事件物理一致的假设情景。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在天气和气候建模中的应用较为分散，通常需要为不同的任务单独训练高度专业化的模型。研究旨在通过创建一个统一的基础模型来改变这种状况，该模型可以跨多种任务工作，而不需要对每个任务进行专门的微调。

Method: WIND是通过自我监督的视频重建目标预训练而成的，利用无条件视频扩散模型从噪声状态迭代重建大气动力学。在推理阶段，各类领域特定问题被严格地作为反问题处理，并通过后验采样来解决这些问题。

Result: WIND能够在不需任何任务特定微调的情况下替换专业基线，成功应用于概率预报、空间和时间降尺度、稀疏重建及强制守恒定律等问题上。此外，它还展示了生成全球变暖情境下极端天气事件物理一致替代情景的能力。

Conclusion: 结合生成式视频建模与反问题求解方法，WIND为基于AI的大气建模提供了一种计算效率更高的范式转变。

Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.

</details>


### [25] [Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints](https://arxiv.org/abs/2602.03940)
*Olaf Yunus Laitinen Imanov,Duygu Erisken,Derya Umut Kulali,Taner Yilmaz,Rana Irem Turhan*

Main category: cs.LG

TL;DR: 该论文介绍了一个名为AURA的多智能体强化学习系统，用于在严格监管约束下实时选择经济适用房建设地点。AURA能够优化可达性、环境影响、建造成本和社会公平性，并确保选址方案的可行性。与强大的基线方法相比，AURA在遵守94.3%法规的同时，提高了37.2%的帕累托超体积。案例研究表明，AURA大幅缩短了选址时间，并识别出更多可行且更优的地点。


<details>
  <summary>Details</summary>
Motivation: 鉴于经济适用房短缺问题严重，而土地稀缺和法规限制使得选址过程缓慢且复杂，研究者开发了AURA系统来解决这一难题。旨在通过技术手段提高经济适用房选址效率，同时考虑到可达性、环境影响、成本以及社会公平等多方面因素。

Method: AURA基于层次化的多智能体强化学习框架构建，将任务建模为一个受约束的多目标马尔可夫决策过程。采用监管意识的状态编码方式处理127项联邦和地方约束条件；利用帕累托约束策略梯度法保证解决方案的可行性；并通过奖励分解机制区分即时成本与长期社会效益。

Result: 实验结果显示，在8个美国大都会区的数据集上（包含47,392个候选地块），AURA达到了94.3%的法规遵从率，并且相比于强基准提升了37.2%的帕累托超体积。纽约市2026年案例研究中，AURA将选址时间从18个月减少至72小时，并找到了比专家选择多23%的可行站点；这些站点拥有更好的公共交通可达性和更低的环境影响。

Conclusion: AURA系统展示了利用先进的多智能体强化学习技术有效应对经济适用房选址挑战的可能性。它不仅极大地提高了选址过程的速度与准确性，而且在多个关键指标上优于传统方法。

Abstract: Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.

</details>


### [26] [Grables: Tabular Learning Beyond Independent Rows](https://arxiv.org/abs/2602.03945)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: 本文提出了一种新的模块化接口grables，旨在通过将表格转换为图结构来处理具有依赖关系的数据行，并通过实验验证了该方法在合成任务、交易数据和临床试验数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的行级预测器在处理事务性、时序性和关联性表格时表现不佳，因为这些表格中的标签与其他行相关联，而不仅仅是独立同分布的。

Method: 引入了一个名为grables的新框架，该框架将如何将表格提升到图形（构造函数）与如何在该图形上计算预测（节点预测器）分开，从而明确指出了表达能力的来源。

Result: 实验结果表明，消息传递能够捕捉到行间依赖关系，这是行本地模型所忽略的；同时，结合显式提取行间结构并将其输入给强大的表格学习者的混合方法能够持续获得性能提升。

Conclusion: 通过使用grables接口，可以更有效地利用表格数据中存在的结构信息，特别是当目标变量受到全局计数、重叠以及关系模式影响时。

Abstract: Tabular learning is still dominated by row-wise predictors that score each row independently, which fits i.i.d. benchmarks but fails on transactional, temporal, and relational tables where labels depend on other rows. We show that row-wise prediction rules out natural targets driven by global counts, overlaps, and relational patterns. To make "using structure" precise across architectures, we introduce grables: a modular interface that separates how a table is lifted to a graph (constructor) from how predictions are computed on that graph (node predictor), pinpointing where expressive power comes from. Experiments on synthetic tasks, transaction data, and a RelBench clinical-trials dataset confirm the predicted separations: message passing captures inter-row dependencies that row-local models miss, and hybrid approaches that explicitly extract inter-row structure and feed it to strong tabular learners yield consistent gains.

</details>


### [27] [eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models](https://arxiv.org/abs/2602.03986)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: cs.LG

TL;DR: 研究了预训练模型的群对称化对共形预测（CP）的影响，并提出通过群平均注入几何信息来减少不确定性区域，从而在行人轨迹预测中实现更紧凑的非一致性分数和更尖锐的预测集。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽然提供了数据可交换性假设下的正式覆盖保证，但在其不确定性区域在长期任务中可能会显著增大，导致统计保证失去意义。为了缓解这个问题，文章提出了一种新的方法来利用几何信息并通过群对称化技术改进共形预测的表现。

Method: 通过群平均预训练预测器将非一致性质量分布到轨道上，使每个样本作为轨道的代表，从而利用轨道诱导元素的对称群来缓解不确定性。这种方法能够证明地产生递增凸序中的收缩非一致性分数，意味着提高了指数尾界和预期中更尖锐的共形预测集。

Result: 理论分析表明该方法可以提高共形预测的性能，特别是在高置信度水平下。随后，作者设计了一个实验来验证这些理论主张在行人轨迹预测中的有效性。

Conclusion: 本文提出的方法通过结合群对称化和共形预测，在理论上和实验上都展示了对于改善不确定性的潜力，特别是在需要高精度预测的应用场景中。

Abstract: We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.

</details>


### [28] [When Chains of Thought Don't Matter: Causal Bypass in Large Language Models](https://arxiv.org/abs/2602.03994)
*Anish Sathyanarayanan,Aditya Nagarsekar,Aarush Rathore*

Main category: cs.LG

TL;DR: 研究发现，即使CoT提示详尽且具有策略性，并被表面级操纵检测器标记，模型答案往往在因果上独立于CoT内容。提出了一种诊断框架来审核这种失效模式，结合了可解释的行为模块和因果探针技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证通过惩罚不忠实的推理是否可以强制实现CoT提示确实暴露模型的推理过程并提高透明度这一假设。

Method: 采用了一种诊断框架，该框架包括一个用于评分CoT文本中与操纵相关信号的可解释行为模块以及一个通过隐藏状态修补测量CoT中介影响（CMI）的因果探针。

Result: 即使增加了可检测到的操纵信号，许多问答项目仍然表现出几乎完全绕过（CMI≈0）的情况；而某些逻辑问题显示出更强的中介作用（CMI高达0.56）。层分析揭示了即便平均CMI较低时也存在狭窄且依赖任务的“推理窗口”。

Conclusion: 研究表明，表面上对CoT提示的遵守并不保证其因果依赖性，强调了需要更深入理解模型如何利用给出的推理路径。

Abstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows'' even when mean CMI is low.

</details>


### [29] [PromptSplit: Revealing Prompt-Level Disagreement in Generative Models](https://arxiv.org/abs/2602.04009)
*Mehdi Lotfian,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: 本文提出了一种名为PromptSplit的框架，用于检测和分析生成模型之间由提示词引发的行为差异。该方法通过形成提示与输出特征的张量积嵌入，并计算相应的核协方差矩阵来工作，还利用了随机投影近似技术以确保可扩展性。实验表明，PromptSplit能够准确地检测到真实的行为差异，并识别出导致这些差异的具体提示词。


<details>
  <summary>Details</summary>
Motivation: 随着基于提示词引导的生成式AI模型在视觉和语言领域的迅速扩展，需要一种系统的方法来确定哪些类型的提示会导致不同的模型行为。由于这些模型采用不同的数据集和架构进行训练，因此有必要开发一种工具来帮助理解不同提示词对模型行为的影响。

Method: PromptSplit是一种基于核的方法，它首先为每个比较的模型对构造一个联合提示-输出表示，通过形成提示和图像（或文本）特征的张量积嵌入，并计算相应的核协方差矩阵。然后使用加权差分矩阵的特征空间来识别跨提示的主要行为差异方向。为了保证可扩展性，采用了随机投影近似技术，将计算复杂度降低至O(nr^2 + r^3)。此外，文章还提供了理论分析，证明了这种近似的预期偏差是受控的。

Result: 实验结果表明，在文本到图像、文本到文本以及图像描述等设置下，PromptSplit能够准确检测到真实存在的行为差异，并能分离出造成这些差异的具体提示词。这为检测生成模型之间的不一致提供了一个可解释性强的工具。

Conclusion: PromptSplit作为检测和分析生成模型间因提示词引起的行为差异的有效工具，不仅能够准确识别出模型间的分歧点，还能指出具体哪些提示词造成了这些分歧，为理解与改进生成模型提供了有价值的见解。

Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.

</details>


### [30] [Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.04019)
*Yichen Xu,Yuyang Liang,Shan Dai,Tianyang Hu,Tsz Nam Chan,Chenhao Ma*

Main category: cs.LG

TL;DR: 本文提出了一种统一的投影残差视角来理解参数高效微调(PEFT)，并基于此引入了Layer Card诊断工具，该工具能够帮助选择最佳的层进行微调，在保持模型性能的同时降低微调成本。实验表明，通过选择性地调整部分层次，可以接近全层LoRA的表现，同时显著减少微调成本和推理时的适配器层数。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）规模的增长，全参数微调的成本变得非常高昂，使得参数高效微调(PEFT)成为下游任务适应的首选策略。在可扩展服务中的推断延迟限制以及边缘或快速部署场景下的微调成本问题，使得不得不考虑哪些层需要被微调。然而，当前的做法通常是将PEFT均匀应用于所有层，而缺乏对层选择的理解或利用。

Method: 本研究开发了一个基于冻结基础模型之上的统一投影残差视图来理解PEFT。在局部二次近似下，逐层适应由三个量控制：(i) 投影残差范数(resnorm)，衡量一层能捕捉到多少可纠正偏差；(ii) 激活能量，决定特征条件化；(iii) 层耦合，量化跨层间残差相互作用强度。此外，研究还引入了Layer Card作为可重用诊断工具，它为给定模型的每一层总结了残差信号强度、计算成本和性能。

Result: 研究表明，对于平方损失和线性适配器而言，resnorm等于归一化的梯度范数，激活能量控制着病态条件和噪声放大，弱耦合导致层间贡献近似相加。基于这些见解，使用Layer Card指导放置可以在灵活优先考虑不同目标（如最大化性能或减少微调成本）的情况下改进所选调整层的选择。进一步地，在Qwen3-8B上，选择性地调整一部分层即可达到接近全层LoRA的效果，同时大幅减少了微调成本及推理过程中适配器增强层的数量。

Conclusion: 本文的工作为理解和优化参数高效微调提供了新的视角，并通过引入Layer Card诊断工具支持更明智地选择适合微调的层，从而在保证模型性能的同时有效降低了资源消耗。

Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.

</details>


### [31] [Group Contrastive Learning for Weakly Paired Multimodal Data](https://arxiv.org/abs/2602.04021)
*Aditya Gorla,Hugues Van Assel,Jan-Christian Huetter,Heming Yao,Kyunghyun Cho,Aviv Regev,Russell Littman*

Main category: cs.LG

TL;DR: 本文提出了GROOVE，一种针对弱配对跨模态数据的半监督多模态表示学习方法，通过引入GroupCLIP损失函数以及即时反向翻译自编码器框架来促进模态间纠缠表达的同时保持组内一致性。此外，还提出了一套综合评估框架以系统地评估不同场景下的表示学习者性能。实验结果表明GROOVE在交叉模态匹配和插补任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法对于完全配对或单一模态的数据处理较好，但在处理仅有弱共享扰动标签关联而缺乏直接对应的高内容扰动数据时存在不足。为了解决这一问题，并有效地利用这种弱配对信息来进行多模态表示学习，研究团队开发了新的解决方案。

Method: 1. 提出GroupCLIP，这是一种新的群组级对比损失函数，旨在填补针对弱配对设置的对比学习空白。
2. 将GroupCLIP与即时反向翻译自编码器框架相结合，以促进跨模态间的纠缠表示同时保持共享潜在空间内的组级一致性。
3. 引入了一个全面的组合评估框架，该框架使用多种最优传输对齐器系统地评估表示学习者，并包括了可以系统改变共享与模态特定扰动效果的新颖模拟，以便于原则性地评估方法的鲁棒性。

Result: 1. 组合基准测试显示，目前还没有一个对齐器能够统一地适用于所有设置或模态对。
2. 在模拟环境及两个真实的单细胞遗传扰动数据集上，GROOVE在下游跨模态匹配和插补任务中的表现优于或至少等同于现有方法。
3. 消融研究表明，GroupCLIP是推动性能提升的关键因素。

Conclusion: 本研究表明，在只有弱配对信息可用的情况下，利用群组级约束对于有效的多模态表示学习非常重要。所提出的GROOVE方法不仅为此类问题提供了解决方案，而且其核心组件GroupCLIP被证明是提高模型性能的主要驱动力。

Abstract: We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.

</details>


### [32] [The Illusion of Generalization: Re-examining Tabular Language Model Evaluation](https://arxiv.org/abs/2602.04031)
*Aditya Gorla,Ratish Puduppully*

Main category: cs.LG

TL;DR: 对Tabula-8B的系统重评估显示，其在表格预测中的表现主要依赖于四分位分类任务，顶级数据集存在普遍污染问题，且指令调优在没有表格暴露的情况下可以恢复大部分标准分类性能。这些发现表明所声称的一般化能力可能反映了评估过程中的问题而非真正的表格推理学习。


<details>
  <summary>Details</summary>
Motivation: 研究旨在重新评估以Tabula-8B为代表的表格语言模型（TLMs）是否真的实现了对于表格预测的新兴泛化能力。

Method: 通过使用来自UniPredict基准的165个数据集对Tabula-8B进行了系统性的重新评价。

Result: 二元和分类任务几乎未超过多数类基线；表现最好的数据集普遍存在污染现象；指令调整不涉及表格内容时仍能恢复92.2%的标准分类性能，在四分位分类上熟悉格式可弥补71.3%的差距。

Conclusion: 研究结论认为所谓的泛化效果很可能归因于评估过程中出现的问题，并非真正学到的表格推理技能。提出了加强TLM评估的建议。

Abstract: Tabular Language Models (TLMs) have been claimed to achieve emergent generalization for tabular prediction. We conduct a systematic re-evaluation of Tabula-8B as a representative TLM, utilizing 165 datasets from the UniPredict benchmark. Our investigation reveals three findings. First, binary and categorical classification achieve near-zero median lift over majority-class baselines and strong aggregate performance is driven entirely by quartile classification tasks. Second, top-performing datasets exhibit pervasive contamination, including complete train-test overlap and task-level leakage that evades standard deduplication. Third, instruction-tuning without tabular exposure recovers 92.2% of standard classification performance and on quartile classification, format familiarity closes 71.3% of the gap with the residual attributable to contaminated datasets. These findings suggest claimed generalization likely reflects evaluation artifacts rather than learned tabular reasoning. We conclude with recommendations for strengthening TLM evaluation.

</details>


### [33] [DADP: Domain Adaptive Diffusion Policy](https://arxiv.org/abs/2602.04037)
*Pengcheng Wang,Qinghang Liu,Haotian Lin,Yiheng Li,Guojian Zhan,Masayoshi Tomizuka,Yixiao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法DADP（Domain Adaptive Diffusion Policy），通过无监督解耦和领域感知扩散注入来实现鲁棒的自适应。实验表明，DADP在运动和操作等挑战性基准上表现出优越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 学习能够适应未见过的转换动态的策略是基于学习控制中的一个基本挑战。尽管通过领域表示学习捕捉特定领域的信息已经取得了显著进展，但作者发现选择与当前步骤相邻的上下文会导致学习到的表示混合静态领域信息与变化的动力学属性，从而混淆条件策略并限制零样本适应。

Method: 提出了DADP方法，该方法首先引入滞后上下文动力学预测策略，通过增加时间间隔来过滤掉瞬态属性，从而无监督地解开静态领域表示；其次，直接将学到的领域表示整合到生成过程中，通过偏置先验分布和重新定义扩散目标来实现。

Result: 广泛的实验证明了DADP在运动和操作任务上的优越表现及更好的泛化能力。

Conclusion: DADP为解决因领域表示中静态信息与动态属性纠缠导致的学习型控制适应性问题提供了一个有效方案。

Abstract: Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.

</details>


### [34] [Partition Trees: Conditional Density Estimation over General Outcome Spaces](https://arxiv.org/abs/2602.04042)
*Felipe Angelim,Alessandro Leite*

Main category: cs.LG

TL;DR: 提出了一种基于树的框架Partition Trees，用于对一般结果空间进行条件密度估计，并引入了其集成扩展Partition Forests。该方法提供了非参数化、可扩展的解决方案，实验表明在概率预测方面优于CART样式的树，并且与最新的概率树方法和随机森林相比具有竞争力或更优的表现，同时对冗余特征和异方差噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于提供一个能够处理连续和分类变量的统一框架，以实现条件密度估计，同时避免对目标分布做出参数假设。

Method: 通过直接最小化条件负对数似然来学习树模型，这些树模型将条件分布建模为数据自适应分区上的分段常数密度。此外，还提出了Partition Forests作为集合扩展，通过对条件密度求平均值来获得。

Result: 实证研究表明，Partition Trees在概率预测上优于CART风格的树，并且与最先进的概率树方法和随机森林相比表现相当甚至更好。此外，该方法对于冗余特征和异方差噪声表现出鲁棒性。

Conclusion: Partition Trees及其集成形式Partition Forests提供了一种新颖而有效的非参数方法来进行条件密度估计，它不仅适用于广泛的数据类型而且具有良好的性能及鲁棒性。

Abstract: We propose Partition Trees, a tree-based framework for conditional density estimation over general outcome spaces, supporting both continuous and categorical variables within a unified formulation. Our approach models conditional distributions as piecewise-constant densities on data adaptive partitions and learns trees by directly minimizing conditional negative log-likelihood. This yields a scalable, nonparametric alternative to existing probabilistic trees that does not make parametric assumptions about the target distribution. We further introduce Partition Forests, an ensemble extension obtained by averaging conditional densities. Empirically, we demonstrate improved probabilistic prediction over CART-style trees and competitive or superior performance compared to state-of-the-art probabilistic tree methods and Random Forests, along with robustness to redundant features and heteroscedastic noise.

</details>


### [35] [Agentic AI-Empowered Dynamic Survey Framework](https://arxiv.org/abs/2602.04071)
*Furkan Mumcu,Lokman Bekit,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 该论文提出了一种动态调查框架，将调查文章视为随着研究发展而不断演化的活文档，支持现有调查文章的持续更新，有效整合新出现的研究同时保持调查的一致性和结构。


<details>
  <summary>Details</summary>
Motivation: 由于研究产出的快速增长，综述文章很快就会过时，导致文献中的冗余和碎片化问题。因此，作者认为应该把撰写综述看作是一个长期维护的问题，而不是一次性的任务。

Method: 提出了一个具有能动性的动态调查框架，通过逐步整合新工作来支持现有调查论文的持续更新，同时保持调查结构并尽量减少不必要的破坏。

Result: 通过回顾性实验设置证明，所提出的框架能够有效地识别和整合新兴研究，同时保持现有调查的连贯性和结构。

Conclusion: 新的动态调查框架提供了一种方法来应对研究快速发展带来的挑战，使调查文章可以随时间保持相关性和准确性。

Abstract: Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.

</details>


### [36] [Stroke Lesions as a Rosetta Stone for Language Model Interpretability](https://arxiv.org/abs/2602.04074)
*Julius Fridriksson,Roger D. Newman-Norlund,Saeed Ahmadi,Regan Willis,Nadra Salman,Kalil Warren,Xiang Guan,Yong Yang,Srihari Nelakuditi,Rutvik Desai,Leonardo Bonilha,Jeff Charney,Chris Rorden*

Main category: cs.LG

TL;DR: 本研究提出了一种新的框架BLUM，利用脑损伤-症状映射作为外部参考结构来评估大型语言模型（LLMs）扰动的影响。通过对比LLM与人类患者在特定任务中的错误模式，发现两者存在显著相似性，为理解LLMs的语言功能提供了新视角，并建议进一步探讨行为一致性是否反映了共同的计算原理。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展示了惊人的能力，但目前缺乏验证哪些组件对于语言功能真正必要的方法。现有的解释性方法依赖内部指标且缺乏外部验证。

Method: 研究者开发了Brain-LLM Unified Model (BLUM)框架，采用病变-症状映射技术作为评估LLMs扰动效果的外部参考。通过对患有慢性中风失语症的个体数据进行分析，训练症状到病变的模型，对变压器层实施系统性扰动，并将扰动后的LLMs和人类患者的临床评估结果进行比较。

Result: 结果显示，LLM的错误模式与人类患者的错误模式高度相似，预测的病变位置与实际病变位置在图片命名条件下的匹配率达到了67%，句子完成条件下则为68.3%。语义主导型错误对应于腹侧流路径的病变模式，而音素主导型错误则对应于背侧流路径的病变模式。

Conclusion: 这项研究开辟了评估LLMs可解释性的新方法学途径，其中临床神经科学提供了外部验证手段，并确立了人类病变-症状映射作为评价人工语言系统的参考框架，同时激励了直接探索行为一致性背后是否存在着共享的计算原则。

Abstract: Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.

</details>


### [37] [A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models](https://arxiv.org/abs/2602.04082)
*Yicheng Zou,Samuel Lanthaler,Hossein Salahshoor*

Main category: cs.LG

TL;DR: 本文提出了一种基于分数的条件扩散算子的概率框架，用于在高频区域近似波。通过与其它确定性方法对比，实验结果表明该概率神经算子在$L^2$、$H^1$和能量范数下能够一致地产生最稳健的预测，并且能捕捉输入声速图传播到解场中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 确定性的神经算子在许多偏微分方程上表现良好，但在高频率波动现象的逼近上遇到了困难，因为强输入-输出敏感性和谱偏见模糊了振荡。为此，文章提倡采用概率方法来解决高频区域内的波动问题。

Method: 文章发展了一个基于分数的条件扩散算子的概率框架，并对Helmholtz算子进行了稳定性分析。接着，在广泛的频率范围内进行了数值实验，将结果与其他流行的基于数据驱动和机器学习的方法进行比较。

Result: 结果显示，所提出的概率神经算子在$L^2$、$H^1$以及能量范数方面始终产生具有最低误差的鲁棒预测。此外，与所有其他测试的确定性方法不同，该框架显著地捕捉到了输入声速映射传播到解场中的不确定性。

Conclusion: 研究结果表明，概率算子学习是一种有原则且有效的解决复杂PDEs如Helmholtz方程在挑战性的高频区域内的方法。

Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.

</details>


### [38] [Federated Concept-Based Models: Interpretable models with distributed supervision](https://arxiv.org/abs/2602.04093)
*Dario Fenoglio,Arianna Casanova,Francesco De Santis,Mohan Li,Gabriele Dominici,Johannes Schneider,Martin Gjoreski,Marc Langheinrich,Pietro Barbiero,Giovanni De Felice*

Main category: cs.LG

TL;DR: 提出了一种新的方法Federated Concept-based Models (F-CMs)，该方法能够在不断变化的联邦学习环境中部署基于概念的模型，同时保持机构隐私，并且在准确性、干预效果上优于非自适应联邦基线。


<details>
  <summary>Details</summary>
Motivation: 基于概念的模型（CMs）通过将预测建立在人类可理解的概念基础上来提高深度学习的可解释性，但获取概念注释的成本高昂且难以大规模获得。联邦学习可以利用分布在多个数据所有者中的概念注释来进行跨机构训练以缓解这一限制，但联邦学习缺乏可解释的建模范式。将CMs与联邦学习整合具有挑战性，因为现实世界中的联邦学习是异构和非静态的。

Method: 提出了Federated Concept-based Models (F-CMs)方法，在不断发展的联邦学习设置中部署CMs。F-CMs能够聚合跨机构的概念级信息，并高效地调整模型架构以响应可用概念监督的变化，同时保护机构隐私。

Result: 实验表明，F-CMs保留了完全概念监督训练环境下所具有的准确性和干预有效性，同时优于非自适应联邦基线。此外，F-CMs使得对于某一机构不可用的概念也能进行可解释的推理，这是相对于现有方法的关键创新之处。

Conclusion: 本研究提出的方法F-CMs成功地在联邦学习环境中实现了基于概念模型的应用，不仅提高了模型的可解释性，还解决了概念注释稀缺的问题，同时保证了不同参与方之间的隐私安全。

Abstract: Concept-based models (CMs) enhance interpretability in deep learning by grounding predictions in human-understandable concepts. However, concept annotations are expensive to obtain and rarely available at scale within a single data source. Federated learning (FL) could alleviate this limitation by enabling cross-institutional training that leverages concept annotations distributed across multiple data owners. Yet, FL lacks interpretable modeling paradigms. Integrating CMs with FL is non-trivial: CMs assume a fixed concept space and a predefined model architecture, whereas real-world FL is heterogeneous and non-stationary, with institutions joining over time and bringing new supervision. In this work, we propose Federated Concept-based Models (F-CMs), a new methodology for deploying CMs in evolving FL settings. F-CMs aggregate concept-level information across institutions and efficiently adapt the model architecture in response to changes in the available concept supervision, while preserving institutional privacy. Empirically, F-CMs preserve the accuracy and intervention effectiveness of training settings with full concept supervision, while outperforming non-adaptive federated baselines. Notably, F-CMs enable interpretable inference on concepts not available to a given institution, a key novelty with respect to existing approaches.

</details>


### [39] [CoRe: Context-Robust Remasking for Diffusion Language Models](https://arxiv.org/abs/2602.04096)
*Kevin Zhai,Sabbir Mollah,Zhenyi Wang,Mubarak Shah*

Main category: cs.LG

TL;DR: 本文提出了一种名为Context-Robust Remasking (CoRe) 的无需训练的框架，旨在解决Masked Diffusion Models (MDMs) 中由于上下文刚性导致的解码问题。通过探测目标掩码上下文扰动对令牌敏感性的方法来识别出容易受上下文影响的令牌，并将修订形式化为一个鲁棒优化目标，优先处理不稳定的令牌。实验表明该方法在推理和代码基准测试中均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 标准Masked扩散模型(MDMs)中的解码受到上下文刚性的影响，这意味着令牌基于暂时的高度置信度被保留下来，而忽视了早期预测缺乏完整上下文的事实。这会导致级联效应，即初始的不一致会误导后续生成过程。现有的修正策略试图通过依赖静态置信度分数来缓解这个问题，但这些信号本质上是短视的；对于模型自身而言，不一致的令牌可能看起来非常自信。

Method: 作者提出了Context-Robust Remasking (CoRe)，这是一种无需额外训练即可在推断时进行修正的框架。它不是简单地信任静态令牌概率，而是通过探测针对特定掩码上下文扰动的敏感性来识别那些易受上下文影响（即脆弱）的令牌。接着，他们将修订步骤形式化为一个关于上下文变化的鲁棒优化目标，并有效地近似这个目标以优先考虑不稳定令牌进行修订。

Result: 在LLaDA-8B-Base上的实验结果显示，与计算匹配的基线相比，CoRe在推理和代码基准测试中均提供了持续改进，特别是在MBPP任务上提升了高达9.2个百分点。

Conclusion: 提出的CoRe框架能够在不增加训练成本的情况下有效提高Masked扩散模型的解码质量，尤其是在需要精确上下文理解的任务中表现出色。

Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.

</details>


### [40] [Rethinking Perplexity: Revealing the Impact of Input Length on Perplexity Evaluation in LLMs](https://arxiv.org/abs/2602.04099)
*Letian Cheng,Junyan Wang,Yan Gao,Elliott Wen,Ting Dang,Hong Jia*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估框架LengthBenchmark，该框架明确地将输入长度、评估协议设计和系统级成本整合起来，以解决大语言模型在不同上下文长度下进行评估时的公平性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 困惑度是评估大型语言模型（LLMs）预测质量的一个广泛采用的指标，但当使用不相关的长输入时，困惑度可能变得不可靠，这引起了基准测试和系统部署方面的担忧。此外，先前的工作很少将输入长度视为影响公平性和效率的一级系统变量来系统性地研究其对困惑度的影响。

Method: 引入了LengthBenchmark这一系统意识评估框架，它显式地结合了输入长度、评估协议设计以及系统级别的成本。通过两种评分协议（直接累积和固定窗口滑动）对代表性LLM在不同上下文长度下的表现进行了评估，并且还测量了延迟、内存占用和评估成本。

Result: 分析表明：(i) 滑动窗口评估始终会夸大短输入的表现；(ii) 不论是全精度还是量化模型，在评估段长度增加时都显示出改进。

Conclusion: LengthBenchmark不仅关注准确性导向的度量标准，还考虑到了延迟、内存占用和评估成本等因素，从而将预测度量与实际部署联系起来。结果表明，长度偏差是一个普遍现象，它破坏了跨模型比较的公平性。

Abstract: Perplexity is a widely adopted metric for assessing the predictive quality of large language models (LLMs) and often serves as a reference metric for downstream evaluations. However, recent evidence shows that perplexity can be unreliable, especially when irrelevant long inputs are used, raising concerns for both benchmarking and system deployment. While prior efforts have employed selective input filtering and curated datasets, the impact of input length on perplexity has not been systematically studied from a systems perspective and input length has rarely been treated as a first-class system variable affecting both fairness and efficiency. In this work, we close this gap by introducing LengthBenchmark, a system-conscious evaluation framework that explicitly integrates input length, evaluation protocol design, and system-level costs, evaluating representative LLMs under two scoring protocols (direct accumulation and fixed window sliding) across varying context lengths. Unlike prior work that focuses solely on accuracy-oriented metrics, LengthBenchmark additionally measures latency, memory footprint, and evaluation cost, thereby linking predictive metrics to deployment realities. We further incorporate quantized variants not as a main contribution, but as robustness checks, showing that length-induced biases persist across both full-precision and compressed models. This design disentangles the effects of evaluation logic, quantization, and input length, and demonstrates that length bias is a general phenomenon that undermines fair cross-model comparison. Our analysis yields two key observations: (i) sliding window evaluation consistently inflates performance on short inputs, and (ii) both full-precision and quantized models appear to realise gains as the evaluated segment length grows.

</details>


### [41] [Supervised Learning as Lossy Compression: Characterizing Generalization and Sample Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.04107)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 本文提出了一种新的信息论视角来看待机器学习中的泛化问题，通过将学习问题置于有损压缩的框架内，并应用有限码长分析来推导样本复杂度和泛化误差的下界。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解机器学习算法的泛化能力及其与归纳偏置之间的关系，文章从信息论角度出发，提供了一个新颖的学习过程建模方式。

Method: 该方法将训练数据采样视为编码过程，模型构建视为解码过程，并利用有限码长分析技术来为给定随机学习算法及其最优采样策略推导出样本复杂度和泛化错误率的理论下界。

Result: 研究得出了能够清晰区分学习算法过拟合程度及与任务间不匹配程度的新界限表达式，并且进一步解析了过拟合项，揭示其与现有信息论边界指标和稳定性理论间的联系。

Conclusion: 本研究不仅为理解机器学习中的泛化提供了新的视角，还统一了信息论边界和稳定性理论下的相关观点，为未来的研究开辟了新方向。

Abstract: This paper presents a novel information-theoretic perspective on generalization in machine learning by framing the learning problem within the context of lossy compression and applying finite blocklength analysis. In our approach, the sampling of training data formally corresponds to an encoding process, and the model construction to a decoding process. By leveraging finite blocklength analysis, we derive lower bounds on sample complexity and generalization error for a fixed randomized learning algorithm and its associated optimal sampling strategy. Our bounds explicitly characterize the degree of overfitting of the learning algorithm and the mismatch between its inductive bias and the task as distinct terms. This separation provides a significant advantage over existing frameworks. Additionally, we decompose the overfitting term to show its theoretical connection to existing metrics found in information-theoretic bounds and stability theory, unifying these perspectives under our proposed framework.

</details>


### [42] [Rate-Optimal Noise Annealing in Semi-Dual Neural Optimal Transport: Tangential Identifiability, Off-Manifold Ambiguity, and Guaranteed Recovery](https://arxiv.org/abs/2602.04110)
*Raymond Chu,Jaewoong Choi,Dohyun Kwon*

Main category: cs.LG

TL;DR: 本文探讨了半对偶神经最优传输在数据集中于低维流形时可能收敛到错误或退化解的问题，并通过加性噪声平滑方法提供了新的映射恢复保证。研究提出了一个可计算的终端噪声水平$\varepsilon_{\\mathrm{stat}}(N)$，该值能够达到与数据内在维度$m$相关的最佳统计速率。此外，文章还指出随着$\varepsilon$逐渐减小，减少的半对偶目标变得越来越不适定，从而为退火过程提供了一个有原则的停止规则。


<details>
  <summary>Details</summary>
Motivation: 半对偶神经最优传输在学习运输图时可能会收敛到不正确的或退化的解。本文旨在完全描述当数据集中在低维流形上时出现的虚假解问题，并探索解决方法及其理论依据。

Method: 采用加性噪声平滑作为解决方案，并对其进行了深入研究；提出了基于数据内在维度而非环境维度来确定最佳统计率的可计算终端噪声水平公式；分析了最优计划的定量稳定性、平滑引起的偏差以及有限样本误差。

Result: 证明了随着噪声消失，可以实现地图恢复的新保证；给出了一个依赖于数据内在维度$m$而不是环境维度的最佳统计速率下的可计算终端噪声水平$\varepsilon_{\\mathrm{stat}}(N)$；揭示了当$\varepsilon$趋近于0时，降低的半对偶目标会变得越来越不适定。

Conclusion: 对于处理数据集中的低维流形情况，适当的噪声水平选择不仅有助于提高统计准确性，而且还可以避免优化条件恶化。提出的方法为实践中如何设定退火过程提供了一条实用且有理论支持的指导方针。

Abstract: Semi-dual neural optimal transport learns a transport map via a max-min objective, yet training can converge to incorrect or degenerate maps. We fully characterize these spurious solutions in the common regime where data concentrate on low-dimensional manifold: the objective is underconstrained off the data manifold, while the on-manifold transport signal remains identifiable. Following Choi, Choi, and Kwon (2025), we study additive-noise smoothing as a remedy and prove new map recovery guarantees as the noise vanishes. Our main practical contribution is a computable terminal noise level $\varepsilon_{\mathrm{stat}}(N)$ that attains the optimal statistical rate, with scaling governed by the intrinsic dimension $m$ of the data. The formula arises from a theoretical unified analysis of (i) quantitative stability of optimal plans, (ii) smoothing-induced bias, and (iii) finite-sample error, yielding rates that depend on $m$ rather than the ambient dimension. Finally, we show that the reduced semi-dual objective becomes increasingly ill-conditioned as $\varepsilon \downarrow 0$. This provides a principled stopping rule: annealing below $\varepsilon_{\mathrm{stat}}(N)$ can $\textit{worsen}$ optimization conditioning without improving statistical accuracy.

</details>


### [43] [Turning mechanistic models into forecasters by using machine learning](https://arxiv.org/abs/2602.04114)
*Amit K. Chakraborty,Hao Wang,Pouria Ramazi*

Main category: cs.LG

TL;DR: 该研究提出了一种结合常数和时变参数来推断系统方程的方法，通过从数据中直接学习参数的时间演变，从而提高了建模精度和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动发现方法通常假设系数是时间不变的，这限制了它们捕捉演化系统动态的能力。为了解决这一局限性，本研究旨在允许部分参数随时间变化，并直接从数据中学习这些参数的时间演变，以便能够更准确地描述复杂系统的动态行为。

Method: 研究人员开发了一种新的框架，该框架不仅能够从时间序列数据中推断出包含常数和时变参数的系统方程，而且还能通过预测时变参数并将其代入学到的方程中来转化为预测模型。

Result: 提出的模型在多个数据集上进行了验证，包括易感-感染-恢复（SIR）、消费者-资源、温室气体浓度以及蓝藻细胞计数等。结果显示，在学习时间序列方面平均绝对误差低于3%，而在最多一个月的未来预测上则低于6%。此外，与CNN-LSTM及梯度提升机(GBM)相比，该模型在大多数数据集上的预测表现均优于这两种方法。

Conclusion: 将时变参数整合到微分方程的数据驱动发现过程中，可以显著提高模型的准确性及其对未来事件的预测能力。

Abstract: The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\% for learning a time series and below 6\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.

</details>


### [44] [Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach](https://arxiv.org/abs/2602.04116)
*Sicheng Liu,Xunkai Li,Daohan Su,Ru Zhang,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架PLANET，旨在解决现有MGFMs在模态交互和对齐方面的局限性。通过采用分而治之的策略，PLANET能够在不同粒度上解耦模态间的交互与对齐问题，从而显著提升在多种图中心及多模态生成任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的Multimodal Graph Foundation Models (MGFMs)未能有效处理模态间交互以及实现良好的模态对齐，这限制了它们捕捉跨模态复杂语义的能力。为克服这些挑战，并扩大模型的应用范围至更广泛的下游任务，研究者们提出了改进方案。

Method: 引入了名为PLANET的新框架，该框架利用分而治之的方法来分别处理嵌入级别和节点级别的模态交互与对齐问题。具体来说，通过Embedding-wise Domain Gating (EDG)增强局部语义信息，同时借助Node-wise Discretization Retrieval (NDR)建立离散化语义表示空间以促进全局模态对齐。

Result: 实验结果表明，在多种图相关的及多模态生成任务中，PLANET相比最先进基准方法表现出显著优势。

Conclusion: 通过针对性地优化模态交互与对齐机制，PLANET为开发更加高效且适应性强的MGFMs提供了新思路，有望推动相关领域进一步发展。

Abstract: Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.

</details>


### [45] [Generative Neural Operators through Diffusion Last Layer](https://arxiv.org/abs/2602.04139)
*Sungwon Park,Anthony Zhou,Hongjoong Kim,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出了一种名为扩散最后一层（DLL）的轻量级概率头，可以附加到任意神经算子骨干上，以直接在函数空间中通过低秩Karhunen-Loève展开参数化条件输出分布，从而有效且富有表现力地建模预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 针对许多实际系统固有的随机性问题，为了实现可靠部署，需要有原则的不确定性量化方法。受到偏微分方程解分布通常表现出相对平滑性和低维结构特征的启发，提出了该方法。

Method: 通过将一个轻量级的概率头部——扩散最后一层（DLL）附加到任意神经算子架构上，直接在函数空间内利用低秩Karhunen-Loève展开来参数化条件输出分布。

Result: DLL在随机PDE算子学习基准测试中提升了泛化能力和不确定性感知预测。此外，在确定性的长周期滚动设置下，DLL提高了滚动稳定性，并为骨干神经算子提供了有意义的认识论不确定性估计。

Conclusion: DLL作为一种简单有效的添加组件，能够显著提高基于神经算子模型处理随机系统时的性能和可靠性。

Abstract: Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.

</details>


### [46] [Pruning for Generalization: A Transfer-Oriented Spatiotemporal Graph Framework](https://arxiv.org/abs/2602.04153)
*Zihao Jing,Yuxi Long,Ganlin Feng*

Main category: cs.LG

TL;DR: 本文提出了一种面向迁移的时空框架TL-GPSTGN，通过选择性地修剪非优化图上下文来提高样本效率和分布外泛化能力。该方法在大规模交通基准测试中，在低数据迁移场景下持续优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列预测在图结构域中的实际应用至关重要，但现有的时空模型在数据稀缺和跨域变化情况下性能下降。为了解决这些问题，作者从结构感知上下文选择的角度出发。

Method: 提出了TL-GPSTGN框架，利用信息论与相关性标准提取结构化信息子图及特征，形成紧凑且语义基础的表示，并将其整合进时空卷积架构中以捕捉复杂的多变量动态。

Result: 在大规模交通基准上的评估表明，TL-GPSTGN在低数据量转移场景中始终优于基线方法。

Conclusion: 研究表明，显式的上下文剪枝作为归纳偏置能够有效增强基于图的预测模型的鲁棒性。

Abstract: Multivariate time series forecasting in graph-structured domains is critical for real-world applications, yet existing spatiotemporal models often suffer from performance degradation under data scarcity and cross-domain shifts. We address these challenges through the lens of structure-aware context selection. We propose TL-GPSTGN, a transfer-oriented spatiotemporal framework that enhances sample efficiency and out-of-distribution generalization by selectively pruning non-optimized graph context. Specifically, our method employs information-theoretic and correlation-based criteria to extract structurally informative subgraphs and features, resulting in a compact, semantically grounded representation. This optimized context is subsequently integrated into a spatiotemporal convolutional architecture to capture complex multivariate dynamics. Evaluations on large-scale traffic benchmarks demonstrate that TL-GPSTGN consistently outperforms baselines in low-data transfer scenarios. Our findings suggest that explicit context pruning serves as a powerful inductive bias for improving the robustness of graph-based forecasting models.

</details>


### [47] [Benchmarking Uncertainty Quantification of Plug-and-Play Diffusion Priors for Inverse Problems Solving](https://arxiv.org/abs/2602.04189)
*Xiaoyu Qiu,Taewon Yang,Zhanhao Liu,Guanyang Wang,Liyue Shen*

Main category: cs.LG

TL;DR: 本文针对现有插件式扩散先验求解器（PnPDP）在逆问题中仅评估单一样本点估计准确度的局限性，提出了一种系统化的研究方法来评估这些求解器的不确定性量化（UQ）。通过设计严格的玩具模型模拟实验和实际科学逆问题的应用，文章为PnPDP求解器提供了一个基于UQ的新分类，并观察到与分类一致的不确定性行为，为理解和评价PnPDP提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 当前对于插件式扩散先验求解器（PnPDP）重建质量的评价主要集中在单一样本上的点估计准确性指标上，这忽略了求解器的随机性和逆问题内在的不确定性，而后者对于科学任务至关重要。这种做法导致了评估标准与实际需求之间的不匹配，即忽视了对分布特性的考量如不确定性。

Method: 作者们设计了一系列严格的玩具模型仿真测试，以考察不同PnPDP求解器的不确定性表现，并据此提出了一个基于不确定性量化的分类体系。此外，还通过对玩具仿真及多种真实世界中的科学逆问题进行广泛实验，进一步验证所提分类的有效性。

Result: 研究表明，观察到的不同PnPDP求解器在不确定性方面的表现与其提出的分类框架相符，并且得到了理论上的支持。这为理解和评价PnPDP提供了新的视角。

Conclusion: 这项工作揭示了当前PnPDP求解器评估方法中存在的差距，并通过引入不确定性量化作为关键考量因素，提出了一个新的评估基准。所提出的基于UQ的分类有助于更全面地理解PnPDP求解器的性能。

Abstract: Plug-and-play diffusion priors (PnPDP) have become a powerful paradigm for solving inverse problems in scientific and engineering domains. Yet, current evaluations of reconstruction quality emphasize point-estimate accuracy metrics on a single sample, which do not reflect the stochastic nature of PnPDP solvers and the intrinsic uncertainty of inverse problems, critical for scientific tasks. This creates a fundamental mismatch: in inverse problems, the desired output is typically a posterior distribution and most PnPDP solvers induce a distribution over reconstructions, but existing benchmarks only evaluate a single reconstruction, ignoring distributional characterization such as uncertainty. To address this gap, we conduct a systematic study to benchmark the uncertainty quantification (UQ) of existing diffusion inverse solvers. Specifically, we design a rigorous toy model simulation to evaluate the uncertainty behavior of various PnPDP solvers, and propose a UQ-driven categorization. Through extensive experiments on toy simulations and diverse real-world scientific inverse problems, we observe uncertainty behaviors consistent with our taxonomy and theoretical justification, providing new insights for evaluating and understanding the uncertainty for PnPDPs.

</details>


### [48] [LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data](https://arxiv.org/abs/2602.04192)
*Vivek Anand,Alec Helbling,Mark Davenport,Gordon Berman,Sankar Alagapan,Christopher Rozell*

Main category: cs.LG

TL;DR: 本文提出了一种名为LORE（低秩序数嵌入）的框架，可以从噪声三元组比较中同时学习内在维度和序数嵌入，而不需要预先设定嵌入维度。通过迭代加权算法优化目标，并在合成数据集、模拟感知空间以及真实众包序数判断上进行了广泛的实验，表明LORE能够学习到紧凑、可解释且高度准确的低维嵌入，从而恢复主观感知的潜在几何结构。


<details>
  <summary>Details</summary>
Motivation: 从像味觉、嗅觉或美学这样的主观感知空间中学习内在维度性是一个具有挑战性的问题。现有方法通常需要预先设置嵌入维度，这限制了其灵活性和准确性。

Method: 引入了LORE (Low Rank Ordinal Embedding) 框架，该框架利用非凸Schatten-$p$准范数来正则化解，从而自动联合恢复序数嵌入及其维度。采用迭代加权算法来优化这一联合目标，并建立了收敛保证。

Result: 在合成数据集、模拟感知空间及真实世界众包序数判断上的广泛实验表明，LORE能够学习到紧凑、可解释且非常精确的低维嵌入，这些嵌入能够很好地恢复主观感知的潜在几何结构。

Conclusion: 通过同时推断内在维度性和序数嵌入，LORE使得心理物理学中的感知建模更加可解释且数据效率更高，为机器学习中从序数数据发现低维结构开辟了新的方向。

Abstract: Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, "Is A more similar to B than C?". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-$p$ quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.

</details>


### [49] [From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction](https://arxiv.org/abs/2602.04201)
*Yanjie Tong,Peng Chen*

Main category: cs.LG

TL;DR: 提出了一种名为STRIDE的新框架，用于从稀疏传感器测量中重建高维时空场。通过使用FMMNN作为隐式神经表示（INR）的骨干，STRIDE在极稀疏传感条件下表现优于其他方法，并支持超分辨率和抗噪性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以跨轨迹和参数设置泛化，或依赖于与离散化相关的解码器，这些解码器不能自然地跨越网格和分辨率转移。为了解决从稀疏点传感器测量中重建高维时空场这一中心挑战。

Method: 提出了STRIDE（Spatio-Temporal Recurrent Implicit DEcoder），一个两阶段框架，首先将传感器测量的短窗口映射到具有时间编码器的潜在状态，然后利用调制的隐式神经表示（INR）解码器在任意查询位置重建场。采用FMMNN作为INR的后端，以改善对复杂空间场的表示并提供比基于正弦的INR更稳定的优化。

Result: 在四个涵盖混沌动力学和波传播的挑战基准测试中，STRIDE在极稀疏传感条件下超越了强大的基线模型，支持超分辨率，并且对噪声保持鲁棒性。

Conclusion: STRIDE为从稀疏点传感器测量中重建高维时空场提供了有效的方法，特别是在处理复杂的时空动态时展现出优越性能。

Abstract: Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.

</details>


### [50] [From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution](https://arxiv.org/abs/2602.04255)
*Hanlin Pan,Yuhao Tang,Wanfu Gao*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过将部分多标签学习(PML)中的标签消歧和特征选择任务建模为部分可观测马尔可夫决策过程(POMDP)，从而把PML风险最小化转化为预期收益最大化。该方法分为两个阶段：第一阶段使用强化学习训练转换策略以生成高质量的硬伪标签；第二阶段则逐步选择特征并输出一个可解释的全局排名。理论分析支持了PML-POMDP之间的对应关系，并分解了误差来源。实验表明该框架在多个指标和数据集上具有优势。


<details>
  <summary>Details</summary>
Motivation: 在部分多标签学习（PML）中，真实标签是未观察到的，这使得标签消歧变得重要但又困难。主要挑战在于模糊候选标签可能将错误传播到下游任务如特征工程。为了解决这个问题，作者提出了联合建模消歧与特征选择任务的方法。

Method: 文章采用的方法是将消歧及特征选择任务作为部分可观测马尔可夫决策过程（POMDP）来处理，旨在将PML的风险最小化问题转变为期望回报的最大化。整个过程分为两步：第一步利用强化学习训练一个transformer策略以产生高质量的硬伪标签；第二步则是将特征选择描述为一个顺序强化学习问题，逐步选择特征并给出一个易于理解的全局排序。

Result: 研究表明，所提出的框架不仅在理论上能够证明PML与POMDP之间的联系以及超额风险界限，而且还能有效地将误差分解为伪标签质量和样本大小两个部分。此外，通过多种度量标准和数据集上的实验验证了该框架的优势。

Conclusion: 通过将PML问题重新定义为POMDP形式，并结合强化学习技术来解决标签消歧与特征选择的问题，本研究提出的新方法显示出了优越性。理论分析和实验证据均支持了这种方法的有效性和实用性。

Abstract: In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.

</details>


### [51] [Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning](https://arxiv.org/abs/2602.04265)
*Wenze Lin,Zhen Yang,Xitai Jiang,Pony Ma,Gao Huang*

Main category: cs.LG

TL;DR: 本文提出了一种名为T2T(Thickening-to-Thinning)的动态奖励框架，旨在解决强化学习中遇到的熵崩溃、过度冗长及对难题探索不足的问题。通过在错误尝试时鼓励更广泛的搜索，在正确解答后减少冗余来促进模型的信心和推理能力的发展。实验表明，T2T在多个数学基准测试上优于标准GRPO及其他最新基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习与可验证奖励（RLVR）虽然能够增强大型语言模型（LLMs）中的推理能力，但存在如熵崩溃、过度冗长以及对于复杂问题探索不够充分等问题。尤其是现有的奖励机制无法很好地平衡解决问题时所需的广泛搜索与掌握知识后追求效率之间的需求。

Method: 提出了T2T(Thickening-to-Thinning)，一种受人类学习过程启发而设计的双阶段动态奖励框架。当模型给出的答案不正确时，T2T通过增加轨迹长度来扩大搜索空间并探索新的解决方案；一旦模型找到正确的答案，则转向缩短路径长度以避免冗余，并促进模型自信心的增长及其推理能力的固化。

Result: 通过对Qwen系列及Deepseek模型进行大量实验，在MATH-500, AIME, AMC等数学基准测试上评估了T2T的表现。结果显示，T2T不仅显著超越了传统的GRPO算法，同时也优于最近提出的其他基线方法。

Conclusion: T2T作为一种新型的动态奖励策略，有效解决了现有RLVR方案中存在的关键问题，促进了大型语言模型在处理复杂任务时的性能提升。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes "thickening" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to "thinning", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.

</details>


### [52] [Multi-Integration of Labels across Categories for Component Identification (MILCCI)](https://arxiv.org/abs/2602.04270)
*Noga Mudrik,Yuxi Chen,Gal Mishne,Adam S. Charles*

Main category: cs.LG

TL;DR: 介绍了一种新的数据分析方法MILCCI，该方法可以识别数据中的可解释组件、捕捉跨试验的变异性，并整合标签信息来理解每个类别在数据中的表示。通过合成和真实世界例子证明了其性能。


<details>
  <summary>Details</summary>
Motivation: 在时间序列分析中，理解标签如何在多试验观测中编码以及区分每个标签条目在各个类别中的不同影响是一个关键挑战。

Method: 提出了MILCCI，一种能够识别数据背后可解释成分的新颖数据驱动方法；它捕获跨试验的变化性，并结合标签信息来理解数据内每一类别的表现。MILCCI扩展了一个稀疏的每试验分解，利用每个类别内的标签相似性来实现细微的、基于标签的跨试验调整成分组成，并区分每一类别的贡献。

Result: MILCCI通过合成实例及现实案例（如投票模式、在线页面浏览趋势和神经元记录）展示了其性能。

Conclusion: MILCCI提供了一种有效的方法来解析带有复杂标签结构的大规模时间序列数据集，有助于更好地理解各类别标签对数据的影响。

Abstract: Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.

</details>


### [53] [Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms](https://arxiv.org/abs/2602.04277)
*Priyankkumar Dhrangdhariya,Soumyadipta Maiti,Venkataramana Runkana*

Main category: cs.LG

TL;DR: 该研究提出了一种集成生成设计和机器学习驱动的框架，用于优化适用于乘用车的UPTIS类型辐条结构。通过使用高阶多项式表示法对上、下辐条轮廓进行参数化，并利用PCHIP几何变化创建了约250个生成设计。采用KRR（刚度）和XGBoost（耐用性和振动）模型实现了强预测准确性，减少了对计算密集型FEM仿真的依赖。通过粒子群优化和贝叶斯优化进一步提高了性能。结果表明，与基线相比，新设计在刚度可调性、耐用性提高以及振动减少方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 非充气轮胎为传统充气轮胎提供了一个有前景的替代方案，但其不连续的辐条结构在刚度调节、耐久性及高速振动方面存在挑战。本研究旨在开发一种系统方法来克服这些难题，特别是针对UPTIS类型的辐条结构。

Method: 研究人员首先通过高阶多项式表示法对上下辐条轮廓进行了参数化处理，然后基于PCHIP技术产生了大约250种设计方案。接着，运用KRR算法预测刚度，而XGBoost则被用来估计耐用性和振动表现。最后，借助粒子群优化(PSO)与贝叶斯优化方法对设计进行了优化。

Result: 最终的设计显示，在刚度调整范围上达到了53%的提升；耐用性提升了最多达50%；并且相比于基础设计，震动减少了43%。此外，PSO提供了快速且有针对性的收敛速度，而贝叶斯优化则有效地探索了多目标间的权衡关系。

Conclusion: 所提出的框架能够系统地发展高性能下一代UPTIS辐条结构，不仅提高了非充气轮胎的关键性能指标，还展示了如何结合生成设计与机器学习以加速产品开发周期。

Abstract: Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.

</details>


### [54] [Convolution Operator Network for Forward and Inverse Problems (FI-Conv): Application to Plasma Turbulence Simulations](https://arxiv.org/abs/2602.04287)
*Xingzhuo Chen,Anthony Poole,Ionut-Gabriel Farcas,David R. Hatch,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: 提出了FI-Conv框架，基于U-Net架构但用ConvNeXt V2块替换了大部分卷积层，用于预测系统演化和估计复杂时空动态（如湍流）中的参数。该方法在Hasegawa-Wakatani方程所描述的湍流等离子体场预测任务上表现出色，并且能够通过基于梯度下降的逆向估计方法准确推断PDE参数。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种能够有效处理具有复杂时空动态系统的前向预测及参数估计问题的新方法。

Method: 采用了改进的U-Net架构，其中大部分卷积层被替换为ConvNeXt V2块。模型接收初始状态、PDE参数以及演化时间作为输入来预测系统未来状态。此外，还开发了一种基于梯度下降的逆向估计技术以从给定的数据中准确地推断出PDE参数。

Result: FI-Conv能够在短时间内(t ~ 3)准确地进行前向预测，并且长时间内(t ~ 100)保持对物理量统计特性的良好捕捉。同时，提出的逆向估计方法也能够精确地从数据中恢复PDE参数。

Conclusion: FI-Conv作为一种新的解决方案，在处理像湍流这样具有挑战性的复杂时空动态问题时表现出了显著的优势，可作为现有物理学信息机器学习方法的有效替代方案。

Abstract: We propose the Convolutional Operator Network for Forward and Inverse Problems (FI-Conv), a framework capable of predicting system evolution and estimating parameters in complex spatio-temporal dynamics, such as turbulence. FI-Conv is built on a U-Net architecture, in which most convolutional layers are replaced by ConvNeXt V2 blocks. This design preserves U-Net performance on inputs with high-frequency variations while maintaining low computational complexity. FI-Conv uses an initial state, PDE parameters, and evolution time as input to predict the system future state. As a representative example of a system exhibiting complex dynamics, we evaluate the performance of FI-Conv on the task of predicting turbulent plasma fields governed by the Hasegawa-Wakatani (HW) equations. The HW system models two-dimensional electrostatic drift-wave turbulence and exhibits strongly nonlinear behavior, making accurate approximation and long-term prediction particularly challenging. Using an autoregressive forecasting procedure, FI-Conv achieves accurate forward prediction of the plasma state evolution over short times (t ~ 3) and captures the statistic properties of derived physical quantities of interest over longer times (t ~ 100). Moreover, we develop a gradient-descent-based inverse estimation method that accurately infers PDE parameters from plasma state evolution data, without modifying the trained model weights. Collectively, our results demonstrate that FI-Conv can be an effective alternative to existing physics-informed machine learning methods for systems with complex spatio-temporal dynamics.

</details>


### [55] [Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling](https://arxiv.org/abs/2602.04323)
*Dian Jin,Yancheng Yuan,Xiaoming Tao*

Main category: cs.LG

TL;DR: 提出了一种新的模型CEITNet，用于从原子结构预测高阶晶体张量属性。该方法通过构建多通道笛卡尔局部环境张量并对通道空间进行可学习的交互，实现了高效地构造高阶张量。在多个基准数据集上，CEITNet在关键准确性指标上超越了先前的方法，并且提供了高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 从原子结构端到端预测高阶晶体张量性质仍然是一个挑战。虽然球谐等变模型具有表达能力，但它们的克莱布什-戈登张量积对于更高阶的目标会产生大量的计算和内存成本。

Method: 提出了Cartesian Environment Interaction Tensor Network (CEITNet)，该方法为每个原子构建一个多通道笛卡尔局部环境张量，并通过可学习的通道空间交互来进行灵活的多体混合。通过在通道空间中学习并使用笛卡尔张量基来组装等变输出，使得能够有效地构造高阶张量。

Result: 在针对2阶介电、3阶压电以及4阶弹性张量预测的基准数据集上，CEITNet在关键准确性指标方面超过了之前高阶预测方法的表现，同时提供了很高的计算效率。

Conclusion: CEITNet提供了一种新颖而有效的方法来解决基于原子结构预测高阶晶体张量属性的问题，它不仅提高了预测精度，还显著提升了计算效率。

Abstract: End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Environment Interaction Tensor Network (CEITNet), an approach that constructs a multi-channel Cartesian local environment tensor for each atom and performs flexible many-body mixing via a learnable channel-space interaction. By performing learning in channel space and using Cartesian tensor bases to assemble equivariant outputs, CEITNet enables efficient construction of high-order tensor. Across benchmark datasets for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction, CEITNet surpasses prior high-order prediction methods on key accuracy criteria while offering high computational efficiency.

</details>


### [56] [RISE: Interactive Visual Diagnosis of Fairness in Machine Learning Models](https://arxiv.org/abs/2602.04339)
*Ray Chen,Christan Grant*

Main category: cs.LG

TL;DR: 本文介绍了一个名为RISE的交互式可视化工具，该工具通过将排序后的残差转换为可解释的模式来帮助评估领域迁移下的公平性问题。RISE能够实现局部差异诊断、跨环境子群比较以及发现隐藏的公平性问题，并揭示了聚合统计信息可能忽略的准确性与公平性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 在领域迁移背景下评价模型公平性具有挑战性，因为简单的标量指标往往掩盖了不平等现象的具体表现形式及其产生原因。

Method: 提出了一种名为RISE（Residual Inspection through Sorted Evaluation）的交互式可视化工具，该工具将排序后的残差转化为易于理解的形式，并将其与正式的公平概念相联系。

Result: RISE工具能够支持对局部差异进行诊断、不同环境中的子群体对比分析以及检测潜在的公平性问题。此外，它还能够暴露被总体统计数据所忽视的准确性和公平性之间的权衡关系，从而有助于更加明智地选择模型。

Conclusion: RISE作为一种新颖的交互式可视化方法，在面对领域偏移时提供了更深入细致的公平性评估手段，对于促进负责任的人工智能发展具有重要意义。

Abstract: Evaluating fairness under domain shift is challenging because scalar metrics often obscure exactly where and how disparities arise. We introduce \textit{RISE} (Residual Inspection through Sorted Evaluation), an interactive visualization tool that converts sorted residuals into interpretable patterns. By connecting residual curve structures to formal fairness notions, RISE enables localized disparity diagnosis, subgroup comparison across environments, and the detection of hidden fairness issues. Through post-hoc analysis, RISE exposes accuracy-fairness trade-offs that aggregate statistics miss, supporting more informed model selection.

</details>


### [57] [UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching](https://arxiv.org/abs/2602.04344)
*Kou Misaki,Takuya Akiba*

Main category: cs.LG

TL;DR: 本文提出了一种名为UnMaskFork (UMF)的框架，它通过将解码路径构建为搜索树并采用蒙特卡洛树搜索来优化生成路径，从而有效地利用了Masked Diffusion Language Models (MDLMs)的非自回归生成过程。UMF在复杂的编码基准测试中始终优于现有的测试时间缩放基线，并且在数学推理任务上也表现出良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于Masked Diffusion Language Models (MDLMs)具有迭代和非自回归生成过程，因此它们自然适合于先进的搜索策略。作者旨在通过设计一种新的框架来充分利用这一特性，以提高模型在推理时的表现，特别是在复杂编码和数学推理等任务上。

Method: 提出了UnMaskFork (UMF)框架，该框架把unmasking轨迹当作一个搜索树来处理，并使用蒙特卡洛树搜索方法来寻找最优的生成路径。与依赖随机抽样的标准缩放方法不同，UMF是通过多个MDLM执行确定性的部分解码动作来探索搜索空间。

Result: 实验评估表明，在复杂的编码基准测试中，UMF相比现有测试时间缩放基线表现更优；同时，在数学推理任务上也显示出了强大的可扩展性。

Conclusion: 本研究表明，通过利用MDLMs独特的非自回归性质以及引入如UMF这样的高级搜索策略，可以有效提升大型语言模型在需要额外计算资源进行推理时的表现。

Abstract: Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.

</details>


### [58] [MirrorLA: Reflecting Feature Map for Vision Linear Attention](https://arxiv.org/abs/2602.04346)
*Weikang Meng,Liangyu Huo,Yadan Luo,Yaowei Wang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为MirrorLA的新方法，通过利用可学习的Householder反射来旋转特征几何到非负象限，从而最大化信息保留。该方法在保持线性效率的同时，不牺牲表示保真度，并在标准基准测试中达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然显著降低了Transformer的计算复杂度，但其性能始终落后于基于softmax的注意力机制。研究发现，这种退化的原因在于对核特征映射施加了非负约束：像ReLU这样的标准投影作为“被动截断”操作符，无差别地丢弃了负域中的语义信息。

Method: 提出了一种名为MirrorLA的几何框架，该框架用主动重定向替代了被动截断。通过使用可学习的Householder反射，MirrorLA将特征几何旋转到非负象限以最大化信息保留。此方法首先通过块等距优化局部可分辨性，然后使用方差感知调制稳定长上下文动态以多样化激活，并最终通过跨头反射整合分散子空间，诱导全局协方差混合。

Result: MirrorLA在多个标准基准测试上实现了最先进的性能，证明了严格线性效率可以在不牺牲表示保真度的情况下实现。

Conclusion: MirrorLA提供了一个有效的方法来解决线性注意力模型中由于非负约束导致的信息丢失问题，通过引入主动重定向和一系列几何变换技术，在保持高效率的同时提升了模型的表现力。

Abstract: Linear attention significantly reduces the computational complexity of Transformers from quadratic to linear, yet it consistently lags behind softmax-based attention in performance. We identify the root cause of this degradation as the non-negativity constraint imposed on kernel feature maps: standard projections like ReLU act as "passive truncation" operators, indiscriminately discarding semantic information residing in the negative domain. We propose MirrorLA, a geometric framework that substitutes passive truncation with active reorientation. By leveraging learnable Householder reflections, MirrorLA rotates the feature geometry into the non-negative orthant to maximize information retention. Our approach restores representational density through a cohesive, multi-scale design: it first optimizes local discriminability via block-wise isometries, stabilizes long-context dynamics using variance-aware modulation to diversify activations, and finally, integrates dispersed subspaces via cross-head reflections to induce global covariance mixing. MirrorLA achieves state-of-the-art performance across standard benchmarks, demonstrating that strictly linear efficiency can be achieved without compromising representational fidelity.

</details>


### [59] [Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation](https://arxiv.org/abs/2602.04352)
*Sayan Biswas,Davide Frey,Romaric Gaudel,Nirupam Gupta,Anne-Marie Kermarrec,Dimitri Lerévérend,Rafael Pires,Rishi Sharma,François Taïani,Martijn de Vos*

Main category: cs.LG

TL;DR: Mosaic Learning, a new decentralized learning (DL) framework, improves upon traditional DL by fragmenting models and spreading them across the network, leading to better test accuracy and state-of-the-art convergence rates without additional communication costs.


<details>
  <summary>Details</summary>
Motivation: The motivation behind Mosaic Learning is to enhance decentralized learning by addressing the inefficiencies of redundant communication and enabling more diverse information propagation, while maintaining or improving the performance and efficiency of collaborative machine learning in settings where data cannot be centrally hosted.

Method: Mosaic Learning decomposes ML models into smaller fragments and disseminates these fragments independently throughout the network. This approach exploits parameter correlations within an ML model, theoretically improving the contraction rate by reducing the highest eigenvalue of a simplified system, and it achieves this without increasing the communication cost.

Result: Empirical evaluations on four different learning tasks demonstrated that Mosaic Learning can achieve up to 12 percentage points higher node-level test accuracy compared to epidemic learning, which is a state-of-the-art baseline method. The theoretical analysis also shows that Mosaic Learning has a state-of-the-art worst-case convergence rate.

Conclusion: Mosaic Learning sets a new standard for decentralized learning by offering improved performance and efficiency through its innovative model fragmentation and dissemination approach, making it a promising alternative for collaborative ML in distributed environments.

Abstract: Decentralized learning (DL) enables collaborative machine learning (ML) without a central server, making it suitable for settings where training data cannot be centrally hosted. We introduce Mosaic Learning, a DL framework that decomposes models into fragments and disseminates them independently across the network. Fragmentation reduces redundant communication across correlated parameters and enables more diverse information propagation without increasing communication cost. We theoretically show that Mosaic Learning (i) shows state-of-the-art worst-case convergence rate, and (ii) leverages parameter correlation in an ML model, improving contraction by reducing the highest eigenvalue of a simplified system. We empirically evaluate Mosaic Learning on four learning tasks and observe up to 12 percentage points higher node-level test accuracy compared to epidemic learning (EL), a state-of-the-art baseline. In summary, Mosaic Learning improves DL performance without sacrificing its utility or efficiency, and positions itself as a new DL standard.

</details>


### [60] [EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets](https://arxiv.org/abs/2602.04365)
*Jiapeng Liu,Liang Li,Bing Li,Peng Fu,Xiyan Gao,Chengyang Fang,Xiaoshuai Hao,Can Ma*

Main category: cs.LG

TL;DR: 本文提出了一种名为EXaMCaP的方法，通过最大化熵增来从图表理解数据集中选择子集，以评估多模态大型语言模型的能力提升。实验表明该方法在探测能力提升方面优于基线，并且对于不同大小的子集和多种模型架构都表现出良好的效果。


<details>
  <summary>Details</summary>
Motivation: 全集微调多模态大型语言模型（MLLMs）以评估图表理解（ChartU）训练集带来的能力提升会耗费大量时间，阻碍了ChartU数据集的迭代精炼过程。因此，需要一种有效的方法来选择能够代表整体的数据子集，以便更高效地评估模型的能力提升。

Method: 提出了EXaMCaP方法，它基于熵增最大化的原理来从大规模的ChartU数据集中挑选出一个高多样性的子集。由于枚举所有可能的子集不切实际，EXaMCaP采用迭代方式选取样本，使得相对于当前集合的熵增达到最大，从而近似于整个数据集的最大熵子集。

Result: 实验结果表明，EXaMCaP不仅在探测ChartU训练集给MLLMs带来的能力提升方面优于基线方法，而且对于不同大小的子集以及各种MLLM架构都非常有效。

Conclusion: EXaMCaP提供了一个有效策略，通过选择具有代表性的子集来快速评估MLLMs从ChartU训练集中获得的能力提升，促进了数据集的迭代改进过程。

Abstract: Recent works focus on synthesizing Chart Understanding (ChartU) training sets to inject advanced chart knowledge into Multimodal Large Language Models (MLLMs), where the sufficiency of the knowledge is typically verified by quantifying capability gains via the fine-tune-then-evaluate paradigm. However, full-set fine-tuning MLLMs to assess such gains incurs significant time costs, hindering the iterative refinement cycles of the ChartU dataset. Reviewing the ChartU dataset synthesis and data selection domains, we find that subsets can potentially probe the MLLMs' capability gains from full-set fine-tuning. Given that data diversity is vital for boosting MLLMs' performance and entropy reflects this feature, we propose EXaMCaP, which uses entropy gain maximization to select a subset. To obtain a high-diversity subset, EXaMCaP chooses the maximum-entropy subset from the large ChartU dataset. As enumerating all possible subsets is impractical, EXaMCaP iteratively selects samples to maximize the gain in set entropy relative to the current set, approximating the maximum-entropy subset of the full dataset. Experiments show that EXaMCaP outperforms baselines in probing the capability gains of the ChartU training set, along with its strong effectiveness across diverse subset sizes and compatibility with various MLLM architectures.

</details>


### [61] [Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis](https://arxiv.org/abs/2602.04369)
*Zongjiang Shang,Dongliang Cui,Binqing Wu,Ling Chen*

Main category: cs.LG

TL;DR: 提出了一种名为MSH-LLM的多尺度超图方法，通过增强时间序列的多尺度语义信息、引入跨模态对齐模块和混合提示机制来优化大型语言模型在时间序列分析中的应用，实验结果显示该方法在多个实际数据集上取得了最先进成果。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练大型语言模型（LLMs）在时间序列分析中虽然取得了一定的成功，但尚未充分利用其处理自然语言与时间序列之间多尺度结构的能力。

Method: 开发了MSH-LLM方法，包括设计用于加强时间序列语义空间多尺度信息的超边机制、实现不同尺度下自然语言与时间序列之间模态对齐的跨模态对齐(CMA)模块以及通过提供上下文信息来提高LLMs理解时间序列多尺度时间模式能力的混合提示(MoP)机制。

Result: 在5个不同应用场景下的27个真实世界数据集上的实验结果表明，MSH-LLM达到了最先进的性能表现。

Conclusion: 通过引入MSH-LLM方法增强了大型语言模型处理时间序列多尺度特性方面的能力，为相关领域提供了新的解决方案，并在广泛的应用场景中验证了其有效性。

Abstract: Recently, there has been great success in leveraging pre-trained large language models (LLMs) for time series analysis. The core idea lies in effectively aligning the modality between natural language and time series. However, the multi-scale structures of natural language and time series have not been fully considered, resulting in insufficient utilization of LLMs capabilities. To this end, we propose MSH-LLM, a Multi-Scale Hypergraph method that aligns Large Language Models for time series analysis. Specifically, a hyperedging mechanism is designed to enhance the multi-scale semantic information of time series semantic space. Then, a cross-modality alignment (CMA) module is introduced to align the modality between natural language and time series at different scales. In addition, a mixture of prompts (MoP) mechanism is introduced to provide contextual information and enhance the ability of LLMs to understand the multi-scale temporal patterns of time series. Experimental results on 27 real-world datasets across 5 different applications demonstrate that MSH-LLM achieves the state-of-the-art results.

</details>


### [62] [Reducing the labeling burden in time-series mapping using Common Ground: a semi-automated approach to tracking changes in land cover and species over time](https://arxiv.org/abs/2602.04373)
*Geethen Singh,Jasper A Slingsby,Tamara B Robinson,Glenn Moncrieff*

Main category: cs.LG

TL;DR: 该论文提出了一种名为'Common Ground'的方法，通过利用时间上稳定的区域作为动态区域的隐式监督源，从而在不需手动更新参考标签的情况下实现有效的时序泛化。这种方法在入侵树种制图中比直接应用单一时段训练模型提高了21-40%的分类准确率，并且相比黄金标准方法也有10-16%的提高。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据可靠分类依赖于一致且最新的参考标签，但收集每个时段的新标记数据既昂贵又难以操作，尤其是在动态或偏远的生态系统中。本文旨在解决这一挑战，提出了一种不需要在初始时间点t0之后手动更新参考标签即可达成有效时间泛化的解决方案。

Method: 本研究基于变化检测和半监督学习（SSL）的概念，提出了最高效的方法——“共同基础”，它采用半监督框架，利用时间上稳定不变的区域（这些区域在不同时间点之间光谱或语义特征几乎没有变化）作为对动态变化区域的一种隐含监督手段。

Result: 对于入侵树种制图，“共同基础”方法相较于单纯的时间迁移提高了21-40%的分类准确性；与黄金标准方法相比也观察到了10-16%更高的准确性。而在欧洲大范围土地覆盖类别绘制方面，则仅观察到约2%的准确性提升，相对而言改进幅度较小。

Conclusion: 结合稳定参考筛选与半监督学习以实现可扩展、标签高效的多时相遥感分类被证明是有效的。

Abstract: Reliable classification of Earth Observation data depends on consistent, up-to-date reference labels. However, collecting new labelled data at each time step remains expensive and logistically difficult, especially in dynamic or remote ecological systems. As a response to this challenge, we demonstrate that a model with access to reference data solely from time step t0 can perform competitively on both t0 and a future time step t1, outperforming models trained separately on time-specific reference data (the gold standard). This finding suggests that effective temporal generalization can be achieved without requiring manual updates to reference labels beyond the initial time step t0. Drawing on concepts from change detection and semi-supervised learning (SSL), the most performant approach, "Common Ground", uses a semi-supervised framework that leverages temporally stable regions-areas with little to no change in spectral or semantic characteristics between time steps-as a source of implicit supervision for dynamic regions. We evaluate this strategy across multiple classifiers, sensors (Landsat-8, Sentinel-2 satellite multispectral and airborne imaging spectroscopy), and ecological use cases. For invasive tree species mapping, we observed a 21-40% improvement in classification accuracy using Common Ground compared to naive temporal transfer, where models trained at a single time step are directly applied to a future time step. We also observe a 10 -16% higher accuracy for the introduced approach compared to a gold-standard approach. In contrast, when broad land cover categories were mapped across Europe, we observed a more modest 2% increase in accuracy compared to both the naive and gold-standard approaches. These results underscore the effectiveness of combining stable reference screening with SSL for scalable and label-efficient multi-temporal remote sensing classification.

</details>


### [63] [Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting](https://arxiv.org/abs/2602.04384)
*Fabio Turazza,Alessandro Neri,Marcello Pietri,Maria Angela Butturi,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 本研究探讨了联邦学习(FL)在可持续供应链管理(SSCM)中的应用，特别是在处理易腐商品的食品零售领域。通过基于区块链的FL模型，零售商可以在不直接共享数据的情况下协作训练模型，从而提高需求预测准确性、减少浪费并提高效率。


<details>
  <summary>Details</summary>
Motivation: 有效的市场需求预测对于减少食物浪费至关重要，但数据隐私问题往往阻碍零售商之间的合作，限制了预测准确性的改进潜力。

Method: 首先为孤立零售商情境下的需求预测和废物评估开发了一个基准预测模型。然后，引入了一种基于区块链的联邦学习(FL)模型，该模型可以在多个零售商之间协作训练，而无需直接共享数据。

Result: 初步结果显示，FL模型的表现几乎等同于各方互相分享数据的理想情况，并且明显优于没有数据共享时单个零售商建立的模型。这有助于减少浪费并提高效率。

Conclusion: 联邦学习提供了一种新的方法来改善可持续供应链管理中对易腐货物的需求预测，同时尊重各参与方的数据隐私。

Abstract: Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.

</details>


### [64] [LoRDO: Distributed Low-Rank Optimization with Infrequent Communication](https://arxiv.org/abs/2602.04396)
*Andrej Jovanović,Alex Iacob,Mher Safaryan,Ionut-Vlad Modoranu,Lorenzo Sani,William F. Shen,Xinchi Qiu,Dan Alistarh,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 本文提出了一种名为LoRDO的新框架，该框架结合了低秩优化与不频繁同步策略，旨在解决分布式训练中因互联带宽限制而产生的问题。通过引入全秩准双曲更新来恢复子空间探索，LoRDO在减少通信量的同时保持了与使用低秩DDP方法相近的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于DDP的分布式基础模型训练受到互联带宽的限制。尽管减少同步频率的方法可以缓解这个问题，但它们仍然受限于优化器状态对内存和通信的需求。虽然低秩优化器能够减轻这些约束，但在本地更新模式下，由于缺乏计算低秩投影所需的全批量梯度信息而导致性能下降。

Method: 提出了一个名为LoRDO的原则性框架，它将低秩优化与不频繁同步相结合。首先展示了基于伪梯度的全局投影理论上更优，但会永久地将优化轨迹限制在一个低秩子空间内。为了解决这个问题并恢复子空间探索能力，引入了一种全秩准双曲更新机制。

Result: LoRDO在语言建模及下游任务上达到了与低秩DDP近似的效果，同时对于125M至720M规模的模型减少了约10倍的通信需求。此外，在非常低内存设置（小秩/批次大小）的情况下，LoRDO表现得更好。

Conclusion: LoRDO提供了一个有效结合低秩优化与减少通信开销的解决方案，能够在维持模型性能的同时显著降低大规模分布式训练中的通信成本。

Abstract: Distributed training of foundation models via $\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate these constraints; however, in the local-update regime, workers lack access to the full-batch gradients required to compute low-rank projections, which degrades performance. We propose $\texttt{LoRDO}$, a principled framework unifying low-rank optimization with infrequent synchronization. We first demonstrate that, while global projections based on pseudo-gradients are theoretically superior, they permanently restrict the optimization trajectory to a low-rank subspace. To restore subspace exploration, we introduce a full-rank quasi-hyperbolic update. $\texttt{LoRDO}$ achieves near-parity with low-rank $\texttt{DDP}$ in language modeling and downstream tasks at model scales of $125$M--$720$M, while reducing communication by $\approx 10 \times$. Finally, we show that $\texttt{LoRDO}$ improves performance even more in very low-memory settings with small rank/batch size.

</details>


### [65] [Theory of Speciation Transitions in Diffusion Models with General Class Structure](https://arxiv.org/abs/2602.04404)
*Beatrice Achilli,Marco Benedetti,Giulio Biroli,Marc Mézard*

Main category: cs.LG

TL;DR: 本研究开发了一个适用于所有具有明确分类的目标分布的扩散模型的物种形成通用理论，通过贝叶斯分类形式化了类别结构的概念，并以自由熵差异来表征物种形成时间。该理论不仅涵盖了先前研究过的高斯混合模型，还扩展到了不能通过一阶矩区分而可能通过更高阶或集体特征区分的类别。此外，该框架预测了随着越来越细致的类别承诺而出现的连续物种形成时间。这些结果通过对一维伊辛模型在不同温度下的混合物和零均值但具有不同协方差结构的高斯混合物两个可解析的例子进行了说明。


<details>
  <summary>Details</summary>
Motivation: 现有的关于扩散模型中反向动力学的理论分析仅限于能够通过第一矩（如具有明显分离均值的高斯混合）识别类别的场景。本工作的动机是开发一个更广泛的、可以应用于任意目标分布的物种形成理论，即使在类别无法通过第一矩区分的情况下。

Method: 研究人员采用了一种新的方法来形式化类别结构，使用贝叶斯分类法，并且基于自由熵差来定义物种形成时间。他们还考虑了多个类别的存在，并预言了随着对类别承诺逐渐细化而发生的连续物种形成时间。为验证理论，他们选择了两个可解析案例：不同温度下一维伊辛模型的混合以及零均值但协方差结构不同的高斯模型的混合，并通过将问题映射到随机场伊辛模型并利用复制方法求解来获得显式表达式。

Result: 研究结果提供了一个统一且广泛适用的描述，用于解释基于扩散的生成模型中的物种形成转变。它不仅恢复了之前在高斯混合模型中已知的结果，而且还扩展到了那些类别之间不可以通过第一矩区分的情况，而是通过更高阶或集体特征区别的情况。

Conclusion: 本研究表明，提出的理论框架能有效地描述扩散模型中更普遍存在的物种形成现象，不仅局限于简单的高斯混合模型，而且对于那些通过更高阶特性区分的复杂类别也同样有效。这一发现为理解扩散模型背后的机制提供了新的视角。

Abstract: Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.

</details>


### [66] [Separation-Utility Pareto Frontier: An Information-Theoretic Characterization](https://arxiv.org/abs/2602.04408)
*Shizhou Xu*

Main category: cs.LG

TL;DR: 本研究探讨了效用与分离（一种公平性标准）之间的帕累托前沿，并通过信息论视角证明了其特性。基于理论分析，我们开发了一种基于条件互信息(CMI)的实证正则化方法，该方法可兼容任何通过梯度优化训练的深度模型。实验结果表明，所提出的方法在减少分离违规的同时，能够匹配或超越现有基线方法的效用。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索预测独立于敏感属性条件下真实结果的公平性标准——分离，以及它与模型效用之间的最优权衡。

Method: 采用信息论方法来表征效用-分离帕累托前沿，并证明了其凹性；提出了一个基于条件互信息(CMI)的实证正则项，用于监控和减少训练过程中的分离违规现象。

Result: 所提方法在COMPAS、UCI Adult、UCI Bank和CelebA等多个数据集上显著减少了分离违规情况，同时保持或提高了模型的效用。

Conclusion: 这项研究为深度学习中实施分离提供了一种可证明、稳定且灵活的方法。

Abstract: We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.

</details>


### [67] [Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning](https://arxiv.org/abs/2602.04491)
*Yuxi Guo,Paul Sheridan*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力头剪枝算法——贪婪梯度范数（Greedy-Gnorm），该算法在每次剪枝步骤后动态地重新计算注意力头的重要性，从而在大幅减少模型大小的同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的剪枝方法通常依赖于静态重要性评分，无法捕捉到在迭代移除过程中注意力头角色的变化。为了解决这一问题，并提高Transformer模型压缩的有效性和能源效率，提出了这种新方法。

Method: 通过从保留验证集中估计Q/K/V梯度块的l2-范数，并在每次贪心迭代时更新，来对每个注意力头进行评分。这种方法可以更好地反映随着剪枝进展而变化的基于梯度的重要性。

Result: 广泛的实验表明，与注意力熵相比，Greedy-Gnorm在BERT、ALBERT、RoBERTa和XLM-RoBERTa等模型上能够更一致地保持准确性，即使是在大量注意力头被移除的情况下。

Conclusion: Greedy-Gnorm提供了一种有希望的方法，用于实现更加节能的Transformer模型部署，因为它能在有效减少模型大小的同时维持良好的任务表现。

Abstract: Attention head pruning has emerged as an effective technique for transformer model compression, an increasingly important goal in the era of Green AI. However, existing pruning methods often rely on static importance scores, which fail to capture the evolving role of attention heads during iterative removal. We propose Greedy-Gradient norm (Greedy-Gnorm), a novel head pruning algorithm that dynamically recalculates head importance after each pruning step. Specifically, each head is scored by the elementwise product of the l2-norms of its Q/K/V gradient blocks, as estimated from a hold-out validation set and updated at every greedy iteration. This dynamic approach to scoring mitigates against stale rankings and better reflects gradient-informed importance as pruning progresses. Extensive experiments on BERT, ALBERT, RoBERTa, and XLM-RoBERTa demonstrate that Greedy-Gnorm consistently preserves accuracy under substantial head removal, outperforming attention entropy. By effectively reducing model size while maintaining task performance, Greedy-Gnorm offers a promising step toward more energy-efficient transformer model deployment.

</details>


### [68] [Gradient Flow Through Diagram Expansions: Learning Regimes and Explicit Solutions](https://arxiv.org/abs/2602.04548)
*Dmitry Yarotsky,Eugene Golikov,Yaroslav Gusev*

Main category: cs.LG

TL;DR: 本文开发了一个通用的数学框架来分析大规模学习问题中的梯度流（GF）的缩放机制，并通过类似费曼图的图表形式展开损失演化过程，从而揭示了不同的学习阶段并为某些情况提供了非线性GF的解析解。研究重点是高阶张量的典型多项式分解模型的学习过程，展示了多种极端懒惰和丰富GF状态。此外，提出了一种将形式化的损失展开转换为PDE的方法，在许多情况下可以通过特征线法求解。理论预测与实验结果一致。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解在大型学习问题中梯度流的行为，作者们旨在创建一个能够解释不同学习阶段的数学框架，并且希望能够找到特定条件下非线性梯度流的精确解。

Method: 研究人员开发了一种新的数学工具，即利用类似于费曼图的形式化幂级数扩展来表达损失函数的变化。他们还展示了如何将这种形式化的损失展开转化为偏微分方程（PDE），并在很多情况下使用特征线法进行求解。

Result: 研究表明，对于高阶张量的CP分解学习而言，存在几种不同的极限懒散和富足GF模式，包括自由演化、NTK以及欠参数化和过参数化平均场等。这些模式依赖于参数缩放、张量阶数及模型对称性。同时，通过将损失展开转换成PDE，发现该方法在广泛的场景下有效且可解。

Conclusion: 通过所提出的数学框架，可以更深入地了解大规模学习任务中梯度下降算法的行为，并能够预测不同条件下的学习效果。这不仅有助于理论上的理解，也为实际应用提供了一定指导意义。

Abstract: We develop a general mathematical framework to analyze scaling regimes and derive explicit analytic solutions for gradient flow (GF) in large learning problems. Our key innovation is a formal power series expansion of the loss evolution, with coefficients encoded by diagrams akin to Feynman diagrams. We show that this expansion has a well-defined large-size limit that can be used to reveal different learning phases and, in some cases, to obtain explicit solutions of the nonlinear GF. We focus on learning Canonical Polyadic (CP) decompositions of high-order tensors, and show that this model has several distinct extreme lazy and rich GF regimes such as free evolution, NTK and under- and over-parameterized mean-field. We show that these regimes depend on the parameter scaling, tensor order, and symmetry of the model in a specific and subtle way. Moreover, we propose a general approach to summing the formal loss expansion by reducing it to a PDE; in a wide range of scenarios, it turns out to be 1st order and solvable by the method of characteristics. We observe a very good agreement of our theoretical predictions with experiment.

</details>


### [69] [Finding Structure in Continual Learning](https://arxiv.org/abs/2602.04555)
*Pourya Shamsolmoali,Masoumeh Zareapoor*

Main category: cs.LG

TL;DR: 本文提出了一种基于Douglas-Rachford Splitting (DRS) 的持续学习目标重构方法，通过两个解耦目标的协商来实现新任务学习与旧知识稳定性的有效平衡，无需辅助模块或复杂附加组件。


<details>
  <summary>Details</summary>
Motivation: 传统的连续学习方法在处理从任务流中学习时遇到的主要问题是可塑性与稳定性之间的直接权衡，通常会导致对过去信息的灾难性遗忘。现有解决方法往往依赖于复杂的策略如外部记忆回放或参数正则化，这可能效率低下且不够优雅。

Method: 作者们引入了Douglas-Rachford Splitting (DRS) 技术来重新定义连续学习的目标函数。该方法将学习过程视为两个独立但相互作用的目标之间的协商：一个促进对新任务的学习能力（即塑料性），另一个确保旧知识的稳定性。通过迭代地利用各自的近端算子找到共识点，DRS提供了一种更原则性和稳定的动态学习机制。

Result: 所提出的方法能够在不依赖任何辅助模块或复杂插件的情况下，有效地达成稳定性和可塑性之间的平衡。实验结果表明，这种方法不仅简化了系统设计，还增强了连续学习系统的性能。

Conclusion: 基于Douglas-Rachford Splitting (DRS) 的连续学习框架为解决传统连续学习挑战提供了新的视角和工具，展示了在保持简单性的同时提升系统效能的可能性。

Abstract: Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.

</details>


### [70] [Probabilistic Label Spreading: Efficient and Consistent Estimation of Soft Labels with Epistemic Uncertainty on Graphs](https://arxiv.org/abs/2602.04574)
*Jonathan Klees,Tobias Riedlinger,Peter Stehr,Bennet Böddecker,Daniel Kondermann,Matthias Rottmann*

Main category: cs.LG

TL;DR: 本文提出了一种概率标签传播方法，可以在减少标注预算的情况下提高图像数据集的标注质量，并在以数据为中心的图像分类基准上达到了新的技术水平。


<details>
  <summary>Details</summary>
Motivation: 安全的人工智能感知任务仍然是一个重大挑战，部分原因是缺乏高质量标签的数据。注释本身存在随机性和知识性不确定性，但在注释和评估过程中通常忽略了这一点。尽管众包能够为每张图片收集多个注释来估计这些不确定性，但这种方法由于需要大量的注释工作而难以大规模实施。

Method: 本文引入了一种概率标签传播方法，它基于特征空间上的标签平滑假设，使用基于图的扩散方法传播单个注释，从而提供标签的随机性和知识性不确定性的可靠估计。作者证明了即使每个数据点的注释数量趋近于零时，标签传播也能产生一致的概率估计。

Result: 实验结果表明，与基线相比，该方法显著减少了达到所需标注质量所需的标注预算，并在以数据为中心的图像分类基准上取得了新的技术最佳水平。

Conclusion: 所提出的方法为解决人工智能感知任务中因标签质量不足而产生的问题提供了有效途径，同时大幅降低了人工标注成本。

Abstract: Safe artificial intelligence for perception tasks remains a major challenge, partly due to the lack of data with high-quality labels. Annotations themselves are subject to aleatoric and epistemic uncertainty, which is typically ignored during annotation and evaluation. While crowdsourcing enables collecting multiple annotations per image to estimate these uncertainties, this approach is impractical at scale due to the required annotation effort. We introduce a probabilistic label spreading method that provides reliable estimates of aleatoric and epistemic uncertainty of labels. Assuming label smoothness over the feature space, we propagate single annotations using a graph-based diffusion method. We prove that label spreading yields consistent probability estimators even when the number of annotations per data point converges to zero. We present and analyze a scalable implementation of our method. Experimental results indicate that, compared to baselines, our approach substantially reduces the annotation budget required to achieve a desired label quality on common image datasets and achieves a new state of the art on the Data-Centric Image Classification benchmark.

</details>


### [71] [Stochastic Decision Horizons for Constrained Reinforcement Learning](https://arxiv.org/abs/2602.04599)
*Nikola Milosevic,Leonard Franz,Daniel Haeufle,Georg Martius,Nico Scherf,Pavel Kolev*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机决策视野的控制即推理公式，通过减弱奖励贡献和缩短有效规划视野来处理约束违反问题。这种方法保持了离策略actor-critic学习的重播兼容性，并且实验表明它在标准基准测试上提高了样本效率并提供了有利的回报-违反权衡。此外，采用虚拟终止的MPO(VT-MPO)能够有效地扩展到高维度肌肉骨骼Hyfydy设置。


<details>
  <summary>Details</summary>
Motivation: 传统的CMDPs中使用加法成本约束和对偶变量的方法往往限制了离策略学习的可扩展性。因此，研究者们提出了一个新的基于随机决策视野的控制作为推断公式，旨在解决这一问题，提高强化学习中处理安全性和其他辅助目标时的效率与效果。

Method: 本文介绍了一种新的方法，该方法利用随机决策视野的概念，在约束被违反时减少奖励贡献并缩短有效规划视野。这种方法定义了两种违规语义：吸收型和虚拟终止型，虽然它们共享相同的生存加权回报，但导致不同的优化结构，从而实现SAC/MPO风格的政策改进。

Result: 实验结果展示了所提方法在标准基准测试中的样本效率有所提高，并且在回报-违反权衡方面表现良好。特别是，采用虚拟终止的MPO（VT-MPO）成功地应用于复杂的高维肌肉骨骼Hyfydy环境中。

Conclusion: 本研究表明，通过引入基于随机决策视野的控制即推理框架，可以有效提升CMDPs下强化学习算法处理约束的能力及其在复杂任务上的表现。

Abstract: Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.

</details>


### [72] [Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations](https://arxiv.org/abs/2602.04608)
*Maya Janvier,Julien Salomon,Etienne Meunier*

Main category: cs.LG

TL;DR: 本文提出了一种通过在训练期间正则化NDE模型的雅可比矩阵方向导数的方法，以稳定长周期积分。这种方法成本远低于长时间展开训练，并成功提高了多个常微分方程和偏微分方程长期仿真的稳定性。


<details>
  <summary>Details</summary>
Motivation: 混合模型和神经微分方程(NDE)对于物理系统的建模变得越来越重要，但它们经常遇到长期积分过程中的稳定性和准确性问题。虽然对展开轨迹进行训练可以限制这些发散现象，但由于需要在一个迭代过程中计算梯度，这很快就会变得过于昂贵。

Method: 作者设计了两种正则化方法来解决这个问题：一种是在已知动力学的情况下直接推导动力学的方向导数；另一种是在未知动力学情况下使用有限差分法近似方向导数。

Result: 这两种方法虽然在训练过程中的成本远低于长时间展开，但在提高多种常微分方程和偏微分方程长期模拟的稳定性方面是成功的。

Conclusion: 通过正则化NDE模型的雅可比矩阵方向导数，可以在保持低成本的同时有效提升长周期积分的稳定性，为大规模系统中NDE方法的长周期积分训练开辟了道路。

Abstract: Hybrid models and Neural Differential Equations (NDE) are getting increasingly important for the modeling of physical systems, however they often encounter stability and accuracy issues during long-term integration. Training on unrolled trajectories is known to limit these divergences but quickly becomes too expensive due to the need for computing gradients over an iterative process. In this paper, we demonstrate that regularizing the Jacobian of the NDE model via its directional derivatives during training stabilizes long-term integration in the challenging context of short training rollouts. We design two regularizations, one for the case of known dynamics where we can directly derive the directional derivatives of the dynamic and one for the case of unknown dynamics where they are approximated using finite differences. Both methods, while having a far lower cost compared to long rollouts during training, are successful in improving the stability of long-term simulations for several ordinary and partial differential equations, opening up the door to training NDE methods for long-term integration of large scale systems.

</details>


### [73] [Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting](https://arxiv.org/abs/2602.04609)
*Chenxi Hu,Yue Ma,Yifan Wu,Yunhe Hou*

Main category: cs.LG

TL;DR: 提出了一种名为AdaCNP的概率预测模型，用于在数据稀缺条件下提高极端天气期间电力系统负荷预测的准确性。通过学习共享嵌入空间中的相似性并重新加权历史信息，该模型能够有效适应罕见的极端模式，并提供更可靠的风险感知决策支持。实验表明，与最强基线相比，AdaCNP在极端时期能将均方误差降低22%，同时达到最低负对数似然值，表明其概率输出更加可信。


<details>
  <summary>Details</summary>
Motivation: 极端天气会显著改变用电行为，导致负载曲线出现剧烈波动。在此类情况下若预测不准确，则电力系统更容易遭遇供应短缺或局部过载问题，进而需要采取紧急措施如减载，并增加服务中断和公共安全风险。鉴于极端事件可能触发负荷模式中突然的制度转变，而相关极端样本又稀少且不规律，因此开发出一种能够在数据稀缺条件下仍能保持高精度预测的方法变得尤为重要。

Method: 提出了AdaCNP，一种专为数据稀缺条件设计的概率预测模型。该方法通过在一个共享嵌入空间里学习相似度来工作；对于每个目标数据点，它评估每个历史上下文片段与当前状况的相关程度，并据此重新调整上下文信息的权重。这样的设计即使在极端样本很少的情况下也能突出最具有信息量的历史证据，并允许少量样本即实现对之前未见过的极端模式的适应。此外，AdaCNP无需针对目标领域进行昂贵的微调就能生成预测分布，从而支持风险意识决策。

Result: 通过对真实世界电力系统负荷数据进行测试并与一系列代表性基线比较后发现，AdaCNP在极端时期表现得更为稳健，相较于最强基线平均减少了22%的均方误差，并且达到了最低的负对数似然值，这说明了其概率输出结果更加可靠。

Conclusion: 研究结果表明，AdaCNP能够有效地缓解突发分布变化与极端样本稀缺所带来的综合影响，为极端事件下的弹性电力系统运行提供了更加值得信赖的预测。

Abstract: Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.

</details>


### [74] [QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning](https://arxiv.org/abs/2602.04620)
*Doyeon Lee,Eunyi Lyou,Hyunsoo Cho,Sookyung Kim,Joonseok Lee,Jaemoo Choi*

Main category: cs.LG

TL;DR: 本文提出了一种名为QUATRO的新方法，该方法通过直接实施信任区域约束来改进基于GRPO风格的强化学习的LLM微调算法，从而实现更稳定和可控制的策略更新。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GRPO风格的强化学习的LLM微调算法依赖于启发式信任区域近似，这可能导致优化行为不稳定，因为全局重要性比率裁剪和分组归一化无法调节那些重要性比率超出裁剪范围的样本。

Method: 提出了Query-Adaptive Trust-Region Policy Optimization (QUATRO)，它通过一个原则性的优化过程直接实施信任区域约束，提供了一个清晰且可解释的目标函数，允许对策略更新进行显式的控制，并支持稳定的、熵控的优化。

Result: 在不同的数学推理基准测试中验证了QUATRO，在增加的策略陈旧性和激进的学习率下仍能保持稳定的训练过程，并在整个训练过程中维持良好的熵控制。

Conclusion: QUATRO为基于GRPO风格的强化学习提供了更加稳健和可控的方法来进行LLM微调，特别是在面对策略陈旧性和高学习率挑战时表现出了优越性。

Abstract: GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.

</details>


### [75] [MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction](https://arxiv.org/abs/2602.04643)
*Yanan He,Yunshi Wen,Xin Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: 本文提出了一种新的架构MTS-JEPA，用于解决JEPA在多变量时间序列异常预测中的局限性。通过结合多分辨率预测目标和软码本瓶颈，该方法能够区分瞬态冲击与长期趋势，并捕捉离散状态转换，从而实现早期预警协议下的最佳性能。


<details>
  <summary>Details</summary>
Motivation: JEPA虽然为建模这些系统的潜在演变提供了一个有希望的框架，但其应用受到表示崩溃的影响，并且无法跨不同时间尺度捕获前兆信号。

Method: 提出了MTS-JEPA架构，它整合了多分辨率预测目标与软码本瓶颈的设计，明确地将瞬态冲击与长期趋势解耦，并利用码本来捕捉离散的状态转换。

Result: 实证评估表明，所提出的方法能够有效防止退化解，并在早期预警协议下达到最先进水平的表现。

Conclusion: 通过引入MTS-JEPA，成功解决了JEPA在处理多变量时间序列时遇到的问题，特别是在识别异常情况方面表现优异。

Abstract: Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.

</details>


### [76] [SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF](https://arxiv.org/abs/2602.04651)
*Dipan Maity*

Main category: cs.LG

TL;DR: 本文提出了一种新的RLHF算法SAFE，它结合了悲观价值估计与多层稳定框架，包括熵门控KL调节和PID控制的自适应阈值。实验表明，SAFE相比PPO在训练平均奖励上提高了5.15%，并且具有更稳定的KL控制和较少的奖励崩溃情况。


<details>
  <summary>Details</summary>
Motivation: 当前用于语言模型强化学习微调（LM-RLHF）的方法PPO虽然实证表现良好，但其动机基于启发式，处理KL散度约束时采用的是临时方法，并且存在奖励振荡、熵塌陷、价值函数漂移以及突然策略分歧等问题，需要频繁重启和广泛的超参数调整。

Method: 提出了名为SAFE的新RLHF算法，该算法利用双重软最小值批评家进行悲观价值估计，并引入了一个新的多层次稳定框架，此框架结合了基于熵门控的KL调节及PID控制的自适应阈值机制。不同于标准PPO对称性的KL惩罚方式，SAFE能够区分高熵探索与低熵模式崩溃，并根据奖励速度动态调整惩罚力度。

Result: 在30亿参数规模的模型上进行实验的结果显示，SAFE比PPO获得了高出5.15%的训练平均奖励(0.725对比0.689)，同时几乎不存在奖励崩溃现象，在KL控制方面也优于PPO。此外，本方法计算开销小，提供了一个可解释性强、抗崩溃的RLHF框架，能够在保持快速学习的同时确保长期优化的稳定性，适合生产环境部署。

Conclusion: 通过引入SAFE算法，研究者们为LM-RLHF提供了一个更加稳定且高效的解决方案，不仅改善了训练过程中的各种问题如奖励不稳定性和熵塌陷等，还增强了最终模型的质量和实用性。

Abstract: Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE

</details>


### [77] [Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design](https://arxiv.org/abs/2602.04663)
*Jaemoo Choi,Yuchen Zhu,Wei Guo,Petr Molodyk,Bo Yuan,Jinbin Bai,Yi Xin,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 本文系统地分析了强化学习设计空间中的三个因素：策略梯度目标、似然估计器和rollout采样方案，发现基于最终生成样本计算的证据下界(ELBO)模型似然估计器是实现有效、高效且稳定的RL优化的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中在构建新的目标上，这些目标基于已经高度工程化的LLM目标，并使用临时的似然估计器，而没有彻底调查这种估计如何影响整体算法性能。因此，研究者们希望提供一个关于RL设计空间的系统性分析，以找出更有效的解决方案。

Method: 通过解耦策略梯度目标、似然估计器和rollout采样方案这三个因素来进行系统性分析。特别强调采用基于最终生成样本计算的证据下界(ELBO)模型似然估计器的方法。

Result: 该方法在多个奖励基准测试中得到了验证，并观察到了所有任务的一致趋势。GenEval得分从0.24提高到了0.95，耗时仅为90 GPU小时，效率比FlowGRPO高出4.6倍，比当前最先进的DiffusionNFT方法高出2倍，而且没有出现奖励黑客攻击的情况。

Conclusion: 采用基于最终生成样本计算的证据下界(ELBO)模型似然估计器是实现有效、高效且稳定的强化学习优化的主要因素，其重要性超过了特定策略梯度损失函数的影响。

Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.

</details>


### [78] [Delving into Muon and Beyond: Deep Analysis and Extensions](https://arxiv.org/abs/2602.04669)
*Xianbiao Qi,Marco Chen,Jiaquan Ye,Yelin He,Rong Xiao*

Main category: cs.LG

TL;DR: 本文通过统一的谱视角探讨了Muon优化器的工作机制及其与Adam等自适应优化器的关系，发现RMS归一化更新比动量更新更稳定，而谱压缩在动量更新中提供了显著的稳定优势。不过，Muon（p=0）并不总是优于Adam，表明Muon可以被看作是一种有效的谱归一化形式，但不是普遍优越的优化方法。


<details>
  <summary>Details</summary>
Motivation: Muon优化器由于其强大的实际性能和对矩阵形状参数的正交化更新引起了广泛关注，但其内在机制以及与Adam等自适应优化器之间的关系仍不够清晰。

Method: 作者从统一的谱角度出发，将Muon视为一系列形如U oldsymbolΣ^{p} V' 的谱变换中的p = 0端点，并探索了当p取1/2、1/4及1时的变化情况。这些变换既应用于类似动量SGD的第一矩更新，也应用于像Adam那样的RMS归一化梯度更新。为了提高计算效率，开发了一种耦合牛顿迭代法以避免显式的奇异值分解。

Result: 实验结果表明，RMS归一化更新相比第一矩更新能够提供更加稳定的优化过程。此外，在使用第一矩更新时，谱压缩显示出很好的稳定效果；然而，对于Muon更新（即p=0的情况），它并没有一致地超越Adam的表现。

Conclusion: 研究指出，虽然Muon可以被视为一种有效的谱归一化形式，但它并不总是优于其他优化方法如Adam。

Abstract: The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we aim to address these questions through a unified spectral perspective. Specifically, we view Muon as the p = 0 endpoint of a family of spectral transformations of the form U \boldsymbolΣ^{p} V' , and consider additional variants with p = 1/2 , p = 1/4 , and p = 1 . These transformations are applied to both first-moment updates, as in momentum SGD, and to root-mean-square (RMS) normalized gradient updates as in Adam. To enable efficient computation, we develop a coupled Newton iteration that avoids explicit singular value decomposition. Across controlled experiments, we find that RMS-normalized updates yield more stable optimization than first-moment updates. Moreover, while spectral compression provides strong stabilization benefits under first-moment updates, the Muon update (p = 0) does not consistently outperform Adam. These results suggest that Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method. Our source code will be released at https://github.com/Ocram7/BeyondMuon.

</details>


### [79] [Generalized Schrödinger Bridge on Graphs](https://arxiv.org/abs/2602.04675)
*Panagiotis Theodoropoulos,Juno Nam,Evangelos Theodorou,Jaemoo Choi*

Main category: cs.LG

TL;DR: 本文提出了一种新的可扩展的数据驱动框架——广义图上薛定谔桥(GSBoG)，用于在任意图上学习可执行的受控连续时间马尔可夫链(CTMC)策略，解决了现有方法在稀疏拓扑、图大小和时间范围上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的图-运输方法存在表达力不足的问题，依赖于限制性的假设，难以在稀疏拓扑中泛化，并且随着图的大小和时间范围增加而难以扩展。

Method: 通过引入广义图上薛定谔桥（GSBoG），一种新颖的可扩展数据驱动框架，该框架能够在状态成本增强的动力学下，在任意图上学到可执行的受控连续时间马尔可夫链(CTMC)策略。GSBoG通过似然优化方法实现，满足端点边际条件的同时优化了考虑状态依赖运行成本的中间行为。

Result: 广泛的实验表明，GSBoG能够可靠地学习准确且符合拓扑结构的策略，同时优化应用程序特定的中间状态成本，展示了其广泛的应用性和为一般图上的成本感知动态传输开辟了新途径。

Conclusion: GSBoG提供了一个有效解决图上传输问题的新方法，特别适用于需要考虑实际应用中成本因素的情况，具有很好的可扩展性和实用性。

Abstract: Transportation on graphs is a fundamental challenge across many domains, where decisions must respect topological and operational constraints. Despite the need for actionable policies, existing graph-transport methods lack this expressivity. They rely on restrictive assumptions, fail to generalize across sparse topologies, and scale poorly with graph size and time horizon. To address these issues, we introduce Generalized Schrödinger Bridge on Graphs (GSBoG), a novel scalable data-driven framework for learning executable controlled continuous-time Markov chain (CTMC) policies on arbitrary graphs under state cost augmented dynamics. Notably, GSBoG learns trajectory-level policies, avoiding dense global solvers and thereby enhancing scalability. This is achieved via a likelihood optimization approach, satisfying the endpoint marginals, while simultaneously optimizing intermediate behavior under state-dependent running costs. Extensive experimentation on challenging real-world graph topologies shows that GSBoG reliably learns accurate, topology-respecting policies while optimizing application-specific intermediate state costs, highlighting its broad applicability and paving new avenues for cost-aware dynamical transport on general graphs.

</details>


### [80] [REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency](https://arxiv.org/abs/2602.04677)
*Ondrej Tybl,Lukas Neumann*

Main category: cs.LG

TL;DR: 本文提出了一种基于鲁棒统计学的新型知识蒸馏框架REDistill，通过采用幂散度损失代替传统的KL散度来适应性地降低不可靠教师输出的影响，同时保持有用的信息。该方法不需要额外的超参数调整，在CIFAR-100和ImageNet-1k数据集上实验表明它可以提高不同架构下学生模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法依赖于假设教师模型提供可靠软目标的前提下进行，但在实践中教师模型预测往往存在噪声或过度自信的问题。现有基于修正的方法依赖于特定情境下的启发式规则及大量的超参数调优，这限制了其泛化能力。

Method: REDistill框架利用幂散度损失替代标准的知识蒸馏目标函数，该损失函数是KL散度的一种推广形式，能够自适应地减轻不可靠教师输出的影响，同时保留有信息量的logits关系。这种方法仅需要logits作为输入，可以无缝集成到现有的知识蒸馏流程中，并且几乎不增加计算开销。

Result: 在CIFAR-100和ImageNet-1k上的广泛实验显示，REDistill能够持续提升多种教师-学生架构配置下的学生模型准确率。特别值得注意的是，这一改进是在没有针对具体模型进行超参数调整的情况下实现的，证明了REDistill具有强大的鲁棒性和对未见过的教师-学生组合的良好泛化性能。

Conclusion: REDistill为处理教师模型中的噪音问题提供了统一且可解释的方法论，展示了其在改善学生模型性能方面的有效性与高效性，以及无需特定超参调整即可良好工作的特性。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.

</details>


### [81] [Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting](https://arxiv.org/abs/2602.04678)
*Zhen Zhou,Zhirui Wang,Qi Hong,Yunyang Shi,Ziyuan Gu,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 提出了一种新的多专家学习分布标签（LDL）框架，通过具有分布学习能力的混合专家架构解决时间序列预测中的准确性与不确定性量化问题。该方法包括多专家LDL和模式感知LDL-MoE两种互补方法，前者利用多个参数不同的专家捕捉多样化的时间模式，后者则通过专门子专家将时间序列分解为可解释的组成部分。两种框架都扩展了传统的点预测到分布学习，并使用最大均值差异（MMD）进行丰富的不确定性量化。在M5数据集衍生的汇总销售数据上评估表明，连续多专家LDL总体表现最佳，而模式感知LDL-MoE通过组件分析提供了增强的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用中的时间序列预测不仅需要高预测精度，还需要可解释的不确定性量化。传统点预测方法往往无法捕捉时间序列数据中的内在不确定性，而现有的概率方法在计算效率与可解释性之间难以取得平衡。

Method: 提出了一种名为多专家学习分布标签（LDL）的新框架，该框架基于具备分布学习能力的混合专家架构。具体包含了两种补充方法：一是多专家LDL，它采用多个带有不同学习参数的专家来捕捉多样的时间模式；二是模式感知LDL-MoE，通过专业化的子专家明确地将时间序列分解成可解释的部分（趋势、季节性、变化点、波动性）。

Result: 在从M5数据集中提取的汇总销售数据上对所提方法进行了评估，结果显示相较于基线方法，新方法表现出色。连续多专家LDL在整体性能上达到最优，而模式感知LDL-MoE则通过对各部分的分析增强了结果的可解释性。

Conclusion: 提出的框架成功地在预测准确性和可解释性之间取得了平衡，使其非常适合于那些既重视性能又重视可操作见解的实际预测应用场景。

Abstract: Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance computational efficiency with interpretability. We propose a novel Multi-Expert Learning Distributional Labels (LDL) framework that addresses these challenges through mixture-of-experts architectures with distributional learning capabilities. Our approach introduces two complementary methods: (1) Multi-Expert LDL, which employs multiple experts with different learned parameters to capture diverse temporal patterns, and (2) Pattern-Aware LDL-MoE, which explicitly decomposes time series into interpretable components (trend, seasonality, changepoints, volatility) through specialized sub-experts. Both frameworks extend traditional point prediction to distributional learning, enabling rich uncertainty quantification through Maximum Mean Discrepancy (MMD). We evaluate our methods on aggregated sales data derived from the M5 dataset, demonstrating superior performance compared to baseline approaches. The continuous Multi-Expert LDL achieves the best overall performance, while the Pattern-Aware LDL-MoE provides enhanced interpretability through component-wise analysis. Our frameworks successfully balance predictive accuracy with interpretability, making them suitable for real-world forecasting applications where both performance and actionable insights are crucial.

</details>


### [82] [Static and auto-regressive neural emulation of phytoplankton biomass dynamics from physical predictors in the global ocean](https://arxiv.org/abs/2602.04689)
*Mahima Lakra,Ronan Fablet,Lucas Drumetz,Etienne Pauthenet,Elodie Martinez*

Main category: cs.LG

TL;DR: 本研究探索了深度学习模型在基于卫星观测和环境条件预测全球海洋中浮游植物生物量时空分布方面的应用。U-Net架构在再现浮游植物生物量的季节性和年际变化模式方面优于其他测试模型，且结合自回归版本可以有效进行短期（最多五个月）预测。


<details>
  <summary>Details</summary>
Motivation: 由于参数化有限、观测数据稀疏以及海洋过程的复杂性，准确模拟浮游植物动态对于生物地球化学数值模型来说仍然是一个重大挑战。研究旨在通过使用深度学习方法来克服这些限制，以提高对浮游植物生物量分布的预测准确性。

Method: 首先比较了几种不同的深度学习架构，发现U-Net在再现浮游植物生物量的季节性和年际模式上表现最佳。随后，采用了一到两个月的环境数据作为输入，并尝试了U-Net的一个自回归版本用于改进长期预测。

Result: U-Net架构能够更准确地再现浮游植物生物量的变化模式，尽管它倾向于低估低频变化的幅度。自回归版本的U-Net对于短期（最多五个月）预测效果良好。

Conclusion: 结合物理海洋预测因子与深度学习技术可用于重建及短期预测浮游植物动态，这为监测海洋健康和支持海洋生态系统管理提供了强有力的工具，尤其是在气候变化背景下。

Abstract: Phytoplankton is the basis of marine food webs, driving both ecological processes and global biogeochemical cycles. Despite their ecological and climatic significance, accurately simulating phytoplankton dynamics remains a major challenge for biogeochemical numerical models due to limited parameterizations, sparse observational data, and the complexity of oceanic processes. Here, we explore how deep learning models can be used to address these limitations predicting the spatio-temporal distribution of phytoplankton biomass in the global ocean based on satellite observations and environmental conditions. First, we investigate several deep learning architectures. Among the tested models, the UNet architecture stands out for its ability to reproduce the seasonal and interannual patterns of phytoplankton biomass more accurately than other models like CNNs, ConvLSTM, and 4CastNet. When using one to two months of environmental data as input, UNet performs better, although it tends to underestimate the amplitude of low-frequency changes in phytoplankton biomass. Thus, to improve predictions over time, an auto-regressive version of UNet was also tested, where the model uses its own previous predictions to forecast future conditions. This approach works well for short-term forecasts (up to five months), though its performance decreases for longer time scales. Overall, our study shows that combining ocean physical predictors with deep learning allows for reconstruction and short-term prediction of phytoplankton dynamics. These models could become powerful tools for monitoring ocean health and supporting marine ecosystem management, especially in the context of climate change.

</details>


### [83] [Bounded-Abstention Multi-horizon Time-series Forecasting](https://arxiv.org/abs/2602.04714)
*Luca Stradiotti,Laurens Devos,Anna Monreale,Jesse Davis,Andrea Pugnana*

Main category: cs.LG

TL;DR: 本文针对多时间范围预测中模型可能做出错误预测的问题，提出了学习弃权框架的改进方法，专门适应连续多个时间点的预测场景。通过定义三种弃权方式，并为每种情况设计最优策略及相应算法，实验证明该方法在24个数据集上优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 多时间范围的时间序列预测要求同时对一系列连续时间点进行预测，这在医疗保健和金融等领域尤为重要，因为错误预测可能导致高昂的成本并降低信任度。传统的学习弃权框架允许模型在高风险误报情况下选择不提供预测，但这些策略并不适用于需要连续预测的情况，忽略了预测之间的结构相关性。

Method: 作者首先正式定义了多时间范围预测中的学习弃权问题，并指出其结构化特性引入了一系列新的弃权挑战。基于此，文章提出了三种自然的模型弃权方式来适应多时间范围预测的需求。接着，对于提出的每一种弃权类型，进行了理论分析以确定最佳弃权策略，并开发了实现这些策略的具体算法。

Result: 通过对24个不同数据集的广泛测试表明，所提出的方法相比现有的基线方法具有显著更好的性能表现。

Conclusion: 本研究成功地将学习弃权框架扩展到了多时间范围预测领域，不仅定义了适合该场景下的新型弃权问题，还为此类问题提供了有效的解决方案。实验结果证明了所提方法的有效性和优越性。

Abstract: Multi-horizon time-series forecasting involves simultaneously making predictions for a consecutive sequence of subsequent time steps. This task arises in many application domains, such as healthcare and finance, where mispredictions can have a high cost and reduce trust. The learning with abstention framework tackles these problems by allowing a model to abstain from offering a prediction when it is at an elevated risk of making a misprediction. Unfortunately, existing abstention strategies are ill-suited for the multi-horizon setting: they target problems where a model offers a single prediction for each instance. Hence, they ignore the structured and correlated nature of the predictions offered by a multi-horizon forecaster. We formalize the problem of learning with abstention for multi-horizon forecasting setting and show that its structured nature admits a richer set of abstention problems. Concretely, we propose three natural notions of how a model could abstain for multi-horizon forecasting. We theoretically analyze each problem to derive the optimal abstention strategy and propose an algorithm that implements it. Extensive evaluation on 24 datasets shows that our proposed algorithms significantly outperforms existing baselines.

</details>


### [84] [Identifying Intervenable and Interpretable Features via Orthogonality Regularization](https://arxiv.org/abs/2602.04718)
*Moritz Miller,Florent Draye,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 该论文通过在固定稀疏自动编码器周围微调语言模型，解耦了几乎正交的特征，减少了特征间的干扰和叠加，并保持了目标数据集上的性能不变。进一步研究发现，随着正交性惩罚更加严格，嵌入特征解释之间的距离增加，这有利于可解释性。基于'独立因果机制'原则，研究表明正交性促进了易于因果干预的模块化表示，并且这些越来越正交化的特征允许进行孤立干预。


<details>
  <summary>Details</summary>
Motivation: 旨在通过增强语言模型中特征的正交性来提高模型的可解释性和模块化程度，从而使得模型内部的表示更易于理解与控制。

Method: 采用了对语言模型中的解码器矩阵施加正交性惩罚的方法，以此来分离出几乎正交的特征。同时，依据独立因果机制的原则，探索了正交性如何促进适合因果干预的模块化表示。

Result: 结果显示，即使在提高了特征间正交性的条件下，模型在目标数据集上的表现依然保持稳定；并且，随着正交性要求的提升，不同特征之间变得更加独立，为模型提供了更好的可解释性基础。此外，实验证明了这些更加正交化的特征确实能够支持孤立的干预操作。

Conclusion: 通过引入正交性惩罚，不仅能够减少特征间的相互干扰，而且还能保持模型性能的同时显著提升其可解释性。这种方法为构建更加透明、可控的语言模型提供了一种新思路。

Abstract: With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\texttt{https://github.com/mrtzmllr/sae-icm}$.

</details>


### [85] [Benchmarking and Enhancing PPG-Based Cuffless Blood Pressure Estimation Methods](https://arxiv.org/abs/2602.04725)
*Neville Mathew,Yidan Shen,Renjie Hu,Maham Rahimi,George Zouridakis*

Main category: cs.LG

TL;DR: 本研究创建了一个标准化的基准子集NBPDB，用于在生理受控条件下对基于PPG的血压估计模型进行公平评估。结果表明，现有模型未达到AAMI/ISO 81060-2精度要求；但通过加入人口统计学数据（如年龄、性别和体重指数）作为额外输入后，所有模型的表现均有所改善，特别是MInception模型，在添加了这些数据后误差减少了23%，达到了接近AAMI/ISO标准定义的数值限制的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于光电容积脉搏波(PPG)信号的无袖带血压筛查为大规模心血管健康评估提供了一条实用途径，但现有的基于PPG的血压估计模型尚未一致达到既定的临床数值限制（如AAMI/ISO 81060-2标准）。此外，先前的评估通常缺乏严格的实验控制，且常用的公开数据集存在异质性，缺乏生理控制条件下的公正基准测试。

Method: 研究人员创建了一个名为NBPDB的标准基准子集，包含从MIMIC-III和VitalDB中提取的1,103名健康成年人的101,453个高质量PPG片段。使用此数据集系统地基准测试了几个最先进的基于PPG的模型，并通过对这些模型进行修改并添加患者的人口统计数据（如年龄、性别及体质指数）作为额外输入来提高模型准确性。

Result: 测试结果显示，没有任何一个被评估模型能够满足AAMI/ISO 81060-2的精度要求（平均误差<5mmHg且标准差<8mmHg）。然而，经过改进后的模型性能普遍得到提升，尤其是MInception模型在加入人口统计信息后误差减少了23%，其收缩压(SBP)与舒张压(DBP)的平均绝对误差分别为4.75mmHg和2.90mmHg，达到了接近AAMI/ISO标准定义的数值限制的准确性水平。

Conclusion: 现有的基于PPG的血压估计模型在标准化条件下缺乏临床实用性；而结合人口统计学信息显著提高了它们的准确性和生理有效性。

Abstract: Cuffless blood pressure screening based on easily acquired photoplethysmography (PPG) signals offers a practical pathway toward scalable cardiovascular health assessment. Despite rapid progress, existing PPG-based blood pressure estimation models have not consistently achieved the established clinical numerical limits such as AAMI/ISO 81060-2, and prior evaluations often lack the rigorous experimental controls necessary for valid clinical assessment. Moreover, the publicly available datasets commonly used are heterogeneous and lack physiologically controlled conditions for fair benchmarking. To enable fair benchmarking under physiologically controlled conditions, we created a standardized benchmarking subset NBPDB comprising 101,453 high-quality PPG segments from 1,103 healthy adults, derived from MIMIC-III and VitalDB. Using this dataset, we systematically benchmarked several state-of-the-art PPG-based models. The results showed that none of the evaluated models met the AAMI/ISO 81060-2 accuracy requirements (mean error $<$ 5 mmHg and standard deviation $<$ 8 mmHg). To improve model accuracy, we modified these models and added patient demographic data such as age, sex, and body mass index as additional inputs. Our modifications consistently improved performance across all models. In particular, the MInception model reduced error by 23\% after adding the demographic data and yielded mean absolute errors of 4.75 mmHg (SBP) and 2.90 mmHg (DBP), achieves accuracy comparable to the numerical limits defined by AAMI/ISO accuracy standards. Our results show that existing PPG-based BP estimation models lack clinical practicality under standardized conditions, while incorporating demographic information markedly improves their accuracy and physiological validity.

</details>


### [86] [Decomposing Query-Key Feature Interactions Using Contrastive Covariances](https://arxiv.org/abs/2602.04752)
*Andrew Lee,Yonatan Belinkov,Fernanda Viégas,Martin Wattenberg*

Main category: cs.LG

TL;DR: 研究人员提出了一种对比协方差方法，用于将查询-键（QK）空间分解为低秩、人类可解释的组件，从而理解Transformer模型中注意力头为何关注特定令牌。该方法在简化设置下进行了分析和实证研究，并应用于大型语言模型来识别类别语义特征和绑定特征的人类可解释QK子空间。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力头在Transformer中扮演着核心角色，但我们缺乏工具来理解模型为什么会关注某个特定的令牌。为了解决这个问题，本研究聚焦于查询-键(QK)空间——即查询与键之间的双线性联合嵌入空间。

Method: 提出了一种对比协方差方法，用以将QK空间分解成低秩且对人类来说易于理解的部分。当键和查询中的特征在这些低秩子空间内对齐时，就会产生较高的注意力分数。首先，在一个简化的场景下对该方法进行了理论分析和实验验证；随后，将此方法应用于大规模语言模型，以发现与分类语义特征及结合特征相关的、人类可以理解的QK子空间。

Result: 通过所提出的方法成功地在大规模语言模型中识别出了一些人类可理解的QK子空间，它们对应着特定类型的语义信息或特征绑定情况。此外，还展示了如何根据已识别的特征来归因注意力得分。

Conclusion: 本研究表明，通过对QK空间进行结构化处理，可以揭示Transformer模型内部运作机制，特别是关于注意力分配背后的逻辑。这不仅增进了我们对于现有模型的理解，也为未来开发更透明、可控的语言模型提供了新的视角。

Abstract: Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.

</details>


### [87] [A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates](https://arxiv.org/abs/2602.04757)
*Yuchen Ye,Zixuan Qi,Shixuan Li,Wei Qi,Yanpeng Cai,Chaoxia Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种基于TransUNet的双阶段多源降水融合框架(DDL-MSPMF)，通过集成六个多源降水产品和四个ERA5近地表物理预测因子，以0.25度分辨率估计中国2001-2020年期间的日降水量。该方法在季节性表现上优于多种深度学习和混合基线，并提高了对极端降水事件的检测能力，特别是在数据稀缺地区同样适用。


<details>
  <summary>Details</summary>
Motivation: 现有的多源降水产品（MSPs）由于空间异质偏差以及对极端事件技能有限，限制了其在水文领域的应用价值。为解决这些问题并提高降水数据的可用性和准确性，特别是针对极端降水事件的监测能力，提出了新的多源降水融合框架。

Method: 采用一种名为DDL-MSPMF的双阶段TransUNet基础架构，结合六个MSPs与四个ERA5近地表物理预测变量。第一阶段分类器估算日降水发生概率；第二阶段回归器将分类器输出与所有预测变量融合，以0.25度的空间分辨率对中国2001至2020年的每日降水量进行估计。

Result: 与多个深度学习及混合基线相比，TransUNet-TransUNet配置在季节性表现方面最优（R=0.75; RMSE=2.70 mm/天），并且相对于单回归器设置增强了鲁棒性。对于强降水（>25毫米/天），DDL-MSPMF在中国东部大部分地区提高了公平威胁评分，并更好地再现了2021年7月郑州暴雨的空间格局。青藏高原上的独立评估进一步支持了其在数据稀缺地区的适用性。

Conclusion: 所提出的DDL-MSPMF框架提供了一种可扩展且可解释的方法来融合降水信息并对极端事件进行评估，显示出改进的极端事件检测能力，同时保持了季节平均校正的优势。

Abstract: Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.

</details>


### [88] [Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations](https://arxiv.org/abs/2602.04761)
*Hang Yu,Yu-Hu Yan,Peng Zhao*

Main category: cs.LG

TL;DR: 本文研究了带状凸优化中基于梯度变化的在线学习，并提出了对非连续梯度变化的改进分析，从而在凸函数和强凸函数上都提高了维度依赖性。此外，还展示了该技术在单点带状线性优化中的应用，并验证了其在动态/通用遗憾最小化等更复杂任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于与博弈论、优化等领域有深刻的联系，在线学习中的基于梯度变化的方法受到了越来越多的关注。然而，这类方法在仅提供部分反馈（如带状反馈）的情景下还未被充分探索。本研究旨在改善这一现状，特别是在带状凸优化（BCO）背景下利用两点反馈时的表现。

Method: 通过对非连续梯度变化进行精细化分析，提出了一种新的方法来提高对于凸函数及强凸函数情况下维度依赖性的处理效果。该方法不仅适用于两点反馈场景，还扩展到了单点带状线性优化问题上，特别是当优化域为超矩形时。

Result: 成功地改进了凸函数和强凸函数条件下带状反馈下的梯度变化在线学习算法的维度依赖性；提出了首个针对超矩形单点带状线性优化问题的梯度变化界限；并且在动态/通用遗憾最小化以及带状游戏中展示了快速收敛率。

Conclusion: 这项工作通过改进非连续梯度变化的分析，为带状凸优化提供了一个更为有效的解决方案，同时也证明了这种方法在处理更广泛的问题类型时的有效性和灵活性。

Abstract: Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.

</details>


### [89] [Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty](https://arxiv.org/abs/2602.04763)
*Rui Liu,Pratap Tokekar,Ming Lin*

Main category: cs.LG

TL;DR: 提出了一种名为A2MAML的方法，用于处理多智能体系统中异构多模态传感器的不确定性问题。该方法通过主动选择可靠的智能体-模态对，并利用贝叶斯逆方差加权聚合信息，从而在协作事故检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体合作框架通常仅在智能体层面进行推理，假设感知同质化且隐式处理不确定性，这在传感器故障时限制了系统的鲁棒性。随着多智能体系统配备更多样化的多模态传感器，需要一种新的方法来有效处理特定于模态和智能体的不确定性。

Method: A2MAML将每个模态特有的特征建模为具有不确定性预测的随机估计，积极挑选出可靠的智能体-模态组合，并通过贝叶斯逆方差权重合并信息。这种方法允许细粒度的模态级别融合，支持非对称的模态可用性，并提供了一种抑制被破坏或噪声模态的原则性机制。

Result: 广泛的实验表明，在连接的自动驾驶场景中的协作事故检测上，A2MAML相比单智能体和协作基线始终表现更好，达到了高达18.7%更高的事故检测率。

Conclusion: A2MAML提供了一个针对不确定性的、模态级合作的有效方案，能够提高多智能体系统在面对传感器损坏情况下的鲁棒性，特别是在复杂如自动驾驶的应用场景中显示出显著优势。

Abstract: Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle uncertainty implicitly, limiting robustness under sensor corruption. We propose Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty (A2MAML), a principled approach for uncertainty-aware, modality-level collaboration. A2MAML models each modality-specific feature as a stochastic estimate with uncertainty prediction, actively selects reliable agent-modality pairs, and aggregates information via Bayesian inverse-variance weighting. This formulation enables fine-grained, modality-level fusion, supports asymmetric modality availability, and provides a principled mechanism to suppress corrupted or noisy modalities. Extensive experiments on connected autonomous driving scenarios for collaborative accident detection demonstrate that A2MAML consistently outperforms both single-agent and collaborative baselines, achieving up to 18.7% higher accident detection rate.

</details>


### [90] [Billion-Scale Graph Foundation Models](https://arxiv.org/abs/2602.04768)
*Maya Bechler-Speicher,Yoel Gottlieb,Andrey Isakov,David Abensur,Ami Tavory,Daniel Haimovich,Ido Guy,Udi Weinsberg*

Main category: cs.LG

TL;DR: 本研究介绍了GraphBFF，这是一种用于构建十亿参数级别的图基础模型（GFM）的端到端方案，适用于任意异构的大规模图。通过使用GraphBFF Transformer，研究人员展示了通用图的第一个神经缩放法则，并在多个下游任务上实现了显著的零样本和少量样本性能。


<details>
  <summary>Details</summary>
Motivation: 由于基础模型已经通过大规模预训练和轻量级适应改变了语言和视觉领域，但对于一般的真实世界图来说扩展这种范式具有挑战性。因此，这项工作的动机是提出一个针对任意异构、十亿规模图的基础模型解决方案。

Method: 该研究提出了GraphBFF框架，包括了灵活且可扩展的GraphBFF Transformer架构，以及用于数据批处理、预训练和微调的具体方法论，以支持大规模GFM的构建。

Result: 研究者用1.4亿参数的GraphBFF Transformer进行了预训练，处理了十亿样本。在十个不同的真实世界下游任务中，无论是在节点还是链接级别分类和回归方面，GraphBFF都展现了出色的零样本和少量样本表现，最高可达31点PRAUC的优势。

Conclusion: 研究表明，GraphBFF框架能够有效建立面向工业规模图学习的实用和原则性的基础模型。此外，还讨论了将GFM发展成为实际应用的关键挑战与开放机会。

Abstract: Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.

</details>


### [91] [NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image](https://arxiv.org/abs/2602.04769)
*Yan Chen,Jie Peng,Moajjem Hossain Chowdhury,Tianlong Chen,Yunmei Liu*

Main category: cs.LG

TL;DR: 提出了一种新的框架NeuroCanvas，通过选择与癫痫相关的EEG通道并将其转换为结构化的视觉表示来提高癫痫检测的准确性和计算效率。实验结果表明，该方法在F1分数上提高了20%，推理延迟减少了88%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型（LLM）处理EEG信号的方法面临多通道异质性和计算低效的问题，需要一种既能有效筛选相关通道又能提升计算效率的新方法。

Method: 提出了NeuroCanvas框架，包括熵引导通道选择器（ECS）和神经信号画布（CNS）两个模块。ECS用于挑选出与癫痫相关的EEG通道；CNS则将这些多通道的EEG信号转化为紧凑的视觉标记，以此来解决计算效率问题。

Result: 在多个癫痫检测数据集上的评估显示，NeuroCanvas能够显著提高F1分数达20%，同时减少推理延迟高达88%。

Conclusion: NeuroCanvas提供了一个可扩展且有效的解决方案，适用于临床实践中实时、资源高效的癫痫检测。

Abstract: Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\%$ in F1 score and reductions of $88\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.

</details>


### [92] [Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification](https://arxiv.org/abs/2602.04775)
*Yuqi Li,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: 本文提出了一种新的不确定性感知ROC框架，专为区间预测设计，引入了$AUC_L$和$AUC_U$两个新指标，支持选择性预测，并在实际数据集上验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在高风险预测中，通过区间预测量化不确定性对于可靠的决策至关重要。然而，现有的评估工具如ROC曲线和AUC值是为点估计设计的，无法捕捉预测不确定性对排序性能的影响。

Method: 提出了一个专门针对区间预测的不确定性感知ROC框架，该框架引入了两个新的度量标准：$AUC_L$ 和 $AUC_U$。此方法自然地支持通过允许模型放弃对重叠区间的情况进行排序来优化弃权率与区分可靠性的权衡。

Result: 实验证明，在有效的类条件覆盖率下，$AUC_L$ 和 $AUC_U$ 提供了理论最优AUC ($AUC^*$) 的正式下限和上限，表征了可实现区分的物理极限。所提出的框架广泛适用于区间预测模型，无论采用何种区间构建方法。

Conclusion: 实验结果表明，该框架不仅正确而且具有实用价值，能够用于不确定性意识的评估和决策。

Abstract: In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.

</details>


### [93] [Dynamical Regimes of Multimodal Diffusion Models](https://arxiv.org/abs/2602.04780)
*Emil Albrychiewicz,Andrés Franco Valiente,Li-Ching Chen*

Main category: cs.LG

TL;DR: 本文提出了一个基于耦合Ornstein-Uhlenbeck过程的理论框架，来解释扩散模型在多模态生成中的机制。通过非平衡统计物理和动态相变原理，揭示了多模态生成受谱层次交互时间尺度而非同时分辨率控制的事实，并提出了“同步间隙”概念。此外，还导出了对称和各向异性耦合条件下物种形成与崩溃时间的解析条件，证明了耦合强度作为谱滤波器的作用，能够强制执行可调的时间层次结构。实验结果支持这些预测，并建议采用针对特定模式时间尺度的时间依赖性耦合计划，为调整提供了潜在的替代方案。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在合成高维数据方面取得了前所未有的保真度，但对其背后的多模态生成机制的理解仍然不足。

Method: 使用耦合Ornstein-Uhlenbeck过程作为可处理模型构建理论框架；应用非平衡统计物理和动态相变的概念来分析多模态生成；通过推导出关于物种形成及崩溃时间点的解析条件，研究不同耦合强度下系统稳定性；利用MNIST数据集训练的扩散模型以及精确得分采样器进行验证实验。

Result: 证明了多模态生成由谱层次的交互时间尺度而非同时分辨率所控制；提出并定义了‘同步间隙’的概念；展示了耦合强度如何作为谱滤波器影响生成过程中可调节的时间层次；通过实验支持了上述理论预测。

Conclusion: 本文的研究成果不仅增进了对于扩散模型中多模态生成机制的理解，同时也为设计更有效的生成策略（如针对特定模式时间尺度的时间依赖性耦合计划）提供了新的视角。

Abstract: Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.

</details>


### [94] [Legendre Memory Unit with A Multi-Slice Compensation Model for Short-Term Wind Speed Forecasting Based on Wind Farm Cluster Data](https://arxiv.org/abs/2602.04782)
*Mumin Zhang,Haochen Zhang,Xin Zhi Khoo,Yilin Zhang,Nuo Chen,Ting Zhang,Junjie Tang*

Main category: cs.LG

TL;DR: 本文提出了一种新的集成模型WMF-CPK-MSLMU，用于风力发电集群的短期风速预测。该模型结合了加权均值滤波（WMF）、基于肯德尔秩相关系数的补偿参数（CPK）以及多片段Legendre记忆单元（MSLMU），能够有效提高预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的风电场聚集以实现集成，风电场集群的短期风速预测对于电力系统的正常运行至关重要。为了充分利用具有时空关联性的集群数据，达到准确、快速且稳健的风速预测效果，研究者们开发出了新的预测方法。

Method: 首先使用加权平均滤波(WMF)对单个风电场层面的风速数据进行降噪处理；接着创新性地应用Legendre记忆单元(LMU)来进行风速预测，并结合基于肯德尔等级相关系数的补偿参数(CPK)，构建多切片LMU(MSLMU)；最后提出了一个包含三个关键模块——数据预处理、预测及多切片补偿——的新型集成模型WMF-CPK-MSLMU。

Result: 实验结果表明，在不同风电场集群上的测试显示，与现有模型相比，所提出的集成模型WMF-CPK-MSLMU在风电场集群的短期预测中表现出更高的有效性和优越性。

Conclusion: 通过联合建模风电场之间的线性和非线性依赖关系来捕捉时空相关性，利用CPK导出的权重代替随机初始化，使得跨集群风电场中的隐藏节点能够被充分激活，从而增强了预测性能。此外，CPK自适应地为MSLMU中的补偿模型加权并补充缺失的数据，进一步提高了整个模型的准确性与鲁棒性。

Abstract: With more wind farms clustered for integration, the short-term wind speed prediction of such wind farm clusters is critical for normal operation of power systems. This paper focuses on achieving accurate, fast, and robust wind speed prediction by full use of cluster data with spatial-temporal correlation. First, weighted mean filtering (WMF) is applied to denoise wind speed data at the single-farm level. The Legendre memory unit (LMU) is then innovatively applied for the wind speed prediction, in combination with the Compensating Parameter based on Kendall rank correlation coefficient (CPK) of wind farm cluster data, to construct the multi-slice LMU (MSLMU). Finally, an innovative ensemble model WMF-CPK-MSLMU is proposed herein, with three key blocks: data pre-processing, forecasting, and multi-slice compensation. Advantages include: 1) LMU jointly models linear and nonlinear dependencies among farms to capture spatial-temporal correlations through backpropagation; 2) MSLMU enhances forecasting by using CPK-derived weights instead of random initialization, allowing spatial correlations to fully activate hidden nodes across clustered wind farms.; 3) CPK adaptively weights the compensation model in MSLMU and complements missing data spatially, to facilitate the whole model highly accurate and robust. Test results on different wind farm clusters indicate the effectiveness and superiority of proposed ensemble model WMF-CPK-MSLMU in the short-term prediction of wind farm clusters compared to the existing models.

</details>


### [95] [From independent patches to coordinated attention: Controlling information flow in vision transformers](https://arxiv.org/abs/2602.04784)
*Kieran A. Murphy*

Main category: cs.LG

TL;DR: 研究通过在视觉转换器中引入变分信息瓶颈，来控制和测量注意力机制传递的信息量，从而探索从局部补丁处理到全局注意力表达的连续谱。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解视觉转换器内部如何通过注意力机制构建全局视觉表示，并提高模型的可解释性和可控性。

Method: 在所有通过注意力机制写入残差流的操作中插入变分信息瓶颈，不改变其他架构，以此训练具有明确信息成本的模型。

Result: 成功地在ImageNet-100上展示了分类行为与信息路由随信息量变化而演变的过程，并提供了关于全局视觉表示如何从局部补丁处理中产生的初步见解。

Conclusion: 这种方法倾向于学习那些内部通信受到约束的解决方案，使得模型更易于进行机制分析且更加可控。

Abstract: We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.

</details>


### [96] [Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation](https://arxiv.org/abs/2602.04785)
*Congjing Zhang,Ryan Feng Lin,Ruoxuan Bao,Shuai Huang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Team-then-Trim (T$^2$) 的框架，该框架通过一组大型语言模型（LLMs）协作生成高质量的表格数据，并通过严格的三阶段插件数据质量控制流程来提升数据质量。实验结果表明T$^2$在产生高质量表格数据方面优于现有技术，特别适用于直接收集数据不可行的情况。


<details>
  <summary>Details</summary>
Motivation: 由于获取高质量表格数据通常既费时又昂贵，现有的表格数据集往往存在诸如类别不平衡、选择偏差和低保真度等问题。为了解决这些挑战，本研究旨在开发一种新的方法来合成高质量的表格数据，以支持那些难以直接收集足够数据的应用场景。

Method: 提出了一个名为Team-then-Trim (T$^2$) 的新框架，它利用一组专门训练过的大型语言模型（LLMs），基于领域知识指导依次生成不同的数据组件。之后，使用了一个包含三个阶段的数据质量控制流程来评估并提高合成数据的质量。

Result: 通过在模拟和真实世界数据集上的实验证明，T$^2$方法在生成高质量表格数据方面比当前最先进的方法表现更好。

Conclusion: T$^2$框架展示了其在难以直接获得充足数据情况下支持下游模型的强大潜力。

Abstract: While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.

</details>


### [97] [Maximum-Volume Nonnegative Matrix Factorization](https://arxiv.org/abs/2602.04795)
*Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.LG

TL;DR: 本文提出了一种新的非负矩阵分解方法——最大体积NMF（MaxVol NMF），通过最大化因子H的体积来获取更稀疏的解，并且在存在噪声的情况下比最小体积NMF（MinVol NMF）表现得更好。此外，还提出了两种算法来解决MaxVol NMF，并展示了一种归一化变体，该变体在标准NMF和正交NMF之间提供了连续性，在高光谱解混应用中表现出了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 为了获得更加可解释且唯一的解决方案，研究人员已经使用了最小体积NMF（MinVol NMF）来减小W因子的体积。然而，当存在噪声时，MinVol NMF的行为与无噪声情况下的行为不同。本文考虑了另一种方法，即最大化H因子的体积，这种方法被称为最大体积NMF（MaxVol NMF）。

Method: 论文提出了两种解决MaxVol NMF的新算法，并且介绍了一个归一化的MaxVol NMF版本，这个版本在标准NMF和正交NMF之间提供了一种连续性。作者们还证明了具有最大体积的MaxVol NMF解可以将X的列聚类成不相交的簇，而具有最小体积的MinVol NMF解则是秩亏缺的。

Result: 研究表明，在实际应用中，MaxVol NMF比MinVol NMF更能有效地提取稀疏分解，并且不会生成秩亏缺的解。此外，归一化的MaxVol NMF版本展示了比MinVol NMF和非归一化MaxVol NMF更好的性能。

Conclusion: 最大体积NMF（MaxVol NMF）是一种有效的方法，它能够在有噪声的情况下提供稀疏分解，并避免产生秩亏缺的解。其归一化版本在高光谱解混等应用场景下表现出色。

Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.

</details>


### [98] [Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning](https://arxiv.org/abs/2602.04807)
*Wolfgang Maass,Sabine Janzen,Prajvi Saxena,Sach Mukherjee*

Main category: cs.LG

TL;DR: 本文提出了一种名为Afferent Learning的框架，该框架能够生成计算性传入痕迹（CATs），作为适应性的内部风险信号用于避免损伤的学习。通过结合进化优化与强化学习，该框架能够在长时间跨度下为生物力学数字孪生体提供更高效且年龄鲁棒性更强的策略学习方法。


<details>
  <summary>Details</summary>
Motivation: 受生物系统的启发，研究者们旨在开发一种能够产生适应性、内部风险信号的框架，以促进损伤规避学习。其目的是通过构建一个两层架构来实现这一目标：外层利用进化优化发现有助于有效策略学习的传入感知架构；内层则通过强化学习训练使用这些信号进行损伤规避策略的学习。

Method: 采用了一种双层架构的方法：外层运用进化优化技术寻找可以支持有效策略学习的传入感知架构；内层则通过强化学习算法，基于前一层找到的架构来训练损伤规避策略。此外，还提供了理论上的收敛保证，并在具有挑战性的生物力学数字孪生环境中进行了实验验证。

Result: 实验结果表明，在长时间跨度的应用场景中，基于CAT的进化架构相比人工设计的基础架构能实现更高的效率和更好的年龄鲁棒性，具体表现为减少了23%的高风险行为。消融研究表明CAT信号、进化过程以及预测差异都是至关重要的因素。

Conclusion: Afferent Learning框架通过引入计算性传入痕迹作为内部风险信号，成功地促进了在复杂环境中更加高效和鲁棒的策略学习。该方法不仅适用于生物力学数字孪生领域，也为其他需要长期规划与适应变化条件下的学习任务提供了新的思路。

Abstract: We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.

</details>


### [99] [The Key to State Reduction in Linear Attention: A Rank-based Perspective](https://arxiv.org/abs/2602.04852)
*Philipp Nazari,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文分析了线性注意力机制中秩的作用，并提出了一种新的硬件感知方法，通过结构化剪枝键和查询矩阵来减少状态大小，同时保持与现有CUDA内核的兼容性。实验结果表明，该框架可以移除50%的查询和键通道，而困惑度仅略有增加。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然计算效率高且表达能力强，但训练后的模型状态往往表现出低秩结构，这表明这些模型在实践中未充分利用其容量。为了探究这一现象并解决相关问题，作者进行了理论分析，并提出了新的解决方案。

Method: 提供了一个关于线性注意力中秩作用的理论分析；提出了一个新颖的、硬件感知的方法，通过结构化剪枝关键和查询矩阵来减小状态规模，同时保持与现有CUDA内核的兼容性；基于理论分析，引入了一种基于揭示秩的QR分解的新结构化剪枝方法。

Result: 实证研究显示，在不同规模的模型以及各种下游任务上评估时，所提出的框架能够有效地减少状态大小。特别地，该框架能够在只增加少量困惑度的情况下移除50%的查询和键通道。

Conclusion: 通过理论分析和实验验证，证明了提出的结构化剪枝方法能有效减少线性注意力模型的状态大小，同时对性能影响极小，为开发更快、更节省内存的模型提供了新途径。

Abstract: Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.

</details>


### [100] [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863)
*Ishaq Aden-Ali,Noah Golowich,Allen Liu,Abhishek Shetty,Ankur Moitra,Nika Haghtalab*

Main category: cs.LG

TL;DR: 本文提出了一种名为Logit-Linear-Selection (LLS)的方法，该方法能够从通用偏好数据集中选取子集以激发多种隐藏效应。通过应用LLS到真实世界的数据集中，可以训练出展现特定行为的模型，比如拥有特定偏好、以数据集中不存在的语言回应提示或呈现不同的个性等。这种效果在选定的子集上对于不同架构的模型都是一致的，显示了其普遍性和通用性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）训练中算法和数据集多样性的增加，理解数据集如何影响模型属性变得至关重要。最近的研究表明，数据集可能传递无法直接从单个数据点观察到的信号，这给基于数据集理解LLM训练提出了概念上的挑战，并暗示了对这类现象缺乏基本解释。

Method: 受最近关于LLM线性结构工作的启发，研究者们揭示了一个通用机制，通过这个机制可以在通用数据集中产生隐藏的副文本。为此，他们引入了Logit-Linear-Selection (LLS) 方法，该方法指导如何选择通用偏好数据集中的子集来激发广泛隐藏的效果。

Result: 通过将LLS应用于发现真实世界数据集中的子集，使得在此基础上训练出的模型表现出各种行为，包括但不限于：持有特定偏好；以数据集中未出现的语言响应提示；以及扮演不同的角色。重要的是，这种效果在所选子集上跨具有不同架构的模型持续存在，证明了其普适性和普遍性。

Conclusion: 本研究表明，通过采用Logit-Linear-Selection (LLS) 方法可以从通用数据集中识别出能引起模型特定行为的子集，从而帮助我们更好地理解数据集是如何影响大型语言模型特性的。此外，这种方法展示了其在不同模型架构间的适用性，进一步强调了它作为分析工具的价值。

Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.

</details>


### [101] [Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism](https://arxiv.org/abs/2602.04870)
*Chenwei Cui,Rockwell Jackson,Benjamin Joseph Herrera,Ana María Tárano,Hannah Kerner*

Main category: cs.LG

TL;DR: 本文提出了一种新的架构和并行方法，即多头潜在MoE和头并行（HP），旨在解决专家并行（EP）在训练大规模稀疏混合专家模型时的通信成本、负载不平衡及数据依赖性通信等问题。该方法不仅与EP兼容，还显著提高了训练速度，同时保持了相同的性能表现，使得数十亿参数基础模型的研究变得更加可行。


<details>
  <summary>Details</summary>
Motivation: 现有的专家并行（EP）技术虽然能够在一定程度上支持大型语言模型的条件计算，但在激活专家数量增加时面临着线性增长的通信开销问题、负载不平衡以及需要额外元数据交换的数据相关通信挑战。为了解决这些问题，并进一步提高大模型训练效率，作者提出了一个新的架构和并行策略。

Method: 作者们引入了多头潜在MoE架构结合头并行（Head Parallel, HP）方法，旨在实现与激活专家数目无关的固定通信成本、完全平衡的数据流处理能力以及确定性的通信模式。此外，他们还开发了IO感知路由和专家计算加速机制来进一步优化所提方案的表现。

Result: 实验结果表明，与使用EP的传统MoE相比，采用HP的多头潜在MoE在相同性能下能够达到最高1.61倍的训练速度提升；当粒度加倍时，尽管整体性能有所增强，但依然保持着1.11倍的速度优势。

Conclusion: 通过提出多头潜在MoE加头并行这一创新组合，研究者们成功地克服了现有EP方法的一些关键局限性，极大地促进了超大规模语言模型领域的研究进展。

Abstract: Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.

</details>


### [102] [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879)
*Penghui Qi,Xiangxin Zhou,Zichen Liu,Tianyu Pang,Chao Du,Min Lin,Wee Sun Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习算法Divergence Proximal Policy Optimization (DPPO)，旨在解决PPO在大型语言模型（LLMs）微调过程中存在的问题。DPPO通过直接估计策略差异来替换PPO中的启发式剪裁机制，从而提高了训练的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 作者认为PPO中核心的比例剪裁机制并不适合处理大型语言模型固有的大词汇量。PPO根据采样令牌的概率比约束策略更新，这导致低概率令牌的更新受到过度惩罚，而高概率令牌的重大变化则被低估，造成训练效率低下和不稳定。

Method: 提出了Divergence Proximal Policy Optimization (DPPO)算法，该算法利用对策略差异（如总变异或KL散度）的直接估计来替代PPO中的比例剪裁机制。此外，为减少内存占用，引入了二进制和Top-K近似方法以几乎无额外开销的方式捕捉关键差异。

Result: 广泛的实证评估表明，与现有方法相比，DPPO在训练稳定性和效率方面表现出色，为基于RL的LLM微调提供了更加稳健的基础。

Conclusion: 通过引入DPPO及其配套的高效近似方法，研究成功地解决了PPO在处理大型语言模型时所面临的问题，显著提升了训练过程中的稳定性和效率。

Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

</details>


### [103] [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/abs/2602.04881)
*Ajesh Koyatan Chathoth*

Main category: cs.LG

TL;DR: 本文探讨了对比持续学习（CCL）在物联网中的应用，结合算法设计与物联网系统现实条件，并提出了统一的问题公式、目标函数及面向物联网的参考架构。


<details>
  <summary>Details</summary>
Motivation: 随着物联网部署环境变得越来越非静态和动态，传感器漂移、用户行为变化以及用户隐私需求多样性等因素影响着应用程序的实用性。持续学习通过随时间调整模型而不发生灾难性遗忘来解决这一问题。同时，对比学习作为一种强大的表示学习范式，在自监督方式下提高了鲁棒性和样本效率。

Method: 本文回顾了对比持续学习(CCL)在物联网中的应用情况，将算法设计（如重播、正则化、蒸馏、提示）与物联网系统实际情况（TinyML约束、间歇连接性、隐私保护）相结合。提出了一种统一的问题表述方法，推导出融合对比损失和蒸馏损失的共同目标函数，并为设备端、边缘端及云端CCL提供了一个面向物联网的参考架构。

Result: 文章提供了关于评估协议和度量标准的指导，并强调了物联网领域内的一些独特开放挑战，比如处理表格型和流式物联网数据、概念漂移、联邦设置以及节能训练等问题。

Conclusion: 对比持续学习对于提高物联网应用在不断变化环境下的适应能力和性能至关重要。本文不仅为该领域的研究者提供了全面的技术概述，还指出了未来研究方向。

Abstract: Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.

</details>


### [104] [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/abs/2602.04883)
*Yanru Qu,Cheng-Yen Hsieh,Zaixiang Zheng,Ge Liu,Quanquan Gu*

Main category: cs.LG

TL;DR: 介绍了蛋白质自回归建模（PAR），这是一种通过粗到细的多尺度预测来生成蛋白质骨架的框架。PAR结合了多尺度下采样操作、自回归转换器和基于流的骨架解码器三个关键组件，并通过采用噪声上下文学习和预定采样缓解了暴露偏差问题，从而支持高质量的蛋白质结构生成。


<details>
  <summary>Details</summary>
Motivation: 为了利用蛋白质的层级特性，开发了一种能够从粗略拓扑到细化结构细节逐级生成蛋白质骨架的方法，以提高蛋白质结构生成的质量和灵活性。

Method: PAR框架包含三个核心部分：1) 多尺度下采样操作，在训练过程中表示不同尺度下的蛋白质结构；2) 一个自回归变压器，编码多尺度信息并产生条件嵌入用于指导结构生成；3) 基于流的骨架解码器，根据这些嵌入生成骨架原子。此外，为解决自回归模型中的暴露偏差问题，采用了噪声上下文学习与计划采样策略。

Result: PAR不仅在无条件生成基准上有效学习了蛋白质分布并产生了高质量的设计，而且展示了良好的扩展行为。更重要的是，它还表现出了强大的零样本泛化能力，允许灵活的人工提示条件生成及基序支架构建而无需微调。

Conclusion: PAR作为一种有前景的蛋白质结构生成框架，其结合了多尺度处理、高效的信息编码机制以及创新的问题解决方法，为蛋白质设计提供了新的工具。

Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [105] [Pending Conflicts Make Progress Impossible](https://arxiv.org/abs/2602.04013)
*Petr Kuznetsov,Pierre Sutra,Guillermo Toyos-Marfurt*

Main category: cs.DC

TL;DR: 本研究探讨了共享对象的可交换性感知、线性化实现中的进展条件，提出了冲突无障碍自由的概念，并证明了在异步读写共享内存模型中无法实现冲突无障碍自由的通用构造，揭示了冲突感知通用构造的基本局限性。


<details>
  <summary>Details</summary>
Motivation: 基于可以并行执行交换操作的观察，研究旨在探索一种新的进展条件，以允许只要步骤竞争仅由交换操作引起时就能取得进展。

Method: 提出了一种新的进展条件——冲突无障碍自由：如果进程运行足够长的时间而没有遇到与冲突（非交换）操作的步骤竞争，则保证完成其操作。

Result: 证明了在异步读写共享内存模型下，不可能实现冲突无障碍自由的通用构造。

Conclusion: 这项工作展示了冲突感知通用构造的一个基本限制：仅仅是调用冲突操作就施加了同步成本，进展需要最终解决待定冲突。

Abstract: In this work, we study progress conditions for commutativity-aware, linearizable implementations of shared objects. Motivated by the observation that commuting operations can be executed in parallel, we introduce conflict-obstruction-freedom: a process is guaranteed to complete its operation if it runs for long enough without encountering step contention with conflicting (non-commuting) operations. This condition generalizes obstruction-freedom and wait-freedom by allowing progress as long as step contention is only induced by commuting operations. We prove that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model. This result exposes a fundamental limitation of conflict-aware universal constructions: the mere invocation of conflicting operations imposes a synchronization cost. Progress requires eventual resolution of pending conflicts.

</details>


### [106] [Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN](https://arxiv.org/abs/2602.04652)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.DC

TL;DR: 本文通过实验量化了将5G风格的LDPC5G解码从Grace CPU卸载到NVIDIA DGX Spark平台上的集成Blackwell GB10 GPU的好处，发现GPU/CPU吞吐量平均提升了约6倍，并且在执行相同工作负载时，GPU保持在时隙要求内而CPU则超出了0.5ms的时隙限制。


<details>
  <summary>Details</summary>
Motivation: 随着带宽、调制阶数和用户复用增加，现有许多开放和专有堆栈仍在通用CPU上执行LDPC，这引发了关于错过时隙事件和有限可扩展性的担忧。因此，研究者们希望探索将5G样式的LDPC5G解码任务从CPU卸载到GPU上是否能够提高处理效率和减少功耗。

Method: 研究者使用NVIDIA Sionna PHY/SYS基于TensorFlow构建了一个类似NR的链路级链，包括LDPC5G编码器/解码器、16-QAM调制和AWGN信道。他们调整了并行解码的码字数量与置信传播迭代次数，仅测量解码阶段的时间，同时记录CPU和GPU利用率及功耗。

Result: 结果表明，对于相同的任务，GPU相比CPU平均提供了大约6倍的吞吐量加速；当进行20次迭代时，单个码字的CPU延迟达到了约0.71毫秒（超过了0.5毫秒的时隙），而GB10 GPU在同一工作负载下仅消耗了时隙的6-24%。此外，基于CPU的LDPC解码通常需要大约十个Grace核心，相比之下，基于GPU的解码仅比GPU空闲状态多消耗约10-15瓦功率，同时保留了大部分CPU容量用于更高层次的任务。

Conclusion: 本研究表明，通过利用GPU来卸载LDPC5G解码任务不仅显著提高了处理速度还降低了能耗，为未来在Grace/Blackwell以及Aerial/ACAR/AODT等平台上评估LDPC和其他物理层内核提供了一种可重用且可脚本化的评估方法。

Abstract: Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\times$, with per-codeword CPU latency reaching $\approx 0.71$\,ms at 20 iterations (exceeding the 0.5\,ms slot), while the GB10 GPU remains within 6--24\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\approx10-15$\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.

</details>


### [107] [A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources](https://arxiv.org/abs/2602.04697)
*Davide Basile,Valerio Goretti,Luca Barbaro,Hajo A. Reijers,Claudio Di Ciccio*

Main category: cs.DC

TL;DR: 本文提出了一种名为CONFINE的方法，该方法利用可信执行环境(TEEs)来保护跨组织流程挖掘中的数据机密性。通过一个四阶段协议确保数据的安全交换与处理，并采用分段策略避免TEE内存限制带来的问题。实验结果表明，此方法可以处理实际工作负载，并且具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 跨组织的业务流程分析面临的主要挑战之一是保持数据的保密性。参与方可能不愿意向其他参与者或第三方服务提供商披露信息。为了解决这个问题，作者提出了一个能够保护数据秘密性的跨组织流程挖掘方案。

Method: 提出的方法称为CONFINE，它使用可信执行环境（TEEs）部署安全应用程序，这些程序能够在不泄露敏感信息的情况下对多方事件日志进行安全挖掘。此外，还设计了一个支持四个阶段协议的架构，以确保跨组织边界的数据传输和聚合过程中的安全性。针对TEEs容量有限的问题，采用了基于分割的策略，将事件日志分成较小批次传输给TEE。

Result: 通过对真实世界及合成数据集上的实现进行了评估，结果显示所提方法能有效应对实际工作量。观察到相对于事件日志大小呈现对数增长的内存消耗以及随着提供数据组织数量线性增长的趋势，这表明了该方法具备较好的可扩展性和进一步优化的空间。

Conclusion: 研究证明了使用可信执行环境来进行保密性保护的跨组织流程挖掘是可行的，并且通过合理的架构设计和技术手段可以有效地解决现有的一些关键问题。

Abstract: Process mining techniques enable organizations to gain insights into their business processes through the analysis of execution records (event logs) stored by information systems. While most process mining efforts focus on intra-organizational scenarios, many real-world business processes span multiple independent organizations. Inter-organizational process mining, though, faces significant challenges, particularly regarding confidentiality guarantees: The analysis of data can reveal information that the participating organizations may not consent to disclose to one another, or to a third party hosting process mining services. To overcome this issue, this paper presents CONFINE, an approach for secrecy-preserving inter-organizational process mining. CONFINE leverages Trusted Execution Environments (TEEs) to deploy trusted applications that are capable of securely mining multi-party event logs while preserving data secrecy. We propose an architecture supporting a four-stage protocol to secure data exchange and processing, allowing for protected transfer and aggregation of unaltered process data across organizational boundaries. To avoid out-of-memory errors due to the limited capacity of TEEs, our protocol employs a segmentation-based strategy, whereby event logs are transmitted to TEEs in smaller batches. We conduct a formal verification of correctness and a security analysis of the guarantees provided by the TEE core. We evaluate our implementation on real-world and synthetic data, showing that the proposed approach can handle realistic workloads. The results indicate logarithmic memory growth with respect to the event log size and linear growth with the number of provisioning organizations, highlighting scalability properties and opportunities for further optimization.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [108] [Accountability in Open Source Software Ecosystems: Workshop Report](https://arxiv.org/abs/2602.04026)
*Nandini Sharma,Thomas Bock,Rich Bowen,Sayeed Choudhury,Brian Fitzgerald,Matt Germonprez,Jim Herbsleb,James Howison,Tom Hughes,Min Kyung Lee,Stephanie Lieggi,Andreas Liesenfeld,Georg Link,Nicholas Matsakis,Audris Mockus,Narayan Ramasubbu,Christopher Robinson,Gregorio Robles,Nithya Ruff,Sonali Shah,Igor Steinmacher,Bogdan Vasilescu,Stephen Walli,Christopher Yoo*

Main category: cs.SE

TL;DR: 本文组织了一次研讨会，旨在探讨开源软件生态系统中的问责制角色，并激发相关研究议程和有意义的利益相关者参与想法。


<details>
  <summary>Details</summary>
Motivation: 开源软件生态系统由多种利益相关者组成，包括但不限于非营利组织、志愿者贡献者、用户和公司。这些利益相关者的需求和动机往往是多样化的、未知的，有时甚至是冲突的。因此，不清楚开源社区如何识别并与其利益相关者互动、理解他们的需求以及如何对这些需求负责。

Method: 通过召集24位研究开源软件社区的专家学者和实践者进行探索性研讨会讨论。该研讨会名为“问责制与开源软件生态系统”，于10月14日至15日在卡内基梅隆大学校园内举行。

Result: 研讨会展开了关于开源软件生态系统中问责制作用的重要而紧迫问题的对话，并为实践者激发了令人兴奋的研究议程和有意义的利益相关者参与想法。

Conclusion: 本次研讨会促进了对于开源软件生态系统内问责制角色的理解，并为未来的研究方向及实践者如何更好地与利益相关者互动提供了思路。

Abstract: Open source software ecosystems are composed of a variety of stakeholders including but not limited to non-profit organizations, volunteer contributors, users, and corporations. The needs and motivations of these stakeholders are often diverse, unknown, and sometimes even conflicting given the engagement and investment of both volunteers and corporate actors. Given this, it is not clear how open source communities identify and engage with their stakeholders, understand their needs, and hold themselves accountable to those needs. We convened 24 expert scholars and practitioners studying and working with open source software communities for an exploratory workshop discussion on these ideas. The workshop titled "Accountability and Open Source Software Ecosystems" was organized on Oct 14-15 on campus in Carnegie Mellon University, Pittsburgh, PA. The purpose of this in-person workshop was to initiate conversations that explore important and urgent questions related to the role of accountability in open source software ecosystems, and to inspire an exciting research agenda and meaningful stakeholder engagement ideas for practitioners.

</details>


### [109] [I Can't Believe It's Not a Valid Exploit](https://arxiv.org/abs/2602.04165)
*Derin Gezgin,Amartya Das,Shinhae Kim,Zhengdong Huang,Nevena Stojkovic,Claire Wang*

Main category: cs.SE

TL;DR: 本研究开发了一个名为PoC-Gym的框架，用于通过大型语言模型（LLMs）生成Java安全漏洞的概念验证（PoC），并系统地验证所生成的利用代码的有效性。结果表明，与静态分析工具结合使用可以提高PoC生成的成功率，但手动检查显示大部分生成的PoC无效，这表明基于LLM的PoC生成的成功率可能被高估了。


<details>
  <summary>Details</summary>
Motivation: 最近，大型语言模型（LLMs）被应用于安全漏洞检测任务中，包括生成概念验证（PoC）攻击代码。有研究表明，为LLMs提供额外指导能够改善PoC生成的结果，因此有必要进一步评估这种做法的有效性。此外，当前对于LLM生成PoC的有效性验证机制存在局限性，难以准确衡量其真实效果。

Method: 研究人员开发了PoC-Gym框架，该框架旨在通过LLMs生成针对Java安全漏洞的PoC，并对生成的利用代码进行系统性的有效性验证。他们还探索了将静态分析工具提供的指导和标准融入到这一过程中，以期提高PoC生成的质量。

Result: 实验采用Claude Sonnet 4、GPT-5 Medium和gpt-oss-20b等模型运行PoC-Gym后发现，当加入静态分析工具的指导时，PoC生成的成功率比之前的基准FaultLine高出21%。然而，对成功及失败案例的手动审查揭示出71.5%的PoC实际上是无效的。

Conclusion: 尽管结合静态分析工具确实提高了基于LLM的PoC生成成功率，但大量生成的PoC被证明是无效的，这意味着目前报告的成功率可能大大误导了人们对实际效果的理解。此发现强调了改进现有验证机制的重要性。

Abstract: Recently Large Language Models (LLMs) have been used in security vulnerability detection tasks including generating proof-of-concept (PoC) exploits. A PoC exploit is a program used to demonstrate how a vulnerability can be exploited. Several approaches suggest that supporting LLMs with additional guidance can improve PoC generation outcomes, motivating further evaluation of their effectiveness. In this work, we develop PoC-Gym, a framework for PoC generation for Java security vulnerabilities via LLMs and systematic validation of generated exploits. Using PoC-Gym, we evaluate whether the guidance from static analysis tools improves the PoC generation success rate and manually inspect the resulting PoCs. Our results from running PoC-Gym with Claude Sonnet 4, GPT-5 Medium, and gpt-oss-20b show that using static analysis for guidance and criteria lead to 21% higher success rates than the prior baseline, FaultLine. However, manual inspection of both successful and failed PoCs reveals that 71.5% of the PoCs are invalid. These results show that the reported success of LLM-based PoC generation can be significantly misleading, which is hard to detect with current validation mechanisms.

</details>


### [110] [Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation](https://arxiv.org/abs/2602.04195)
*Guang Yang,Xing Hu,Xiang Chen,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出了一种名为语义共识解码（SCD）的被动防御方法，用于在推理时保护大型语言模型免受硬件设计中后门攻击的影响。通过从用户规范中提取功能性需求并在生成过程中融合全规范与提取的功能性需求输出分布，SCD能够显著降低攻击成功率而几乎不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在硬件设计中的Verilog代码生成应用日益增多，这些模型容易受到后门攻击，即攻击者可以在训练阶段注入恶意触发器以诱导产生易受攻击的硬件设计。鉴于硬件木马一旦制造完成就很难修复的特点，现有的主动防御措施对于第三方LLM用户来说不够实用，而被动防御手段则难以应对自然融入设计规范的语义隐蔽触发器。因此，需要开发一种新的防御策略来有效对抗此类威胁。

Method: 基于假设攻击者倾向于将触发器嵌入非功能性要求而非决定硬件行为的功能性规格这一观点，作者们提出了语义共识解码（Semantic Consensus Decoding, SCD）。该方法包含两个核心部分：一是从用户提供的规范中提取出关键功能需求；二是采用共识解码技术，根据完整的用户规范和提取出的功能需求自适应地结合输出分布。当两者之间存在显著差异时，SCD会自动抑制可疑成分。

Result: 通过对三种代表性后门攻击进行广泛实验，结果显示SCD能将平均攻击成功率由89%降至不足3%，同时对生成质量几乎没有负面影响。

Conclusion: 研究结果表明，SCD作为一种有效的被动防御机制，在不显著影响正常生成性能的前提下，能够大幅度降低针对使用大型语言模型进行硬件设计时可能遭遇的后门攻击风险。

Abstract: Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.

</details>


### [111] [Generative AI in Systems Engineering: A Framework for Risk Assessment of Large Language Models](https://arxiv.org/abs/2602.04358)
*Stefan Otten,Philipp Reis,Philipp Rigoll,Joshua Ransiek,Tobias Schürmann,Jacob Langner,Eric Sax*

Main category: cs.SE

TL;DR: 本文提出了一种用于评估系统工程环境中大型语言模型（LLM）应用的框架——LLM风险评估框架（LRF）。该框架基于自主性和影响两个维度对LLM应用进行分类，帮助组织确定相应的风险水平，并支持制定适当的验证策略、人类监督程度以及必要的对策，以确保安全透明地部署LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在工程生命周期中提供了显著的机会，但组织在评估与LLM使用相关的风险方面面临重大挑战，导致集成不一致、未知的失败模式和有限的可扩展性。

Method: 提出了一个名为LLM风险评估框架（LRF）的结构化方法，它通过衡量LLM应用的自主性（从辅助到完全自动化决策）及其对工程过程和系统元素潜在影响的程度来对其进行分类。

Result: LRF能够跨开发周期一致地确定对应的风险等级，支持组织识别适当的验证策略、所需的人类监督级别及为确保安全透明部署所需的对策。

Conclusion: LRF为复杂工程环境中的LLM风险意识采纳奠定了基础，并朝着系统工程领域内标准化AI保证实践迈出了第一步。

Abstract: The increasing use of Large Language Models (LLMs) offers significant opportunities across the engineering lifecycle, including requirements engineering, software development, process optimization, and decision support. Despite this potential, organizations face substantial challenges in assessing the risks associated with LLM use, resulting in inconsistent integration, unknown failure modes, and limited scalability. This paper introduces the LLM Risk Assessment Framework (LRF), a structured approach for evaluating the application of LLMs within Systems Engineering (SE) environments. The framework classifies LLM-based applications along two fundamental dimensions: autonomy, ranging from supportive assistance to fully automated decision making, and impact, reflecting the potential severity of incorrect or misleading model outputs on engineering processes and system elements. By combining these dimensions, the LRF enables consistent determination of corresponding risk levels across the development lifecycle. The resulting classification supports organizations in identifying appropriate validation strategies, levels of human oversight, and required countermeasures to ensure safe and transparent deployment. The framework thereby helps align the rapid evolution of AI technologies with established engineering principles of reliability, traceability, and controlled process integration. Overall, the LRF provides a basis for risk-aware adoption of LLMs in complex engineering environments and represents a first step toward standardized AI assurance practices in systems engineering.

</details>


### [112] [A Framework of Critical Success Factors for Agile Software Development](https://arxiv.org/abs/2602.04467)
*Ridewaan Hanslo,Maureen Tanner*

Main category: cs.SE

TL;DR: 本研究通过分析53项主要研究，识别出敏捷项目中的21个关键成功因素（CSF），并将其归类为五个主题：组织、人员、技术、过程和项目。团队效能和项目管理是最常被提及的CSF，突出了人员与过程因素的重要性。基于这些发现，提出了一个理论框架以探索这些因素如何促进项目成功，并为未来的研究提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 尽管敏捷软件开发很受欢迎，但实现项目的一致成功仍然是一个挑战。因此，本研究旨在通过系统性文献回顾来确定敏捷项目的关键成功因素，从而帮助研究人员和实践者更好地理解影响成功的各种因素。

Method: 采用主题综合与内容分析法对53篇初步研究进行了分析，得出了影响敏捷项目成功的21个关键因素，并将这些因素分为五大类别。

Result: 研究结果揭示了21个关键成功因素，并根据它们所属的主题进行分类。其中，团队效能和项目管理被认为是最重要的两个因素，表明人员和流程方面在敏捷项目中扮演着至关重要的角色。

Conclusion: 该研究为理解和改善敏捷软件开发项目的成功率提供了一个新的视角，即通过关注特定的关键成功因素。此外，提出的一个理论框架可用于进一步验证这些发现，并建议使用定量方法来测试这个框架的有效性。

Abstract: Despite the popularity of Agile software development, achieving consistent project success remains challenging. This systematic literature review identifies critical success factors (CSFs) in Agile projects by analyzing 53 primary studies. Employing thematic synthesis with content analysis, our analysis yielded 21 CSFs categorized into five themes: organizational, people, technical, process, and project. Team effectiveness and project management emerged as the most frequently cited CSFs, highlighting the importance of people and process factors. These interpreted themes and factors contributed to the development of a theoretical framework to identify how these factors contribute to project success. This study offers valuable insights for researchers and practitioners, guiding future research to validate these findings and test the proposed framework using quantitative methods.

</details>


### [113] [Demonstrating ARG-V's Generation of Realistic Java Benchmarks for SV-COMP](https://arxiv.org/abs/2602.04786)
*Charles Moloney,Robert Dyer,Elena Sherman*

Main category: cs.SE

TL;DR: 本文介绍了ARG-V工具在自动生成符合SV-COMP格式的Java验证基准测试中的应用。研究表明，对于新生成的一组68个现实基准测试，所有四个领先的Java验证器在准确性和召回率上都比在现有基准测试集上的表现有所下降。这些发现强调了ARG-V提高验证工具评估全面性和现实性的潜力，并为验证器开发者提供了改进其工具对实际软件适用性的路线图。


<details>
  <summary>Details</summary>
Motivation: 随着SV-COMP竞赛中程序基准测试集的扩展，重要的是要考虑新添加的程序是否会导致验证器表现出与现有基准不同的行为，以减少外部因素对比赛结果有效性的威胁。

Method: 使用名为ARG-V的工具来自动生成符合SV-COMP格式的新Java验证基准测试。

Result: 研究发现，在新生成的一组68个现实基准测试上，所有四个主要的Java验证器的表现（准确性与召回率）相比现有的基准测试集有所降低。

Conclusion: ARG-V工具展示了其在提升验证工具评估的全面性和真实性方面的潜力，同时也为验证器开发人员提供了一个改进方向，以增强其工具对真实世界软件的适用性。

Abstract: The SV-COMP competition provides a state-of-the-art platform for evaluating software verification tools on a standardized set of verification tasks. Consequently, verifier development outcomes are influenced by the composition of program benchmarks included in SV-COMP. When expanding this benchmark corpus, it is crucial to consider whether newly added programs cause verifiers to exhibit behavior distinct from that observed on existing benchmarks. Doing so helps mitigate external threats to the validity of the competition's results.
  In this paper, we present the application of the ARG-V tool for automatically generating Java verification benchmarks in the SV-COMP format. We demonstrate that, on a newly generated set of 68 realistic benchmarks, all four leading Java verifiers decrease in accuracy and recall compared to their performance on the existing benchmark suite. These findings highlight the potential of ARG-V to enhance the comprehensiveness and realism of verification tool evaluation, while also providing a roadmap for verifier developers aiming to improve their tools' applicability to real-world software.

</details>


### [114] [Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software](https://arxiv.org/abs/2602.04799)
*Nils Chur,Thorsten Berger,Einar Broch Johnsen,Andrzej Wąsowski*

Main category: cs.SE

TL;DR: 研究了184个开源机器人软件中的实际控制器实现，发现实际软件实现中对控制器的离散化处理随意、存在实时可靠性问题及测试实践不足等问题，强调了改进实现指南和严格验证技术的需求，以确保机器人控制器在实践中的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管控制理论为标准控制器设计提供了安全保证，但控制器在软件中的实际实现引入了复杂性，这些复杂性往往被忽视。控制器通常是在连续空间中设计的，而软件则在离散空间中执行，这破坏了一些理论上的保证。对于控制器的实际实现以及如何在真实世界软件系统中确保其理论保证的研究较少。

Method: 调查了184个来自开源机器人软件的实际控制器实现，检查了它们的应用背景、实现特性以及用来确保正确性的测试方法。

Result: 发现控制器实现中常常以临时方式处理离散化问题，导致潜在的实时可靠性问题。还遇到了诸如时间不一致、缺乏适当的错误处理以及对实时约束考虑不足等挑战。此外，测试实践较为肤浅，并未使用系统的方法来验证理论保证，可能存在预期行为与实际行为之间的不一致。

Conclusion: 需要改进控制器实现指南并采用更严格的验证技术，以确保实践中机器人控制器的可靠性和安全性。

Abstract: A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.

</details>


### [115] [Do Developers Read Type Information? An Eye-Tracking Study on TypeScript](https://arxiv.org/abs/2602.04824)
*Samuel W. Flint,Robert Dyer,Bonita Sharif*

Main category: cs.SE

TL;DR: 研究通过眼动追踪实验，探讨了开发者在代码理解和错误定位任务中是否阅读类型注解。结果显示，开发者并不会更多地直接查看包含类型注解或声明的行，这对工具开发、社区标准建立及教育都有启示意义。


<details>
  <summary>Details</summary>
Motivation: 为了证明开发者确实将类型注解放在代码中作为文档使用，从而理解他们如何以及在什么情境下利用类型信息，进而帮助设计更好的开发工具和指导教育决策。

Method: 采用眼动追踪技术对26名本科生进行实验，观察他们在TypeScript语言环境下执行代码总结和错误定位任务时是否会阅读类型注解。

Result: 发现开发者在执行代码总结和错误定位任务时，并不会因为类型注解的存在而更频繁地直接注视含有这些注解或类型声明的行。

Conclusion: 尽管静态类型标注被认为有助于编程，但本研究表明开发者未必会主动阅读它们来辅助理解代码或定位错误。这提示我们需要改进类型信息呈现方式、制定良好实践标准，并在教学中强调正确的阅读习惯。

Abstract: Statically-annotated types have been shown to aid developers in a number of programming tasks, and this benefit holds true even when static type checking is not used. It is hypothesized that this is because developers use type annotations as in-code documentation. In this study, we aim to provide evidence that developers use type annotations as in-code documentation. Understanding this hypothesized use will help to understand how, and in what contexts, developers use type information; additionally, it may help to design better development tools and inform educational decisions. To provide this evidence, we conduct an eye tracking study with 26 undergraduate students to determine if they read type annotations during code comprehension and bug localization in the TypeScript language. We found that developers do not look directly at lines containing type annotations or type declarations more often when they are present, in either code summarization or bug localization tasks. The results have implications for tool builders to improve the availability of type information, the development community to build good standards for use of type annotations, and education to enforce deliberate teaching of reading patterns.

</details>


### [116] [When Code Becomes Abundant: Redefining Software Engineering Around Orchestration and Verification](https://arxiv.org/abs/2602.04830)
*Karina Kohl,Luigi Carro*

Main category: cs.SE

TL;DR: The paper argues that Software Engineering (SE) needs to be redefined due to the impacts of AI automation and energy constraints, emphasizing human discernment, architectural control, and verification over code construction.


<details>
  <summary>Details</summary>
Motivation: The motivation is driven by the pressures SE faces from AI automation, which reduces the cost of code production, and hardware-energy constraints, which increase the costs of failure. The need for SE to focus more on human discernment, intent articulation, and systematic verification arises to address these challenges.

Method: The paper takes a conceptual and argumentative approach, outlining the need for a paradigm shift in SE. It does not employ empirical methods but rather constructs a case based on logical reasoning and current trends in technology and SE practices.

Result: The result is a call for a fundamental transformation in SE's research priorities, educational curricula, and industrial practices, moving away from traditional code construction towards a more accountability-focused, human-centered discipline underpinned by automation.

Conclusion: The conclusion asserts that SE must evolve beyond its traditional scope to incorporate greater emphasis on understanding and expressing intent, exercising architectural control, and conducting rigorous verification, thereby addressing the emerging risks and opportunities brought about by AI and energy constraints.

Abstract: Software Engineering (SE) faces simultaneous pressure from AI automation (reducing code production costs) and hardware-energy constraints (amplifying failure costs). We position that SE must redefine itself around human discernment-intent articulation, architectural control, and verification-rather than code construction. This shift introduces accountability collapse as a central risk and requires fundamental changes to research priorities, educational curricula, and industrial practices. We argue that Software Engineering, as traditionally defined around code construction and process management, is no longer sufficient. Instead, the discipline must be redefined around intent articulation, architectural control, and systematic verification. This redefinition shifts Software Engineering from a production-oriented field to one centered on human judgment under automation, with profound implications for research, practice, and education.

</details>
