<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 46]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [From Ad-Hoc Scripts to Orchestrated Pipelines: Architecting a Resilient ELT Framework for Developer Productivity Metrics](https://arxiv.org/abs/2602.21568)
*Yuvraj Agrawal,Pallav Jain*

Main category: cs.SE

TL;DR: 该论文讨论了从传统的调度方式迁移到使用有向无环图（DAG）编排和奖牌架构的ELT管道的过程，以提高开发者生产力仪表盘的数据可靠性。通过分离数据提取与转换，并实施基于状态的依赖管理，提高了工程分析的可持续性。


<details>
  <summary>Details</summary>
Motivation: 开发人员生产力仪表板对于可视化DevOps性能指标至关重要，但这些仪表板的实用性经常因为数据可靠性问题而受到损害。在平台早期版本中，临时的摄取脚本导致数据缺口未被及时发现，降低了组织信任度。

Method: 迁移至一个健壮的Extract-Load-Transform (ELT) 管道，利用Directed Acyclic Graph (DAG) 编排以及Medallion架构来解决上述问题。方法包括解耦数据抽取与变换过程、确保原始历史记录不可变以便重新定义度量标准、并引入基于状态的依赖管理。

Result: 实现了更加可靠的数据处理流程，支持度量标准的重新定义而不丢失历史信息，并且通过基于状态的依赖管理增强了系统的鲁棒性。

Conclusion: 将度量管道视为生产级分布式系统是实现可持续工程分析的前提条件。

Abstract: Developer Productivity Dashboards are essential for visualizing DevOps performance metrics such as Deployment Frequency and Change Failure Rate (DORA). However, the utility of these dashboards is frequently undermined by data reliability issues. In early iterations of our platform, ad-hoc ingestion scripts (Cron jobs) led to "silent failures," where data gaps went undetected for days, eroding organizational trust. This paper reports on our experience migrating from legacy scheduling to a robust Extract-Load-Transform (ELT) pipeline using Directed Acyclic Graph (DAG) orchestration and Medallion Architecture. We detail the operational benefits of decoupling data extraction from transformation, the necessity of immutable raw history for metric redefinition, and the implementation of state-based dependency management. Our experience suggests that treating the metrics pipeline as a production-grade distributed system is a prerequisite for sustainable engineering analytics.

</details>


### [2] [Uncertainty Modeling for SysML v2](https://arxiv.org/abs/2602.21641)
*Man Zhang,Yunyang Li,Tao Yue*

Main category: cs.SE

TL;DR: 本文提出了一种将PSUM元模型系统地集成到SysML v2建模框架中的方法，以支持不确定性表示和传播，同时保持与SysML v2语法和语义的一致性。通过七个案例研究验证了该方法的有效性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现代工程系统中固有的不确定性需要在基于模型的系统工程（MBSE）中得到适当的处理。尽管OMG最近发布了用于不确定性建模的标准规范PSUM，并且SysML v2作为新一代系统建模语言也已发布，但后者缺乏与PSUM一致的原生构造来直接表示不确定性。

Method: 文章建议对SysML v2进行系统扩展，以便将PSUM元模型纳入其中，从而允许明确指定不确定性的来源、结构化描述不确定性以及在系统模型内一致地传播不确定性。

Result: 通过七项案例研究验证表明，提出的扩展(PSUM-SysMLv2)具有表达力并且适用于关注不确定性的MBSE，并可能实现不确定性及其传播分析。

Conclusion: 提出的SysML v2扩展能够有效支持不确定性建模，为不确定性感知的MBSE提供了一个有力工具。

Abstract: Uncertainty is inherent in modern engineered systems, including cyber-physical systems, autonomous systems, and large-scale software-intensive infrastructures (such as microservice-based systems) operating in dynamic and partially observable environments. The recent publication of Precise Semantics for Uncertainty Modeling (PSUM) by the Object Management Group represents the first standardized specification for uncertainty modeling within the Model-Based Systems Engineering (MBSE) community, providing formally defined semantics for representing and reasoning about uncertainty in models. In parallel, the second version of Systems Modeling Language (SysML v2) was released as the next-generation systems modeling language, offering improved semantic rigor and reusability, yet lacking native constructs aligned with PSUM for first-class uncertainty representation. This paper proposes a systematic extension of SysML v2 that incorporates the PSUM metamodel into its modeling framework. The extension enables explicit specification of indeterminacy sources, structured characterization of uncertainties, and consistent propagation of uncertainty within system models, while preserving conformance with SysML v2 syntax and semantics. We validate the approach through seven case studies. Results demonstrate that the proposed extension (PSUM-SysMLv2) is expressive and applicable for uncertainty-aware MBSE, and potentially enables uncertainty and uncertainty propagation analyses.

</details>


### [3] [EditFlow: Benchmarking and Optimizing Code Edit Recommendation Systems via Reconstruction of Developer Flows](https://arxiv.org/abs/2602.21697)
*Chenyan Liu,Yun Lin,Jiaxin Chang,Jiawei Liu,Binhang Qi,Bo Jiang,Zhiyong Huang,Jin Song Dong*

Main category: cs.SE

TL;DR: EditFlow旨在通过重建开发者的编辑流程来优化代码编辑推荐系统，解决现有大型语言模型在辅助编程时虽然技术准确但降低开发者生产力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型尽管在技术准确性方面表现出色，却未能有效提升开发者的实际生产力；使用AI辅助反而让任务完成速度减慢，并且大部分建议会打断开发者的思维流。问题核心在于这些模型基于静态提交快照训练而成，忽略了时间顺序信息以及与开发者自然推理过程相匹配的逐步、上下文敏感步骤。

Method: 提出EditFlow框架，它通过重建开发者的编辑流程来对后续代码编辑推荐系统进行基准测试和优化。该方法针对收集反映开发者编辑流程的数据难题、需要类似数字孪生的模拟环境以评估推荐性能、以及为不同规模和架构的异构系统提供统一优化策略这三大挑战提出解决方案。

Result: EditFlow能够更有效地支持开发者工作流程中的代码编辑推荐，潜在地提高了开发效率并减少了AI辅助带来的干扰。

Conclusion: 通过引入EditFlow，可以更好地使代码编辑推荐系统适应开发者的思维流动方式，从而提高整体生产力。

Abstract: Large language models (LLMs) for code editing have achieved remarkable progress, yet recent empirical studies reveal a fundamental disconnect between technical accuracy and developer productivity. Despite their strong benchmark performance, developers complete tasks 19% slower when using AI assistance, with over 68.81% of recommendations disrupting their mental flow. This misalignment stems from the use of static commit snapshots that lack temporal information, causing models to optimize for end results rather than the incremental, context-sensitive steps that align with developers' natural reasoning process.
  To bridge this gap, we present EditFlow, which benchmarks and optimizes subsequent code edit recommendation systems through the reconstruction of developer editing flows. EditFlow addresses three key challenges. First, collecting edit-order data that reflects developers' flow is inherently difficult: manual annotation introduces prohibitive overhead, while development logs capture only single trajectories instead of all plausible editing flows. Second, benchmarking recommendation performance against developers' ongoing editing flow requires a digital-twin-like simulation that can faithfully simulate the editing process. Third, existing heterogeneous systems vary drastically in scale and architecture, posing challenges for developing a unified optimization strategy that endows all models with mental-flow awareness regardless of design or capability.
  ......

</details>


### [4] [Proto-ML: An IDE for ML Solution Prototyping](https://arxiv.org/abs/2602.21734)
*Selin Coban,Miguel Perez,Horst Lichter*

Main category: cs.SE

TL;DR: 本文介绍了一种名为Proto-ML的IDE，旨在加强机器学习原型设计工作流程。通过提供三个扩展包来支持从原型实现、分析到知识管理的任务，Proto-ML有助于提高原型设计效率和促进透明度及可重用性更强的机器学习解决方案开发。


<details>
  <summary>Details</summary>
Motivation: 现有工具在支持机器学习解决方案开发过程中的有效协作和知识重用方面存在不足。为了克服这些限制，包括利益相关者参与不足、跨项目知识重用有限以及工具支持分散等问题，本研究提出了一个专门针对这些问题的新IDE。

Method: 提出并开发了Proto-ML IDE，该IDE由三个扩展包组成：原型实现、分析和知识管理。这些组件共同作用于改善整个开发过程中从评估原型质量到达成利益相关者期望等任务的支持情况。

Result: 初步用户反馈表明，使用Proto-ML可以提高原型设计的效率，并且有助于创建更加透明和易于重用的机器学习解决方案。

Conclusion: Proto-ML作为一个为机器学习原型设计量身定制的IDE，展现了其在提升团队协作效率、促进知识共享以及增强解决方案可重用性方面的潜力。

Abstract: Prototyping plays a critical role in the development of machine learning (ML) solutions, yet existing tools often provide limited support for effective collaboration and knowledge reuse among stakeholders. This paper introduces Proto-ML, an IDE designed to strengthen ML prototyping workflows. By addressing key deficiencies such as insufficient stakeholder involvement, limited cross-project knowledge reuse, and fragmented tool support, Proto-ML offers a unified framework that enables structured documentation of prototyping activities and promotes knowledge sharing across projects.
  The Proto-ML IDE consists of three extension bundles: prototype implementation, analysis, and knowledge management. These extensions support tasks ranging from evaluating prototype quality against defined criteria to incorporating stakeholder perspectives throughout the development process. Preliminary user feedback suggests that Proto-ML can increase prototyping efficiency and foster more transparent and reusable ML solution development.

</details>


### [5] [An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention](https://arxiv.org/abs/2602.21800)
*Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 本文探讨了零样本、仅推理的方法，以改进位置编码和优化注意力机制，从而解决大型语言模型在处理长代码序列时由于固定上下文长度限制导致的泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在软件工程领域取得了显著进步，能够执行各种与代码相关的任务，但其有效性受到固定上下文长度的限制，特别是在处理长且特定领域的代码序列时表现不佳。

Method: 研究采用了零样本、仅推理的方法来探索如何通过改进位置编码和优化注意力机制来促进代码中的上下文长度外推。

Result: 提供了对当前有助于代码中上下文长度外推方法的全面分析，尤其是在长代码补全任务背景下。

Conclusion: 通过改善位置编码和优化注意力机制，可以提高大型语言模型处理长代码序列的能力，从而克服固定上下文长度带来的局限性。

Abstract: The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.

</details>


### [6] [From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models](https://arxiv.org/abs/2602.21833)
*Norman Peitek,Julia Hess,Sven Apel*

Main category: cs.SE

TL;DR: 本研究系统地探讨了大型语言模型（LLMs）在代码重构以提高可读性方面的能力，通过使用GPT5.1对230个Java代码片段进行五轮迭代实验，揭示了模型在重构过程中表现出的趋同性和稳定性，并指出明确提示特定可读性因素对重构动态有轻微影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型可以快速重构代码，但其质量可能存在不一致性和不可预测的行为。因此，本研究旨在探索这些模型在改进代码可读性的重构任务中的能力。

Method: 研究采用了大规模实验方法，利用GPT5.1处理230段Java代码，在不同提示策略下进行了关于代码可读性的五个迭代周期的重构。此外，还对重构过程中的细粒度代码变更进行了分类分析，并检查了功能正确性和结果稳健性。

Result: 研究发现表明：一、迭代式代码重构经历了一个初始重组阶段后趋于稳定；二、这种收敛模式对于不同的代码变体来说是相当稳健的；三、针对特定可读性因素的具体提示稍微改变了重构动态。

Conclusion: 这些发现为评估基于LLM的代码重构可靠性提供了实证基础，并为未来的研究方向开辟了道路，比如跨模型比较分析以及LLM重构代码中其他软件质量维度的系统评估。

Abstract: Large language models (LLMs) are increasingly used for automated code refactoring tasks. Although these models can quickly refactor code, the quality may exhibit inconsistencies and unpredictable behavior. In this article, we systematically study the capabilities of LLMs for code refactoring with a specific focus on improving code readability.
  We conducted a large-scale experiment using GPT5.1 with 230 Java snippets, each systematically varied and refactored regarding code readability across five iterations under three different prompting strategies. We categorized fine-grained code changes during the refactoring into implementation, syntactic, and comment-level transformations. Subsequently, we investigated the functional correctness and tested the robustness of the results with novel snippets.
  Our results reveal three main insights: First, iterative code refactoring exhibits an initial phase of restructuring followed by stabilization. This convergence tendency suggests that LLMs possess an internalized understanding of an "optimally readable" version of code. Second, convergence patterns are fairly robust across different code variants. Third, explicit prompting toward specific readability factors slightly influences the refactoring dynamics.
  These insights provide an empirical foundation for assessing the reliability of LLM-assisted code refactoring, which opens pathways for future research, including comparative analyses across models and a systematic evaluation of additional software quality dimensions in LLM-refactored code.

</details>


### [7] [Enhancing LLM-Based Test Generation by Eliminating Covered Code](https://arxiv.org/abs/2602.21997)
*WeiZhe Xu,Mengyu Liu,Fanxin Kong*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型的可扩展单元测试生成方法，通过上下文信息检索和迭代式代码消除测试生成两步法，有效提高了对复杂方法的测试覆盖率，并在开源项目评估中超越了现有基于大语言模型和搜索的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型（LLM）的测试生成方案对于小而孤立的代码片段表现良好，但在处理复杂的待测方法时遇到困难。为了解决这个问题并提高测试覆盖率，研究者们开发了一种新的、可扩展的单元测试生成方法。

Method: 该方法包括两个关键步骤：首先，利用大语言模型与静态分析技术相结合进行上下文信息检索，收集与待测复杂方法相关的背景信息；其次，采取一种迭代式的测试生成策略，伴随代码段的逐步排除，持续生成针对特定代码片段的单元测试，同时跟踪已实现的覆盖率情况。

Result: 经过对多个开源项目的全面评估，所提出的方法相较于当前最先进的基于大语言模型及搜索的技术，在提升复杂方法测试覆盖率方面表现出色。

Conclusion: 这项工作展示了一种创新性的途径来增强自动化测试生成过程中的覆盖效率，特别是在面对具有挑战性的复杂软件组件时。

Abstract: Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.

</details>


### [8] [Detecting UX smells in Visual Studio Code using LLMs](https://arxiv.org/abs/2602.22020)
*Andrés Rodriguez,Juan Cruz Gardey,Alejandra Garrido*

Main category: cs.SE

TL;DR: 本研究通过挖掘和分类GitHub仓库中用户报告的问题，提出了一种利用LLM辅助的方法来检测Visual Studio Code中的用户体验问题（UX smells）。结果表明，大多数的用户体验问题集中在信息性、清晰度、直观性和效率上，这些都是开发者最看重的品质。


<details>
  <summary>Details</summary>
Motivation: 集成开发环境对开发者的日常体验有很大影响，但对其可用性和用户体验的研究仍然有限。为了改善这一情况，研究者们提出了一个新方法来识别这些工具中存在的用户体验问题。

Method: 该研究采用了基于大语言模型（LLM）的帮助，从Visual Studio Code的GitHub仓库中挖掘并分类了用户报告的问题。接着，使用经过验证的分类法及专家评审来识别出常见的用户体验问题。

Result: 研究结果显示，大部分用户体验问题主要出现在信息性、清晰度、直观性以及效率这几个方面，而这些都是开发者最为重视的质量特性。

Conclusion: 这项工作强调了在IDEs中提升用户体验的重要性，并指出未来改进的方向应集中在提高信息呈现质量、界面清晰度、操作直观性以及整体效率等方面。

Abstract: Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.

</details>


### [9] [Visual Milestone Planning in a Hybrid Development Context](https://arxiv.org/abs/2602.22076)
*Eduardo Miranda*

Main category: cs.SE

TL;DR: 本文介绍了视觉里程碑规划（VMP）方法，它采用敏捷词汇以促进敏捷实践者接受此方法作为混合开发过程的前端。通过使用一种称为里程碑计划矩阵的新构造来记录产品待办事项到里程碑的分配，并通过类似俄罗斯方块游戏的方式在资源和时间比例调度画布上安排工作包来确定里程碑的到期日。


<details>
  <summary>Details</summary>
Motivation: 为了使敏捷实践者能够更容易地采用视觉里程碑规划方法，作为混合开发流程的一部分，从而提高团队成员之间的共同理解和承诺。

Method: 介绍了一种名为视觉里程碑规划（VMP）的方法，其中包括了里程碑计划矩阵这一新工具来记录产品待办事项与里程碑之间的关联；并通过将代表工作的便签分组放入被称为工作包的时间盒中，在一个类似于玩俄罗斯方块游戏的过程中，于资源及时间标度计划画布上进行布局调整，以此来决定各里程碑的具体截止日期。

Result: 提出的方法为敏捷团队提供了一个可视化且协作式的规划途径，有助于增强对工作方法的共识以及团队成员间对于计划执行的责任感。

Conclusion: 视觉里程碑规划（VMP）作为一种结合了敏捷术语的直观合作规划手段，可以有效地帮助敏捷从业者理解并参与到项目规划过程中，同时支持更灵活的开发流程。

Abstract: This paper explains the Visual Milestone Planning (VMP) method using an agile vocabulary to facilitate its adoption by agile practitioners as a front end for a hybrid development process. VMP is a visual and collaborative planning approach which promotes a shared understanding of the work approach and commitment through the direct manipulation by team members of the reified planning constructs involved in the development of the plan. Once the product backlog has been established and relevant milestones identified, a novel construct called the milestone planning matrix is used to document the allocation of product backlog items to milestones. The milestones due dates are later determined by grouping sticky notes representing the work to be performed into time-boxes called work packages and accommodating them on a resource and time scaled scheduling canvas very much as it would be done in a Tetris game.

</details>


### [10] [SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents](https://arxiv.org/abs/2602.22124)
*Patrick Tser Jern Kon,Archana Pradeep,Ang Chen,Alexander P. Ellis,Warren Hunt,Zijian Wang,John Yang,Samuel Thompson*

Main category: cs.SE

TL;DR: 介绍了SWE-Protégé框架，通过将软件修复视为专家-学徒合作问题，使小型语言模型能够在长周期软件工程任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在成本、延迟和适应性方面具有优势，但在处理长周期软件工程任务时效果不佳，存在动作循环和低解决率的问题。

Method: 提出了SWE-Protégé框架，该框架让小型语言模型作为唯一决策者，同时学会从强大的专家模型那里有选择地寻求指导、识别停滞状态，并遵循专家反馈。方法结合了基于专家增强轨迹的监督微调以及鼓励避免退化循环和非生产性专家协作的自主强化学习。

Result: 通过对Qwen2.5-Coder-7B-Instruct进行轻度后训练，在SWE-bench Verified上达到了42.4%的Pass@1准确率，相较于先前的小型语言模型技术有了+25.4%的改进，同时仅少量使用专家帮助（每个任务约4次调用和总token的11%）。

Conclusion: SWE-Protégé提供了一种有效的方法来提高小型语言模型在复杂软件工程任务中的性能，通过优化与专家模型的合作方式显著提升了成功率。

Abstract: Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [11] [Topological Relational Theory: A Simplicial-Complex View of Functional Dependencies, Lossless Decomposition, and Acyclicity](https://arxiv.org/abs/2602.21213)
*Bilge Senturk,Faruk Alpay*

Main category: cs.DB

TL;DR: 本文通过将函数依赖编码为抽象单纯复形的单纯形，开发了一种拓扑视角来研究关系模式设计。定义了单纯正则形式（SNF），并重新表述了经典的无损连接分解准则。此外，还展示了如何使用诱导子复形覆盖的神经来提供可计算证明，并讨论了边界矩阵计算Betti数作为轻量级模式诊断的应用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过引入拓扑学概念到关系数据库模式设计中，以新的方式理解和处理函数依赖及模式分解问题。通过这种方式，可以更有效地诊断和解决循环依赖结构等问题。

Method: 1. 将函数依赖编码为抽象单纯复形的单纯形。
2. 定义单纯正则形式（SNF）作为正维度上依赖复形的同调非循环性。
3. 用拓扑方法重新表述了无损连接分解的标准。
4. 展示了利用诱导子复形覆盖的神经来检测阻碍连接树结构的一阶循环。
5. 提出了通过边界矩阵计算Betti数的方法，用于识别未解释的依赖循环。

Result: 1. 成功地将拓扑学概念应用于关系模式设计中，提供了诊断循环依赖结构的新工具。
2. 给出了无损连接分解的一个新的拓扑解释。
3. 展示了如何使用神经中的1-循环来识别多路分解中的连接行为问题。
4. 提出了一种基于Betti数计算的轻量级模式诊断方法，能够补充标准FD-chase测试。

Conclusion: 通过引入拓扑学的概念和技术，本文为关系模式设计提供了一个新颖且强大的框架，特别是对于理解复杂依赖结构以及进行有效的模式分解具有重要意义。所提出的方法不仅增强了对现有问题的理解，也为未来的研究开辟了新方向。

Abstract: We develop a topological lens on relational schema design by encoding functional dependencies (FDs) as simplices of an abstract simplicial complex. This dependency complex exposes multi-attribute interactions and enables homological invariants (Betti numbers) to diagnose cyclic dependency structure. We define Simplicial Normal Form (SNF) as homological acyclicity of the dependency complex in positive dimensions, i.e., vanishing reduced homology for all $n \ge 1$. SNF is intentionally weaker than contractibility and does not identify homology with homotopy. For decompositions, we give a topological reformulation of the classical binary lossless-join criterion: assuming dependency preservation, a decomposition is lossless exactly when the intersection attributes form a key for at least one component. Topologically, this yields a strong deformation retraction that trivializes the relevant Mayer--Vietoris boundary map. For multiway decompositions, we show how the nerve of a cover by induced subcomplexes provides a computable certificate: a 1-cycle in the nerve (detected by $H_1$) obstructs join-tree structure and aligns with cyclic join behavior in acyclic-scheme theory. Finally, we discuss an algorithmic consequence: Betti numbers of the dependency complex (or of a decomposition nerve) can be computed from boundary matrices and used as a lightweight schema diagnostic to localize "unexplained" dependency cycles, complementing standard FD-chase tests.

</details>


### [12] [Premature Dimensional Collapse and Tensor-based Execution Paths for High-Dimensional Relational Operations in Cost-Based Database Systems](https://arxiv.org/abs/2602.21237)
*Il-Sun Chang*

Main category: cs.DB

TL;DR: 该论文提出了一种基于张量的执行路径，通过延迟过早线性化并保持高维局部性来解决关系操作在内存压力下导致的执行不稳定和尾部延迟放大问题。实验表明，在受限的内存设置下，相比传统方法多秒的P99延迟，所提方法能将P99延迟降低到秒级以下。


<details>
  <summary>Details</summary>
Motivation: 当前的成本基础数据库管理系统(DBMS)在处理高维度关系运算时，由于内存限制经常触发哈希表溢出及外部物化等现象，导致执行不稳定性和尾部延迟放大。研究发现，中间表示在内存压力下过早线性化会引发不成比例的I/O放大以及类似相变的延迟行为。

Method: 提出了一种基于张量的执行策略，旨在延缓这种过早的线性化过程，并通过后期物化与结构化的中间布局保持更高的维度局部性。

Result: 利用修改后的PostgreSQL原型系统和受控微基准测试显示，在内存受限条件下（例如work_mem=1MB），传统执行方式可能产生数百兆字节的数据溢出，并且P99延迟超过数秒；而提议的方法则能够保持稳定的执行性能，并将P99延迟减少至亚秒级别。

Conclusion: 结果表明，表示时机是执行稳定性的一个关键设计变量，这补充了以往集中在基数估计和操作符吞吐量上的优化工作。

Abstract: Modern cost-based DBMSs frequently exhibit execution instability and tail-latency amplification when high-dimensional relational operations trigger memory-regime transitions such as hash-table spilling and external materialization. We identify a structural failure mode in which intermediate representations are prematurely linearized under memory pressure, causing disproportionate I/O amplification and phase-transition-like latency behavior. To mitigate this, we propose a tensor-based execution path that delays premature linearization and preserves higher-dimensional locality through late materialization and structured intermediate layouts. Using a modified PostgreSQL-based prototype and controlled microbenchmarks, we show that under constrained memory settings (e.g., work_mem=1MB) conventional execution can spill hundreds of megabytes and exceed multi-second P99 latency, while the proposed path maintains stable execution and reduces P99 latency to sub-second levels. Our results suggest that representation timing is a first-class design variable for execution stability, complementing traditional optimization efforts focused on cardinality estimation and operator throughput.

</details>


### [13] [PiPNN: Ultra-Scalable Graph-Based Nearest Neighbor Indexing](https://arxiv.org/abs/2602.21247)
*Tobias Rubel,Richard Wen,Laxman Dhulipala,Lars Gottesbüren,Rajesh Jayaram,Jakub Łącki*

Main category: cs.DB

TL;DR: PiPNN, a new graph construction algorithm for Approximate Nearest Neighbor Search, significantly reduces the time to build high-quality indexes by using an innovative online pruning method called HashPrune. It outperforms existing methods in terms of speed and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind PiPNN is to address the slow index building times of current state-of-the-art graph-based methods (e.g., HNSW, Vamana) for Approximate Nearest Neighbor Search, which are hampered by their reliance on random-access-heavy beam searches.

Method: PiPNN introduces HashPrune, an online pruning algorithm that dynamically maintains sparse collections of edges. This allows PiPNN to partition the dataset into overlapping sub-problems, perform bulk distance comparisons efficiently through dense matrix multiplication, and stream a subset of edges into HashPrune, all while ensuring bounded memory usage during index construction.

Result: PiPNN can build state-of-the-art indexes up to 11.6x faster than Vamana, 12.9x faster than HNSW, 19.1x faster than MIRAGE, and 17.3x faster than FastKCNA. It also achieves higher query throughput and enables the creation of high-quality ANN indexes on billion-scale datasets in under 20 minutes using a single multicore machine.

Conclusion: By overcoming the 'search bottleneck' and offering superior scalability, PiPNN represents a significant advancement in the field of Approximate Nearest Neighbor Search, enabling faster and more efficient index construction without compromising on quality.

Abstract: The fastest indexes for Approximate Nearest Neighbor Search today are also the slowest to build: graph-based methods like HNSW and Vamana achieve state-of-the-art query performance but have large construction times due to relying on random-access-heavy beam searches. We introduce PiPNN (Pick-in-Partitions Nearest Neighbors), an ultra-scalable graph construction algorithm that avoids this ``search bottleneck'' that existing graph-based methods suffer from.
  PiPNN's core innovation is HashPrune, a novel online pruning algorithm which dynamically maintains sparse collections of edges. HashPrune enables PiPNN to partition the dataset into overlapping sub-problems, efficiently perform bulk distance comparisons via dense matrix multiplication kernels, and stream a subset of the edges into HashPrune. HashPrune guarantees bounded memory during index construction which permits PiPNN to build higher quality indices without the use of extra intermediate memory.
  PiPNN builds state-of-the-art indexes up to 11.6x faster than Vamana (DiskANN) and up to 12.9x faster than HNSW. PiPNN is significantly more scalable than recent algorithms for fast graph construction. PiPNN builds indexes at least 19.1x faster than MIRAGE and 17.3x than FastKCNA while producing indexes that achieve higher query throughput. PiPNN enables us to build, for the first time, high-quality ANN indexes on billion-scale datasets in under 20 minutes using a single multicore machine.

</details>


### [14] [BuffCut: Prioritized Buffered Streaming Graph Partitioning](https://arxiv.org/abs/2602.21248)
*Linus Baumgärtner,Adil Chhabra,Marcelo Fonseca Faraj,Christian Schulz*

Main category: cs.DB

TL;DR: 提出了一种名为BuffCut的新型缓冲流式图分割算法，通过结合优先级缓冲与批量多级分配策略，在保持资源效率的同时提高了对抗性流顺序下的分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有的流式图分割算法虽然在处理大规模数据时具有很高的资源效率和可扩展性，但其对于数据流顺序非常敏感，导致边缘切割数量往往高于内存中的方法。因此，研究者旨在开发一种新的方法来缩小这种质量差距，特别是在不利的流顺序下。

Method: BuffCut算法通过维护一个有界的优先级缓冲区延迟低效决策，并调节节点分配的顺序；同时，它通过迭代地将最高优先级的节点从缓冲区插入到批次中，逐步构建具有高局部性的批次，并对每个批次采用多级分配算法进行分配。

Result: 实验结果表明，无论是在真实世界还是合成图上，BuffCut的表现都优于目前最先进的缓冲流式方法。相较于最强的基于优先级缓冲基线方法，BuffCut实现了20.8%的更少边缘切割，运行速度提高了2.9倍，使用的内存减少了11.3倍。相比次优缓冲方法，它在仅增加1.8倍运行时间和1.09倍内存消耗的情况下，减少了15.8%的边缘切割。

Conclusion: 本研究表明，通过引入优先级缓冲机制和批量多级分配策略，可以显著提高流式图分割的质量，即使面对不利的数据流顺序也能有效降低边缘切割的数量。

Abstract: Streaming graph partitioners enable resource-efficient and massively scalable partitioning, but one-pass assignment heuristics are highly sensitive to stream order and often yield substantially higher edge cuts than in-memory methods. We present BuffCut, a buffered streaming partitioner that narrows this quality gap, particularly when stream ordering is adversarial, by combining prioritized buffering with batch-wise multilevel assignment. BuffCut maintains a bounded priority buffer to delay poorly informed decisions and regulate the order in which nodes are considered for assignment. It incrementally constructs high-locality batches of configurable size by iteratively inserting the highest-priority nodes from the buffer into the batch, effectively recovering locality structure from the stream. Each batch is then assigned via a multilevel partitioning algorithm. Experiments on diverse real-world and synthetic graphs show that BuffCut consistently outperforms state-of-the-art buffered streaming methods. Compared to the strongest prioritized buffering baseline, BuffCut achieves 20.8% fewer edge cuts while running 2.9 times faster and using 11.3 times less memory. Against the next-best buffered method, it reduces edge cut by 15.8% with only modest overheads of 1.8 times runtime and 1.09 times memory.

</details>


### [15] [Quality of Descriptive Information on Cultural Heritage Objects: Definition and Empirical Evaluation](https://arxiv.org/abs/2602.21249)
*Markus Matoni,Arno Kesper,Gabriele Taentzer*

Main category: cs.DB

TL;DR: 本文针对文化遗产对象描述信息的质量问题，定义了一组质量维度，并通过实际案例验证了这些定义的有效性，从而为该领域提供了全面的数据质量定义。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏广泛接受的、独立于领域的数据质量定义，尤其是针对文化遗产对象描述信息的质量评估框架尚不完善。现有的质量定义往往过于理论化，缺乏基于实际数据问题的实证验证。

Method: 首先，基于对现有维度的深入分析，定义了一组专门用于捕捉文化遗产对象描述信息特征的质量维度，并通过领域特定的例子加以说明。然后，使用从文化遗产领域收集的一系列实际数据质量问题来评估所提出质量定义的实际适用性。

Result: 通过实证评估支持了提出的质量定义，为文化遗产领域的描述信息提供了一个全面且实用的数据质量定义。

Conclusion: 本研究成功地提出了一个适用于文化遗产对象描述信息的数据质量定义框架，并通过实证分析证明了其有效性，填补了这一领域内数据质量管理上的空白。

Abstract: Effective data processing depends on the quality of the underlying data. However, quality issues such as inconsistencies and uncertainties, can significantly impede the processing and subsequent use of data. Despite the centrality of data quality to a wide range of computational tasks, there is currently no broadly accepted, domain-independent consensus on the definition of data quality. Existing frameworks primarily define data quality in ways that are tailored to specific domains, data types, or contexts of use. Although quality assessment frameworks exist for specific domains, such as electronic health record data and linked data, corresponding approaches for descriptive information about cultural heritage objects remain underdeveloped. Moreover, existing quality definitions are often theoretical in nature and lack empirical validation based on real-world data problems. In this paper, we address these limitations by first defining a set of quality dimensions specifically designed to capture the characteristics of descriptive information about cultural heritage objects. Our definition is based on an in-depth analysis of existing dimensions and is illustrated through domain-specific examples. We then evaluate the practical applicability of our proposed quality definition using a curated set of real-world data quality problems from the cultural heritage domain. This empirical evaluation substantiates our definition of data quality, resulting in a comprehensive definition of data quality in this domain.

</details>


### [16] [I/O Optimizations for Graph-Based Disk-Resident Approximate Nearest Neighbor Search: A Design Space Exploration](https://arxiv.org/abs/2602.21514)
*Liang Li,Shufeng Gong,Yanan Yang,Yiduo Wang,Jie Wu*

Main category: cs.DB

TL;DR: 本文提出了一种针对基于SSD的近似最近邻搜索的I/O优先框架，通过内存布局、磁盘布局和搜索算法三个维度组织技术，并引入了一个页级复杂性模型来解释页面局部性和路径长度如何共同决定页面读取。通过在四个公开数据集上的一致实现，研究了单因素效应和跨维度协同作用。提出的OctopusANN系统相比现有最先进系统Starling和DiskANN，在匹配的召回率下实现了更高的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 近似最近邻（ANN）搜索在基于SSD的支持索引中变得越来越受I/O限制（I/O占查询延迟的70-90%）。面对这一挑战，作者们旨在开发一种新的方法来优化磁盘上的ANN性能，减少I/O瓶颈并提高搜索效率。

Method: 提出一个I/O优先框架，该框架沿内存布局、磁盘布局以及搜索算法三个方向组织技术；引入了页级复杂度模型来解释页面局部性和路径长度如何共同影响页面读取次数，并通过实证验证了该模型的有效性。

Result: 发现内存驻留导航与动态宽度单独使用时提供了最强劲的性能提升；页面重排和页面搜索单独效果较弱但联合使用时具有互补优势；综合考虑上述因素设计的OctopusANN系统比现有最先进系统Starling提高了4.1-37.9%的吞吐量，在保持Recall@10=90%的情况下，相较于DiskANN提升了87.5-149.5%的吞吐量。

Conclusion: 本研究表明，通过系统地组合不同技术而非孤立调整，可以显著改善基于磁盘的ANN搜索性能。为不同并发级别及精度约束条件下的存储中心或混合设计方案的选择提供了可操作指南。

Abstract: Approximate nearest neighbor (ANN) search on SSD-backed indexes is increasingly I/O-bound (I/O accounts for 70--90\% of query latency). We present an I/O-first framework for disk-based ANN that organizes techniques along three dimensions: memory layout, disk layout, and search algorithm. We introduce a page-level complexity model that explains how page locality and path length jointly determine page reads, and we validate the model empirically. Using consistent implementations across four public datasets, we quantify both single-factor effects and cross-dimensional synergies. We find that (i) memory-resident navigation and dynamic width provide the strongest standalone gains; (ii) page shuffle and page search are weak alone but complementary together; and (iii) a principled composition, OctopusANN, substantially reduces I/O and achieves 4.1--37.9\% higher throughput than the state-of-the-art system Starling and 87.5--149.5\% higher throughput than DiskANN at matched Recall@10=90\%. Finally, we distill actionable guidelines for selecting storage-centric or hybrid designs across diverse concurrency levels and accuracy constraints, advocating systematic composition rather than isolated tweaks when pushing the performance frontier of disk-based ANN.

</details>


### [17] [RAC: Relation-Aware Cache Replacement for Large Language Models](https://arxiv.org/abs/2602.21547)
*Yuchong Wu,Zihuan Xu,Wangze Ni,Peng Cheng,Lei Chen,Xuemin Lin,Heng Tao Shen,Kui Ren*

Main category: cs.DB

TL;DR: 提出了一种名为Relation-Aware Cache (RAC)的在线淘汰策略，通过利用请求之间的语义关系来指导缓存淘汰决策，旨在解决大规模语言模型服务在扩展时遇到的成本和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLM）服务的扩展面临着显著的成本和延迟挑战，而现有的缓存替换策略基于如最近使用或频率这样的有限窗口统计信息，在实际LLM工作负载中表现不佳，这些工作负载表现出长重用距离和局部稀疏重复性。

Method: 提出了Relation-Aware Cache (RAC)，一种在线淘汰策略，它利用请求间的语义关系来指导淘汰决策。该方法综合了两种关联感知信号：主题普遍性（Topical Prevalence），用于捕捉长期重用；以及结构重要性（Structural Importance），利用局部主题内依赖结构来区分条目对未来重用的价值。

Result: 广泛的评估表明，RAC能够在多样化的工作负载下保持高效性，在缓存命中率方面比最先进的基线高出20%到30%。

Conclusion: 本研究证明了RAC作为一种创新的缓存管理方案，能够有效应对大型语言模型带来的成本与延迟挑战，为相关领域提供了新的解决方案。

Abstract: The scaling of Large Language Model (LLM) services faces significant cost and latency challenges, making effective caching under tight capacity crucial. Existing cache replacement policies, from heuristics to learning-based methods, predominantly rely on limited-window statistics such as recency and frequency. We show these signals are not robust for real-world LLM workloads, which exhibit long reuse distances and sparse local recurrence.
  To address these limitations, we propose Relation-Aware Cache (RAC), an online eviction strategy that leverages semantic relations among requests to guide eviction decisions. RAC synthesizes two relation-aware signals: (1) Topical Prevalence, which aggregates access evidence at the topic level to capture long-horizon reuse; and (2) Structural Importance, which leverages local intra-topic dependency structure to discriminate entries by their future reuse value. Extensive evaluations show that RAC maintains high effectiveness across diverse workloads, consistently surpassing state-of-the-art baselines by 20%--30% in cache hit ratio.

</details>


### [18] [Quantum Computing for Query Containment of Conjunctive Queries](https://arxiv.org/abs/2602.21803)
*Luisa Gerlach,Tobias Köppl,Renè Zander,Nicole Schweikardt,Stefanie Scherzinger*

Main category: cs.DB

TL;DR: 本文首次尝试将量子计算应用于集合语义下的合取查询的查询包含问题，通过将其转化为可在基于门的量子硬件上解决的优化问题，并在某些情况下直接映射到量子退火器。实验表明该方法是有效且可扩展的，为查询包含问题提供了一个新的计算视角。


<details>
  <summary>Details</summary>
Motivation: 尽管查询包含问题是数据库研究中的一个基础问题，并且在理论研究中得到了广泛探讨，但由于其背后的决策问题具有较高的计算复杂度甚至有时不可判定，因此商业数据库系统尚未充分利用由查询包含带来的优化机会。

Method: 提出了一种新的形式化方法，将查询包含问题作为优化问题来处理，该问题可以在基于门的量子硬件上求解，并且在某些情况下可以直接映射到量子退火器上。

Result: 正式证明了该形式化的正确性，并通过模拟软件和量子设备进行了原型实现评估。实验成功地展示了所提方法的有效性和可扩展性，在当前量子硬件限制内工作良好。

Conclusion: 本研究展示量子优化可以有效地解决查询包含问题，为这一领域贡献了一个新的计算视角。

Abstract: We address the problem of checking query containment, a foundational problem in database research. Although extensively studied in theory research, optimization opportunities arising from query containment are not fully leveraged in commercial database systems, due to the high computational complexity and sometimes even undecidability of the underlying decision problem. In this article, we present the first approach to applying quantum computing to the query containment problem for conjunctive queries under set semantics. We propose a novel formulation as an optimization problem that can be solved on gate-based quantum hardware, and in some cases directly maps to quantum annealers. We formally prove this formulation to be correct and present a prototype implementation which we evaluate using simulator software as well as quantum devices. Our experiments successfully demonstrate that our approach is sound and scales within the current limitations of quantum hardware. In doing so, we show that quantum optimization can effectively address this problem. Thereby, we contribute a new computational perspective on the query containment problem.

</details>


### [19] [Detecting Logic Bugs of Join Optimizations in DBMS](https://arxiv.org/abs/2602.21955)
*Xiu Tang,Sai Wu,Dongxiang Zhang,Feifei Li,Gang Chen*

Main category: cs.DB

TL;DR: 本文提出了一种新的测试框架TQS，旨在检测涉及多表连接查询的逻辑错误。通过数据导向的模式和查询生成（DSG）以及知识导向的查询空间探索（KQE）两个关键组件，TQS能够有效地找到数据库管理系统中关于连接优化的逻辑错误，并在四个流行的DBMS上进行了评估，成功检测到了115个bug。


<details>
  <summary>Details</summary>
Motivation: 现有的基于生成的调试工具仅限于单表查询，对于包含连接操作符的多表查询的研究存在明显空白。为了填补这一空白，特别是针对由查询优化器不当实现引起的逻辑错误，提出了新的解决方案。

Method: TQS采用了数据导向的模式和查询生成（DSG）与知识导向的查询空间探索（KQE）。其中，DSG利用数据库规范化技术生成测试架构并通过位图索引追踪结果；而KQE则将问题形式化为同构图集发现，并结合图嵌入与加权随机游走以生成查询。

Result: 实验结果表明，TQS能够在MySQL、MariaDB、TiDB及一个匿名的行业领先云原生数据库X-DB上有效识别出由于连接优化导致的逻辑错误，在24小时内共发现了115个bug。

Conclusion: TQS证明了其在检测数据库管理系统中因多表连接查询所引发逻辑错误的有效性，展示了其作为自动化测试工具的强大潜力。

Abstract: Generation-based testing techniques have shown their effectiveness in detecting logic bugs of DBMS, which are often caused by improper implementation of query optimizers. Nonetheless, existing generation-based debug tools are limited to single-table queries and there is a substantial research gap regarding multi-table queries with join operators. In this paper, we propose TQS, a novel testing framework targeted at detecting logic bugs derived by queries involving multi-table joins. Given a target DBMS, TQS achieves the goal with two key components: Data-guided Schema and Query Generation (DSG) and Knowledge-guided Query Space Exploration (KQE). DSG addresses the key challenge of multi-table query debugging: how to generate ground-truth (query, result) pairs for verification. It adopts the database normalization technique to generate a testing schema and maintains a bitmap index for result tracking. To improve debug efficiency, DSG also artificially inserts some noises into the generated data. To avoid repetitive query space search, KQE forms the problem as isomorphic graph set discovery and combines the graph embedding and weighted random walk for query generation. We evaluated TQS on four popular DBMSs: MySQL, MariaDB, TiDB and the gray release of an industry-leading cloud-native database, anonymized as X-DB. Experimental results show that TQS is effective in finding logic bugs of join optimization in database management systems. It successfully detected 115 bugs within 24 hours, including 31 bugs in MySQL, 30 in MariaDB, 31 in TiDB, and 23 in X-DB respectively.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [20] [A Generic Web Component for WebRTC Pub-Sub](https://arxiv.org/abs/2602.22011)
*Kundan Singh*

Main category: cs.MM

TL;DR: 视频-io是一个通用的Web组件，用于在WebRTC应用中发布或订阅媒体流。它使用命名流抽象而非通话或会议室抽象，使得应用程序逻辑保留在端点处，并允许开发者编写一次代码即可在不同服务器或服务上运行web应用。


<details>
  <summary>Details</summary>
Motivation: 当前的视频会议服务主要采用通话或会议室抽象，这限制了其应用场景。为了超越这些限制并促进端点处的创新，作者提出了视频-io组件。

Method: 视频-io通过使用命名流抽象来实现更广泛的应用场景。大部分应用程序逻辑被保持在端点处，而特定于供应商的访问控制或信号协商则由服务特定连接器实现负责处理。

Result: 展示了视频-io组件的灵活性，通过为十个不同的现有系统和服务实现连接器。这种方式让web应用能够脱离特定供应商的服务，从而促进终端用户层面的更多创新。

Conclusion: 视频-io提供了一种新的方法来构建WebRTC应用，它不仅限于传统的通话或会议场景，而且支持跨平台和跨服务部署，鼓励了更多的技术创新。

Abstract: We present video-io, a generic web component to publish or subscribe to a media stream in WebRTC (web real-time communication) applications. Unlike a call or conference room abstraction of existing video conferencing services, it uses a named stream abstraction, which is useful in many scenarios beyond just a call or conference. It keeps most of the application logic in the endpoint using the extensive application interface of this component, and keeps any vendor specific access control or signaling negotiation in a service-specific connector implementation. This allows an app developer to write once, and be able to run the web app on different servers or services. We also demonstrate its flexibility by implementing the connector for ten different existing systems and services. Decoupling the app from the hosted vendor service promotes innovation in the endpoint beyond what a single vendor locked client app can offer.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [21] [General Convex Agreement with Near-Optimal Communication](https://arxiv.org/abs/2602.21411)
*Marc Dufay,Diana Ghinea,Anton Paramonov*

Main category: cs.DC

TL;DR: 本文研究了凸协议（CA）在抽象凸性空间中的通信复杂度问题，提出了具有接近最优通信复杂度的确定性同步CA协议，并利用提取器图实现了对适应性对手有弹性的参与者到委员会的确定性分配。


<details>
  <summary>Details</summary>
Motivation: 凸协议（CA）是拜占庭协议（BA）的一种强化形式，要求达成一致的输出位于诚实参与者的输入的凸包内。这种有效性条件受到实际聚合任务（如鲁棒学习或传感器融合）的驱动，在这些任务中，诚实输入不必完全相同，但应该仍然约束决策。虽然标准方法对于大L位输入的CA具有Θ(Ln^2)的通信复杂度，但与BA的下界Ω(Ln)比特相比存在差距。

Method: 作者们探讨了抽象凸性空间中的这一差距，并提出了具有近似最优通信复杂度的确定性同步CA协议。当L = Ω(n · κ)，其中κ为安全参数时，对于有限的凸性空间，他们实现了O(L· nlog n)的通信量；对于欧几里得空间R^d，则实现了O(L· n^(1+o(1)))的通信量。此外，通过使用提取器图来实现对适应性对手有弹性的参与者到委员会的确定性分配是他们的主要技术贡献。

Result: 所提出的协议具有渐近最优轮复杂度O(n)，并且当输入长度L预先固定时，对于任何常数ε>0，可达到接近最优的弹性t < n/(ω+ε)，其中ω是凸性空间的Helly数。如果L未知，仍可对任何常数ε > 0达到弹性t<n/(ω+ε+1)。

Conclusion: 该论文成功地缩小了CA与BA之间在通信复杂度上的差距，特别是在抽象凸性空间中。通过引入新的技术手段——利用提取器图进行参与者到委员会的分配，从而提高了协议面对适应性对手时的安全性和效率。

Abstract: Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \emph{communication}: standard approaches for CA have a communication complexity of $Θ(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $Ω(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.
  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = Ω(n \cdot κ)$, where $κ$ is a security parameter, we achieve $O(L\cdot n\log n)$ communication for finite convexity spaces and $O(L\cdot n^{1+o(1)})$ communication for Euclidean spaces $\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(ω+\varepsilon)$ for any constant $\varepsilon>0$, where $ω$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(ω+\varepsilon+1)$ for any constant $\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.
  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.

</details>


### [22] [Multi-Layer Scheduling for MoE-Based LLM Reasoning](https://arxiv.org/abs/2602.21626)
*Yifan Sun,Gholamreza Haffar,Minxian Xu,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: 本文提出了一种针对基于混合专家模型的大规模语言模型服务的多层次调度框架，通过在请求级、引擎级和专家级三个层次上进行优化，显著降低了首次生成词元的时间延迟（TTFT）和每输出词元时间（TPOT），实验结果显示相比现有最先进推理框架vLLM有明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）虽然在许多任务上取得了成功，但因其巨大的计算需求和延迟问题，在大规模部署时仍面临效率挑战。当前常用的推理框架依赖于如先来先服务(FCFS)等简单调度策略，这导致系统资源未能充分利用，并可能遇到头部阻塞和负载不均等问题。随着混合专家(MoE)模型的发展，也带来了新的调度难题。因此，需要一种更高效的调度机制来解决这些问题。

Method: 研究中提出了一种专为基于MoE的LLM服务设计的多层调度框架，涵盖请求级、引擎级及专家级三个层面：
1. 请求级别：采用最短作业优先(SJF)与优先级感知老化算法以提高吞吐量并减少延迟。
2. 引擎级别：开发了考虑前缀令牌负载、KV缓存利用率以及用户粘性的负载感知分发策略，旨在实现更好的资源配置。
3. 专家级别：集中于缓解专家热点问题并通过策略性地放置跨层专家依赖关系来平衡负载和提高路由效率。

Result: 通过对超过100次不同工作负载分布下的实验分析表明，所提出的方法相较于最先进的推理框架vLLM，在首次生成词元的时间延迟(TTFT)方面最多可降低17.8%，而在每输出词元时间(TPOT)延迟方面则能够减少高达13.3%。

Conclusion: 本研究表明，通过引入一种专门为基于MoE架构的大规模语言模型定制的多层级调度框架，可以在实际应用中有效提升LLM服务的性能表现，特别是在降低延迟方面展现出了显著优势。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.

</details>


### [23] [Lamport's Arrow of Time: The Category Mistake in Logical Clocks](https://arxiv.org/abs/2602.21730)
*Paul Borrill*

Main category: cs.DC

TL;DR: 本文探讨了Lamport 1978年论文中关于分布式系统逻辑时钟和happens-before关系的假设，指出了其潜在地假定了因果关系形成了全局定义良好的有向无环图（DAG）。文章通过不同理论分析了这一假设混淆了认识论构造与本体论主张，并提出基于互信息守恒而非时间优先级作为分布式一致性更基础的概念。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示Lamport提出的分布式系统模型中隐藏的深层假设，即认为因果性导致了一个全局定义良好且仅向前发展的结构，这实际上混淆了逻辑消息排序的认识论构建与物理因果性的本体论声明。

Method: 通过对Ryle的范畴错误分析、Shannon的信道模型、TLA+、Bell定理以及Fischer-Lynch-Paterson和Brewer CAP定理的研究，文章展示了这种混淆如何贯穿于这些重要的计算机科学与物理学理论之中。

Result: 结果显示，特殊相对论和广义相对论只允许局部因果结构的存在，而最近关于不确定因果顺序的工作表明自然允许没有明确因果顺序的相关性。因此，文章建议使用互信息守恒来替代传统的时间优先级概念以作为分布式一致性的基础。

Conclusion: 结论是，为了更好地理解分布式系统中的事件一致性，需要从依赖于绝对因果序转向考虑更加基础的信息守恒原则。

Abstract: Lamport's 1978 paper introduced the happens-before relation and logical clocks, freeing distributed systems from dependence on synchronized physical clocks. This is widely understood as a move away from Newtonian absolute time. We argue that Lamport's formalism retains a deeper and largely unexamined assumption: that causality induces a globally well-defined directed acyclic graph (DAG) over events -- a forward-in-time-only (FITO) structure that functions as an arrow of time embedded at the semantic level. Following Ryle's analysis of category mistakes, we show that this assumption conflates an epistemic construct (the logical ordering of messages) with an ontic claim (that physical causality is globally acyclic and monotonic). We trace this conflation through Shannon's channel model, TLA+, Bell's theorem, and the impossibility results of Fischer-Lynch-Paterson and Brewer's CAP theorem. We then show that special and general relativity permit only local causal structure, and that recent work on indefinite causal order demonstrates that nature admits correlations with no well-defined causal ordering. We propose that mutual information conservation, rather than temporal precedence, provides a more fundamental primitive for distributed consistency.

</details>


### [24] [DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism](https://arxiv.org/abs/2602.21788)
*Yifan Niu,Han Xiao,Dongyi Liu,Wei Zhou,Jia Li*

Main category: cs.DC

TL;DR: 本文提出了一种名为动态混合并行（DHP）的新策略，该策略能自适应地在多模态大语言模型（MLLMs）训练过程中重新配置通信组和平行度，以提高硬件效率和训练吞吐量。实验结果表明，与Megatron-LM和DeepSpeed相比，DHP能够实现高达1.36倍的训练吞吐量提升，并且在大规模NPU集群上保持接近线性的扩展效率。


<details>
  <summary>Details</summary>
Motivation: 现有的训练框架主要依赖于静态并行策略，在面对数据异构性时存在严重的负载不平衡、冗余通信以及次优的硬件利用率问题。因此，开发一种能够在数据高度变化的情况下仍保持高硬件效率的有效并行策略变得十分必要。

Method: 提出了动态混合并行（DHP），这是一种可以自适应调整通信组和平行度的高效并行策略。此外，还推广了非2的幂次方平行度的应用，并开发了一个多项式时间算法来生成几乎最优的并行策略，每个训练批次仅需毫秒级开销。

Result: 实验结果显示，相较于Megatron-LM和DeepSpeed，DHP能够达到最高1.36倍的训练吞吐量加速，并且即使是在极端的数据变化情况下也能维持高水平的硬件使用效率。

Conclusion: 通过引入动态混合并行技术，研究成功解决了由于数据异构性导致的传统静态并行策略所面临的问题，显著提高了多模态大语言模型训练过程中的性能表现。

Abstract: Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.

</details>


### [25] [A task-based data-flow methodology for programming heterogeneous systems with multiple accelerator APIs](https://arxiv.org/abs/2602.21897)
*Aleix Boné,Alejandro Aguirre,David Álvarez,Pedro J. Martinez-Ferrer,Vicenç Beltran*

Main category: cs.DC

TL;DR: 本文提出了一种基于任务的数据流方法和任务感知API（TA-libs），以克服在单个应用程序中结合多个加速器编程模型时的限制，并实现多种加速器编程模型的无缝集成，同时利用每个API提供的最佳内核。


<details>
  <summary>Details</summary>
Motivation: 异构节点正在成为高性能计算(HPC)和AI基础设施中的标准配置，但这些平台的使用需要协调多个低级加速器API，如CUDA、SYCL和Triton等，有时还需与优化过的供应商数学库结合使用。不同API或库引入了各自的抽象、执行语义和同步机制，在一个应用中结合使用它们容易出错且耗时费力。

Method: 作者提出重用一种基于任务的数据流方法论，以及任务感知API（TA-libs），来克服上述局限性并简化多种加速器编程模型的整合过程。具体来说，通过Task-Aware SYCL (TASYCL) 和 Task-Aware CUDA (TACUDA)，将单独的加速器调用提升为一等公民的任务。当多个原生运行时在同一多核CPU上共存时，可能会因争夺线程而导致超订阅及性能波动问题。为了解决这个问题，他们还统一了这些运行时的线程管理到nOS-V任务调度和线程库下，并贡献了一个新的PoCL（便携式OpenCL）运行时端口。

Result: 结果表明，任务感知库与nOS-V库相结合，使得单一应用程序能够透明而高效地利用多种加速器编程模型。所提出的方法论不仅适用于当前的异构节点，而且很容易扩展到未来集成了更丰富组合的CPU、GPU、FPGA和AI加速器系统。

Conclusion: 该研究展示了一种有效的方法，即通过任务感知库和nOS-V库来促进不同类型加速器编程模型之间的无缝协作，从而提高开发效率和程序性能。这种方法不仅解决了现有挑战，也为未来更加复杂的异构计算环境提供了良好的支持。

Abstract: Heterogeneous nodes that combine multi-core CPUs with diverse accelerators are rapidly becoming the norm in both high-performance computing (HPC) and AI infrastructures. Exploiting these platforms, however, requires orchestrating several low-level accelerator APIs such as CUDA, SYCL, and Triton. In some occasions they can be combined with optimized vendor math libraries: e.g., cuBLAS and oneAPI. Each API or library introduces its own abstractions, execution semantics, and synchronization mechanisms. Combining them within a single application is therefore error-prone and labor-intensive. We propose reusing a task-based data-flow methodology together with Task-Aware APIs (TA-libs) to overcome these limitations and facilitate the seamless integration of multiple accelerator programming models, while still leveraging the best-in-class kernels offered by each API.
  Applications are expressed as a directed acyclic graph (DAG) of host tasks and device kernels managed by an OpenMP/OmpSs-2 runtime. We introduce Task-Aware SYCL (TASYCL) and leverage Task-Aware CUDA (TACUDA), which elevate individual accelerator invocations to first-class tasks. When multiple native runtimes coexist on the same multi-core CPU, they contend for threads, leading to oversubscription and performance variability. To address this, we unify their thread management under the nOS-V tasking and threading library, to which we contribute a new port of the PoCL (Portable OpenCL) runtime.
  These results demonstrate that task-aware libraries, coupled with the nOS-V library, enable a single application to harness multiple accelerator programming models transparently and efficiently. The proposed methodology is immediately applicable to current heterogeneous nodes and is readily extensible to future systems that integrate even richer combinations of CPUs, GPUs, FPGAs, and AI accelerators.

</details>


### [26] [IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs](https://arxiv.org/abs/2602.22017)
*Chris Egersdoerfer,Arnav Sareen,Jean Luca Bez,Suren Byna,Dongkuan,Xu,Dong Dai*

Main category: cs.DC

TL;DR: 提出了一种名为IOAgent的工具，旨在帮助科学家们更有效地诊断HPC存储系统中的I/O性能问题。该工具结合了模块化的预处理器、基于RAG的领域知识集成器和树状合并器，能够从Darshan跟踪文件中准确诊断I/O问题，并提供详细的解释和参考。通过多样化的标记作业跟踪测试集TraceBench进行评估，结果显示IOAgent的表现优于或等同于现有的I/O诊断工具，并且与专有及开源大语言模型兼容。


<details>
  <summary>Details</summary>
Motivation: 随着HPC（高性能计算）存储栈复杂性的快速增长，领域科学家在有效利用HPC存储系统以达到理想的I/O性能方面遇到了越来越多的挑战。当前解决I/O问题高度依赖有限数量的I/O专家，这导致了获取专家支持变得越来越困难，成为科学家提高生产力的主要瓶颈。尽管大型语言模型（LLM）的进步为构建自动化工具提供了可能性，但仍然存在诸如无法处理长上下文窗口、缺乏关于HPC I/O的准确领域知识以及在复杂交互过程中生成幻觉等问题。

Method: 设计并实现了一个名为IOAgent的系统，它包括一个模块化预处理器用于处理输入数据，一个基于检索增强生成（RAG）的领域知识集成器来整合相关的专业知识，以及一个树状合并器来综合分析结果。此系统模仿I/O专家的行为模式，不仅提供具体的问题诊断，还能给出相应的理由和参考资料，并允许用户通过互动界面提出后续问题。

Result: 使用自建的多样化标记作业跟踪测试套件TraceBench对IOAgent进行了广泛评估。实验表明，IOAgent能够在准确性与实用性方面匹配甚至超越现有最先进的I/O诊断工具。此外，研究还展示了IOAgent对于不同类型的大型语言模型具有良好的适应性。

Conclusion: IOAgent作为一种新的解决方案，有望成为未来科学家们在面对复杂的HPC I/O子系统时的强大助手。它不仅解决了当前I/O性能诊断过程中的若干关键难题，也为进一步研究如何更好地应用AI技术于科学计算领域奠定了基础。

Abstract: As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.

</details>


### [27] [PASTA: A Modular Program Analysis Tool Framework for Accelerators](https://arxiv.org/abs/2602.22103)
*Mao Lin,Hyeran Jeon,Keren Zhou*

Main category: cs.DC

TL;DR: PASTA是一个针对加速器的低开销、模块化的程序分析工具框架，提供统一接口来捕获和分析运行时事件。它支持快速开发自定义工具，并在NVIDIA和AMD GPU上展示了其广泛应用性及性能优势。


<details>
  <summary>Details</summary>
Motivation: 随着现代计算系统中硬件加速器复杂性和多样性的增加，需要更加灵活且低开销的程序分析工具。

Method: 提出了PASTA，一个面向加速器的低开销与模块化程序分析工具框架。PASTA抽象了底层剖析API和多种深度学习框架，为用户提供了一个统一的界面以多层级地捕捉和分析运行时事件。

Result: 通过主流深度学习工作负载在NVIDIA和AMD GPU上的广泛测试（包括单GPU和多GPU场景），证明了PASTA具有广泛的适用性。特别是，在NVIDIA GPU上，PASTA能够以显著更低的开销提供详细的性能洞察，比传统分析工具快达1.3*10^4倍。

Conclusion: PASTA在可用性、可扩展性和效率之间达到了实际平衡，非常适合基于现代加速器的计算环境。

Abstract: The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program AnalysiS Tool Framework for Accelerators. PASTA abstracts over low-level profiling APIs and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. Its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. We demonstrate the utility of PASTA by developing several analysis tools, including a deep learning workload characterization tool and a UVM optimization tool. Through extensive evaluation on mainstream deep learning workloads tested on NVIDIA and AMD GPUs under both single- and multi-GPU scenarios, we demonstrate PASTA's broad applicability. On NVIDIA GPUs, we further show that PASTA provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its GPU-accelerated backend. PASTA strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.

</details>


### [28] [LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models](https://arxiv.org/abs/2602.22158)
*Minqiu Sun,Xin Huang,Luanzheng Guo,Nathan R. Tallent,Kento Sato,Dong Dai*

Main category: cs.DC

TL;DR: 本文提出了一种名为LLMTailor的检查点合并框架，该框架可以通过选择性地存储有显著更新的层来减少大型语言模型训练中的存储开销和资源竞争。实验表明，这种方法可以有效减小检查点大小和时间，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）训练中的检查点技术虽然对于容错至关重要，但因为它们需要定期存储整个模型及其优化器的状态，导致了巨大的存储开销和资源竞争问题。研究发现，LLMs中不同层之间的更新是非均匀分布的，这暗示着只对那些经历了重要更新的层进行选择性检查点可能是减少开销的一个有效方法。然而，目前没有工具能够提供对权重和优化器状态的细粒度控制以实现这一点。

Method: 为了解决上述挑战，作者们提出了LLMTailor，这是一个检查点合并框架，它能从不同的检查点中筛选并组装层以形成一个复合检查点。通过这种方式，LLMTailor允许使用各种选择性检查点策略。

Result: 评估结果显示，LLMTailor不仅能够兼容多种选择性检查点策略，并且在减少检查点大小（例如Llama3.1-8B减少了4.3倍）以及加快检查点过程（例如Qwen2.5-7B快了2.8倍）方面表现出色，同时保证了模型的质量不受影响。

Conclusion: 本研究表明，通过采用如LLMTailor这样的创新方法，可以在不牺牲模型性能的情况下大幅降低大型语言模型训练期间的存储需求和相关成本。

Abstract: Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [29] [Revisiting RAG Retrievers: An Information Theoretic Benchmark](https://arxiv.org/abs/2602.21553)
*Wenqing Zheng,Dmitri Kalaev,Noah Fatsi,Daniel Barcklow,Owen Reinert,Igor Melnyk,Senthil Kumar,C. Bayan Bruss*

Main category: cs.IR

TL;DR: 本文提出了MIGRASCOPE，一种基于互信息的RAG检索器分析范围，通过引入基于信息和统计估计理论的原则性度量来量化检索质量、冗余性、协同效应及边际贡献，并发现精心挑选的检索器组合优于任何单一检索器。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统严重依赖检索模块为大型语言模型提供相关上下文，但缺乏对不同排名原则（如词汇匹配、密集嵌入或图表引用）构建的各种检索器之间差异和重叠的系统理解。现有基准主要比较整个RAG流程或引入新数据集，在选择或结合检索器方面提供的指导很少。直接比较检索器的研究使用的评估工具有限，无法捕捉到它们的互补优势和重叠强度。

Method: 开发了MIGRASCOPE，一种基于互信息的RAG检索器分析框架，重新审视了最先进检索器，并引入了基于信息论和统计估计理论的原则性指标来量化检索质量、冗余性、协同作用以及边际贡献。

Result: 研究结果表明，如果精心选择，检索器集合的表现将优于任何单一检索器。使用所开发工具对主要RAG语料库进行分析后，提供了关于最先进的检索器贡献水平的独特见解。

Conclusion: 本研究为现代检索技术的结构提供了新的视角，并为设计强大且高效的RAG系统提供了可操作的指导。

Abstract: Retrieval-Augmented Generation (RAG) systems rely critically on the retriever module to surface relevant context for large language models. Although numerous retrievers have recently been proposed, each built on different ranking principles such as lexical matching, dense embeddings, or graph citations, there remains a lack of systematic understanding of how these mechanisms differ and overlap. Existing benchmarks primarily compare entire RAG pipelines or introduce new datasets, providing little guidance on selecting or combining retrievers themselves. Those that do compare retrievers directly use a limited set of evaluation tools which fail to capture complementary and overlapping strengths. This work presents MIGRASCOPE, a Mutual Information based RAG Retriever Analysis Scope. We revisit state-of-the-art retrievers and introduce principled metrics grounded in information and statistical estimation theory to quantify retrieval quality, redundancy, synergy, and marginal contribution. We further show that if chosen carefully, an ensemble of retrievers outperforms any single retriever. We leverage the developed tools over major RAG corpora to provide unique insights on contribution levels of the state-of-the-art retrievers. Our findings provide a fresh perspective on the structure of modern retrieval techniques and actionable guidance for designing robust and efficient RAG systems.

</details>


### [30] [Retrieval Challenges in Low-Resource Public Service Information: A Case Study on Food Pantry Access](https://arxiv.org/abs/2602.21598)
*Touseef Hasan,Laila Cure,Souvika Sarkar*

Main category: cs.IR

TL;DR: 研究开发了一种基于AI的对话检索系统，用于改善食品储藏室等关键公共服务信息系统的访问，特别是在资源有限的环境下。初步评估显示了系统在处理不明确查询和不一致知识库方面的局限性，指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 公共信息服务系统通常存在碎片化、格式不一致以及信息过时等问题，这些问题导致了低资源检索环境，阻碍了对重要服务的及时获取。本研究通过解决食品储藏室访问这一社会紧迫问题来探讨此类设置下的检索挑战。

Method: 研究人员开发了一个由AI驱动的对话式检索系统，该系统能够抓取并索引公开可用的食物储藏室数据，并采用检索增强生成（RAG）流程支持通过网页界面进行自然语言查询。此外，他们还利用社区提供的查询进行了初步评估研究，以检查系统在实际场景中的表现。

Result: 分析揭示了系统在检索鲁棒性、处理定义不足的查询以及基于不一致的知识库进行操作方面存在的关键限制。

Conclusion: 这项持续的研究暴露了低资源环境中基础信息检索所面临的挑战，并为未来关于提高关键公共资源访问性的稳健对话检索研究提供了动力。

Abstract: Public service information systems are often fragmented, inconsistently formatted, and outdated. These characteristics create low-resource retrieval environments that hinder timely access to critical services. We investigate retrieval challenges in such settings through the domain of food pantry access, a socially urgent problem given persistent food insecurity. We develop an AI-powered conversational retrieval system that scrapes and indexes publicly available pantry data and employs a Retrieval-Augmented Generation (RAG) pipeline to support natural language queries via a web interface. We conduct a pilot evaluation study using community-sourced queries to examine system behavior in realistic scenarios. Our analysis reveals key limitations in retrieval robustness, handling underspecified queries, and grounding over inconsistent knowledge bases. This ongoing work exposes fundamental IR challenges in low-resource environments and motivates future research on robust conversational retrieval to improve access to critical public resources.

</details>


### [31] [AQR-HNSW: Accelerating Approximate Nearest Neighbor Search via Density-aware Quantization and Multi-stage Re-ranking](https://arxiv.org/abs/2602.21600)
*Ganap Ashit Tewary,Nrusinga Charan Gantayat,Jeff Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架AQR-HNSW，通过集成三种策略来提高HNSW算法在大规模向量数据库中的可扩展性。该框架实现了密度感知自适应量化、多状态重排以及针对SIMD优化的实现方法，从而大幅提高了查询性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 随着向量数据库增长至数十亿级别的嵌入向量，现有的主导近似最近邻搜索算法HNSW面临内存消耗大、距离计算开销高以及对异构数据分布表现不佳的问题。为了克服这些瓶颈并提高HNSW的可扩展性，提出了新的解决方案。

Method: 1. 密度感知自适应量化：达到4倍压缩同时保持距离关系。
2. 多状态重排技术：减少不必要的计算达35%。
3. 针对SIMD优化的实现：跨架构每周期执行16到64次操作。

Result: 与最先进的HNSW实现相比，在标准基准测试中展示了2.5到3.3倍更高的每秒查询次数（QPS），同时保持超过98%的召回率，索引图的内存减少了75%，并且索引构建速度加快了5倍。

Conclusion: AQR-HNSW框架成功地解决了HNSW在处理大规模数据集时遇到的主要挑战，包括内存使用、计算效率及处理不同类型数据的能力，为工业界提供了更高效、更具成本效益的ANN搜索解决方案。

Abstract: Approximate Nearest Neighbor (ANN) search has become fundamental to modern AI infrastructure, powering recommendation systems, search engines, and large language models across industry leaders from Google to OpenAI. Hierarchical Navigable Small World (HNSW) graphs have emerged as the dominant ANN algorithm, widely adopted in production systems due to their superior recall versus latency balance. However, as vector databases scale to billions of embeddings, HNSW faces critical bottlenecks: memory consumption expands, distance computation overhead dominates query latency, and it suffers suboptimal performance on heterogeneous data distributions. This paper presents Adaptive Quantization and Rerank HNSW (AQR-HNSW), a novel framework that synergistically integrates three strategies to enhance HNSW scalability. AQR-HNSW introduces (1) density-aware adaptive quantization, achieving 4x compression while preserving distance relationships; (2) multi-state re-ranking that reduces unnecessary computations by 35%; and (3) quantization-optimized SIMD implementations delivering 16-64 operations per cycle across architectures. Evaluation on standard benchmarks demonstrates 2.5-3.3x higher queries per second (QPS) than state-of-the-art HNSW implementations while maintaining over 98% recall, with 75% memory reduction for the index graph and 5x faster index construction.

</details>


### [32] [Trie-Aware Transformers for Generative Recommendation](https://arxiv.org/abs/2602.21677)
*Zhenxiang Xu,Jiawei Chen,Sirui Chen,Yong He,Jieyu Yang,Chuan Yuan,Ke Ding,Can Wang*

Main category: cs.IR

TL;DR: 本文提出了一种新的生成式推荐方法TrieRec，通过引入两种位置编码来增强Transformer模型，使其能够更好地捕捉项目令牌的层级结构信息。实验表明，在四个真实数据集上平均提高了8.83%的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐系统在处理项目令牌时，通常会忽略其背后的拓扑结构，而只是简单地将这些令牌线性化处理。为了解决这一问题，作者们提出了TrieRec，一种能够理解并利用这种层级结构的方法。

Method: TrieRec通过向Transformer添加结构性归纳偏置来实现，主要通过两种位置编码：1）trie感知绝对位置编码，它将令牌（节点）的局部结构上下文整合到令牌表示中；2）拓扑感知相对位置编码，它向自注意力机制注入成对结构关系以捕获由拓扑引起语义相关性。

Result: 研究者们在三个代表性的生成式推荐框架内实现了TrieRec，并在四个实际数据集上进行了测试。结果表明，相比基准方法，TrieRec在所有测试场景下都取得了显著提升，平均改善了8.83%。

Conclusion: TrieRec作为一种新型的生成式推荐方法，成功地将项目的层级结构融入到了模型学习过程中，从而有效提升了推荐效果。此外，该方法还具有模型无关、高效及无需额外超参数调整的优点。

Abstract: Generative recommendation (GR) aligns with advances in generative AI by casting next-item prediction as token-level generation rather than score-based ranking. Most GR methods adopt a two-stage pipeline: (i) \textit{item tokenization}, which maps each item to a sequence of discrete, hierarchically organized tokens; and (ii) \textit{autoregressive generation}, which predicts the next item's tokens conditioned on the tokens of user's interaction history. Although hierarchical tokenization induces a prefix tree (trie) over items, standard autoregressive modeling with conventional Transformers often flattens item tokens into a linear stream and overlooks the underlying topology.
  To address this, we propose TrieRec, a trie-aware generative recommendation method that augments Transformers with structural inductive biases via two positional encodings. First, a \textit{trie-aware absolute positional encoding} aggregates a token's (node's) local structural context (\eg depth, ancestors, and descendants) into the token representation. Second, a \textit{topology-aware relative positional encoding} injects pairwise structural relations into self-attention to capture topology-induced semantic relatedness. TrieRec is also model-agnostic, efficient, and hyperparameter-free. In our experiments, we implement TrieRec within three representative GR backbones, achieving notably improvements of 8.83\% on average across four real-world datasets.

</details>


### [33] [Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing](https://arxiv.org/abs/2602.21756)
*Deogyong Kim,Junseong Lee,Jeongeun Lee,Changhoe Kim,Junguel Lee,Jungseok Lee,Dongha Lee*

Main category: cs.IR

TL;DR: 本文提出了一种名为Persona4Rec的推荐框架，通过离线推理构建可解释的人物表示，以实现轻量级和可扩展的实时推理。这种方法不仅在性能上与基于大型语言模型（LLM）的重排器相当，而且大大减少了推理时间，同时提供了直观且基于评论的解释。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）能够捕捉用户兴趣和项目特征之间的细微语义差异，从而为推荐系统带来了新的机会，但这些方法通常需要昂贵的在线推理时间计算，导致高延迟问题，阻碍了实际部署。

Method: 提出了Persona4Rec推荐框架，在离线阶段利用LLMs对项目评价进行推理，推断出不同类型的用户可能参与项目的多样化动机，并将这些推断出的动机具体化为人格表示；然后学习将用户画像与最有可能的项目侧人格相匹配，有效地将用户-项目相关性转换为用户-人格相关性。在线阶段，则可以快速计算相关性而无需调用昂贵的LLM推理。

Result: 广泛的实验表明，Persona4Rec在保持与最近基于LLM的重排序器相似性能的同时，显著降低了推理时间。此外，定性分析证实了人物表示不仅能驱动高效的评分，还提供了直观且基于评论的解释。

Conclusion: Persona4Rec提供了一个实用且可解释的解决方案，适用于下一代推荐系统，它解决了现有基于LLM的方法存在的高延迟问题，同时增强了推荐结果的可解释性和效率。

Abstract: Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferring diverse user motivations that explain why different types of users may engage with an item; these inferred motivations are materialized as persona representations, providing multiple, human-interpretable views of each item. Unlike conventional approaches that rely on a single item representation, Persona4Rec learns to align user profiles with the most plausible item-side persona through a dedicated encoder, effectively transforming user-item relevance into user-persona relevance. At the online stage, this persona-profiled item index allows fast relevance computation without invoking expensive LLM reasoning. Extensive experiments show that Persona4Rec achieves performance comparable to recent LLM-based rerankers while substantially reducing inference time. Moreover, qualitative analysis confirms that persona representations not only drive efficient scoring but also provide intuitive, review-grounded explanations. These results demonstrate that Persona4Rec offers a practical and interpretable solution for next-generation recommender systems.

</details>


### [34] [Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation](https://arxiv.org/abs/2602.21957)
*Yuchun Tu,Zhiwei Li,Bingli Sun,Yixuan Li,Xiao Song*

Main category: cs.IR

TL;DR: 本文提出了一种名为Cluster-Guided FedRec (CGFedRec) 的框架，该框架通过将上传的嵌入转换为紧凑的聚类标签来改善联邦推荐系统中的通信效率和推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦推荐方法通常依赖于在服务器和客户端之间同步高维项目表示，这假设了嵌入坐标精确对齐对于跨客户端协作是必要的。作者认为建立项目之间的相对语义关系比强制共享表示更有效，并且可以捕捉到细粒度的用户个性化同时保持全局一致性。

Method: 提出了Cluster-Guided FedRec (CGFedRec) 框架，该框架能够将上传的嵌入转换为紧凑的聚类标签。服务器作为全局结构发现者学习项目聚类并仅分发结果标签，从而切断了项嵌入的下游传输，使得客户端无需维护全局共享的项嵌入。

Result: 广泛的实验表明，这种方法显著提高了通信效率，同时在多个数据集上保持了较高的推荐准确性。

Conclusion: CGFedRec框架通过允许项目表示在每个客户端局部变化，以及通过只传递聚类标签而非完整的嵌入向量，有效地注入了全局协作信号到本地项目表示中，从而提高了联邦推荐系统的通信效率和推荐质量。

Abstract: Federated recommendation facilitates collaborative model training across distributed clients while keeping sensitive user interaction data local. Conventional approaches typically rely on synchronizing high-dimensional item representations between the server and clients. This paradigm implicitly assumes that precise geometric alignment of embedding coordinates is necessary for collaboration across clients. We posit that establishing relative semantic relationships among items is more effective than enforcing shared representations. Specifically, global semantic relations serve as structural constraints for items. Within these constraints, the framework allows item representations to vary locally on each client, which flexibility enables the model to capture fine-grained user personalization while maintaining global consistency. To this end, we propose Cluster-Guided FedRec framework (CGFedRec), a framework that transforms uploaded embeddings into compact cluster labels. In this framework, the server functions as a global structure discoverer to learn item clusters and distributes only the resulting labels. This mechanism explicitly cuts off the downstream transmission of item embeddings, relieving clients from maintaining global shared item embeddings. Consequently, CGFedRec achieves the effective injection of global collaborative signals into local item representations without transmitting full embeddings. Extensive experiments demonstrate that our approach significantly improves communication efficiency while maintaining superior recommendation accuracy across multiple datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Latent Context Compilation: Distilling Long Context into Compact Portable Memory](https://arxiv.org/abs/2602.21221)
*Zeju Li,Yizhou Zhou,Qiang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为潜上下文编译的框架，该框架通过一次性LoRA模块将长上下文提炼成紧凑的缓冲令牌，这些令牌与冻结的基础模型兼容。通过自对齐优化策略消除了对合成上下文相关QA对的需求，并且实验表明这种方法即使在16倍压缩比下也能保持细节和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前高效的长上下文语言模型部署面临两个问题：摊销压缩方法难以处理分布外泛化；而测试时训练则带来了高昂的合成数据成本并需要修改模型权重，导致状态参数复杂化，不利于同时服务。

Method: 提出了一个名为潜上下文编译的新框架，利用一次性的LoRA模块作为编译器，将长上下文提炼为紧凑的缓冲令牌——无状态、可移植的记忆制品，能够与冻结的基础模型即插即用。还引入了自对齐优化策略，通过使用与上下文无关的随机查询来规范上下文重建任务，迫使压缩后的令牌位于模型现有的指令跟随流形内。

Result: 实验结果显示，潜上下文编译能够在保留细粒度细节和推理能力的同时，实现高达16倍的压缩比率，这表明它成功地将记忆密度与模型参数解耦。

Conclusion: 潜上下文编译提供了一个新的解决方案，可以有效地解决长上下文语言模型部署中的挑战，特别是在高比例压缩情况下仍能保持模型性能。

Abstract: Efficient long-context LLM deployment is stalled by a dichotomy between amortized compression, which struggles with out-of-distribution generalization, and Test-Time Training, which incurs prohibitive synthetic data costs and requires modifying model weights, creating stateful parameters that complicate concurrent serving. We propose Latent Context Compilation, a framework that fundamentally shifts context processing from adaptation to compilation. By utilizing a disposable LoRA module as a compiler, we distill long contexts into compact buffer tokens -- stateless, portable memory artifacts that are plug-and-play compatible with frozen base models. Crucially, we introduce a self-aligned optimization strategy that eliminates the need for synthetic context-relevant QA pairs. By regularizing context reconstruction task with context-agnostic random queries, we force compressed tokens to reside within the model's existing instruction-following manifold. Experiments with Llama-3.1-8B demonstrate that Latent Context Compilation preserves fine-grained details and reasoning capabilities where prior methods falter, effectively decoupling memory density from model parameters even at a 16x compression ratio.

</details>


### [36] [AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression](https://arxiv.org/abs/2602.21233)
*Rui Cen,QiangQiang Hu,Hong Huang,Hong Liu,Song Liu,Xin Luo,Lin Niu,Yifan Tan,Decheng Wu,Linchuan Xie,Rubing Yang,Guanghua Yu,Jianchen Zhu*

Main category: cs.LG

TL;DR: AngelSlim是一个由腾讯混元团队开发的大模型压缩工具包，整合了量化、推测解码、令牌修剪和蒸馏等先进技术，提供了从模型压缩到工业规模部署的统一管道。


<details>
  <summary>Details</summary>
Motivation: 为了简化大型模型从压缩到工业级部署的过程，并提供高效加速，腾讯混元团队开发了AngelSlim这一全面且多功能的工具包。

Method: AngelSlim通过结合量化（包括FP8和INT8后训练量化）、推测解码框架、无需训练的稀疏注意力框架以及针对多模态模型的专业修剪策略来实现其功能。

Result: 该工具包能够在不牺牲输出正确性的情况下提高1.8至2.0倍吞吐量；减少长上下文场景中的首次令牌时间；并且对于视觉和音频令牌优化具有特别设计的方法。

Conclusion: AngelSlim通过低级别实现整合这些压缩策略，使得研究人员能够专注于算法研究，并支持借助工具辅助进行部署。

Abstract: This technical report introduces AngelSlim, a comprehensive and versatile toolkit for large model compression developed by the Tencent Hunyuan team. By consolidating cutting-edge algorithms, including quantization, speculative decoding, token pruning, and distillation. AngelSlim provides a unified pipeline that streamlines the transition from model compression to industrial-scale deployment. To facilitate efficient acceleration, we integrate state-of-the-art FP8 and INT8 Post-Training Quantization (PTQ) algorithms alongside pioneering research in ultra-low-bit regimes, featuring HY-1.8B-int2 as the first industrially viable 2-bit large model. Beyond quantization, we propose a training-aligned speculative decoding framework compatible with multimodal architectures and modern inference engines, achieving 1.8x to 2.0x throughput gains without compromising output correctness. Furthermore, we develop a training-free sparse attention framework that reduces Time-to-First-Token (TTFT) in long-context scenarios by decoupling sparse kernels from model architectures through a hybrid of static patterns and dynamic token selection. For multimodal models, AngelSlim incorporates specialized pruning strategies, namely IDPruner for optimizing vision tokens via Maximal Marginal Relevance and Samp for adaptive audio token merging and pruning. By integrating these compression strategies from low-level implementations, AngelSlim enables algorithm-focused research and tool-assisted deployment.

</details>


### [37] [Group Orthogonalized Policy Optimization:Group Policy Optimization as Orthogonal Projection in Hilbert Space](https://arxiv.org/abs/2602.21269)
*Wang Zixian*

Main category: cs.LG

TL;DR: 本文提出了一种新的大型语言模型对齐算法——组正交化策略优化（GOPO），该方法基于Hilbert函数空间的几何学。通过将对齐问题提升到Hilbert空间L2(pi_k)中，GOPO避免了概率单纯形上的优化以及Kullback-Leibler散度带来的指数曲率问题。在实际应用中，GOPO从无限维L2(pi_k)投影到由群体采样诱导的有限经验子空间，并且由于群归一化优势总和为零，强制概率守恒的拉格朗日乘数恰好消失，从而简化了约束投影为无约束的经验损失。实验表明，在数学推理基准测试中，GOPO保持了稳定的梯度动态和熵保留的同时达到了具有竞争力的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在开发一种新的对齐算法来解决现有方法在处理大型语言模型时遇到的问题，特别是当使用传统的基于概率单纯形的方法进行优化时，会遭遇由于Kullback-Leibler散度导致的高曲率问题。GOPO通过将对齐过程置于Hilbert空间内执行，以期克服这些限制。

Method: 提出了组正交化策略优化（Group Orthogonalized Policy Optimization, GOPO）算法，该算法利用Hilbert空间L2(pi_k)中的线性正交条件代替传统方法中的单纯形约束。通过对目标函数施加边界v >= -1的限制，使得投影结果自然地产生稀疏性，从而能够自动排除灾难性差的动作。此外，GOPO还通过将无限维空间映射至有限经验子空间来实现理论与实践之间的联系。

Result: 实验证明，在数学推理相关任务上，相比于依赖于裁剪技术的传统方法，GOPO不仅能够在保持稳定梯度动态和熵不变的情况下达到相似或更好的泛化能力，而且还能有效避免后者在特定条件下出现的表现停滞现象。

Conclusion: 组正交化策略优化（GOPO）提供了一个创新性的视角来解决大型语言模型中的对齐挑战，通过引入Hilbert空间框架下的新方法论，成功地提升了模型训练过程中的稳定性及最终性能。

Abstract: We present Group Orthogonalized Policy Optimization (GOPO), a new alignment algorithm for large language models derived from the geometry of Hilbert function spaces. Instead of optimizing on the probability simplex and inheriting the exponential curvature of Kullback-Leibler divergence, GOPO lifts alignment into the Hilbert space L2(pi_k) of square-integrable functions with respect to the reference policy. Within this space, the simplex constraint reduces to a linear orthogonality condition <v, 1> = 0, defining a codimension-one subspace H0. Minimizing distance to an unconstrained target u_star yields the work-dissipation functional J(v) = <g, v> - (mu / 2) ||v||^2, whose maximizer follows directly from the Hilbert projection theorem. Enforcing the boundary v >= -1 produces a bounded Hilbert projection that induces exact sparsity, assigning zero probability to catastrophically poor actions through a closed-form threshold. To connect this functional theory with practice, GOPO projects from infinite-dimensional L2(pi_k) to a finite empirical subspace induced by group sampling. Because group-normalized advantages sum to zero, the Lagrange multiplier enforcing probability conservation vanishes exactly, reducing the constrained projection to an unconstrained empirical loss. The resulting objective has constant Hessian curvature mu I, non-saturating linear gradients, and an intrinsic dead-zone mechanism without heuristic clipping. Experiments on mathematical reasoning benchmarks show that GOPO achieves competitive generalization while maintaining stable gradient dynamics and entropy preservation in regimes where clipping-based methods plateau.

</details>


### [38] [Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling](https://arxiv.org/abs/2602.21319)
*Marion Neumeier,Niklas Roßberg,Michael Botsch,Wolfgang Utschick*

Main category: cs.LG

TL;DR: 本研究提出了一种改进的基于扩散的轨迹预测框架cVMDx，通过DDIM采样和高斯混合模型提高了效率、鲁棒性和多模态预测能力，并在公开数据集highD上展示了比cVMD更高的准确度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的生成模型（如cVMD）虽然在捕捉多模式未来方面表现出色，但存在采样速度慢、对生成多样性利用不足以及场景编码脆弱的问题。为了解决这些问题，提出了新的框架cVMDx来提高自动驾驶中轨迹预测的准确性及不确定性感知。

Method: 引入了cVMDx框架，采用DDIM采样技术大幅减少了推理时间，同时通过拟合高斯混合模型从生成的轨迹中提供可处理的多模态预测。此外，还评估了CVQ-VAE变体用于场景编码的效果。

Result: 实验结果表明，在公开可用的highD数据集上，cVMDx相比cVMD不仅实现了更高的精度，而且显著提升了效率，能够实现完全随机化的多模态轨迹预测。

Conclusion: cVMDx作为一种增强型的基于扩散的轨迹预测方法，成功解决了现有方法的一些关键限制，为实现更高效、更可靠的自动驾驶系统提供了新的途径。

Abstract: Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings.
  This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction.

</details>


### [39] [Dynamic Symmetric Point Tracking: Tackling Non-ideal Reference in Analog In-memory Training](https://arxiv.org/abs/2602.21321)
*Quan Xiao,Jindan Li,Zhaoxian Wu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 本文提出了针对模拟内存计算设备中权重更新不对称性问题的动态对称点(SP)估计方法，通过理论分析和实验验证了该方法的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 在模拟内存计算(AIMC)设备上进行训练时，由于非理想模拟器件特性导致权重更新存在不对称性，这会引起权重更新向特定于设备的对称点(SP)系统漂移，而这一点通常与训练目标的最佳值不一致。现有解决此偏差的方法需要预先校准SP至零，但这一过程成本高昂且残留校准误差会直接影响训练准确性。

Method: 研究者首先对SP校准所需的脉冲复杂度及其产生的估计误差进行了理论表征。接着提出了一种可以在模型训练过程中跟踪SP位置的动态SP估计方法，并证明了其收敛性保证。此外，还基于数字信号处理中的斩波和滤波技术开发了该方法的一个增强版本。

Result: 数值实验表明，所提方法不仅能够有效减少因SP偏移引起的训练精度下降问题，而且提高了AIMC设备上模型训练的整体效率。

Conclusion: 通过引入动态SP估计机制，本研究为克服AIMC设备上的训练挑战提供了一个新的视角，有助于提高这类节能计算平台在大规模视觉和语言模型部署中的实用性。

Abstract: Analog in-memory computing (AIMC) performs computation directly within resistive crossbar arrays, offering an energy-efficient platform to scale large vision and language models. However, non-ideal analog device properties make the training on AIMC devices challenging. In particular, its update asymmetry can induce a systematic drift of weight updates towards a device-specific symmetric point (SP), which typically does not align with the optimum of the training objective. To mitigate this bias, most existing works assume the SP is known and pre-calibrate it to zero before training by setting the reference point as the SP. Nevertheless, calibrating AIMC devices requires costly pulse updates, and residual calibration error can directly degrade training accuracy. In this work, we present the first theoretical characterization of the pulse complexity of SP calibration and the resulting estimation error. We further propose a dynamic SP estimation method that tracks the SP during model training, and establishes its convergence guarantees. In addition, we develop an enhanced variant based on chopping and filtering techniques from digital signal processing. Numerical experiments demonstrate both the efficiency and effectiveness of the proposed method.

</details>


### [40] [C$^{2}$TC: A Training-Free Framework for Efficient Tabular Data Condensation](https://arxiv.org/abs/2602.21717)
*Sijia Xu,Fan Li,Xiaoyang Wang,Zhengyi Yang,Xuemin Lin*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的表格数据集压缩框架C$^{2}$TC，通过优化类别分配和特征表示来实现高效可扩展的数据压缩。


<details>
  <summary>Details</summary>
Motivation: 随着表格数据规模的增长，基于学习的分析系统面临着计算和存储方面的挑战，需要更高效的数据利用方式。现有的数据集压缩方法存在计算复杂度高以及忽视表格数据特性的局限性。

Method: 提出了C$^{2}$TC框架，它将数据集压缩目标重新定义为一个新颖的自适应类群分配问题(CCAP)，并开发了HFILS启发式局部搜索算法解决CCAP问题。此外，还提出了一种混合分类特征编码(HCFE)以保持异构离散属性的语义一致性。

Result: 在10个真实世界数据集上的实验表明，与最先进基线相比，C$^{2}$TC至少提高了两个数量级的效率，同时实现了更好的下游性能。

Conclusion: C$^{2}$TC作为一种无需训练的表格数据集压缩方法，在提高效率的同时也保证了高质量的结果，为处理大规模表格数据提供了一个新的解决方案。

Abstract: Tabular data is the primary data format in industrial relational databases, underpinning modern data analytics and decision-making. However, the increasing scale of tabular data poses significant computational and storage challenges to learning-based analytical systems. This highlights the need for data-efficient learning, which enables effective model training and generalization using substantially fewer samples. Dataset condensation (DC) has emerged as a promising data-centric paradigm that synthesizes small yet informative datasets to preserve data utility while reducing storage and training costs. However, existing DC methods are computationally intensive due to reliance on complex gradient-based optimization. Moreover, they often overlook key characteristics of tabular data, such as heterogeneous features and class imbalance. To address these limitations, we introduce C$^{2}$TC (Class-Adaptive Clustering for Tabular Condensation), the first training-free tabular dataset condensation framework that jointly optimizes class allocation and feature representation, enabling efficient and scalable condensation. Specifically, we reformulate the dataset condensation objective into a novel class-adaptive cluster allocation problem (CCAP), which eliminates costly training and integrates adaptive label allocation to handle class imbalance. To solve the NP-hard CCAP, we develop HFILS, a heuristic local search that alternates between soft allocation and class-wise clustering to efficiently obtain high-quality solutions. Moreover, a hybrid categorical feature encoding (HCFE) is proposed for semantics-preserving clustering of heterogeneous discrete attributes. Extensive experiments on 10 real-world datasets demonstrate that C$^{2}$TC improves efficiency by at least 2 orders of magnitude over state-of-the-art baselines, while achieving superior downstream performance.

</details>


### [41] [Efficient Opportunistic Approachability](https://arxiv.org/abs/2602.21328)
*Teodor Vanislavov Marinov,Mehryar Mohri,Princewill Okoroafor,Jon Schneider,Julian Zimmert*

Main category: cs.LG

TL;DR: 本文提出了一种有效的机会接近性算法，无需在线校准子程序即可实现$O(T^{-1/4})$的接近率，并且在对手行动集维度最多为2的情况下，可以达到最优的$O(T^{-1/2})$接近率。


<details>
  <summary>Details</summary>
Motivation: 研究者们希望在对手限制自己行动空间时，学习者能够获得更强的保证（即接近更小的集合）。然而，现有的算法需要能够产生对手动作的在线校准预测，这通常导致计算时间指数级增长并且接近率较低。

Method: 本文介绍了一种新的机会接近性算法，该算法不需要依赖于在线校准子程序来预测对手的动作，从而实现了更高的效率和更好的接近率。

Result: 新算法对于一般情况下的机会接近性能达到$O(T^{-1/4})$的速度；另外还提出了一个低效版本，其速度为$O(T^{-1/3})$。特别地，当对手行动集的维度不超过2时，可以获得最佳速度$O(T^{-1/2})$。

Conclusion: 通过提出的新算法，研究人员克服了之前方法中对在线校准预测的需求，显著提高了机会接近性问题中的接近速率，特别是在低维场景下达到了最优表现。

Abstract: We study the problem of opportunistic approachability: a generalization of Blackwell approachability where the learner would like to obtain stronger guarantees (i.e., approach a smaller set) when their adversary limits themselves to a subset of their possible action space. Bernstein et al. (2014) introduced this problem in 2014 and presented an algorithm that guarantees sublinear approachability rates for opportunistic approachability. However, this algorithm requires the ability to produce calibrated online predictions of the adversary's actions, a problem whose standard implementations require time exponential in the ambient dimension and result in approachability rates that scale as $T^{-O(1/d)}$. In this paper, we present an efficient algorithm for opportunistic approachability that achieves a rate of $O(T^{-1/4})$ (and an inefficient one that achieves a rate of $O(T^{-1/3})$), bypassing the need for an online calibration subroutine. Moreover, in the case where the dimension of the adversary's action set is at most two, we show it is possible to obtain the optimal rate of $O(T^{-1/2})$.

</details>


### [42] [VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery](https://arxiv.org/abs/2602.21381)
*Gene Yu,Ce Guo,Wayne Luk*

Main category: cs.LG

TL;DR: 提出了一种名为VCDF的方法，该方法通过评估时间子集间的因果关系稳定性来提高时间序列因果发现的鲁棒性。无需修改基础算法即可应用于多种方法如VAR-LiNGAM和PCMCI，并在合成数据集、模拟fMRI数据及IT监控场景下显示出改进的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列因果发现方法对噪声、非平稳性和采样变异性敏感，需要一种能够提高这些方法鲁棒性的框架。

Method: 开发了验证共识驱动框架（VCDF），它通过跨分块时间子集评估因果关系的稳定性来增强任何给定因果发现技术的可靠性。

Result: 实验表明，VCDF可以在不同特征的数据上提高VAR-LiNGAM等方法的窗口和摘要F1分数约0.08-0.12，在长度为1000及以上的序列中可达0.18的绝对改善。此外，在模拟fMRI数据与IT监控实际应用场景下也展示了更好的结构准确度与稳定性。

Conclusion: VCDF提供了一个有效的时间序列因果发现可靠性层，能够在不改变底层建模假设的情况下提升模型性能。

Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.

</details>


### [43] [Defensive Generation](https://arxiv.org/abs/2602.21390)
*Gabriele Farina,Juan Carlos Perdomo*

Main category: cs.LG

TL;DR: 提出了一个名为Defensive Generation的方法，该方法能够在线生成不可被无限类测试（包括检查生成分布的高阶矩）所证伪的非伯努利结果的生成模型。此方法在样本数量上几乎线性时间运行，并达到了生成误差最优的T^{-1/2}消失率。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效地在线生成标量、多分类和向量值结果的生成模型，这些模型不能基于观察到的数据以及预定义的一系列计算测试被证伪。

Method: 通过将在线高维多校准与RKHS的关系扩展到预期变分不等式问题的最新进展上，开发了针对前者（即在线高维多校准）的有效算法。接着，将这种算法机制应用于结果不可区分性的问题中，提出了一种叫做Defensive Generation的过程。

Result: Defensive Generation是首个能够有效在线生产出对于无限类别测试（包含那些检查生成分布更高阶矩的测试）来说不可证伪的非伯努利结果生成模型的方法。此外，该方法执行速度接近于线性时间复杂度，并且达成了T^{-1/2}这个最优渐进错误率。

Conclusion: 这项工作不仅拓展了在线高维多校准与特定数学问题之间的联系，还首次实现了对广泛测试集都具有鲁棒性的高效在线生成模型。

Abstract: We study the problem of efficiently producing, in an online fashion, generative models of scalar, multiclass, and vector-valued outcomes that cannot be falsified on the basis of the observed data and a pre-specified collection of computational tests. Our contributions are twofold. First, we expand on connections between online high-dimensional multicalibration with respect to an RKHS and recent advances in expected variational inequality problems, enabling efficient algorithms for the former. We then apply this algorithmic machinery to the problem of outcome indistinguishability. Our procedure, Defensive Generation, is the first to efficiently produce online outcome indistinguishable generative models of non-Bernoulli outcomes that are unfalsifiable with respect to infinite classes of tests, including those that examine higher-order moments of the generated distributions. Furthermore, our method runs in near-linear time in the number of samples and achieves the optimal, vanishing T^{-1/2} rate for generation error.

</details>


### [44] [FedVG: Gradient-Guided Aggregation for Enhanced Federated Learning](https://arxiv.org/abs/2602.21399)
*Alina Devkota,Jacob Thrasher,Donald Adjeroh,Binod Bhattarai,Prashnna K. Gyawali*

Main category: cs.LG

TL;DR: FedVG, a new federated learning framework, utilizes a global validation set to optimize model training across clients, improving performance in data heterogeneity. It evaluates client models based on layerwise gradient norms, allowing for adaptive and informed aggregation, and can be integrated with other FL methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of client drift and poor generalization in Federated Learning (FL) due to data heterogeneity among clients, by proposing a method that optimizes the training process without compromising privacy.

Method: FedVG employs a global validation set, which can be constructed from public datasets, to guide the optimization. It assesses each client's model generalization through the magnitude of layerwise gradients, assigning a score that indicates the required adjustments for better performance on the global validation set.

Result: Experiments show FedVG enhances performance, especially in environments with high data heterogeneity, and it can be combined with other state-of-the-art FL algorithms to further improve their outcomes.

Conclusion: FedVG provides a solution to the issue of client drift in FL, offering an effective way to leverage a global validation set for more adaptive and efficient federated model aggregation.

Abstract: Federated Learning (FL) enables collaborative model training across multiple clients without sharing their private data. However, data heterogeneity across clients leads to client drift, which degrades the overall generalization performance of the model. This effect is further compounded by overemphasis on poorly performing clients. To address this problem, we propose FedVG, a novel gradient-based federated aggregation framework that leverages a global validation set to guide the optimization process. Such a global validation set can be established using readily available public datasets, ensuring accessibility and consistency across clients without compromising privacy. In contrast to conventional approaches that prioritize client dataset volume, FedVG assesses the generalization ability of client models by measuring the magnitude of validation gradients across layers. Specifically, we compute layerwise gradient norms to derive a client-specific score that reflects how much each client needs to adjust for improved generalization on the global validation set, thereby enabling more informed and adaptive federated aggregation. Extensive experiments on both natural and medical image benchmarking datasets, across diverse model architectures, demonstrate that FedVG consistently improves performance, particularly in highly heterogeneous settings. Moreover, FedVG is modular and can be seamlessly integrated with various state-of-the-art FL algorithms, often further improving their results. Our code is available at https://github.com/alinadevkota/FedVG.

</details>


### [45] [Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning](https://arxiv.org/abs/2602.21420)
*Yuanda Xu,Hejian Sang,Zhengze Zhou,Ran He,Zhipeng Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.

</details>


### [46] [On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation](https://arxiv.org/abs/2602.21424)
*Alexander Galozy*

Main category: cs.LG

TL;DR: 本文提出了行为依赖性的概念，用以描述在固定观察下内部信息变化对动作选择的影响。基于此定义了ε-行为等价性和策略内的行为距离来量化探针敏感性。研究发现，表现出非平凡行为依赖性的策略集合在凸聚合下不封闭；行为距离在凸组合下收缩；当主导模式梯度与最陡收缩方向一致时，在偏斜混合目标上进行梯度上升可以减少行为距离。通过最小化带实验和部分可观察网格世界实验验证了这些机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）智能体在部分可观测环境中通常会根据内部积累的信息（如记忆或推断的潜在上下文）来调节其行动。这种现象被形式化为行为依赖性：即在固定观测条件下，内部信息的变化导致动作选择的变化。本文旨在探索这种行为依赖性，并提出一种方法来量化不同策略之间的行为差异。

Method: 本文首先定义了一个关于行为依赖性的概念框架，包括ε-行为等价性和策略内行为距离的概念。接着，通过理论分析确立了三个结构结果：1) 表现出显著行为依赖性的策略集对于凸聚合操作来说不是封闭的；2) 凸组合会导致行为距离减小；3) 当主要模式梯度与最大收缩方向一致时，使用倾斜混合目标函数执行梯度上升可以进一步降低行为距离。为了验证这些理论发现，作者设计并实施了两个实验：一个是最小化带问题，另一个是部分可观察的网格世界环境。

Result: 实验结果表明，在所考察的设置中，行为距离确实随着凸聚合以及持续优化带有偏差的潜在先验而减少。此外，在这些实验中还观察到，行为距离的减少预示着潜在先验转移下的性能下降。

Conclusion: 本研究表明，在某些结构性条件下，基于探针条件的行为分离特性不会在常见的策略转换过程中保持不变。这为理解强化学习系统中的行为动态提供了一个新的视角。

Abstract: Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect to internal information under fixed observations. This induces a probe-relative notion of $ε$-behavioural equivalence and a within-policy behavioural distance that quantifies probe sensitivity. We establish three structural results. First, the set of policies exhibiting non-trivial behavioural dependency is not closed under convex aggregation. Second, behavioural distance contracts under convex combination. Third, we prove a sufficient local condition under which gradient ascent on a skewed mixture objective decreases behavioural distance when a dominant-mode gradient aligns with the direction of steepest contraction. Minimal bandit and partially observable gridworld experiments provide controlled witnesses of these mechanisms. In the examined settings, behavioural distance decreases under convex aggregation and under continued optimisation with skewed latent priors, and in these experiments it precedes degradation under latent prior shift. These results identify structural conditions under which probe-conditioned behavioural separation is not preserved under common policy transformations.

</details>


### [47] [Proximal-IMH: Proximal Posterior Proposals for Independent Metropolis-Hastings with Approximate Operators](https://arxiv.org/abs/2602.21426)
*Youguang Chen,George Biros*

Main category: cs.LG

TL;DR: 本文提出了一种名为Proximal-IMH的方法，通过解决一个辅助优化问题来纠正从近似后验分布中抽取的样本，从而消除偏差。这种方法在科学、工程和成像中的贝叶斯逆问题中用于从后验分布采样。实验结果表明，与现有的IMH变体相比，Proximal-IMH方法在多模态先验和非线性输入输出操作员的数据驱动先验下表现得更可靠。


<details>
  <summary>Details</summary>
Motivation: 在科学、工程和成像领域，从贝叶斯逆问题产生的后验分布进行采样时面临挑战，尤其是当精确后验采样成本过高时。本文旨在提供一种有效的方法来改善这一过程，通过利用近似后验分布并修正其偏差以提高接受率和混合效率。

Method: 文章提出了Proximal-IMH方法，该方法基于独立Metropolis-Hastings (IMH)算法，并引入了一个额外的优化步骤来校正从廉价但可能有较大偏差的近似后验分布中抽样的结果。这样做的目的是为了平衡对准确模型的遵循与围绕近似参考点的稳定性。

Result: 理论分析证明了，在理想条件下，proximal correction能够加强近似后验与真实后验之间的匹配度，进而提升了接受率及混合效果。此外，数值实验证明了对于包含非线性输入-输出算子的多模式和数据驱动先验情况，Proximal-IMH的表现优于现有的IMH变体。

Conclusion: Proximal-IMH为处理复杂的贝叶斯逆问题提供了一种有效的解决方案，特别是在那些直接从精确后验分布采样成本过高的情况下。它不仅提高了采样效率，而且增强了对复杂模型的支持能力。

Abstract: We consider the problem of sampling from a posterior distribution arising in Bayesian inverse problems in science, engineering, and imaging. Our method belongs to the family of independence Metropolis-Hastings (IMH) sampling algorithms, which are common in Bayesian inference. Relying on the existence of an approximate posterior distribution that is cheaper to sample from but may have significant bias, we introduce Proximal-IMH, a scheme that removes this bias by correcting samples from the approximate posterior through an auxiliary optimization problem. This yields a local adjustment that trades off adherence to the exact model against stability around the approximate reference point. For idealized settings, we prove that the proximal correction tightens the match between approximate and exact posteriors, thereby improving acceptance rates and mixing. The method applies to both linear and nonlinear input-output operators and is particularly suitable for inverse problems where exact posterior sampling is too expensive. We present numerical experiments including multimodal and data-driven priors with nonlinear input-output operators. The results show that Proximal-IMH reliably outperforms existing IMH variants.

</details>


### [48] [Provably Safe Generative Sampling with Constricting Barrier Functions](https://arxiv.org/abs/2602.21429)
*Darshan Gadginmath,Ahmed Allibhoy,Fabio Pasqualetti*

Main category: cs.LG

TL;DR: 提出了一种安全过滤框架，通过控制屏障函数和二次规划技术，在保持生成模型原有结构的同时，确保生成样本满足硬性约束条件。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于流的生成模型在关键安全领域应用时缺乏正式保证的问题，即生成样本可能不满足硬性约束条件。

Method: 设计了一个从初始噪声分布到最终数据分布逐渐收紧的安全管，并利用控制屏障函数（CBFs）通过凸二次规划（QP）在每个采样步骤中合成反馈控制输入。

Result: 该机制保证了安全采样的同时，在每一步采样中最小化了与原始模型之间的分布偏移（以KL散度衡量）。

Conclusion: 本研究提出的框架适用于任何预训练的基于流的生成方案，无需重新训练或架构修改，成功应用于受限图像生成、物理一致轨迹采样以及安全机器人操作策略，实现了100%的约束满足率并保持了语义保真度。

Abstract: Flow-based generative models, such as diffusion models and flow matching models, have achieved remarkable success in learning complex data distributions. However, a critical gap remains for their deployment in safety-critical domains: the lack of formal guarantees that generated samples will satisfy hard constraints. We address this by proposing a safety filtering framework that acts as an online shield for any pre-trained generative model. Our key insight is to cooperate with the generative process rather than override it. We define a constricting safety tube that is relaxed at the initial noise distribution and progressively tightens to the target safe set at the final data distribution, mirroring the coarse-to-fine structure of the generative process itself. By characterizing this tube via Control Barrier Functions (CBFs), we synthesize a feedback control input through a convex Quadratic Program (QP) at each sampling step. As the tube is loosest when noise is high and intervention is cheapest in terms of control energy, most constraint enforcement occurs when it least disrupts the model's learned structure. We prove that this mechanism guarantees safe sampling while minimizing the distributional shift from the original model at each sampling step, as quantified by the KL divergence. Our framework applies to any pre-trained flow-based generative scheme requiring no retraining or architectural modifications. We validate the approach across constrained image generation, physically-consistent trajectory sampling, and safe robotic manipulation policies, achieving 100% constraint satisfaction while preserving semantic fidelity.

</details>


### [49] [Causal Decoding for Hallucination-Resistant Multimodal Large Language Models](https://arxiv.org/abs/2602.21441)
*Shiwei Tan,Hengyi Wang,Weiyi Qin,Qi Xu,Zhigang Hua,Hao Wang*

Main category: cs.LG

TL;DR: 提出了一种因果解码框架来减少多模态大语言模型在视觉-语言任务中出现的对象幻觉问题，该方法能在不降低输出质量的前提下显著提高生成内容的忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型虽然能够提供详细的响应，但在处理视觉-语言任务时容易产生对象幻觉（即描述了图片中不存在的对象），这影响了模型的实际可靠性。以往解决这一问题的方法通常依赖于启发式惩罚、事后修正或通用解码调整等手段，但这些方法并未直接干预导致对象幻觉产生的机制，因此效果有限。

Method: 本文介绍了一种新的因果解码框架，通过在生成过程中实施针对性的因果干预来抑制虚假对象的提及。这种方法通过重塑解码动态以减弱错误依赖关系，减少了错误对象标记的出现，同时保持了描述的质量。

Result: 实验表明，在图像描述和问答基准测试中，所提出的框架大幅降低了对象幻觉的发生率，并且达到了最先进的忠实度水平，同时没有损害整体输出质量。

Conclusion: 本文提出的因果解码框架有效解决了多模态大语言模型中的对象幻觉问题，提高了模型响应的可靠性和准确性。

Abstract: Multimodal Large Language Models (MLLMs) deliver detailed responses on vision-language tasks, yet remain susceptible to object hallucination (introducing objects not present in the image), undermining reliability in practice. Prior efforts often rely on heuristic penalties, post-hoc correction, or generic decoding tweaks, which do not directly intervene in the mechanisms that trigger object hallucination and thus yield limited gains. To address this challenge, we propose a causal decoding framework that applies targeted causal interventions during generation to curb spurious object mentions. By reshaping the decoding dynamics to attenuate spurious dependencies, our approach reduces false object tokens while maintaining descriptive quality. Across captioning and QA benchmarks, our framework substantially lowers object-hallucination rates and achieves state-of-the-art faithfulness without degrading overall output quality.

</details>


### [50] [D-Flow SGLD: Source-Space Posterior Sampling for Scientific Inverse Problems with Flow Matching](https://arxiv.org/abs/2602.21469)
*Meet Hemant Parikh,Yaqin Chen,Jian-Xun Wang*

Main category: cs.LG

TL;DR: 本研究探讨了在Flow Matching (FM)先验下科学反问题的无训练条件生成方法，并提出了D-Flow SGLD，一种能够在不重新训练先验或修改学习到的FM动态的情况下，通过新测量算子探索源后验的方法。该方法在多个问题层次上进行了基准测试，证明了其在科学反问题中的实用性。


<details>
  <summary>Details</summary>
Motivation: 数据同化和科学反问题需要从稀疏且带有噪声的观测中重建高维物理状态，理想情况下，这些状态应具备不确定性感知的后验样本，同时保持对学习先验和管理物理学的忠实度。虽然扩散模型的无训练条件生成已经相当成熟，但对于FM先验下的相应条件设置和后验抽样策略的研究相对较少，特别是在科学基准测试中，除了测量误差外还需要评估保真度。

Method: 本文研究了FM先验下科学反问题的无训练条件生成，并将现有推理时策略按照测量信息注入的位置分为两类：（i）利用似然信息扰动采样轨迹的引导传输动力学；（ii）执行后验推断的同时固定已学习传输的源分布推断。基于后者，提出了一种名为D-Flow SGLD的方法，它通过预处理随机梯度朗之万动力学增强可微分源推断，从而能够针对由新的测量算子引起的源后验进行可扩展探索，而无需重新训练先验或调整已学习的FM动态。

Result: 代表性方法在不同层次的问题上进行了基准测试，包括2D玩具后验、混沌Kuramoto-Sivashinsky轨迹以及壁面湍流重构等。在这些情境下，研究量化了测量同化、后验多样性与物理/统计保真度之间的权衡关系，并确立了D-Flow SGLD作为适用于科学反问题的实际FM兼容后验抽样器的地位。

Conclusion: D-Flow SGLD被证明是一种有效的方法，可以在不改变已学习FM动态的前提下，为科学反问题提供不确定性意识的后验样本。该方法促进了对于如何在给定新型测量操作符时探索源后验的理解，并展示了其在维持物理与统计保真度方面的能力。

Abstract: Data assimilation and scientific inverse problems require reconstructing high-dimensional physical states from sparse and noisy observations, ideally with uncertainty-aware posterior samples that remain faithful to learned priors and governing physics. While training-free conditional generation is well developed for diffusion models, corresponding conditioning and posterior sampling strategies for Flow Matching (FM) priors remain comparatively under-explored, especially on scientific benchmarks where fidelity must be assessed beyond measurement misfit. In this work, we study training-free conditional generation for scientific inverse problems under FM priors and organize existing inference-time strategies by where measurement information is injected: (i) guided transport dynamics that perturb sampling trajectories using likelihood information, and (ii) source-distribution inference that performs posterior inference over the source variable while keeping the learned transport fixed. Building on the latter, we propose D-Flow SGLD, a source-space posterior sampling method that augments differentiable source inference with preconditioned stochastic gradient Langevin dynamics, enabling scalable exploration of the source posterior induced by new measurement operators without retraining the prior or modifying the learned FM dynamics. We benchmark representative methods from both families on a hierarchy of problems: 2D toy posteriors, chaotic Kuramoto-Sivashinsky trajectories, and wall-bounded turbulence reconstruction. Across these settings, we quantify trade-offs among measurement assimilation, posterior diversity, and physics/statistics fidelity, and establish D-Flow SGLD as a practical FM-compatible posterior sampler for scientific inverse problems.

</details>


### [51] [The Design Space of Tri-Modal Masked Diffusion Models](https://arxiv.org/abs/2602.21472)
*Louis Bethune,Victor Turrisi,Bruno Kacper Mlodozeniec,Pau Rodriguez Lopez,Lokesh Boominathan,Nikhil Bhendawade,Amitis Shidani,Joris Pelemans,Theo X. Olausson,Devon Hjelm,Paul Dixon,Joao Monteiro,Pierre Ablin,Vishnu Banna,Arno Blaas,Nick Henderson,Kari Noriy,Dan Busbridge,Josh Susskind,Marco Cuturi,Irina Belousova,Luca Zappella,Russ Webb,Jason Ramapuram*

Main category: cs.LG

TL;DR: 本文介绍了一种从文本、图文和音频文本数据中从头开始预训练的首个三模态掩码扩散模型，通过系统分析多模态扩展规律等，并提出一种新的基于SDE的重参数化方法，最终在文本生成、文到图任务以及文到语音任务上取得了很好的结果。


<details>
  <summary>Details</summary>
Motivation: 以往的工作主要集中在使用单模态模型进行双模态生成，而本文旨在探索一个全新的方向——构建能够处理文本、图像-文本和音频-文本三种模态信息的三模态掩码扩散模型。

Method: 研究者们首先从零开始对一个拥有30亿参数的三模态模型进行了预训练，该过程涉及了6.4万亿个标记的数据集。接着，他们深入分析了多模态扩展法则、模态混合比例、噪声调度策略及批量大小的影响，并在此基础上提出了一个新的基于随机微分方程（SDE）的重参数化技术。

Result: 实验结果显示，在文本生成、文本到图像转换以及文本到语音合成任务中，所提出的三模态模型均表现出了优异性能。此外，新提出的SDE重参数化方法成功地解决了最优批量大小调整的问题。

Conclusion: 这项工作代表了迄今为止最大规模的多模态离散扩散模型开放研究之一，为理解跨多种模态的扩展行为提供了宝贵的见解。

Abstract: Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities.

</details>


### [52] [GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning](https://arxiv.org/abs/2602.21492)
*Ningyuan Yang,Weihua Du,Weiwei Sun,Sean Welleck,Yiming Yang*

Main category: cs.LG

TL;DR: 提出了一种名为GradAlign的梯度对齐数据选择方法，用于大型语言模型的强化学习，通过使用一个小而可信的验证集来优先选择策略梯度与验证梯度对齐的训练问题，从而生成自适应课程。在三个具有挑战性的数据场景下评估了GradAlign的表现，并发现它始终优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）已经成为大型语言模型（LLMs）后训练阶段的核心范式，但其性能极易受到训练问题质量的影响。这种敏感性源于RL的非稳态特性：rollouts由不断演化的策略产生，学习过程受到探索和奖励反馈的影响，这与轨迹固定的监督微调不同。因此，先前的工作通常依赖于人工策划或简单的启发式过滤器（例如准确性），但这可能导致引入错误或低效的问题。

Method: 提出了GradAlign，一种基于梯度对齐的数据选择方法，旨在解决RL中因非稳态特性导致的学习效率低下问题。该方法利用一小部分可信验证集来优先考虑那些其策略梯度与验证集上计算得到的梯度方向一致的训练样本，以此形成一个能够自我调整的学习路径。

Result: GradAlign在处理不可靠奖励信号、分布不平衡以及低效训练语料库这三个难题时表现优异，持续超越了现有的基准方法，证明了方向性梯度信号对于应对非稳态策略优化的重要性，有助于实现更稳定的学习过程及提升最终模型性能。

Conclusion: 研究结果表明，GradAlign是一种有效的方法，能够为大型语言模型的强化学习提供更好的训练问题选择，进而促进更稳健的训练流程和增强最终模型的能力。

Abstract: Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign

</details>


### [53] [Learning Recursive Multi-Scale Representations for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.21498)
*Boyuan Li,Zhen Liu,Yicheng Luo,Qianli Ma*

Main category: cs.LG

TL;DR: 提出了一种名为ReIMTS的递归多尺度建模方法，用于不规则多变量时间序列（IMTS）的预测。该方法通过保持时间戳不变并递归地将每个样本分割成具有逐渐缩短时间段的子样本，来捕捉全局到局部的依赖关系，从而提高预测准确性。实验表明，与现有方法相比，ReIMTS在不同模型和真实数据集上的平均性能提高了27.1%。


<details>
  <summary>Details</summary>
Motivation: 现有的处理不规则多变量时间序列的方法通常会采用重采样技术来获取粗略的时间序列数据，但这样做可能会改变原始时间戳，并破坏了有价值的时间采样模式信息。此外，IMTS往往在多个时间尺度上展示出不同的依赖性。为了解决这些问题，提出了新的方法。

Method: ReIMTS是一种针对不规则多变量时间序列的递归多尺度建模方法，它避免了使用重采样的手段，而是直接基于原有的时间戳信息，通过递归地将每一个样本划分为越来越短的时间段子样本的方式来进行。基于这些从长到短的子样本中的原始时间戳，开发了一个能够意识到不规则性的表示融合机制，以捕捉从全局到局部的依赖关系。

Result: 广泛的实验证明了ReIMTS方法的有效性，在不同类型的模型以及实际应用的数据集中，相对于其他方法，ReIMTS在预测任务中实现了平均27.1%的性能提升。

Conclusion: ReIMTS提供了一种有效的方法来处理不规则多变量时间序列问题，通过保留原始时间戳信息并利用递归多尺度建模策略，显著提升了预测任务的表现。

Abstract: Irregular Multivariate Time Series (IMTS) are characterized by uneven intervals between consecutive timestamps, which carry sampling pattern information valuable and informative for learning temporal and variable dependencies. In addition, IMTS often exhibit diverse dependencies across multiple time scales. However, many existing multi-scale IMTS methods use resampling to obtain the coarse series, which can alter the original timestamps and disrupt the sampling pattern information. To address the challenge, we propose ReIMTS, a Recursive multi-scale modeling approach for Irregular Multivariate Time Series forecasting. Instead of resampling, ReIMTS keeps timestamps unchanged and recursively splits each sample into subsamples with progressively shorter time periods. Based on the original sampling timestamps in these long-to-short subsamples, an irregularity-aware representation fusion mechanism is proposed to capture global-to-local dependencies for accurate forecasting. Extensive experiments demonstrate an average performance improvement of 27.1\% in the forecasting task across different models and real-world datasets. Our code is available at https://github.com/Ladbaby/PyOmniTS.

</details>


### [54] [WaterVIB: Learning Minimal Sufficient Watermark Representations via Variational Information Bottleneck](https://arxiv.org/abs/2602.21508)
*Haoyuan He,Yu Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.LG

TL;DR: 提出了一种名为WaterVIB的新框架，该框架通过变分信息瓶颈将编码器重新定义为信息筛子，从而在对抗生成性净化攻击时提供更强大的鲁棒性水印。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒水印方法在面对基于再生的AIGC攻击时存在严重漏洞，因为它们将水印与容易被重写的高频覆盖纹理纠缠在一起。

Method: WaterVIB框架利用变分信息瓶颈来强制模型学习消息的最小充分统计量，以过滤掉易受生成性变化影响的多余覆盖细节，仅保留对再生不变的基本信号。

Result: 广泛的实验证明了WaterVIB在未知扩散编辑面前具有优越的零样本韧性，并且明显优于最先进的方法。

Conclusion: 研究证明了优化这一瓶颈是抵御分布偏移攻击鲁棒性的必要条件，并且WaterVIB能有效提升水印抵抗基于AI的内容生成攻击的能力。

Abstract: Robust watermarking is critical for intellectual property protection, whereas existing methods face a severe vulnerability against regeneration-based AIGC attacks. We identify that existing methods fail because they entangle the watermark with high-frequency cover texture, which is susceptible to being rewritten during generative purification. To address this, we propose WaterVIB, a theoretically grounded framework that reformulates the encoder as an information sieve via the Variational Information Bottleneck. Instead of overfitting to fragile cover details, our approach forces the model to learn a Minimal Sufficient Statistic of the message. This effectively filters out redundant cover nuances prone to generative shifts, retaining only the essential signal invariant to regeneration. We theoretically prove that optimizing this bottleneck is a necessary condition for robustness against distribution-shifting attacks. Extensive experiments demonstrate that WaterVIB significantly outperforms state-of-the-art methods, achieving superior zero-shot resilience against unknown diffusion-based editing.

</details>


### [55] [Training Generalizable Collaborative Agents via Strategic Risk Aversion](https://arxiv.org/abs/2602.21515)
*Chengrui Qu,Yizhou Zhang,Nicholas Lanzetti,Eric Mazumdar*

Main category: cs.LG

TL;DR: 本文针对现有方法在学习合作问题策略时产生的解决方案脆弱性问题，提出了一种基于策略风险规避的概念来增强与未见过的伙伴的合作泛化能力。通过将策略风险规避整合到标准策略优化方法中，开发了一种多智能体强化学习算法。实验结果表明，该方法能够与异质和未见过的伙伴在协作任务中实现可靠合作。


<details>
  <summary>Details</summary>
Motivation: 现有的学习合作问题策略的方法往往产生出脆弱的解决方案，当遇到新的合作伙伴时容易失败。这些问题主要归因于训练期间的搭便车现象以及缺乏策略鲁棒性。为了改善这一状况，研究引入了策略风险规避的概念，旨在提高与未知伙伴合作时的泛化能力和鲁棒性。

Method: 文章提出了一个基于策略风险规避概念的多智能体强化学习（MARL）算法，该算法将策略风险规避作为原则性的归纳偏置集成到了标准的策略优化方法之中。这种方法不仅设计上对于伙伴行为的变化具有鲁棒性，在合作游戏中还显示出更优的均衡结果，并且减少了或完全避免了搭便车现象。

Result: 通过对多个协作基准测试（包括一个LLM协作任务）进行实验验证，证明了所提理论的有效性。结果显示，该方法能够在各种协作任务中与不同类型的、以前未见过的伙伴实现一致而可靠的协作。

Conclusion: 通过引入并应用策略风险规避的概念，研究成功地提高了多智能体系统在面对新伙伴时的协作稳定性和效率。这为未来构建更加灵活、适应性强的协作型人工智能提供了新的视角和技术手段。

Abstract: Many emerging agentic paradigms require agents to collaborate with one another (or people) to achieve shared goals. Unfortunately, existing approaches to learning policies for such collaborative problems produce brittle solutions that fail when paired with new partners. We attribute these failures to a combination of free-riding during training and a lack of strategic robustness. To address these problems, we study the concept of strategic risk aversion and interpret it as a principled inductive bias for generalizable cooperation with unseen partners. While strategically risk-averse players are robust to deviations in their partner's behavior by design, we show that, in collaborative games, they also (1) can have better equilibrium outcomes than those at classical game-theoretic concepts like Nash, and (2) exhibit less or no free-riding. Inspired by these insights, we develop a multi-agent reinforcement learning (MARL) algorithm that integrates strategic risk aversion into standard policy optimization methods. Our empirical results across collaborative benchmarks (including an LLM collaboration task) validate our theory and demonstrate that our approach consistently achieves reliable collaboration with heterogeneous and previously unseen partners across collaborative tasks.

</details>


### [56] [Muon+: Towards Better Muon via One Additional Normalization Step](https://arxiv.org/abs/2602.21545)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种对Muon优化器的改进版本，称为Muon+，通过在正交化后添加额外的归一化步骤来提高大型语言模型预训练的效果。实验结果表明，Muon+相比Muon在不同规模和架构的语言模型上都能提供一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 作者希望进一步提高Muon优化器在大规模语言模型预训练过程中的表现，为此引入了额外的归一化步骤来增强其效果。

Method: 提出了Muon+，它在原有Muon基础上增加了正交化后的归一化处理。该方法被应用于从130M到774M参数的GPT风格模型以及从60M到1B参数的LLaMA风格模型中进行测试。

Result: 实验结果显示，在不同的模型规模下，使用Muon+比单独使用Muon可以获得更好的训练和验证困惑度（perplexity）。此外，还将token-to-parameter比率扩展到了约200这一工业级别。

Conclusion: 研究证明了Muon+作为对Muon的一个简单而有效的改进，在广泛的模型尺度和架构上都表现出色，并且在计算最优训练条件下也能带来显著的好处。

Abstract: The Muon optimizer has demonstrated promising performance in pre-training large language models through gradient (or momentum) orthogonalization. In this work, we propose a simple yet effective enhancement to Muon, namely Muon+, which introduces an additional normalization step after orthogonalization. We demonstrate the effectiveness of Muon+ through extensive pre-training experiments across a wide range of model scales and architectures. Our evaluation includes GPT-style models ranging from 130M to 774M parameters and LLaMA-style models ranging from 60M to 1B parameters. We comprehensively evaluate the effectiveness of Muon+ in the compute-optimal training regime and further extend the token-to-parameter (T2P) ratio to an industrial level of $\approx 200$. Experimental results show that Muon+ provides a consistent boost on training and validation perplexity over Muon. We provide our code here: https://github.com/K1seki221/MuonPlus.

</details>


### [57] [Extending Sequence Length is Not All You Need: Effective Integration of Multimodal Signals for Gene Expression Prediction](https://arxiv.org/abs/2602.21550)
*Zhao Yang,Yi Duan,Jiwei Zhu,Ying Ba,Chuan Cao,Bing Su*

Main category: cs.LG

TL;DR: 研究发现，对于当前模型而言，长序列建模可能降低性能。更重要的是整合目标基因附近的多模态表观遗传信号。为此提出Prism框架，通过学习高维表观遗传特征的多种组合来表示不同的背景染色质状态，并使用后门调整来减轻混淆效应，从而在仅使用短序列的情况下达到最先进的基因表达预测性能。


<details>
  <summary>Details</summary>
Motivation: 先前的工作往往集中在扩展输入序列长度以定位远端增强子上，而这项研究揭示了对当前模型来说，长序列建模实际上会降低性能。研究转而强调目标基因附近近端多模态表观遗传信号的重要性，并指出这些信号对于提高基因表达预测准确性至关重要。

Method: 提出了Prism框架，该框架能够学习高维度表观遗传特征的多种组合方式，以此来描绘出不同的背景染色质状态，并采用后门调整策略减少由于背景模式带来的混淆影响。

Result: 实验结果表明，通过正确处理多模态表观遗传信号，在仅利用较短DNA序列的情况下也能实现最佳水平的基因表达预测效果。

Conclusion: 研究表明，相较于单纯增加序列长度，有效整合目标基因邻近区域内的多模态表观遗传信息对于改善基因表达预测模型更为关键。

Abstract: Gene expression prediction, which predicts mRNA expression levels from DNA sequences, presents significant challenges. Previous works often focus on extending input sequence length to locate distal enhancers, which may influence target genes from hundreds of kilobases away. Our work first reveals that for current models, long sequence modeling can decrease performance. Even carefully designed algorithms only mitigate the performance degradation caused by long sequences. Instead, we find that proximal multimodal epigenomic signals near target genes prove more essential. Hence we focus on how to better integrate these signals, which has been overlooked. We find that different signal types serve distinct biological roles, with some directly marking active regulatory elements while others reflect background chromatin patterns that may introduce confounding effects. Simple concatenation may lead models to develop spurious associations with these background patterns. To address this challenge, we propose Prism, a framework that learns multiple combinations of high-dimensional epigenomic features to represent distinct background chromatin states and uses backdoor adjustment to mitigate confounding effects. Our experimental results demonstrate that proper modeling of multimodal epigenomic signals achieves state-of-the-art performance using only short sequences for gene expression prediction.

</details>


### [58] [From Basis to Basis: Gaussian Particle Representation for Interpretable PDE Operators](https://arxiv.org/abs/2602.21551)
*Zhihao Li,Yu Feng,Zhilu Lai,Wei Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于高斯基表示的流体PDE动力学学习方法，通过引入高斯粒子算子实现了模态空间中的操作，具有近线性复杂度和固有的可解释性，并在标准PDE基准测试和真实数据集上达到了最先进的准确度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经算子和Transformer模型的学习流体PDE动力学的方法通常缺乏可解释性，难以处理局部高频结构，并且随着空间样本数量增加而面临二次成本问题。

Method: 提出了使用带有明确几何属性（中心、各向异性尺度、权重）的高斯基来表示场，形成了一个紧凑、与网格无关且可以直接可视化的状态；基于此表示法，引入了在模态空间中作用的高斯粒子算子：学习到的高斯模态窗口执行Petrov-Galerkin测量，而PG高斯注意力机制则允许全局跨尺度耦合。

Result: 所提方法在标准PDE基准测试及真实数据集中取得了竞争力强的最新准确度表现，同时提供了内在的可解释性支持。

Conclusion: 提出的基于高斯基的方法为流体PDE动力学建模提供了一条新的路径，它不仅能够有效处理局部高频结构的问题，还降低了计算成本，并增强了结果的可解释性。

Abstract: Learning PDE dynamics for fluids increasingly relies on neural operators and Transformer-based models, yet these approaches often lack interpretability and struggle with localized, high-frequency structures while incurring quadratic cost in spatial samples. We propose representing fields with a Gaussian basis, where learned atoms carry explicit geometry (centers, anisotropic scales, weights) and form a compact, mesh-agnostic, directly visualizable state. Building on this representation, we introduce a Gaussian Particle Operator that acts in modal space: learned Gaussian modal windows perform a Petrov-Galerkin measurement, and PG Gaussian Attention enables global cross-scale coupling. This basis-to-basis design is resolution-agnostic and achieves near-linear complexity in N for a fixed modal budget, supporting irregular geometries and seamless 2D-to-3D extension. On standard PDE benchmarks and real datasets, our method attains state-of-the-art competitive accuracy while providing intrinsic interpretability.

</details>


### [59] [Training-free Composition of Pre-trained GFlowNets for Multi-Objective Generation](https://arxiv.org/abs/2602.21565)
*Seokwon Yoon,Youngbin Choi,Seunghyuk Cho,Seungbeom Lee,MoonJeong Park,Dongwoo Kim*

Main category: cs.LG

TL;DR: 提出了一种无需额外训练即可在推理时组合预训练GFlowNets的方法，支持从线性标量化到复杂非线性逻辑运算符的多样化奖励组合，实验证明该方法在合成2D网格和真实分子生成任务上表现与需要额外训练的基线相当。


<details>
  <summary>Details</summary>
Motivation: 现实应用中经常涉及多个相互冲突的目标，但现有的多目标设置下的GFlowNets方法需要对每组目标进行额外训练，这限制了它们的应用并带来了巨大的计算开销。

Method: 提出了一种无需训练的混合策略，可以在推理阶段组合预训练的GFlowNets，以实现快速适应而无需微调或重新训练。框架灵活，能够处理从线性标量化到复杂的非线性逻辑运算符的各种奖励组合。

Result: 通过实验，在合成2D网格和实际分子生成任务上，所提出的方法达到了与需要额外训练的基准相媲美的性能。

Conclusion: 本研究提出的无训练混合策略为GFlowNets在多目标场景下的应用提供了更高效、更灵活的解决方案，能够在不牺牲性能的情况下显著减少计算资源的需求。

Abstract: Generative Flow Networks (GFlowNets) learn to sample diverse candidates in proportion to a reward function, making them well-suited for scientific discovery, where exploring multiple promising solutions is crucial. Further extending GFlowNets to multi-objective settings has attracted growing interest since real-world applications often involve multiple, conflicting objectives. However, existing approaches require additional training for each set of objectives, limiting their applicability and incurring substantial computational overhead. We propose a training-free mixing policy that composes pre-trained GFlowNets at inference time, enabling rapid adaptation without finetuning or retraining. Importantly, our framework is flexible, capable of handling diverse reward combinations ranging from linear scalarization to complex non-linear logical operators, which are often handled separately in previous literature. We prove that our method exactly recovers the target distribution for linear scalarization and quantify the approximation quality for nonlinear operators through a distortion factor. Experiments on a synthetic 2D grid and real-world molecule-generation tasks demonstrate that our approach achieves performance comparable to baselines that require additional training.

</details>


### [60] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Duel-Evolve的进化优化算法，该算法利用来自同一语言模型的成对偏好来替代外部标量奖励，通过贝叶斯Bradley-Terry模型聚合这些噪声候选比较，从而估计候选质量。这种方法不需要奖励模型、搜索过程中的真实标签或手工制作的评分函数，并且在MathBench和LiveCodeBench测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对于许多任务而言，现有的方法使用校准后的标量评估器来指导搜索，但这样的分数可能不可用、过于稀疏或不可靠。相比之下，成对比较往往更容易获得，仍然能够提供改进方向的有效信号，并且可以从语言模型本身获得而无需外部监督。基于这一观察，研究者们开发了Duel-Evolve算法。

Method: Duel-Evolve是一种进化优化算法，它使用来自生成候选者的同一大型语言模型（LLM）的成对偏好取代了外部标量奖励。通过贝叶斯Bradley-Terry模型汇总这些噪音候选比较，产生具有不确定性意识的候选质量估计。这些质量估计指导着比较预算向可能最优解分配，同时选择高质量父代以生成改进后的候选者。

Result: 在MathBench上，Duel-Evolve比现有方法和基线高出20个百分点；在LiveCodeBench上，与可比较的迭代方法相比，它提高了超过12个百分点。

Conclusion: 结果表明，在大规模离散输出空间上进行测试时改善，成对自我偏好提供了强大的优化信号。此方法不需要任何奖励模型、搜索期间的真实标签或手工制作的评分函数。

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [61] [NGDB-Zoo: Towards Efficient and Scalable Neural Graph Databases Training](https://arxiv.org/abs/2602.21597)
*Zhongwei Xie,Jiaxin Bai,Shujie Liu,Haoyu Huang,Yufei Li,Yisen Gao,Hong Ting Tsang,Yangqiu Song*

Main category: cs.LG

TL;DR: 介绍了NGDB-Zoo，一个统一框架，通过结合操作级训练和语义增强来解决神经图数据库的训练效率和表达力问题。该框架提高了处理吞吐量，并且能够整合预训练文本编码器的高维语义先验知识，同时避免I/O停滞或内存溢出的问题。在六个基准测试中表现出色，特别是在大规模图数据集上保持了高GPU利用率并显著减少了混合神经-符号推理中的表示摩擦。


<details>
  <summary>Details</summary>
Motivation: 现有的神经图数据库（NGDBs）虽然能够处理不完整知识结构上的复杂逻辑推理，但其训练效率和表达能力受到固定查询级别批处理和结构专属嵌入的限制。

Method: 提出了NGDB-Zoo，一个将操作级训练与语义增强相结合的统一框架。通过将逻辑运算符与查询拓扑解耦，使得训练循环转变为动态调度的数据流执行，从而实现多流并行化。此外，还设计了一种解耦架构以集成来自预训练文本编码器（PTEs）的高维语义先验，而不引起I/O停滞或内存溢出。

Result: NGDB-Zoo相比基线方法实现了1.8倍至6.8倍的吞吐量提升。在包括ogbl-wikikg2和ATLAS-Wiki在内的六个基准测试中，它能够在不同的逻辑模式下维持高GPU利用率，并大幅缓解了混合神经-符号推理中的表示摩擦问题。

Conclusion: NGDB-Zoo提供了一个有效的方法来克服现有神经图数据库面临的训练效率低及表达力受限的问题，通过引入创新性的操作级训练机制和语义增强策略，不仅提高了处理速度，也增强了模型对于复杂逻辑关系的理解能力。

Abstract: Neural Graph Databases (NGDBs) facilitate complex logical reasoning over incomplete knowledge structures, yet their training efficiency and expressivity are constrained by rigid query-level batching and structure-exclusive embeddings. We present NGDB-Zoo, a unified framework that resolves these bottlenecks by synergizing operator-level training with semantic augmentation. By decoupling logical operators from query topologies, NGDB-Zoo transforms the training loop into a dynamically scheduled data-flow execution, enabling multi-stream parallelism and achieving a $1.8\times$ - $6.8\times$ throughput compared to baselines. Furthermore, we formalize a decoupled architecture to integrate high-dimensional semantic priors from Pre-trained Text Encoders (PTEs) without triggering I/O stalls or memory overflows. Extensive evaluations on six benchmarks, including massive graphs like ogbl-wikikg2 and ATLAS-Wiki, demonstrate that NGDB-Zoo maintains high GPU utilization across diverse logical patterns and significantly mitigates representation friction in hybrid neuro-symbolic reasoning.

</details>


### [62] [Multimodal Survival Modeling and Fairness-Aware Clinical Machine Learning for 5-Year Breast Cancer Risk Prediction](https://arxiv.org/abs/2602.21648)
*Toktam Khatibi*

Main category: cs.LG

TL;DR: 该论文提出了一种多模态机器学习框架，用于乳腺癌5年总体生存预测，结合了临床变量与高维转录组和拷贝数变异特征。通过比较弹性网正则化的Cox模型（CoxNet）和基于XGBoost的梯度提升生存树模型，结果显示两种方法在性能上各有优势，但均表现出良好的校准性和公平性。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型在实际应用中常因校准不佳、可移植性差及亚群差异而表现欠佳，特别是在高维多模态癌症数据集中这些问题更为突出。

Method: 研究者开发了一个完全可重复的多模态机器学习框架，整合了来自METABRIC队列的临床变量以及高维转录组学和拷贝数变异特征。经过方差-稀疏性过滤和降维处理后，使用分层训练/验证/测试拆分法训练模型，并基于验证集调整超参数。比较了两种生存分析方法：一种是弹性网正则化的Cox模型(CoxNet)，另一种是利用XGBoost实现的梯度提升生存树模型。

Result: CoxNet在验证集和测试集上的AUC分别为98.3和96.6，AP值为90.1和80.4；XGBoost对应的AUC为98.6和92.5，AP值为92.5和79.9。公平性诊断表明，在不同年龄组、雌激素受体状态、分子亚型及更年期状态之间具有稳定的区分能力。

Conclusion: 本研究介绍了一种面向治理的多模态生存分析框架，强调了在校准、公平审计、鲁棒性和可重复性方面的考虑，适用于高维度临床机器学习环境。

Abstract: Clinical risk prediction models often underperform in real-world settings due to poor calibration, limited transportability, and subgroup disparities. These challenges are amplified in high-dimensional multimodal cancer datasets characterized by complex feature interactions and a p >> n structure. We present a fully reproducible multimodal machine learning framework for 5-year overall survival prediction in breast cancer, integrating clinical variables with high-dimensional transcriptomic and copy-number alteration (CNA) features from the METABRIC cohort.
  After variance- and sparsity-based filtering and dimensionality reduction, models were trained using stratified train/validation/test splits with validation-based hyperparameter tuning. Two survival approaches were compared: an elastic-net regularized Cox model (CoxNet) and a gradient-boosted survival tree model implemented using XGBoost. CoxNet provides embedded feature selection and stable estimation, whereas XGBoost captures nonlinear effects and higher-order interactions.
  Performance was assessed using time-dependent area under the ROC curve (AUC), average precision (AP), calibration curves, Brier score, and bootstrapped 95 percent confidence intervals. CoxNet achieved validation and test AUCs of 98.3 and 96.6, with AP values of 90.1 and 80.4. XGBoost achieved validation and test AUCs of 98.6 and 92.5, with AP values of 92.5 and 79.9. Fairness diagnostics showed stable discrimination across age groups, estrogen receptor status, molecular subtypes, and menopausal state.
  This work introduces a governance-oriented multimodal survival framework emphasizing calibration, fairness auditing, robustness, and reproducibility for high-dimensional clinical machine learning.

</details>


### [63] [Error-awareness Accelerates Active Automata Learning](https://arxiv.org/abs/2602.21674)
*Loes Kruger,Sebastian Junges,Jurriaan Rot*

Main category: cs.LG

TL;DR: 本文提出了一种改进的主动自动机学习算法，针对系统中多数输入导致错误的问题，并根据不同程度的知识调整算法以提高学习效率。实验表明，这种方法在有强领域知识的情况下可以将学习速度提高几个数量级，在有限领域知识下也能提高一个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有的主动自动机学习算法难以处理大型模型，尤其是在系统面临许多可能输入时。即使在每个状态下大多数输入都会导致错误，现代算法也难以扩展。本文旨在更有效地学习这类系统，特别是当错误是可观察且已知其输出时。

Method: 作者考虑了关于哪些输入不会在哪个状态产生错误的不同程度的知识，并为每种知识水平提供了现有最优AAL算法L#的相应调整版本，以便充分利用这些领域知识来优化学习过程。

Result: 实证评估显示，该方法利用强大的但现实的领域知识时能够将学习加速几个数量级；而在仅有有限领域知识的情况下，也能实现单个数量级的学习加速。

Conclusion: 通过结合不同程度的领域知识来调整主动自动机学习算法，可以在面对大量可能导致错误的输入时显著提高学习效率。

Abstract: Active automata learning (AAL) algorithms can learn a behavioral model of a system from interacting with it. The primary challenge remains scaling to larger models, in particular in the presence of many possible inputs to the system. Modern AAL algorithms fail to scale even if, in every state, most inputs lead to errors. In various challenging problems from the literature, these errors are observable, i.e., they emit a known error output. Motivated by these problems, we study learning these systems more efficiently. Further, we consider various degrees of knowledge about which inputs are non-error producing at which state. For each level of knowledge, we provide a matching adaptation of the state-of-the-art AAL algorithm L# to make the most of this domain knowledge. Our empirical evaluation demonstrates that the methods accelerate learning by orders of magnitude with strong but realistic domain knowledge to a single order of magnitude with limited domain knowledge.

</details>


### [64] [Hierarchical Lead Critic based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.21680)
*David Eckel,Henri Meeß*

Main category: cs.LG

TL;DR: 本文提出了一种新的序列训练方案和多智能体强化学习架构，称为层次领导批评家（HLC），该方法能够结合局部和全局视角，在不同层次上学习，从而在合作、非通信和部分可观察的多智能体强化学习基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体强化学习方法通常局限于局部或全局视角之一，限制了其处理需要多智能体协调的复杂任务的能力。为了解决这个问题，提出了一个可以从多个视角在不同层级学习的新颖训练方案和架构。

Method: 提出了层次领导批评家（HLC）框架，受到团队结构中自然出现的分布启发，将遵循高层次目标与低层次执行相结合。HLC通过引入多层次，利用局部和全局视角来提高性能，同时保持高样本效率和稳健策略。

Result: 实验结果表明，HLC在合作性、无通讯以及部分可观测的MARL基准上优于单层次基线，并且随着智能体数量和难度的增加，它能够稳健地扩展。

Conclusion: 通过引入多层级并结合局部与全局视角进行学习，HLC证明了这种方法可以提高多智能体强化学习的表现，特别是在样本效率和策略稳健性方面。

Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, where following high-level objectives combines with low-level execution. HLC demonstrates that introducing multiple hierarchies, leveraging local and global perspectives, can lead to improved performance with high sample efficiency and robust policies. Experimental results conducted on cooperative, non-communicative, and partially observable MARL benchmarks demonstrate that HLC outperforms single hierarchy baselines and scales robustly with increasing amounts of agents and difficulty.

</details>


### [65] [TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts](https://arxiv.org/abs/2602.21693)
*Jiafeng Lin,Yuxuan Wang,Huakun Luo,Zhongyi Pei,Jianmin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为TiMi的方法，该方法通过利用大型语言模型的因果推理能力来生成对未来发展的推断，从而指导时间序列预测。TiMi引入了多模态混合专家模块作为轻量级插件，以增强基于Transformer的时间序列模型在多模odal预测中的能力，无需显式的表征级别对齐。实验表明，TiMi在十六个真实世界多模态预测基准上表现出色，优于先进基线，并且具有强大的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态时间序列预测能够通过利用其他模式中的丰富信息提供比传统单一模式模型更准确的预测。然而，由于模态对齐的基本挑战，现有方法往往难以有效将多模态数据（特别是文本信息）纳入预测中。文本信息如紧急报告和政策公告等对时间序列波动有因果影响。因此，本文旨在探讨文本信息在数值预测中的作用，并提出一种新的方法来克服这些挑战。

Method: 提出了Time series transformers with Multimodal Mixture-of-Experts (TiMi)，它利用大型语言模型(LLMs)产生对未来发展的推测，以此为时间序列预测提供指导。此外，为了无缝整合外生因素与时间序列进入预测过程，研究者们引入了一个名为Multimodal Mixture-of-Experts (MMoE) 的模块作为一个轻量级插件，增强了基于Transformer的时间序列模型处理多模态预报的能力，而不需要在表示层面上进行明确的对齐。

Result: 实验结果表明，在十六个实际应用的多模态预测基准测试中，所提出的TiMi方法一致地达到了最先进水平的表现，不仅超越了现有的高级基线方法，还展现了良好的适应性和解释性。

Conclusion: TiMi作为一种新颖的方法，成功地提升了多模态时间序列预测的效果，尤其是通过有效利用文本信息来改进预测准确性。其设计允许更好地整合不同类型的输入数据，并提供了优秀的性能、适应性及可解释性。

Abstract: Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and policy announcements. In this paper, we reflect on the role of textual information in numerical forecasting and propose Time series transformers with Multimodal Mixture-of-Experts, TiMi, to unleash the causal reasoning capabilities of LLMs. Concretely, TiMi utilizes LLMs to generate inferences on future developments, which serve as guidance for time series forecasting. To seamlessly integrate both exogenous factors and time series into predictions, we introduce a Multimodal Mixture-of-Experts (MMoE) module as a lightweight plug-in to empower Transformer-based time series models for multimodal forecasting, eliminating the need for explicit representation-level alignment. Experimentally, our proposed TiMi demonstrates consistent state-of-the-art performance on sixteen real-world multimodal forecasting benchmarks, outperforming advanced baselines while offering both strong adaptability and interpretability.

</details>


### [66] [Learning Complex Physical Regimes via Coverage-oriented Uncertainty Quantification: An application to the Critical Heat Flux](https://arxiv.org/abs/2602.21701)
*Michele Cazzola,Alberto Ghione,Lucia Sargentini,Julien Nespoulous,Riccardo Finotello*

Main category: cs.LG

TL;DR: 本文针对科学机器学习中多物理机制系统正确表示的挑战，通过对比分析不同的不确定性量化（UQ）方法在临界热流密度(CHF)预测上的应用效果，发现覆盖导向的学习方法能够更有效地调整模型以适应复杂的物理机制，从而不仅提高预测准确性，还提供了与内在变异性相匹配的物理一致性不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 科学研究中的机器学习面临的一个核心挑战是如何准确地表示由多种物理机制控制的系统。传统的数据分析技术往往无法捕捉到这些系统的数据本质，因为系统响应由于其随机性和不同物理机制的存在，在状态空间内变化极大。因此，不确定性量化(UQ)不仅仅是安全评估工具，更是支持学习任务本身的关键，帮助模型内化数据的行为特征。

Method: 研究聚焦于OECD/NEA专家小组提供的临界热流密度(CHF)基准和数据集，采用比较分析法来评估不同UQ方法对于物理表现的影响。具体来说，将后验方法即一致性预测与端到端覆盖导向流程包括(贝叶斯)异方差回归及质量驱动损失函数进行对比。

Result: 结果表明，尽管后验方法保证了统计校准性，但覆盖导向的学习方法能够有效重塑模型表征，使其更好地匹配复杂物理机制。这样得到的模型不仅能提供高精度预测，还能生成随CHF内在变异性动态调整的物理一致性不确定性估计。

Conclusion: 本研究表明，在处理像CHF这样的多物理机制问题时，采用特定设计的不确定性量化方法可以显著提升模型的表现力和可靠性。特别地，将不确定性视为优化过程中的一个活跃组成部分而非最终度量指标的方法，能够使模型同时模拟预测及其行为特性，从而实现更加精准且可靠的预测。

Abstract: A central challenge in scientific machine learning (ML) is the correct representation of physical systems governed by multi-regime behaviours. In these scenarios, standard data analysis techniques often fail to capture the nature of the data, as the system's response varies significantly across the state space due to its stochasticity and the different physical regimes. Uncertainty quantification (UQ) should thus not be viewed merely as a safety assessment, but as a support to the learning task itself, guiding the model to internalise the behaviour of the data. We address this by focusing on the Critical Heat Flux (CHF) benchmark and dataset presented by the OECD/NEA Expert Group on Reactor Systems Multi-Physics. This case study represents a test for scientific ML due to the non-linear dependence of CHF on the inputs and the existence of distinct microscopic physical regimes. These regimes exhibit diverse statistical profiles, a complexity that requires UQ techniques to internalise the data behaviour and ensure reliable predictions. In this work, we conduct a comparative analysis of UQ methodologies to determine their impact on physical representation. We contrast post-hoc methods, specifically conformal prediction, against end-to-end coverage-oriented pipelines, including (Bayesian) heteroscedastic regression and quality-driven losses. These approaches treat uncertainty not as a final metric, but as an active component of the optimisation process, modelling the prediction and its behaviour simultaneously. We show that while post-hoc methods ensure statistical calibration, coverage-oriented learning effectively reshapes the model's representation to match the complex physical regimes. The result is a model that delivers not only high predictive accuracy but also a physically consistent uncertainty estimation that adapts dynamically to the intrinsic variability of the CHF.

</details>


### [67] [From Words to Amino Acids: Does the Curse of Depth Persist?](https://arxiv.org/abs/2602.21750)
*Aleena Siji,Amir Mohammad Karimi Mamaghan,Ferdinand Kapl,Tobias Höppe,Emmanouil Angelis,Andrea Dittadi,Maurice Brenner,Michael Heinzinger,Karl Henrik Johansson,Kaitlin Maile,Johannes von Oswald,Stefan Bauer*

Main category: cs.LG

TL;DR: 研究发现，蛋白质语言模型（PLM）在深度增加时表现出与大型语言模型（LLM）相似的深度效率问题，即深层对最终输出贡献较小。这一观察结果适用于多种训练目标和模型规模，并提示未来需要探索更高效的深度架构和训练方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于最近关于自回归大型语言模型的研究指出深度诅咒现象——深层对于最终预测贡献不大，研究者们自然地提出疑问：这种深度效率低下的情况是否也存在于蛋白质语言模型中？特别是考虑到许多广泛使用的蛋白质语言模型并非自回归类型，且有些还能接受蛋白质序列和结构作为输入。

Method: 本研究通过分析六种流行的、跨越不同家族及规模的蛋白质语言模型来探讨上述问题，这些模型覆盖了三种训练目标：自回归、掩码以及扩散。研究者采用了一套统一的探测-扰动测量方法来量化随着层数增加每一层贡献的变化情况。

Result: 研究结果显示，在所有被研究的模型中都观察到了随深度变化的一致模式，这进一步证实了先前关于大型语言模型的研究发现：较深层级更多依赖于自身的计算，并主要是在微调最终输出分布；而且这种效应在更深的模型中更为显著。

Conclusion: 综上所述，该研究表明蛋白质语言模型确实存在某种程度上的深度效率低下问题，这为开发更加深度高效的架构及训练方法提供了动力。

Abstract: Protein language models (PLMs) have become widely adopted as general-purpose models, demonstrating strong performance in protein engineering and de novo design. Like large language models (LLMs), they are typically trained as deep transformers with next-token or masked-token prediction objectives on massive sequence corpora and are scaled by increasing model depth. Recent work on autoregressive LLMs has identified the Curse of Depth: later layers contribute little to the final output predictions. These findings naturally raise the question of whether a similar depth inefficiency also appears in PLMs, where many widely used models are not autoregressive, and some are multimodal, accepting both protein sequence and structure as input. In this work, we present a depth analysis of six popular PLMs across model families and scales, spanning three training objectives, namely autoregressive, masked, and diffusion, and quantify how layer contributions evolve with depth using a unified set of probing- and perturbation-based measurements. Across all models, we observe consistent depth-dependent patterns that extend prior findings on LLMs: later layers depend less on earlier computations and mainly refine the final output distribution, and these effects are increasingly pronounced in deeper models. Taken together, our results suggest that PLMs exhibit a form of depth inefficiency, motivating future work on more depth-efficient architectures and training methods.

</details>


### [68] [Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction](https://arxiv.org/abs/2602.21757)
*Xiannan Huang,Quan Yuan,Chao Yang*

Main category: cs.LG

TL;DR: 提出了一种名为FORESEE的轻量级在线适应框架，用于提高短期交通需求预测的准确性与鲁棒性，同时保持计算效率。该方法通过昨日预测误差修正今日预测，并结合时空平滑组件传播误差信号，以适应不断变化的城市动态。实验表明FORESEE能够持续提高预测准确性，即使在分布变化很小的情况下也能维持稳健性，并且相比于现有在线方法具有最低的计算开销。


<details>
  <summary>Details</summary>
Motivation: 准确预测短期交通需求对智能交通系统至关重要，但深度学习模型在面对由外部事件或城市发展引起的数据分布变化时，其准确性会显著下降。频繁地重新训练模型来适应这些变化会导致巨大的计算成本，特别是对于大规模或基础模型而言。

Method: 提出了FORESEE（Forecasting Online with Residual Smoothing and Ensemble Experts），一种无需更新基础模型参数的轻量级在线适应框架。它利用前一天的预测误差来校正当前地区的预测结果，并通过混合专家机制指导下的指数平滑技术稳定这一过程。此外，还引入了一个自适应时空平滑组件，能够在相邻地区和时间段之间传播错误信号，从而捕捉需求模式的一致性变化。

Result: 在七个真实世界数据集上进行的广泛实验表明，使用三种不同的骨干模型时，FORESEE始终能够提高预测准确性，即使是在分布变化极小的情况下也能保持稳定性（避免性能下降），并且相较于其他现有的在线方法实现了最低的计算开销。

Conclusion: 通过实现几乎零额外计算成本下的实时适应性调整，FORESEE为在动态城市环境中部署可靠且最新的预测系统铺平了道路。

Abstract: Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving urban dynamics. Frequent model retraining to adapt to such changes incurs prohibitive computational costs, especially for large-scale or foundation models. To address this challenge, we propose FORESEE (Forecasting Online with Residual Smoothing and Ensemble Experts), a lightweight online adaptation framework that is accurate, robust, and computationally efficient. FORESEE operates without any parameter updates to the base model. Instead, it corrects today's forecast in each region using yesterday's prediction error, stabilized through exponential smoothing guided by a mixture-of-experts mechanism that adapts to recent error dynamics. Moreover, an adaptive spatiotemporal smoothing component propagates error signals across neighboring regions and time slots, capturing coherent shifts in demand patterns. Extensive experiments on seven real-world datasets with three backbone models demonstrate that FORESEE consistently improves prediction accuracy, maintains robustness even when distribution shifts are minimal (avoiding performance degradation), and achieves the lowest computational overhead among existing online methods. By enabling real-time adaptation of traffic forecasting models with negligible computational cost, FORESEE paves the way for deploying reliable, up-to-date prediction systems in dynamic urban environments. Code and data are available at https://github.com/xiannanhuang/FORESEE

</details>


### [69] [Generalisation of RLHF under Reward Shift and Clipped KL Regularisation](https://arxiv.org/abs/2602.21765)
*Kenton Tang,Yuzhu Chen,Fengxiang He*

Main category: cs.LG

TL;DR: 本文开发了针对从人类反馈中强化学习（RLHF）的泛化理论，特别考虑了奖励偏移和KL剪裁正则化的影响，并提出了泛化误差来源于提示和展开的采样误差、奖励偏移误差以及KL剪裁误差。此外，还讨论了使用有限空间上均匀先验初始化RLHF参数的情况，以及通过随机梯度下降训练RLHF作为Ornstein-Uhlenbeck过程的情况。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型中的对齐与适应严重依赖于从人类反馈中进行的强化学习(RLHF)，但其泛化能力的理论理解仍不成熟，特别是在学习到的奖励可能发生变化以及KL控制被估计和剪裁的情况下。为了解决这个问题，研究者们开发了一种能够明确考虑到这些因素的RLHF泛化理论。

Method: 研究者发展了一套针对RLHF的泛化理论，该理论特别考虑到了两个方面：1. 奖励偏移：即奖励模型是在早期或混合行为策略的数据上训练的，而RLHF则基于当前策略自身的展开来优化；2. KL剪裁正则化：KL正则化器是从抽样的对数概率比值中估计出来的，然后为了稳定目的对其进行剪裁，这给RLHF带来了一定的误差。

Result: 给出了RLHF的泛化边界，表明泛化误差来自于提示和展开中的采样误差、奖励偏移误差以及KL剪裁误差。同时，也探讨了当以有限空间上的均匀先验初始化RLHF参数时，以及通过随机梯度下降法（被视为Ornstein-Uhlenbeck过程）训练RLHF时的一些特殊情况。

Conclusion: 这项工作为理解和改进RLHF提供了一个新的视角，特别是对于如何设置最佳的KL剪裁阈值以及如何在提示、展开和偏好数据之间分配预算提供了实用建议。

Abstract: Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF optimises the current policy on its own rollouts; and (2) \emph{clipped KL regularisation}: the KL regulariser is estimated from sampled log-probability ratios and then clipped for stabilisation, resulting in an error to RLHF. We present generalisation bounds for RLHF, suggesting that the generalisation error stems from a sampling error from prompts and rollouts, a reward shift error, and a KL clipping error. We also discuss special cases of (1) initialising RLHF parameters with a uniform prior over a finite space, and (2) training RLHF by stochastic gradient descent, as an Ornstein-Uhlenbeck process. The theory yields practical implications in (1) optimal KL clipping threshold, and (2) budget allocation in prompts, rollouts, and preference data.

</details>


### [70] [Easy to Learn, Yet Hard to Forget: Towards Robust Unlearning Under Bias](https://arxiv.org/abs/2602.21773)
*JuneHyoung Kwon,MiHyeon Kim,Eunju Lee,Yoonji Lee,Seunghoon Lee,YoungBin Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器遗忘框架CUPID，旨在解决由于数据中存在意外偏差导致的"捷径遗忘"问题。通过根据样本锐度将遗忘集划分为因果和偏差近似子集，并解耦模型参数为因果路径和偏差路径，最终实现针对性更新，以改善遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，机器学习模型可能会从数据中的虚假相关性学到非预期的偏差，这使得机器遗忘变得复杂。本文研究了从这种有偏模型中进行遗忘的独特挑战，并发现了一个新现象——“捷径遗忘”，即模型容易学习却难以忘记那些与偏差一致的样本。

Method: 提出了名为CUPID的新遗忘框架。该方法首先基于样本锐度将遗忘集合划分成因果-近似子集和偏差-近似子集；接着解耦模型参数为因果路径和偏差路径两部分；最后通过向各自路径路由细化后的因果梯度和偏差梯度来执行有针对性的更新。

Result: 在包括Waterbirds、BAR以及Biased NICO++在内的几个带有偏差的数据集上进行了广泛的实验，结果表明所提出的方法达到了最先进的遗忘性能，并有效地缓解了捷径遗忘问题。

Conclusion: 针对具有潜在偏差的数据集，CUPID提供了一种有效的方法来增强机器遗忘的效果，从而提高数据隐私性和模型可靠性。

Abstract: Machine unlearning, which enables a model to forget specific data, is crucial for ensuring data privacy and model reliability. However, its effectiveness can be severely undermined in real-world scenarios where models learn unintended biases from spurious correlations within the data. This paper investigates the unique challenges of unlearning from such biased models. We identify a novel phenomenon we term ``shortcut unlearning," where models exhibit an ``easy to learn, yet hard to forget" tendency. Specifically, models struggle to forget easily-learned, bias-aligned samples; instead of forgetting the class attribute, they unlearn the bias attribute, which can paradoxically improve accuracy on the class intended to be forgotten. To address this, we propose CUPID, a new unlearning framework inspired by the observation that samples with different biases exhibit distinct loss landscape sharpness. Our method first partitions the forget set into causal- and bias-approximated subsets based on sample sharpness, then disentangles model parameters into causal and bias pathways, and finally performs a targeted update by routing refined causal and bias gradients to their respective pathways. Extensive experiments on biased datasets including Waterbirds, BAR, and Biased NICO++ demonstrate that our method achieves state-of-the-art forgetting performance and effectively mitigates the shortcut unlearning problem.

</details>


### [71] [Excitation: Momentum For Experts](https://arxiv.org/abs/2602.21798)
*Sagi Shaier*

Main category: cs.LG

TL;DR: 提出了一种新的优化框架Excitation，专门用于加速稀疏架构如混合专家(MoE)模型的学习。它通过动态调整更新来提高高利用率专家的学习效率，并可以抑制低利用率的专家，从而促进路由专业化。该方法解决了深度MoE中的“结构混淆”问题，提高了模型在语言和视觉任务上的收敛速度和最终性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决稀疏架构如混合专家（MoE）模型中遇到的学习效率低下以及“结构混淆”的问题，提出了Excitation这一新型优化框架。目标是通过动态调节学习过程来增强这些模型的专业化程度与训练稳定性。

Method: Excitation通过基于批次级别的专家利用率动态地调制参数更新，对高度利用的专家给予更大的更新幅度，同时可以选择性地减少对低利用率专家的更新。这种方法不需要额外的每参数优化器状态或可学习参数，因此非常适合内存受限的情况。

Result: 实验表明，在语言和视觉任务上，Excitation能够显著加快MoE模型的收敛速度并提升最终表现，证明了主动更新调节对于有效条件计算的重要性。

Conclusion: Excitation作为一个通用性强、易于集成的解决方案，不仅解决了现有优化器在处理深层MoE时面临的挑战，还为稀疏架构下的高效学习提供了一个强有力的支持。

Abstract: We propose Excitation, a novel optimization framework designed to accelerate learning in sparse architectures such as Mixture-of-Experts (MoEs). Unlike traditional optimizers that treat all parameters uniformly, Excitation dynamically modulates updates using batch-level expert utilization. It introduces a competitive update dynamic that amplifies updates to highly-utilized experts and can selectively suppress low-utilization ones, effectively sharpening routing specialization. Notably, we identify a phenomenon of "structural confusion" in deep MoEs, where standard optimizers fail to establish functional signal paths; Excitation acts as a specialization catalyst, "rescuing" these models and enabling stable training where baselines remain trapped. Excitation is optimizer-, domain-, and model-agnostic, requires minimal integration effort, and introduces neither additional per-parameter optimizer state nor learnable parameters, making it highly viable for memory-constrained settings. Across language and vision tasks, Excitation consistently improves convergence speed and final performance in MoE models, indicating that active update modulation is a key mechanism for effective conditional computation.

</details>


### [72] [DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion](https://arxiv.org/abs/2602.21824)
*Marcel Lamott,Saifullah Saifullah,Nauman Riaz,Yves-Noel Weweler,Tobias Alt-Veit,Ahmad Sarmad Ali,Muhammad Armaghan Shakir,Adrian Kalwa,Momina Moetesum,Andreas Dengel,Sheraz Ahmed,Faisal Shafait,Ulrich Schwanecke,Adrian Ulges*

Main category: cs.LG

TL;DR: 本文提出了一种新的可控合成文档生成框架DocDjinn，该框架利用视觉-语言模型从未标记的种子样本中生成带有注释的文档。通过基于聚类的种子选择和参数化采样方法，生成视觉上合理且语义一致的合成文档，并通过扩散式手写和上下文视觉元素丰富文档内容。实验结果表明，仅使用100个真实训练样本时，该框架能够达到完整真实数据集平均87%的表现。


<details>
  <summary>Details</summary>
Motivation: 有效的文档智能模型依赖于大量标注过的训练数据。然而，获取足够的高质量数据由于数据采集劳动密集型和成本高昂而面临重大挑战。此外，利用语言模型来标注真实文档引发了对数据隐私的关注。因此，合成文档生成作为一种有前景且能保护隐私的替代方案被提出。

Method: 提出了DocDjinn框架，这是一种用于可控合成文档生成的新方法，它使用视觉-语言模型（VLMs）从无标签的种子样本中产生带有注解的文档。通过采用基于聚类的种子选择与参数化抽样技术，生成既在视觉上可信又保持语义一致性的人工文档；并且通过引入基于扩散的手写风格以及具有情境意义的视觉元素，进一步增强了文档的真实性与多样性。

Result: 评估了十一个基准测试，涵盖了关键信息抽取、问答、文档分类和文档布局分析等领域。结果显示，在仅有100个实际训练样本的情况下，本框架能够实现接近于完整现实世界数据集平均87%的表现。

Conclusion: 这是首次展示VLMs能够从无标签样本大规模生成忠实标注文档数据集的工作，这些合成的数据集可以有效地丰富或近似真实手动标注数据，适用于多种文档理解任务。

Abstract: Effective document intelligence models rely on large amounts of annotated training data. However, procuring sufficient and high-quality data poses significant challenges due to the labor-intensive and costly nature of data acquisition. Additionally, leveraging language models to annotate real documents raises concerns about data privacy. Synthetic document generation has emerged as a promising, privacy-preserving alternative. We propose DocDjinn, a novel framework for controllable synthetic document generation using Vision-Language Models (VLMs) that produces annotated documents from unlabeled seed samples. Our approach generates visually plausible and semantically consistent synthetic documents that follow the distribution of an existing source dataset through clustering-based seed selection with parametrized sampling. By enriching documents with realistic diffusion-based handwriting and contextual visual elements via semantic-visual decoupling, we generate diverse, high-quality annotated synthetic documents. We evaluate across eleven benchmarks spanning key information extraction, question answering, document classification, and document layout analysis. To our knowledge, this is the first work demonstrating that VLMs can generate faithful annotated document datasets at scale from unlabeled seeds that can effectively enrich or approximate real, manually annotated data for diverse document understanding tasks. We show that with only 100 real training samples, our framework achieves on average $87\%$ of the performance of the full real-world dataset. We publicly release our code and 140k+ synthetic document samples.

</details>


### [73] [xai-cola: A Python library for sparsifying counterfactual explanations](https://arxiv.org/abs/2602.21845)
*Lin Zhu,Lei You*

Main category: cs.LG

TL;DR: 介绍了一个开源Python库xai-cola，该库旨在减少由任意生成器产生的反事实解释中的冗余特征变化，同时保持其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数反事实解释（CE）生成器所产生的解释通常非常冗余。为了减少这些多余的特征变化并保持解释的有效性，提出了这个解决方案。

Method: 通过提供一个端到端的管道，xai-cola能够处理由pandas DataFrame格式的原始表格数据、预处理对象以及训练好的scikit-learn或PyTorch模型输入生成的反事实解释。用户可以使用内置或是外部导入的CE生成器，并且库中实现了几种稀疏化策略及可视化例程来分析和比较稀疏化的反事实。

Result: 实验证明，xai-cola能够在多个CE生成器上产生更稀疏的反事实解释，在给定设置下减少了多达50%的修改特征数量。

Conclusion: xai-cola作为一个实用工具，有效地帮助了研究人员和从业者简化复杂的反事实解释，从而使得机器学习模型更加透明易懂。

Abstract: Counterfactual explanation (CE) is an important domain within post-hoc explainability. However, the explanations generated by most CE generators are often highly redundant. This work introduces an open-source Python library xai-cola, which provides an end-to-end pipeline for sparsifying CEs produced by arbitrary generators, reducing superfluous feature changes while preserving their validity. It offers a documented API that takes as input raw tabular data in pandas DataFrame form, a preprocessing object (for standardization and encoding), and a trained scikit-learn or PyTorch model. On this basis, users can either employ the built-in or externally imported CE generators. The library also implements several sparsification policies and includes visualization routines for analysing and comparing sparsified counterfactuals. xai-cola is released under the MIT license and can be installed from PyPI. Empirical experiments indicate that xai-cola produces sparser counterfactuals across several CE generators, reducing the number of modified features by up to 50% in our setting. The source code is available at https://github.com/understanding-ml/COLA.

</details>


### [74] [Learning in the Null Space: Small Singular Values for Continual Learning](https://arxiv.org/abs/2602.21919)
*Cuong Anh Pham,Praneeth Vepakomma,Samuel Horváth*

Main category: cs.LG

TL;DR: 本文提出了一种名为NESS的持续学习方法，该方法通过直接在权重空间中应用正交性来缓解灾难性遗忘，并通过最小奇异值构建近似的零空间以实现任务特定更新。实验表明，这种方法可以在多个基准数据集上表现出竞争力，减少遗忘，并保持跨任务的稳定准确性。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的主要挑战，即在允许进一步学习的同时缓解灾难性遗忘。已有基于正交性的训练方法虽然有效且理论性质强，但多通过梯度投影来强制执行正交性。

Method: NESS（从最小奇异值估计的零空间）是一种持续学习方法，它直接在权重空间而不是通过梯度操作来应用正交性。具体来说，NESS利用每一层输入表示的最小奇异值构造一个近似的零空间，并通过限制在这个子空间内的紧凑低秩适应（LoRA风格）公式来参数化任务特定更新。子空间基固定以保持零空间约束，每个任务仅学习一个可训练矩阵。

Result: 理论分析和在三个基准数据集上的实验表明，NESS能够实现具有竞争力的表现、低遗忘以及跨任务的稳定准确性。

Conclusion: 小奇异值在持续学习中扮演重要角色。通过直接在权重空间内利用这些值来维持与先前任务输入空间近乎正交的方向，NESS提供了一种有效的持续学习解决方案。

Abstract: Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on this principle, we introduce NESS (Null-space Estimated from Small Singular values), a CL method that applies orthogonality directly in the weight space rather than through gradient manipulation. Specifically, NESS constructs an approximate null space using the smallest singular values of each layer's input representation and parameterizes task-specific updates via a compact low-rank adaptation (LoRA-style) formulation constrained to this subspace. The subspace basis is fixed to preserve the null-space constraint, and only a single trainable matrix is learned for each task. This design ensures that the resulting updates remain approximately in the null space of previous inputs while enabling adaptation to new tasks. Our theoretical analysis and experiments on three benchmark datasets demonstrate competitive performance, low forgetting, and stable accuracy across tasks, highlighting the role of small singular values in continual learning. The code is available at https://github.com/pacman-ctm/NESS.

</details>


### [75] [Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions](https://arxiv.org/abs/2602.21959)
*Dusica Marijan,Hamza Haruna Mohammed,Bakht Zaman*

Main category: cs.LG

TL;DR: 本文综述了海运中燃油消耗的估算与优化方法，提出了基于物理、机器学习及混合模型的分类，并探讨了各类方法的优势与局限性。同时强调了数据融合技术的重要性以及可解释AI在提高模型透明度方面的作用。


<details>
  <summary>Details</summary>
Motivation: 为了减少碳排放并降低航运成本，提高船舶的燃油效率至关重要。文章旨在提供一种全面的方法来评估和优化海上运输中的燃油消耗问题。

Method: 对现有的燃油消耗估计与优化方法进行了分类（基于物理的方法、机器学习方法和混合模型），讨论了每种类型方法的优点和局限；提出利用AIS、船上传感器和气象数据的数据融合技术以提高预测准确性；首次探讨了可解释AI在增强决策过程中的模型透明度方面的新兴作用。

Result: 明确了不同类型燃油消耗估算方法的特点；强调了高质量数据对于提高预测精度的重要性；指出了当前研究中存在的主要挑战，如数据质量、可用性和实时优化的需求等。

Conclusion: 未来的研究方向应集中在混合模型的发展、实现实时优化以及数据集标准化上，以解决现有研究中的不足之处。

Abstract: To reduce carbon emissions and minimize shipping costs, improving the fuel efficiency of ships is crucial. Various measures are taken to reduce the total fuel consumption of ships, including optimizing vessel parameters and selecting routes with the lowest fuel consumption. Different estimation methods are proposed for predicting fuel consumption, while various optimization methods are proposed to minimize fuel oil consumption. This paper provides a comprehensive review of methods for estimating and optimizing fuel oil consumption in maritime transport. Our novel contributions include categorizing fuel oil consumption \& estimation methods into physics-based, machine-learning, and hybrid models, exploring their strengths and limitations. Furthermore, we highlight the importance of data fusion techniques, which combine AIS, onboard sensors, and meteorological data to enhance accuracy. We make the first attempt to discuss the emerging role of Explainable AI in enhancing model transparency for decision-making. Uniquely, key challenges, including data quality, availability, and the need for real-time optimization, are identified, and future research directions are proposed to address these gaps, with a focus on hybrid models, real-time optimization, and the standardization of datasets.

</details>


### [76] [Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data](https://arxiv.org/abs/2602.22018)
*Sterre de Jonge,Elisabeth J. Vinke,Meike W. Vernooij,Daniel C. Alexander,Alexandra L. Young,Esther E. Bron*

Main category: cs.LG

TL;DR: 本文提出了一种新的疾病进展模型——混合事件模型，该模型能够处理离散和连续两种类型的数据，并在模拟实验和真实世界数据上展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大多数疾病进展模型仅适用于单一类型的数据（如连续数据），这限制了它们对异质性的、现实世界数据集的应用。为了解决这一局限性，研究者提出了一个可以同时处理离散和连续数据类型的新型疾病进展模型。

Method: 研究人员开发了一个名为混合事件模型的新疾病进展模型，并将其整合到子类型与阶段推断（SuStaIn）框架中，形成了Mixed-SuStaIn方法。这样既支持子类型也支持进展建模。

Result: 通过模拟实验以及来自阿尔茨海默病神经影像学倡议的真实数据证明，Mixed-SuStaIn在处理混合数据集时表现良好。

Conclusion: Mixed-SuStaIn提供了一种强大的手段来分析包含不同类型数据的复杂生物医学数据集，特别是对于像阿尔茨海默病这样具有长期发展轨迹的疾病来说非常有用。

Abstract: Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness of Mixed-SuStaIn through simulation experiments and real-world data from the Alzheimer's Disease Neuroimaging Initiative, showing that it performs well on mixed datasets. The code is available at: https://github.com/ucl-pond/pySuStaIn.

</details>


### [77] [On Imbalanced Regression with Hoeffding Trees](https://arxiv.org/abs/2602.22101)
*Pantia-Marina Alchirch,Dimitrios I. Diochnos*

Main category: cs.LG

TL;DR: 本文探讨了核密度估计(KDE)和层次收缩(HS)方法在流式环境中的应用，特别是在不平衡回归任务中。研究表明KDE在数据流的早期阶段是有益的，而HS很少提供性能优势。


<details>
  <summary>Details</summary>
Motivation: 针对现实世界中连续数据流的应用，研究者们希望找到更有效的机器学习模型来解决感兴趣的回归任务。已有工作表明，核密度估计（KDE）在批量学习中的不平衡回归任务中表现良好，而层次收缩（HS）则为决策树提供了不改变其结构的后处理正则化方法。基于这些背景，本文旨在将这两种方法扩展到流式环境中，并评估它们对于在线设置下常用的回归数据集的影响。

Method: 通过使用一种伸缩论证的方法，作者们成功地把KDE应用于流式环境中，并且将HS实施扩展到了增量决策树模型上。然后，他们利用这些改进来考察在在线环境下常用于回归的数据集中，具备这些选项的决策树的表现情况。

Result: 实验结果表明，在数据流的早期部分，KDE对决策树有积极影响；相比之下，HS几乎不提供任何性能上的好处。

Conclusion: 本研究表明，当面对流式数据时，采用KDE可以在初期提升模型性能，但HS技术并未显示出显著的优势。

Abstract: Many real-world applications provide a continuous stream of data that is subsequently used by machine learning models to solve regression tasks of interest. Hoeffding trees and their variants have a long-standing tradition due to their effectiveness, either alone or as base models in broader ensembles. At the same time a recent line of work in batch learning has shown that kernel density estimation (KDE) is an effective approach for smoothed predictions in imbalanced regression tasks [Yang et al., 2021]. Moreover, another recent line of work for batch learning, called hierarchical shrinkage (HS) [Agarwal et al., 2022], has introduced a post-hoc regularization method for decision trees that does not alter the structure of the learned tree. Using a telescoping argument we cast KDE to streaming environments and extend the implementation of HS to incremental decision tree models. Armed with these extensions we investigate the performance of decision trees that may enjoy such options in datasets commonly used for regression in online settings. We conclude that KDE is beneficial in the early parts of the stream, while HS hardly, if ever, offers performance benefits. Our code is publicly available at: https://github.com/marinaAlchirch/DSFA_2026.

</details>


### [78] [Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination](https://arxiv.org/abs/2602.22130)
*Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Sihan Liu*

Main category: cs.LG

TL;DR: 本文解决了在均值偏移污染模型下，对于一般基础分布的均值估计问题所需的样本复杂度，并提出了傅里叶见证的概念作为上下界证明的关键技术。


<details>
  <summary>Details</summary>
Motivation: 先前的工作已经确定了高斯和拉普拉斯分布下的均值估计任务的样本复杂度，但对于一般的基础分布，这一问题仍然开放。本文旨在解决这一开放性问题，即确定在均值偏移污染模型中对一般基础分布进行均值估计所需的样本复杂度。

Method: 通过引入傅里叶分析，特别是提出“傅里叶见证”的概念，为上界和下界的证明提供了关键的技术支持。研究者们开发了一种样本效率高的算法，该算法能够基于基础分布特征函数满足温和谱条件的前提下，以任意所需精度估计目标均值。

Result: 研究表明，在基础分布的特征函数满足某些温和谱条件下，存在一种样本高效的算法可以将目标均值估计到任意期望的精度。此外，还给出了一个与之定性匹配的样本复杂度下界。

Conclusion: 本研究基本解决了在均值偏移污染模型下对一般基础分布执行均值估计任务时面临的样本复杂度问题，表明了在一定条件下一致估计是可能实现的。同时，引入的新方法——傅里叶见证——为处理此类问题提供了一个强有力的技术手段。

Abstract: We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber's contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. We complement our upper bound with a qualitatively matching sample complexity lower bound. Our techniques make critical use of Fourier analysis, and in particular introduce the notion of a Fourier witness as an essential ingredient of our upper and lower bounds.

</details>


### [79] [Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual](https://arxiv.org/abs/2602.22146)
*Yining Li,Peizhong Ju,Ness Shroff*

Main category: cs.LG

TL;DR: 本文提出了一种用于安全RLHF（基于人类反馈的强化学习）的通用原始对偶框架，该框架统一了包括安全-RLHF、一次性以及多次基于的方法在内的一大类现有对齐算法。基于此框架，作者引入了一种乐观原始对偶算法(OPD)，通过预测更新来稳定鞍点动力学，并为所提方法建立了最后迭代收敛保证，揭示了乐观主义在缓解约束对齐目标固有振荡方面的重要作用。


<details>
  <summary>Details</summary>
Motivation: 虽然可以从人类反馈中学习到强化学习(RLHF)对于使大型语言模型(LLMs)与人类偏好保持一致起着重要作用，但现有的标准原始对偶方法只能保证在分布策略下收敛，而在实际应用中的策略参数化可能导致不稳定或发散。因此，需要一种新的方法来解决这些问题并提高RLHF的安全性和稳定性。

Method: 本文提出了一个通用的原始对偶框架，旨在解决安全RLHF问题，并且能够统一一系列现有的对齐算法。此外，还介绍了一种乐观原始对偶算法(OPD)，该算法通过对原始变量和对偶变量进行预测性更新以稳定鞍点动态。

Result: 研究表明，提出的乐观原始对偶算法不仅能够在分布空间中精确优化策略时实现收敛，而且当使用参数化策略时也能够收敛至最优解附近的一个邻域内，其中差距与近似误差及偏差有关。

Conclusion: 研究结果表明，乐观主义在减少受限对齐目标带来的内在波动方面发挥着关键作用，从而填补了约束RL与实际RLHF之间重要的理论空白。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.

</details>


### [80] [Learning and Naming Subgroups with Exceptional Survival Characteristics](https://arxiv.org/abs/2602.22179)
*Mhd Jawad Al Rahwanji,Sascha Xu,Nils Philipp Walter,Jilles Vreeken*

Main category: cs.LG

TL;DR: 本文提出了一种全新的完全可微且非参数的方法Sysurv，该方法利用随机生存森林学习个体生存曲线，并自动学习条件及如何将这些条件组合成内在可解释的规则，从而挑选出生存特征异常的子群体。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，识别出比其他群体生存时间更长或更短的子群体非常重要。然而，现有的发现具有特殊生存特征子群体的方法需要对生存模型做出严格的假设（例如比例风险），并且需要预离散化的特征。此外，由于它们比较的是平均统计值，因此往往会忽略个体差异。

Method: 作者提出了Sysurv，这是一种完全可微、非参数化的方法，它利用随机生存森林来学习个体的生存曲线，并能自动学习条件以及如何把这些条件结合成内在可解释的规则，以选择具有特殊生存特性的子群体。

Result: 通过对广泛的数据集和设置进行实证评估，包括一项关于癌症数据的案例研究，表明Sysurv能够揭示有见地且可操作的生存子群体。

Conclusion: Sysurv作为一种新颖的方法，在无需对生存模型做严格假设的情况下，能够有效识别出具有显著生存特征的子群体，为医学和预测性维护等领域提供了新的视角。

Abstract: In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.

</details>
