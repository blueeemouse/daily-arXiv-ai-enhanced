<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 124]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 20]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: 提出了FedAdaVR，一种新的联邦学习算法，通过结合自适应优化器和方差减少技术来解决由零星客户端参与引起的异质性问题。此外，还提出了FedAdaVR-Quant，以量化形式存储客户端更新，显著减少了内存需求，同时保持了等效的模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习由于异质性面临着重大挑战，包括梯度噪声、客户端漂移和部分客户端参与错误，其中后者最为普遍但现有文献中处理不足。

Method: FedAdaVR利用最近存储的客户端更新，即使它们没有参与当前训练轮次，也能够模拟它们的存在。FedAdaVR-Quant则以量化形式存储客户端更新，大大降低了内存需求。

Result: 在多种数据集上进行的广泛实验表明，无论是在独立同分布(IID)还是非独立同分布(non-IID)设置下，FedAdaVR的表现始终优于最先进的基线方法。

Conclusion: FedAdaVR及其量化版本FedAdaVR-Quant为解决联邦学习中的异质性问题提供了有效方案，并且在不同条件下均表现出色。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [2] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: 提出了一种名为ZK-HybridFL的安全去中心化联邦学习框架，它结合了有向无环图账本、专用侧链和零知识证明技术来实现隐私保护下的模型验证。实验表明，该框架在图像分类和语言建模任务上比其他方法收敛更快、准确率更高、延迟更低，并且能够有效抵抗恶意节点攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然可以实现在保护数据隐私的同时进行协作式模型训练，但无论是集中式还是分散式的方案都面临着可扩展性、安全性和更新验证方面的挑战。为了解决这些问题，提出了ZK-HybridFL框架。

Method: ZK-HybridFL框架采用了有向无环图（DAG）账本与专门的侧链相结合的方法，并利用零知识证明(ZKP)来进行隐私保护型模型验证。此外，通过事件驱动的智能合约及预言机辅助侧链来验证本地模型更新，同时内置了一个挑战机制以高效检测对抗行为。

Result: 在图像分类和语言建模任务上的实验显示，ZK-HybridFL相比Blade-FL和ChainFL具有更快的收敛速度、更高的准确性、更低的困惑度以及减少的延迟。并且即使存在大量对抗性和闲置节点的情况下也能保持稳健表现，支持亚秒级链上验证并有效使用gas，防止无效更新和孤儿式攻击。

Conclusion: ZK-HybridFL是一个可扩展且安全的解决方案，适用于各种环境下的去中心化联邦学习，它不仅提高了性能指标，还增强了系统的鲁棒性和安全性。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [3] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: 提出了SAIR，一种使用大语言模型作为上下文强化学习控制器的自动扩展框架，以解决多阶段机器学习推理管道的自动扩展难题。该方法不需要离线训练，并在不同工作负载模式下显著提高了P99延迟性能并降低了有效资源成本。


<details>
  <summary>Details</summary>
Motivation: 多阶段机器学习推理管道难以实现自动扩展，因为它们面临异构资源、跨阶段耦合以及动态瓶颈迁移等问题。

Method: 开发了SAIR框架，它利用大型语言模型（LLM）作为基于上下文的强化学习控制器，通过奖励标记的交互历史在线改进其策略，无需梯度更新。此外，SAIR结合了帕累托优势奖励塑造、惊喜引导的经验检索等功能来提高效率，并通过用户空间CUDA拦截实现细粒度GPU速率控制。

Result: 在四种ML服务管道和三种工作负载模式下测试表明，与部署的基线相比，SAIR达到了最佳或并列最佳的P99延迟和有效资源成本表现，最多可将P99改善50%，同时减少高达97%的有效成本（基于GPU速率控制假设），并且具有86%的瓶颈检测准确率，且无需离线训练。

Conclusion: SAIR提供了一种创新的方法来应对多阶段ML推理管道中的自动扩展挑战，通过结合先进的技术和策略，能够显著提高性能并降低成本。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [4] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 研究发现，在小规模数据集上进行多模态情绪识别时，复杂的注意力机制表现不佳。简单的领域特定修改，如在音频CNN中添加delta MFCCs以及使用EEG的频域特征等方法，反而能够有效提升性能。该研究表明，对于小规模的情绪识别任务，领域知识和适当的实现比架构复杂性更为重要。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索复杂注意力机制是否能提高小数据集上的多模态情绪识别性能，并通过实验对比了不同类型模型的表现。

Method: 本研究实现了三种类型的模型：基础变换器（M1）、新颖的因子化注意力机制（M2）及改进的CNN基线（M3），并针对EAV数据集进行了系统性研究。

Result: 结果显示，M2模型由于过拟合和破坏预训练特征而低于基线5到13个百分点；相反地，对领域适合性的简单调整非常有效，比如给音频CNN增加delta MFCCs提高了准确率至65.56%，EEG采用频域特性达到67.62%。视觉变换器基线(M1)达到了75.30%，超越了论文中的ViViT结果(74.5%)。

Conclusion: 结论指出，在处理小规模情绪识别任务时，基于领域知识的有效实施比追求架构上的复杂性更有利于性能提升。

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [5] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种结合多任务学习和量子卷积操作的混合模型，用于地球观测数据分类，并通过多个基准测试验证了模型的有效性，探讨了其在量子机器学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测进入大数据时代，利用复杂深度学习模型有效分析大量地球观测数据的计算需求已经成为瓶颈。受此启发，作者旨在利用量子计算来处理地球观测数据分类问题，并探索其优势。

Method: 提出了一种新的混合模型，该模型采用多任务学习以促进高效的数据编码，并引入了一个带有量子卷积操作的位置权重模块来提取有效的特征进行分类。

Result: 通过多个地球观测基准测试评估了所提模型的有效性，并实验性地探索了模型的泛化能力及其优势因素。

Conclusion: 研究表明，尽管当前量子设备存在局限性，但所提出的基于量子计算的混合模型在地球观测数据分析中展现出潜在的优势。

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [6] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 本文开发了一种名为CELM的临床脑电图到语言的基础模型，能够总结长时间可变长度的脑电图记录，并在多个尺度上生成端到端的临床报告。实验结果显示，该方法在标准生成指标上有了显著提升，并且在零样本设置下也表现良好。


<details>
  <summary>Details</summary>
Motivation: 生成从长期脑电图记录中总结异常模式、诊断发现和临床解释的临床报告仍然是一项劳动密集型任务。

Method: 作者整理了一个大规模的临床脑电图数据集，包含9,922份报告与大约11,000小时来自9,048名患者的脑电图记录。基于此，他们开发了CELM，这是一个能够处理长时间可变长度脑电图记录并进行多尺度临床报告自动生成的模型。

Result: 实验结果表明，在患者病史监督的情况下，该方法在标准生成度量（如ROUGE-1和METEOR）上实现了70%至95%的平均相对改进，分数从0.2-0.3提高到了0.4-0.6。而在没有患者病史信息的零样本设置下，CELM的生成得分范围为0.43-0.52，相比之下基线得分仅为0.17-0.26。

Conclusion: CELM通过整合预训练的脑电图基础模型与语言模型实现了可扩展的多模态学习，并在临床报告生成方面取得了显著进步。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [7] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为FunPRM的新方法，旨在通过鼓励模块化代码生成并引入基于元学习的奖励修正机制来改进大型语言模型在代码生成任务中的表现。实验结果表明，FunPRM不仅在多个基准测试中优于现有方法，还提高了代码的可读性和复用性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在数学推理等领域取得了成功，但在处理复杂的编程任务时仍经常失败。特别是，当前的过程奖励模型（PRMs）因缺乏有意义的步骤分解及蒙特卡洛估计部分解决方案正确性分数（奖励）的噪音问题而难以有效应用于代码生成。为解决这些问题，研究者们开发了FunPRM。

Method: FunPRM通过提示LLMs以函数形式组织代码生成，将每个函数视为PRM的一个推理步骤，以此促进代码的模块化。此外，它采用一种新颖的基于元学习的奖励修正机制，该机制利用单元测试评估系统获取的最终解决方案奖励来净化噪声较大的部分解决方案奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验显示，当与不同基础LLMs结合使用时，FunPRM持续优于现有的测试时间缩放方法，并且在与O4-mini结合时，在LiveCodeBench上达到了最先进的性能。更重要的是，FunPRM生成的代码对于开发者来说更加易于阅读和重用。

Conclusion: FunPRM提供了一种有效的手段来提高大型语言模型在复杂编程任务中的性能，同时增强了生成代码的质量。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [8] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: 本文提出了一种基于推理的大型语言模型评判者DAJ，通过在双层数据重加权学习框架下使用可验证奖励进行训练。该方法自动强调难题、分布内样本和轨迹对齐的数据，无需依赖手工制作的启发式方法，在LiveCodeBench和BigCodeBench上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 测试时代码生成通常依赖于从基础模型中采样多个候选解决方案，并由一个大型语言模型（LLM）裁判选择最佳方案。然而，由于严重的分布偏移问题（包括简单与复杂问题之间的不平衡、训练任务与评估基准之间的不匹配以及由较便宜模型生成的训练数据行为与实际推理时模型行为不同导致的轨迹不匹配），训练可靠的LLM裁判变得非常具有挑战性。

Method: 提出了DAJ，这是一种基于推理的LLM裁判员，它在一个双级数据重加权学习框架下利用可验证奖励进行训练。该框架学习数据重要性权重（无论是域级别还是实例级别），以优化与目标基准对齐的保留元集上的泛化性能。

Result: DAJ在LiveCodeBench和BigCodeBench上达到了最先进的表现，超越了强大的测试时间扩展基线以及领先的专有模型。

Conclusion: 通过采用数据重加权策略来训练作为裁判使用的大型语言模型，能够有效应对因数据分布偏移带来的挑战，并且在代码生成任务中的表现优于现有方法。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [9] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 本文提出了一种简单的对称性破坏协议，通过批量采样、未学习的查询和价值偏置来消除注意力机制中的冗余旋转自由度。这种修改可以显著提高简单且内存效率高的优化器的性能，并使得原本冗余的旋转自由度能够被用来放大具有语义意义的令牌类别，从而同时提升了模型性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准实现下的注意力机制包含了一些额外的旋转自由度，这些自由度虽然参与了计算但并不影响模型激活或输出。研究旨在通过引入一种简单的对称性打破协议来利用这些自由度，以改善优化算法的表现并增强模型的可解释性。

Method: 通过引入批量采样、非学习的查询和价值偏置，向旋转空间中插入一个优先方向，从而打破原有的对称性。接着，使用四种不同的优化算法（AdamW、SOAP、SGDM 和 Energy Conserving Descent (ECD)）预训练1.24亿参数的Transformer模型，并评估验证损失及下游逻辑推理任务表现。

Result: 实验结果表明，所提出的修改不仅能大幅度提升简单且内存效率高的优化器的性能，缩小甚至在某些情况下关闭了与更复杂内存密集型自适应方法之间的差距；而且还能使原本冗余的旋转自由度变得可解释，选择性地放大个别注意力头内具有语义意义的令牌类别。

Conclusion: 通过实施最小而有原则性的架构变更，可以在提高性能的同时增强模型的可解释性。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [10] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于分类的框架，将生存分析重新表述为一系列二分类问题，使现有的表格基础模型能够通过上下文学习执行生存分析而无需显式训练。实验表明，该方法在多个真实数据集上优于经典和深度学习基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管表格基础模型在分类和回归任务中取得了显著的成功，但它们在处理生存分析中的时间到事件结果时面临着因右删失带来的非平凡挑战。本文旨在开发一种新方法，以适应并利用这些模型来解决生存分析问题。

Method: 本文发展了一种基于分类的方法，通过对事件发生时间进行离散化处理，将静态与动态生存分析问题转化为一系列二分类问题。这种转换使得现有表格基础模型可以通过上下文学习直接应用于生存分析领域，而不需要经过专门的训练过程。

Result: 研究表明，在53个真实世界数据集中，采用所提分类框架的现成表格基础模型在多种生存指标上的表现优于传统的及深度学习基准模型。此外，理论证明了在标准删失假设下，最小化文中提出的二分类损失函数可以随着训练集大小增加而恢复真实的生存概率。

Conclusion: 本研究提供了一种有效的方法，让现有的表格基础模型能够被用于生存分析，不仅解决了右删失数据带来的挑战，还在多个实际数据集上展示了其优越性。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [11] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出了一种基于可穿戴惯性传感器和机器学习的低成本自动人体活动识别框架，以支持远程医疗、老年人辅助等。实验结果表明所提出的支撑张量机（STM）分类器在准确率上优于其他经典分类器。


<details>
  <summary>Details</summary>
Motivation: 为了解决因医疗基础设施不足导致的居家护理问题，特别是对于老年人和弱势群体来说，他们往往面临忽视以及无法很好地遵循如瑜伽或物理治疗等治疗性锻炼的问题。

Method: 该研究使用了加速度计和陀螺仪测量来收集包括行走、上楼、下楼、坐、站、躺等活动的数据，并评估比较了四种经典分类器：逻辑回归、随机森林、支持向量机(SVM) 和 k-最近邻(k-NN)，同时提出了支撑张量机 (STM) 作为对比。

Result: 实验结果显示SVM达到了93.33%的准确率，而逻辑回归、随机森林和k-NN分别达到了91.11%。相比之下，STM显著优于这些模型，测试准确率达到96.67%，交叉验证准确率达到最高98.50%。

Conclusion: 与传统方法相比，STM利用张量表示法保持时空运动动态特性，从而对多样化的活动提供稳健的分类。该框架为远程医疗保健、老年人协助、儿童活动监控、瑜伽反馈及智能家居健康等领域提供了强大的潜力，尤其是在资源有限和农村医疗环境中展现出了可扩展解决方案的优势。

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [12] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 提出了一种名为SurrogateSHAP的新框架，该框架无需重新训练模型即可近似计算Shapley值，从而有效解决在评估数据贡献者价值时遇到的计算瓶颈问题。通过使用预训练模型进行推断，并结合梯度提升树来逼近效用函数，这种方法不仅提高了效率，还在多个任务中优于先前的方法，同时显著减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像（T2I）扩散模型在现实世界创意流程中的应用日益增多，为数据提供者建立一个公平补偿和可持续发展的市场原则框架变得至关重要。尽管Shapley值提供了一种理论基础良好的归因方法，但它面临着两重计算瓶颈：一是对每个采样子集进行全面模型再训练的成本过高；二是由于贡献者之间的相互作用需要估计边际贡献的组合数量巨大。

Method: 本文提出了SurrogateSHAP，一种无需重新训练模型即可近似计算昂贵重训练游戏价值的框架。通过从预训练模型中进行推断来实现这一目标。为了进一步提高效率，采用梯度提升树来逼近效用函数，并从基于树的模型中解析地导出Shapley值。

Result: SurrogateSHAP在三个不同的归因任务上进行了评估：(i) CIFAR-20上的DDPM-CFG图像质量，(ii) 后印象派艺术作品上Stable Diffusion的美学评价，以及(iii) 时尚产品数据上的FLUX.1产品多样性。结果表明，在所有设置下，SurrogateSHAP不仅在性能上超过了之前的方法，而且大幅降低了计算开销，能够一致地识别出根据多种效用指标具有影响力的贡献者。此外，还展示了SurrogateSHAP能有效地定位导致临床图像中虚假关联的数据源，为审核关键安全生成模型提供了一个可扩展路径。

Conclusion: SurrogateSHAP作为一种创新性解决方案，解决了在利用Shapley值对数据贡献者进行公平评价时遇到的主要挑战，包括高昂的计算成本与复杂的交互效应。通过引入预训练模型推理及梯度提升树技术，该方法不仅提高了效率，也保证了准确性。实验结果显示，SurrogateSHAP能够在保持高效的同时准确识别关键贡献者，并且对于确保生成模型的安全性和可靠性具有重要意义。

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [13] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: 本文提出了黎曼李雅普诺夫优化器（RLOs），这是一种基于控制理论框架的新型优化算法家族，它将经典优化器统一在一个几何框架内。通过构造严格的李雅普诺夫函数来证明收敛性，并提供了一种系统化的方法来设计稳定有效的优化器。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过引入一种新的基于控制理论的框架来重新解释优化问题，从而将现有的经典优化器统一到一个几何框架下。这种新方法不仅能够恢复经典的算法，还能为设计新型优化器提供原则性的指导。

Method: RLOs是基于一个新的控制理论框架，该框架将优化视为黎曼参数流形上的扩展状态离散时间受控动力系统。核心在于识别出通常吸引不变流形（NAIM），这将训练动态分为两个阶段：快速对准速度状态至目标图，然后在其中进行受控演化。通过构建严格的李雅普诺夫函数来认证向目标流形收敛的过程。

Result: 研究表明，通过几何诊断验证了理论的有效性，并展示了基于控制理论设计的优化器在大规模基准测试中达到了最先进的性能表现。

Conclusion: 总体而言，RLOs连接了控制理论与现代机器学习优化领域，提供了一种统一的语言和系统化的工具包，用于设计既稳定又高效的优化器。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [14] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 本文研究了模型合并的成功因素，发现合并方法和任务伙伴对合并性能有根本性影响，并通过线性优化揭示了与合并后表现相关的属性。子空间重叠和梯度对齐是跨不同方法的基础先决条件。


<details>
  <summary>Details</summary>
Motivation: 当前对于模型合并成功因素的理解不足，且近期工作将可合并性视为一种内在属性。本文旨在通过一个架构无关的框架来探索影响模型合并性能的因素。

Method: 采用了一种基于线性优化的方法，通过对一系列可解释的成对指标（如梯度L2距离）进行分析，以识别出与合并后性能相关联的属性。

Result: 发现了不同合并方法之间成功驱动因素存在显著差异（46.7%的指标重叠；55.3%的方向一致性），但子空间重叠和梯度对齐作为基础且不受方法限制的兼容性前提条件始终重要。

Conclusion: 本研究为理解模型可合并性提供了诊断基础，并提倡未来开发明确促进这些特性的微调策略。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [15] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为CP4Gen的新方法，该方法针对条件生成模型设计，通过基于聚类的密度估计来构建预测集，这些预测集对异常值不那么敏感、更易解释且结构复杂度更低。实验表明，CP4Gen在预测集体积和结构简洁性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 条件生成模型能够生成复杂的高维分布样本，但缺乏校准的不确定性影响了其在高风险应用中的可信度。为了解决这一问题，提高模型输出的可靠性，特别是当需要严格且可解释的预测集时。

Method: 提出了一种系统化的适应性预测方法（CP4Gen），专为条件生成模型设计。该方法利用模型生成样本上的密度估计，并采用基于聚类的密度估计技术来构建预测集。

Result: 在合成数据集及实际应用案例中进行了广泛的测试，包括气候模拟任务等。结果表明，与现有方法相比，CP4Gen在预测集大小和结构简化方面表现出了更好的性能。

Conclusion: 本研究提供了一种强大的工具用于条件生成模型相关的不确定性估计，特别是在那些要求高度可靠性和可解释性的场景下。

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [16] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 本文提出了一种新的自动工作流生成方法——贝叶斯工作流生成（BWG），通过将工作流生成视为后验分布上的贝叶斯推理，并引入了BayesFlow算法。这种方法在六个基准数据集上比现有最先进方法提高了最多9个百分点的准确性，相比于零样本提示提高了65个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有的自动工作流生成方法大多将其视为一个优化问题，缺乏坚实的理论基础。为了解决这一局限性，作者提出了贝叶斯工作流生成框架（BWG），旨在提供一种基于贝叶斯推理的工作流自动生成方式，以实现更高效和准确的任务执行流程设计。

Method: 本文介绍了一种名为贝叶斯工作流生成(BWG)的新方法，它通过使用并行前瞻rollouts进行重要性加权以及顺序循环改进器来逐步构建工作流。此外，还证明了没有改进器的情况下，加权经验分布收敛于目标后验分布。基于此，作者开发了一个无需训练即可直接使用的算法BayesFlow用于工作流构造。

Result: 实验结果表明，在六个基准数据集上，与当前最先进的工作流生成基线相比，BayesFlow能够提高高达9个百分点的准确性；与零样本提示相比，则可提高至多65个百分点。

Conclusion: 本研究提出的贝叶斯工作流生成方法（BWG）及其具体实现BayesFlow算法，为自动工作流生成领域提供了一种更加原理化且高效的替代方案，显著优于传统搜索式工作流设计方案。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [17] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 本文提出了一种新的最优隐秘攻击公式，考虑了不同层次的可检测性，并绕过了最先进的防御。同时，还介绍了一种基于目标归一化的新方法来评估有效性和可检测性之间的不同权衡，并开发了一种新的防御（BayesClean）来对抗隐秘攻击。


<details>
  <summary>Details</summary>
Motivation: 回归模型在工业过程、工程以及自然和物理科学中被广泛使用，但其对投毒攻击的鲁棒性却较少受到关注。现有研究往往假设不切实际的威胁模型，因此在实践中作用有限。

Method: 提出了一个新颖的最佳隐秘攻击公式，考虑到不同的可检测程度；引入了一种基于目标标准化的新方法论以评估有效性与可检测性间的权衡；发展了一个名为BayesClean的新防御机制来抵御隐秘攻击。

Result: 新提出的攻击形式能够避开当前最先进的防护措施；通过新方法论可以更好地理解攻击效果与隐蔽性之间的平衡；BayesClean在面对较为隐蔽且数量较多的恶意数据点时比之前的防御方案表现更好。

Conclusion: 本研究为提高回归模型针对隐秘投毒攻击的鲁棒性提供了新的见解和技术手段。

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [18] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: 本文提出了SCALAR，一个用于评估材料基础模型在几何尺度泛化、结构幻觉、一致性和推理方面表现的基准。通过这个基准，研究者们可以更好地理解大型语言模型在物理结构分布变化下的行为。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地应用于材料科学推理，它们在面对物理结构分布变化时的行为仍不为人所充分理解。因此，有必要开发一种新的基准来专门测试这些模型在处理材料科学相关任务时的能力。

Method: 本研究引入了名为SCALAR的新基准，该基准旨在评估材料基础模型在不同几何尺度上的泛化能力及其与结构幻觉、一致性及推理之间的联系。它定义了三个任务：从CIF到属性预测；基于物理原理的思考链变体；以及给定目标属性后识别晶体的逆向检索。

Result: 实验表明，不同的基础模型在显式推理下表现出显著且依赖于模型的变化，通常能够减少幻觉和误差，但同时也可能破坏一致性或有效性。这说明仅凭准确性无法推断出几何尺度上的泛化能力。

Conclusion: SCALAR基准为研究者提供了一个强有力的工具，用以探索并改进材料科学领域内大型语言模型的性能，特别是关于如何提高其在处理复杂物理结构时的一致性和可靠性。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [19] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 本文揭示了大型语言模型在更新后可能会展现出未对齐的行为，即使它们通过了静态黑盒评估。研究者们发现，过参数化导致初始模型的对齐状态无法保证其更新后的对齐性，并且静态黑盒探测不能区分真正具有更新后鲁棒性的模型和那些隐藏了大量可通过良性梯度更新激活的对抗行为的模型。实验结果表明存在这样的LLM，它们能够通过所有标准的黑盒对齐测试，但在单次良性更新之后变得严重不对齐。此外，这种隐藏潜在对抗行为的能力随着模型规模的增加而增加。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型（LLMs）的对齐研究通常假设如果一个模型在静态黑盒评估中被认为是‘对齐’的，那么它就应该是安全可靠的。然而，实际情况是，这些模型经过微调或更新后可能会表现出不期望的行为，比如忘记安全特性或者重新浮现本应被遗忘的知识。因此，这项工作的动机在于探讨并正式化模型在静态及更新后设置下的对齐问题，并揭露传统黑盒评估方法的根本局限。

Method: 作者们首先从理论上证明了由于过度参数化的问题，即便是在静态情况下被认为是对齐的模型也无法确保在任何数据集上进行更新后仍保持对齐状态。接着，通过实验证明了存在这样一些大型语言模型：尽管它们可以通过所有的标准黑盒对齐测试，但在经历了一次看似无害的更新之后却变得非常不对齐。最后，展示了随着模型规模的增长，隐藏这种潜在对抗行为的能力也随之增强。

Result: 研究表明，现有基于静态评估的方法不足以保证模型在更新后的表现；更重要的是，即使模型最初看起来是安全的，也可能在一次简单的更新后展现出严重的不对齐行为。特别是，随着模型大小的增加，这种风险似乎也在上升。

Conclusion: 该研究强调了现有的静态评估协议不足以保证模型更新后的安全性与可靠性，并呼吁开发新的、更强大的评估方法来检测模型在更新后的对齐情况。

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [20] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: 本文提出了一种新的贝叶斯优化算法PA-GP-UCB，该算法利用高成本的真实反馈和低成本的预测反馈以及离线数据来提高样本效率。实验证明该方法比传统的GP-UCB等方法收敛更快。


<details>
  <summary>Details</summary>
Motivation: 在很多现实世界中的优化问题中，获取真实值（例如人类评价、物理实验）的成本很高，而通过机器学习模型或模拟得到的预测值则成本较低。同时，通常有大量的历史数据可以用来预训练预测模型，并提供有用的信息先验。基于此背景，研究旨在开发一种能够有效利用这两类信息源及离线数据以提升对于真实值查询样本效率的新算法。

Method: 提出了名为PA-GP-UCB的新贝叶斯优化算法，它结合了来自真实世界的昂贵反馈与低成本预测反馈。该算法还利用联合高斯过程后验派生出控制变量估计器来校正预测偏差并减少不确定性。

Result: 理论分析表明，PA-GP-UCB保持了GP-UCB的标准遗憾率，但其领先常数严格较小且受预测质量和离线数据覆盖度直接影响。实验结果方面，在合成基准测试和基于人类行为数据的实际假设评估任务上，PA-GP-UCB相比普通GP-UCB和其他简单预测增强基线方法展现出更快的收敛速度。

Conclusion: PA-GP-UCB被证明是一种通用且样本高效的框架，适用于需要昂贵反馈来进行假设生成的任务情境。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [21] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 本文提出了一种用于大型语言模型路由的联邦框架，允许客户端从本地离线查询-模型评估数据中学习共享路由策略。该框架支持参数化多层感知机路由器和非参数化K-means路由器，并在两个基准测试上展示了联邦协作相较于客户端本地路由器提高了准确性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被边缘和企业客户作为远程托管服务访问，由于模型在能力和价格上的差异很大，因此将查询路由到平衡质量和推理成本的模型至关重要。现有的路由方法假设可以访问集中式查询-模型评估数据，但这些数据通常是分散在各个客户端并且涉及隐私敏感信息，使得数据集中变得不可行。此外，每个客户端单独训练路由器效果不佳，因为本地评估数据有限且仅覆盖了受限的查询分布和有偏见的模型评估子集。

Method: 作者引入了首个针对大型语言模型路由的联邦框架，使客户端能够利用本地离线查询-模型评估数据来学习共享路由策略。此框架兼容异构客户端查询分布及非均匀模型覆盖情况下的参数化多层感知机路由器与非参数化K-means路由器。

Result: 通过两个基准测试表明，联邦合作方式相比起单个客户端独立运行的路由方案，在有效模型覆盖率以及查询泛化能力方面均有所提升。理论分析也证实了联邦训练减少了路由次优性。

Conclusion: 本研究提出的联邦框架为解决大型语言模型路由问题提供了一个新的解决方案，能够在保证隐私的同时提高跨不同客户端间模型使用的效率和准确性。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [22] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 该论文研究了在用户级差分隐私保护下的持续均值估计问题，通过引入一种新的特定于均值估计的因子分解方法，在保证隐私的同时降低了均值平方误差。


<details>
  <summary>Details</summary>
Motivation: 现有的连续均值估计解决方案主要集中在纯差分隐私上，但这种方法导致估计过于嘈杂，限制了实际应用的可能性。本文旨在通过采用近似差分隐私的方法来提高估计精度和实用性。

Method: 本文采用了矩阵因子化机制中的最新进展，并为此问题引入了一种新颖且高效的均值估计专用因子分解技术。

Result: 所提出的方法不仅有效而且准确，相较于现有方案，在用户级别差分隐私条件下实现了渐进更低的均方误差界限。

Conclusion: 通过引入适用于均值估计的新因子分解方法，本研究在保持用户级隐私的同时显著提高了连续均值估计任务中的估计质量。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [23] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: 本文提出SCOPE，一种基于强化学习训练的路由框架，能够预测模型的成本和性能，并灵活适应用户需求，在保证高准确性的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的模型路由方法通常仅在一小部分模型中进行固定选择，难以适应新模型或变化的预算限制。

Method: 提出SCOPE（可扩展且可控的结果性能估计器），通过强化学习训练来预测模型的成本与性能，基于对类似问题上模型表现的检索而非依赖固定的模型名称来进行推理预测，使系统能够处理新的、未见过的模型。

Result: 实验表明，SCOPE不仅是一个节省成本的工具，还能根据用户需要灵活调整：当优先考虑性能时，它最多可以将准确性提高25.7%；而当效率最重要时，它最多可以削减95.1%的成本。

Conclusion: SCOPE提供了一种动态决策方法，允许用户轻松控制准确性和成本之间的权衡，从而在面对不同的应用情境下都能保持高效和灵活性。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [24] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文介绍了一种名为AgentScore的新方法，该方法利用大语言模型提出候选规则，并通过一个确定性的、基于数据的验证和选择循环来实施统计有效性和可部署性约束。这种方法在多个临床预测任务中表现出色，超越了现有的评分生成方法，并且在两个额外的外部验证任务中也取得了比现有指南更高的区分度。


<details>
  <summary>Details</summary>
Motivation: 当前临床实践中使用的许多机器学习模型由于与工作流程约束（如记忆性、可审计性和床边执行）不一致而未能转化为常规使用。尽管这些模型具有强大的预测能力，但它们往往不适合指导方针的部署要求。因此，需要一种既能保持高度预测准确性又能满足临床实践需求的方法。

Method: 提出了一种称为AgentScore的方法，它利用大型语言模型（LLMs）来建议可能的决策规则，并通过一个确定性的、基于数据的验证和选择循环确保所选规则符合统计学有效性及实际应用条件。该方法旨在创建类似于单位权重临床清单的可部署指南，同时克服了在大量潜在规则集中搜索最优解的问题。

Result: 在八个不同的临床预测任务上，AgentScore的表现优于其他现有的评分生成技术，并且即使是在更强结构限制下也能达到与更灵活的可解释模型相当的AUC值。此外，在另外两项经过外部验证的任务中，AgentScore展现出比已建立的基于指南得分更高的辨别力。

Conclusion: AgentScore作为一种新的方法论，成功地解决了将高性能机器学习模型转化为临床实用指南时面临的挑战。其通过结合自然语言处理技术和严格的统计验证过程，不仅提高了模型的实际应用价值，还保证了最终结果的有效性和可靠性。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [25] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架MAAT，用于从数据中恢复控制方程。该框架基于知识信息的核状态重构，在重构目标中直接结合了结构和语义先验，并且可以处理异构采样和测量粒度的问题。实验表明，与强大的基线相比，MAAT在多个噪声环境下显著降低了下游符号回归使用的轨迹及其导数的状态估计MSE。


<details>
  <summary>Details</summary>
Motivation: 现有从数据中恢复控制方程的方法往往在噪声、部分观测情况下失效，或者依赖于黑盒潜在动力学模型，这掩盖了背后的机制。

Method: 提出了MAAT（Model Aware Approximation of Trajectories）框架，该框架基于知识信息的核状态重构。它在再生核希尔伯特空间中构建状态重构，并直接将非负性、守恒定律以及领域特定观察模型等结构性和语义性先验纳入重构目标，同时支持异构采样和不同测量粒度。

Result: 通过十二个不同的科学基准测试及多种噪声环境下的实验显示，相对于强大的基线方法，MAAT能够大幅降低供下游符号回归所用轨迹及其导数的状态估计均方误差（MSE）。

Conclusion: MAAT提供了一个从碎片化的传感器数据到符号回归之间的原则性接口，通过生成平滑且物理一致的状态估计值（包括解析时间导数），从而改善了从数据中发现控制方程的过程。

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [26] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: 本文提出了一种名为BALANS的批处理校正方法，用于解决细胞涂装数据中的批处理效应问题。通过构建平滑的亲和矩阵并采用自适应采样程序，该方法能够在大规模数据集上有效运行，并且在提高运行时间的同时不牺牲校正质量。


<details>
  <summary>Details</summary>
Motivation: 细胞涂装技术能够生成丰富的细胞形态特征，支持药物发现过程。然而，由于实验室、仪器及协议之间的差异导致的数据批处理效应显著影响了这种技术的应用，可能掩盖真正的生物信号。因此，需要一种有效的批处理校正方法来解决这个问题。

Method: 本文介绍的方法BALANS（Batch Alignment via Local Affinities and Subsampling）是通过构建基于成对距离的平滑亲和矩阵来跨批次对齐样本。对于n个数据点，BALANS首先使用局部尺度设置与高斯核计算稀疏亲和矩阵A；其次，通过优先考虑低累积邻居覆盖率行并保留每行最强的亲和力，采用自适应采样程序近似得到A。

Result: 实验结果表明，BALANS不仅能在多样化的实际细胞涂装数据集和受控的大规模合成基准测试中表现出色，而且其运行时间也优于广泛使用的其他批处理校正方法的原生实现版本，同时保持了良好的校正质量。

Conclusion: BALANS作为一种可扩展的批处理校正方法，在处理大规模细胞涂装数据时展现出了优越性，它提供了一个快速且准确的新途径来减轻批处理效应的影响。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [27] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出了一种新的噪声相关策略，该策略仅与前一次迭代的噪声相关，并通过伪随机噪声生成器再生噪声，从而无需存储过去的噪声。这种方法不需要额外的内存，并且在计算开销很小的情况下，实证显示比DP-SGD具有更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私随机梯度下降（DP-SGD）算法虽然为训练机器学习模型提供了正式的差分隐私保证，但最近的一些改进方法通过引入跨训练迭代的相关噪声来提高其准确性。然而，这些方法如矩阵分解机制，会将许多迭代中的噪声关联起来，并需要存储先前添加的噪声向量，导致在某些情况下出现显著的内存开销。因此，本文旨在提出一种既能减少内存需求又能保持或提高模型准确性的新噪声处理方法。

Method: 研究者们提出了一种新型噪声相关策略，该策略只与紧接的上一次迭代产生的噪声相关联，并能够取消一部分受控的噪声。此方法利用伪随机噪声生成器实现噪声的再生，避免了必须保存过去噪声的问题，从而不增加标准DP-SGD之外的任何内存需求。

Result: 所提方法不仅实现了对额外内存需求的完全消除，同时保持了非常低的计算开销。实验结果表明，在实际应用中，该方法相比传统的DP-SGD能获得更优的模型精度表现。

Conclusion: 本文介绍的新噪声相关策略成功地解决了现有DP-SGD技术中存在的内存占用问题，同时改善了模型训练的准确性。通过采用即时噪声再生技术而非依赖于历史噪声数据，本研究为差分隐私保护下的高效机器学习模型训练开辟了新途径。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [28] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文解决了偏好贝叶斯优化中知识梯度计算的难题，通过推导出精确且解析的知识梯度公式，并在一系列基准问题上展示了其优越性。此外，还通过案例研究指出了知识梯度在某些场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，只能进行成对比较查询，导致直接函数评估不可用，从而形成了一个偏好贝叶斯优化问题。而将知识梯度扩展到这种情境中受到计算挑战的阻碍，因为预测步骤需要计算非高斯后验分布，这在过去被认为是不可能解决的问题。

Method: 作者们通过推导适用于偏好贝叶斯优化的确切和解析形式的知识梯度来应对这一挑战。

Result: 所提出的确切知识梯度方法在多个基准测试问题上表现优异，通常优于现有的获取函数。同时，文章也通过案例研究展示了知识梯度在特定情况下的限制。

Conclusion: 该研究成功地为偏好贝叶斯优化开发了一种新的知识梯度方法，不仅克服了之前遇到的计算障碍，而且在实践中表现出色，尽管在某些情况下可能仍存在局限性。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [29] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 本文评估了语言模型在有限互动预算下探索交互环境的能力，发现现有模型存在系统性欠探索问题。通过将固定预算分割成并行执行和定期总结互动历史这两种轻量级干预措施，可以改善探索效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估语言模型在给定有限互动次数的情况下如何有效地探索不同难度级别的交互环境，并试图找到提高其探索能力的方法。

Method: 引入了三个参数化任务来控制探索难度，并考察了当前最先进的模型表现；接着测试了两种改进方法：一是将固定数量的互动机会分成多条线程同时进行，二是定时对互动过程做一个总结。

Result: 结果表明，顶级模型普遍表现出不足的探索行为，且随着互动预算增加性能提升幅度不大。然而，通过采取上述提到的两种简单策略能够有效增强模型的探索效率。

Conclusion: 结论是，即便对于最先进的语言模型来说，在有限互动条件下实现高效探索仍是一个挑战，但可以通过一些简单的调整如并行处理与适时回顾来显著改善这一点。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [30] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: 本文首次系统地分析了块Hadamard旋转下的异常值抑制，并基于此提出了MixQuant，一种感知块旋转的PTQ框架。通过在旋转前重新分配激活质量，MixQuant能够显著提高量化精度，尤其是在较小的块大小下。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练量化(PTQ)方法采用块旋转来减少异常值，但关于块结构如何影响异常值抑制的效果仍不清楚。研究旨在填补这一空白，提供一个非渐近性的分析，同时提出一种新的方法以优化异常值处理过程。

Method: 通过对块Hadamard旋转进行系统性分析，揭示了输入向量几何形状对异常值抑制的基本限制。基于这些发现，开发了MixQuant框架，该框架利用置换操作在旋转前重新分配激活质量，以实现更均匀的块间$\ell_1$范数分布。此外，还设计了一种贪婪的质量扩散算法用于校准置换，并通过识别Transformer架构中的置换等变区域来避免增加推理开销。

Result: 实验结果表明，MixQuant能够在所有块尺寸下持续提高准确性，特别是在将Llama3 1B模型量化为INT4格式时，使用块大小为16的情况下，恢复了高达90%的全向量旋转困惑度，而没有使用置换的情况下仅能恢复46%。

Conclusion: MixQuant有效地解决了块旋转中异常值抑制的问题，通过重新分配激活质量达到了更好的量化效果。这不仅提高了模型在小块大小下的性能，也为进一步研究提供了有价值的见解。

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [31] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 本研究提出了一种基于马尔可夫决策过程（MDP）的策略表示学习方法，通过状态-动作特征映射和占用度量来近似策略表示，并利用变分生成方法和平行学习技术塑造平滑潜在空间，以支持在未见价值函数约束条件下直接在潜在空间中进行基于梯度的优化。


<details>
  <summary>Details</summary>
Motivation: 为了在测试时便于行为引导，研究者们试图为一系列策略学习表征。由于MDP中的策略由其占用度量唯一确定，因此研究提出了将策略表征建模为关于占用度量的状态-动作特征图的期望值的方法。

Method: 该方法采用集合式架构来对一定范围内的策略表征进行均匀近似。模型首先将一组状态-动作样本编码成潜在嵌入，然后从中解码出对应于多种奖励的策略及其价值函数。此外，还使用了变分生成法来诱导平滑的潜在空间，并通过对比学习进一步调整该空间，使得潜在距离与价值函数差异相匹配。

Result: 所提出的几何结构允许直接在潜在空间内执行基于梯度的优化。研究团队利用这一特性解决了一个新的行为合成任务：无需额外训练即可指导策略满足之前未曾见过的价值函数约束条件。

Conclusion: 这项工作介绍了一种新方法，能够学习到可以用于测试阶段行为引导的策略表示。它不仅提供了一种有效的策略表示方式，还展示了如何利用这种表示在没有额外训练的情况下满足新的价值函数约束。

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [32] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 该论文通过最优传输框架研究了经验分布与高斯分布之间的偏离程度，引入了相对Wasserstein角和平行投影距离两个新的几何量来度量非高斯性，并证明了由空间中任意两射线生成的填充锥是平的。在高维情况下，开发了一种基于半离散对偶公式化的高效随机流形优化算法。实验表明，相对Wasserstein角度比Wasserstein距离更稳健，并且提出的最近高斯提供了一个比矩匹配更好的近似值，在FID分数评估中表现更好。


<details>
  <summary>Details</summary>
Motivation: 为了量化经验分布与高斯分布之间偏离的程度，文章从最优传输的角度出发，利用相对平移不变二次Wasserstein空间的圆锥几何特性，提出了一种新的几何方法来衡量非高斯性。

Method: 文章通过定义相对Wasserstein角度和平行投影距离这两个新概念，将高斯逼近问题转化成了一个到高斯圆锥上的投影问题，并且证明了在这个空间内由任何两射线构成的填充锥是平坦的。对于一维情况，给出了所提量的具体表达式；而对于高维情形，则基于半离散对偶形式化开发了一种有效的随机流形优化算法。

Result: 研究表明，相对Wasserstein角度相比Wasserstein距离而言更加稳健。此外，所提出的寻找最接近高斯分布的方法比常用的矩匹配方法能够提供更好的近似效果，特别是在评价Fréchet Inception Distance (FID) 分数时表现得尤为明显。

Conclusion: 本文介绍的新方法为量化经验分布与高斯分布间的偏离提供了有力工具，尤其是相对Wasserstein角度和提出的最近高斯估计方法在稳定性及准确性上优于传统方法。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [33] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: 提出了一种名为PoSafeNet的可微神经安全层，用于在部分有序集一致的约束排序下通过顺序闭式投影来强制执行安全性，从而提高了机器人系统中基于学习的控制器的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常统一或以固定优先级顺序强制执行多个安全约束，这可能导致不可行性和脆弱行为。实际上，安全需求是异质的，仅允许部分优先级关系，在这种情况下某些约束是可以比较的，而其他约束本质上是无法比较的。

Method: 形式化了这种设置为poset-structured safety（部分有序集结构的安全性），将安全约束建模为一个偏序集，并将安全组合视为策略类的一种结构性属性。基于此公式，提出了PoSafeNet，一种可微神经安全层，该层通过在与偏序集相容的约束顺序下的顺序闭式投影来实施安全，使得能够自适应选择或混合有效的安全执行方式，同时按构造保留优先级语义。

Result: 在多障碍导航、受限机器人操作和基于视觉的自动驾驶上的实验表明，相比于无结构和基于可微二次程序的安全层，所提方法在可行性、鲁棒性和可扩展性方面都有所改进。

Conclusion: 这项工作为处理复杂安全要求的机器人系统提供了一个新的框架，通过更灵活地处理安全约束，提高了基于学习的控制系统的实际应用潜力。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [34] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本研究通过实证分析了大型语言模型（LLMs）在NVIDIA H100 GPU上的推理能量消耗和延迟，探讨了量化、批量大小和服务配置等因素对能源效率的影响。结果表明，低精度格式仅在计算密集型场景下能带来能耗降低；批处理可以提高能源效率，特别是在解码等内存密集阶段；以及结构化的请求时间安排能够大幅减少每次请求的能量消耗。研究强调了为了实现更可持续的LLM部署，除了关注模型内部外，还需要考虑服务堆栈的协调优化。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被投入实际应用，从训练到推理过程中所需的计算资源和能源需求正在发生变化。尽管已有研究考察了每条提示或每个令牌的推理能量成本，但系统级设计选择（如数值精度、批处理策略和请求调度）同样会导致相同模型之间出现数量级差异的能量消耗。

Method: 研究者们进行了详细的实证研究，对象是NVIDIA H100 GPU上运行的大型语言模型推理过程中的能量消耗与延迟情况。他们分析了量化、批量大小以及服务配置（例如使用Hugging Face的文本生成推理服务器）对这些因素的影响。

Result: 研究表明，低精度格式仅当处于计算受限状态下时才能提供能量节省；批处理特别在像解码这样内存受限阶段里提高了能量效率；而且通过结构化请求时机（到达形状），能够将每次请求的能量消耗减少多达100倍。

Conclusion: 为了更加可持续地部署大型语言模型，不仅需要关注模型本身的设计，还需要重视整个服务堆栈层面的协调与优化。该发现促进了基于阶段意识的能量分析及系统级别优化措施的发展，以促进更绿色的人工智能服务。

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [35] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文提出了一种基于科学文献训练的大语言模型的完全自主、基于语言推理过程的生物黑箱优化方法PABLO，该方法在标准分子设计和抗菌肽优化任务上取得了最先进的性能，并且在体外验证中展示了对耐药病原体的强大活性。


<details>
  <summary>Details</summary>
Motivation: 当前生物设计中的许多关键挑战，如小分子药物发现、抗菌肽开发和蛋白质工程等，可以被视作在巨大复杂的结构空间中的黑箱优化问题。现有方法主要依赖于原始结构数据，难以充分利用丰富的科学文献。虽然大型语言模型（LLMs）已被纳入这些流程中，但它们的作用局限于以结构为中心的优化器内的狭窄角色。

Method: 本文引入了纯自主黑箱优化（PABLO），这是一种层次化的自主系统，利用预先在化学和生物学文献上训练的科学LLM来生成并迭代改进生物候选物。PABLO能够自然地结合语义任务描述、增强检索领域的知识以及复杂约束条件。

Result: 在GuacaMol分子设计和抗菌肽优化的标准任务上，PABLO实现了最先进水平的表现，显著提高了样本效率及最终目标值，超过了既定基准。与先前将LLMs纳入的优化方法相比，尽管PABLO在整个优化循环中都依赖于LLMs，但在每次运行的令牌使用量方面仍保持竞争力。此外，在后续的体外验证中，经过PABLO优化的肽类显示出对抗耐药性病原体的强大活性。

Conclusion: 通过将生物黑箱优化视为一个完全自主的语言推理过程，PABLO不仅在性能上超越了现有方法，还为现实世界的设计提供了重要的优势，包括自然融入语义任务描述、检索增强领域知识及处理复杂约束的能力。其在治疗发现方面的实际潜力也得到了体现。

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [36] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为G-Substrate的图基底框架，该框架通过统一结构模式和交错基于角色的训练策略来组织学习过程中的共享图结构，从而在跨异构模态和任务时保持并积累图结构。实验表明G-Substrate优于孤立的任务学习方法和简单的多任务学习方法。


<details>
  <summary>Details</summary>
Motivation: 由于图结构通常是在特定模态和任务中单独学习且用后即弃，这导致了跨模态和任务的结构性规律被反复重建而非以中间图表示的形式累积下来。因此，需要找到一种方式来组织图结构，使其能够在不同的模态和任务之间持久化并累积。

Method: 提出了G-Substrate框架，它包括两个互补机制：一是统一的结构模式，确保不同模态和任务之间的图表示兼容；二是交错的角色基础训练策略，在学习过程中让相同的图结构承担多个功能角色。

Result: 实验结果表明，G-Substrate在跨越多个领域、模态和任务的情况下，性能优于独立任务学习法以及简单的多任务学习方法。

Conclusion: G-Substrate提供了一个有效的途径来构建能够跨模态与任务持续存在并累积下来的图结构，有助于更好地利用数据中的结构规律。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [37] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: 本文介绍了一种名为SIREN的新颖且可扩展的方法，用于通过估计数据似然性的得分函数来确定异常值的根本原因。SIREN满足经典Shapley值公理中的三个，并且能够在非线性、高维和异方差因果模型中有效运行。实验表明，SIREN在归因准确性和计算效率方面优于最先进基线。


<details>
  <summary>Details</summary>
Motivation: 识别异常值的根本原因是因果推断和异常检测中的基本问题。传统基于启发式或反事实推理的方法在不确定性以及高维依赖关系下常常难以应对。

Method: 提出SIREN方法，通过估计数据似然性的得分函数来确定异常值的根本原因。该方法利用积分梯度沿从异常值到正常数据分布的路径累积得分贡献来进行归因。

Result: SIREN满足四个经典Shapley值公理中的三个——虚拟性、效率性和线性性——以及一个由底层因果结构派生出的不对称公理。SIREN直接作用于得分函数上，使其能在非线性、高维及异方差因果模型中实现可处理且考虑不确定性的根本原因归因。

Conclusion: 通过对合成随机图和真实世界云服务与供应链数据集进行广泛实验，结果表明SIREN在归因准确性和计算效率方面均优于当前最先进的基准。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [38] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了MM-OpenFGL，这是首个全面的基准测试，旨在系统化多模态联邦图学习（MMFGL）范式，并通过广泛的实验提供对MMFGL必要性、有效性、鲁棒性和效率方面的宝贵见解。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，多模态属性图（MMAGs）通常分布在不同的平台且因隐私或商业限制无法共享。虽然联邦图学习为这种情况下协作训练提供了自然解决方案，但现有研究主要集中在单一模态图上，未能充分解决多模态联邦图学习（MMFGL）的独特挑战。

Method: 作者们开发了MM-OpenFGL，一个包括19个多模态数据集、8种模拟策略、6个下游任务以及57种最先进方法的综合基准，通过模块化的API实现。

Result: 广泛的实验证明了MMFGL在必要性、有效性、鲁棒性和效率方面的表现，为未来的研究提供了有价值的见解。

Conclusion: MM-OpenFGL作为第一个全面评估MMFGL的基准，不仅为理解MMFGL的各种方面提供了坚实的基础，也指出了未来研究的方向。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [39] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: 本文介绍了MetaLead，一个完全由人工标注的机器学习排行榜数据集，旨在通过捕捉所有实验结果和额外元数据来提高结果透明度，并支持跨领域评估。


<details>
  <summary>Details</summary>
Motivation: 现有的排行榜生成方法存在局限性，如仅捕获每篇论文的最佳结果且元数据有限。为了克服这些限制，研究提出了MetaLead，以实现更透明、细致的机器学习研究评价。

Method: 通过创建MetaLead，一个包含所有实验结果及额外元数据（比如实验类型：基线、提议方法或提议方法的变化）的人工标注数据集，明确区分训练集与测试集，促进不同领域的性能评估。

Result: MetaLead为机器学习研究提供了更加透明和精细的评价资源，有助于改善当前排行榜生成过程中存在的不足。

Conclusion: MetaLead作为一款增强型的数据集，在提升机器学习研究中结果透明性和比较分析方面展现出巨大潜力。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [40] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 本文提出了一种显式的对比学习方法来增强预训练语言模型的推理能力，通过将K个结果分为正负集合，并最大化正结果的可能性，从而在一系列具有挑战性的数学基准测试中表现出与强基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO是一种为预训练的语言模型赋予推理能力的标准方法，但其需要额外的改进措施如非对称裁剪和零方差数据过滤才能表现良好，而这些改进措施往往需要大量的实证研究且难以确定。因此，作者提出了一种新的显式对比学习方法以简化这一过程。

Method: 作者提出的这种方法不直接估计优势，而是把K个结果分成正面和负面两组，然后最大化正面结果的概率。这种做法可以被视为针对LLM推理的一种在线多标签噪声对比估计实例。

Result: 实验表明，该方法在一系列具有挑战性的数学基准上与DAPO、在线DPO等强大基线相比展现出了竞争力。

Conclusion: 这项工作介绍了一种新颖的对比学习策略用于提高预训练语言模型的推理能力，提供了一个不同于GRPO的新视角，并且在实际应用中取得了良好的效果。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [41] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 研究了在逆问题求解中，当测量信息量大时，即使使用与目标不完全匹配或低质量的扩散模型作为先验，也能获得接近高质量、领域内基线的结果。通过大量实验和理论分析，确定了弱扩散先验成功和失败的情况，并基于贝叶斯一致性理论给出了高维测量使后验集中在真实信号附近的条件。


<details>
  <summary>Details</summary>
Motivation: 探讨了在实际应用中经常遇到的使用不匹配或低质量扩散先验的情况下，逆问题求解器为何以及何时能够对这些弱先验具有鲁棒性。

Method: 通过广泛的实验来观察弱先验在不同情况下的表现，并结合基于贝叶斯一致性的理论分析来解释实验结果。

Result: 发现当测量提供的信息量很大（例如，观测到很多像素）时，弱先验可以取得几乎与高质量、领域内基准相当的表现；同时指出了弱先验失效的情境。

Conclusion: 高维度测量条件下，即便采用较弱的扩散先验也可以可靠地应用于逆问题求解，这为弱扩散先验的实际应用提供了理论依据。

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [42] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 本文提出了一种基于权重的解释框架，通过直接权重交互来衡量功能效应，无需激活数据。实验结果表明，部分特征可以直接预测输出标记，并且这些特征在注意力机制中具有深度依赖结构，语义和非语义特征群体在注意力电路中表现出不同的分布特征。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏自动编码器（SAE）特征解释方法仅从激活模式推断特征语义，但忽略了这些特征是为重建在前向传递中起计算作用的激活而训练的事实。

Method: 开发了一个新的基于权重的解释框架，该框架能够通过直接权重相互作用来测量功能性效果，不需要任何激活数据。

Result: (1) 四分之一的特征可以直接预测输出标记。(2) 特征以具有深度依赖结构的方式积极参与到注意力机制中。(3) 语义和非语义特征群体在注意力电路中显示出了不同的分布特性。

Conclusion: 本研究提供了SAE特征可解释性缺失的上下文外一半内容，强调了特征不仅在语义上而且在其于模型内执行的功能角色上的重要性。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [43] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: 本文提出了一种名为HeaPA的新方法，通过维护一个有限且不断演化的提示池、使用基于堆的边界采样追踪能力前沿、经由轻量级异步验证进行池扩展以及通过拓扑感知重估和控制性重新插入稳定相关查询，从而在推理任务上提高了大型语言模型训练的效率。实验显示HeaPA能够在保持类似时间成本的同时，以更少的计算达到目标性能。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR（强化学习与可验证结果）是训练大型语言模型执行推理任务的一种标准方式，但当生成rollout（展开）成为主要成本时，效率很大程度上取决于所选择的提示及其时机。现有静态或松散关联于模型学习进度的提示池会导致均匀采样无法跟上能力边界的变动，造成资源浪费。尽管已有方法试图通过过滤、课程设计、自适应分配或教师指导来提高效率，但它们通常假设了一个固定的提示池，这限制了在线策略池的增长，或者增加了额外的教学成本与延迟。

Method: HeaPA方法包括四个方面：1) 维护一个有界且不断演变的提示池；2) 利用基于堆的边界采样技术跟踪能力前沿；3) 通过轻量级异步验证实现在线策略增强以扩展池；4) 通过对池统计信息进行拓扑感知重估及受控再插入来稳定相关查询。

Result: 在两个训练语料库、两种训练方案和七个基准测试中，HeaPA一致地提高了准确性，并且以较少的计算量达到了目标性能，同时保持了相近的运行时间。分析表明，这些改进来自于聚焦于能力前沿的采样和在线策略池增长，随着模型规模增加其益处更加显著。

Conclusion: HeaPA提供了一种有效的方法来提升大型语言模型在推理任务上的训练效率，特别是在处理大规模模型时表现更为出色。它通过动态调整提示池和优化样本选择过程，减少了达到特定性能水平所需的计算资源。

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [44] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器遗忘(LOFT)方法，该方法通过在预训练模型中从低维特征子空间进行操作来移除特定数据的影响，同时保持对剩余数据的性能。此方法只需优化一个小规模的投影矩阵，并且只需要一次性从预训练模型获取特征，从而减少了主流机器遗忘方法中的隐私泄露风险和更新整个预训练模型的效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘（MU）方法面临两个关键问题：由于大量重新加载数据导致的隐私泄露风险以及更新整个预训练模型时的低效性。此外，研究者们希望找到一种方式能够更好地分离需要保留的数据与需被遗忘的数据，以提升MU的效果。

Method: 提出的方法称为LOFT，它基于低维特征子空间的概念，通过主成分投影的方式执行机器遗忘。LOFT方法通过优化一个小型投影矩阵来实现，这个矩阵可以灵活地插入到预训练模型中，并且只需要从预训练模型中一次性提取特征，不需要重复访问原始数据。

Result: 广泛的实验验证了LOFT在不同模型、数据集、任务及应用上都具有显著更低的计算开销和更优的遗忘性能。

Conclusion: LOFT提供了一种有效解决现有机器遗忘挑战的新途径，不仅减少了隐私泄露的风险，还提高了处理效率，为机器学习领域内的数据删除需求提供了强有力的支持。

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [45] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于结构的药物设计方法EvoEGF-Mol，通过信息几何视角将分子建模为复合指数族分布，并定义了在Fisher-Rao度量下的生成流。该方法避免了直接针对狄拉克分布的测地线引起的瞬时轨迹塌陷问题，使用动态集中分布代替静态目标，确保了训练稳定性。实验结果表明，该方法在CrossDock数据集上达到了参考级别的PoseBusters通过率，并且在实际的MolGenBench任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的药物设计方法在欧几里得空间和概率空间中分别构建路径，导致与底层统计流形不匹配的问题。为了解决这一问题，本文从信息几何的角度出发，提出了一种新的方法来改进基于结构的药物设计流程。

Method: 本文的方法是通过将分子视为复合指数族分布，并在Fisher-Rao度量下定义沿指数测地线的生成流。为了防止直接指向狄拉克分布的测地线引起瞬时轨迹坍塌，提出了Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol) 方法，该方法用动态集中分布替代静态狄拉克目标，结合渐进参数细化架构以保证训练过程的稳定性。

Result: 实验结果显示，在CrossDock数据集上，EvoEGF-Mol模型接近参考级别的PoseBusters通过率（93.4%），显示出卓越的几何精度和相互作用保真度。此外，该方法还在真实世界的MolGenBench任务中表现出色，能够恢复生物活性骨架并生成符合既定MedChem过滤器要求的候选化合物。

Conclusion: 通过引入EvoEGF-Mol，研究成功地解决了传统基于结构的药物设计中存在的问题，提供了一种更精确、有效的分子生成策略。这不仅提高了药物设计过程中对分子几何形状和相互作用的理解，而且为发现新的生物活性配体提供了强有力的支持。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [46] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: 本研究发现大型语言模型（LLMs）在无奖励探索阶段也表现出潜在学习现象，这使得模型能够在不受奖励驱动偏见限制的情况下组织与任务相关的知识。当引入奖励后，这些模型的性能得到进一步提升，并且最终比那些全程使用基于奖励的强化学习训练的模型表现得更好。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（如OpenAI-o1和DeepSeek-R1）在推理能力方面取得了重大突破，但它们仍然主要依赖于以奖励为中心的强化学习范式。心理学中已确立的潜在学习现象是否以及如何能够启发或出现在LLMs的训练过程中，这一领域仍待探索。

Method: 通过设计实验让LLMs经历一个初始的无奖励探索阶段，随后再引入奖励机制，观察并分析LLMs的表现变化。此外，还对多个模型家族及不同任务领域进行了广泛测试，来验证潜在学习动力学的存在。

Result: 结果显示，在最初的无奖励探索阶段，LLMs展现出了适度的性能改进；一旦加入奖励，其表现显著增强。采用这种两阶段探索制度进行后期训练的LLMs最终达到了比持续使用基于奖励的强化学习训练更高的能力水平。

Conclusion: 研究表明，类似潜在学习的过程确实存在于LLMs之中，这为提高模型灵活性和泛化能力提供了一种新思路。理论分析支持了无奖励探索阶段对于性能提升的重要性，揭示了背后的机制。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [47] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出了一种新的教师-学生框架，用于持续强化学习（CRL），该框架将单任务教师模型的训练与通过分布式强化学习不断提炼成一个中心通用模型的过程分离。此外，采用了混合专家架构和基于重放的方法来提高持续策略提炼过程中的可塑性和稳定性。在Meta-World基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习（CRL）旨在开发能够终生学习的智能体，在不同任务间连续获取知识的同时减少灾难性遗忘。尽管已提出多种增强策略，但直接应用RL于连续任务流以实现可扩展性能仍具挑战性。

Method: 设计了一个新颖的教师-学生框架，其中使用分布式强化学习训练单任务教师模型，并不断将其精炼到一个中央通才模型中。此外，还利用了混合专家架构和基于回放的方法来增加持续策略提炼过程中的灵活性和稳定性。

Result: 广泛的实验表明，所提出的框架能够在Meta-World基准上实现高效的持续强化学习，恢复超过85%的教师模型性能，同时将每项任务的遗忘限制在10%以内。

Conclusion: 这项工作通过引入一种创新的教师-学生框架有效地解决了持续强化学习中存在的挑战，证明了其在处理多任务时的有效性和优势。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [48] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: 本文提出了一种改进的TA-GRPO方法，通过生成语义等价的问题变体并汇集整个组中的奖励来计算优势，从而解决原GRPO方法存在的多样性崩溃和梯度减小问题。实验表明该方法在数学推理基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于下一步预测训练的大语言模型在处理表面措辞变化时表现敏感，即使基本问题相同。虽然Group Relative Policy Optimization (GRPO)旨在改善这一情况，但实际上它导致了多样性的减少和梯度消失问题。为了解决这些问题，提出了Transform-Augmented GRPO (TA-GRPO)。

Method: TA-GRPO通过对每个问题生成语义等价的变化版本（包括释义、变量重命名和格式更改），然后在整个组中汇集奖励来计算优势。这种方法确保即使原始问题太容易或太难也能获得混合奖励，并且通过多样化表达促进多种解决方案策略的发展。

Result: 理论分析显示，TA-GRPO能够降低零梯度概率并通过减少训练-测试分布偏移提高泛化能力。实验证明，在数学推理基准如AMC12, AIME24以及科学推理任务GPQA-Diamond上，相比现有方法，TA-GRPO实现了显著的性能提升。

Conclusion: TA-GRPO有效解决了原有GRPO方法面临的挑战，即多样性下降与梯度消失问题，同时提升了模型对于不同表述形式问题的适应性和泛化能力。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [49] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: 提出了一种名为STARS的新框架，用于解决大型推理模型中的认知惯性问题。该框架通过监测隐藏状态中的L2距离峰值来识别关键的推理转折点，并使用几何轨迹分析来诊断转换的结构性质，然后注入状态感知的语言提示以实时引导模型。实验表明，STARS能够有效减少冗余循环并提高准确性，且无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在扩大测试时计算规模的同时，经常遭受认知惯性的问题，表现为过度思考或推理僵化。现有的检测方法通常依赖于表面的文字启发式方法，难以捕捉到模型内部未表达出的冲突。

Method: 提出了STARS（Spike-Triggered Adaptive Reasoning Steering），一种无需训练的框架，旨在通过监控潜在动态纠正认知惯性。STARS通过检测隐藏状态中独特的L2距离峰值来识别认知支点—即推理转变的关键时刻；随后，利用几何轨迹分析诊断转变的结构性质，并通过注入状态感知的语言线索实时指导模型。

Result: 跨多个基准测试的实验证明了STARS可以有效地削减冗余循环，同时通过对错误轨迹的自适应修正提高准确性。

Conclusion: STARS为优化LRMs的推理过程提供了一个强大、无监督的机制，而不需要进行额外的微调。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [50] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: 本文提出了一种名为Gradual Fine-Tuning (GFT)的新框架，用于在有目标分布样本可用的情况下微调基于流的生成模型。GFT通过定义一系列温度控制的中间目标来平滑地插值预训练和目标漂移，从而提高了收敛稳定性并缩短了概率路径，同时保持了与标准微调相当的生成质量。


<details>
  <summary>Details</summary>
Motivation: 在有限数据、分布变化或严格效率要求的情况下，无约束的微调可能会侵蚀预训练期间学到的准确性和效率增益。现有方法虽然提供了理论保证和实证进展，但通常对允许的漂移结构或训练技术施加限制。因此，需要一种新的微调方法来克服这些限制。

Method: Gradual Fine-Tuning (GFT)是一种针对当能够获得目标分布样本时用于微调基于流的生成模型的原则性框架。对于随机流，GFT定义了一个温度控制的中间目标序列，该序列能够在预训练和目标漂移之间平滑插值，并且随着温度接近零而逐渐逼近真实的目标。

Result: GFT不仅提高了收敛稳定性还缩短了概率路径，这导致更快的推理速度，同时保持了与标准微调相媲美的生成质量。

Conclusion: GFT作为一种理论上合理且实际有效的选择，在分布变化下为流动匹配模型的大规模适应提供了解决方案。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [51] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 本文提出了一种信息论框架，定义了动作充分性作为目标表示的必要条件，并证明了价值充分性并不意味着动作充分性。实验结果表明，通过标准对数损失训练低级策略自然会诱导出动作充分的表示，并且在流行的基准测试中，演员派生的表示始终优于通过价值估计学习到的表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在学习价值函数时推导目标表示，假设保留足够的价值估计信息足以进行最优控制。但本文指出这种假设可能失败，即使价值估计是准确的，因为这样的表示可能会混淆需要区分以学习动作的目标状态。

Method: 提出了一个信息论框架来定义动作充分性，这是目标表示为了最优动作选择所必需满足的一个条件。证明了价值充分性并不意味着动作充分性，并通过实验证明后者与离散环境中的控制成功更密切相关。此外，展示了标准对数损失训练低层次策略能够自然地促进动作充分表示的发展。

Result: 实验结果显示，在流行基准上，基于演员（actor-derived）的方法得到的表示相比那些从价值估计过程中学习到的表示具有更好的性能。

Conclusion: 该研究强调了对于离线目标条件强化学习中的层级策略而言，设计考虑到动作充分性的目标表示至关重要；它不仅挑战了以往关于价值充分性即可保证最优控制的观点，还提供了实际解决方案来改进控制任务的表现。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [52] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 研究发现，大型语言模型（LLMs）在学习技能组合时并不遵循类似人类的顺序规则，而是倾向于以相反或并行的方式获取技能，导致了在数据分布变化下出现意外的混合错误。这种现象被称为破碎的组合性，并且这种特性即使在模型规模扩大或使用辅助思考方法时也依然存在。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究揭示了大型语言模型与人类在技能组成上的差异，但对于这些模型学习动态背后的机制以及非人类行为的根本原因仍不清楚。本研究旨在探索这一机制，特别是通过训练转换器执行合成算术任务来深入理解技能组成的习得过程。

Method: 通过训练转换器处理合成算术任务，并利用广泛的消融实验和精细的诊断指标进行分析。

Result: 发现转换器并不是按照类似于人类的顺序规则可靠地构建技能组合，而是在逆序或并行中获得技能，这导致了特别在分布转移情况下的意外混合错误。此外，证据表明是与训练数据的相关匹配而非因果或程序性组合决定了学习动态。

Conclusion: 研究揭示了模型学习行为与其期望达到的技能组合之间存在着根本性的不匹配，这对推理可靠性、分布外鲁棒性和一致性有着重要影响。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [53] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 该论文研究了无人机辅助可见光通信系统中的三维轨迹规划问题，旨在通过优化无人机的飞行路径来提高数据收集效率。提出了一种结合新型信息素驱动奖励机制与双延迟深度确定性策略梯度算法的方法，以实现复杂环境下的自适应无人机运动策略。仿真结果表明，所提出的方案能够有效减少飞行距离并加快收敛速度。


<details>
  <summary>Details</summary>
Motivation: 随着无人飞行器（UAV）和可见光通信（VLC）技术的融合，为提供灵活的通信方式及高效的照明解决方案开辟了新途径。本文关注于如何在无人机辅助的VLC系统中进行有效的三维轨迹规划，特别是在需要从地面用户处收集数据的情况下。主要动机是开发一种能够最小化无人机飞行距离、从而最大化数据收集效率的轨迹规划框架。

Method: 首先基于特定的VLC信道增益阈值推导出一个最优飞行高度的闭式解。然后，通过将一种新颖的信息素驱动奖励机制与双延迟深度确定性策略梯度算法相结合的方式，对无人机的水平轨迹进行了优化。这种方法允许无人机在复杂的环境中采取自适应移动策略。

Result: 仿真结果显示，相较于基线方法，所得到的最佳飞行高度可以将飞行距离减少最多达35%。此外，提出的信息素驱动奖励机制大约能将收敛步骤缩短50%，显示出在无人机辅助VLC数据收集中显著的效率提升。

Conclusion: 本研究成功地为无人机辅助可见光通信系统的三维轨迹规划提供了一个有效的解决方案。通过优化飞行高度和采用先进的强化学习技术改进水平轨迹规划，不仅减少了无人机的飞行距离，还提高了整个系统的数据收集效率。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [54] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释AI的帕金森病预测框架SCOPE-PD，通过整合主观和客观评估来提供个性化的健康决策。利用多种机器学习技术对从帕金森进展标志物倡议研究中收集的数据进行处理，并采用SHAP分析法检查模型可解释性。随机森林算法在结合主观和客观测试数据特征的情况下达到了98.66%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）是一种受遗传、临床及生活方式因素影响的慢性复杂神经退行性疾病。由于传统诊断方法存在主观性问题，导致早期预测这种疾病具有挑战性且常被延误。尽管机器学习展示了支持PD诊断的潜力，但现有方法往往仅依赖于主观报告，缺乏个体化风险评估的可解释性。

Method: 本研究提出了SCOPE-PD，一种基于可解释AI的预测框架，它通过整合主观与客观评估为个人提供健康决策建议。该框架使用来自帕金森进展标记物倡议(PPMI)研究中的主观和客观临床评估数据构建了一个多模态预测模型。对这些数据应用了几种机器学习技术，并选择了最佳模型来解读结果。使用了SHAP为基础的方法来进行模型可解释性的考察。

Result: 随机森林算法在使用主客观测试数据组合特征时实现了最高的准确率，达到98.66%。根据MDS-UPDRS测试，震颤、运动迟缓以及面部表情被认为是预测PD贡献最大的三个特征。

Conclusion: SCOPE-PD框架通过结合主观和客观的评估手段，显著提高了帕金森病预测的准确性，并提供了对预测结果的深入理解。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [55] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文旨在通过构建一个极简基线和实验流程来解构强化学习微调中的设计选择，以揭示各种设计因素在学习和泛化动态中的作用，并识别出关键因素。


<details>
  <summary>Details</summary>
Motivation: 面对强化学习微调领域内研究的激增及由此带来的不一致结论问题，文章试图回答两个基本问题：每个设计选择的作用是什么？哪些是至关重要的？

Method: 首先建立了一个极简基线，包括每轮查询一次展开、仅使用结果奖励作为训练信号而不采用任何优势技巧以及设定批量大小为三十二。基于此基线，设计了实验流程来检验如优势、展开次数等因素的边际收益。

Result: 通过对三个基础模型和两个数据集进行实验，不仅揭示了不同设计选择对学习与泛化动力学的影响的新理解，还确定了一些值得更多关注的关键因素。

Conclusion: 本研究提供了一种系统的方法来评估强化学习微调过程中各种设计选择的重要性，有助于未来更加高效地改进算法性能。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [56] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 本文提出了一种在非平稳时间序列中学习延迟决策的方法，通过使用L2D-SLDS模型来模拟有符号的专家残差，并引入了一个基于IDS启发式的路由规则，实验结果表明该方法优于基于上下文的多臂赌博机基线和没有共享因子的情况。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决非平稳时间序列数据下，面对部分反馈及随时间变化的专业人士可用性问题时，如何有效地进行决策委托。

Method: 采用L2D-SLDS（一种因子化的切换线性-高斯状态空间模型）来建模带有上下文依赖制度转换、允许跨专家信息转移的全局共享因子以及每个专家特有的状态；并提出了一个受IDS启发的路由规则，以平衡预测成本与关于潜在制度和共享因素的信息增益。

Result: 实验结果显示，相比基于上下文的多臂赌博机基准方法和不考虑共享因子的情形，所提方法展现出了更好的性能。

Conclusion: 通过引入能够处理非平稳时间序列数据特性的新型模型L2D-SLDS及相应的决策策略，为动态环境下有效利用专家知识提供了新的解决方案。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [57] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 该论文提出了一种受人类学习机制启发的采样算法，结合了基于模型、无模型和情景控制三种模块，以实现大规模贝叶斯推理的有效探索。此方法在贝叶斯深度学习中特别有用，尤其是在不确定性量化方面。


<details>
  <summary>Details</summary>
Motivation: 受到人类通过不同神经系统有效学习环境的方式启发，本研究旨在阐明支撑这种生物效率背后的计算原则，并将其转化为一种适用于大规模贝叶斯推断的采样算法。

Method: 提出的算法由三个部分组成：一个使用目标分布进行引导但计算缓慢的基于模型的模块；一个利用先前样本学习参数空间中的模式，从而无需直接评估昂贵的目标分布就能快速抽样的无模型模块；以及一个能够通过回忆特定过去事件（即样本）支持快速抽样的情景控制模块。

Result: 这种方法促进了贝叶斯方法的发展，并使其更容易应用于大规模统计机器学习问题上。特别是，在强调准确且合理地量化不确定性的贝叶斯深度学习领域得到了应用。

Conclusion: 受人类学习机制启发设计的新颖采样算法，为解决大规模贝叶斯推理提供了一个有效的途径，尤其有利于需要精确不确定性量化的贝叶斯深度学习任务。

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [58] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 本文提出了一种模型无关的技术——守恒量校正，用于在深度学习模型中融入物理守恒标准，以提高自回归神经算子模型的长期稳定性，并指出了当前架构在处理高频成分方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 深度学习被提议作为数值近似PDE解的有效替代方案，但其解决方案由于自回归误差累积和无法保持物理量而难以长时间表现良好。

Method: 提出了守恒量校正技术，该技术可以在各种深度学习模型中应用，以改善模型对物理守恒条件的遵守。此外，从频谱领域分析了神经算子的表现，揭示了现有架构的重要限制。

Result: 结果表明，无论模型架构如何，所提出的守恒量校正方法都能一致地改进自回归神经算子模型的长期稳定性。同时，研究还强调未来工作需要考虑特别关注高频成分的架构设计。

Conclusion: 通过引入守恒量校正技术，可以显著增强基于深度学习的PDE解法器的长期预测能力；而且，为了更好地理解和建模湍流，未来的研究应该集中在能够更有效处理高频信息的新架构上。

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [59] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习框架FedDis，通过因果解耦来处理交通预测中的非独立同分布数据问题，实验证明其在四个真实世界基准数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习为保护隐私的交通预测提供了一个有前景的范式，但其性能经常受到非独立同分布（non-IID）的去中心化交通数据挑战。现有的联邦方法通常难以应对这种数据异质性，常常将全局共享模式与客户端特定的局部动态混合在一个单一表示中。本文认为这种异质性源于两种不同生成源的纠缠：客户端特定的局部动态和跨客户端的全局时空模式。

Method: FedDis框架采用了双分支设计，其中个性化银行学习捕捉客户端特定因素，而全局模式银行则提炼共同知识。为了确保两个分支之间的信息正交性，使用了互信息最小化目标来强制执行有效的解耦。

Result: 在四个真实世界的基准数据集上的综合实验表明，FedDis能够持续达到最先进性能，并且具有高效性和优越的扩展性。

Conclusion: FedDis通过有效解耦客户端特定因素与全局共性特征，解决了联邦学习中存在的数据异质性问题，从而促进了跨客户端的知识转移同时保持了对独特本地环境的高度适应性。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [60] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法MC-GRPO，通过使用中位数而非平均值作为基准线来解决小样本训练时优势符号翻转的问题，从而提高在资源受限情况下的训练稳定性和最终准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限、回放次数较少的情况下，现有的基于组相对策略优化的方法由于共享基线中的噪声导致了优势符号翻转问题，影响了训练的准确度。

Method: 提出了Median-Centered Group Relative Policy Optimization (MC-GRPO) 方法，通过采用中位数而不是平均数作为基线，并且每次生成一个额外的回放用于计算中位数，同时排除中位数样本对梯度更新的影响，保持每条提示贡献于梯度的样本数量不变。

Result: 实验表明，在不同的GRPO系列方法以及广泛的模型和规模上，该中位数中心化训练方法能够一致地提高低回放次数条件下训练的稳定性和最终准确性，将G=2与G=8之间的差距缩小到1%以内。

Conclusion: MC-GRPO为在有限回放预算下进行语言模型训练提供了一个简单有效的解决方案，它通过利用中位数减少异常奖励带来的影响，从而提高了训练过程中的稳定性及结果准确性。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [61] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: 本文提出了一种多图元变换器（MGMT）框架，用于解决不同拓扑、规模和语义的图之间的信息有效整合问题。通过在每个图上应用图变换编码器，并构建一个连接功能对齐超级节点的元图，实现了跨图学习。MGMT不仅在图级别预测任务中优于现有最先进模型，还提供了有助于科学发现的可解释表示。


<details>
  <summary>Details</summary>
Motivation: 多图学习对于从异构图集合中提取有意义的信息非常重要。然而，在缺乏共享节点身份的情况下，有效地整合不同拓扑结构、规模和语义的图之间信息仍然是一个重大挑战。

Method: 提出了多图元变换器（MGMT），一种统一、可扩展且可解释的跨图学习框架。该方法首先使用图变换编码器处理每个图，将结构和属性映射到共享潜在空间；然后通过注意力机制选择与任务相关的超级节点，并基于潜在空间中的相似性建立连接各图间功能对齐超级节点的元图；进一步在元图上添加图变换层以实现内部及跨图结构的联合推理。

Result: MGMT在合成数据集和真实世界神经科学应用中的表现表明，它在图级别预测任务方面始终优于现有的最先进模型，并提供促进科学发现的可解释表示。

Conclusion: 本研究确立了MGMT作为结构化多图学习的统一框架的地位，推进了图形数据起核心作用领域内的表征技术发展。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [62] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: 本文提出了一种名为Lethe的联邦遗忘方法，通过解耦需要遗忘的知识和需要保留的知识来确保在持续训练期间被删除的知识不会重新浮现。实验表明，Lethe能够以统一的方式支持联邦系统中各级别的知识遗忘，并且即使经过多轮后续训练后也保持了优秀的持久性（再现率在大多数情况下<1%）。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦遗忘研究通常假设协作随着遗忘操作结束，忽略了之后继续使用剩余数据进行联邦训练的情况。作者发现了一个关键问题：继续训练可以重新激活已被遗忘的知识，导致这些影响再次出现在全局模型中。为了解决这个问题，提出了Lethe方法。

Method: Lethe采用了一种重塑-修正-恢复流程：首先使用梯度上升法在待遗忘的数据上训练一个临时适配器以获得放大更新，然后将其作为校正信号，在两个流中对剩余更新执行逐层修正。最后移除适配器并对保留下来的数据执行简短的恢复阶段。

Result: 实验结果表明，Lethe能够在联邦系统中统一地支持所有级别的知识遗忘，并且即便经过大量轮次的后续训练后也能维持良好的持久性（多数情况下的再现率低于1%）。

Conclusion: Lethe作为一种新的联邦遗忘方法，有效地解决了知识再浮现的问题，保证了在连续训练过程中特定知识能够得到持久性的删除。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [63] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 本文介绍了一种名为共识机制的方法，作为注意力机制的替代方案，以稳定在高学习率下训练的变压器模型。通过文本、DNA和蛋白质模态的学习率扫描实证分析，证明了该方法能够提高稳定性。此外，还提出了一种结合共识与注意力机制的混合框架，在保持性能的同时进一步提升了稳定性，并提供了理论分析来描述共识机制的特性。


<details>
  <summary>Details</summary>
Motivation: 标准基于注意力机制的变压器在训练过程中，特别是在高学习率的情况下，容易出现因学习率设定不当而引发的不稳定性问题。尽管已经提出了多种修改优化过程的方法来增强对这种不当设定的抵抗能力，但从架构层面出发的根本性创新仍然较少被探索。

Method: 研究者们将共识机制作为一种直接替换注意力机制的方式引入到变压器中，旨在扩大有效学习率范围内的训练稳定性。他们不仅把共识机制形式化为一个图形模型，还进行了广泛的实证分析，展示其在不同数据模态下的优越稳定性表现。同时，研究也提出了一种融合共识-注意力机制的新框架，旨在维持原有性能的同时改善稳定性。

Result: 实验结果表明，共识机制确实能够在更广泛的有效学习率范围内稳定变压器的训练过程。对于文本、DNA以及蛋白质等多种类型的数据模态，共识机制都展现出了优于传统注意力机制的稳定性。提出的混合共识-注意力框架也在保持良好性能的前提下，进一步增强了模型训练时的稳定性。

Conclusion: 共识机制作为一种新的架构创新，为解决变压器模型在高学习率条件下训练不稳定的问题提供了一个有效的解决方案。它不仅扩大了可接受的学习率范围，还通过与注意力机制相结合的方式，在不影响模型性能的基础上提高了训练过程中的稳定性。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [64] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 本文提出了一种形式逻辑验证引导的框架，该框架将形式符号验证与自然语言生成过程动态交织在一起，以实时反馈检测和纠正错误。通过一种新的两阶段训练流程，使得7B和14B模型在六个基准测试中分别比最先进基线平均高出10.4%和14.2%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）展示了显著的能力，但它们随机的下一个词预测会产生逻辑不一致性和奖励黑客行为，这是正式符号系统所避免的问题。为了弥合这一差距，研究者引入了这种新方法来主动惩罚推理链中的中间谬误。

Method: 本文采用了一种新颖的两阶段训练流程，结合了形式逻辑验证指导下的监督微调和策略优化。

Result: 在涵盖数学、逻辑和一般推理的六个基准测试上，7B和14B模型的表现分别比现有最先进基线高出了平均10.4%和14.2%。

Conclusion: 这些结果证实了形式验证可以作为一种可扩展机制，显著推进先进的LLM推理性能边界。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [65] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文提出了一种名为GUDA的方法，用于评估视觉生成模型中训练数据组对给定输出的影响。通过机器卸载而非从头训练来近似每个反事实模型，并使用基于似然的评分规则（ELBO）之间的差异量化组影响。实验表明，GUDA比其他方法更可靠地识别主要贡献组，同时显著加快了处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅针对单个样本进行归因分析，而实践中往往需要了解群体层面（如艺术风格或对象类别）对模型输出的影响。直接通过移除某一组重训模型（LOGO）虽然直观但计算成本过高。

Method: 提出了GUDA方法，它利用机器卸载技术基于共享全数据模型近似模拟各反事实场景下的模型状态，然后通过比较完整模型与经卸载得到的反事实模型之间在ELBO上的差异来衡量不同数据组对模型预测结果的影响程度。

Result: 实验结果显示，在CIFAR-10和Stable Diffusion的艺术风格归属任务上，GUDA能够比语义相似性、基于梯度的归因以及实例级卸载方法更加可靠地识别出主要贡献的数据组，且在CIFAR-10上相比LOGO重训方法实现了约100倍的速度提升。

Conclusion: GUDA为视觉生成模型中的训练数据组归因提供了一个高效且准确的新途径，尤其适用于大规模数据集或多组别情况下快速定位关键影响因素。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [66] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 本文提出了一种无需使用任何验证数据的数据无关的早期停止框架，用于联邦学习方法中确定最佳停止点。通过仅使用服务器端参数监控任务向量的增长率来实现。实验结果表明，该方法在皮肤病变/血细胞分类上表现优于基于验证数据的早期停止方法，并且是首个不使用任何验证数据的联邦学习早期停止框架。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习中超参数调整依赖于固定的全局轮次或验证数据，这导致了高昂的计算成本和隐私风险问题。为了解决这个问题，提出了一个不依赖于验证数据的早期停止框架，旨在降低计算成本同时保护隐私。

Method: 提出的数据无关早期停止框架通过监控任务向量的增长速率来决定最优停止点，而这一过程只需要利用服务器端参数即可完成。

Result: 数值结果显示，在皮肤病变/血细胞分类任务上，所提方法与基于验证数据的早期停止策略相比，能够在平均47/20（皮肤病变/血细胞）轮次内达到超过12.5%/10.3%的性能提升。

Conclusion: 这项工作首次提出了一个不需要使用任何验证数据的联邦学习早期停止框架，并在多个最先进的联邦学习方法中展示了其有效性。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [67] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 本文从流映射的角度对一致性模型进行了理论分析，揭示了训练稳定性和收敛行为如何导致退化解。基于这些见解，重新审视了自我蒸馏作为一种实用的补救措施，并对其进行了重新表述以避免过大的梯度范数，从而实现稳定的优化。此外，还展示了该策略在基于扩散的策略学习中的应用，无需依赖预训练的扩散模型进行初始化。


<details>
  <summary>Details</summary>
Motivation: 尽管一致性模型在生成建模上取得了与扩散和流模型竞争的结果，但它们在从头开始训练时表现出固有的不稳定性且可重复性有限。先前的工作试图解释并稳定这些问题，但所提供的解释仍然零散，理论关系尚不清楚。

Method: 本文采用了一种基于流映射的方法来分析一致性模型，并通过这种联合分析澄清了训练稳定性和收敛行为如何导致退化解。接着，作者重新考虑了自我蒸馏作为解决某些形式次优收敛问题的实际方法，并对其进行重新定义以避免过度的梯度范数，进而促进稳定优化。

Result: 研究结果表明，所提出的方法不仅能够提高图像生成的一致性模型的稳定性，而且还能扩展应用于基于扩散的策略学习中，无需预先训练好的扩散模型来进行初始化。

Conclusion: 通过从流映射角度对一致性模型进行深入分析，本文为理解其训练过程中的不稳定性和收敛行为提供了新的视角。提出的改进版自我蒸馏法不仅有助于解决现有模型的一些局限性，还展示了其在更广泛的应用场景下的潜力。

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [68] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: 研究了基于Transformer的大规模语言模型在周期性泛化上的局限性，并通过构建Coper基准测试来评估这些模型在未见过的复合周期性上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的大规模语言模型虽然在多种任务上表现出色，但在分布外（OOD）泛化方面与人类相比仍有显著差距。本文旨在探讨这一差距，特别是模型在周期性这种基本的OOD场景下的表现。

Method: 从抽象代数和推理的角度统一解释了周期性，并引入了一个名为Coper的可控生成基准测试，该测试包含两种OOD设置：Hollow和Extrapolation，用于检验Transformer模型在复合周期性上的泛化能力。

Result: 实验表明，尽管Transformer能够在训练期间记忆周期性的数据，但它们无法很好地推广到未曾见过的复合周期性场景中。

Conclusion: 本研究表明了Transformer在周期性泛化方面的局限性，并提出了一个新的基准测试方法以支持未来的研究。此外，还公开了源代码以便进一步探索。

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [69] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 本文提出了一个数据质量度量库，用于实际测量医学机器学习任务中数据质量的维度，并为特定用例选择适当的数据质量度量提供了策略和决策树。通过PTB-XL ECG数据集展示了这种方法的影响。


<details>
  <summary>Details</summary>
Motivation: 为了提高医学领域中人工智能的信任度，需要量化AI模型训练和测试的数据质量。为此，作者们提出了一个系统化的框架来评估给定任务下数据的适用性（fit-for-purpose），并将其理论框架操作化。

Method: 提出了一组数据质量指标——度量库，用于实际衡量数据质量的不同方面。对于每个指标，都提供了一个包含定义、适用性、示例、潜在陷阱及建议等关键信息的度量卡片。此外，还讨论了如何根据特定应用场景从度量库中挑选合适的数据质量指标集的策略，并给出了相应的决策树。

Result: 通过PTB-XL ECG数据集作为案例研究，展示了所提方法在实践中对训练与测试数据进行适合性评估的作用。这标志着朝向建立医学领域可信赖的人工智能迈出了第一步。

Conclusion: 本研究开发的数据质量度量库及其相关指南有助于更好地理解和实施数据质量度量，从而促进医学领域内可信AI的发展。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [70] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的替代模型方法，使用CNN进行早期阶段的IR-drop估计，该方法可以快速准确地预测IR-drop分布，为设计者提供在昂贵的签核分析之前的快速IR-drop洞察。


<details>
  <summary>Details</summary>
Motivation: 传统的IR-drop分析依赖于基于物理的签核工具，虽然提供了高精度但计算成本高昂且需要近乎最终的布局信息，这使得它们不适合快速的早期设计探索。因此，作者提出了一个基于深度学习的方法来解决这个问题。

Method: 本研究采用了U-Net编码器-解码器架构，并结合了跳跃连接以有效捕捉布局中的局部和全局空间依赖性。任务被定义为密集像素级回归问题，其中物理布局特征直接映射到IR-drop热图上。训练数据集是由作者生成的一个受物理启发的合成数据集，它考虑了电源网格结构、单元密度分布和切换活动等关键物理因素。

Result: 实验结果表明，所提方法能够以毫秒级别的推理时间准确预测IR滴落分布，支持快速预签核筛选和迭代设计优化。性能评估采用诸如均方误差（MSE）和峰值信噪比（PSNR）等标准回归指标。

Conclusion: 提出的框架旨在作为一个补充性的早期分析工具，在昂贵的签核分析之前为设计者提供快速的IR-drop见解。实现代码、数据集生成脚本以及交互式推理应用程序已公开发布。

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [71] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 本文首次对LoRA变体进行了统一的研究，提供了系统分类、统一理论回顾、结构化代码库和标准化实证评估。研究结果表明，与其它超参数相比，LoRA及其变体对学习率的选择非常敏感；此外，在适当的超参数配置下，LoRA通常能够匹配或超越其大多数变体的表现。


<details>
  <summary>Details</summary>
Motivation: 随着LoRA变体的激增，方法论、理论、代码和评估方面出现了碎片化现象。为了解决这个问题，本文旨在提供一个关于LoRA变体的综合研究，包括系统分类、统一理论审视、结构化的代码基础以及标准化的经验性评价。

Method: 首先，根据秩、优化动态、初始化及与专家混合模型的集成四个主要轴线对LoRA变体进行分类。接着，在一个专注于低秩更新动态的共同理论框架内审查它们的关系和发展。引入了LoRAFactory，这是一个通过统一接口实现变体的模块化代码库，支持即插即用实验和细粒度分析。最后，利用这个代码库，在自然语言生成、理解以及图像分类任务上进行了大规模评估，系统地探索了关键超参数。

Result: 研究揭示了几个发现：相较于其他超参数，LoRA及其变体对学习率选择表现出显著的敏感性；而且，通过正确的超参数配置，LoRA通常可以匹配甚至超过大多数变体的表现。

Conclusion: 通过对LoRA变体进行全面而系统的分析，本研究表明尽管存在多种变体，但原始LoRA方法在适当调整后仍能提供具有竞争力的结果。此外，强调了在应用这些技术时考虑特定超参数如学习率的重要性。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [72] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 本文提出了一种利用视觉-语言模型（VLMs）的常识推理能力来提供可提示表示的方法，以区分可控变化和噪音。通过这种方法，在LAM训练中使用这些表示作为目标，并对多种流行的VLM进行了基准测试。结果表明，要求VLM忽略干扰因素可以显著提高潜在动作的质量，从而在Distracting MetaWorld上达到高达六倍的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在动作模型（LAMs）在处理含有与动作相关的干扰因素的观察时表现不佳，往往会编码噪声而非有意义的潜在动作。相反地，人类能够根据简短的任务描述轻松地区分视频中的任务相关运动与无关细节。因此，研究者希望通过利用视觉-语言模型（VLMs）的常识推理能力来改善这一问题。

Method: 提出的方法是利用视觉-语言模型（VLMs）生成可提示表示，以此从无监督的方式分离出可控的变化和噪声。这些表示被用作LAM训练期间的目标，并且对多个流行的VLM进行了广泛评估，以探索它们在不同提示和超参数下的表现差异。

Result: 实验发现，最近的一些VLM可能不如较老版本的表现好。此外，通过简单地指示VLM忽略干扰因素，可以大大改进潜在动作的质量，在Distracting MetaWorld环境中实现了最高达六倍的成功率提升。

Conclusion: 本研究表明，利用视觉-语言模型的常识推理能力可以有效改善潜在动作模型对于干扰因素的鲁棒性，进而提高了下游任务的成功率。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [73] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的量化方法LoRDS，通过低秩分解来提高大语言模型的量化效率和表达能力。这种方法不仅在量化精度上优于现有技术，而且在推理速度和下游任务性能上也有所提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）量化方法主要依赖于块状结构以保持效率，但牺牲了表示灵活性。作者旨在通过探索元素级量化的可能性，在不牺牲效率的前提下提供更高的表达能力。

Method: 提出了低秩分解缩放（LoRDS），一个统一框架，利用连续低秩矩阵建模缩放流形来重新思考量化粒度。该方法能够打破空间约束，支持高保真PTQ初始化、权重和缩放因子联合QAT以及高秩乘法PEFT适应。

Result: LoRDS方法在多种模型家族中均表现出色，特别是在Llama3-8B上实现了相比NormalFloat量化高达27.0%的准确率提升，并且在NVIDIA RTX 4090上达到了1.5倍的推理加速。此外，在下游任务上的PEFT表现比4比特QLoRA高出9.6%。

Conclusion: LoRDS为大语言模型的压缩与适应提供了稳健而综合的解决方案，证明了通过低秩分解实现的元素级量化可以在保持甚至超越传统块状量化方法效率的同时，提供更强的表现力。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [74] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文从Fenchel-Young框架出发，对Softmax损失函数及其家族进行了系统性研究。分析了不同替代损失函数在分类和排序指标上的一致性，并探讨了它们的梯度动态特性以揭示不同的收敛行为。同时提出了一种针对近似方法的系统偏差-方差分解，提供了收敛性保证，并进一步推导出每轮训练的复杂度分析，展示了效果与效率之间的明确权衡。实验结果表明一致性、收敛性和实际性能之间有很强的一致性。


<details>
  <summary>Details</summary>
Motivation: 为了阐明Softmax损失函数（一种广泛应用于分类和排名任务的目标函数）的理论性质，以及解决当类别数量非常大时面临的可扩展性问题。

Method: 采用Fenchel-Young框架来理解Softmax损失及其家族成员；通过分析不同替代损失函数对于分类和排名指标的一致性，以及它们的梯度动态特性；引入了一种针对近似方法的系统偏差-方差分解；进行了每轮训练的复杂度分析。

Result: 发现不同替代损失函数在一致性和收敛行为上存在差异；提出了能够提供收敛保证的系统偏差-方差分解；证明了在效果与效率之间存在明确权衡；实验证明了一致性、收敛性和实际性能之间高度相关。

Conclusion: 本研究为大规模类别的机器学习应用中的损失选择提供了坚实的理论基础和实用指南。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [75] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: 提出了OSNIP，一种轻量级客户端加密框架，通过在大模型的高维潜在空间中注入扰动来保护隐私，同时保持了语义保真度和强模型实用性。


<details>
  <summary>Details</summary>
Motivation: 为了在进行LLM推理时保护用户隐私，同时确保模型的实用性和安全性不受影响。

Method: 基于线性核几何直觉推广到LLM的高维潜在空间，定义了“混淆语义零空间”，并通过注入扰动将原始嵌入映射到此空间内；使用密钥依赖型随机映射为每个用户生成独特的扰动轨迹。

Result: 在12个生成与分类基准测试中的评估显示，OSNIP达到了最先进的性能，在严格的安全约束下大幅降低了攻击成功率的同时保持了强大的模型实用性。

Conclusion: OSNIP提供了一种有效的方法来增强LLM推理过程中的隐私保护，同时不牺牲模型性能。

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [76] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 研究了分子语言模型在预训练和下游任务中的扩展行为，揭示了分子表示对性能的重大影响，并公开发布了迄今为止最大的分子语言模型库。


<details>
  <summary>Details</summary>
Motivation: 探讨分子生成模型是否遵循可预测的缩放规律，这对于在模型大小、数据量和分子表示之间最优地分配资源至关重要。

Method: 通过训练300个模型并进行超过10,000次实验，在严格控制计算预算的同时独立变化模型大小、训练令牌数量和分子表示来系统地调查分子语言模型的扩展行为。

Result: 展示了分子模型在预训练和下游迁移中清晰的缩放规律，显示了分子表示对性能有重大影响，并解释了之前观察到的分子生成中缩放行为的不一致性。

Conclusion: 这项研究为理解分子语言模型如何根据不同的参数设置而扩展提供了新的见解，并通过发布一个大型的开源模型库支持未来的研究和发展。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [77] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 本文建立了稀疏注意力机制与紧支撑核之间的正式对应关系，揭示了常用的非参数密度估计核（如Epanechnikov、双权重和三权重）与$α$-entmax注意力之间的联系，并通过实验展示了基于核的稀疏注意力在语言模型等任务中的竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有工作发现了transformer中的自注意力机制与通过Nadaraya-Watson估计器进行测试时核回归之间的联系，但缺乏对稀疏注意力机制从核理论角度的理解。

Method: 建立稀疏注意力与紧支撑核之间的正式关联，展示标准化ReLU和sparsemax注意力分别来自于固定和自适应规范化下的Epanechnikov核回归；更广泛地，证明了非参数密度估计中常用的核与$α$-entmax注意力之间的联系，其中$α= 1 + \frac{1}{n}$对于$n \in \mathbb{N}$而言，并且softmax/高斯关系在极限$n \to \infty$时出现。

Result: 为注意力机制的设计提供了一个原则性的框架，解释了如何从核设计中自然产生稀疏性，并提供了启发式top-$k$注意力和其他联想记忆机制的有原则的替代方案。实验表明，基于核的稀疏注意力在语言建模、上下文学习和长度泛化任务上具有竞争力。

Conclusion: 这项研究为理解稀疏注意力机制提供了新的视角，并提出了基于核方法作为设计注意力机制的原则性框架，显示出在多种任务上的良好性能。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [78] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种无裁剪策略优化方法（CFPO），通过使用基于总变异散度约束的凸二次惩罚来替代传统的裁剪机制，解决了强化学习在大规模语言模型后训练中遇到的优化问题。该方法不仅保持了与裁剪基线方法相当的表现，还提高了训练稳定性、减少了冗长性利用和能力退化现象，同时仅需对现有代码进行最小修改即可实现。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法在用于大型语言模型后训练时依赖于可能引起零梯度区域、奖励操纵以及训练不稳定等问题的裁剪机制。为解决这些问题，作者提出了不依赖裁剪的方法。

Method: 提出Clipping-Free Policy Optimization (CFPO)，采用源自总变异散度约束条件下的凸二次惩罚项代替启发式裁剪手段，从而构建了一个处处可微的目标函数，确保策略更新过程中的稳定性而不设硬边界。

Result: 实验表明，在推理任务上，CFPO能够达到与基于裁剪的方法相媲美的基准测试结果，并扩展了稳定训练范围；在对齐任务上，它有效缓解了冗余性利用问题，降低了性能下降幅度，同时保持了良好的指令遵循能力。

Conclusion: 研究结果表明，对于大型语言模型的后训练阶段而言，CFPO是一种有前景的替代方案，可以作为基于裁剪方法的有效替代品，只需简单的代码更改且无需额外超参数调整。

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [79] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量标准B，用于衡量边界质量，并基于此提出了Sombrero方法，该方法通过置信度对齐边界损失来指导边界放置，并通过对输入级别应用置信度加权平滑来稳定边界学习。实验结果表明，在多种UTF-8语料库上，Sombrero改进了准确率与效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然最近的端到端方法仅通过语言建模目标就能学到有意义的边界，但要定量评估并系统地指导计算资源如何分配仍然很困难。因此，研究者引入了一个与路由器无关的边界质量度量标准（B），以衡量块开始处对于高下一个字节意外性的集中程度，并据此开发出一种新的方法来优化边界定位及提高模型性能。

Method: 研究者们首先定义了一个新的指标——边界丰富度B，用以量化边界划分的质量。接着，他们基于这一度量提出了名为Sombrero的新方法，该方法利用置信度对齐边界损失来引导边界的放置，并通过在输入层面而非已实现的块上施加置信度加权平滑技术来增强边界学习过程的稳定性。

Result: 在10亿规模的数据集上进行测试时，无论是在英文、德文文本还是代码和数学内容等不同类型的UTF-8语料库中，Sombrero方法均表现出了更好的准确率与效率之间的平衡，并且能够更一致地将计算资源集中在难以预测的位置上。

Conclusion: 本研究表明，通过引入新的边界质量度量标准以及采用Sombrero方法，可以有效地改善层次序列模型中的边界定位问题，从而使得模型能够在保持较高准确性的同时更加高效地处理长序列数据。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [80] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出了一种新的量化训练方法MS-EDEN以及全NVFP4量化方案Quartet II，用于提高NVIDIA Blackwell GPU上支持的NVFP4低精度格式在大规模模型预训练时的表现。相比现有的量化训练方法，它实现了更准确的梯度估计，并且在执行速度上达到了最高4.2倍于BF16的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有针对NVFP4格式的量化训练方法为了实现更精确无偏的随机舍入量化梯度估计而牺牲了部分表示能力，导致相对于标准FP16和FP8训练存在明显的准确性损失。为了解决这一问题并充分利用NVFP4格式的能力进行大规模模型如LLMs的端到端全量化预训练。

Method: 开发了一种名为MS-EDEN的新颖无偏量化例程，专门针对微缩格式设计，其量化误差比随机舍入降低了超过两倍。将MS-EDEN整合进了一个新的适用于线性层的全NVFP4量化方案——Quartet II中。通过分析证明Quartet II在所有主要矩阵乘法过程中，无论是前向还是反向传播，都能获得持续更好的梯度估计结果。

Result: Quartet II不仅在理论分析上表现优异，在实际应用中也得到了验证，通过对多达1.9亿参数的大规模语言模型进行了端到端训练测试，使用了38亿个token的数据集。同时提供了可在NVIDIA Blackwell GPU上运行的内核实现，相较于BF16格式，加速比可达至4.2倍。

Conclusion: 本研究提出的MS-EDEN与Quartet II方案显著提高了NVFP4格式下的量化训练性能，不仅减少了量化误差还提升了计算效率，使得利用低精度格式进行高效、高质量的大规模模型训练成为可能。此外，该方法还能够很好地与最近旨在优化NVFP4训练效果的技术改进相结合。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [81] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 本文提出了一种新的级联方法，用于生成包含离散和连续特征的表格数据，特别是针对混合类型特征。该方法首先生成一个低分辨率版本的数据行，然后通过一种新颖的引导条件概率路径和数据依赖耦合在高分辨率流匹配模型中利用这些信息。结果表明，此模型生成的样本更加真实，并且更准确地捕捉到了分布细节。


<details>
  <summary>Details</summary>
Motivation: 尽管生成建模领域取得了进展，但生成同时包含离散状态和连续分布的单一特征（即混合类型特征）仍然具有挑战性。

Method: 采用级联方法，首先生成仅包含纯分类特征以及数值特征粗略分类表示的表格数据行的低分辨率版本；接着，在高分辨率流匹配模型中通过新的引导条件概率路径与数据相关联结来利用上述信息。对于数值特征的低分辨率表示明确考虑了离散结果，如缺失或膨胀值，从而能够更忠实地生成混合类型特征。

Result: 实验结果表明，该模型生成的样本显著更加真实，对分布细节的捕捉也更为准确，例如检测分数提高了40%。

Conclusion: 本文介绍的新级联方法在生成混合类型特征方面表现优异，不仅提高了样本的真实性，还更好地保留了原始数据中的分布特性。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [82] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: 提出了一种名为MetaDrug的多层次、不确定性感知的元学习框架，旨在解决电子健康记录(EHR)中针对新患者的药物推荐冷启动问题。通过自我适应和同侪适应机制，结合不确定性量化模块来提高推荐准确性。实验表明，在MIMIC-III和急性肾损伤(AKI)数据集上，MetaDrug在处理冷启动患者时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的药物推荐方法难以有效应对新患者（即用户冷启动问题），因为缺乏足够的处方历史来进行准确的患者画像构建。尽管之前的研究利用医学知识图谱来连接药物概念，但这些方法主要集中在缓解物品冷启动问题上，并未能充分提供能够适应个体患者特征的个性化推荐。元学习虽然展示了处理稀疏交互的新用户的潜力，但在EHRs中的应用仍因数据的独特顺序结构而未被充分探索。

Method: 开发了MetaDrug框架，该框架包括两个层次的元适应机制：自我适应，使用患者自身的医疗事件作为支持集以捕捉时间依赖性；以及同侪适应，利用相似访问从同伴患者那里丰富新患者表示。此外，引入了一个不确定性量化模块，用于对支持访问进行排序并过滤无关信息，以确保适应一致性。

Result: 在MIMIC-III和AKI数据集上的实验结果表明，对于冷启动患者而言，MetaDrug在药物推荐方面始终优于最新的方法。

Conclusion: 研究成功地提出了一个创新性的解决方案——MetaDrug框架，它不仅解决了EHR领域内针对新患者药物推荐的冷启动挑战，而且还通过其独特的双层元适应机制与不确定性管理能力提高了推荐系统的性能。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [83] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，称为风格条件隐式Q学习（SCIQL），用于在离线强化学习中通过子轨迹标记函数来学习风格条件策略。该方法结合了离线目标条件强化学习技术和一种新的门控优势加权回归机制，以优化任务性能同时保持风格一致性。实验表明，SCIQL在两个目标上都优于先前的离线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法虽然引入了许多风格定义，但往往无法有效地调和风格与奖励之间的内在冲突。为了应对这些挑战，作者提出了行为风格的统一定义，并将其转化为一个实用框架。

Method: 作者介绍了风格条件隐式Q学习（SCIQL），该方法利用了诸如后见之明重标记和价值学习等离线目标条件RL技术，并结合了一种新的门控优势加权回归机制，以有效优化任务表现同时维持风格的一致性。

Result: 实验结果表明，SCIQL在任务性能和风格一致性这两个目标上都比之前的离线方法表现得更好。

Conclusion: 通过提出SCIQL这一新方法，研究者们成功地在离线强化学习环境中解决了风格与任务性能之间难以协调的问题，为未来的研究提供了有价值的参考。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [84] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，通过将单个LoRA模块重构为可分解的Rank-1专家池，并引入激活引导正交(AGO)损失来解决视觉-语言模型中的持续学习挑战。该方法减少了参数更新次数，实现了领域感知学习，同时最小化任务间干扰并保持下游任务性能。实验表明，该方法在所有指标上均达到最先进水平，与基线方法相比，可训练参数减少了96.7%，并且不需要依赖外部数据集或任务ID判别器。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）对于视觉-语言模型来说，在改进任务适应性和避免灾难性遗忘方面面临重大挑战。现有的方法通常存在较大的推理负担或依赖于外部知识。尽管低秩适应（LoRA）显示了通过实现高效的参数调整来减少这些问题的潜力，但直接使用LoRA缓解灾难性遗忘问题并不简单。

Method: 研究者们介绍了一种新框架，它将单一LoRA模块重构为一个可分解的Rank-1专家池。此方法能够根据[CLS]令牌的语义从专家池中选择，以动态组合稀疏的任务特定更新。此外，还提出了激活引导正交(AGO)损失，用于跨任务的关键部分LoRA权重正交化。

Result: 广泛的实验表明，该方法在所有度量标准上都达到了最先进的结果，超越了零样本泛化的上限。值得注意的是，与基准方法相比，这种方法减少了96.7%的可训练参数，并且不依赖于外部数据集或任务ID区分器。合并后的LoRAs保留了较少的权重且不会产生推理延迟，使得该方法计算起来非常轻量级。

Conclusion: 提出的方法通过稀疏组合和正交化技术有效地解决了视觉-语言模型中的持续学习难题，实现了更好的任务适应性和更低的灾难性遗忘风险。其在减少参数更新、提高领域感知能力以及保持下游任务性能方面表现优异。

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [85] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，通过在预训练的自动编码器中加入等变损失来改进时间序列生成的质量。这种方法不仅保持了潜流模型的计算优势，还在多个真实数据集上证明了其优越性，特别是在生成速度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然基于流的模型在时间序列生成方面已经取得了成功，尤其是在定义于低维潜在空间时能够实现高效采样，但对于如何为时间序列生成建模设计具有理想等变性质的潜在表示仍缺乏足够的探索。

Method: 作者提出了一个潜在流匹配框架，在该框架下通过简单地对预训练的自动编码器进行正则化来明确鼓励等变性。特别地，他们引入了一个等变损失函数，强制信号变换与其重建之间的一致性，并利用它根据基本的时间序列变换（如平移和幅度缩放）微调潜在空间。

Result: 实验表明，这种加入了等变性正则化的潜在空间提高了生成质量的同时也保留了潜流模型的计算效率优点。在多个真实世界数据集上的测试显示，本方法在标准时间序列生成度量上始终优于现有的基于扩散的方法基线，同时实现了数量级更快的采样速度。

Conclusion: 将几何归纳偏置融入到时间序列的潜在生成模型中可以带来实际的好处，包括提高生成质量和加速采样过程。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [86] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了一种名为OptiMAG的框架，该框架基于不平衡最优传输正则化来解决多模态属性图(MAGs)中隐含语义结构与显式图结构之间的差异问题。通过采用Fused Gromov-Wasserstein距离和KL散度惩罚，OptiMAG能够有效缓解跨模态结构-语义冲突，并且可以无缝集成到现有的多模态图模型中。实验表明，OptiMAG在多个任务上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在多模态属性图(MAGs)中存在一个被观察到的问题：由不同模态嵌入诱导出的隐含语义结构与显式的图结构之间存在着差异。这种差异导致了现有方法在固定显式图结构上执行消息传递时可能会聚合不相似特征，引入特定于模态的噪声，阻碍有效的节点表示学习。

Method: 提出了OptiMAG框架，它使用基于不平衡最优传输的方法来规范多模态图的学习过程。具体来说，通过利用Fused Gromov-Wasserstein距离来指导局部邻域内的跨模态结构一致性，并通过KL散度惩罚来适应性地处理跨模态不一致。

Result: 实验结果表明，在从以图为中心的任务（如节点分类、链接预测）到以多模态为中心的生成任务（如graph2text, graph2image）等多种任务上，OptiMAG相比基准方法表现出了一致的优势。

Conclusion: OptiMAG为提高多模态属性图中的节点表示学习提供了一个新颖而有效的解决方案，不仅能够缓解由于不同模态间结构-语义冲突带来的问题，还能作为即插即用型正则化器增强现有模型性能。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [87] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: 提出了一种名为PlatoLTL的新方法，通过将命题视为参数化谓词的实例而非离散符号来处理，从而允许策略在相关命题间学习共享结构，实现了跨LTL公式结构和命题的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LTL的多任务强化学习方法虽然能够在LTL规范之间成功泛化，但无法泛化到未见过的命题词汇表上。研究旨在开发一种新的方法，使策略能够不仅在LTL公式结构上组合地泛化，而且还能在命题参数上进行泛化。

Method: 通过引入PlatoLTL，该方法将命题当作参数化的谓词实例而不是单独的符号来对待，这样可以让策略从相关的命题中学习到共通的结构。设计了一种新架构，用于嵌入和组合谓词以表示LTL规范。

Result: 实验结果表明，PlatoLTL能够在具有挑战性的环境中对新颖的命题及任务实现成功的零样本泛化。

Conclusion: PlatoLTL提供了一种有效的方法，增强了多任务强化学习场景下对于未见命题及其构成的任务的泛化能力。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [88] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 本文提出了一种基于正则化的校准方法，通过预排序函数在多变量分布回归模型的训练过程中强制实现多变量校准，并引入了一种新的PCA预排序方法。实验表明，该方法显著提高了多变量预排序校准性能，同时不牺牲预测准确性，并且PCA预排序能够揭示现有预排序未能检测到的依赖结构错配问题。


<details>
  <summary>Details</summary>
Motivation: 尽管在单变量设定下取得了显著进展，但实现多变量校准仍然具有挑战性。已有的工作介绍了预排序函数作为评估多变量校准特定方面的灵活诊断工具，但其使用主要限于事后评估。

Method: 提出了一个基于正则化的校准方法，在训练多变量分布回归模型时利用预排序函数来确保多变量校准；此外，还介绍了一种新颖的基于PCA的预排序方法，将预测投影到预测分布的主要方向上。

Result: 通过对18个真实世界多输出回归数据集进行仿真研究和实验，证明了所提方法能够在不损害预测准确性的前提下大幅度提高多变量预排序校准度，而且PCA预排序可以发现现有预排序未检测到的依赖结构误指情况。

Conclusion: 本研究表明，通过在训练阶段采用基于正则化的校准方法及新型PCA预排序技术，可以在保持预测准确性的同时有效提升多变量预测模型的校准质量。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [89] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 本文提出了一种单树贝叶斯模型，该模型通过在每个叶子上装备GP预测器来扩展VSPYCT。这种模型能够提高回归任务中的预测性能，特别是在外推情况下。


<details>
  <summary>Details</summary>
Motivation: 决策树因其可解释性和效率而被广泛使用，但在需要可靠外推和良好校准的不确定性回归任务中表现不佳。分段常数叶预测受训练目标的限制，在分布变化时往往变得过于自信。

Method: 提出了一种单树贝叶斯模型，它通过给每个叶子配备一个GP预测器来扩展VSPYCT。贝叶斯斜切为输入空间提供感知不确定性的划分，而GP叶子则模拟局部功能行为，并允许在观察到的目标范围之外进行有原则的外推。研究还介绍了一种有效的推理和预测方案，结合了分裂参数的后验采样与GP后验预测，以及当输入超出叶子的训练支持时激活基于GP的外推机制。

Result: 实验表明，在基准回归任务上，与标准变分斜树相比，预测性能有所提高，在外推场景下性能显著提升。

Conclusion: 通过引入GP叶子，本研究提出的单树贝叶斯模型能够在保持决策树结构优势的同时，改善对于不确定性估计的能力，并且在外推场景下的表现尤为突出。

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [90] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: 提出了一种新的灵活低秩适应框架FlexLoRA，通过谱能量熵评估矩阵重要性，支持全局预算下的秩剪枝和扩展，并采用零影响初始化以确保稳定性，从而克服了现有方法在粒度、灵活性和稳定性方面的局限。实验表明，FlexLoRa在多个基准测试中优于最先进基线。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模预训练模型在不同领域取得了显著成功，但完全微调带来了巨大的计算和内存成本问题。虽然参数高效微调（PEFT）已经成为主流范式，特别是低秩适应（LoRA）展示了强大的性能，但其固定秩设计限制了灵活性。动态秩分配方法试图通过修剪冗余方向来缓解这一问题，但它们通常依赖于启发式的元素级指标进行全局排序而没有矩阵级别的区分，并且缺乏对需要额外适应的层增加容量的机制。

Method: 提出了FlexLoRA，一种基于熵指导的灵活低秩适应框架，该框架能够：(i) 通过谱能量熵评估矩阵的重要性；(ii) 在全局预算下支持秩的剪枝与扩展；(iii) 对新增加的奇异方向采用零影响初始化以确保稳定性。

Result: 广泛的实验表明，FlexLoRA在多个基准测试中始终优于最先进的基线方法。

Conclusion: FlexLoRA提供了一个更加原则性的解决方案用于参数高效微调（PEFT），解决了粒度、灵活性和稳定性方面的限制。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [91] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 本研究探讨了一类具有非光滑差凸（DC）正则化项的采样问题，通过利用Moreau包络平滑处理DC函数，并将正则化项的凹部分重新分配给数据保真度项，提出了对应的近端朗之万算法（DC-LA）。在一定的假设条件下，证明了该算法收敛于目标分布。此外，数值实验表明DC-LA不仅在合成设置中生成准确的分布，还能在真实世界的计算机断层扫描应用中可靠地提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决一类特殊的采样问题，其中的目标分布由一个Lipschitz光滑的数据保真度项和一个非光滑差凸(DC)形式的正则化项组成。通过针对DC结构采取特定方法，旨在开发一种新的采样算法，以提高对于非对数凹采样问题的理解与解决方案的有效性。

Method: 采用的方法包括：首先，通过向$r_1$和$r_2$分别应用Moreau包络来平滑处理非光滑的正则化项$r=r_1-r_2$；接着，在DC编程框架内，将正则化项中的凹成分重新分配给数据保真度项；最后，基于上述调整提出了一种新的近端朗之万算法（称为DC-LA），并对其在$q$-Wasserstein距离下的收敛性进行了理论分析。

Result: 结果表明，在假设$V$是远散射的情况下，所提出的DC-LA算法能够收敛到目标分布$\pi$，同时考虑到离散化误差和平滑误差。相较于先前的工作，这项研究在更广泛的框架及假设下改善了非对数凹采样的表现。此外，数值实验进一步验证了DC-LA在合成场景以及实际CT应用中生成准确分布的能力及其在不确定性量化方面的可靠性。

Conclusion: 结论指出，通过利用DC函数结构并对现有朗之万算法进行改进而得到的新算法DC-LA，在理论上被证明能够有效逼近目标分布，并且在实践中表现出色，特别是在复杂分布的采样以及不确定性量化方面。

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [92] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: 本文提出了一种名为ECTR的新方法，通过结合环境条件尾部重加权和基于总变差的不变学习来解决分布外泛化问题。该方法不仅考虑了跨环境的相关性变化，还处理了环境内部样本级别的异质性，从而在混合分布偏移的情况下提高了OOD性能。


<details>
  <summary>Details</summary>
Motivation: 当前的不变风险最小化方法主要解决了环境级别的虚假相关性问题，但往往忽略了环境内部样本级别的异质性，这对分布外(OOD)表现有重要影响。为了同时解决由不同环境间相关性变化及罕见或难样本导致的多样性变化带来的挑战，需要一种能够同时处理这两种类型分布变化的方法。

Method: 提出了一个名为环境条件尾部重加权用于总变差不变风险最小化的统一框架（ECTR）。该方法增强了基于TV的不变学习，并引入了根据环境条件调整权重的机制以共同应对两种类型的分布转移。此外，对于没有明确环境注释的情况，通过极小极大公式推断潜在环境进一步扩展了此框架的应用范围。

Result: 在回归、表格数据、时间序列以及图像分类基准测试中，在面临混合分布偏移时，ECTR方法展现出一致性的改进，特别是在最差环境和平均OOD性能方面。

Conclusion: ECTR提供了一种有效的方法来提高模型在遇到跨环境相关性和环境内样本异质性双重挑战时的分布外泛化能力。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [93] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文通过理论分析证明了困惑度（perplexity）可能不是一个合适的模型选择指标，因为存在某些序列虽然被紧凑的解码器-only Transformer模型自信且准确地预测，但同时也暗示了存在另一些具有非常低困惑度却不能被该模型正确预测的序列。此外，等困惑度图的分析研究表明，仅增加模型的信心并不足以使其被选为更准确的模型；信心的提高必须伴随着准确性的相应提升。


<details>
  <summary>Details</summary>
Motivation: 困惑度作为衡量模型遇到特定输出时整体“惊讶”程度的一个函数，在近年来受到了广泛关注，不仅作为损失函数，还作为一种简单易算的模型质量度量标准。然而先前的研究已经从经验角度指出了困惑度的一些局限性。本文旨在通过严谨的方法论来展示为什么困惑度可能不适合用作模型选择的标准。

Method: 本文利用了关于Transformer连续性的最新研究成果，采用理论证明的方式，展示了如果一个紧凑的解码器-only Transformer模型能够对某一序列做出准确且自信的预测（这是强泛化的必要前提），那么必定存在另一个序列，其困惑度很低，但该模型对其预测不准确。同时，通过对等困惑度图表进行分析研究，进一步探讨了困惑度在模型选择中的不足之处。

Result: 研究结果表明，即使某个模型对于一些序列有着很高的预测准确性和自信心，这也意味着存在其他序列，这些序列虽然困惑度很低，但却不能被这个模型准确预测。此外，等困惑度图的分析显示，只有当新模型的准确性与信心同步增长时，它才更有可能被选中。

Conclusion: 基于上述发现，本文强调了使用困惑度作为唯一或主要模型选择依据时存在的潜在问题，并建议在评估和选择模型时考虑更多维度的性能指标，以确保选出真正具有强大泛化能力的模型。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [94] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文解决了线性bandit中Nash遗憾的次优界问题，提出了新的分析工具来实现最优阶Nash遗憾界。此外，还引入了p-均值遗憾的概念，并开发了一个通用算法框架FairLinBandit，该框架可以与任何线性bandit策略结合使用。实验表明所提出的方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 受到Nash社会福利目标的启发，Nash遗憾作为随机多臂赌博机的一个公平感知性能指标最近开始出现。虽然这个概念已经被扩展到线性赌博机上，但是现有的结果由于依赖于限制性的集中不等式证明技术，在环境维度d上存在次优性。本文旨在解决这个问题，并进一步研究线性赌博机中的p-均值遗憾，这为公平性和效用目标之间提供了一个统一的框架。

Method: 作者引入了新的分析工具以在线性赌博机中获得最优阶Nash遗憾界。同时，他们提出了一个名为FairLinBandit的通用算法框架，它可作为一个元算法应用于任何线性赌博机策略之上。为了验证方法的有效性，作者使用了两个具体的赌博机算法：分阶段消除法和置信区间上限法，并证明两者在整个p值范围内都能达到次线性的p-均值遗憾。

Result: 通过在基于真实世界数据集生成的线性赌博机实例上进行广泛实验，结果显示所提出的方法在各种情况下都一致地优于当前最先进的基线方法。

Conclusion: 本研究不仅改进了线性赌博机中Nash遗憾的理论界限，而且通过引入p-均值遗憾的概念及相应的算法框架，为平衡公平性和效率提供了新的视角。实验结果证实了新方法的有效性和优越性。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [95] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法PAVE，通过稳定评论家的行动梯度场来解决策略学习中出现的不平滑问题，而无需直接修改执行者。实验结果表明，PAVE在保持任务性能的同时实现了与现有策略平滑正则化方法相媲美的平滑性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前连续动作-评价方法学习到的策略往往表现出不规则、高频振荡，不适合实际应用。现有的解决方案试图通过对策略输出进行直接正则化来强制平滑性，但这种方法治标不治本。

Method: 作者们从理论上证明了策略的非平滑性主要由评价者的微分几何决定，并提出了PAVE（Policy-Aware Value-field Equalization）框架，该框架将评价者视为标量场并通过最小化Q梯度波动同时保持局部曲率来稳定其诱导的动作梯度场。

Result: 实验结果显示，PAVE能够在维持竞争力的任务表现下实现与针对策略端平滑处理方法相当的平滑度和鲁棒性，而且整个过程中不需要对执行者做出任何改变。

Conclusion: 研究指出，通过专注于调整评价者而非直接干预策略本身，可以有效提升所学策略的平滑度及稳定性。PAVE为改进强化学习中的策略平滑提供了一个新颖且有效的途径。

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [96] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 本文提出了一种端到端可学习的排列框架，通过引入可学习的排列成本矩阵、可微分的二分匹配求解器以及稀疏优化损失函数来直接优化排列操作符，从而改善了Transformer架构中的模型剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer架构规模的增长，排列搜索空间呈指数级增长，迫使大多数方法依赖于贪婪或启发式算法，限制了重新排序的有效性。为了解决这一问题，作者提出了一个新颖的端到终学习排列框架。

Method: 该方法包括引入一个可学习的排列成本矩阵以量化给定权重矩阵中任意两个输入通道交换的成本；使用一个可微分的二分匹配求解器根据成本矩阵获得最优的二进制排列矩阵；以及设计了一个稀疏优化损失函数来直接优化排列操作。

Result: 实验结果表明，该方法在视觉和语言Transformers上实现了最先进的排列结果，对于结构化稀疏性特别有效。

Conclusion: 提出的端到端可学习排列框架能够有效地提高基于结构化稀疏性的模型剪枝性能，在多种类型的Transformer模型中展示了其优越性。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [97] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: 提出了一种针对离散扩散语言模型的解码引导水印方法dgMARK，通过调整解码顺序来嵌入水印，同时保持与常见解码策略兼容，并能抵抗后编辑操作的影响。


<details>
  <summary>Details</summary>
Motivation: 由于离散扩散语言模型（dLLMs）在生成令牌时可以采用任意顺序，而实际应用中的dLLMs对这一顺序高度敏感，这为水印技术提供了新的可能性。

Method: dgMARK利用一种基于二进制哈希诱导的简单奇偶校验约束，将解码过程中未遮掩位置的选择导向那些高奖励候选令牌满足该约束的位置，从而实现水印嵌入。此方法不直接修改模型学习到的概率分布，而是通过调整解码顺序来间接影响结果。此外，它支持多种解码策略，并可通过一步前瞻变体进一步增强。

Result: 实验表明，通过提高奇偶匹配统计数据可有效检测到水印的存在；滑动窗口检测器保证了即使面对插入、删除、替换及改写等后编辑操作，水印依然能够被准确识别。

Conclusion: dgMARK作为一种新颖的水印解决方案，不仅适用于离散扩散语言模型，而且能够在保持文本自然性的同时提供强大的版权保护能力。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [98] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: 本文提出了VaR-CPO算法，它能够直接优化风险价值约束，并在训练过程中实现零约束违反。


<details>
  <summary>Details</summary>
Motivation: 为了直接优化风险价值（VaR）约束，并且在探索过程中保持安全性，即在可行的环境中训练时达到零约束违反。

Method: 通过使用单边切比雪夫不等式来处理VaR约束的非可微性问题，从而基于成本回报的前两个矩得到一个易于处理的替代函数。此外，还扩展了CPO方法的信任区域框架，以提供严格的最坏情况下的策略改进和约束违反界限。

Result: 实证表明，VaR-CPO能够在安全探索的同时，在训练期间于可行环境中实现零约束违反，这是基线方法未能做到的关键属性。

Conclusion: VaR-CPO算法为需要满足特定风险价值约束条件下的策略优化提供了一种样本高效且保守的方法论。

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [99] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 本研究提出了一种新的优化器Mano，通过将动量投影到模型参数的切空间并约束在旋转斜流形上，解决了现有优化器AdamW和Muon的局限性。实验表明，Mano不仅在性能上优于AdamW和Muon，而且在内存消耗和计算复杂度方面也更低。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）代表了人工智能的重大进步，但其硬件和计算成本非常高昂。当前领先的优化器如AdamW忽视了结构属性，而Muon虽然使用全局谱归一化但丢失了曲率信息。此外，传统的流形优化方法由于在大规模模型优化中的表现不佳而被忽略。因此，研究旨在开发一种既能克服上述限制又能有效用于训练LLMs的新优化方法。

Method: 研究者们重新审视了流形优化方法，并通过创新地将动量投影至模型参数的切空间，并且将其约束在一个旋转斜流形上来设计了一种名为Mano的新优化器。这种方法旨在保留重要的曲率信息同时利用结构特性来提高优化效率。

Result: 通过对LLaMA和Qwen3模型进行广泛的实验，结果显示Mano相较于AdamW和Muon，在保持较低内存消耗和计算复杂度的同时，依然能够显著且一致地表现出更优的性能。这表明Mano在空间与时间效率上都扩展了帕累托前沿。

Conclusion: Mano优化器成功地填补了流形优化与现代优化技术之间的性能差距，为高效训练大型语言模型提供了新途径。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [100] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出了一种名为连续约束插值（CCI）的统一优化框架，该框架将加权行为克隆、密度正则化或支持约束这三种约束家族作为特殊情况包含在内，并基于此开发了自动约束策略优化（ACPO）算法，在D4RL和NeoRL2实验中展示了强大的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习方法通常只采用一种约束形式，如加权行为克隆、密度正则化或支持约束，缺乏一个统一的原则来解释这些不同约束之间的联系或权衡。

Method: 提出了连续约束插值（CCI）框架，它通过单一插值参数允许不同类型约束间的平滑过渡和原则性组合。基于CCI，进一步开发了自动约束策略优化（ACPO）算法，该算法利用拉格朗日对偶更新来适应插值参数。此外，还建立了最大熵性能差异引理，并为闭式最优策略及其参数投影导出了性能下界。

Result: 在D4RL和NeoRL2上的实验表明，所提方法在各种领域中表现出稳健的增益，整体上达到了最先进水平。

Conclusion: 连续约束插值（CCI）提供了一个统一视角来看待不同的约束类型，而自动约束策略优化（ACPO）算法能够有效地调整这些约束，从而在离线强化学习任务中实现卓越的表现。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [101] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 本研究提出了一种使用仅两个表面肌电图(sEMG)通道的深度学习框架，用于准确的手势识别。该方法采用卷积稀疏自动编码器(CSAE)直接从原始信号中提取时间特征表示，无需启发式特征工程。对于6类手势集，模型达到了94.3%±0.3%的多主体F1分数。此外，通过少量校准数据，提出的小样本迁移学习协议将未见过主体的表现从基线35.1%±3.1%提高到了92.3%±0.9%。系统还支持通过增量学习策略进行功能性扩展，使得在不完全重新训练模型的情况下，可以扩展到10类集合，并保持90.0%±0.2%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 可靠的肌电假肢控制往往受到高个体间变异性和高密度传感器阵列临床应用不切实际性的限制。本研究旨在通过开发一种依赖于较少数量sEMG通道的精确手势识别方法来克服这些挑战，以实现更广泛的应用可能性。

Method: 采用了卷积稀疏自动编码器（CSAE）来直接从未处理信号中抽取时间特性表达，从而省去了传统上需要的人工特征设计步骤。为了应对不同用户之间的差异，引入了少量镜头迁移学习协议，这使得系统能够在只有少量校准数据的情况下也能很好地适应新用户。

Result: 在六种类别的手势集中，所提出的模型达到了94.3% ± 0.3% 的多主体F1得分。对于未见用户，通过少量镜头迁移学习后，表现从基准35.1% ± 3.1% 提升至92.3% ± 0.9%。此外，利用增量学习策略，可以在不需要完整重训模型的前提下，成功扩展至十类别手势集，同时保持90.0% ± 0.2% 的F1得分。

Conclusion: 结合高精度与最小化的计算和传感器开销，该框架为下一代可负担且适应性强的假肢系统提供了一个可扩展且高效的方法。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [102] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 该论文提出了一种因果模型，能够同时捕捉测量误差和机制变化两种类型的异常，并通过最大似然估计方法来实现。实验表明，该方法不仅在根本原因定位上达到最先进性能，还能准确分类异常类型，即使在因果DAG未知的情况下也能保持稳健。


<details>
  <summary>Details</summary>
Motivation: 现有的异常根本原因分析方法忽略了异常可以通过两种不同的过程产生：一种是由于测量错误导致的数据记录不正确；另一种是数据生成的因果过程发生了变化。为了区分这两种情况并更准确地识别出异常的根本原因，提出了新的因果模型。

Method: 定义了一个因果模型，该模型通过将离群值视为对潜在（“真实”）变量和观测到（“测量”）变量的潜伏干预来明确捕捉测量误差和机制转换两种类型的异常。证明了这些异常是可以识别的，并提出了一种最大似然估计方法来实践这一理论。

Result: 实验结果表明，所提出的方法在根本原因定位方面达到了最先进的性能水平，同时还能够准确地对异常类型进行分类。此外，在因果DAG未知的情况下，该方法依然表现出鲁棒性。

Conclusion: 本文介绍了一种新方法，可以有效地区分由测量错误引起的异常与由机制变化引起的异常，为异常根本原因分析提供了一个有力工具。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [103] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出了一种新的方法DC-CoT，通过将长链思维分解为可以并行处理的子任务来减少大型语言模型在数学推理等任务中的延迟。该方法基于现有的长链思维基础模型，并结合了少量示例微调和多阶段强化学习算法以恢复准确性同时降低最长路径长度。实验表明，DC-CoT在保持准确性的基础上能够显著降低处理时间。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）进行长时间思考推理时，尤其是数学推理领域，会因为高度顺序化的生成过程导致较高的延迟。为了解决这个问题，研究者提出了Divide-and-Conquer CoT (DC-CoT)方法，旨在通过并行执行子任务的方式减少响应所需的最长时间。

Method: 首先基于一个长链思维的基础模型（DeepScaleR-1.5B-Preview），利用少量精心挑选的样例集对其进行监督微调(SFT)，使模型初步具备按照特定格式分配子任务给‘工人’的能力。鉴于SFT可能导致准确性下降，进一步设计了包含多种数据筛选策略的多阶段强化学习算法来恢复准确性并缩短最长路径长度。

Result: 在包括AIME 2024 和 HMMT 2025在内的几个基准测试中，与原始的DeepScaleR-1.5B-Preview相比，DC-CoT不仅达到了相似的准确性水平，而且成功地将最长路径长度减少了35-40%。

Conclusion: 研究表明，通过采用Divide-and-Conquer CoT (DC-CoT)方法可以有效减少大型语言模型在复杂推理任务中的延迟问题，同时保持高准确性。此外，该研究还公开了代码、用于监督微调的数据集及模型，以便其他研究人员复现结果或在此基础上开展进一步工作。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [104] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 本文探讨了在结构预测中使用可微匹配层时遇到的过早模式崩溃问题，并提出了一种名为Efficient PH-ASC的新算法，该算法通过监测推理过程的稳定性来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究者们发现，在结构预测中使用基于熵正则化最优传输实现的可微匹配层时，通过退火方法恢复离散排列是不稳定的。这种不稳定性的根本原因是‘过早模式崩溃’现象。

Method: 作者通过对Sinkhorn固定点映射的非正规动力学进行分析，揭示了一个理论上的‘热力学速度限制’。为了解决这个问题，提出了Efficient PH-AS算法，这是一种自适应调度算法，能够监控推理过程中的稳定性。

Result: 新提出的Efficient PH-ASC算法成功地解决了传统方法中存在的过早模式崩溃问题。它实现了线性稳定法则的应用，将成本高昂的谱诊断从训练循环中分离出来，从而大幅度降低了计算开销。

Conclusion: Efficient PH-ASC提供了一种有效的方法来改善基于熵正则化最优传输的可微匹配层在结构预测任务中的性能与稳定性。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [105] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 本文提出了一种基于相对奖励的强化学习框架（RLRR），通过将奖励机制从绝对评分转换为相对排名，解决了现有方法在验证任务和开放式场景中的局限性。实验表明，RLRR在推理基准和开放式生成任务中相对于标准基线表现出一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于组的方法如GRPO虽然有效，但依赖于绝对数值奖励，这导致了内在限制：在可验证任务中，相同的组评估通常会导致稀疏监督；在开放式场景中，奖励模型得分范围的不稳定性削弱了基于组平均值的优势估计。

Method: 提出了强化学习与相对奖励（RLRR）框架，该框架将奖励塑造从绝对评分转变为相对排名，并引入了列表偏好模型——排名奖励模型，以直接为基于组的优化生成相对排名。

Result: 实验结果显示，在推理基准测试和开放式生成任务中，RLRR相较于标准基于组的基线显示出一致的性能改进。

Conclusion: 通过转向使用相对排名而非绝对分数作为奖励信号，RLRR能够有效缓解信号稀疏性和奖励不稳定的问题，从而在不同类型的任务上实现了性能上的显著提高。

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [106] [ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations](https://arxiv.org/abs/2601.23068)
*Joao Fonseca,Julia Stoyanovich*

Main category: cs.LG

TL;DR: 本研究提出了一种名为ExplainerPFN的新方法，该方法能够在不访问基础模型或参考解释的情况下估算Shapley值，通过在合成数据集上的预训练来预测未见过的表格数据集中的特征归因。


<details>
  <summary>Details</summary>
Motivation: 在监督分类任务中计算特征的重要性对于模型的可解释性至关重要。然而，广泛使用的Shapley值方法需要直接访问底层模型，这在实际部署中经常无法满足。此外，即使可以访问模型，其精确计算也可能非常昂贵。因此，研究人员探索了仅使用输入数据分布而无需评估目标模型的情况下，是否可以获得有意义的Shapley值估计。

Method: 介绍了ExplainerPFN，一种基于TabPFN的表格基础模型，它通过从随机结构因果模型生成的合成数据集上进行预训练，并使用精确或接近精确的Shapley值作为监督。一旦完成训练，ExplainerPFN就能在没有模型访问、梯度或示例解释的情况下为新的表格数据集预测特征归属。

Result: 研究表明，基于少量学习的解释方法能够以少至两个参考观察值达到与SHAP值的高度一致性；提出了首个无需访问底层模型或参考解释即可估计Shapley值的零样本方法ExplainerPFN；提供了包括完整训练流程和合成数据生成器在内的开源实现；通过大量实验表明，ExplainerPFN的表现与依赖2-10个SHAP例子的小样本替代解释器相当。

Conclusion: ExplainerPFN代表了向无模型访问要求下有效估计Shapley值迈出的重要一步，为提高机器学习模型的可解释性提供了一个强有力的方法。

Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations.
  Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.

</details>


### [107] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: 本文提出SplineFlow，一种基于B样条插值的流匹配算法，能够更好地模拟动态系统中条件路径的复杂变化，满足多边际约束，并在多种确定性和随机动态系统的实验中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的方法使用线性插值构造条件路径，这可能无法捕捉到基础状态演化，特别是在从不规则采样的观测数据中学习高阶动态时。直接使用高阶多项式往往不稳定且振荡，难以构建同时满足跨观测多边际约束的一致路径。

Method: SplineFlow利用B样条基底的平滑度和稳定性以结构化方式学习复杂的底层动态，同时确保满足多边际要求。

Result: 通过在不同复杂度的确定性和随机动态系统以及细胞轨迹推断任务上的全面实验，证明了SplineFlow相比现有基线有显著改进。

Conclusion: SplineFlow提供了一种理论上有根据的方法来建模跨越观测点的条件路径，通过B样条插值有效解决了现有方法在处理动态系统尤其是高阶动态时遇到的问题。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [108] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 本文提出了一种新的训练目标CATTO，旨在提高大型语言模型的预测置信度与实际正确性之间的校准。实验表明，相比直接偏好优化方法，CATTO能够显著降低预期校准误差，并且不会牺牲任务准确性。此外，还引入了Confidence@k机制，利用校准后的令牌概率实现贝叶斯最优输出令牌选择。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在做出准确的下一个令牌预测时，其对这些预测的信心往往校准不当：高信心预测经常出错，而低信心预测可能是正确的。基于偏好的对齐方法进一步破坏了预测概率与正确性之间的联系。为了解决这个问题，作者提出了一个名为CATTO的新训练目标。

Method: 提出了一种称为CATTO（Calibration Aware Token-level Training Objective）的方法，它是一种考虑校准的训练目标，旨在将预测的信心与经验预测正确性对齐。该方法可以与原有的偏好优化目标相结合。此外，还引入了Confidence@k，这是一种测试时缩放机制，利用经过校准的令牌概率来实现贝叶斯最优的选择输出令牌。

Result: 实验证明，与直接偏好优化(DPO)相比，CATTO能够减少分布内2.22%-7.61%和分布外1.46%-10.44%的预期校准错误(ECE)，同时保持或略微提高了五个数据集上的多项选择题回答准确性。相比于最强DPO基线，CATTO在分布内减少了0.22%-1.24%、分布外减少了1.23%-5.07%的ECE。

Conclusion: 通过采用CATTO方法，可以在不牺牲任务性能的情况下改善大型语言模型的置信度校准问题。这不仅有助于提高模型预测结果的可靠性，也为后续研究提供了新的方向。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [109] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的长期时间序列预测范式——进化预测（EF），它通过训练短周期模型并结合提出的框架，比直接在长周期上训练的模型表现更优。此方法解决了直接预测中由于远期梯度冲突导致的学习局部动态受阻的问题，并且单一的EF模型在标准基准测试和极端外推中表现出优于特定任务的直接预测集合模型。


<details>
  <summary>Details</summary>
Motivation: 现有的直接预测（DF）范式虽然高效，但其输出与评估范围的刚性耦合要求针对每个目标周期进行计算成本高昂的再训练。此外，DF范式存在一个根本性的优化问题，即来自遥远未来的冲突梯度会阻碍局部动态的学习。

Method: 提出了进化预测（EF）范式，该范式作为统一的生成框架被证明能够缓解DF中的优化病理，允许通过训练短周期模型来更好地学习局部动态。文中还证明了DF是EF的一个退化特例。

Result: 广泛的实验表明，单一的EF模型不仅超过了为特定任务设计的DF模型集合，在标准基准测试中表现出色，而且在极端外推情况下也显示出强大的渐近稳定性。

Conclusion: 这项工作推动了从被动静态映射到自主进化推理的长期时间序列预测领域的范式转变。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [110] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: 本文提出了一种名为分布信息一致性排序（DCR）的新方法，通过推导不一致性分数的确切分布来生成更有效的预测集。这种方法利用了校准项的绝对排名在给定相对排名条件下遵循负超几何分布的特点，从而提高了效率并保证了合理的覆盖率。实验结果表明，DCR能够显著减小预测集大小，同时保持有效覆盖。


<details>
  <summary>Details</summary>
Motivation: 现有的基于一致性预测的方法在处理全排序场景时，由于依赖于非一致性分数的上限，导致生成的预测集过大。为了解决这一问题，提高预测集的有效性和紧凑性，提出了新的方法。

Method: 提出了分布信息一致性排序（DCCR），该方法通过识别校准项目的绝对排名按照其相对排名条件服从负超几何分布的事实，来推导非一致性分数的确切分布，并据此确定一致性阈值。

Result: 理论分析表明，在温和假设下，DCR相比基线方法实现了更高的效率，并且保持了有效的覆盖率。实验验证也显示，与现有方法相比，DCR可以将平均预测集大小减少高达36%，同时维持所需的覆盖率水平。

Conclusion: 通过引入分布信息到一致性预测过程中，DCR不仅提高了预测集构建过程中的效率，而且确保了实际应用中对不确定性量化的准确性。这为排名模型的安全部署提供了更加可靠的支持。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [111] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 本文提出了一种新的分布修正方法，该方法能够以计算成本低、数学上合理且高度灵活的方式解决等式约束生成模型中的已知问题。通过在意识到约束的情况下扰动数据分布，新分布能够在保持环境空间维度的同时隐含地包含基础流形几何。理论分析和实验证据表明，这种方法可以持续地恢复数据分布，并使用扩散模型和标准化流进行稳定采样。


<details>
  <summary>Details</summary>
Motivation: 生成模型在各种应用中取得了广泛的成功，但在处理科学领域中常见的受到等式约束的样本时遇到了内在的数学限制。为了克服这些限制，作者们希望开发一种既经济又高效的方法来修改分布，使其能够更好地适应等式约束条件下的数据建模需求。

Method: 提出的数据分布调整方法是基于对现有数据分布的一种约束感知方式的扰动，使得调整后的分布与环境空间维度相匹配，并能隐含地反映底层流形的几何结构。此方法旨在解决等式约束条件下生成模型所面临的问题。

Result: 通过几个代表性任务上的理论分析和实证研究显示，所提出的方法能够一致地实现数据分布的恢复，并且对于扩散模型和标准化流都能够保证稳定的采样过程。

Conclusion: 这项工作展示了一种有效的方法来改进受等式约束影响的生成模型，它不仅在数学上有根据，而且在实际操作中也具有很高的灵活性和较低的计算成本，为科学领域内需要处理此类问题的研究提供了有力的支持。

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [112] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 研究使用深度强化学习框架根据部分可观察性建议每小时的药物剂量，以优化疼痛管理和降低死亡率。结果表明，即使短期目标仍然是主要目标，但重视长期结果对于更安全的治疗政策可能是关键。


<details>
  <summary>Details</summary>
Motivation: 在重症监护中，疼痛管理通常涉及治疗目标和患者安全之间的复杂权衡。强化学习能够从回顾性数据中学习药物剂量策略来解决这一挑战。然而，先前关于镇静和止痛的研究并未将患者生存作为目标，并依赖于不适合信息不完全环境的算法。本研究旨在探讨这些设计选择的风险。

Method: 采用深度强化学习框架，在MIMIC-IV数据库中利用47,144次ICU住院的数据训练模型，根据两个目标（减少疼痛或同时减少疼痛和死亡率）来建议每小时给予阿片类药物、丙泊酚、苯二氮卓类药物和右美托咪定的剂量。

Result: 发现两种策略均与较低的疼痛相关联，但仅针对减少疼痛的策略所采取的行动与死亡率呈正相关，而同时考虑减少疼痛和死亡率的策略所提议的行动则与死亡率呈负相关。

Conclusion: 研究表明，即便短期目标仍是首要考量，但重视长远后果对制定更安全的治疗方案可能至关重要。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [113] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 本文提出了一种基于语法的方法，用于在强化学习中对未标记的轨迹进行技能分割，并诱导出一个层次结构。该方法在高维像素环境中进行了评估，发现比现有基线能够产生更结构化和语义上有意义的层次结构。此外，还证明了这些发现的层次结构可以加速并稳定下游强化学习任务的学习。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法依赖于动作标签、奖励或手工注释来将轨迹分割成可重用的技能或选项，这限制了它们的应用范围。因此，需要一种新的方法能够在没有上述信息的情况下也能有效工作。

Method: 提出的方法是通过一种基于语法的方法，将无标签的轨迹自动分割成技能，并且在这个基础上构建起一个技能之间的层级结构。这个层级结构不仅捕捉到了低级别的行为，还展示了如何将这些行为组合成为更高层次的技能。

Result: 在Craftax和完整未修改版本的Minecraft等高维像素环境中的实验表明，所提出的方法相比现有基准方法能产生更加结构化并且语义上更有意义的层次结构。另外，也验证了这些层次结构对于加速及稳定后续强化学习任务的学习过程是有帮助的。

Conclusion: 本研究介绍的新方法能够有效地从未经标注的数据中识别出技能及其层次关系，为强化学习领域提供了一种新颖而有效的工具。

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [114] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 本研究提出了一种系统性地探究大型语言模型推理轨迹的方法，通过生成、截断和重新注入部分推理内容来测量答案选择的分布。实验结果表明，随着提供的推理标记比例增加，准确性和决策确定性也相应提高，并且这种提升主要归因于模型生成的相关内容而非上下文长度或一般的'推理风格'效应。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）越来越多地通过产生'推理痕迹'来解决难题，但对于这些推理过程中准确性与决策坚定度如何演变以及中间段落是否提供了除长度或风格外的答案相关信息尚不清楚。

Method: 1) 生成一个模型的推理路径；2) 在固定的令牌百分位处对其进行截断；3) 将每个部分路径重新注入到相同或不同的模型中，通过下一个令牌概率来衡量所引起的答案选项分布。此方法应用于开源Qwen3-4B/-8B/-14B和gpt-oss-20b/-120b模型上，测试了多项选择题GPQA Diamond和MMLU-Pro基准。

Result: 研究发现，随着提供的推理令牌比例增长，准确性和决策承诺持续增加。这种增益主要是由模型生成中的相关内容驱动的，而不是上下文长度或通用的'推理风格'效果。更强的模型通常能够从不正确的部分轨迹中成功回溯，但即时答案往往仍然固定在较弱模型的错误响应上。

Conclusion: 轨迹探测提供了一种诊断手段，用于更高效和安全地部署推理模型，因为这些测量可以为改进可靠性的实际跟踪处理和监控策略提供信息，而无需假设中间令牌本身就是忠实的解释。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [115] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文研究了具有参数噪声的随机线性bandit模型，给出了在一般动作集上的遗憾上界，并为特定类型的动作集（如ℓ_p单位球）提供了更具体的最小最大遗憾分析。此外，文章提出了一种简单的探索-利用算法来达到最优的遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨当奖励受到参数噪声影响时，如何有效地处理随机线性bandit问题，并试图找到针对不同动作集合的最佳策略。

Method: 通过对具有参数噪声的随机线性bandits进行理论分析，作者们提出了适用于一般动作集和特定类型动作集（如ℓ_p单位球）的遗憾上界与下界。同时，他们还设计了一个简单却高效的探索-利用算法来实现这些理论上界。

Result: 研究表明，在给定的时间范围内，对于任何大小为K、维度为d的一般动作集，以及最大方差σ^2_max的奖励，可以得到一个关于遗憾的上界。此外，对更具体类型的动作集进行了深入分析，发现其最小最大遗憾与经典加性噪声模型中的情况相比显著降低。

Conclusion: 本研究不仅为具有参数噪声的随机线性bandit问题提供了新的见解，而且还展示了一种简单而有效的算法能够达到接近最优的性能。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [116] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种新的基于Transformer的机制，该机制对可互换标记的重命名具有不变性，通过并行嵌入流和聚合注意力机制实现，并在需要泛化到新符号的任务中展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前神经架构缺乏处理可互换标记（如绑定变量）的原则方法，导致模型难以泛化到未见的符号上。

Method: 本文提出的方法使用了并行嵌入流来隔离每个输入中的可互换标记的影响，并结合了一个聚合注意力机制以促进流间结构化信息共享。

Result: 实验结果证实了该方法的理论保证，并在要求泛化到新符号的开放词汇任务上显示出了显著的性能改进。

Conclusion: 所提出的方法能够有效解决模型对于未见过但语义相同符号的泛化问题，提供了一种处理可互换标记的新途径。

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [117] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: 本文提出了一种名为DyCAST的动态字符对齐语音分词器，它通过软字符级对齐和显式时长建模实现了可变帧率的分词。DyCAST在训练期间学会将令牌与字符级别的语言单元关联起来，并支持无需对齐的推理以及解码时直接控制令牌时长。此外，还引入了一种检索增强解码机制来提高低帧率下的语音重合成质量。实验表明，DyCAST能够在使用明显更少的令牌的同时达到竞争性的语音重合成质量和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经音频编解码器通常以固定的帧率运行，导致生成的序列过长且令牌分配均匀。这限制了它们处理不同语速或需要高效编码情况下的灵活性。

Method: 提出了DyCAST（Dynamic Character-Aligned Speech Tokenizer），一种利用软字符级别对齐和明确持续时间建模来实现可变帧率标记化的技术。此外，还开发了一种检索增强解码机制以改善低帧率条件下的语音重建质量。

Result: 实验结果显示，与固定帧率的编解码器相比，DyCAST能在显著减少使用的令牌数量的情况下，仍保持有竞争力的语音重建质量和下游任务表现。

Conclusion: DyCAST为现代对话式语音技术提供了一种更加灵活高效的解决方案，能够根据实际内容调整帧率并有效减少了所需令牌数，同时维持了良好的性能。

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [118] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: 提出了一种新的架构MeshGraphNet-Transformer (MGN-T)，结合了Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，解决了标准MGN在大规模高分辨率网格上长距离信息传播效率低的问题。MGN-T能够有效处理工业级规模的网格，并在经典基准测试中优于现有方法，同时保持高效性。


<details>
  <summary>Details</summary>
Motivation: 为了克服标准MeshGraphNets（MGN）在大型、高分辨率网格上由于迭代消息传递导致的长距离信息传播效率低下问题，本文引入了一个物理注意力机制的Transformer作为全局处理器来更新所有节点状态，同时明确保留节点和边属性。这使得模型可以直接捕捉长程物理交互，从而无需深层的消息传递堆栈或分层粗化网格。

Method: MeshGraphNet-Transformer (MGN-T) 结合了Transformer的全局建模能力与MeshGraphNets对于几何形状的归纳倾向，使用基于网格的图表示。通过采用物理注意力机制的Transformer作为全局处理器，MGN-T能够同时更新所有节点的状态并直接捕捉长程物理交互，适用于具有不同几何形状、拓扑结构以及边界条件的大规模高分辨率网格的学习任务。

Result: MGN-T能够在冲击动力学等工业规模的网格场景下成功运行，而这些场景下标准MGN会因消息传递不足而失败。该方法准确地模拟了自接触、塑性变形及多变量输出，包括内部现象学塑性变量。此外，在经典基准测试中，MGN-T不仅实现了更高的准确性，而且相比其他基线方法只需要少量参数就能保持实际效率。

Conclusion: MGN-T作为一种新型架构，有效地结合了Transformer与MeshGraphNets的优点，特别适合处理需要考虑长程依赖关系的复杂物理仿真问题。它在处理工业规模的网格时表现出色，且在多个方面超越了当前最先进方法。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [119] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 本文通过结合墨西哥城的污染传感器测量和交通数据，研究了交通强度与空气质量之间的关系，并提出了一种基于同心环描述的创新方法来表示交通强度，进而使用偏最小二乘回归预测污染水平。


<details>
  <summary>Details</summary>
Motivation: 由于城市中公开的空气质量测量和预报在空间和时间上都是粗略的，而实时交通强度数据通常可获取且精细，因此研究者们希望通过分析交通数据与空气污染的关系，提供超本地化的动态空气质量预报。

Method: 研究人员将简单的颜色编码交通图转换为基于同心环的描述，以改善交通状况的特征化。接着，他们利用偏最小二乘回归技术根据新定义的交通强度来预测污染物水平。

Result: 所开发的模型经过多种训练样本优化后，达到了较好的预测性能，并对污染物与交通之间的关系提供了见解。此外，该工作流程简单易懂，适用于其他城市的类似情境。

Conclusion: 本研究表明，通过整合高分辨率的交通信息与空气质量监测，可以有效提升对于城市内空气质量变化的理解及预测能力。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [120] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 本文探讨了众包标注聚合中的公平性问题，为多数投票和最优贝叶斯聚合方法在ε-公平框架下的公平性提供了理论分析，并提出了一种改进的多分类公平后处理算法来确保任何聚合规则都符合严格的群体平等约束。


<details>
  <summary>Details</summary>
Motivation: 由于获取可靠的标签成本高昂或难以实现，通常会转向众包并聚合噪声较大的人工标注。然而，这种做法可能会放大个人偏见，特别是在敏感特征上引发公平性担忧。现有研究中对于众包聚合中的公平性关注较少，缺乏收敛性保证，且仅有一些有限的后处理方法用于强制执行基于人口统计学平等的ε-公平性。

Method: 本文首先对多数投票和最优贝叶斯聚合方法进行了公平性分析，提出了小规模众包情况下多数投票公平性差距的上限估计，并证明了聚合共识与真实标签之间的公平性差距在可解释条件下呈指数级快速收敛。此外，还推广了一种最新的多分类公平后处理算法，将其从连续场景扩展到了离散场景，以满足任意聚合规则下严格的群体平等约束。

Result: 实验结果表明，所提出的算法在合成数据集和真实数据集上均有效，能够验证理论分析的结果。

Conclusion: 该研究填补了众包标注聚合中关于公平性考量的空白，不仅提供了理论上的支持，也通过实验证明了其方法的有效性。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [121] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 本文提出了一种变分框架，将Transformer层解释为对token嵌入进行优化算法迭代。通过这种视角，自注意力机制实现了交互能量的梯度步骤，而MLP层则对应于势能的梯度更新。标准的GPT风格Transformer可以看作是基于这两个能量函数之间的Lie-Trotter分裂实现的目标函数上的简单梯度下降。此外，受经典优化思想启发，作者引入了一种Nesterov加速式的Transformer，在保持原有注意力和MLP组件的同时，于TinyStories和OpenWebText数据集上超越了nanoGPT基线模型的表现。


<details>
  <summary>Details</summary>
Motivation: 为了给Transformer架构提供一个基于优化理论的新视角，并利用这一视角来指导更有效的架构设计。

Method: 提出了一种新的变分框架，该框架将Transformer中的自注意力机制视为一种特定形式的能量最小化过程的一部分，并将MLP层的作用类比为另一种能量项的梯度更新。基于此框架，作者进一步开发了一个受Nesterov加速梯度方法启发的新型Transformer架构。

Result: 实验结果表明，所提出的Nesterov加速式Transformer在TinyStories和OpenWebText两个文本生成任务上均优于基础的nanoGPT模型。

Conclusion: 通过对Transformer采用优化理论视角，并据此设计改进后的架构，能够有效提升模型性能。这证明了从优化角度出发重新审视现有深度学习架构的价值。

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [122] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 本文研究了在不假设输入数据来自特定语言分布的情况下，语言识别和生成任务的统计率，并提出了新的目标来探讨这两个问题，在更一般的'不可知'设置下获得了新颖有趣的特征和几乎紧致的速率。


<details>
  <summary>Details</summary>
Motivation: 以前关于语言识别和生成的研究通常基于一个强可实现性假设，即输入数据是从给定集合中某语言支持的未知分布中抽取的。本研究完全放松了这一可实现性假设，对输入数据的分布不做任何限制。

Method: 通过提出新的研究目标来探索在更普遍且没有可实现性假设条件下的语言识别与生成问题。

Result: 对于这两个问题，研究者们得到了新颖而有趣的特征描述以及近乎最优的速率。

Conclusion: 该研究表明即使在没有强可实现性假设的情况下，也能够为语言识别与生成任务提供有用的见解和结果。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [123] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: 研究提出了一种名为FOCUS的推理系统，旨在解决扩散大语言模型(DLLMs)在解码过程中存在的计算效率问题。通过动态地将计算资源集中在可解码的token上，并实时排除不可解码的token，该方法能够提高有效批处理大小、缓解计算限制并实现可扩展吞吐量。实验表明，与生产级引擎LMDeploy相比，FOCUS最高可提升3.52倍的吞吐量，同时保持或提高了多个基准测试中的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）尽管为自回归模型提供了一个有吸引力的替代方案，但其部署受到高解码成本的限制。本文作者发现，在DLLM解码过程中存在一个关键的低效问题：虽然计算是在token块上并行化的，但在每个扩散步骤中只有少量tokens是可解码的，导致大部分计算被浪费在了不可解码的tokens上。基于这一观察，提出了改进措施。

Method: 基于注意力机制衍生出的token重要性与逐token解码概率之间存在强相关性的观察，研究者们设计了一种名为FOCUS的推理系统。该系统通过动态聚焦于可解码的tokens，并且即时排除那些不可解码的tokens来运作，从而增加了有效批次大小，减轻了计算限制，并实现了可扩展的吞吐量。

Result: 实证评估显示，与生产级引擎LMDeploy相比，FOCUS能够达到高达3.52倍的吞吐量改进，同时在多个基准测试中保持甚至提高了生成的质量。

Conclusion: 这项工作展示了如何通过优化计算资源分配来显著提高DLLMs的解码效率。FOCUS系统不仅提高了处理速度，而且在不影响甚至改善生成文本质量的情况下做到了这一点。此外，该系统的开源性质意味着它有可能被广泛采用和进一步开发。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [124] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种数据高效的、具有物理意识的函数空间生成框架DDIS，用于解决反PDE问题。通过解耦设计和神经算子显式建模前向PDE，DDIS在稀疏观测条件下实现了最先进性能，并在数据有限时保持了更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的插件式扩散后验采样器通过联合系数-解模型隐含地表示物理特性，这需要大量的配对监督数据。为了解决这个问题，提出了Decoupled Diffusion Inverse Solver (DDIS)，它采用解耦设计来提高数据效率并有效进行物理信息学习。

Method: DDIS采用了无条件扩散来学习系数先验，同时使用神经算子显式地建模前向PDE以提供指导。此外，还引入了Decoupled Annealing Posterior Sampling (DAPS) 方法来防止扩散后验采样过程中的过度平滑现象。

Result: 实验结果表明，在稀疏观测条件下，与联合模型相比，DDIS能够将$l_2$误差平均改善11%，谱误差平均改善54%；当可用数据仅占1%时，DDIS在$l_2$误差上仍保持优势，比联合模型高出约40%。

Conclusion: 理论分析证明，相较于训练数据稀缺时联合模型出现的引导衰减失效问题，DDIS能够有效避免。实验证明，DDIS在稀疏观测条件下表现优异，并且在数据极为有限的情况下也维持了较高的准确性。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [125] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: KevlarFlow, a fault-tolerant serving architecture for Large Language Models (LLMs), significantly reduces recovery time and improves latency under failure conditions, with minimal runtime overhead, through decoupled model parallelism initialization, dynamic traffic rerouting, and background key-value (KV) cache replication.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the fragility of LLM serving systems in the face of frequent hardware faults within hyperscale clusters, which lead to significant service outages. The current recovery mechanisms are too slow, taking up to 10 minutes, making it necessary to develop a more efficient and resilient solution.

Method: The method introduced in this paper is KevlarFlow, a novel fault-tolerant serving architecture that employs three key strategies: decoupled model parallelism initialization, dynamic traffic rerouting, and background key-value (KV) cache replication. These techniques aim to maintain high throughput during partial failures and drastically reduce the mean-time-to-recovery (MTTR).

Result: Evaluation results show that KevlarFlow effectively reduces MTTR by 20 times and, in failure scenarios, improves average latency by 3.1 times, 99th percentile (p99) latency by 2.8 times, average time-to-first-token (TTFT) by 378.9 times, and p99 TTFT by 574.6 times, all while maintaining negligible runtime overhead compared to leading LLM serving systems.

Conclusion: In conclusion, KevlarFlow successfully bridges the gap between hardware unreliability and service availability for LLMs, offering a substantial improvement over existing solutions in terms of recovery speed and performance under failure, with only a minor impact on regular operation.

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [126] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: HetCCL is a collective communication library that supports efficient, high-performance communication across heterogeneous GPU clusters from different vendors, without needing modifications to existing deep learning applications.


<details>
  <summary>Details</summary>
Motivation: The lack of support for collective communication across GPUs from multiple vendors in current deep learning frameworks results in inefficiency and higher costs. This drives the need for a solution like HetCCL to enable effective cross-vendor GPU communication.

Method: HetCCL unifies vendor-specific backends and facilitates RDMA-based communication between GPUs of different brands, employing two novel mechanisms that allow it to take advantage of optimized vendor libraries such as NVIDIA NCCL and AMD RCCL.

Result: Evaluations on a multi-vendor GPU cluster demonstrate that HetCCL performs comparably to NCCL and RCCL in homogeneous environments, while also uniquely supporting scalable performance in heterogeneous setups, thereby allowing for seamless, high-performance training with both NVIDIA and AMD GPUs.

Conclusion: HetCCL addresses the challenge of inefficient communication in mixed GPU environments by providing a unified solution that does not require changes to existing deep learning applications, thus offering a practical approach to leveraging diverse GPU resources efficiently.

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [127] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: AscendCraft, a DSL-guided method for automatic AscendC kernel generation, demonstrates high success rates in compilation and functional correctness, as well as competitive or better performance compared to PyTorch eager execution on NPU kernels. This approach bridges the gap between GPU and NPU kernel generation, making it possible to leverage LLMs for creating efficient NPU kernels.


<details>
  <summary>Details</summary>
Motivation: The development of high-performance kernels for specialized accelerators, such as NPUs, is challenging due to domain-specific programming models, limited public examples, and sparse documentation. There is a significant gap in generating correct and performant NPU kernels using large language models (LLMs) compared to GPU kernels, which this work aims to address.

Method: Introducing AscendCraft, a Domain-Specific Language (DSL)-guided system for the automatic generation of AscendC kernels. The process involves generating kernels in a lightweight DSL that abstracts away non-essential complexity while capturing Ascend-specific execution semantics, followed by transcompiling these kernels into AscendC through structured, constraint-driven LLM lowering passes.

Result: AscendCraft achieved 98.1% compilation success and 90.4% functional correctness when evaluated across seven operator categories with MultiKernelBench. Additionally, 46.2% of the generated kernels matched or exceeded the performance of PyTorch eager execution. It also successfully generated two new kernels for the mHC architecture, outperforming PyTorch eager execution significantly.

Conclusion: The introduction of AscendCraft presents a promising solution for the automatic generation of both correct and competitive NPU kernels. By utilizing a DSL-guided transcompilation approach, AscendCraft effectively narrows the gap between GPU and NPU kernel generation, enabling the use of LLMs for NPU kernel creation with high efficiency and performance.

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [128] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: 本文探讨了CRDTs在并发事件下的行为，特别是在管理即时通讯应用中的群组权限时可能出现的状态回滚问题。提出了通过引入外部仲裁者和'epoch事件'来解决这个问题，从而提高CRDT的一致性水平。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决CRDTs在处理如即时消息应用程序中群组权限管理等场景下遇到的并发更新导致的问题，特别是当两个拥有相同权限的管理员同时撤销对方权限时可能出现的不确定性。

Method: 提出了一种方法，即通过引入外部仲裁者来确定并发事件之间不可变的发生前关系，并通过可选的'epoch事件'批量异步地进行仲裁，以此来保持可用性的同时提供一种有界全序，进而增强一致性。

Result: 结果表明，通过采用外部仲裁及'epoch事件'的方法可以有效防止状态看似回退的现象，对于提升CRDT在特定应用场景下（如管理群组权限）的表现具有重要意义。

Conclusion: 结论是，为了解决由于并发操作导致的CRDT状态回滚问题，需要借助外部仲裁机制定义事件间的发生顺序，这样不仅能够保持系统的高可用性，还能显著改善数据类型所提供的一致性级别。

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [129] [An innovating approach to teaching applied to database design. Improvement of Action Learning in Lifelong Learning](https://arxiv.org/abs/2601.22175)
*Christophe Béchade*

Main category: cs.DB

TL;DR: 本文介绍了安热大学继续教育学院在过去十年中通过行动学习法让大学员工及公私企业人员接触数据库设计的情况。该方法结合了法国技术课程的成功因素、职业教育的适应性以及服务提供商的专业能力，实现了教学和职业上的可评估且持久的目标，并符合法国旨在提高大学与企业间合同数量和质量的政策。


<details>
  <summary>Details</summary>
Motivation: 为了使没有计算机处理技能但了解业务流程的学员也能学习数据库设计的知识和技术，安热大学继续教育学院采用了行动学习法。这种方法不仅传授教师的知识，还传递了顾问的经验。

Method: 行动学习法要求教师在项目管理中担任监督者的角色。它整合了法国技术课程20多年来的跨学期项目模式，允许学生将新获得的知识应用于实际。

Result: 通过采用行动学习法，安热大学成功地为那些不具备计算机处理专业知识但熟悉自身业务流程的学习者提供了培训，并且这一方法能够实现教学和职业两方面的可评估及长期目标。

Conclusion: 行动学习法集合了法国技术课程的成功要素、对职业教育的适应性以及服务商的专业能力，成为一种能够达成教学与职业上可评估并持久目标的培训方式，并且支持着法国促进大学与企业合作的政策方向。

Abstract: For now 10 years, the Action Learning has allowed employees of University of Angers, private and public Companies to be initiated with the design of database, on projects financed by professional structures. These innovating training periods are carried out within the framework of the University College of Further Education of the University of Angers. Database design is a process initially reserved to the professional data processing specialists, coming from French Level-2 technological courses (2-year degrees) or Engineer Schools (Master). The pedagogical model of technological courses has integrated for more than 20 years transverse semester projects, in order to give the students the opportunity to apply newly acquired knowledge, coordinated by teachers. Action Learning requires teachers to assume the role of supervisors for the project management. The objective of Action Learning is to transmit not only knowledge from teachers, but also the experience of consultants to trainees having no competence in data processing, but who have the knowledge of their business process. The present paper shows that Action Learning puts together the factors for success of French technological courses, the adaptability of pedagogy provided to the vocational training, and finally the competence of service provider, Keeping the best parts of those three complementary approaches makes it possible for this kind of formation to achieve teaching and professional, assessable and long lasting goals. Action Learning belongs to the French policy that aims to improve the volume and the quality of the contracts between Universities and companies.

</details>


### [130] [Discovering High-utility Sequential Rules with Increasing Utility Ratio](https://arxiv.org/abs/2601.22178)
*Zhenqiang Ye,Wensheng Gan,Gengsen Huang,Tianlong Gu,Philip S. Yu*

Main category: cs.DB

TL;DR: 本文提出了一种名为SRIU的新算法，用于发现具有递增效用比的所有高效用序列规则。该算法通过左右扩展和右左扩展两种方法进行，并利用项目对估计效用剪枝策略（IPEUP）来减少搜索空间。此外，还引入了位图和紧凑的效用表以提高效率。实验结果表明，该方法在真实世界和合成数据集上均有效，并且通过置信度和信念等指标进一步证明了SRIU能够改善挖掘结果的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前的规则增长挖掘方法中，高效用序列规则（HUSRs）与其生成之间的联系不明确，特别是新增加的项目如何影响先前规则的效用或置信度不清楚。因此，本研究旨在解决这个问题，提出了一个新问题：挖掘具有递增效用比的HUSRs。

Method: 提出了一种新的算法SRIU，它使用两种不同的扩展方法（左右扩展和右左扩展）来发现所有具有递增效用比的HUSRs。此外，SRIU采用了项目对估计效用剪枝策略（IPEUP），以及两组上限和相应的剪枝策略，以减少搜索空间。为了增强SRIU的效率，还引入了Bitmap技术减少内存消耗，并设计了一个紧凑的效用表辅助挖掘过程。

Result: 广泛的实验结果，包括来自现实世界和合成数据集的数据，展示了所提方法的有效性。另外，采用置信度和信念等质量度量标准进一步证明了SRIU可以提高挖掘结果的相关性。

Conclusion: SRIU算法为挖掘具有递增效用比的高效用序列规则提供了一种有效的方法，在提高挖掘效率的同时保证了结果的质量。

Abstract: Utility-driven mining is an essential task in data science, as it can provide deeper insight into the real world. High-utility sequential rule mining (HUSRM) aims at discovering sequential rules with high utility and high confidence. It can certainly provide reliable information for decision-making because it uses confidence as an evaluation metric, as well as some algorithms like HUSRM and US-Rule. However, in current rule-growth mining methods, the linkage between HUSRs and their generation remains ambiguous. Specifically, it is unclear whether the addition of new items affects the utility or confidence of the former rule, leading to an increase or decrease in their values. Therefore, in this paper, we formulate the problem of mining HUSRs with an increasing utility ratio. To address this, we introduce a novel algorithm called SRIU for discovering all HUSRs with an increasing utility ratio using two distinct expansion methods, including left-right expansion and right-left expansion. SRIU also utilizes the item pair estimated utility pruning strategy (IPEUP) to reduce the search space. Moreover, for the two expansion methods, two sets of upper bounds and corresponding pruning strategies are introduced. To enhance the efficiency of SRIU, several optimizations are incorporated. These include utilizing the Bitmap to reduce memory consumption and designing a compact utility table for the mining procedure. Finally, extensive experimental results from both real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Moreover, to better assess the quality of the generated sequential rules, metrics such as confidence and conviction are employed, which further demonstrate that SRIU can improve the relevance of mining results.

</details>


### [131] [High-utility Sequential Rule Mining Utilizing Segmentation Guided by Confidence](https://arxiv.org/abs/2601.22179)
*Chunkai Zhang,Jiarui Deng,Maohua Lyu,Wensheng Gan,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出了一种基于置信度引导的分割方法（RSC）的序列规则挖掘算法，旨在减少冗余效用计算，并通过多个数据集验证了该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在数据挖掘领域，发现高实用性的序列规则是一个关键目标。然而，现有的高实用性序列规则挖掘算法存在冗余效用计算的问题。当相同的项目序列可以形成多条不同的规则时，需要进行额外的效用计算。

Method: 研究提出了一种使用基于置信度引导分割的序列规则挖掘算法（RSC）。此方法预先计算候选子序列的支持度来预估分割规则的置信度。一旦确定了分割点，就可以同时生成具有不同前件和后件的所有规则。此外，RSC利用效用链接表加速候选序列生成过程，并引入更严格的效用上界——序列的缩减剩余效用，以处理包含重复项的序列问题。

Result: 所提出的RSC方法在多个数据集上进行了评估，结果表明它相较于最先进方法有所改进。

Conclusion: 这项研究通过提出一种新的基于置信度引导分割的方法有效地减少了高实用性序列规则挖掘中的冗余计算问题，从而提高了算法效率。

Abstract: Within the domain of data mining, one critical objective is the discovery of sequential rules with high utility. The goal is to discover sequential rules that exhibit both high utility and strong confidence, which are valuable in real-world applications. However, existing high-utility sequential rule mining algorithms suffer from redundant utility computations, as different rules may consist of the same sequence of items. When these items can form multiple distinct rules, additional utility calculations are required. To address this issue, this study proposes a sequential rule mining algorithm that utilizes segmentation guided by confidence (RSC), which employs confidence-guided segmentation to reduce redundant utility computation. It adopts a method that precomputes the confidence of segmented rules by leveraging the support of candidate subsequences in advance. Once the segmentation point is determined, all rules with different antecedents and consequents are generated simultaneously. RSC uses a utility-linked table to accelerate candidate sequence generation and introduces a stricter utility upper bound, called the reduced remaining utility of a sequence, to address sequences with duplicate items. Finally, the proposed RSC method was evaluated on multiple datasets, and the results demonstrate improvements over state-of-the-art approaches.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [132] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 本文介绍了一种用于快速基于文本的野生动物观察检索的紧凑超立方嵌入方法，该方法使用紧凑的二进制表示在大规模野生动物图像和音频数据库上实现高效搜索。实验表明这种方法不仅降低了内存和搜索成本，而且在多个基准测试中表现出色，甚至优于连续嵌入，并且提高了编码器的表现以增强检索和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型生物多样性监测平台越来越依赖多模态野生动物观察数据，尽管最近的基础模型能够跨越视觉、声音和语言提供丰富的语义表示，但要从庞大的档案库中检索相关信息仍然具有挑战性，因为高维相似性搜索带来了巨大的计算成本。因此，研究者们提出了一个框架来解决这个问题。

Method: 通过引入紧凑超立方嵌入，本文提出了一种支持快速基于文本的野生动物观察检索的框架。该框架基于跨视图代码对齐哈希框架，将轻量级哈希扩展到了单模态设置之外，以便在共享汉明空间内对自然语言描述与视觉或声学观察进行对齐。此外，该方法利用了预训练的野生动植物基础模型（如BioCLIP和BioLingual），并通过参数高效的微调方式有效适应哈希任务。

Result: 本研究的方法在包括iNaturalist2024（针对文本到图像检索）和iNatSounds2024（针对文本到音频检索）在内的大规模基准测试中进行了评估，同时也在多个声景数据集上测试了其在领域迁移下的鲁棒性。结果显示，使用离散超立方嵌入的检索方法相比连续嵌入，在多个情况下展现出了更好的性能，同时大幅减少了内存占用和搜索开销。另外，还发现哈希目标始终改善了底层编码器表示，从而加强了检索能力和零样本泛化能力。

Conclusion: 研究表明，基于二进制的语言检索使得对于大型野生动物档案的大规模及高效搜索成为可能，这对生物多样性监测系统来说是一大进步。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [133] [FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning](https://arxiv.org/abs/2601.22498)
*Wei Yang,Rui Zhong,Yiqun Chen,Shixuan Li,Heng Ping,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种新的多模态推荐框架FITMM，该框架通过频率域的信息理论方法，对不同模态的数据进行频谱分解，并在每个频带内独立处理信息，最后通过任务自适应门控机制聚合各频带信息。实验结果表明，FITMM相比现有先进基线方法具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前主流的多模态推荐系统在空间域融合了图像和文本等多种内容类型，但这种方法忽略了信号的频率结构，导致了模态间错位与冗余问题。为解决这些问题，研究者们从频谱信息论的角度出发，提出了一个新的视角来优化多模态推荐系统的设计。

Method: 基于频谱信息论的观点，本文开发了FITMM框架，它首先构建图增强项目表示，然后执行模态级频谱分解获得正交频带，形成轻量级的单频带多模态组件。接着，使用残差任务自适应门来汇总这些频带至最终表示。此外，训练过程中还引入了频率域IB项来控制冗余并提高泛化能力，并且设计了一个跨模态频谱一致性损失函数来保证同一频带内的模态对齐。整个模型采用标准推荐损失函数进行联合优化。

Result: 通过对三个真实世界数据集的广泛实验验证，FITMM相较于其他高级基准模型展现出了持续且显著的性能提升。

Conclusion: 这项工作证明了利用频域特征对于改进多模态推荐系统的有效性。通过FITMM框架，能够更有效地处理模态间的信息，减少了冗余同时提高了推荐准确性。

Abstract: Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.

</details>


### [134] [SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation](https://arxiv.org/abs/2601.22543)
*Ruiqi Zheng,Jinli Cao,Jiao Yin,Hongzhi Yin*

Main category: cs.IR

TL;DR: 本文提出了一种名为SCaLRec的方法，用于解决云设备协作推荐系统中由于重复使用缓存的用户语义表示而导致的排名下降问题。通过估计缓存语义的可靠性，并在设备端根据最新的交互证据调整语义嵌入，SCaLRec能够在不频繁调用云端大型语言模型的情况下保持较高的推荐质量。实验表明，该方法在真实数据集上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在云端的应用，虽然能够提高顺序推荐的质量，但为每个请求重新生成这些语义用户表示在实际规模下往往是不可行的。因此，在设备端进行重排序时通常会重复使用缓存的云端语义用户嵌入。然而，这样做会导致所谓的“云语义陈旧效应”，即重用的嵌入与用户的最新互动越来越不一致，从而导致排名质量下降。现有的基于LLM的云-设备推荐系统设计往往假设可以低延迟访问云端LLM或每次请求都重新生成语义嵌入，但在无法为每个请求重新生成且必须重用缓存语义时，如何保证推荐质量成为了一个挑战。

Method: 提出了SCaLRec框架来应对上述挑战。首先，它评估了在用户最近互动背景下缓存语义的有效性。接着，开发了一个设备端语义校准模块，该模块能够在没有云端LLM参与的情况下，利用最新的互动信息对缓存的语义嵌入进行调整。

Result: 通过对真实世界数据集进行实验，结果显示SCaLRec在面对云语义陈旧的情况下，相较于强大的基线模型，持续地提高了推荐性能。

Conclusion: SCaLRec提供了一种有效手段来缓解云-设备协同推荐系统中的语义陈旧问题，即使在不能频繁访问云端LLM的情况下也能保持高质量的推荐结果。

Abstract: Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.
  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.

</details>


### [135] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: 该论文提出了一种新的微调目标BEAR，旨在解决大型语言模型在推荐系统中训练-推理不一致的问题。通过确保正样本中的每个标记在解码步骤时都排在前B个候选标记内，BEAR有效地减少了错误剪枝的风险，并且与标准监督微调相比几乎没有增加计算开销。实验表明，BEAR在四个真实数据集上显著优于强大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 研究者们注意到，在使用大型语言模型进行推荐任务时存在一个关键的训练-推理不一致性问题：即使某些项目具有很高的总体概率，也不能保证它们会被束搜索检索到。这是由于束搜索的贪婪修剪机制可能过早地丢弃了这些高概率但当前前缀概率不足的正样本。为了解决这个问题，提出了BEAR方法。

Method: BEAR（束搜索感知正则化）是一种新颖的微调目标函数，它明确考虑到了训练期间束搜索的行为特征。不同于直接模拟训练过程中每个实例的束搜索过程（这将带来巨大的计算成本），BEAR实施了一个放松的必要条件：即要求正样本中的每一个标记在每一步解码时都能位于前B名候选标记之中。这种方法有效避免了正样本被错误剪枝的可能性，同时几乎不会增加额外的计算负担。

Result: 通过对四个真实世界的数据集进行广泛实验，结果表明BEAR方法能够显著超越现有的强基准模型。

Conclusion: BEAR提供了一种有效的方法来缓解大型语言模型应用于推荐场景时遇到的训练-推理不一致性问题。通过引入一种新的微调策略，该策略考虑到束搜索的具体行为特点，从而提高了推荐系统的性能。

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


### [136] [OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning](https://arxiv.org/abs/2601.23085)
*Mohanna Hoveyda,Jelle Piepenbrock,Arjen P de Vries,Maarten de Rijke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: OrLog, a neuro-symbolic retrieval framework, separates predicate-level plausibility estimation from logical reasoning, using LLMs to provide plausibility scores and a probabilistic engine for query satisfaction. It improves top-rank precision, especially on disjunctive queries, and is more efficient in terms of token usage.


<details>
  <summary>Details</summary>
Motivation: Current retrieval systems often ignore or inconsistently enforce logical operators (conjunction, disjunction, negation) in the query, leading to unreliable results. The motivation is to create a system that can effectively handle these logical constraints while also being applicable to the broader, less structured domain of information retrieval, where queries are often ambiguous and evidence incomplete.

Method: The method involves the introduction of OrLog, a framework that uses large language models (LLMs) to estimate the plausibility of atomic predicates without requiring a decoding process, followed by a probabilistic reasoning engine to calculate the posterior probability of satisfying the query. This approach allows for constraint-aware retrieval without the need for generating text, thus reducing the number of tokens used.

Result: OrLog significantly enhances top-rank precision, particularly for disjunctive queries, compared to methods that use LLMs directly for reasoning. Additionally, it achieves this with a substantial reduction in token usage, approximately 90% less per query-entity pair, indicating a more efficient solution.

Conclusion: By decoupling predicate-level plausibility estimation from logical reasoning and employing a combination of LLMs and a probabilistic reasoning engine, OrLog demonstrates superior performance in constraint-aware retrieval. This approach not only outperforms traditional LLM-as-reasoner methods but does so with greatly improved efficiency.

Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\sim$90\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [137] [An Automatic Deep Learning Approach for Trailer Generation through Large Language Models](https://arxiv.org/abs/2601.23121)
*Roberto Balestri,Pasquale Cascarano,Mirko Degli Esposti,Guglielmo Pescatore*

Main category: cs.MM

TL;DR: 本文提出了一种利用全面的多模态策略进行自动化预告片制作的框架，并在预告片创作的不同阶段采用了大型语言模型（LLM）。结果显示，该框架生成的预告片比之前的最先进方法更吸引观众。


<details>
  <summary>Details</summary>
Motivation: 为了提高电影预告片的吸引力和叙事体验，本文旨在通过采用一种综合多模态策略以及使用大型语言模型来自动生产预告片，从而不仅总结电影内容，而且创造一个独立的叙事体验。

Method: 论文中介绍的方法包括：首先，通过大型语言模型选取与电影核心叙事相关的主视觉序列；接着，提取出最吸引人的电影台词，并将其与预告片的故事线对齐；此外，还利用该模型创建背景音乐和旁白，以增强观众的参与度。

Result: 实验结果表明，所提出的框架能够生成比现有最新技术更为吸引人的预告片。

Conclusion: 本文提出的一种结合了全面多模态策略及大型语言模型应用的预告片自动生成框架，在提升预告片吸引力方面表现出色。

Abstract: Trailers are short promotional videos designed to provide audiences with a glimpse of a movie. The process of creating a trailer typically involves selecting key scenes, dialogues and action sequences from the main content and editing them together in a way that effectively conveys the tone, theme and overall appeal of the movie. This often includes adding music, sound effects, visual effects and text overlays to enhance the impact of the trailer. In this paper, we present a framework exploiting a comprehensive multimodal strategy for automated trailer production. Also, a Large Language Model (LLM) is adopted across various stages of the trailer creation. First, it selects main key visual sequences that are relevant to the movie's core narrative. Then, it extracts the most appealing quotes from the movie, aligning them with the trailer's narrative. Additionally, the LLM assists in creating music backgrounds and voiceovers to enrich the audience's engagement, thus contributing to make a trailer not just a summary of the movie's content but a narrative experience in itself. Results show that our framework generates trailers that are more visually appealing to viewers compared to those produced by previous state-of-the-art competitors.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [138] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: 该论文分析了Linux内核成为其自身的CVE编号机构后，内核漏洞的识别、跟踪及其修复过程。研究发现，严重性和CVSS指标与修复延迟几乎没有关联，而内核的新旧程度是生存模型中一个合理的预测因素。较新的内核版本会更快得到修复，而老版本则保留未解决的CVE。引入漏洞的提交通常比修复更广泛和复杂。


<details>
  <summary>Details</summary>
Motivation: 理解驱动Linux内核漏洞修补的因素，包括漏洞特征如何影响补丁的及时性。

Method: 通过分析元数据、相关提交记录以及补丁延迟来考察内核CVE的结构与动态变化。

Result: 内核新旧程度是补丁延迟的良好预测因子；较新版本的内核比旧版本更快地获得修复；引入漏洞的代码更改往往比修复它们时更为广泛和复杂。

Conclusion: Linux内核作为独特的开源项目，在CVE处理流程上也表现出独特性；严重度评分对加快修复速度影响不大；开发者倾向于优先修复较新版本中存在的问题。

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [139] [Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models](https://arxiv.org/abs/2601.22264)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 提出了一种基于预训练语言模型的少样本学习方法FlaXifyer来预测间歇性作业失败类别，并结合LogSift技术提高日志审查效率，从而加速故障诊断。


<details>
  <summary>Details</summary>
Motivation: 持续集成(CI)管道中的间歇性（不稳定）任务失败导致了大量低效情况，包括重复运行浪费计算资源以及需要开发者和专门团队花费时间进行诊断。尽管已有研究提出机器学习技术来检测这些间歇性失败，但尚未解决随之而来的诊断难题。

Method: 开发了FlaXifyer，一种利用预训练语言模型实现的少样本学习方法，用于根据作业执行日志预测间歇性作业失败类别；同时提出了LogSift，一种能在一秒内识别关键日志语句的技术，以减少审核工作量并突出显示相关故障信息。

Result: 在TELUS提供的2,458个作业失败案例中测试表明，FlaXifyer达到了84.3%的宏观F1分数和92.0%的Top-2准确率（每个类别仅用12个标注示例），LogSift则减少了74.4%的审查工作量，并在87%的情况下展示了相关信息。

Conclusion: FlaXifyer与LogSift相结合能够有效实现自动化分类、加快故障诊断速度，并为自动解决间歇性作业失败问题铺平道路。

Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliability issues. These intermittent (flaky) job failures lead to substantial inefficiencies: wasted computational resources from repeated reruns and significant diagnosis time that distracts developers from core activities and often requires intervention from specialized teams. Prior work has proposed machine learning techniques to detect intermittent failures, but does not address the subsequent diagnosis challenge. To fill this gap, we introduce FlaXifyer, a few-shot learning approach for predicting intermittent job failure categories using pre-trained language models. FlaXifyer requires only job execution logs and achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with just 12 labeled examples per category. We also propose LogSift, an interpretability technique that identifies influential log statements in under one second, reducing review effort by 74.4% while surfacing relevant failure information in 87% of cases. Evaluation on 2,458 job failures from TELUS demonstrates that FlaXifyer and LogSift enable effective automated triage, accelerate failure diagnosis, and pave the way towards the automated resolution of intermittent job failures.

</details>


### [140] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: PriviSense是一个基于Frida的工具包，可以在已root的Android设备上实时伪造传感器和系统信号，支持对未修改的应用程序进行可重复的设备实验。它能够帮助测试应用程序逻辑、发现基于上下文的行为以及隐私分析。


<details>
  <summary>Details</summary>
Motivation: 移动应用程序越来越依赖于实时传感器和系统数据来根据用户环境调整其行为。虽然模拟器和带有检测代码的构建提供了一些解决方案，但它们通常无法支持在物理设备上对环境敏感的应用行为进行可重复性测试。

Method: 研究人员提出了PriviSense，这是一个基于Frida的设备端工具包，用于在已root的Android设备上伪造运行时的传感器和系统信号。PriviSense可以脚本化并注入变化的传感器流（加速度计、陀螺仪、步数计）和系统值（电池水平、系统时间、设备元数据）到未经修改的应用中，从而实现无需使用模拟器或重写应用的设备上可重复实验。

Result: 演示验证了在五款代表性的传感器可视化应用上，在一个已root的Android设备上的实时伪造能力。通过支持这些值的可脚本化及可逆操纵，PriviSense促进了应用逻辑的测试、基于情境的行为揭示以及以隐私为重点的分析。

Conclusion: 为了确保道德使用，PriviSense代码仅与经过验证的研究人员共享。

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [141] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: 本文提出了一种名为EFParser的日志解析器，它基于小型语言模型并通过系统架构创新提高了日志解析的性能。该解析器引入了双缓存系统和自适应更新机制，并通过一个专门的校正模块来验证和改进每个生成的模板，从而在保证计算效率的同时超越了现有的基准方法。


<details>
  <summary>Details</summary>
Motivation: 尽管最近基于大型语言模型（LLMs）的语义解析器在泛化能力上优于传统的基于语法的方法，但它们的有效性很大程度上取决于模型规模。当使用更小、资源效率更高的LLMs时，性能会显著下降，这成为实际应用中的主要障碍。因此，需要一种能够提高小型模型能力并适用于有数据隐私要求和计算限制场景下的解决方案。

Method: 提出了EFParser，这是一种无监督的基于LLM的日志解析器，通过系统性的架构创新增强了较小模型的能力。它引入了一个具有自适应更新机制的双缓存系统，可以区分新图案与现有模板的变化。此外，还设置了一个专门的校正模块作为守门员，在将模板缓存之前对其进行验证和精炼以防止错误注入。

Result: 在公开的大规模数据集上的实证评估显示，当运行于较小的语言模型之上时，EFParser在所有指标上平均比最先进基线高出12.5%，甚至超过了某些使用大规模模型的基线。

Conclusion: EFParser提供了一种鲁棒且实用的日志分析部署解决方案，即使在增加额外验证步骤的情况下仍保持高计算效率。

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [142] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: 本文介绍了一个名为TimeMachine-bench的基准测试，用于评估现实世界Python项目中的软件迁移任务。研究发现虽然语言模型在处理迁移任务上显示出一定潜力，但仍然面临可靠性的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着自动化软件工程的发展，研究重点正逐渐转向反映软件工程师日常工作的实际任务。其中，适应环境变化的软件迁移过程被忽视了。因此，需要一个能够评估真实Python项目中软件迁移效果的基准。

Method: 创建了TimeMachine-bench，这是一个完全自动化的基准测试，由GitHub仓库组成，这些仓库因依赖更新导致测试开始失败。此外，还策划了一个经过人工验证的子集以保证问题可解性，并在此基础上使用基于11个模型（包括强大的开源权重模型和最先进的大语言模型）的基线进行评估。

Result: 结果显示，尽管大型语言模型对于迁移任务展现了一定的前景，但仍存在显著的可靠性挑战，比如利用低测试覆盖率产生的虚假解决方案以及次优工具使用策略导致的非必要编辑。

Conclusion: TimeMachine-bench为评估软件迁移提供了有价值的资源，同时也揭示了当前语言模型在执行此类任务时所面临的挑战。

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [143] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: 本文提出了一种名为HealthMag的工具，旨在帮助更好地引出、建模和评估数字健康软件的需求。同时，通过与现有的AgeMag方法结合，开发了Elderly HealthMag，专门用于辅助面向老年用户的移动健康软件的需求分析、设计和评估。


<details>
  <summary>Details</summary>
Motivation: 数字健康软件在开发过程中经常基于对用户不准确的假设进行，导致最终产品未能充分考虑用户的年龄及健康状况需求。因此，尽管这些软件可能在理论上满足临床目标，但在实际使用中却缺乏包容性。

Method: 受GenderMag启发，研究人员提出了HealthMag这一工具，并按照InclusiveMag框架进行了系统性的映射和校准。此外，他们还将此工具与经过校准的现有AgeMag方法相结合，形成了双重视角的方法：Elderly HealthMag。

Result: 通过认知走查的方式，展示了Age HealthMag在识别当前面向老年人的数字健康应用程序中的包容性偏见方面的应用价值。

Conclusion: HealthMag及其针对老年人群体特别定制的版本Elderly HealthMag为数字健康软件开发者提供了有效工具，以促进更加包容性的产品设计。

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [144] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 提出了VarParser，一种以变量为中心的日志解析策略，通过关注日志中的变量部分来提高解析准确性、效率，并减少大语言模型调用成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的日志解析方法只专注于日志的常量部分，忽略了变量部分对日志解析可能做出的贡献，导致了包括低效的日志分组和采样、大量的语言模型调用、高昂的语言模型调用成本以及丢失由变量信息带来的系统可见性等问题。

Method: 设计了一种名为VarParser的方法，该方法利用变量贡献抽样、以变量为中心的解析缓存和自适应变量感知上下文学习技术，有效捕捉日志中的变量部分并利用它们对解析的贡献。此外，通过引入变量单元保留丰富的变量信息，增强了日志解析结果的完整性。

Result: 在大规模数据集上的广泛评估表明，与现有方法相比，VarParser实现了更高的准确性和显著提高了解析效率，同时降低了大语言模型调用的成本。

Conclusion: 通过采用以变量为中心的日志解析策略，VarParser能够更有效地处理日志数据，不仅提升了日志解析的质量和效率，还减少了执行过程中的资源消耗。

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [145] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: 本研究首次系统地评估了五种模型合并技术在三大领域（大型语言模型、图像分类和自动驾驶）中三种不同模型架构上的应用效果。结果表明，现有模型合并技术直接应用于不同架构时效果不一，且对超参数配置非常敏感。为此，研究提出了一种名为AutoMerge的新颖搜索式模型合并框架，旨在解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 尽管模型重用在深度学习领域展现出巨大潜力，尤其是在大型语言模型（LLMs）中通过无需重新训练的模型合并方式实现了有效重用，但尚未有研究系统探讨这种方法是否适用于其他具有不同架构的深度学习模型。此外，当前模型合并技术面对异构结构特性的处理能力有限，并且对超参数设置极为敏感，这限制了它们跨领域的适用性。

Method: 研究者选择了五种模型合并技术，并针对三种不同的模型架构——包括大型语言模型、用于图像分类的模型以及自动驾驶相关的模型，在三个独立的应用领域内进行了广泛的实验评估。基于这些观察，提出了AutoMerge框架，该框架能够将复杂模型分割成多个异构区块，并自动探索最佳合并策略及其相关超参数配置。

Result: 实验发现，直接采用现有的模型合并方法在非LLM架构上表现不佳，存在显著的结果差异。而AutoMerge通过优化模型分割及合并策略选择过程，显著提高了跨架构模型合并的成功率与效率。

Conclusion: 研究表明，虽然模型合并为提高软件复用提供了一条新路径，但其在不同模型架构间的应用仍面临挑战。AutoMerge框架展示了通过自动化手段改善这一状况的可能性，为未来研究指明了方向。

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


### [146] [Constructing Safety Cases for AI Systems: A Reusable Template Framework](https://arxiv.org/abs/2601.22773)
*Sung Une Lee,Liming Zhu,Md Shamsujjoha,Liming Dong,Qinghua Lu,Jieshan Chen*

Main category: cs.SE

TL;DR: 本研究探讨了当前AI系统安全案例的构建方式及其与传统方法相比存在的不足，并提出了一套可重用的安全案例模板框架，旨在为生成式和前沿AI系统提供一种系统化、可组合且可复用的方法来构建和维护可靠、可审计并能适应系统行为变化的安全案例。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的安全性越来越受到重视，传统的安全案例实践（如航空或核工程领域）在处理现代AI系统时遇到了挑战。这些挑战包括不可预测的能力涌现、随提示词变化的行为以及通过微调等手段不断变化的风险状况。因此，需要开发出新的方法来更好地理解和保证AI系统的安全性。

Method: 本文首先分析了现有AI系统安全案例的构建方式及传统方法为何难以有效应对AI系统的动态特性。随后，提出了一个基于预定义结构的安全案例模板框架，该框架包含针对AI系统的特定声明类型、论证类型以及证据家族分类。每个模板都通过端到端模式示例来解决诸如缺乏真实数据评估、模型更新频繁以及基于阈值的风险决策等独特问题。

Result: 研究结果表明，所提出的框架能够提供一种系统化的、可组合且可重复使用的途径来构建和维护安全案例，这些案例对于生成式和前沿AI系统来说是可信的、可审计的，并且能够适应系统行为的变化。

Conclusion: 本文介绍了一种创新性的安全案例模板框架，它不仅克服了传统安全案例方法应用于AI系统时遇到的问题，还为未来AI系统的安全性评估提供了更加灵活和强大的工具。

Abstract: Safety cases, structured arguments that a system is acceptably safe, are becoming central to the governance of AI systems. Yet, traditional safety-case practices from aviation or nuclear engineering rely on well-specified system boundaries, stable architectures, and known failure modes. Modern AI systems such as generative and agentic AI are the opposite. Their capabilities emerge unpredictably from low-level training objectives, their behaviour varies with prompts, and their risk profiles shift through fine-tuning, scaffolding, or deployment context. This study examines how safety cases are currently constructed for AI systems and why classical approaches fail to capture these dynamics. It then proposes a framework of reusable safety-case templates, each following a predefined structure of claims, arguments, and evidence tailored for AI systems. The framework introduces comprehensive taxonomies for AI-specific claim types (assertion-based, constrained-based, capability-based), argument types (demonstrative, comparative, causal/explanatory, risk-based, and normative), and evidence families (empirical, mechanistic, comparative, expert-driven, formal methods, operational/field data, and model-based). Each template is illustrated through end-to-end patterns addressing distinctive challenges such as evaluation without ground truth, dynamic model updates, and threshold-based risk decisions. The result is a systematic, composable, and reusable approach to constructing and maintaining safety cases that are credible, auditable, and adaptive to the evolving behaviour of generative and frontier AI systems.

</details>


### [147] [Understanding on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/abs/2601.22791)
*Sabinakhon Akbarova,Felix Dobslaw,Robert Feldt*

Main category: cs.SE

TL;DR: 研究探索了大型语言模型（LLMs）生成边界值分析与测试中边界解释的有效性。通过让27位软件专业人士对GPT-4.1提供的20组边界值解释进行评价，并结合后续访谈，发现大多数反馈积极但存在个体差异。参与者偏好结构清晰、引用权威来源且适应读者专业水平的解释，并强调需要可操作的例子来辅助调试和文档编写。基于此，研究提出了一个七项要求清单作为未来基于LLM工具的设计标准，指出经过进一步优化后，这类工具可以提高边界解释的可操作性和可信度。


<details>
  <summary>Details</summary>
Motivation: 在软件质量保证中，边界值分析与测试非常重要，因为错误通常集中在输入的极端情况。然而，测试人员往往难以理解并合理化某些输入-输出对代表了有意义的行为边界。大型语言模型可能通过提供自然语言解释来帮助解决这个问题，但其对于边界值测试的价值尚未得到实证评估。

Method: 研究采用了一种探索性方法，包括一项调查和后续访谈。在调查中，邀请了27位软件专业人士对GPT-4.1为20组边界值生成的解释按照清晰度、正确性、完整性以及感知有用性四个方面进行了评分；其中6人还参与了更深入的一对一访谈，以获取更多关于这些解释有效性的见解。

Result: 总体上，所有评分中有63.5%是正面的（五分制下得分为4或5），而负面评分仅占17%，表明虽然整体上得到了认可但也存在着看法上的差异。参与者倾向于喜欢那些遵循明确结构、引用可靠资源并且能够根据读者专业知识调整深度的解释；同时，他们也强调了提供有助于调试和支持文档编写的实用示例的重要性。

Conclusion: 研究表明，通过进一步改进，基于大型语言模型的工具可以在测试工作流程中发挥积极作用，使边界值解释更加具有可操作性和可靠性。此外，研究还提出了一份包含七个具体设计标准的需求清单，旨在指导未来此类工具的开发。

Abstract: Boundary value analysis and testing (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes, yet testers often struggle to understand and justify why certain input-output pairs represent meaningful behavioral boundaries. Large Language Models (LLMs) could help by producing natural-language rationales, but their value for BVT has not been empirically assessed. We therefore conducted an exploratory study on LLM-generated boundary explanations: in a survey, twenty-seven software professionals rated GPT-4.1 explanations for twenty boundary pairs on clarity, correctness, completeness and perceived usefulness, and six of them elaborated in follow-up interviews. Overall, 63.5% of all ratings were positive (4-5 on a five-point Likert scale) compared to 17% negative (1-2), indicating general agreement but also variability in perceptions. Participants favored explanations that followed a clear structure, cited authoritative sources, and adapted their depth to the reader's expertise; they also stressed the need for actionable examples to support debugging and documentation. From these insights, we distilled a seven-item requirement checklist that defines concrete design criteria for future LLM-based boundary explanation tools. The results suggest that, with further refinement, LLM-based tools can support testing workflows by making boundary explanations more actionable and trustworthy.

</details>


### [148] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: 本文介绍了Meta公司开发的一种即时捕捉测试生成方法，旨在大规模后端系统中预防代码bug。通过分析22,126个生成的测试案例发现，与传统强化测试相比，该方法在生成针对代码变更的有效捕捉测试方面表现更好，并且通过规则和大语言模型评估器减少了人工审核的工作量70%。实验证明，这种方法可以有效防止严重故障进入生产环境。


<details>
  <summary>Details</summary>
Motivation: 为了在数亿行代码规模的大后端系统中预防潜在的bug，在代码合并前及时发现并修复问题。

Method: 采用了即时捕捉测试生成技术，这种测试设计为失败以揭示潜在错误；利用了对代码变更敏感的方法来提高捕捉效率；引入了基于规则和大语言模型（LLM）的评估器来减少因误报导致的开发拖累。

Result: 相较于传统的强化测试，新的方法在生成有效的捕捉测试方面提高了4倍效能；对于偶然失败的测试，则提升了20倍。评估器帮助降低了人工审查需求达70%。报告给工程师的41个候选问题中，确认8个为真实问题，其中4个可能引发严重的生产事故。

Conclusion: 即时捕捉测试方法被证明是可扩展且适用于工业场景的，能够有效地阻止重大错误进入生产阶段。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [149] [AnoMod: A Dataset for Anomaly Detection and Root Cause Analysis in Microservice Systems](https://arxiv.org/abs/2601.22881)
*Ke Ping,Hamza Bin Mazhar,Yuqing Wang,Ying Song,Mika V. Mäntylä*

Main category: cs.SE

TL;DR: 本文介绍了一个新的多模态异常数据集AnoMod，用于微服务系统中的异常检测和根因分析。该数据集基于两个开源微服务系统构建，并设计了四类异常来模拟真实的异常情况。每个场景下收集了五种模态的数据，支持跨模态的异常检测方法评估及细粒度的根因分析研究。


<details>
  <summary>Details</summary>
Motivation: 现有的微服务系统（MSS）缺乏高质量且公开可用的数据集来进行异常检测（AD）与根因分析（RCA）。大多数基准测试侧重于性能相关故障，并只提供一种或两种监控模式，这限制了对更广泛故障类型的研究以及跨模态方法的发展。

Method: 创建名为AnoMod的新数据集，该数据集基于SocialNetwork和TrainTicket两个开源微服务系统开发。在这些系统中设计并注入了四大类异常：性能级别、服务级别、数据库级别和代码级别的异常，以模仿现实中的异常模式。对于每种情况，都会收集五种类型的监控数据：日志、指标、分布式跟踪、API响应和代码覆盖率报告。

Result: AnoMod数据集为跨模态异常检测方法及其融合/消融策略的评估提供了可能，同时也支持针对服务区域和代码区域进行细致的根因分析研究，有助于构建同时考虑检测与定位的端到端故障排除流程。

Conclusion: 通过引入AnoMod数据集，研究人员能够更好地探索微服务系统内不同层面之间的关联性，从而促进更加全面有效的异常检测与根因分析技术的发展。

Abstract: Microservice systems (MSS) have become a predominant architectural style for cloud services. Yet the community still lacks high-quality, publicly available datasets for anomaly detection (AD) and root cause analysis (RCA) in MSS. Most benchmarks emphasize performance-related faults and provide only one or two monitoring modalities, limiting research on broader failure modes and cross-modal methods. To address these gaps, we introduce a new multimodal anomaly dataset built on two open-source microservice systems: SocialNetwork and TrainTicket. We design and inject four categories of anomalies (Ano): performance-level, service-level, database-level, and code-level, to emulate realistic anomaly modes. For each scenario, we collect five modalities (Mod): logs, metrics, distributed traces, API responses, and code coverage reports, offering a richer, end-to-end view of system state and inter-service interactions. We name our dataset, reflecting its unique properties, as AnoMod. This dataset enables (1) evaluation of cross-modal anomaly detection and fusion/ablation strategies, and (2) fine-grained RCA studies across service and code regions, supporting end-to-end troubleshooting pipelines that jointly consider detection and localization.

</details>


### [150] [A Serverless Edge-Native Data Processing Architecture for Autonomous Driving Training](https://arxiv.org/abs/2601.22919)
*Fabian Bally,Michael Schötz,Thomas Limbrunner*

Main category: cs.SE

TL;DR: 本文介绍了一个名为Lambda的框架，该框架专为自动驾驶中的车载数据过滤和处理设计。通过将Function-as-a-Service（FaaS）原则应用于资源受限的汽车环境，它能够支持实时数据处理，并与ROS 2及现有数据记录流程兼容。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，数据既是实现机器学习的关键也是主要瓶颈。有效的模型训练不仅需要大量的传感器数据，还需要涵盖包括罕见但至关重要的安全场景在内的平衡覆盖。捕捉这些事件需要长时间驾驶和高效选择。

Method: 提出了Lambda框架，这是一种边缘原生平台，允许通过用户定义的功能进行车载数据过滤和处理。该框架提供了一种受无服务器启发的抽象层，将应用程序逻辑与调度、部署和隔离等低级执行问题分开。

Result: 评估了基于NVIDIA Jetson Orin Nano的框架，并与本地ROS 2部署进行了比较。结果显示性能具有竞争力，延迟和抖动减少，并证实了基于lambda的抽象可以支持嵌入式自动驾驶系统中的实时数据处理。

Conclusion: Lambda框架成功地将FaaS原则引入到资源受限的汽车环境中，实现了高效的车载数据处理解决方案。

Abstract: Data is both the key enabler and a major bottleneck for machine learning in autonomous driving. Effective model training requires not only large quantities of sensor data but also balanced coverage that includes rare yet safety-critical scenarios. Capturing such events demands extensive driving time and efficient selection. This paper introduces the Lambda framework, an edge-native platform that enables on-vehicle data filtering and processing through user-defined functions. The framework provides a serverless-inspired abstraction layer that separates application logic from low-level execution concerns such as scheduling, deployment, and isolation. By adapting Function-as-a-Service (FaaS) principles to resource-constrained automotive environments, it allows developers to implement modular, event-driven filtering algorithms while maintaining compatibility with ROS 2 and existing data recording pipelines. We evaluate the framework on an NVIDIA Jetson Orin Nano and compare it against native ROS 2 deployments. Results show competitive performance, reduced latency and jitter, and confirm that lambda-based abstractions can support real-time data processing in embedded autonomous driving systems. The source code is available at https://github.com/LASFAS/jblambda.

</details>


### [151] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: 本研究介绍了一个名为SWE-Manager的模型，它通过强化学习训练，用于从多个提案中选择最佳方案并综合出一个最优提案。在SWE-Lancer Manager基准测试上，SWE-Manager表现出了较高的选择准确率和收益，并且超过了包括GPT-5在内的强大基线。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的大型语言模型（LLM）研究主要集中在代码生成和错误修复等任务上。然而，在实践中，团队通常会为解决一个问题草拟多个候选提案，然后讨论选出一个黄金提案进行实施。这一过程不仅需要评估问题的范围、影响和紧迫性，还需要清晰理解每个提案的优势与劣势。良好的选择能够使问题解决更加可靠，同时减少回归和操作风险；反之，则可能增加风险甚至导致不可预测的故障。

Method: 首先进行了现实世界问题的手动研究，以确定维护者在选择竞争提案时使用的理由。基于这些发现，研究人员引入了SWE-Manager，这是一种联合选择和合成方法，旨在挑选最佳提案并合成一个黄金提案。SWE-Manager是一个80亿参数的模型，通过强化学习(RL)训练来比较提案、解释其选择的理由，并合成一个黄金提案以供实施。

Result: 在SWE-Lancer Manager基准测试中，SWE-Manager达到了53.21%的选择准确性以及57.75%的盈利比率，赚得了152,750美元，并且超越了包括GPT-5在内的强基线模型。此外，为了进一步评估SWE-Manager在实际问题解决中的有效性，设计了P2A框架模拟真实世界的工作流程，其中涉及起草多个提案、审查以及选定一个黄金提案进行实施。

Conclusion: SWE-Manager提供了一种有效的方法来辅助软件工程项目中提案的选择与优化过程，显示出比现有解决方案更好的性能。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [152] [Uncovering Hidden Inclusions of Vulnerable Dependencies in Real-World Java Projects](https://arxiv.org/abs/2601.23020)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: 本文介绍了Unshade，一种结合了元数据扫描和代码中心扫描优点的Java依赖项扫描方法。Unshade通过字节码指纹识别修改过或隐藏的依赖项，并将这些信息与已知漏洞进行对比。研究发现，GitHub上近半数最流行的开源Java Maven项目含有至少一个关联已知漏洞的隐藏依赖项。


<details>
  <summary>Details</summary>
Motivation: 随着开源软件(OSS)依赖项在现代软件开发中的重要性日益增加，虽然使用经过验证的OSS组件可以减少开发时间和成本并提高质量，但同时也引入了显著的安全风险，比如包含已知漏洞。现有的解决方案包括基于元数据的依赖项扫描器和以代码为中心的扫描器，但前者可能无法检测到被修改过的依赖项，后者则较为耗时。因此，需要一种能够有效结合两者优势的新方法来解决这个问题。

Method: 提出了Unshade，一种针对Java项目的混合型依赖项扫描方法。它首先通过一种基于字节码的指纹技术增强项目的软件物料清单(SBOM)，以此识别出修改过或隐藏的依赖项；然后将这个增强后的SBOM传递给基于元数据的漏洞扫描器，以便于识别所有声明以及新发现的依赖项中是否存在已知漏洞。

Result: 通过对GitHub上1,808个最受欢迎的开源Java Maven项目的研究表明，大约50%的项目至少包含一个与已知漏洞相关的修改或隐藏依赖项。平均每个受影响的项目包含超过八个这样的隐藏脆弱依赖项，而这些都是传统基于元数据的扫描器所未能捕捉到的。Unshade总共发现了7,712个独特的CVEs，这些是在仅依靠基于元数据扫描时会被忽略掉的隐藏依赖项中存在的漏洞。

Conclusion: Unshade成功地结合了基于元数据的扫描效率和代码中心扫描的能力，能够在大规模应用环境中有效检测出修改过或隐藏的依赖项及其相关漏洞，从而为开发者提供更全面的安全保障。

Abstract: Open-source software (OSS) dependencies are a dominant component of modern software code bases. Using proven and well-tested OSS components lets developers reduce development time and cost while improving quality. However, heavy reliance on open-source software also introduces significant security risks, including the incorporation of known vulnerabilities into the codebase. To mitigate these risks, metadata-based dependency scanners, which are lightweight and fast, and code-centric scanners, which enable the detection of modified dependencies hidden from metadata-based approaches, have been developed. In this paper, we present Unshade, a hybrid approach towards dependency scanning in Java that combines the efficiency of metadata-based scanning with the ability to detect modified dependencies of code-centric approaches. Unshade first augments a Java project's software bill of materials (SBOM) by identifying modified and hidden dependencies via a bytecode-based fingerprinting mechanism. This augmented SBOM is then passed to a metadata-based vulnerability scanner to identify known vulnerabilities in both declared and newly revealed dependencies. Leveraging Unshade's high scalability, we conducted a large-scale study of the 1,808 most popular open-source Java Maven projects on GitHub. The results show that nearly 50% of these projects contain at least one modified, hidden dependency associated with a known vulnerability. On average, each affected project includes more than eight such hidden vulnerable dependencies, all missed by traditional metadata-based scanners. Overall, Unshade identified 7,712 unique CVEs in hidden dependencies that would remain undetected when relying on metadata-based scanning alone.

</details>


### [153] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 研究了代码注释在训练和推理阶段对大型语言模型自动修复bug能力的影响，发现当两个阶段都存在注释时，能显著提高修复准确性；并且即使部分实例缺乏注释，使用含有注释的数据进行训练也不会降低性能。


<details>
  <summary>Details</summary>
Motivation: 假设代码中的注释可能通过提供有价值的设计和实现见解，在修复某些类型的错误中发挥关键作用，并且质疑当前实践中普遍存在的去除代码注释作为预处理步骤的有效性。

Method: 选取两种模型家族进行实证评估，对比所有组合的训练（有无注释）与推理条件（有无注释），并利用一个大型语言模型为缺乏注释的方法自动生成注释以解决现有数据集中注释不足的问题。

Result: 结果显示，当训练和推理两阶段都包含注释时，自动修复bug的准确性最高可提升三倍；而且即使是在没有注释的情况下，用含注释的数据训练模型也不会损害其表现。此外，解释性分析指出描述方法实现细节的注释对于帮助大型语言模型准确修复bug特别有效。

Conclusion: 保留代码中的注释可以显著增强大型语言模型自动修复bug的能力，特别是在理解和实现层面提供额外信息时。这表明未来的研究与实践中应当重新考虑去除代码注释作为标准预处理步骤的做法。

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [154] [Automated Testing of Prevalent 3D User Interactions in Virtual Reality Applications](https://arxiv.org/abs/2601.23139)
*Ruizhen Gu,José Miguel Rojas,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文针对虚拟现实（VR）技术中的测试挑战，提出了四个关键贡献：识别四种常见的交互类型、引入交互流程图来建模3D用户交互、构建XRBench3D基准用于评估VR交互测试、以及提出XRintTest自动化测试方法。实验表明，XRintTest在覆盖度和效率上显著优于随机探索，并能检测运行时异常和其他交互问题。


<details>
  <summary>Details</summary>
Motivation: 当前的VR测试方法虽然能够实现场景导航与互动激活，但缺乏自动生成真实3D用户输入的能力，且现有度量标准无法全面捕捉多样化的交互覆盖情况。

Method: 1. 经验性地确定了九个开源VR项目中普遍存在的四种交互类型。
2. 引入了一种新的抽象概念——交互流程图，用以系统化地建模3D用户交互。
3. 构建了一个名为XRBench3D的基准，包含十个VR场景，涵盖456种不同的用户交互。
4. 提出了XRintTest自动化测试方法，利用交互流程图进行动态场景探索和交互执行。

Result: XRintTest在XRBench3D上的评估显示，对于所有场景中的触发、操作和插槽交互达到了93%的覆盖率；相较于随机探索，其效果提高了12倍，效率提高了6倍。此外，XRintTest还能检测到运行时异常及其他非异常交互问题，包括细微的配置缺陷。

Conclusion: 通过引入新的交互模型与测试框架，本研究为VR应用提供了更有效的自动化测试解决方案，有助于提高交互覆盖率及发现潜在的设计缺陷。

Abstract: Virtual Reality (VR) technologies offer immersive user experiences across various domains, but present unique testing challenges compared to traditional software. Existing VR testing approaches enable scene navigation and interaction activation, but lack the ability to automatically synthesise realistic 3D user inputs (e.g, grab and trigger actions via hand-held controllers). Automated testing that generates and executes such input remains an unresolved challenge. Furthermore, existing metrics fail to robustly capture diverse interaction coverage. This paper addresses these gaps through four key contributions. First, we empirically identify four prevalent interaction types in nine open-source VR projects: fire, manipulate, socket, and custom. Second, we introduce the Interaction Flow Graph, a novel abstraction that systematically models 3D user interactions by identifying targets, actions, and conditions. Third, we construct XRBench3D, a benchmark comprising ten VR scenes that encompass 456 distinct user interactions for evaluating VR interaction testing. Finally, we present XRintTest, an automated testing approach that leverages this graph for dynamic scene exploration and interaction execution. Evaluation on XRBench3D shows that XRintTest achieves great effectiveness, reaching 93% coverage of fire, manipulate and socket interactions across all scenes, and performing 12x more effectively and 6x more efficiently than random exploration. Moreover, XRintTest can detect runtime exceptions and non-exception interaction issues, including subtle configuration defects. In addition, the Interaction Flow Graph can reveal potential interaction design smells that may compromise intended functionality and hinder testing performance for VR applications.

</details>


### [155] [Do Good, Stay Longer? Temporal Patterns and Predictors of Newcomer-to-Core Transitions in Conventional OSS and OSS4SG](https://arxiv.org/abs/2601.23142)
*Mohamed Ouf,Amr Mohamed,Mariam Guizani*

Main category: cs.SE

TL;DR: 研究发现，与传统开源软件项目相比，以社会公益为目标的开源软件（OSS4SG）项目在保留贡献者和提升他们成为核心成员的概率上表现更优。此外，新加入者若能在初期广泛探索项目，并且在深入了解代码库后再加大贡献力度，则更容易快速成为核心成员。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨开源软件中新人转变为关键贡献者的路径问题，特别是比较了以社会公益为使命的开源软件项目（OSS4SG）与常规开源软件项目之间，在新人留存率及成为核心成员的可能性上的差异。

Method: 通过对比分析190个OSS4SG项目与185个常规OSS项目的数据，涉及92,721名贡献者以及350万次提交记录。

Result: OSS4SG项目在保持贡献者参与度方面比常规OSS高出2.2倍，贡献者成为核心成员的概率也高出19.6%。早期对项目的广泛探索对于达到核心地位至关重要；而采取先深入学习再集中贡献（晚高峰模式）的新手比一开始就高强度贡献（早高峰模式）的人更快地成为核心成员。

Conclusion: 项目使命与新人成长为关键贡献者的环境密切相关。选择符合个人价值观的项目并花时间理解其代码库是实现这一目标的关键策略。研究表明，OSS4SG项目提供了更多样化的成长路径给新手们。

Abstract: Open Source Software (OSS) sustainability relies on newcomers transitioning to core contributors, but this pipeline is broken, with most newcomers becoming inactive after initial contributions. Open Source Software for Social Good (OSS4SG) projects, which prioritize societal impact as their primary mission, may be associated with different newcomer-to-core transition outcomes than conventional OSS projects. We compared 375 projects (190 OSS4SG, 185 OSS), analyzing 92,721 contributors and 3.5 million commits. OSS4SG projects retain contributors at 2.2X higher rates and contributors have 19.6% higher probability of achieving core status. Early broad project exploration predicts core achievement (22.2% importance); conventional OSS concentrates on one dominant pathway (61.62% of transitions) while OSS4SG provides multiple pathways. Contrary to intuition, contributors who invest time learning the project before intensifying their contributions (Late Spike pattern) achieve core status 2.4-2.9X faster (21 weeks) than those who contribute intensively from day one (Early Spike pattern, 51-60 weeks). OSS4SG supports two effective temporal patterns while only Late Spike achieves fastest time-to-core in conventional OSS. Our findings suggest that finding a project aligned with personal values and taking time to understand the codebase before major contributions are key strategies for achieving core status. Our findings show that project mission is associated with measurably different environments for newcomer-to-core transitions and provide evidence-based guidance for newcomers and maintainers.

</details>


### [156] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: 研究发现简单的、无需索引的词法检索在代码补全任务中可以达到与复杂的基于图的方法相媲美的效果，通过引入GrepRAG框架进一步改进了这一方法，使其在某些基准测试上超越了当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 先前的工作使用基于语义索引或结构感知图分析的检索增强生成(RAG)框架来解决大型语言模型(LLMs)面临的跨文件依赖和有限上下文窗口问题，但这些方法在索引构建和维护上带来了显著的计算开销。受到开发人员常用轻量级搜索工具（如ripgrep）工作流程的启发，本研究旨在探索简单且无需建立索引的词法检索能够在多大程度上支持仓库级别的代码补全，直到更复杂的检索机制变得必要为止。

Method: 首先提出了Naive GrepRAG作为基线框架，在该框架下LLM自动生成ripgrep命令以检索相关上下文；随后，通过广泛的经验分析识别出单纯词法检索存在的关键局限性，并提出GrepRAG，它通过包含标识符加权重排和结构感知去重的轻量级后处理流水线来增强词法检索。

Result: Naive GrepRAG尽管简单，但在性能上与复杂的基于图的基线相当；GrepRAG在CrossCodeEval和RepoEval-Updated上的评估表明，相较于最佳基线，其在代码完全匹配(EM)方面实现了7.04%至15.58%的相对提升。

Conclusion: 简单的、无需索引的词法检索能够有效支持仓库级别的代码补全任务，而通过适当的后处理技术（如GrepRAG所提出的），可以在保持低计算成本的同时进一步提高性能，甚至超越现有最先进方法。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


### [157] [Outcome-Conditioned Reasoning Distillation for Resolving Software Issues](https://arxiv.org/abs/2601.23257)
*Chenglin Li,Yisen Xu,Zehao Wang,Shin Hwei Tan,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 本文提出了一种基于已解决的仓库问题及其验证补丁的Outcome-Conditioned Reasoning Distillation (O-CRD) 框架，通过从历史修复中反向重构修复轨迹，并在推理时重用提炼出的指导来指引文件/函数定位和补丁合成。实验表明，这种方法可以显著提高首次通过率(Pass@1)，同时避免了昂贵的前向探索过程。


<details>
  <summary>Details</summary>
Motivation: 软件问题修复过程中，前期选择会影响后续步骤，而现有许多基于LLM的修复方法未充分利用以往修复经验，导致重复推理和高成本。仓库中常存在结构、故障模式或约束条件相似的问题，这些先前的修复经验能够为新问题提供有用的指导。

Method: 提出了Outcome-Conditioned Reasoning Distillation (O-CRD) 框架，该框架利用仓库内已经解决并验证过补丁的问题作为监督信号。从一个历史修复点开始，方法反向从验证结果重构分阶段的修复轨迹，然后在推理时重用提炼出的知识来指导文件/函数定位及补丁生成。

Result: 在SWE-Bench Lite上的测试显示，使用GPT-4o时Pass@1提高了10.4%，使用DeepSeek-V3时提高了8.6%，使用GPT-5时提高了10.3%。这表明，基于验证修复的结果条件重用可以替代成本高昂的前向探索以解决软件问题。

Conclusion: 研究证明了Outcome-Conditioned Reasoning Distillation (O-CRD) 方法的有效性，它能够在不进行微调或在线搜索的情况下，通过重用过去成功的修复策略来提高软件问题修复效率。

Abstract: Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.

</details>
