<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 10]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management](https://arxiv.org/abs/2510.15087)
*Kai Yin,Xiangjue Dong,Chengkai Liu,Allen Lin,Lingfeng Shi,Ali Mostafavi,James Caverlee*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Effective and efficient access to relevant information is essential for
disaster management. However, no retrieval model is specialized for disaster
management, and existing general-domain models fail to handle the varied search
intents inherent to disaster management scenarios, resulting in inconsistent
and unreliable performance. To this end, we introduce DMRetriever, the first
series of dense retrieval models (33M to 7.6B) tailored for this domain. It is
trained through a novel three-stage framework of bidirectional attention
adaptation, unsupervised contrastive pre-training, and difficulty-aware
progressive instruction fine-tuning, using high-quality data generated through
an advanced data refinement pipeline. Comprehensive experiments demonstrate
that DMRetriever achieves state-of-the-art (SOTA) performance across all six
search intents at every model scale. Moreover, DMRetriever is highly
parameter-efficient, with 596M model outperforming baselines over 13.3 X larger
and 33M model exceeding baselines with only 7.6% of their parameters. All
codes, data, and checkpoints are available at
https://github.com/KaiYin97/DMRETRIEVER

</details>


### [2] [MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation](https://arxiv.org/abs/2510.15286)
*Xianyang Qi,Yuan Tian,Zhaoyu Hu,Zhirui Kuai,Chang Liu,Hongxiang Lin,Lei Wang*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Industrial recommender systems critically depend on high-quality ranking
models. However, traditional pipelines still rely on manual feature engineering
and scenario-specific architectures, which hinder cross-scenario transfer and
large-scale deployment. To address these challenges, we propose
\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with
Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt
integrates two key components. The \textbf{AutoToken} module automatically
clusters heterogeneous features into semantically coherent tokens, removing the
need for human-defined feature groups. The \textbf{MTmixAttBlock} module
enables efficient token interaction via a learnable mixing matrix, shared dense
experts, and scenario-aware sparse experts, capturing both global patterns and
scenario-specific behaviors within a single framework. Extensive experiments on
the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently
outperforms state-of-the-art baselines including Transformer-based models,
WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,
MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields
further monotonic gains. Large-scale online A/B tests validate the real-world
impact: in the \textit{Homepage} scenario, MTmixAtt increases Payment PV by
\textbf{+3.62\%} and Actual Payment GTV by \textbf{+2.54\%}. Overall, MTmixAtt
provides a unified and scalable solution for modeling arbitrary heterogeneous
features across scenarios, significantly improving both user experience and
commercial outcomes.

</details>


### [3] [GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework](https://arxiv.org/abs/2510.15299)
*Yijia Sun,Shanshan Huang,Zhiyuan Guan,Qiang Luo,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Industrial-scale recommender systems rely on a cascade pipeline in which the
retrieval stage must return a high-recall candidate set from billions of items
under tight latency. Existing solutions ei- ther (i) suffer from limited
expressiveness in capturing fine-grained user-item interactions, as seen in
decoupled dual-tower architectures that rely on separate encoders, or
generative models that lack precise target-aware matching capabilities, or (ii)
build structured indices (tree, graph, quantization) whose item-centric
topologies struggle to incorporate dynamic user preferences and incur
prohibitive construction and maintenance costs.
  We present GRank, a novel structured-index-free retrieval paradigm that
seamlessly unifies target-aware learning with user-centric retrieval. Our key
innovations include: (1) A target-aware Generator trained to perform
personalized candidate generation via GPU-accelerated MIPS, eliminating
semantic drift and maintenance costs of structured indexing; (2) A lightweight
but powerful Ranker that performs fine-grained, candidate-specific inference on
small subsets; (3) An end-to-end multi-task learning framework that ensures
semantic consistency between generation and ranking objectives.
  Extensive experiments on two public benchmarks and a billion-item production
corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\times$
the P99 QPS of state-of-the-art tree- and graph-based retrievers.
  GRank has been fully deployed in production in our recommendation platform
since Q2 2025, serving 400 million active users with 99.95% service
availability. Online A/B tests confirm significant improvements in core
engagement metrics, with Total App Usage Time increasing by 0.160% in the main
app and 0.165% in the Lite version.

</details>


### [4] [Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable ID-based Models](https://arxiv.org/abs/2510.15308)
*Srijan Saket,Ikuhiro Ihara,Vaibhav Sharma,Danish Kalim*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In modern recommendation systems and social media platforms like Meta,
TikTok, and Instagram, large-scale ID-based features often require embedding
tables that consume significant memory. Managing these embedding sizes can be
challenging, leading to bulky models that are harder to deploy and maintain. In
this paper, we introduce a method to automatically determine the optimal
embedding size for ID features, significantly reducing the model size while
maintaining performance.
  Our approach involves defining a custom Keras layer called the dimension mask
layer, which sits directly after the embedding lookup. This layer trims the
embedding vector by allowing only the first N dimensions to pass through. By
doing this, we can reduce the input feature dimension by more than half with
minimal or no loss in model performance metrics. This reduction helps cut down
the memory footprint of the model and lowers the risk of overfitting due to
multicollinearity.
  Through offline experiments on public datasets and an online A/B test on a
real production dataset, we demonstrate that using a dimension mask layer can
shrink the effective embedding dimension by 40-50\%, leading to substantial
improvements in memory efficiency. This method provides a scalable solution for
platforms dealing with a high volume of ID features, optimizing both resource
usage and model performance.

</details>


### [5] [Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs](https://arxiv.org/abs/2510.15428)
*Sho Okazaki,Kohei Kaminishi,Takuma Fujiu,Yusheng Wang,Jun Ota*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fault cause identification in automated manufacturing lines is challenging
due to the system's complexity, frequent reconfigurations, and the limited
reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.
Although FMEA worksheets contain valuable expert insights, their reuse across
heterogeneous lines is hindered by natural language variability, inconsistent
terminology, and process differences. To address these limitations, this study
proposes a process-aware framework that enhances FMEA reusability by combining
manufacturing-domain conceptualization with graph neural network (GNN)
reasoning. First, FMEA worksheets from multiple manufacturing lines are
transformed into a unified knowledge graph through ontology-guided large
language model (LLM) extraction, capturing domain concepts such as actions,
states, components, and parameters. Second, a Relational Graph Convolutional
Network (RGCN) with the process-aware scoring function learns embeddings that
respect both semantic relationships and sequential process flows. Finally, link
prediction is employed to infer and rank candidate fault causes consistent with
the target line's process flow.
  A case study on automotive pressure sensor assembly lines demonstrates that
the proposed method outperforms a state-of-the-art retrieval-augmented
generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),
achieving the best performance (0.523) in fault cause identification. Ablation
studies confirm the contributions of both LLM-driven domain conceptualization
and process-aware learning. These results indicate that the proposed framework
significantly improves the transferability of FMEA knowledge across
heterogeneous lines, thereby supporting operators in diagnosing failures more
reliably and paving the way for future domain-adaptive LLM applications in
smart manufacturing.

</details>


### [6] [Enhance Large Language Models as Recommendation Systems with Collaborative Filtering](https://arxiv.org/abs/2510.15647)
*Zhisheng Yang,Xiaofei Xu,Ke Deng,Li Li*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As powerful tools in Natural Language Processing (NLP), Large Language Models
(LLMs) have been leveraged for crafting recommendations to achieve precise
alignment with user preferences and elevate the quality of the recommendations.
The existing approaches implement both non-tuning and tuning strategies.
Compared to following the tuning strategy, the approaches following the
non-tuning strategy avoid the relatively costly, time-consuming, and
expertise-requiring process of further training pre-trained LLMs on
task-specific datasets, but they suffer the issue of not having the
task-specific business or local enterprise knowledge. To the best of our
knowledge, none of the existing approaches following the non-tuning strategy
explicitly integrates collaborative filtering, one of the most successful
recommendation techniques. This study aims to fill the gap by proposing
critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,
we train a separate machine-learning model called Critic that implements
collaborative filtering for recommendations by learning from the interactions
between many users and items. The Critic provides critiques to LLMs to
significantly refine the recommendations. Extensive experiments have verified
the effectiveness of Critic-LLM-RS on real datasets.

</details>


### [7] [SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15682)
*Ines Besrour,Jingbo He,Tobias Schreieder,Michael FÃ¤rber*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.

</details>


### [8] [GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery](https://arxiv.org/abs/2510.15706)
*Italo Luis da Silva,Hanqi Yan,Lin Gui,Yulan He*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) show strong reasoning and text generation
capabilities, prompting their use in scientific literature analysis, including
novelty assessment. While evaluating novelty of scientific papers is crucial
for peer review, it requires extensive knowledge of related work, something not
all reviewers have. While recent work on LLM-assisted scientific literature
analysis supports literature comparison, existing approaches offer limited
transparency and lack mechanisms for result traceability via an information
retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an
easy-to-use interactive web tool designed to assist users in evaluating the
novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$
enables users to capture the main structure of a scientific paper, explore
related ideas through various perspectives, and assess novelty via providing
verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate
key elements of a paper, explore related papers through various relationships,
and assess novelty with contextual insight. This tool integrates external APIs
such as arXiv and Semantic Scholar with LLMs to support annotation, extraction,
retrieval and classification of papers. This combination provides users with a
rich, structured view of a scientific idea's core contributions and its
connections to existing work. $\textbf{GraphMind}$ is available at
https://oyarsa.github.io/graphmind and a demonstration video at
https://youtu.be/wKbjQpSvwJg. The source code is available at
https://github.com/oyarsa/graphmind.

</details>


### [9] [The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation](https://arxiv.org/abs/2510.15722)
*Da Li,Zecheng Fang,Qiang Yan,Wei Huang,Xuanpu Luo*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-Augmented Generation has made significant progress in the field of
natural language processing. By combining the advantages of information
retrieval and large language models, RAG can generate relevant and contextually
appropriate responses based on items retrieved from reliable sources. This
technology has demonstrated outstanding performance across multiple domains,
but its application in the legal field remains in its exploratory phase. In
this paper, we introduce our approach for "Legal Knowledge Retrieval and
Generation" in CCIR CUP 2025, which leverages large language models and
information retrieval systems to provide responses based on laws in response to
user questions.

</details>


### [10] [FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens](https://arxiv.org/abs/2510.15729)
*Chao Wang,Yixin Song,Jinhui Ye,Chuan Qin,Dazhong Shen,Lingfeng Liu,Xiang Wang,Yanyong Zhang*

Main category: cs.IR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recently, large language models (LLMs) have been explored for integration
with collaborative filtering (CF)-based recommendation systems, which are
crucial for personalizing user experiences. However, a key challenge is that
LLMs struggle to interpret the latent, non-semantic embeddings produced by CF
approaches, limiting recommendation effectiveness and further applications. To
address this, we propose FACE, a general interpretable framework that maps CF
embeddings into pre-trained LLM tokens. Specifically, we introduce a
disentangled projection module to decompose CF embeddings into concept-specific
vectors, followed by a quantized autoencoder to convert continuous embeddings
into LLM tokens (descriptors). Then, we design a contrastive alignment
objective to ensure that the tokens align with corresponding textual signals.
Hence, the model-agnostic FACE framework achieves semantic alignment without
fine-tuning LLMs and enhances recommendation performance by leveraging their
pre-trained capabilities. Empirical results on three real-world recommendation
datasets demonstrate performance improvements in benchmark models, with
interpretability studies confirming the interpretability of the descriptors.
Code is available in https://github.com/YixinRoll/FACE.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [11] [Game mechanics for cyber-harm awareness in the metaverse](https://arxiv.org/abs/2510.15180)
*Sophie McKenzie,Jeb Webb,Robin Doss*

Main category: cs.MM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Educating children and young people to be safe online is essential,
especially as the metaverse, a next-generation internet blending immersive
technologies, promises to reshape their interactions and amplify their
experiences. While virtual reality offers fully immersive, highly interactive,
and multi-sensory engagement, it also heightens cyber harm risks for young or
vulnerable users. To address this, the CyberNinjas VR experience was developed
to educate children aged 8 to 16 on safe metaverse behaviours, providing clear
referral steps for harmful interactions. Understanding user engagement in
metaverse gaming will aid the design of future VR environments which prioritize
safety and inclusivity. This project analyses CyberNinjas to understand how
game mechanics can foster cyber-safe behaviours.

</details>
