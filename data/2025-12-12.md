<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 42]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [PANDAExpress: a Simpler and Faster PANDA Algorithm](https://arxiv.org/abs/2512.10217)
*Mahmoud Abo Khamis,Hung Q. Ngo,Dan Suciu*

Main category: cs.DB

TL;DR: 本文介绍了一种名为PANDAExpress的新算法，通过采用新的概率不等式和动态构造的超平面切割方案，解决了PANDA算法中存在的大$polylog(N)$因子问题，从而使得运行时间与专业算法相当，并保持了PANDA的一般性和强大功能。


<details>
  <summary>Details</summary>
Motivation: PANDA是一种强大的通用算法，用于处理给定输入度约束下的连接查询（CQs）和析取数据记录规则（DDRs）。尽管在特定情况下PANDA能够达到最优运行时间，但其运行时间中隐藏的大$polylog(N)$因子导致其实用性受限。为解决这一弱点，研究者们提出了新方法。

Method: 1. 证明了一个新的概率不等式来上界任意度约束下DDR的输出大小。
2. 根据该不等式的证明直接开发出一种名为PANDAExpress的新算法，该算法比PANDA更简单且更快。
3. PANDAExpress采用了基于整个算法执行过程中精心追踪的数据偏斜统计信息动态构建的任意超平面切割方案，而非像PANDA那样使用轴平行超平面。

Result: PANDAExpress成功地去除了PANDA运行时间中的$polylog(N)$因子，使其运行时间和复杂的专业算法相匹配，同时保留了PANDA的所有一般性和强大能力。

Conclusion: 通过引入PANDAExpress算法，研究人员有效地解决了原有PANDA算法因包含大$polylog(N)$因子而导致实用性不足的问题。PANDAExpress不仅简化了算法流程还提高了效率，使得对于处理连接查询及析取数据记录规则而言，它成为了一个既高效又具有一般适用性的解决方案。

Abstract: PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.
  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\tilde O(\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power.

</details>


### [2] [Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint](https://arxiv.org/abs/2512.10621)
*Siwoo Song,Wonseok Shin,Kunsoo Park,Giuseppe F. Italiano,Zhengyi Yang,Wenjie Zhang*

Main category: cs.DB

TL;DR: 本文提出了一种新的超图模式匹配算法，通过引入交集约束、候选超边空间以及匹配-过滤框架，显著提高了查询处理速度。实验表明，该算法在实际数据集上的性能优于现有最佳算法。


<details>
  <summary>Details</summary>
Motivation: 超图是图的一种泛化，其中超边可以连接多个顶点，从而能够建模涉及多顶点的同时关系。超图模式匹配旨在找到查询超图在数据超图中的所有同构嵌入，这是基础性问题之一。为了提高这一过程的效率，研究者们提出了新的方法。

Method: 1. 引入了交集约束，这是一种对于有效嵌入而言必要且充分的条件，能够极大加速验证过程。
2. 定义了候选超边空间，用作存储查询超图与数据超图之间潜在映射的数据结构。
3. 提出了Match-and-Filter框架，在回溯过程中交替进行匹配和过滤操作以仅保留兼容的候选项于候选超边空间内。

Result: 实验结果基于真实世界数据集显示，本研究所提算法在查询处理时间上比最先进算法快了几个数量级。

Conclusion: 新提出的算法通过采用创新的数据结构和技术有效地解决了超图模式匹配问题，并且在效率上取得了显著提升，为处理复杂关系提供了强有力的支持。

Abstract: A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: 本文提出了一种名为M-ERC4907的方法，扩展了ERC4907标准的功能，以支持多时间槽的批量配置和多名用户的同步授权，实验结果表明该方法显著减少了链上交易数量和总的Gas消耗，提高了可扩展性和资源分配效率。


<details>
  <summary>Details</summary>
Motivation: ERC4907标准虽然允许租赁非同质化代币（NFT），但仅限于单用户、单时间段的授权方式，这极大地限制了它在去中心化的多时间段调度场景中的适用性和效率。为了解决这一局限性，提出了新的方法。

Method: 提出的M-ERC4907扩展方法引入了新功能来支持多个时间段的批量配置以及多名用户的同步授权，从而有效地消除了ERC4907原有的严格顺序授权限制。

Result: 实验是在Remix开发平台上进行的。结果显示，M-ERC4907方法显著地减少了链上的交易量及总体Gas消费，带来了更好的可扩展性和资源分配效率。

Conclusion: 通过实施M-ERC4907方法，可以克服ERC4907标准在多槽调度场景下的局限性，同时提高系统性能和用户体验。

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [4] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: 介绍了一种开源的轻量级分析工具ELANA，用于评估大型语言模型（LLMs）在多GPU和边缘GPU平台上的性能指标，包括模型大小、KV缓存大小、预填充延迟(TTFT)、生成延迟(TPOT)及端到端延迟(TTLT)，并支持Hugging Face上所有公开模型的能量消耗记录。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在不同硬件平台上部署时面临的延迟与功耗问题，以及对模型优化的需求，开发了一个易于使用的基准测试工具。

Method: 通过创建一个名为ELANA的轻量化分析工具，该工具能够评测LLMs的关键性能参数，并且兼容Hugging Face API，方便研究人员根据需要进行定制或调整以适应压缩或低比特宽度模型的研究。

Result: ELANA工具成功地为研究者提供了一个简单易用的命令行界面来评估各种LLM的性能特征，同时也支持可选的能量消耗日志记录功能。

Conclusion: ELANA作为一个开源项目，不仅有助于当前模型部署效率的提升，还促进了下一代高效LLM的研发工作。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [5] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: 本研究提出了一种名为GOODSPEED的新分布式推理框架，通过自适应推测解码优化大型语言模型（LLM）的吞吐量。该框架利用一个中心验证服务器协调多个异构草稿服务器，并采用梯度调度算法动态分配令牌验证任务，以确保跨服务器的比例公平性。研究表明，GOODSPEED在稳态条件下能够收敛到最优吞吐量分配，并且在动态工作负载下保持接近最优性能且误差有界。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然革新了自然语言处理领域，但其实时推理的高计算需求给多用户服务器推测解码及资源受限环境带来了巨大挑战。尽管推测解码作为一种加速LLM推理的技术已经出现，但在多草稿服务器与中心验证服务器合作中如何同时保证高吞吐量和公平性仍是一个未解决的问题。

Method: GOODSPEED框架由一个中心验证服务器和一组运行小型语言模型生成推测令牌的异构草稿服务器组成。为有效管理资源分配，引入了梯度调度算法来动态指派令牌验证任务，旨在最大化对数效用函数从而确保服务器间比例公平。该框架支持并行处理来自所有草稿服务器的推测输出，促进了验证服务器与分布式草稿生成器之间的高效协作。

Result: 通过严格的流体样本路径分析证明，GOODSPEED能够在稳态条件下达到最优吞吐量分配，在动态工作负载下也能维持接近最优表现，并且错误界限得到了证明。

Conclusion: 研究结果表明，GOODSPEED提供了一个可扩展、公平且高效的解决方案，适用于多用户分布式LLM推理系统。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.

</details>


### [6] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: RLTune, a reinforcement learning-based scheduling framework, optimizes GPU allocation for deep learning workloads on heterogeneous clusters, improving utilization, reducing queueing delays, and shortening job completion times without the need for per-job profiling.


<details>
  <summary>Details</summary>
Motivation: The increasing heterogeneity of GPU clusters in cloud platforms and the lack of visibility into application characteristics create challenges for efficient GPU scheduling. Traditional methods often fail to meet the high-throughput, low-latency requirements of large-scale deep learning (DL) workloads due to their reliance on offline profiling or specific application assumptions.

Method: RLTune is an application-agnostic RL-based scheduling system that dynamically prioritizes and allocains DL jobs across a diverse set of GPUs. It combines reinforcement learning for job prioritization with mixed-integer linear programming (MILP) for mapping jobs to nodes, aiming to optimize overall performance metrics such as job completion time, queueing delay, and resource utilization. The model is trained using extensive production traces from major cloud platforms like Microsoft Philly, Helios, and Alibaba.

Result: By implementing RLTune, there was a notable improvement in GPU utilization by up to 20%, a reduction in queueing delay by as much as 81%, and a decrease in job completion time (JCT) by up to 70%. These enhancements were achieved across a variety of workloads without the necessity for individual job profiling, showcasing the adaptability and efficiency of RLTune.

Conclusion: RLTune provides a scalable and practical solution for cloud providers to manage DL workloads more efficiently, fairly, and sustainably. Its ability to generalize well across different types of workloads, without requiring detailed knowledge about each job, makes it a promising approach for optimizing GPU scheduling in heterogeneous environments.

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [7] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: This paper outlines the practical steps and methods used in a Big Data course, including data processing, text analysis, and setting up a distributed computing cluster with Apache Spark.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to provide a comprehensive overview of the methodologies and techniques taught during a Big Data course, which can serve as a reference for students and educators alike.

Method: The method involves working with the Epsilon dataset, conducting text analysis and classification using RestMex, analyzing movie features from IMDb, and implementing a distributed computing environment with Apache Spark on a Linux system using Scala.

Result: The result is a detailed workflow that covers various aspects of big data processing and analysis, culminating in the setup of a functional distributed computing cluster.

Conclusion: The conclusion emphasizes the importance of hands-on experience with big data tools and technologies, and how the described course structure effectively prepares participants for real-world big data challenges.

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [8] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: 本文提出了一种新的宽条带LRCs（CP-LRCs），通过在所有本地校验块之间分解一个全局校验块来嵌入校验块之间的结构化依赖关系，从而降低了单节点和多节点修复的带宽需求，并保持了MDS级别的容错能力。


<details>
  <summary>Details</summary>
Motivation: 现有的局部可修复码（LRCs）在采用宽条带以减少大规模存储系统中的存储开销时表现出结构上的局限性：扩大的局部组增加了单节点修复成本，多节点故障经常触发昂贵的全局修复，并且可靠性急剧下降。

Method: 提出了级联奇偶LRCs (CP-LRCs)，这是一种新的宽条带LRCs，它通过将一个全局奇偶校验块分解到所有局部奇偶校验块中，在奇偶校验块之间嵌入结构化的依赖关系。同时开发了一个通用系数生成框架以及利用这种级联特性的修复算法，并具体实现了CP-Azure 和 CP-Uniform 两种设计实例。

Result: 在阿里巴巴云上的评估显示，对于单节点故障，修复时间减少了最多41%，而对于两个节点故障，则减少了26%。

Conclusion: 通过引入CP-LRCs，可以有效降低单节点或多节点故障下的修复时间和带宽消耗，同时维持高水平的数据可靠性和容错能力。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [9] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: 本文提出了一种新的分层联邦学习方法CFLHKD，通过多教师知识蒸馏实现集群间知识共享同时保持集群特异性个性化。这种方法在标准基准数据集上的评估表明，与代表性基线相比，CFLHKD在集群特定和全局模型准确性方面表现更优，性能提高了3.32-7.57%。


<details>
  <summary>Details</summary>
Motivation: 传统的集群联邦学习（CFL）方法在为每个集群训练独立的全局模型时存在学习碎片化的问题，并且未能充分利用集群集体见解的优势。本文旨在通过引入分层CFL来解决这些问题，以提高训练效率并促进集群间知识共享。

Method: 提出了CFLHKD，一种新颖的个性化方案，用于将层次化的集群知识整合到CFL中。它基于多教师知识蒸馏技术，使得不同集群之间可以共享知识的同时保持各自集群的个性化特征。此外，CFLHKD采用了双层聚合策略来弥合局部与全局学习之间的差距。

Result: 通过对标准基准数据集的广泛评估，CFLHKD在集群特定及全局模型准确度上均优于代表性的基线方法，实现了3.32-7.57%的性能提升。

Conclusion: CFLHKD作为一种创新的个人化方案，在提高集群联邦学习效率和性能方面展示了显著优势。其通过促进集群间的知识交流并保持集群内部特性，为处理大规模分布式IoT环境中的数据异质性和隐私保护问题提供了有效途径。

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [10] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: 提出了一种名为ESS（Extended Sparse Server）的系统设计，通过将Latent-Cache卸载到CPU内存来解决GPU内存限制问题，从而提高解码阶段吞吐量。实验表明，在不同上下文长度下，ESS显著提高了吞吐量，为长上下文大语言模型服务提供了一个实用且可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-V3.2-Exp在长上下文场景中引入了稀疏注意力机制以减少推理延迟，但解码阶段由于潜在缓存随序列长度线性增长与有限GPU内存之间的冲突成为了主要瓶颈，限制了批量大小并抑制了解码阶段吞吐量。

Method: 提出了一种以卸载为中心的系统设计——ESS(Extended Sparse Server)，专为DeepSeek-V3.2-Exp设计。该方法选择性地将潜在缓存卸载至CPU内存，同时保持对延迟敏感的部分位于GPU上，以此释放GPU内存，使批处理大小不再受GPU内存限制的影响。

Result: 高保真模拟显示，对于32K上下文长度，ESS提供了69.4%的吞吐量提升；而对于128K上下文长度，则实现了高达123%的吞吐量改进。

Conclusion: ESS被证明是一个有效的方法，能够显著改善长上下文推理工作负载下的解码阶段吞吐量，进而降低实际部署成本，并为长上下文的大语言模型服务提供了一个既实用又可扩展的解决方案。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [11] [STARS: Semantic Tokens with Augmented Representations for Recommendation at Scale](https://arxiv.org/abs/2512.10149)
*Han Chen,Steven Zhu,Yingrui Li*

Main category: cs.IR

TL;DR: 本文介绍了一种名为STARS的基于Transformer的电商推荐系统框架，该框架旨在解决冷启动产品、用户意图快速变化以及动态上下文（如季节性、节假日和促销活动）等挑战。通过结合双存储用户嵌入、语义项令牌、上下文感知评分机制和低延迟两阶段检索管道等多项创新，STARS在保持服务效率的同时显著提升了推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的电子商务推荐系统面临着诸如冷启动产品、用户意图快速转变及季节性、假日与促销等动态环境因素带来的挑战。同时，这些系统还需要在严格的数十毫秒级延迟限制下提供相关商品推荐。

Method: 提出了一种名为STARS的新颖推荐框架，它采用了几个关键技术创新：区分长期偏好与短期会话意图的双存储用户嵌入；融合预训练文本嵌入、可学习增量及大型语言模型衍生属性标签以增强内容匹配能力、长尾覆盖度和冷启动表现的语义项目令牌；考虑日历与事件偏移量的上下文敏感评分方法；以及一个注重延迟控制的两阶段检索流程，该流程包括离线嵌入生成和在线最大内积搜索加过滤步骤。

Result: 在生产规模数据上的离线评估显示，相比现有LambdaMART系统，STARS将Hit@5指标提高了超过75%。大规模A/B测试结果表明，在600万次访问中实现了统计学上显著的增长，包括总订单数增加0.8%，首页添加到购物车次数增长2.0%，每位用户的访问次数上升0.5%。

Conclusion: 研究结果证明，通过结合语义增强、多意图建模及面向部署的设计策略，可以在不牺牲服务效率的前提下，在实际环境中达到最先进的推荐效果。

Abstract: Real-world ecommerce recommender systems must deliver relevant items under strict tens-of-milliseconds latency constraints despite challenges such as cold-start products, rapidly shifting user intent, and dynamic context including seasonality, holidays, and promotions. We introduce STARS, a transformer-based sequential recommendation framework built for large-scale, low-latency ecommerce settings. STARS combines several innovations: dual-memory user embeddings that separate long-term preferences from short-term session intent; semantic item tokens that fuse pretrained text embeddings, learnable deltas, and LLM-derived attribute tags, strengthening content-based matching, long-tail coverage, and cold-start performance; context-aware scoring with learned calendar and event offsets; and a latency-conscious two-stage retrieval pipeline that performs offline embedding generation and online maximum inner-product search with filtering, enabling tens-of-milliseconds response times. In offline evaluations on production-scale data, STARS improves Hit@5 by more than 75 percent relative to our existing LambdaMART system. A large-scale A/B test on 6 million visits shows statistically significant lifts, including Total Orders +0.8%, Add-to-Cart on Home +2.0%, and Visits per User +0.5%. These results demonstrate that combining semantic enrichment, multi-intent modeling, and deployment-oriented design can yield state-of-the-art recommendation quality in real-world environments without sacrificing serving efficiency.

</details>


### [12] [The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation](https://arxiv.org/abs/2512.10388)
*Ziwei Liu,Yejing Wang,Qidong Liu,Zijian Zhang,Chong Chen,Wei Huang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出了一种名为H2Rec的新框架，该框架通过双分支建模架构和双级对齐策略来协调语义ID（SID）和哈希ID（HID），从而有效地平衡了头部项目和尾部项目的推荐质量，并在三个真实世界数据集上超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统的顺序推荐系统(SRS)通常使用唯一哈希ID(HID)构建项目嵌入，这导致了长尾问题。虽然引入辅助信息的方法试图解决这个问题，但它们往往受到共现信号引起的噪声协作共享或扁平密集嵌入引起语义同质性的影响。语义ID(SID)提供了一种有前景的替代方案，但协作压倒现象阻碍了基于SID方法的发展。

Method: 提出了一个名为H2Rec的新框架，该框架通过双分支建模架构来捕捉SID中的多粒度语义同时保留HID的独特协作身份。此外，还引入了一个双级对齐策略以桥接这两种表示法，促进知识转移并支持稳健的偏好建模。

Result: 在三个真实世界的数据集上进行了广泛的实验，结果表明H2Rec能够有效平衡头部和尾部项目的推荐质量，并且超过了现有的基准方法。

Conclusion: H2Rec作为一种新的框架，成功地解决了传统SRS中头部与尾部项目之间性能跷跷板的问题，通过结合SID和HID的优势实现了更高质量的推荐。

Abstract: Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \textbf{\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\footnote{https://github.com/ziwliu8/H2Rec}.

</details>


### [13] [Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition](https://arxiv.org/abs/2512.10688)
*Lingfeng Liu,Yixin Song,Dazhong Shen,Bing Yin,Hao Li,Yanyong Zhang,Chao Wang*

Main category: cs.IR

TL;DR: 该论文揭示了协同过滤模型中贝叶斯成对排序（BPR）优化固有的流行度偏差问题，并提出了一种方向分解与修正框架（DDC），通过不对称的方向更新来解耦偏好与流行度。实验表明，DDC在减少训练损失、提高推荐质量和公平性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的协同过滤模型存在流行度偏差问题，导致推荐系统过分推荐热门项目而忽视用户对小众内容的真实偏好。本文旨在从几何角度解析这一内在问题，并提出解决方案以改善个性化推荐效果。

Method: 本文通过数学分析证明了BPR优化过程中物品嵌入向量沿着主导的“流行度方向”排列的现象。为解决这一问题，提出了方向分解与修正（DDC）框架，通过对正面交互和个人偏好方向进行引导，同时将负面交互从全局流行度方向分离出去，从而在几何层面上解耦偏好与流行度。

Result: 实验结果显示，在多个基于BPR架构上，DDC能够显著超越当前最先进的去偏方法，将训练损失降至经过大量调优基线水平的不到5%，同时实现更优质的推荐结果和更高的公平性。

Conclusion: DDC框架提供了一个通用且有效的手段来纠正由BPR优化引起的空间几何扭曲问题，进而改善推荐系统的个性化能力和公平性。

Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization](https://arxiv.org/abs/2512.09972)
*Kesheng Chen,Wenjian Luo,Zhenqian Zhu,Yamin Hu,Yiya Xi*

Main category: cs.LG

TL;DR: 本文提出了一种名为BAMBO的新框架，通过引入混合最优块划分策略，自动构建大型语言模型的帕累托集。该方法在减少维度的同时保持了关键粒度，实验表明BAMBO能够发现比基线更优且更全面的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有的合并技术在处理大型语言模型的能力-效率权衡时存在不足，粗粒度的方法只能提供稀疏的次优解集，而细粒度的方法则面临维度灾难的问题。为了解决这些问题，提出了BAMBO框架。

Method: BAMBO采用了一种混合最优块划分策略，将问题形式化为一维聚类问题，并使用动态规划方法来优化块内同质性和块间信息分布。整个过程在一个由q-预期超体积改进(qEHVI)获取函数驱动的进化循环中自动化完成。

Result: 实验结果显示，与基线相比，BAMBO能够探索出更优越且更全面的帕累托前沿，从而支持根据不同操作约束灵活选择模型。

Conclusion: BAMBO框架有效地解决了大型语言模型能力与效率之间的权衡问题，通过创新性地应用混合最优块划分策略及进化算法，实现了对帕累托集合的有效构造。

Abstract: Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.

</details>


### [15] [Latent Action World Models for Control with Unlabeled Trajectories](https://arxiv.org/abs/2512.10016)
*Marvin Alles,Xingyuan Zhang,Patrick van der Smagt,Philip Becker-Ehmck*

Main category: cs.LG

TL;DR: 本文提出了一种潜行动作世界模型，该模型能够同时利用动作标注数据和无动作标注数据学习共享的潜行动作表示，从而在仅需少量动作标注样本的情况下也能训练出有效的动态模型。通过离线强化学习，该方法在DeepMind控制套件上表现出色，并且使用了比纯动作条件基线少一个数量级的动作标注样本。


<details>
  <summary>Details</summary>
Motivation: 传统的世界模型通常依赖于动作标注的轨迹数据，当动作标签稀缺时，其有效性受限。受到人类可以通过直接交互与非互动体验（如视频）结合学习的启发，研究者们探索了如何让世界模型从异构数据中学习，特别是引入了能够同时处理动作标注和无动作标注数据的方法。

Method: 提出了一类称为潜行动作世界模型的方法，它通过学习一种共享的潜行动作表示来联合使用动作标注数据和无动作标注数据。这种潜在空间将观察到的控制信号与从被动观测中推断出的动作对齐，使得单一的动力学模型能够在大规模未标注轨迹上进行训练，同时只需要一小部分带有动作标注的数据。进一步地，利用潜行动作世界模型通过离线强化学习来学习策略。

Result: 实验结果表明，在DeepMind控制套件上的表现优异，而且相比完全依赖动作标注数据的基准方法，所需的动作标注样本量减少了大约一个数量级。这证明了潜行动作使世界模型能够更高效地从被动和互动数据中学习。

Conclusion: 本研究表明，通过引入潜行动作的概念，可以让世界模型有效地利用混合类型的数据源进行学习，特别是显著减少了对昂贵或难以获取的动作标注数据的需求，提高了学习效率。

Abstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.

</details>


### [16] [Cluster-Dags as Powerful Background Knowledge For Causal Discovery](https://arxiv.org/abs/2512.10032)
*Jan Marco Ruiz de Vargas,Kirtan Padh,Niki Kilbertus*

Main category: cs.LG

TL;DR: 本研究利用Cluster-DAGs作为先验知识框架来辅助因果发现过程，提出两种基于约束的改进算法Cluster-PC和Cluster-FCI，分别适用于完全观测和部分观测场景。实验结果表明，这两种新方法在模拟数据上优于无先验知识的传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前因果发现方法在处理高维数据与复杂依赖关系时面临诸多挑战。通过引入系统相关的先验知识可以有效辅助因果发现。

Method: 本研究采用Cluster-DAGs作为一种新的先验知识形式，并基于此开发了两种改进的约束型算法：针对完全观测情况的Cluster-PC及针对部分观测情况的Cluster-FCI。

Result: 实验证明，在使用模拟数据进行评估时，无论是完全还是部分观测条件下，Cluster-PC和Cluster-FCI的表现均优于没有使用先验知识的传统方法。

Conclusion: 通过将Cluster-DAGs作为先验信息整合到因果发现过程中，能够显著提高从数据中恢复因果关系图的能力，特别是在高维度和存在复杂依赖的情况下。

Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.

</details>


### [17] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 本文提出了一种名为HB-SGE的新方法，它结合了重球动量与预测梯度外推，以提高在病态或非凸问题上的稳定性。实验表明，在条件数为50的病态二次问题上HB-SGE能在119次迭代中收敛，而在非凸Rosenbrock函数上则在2,718次迭代中达到收敛，相比之下NAG和标准动量方法会发散。


<details>
  <summary>Details</summary>
Motivation: 加速梯度法如Nesterov加速梯度（NAG）虽然在良好条件下能实现更快的收敛速度，但在病态或非凸问题上由于激进的动量累积而经常发散。因此，研究者们旨在开发一种既能够保持快速收敛又能在各种条件下保持稳定性的优化算法。

Method: 提出了Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE)方法，该方法将重球动量与基于局部泰勒近似的预测梯度外推相结合，以自适应地提供加速度同时保持稳定性。

Result: 实验证明HB-SGE在强凸函数上有收敛保证，并且在病态问题（条件数κ=50的二次问题）以及非凸Rosenbrock函数上表现出色，分别在119次和2,718次迭代内收敛，而SGD和NAG在这些情况下会发散。

Conclusion: HB-SGE作为一种鲁棒的一阶方法，为解决病态或非凸优化问题提供了稳健的选择，尽管对于良好条件的问题来说NAG可能更快，但HB-SGE在多种场景下均优于SGD，并且仅需O(d)的内存开销。

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [18] [Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs](https://arxiv.org/abs/2512.10040)
*Skyler Wu,Aymen Echarghaoui*

Main category: cs.LG

TL;DR: 本文提出四种新的权重策略以改进多参考偏好优化(MRPO)方法，实验表明所有新策略在偏好准确性上优于现有MRPO方法。然而，单参考DPO使用任一参考模型时，其表现始终优于测试中的所有多参考方法，对多参考方法的实际应用价值提出了质疑。


<details>
  <summary>Details</summary>
Motivation: 当前用于设置参考权重的方法是临时且统计学上不合理的，导致性能不可靠。为了改善这一点，并更有效地利用多个参考模型的理想属性来微调大型语言模型(LLMs)，作者引入了四种新的权重策略。

Method: 介绍了四种新的权重策略：两种离线方法利用保留验证信号；一种在线方法使用滑动窗口估计器减少过拟合；以及一种将参考权重视为通过汤普森抽样处理的$K$-臂赌博机问题的在线方法。

Result: 实验结果表明，所提出的四种策略在UltraFeedback和SafeRLHF上的偏好准确性方面均优于现有的MRPO加权方法。但令人惊讶的是，使用任何六个参考模型之一的单参考DPO始终优于所有测试的多参考方法。

Conclusion: 尽管提出了改进多参考偏好优化的新策略，但研究发现单参考直接偏好优化(DPO)方法在多数情况下表现更好，这挑战了多参考方法的实际吸引力。

Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.

</details>


### [19] [Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition](https://arxiv.org/abs/2512.10043)
*João Lucas Luz Lima Sarcinelli,Diego Furtado Silva*

Main category: cs.LG

TL;DR: 本文提出了一种新的三步集成管道，用于零样本命名实体识别（NER），特别是针对资源较少的语言如葡萄牙语。通过选择最优的模型组合，并利用最少的标注数据，该方法在四个葡萄牙语NER数据集中优于单个大型语言模型的表现。此外，研究还表明，在不同源数据集上获得的集成通常在跨数据集配置中优于单个模型，这可能消除了当前任务对标注数据的需求。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在许多自然语言处理任务中表现出色，但在它们在命名实体识别（尤其是对于资源较少的语言如葡萄牙语）方面表现不佳。现有的LLM集成主要集中在文本生成或分类上，而忽略了NER。因此，需要一种新的方法来改善低资源语言上的零样本NER性能。

Method: 提出了一种新颖的三步集成管道，使用本地运行的能力相似的小型LLM进行零样本NER。该方法通过启发式手段选择最佳模型组合，以最小化所需标注的数据量。

Result: 本方法在五个葡萄牙语NER数据集中的四个上超越了单独使用的LLM。另外，研究发现，基于不同源数据集构建的集成通常在跨数据集配置中优于单独的LLM，这暗示着可能不需要为当前任务提供额外的标注数据。

Conclusion: 这项工作通过有效结合多个小型LLM而不需微调，推进了可扩展、低资源和零样本NER的发展。

Abstract: Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs enable local deployment, no single model dominates all tasks, motivating ensemble approaches. However, existing LLM ensembles focus on text generation or classification, leaving NER under-explored. In this context, this work proposes a novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs. Our method outperforms individual LLMs in four out of five Portuguese NER datasets by leveraging a heuristic to select optimal model combinations with minimal annotated data. Moreover, we show that ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating the need for annotated data for the current task. Our work advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning. Code is available at https://github.com/Joao-Luz/local-llm-ner-ensemble.

</details>


### [20] [DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting](https://arxiv.org/abs/2512.10051)
*Moulik Gupta,Achyut Mani Tripathi*

Main category: cs.LG

TL;DR: 本文提出了一种名为DB2-TransF的新架构，它使用可学习的小波系数层替代自注意力机制，从而在时间序列预测任务中有效地捕捉多尺度局部和全局模式。实验表明，该方法在保持预测准确性的同时，显著降低了内存使用量。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要能够有效捕捉复杂时间依赖性的模型，特别是在大规模和高维场景下。虽然基于Transformer的架构在建模长距离依赖性方面表现出色，但其二次计算复杂度对可扩展性和适应性构成了限制。

Method: 作者们引入了DB2-TransF，这是一种受Transformer启发的新架构，通过采用可学习的小波系数层来代替传统的自注意力机制。这样的设计旨在更高效地捕捉时间序列中的多尺度局部与全局模式，并加强多个时间序列之间相关性的建模。

Result: 通过对13个标准预测基准进行广泛实验，结果表明DB2-TransF在时间序列预测任务上达到了与传统Transformers相当甚至更好的预测精度，同时大幅减少了内存占用。

Conclusion: DB2-TransF被证明是一种针对高级时间序列预测的可扩展且资源高效的框架。

Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF

</details>


### [21] [Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens](https://arxiv.org/abs/2512.10056)
*Alireza Namazi,Amirreza Dolatpour Fathkouhi,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出了一种名为Soft-Token Trajectory Forecasting (SoTra)的新方法，通过传播连续概率分布来减少暴露偏差，并学习校准的、意识到不确定性的轨迹。在血糖和血压预测中，这种方法分别降低了18%和约15%的临床风险，支持其在安全关键性预测控制中的应用。


<details>
  <summary>Details</summary>
Motivation: 在糖尿病和血液动力学管理的预测控制中，不同操作区间具有不同的临床风险。传统模型使用教师强迫训练时会遇到暴露偏差问题，导致多步预测不稳定，不适合闭环使用。

Method: 提出了Soft-Token Trajectory Forecasting (SoTra)方法，该方法通过传播连续概率分布（称为“软令牌”）来减轻暴露偏差，并能够学习到经过校准且对不确定性敏感的轨迹。此外，还引入了一个风险感知解码模块，旨在最小化预期的临床危害。

Result: 实验结果显示，在血糖预测方面，SoTra平均降低了基于区域的风险达18%；而在血压预测方面，则大约减少了15%的有效临床风险。

Conclusion: SoTra方法在处理暴露偏差以及生成更稳定可靠的多步预测上表现优异，特别是在安全至关重要的预测控制领域内，如糖尿病管理和血液动力学调控等。

Abstract: Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\%. These improvements support its use in safety-critical predictive control.

</details>


### [22] [MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis](https://arxiv.org/abs/2512.10098)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.LG

TL;DR: MedXAI, a framework that integrates deep learning with clinician-derived knowledge, improves medical image classification by enhancing generalization, reducing bias against rare pathologies, and providing understandable explanations. It shows significant improvements in performance and robustness across various datasets and tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges faced by deep learning models in medical AI, such as poor generalization under domain shifts, bias towards infrequent conditions, and lack of transparency, which are critical for safe and effective clinical application.

Method: MedXAI combines deep vision models with expert knowledge from clinicians, aiming to improve model performance on unseen data, reduce bias for rare conditions, and offer human-understandable explanations through feature localization, diverging from conventional post-hoc explanation methods.

Result: Evaluation on ten multicenter datasets across two distinct tasks - Seizure Onset Zone localization and Diabetic Retinopathy grading - demonstrated consistent performance gains, including a 3% improvement in cross-domain generalization and a 10% increase in F1 score for rare classes, outperforming traditional deep learning approaches.

Conclusion: By integrating expert knowledge with deep learning, MedXAI not only enhances the accuracy and reliability of medical image classification but also provides clear, clinically relevant explanations, making it a promising solution for real-world medical AI applications, especially for diagnosing rare diseases.

Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly un- der domain shifts and rare-class conditions. Deep learning mod- els often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Med- ical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician- derived expert knowledge to improve generalization, reduce rare- class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.

</details>


### [23] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: 提出了一种名为CHyLL的新方法，该方法可以在不进行轨迹分割、事件函数或模式切换的情况下学习混合系统的连续神经表示。通过将状态空间重构为分段光滑商流形，CHyLL能够同时在高维空间中学习无奇异性的神经嵌入以及其中的连续流，从而准确预测混合系统的流动，并应用于随机最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有的混合系统学习方法在处理模式切换和流动中的不连续性时存在困难。为了克服这些限制，作者提出了一个新的框架CHyLL，旨在学习一个没有轨迹分割、事件函数或模式切换的混合系统的连续神经表示。

Method: CHyLL的核心思想是通过重置映射将状态空间在防护面上粘合起来，从而将状态空间重新表述为一个分段平滑的商流形，在这个流形上流动变得空间连续。基于这一洞察和基于微分拓扑学的嵌入定理，CHyLL同时在一个更高维度的空间里学习一种无奇点的神经嵌入及其内部的连续流动。

Result: 研究表明，CHyLL可以比现有方法更准确地预测混合系统的流动，并能识别出混合系统的拓扑不变量。此外，CHyLL还被成功应用于解决随机最优控制问题。

Conclusion: CHyLL提供了一种新颖且有效的方法来学习具有连续性和离散时间动态特性的混合系统流动，它不仅提高了预测精度，而且还能揭示有关系统结构的重要信息。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [24] [Partitioning the Sample Space for a More Precise Shannon Entropy Estimation](https://arxiv.org/abs/2512.10133)
*Gabriel F. A. Bastos,Jugurta Montalvão*

Main category: cs.LG

TL;DR: 本文提出了一种新的离散熵估计器，通过结合可分解性、缺失质量估计和未见结果数量来减少负偏差。实验结果显示该方法在样本不足的情况下优于一些经典估计器，并且与最新的先进估计器性能相当。


<details>
  <summary>Details</summary>
Motivation: 从较小的数据集中可靠地进行数据驱动的香农熵估计是一项关键任务，特别是在样本数可能小于可能结果数量的应用中。

Method: 引入了一种利用可分解性属性并结合对缺失质量和未见结果数量估计的方法，以补偿它们引起的负偏差。

Result: 实验证明了所提方法在欠采样情况下比一些传统估计器表现更好，并且与现有的一些先进估计器相比具有竞争力。

Conclusion: 新提出的熵估计方法为处理小数据集中的熵估计问题提供了一个有效的解决方案，特别是在面临样本不足挑战时。

Abstract: Reliable data-driven estimation of Shannon entropy from small data sets, where the number of examples is potentially smaller than the number of possible outcomes, is a critical matter in several applications. In this paper, we introduce a discrete entropy estimator, where we use the decomposability property in combination with estimations of the missing mass and the number of unseen outcomes to compensate for the negative bias induced by them. Experimental results show that the proposed method outperforms some classical estimators in undersampled regimes, and performs comparably with some well-established state-of-the-art estimators.

</details>


### [25] [Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation](https://arxiv.org/abs/2512.10141)
*Sarwan Ali,Taslim Murad,Imdadullah Khan*

Main category: cs.LG

TL;DR: 该论文提出了一种新的拓扑方法，将分子序列转换为图像，结合了混沌游戏表示（CGR）和代数拓扑中的Rips复形构造。这种方法在抗肿瘤肽数据集上的表现优于基于向量、序列语言模型及现有的基于图像的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的分子序列分类特征工程方法存在稀疏性和计算复杂性问题，而深度学习模型在处理表格生物数据时往往表现不佳。

Method: 通过使用混沌游戏表示法(CGR)将序列元素映射到2D坐标，计算成对距离，并构建Rips复形来捕捉局部结构和全局拓扑特征。

Result: 在乳腺癌和肺癌的数据集上分别达到了86.8%和94.5%的准确率，证明了其相对于基于向量、序列语言模型及现有图像方法的优势。

Conclusion: 所提出的拓扑表征不仅保留了重要的序列信息，还使得能够有效利用基于视觉的深度学习架构来进行分子序列分析。

Abstract: Traditional feature engineering approaches for molecular sequence classification suffer from sparsity issues and computational complexity, while deep learning models often underperform on tabular biological data. This paper introduces a novel topological approach that transforms molecular sequences into images by combining Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. Our method maps sequence elements to 2D coordinates via CGR, computes pairwise distances, and constructs Rips complexes to capture both local structural and global topological features. We provide formal guarantees on representation uniqueness, topological stability, and information preservation. Extensive experiments on anticancer peptide datasets demonstrate superior performance over vector-based, sequence language models, and existing image-based methods, achieving 86.8\% and 94.5\% accuracy on breast and lung cancer datasets, respectively. The topological representation preserves critical sequence information while enabling effective utilization of vision-based deep learning architectures for molecular sequence analysis.

</details>


### [26] [Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences](https://arxiv.org/abs/2512.10147)
*Sarwan Ali,Taslim Murad*

Main category: cs.LG

TL;DR: 该研究提出了一种可扩展的嵌入方法，用于生成SARS-CoV-2刺突蛋白序列的紧凑低维表示，从而提高了病毒序列分析中的分类准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的新冠病毒序列分析方法在处理大规模数据集时存在计算成本高、运行时间长等问题，限制了其实用性。

Method: 研究者们针对SARS-CoV-2刺突蛋白区域最常见的谱系开发了一个利用哈希技术生成紧凑低维序列表示的新方法，并使用这些嵌入来训练多种机器学习模型以实现监督谱系分类。

Result: 所提出的方法达到了高达86.4%的分类准确性，同时将嵌入生成时间减少了多达99.81%。

Conclusion: 这种新提出的嵌入方法为大规模病毒序列分析提供了一种快速有效且可扩展的解决方案。

Abstract: Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.

</details>


### [27] [CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation](https://arxiv.org/abs/2512.10178)
*Keito Inoshita,Xiaokang Zhou,Akira Kawai,Katsutoshi Yada*

Main category: cs.LG

TL;DR: 提出了一种名为CIEGAD的数据增强框架，旨在解决数据稀缺和标签分布不均的问题。该框架通过集群条件构建、层级频率-几何分配以及插值与外推合成共存来控制生成方向，并通过几何约束过滤和LLM作为裁判机制来控制质量。实验表明，CIEGAD能够有效地扩展现实世界数据分布的边缘区域，同时保持生成数据与真实数据的高度一致性及语义多样性，在长尾和多分类任务中尤其提高了F1分数和召回率。


<details>
  <summary>Details</summary>
Motivation: 在实际深度学习部署中，数据稀缺性和标签分布不平衡导致了现实世界数据分布中的语义未覆盖区域问题，这阻碍了模型训练并引起了靠近类边界处的误分类及外围区域的行为不稳定。尽管最近的大规模语言模型（LLMs）为数据增强提供了希望，但尚未完全建立一个能同时实现生成方向控制、领域对齐和质量控制的集成框架。

Method: 提出了Cluster-conditioned Interpolative and Extrapolative framework for Geometry-Aware and Domain-aligned data augmentation (CIEGAD)，该框架通过集群条件构造域概况；利用结合类别频率与几何指标的层级频率-几何分配来进行生成分配；通过插值与外推合成共存精细地控制生成方向；并通过结合几何限制过滤器与LLM-as-a-Judge机制执行质量控制。

Result: 在多个分类任务上的实验显示，CIEGAD有效扩展了现实世界数据分布的外围区域，同时维持了生成数据与真实数据之间的高对齐度以及语义多样性。特别是在长尾和多分类任务中，CIEGAD持续提升了F1得分和召回率。

Conclusion: CIEGAD作为一个面向实践的数据增强框架，能够补充代表性不足的区域，同时保持与真实世界数据的一致性。它证明了分布一致性、多样性和质量三者之间的和谐统一。

Abstract: In practical deep learning deployment, the scarcity of data and the imbalance of label distributions often lead to semantically uncovered regions within the real-world data distribution, hindering model training and causing misclassification near class boundaries as well as unstable behaviors in peripheral areas. Although recent large language models (LLMs) show promise for data augmentation, an integrated framework that simultaneously achieves directional control of generation, domain alignment, and quality control has not yet been fully established. To address these challenges, we propose a Cluster-conditioned Interpolative and Extrapolative framework for Geometry-Aware and Domain-aligned data augmentation (CIEGAD), which systematically complements both in-distribution and out-of-distribution semantically uncovered regions. CIEGAD constructs domain profiles through cluster conditioning, allocates generation with a hierarchical frequency-geometric allocation integrating class frequency and geometric indicators, and finely controls generation directions via the coexistence of interpolative and extrapolative synthesis. It further performs quality control through geometry-constrained filtering combined with an LLM-as-a-Judge mechanism. Experiments on multiple classification tasks demonstrate that CIEGAD effectively extends the periphery of real-world data distributions while maintaining high alignment between generated and real-world data as well as semantic diversity. In particular, for long-tailed and multi-class classification tasks, CIEGAD consistently improves F1 and recall, validating the triple harmony of distributional consistency, diversity, and quality. These results indicate that CIEGAD serves as a practically oriented data augmentation framework that complements underrepresented regions while preserving alignment with real-world data.

</details>


### [28] [MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification](https://arxiv.org/abs/2512.10187)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou*

Main category: cs.LG

TL;DR: 本文介绍了miniF2F-Dafny，这是将数学推理基准miniF2F首次翻译成自动化定理证明器Dafny的工作。结果显示，对于测试集和验证集，Dafny的自动化处理能力无需手动证明步骤即可验证相当一部分问题。对于无法通过空证明解决的问题，作者评估了12个现成的大规模语言模型（LLMs）提供证明提示的能力，并发现最佳模型在采用迭代错误修正后能达到55.7%的成功率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将现有的仅存在于交互式定理证明器中的数学推理基准miniF2F转换为自动化定理证明器版本，以探索自动化系统与大规模语言模型如何协作来提高数学证明的自动化程度。

Method: 方法包括将miniF2F转换到Dafny上，然后使用Dafny尝试自动验证问题；对于Dafny无法单独完成的情况，则利用12种不同的大型语言模型来生成证明提示，并采用了迭代错误修正策略来提高成功率。

Result: 结果表明，Dafny能够独立验证测试集和验证集中约40%至45%的问题而不需要任何手动干预。此外，当结合大型语言模型提供的证明提示时，在特定条件下可以达到超过50%的成功率。

Conclusion: 结论是，通过将低级细节交给自动化工具处理，同时利用大型语言模型提供高层次指导，可以在数学证明任务中实现有效的分工合作。

Abstract: We present miniF2F-Dafny, the first translation of the mathematical reasoning benchmark miniF2F to an automated theorem prover: Dafny. Previously, the benchmark existed only in interactive theorem provers (Lean, Isabelle, HOL Light, Metamath). We find that Dafny's automation verifies 99/244 (40.6%) of the test set and 109/244 (44.7%) of the validation set with empty proofs--requiring no manual proof steps. For problems where empty proofs fail, we evaluate 12 off-the-shelf LLMs on providing proof hints. The best model we test achieves 55.7% pass@4 success rate employing iterative error correction. These preliminary results highlight an effective division of labor: LLMs provide high-level guidance while automation handles low-level details. Our benchmark can be found on GitHub at http://github.com/dafny-lang/miniF2F .

</details>


### [29] [Federated Domain Generalization with Latent Space Inversion](https://arxiv.org/abs/2512.10224)
*Ragja Palakkadavath,Hung Le,Thanh Nguyen-Tang,Svetha Venkatesh,Sunil Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦领域泛化方法，通过采用潜在空间反转技术来增强本地客户端训练中的领域不变性，并且提出了一种重要权重聚合策略来改善模型聚合过程。实验表明该方法在保证隐私的同时优于现有技术，并减少了通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦领域泛化（FedDG）旨在解决联邦学习框架中客户端之间的分布偏移问题。虽然当前的FedDG方法能够提升全局模型的泛化能力，但它们往往通过共享客户端数据统计信息的方式损害了数据隐私。

Method: 提出了两种主要方法：1. 通过一种称为‘潜在空间反转’的新技术，在本地模型训练过程中强制执行领域不变性；2. 提出了一种‘重要权重’聚合策略，以优先考虑对局部模型预测有显著影响的参数进行聚合，特别是在客户端不是独立同分布的情况下。

Result: 广泛的实验结果显示，所提方法不仅在性能上超越了最新的方法，而且还在减少通信开销方面表现得更好。

Conclusion: 本文介绍的方法为联邦领域泛化提供了一个有效解决方案，能够在不牺牲隐私的前提下提高模型的泛化能力和效率。

Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.

</details>


### [30] [Adaptive Information Routing for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.10229)
*Jun Seo,Hyeokjun Choe,Seohui Bae,Soyeon Park,Wonbin Ahn,Taeyoon Lim,Junhyuk Kang,Sangjun Han,Jaehoon Lee,Dongwan Kang,Minjae Kim,Sungdong Yoo,Soonyoung Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的多模态时间序列预测框架——自适应信息路由（AIR），该框架利用文本信息动态指导时间序列模型，以控制多元时间序列信息的组合方式和程度。实验结果表明，AIR通过文本输入有效调节了时间序列模型的行为，显著提高了多种时间序列预测任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列预测方法主要依赖历史数据进行未来值预测，在实际场景中由于可用信息有限而常显不足。为解决这一问题，研究探索了将文本数据等额外数据模态与时间序列数据结合的方法，旨在提高预测准确性。

Method: 提出了自适应信息路由（AIR）框架，不同于现有将文本数据视为可互换辅助特征的做法，AIR利用文本信息来动态地指导时间序列模型如何以及在何种程度上组合多元时间序列信息。此外，还介绍了一个文本精炼流程，使用大型语言模型将原始文本数据转换为适合多模态预测的形式，并建立了一个基于此流程的基准测试。

Result: 通过对真实世界市场数据如原油价格和汇率的实验表明，AIR能够有效地利用文本输入调节时间序列模型的行为，在各种时间序列预测任务中显著提升了预测准确性。

Conclusion: 本研究表明，通过引入自适应信息路由（AIR）框架，可以更有效地利用文本信息来改进时间序列预测模型的表现，从而在现实世界的预测任务中实现更高的准确度。

Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.

</details>


### [31] [R^2-HGP: A Double-Regularized Gaussian Process for Heterogeneous Transfer Learning](https://arxiv.org/abs/2512.10258)
*Duo Wang,Xinming Wang,Chao Wang,Xiaowei Yue,Jianguo Wu*

Main category: cs.LG

TL;DR: 本文提出了一种双正则异构高斯过程框架（R^2-HGP），通过可训练的先验概率映射模型对齐异构输入域，并结合物理知识作为正则项，同时在多源转移GP模型中加入稀疏惩罚来选择最相关的信息源输出并抑制负迁移。该方法在多种评估指标上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的多输出高斯过程(MGP)模型尽管因其灵活性和不确定性量化能力被广泛应用于多源迁移学习场景中，但面对不同领域间输入空间异质性、忽略潜在先验知识与物理信息以及不当的信息共享导致负迁移等问题时仍存在不足。

Method: 提出了一个名为R^2-HGP的新框架，该框架包括：1) 一种可训练的先验概率映射模型用于对齐异构输入；2) 结合物理知识作为正则化项以确保对齐结果符合已知物理规律；3) 在多源迁移GP模型内引入稀疏惩罚机制来自动选取最具信息量的源输出并避免负迁移现象。整个结构集成在一个基于条件变分自编码器(CVAE)的新框架下。

Result: 通过广泛的模拟实验及实际工程案例研究证明了所提R^2-HGP方法的有效性，在各种评估标准下均优于现有最先进的基准方法。

Conclusion: 提出的R^2-HGP框架有效解决了传统多输出高斯过程中存在的异构输入对齐难题、未能充分利用物理知识的问题以及不当信息分享造成的负面影响，为实现更稳定可靠的跨领域迁移提供了新的解决方案。

Abstract: Multi-output Gaussian process (MGP) models have attracted significant attention for their flexibility and uncertainty-quantification capabilities, and have been widely adopted in multi-source transfer learning scenarios due to their ability to capture inter-task correlations. However, they still face several challenges in transfer learning. First, the input spaces of the source and target domains are often heterogeneous, which makes direct knowledge transfer difficult. Second, potential prior knowledge and physical information are typically ignored during heterogeneous transfer, hampering the utilization of domain-specific insights and leading to unstable mappings. Third, inappropriate information sharing among target and sources can easily lead to negative transfer. Traditional models fail to address these issues in a unified way. To overcome these limitations, this paper proposes a Double-Regularized Heterogeneous Gaussian Process framework (R^2-HGP). Specifically, a trainable prior probability mapping model is first proposed to align the heterogeneous input domains. The resulting aligned inputs are treated as latent variables, upon which a multi-source transfer GP model is constructed and the entire structure is integrated into a novel conditional variational autoencoder (CVAE) based framework. Physical insights is further incorporated as a regularization term to ensure that the alignment results adhere to known physical knowledge. Next, within the multi-source transfer GP model, a sparsity penalty is imposed on the transfer coefficients, enabling the model to adaptively select the most informative source outputs and suppress negative transfer. Extensive simulations and real-world engineering case studies validate the effectiveness of our R^2-HGP, demonstrating consistent superiority over state-of-the-art benchmarks across diverse evaluation metrics.

</details>


### [32] [An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis](https://arxiv.org/abs/2512.10308)
*Vasiliki Stoumpou,Maciej Tysarowski,Talhat Azemi,Jawad Haider,Howard L. Haronian,Robert C. Hagberg,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 研究提出了一种可解释的处方框架，结合了预后匹配、反事实结果建模和最优策略树（OPT），为低至中等风险的严重主动脉瓣狭窄患者提供个体化治疗建议，以最小化5年预期死亡率。通过哈特福德医院和圣文森特医院的数据应用该框架，估计出相比实际治疗方案，5年死亡率分别降低了20.3%和13.8%，表明该方法在提高长期预估结果方面具有潜力，并且能够推广到不同机构的数据上。


<details>
  <summary>Details</summary>
Motivation: 对于低至中度风险的严重主动脉瓣狭窄患者，在选择外科主动脉瓣置换术(SAVR)与经导管主动脉瓣置换术(TAVR)之间存在差异，这主要是由于患者异质性和医疗机构偏好导致。尽管现有模型可以预测术后风险，但缺乏直接优化长期结果的可解释个性化治疗推荐方案。

Method: 研究引入了一个可解释的处方框架，该框架整合了预后匹配、反事实结局建模以及一个最佳政策树(OPT)，旨在推荐一种能最小化五年内预期死亡率的治疗方法。利用来自哈特福德医院和圣文森特医院的数据，通过预后匹配及样本加权模拟随机分配，并估算两种手术方式下的反事实死亡率。基于这些反事实预测训练得到的政策模型，将病人划分成临床相关的子群，并推荐与较低估计风险相关联的疗法。

Result: 如果按照OPT提供的建议进行治疗，反事实评估显示在哈特福德医院和圣文森特医院的五年死亡率相比于现实生活中的治疗方案分别减少了20.3%和13.8%，展示了良好的泛化能力，即使是在不同机构未见过的数据上也表现良好。所学习到的决策边界与现实世界的结果及临床观察相一致。

Conclusion: 据我们所知，这是首个提供透明且数据驱动的TAVR与SAVR之间推荐方案的研究，不仅改善了内部和外部队列的预计长期结果，同时也保持了临床相关性，为结构性心脏病的精准医疗提供了更加系统和基于证据的方法论支持。

Abstract: Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.
  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.
  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\% in Hartford and 13.8\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.
  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.

</details>


### [33] [A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale](https://arxiv.org/abs/2512.10341)
*Vinoth Punniyamoorthy,Ashok Gadi Parthi,Mayilsamy Palanigounder,Ravi Kiran Kodali,Bikesh Kumar,Kabilan Kannan*

Main category: cs.LG

TL;DR: 本文提出了一种云原生的隐私保护架构，集成了联邦学习、差分隐私、零知识合规证明以及强化学习驱动的自适应治理，旨在分布式机器学习系统中实现强隐私保障、可验证合规性和跨异构及多云环境的可扩展部署。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习系统需要强大的隐私保证、可验证的合规性以及在异构和多云环境中的可扩展部署。

Method: 该研究介绍了一种云原生隐私保护架构，它结合了联邦学习、差分隐私、零知识合规证明和由强化学习支持的自适应治理。

Result: 原型系统在混合Kubernetes集群上的部署表明，该架构能够减少成员推理风险、一致地执行正式隐私预算，并且在差分隐私下保持模型性能稳定。实验评估显示，该架构在提供持续的风险意识治理的同时，以最小的开销保持了实用性。

Conclusion: 所提出的框架为大规模部署可信且合规的分布式机器学习系统奠定了实际基础。

Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deploy- ment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero- knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership- inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Ex- perimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The pro- posed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.

</details>


### [34] [Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories](https://arxiv.org/abs/2512.10350)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 本文提出了一种几何框架，用于分析语义嵌入空间中的能动轨迹，并通过控制实验识别了两种基本的动态机制：一种是趋向稳定吸引子的收缩重写循环，另一种是产生无界发散的探索性总结和否定循环。结果表明，提示设计直接决定了能动循环的动力学机制，从而能够系统地控制大型语言模型迭代转换中的收敛、发散及轨迹结构。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型的能动系统通过递归反馈回路运行，每个输出成为下一个输入。然而，这些能动回路的几何行为（是否收敛、发散或表现出更复杂的动态）仍不为人所充分理解。因此，需要引入新的分析方法来更好地理解这些系统的动力学特性。

Method: 本文介绍了一个用于在语义嵌入空间中分析能动轨迹的几何框架，将迭代变换视为离散的动力系统。研究区分了发生语言变换的艺术品空间与执行几何测量的嵌入空间。鉴于余弦相似度受嵌入各向异性影响而存在偏差，研究引入了一种等时校准方法，以消除系统偏差并使相似度与人类语义判断对齐，同时保持高局部稳定性。

Result: 通过控制实验发现了两种基本机制：一种为收缩重写循环，其趋向于一个具有减少分散性的稳定吸引子；另一种为探索性总结和否定循环，产生没有聚类形成的无界发散。这两种机制显示了收缩与扩张质量上不同的几何特征。

Conclusion: 研究表明，提示设计直接管理着能动循环的动力学机制，这使得能够在大型语言模型的迭代转换过程中系统地控制收敛、发散以及轨迹结构。

Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems.
  We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors.
  Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion.
  Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.

</details>


### [35] [GPG: Generalized Policy Gradient Theorem for Transformer-based Policies](https://arxiv.org/abs/2512.10365)
*Hangyu Mao,Guangting Dong,Zhicheng Dou*

Main category: cs.LG

TL;DR: 本文提出了一个针对基于Transformer策略的广义策略梯度定理（GPG），并展示了标准策略梯度定理和GRPO是GPG框架下的特例。此外，还探讨了GPG在大型语言模型训练中的应用，为有效策略优化提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 本文旨在开发一种新的方法来改进基于Transformer架构的策略学习效率，并通过提出广义策略梯度（GPG）定理为大型语言模型的训练提供更加高效和灵活的策略优化途径。

Method: 作者们提出了广义策略梯度（GPG）定理，该定理专门为基于Transformer的策略设计。他们证明了现有的标准策略梯度定理以及GRPO都是GPG框架下的特殊情况。

Result: 研究表明，GPG不仅统一了已有的几种重要策略优化方法，而且在应用于大型语言模型时展现了其独特的优势，为这类模型的训练带来了新的视角和技术手段。

Conclusion: 通过引入广义策略梯度定理，本研究不仅为基于Transformer的策略提供了一个更广泛的理论基础，同时也揭示了它在提高大型语言模型训练效率方面的潜力。

Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.

</details>


### [36] [Fitting magnetization data using continued fraction of straight lines](https://arxiv.org/abs/2512.10390)
*Vijay Prakash S*

Main category: cs.LG

TL;DR: 本文研究了铁磁物质在外部磁场作用下的磁化非线性行为，通过使用连分式的直线组合来近似这种非线性函数，并利用该模型解释了磁畴增长和缩小过程中的非线性行为。


<details>
  <summary>Details</summary>
Motivation: 为了理解铁磁材料在外加磁场下磁化的非线性特性，特别是磁畴如何随磁场强度变化而重新排列的过程。

Method: 采用连分式形式的代数表达式来近似磁化曲线的非线性部分，并通过非线性回归估计参数。

Result: 成功地用连分式的直线组合近似描述了磁化过程中的非线性行为，为理解和分析磁畴变化提供了新的数学工具。

Conclusion: 提出的方法能够有效地模拟铁磁材料磁化过程中的非线性现象，对进一步研究磁畴动态行为具有重要意义。

Abstract: Magnetization of a ferromagnetic substance in response to an externally applied magnetic field increases with the strength of the field. This is because at the microscopic level, magnetic moments in certain regions or domains of the substance increasingly align with the applied field, while the amount of misaligned domains decreases. The alignment of such magnetic domains with an applied magnetic field forms the physical basis for the nonlinearity of magnetization. In this paper, the nonlinear function is approximated as a combination of continued fraction of straight lines. The resulting fit is used to interpret the nonlinear behavior in both growing and shrinking magnetic domains. The continued fraction of straight lines used here is an algebraic expression which can be used to estimate parameters using nonlinear regression.

</details>


### [37] [Metacognitive Sensitivity for Test-Time Dynamic Model Selection](https://arxiv.org/abs/2512.10451)
*Le Tuan Minh Trinh,Le Minh Vu Pham,Thi Minh Anh Pham,An Duc Nguyen*

Main category: cs.LG

TL;DR: 该论文提出了一种新的框架，用于评估和利用AI的元认知。通过引入基于心理学的元认知敏感性度量——meta-d'，来表征模型自信度与其准确度之间的关系，并使用这一动态敏感性分数作为上下文来进行测试时的模型选择。实验表明，这种元认知方法能够提高联合推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然可以表达对自己预测的信心，但往往校准不佳，存在一种认知偏差：即所表达的信心并不反映其真实能力。研究旨在探索模型是否真的了解它们自己知道什么，并从人类认知科学中汲取灵感，提出了一个新的框架来评估和利用AI的元认知能力。

Method: 提出了一种名为meta-d'的心理学基础指标，用于衡量元认知敏感性；接着利用这一敏感性得分作为背景信息输入至一个基于强盗算法（bandit-based）的决策者，该决策者在测试阶段学会根据特定任务选择信任哪个专家模型。

Result: 跨多个数据集及不同深度学习模型组合（包括CNNs和VLMs）的实验显示，采用元认知方法相较于单独使用的构成模型，在联合推理准确性上有所提升。

Conclusion: 这项工作为AI模型提供了一个新颖的行为解释视角，将集成选择重新定义为评估短期信号（信心预测分数）与中期特征（元认知敏感性）的问题。

Abstract: A key aspect of human cognition is metacognition - the ability to assess one's own knowledge and judgment reliability. While deep learning models can express confidence in their predictions, they often suffer from poor calibration, a cognitive bias where expressed confidence does not reflect true competence. Do models truly know what they know? Drawing from human cognitive science, we propose a new framework for evaluating and leveraging AI metacognition. We introduce meta-d', a psychologically-grounded measure of metacognitive sensitivity, to characterise how reliably a model's confidence predicts its own accuracy. We then use this dynamic sensitivity score as context for a bandit-based arbiter that performs test-time model selection, learning which of several expert models to trust for a given task. Our experiments across multiple datasets and deep learning model combinations (including CNNs and VLMs) demonstrate that this metacognitive approach improves joint-inference accuracy over constituent models. This work provides a novel behavioural account of AI models, recasting ensemble selection as a problem of evaluating both short-term signals (confidence prediction scores) and medium-term traits (metacognitive sensitivity).

</details>


### [38] [Hybrid Physics-ML Model for Forward Osmosis Flux with Complete Uncertainty Quantification](https://arxiv.org/abs/2512.10457)
*Shiv Ratn,Shivang Rampriyan,Bahni Ray*

Main category: cs.LG

TL;DR: 本研究提出了一种新的鲁棒混合物理-机器学习框架，使用高斯过程回归（GPR）对前向渗透（FO）技术中的水通量进行高度准确且具有不确定性意识的预测。通过在详细非线性FO物理模型预测与实际实验水通量之间的残差误差上训练GPR，并实现完整的不确定性量化方法，该模型即使在数据点较少的情况下也达到了极高的准确性。


<details>
  <summary>Details</summary>
Motivation: 前向渗透（FO）作为一种低能耗膜分离技术前景广阔，但其内部复杂的质量传递现象导致了对其水通量建模的挑战。传统的机制模型难以处理经验参数的变化性问题，而纯数据驱动模型则缺乏物理一致性及严格的不确定性量化。

Method: 本研究采用了一种创新性的鲁棒混合物理-机器学习框架，利用高斯过程回归（GRP）基于FO物理模型预测值与实际实验水通量之间的残差来进行训练。此外，通过将总预测方差分解为模型不确定性（认识论）和输入不确定性（偶然性），实现了全面的不确定性量化分析。

Result: 该模型仅用120个数据点训练后，在独立测试集上达到了平均绝对百分比误差（MAPE）0.26%和决定系数R² 0.999的成绩，证明了其作为FO工艺优化和数字孪生开发的强大可靠的替代模型的有效性。

Conclusion: 研究表明，结合了物理模型精确度与机器学习灵活性的新框架能够提供更加准确且具备不确定性评估能力的水通量预测结果，为前向渗透技术的发展提供了强有力的支持。

Abstract: Forward Osmosis (FO) is a promising low-energy membrane separation technology, but challenges in accurately modelling its water flux (Jw) persist due to complex internal mass transfer phenomena. Traditional mechanistic models struggle with empirical parameter variability, while purely data-driven models lack physical consistency and rigorous uncertainty quantification (UQ). This study introduces a novel Robust Hybrid Physics-ML framework employing Gaussian Process Regression (GPR) for highly accurate, uncertainty-aware Jw prediction. The core innovation lies in training the GPR on the residual error between the detailed, non-linear FO physical model prediction (Jw_physical) and the experimental water flux (Jw_actual). Crucially, we implement a full UQ methodology by decomposing the total predictive variance (sigma2_total) into model uncertainty (epistemic, from GPR's posterior variance) and input uncertainty (aleatoric, analytically propagated via the Delta method for multi-variate correlated inputs). Leveraging the inherent strength of GPR in low-data regimes, the model, trained on a meagre 120 data points, achieved a state-of-the-art Mean Absolute Percentage Error (MAPE) of 0.26% and an R2 of 0.999 on the independent test data, validating a truly robust and reliable surrogate model for advanced FO process optimization and digital twin development.

</details>


### [39] [Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2512.10510)
*Chihyeon Song,Jaewoo Lee,Jinkyoo Park*

Main category: cs.LG

TL;DR: 本文提出了一种自适应重放缓冲区(ARB)方法，通过动态优先采样数据来解决离线到在线强化学习(O2O RL)中固定离线数据集与新收集的在线经验之间的平衡问题。实验表明，ARB能够有效缓解早期性能下降，并显著提高各种O2O RL算法的最终表现。


<details>
  <summary>Details</summary>
Motivation: 离线至在线强化学习（O2O RL）在利用固定的离线数据集和新收集的在线经验之间面临一个关键的困境。标准方法通常依赖于一个固定的数据混合比例，在初期学习稳定性和渐近性能之间难以取得良好平衡。

Method: 提出了自适应重放缓冲区(ARB)，这是一种基于轻量级指标'on-policyness'动态优先数据采样的新方法。该方法不需要复杂的学习过程或固定的比率，而是评估收集到的轨迹与当前策略行为的接近程度，并为轨迹内的每个转换分配相应的采样权重。

Result: 广泛的实验表明，ARB能够在D4RL基准测试上一致地减轻早期性能退化，并且显著改善多种O2O RL算法的最终表现。

Conclusion: 研究表明，一种适应性强、能感知行为变化的重放缓冲区设计对于提升O2O RL算法的整体性能至关重要。

Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.

</details>


### [40] [Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees](https://arxiv.org/abs/2512.10522)
*Zahra Rahiminasab,Michael Yuhas,Arvind Easwaran*

Main category: cs.LG

TL;DR: 本文提出了一种解缠蒸馏编码器（DDE）框架，旨在减少多标签分布外（OOD）推理器的大小，以便在资源受限设备上部署，同时保持解缠。通过将学生-教师蒸馏模型压缩形式化为约束优化问题，并引入解缠约束来实现这一目标。基于Rademacher复杂度提供了理论保证，并通过实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了能够在资源受限的设备上部署多标签分布外（OOD）推理器，同时保持解缠属性，即维持潜在维度与生成因素或图像重要特征之间的一对多映射关系。

Method: 提出了一个名为解缠蒸馏编码器（DDE）的新框架，它将学生-教师蒸馏过程中的模型压缩问题转化为带有限制条件的优化问题，这些限制条件用于确保解缠属性得到保留。此外，还给出了基于Rademacher复杂度的理论分析，以支持解缠过程中进行蒸馏操作的可能性。

Result: 通过实验评估表明，所提出的压缩模型可以在保持良好解缠性质的同时成功地应用于实际场景中，比如部署到NVIDIA等硬件平台上。

Conclusion: 解缠蒸馏编码器（DDE）提供了一种有效的方法，在不影响解缠性能的前提下减少了分布外推理器的规模，使之更适合于资源受限环境下的应用。

Abstract: Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA

</details>


### [41] [Mode-Seeking for Inverse Problems with Diffusion Models](https://arxiv.org/abs/2512.10524)
*Sai Bharath Chandra Gutha,Ricardo Vinuesa,Hossein Azizpour*

Main category: cs.LG

TL;DR: 本文提出了一种变分模式寻优损失（VML），它在每个反向扩散步骤中最小化时，可以引导生成的样本趋向于最大后验估计。对于线性逆问题，VML可以被解析地推导出来而无需近似。基于进一步的理论见解，提出了VML-MAP算法，通过多种数据集上的广泛图像恢复任务实验验证了其在性能和计算时间上优于现有方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的后验采样和MAP估计方法往往依赖于建模近似，并且计算需求较高。为了解决这个问题，研究者们旨在开发一种新的方法，在不牺牲准确性的同时减少计算成本，特别是针对任意逆问题使用预训练无条件扩散模型的情况。

Method: 研究者引入了一种名为变分模式寻优损失（VML）的新方法，该方法在每个反向扩散步骤中最小化时，能促使生成样本朝向最大后验估计方向发展。特别地，对于线性逆问题，研究者能够对VML进行解析推导，避免了近似处理。基于此，他们设计了一个称为VML-MAP的实际算法。

Result: 通过在多个数据集上执行广泛的图像恢复任务实验，证实了VML-MAP算法不仅在解决逆问题方面表现优异，而且相较于现有技术而言显著减少了所需计算时间。

Conclusion: 本研究表明，采用变分模式寻优损失（VML）与提出的VML-MAP算法相结合的方式，能够在保持高精度的同时有效降低求解逆问题的计算复杂度，特别是在处理图像恢复等任务时展现出明显优势。

Abstract: A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.

</details>


### [42] [Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders](https://arxiv.org/abs/2512.10547)
*Qingsen Ma,Dianyun Wang,Jiaming Lyu,Yaoye Wang,Lechen Ning,Sujie Zhu,Zhenbo Xu,Liuyu Xiang,Huining Li,Huijia Wu,Zhaofeng He*

Main category: cs.LG

TL;DR: 本文提出了一种名为STA-Attention的框架，该框架利用Top-K稀疏自动编码器将键值缓存分解为可解释的“语义原子”。通过引入双重预算策略选择性地保留最具信息量的语义组件，同时过滤表示噪声。实验结果表明，这种语义重构方法在保持与原始模型相当的困惑度和零样本性能的同时，有效地结合了机械解释性和忠实注意力建模。


<details>
  <summary>Details</summary>
Motivation: 长上下文大型语言模型中的键值（KV）缓存是主要的记忆瓶颈，但通常被视为不透明的数值张量处理。为了克服这一问题并提高KV缓存的可解释性及效率，作者提出了新的方法。

Method: 提出STA-Attention框架，使用Top-K稀疏自动编码器（而非标准的L1正则化SAE）来消除收缩偏差，确保注意力所需的精确点积几何结构得以保持。基于发现的关键值不对称性，即键向量作为高度稀疏的路由器而值向量携带密集内容负载，引入了双预算策略以优化语义成分的选择。

Result: 在Yi-6B、Mistral-7B、Qwen2.5-32B等模型上的实验表明，所提出的语义重构方法能够维持与原始模型相媲美的困惑度和零样本性能，成功地连接了机制可解释性与忠实注意力建模之间的桥梁。

Conclusion: 通过STA-Attention框架以及双预算策略的应用，可以有效提升长上下文大型语言模型中键值缓存的可解释性和效率，同时保持良好的性能指标。

Abstract: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.

</details>


### [43] [Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning](https://arxiv.org/abs/2512.10573)
*Yi Huang,Qingyun Sun,Yisen Gao,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了一种新的抗标签噪声的信息瓶颈方法LaT-IB，通过引入"最小-充分-清洁"（MSC）标准和三阶段训练框架来提高模型在标签噪声情况下的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈原则虽然有助于有效的表示学习，但其对准确标签的高度依赖使其容易受到现实世界中普遍存在的标签噪声的影响，导致性能下降和过拟合问题。

Method: LaT-IB方法通过引入一个作为互信息正则化器的“最小-充分-清洁”(MSC)准则，并采用噪声感知的潜在解缠将潜在表示分解为与干净标签空间和噪声空间对齐的组件。此外，设计了包括预热、知识注入和鲁棒训练三个阶段的训练框架，逐步引导模型朝向抗噪声表示。

Result: 广泛的实验表明，LaT-IB在标签噪声下实现了优越的鲁棒性和效率，显著提高了在带有标签噪声的真实场景中的鲁棒性和适用性。

Conclusion: LaT-IB方法通过理论上的分析及实践验证被证明能有效解决因标签噪声引起的问题，增强了模型在真实应用场景中的表现。

Abstract: The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a "Minimal-Sufficient-Clean" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.

</details>


### [44] [Multi-Objective Reward and Preference Optimization: Theory and Algorithms](https://arxiv.org/abs/2512.10601)
*Akhil Agnihotri*

Main category: cs.LG

TL;DR: 该论文发展了理论框架和算法，推动了约束强化学习在控制、偏好学习和大型语言模型对齐方面的进步。提出了平均约束策略优化（ACPO）算法处理平均成本准则下的约束马尔可夫决策过程，并通过e-COP方法扩展到有限时间范围设置。此外，还研究了基于人类偏好的强化学习，提出了warmPref-PS和PSPL算法以提高数据收集效率和最优策略识别的鲁棒性。最后，将这些方法应用于大规模模型对齐，提出了MOPO算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决在不同场景下（包括长期运行系统、周期性任务以及需要考虑人类偏好的环境）安全有效地进行决策的问题，同时确保大型语言模型能够按照人类价值观进行对齐。

Method: 1. 平均约束策略优化（ACPO）算法：结合敏感性分析与信任区域更新来处理约束。
2. e-COP：首个用于解决周期性CMDP问题的策略优化方法。
3. warmPref-PS：一种线性bandit的后验抽样策略，整合离线偏好数据。
4. PSPL算法：联合采样奖励模型和从成对轨迹比较中获得的转移动态。
5. MOPO：一个多目标约束优化视角下的迭代算法，适用于数十亿参数的语言模型。

Result: 1. ACPO展示了最先进的经验性能，并且具有理论保证。
2. e-CPO在安全性至关重要的环境中提供了可证明的表现、简单性和可扩展性。
3. warmPref-PS显著减少了遗憾并提高了RLHF的数据收集效率。
4. PSPL提供了贝叶斯简单遗憾保证，并在实践中稳健地识别出最佳策略。
5. MOPO算法能够扩展至拥有数十亿参数的语言模型，在多种对齐设置下保持鲁棒性。

Conclusion: 本论文统一了约束强化学习的不同范式，包括平均成本、周期性和基于偏好的方法，为安全且符合人类价值取向的决策制定提供了理论上的进展和实用工具。

Abstract: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.

</details>


### [45] [Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach](https://arxiv.org/abs/2512.10633)
*C. Bosco,U. Minora,D. de Rigo,J. Pingsdorf,R. Cortinovis*

Main category: cs.LG

TL;DR: 本文提出了一种混合方法，用于预测欧洲五个主要移民路线上的非法越境情况，时间跨度为一年。该方法结合了机器学习技术和移民专家的定性见解，旨在通过纳入人为评估的协变量来提高数据驱动模型的预测能力，直接回应了欧盟移民和庇护条约中提出的预测需求，并通过已知数据测试和验证了其适用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 针对突然的移民模式变化及传统数据集局限性带来的挑战，以及响应欧盟移民与庇护条约中提到的预测需求，旨在开发一种能够结合数据驱动建模与专家判断的新工具，以支持更有效的欧盟移民治理策略决策、预警系统和团结机制。

Method: 采用了一种将机器学习技术与迁移专家提供的定性见解相结合的方法论。特别是引入了一个由人类评估的协变量，以此来增强纯数据驱动模型在面对突发移民趋势变动时的预测性能。

Result: 所提出的方法论经过了实际数据的测试和验证，证明了它在移民相关政策背景下的应用价值和可信度。

Conclusion: 本研究通过整合定量分析（如机器学习）与定性输入（来自领域专家），为欧盟成员国提供了一种新颖且实用的操作工具，有助于改善移民管理相关的战略决策过程。

Abstract: This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.

</details>


### [46] [Token Sample Complexity of Attention](https://arxiv.org/abs/2512.10656)
*Léa Bohbot,Cyril Letrouit,Gabriel Peyré,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 本研究引入了token-sample复杂度的概念，用于衡量在处理n个token时注意力机制向无限token极限收敛的速度。对于具有紧支撑（以及更广泛的亚高斯）分布的情况，研究表明注意力图在半径为R的球上以C(R)/√n的速率一致收敛，其中C(R)随R呈指数增长。针对大R值下估计失去实用价值的问题，研究进一步确定了变换分布矩的收敛率。此外，还探讨了当注意力参数趋于无穷大、softmax接近hardmax时，对数收敛率的情况。实验通过合成高斯数据和真实BERT模型在维基百科文本上的应用验证了这些预测。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型中上下文窗口的不断扩大，了解在极端序列长度下注意力机制的行为变得至关重要。为了更好地理解这一行为，本文提出了token-sample复杂度概念，旨在探索注意力机制如何随着输入序列长度的增长而变化，并且在理论上给出其收敛性分析。

Method: 研究人员首先定义了token-sample复杂度，然后从两个层面评估了有限n下的收敛边界：一是注意力图的点态一致收敛；二是转换后的token分布的矩收敛。对于具有紧支撑或亚高斯分布的数据，研究提供了具体的收敛速度公式。同时，文章还考察了当注意力参数趋向于无穷大时，softmax函数接近hardmax函数情况下注意力机制的收敛特性。

Result: 研究发现，对于具有紧支撑或亚高斯特性的分布，注意力图在给定半径R的球上以C(R)/√n的速度一致收敛，其中C(R)随R呈指数级增长。对于较大R值，该估计失去实际意义，因此进一步研究了变换后分布矩的收敛情况，此时收敛速率为C'(R)/n^β (β<1/2)，且C'(R)与分布支持大小成多项式关系。此外，在特定条件下，还观察到了对数级别的收敛速度。

Conclusion: 通过理论分析和实证研究相结合的方法，本文不仅揭示了不同条件下注意力机制随序列长度增加而表现出的不同收敛特性，而且为未来设计更高效的语言模型提供了一定指导意义。特别是关于注意力层输出特性的深入理解，有助于优化模型架构选择及参数设置。

Abstract: As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^β$ with $β<\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $β$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.

</details>


### [47] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: 本文提出了一种名为DCFO的新方法，专门用于为LOF生成反事实解释，通过将数据空间划分为LOF平滑行为的区域来实现高效基于梯度的优化。实验结果表明，DCFO在接近性和有效性方面优于基准竞争对手。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实解释方法大多忽略了异常值检测带来的独特挑战，并且未能针对经典的、广泛采用的异常值检测算法。局部异常因子（LOF）是一种流行的无监督异常值检测方法，但缺乏可解释性。

Method: 提出了密度基础的异常值反事实（DCFO）方法，该方法通过划分数据空间以允许基于梯度的优化来为LOF生成反事实解释。

Result: 在50个OpenML数据集上的广泛实验验证表明，与基准方法相比，DCFO生成的反事实具有更好的接近性和有效性。

Conclusion: DCFO提供了一种有效的方法来增强LOF的可解释性，从而帮助理解异常值背后的因素，并采取预防措施避免未来出现类似异常值。

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [48] [Learning by Analogy: A Causal Framework for Composition Generalization](https://arxiv.org/abs/2512.10669)
*Lingjing Kong,Shaoan Xie,Yang Jiao,Yetian Chen,Yanhui Guo,Simone Shao,Yan Gao,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个基于因果模块化和最小变化原则的层次数据生成过程，以促进组合泛化能力。该方法能够支持组成概念之间的复杂关系，并且可以从文本-图像对等可观察数据中恢复潜在的层次结构。实验结果表明，这一理论框架在基准数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 尽管组合泛化能力（即理解和生成已学习概念的新组合的能力）对于模型扩展其超出有限经验的能力至关重要，但支持这种关键能力的数据结构和原理仍不被充分理解。

Method: 提出了一种利用因果模块性和最小改变原则来形式化这些直观过程的方法。引入了一个自然编码不同概念级别及其交互机制的层次数据生成过程。

Result: 理论上证明了这种方法能够支持组成概念间的复杂关系，并且证明了从像文本-图像对这样的可观测数据中恢复潜在层次结构的可能性。通过应用我们理论框架中的见解，在基准数据集上实现了显著改进。

Conclusion: 这项工作不仅推进了关于组合泛化如何通过分解高级概念为基本低级概念来实现的理解，而且也为学习这种生成过程提供了必要的步骤。

Abstract: Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.
  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.

</details>


### [49] [HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification](https://arxiv.org/abs/2512.10701)
*Mostafa Anoosha,Zeinab Dehghani,Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架HybridVFL，通过客户端特征解耦与服务端跨模态变压器相结合的方式，改善了垂直联邦学习系统在处理敏感多模态数据时的性能限制。


<details>
  <summary>Details</summary>
Motivation: 标准的垂直联邦学习（VFL）系统由于简单的特征融合方法，在处理诸如移动健康诊断等边缘AI场景中的敏感多模态数据时存在性能局限性。

Method: 提出了HybridVFL框架，该框架采用客户端特征解缠结合服务器端跨模态转换器进行上下文感知融合的方法。

Result: 通过在多模态HAM10000皮肤病变数据集上的系统评估显示，HybridVFL明显优于标准联邦基线方法。

Conclusion: 研究证明了先进的融合机制对于构建鲁棒且保护隐私的系统至关重要，并且HybridVFL提供了一种有效克服现有VFL系统瓶颈的方法。

Abstract: Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.

</details>


### [50] [Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality](https://arxiv.org/abs/2512.10720)
*Lingjing Kong,Shaoan Xie,Guangyi Chen,Yuewen Sun,Xiangchen Song,Eric P. Xing,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的理论框架，通过因果最小性原则为生成模型的潜在表示赋予清晰的因果解释和稳健、组件级可识别的控制。在理论上导出的最小性条件下，学习到的表示可以等同于数据生成过程的真实潜在变量。实证上，将这些约束应用于领先的生成模型中，能够提取其内在的层次概念图，提供对其内部知识组织的新见解，并为透明可靠的系统铺平道路。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型虽然在图像和文本生成等领域取得了革命性的进展，但它们大多作为不透明的黑匣子运行，阻碍了人类的理解、控制和一致性。尽管稀疏自动编码器等方法展示了显著的经验成功，但往往缺乏理论保证，可能带来主观见解的风险。因此，本文的主要目标是为可解释的生成模型建立一个有原则的基础。

Method: 文章引入了一个针对层次选择模型的新颖理论框架，在该框架下，高层概念从低层变量的受限组合中产生，更好地捕捉数据生成中的复杂依赖关系。通过对领先生成模型应用基于理论推导的最小性条件（表现为稀疏或压缩约束），研究者们展示了如何使得学习到的表征与数据生成过程的真实潜在变量相等价。

Result: 研究表明，在满足特定的最小性条件下，确实可以使学习到的表征与实际的数据生成过程中的真实潜在变量相对应。此外，通过将这些约束应用于当前主流的生成模型，研究人员能够从中提取出固有的层次概念图，这不仅提供了对模型内部知识结构的新认识，还为实现细粒度的模型控制提供了可能。

Conclusion: 通过遵循因果最小性原则并施加适当的稀疏或压缩约束，可以使生成模型具备更加透明且可控的特点。所提出的框架不仅有助于理解模型内部的知识组织方式，还促进了开发更为可靠和易于操控的人工智能系统。

Abstract: Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.

</details>


### [51] [Generalized Spherical Neural Operators: Green's Function Formulation](https://arxiv.org/abs/2512.10723)
*Hao Tang,Hao Chen,Chao Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于可设计球形格林函数及其谐波展开的通用算子设计框架，名为格林函数球形神经算子（GSNO），并开发了GSHNet架构。GSNO和GSHNet在扩散MRI、浅水动力学和全球天气预报等领域的评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的解决参数化偏微分方程的神经算子在扩展到球面域时面临挑战，因为需要保持内在几何形状同时避免破坏旋转一致性的失真。虽然当前的球面算子依赖于旋转等变性，但往往缺乏处理现实世界复杂性的灵活性。

Method: 提出了一个基于可设计球形格林函数及其谐波展开的通用算子设计框架，并在此基础上提出了依赖绝对位置和相对位置的格林函数，以实现对等变性和不变性的灵活平衡。接着介绍了格林函数球形神经算子（GSNO）及一种新的谱学习方法。此外，还开发了GSHNet，这是一种结合多尺度谱建模与球面上下采样的层次架构，用以增强全局特征表示。

Result: 通过在扩散MRI、浅水动力学以及全球天气预报上的评估显示，GSNO和GSHNet相比最先进方法表现出色。

Conclusion: GSNO作为一个原则性和通用的球面算子学习框架，在连接严谨理论与现实世界复杂性方面具有重要地位。

Abstract: Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.

</details>


### [52] [Interpretable and Steerable Concept Bottleneck Sparse Autoencoders](https://arxiv.org/abs/2512.10805)
*Akshay Kulkarni,Tsui-Wei Weng,Vivek Narayanaswamy,Shusen Liu,Wesam A. Sakla,Kowshik Thopalli*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，即概念瓶颈稀疏自动编码器（CB-SAE），旨在提高稀疏自动编码器在大型视觉语言模型中的可解释性和可控性。通过修剪低效神经元并用轻量级的概念瓶颈扩充潜在空间，该方法使得模型的可解释性提高了32.1%，可控性提高了14.5%。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏自动编码器(SAEs)有望为大型语言模型(LLMs)和大型视觉语言模型(LVLMs)提供一种统一的方法来实现机制解释性、概念发现和模型导向，但其实现这一潜力需要学习到的特征既具有可解释性又易于控制。现有研究中发现SAE神经元大多表现出较低的可解释性或可控性，甚至两者兼而有之，并且由于SAEs的无监督性质，用户期望的概念往往缺失于所学字典中，限制了它们的实际应用价值。

Method: 为了解决上述问题，提出了一个名为Concept Bottleneck Sparse Autoencoders (CB-SAE)的新颖后处理框架。该框架首先剔除掉那些效用低下的神经元，然后使用与用户定义的概念集对齐的一个轻量级概念瓶颈来扩展潜在空间。

Result: 实验结果表明，在多种LVLMs及图像生成任务上，CB-SAE相较于传统SAE而言，其可解释性提升了32.1%，可控性也得到了14.5%的增长。

Conclusion: 研究表明，通过引入CB-SAE框架能够显著改善稀疏自动编码器在视觉语言模型上的表现，特别是在增强模型的可解释性和可控性方面取得了实质性进展。此外，研究团队还计划公开代码和模型权重以促进相关领域的发展。

Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.

</details>


### [53] [Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments](https://arxiv.org/abs/2512.10835)
*Atahan Cilan,Atay Özgövde*

Main category: cs.LG

TL;DR: 该论文提出了一种强化学习框架，能够在不依赖人类游戏数据的情况下实现可控和多样的玩家行为。通过定义N维连续空间中的玩家行为，并在训练过程中同时输入当前和目标行为向量，基于它们之间距离的归一化减少来计算奖励，从而让策略学习动作如何影响行为统计。实验表明，所提框架相比仅以胜利为目标的基线方法能产生显著更多的行为多样性，并且能够可靠地匹配各种指定的行为向量。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常需要大规模的玩家轨迹、为不同类型的玩家训练单独的模型，或者没有直接映射可解释的行为参数到学习策略上，这限制了其可扩展性和可控性。本研究旨在解决这些问题，提出一种新的框架，可以在不需要大量人类游戏数据的前提下生成多样化的玩家行为。

Method: 研究人员定义了一个N维连续空间来表示玩家行为，并从中均匀采样目标行为向量。这些向量覆盖了代表真实人类风格的子集。训练时，每个智能体接收当前和目标行为向量作为输入，奖励依据是这两个向量间距离的归一化减小值。使用PPO（Proximal Policy Optimization）算法为基础构建多智能体策略。

Result: 实验在一个定制的多人Unity游戏中进行，结果显示，与只追求胜利的基线相比，提出的框架可以产生明显更高的行为多样性，并且能够一致地匹配给定的目标行为向量。

Conclusion: 所提出的方法为自动化游戏测试、游戏平衡调整、模拟类似人类的行为以及在线游戏中替换断开连接的玩家提供了一个可扩展的解决方案。

Abstract: This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.

</details>


### [54] [Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants](https://arxiv.org/abs/2512.10857)
*Chirag Modi,Jiequn Han,Eric Vanden-Eijnden,Joan Bruna*

Main category: cs.LG

TL;DR: 本文提出了一种名为自洽随机插值(SCSI)的新方法，通过迭代更新损坏数据和干净数据样本之间的传输映射来解决逆问题，并在自然图像处理和科学重建的逆问题上展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 在许多科学和工程领域中，我们只能观察到通过噪声、条件不佳的通道被污染的测量结果。因此，需要一种方法可以从这些受污染的数据中构建原始数据的生成模型。

Method: 引入了基于随机插值的新方法：通过仅访问受污染的数据集以及对污染通道的黑盒访问，迭代地更新受污染与干净数据样本间的传输映射。

Result: 在适当的条件下，该迭代过程会收敛于一个能够有效反转污染通道的自洽传输映射，从而为清洁数据提供了一个生成模型。SCSI方法计算效率高、非常灵活，并享有理论保证。

Conclusion: 通过实验验证了SCSI方法在自然图像处理和科学重建中的逆问题上的优越表现，并且在适当假设下建立了方案的收敛性保证。

Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.

</details>


### [55] [Scaling Behavior of Discrete Diffusion Language Models](https://arxiv.org/abs/2512.10858)
*Dimitri von Rütte,Janis Fluri,Omead Pooladzandi,Bernhard Schölkopf,Thomas Hofmann,Antonio Orvieto*

Main category: cs.LG

TL;DR: 研究了不同噪声类型下离散扩散语言模型（DLMs）的扩展行为，发现均匀扩散比掩码扩散在计算效率上更优，并且成功训练了一个拥有100亿参数、进行了$10^{22}$次浮点运算的均匀扩散模型。


<details>
  <summary>Details</summary>
Motivation: 探索离散扩散语言模型（DLMs）与自回归语言模型（ALMs）相比的扩展行为，特别是不同噪声类型对DLMs性能的影响。

Method: 通过平滑地插值于掩码扩散和均匀扩散之间来研究DLMs在不同噪声类型下的扩展行为，同时密切关注批量大小和学习率等关键超参数的作用。

Result: 结果表明，DLMs的扩展行为高度依赖于噪声类型，并且与ALMs明显不同。尽管所有噪声类型在计算受限的扩展中收敛到相似的损失值，但均匀扩散相较于掩码扩散需要更多的参数但较少的数据来进行计算高效的训练。

Conclusion: 均匀扩散是数据受限场景下很有前景的选择；基于此，研究人员成功将一个采用均匀扩散机制的语言模型扩展到了100亿参数规模，验证了预测的扩展行为。

Abstract: Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.
  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [56] [Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives](https://arxiv.org/abs/2512.10079)
*Federico Formica,Mark Lawford,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文反思了近期将领域知识融入基于搜索的软件测试（SBST）框架中的实验结果，强调了一些大胆和意外的结果，并从新的角度重新审视这些技术，为未来的研究指明了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基于搜索的软件测试能够自动产生大量测试案例以发现需求违规情况，但缺乏工程师所拥有的领域知识。因此，需要一种方法来结合工程师的专业知识与现有的SBST框架，以提高测试效率和质量。

Method: 通过回顾最近将领域知识整合进SBST框架的相关研究及其实验结果，特别关注那些非预期或特别显著的结果。

Result: 揭示了在SBST中加入领域知识后出现的一些意外发现，这些发现促使我们从不同角度重新思考如何更好地利用领域知识改进SBST方法。

Conclusion: 该研究表明，通过结合领域知识可以显著影响SBST的效果；同时也指出，对于如何最有效地融合这两种资源还需要进一步探索。

Abstract: Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.

</details>


### [57] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: 介绍了一种名为ATLAS的自动化流水线，它能够大规模合成已验证的程序，以解决训练大型语言模型进行程序验证时遇到的数据瓶颈问题。通过这种方法，显著提高了Qwen 2.5 7B Coder在DafnyBench和DafnySynthesis上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在程序验证方面展现出潜力，但可用作训练的已验证代码稀缺限制了进一步的发展。

Method: 开发了ATLAS，一个可以自动生成包含规范、实现与证明的完整Dafny程序的自动化流水线。通过对整个合成过程进行分解，并执行多个专门任务，从而生成大量可用于训练的例子。

Result: 使用该数据集微调后的Qwen 2.5 7B Coder，在DafnyBench上提升了23个百分点，在DafnySynthesis上则提升了50个百分点。

Conclusion: 实验结果表明，通过合成方法生成的已验证代码能够有效增强大型语言模型在形式化验证方面的能力。

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [58] [Does SWE-Bench-Verified Test Agent Ability or Model Memory?](https://arxiv.org/abs/2512.10218)
*Thanosan Prathifkumar,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 研究发现，SWE-Bench-Verified数据集可能与大型语言模型的训练数据重叠，导致在该基准测试上的得分不能真实反映模型解决实际软件问题的能力。通过对比实验显示，模型在SWE-Bench-Verified上的表现远优于其他基准，暗示这些模型在训练过程中可能已经接触过许多SWE-Bench-Verified的任务。


<details>
  <summary>Details</summary>
Motivation: 探索SWE-Bench-Verified作为评估大型语言模型解决GitHub问题能力的标准的有效性，并考察其是否因与模型训练数据重叠而影响了评价结果的真实性。

Method: 选取两个Claude模型，在仅有问题文本以及问题文本加上文件路径的情况下，分别在SWE-Bench-Verified、BeetleBox和SWE-rebench上进行测试，比较不同条件下的性能差异。

Result: 模型在SWE-Bench-Verified上的表现显著优于其他基准，特别是在识别编辑过的文件方面，这表明模型可能在训练期间就已经熟悉了SWE-Bench-Verified中的任务。

Conclusion: 依赖于如SWE-Bench-Verified这样可能存在污染的老基准来评估模型存在风险，建议转向专门为避免污染设计的新数据集。

Abstract: SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.

</details>


### [59] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 本研究通过提高问题报告质量、实证分析开发人员工作流程以及自动化认知需求高的解决任务三个方面，推进了AI驱动的问题解决方法，以支持更可维护和高质量的软件系统。


<details>
  <summary>Details</summary>
Motivation: 软件质量问题的有效解决至关重要，但开发者经常面临诸如低质量的问题报告、对实际工作流程理解有限以及缺乏自动化支持等挑战。

Method: 首先，通过提出利用LLM推理和特定应用信息的技术来提高问题报告的质量；其次，对传统和AI增强系统中的开发者工作流程进行实证特征描述；最后，通过ML、DL和基于LLM的方法自动化高认知需求的解决任务，如错误UI定位和解决方案识别。

Result: 该研究提供了经验性见解、实用工具及自动化方法，以促进AI驱动的问题解决。

Conclusion: 这项工作的综合成果有助于改进AI驱动的问题解决过程，从而支持更加可维护且高质量的软件系统的开发。

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [60] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: 本文提出了一种新的自监督框架UniCoR，旨在解决代码检索中的三个挑战：语义理解不足、混合查询融合效率低下以及跨语言场景下的泛化能力弱。通过多视角对比学习模块和表示分布一致性学习模块，UniCoR能够增强模型对不同模态间语义的理解，并提高跨语言的一致性和泛化能力。实验结果表明，与基线模型相比，UniCoR在MRR和MAP指标上分别平均提高了8.64%和11.54%，并在混合代码检索及跨语言情景下表现出良好的稳定性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理混合模式（自然语言加代码片段）的代码检索时，特别是在跨语言环境下，存在语义理解不充分、混合查询融合效率低以及泛化能力差的问题。

Method: 提出了UniCoR框架，包含一个多视角监督对比学习模块以加强语义理解和模态融合，以及一个表示分布一致性学习模块来改善跨语言环境下的泛化能力。

Result: UniCoR在基准测试中表现优于所有基线模型，在MRR上实现了8.64%的平均提升，在MAP上实现了11.54%的提升。此外，它还在混合代码检索和跨语言场景中展示了稳定性和泛化能力。

Conclusion: 通过引入UniCoR，研究解决了代码检索领域内关于语义理解、混合查询处理及跨语言适应性方面的关键难题，为更高效准确地进行代码搜索提供了新途径。

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [61] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 本研究通过分析LMSYS-Chat-1M和WildChat数据集，探讨了人与大型语言模型在编码协作中的合作机制、指令遵循能力以及用户满意度。发现任务类型影响交互模式，修复错误和代码重构对LLM的挑战更大，而结构化知识查询和算法设计让用户更满意。


<details>
  <summary>Details</summary>
Motivation: 尽管存在像LMSYS-Chat-1M和WildChat这样的真实世界用户-LLM对话数据集，但对于人类-LLM在编程场景中合作机制的研究很少。因此，这项研究旨在探索人们在互动过程中经历的曲折路径、LLMs遵循指示的能力以及用户的满意度。

Method: 采用LMSYS-Chat-1M和WildChat数据集进行实证分析，以探索人类-LLM的合作机制、LLMs遵循指示的能力及用户满意度。

Result: 1) 任务类型决定了交互模式（线性、星形和树状），其中代码质量优化倾向于线性模式，设计驱动的任务倾向于树状结构，而查询则偏好星形模式；2) 错误修正和代码重构对于LLMs来说更具挑战性，其不遵从率明显高于信息查询；3) 在代码质量优化和需求驱动开发任务上用户满意度较低，而在结构化知识查询和算法设计方面则显示出更高的满意度。

Conclusion: 该工作拓宽了对人类-LLM协同作用的理解，并支持更有效的AI辅助开发。这些见解为改进LLM界面和提高编码协作中的用户满意度提供了建议，同时指出了未来关于适应性对话系统研究的方向。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [62] [Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories](https://arxiv.org/abs/2512.10618)
*Georgia M. Kapitsaki,Maria Papoutsoglou,Christoph Treude,Ioanna Theophilou*

Main category: cs.SE

TL;DR: 本研究通过分析GitHub仓库中的32,820个问题，探讨了开源软件开发者为遵守隐私法规（如GDPR和CCPA）而讨论的问题类型。研究发现开发者主要关注具体的用户权利（例如删除权、退出权、访问权），而关于同意管理、功能实现、错误修复及cookies管理的讨论最为常见。基于此，提出了一个包含六个集群共24类讨论主题的分类法，以帮助从业者优先处理合规性问题，并为教育界提供改进课程的参考。


<details>
  <summary>Details</summary>
Motivation: 随着隐私立法如欧盟通用数据保护条例(GDPR)和加州消费者隐私法案(CCPA)的实施，软件开发方式受到了影响，但缺乏关于开源软件开发者如何讨论以符合隐私法律要求的实际证据。

Method: 研究者们从GitHub仓库中挖掘并分析了32,820个问题，部分自动分析用于识别提到的法律用户权利与原则，并对1,186个样本问题根据所涉及的关注点类型进行了手动分析。

Result: 研究人员制定了包括特性/错误、同意相关、文档、数据存储/共享、适应性和一般合规性的六大类共24种讨论类别。结果显示，开发者主要集中在特定的用户权利上（如删除权、退出权、访问权），而对于其他权利的讨论较少；大多数讨论集中在用户同意、用户权利功能、bug以及cookie管理上。

Conclusion: 创建的分类可以帮助实践者了解哪些议题是为法律合规性而讨论的，从而确保他们在系统中首先解决这些问题。此外，教育界可以据此调整课程设置，更好地教育未来的工程师关于隐私法的关注点；研究界则能够识别差距和改进领域，以支持并加速数据隐私法的合规进程。

Abstract: Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.

</details>


### [63] [Zorya: Automated Concolic Execution of Single-Threaded Go Binaries](https://arxiv.org/abs/2512.10799)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: 本文通过改进Zorya框架，增强了对Go语言二进制文件的漏洞检测能力，特别是通过增加具体未执行路径中的错误检测和多层次过滤机制来提高效率。实验表明，这种方法在处理五个Go漏洞时能够实现显著的速度提升，并且比现有工具更有效地识别所有崩溃情况。


<details>
  <summary>Details</summary>
Motivation: 随着Go语言在关键基础设施中的应用日益广泛，对其二进制文件进行系统性漏洞检测的需求变得迫切起来。然而，现有的符号执行工具面对Go语言特有的运行时复杂性和可扩展性挑战时显得力不从心。

Method: 研究者们基于Zorya——一个将Go语言二进制文件转换为Ghidra P-Code中间表示形式的共模执行框架——进行了功能拓展。他们加入了对实际上没有被执行路径中bug的探测以及一个多层过滤机制，以将符号推理集中在与程序崩溃相关的路径上。

Result: 评估结果显示，在分析五个特定的Go语言漏洞案例时，所提出的崩溃可达性门控方法能够实现1.8至3.9倍的速度提升，同时过滤掉33%到70%的分支。此外，Zorya成功地检测到了所有的崩溃情形，而相比之下其他工具最多只能发现两个。对于复杂的程序而言，采用函数模式分析被证明是至关重要的，其运行速度大约是直接从主函数开始分析时的百倍。

Conclusion: 这项研究表明，针对具有运行时安全检查的语言生态系统，专门定制的共模执行技术可以实现实际可行的漏洞检测。

Abstract: Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.

</details>
