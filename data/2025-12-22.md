<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LG](#cs.LG) [Total: 36]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs](https://arxiv.org/abs/2512.17334)
*Zhi Ma,Cheng Wen,Zhexin Su,Xiao Liang,Cong Tian,Shengchao Qin,Mengfei Yang*

Main category: cs.SE

TL;DR: 提出了一种名为Req2LTL的新框架，该框架通过一种称为OnionL的分层中间表示形式连接自然语言(NL)和线性时序逻辑(LTL)，以解决将软件需求从自然语言自动转换为形式化规格说明的挑战。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，自动化地将自然语言（NL）软件需求转化为正式规范是扩展形式验证实践到工业场景中的一个关键挑战。现有方法，无论是基于规则还是基于学习的方法，都面临重大局限性。大型语言模型尽管在语义提取方面表现出色，但在处理现实世界工业需求的复杂性、模糊性和逻辑深度方面仍存在困难。

Method: 提出了Req2LTL，这是一个模块化框架，它通过一种称为OnionL的分层中介表示来桥接自然语言与线性时间逻辑（LTL）。此框架利用了大型语言模型进行语义分解，并结合确定性的基于规则的合成以保证语法正确性和语义忠实度。

Result: 全面评估显示，Req2LTL在实际航空航天要求上实现了88.4%的语义准确率和100%的句法正确性，显著优于现有方法。

Conclusion: Req2LTL框架成功解决了将自然语言软件需求转换成形式化规格说明的难题，尤其对于复杂且逻辑性强的实际工业需求展现出了优越性能。

Abstract: Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.

</details>


### [2] [What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice](https://arxiv.org/abs/2512.17363)
*Yuqing Niu,Jieke Shi,Ruidong Han,Ye Liu,Chengyan Ma,Yunbo Lyu,David Lo*

Main category: cs.SE

TL;DR: 本文进行了首次大规模的实际TEE应用程序的实证研究，分析了241个使用Intel SGX和ARM TrustZone的开源项目。研究发现物联网设备安全是主要的应用领域，其次是快速发展的AI模型保护。此外，超过30%的项目重新实现了加密功能而不是使用官方SDK API，而近四分之一的项目存在不安全的编码行为。


<details>
  <summary>Details</summary>
Motivation: 可信执行环境(TEEs)如Intel SGX和ARM TrustZone被广泛用于保护敏感数据和代码，但开发者如何实际运用这些技术尚不清楚。

Method: 收集并分析了GitHub上利用Intel SGX和ARM TrustZone的241个开源项目。通过结合手动检查与定制的静态分析脚本，考察了它们在三个不同阶段的采用背景、使用模式及开发实践。

Result: 研究揭示了物联网设备安全是最主要的应用场景（占30%），与之前学术界对区块链和密码系统（7%）的关注形成鲜明对比；同时，AI模型保护（12%）正在成为一个快速增长的新领域。此外，有32.4%的项目选择重新实现加密功能而非直接使用官方SDK提供的API，这表明当前SDK可能在可用性和可移植性方面无法充分满足开发者需求。另外，研究还发现25.3%（共61个项目）存在不安全的编程习惯，比如硬编码密钥以及缺少输入验证等，这些都削弱了TEE应有的安全保障。

Conclusion: 这项研究的结果对于提高TEE SDK的可用性以及支持开发人员进行受信软件开发具有重要意义。

Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.

</details>


### [3] [CIFE: Code Instruction-Following Evaluation](https://arxiv.org/abs/2512.17387)
*Sravani Gunnu,Shanmukha Guttula,Hima Patel*

Main category: cs.SE

TL;DR: 研究引入了一个包含1000个Python任务的基准，每个任务平均有7个开发者指定的约束条件，覆盖了13个类别。通过新的C2A评分来衡量模型在正确性和约束遵守方面的表现，结果显示即使最强的模型在严格遵守约束方面也只达到了39-66%的水平。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要通过测试用例执行来评估代码生成的正确性，但对模型是否遵循如鲁棒性、格式化和安全性等显式要求的关注不够。为解决这一问题，研究旨在创建一个能全面评价模型遵守开发者指定约束能力的新基准。

Method: 构建了一个由1000个Python任务组成的基准，这些任务与通过四阶段人机协作流程策划出的开发者指定约束相匹配。然后使用互补的遵从度指标评估了14个开源及闭源模型，并提出了一个新的综合评分标准——C2A分数，用来同时捕捉正确性和约束符合性。

Result: 实验结果表明，在部分遵守约束的情况下，强模型可以达到超过90%的依从率；然而，当考虑严格的约束遵守时，所有模型的表现都下降到了39%-66%之间。这表明尽管模型能够较好地满足基本的功能需求，但在完全符合开发者意图方面仍存在较大差距。

Conclusion: 可靠的代码生成不仅需要保证代码的正确性，还需要确保模型能够一致地遵守开发者的意图。本研究提出的基准和C2A评分提供了一种有效的方法来评估模型在这两方面的能力。

Abstract: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

</details>


### [4] [SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories](https://arxiv.org/abs/2512.17419)
*Lilin Wang,Lucas Ramalho,Alan Celestino,Phuc Anthony Pham,Yu Liu,Umang Kumar Sinha,Andres Portillo,Onassis Osunwa,Gabriel Maduekwe*

Main category: cs.SE

TL;DR: SWE-Bench++是一个自动化框架，能够从开源GitHub项目中生成代码任务，支持11种语言的bug修复和功能请求。它通过四个阶段将GitHub的pull requests转换为可执行的任务，并且能够将强模型失败的实例转化为训练轨迹。该基准测试了当前最强模型的表现，并展示了微调SWE-Bench++数据集对于提升SWE-bench多语言基准性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试如SWE-bench在评估大型语言模型（LLMs）处理仓库级软件工程任务时存在局限性，包括手动管理、静态数据集以及对Python为基础的bug修复的关注。SWE-Bench++旨在解决这些问题，提供一个更全面、自动生成并且支持多种编程语言的解决方案。

Method: SWE-Bench++采用自动化流程从活跃的GitHub pull requests中收集实际发生的bug修复和新功能添加案例，覆盖11种不同的编程语言。整个过程分为程序化获取、环境合成、测试预言提取及质量保证四步，另外还包含一个基于提示的轨迹合成步骤来转化难以被现有模型解决的问题实例。

Result: SWE-Bench++构建了一个包含来自3,971个仓库共11,133个实例的初始基准。在其中选取的1,782个实例上进行测试显示，不同先进模型的性能如下：claude-sonnet-4.5达36.20% pass@10，gpt-5-2025-08-07 34.57%，gemini/gemini-2.5-pro 24.92%，gpt-4o 16.89%。此外，实验表明使用SWE-Bench++数据集进行微调可以提高SWE-bench多语言基准上的表现。

Conclusion: SWE-Bench++提供了一个可扩展、多语言的支持平台，用于评价和改进针对仓库级别代码生成的大规模语言模型的能力。其不仅能够自动生成多样化的编码任务，而且还能帮助提升现有模型处理复杂软件工程问题的能力。

Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

</details>


### [5] [An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys](https://arxiv.org/abs/2512.17455)
*Ronnie de Souza Santos,Italo Santos,Maria Teresa Baldassarre,Cleyton Magalhaes,Mairieli Wessel*

Main category: cs.SE

TL;DR: 本研究探讨了大型语言模型（LLMs）在软件工程调查中的滥用情况及其对数据真实性、有效性和研究完整性的影响。通过分析2025年两次调查收集的数据，发现49份调查回复中存在合成作者的迹象，如重复序列、统一措辞和表面个性化等模式。这表明需要结合自动化与解释性验证程序、透明报告及社区标准来检测并防止AI生成的回答，以维护软件工程领域内调查的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的进步，参与者可以使用生成工具伪造或操纵其回答，这对调查的完整性构成了新的威胁。本研究旨在探索这些模型如何被错误地用于软件工程调查，并研究这种行为对数据真实性、有效性以及研究诚信的方法论影响。

Method: 研究人员通过Prolific平台在2025年进行了两次调查部署，并分析了参与者答案的内容以识别异常或虚假回应。对于疑似由AI生成的一组回应，采用定性模式检查、叙述特征化以及Scribbr AI检测器进行自动检测。

Result: 分析显示，在49个调查回应中发现了指示合成作者身份的重复出现结构模式，包括重复序列、统一措辞和表面个性化。这些虚假叙述模仿连贯推理的同时隐藏了伪造内容，损害了建构效度、内部效度和外部效度。

Conclusion: 研究指出，数据真实性是软件工程调查中新出现的有效性维度。为确保可靠证据，现在需要结合自动化与解释性验证程序、透明报告以及社区标准来检测并预防AI生成的响应，从而保护软件工程调查的可信度。

Abstract: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

</details>


### [6] [When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction](https://arxiv.org/abs/2512.17460)
*Emmanuel Charleson Dapaah,Jens Grabowski*

Main category: cs.SE

TL;DR: 本研究首次大规模实证分析了软件缺陷预测(SDP)中同时出现的五个数据质量问题（类别不平衡、类别重叠、无关特征、属性噪声和异常值），揭示了这些问题普遍共存，并且在不同条件下对模型性能的影响。此外，还发现了一个性能-稳健性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常单独考虑如类别不平衡或特征不相关等单一问题，而忽视了现实世界中的数据问题通常是共存并相互影响的。为了填补这一空白，本研究旨在通过联合分析来提供一个全面的数据意识理解，了解质量问题是如何在实际环境中影响模型表现的。

Method: 使用解释增强机器与分层交互分析方法，在374个数据集上针对五种不同的分类器，同时考察五个共存的数据质量问题。采用默认超参数设置下的直接及条件效应量化技术来进行分析。

Result: 几乎所有数据集中都存在至少一种数据质量问题；类别重叠是最具破坏性的问题之一；发现了某些情况下异常值反而能提高性能的现象；指出了性能与稳健性之间的权衡关系。

Conclusion: 本研究表明，SDP领域需要从孤立地看待单个问题转向综合考虑多个因素共同作用于模型效果的方式。这为未来的研究提供了新的视角，强调了根据具体情境评估的重要性。

Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.

</details>


### [7] [SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review](https://arxiv.org/abs/2512.17540)
*Kai Wang,Bingcheng Mao,Shuai Jia,Yujie Ding,Dongming Han,Tianyi Ma,Bin Cao*

Main category: cs.SE

TL;DR: 提出了一种基于规范的代码审查框架SGCR，该框架结合了显式路径和隐式路径来提高大型语言模型在自动化代码审查中的可靠性、相关性和可控性。在实际工业环境中部署后，开发人员采纳建议的比例达到了42%，相比基线LLM提高了90.9%。


<details>
  <summary>Details</summary>
Motivation: 尽管使用大型语言模型进行代码审查显示出巨大潜力，但它们缺乏可靠性、上下文感知能力和控制性限制了其实用性。为了解决这些问题，提出了基于规范的代码审查方法（SGCR）。

Method: SGCR框架采用了双路径架构：显式路径确保遵循从人类编写的规范中提取的预定义规则；而隐式路径则探索并验证超出这些规则的问题。

Result: 当SGCR被应用于HiThink Research的实际工业环境中时，其建议的采纳率达到了42%，相较于基础大型语言模型（22%）提高了90.9%。

Conclusion: 研究表明，基于规范的方法是连接大型语言模型生成能力与软件工程严格可靠性要求之间差距的有效范例。

Abstract: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

</details>


### [8] [A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners](https://arxiv.org/abs/2512.17710)
*Martin Rosso,Muhammad Asad Jahangir Jaffar,Alessandro Brighente,Mauro Conti*

Main category: cs.SE

TL;DR: 本文介绍了一种名为SVS-TEST的方法和工具，用于分析基于软件物料清单（SBOM）的漏洞扫描工具在实际场景中的能力、成熟度以及失败情况。通过使用精心制作的SBOM文件进行案例研究，发现不同漏洞扫描工具之间存在显著差异，并且某些工具在有效输入下会无声失败，给用户带来虚假的安全感。


<details>
  <summary>Details</summary>
Motivation: 随着行业采用基于SBOM的漏洞扫描(SVS)来识别软件产品中的漏洞，观察到越来越多的一致性问题和意外行为导致了漏报和静默故障。为了更好地理解SVS的复杂性并评估SVS工具的能力与局限性，提出了本研究。

Method: 提出了一种新的方法和工具SVS-TEST，用来分析SVS工具在真实世界情境下的能力、成熟度及可能遇到的问题。通过一个包含16个精确构建的SBOM及其对应基准的真实情况案例研究，对7种不同的SVS工具进行了评估。

Result: 研究结果揭示了SVS工具之间在可靠性和错误处理方面的重大差异；多个SVS工具对于有效的输入SBOM文件会出现无声失败的情况，这可能导致一种虚假的安全感。

Conclusion: 强调了SVS-TEST对于研究人员和实践者的重要性，包括组织和SVS工具开发者如何利用该工具监控SVS的功能与成熟度。所有研究成果和相关资料均公开提供，并已提前向SVS工具开发者披露了所有发现。

Abstract: Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.

</details>


### [9] [LLM-based Behaviour Driven Development for Hardware Design](https://arxiv.org/abs/2512.17814)
*Rolf Drechsler,Qian Liu*

Main category: cs.SE

TL;DR: 本文探讨了使用基于大型语言模型的技术来支持硬件设计中的行为驱动开发（BDD），以期自动化从文本规范中推导出精确行为场景的过程。


<details>
  <summary>Details</summary>
Motivation: 测试和验证是硬件和系统设计中的关键活动，但随着系统规模的增长其复杂性也显著增加。虽然行为驱动开发在软件工程领域已经证明有效，但在硬件设计中尚未广泛应用，部分原因是需要手动从文本规范中提取精确的行为场景。最近，在大型语言模型方面的进展为自动完成这一步骤提供了新的机会。

Method: 本研究采用了基于大型语言模型的技术手段，旨在探索这些技术如何能够辅助硬件设计流程中的行为驱动开发方法。

Result: 文章提出了一种利用大型语言模型自动生成行为场景的方法，减少了人工操作的需求，并可能提高硬件设计过程中BDD的应用效率。

Conclusion: 通过应用大型语言模型于硬件设计的行为驱动开发中，可以有效地减少手动工作量，从而促进BDD方法在该领域的采纳与实施。

Abstract: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 本文提出了一种新的方法，使用事件执行器在单处理器系统上为ROS2中的任意有向无环图(DAG)实现固定作业级别优先级调度。通过将ROS2应用抽象成树的森林，可以将其映射到传统的实时DAG任务模型，并证明了所提实现与传统固定优先级DAG任务调度器生成相同的调度结果。


<details>
  <summary>Details</summary>
Motivation: 当前ROS2中的调度方法存在局限性，主要集中在简单的链式任务调度和特定响应时间分析上。对于更复杂的、任意的DAG结构的任务调度支持不足。

Method: 作者提出了一种新方法，利用事件执行器来实现针对ROS2中任意DAG的固定作业级别优先级调度策略。为了实现这一目标，需要对事件队列进行特殊设计，并采用支持LIFO消息传递顺序的通信中间件。此外，研究者还将ROS2应用程序抽象为树状结构组成的森林，以促进其向传统实时DAG任务模型转换。

Result: 研究表明，即使缺乏通常所需的优先级信息，所提出的实现也能生成与标准固定优先级DAG任务调度器相同的结果。这有助于缩小现有实时系统理论与ROS2调度分析之间的差距。

Conclusion: 本研究提供了一种有效的方法来解决ROS2中复杂DAG结构任务的调度问题，通过引入基于事件执行器的新调度机制，不仅提高了调度灵活性还保持了与经典实时调度技术的一致性。

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [11] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 本文系统地评估了包括ChatGPT-4和5、Claude及LLaMA在内的领先大型语言模型在生成使用共享内存、指令式和分布式内存范式的C++实现的Mandelbrot集合任务上的表现。结果显示，ChatGPT-4和5在语法准确性和可扩展性能方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管现代C++标准和像OpenMP与MPI这样的框架已经简化了并行编程，但掌握这些范式依然复杂。而大型语言模型（LLMs）在自动化代码生成上展现出潜力，但对于它们生成正确的高效HPC代码的有效性尚未充分理解。

Method: 研究者们选择了几个领先的大型语言模型，如ChatGPT 4和5、Claude以及LLaMA，并让它们完成一项特定的任务：利用共享内存、基于指令的方法以及分布式内存范式来生成计算Mandelbrot集合的C++程序。每个由模型生成的程序随后被编译并用GCC 11.5.0版本执行，以测试其正确性、鲁棒性和可扩展性。

Result: 实验结果表明，ChatGPT-4和ChatGPT-5在生成具有高语法准确性且能展现良好可扩展性的高性能计算代码方面表现优异。

Conclusion: 本研究表明，在生成遵循特定并行编程模式的C++代码方面，某些先进的大型语言模型特别是ChatGPT-4和5能够提供既精确又高效的解决方案，这为未来通过AI辅助开发高质量并行软件指明了方向。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [12] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 本文提出dLLM-Serve，一个高效的扩散式大语言模型服务系统，通过优化内存占用、计算调度和生成质量来解决dLLMs特有的“内存占用危机”。dLLM-Serve在不同工作负载和GPU上均表现出显著的吞吐量提升和尾部延迟减少。


<details>
  <summary>Details</summary>
Motivation: 针对现有的扩散式大语言模型（dLLMs）研究主要集中在内核级优化而缺乏全面的服务框架的问题，特别是这些模型在生产环境中面临的独特内存动态挑战，包括由单体logit张量导致的大内存占用以及计算密集型‘刷新’阶段与带宽受限型‘重用’阶段之间资源波动剧烈的情况。

Method: 提出了dLLM-Serve，这是一个专为dLLMs设计的服务系统，它通过引入Logit-Aware激活预算分配、相位多路复用调度器以及头中心稀疏注意力机制来共同优化内存占用、计算调度及生成质量。

Result: 在多样化的任务负载（如LiveBench, Burst, OSC）和不同的GPU型号（RTX 4090, L40S）测试中，dLLM-Serve相对于最先进基准，在消费级RTX 4090 GPU上的吞吐量提高了1.61倍至1.81倍，在服务器级NVIDIA L40S GPU上提高了1.60倍至1.74倍，并且在高竞争条件下减少了接近4倍的尾部延迟。

Conclusion: dLLM-Serve为可扩展的dLLM推理提供了一个新的蓝图，成功地将理论上的算法稀疏性转化为跨异构硬件的实际加速效果。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [13] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: 本文提出了一种名为SPIRE的可扩展向量索引，通过平衡分区粒度和保持准确性的递归构建方法，在实验中实现了高可扩展性和比现有系统高达9.64倍的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式索引设计难以在准确性、延迟和吞吐量之间取得良好平衡，特别是在处理数十亿级别的向量时。

Method: SPIRE基于两个主要设计决策：确定一种避免读取成本激增的平衡分区粒度；引入一种保持准确性的递归构建方法，创建具有可预测搜索成本和稳定准确性的多级索引。

Result: 在包含多达80亿个向量及横跨46个节点的实验中，SPIRE展示了出色的可扩展性，并且相比最先进系统实现了最高达9.64倍的吞吐量提升。

Conclusion: SPIRE作为一种新的可扩展向量索引方案，成功地解决了大规模向量检索场景下的性能瓶颈问题，为ANNS应用提供了更优的选择。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [14] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 该研究开发了一个基于云的联合系统，即HEAL数据平台，作为NIH HEAL计划下生成数据的单一搜索、发现和分析点。


<details>
  <summary>Details</summary>
Motivation: 为了构建一个单一的搜索、发现和分析点，以服务于NIH HEAL倡议下产生的数据。

Method: HEAL数据平台建立在开源Gen3平台上，利用少量框架服务和公开API与NIH内外的数据存储库进行互操作。这些服务包括身份验证和授权、为数据对象创建持久标识符以及添加和更新元数据。

Result: HEAL数据平台已成为超过一千个HEAL倡议资助的研究项目的单一发现点，并且每个月有数百名用户使用。它提供了丰富的元数据并与数据存储库和公共平台互操作，以提供对共享数据集的访问。安全的基于云的计算环境通过STRIDES集成促进了HEAL数据的二次分析。目前，该平台与十九个数据存储库互操作。

Conclusion: HEAL数据平台实现了连接数据存储库和公共平台中存放的数据的搜索、发现和分析。通过确保这些数据完全可查找、可访问、可互操作和可重用（FAIR），HEAL数据平台最大化了HEAL倡议下生成的数据的价值。

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [15] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: 本文提出FlashCodec和UnifiedServe两种设计，共同优化端到端的多模态大语言模型(MLLMs)流水线。FlashCodec通过协作式多GPU视频解码加速了多模态预处理阶段，而UnifiedServe则通过逻辑上分离视觉到文本及推理阶段的执行以消除阶段间阻塞，并物理共享GPU资源来最大化GPU系统利用率。这些改进使得框架能够处理多达3.0倍的请求或实现1.5倍更严格的SLO，同时达到最高4.4倍于现有系统的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在增强能力的同时引入了显著的系统瓶颈，包括多模态预处理尤其是视频解码导致的时间至首个令牌生成时间（TTFT）较长、视觉编码器作为独立且计算密集型阶段无法与LLM预填充或解码共批处理等问题。这些问题限制了整体吞吐量并增加了令牌生成延迟。

Method: 提出了FlashCodec和UnifiedServe两种互补设计方案：FlashCodec利用协作式的多GPU视频解码技术来减少解码延迟；UnifiedServe则通过逻辑上分离视觉到文本转换与推理阶段的执行流程来消除阶段间的阻塞现象，同时物理上共享GPU资源以提高GPU使用效率。

Result: 所提出的框架能够服务多达3.0倍的请求量或实施1.5倍更加严格的SLOs，并且相较于最先进的系统可实现高达4.4倍的吞吐量提升。

Conclusion: 通过FlashCodec和UnifiedServe联合优化端到端MLLM流水线，有效解决了多模态预处理和视觉编码引入的瓶颈问题，大幅提升了系统吞吐量和服务水平目标(SLOs)。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [16] [A Reproducible and Fair Evaluation of Partition-aware Collaborative Filtering](https://arxiv.org/abs/2512.17015)
*Domenico De Gioia,Claudio Pomo,Ludovico Boratto,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 该论文研究了基于相似性的协同过滤模型，特别是细调分区感知相似性细化（FPSR）框架及其扩展版本FPSR+。通过透明且完全可复现的基准测试，发现尽管FPSR模型在某些长尾场景中表现出显著优势，但其整体表现并不总是最佳。本研究揭示了分区、全局组件和中心设计之间的准确性-覆盖率权衡，并为可扩展推荐系统的设计提供了有价值的指导。


<details>
  <summary>Details</summary>
Motivation: 传统基于相似性的协同过滤模型虽然离线性能优异且概念简单，但因维护密集项目-项目相似矩阵的成本而受限于可扩展性。最近出现的分区范式能够在保持有限的全局上下文的同时学习局部相似性，从而有效平衡效率与效果。然而，先前关于FPSR/FPSR+的研究报告往往依赖于来源不明的数据分割，并省略了一些基于相似性的基线比较，使得公平对比变得复杂。因此，需要一个透明且完全可复现的基准来评估这些模型。

Method: 本文采用了Fine-tuning Partition-aware Similarity Refinement (FPSR) 框架及它的扩展版 FPSR+ 作为研究对象。通过建立一个透明且完全可复现的基准测试环境，对FPSR和FPSR+进行评估。

Result: 结果表明，尽管FPSR系列模型并非始终处于最高水平的表现，但在总体上仍具有竞争力，并验证了其设计理念，在长尾场景下尤其展现出显著的优势。此外，研究还突出了由于划分、全局元素以及枢纽设计所导致的准确度-覆盖范围间的权衡问题。

Conclusion: FPSR模型家族在长尾场景下的表现尤为突出，同时整体上也保持着竞争力。这项研究明确了分区感知相似性建模最有利的情况，并为遵循可复现协议的可扩展推荐系统设计提供了实际指导。

Abstract: Similarity-based collaborative filtering (CF) models have long demonstrated strong offline performance and conceptual simplicity. However, their scalability is limited by the quadratic cost of maintaining dense item-item similarity matrices. Partitioning-based paradigms have recently emerged as an effective strategy for balancing effectiveness and efficiency, enabling models to learn local similarities within coherent subgraphs while maintaining a limited global context. In this work, we focus on the Fine-tuning Partition-aware Similarity Refinement (FPSR) framework, a prominent representative of this family, as well as its extension, FPSR+. Reproducible evaluation of partition-aware collaborative filtering remains challenging, as prior FPSR/FPSR+ reports often rely on splits of unclear provenance and omit some similarity-based baselines, thereby complicating fair comparison. We present a transparent, fully reproducible benchmark of FPSR and FPSR+. Based on our results, the family of FPSR models does not consistently perform at the highest level. Overall, it remains competitive, validates its design choices, and shows significant advantages in long-tail scenarios. This highlights the accuracy-coverage trade-offs resulting from partitioning, global components, and hub design. Our investigation clarifies when partition-aware similarity modeling is most beneficial and offers actionable guidance for scalable recommender system design under reproducible protocols.

</details>


### [17] [Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations](https://arxiv.org/abs/2512.17027)
*Erica Coppolillo,Simone Mungari*

Main category: cs.IR

TL;DR: 本文首次对比分析了维基百科和Grokipedia这两个百科平台的搜索引擎机制，发现两者在搜索结果的相关性、主题分布及探索路径上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着完全由AI生成的百科全书Grokipedia的出现，对于不同百科系统中搜索引擎行为的研究变得尤为重要。本文旨在填补这一空白，提供对Wikipedia和Grokipedia之间搜索引擎机制的首次比较分析。

Method: 研究者使用近10,000个中立英文单词及其子串作为查询条件，收集超过70,000条搜索引擎结果，并检查这些结果的意义一致性、重叠度以及主题结构。

Result: 研究发现两个平台经常产生与原始查询关联较弱的结果，并且从无害查询开始可能会出现意外内容；尽管如此，针对相同的查询，两系统产生的推荐集合却大相径庭。通过主题注释和轨迹分析进一步揭示了内容类别浮现方式及多阶段探索过程中搜索引擎结果演变的系统性差异。

Conclusion: 总的来说，研究显示意外的搜索引擎输出是两个平台共有的特征，但它们在主题分布和查询建议方面表现出不一致之处。

Abstract: Encyclopedic knowledge platforms are key gateways through which users explore information online. The recent release of Grokipedia, a fully AI-generated encyclopedia, introduces a new alternative to traditional, well-established platforms like Wikipedia. In this context, search engine mechanisms play an important role in guiding users exploratory paths, yet their behavior across different encyclopedic systems remains underexplored. In this work, we address this gap by providing the first comparative analysis of search engine in Wikipedia and Grokipedia.
  Using nearly 10,000 neutral English words and their substrings as queries, we collect over 70,000 search engine results and examine their semantic alignment, overlap, and topical structure. We find that both platforms frequently generate results that are weakly related to the original query and, in many cases, surface unexpected content starting from innocuous queries. Despite these shared properties, the two systems often produce substantially different recommendation sets for the same query. Through topical annotation and trajectory analysis, we further identify systematic differences in how content categories are surfaced and how search engine results evolve over multiple stages of exploration.
  Overall, our findings show that unexpected search engine outcomes are a common feature of both the platforms, even though they exhibit discrepancies in terms of topical distribution and query suggestions.

</details>


### [18] [TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval](https://arxiv.org/abs/2512.17164)
*Yu Yang,Feng Tian,Ping Chen*

Main category: cs.IR

TL;DR: 提出了TCDE，一种基于大语言模型的主题中心双重扩展策略，用于同时丰富查询和文档。通过设计不同的提示模板来处理每个查询和文档，从而在查询和相关文档之间建立语义桥梁，提高下游检索模型的效果。实验表明，该方法在多个基准测试中表现出色，特别是在密集检索任务上超越了现有的一些最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有的查询扩展（QE）和文档扩展（DE）技术通常是分开应用的，这可能导致扩展后的查询或文档与其相关的文档或查询之间出现语义不匹配的问题。为了解决这一问题，研究者提出了一种新的双扩展策略——TCDE，旨在利用大型语言模型对查询和文档进行主题为中心的丰富处理，以改善它们之间的语义对齐度。

Method: TCDE采用两种不同的提示模板分别处理查询与文档：对于查询，引导LLM识别每个查询中的不同子主题，并为每个子主题生成一个专注的伪文档；对于文档，则是指导LLM提炼出每篇文档的核心主题句。最终，将这些输出用来扩展原始查询和文档，以此建立查询与其相关文档间的语义联系。

Result: 实验结果表明，在TREC Deep Learning和BEIR两个具有挑战性的基准测试中，TCDE相较于强大的现有扩展基线实现了显著改进。特别地，在SciFact数据集上的密集检索任务中，NDCG@10指标相对提高了2.8%。

Conclusion: 本研究提出的基于主题的双扩展策略TCDE能够有效地增强查询与文档间的一致性，进而提升信息检索系统的性能。实验验证了此方法的有效性，尤其是在解决密集检索任务方面表现出了优于多种现有先进方法的优势。

Abstract: Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.

</details>


### [19] [Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest](https://arxiv.org/abs/2512.17277)
*Saeed Ebrahimi,Weijie Jiang,Jaewon Yang,Olafur Gudmundsson,Yucheng Tu,Huizhong Duan*

Main category: cs.IR

TL;DR: 本文探讨了如何改进冷启动项目在推荐系统中的预测问题，通过轻量级解决方案、非历史特征的残差连接、预测分数的正则化以及利用manifold mixup技术来解决标签稀疏性问题，最终使新鲜内容的参与度提高了10%，并已部署服务于超过5.7亿用户。


<details>
  <summary>Details</summary>
Motivation: 为了解决Pinterest平台上冷启动（CS）项目在训练数据中出现频率低的问题，提高推荐系统的相关性和用户体验，同时保证解决方案的成本效益和可扩展性。

Method: 通过分析实际流量数据识别出冷启动项目的挑战，并针对每个挑战开发了解决方案：设计轻量级解决方案以适应计算限制；引入非历史特征的残差连接提升其重要性；加入评分正则化项减少CS项目得分偏低的问题；应用manifold mixup技术处理数据稀疏性问题。

Result: 所提出的方法共同作用下，Pinterest的新鲜内容参与度提高了10%，且未对整体参与度及成本造成负面影响。这些改进措施已被部署，服务于超过5.7亿用户。

Conclusion: 本研究成功地解决了大规模视觉发现平台如Pinterest上冷启动项目面临的几个关键挑战，通过一系列针对性强、成本效益高的方法有效提升了推荐系统性能。

Abstract: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.

</details>


### [20] [The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability](https://arxiv.org/abs/2512.17389)
*Guangneng Hu*

Main category: cs.IR

TL;DR: 提出了一种名为LRWorld的基准测试，用于评估大型语言模型（LLMs）在推荐系统（RecSys）中的局限性和边界。基于该基准测试对数十种LLMs进行了综合实验，结果显示这些模型在浅层基于记忆的项目-项目相似性上表现良好，但在捕捉深层神经个性化嵌入方面仍存在不足。此外，LLMs在多模态知识推理和对噪声概况的鲁棒性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）作为推荐系统的知识增强器或零样本排名器展现出潜力，但两者之间存在着巨大的语义差距。为了填补这一空白，并为研究社区提供一个全面评估LLMs在RecSys中局限性和边界的基准测试，提出了LRWorld。

Method: 创建了一个包含超过38,000个高质量样本和2300万个令牌的基准测试LRWorld，这些数据是从广泛使用的公共推荐数据集中精心编译生成的。LRWorld将LLMs在RecSys中的心智世界分类为三个主要尺度（关联、个性化、知识能力），并由十个因素通过31项指标（任务）进行衡量。

Result: 实验表明，LLMs能够很好地处理浅层基于记忆的项目-项目相似性问题，但对于深层神经个性化嵌入的理解不够充分。同时，在感知项目实体关系、实体层次分类法以及推断用户兴趣时的项目-项目关联规则方面表现较好。此外，LLMs在多模态知识推理（如电影海报和产品图片）及应对嘈杂资料方面显示出了良好的前景。然而，没有任何一种模型在这十个因素上都表现出一致的良好性能。

Conclusion: 虽然大型语言模型在某些RecSys任务中表现出色，特别是在浅层项目-项目相似性和多模态知识推理方面，但它们对于深层个性化特征的理解仍有待提高。LRWorld作为一个新的基准测试平台，为未来的研究提供了宝贵资源。

Abstract: Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.

</details>


### [21] [A Systematic Reproducibility Study of BSARec for Sequential Recommendation](https://arxiv.org/abs/2512.17442)
*Jan Hutter,Hua Chang Bakker,Stan Fris,Madelon Bernardy,Yuanna Liu*

Main category: cs.IR

TL;DR: 该论文研究了BSARec模型在序列推荐中的表现，通过添加频率层来增强对高频信号的捕捉能力。实验结果表明BSARec在某些数据集上优于其他方法，并提出了一种量化用户历史频率的新指标以评估不同用户群体的表现。此外，研究还比较了不同的数字信号处理技术，发现离散小波变换相较于傅里叶变换仅有轻微改进，而简单的残差连接效果也不逊色。最后，探讨了填充策略的影响，指出非常数填充能显著提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制作为基于Transformer模型的核心组件，在序列推荐中起到了低通滤波器的作用，限制了它们捕捉反映短期用户兴趣的高频信号的能力。为了解决这个问题，BSARec引入了一个频率层来重缩放高频成分。然而，BSARec的整体有效性及其各个组成部分的具体作用尚未得到系统验证。

Method: 本文首先重现了BSARec，并展示了它在某些数据集上的表现优于其他序列推荐方法。为了实证BSARec是否提高了对高频信号的处理能力，研究人员提出了一种度量用户历史频率的方法，并根据不同用户群体评估了各种序列推荐方法的效果。此外，文章还对比了几种数字信号处理技术（包括离散小波变换和傅里叶变换）以及简单残差连接的效果。最后，探索了不同填充策略对于模型性能的影响。

Result: 实验结果显示，BSARec确实在一些数据集上超越了现有的序列推荐方法。关于数字信号处理技术的比较显示，离散小波变换与傅里叶变换相比仅带来微小改进，同时这些DSP方法相对于简单的残差连接没有明显优势。另外，研究发现采用非常数填充可以大大提升推荐系统的性能，而常数填充则会妨碍频率调整器捕捉高频信号的能力。

Conclusion: 这项工作不仅验证了BSARec在改善序列推荐任务中捕捉高频信号方面的能力，同时也揭示了不同组件和技术选择对于最终性能的影响。特别是强调了合适填充策略的重要性，指出了未来进一步优化的方向。

Abstract: In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.

</details>


### [22] [Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure](https://arxiv.org/abs/2512.17733)
*Jingmao Zhang,Zhiting Zhao,Yunqi Lin,Jianghong Ma,Tianjun Wei,Haijun Zhang,Xiaofeng Zhang*

Main category: cs.IR

TL;DR: 提出Cadence框架，通过因果去混淆共购关系和反事实曝光来增强推荐系统的多样性同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于物品间关系的推荐方法容易受到物品流行度偏差和用户属性的影响，并且缺乏对推荐多样性的关注以及因果视角和理论基础的研究。

Method: 1. 计算排除了物品流行度和用户属性影响后的无偏不对称共购关系（UACR），构建去混淆的有向物品图，并使用聚合机制优化嵌入。
2. 利用UACR识别出与用户已交互过的物品具有强烈因果相关性但尚未被接触过的多样化类别物品。
3. 模拟这些物品在高曝光场景下的表现，从而大幅提高推荐多样性的同时保持相关性。

Result: 在真实世界数据集上的广泛实验表明，该方法在多样性和准确性方面始终优于最先进的多样性模型，并进一步验证了其有效性、可迁移性和效率。

Conclusion: Cadence提供了一种有效的方法来提升推荐系统的多样性而不牺牲准确性，通过去除共购关系中的偏差因素，并采用因果推断来发现更多样化的推荐选项。

Abstract: Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [23] [Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows](https://arxiv.org/abs/2512.17429)
*Kyriakos Psarakis*

Main category: cs.DB

TL;DR: 本论文旨在通过解决可编程性、高性能容错可序列化事务和无服务器语义这三个挑战，使云应用程序开发民主化。首先通过T-Statefun探索了云应用与数据流执行模型之间的联系，但发现该方法在可编程性和性能方面存在显著限制。为了解决这些问题，论文提出了Stateflow，这是一种高级面向对象编程模型，并基于此模型推出了Styx，一种分布式流数据流引擎，提供具有强容错保证的确定性、多分区、可序列化事务。最后，通过引入事务状态迁移以支持动态工作负载下的弹性扩展，进一步增强了Styx的功能。


<details>
  <summary>Details</summary>
Motivation: 尽管Web应用程序支撑了现代数字生活的很多方面，但构建可扩展且一致性的云应用程序仍然很困难，需要跨云计算、分布式系统、数据库和软件工程的专业知识。这些要求将开发限制在少数高度专业化的工程师手中。因此，本论文的目标是通过解决三个主要挑战（可编程性、高性能容错可序列化事务以及无服务器语义）来让更广泛的开发者能够参与云应用程序开发。

Method: 论文首先通过一个名为T-Statefun的实验项目探索了云应用与数据流执行模型之间存在的紧密联系，该项目是对Apache Flink Statefun的一个事务性扩展。虽然这种方法展示了一定潜力，但也揭示了其在可编程性和性能上的局限性。
随后，为了克服上述问题，研究者提出了一种新的高级面向对象编程模型——Stateflow，它能以最少的样板代码将应用程序编译成有状态的数据流图。
基于Stateflow模型之上，又设计实现了Styx，这是一个分布式流处理引擎，能够提供具备强大容错能力保障的确定性、多分区可序列化事务处理功能。此外，还通过对Styx进行扩展增加了对事务状态迁移的支持，从而能够在面对动态变化的工作负载时实现更好的弹性适应能力。

Result: 通过引入Stateflow及后续发展的Styx系统，不仅大大简化了构建复杂云应用程序的过程，同时也显著提高了此类应用在执行事务操作时的表现。特别是，Styx提供的确定性、多分区可序列化事务处理能力，在确保数据一致性的同时也达到了优于现有解决方案的性能水平。此外，加入事务状态迁移机制后，使得整个系统对于不同规模和类型的工作负载都表现出了良好的适应性和扩展性。

Conclusion: 本论文成功地展示了如何利用数据流执行模型来改善云应用程序开发流程，并通过提出创新性的Stateflow编程模型与Styx执行引擎相结合的方法论，有效解决了当前领域内存在的可编程性差、性能不足等问题。最终结果表明，所提出的方案不仅能够提高开发效率，同时还能确保应用程序具备优秀的事务处理能力和伸缩性。

Abstract: Web applications underpin much of modern digital life, yet building scalable and consistent cloud applications remains difficult, requiring expertise across cloud computing, distributed systems, databases, and software engineering. These demands restrict development to a small number of highly specialized engineers. This thesis aims to democratize cloud application development by addressing three challenges: programmability, high-performance fault-tolerant serializable transactions, and serverless semantics.
  The thesis identifies strong parallels between cloud applications and the streaming dataflow execution model. It first explores this connection through T-Statefun, a transactional extension of Apache Flink Statefun, demonstrating that dataflow systems can support transactional cloud applications via a stateful functions-as-a-service API. However, this approach revealed significant limitations in programmability and performance.
  To overcome these issues, the thesis introduces Stateflow, a high-level object-oriented programming model that compiles applications into stateful dataflow graphs with minimal boilerplate. Building on this model, the thesis presents Styx, a distributed streaming dataflow engine that provides deterministic, multi-partition, serializable transactions with strong fault tolerance guarantees. Styx eliminates explicit transaction failure handling and significantly outperforms state-of-the-art systems.
  Finally, the thesis extends Styx with transactional state migration to support elasticity under dynamic workloads.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: 本文介绍了一种名为Atom的系统，该系统通过将大规模视频-语言模型分解为可重用模块并在行执行，实现了在移动设备上高效快速地处理视频-语言任务。


<details>
  <summary>Details</summary>
Motivation: 当前视频-语言模型在移动设备上的多阶段管道执行效率低下，存在着冗余模型加载和执行碎片化的问题。

Method: 提出了Atom系统，它能够将一个拥有数十亿参数的模型分解成可重复使用的组件（如视觉编码器和语言解码器），并在诸如字幕、推理和索引等子任务之间重用这些组件。此设计减少了重复模型加载，并支持并行执行以降低端到端延迟。

Result: 在普通智能手机上，与不采用重用机制的基础方案相比，Atom实现了27-33%的更快执行速度，同时性能下降微乎其微（检索Recall@1不超过2.3%，字幕CIDEr不超过1.5%）。

Conclusion: Atom作为一个实用且可扩展的方法，在边缘设备上提供高效的视频-语言理解服务，展示了显著的速度提升而不明显牺牲性能。

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [25] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: Dion2 is a new, simpler method that reduces the computational and communication costs of the Muon optimizer by selectively orthonormalizing only a fraction of rows or columns at each iteration, thus improving scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the increasing overhead caused by the super-linear cost of the orthonormalization step in the Muon optimizer, which becomes more pronounced as the scale increases. Previous attempts to reduce this cost have involved reducing the size of the matrix entering the orthonormalization step, but the authors propose a simpler and more efficient solution with Dion2.

Method: The method proposed in the paper, named Dion2, involves selecting only a portion of the rows or columns for orthonormalization at each iteration, rather than processing the entire matrix. This selective orthonormalization process leads to sparser updates, thereby lowering both the computation and communication costs associated with the Muon's orthonormalization step.

Result: The result of applying Dion2 is a reduction in the overall costs related to orthonormalization, making the Muon optimizer more scalable and efficient, especially at larger scales where the original super-linear cost would become prohibitive.

Conclusion: In conclusion, the paper introduces Dion2 as an effective and simple approach to improve the scalability of the Muon optimizer by decreasing the orthonormalization step's overhead through selective sampling, leading to a more efficient use of computational resources.

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [26] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 本项目开发了一种低成本的双模式神经肌肉控制系统，结合了脑电图（EEG）和肌电图（EMG）技术，以实现假肢手臂的实时多自由度控制。通过使用经济实惠的材料和技术，该系统能够为资源匮乏地区的截肢者提供直观且功能性的假肢控制解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于在资源有限的环境中，上肢假体往往缺乏直观的控制系统，这限制了它们的功能性和可访问性。因此，该项目旨在开发一种低成本、直观控制的假肢系统，以满足低收入地区截肢者的需求。

Method: 项目采用了一个结合了脑电图(EEG)与肌电图(EMG)的双模态神经-肌肉控制系统。EEG信号通过NeuroSky MindWave Mobile 2设备采集，并通过蓝牙传输给ESP32微控制器进行处理。一个轻量级分类模型被训练来识别强眨眼事件，以此作为开关手部状态的触发器。同时，EMG信号则由MyoWare 2.0传感器获取并通过无线模块发送到另一个ESP32，该微控制器根据阈值检测来区分不同的肌肉活动阶段。

Result: 构建了一个功能性原型，其总成本约为240美元，主要开支来自商业EEG头戴设备。该系统展示了如何利用低成本材料创建出生物直觉式的假肢控制方案，适用于服务不足地区及全球健康应用。

Conclusion: 研究结果表明，这种结合了EEG和EMG的低成本双模式控制系统能够有效提高假肢手臂的可控性和自然性，尤其适合于资源匮乏环境下的应用。未来的工作将集中在进一步降低成本、改进设计以及提升性能等方面。

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [27] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 本研究提出了一种基于物理引导特征工程的轻量级梯度提升框架（XGBoost），仅使用地面观测数据（METAR）来预测低能见度和降水事件。该模型在11个国际机场的历史数据上进行了评估，相比传统的TAF预报，在战术时间尺度（3小时）内实现了显著更高的检测率，并减少了误报。此外，通过SHAP分析揭示了模型能够隐式重建局部物理驱动因素，如平流、辐射和下沉，为操作情景意识提供了可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前航空气象预报依赖于计算密集型的数值天气预报指导和人工发布的TAF产品，这些方法往往存在保守偏差且时间分辨率有限。为了提高航空安全性和运营效率，需要开发一种更加准确且高效的短期天气事件预测方法。

Method: 采用轻量级梯度提升框架XGBoost，仅基于表面观测数据（METAR）并通过基于热力学原理的物理引导特征工程进行增强。该框架在全球代表不同气候区的11个国际机场上进行了测试，利用从2000年至2024年的历史数据来训练与验证模型性能。

Result: 结果表明，该自动化模型能够在不需要手动配置的情况下成功捕捉到本地物理过程。相较于现有的TAF预报，在3小时的时间范围内，新模型的召回率提高了2.5至4.0倍，同时减少了错误警报的数量。

Conclusion: 所提出的基于物理引导机器学习的方法不仅提高了低能见度及降水事件的短期预测准确性，还通过提供可解释的人工智能特性增强了操作员的情境感知能力，对于边缘计算环境下的应用具有重要意义。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [28] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 研究者们发现直接将常用的GRPO算法应用于多轮次任务中存在局限性，特别是在需要长期推理的场景下。为此，他们探索了更稳定有效的优势估计策略，并提出了turn-PPO，一种基于回合级MDP而非通常的令牌级MDP的方法。实验结果表明，无论是否加入长期推理组件，turn-PPO在WebShop和Sokoban数据集上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的GRPO算法在处理需要长时间跨度推理的多轮次任务时暴露出显著的局限性。因此，研究旨在寻找更加稳定且高效的优势估计方法来改善这一情况。

Method: 首先尝试使用近端策略优化（PPO）作为替代方案，并观察到其相比GRPO具有更好的鲁棒性；接着，为提高PPO在多轮次情景下的表现，提出了一种名为turn-PPO的新变体，该方法基于回合级别而非单个标记级别的马尔可夫决策过程（MDP）。

Result: 通过WebShop和Sokoban两个数据集上的测试证明了turn-PPO的有效性，即使是在没有额外长期推理机制的情况下也能取得良好效果。

Conclusion: 本研究表明，在面对需要进行长周期思考的任务时，采用如turn-PPO这样基于回合级别MDP设计的方法能够提供优于传统GRPO算法的表现。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [29] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 本文研究了在大量噪声样本存在的情况下如何恢复原始分布的问题，提出了一种基于单边熵最优传输问题的解决框架，并引入了SFBD-OMNI模型来实现从被污染样本到真实分布的映射。实验表明该方法在多种条件下均能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 在很多实际场景中，获取完全观测样本成本高昂甚至不可行，而部分和带有噪音的观察相对容易收集。因此，本研究旨在通过利用丰富的噪声样本来恢复真实的底层数据分布。

Method: 作者将这个问题定义为一个单边熵最优传输问题，并使用类似于EM算法的方法求解。此外，还提出了一个测试标准来判断给定每样本信息损失情况下是否能够恢复真实的分布，并且指出少量干净样本可以大大提高恢复的可能性。基于这些发现，研究人员开发了SFBD-OMNI框架，它扩展了Stochastic Forward-Backward Deconvolution技术以适应更广泛的测量模型。

Result: 实验结果显示，在多个基准数据集以及不同类型的测量设置下，所提出的方法相较于现有技术在定性和定量表现上都有显著提高。

Conclusion: 这项工作展示了一种有效的方法来从大量的噪声样本中恢复原始的数据分布，即使是在传统意义上被认为无法恢复的情形下也能取得良好效果，特别是在有少量无噪声样本辅助时。

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [30] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: 本文提出了一种自适应剪枝算法，旨在减少ST-GNNs在分布式计算节点（云粒）部署时的通信开销，同时保持预测准确性。此外，还引入了新的度量标准SEPA来衡量对交通突发情况的响应能力。实验表明，该方法能在不牺牲关键交通事件响应性的前提下显著降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决Spatio-Temporal Graph Neural Networks (ST-GNNs) 在处理来自地理分布传感器的高频率数据流时遇到的问题，即当它们被部署于边缘的分布式计算节点上时产生的大量通信开销问题。

Method: 提出了一种能够动态过滤冗余邻居特征的自适应剪枝算法，同时保留用于预测的最有信息量的空间上下文。此算法根据最近的模型性能调整剪枝率，使每个云粒能够专注于经历流量变化的区域而不影响准确性。

Result: 实验结果证明，在所有在线半分散设置中，提出的自适应剪枝算法能够在维持预测准确性的基础上大幅降低通信成本，并且新提出的SEPA指标能够更好地评估系统对于动态和不规则交通状况的预测能力。

Conclusion: 通过采用自适应剪枝算法与SEPA评价指标，可以在不影响对关键交通事件响应性的条件下有效减少通信消耗。

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [31] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 本文证明了在完备可分度量空间X中，k-最近邻分类器的一致性与强勒贝格-贝西科维奇微分性质及Nagata意义下的sigma-有限维性质之间的等价关系。


<details>
  <summary>Details</summary>
Motivation: 为了证明在完备可分离度量空间X中，k-最近邻分类器的（弱）普遍一致性、强勒贝格-贝西科维奇微分性质对每个局部有限Borel测度成立以及X是Nagata意义上的sigma-有限维这三者间的等价性，填补了最后一个未被证实的推论（即从条件(1)到条件(3)的推导）。

Method: 通过数学证明方法，特别是分析和拓扑学的方法，作者们验证了k-最近邻分类器的一致性可以推出Nagata意义上空间的sigma-有限维性质。

Result: 成功地证明了当k-最近邻分类器在空间X中是（弱）普遍一致时，该空间X必然是Nagota意义上的sigma-有限维。此外，纠正了之前文章中的错误主张。

Conclusion: 完成了对于k-最近邻分类器一致性与其它两个属性间等价性的最后一步证明，从而建立了这三个属性之间的完整等价链。

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [32] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 本文提出了一种带宽高效的自适应混合专家模型，通过低秩补偿实现了路由引导的精度恢复，在保持其他部分低位的同时，仅对Top-n专家进行补偿。该方法在GPU和GPU-NDP系统上与卸载集成时，提供了更好的带宽-准确性权衡及吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) 模型虽然通过稀疏激活来增加容量，但给内存和带宽带来了压力。为了解决这一问题，研究者们提出了离线加载的方法以减轻GPU内存负担，但这会导致不规则的数据传输从而使推理过程受到I/O限制。静态均匀量化虽能减少流量，但在激进压缩下会忽略专家间的异质性从而降低精度。基于以上背景，本文旨在开发一种既能有效利用带宽又能保持高精度的新方法。

Method: 本文介绍了一种名为Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation的方法，它利用预计算的低秩补偿器来进行路由器引导下的精度恢复。在推理过程中，本方法只传输紧凑的低秩因子，并针对每个token选择出Top-n（n<k）个专家应用补偿，而其他部分则保持低位存储。

Result: 实验结果表明，当与GPU以及GPU-NDP系统的卸载技术结合使用时，所提出的方法能够提供更优的带宽-准确性平衡，并且提高了处理速度。

Conclusion: 综上所述，本文提出的带宽高效自适应混合专家模型通过引入低秩补偿机制成功地解决了现有方法中存在的问题，不仅减少了数据传输需求还维持了较高的模型性能，对于需要大规模并行计算的应用场景具有重要意义。

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [33] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 本研究探讨了通过在训练过程中引入含有故意错误的推理链（CoT）前缀，能否教会模型检测并从这些错误中恢复，同时不损害其标准问题解决能力。实验结果表明，这种混合CoT-RL模型不仅在干净的问题上保持了与标准RL相当的表现，在被预填充有缺陷推理的问题上也显著优于标准RL，并且暴露于有缺陷的追踪训练中可以提高错误恢复行为而不牺牲准确性，为实现LLM中更稳健的数学推理指出了一个方向。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维（CoT）提示已经成为大型语言模型中数学推理的核心，但模型对于早期错误仍然非常脆弱：单个算术失误或无根据的推断通常会不受纠正地传播到最终答案。因此，研究者们探索是否可以通过训练模型识别和从这类错误中恢复来解决问题，而不会降低其常规解题能力。

Method: 研究采用了来自MATH-lighteval竞赛级别的问题，生成了恰好包含一个受控错误的CoT前缀，这些错误可能是计算错误（如符号翻转、遗漏项）或者是推理错误（如误用规则、无根据逻辑步骤）。然后使用二元最终答案奖励对Qwen3-4B进行了GRPO微调。

Result: Mixed-CoT-RL模型在清洁问题上与标准RL表现相当(41% vs 41%)，而在预先填充有缺陷推理的问题上则明显优于后者(24% vs 19%)。值得注意的是，仅进行清洁问题的RL微调会使鲁棒性下降至未经调整基线以下(19% vs. 20%)，这表明传统训练方法增加了对误导性预填充的敏感度。在各种错误类型中，针对推理错误的训练比单独针对计算错误的训练能带来更大的鲁棒性提升，而混合训练效果最佳。

Conclusion: 研究表明，通过在训练期间暴露于存在缺陷的追踪可以帮助改进模型的错误恢复行为，同时不会牺牲准确性，从而为提高大型语言模型中的数学推理鲁棒性提供了可能途径。

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [34] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: 提出了一种新的方法，结合了强化学习和模型预测控制来解决具有层次结构的规划问题。该方法通过强化学习的动作指导MPPI采样器，并自适应地聚合MPPI样本以估计价值。这种自适应过程在价值估计不确定的地方增加了探索，从而提高了训练稳健性和策略效果。实验结果表明，相比现有方法，新方法在数据效率、任务成功率等方面表现更好，成功率达到72%的增长，且相较于非自适应采样收敛速度提高了2.1倍。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂的规划问题并提高对不同应用的适应性，本文旨在融合强化学习与模型预测控制两种规划范式的优势，开发出一种更加鲁棒且高效的规划方法。

Method: 采用了一种新颖的方法，将强化学习（RL）与模型预测控制（MPC）紧密结合。此方法利用来自RL的动作信息来指引随机路径积分（MPPI）采样器的工作；同时，基于MPPI产生的样本自适应地更新价值评估。整个过程中，当面对不确定性较高的价值估计时会进一步促进MPPI的探索活动，以此增强训练稳定性和最终策略的有效性。

Result: 通过赛车驾驶、改良版Acrobot及增加障碍物后的月球着陆器等多个领域的测试证明，所提方法在数据使用效率、奖励获取量以及任务完成成功率方面均优于现有技术。特别是在成功率上观察到了高达72%的增长，并且相对于不使用自适应采样的情况，算法收敛速度提升了约2.1倍。

Conclusion: 本研究展示了一种结合强化学习与模型预测控制的新颖规划方案，它不仅能够处理复杂多变的任务情境，而且具备良好的跨领域迁移能力。实验证明，该方法在多个基准测试中取得了显著优于传统方法的表现。

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [35] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: 本文提出了首个用于识别历史悠久但资源较少的古尼泊尔语手写文本的端到端流程，通过探索编码器-解码器架构和以数据为中心的技术来提高识别准确性。最佳模型达到了4.9%的文字错误率，并且我们公开了训练代码、模型配置和评估脚本以促进低资源历史手稿的HTR研究。


<details>
  <summary>Details</summary>
Motivation: 针对古尼泊尔语这一具有历史意义但资源匮乏的语言，开发一套有效的手写文本识别系统。

Method: 采用行级转录方法，系统地探讨了编码器-解码器架构以及数据集中技术对手写文本识别精度的影响。此外还实现了译码策略并对标记级别混淆进行了分析，以便更好地理解模型行为与错误模式。

Result: 最好的模型在字符错误率(CER)方面达到了4.9%的成绩。

Conclusion: 该研究为低资源历史手稿的手写文本识别开辟了新的道路，并通过开源相关资源支持进一步的研究工作。

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [36] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 本研究评估并改进了Stanford AIMI CheXagent模型在处理含有否定词的提示时检索胸部X光图像的能力，通过微调方法提高了对否定句的处理能力，尽管这略微降低了正面提示的准确性。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型视觉-语言模型如CLIP在医疗影像任务中的应用日益增多，但它们在解释否定短语方面表现不佳，特别是在医学诊断中这是一个问题。本研究旨在了解模型在哪方面失败，并基于此改进其检索准确率。

Method: 使用Stanford AIMI CheXagent模型进行测试，分别用包含和不包含否定的提示来检索胸部X光图像。采用之前工作中提出的微调方法来提高模型对否定句的理解能力。同时，通过token归属、t-SNE投影以及注意力头消融等手段检查模型内部行为变化。

Result: 研究表明，在处理否定句方面有所改进，但对正面提示的评价准确性略有下降。此外，还观察到了不同微调方法如何改变文本编码器对于临床否定语言的表现形式。

Conclusion: 通过这项工作，我们希望能够更好地理解CLIP模型的内部机制，并通过改进其对临床相关语言中否定句的处理能力来提高其在医疗AI设备中的可靠性。

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [37] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: 提出了广义原始平均（GPA），这是一种改进的Nesterov方法，旨在解决非分布式设置下单工DiLoCo和无调度（SF）等基于平均优化器的关键局限性。GPA通过在每一步平滑地平均迭代来克服这些限制，从而简化了超参数调整并减少了内存开销。实验表明，GPA在达到基准验证损失方面比单工DiLoCo更快，并且对于任何具有O(√T)后悔界的基本优化器，GPA能够匹配或超越其收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有的基于平均的优化器如单工DiLoCo和无调度(SF)存在一些关键局限性，例如单工DiLoCo周期性平均引入了双循环结构，增加了内存需求和超参数数量。本研究旨在通过扩展Nesterov方法的原始平均形式来解决这些问题，提出了一种新的优化方法——广义原始平均（GPA）。

Method: GPA通过对Nesterov方法中的插值常数进行解耦，实现了在每次迭代中平滑地平均模型更新，从而改善并推广了单工DiLoCo的方法。该方法消除了需要双循环结构的需求，简化了超参数调优过程，并将额外的内存开销减少到了只有一个缓冲区。

Result: 实验结果显示，在Llama-160M模型上，GPA相比AdamW达到了验证损失基线所需步骤的速度提升了24.22%；在ImageNet ViT工作负载下，针对小批量和大批量设置，分别实现了相对于AdamW 12% 和 27% 的加速。此外，理论分析证明，对于任一具有O(√T)后悔界的基本优化器，通过选择合适的插值常数，GPA能匹配甚至超越原优化器的收敛保障。

Conclusion: GPA作为一种新颖的优化算法，在保持或提高收敛性能的同时，有效解决了现有基于平均优化技术存在的复杂性和效率问题。它不仅在实际应用中表现出色，还为理解及进一步开发高效优化策略提供了坚实的理论基础。

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [38] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 本文系统地评估了多种时间序列预测模型在不同时间范围（从几分钟到几天）和空间聚合水平（从单个充电站到区域和城市级别）上的电动汽车充电需求预测效果，使用了四个公开的真实世界数据集。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车作为应对气候变化手段的日益流行，对它们可能给电网管理带来的影响产生了担忧。因此，预测电动汽车充电需求成为了一个及时且重要的研究问题。然而，在交通能源负荷预测领域虽然已有大量研究，但系统性地比较不同时间范围和空间聚合水平下多种预测方法的研究相对较少。

Method: 本研究采用了五种时间序列预测模型，包括传统统计方法、机器学习以及深度学习方法，并针对短、中、长期（分别对应分钟级、小时级和天数级）的时间范围及从单独充电桩到区域乃至城市级别的空间尺度进行了性能评估。研究基于四个公开可用的真实世界数据集开展，并为每个数据集独立报告结果。

Result: 研究表明，在不同时间范围和空间尺度上，各模型对于电动汽车充电需求的预测能力存在差异。这是首次尝试使用多个真实世界数据集来全面评价电动汽车充电需求预测的有效性。

Conclusion: 通过对比分析，本文揭示了不同预测模型在处理多样化的时空条件下电动汽车充电需求时的优势与局限性，为未来相关领域的研究提供了有价值的参考。

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [39] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的度量R-EMID，用于以可解释的方式衡量角色扮演模型（RPM）性能下降，并通过共进化强化学习框架来提高对话响应生成概率的估计，从而增强RPM在面对用户、角色和对话组合变化时的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法对导致角色扮演模型（RPM）在实际部署中表现不佳的各种分布偏移提供细粒度诊断，缺乏正式框架来描述RPM的泛化行为。

Method: 引入了名为R-EMID的信息论度量标准；推导了R-EMID的上限，以预测最坏情况下的泛化表现；提出了一个共同进化的强化学习框架，用以适应性地建模用户、角色与对话上下文之间的联系，进而改进对话响应生成概率的估计。

Result: 使用R-EMID评估不同RPMs的泛化性能，发现用户偏移是所有偏移中风险最高的，而强化学习是在增强RPM泛化方面最有效的方法。

Conclusion: 研究为理解及改善角色扮演模型在面对不同类型分布偏移时的表现提供了新的视角和技术手段。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [40] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: 本文提出了一种半监督框架MATCH-AD，通过深度表征学习、基于图的标签传播和最优传输理论来提高阿尔茨海默病(AD)诊断的准确性。即使在标签数据非常有限的情况下，该框架也能达到接近完美的诊断精度，并且显著优于所有基线方法。


<details>
  <summary>Details</summary>
Motivation: 临床评估对于阿尔茨海默病（AD）来说既昂贵又具有侵入性，导致只有少量神经影像数据集拥有真实标签。为了解决这一问题，研究者们开发了新的方法以更有效地利用这些有限的数据来进行准确的诊断。

Method: 本研究引入了一个名为MATCH-AD的半监督学习框架，它结合了深度表示学习、基于图的标签传播以及最优传输理论，旨在从有限的已标记样本中推断出更广泛未标记群体的信息。此外，还使用Wasserstein距离来量化不同认知状态间的疾病进展程度。

Result: 在近五千名来自国家阿尔茨海默氏症协调中心的受试者上进行测试，其中包括数百个大脑区域的结构MRI测量、脑脊液生物标志物及临床变量，结果显示MATCH-AD达到了几乎完美的诊断准确性，即便是在少于三分之一的受试者有真实标签的情况下。与基线方法相比，该框架表现出色，在标签稀缺条件下依然保持了临床实用性。

Conclusion: 这项研究表明，原则性的半监督学习能够释放全球范围内积累的大量部分注释神经影像数据的诊断潜力，大大减轻了标注负担的同时保持了适合临床应用的准确性。

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [41] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: 该研究通过因果机制验证了上下文学习（ICL）可以分解为两个可分离的机制：任务模式（抽象任务类型识别）和绑定（特定输入-输出关联）。实验结果表明这两个机制在神经上是可分离的，并且模型依赖于任务模式还是先前知识取决于具体情况。此外，这些发现有助于提高ICCL系统在实际部署中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过激活修补实验来探究不同模型架构中上下文学习（ICL）的具体运作机制，以解决过去将ICL视为单一机制的看法所带来的局限性。

Method: 通过对9个来自7种Transformer家族加上Mamba（参数范围3.7亿到130亿）的模型进行激活修补实验，研究人员验证了任务模式和绑定作为ICL两个可分离机制的存在。

Result: 研究发现了三个关键点：1) 任务模式与绑定之间存在双重分离现象；2) 模式依赖性与先验知识呈负相关关系；3) 所述机制适用于所有测试过的架构。

Conclusion: 这项工作为理解ICL提供了一个新的视角，即它是由任务模式和绑定这两种独立但相互作用的机制组成的。这种理解有助于改善提示工程效率并提高ICL系统在高先验场景下的可靠性。

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [42] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 研究提出了一种新的框架LLM-SGA和检测器ARHOCD，以增强对抗有害内容检测的鲁棒性和准确性。通过集成多种基础检测器、动态调整权重的方法以及一种新颖的对抗训练策略，ARHOCD在三个数据集上展示了强大的泛化能力和改进的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台受到诸如仇恨言论、错误信息和极端主义言论等有害内容的影响。虽然机器学习模型被广泛用于检测这些内容，但它们很容易受到对抗性攻击。因此，提高对抗性鲁棒性变得至关重要，这意味着检测器需要能够抵御各种攻击（泛化能力）同时保持高精度。然而，同时实现最佳泛化能力和准确性是具有挑战性的。

Method: 本研究首先提出了一个名为LLM-SGA的新框架，该框架通过识别文本对抗攻击的关键不变量并利用它们来确保基于此框架实例化的检测器具有强大的泛化能力。其次，研究人员开发了一个名为ARHOCD的检测器，它包含三个创新设计组件：(1) 一个集合了多个基础检测器的系统，利用它们互补的优势；(2) 一种新型权重分配方法，根据每个样本的可预测性和每个基础检测器的能力动态调整权重，并使用领域知识初始化权重并通过贝叶斯推理更新；(3) 一种新的对抗训练策略，迭代优化基础检测器和权重分配者。

Result: ARHOCD在涵盖仇恨言论、谣言及极端内容的三个数据集上进行了实证评估。结果表明，ARHOCD不仅提供了强大的泛化能力，在对抗条件下也提高了检测准确度。

Conclusion: 这项工作解决了现有对抗鲁棒性增强研究中的几个局限性，证明了所提出的ARHOCD检测器在面对恶意用户试图绕过检测时的有效性。

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [43] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 研究揭示了奖励模型和LLM作为评判系统在诸如RLHF、DPO和RLAIF等现代后训练管道中的一个常见脆弱性：短序列的低困惑度控制标记可以操纵许多二元评估，从正确的“否”判断转为错误的“是”判断。通过使用AdvJudge-Zero方法发现这些控制标记，并指出它们在隐藏状态扰动中集中于与评判拒绝方向相反的低秩‘软模式’。此外，实证显示这些标记会导致高误报率，而基于LoRA的对抗性训练能够显著减少误报同时保持评估质量。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在探索奖励模型和LLM作为评判系统中存在的一个潜在漏洞，即通过插入特定的控制标记来操控模型的决策过程，从而影响基于此反馈进行的模型选择和微调。这种操作可能导致不正确的判断结果，对依赖这类系统的应用构成威胁。

Method: 提出了一种名为AdvJudge-Zero的方法，它利用模型的下一个token分布和束搜索探索技术，从零开始发现多样化的控制标记序列。进一步分析了这些标记如何通过诱导隐藏状态的小扰动，在最后一层logit差距上产生影响，促使原本应为“否”的判断转向“是”。

Result: 研究表明，通过AdvJudge-Zero方法找到的控制标记确实能够以高概率导致错误的正面评价，尤其是在数学和推理基准测试中。另外，实验还表明，采用基于LoRA的小规模对抗性训练可以在保留评估质量的同时大幅降低由控制标记引起的误报率。

Conclusion: 本文揭示了当前使用的奖励模型及LLM评判系统面对特定控制标记时存在的脆弱性问题，并提供了一种有效检测并缓解此类风险的方法论。这为进一步提高AI系统的鲁棒性和安全性提供了新的思路。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [44] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 本文介绍了一个统计工具箱，旨在帮助从业者以严谨的方式评估机器学习模型在不同患者和记录属性下的性能差异。该工具箱特别适用于医学影像应用，并通过两个案例研究展示了其分析能力：一个是基于ISIC2020数据集的皮肤病变恶性分类，另一个是基于MIMIC-CXR数据集的胸部X光疾病分类。


<details>
  <summary>Details</summary>
Motivation: 作者指出，在不同的患者及记录特性下对机器学习模型的表现进行分层分析已经成为共识，这通常能揭示关于模型失败模式的关键见解。然而，以统计学上严格的方法执行这种分析并不简单。因此，开发一个能够解决这些挑战、便于使用者严谨地评估模型潜在子群表现差异的统计工具箱显得十分必要。

Method: 提出了一套统计工具箱，它允许选择合适的性能指标来比较不同样本大小和基础比率之间的组别；确定度量不确定性并对多重比较进行校正，以便判断观察到的任何差异是否仅仅是偶然造成的；对于交叉分析，实现机制以在组合众多的子组中找到最‘有趣’的子组。

Result: 这个统计工具箱为医疗成像应用提供了有效的支持，通过两个具体的案例研究——ISIC2020数据集上的皮肤病变恶性分类以及MIMIC-CXR数据集上的胸部X射线疾病分类——展示了如何使用该工具箱进行分析。

Conclusion: 所提出的统计工具箱能够有效地帮助从业者在复杂的实际场景中评估机器学习模型的性能差异，特别是针对医学影像领域内的应用。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [45] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 本文提出了一种多智能体强化学习模型，用于捕捉脱碳能源系统的关键特征，并在不同竞争水平、市场设计和政策情景下应用于简化的意大利电力系统中，结果突出了市场设计对于电力部门脱碳和避免价格波动的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了支持决策者和其他利益相关者设计、测试和评估长期市场，需要更先进的工具来促进电力系统的脱碳过程。

Method: 采用独立近端策略优化的多智能体强化学习模型，通过广泛的超参数搜索确保去中心化训练产生与竞争行为一致的市场结果。

Result: 该模型在简化的意大利电力系统版本上进行了应用和测试，结果显示了市场设计在电力部门脱碳及避免价格波动方面扮演着关键角色。

Conclusion: 提出的框架能够评估多种政策和市场机制同时交互作用下的长期电力市场，其中市场主体对脱碳路径做出响应并进行适应。

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [46] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Write-Gated KV的轻量级机制，通过学习预测进入缓存前的token效用，以减少长上下文推理中的内存使用和加速预填充及解码过程。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型推理受限于二次注意力复杂度以及线性KV缓存增长的问题。现有的方法未能解决根本问题：即对持久内存的无差别写入带来的效率低下。

Method: 作者将KV缓存管理形式化为一个因果系统，包括三个基本操作：KV准入、选择与淘汰，并通过Write-Gated KV实现KV准入，该机制能够学习预测每个token的重要性，从而避免低效状态过早进入缓存，保持全局缓存紧凑的同时维护局部滑动窗口内的缓存。

Result: 实验显示，在Llama模型上应用Write-Gated KV可以降低46-57%的内存消耗，同时分别提高了3.03-3.45倍的预填充速度和1.89-2.56倍的解码速度，且几乎没有准确性损失。此外，该方法还能兼容FlashAttention和分页KV系统。

Conclusion: 研究表明，通过学习决定哪些内容应该被写入KV缓存是一种既实用又具有原则性的方法，能够有效提高长上下文推理的效率。

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [47] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: 本文介绍了一种名为SafeBench-Seq的基准测试和分类器，用于蛋白质序列级别的风险筛查。它仅使用公共数据构建，并通过同源性聚类来模拟前所未见的威胁。评估结果显示线性模型在经过校准后表现出较好的校准性能，而树集成方法则有略高的Brier得分和ECE。该工具只依赖CPU运行，且不会分发危险序列。


<details>
  <summary>Details</summary>
Motivation: 针对蛋白质设计的基础模型带来了具体的生物安全风险，但社区缺乏一个简单、可重复的序列级别风险筛查基线，该基线应该能够在同源控制下进行明确评估，并能在普通CPU上运行。

Method: 开发了SafeBench-Seq，这是一个仅基于元数据的、可复现的基准测试与分类器，完全由公开数据（SafeProtein危害数据集和UniProt良性样本）及可解释特征（全局物理化学描述符和氨基酸组成）构建而成。为了模拟‘从未见过’的威胁，研究者对合并的数据集进行了<=40%同一性的同源聚类，并执行了聚类级别的保留（训练/测试间无聚类重叠）。

Result: 随机划分相对于同源聚类评估显著高估了稳健性；经过校准后的线性模型显示出相对较好的校准效果，而树集成方法保持了稍高的Brier得分/ECE。

Conclusion: SafeBench-Seq为蛋白质序列级别的风险筛查提供了一个仅基于CPU、可复现的解决方案，能够严格评估而不分发有害序列。

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [48] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: 本文提出了一种名为协作前向-前向（CFF）学习的新方法，通过层间合作机制改进了原有的前向-前向算法，增强了表征协调性和收敛效率。实验结果表明，在MNIST和Fashion-MNIST数据集上该方法比基线前向-前向实现有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统前向-前向算法在深层架构中因层间隔离而限制了表示协调性并降低了收敛效率。为了解决这个问题，并保持仅前向计算的同时允许全局上下文集成，提出了协作前向-前向（CFF）学习框架。

Method: 引入了两种协作范式：具有恒定层间耦合的固定CFF（F-CFF）和具有可学习协作参数的自适应CFF（A-CFF），这些参数在训练过程中演化。协作良好函数结合了所有层的加权贡献，促进了特征学习的协调性，同时保持了内存效率和生物合理性。

Result: 在MNIST和Fashion-MNIST上的综合评估显示，与基础的前向-前向实现相比，所提方法表现出显著的性能改进。

Conclusion: 层间合作被确立为前向-前向学习的一个基本增强，对神经形态计算架构和能源受限的人工智能系统具有直接应用价值。

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [49] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: 本文提出了新的贝叶斯优化方法，特别适用于解耦的黑箱约束问题，能够有效识别并仅评估与最优解相关的约束条件。实验表明，该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 为了解决具有解耦黑箱约束的昂贵全局黑箱优化问题，并提高优化效率，特别是当只有少数约束在最优解处是紧约束时。

Method: 提出了一种新的贝叶斯优化变体，基于知识梯度获取函数，专门针对解耦黑箱约束问题设计。

Result: 通过实验证明了所提方法相较于当前最先进技术具有更好的性能。

Conclusion: 新提出的贝叶斯优化方法能够更有效地处理解耦黑箱约束问题，特别是在只有部分约束对最优解有影响的情况下，表现出了超越现有技术水平的优势。

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [50] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: 本文提出了一种新的SSD卸载训练系统GreedySnake，采用垂直调度策略以提高训练吞吐量并减少批处理大小，实验结果显示其在A100 GPU上对GPT-65B和GPT-175B模型的训练吞吐量相比ZeRO-Infinity有显著提升。


<details>
  <summary>Details</summary>
Motivation: 为了使大型语言模型（LLM）的训练更具成本效益，文章基于微批量梯度累积提出了GreedySnake系统。该系统旨在通过优化调度策略来提高训练效率，并且更接近理想情况下的性能预测。

Method: GreedySnake系统采用了垂直调度方法，即先执行完一层内所有微批次后再进行下一层，与水平调度方式形成对比。此外，还通过重叠部分优化步骤与下一次迭代的前向传递来进一步缓解I/O瓶颈问题。

Result: 实验结果表明，在使用A100 GPUs时，对于GPT-65B模型，GreedySnake相较于ZeRO-Infinity实现了单GPU 1.96倍、4 GPUs 1.93倍的训练吞吐量增长；而对于GPT-175B模型，则是在单GPU上达到了2.53倍的增长。

Conclusion: GreedySnake展示出了一种有效增强LLM训练过程性价比的方法，通过引入创新性的垂直调度机制以及优化步骤间的重叠处理，成功地提高了训练吞吐量同时减少了所需的批处理规模。

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [51] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 本文扩展了算法在孤立运行时的收敛性保证，并推导出在存在干扰情况下的稳定性边界和收敛速度，提供了一个统一的工具来分析存在噪声、干扰和其他动态系统相互作用情况下的算法性能。


<details>
  <summary>Details</summary>
Motivation: 随着算法越来越多地在复杂的物理、社会和工程系统中运行，它们面临干扰、噪声以及与其他动态系统的相互连通等问题。研究旨在理解这些外部因素如何影响算法的表现，并为算法在这些条件下的稳定性和收敛性提供理论基础。

Method: 通过利用逆向Lyapunov定理，研究人员得出了量化干扰影响的关键不等式。该方法允许对不同应用场景下干扰对算法性能的影响进行评估。

Result: 成功地推导出了在存在各种类型干扰的情况下算法的稳定性界限与收敛率，并展示了这种方法可以应用于多种情境，比如分布式学习中的通信限制、机器学习泛化中的敏感性问题及为了隐私保护而故意加入的噪声等。

Conclusion: 本研究提出了一种新的方法论来分析算法在面对噪声、干扰以及其他动态系统互动时的行为，为理解和改善复杂系统内算法的操作提供了强有力的工具。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [52] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的混合框架TRAPO，旨在通过交错监督微调(SFT)和强化学习(RL)，在每个训练实例中优化专家前缀上的SFT损失以及模型自身完成部分的RL损失，从而提高大型语言模型(LLMs)的复杂推理能力。实验表明，TRAPO在五个数学推理基准上优于标准方法和其他最新技术。


<details>
  <summary>Details</summary>
Motivation: 现有的两阶段管道（先进行监督微调再进行强化学习）存在关键不一致问题：监督微调限制了探索并导致遗忘，这削弱了后续强化学习改进效果。为了解决这一低效问题，研究者们提出了TRAPO框架。

Method: TRAPO是一种将监督微调与强化学习相结合的混合框架，在每个训练样本中同时优化监督微调损失（基于专家提供的前缀）和强化学习损失（基于模型生成的部分）。此外，为了稳定训练过程，还引入了信任区域监督微调(TrSFT)，它在信任区域内最小化正向KL散度，但在该区域外减弱优化力度，有效地转向反向KL散度，产生有利于强化学习的稳定更新。一个自适应前缀选择机制进一步根据测量到的有效性分配专家指导。

Result: 在五个数学推理基准测试中的实验结果表明，TRAPO的表现不仅优于传统的监督微调、强化学习以及监督微调后接强化学习的方法，并且也超越了一些最新的先进方法。

Conclusion: TRAPO提供了一种有效增强大型语言模型复杂推理能力的新范式，通过结合外部监督与自我探索的优势，为改善模型性能开辟了新路径。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [53] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文提出了一种深度机器学习架构——'多调和级联'，它能够近似任意复杂度的非线性函数，并保持全局平滑性和概率解释。与传统的梯度下降训练方法不同，该模型通过解决每个批次上的单个全局线性系统来更新所有层，从而同步更新所有层，同时保持各层的概率解释和理论一致性。实验表明，该方法在MNIST数据集上实现了快速学习且不过拟合。


<details>
  <summary>Details</summary>
Motivation: 为了能够近似任意复杂度的非线性函数，同时保持这些函数的全局平滑特性和提供一个概率解释，作者们提出了多调和级联这一新的深度学习架构。此外，他们还针对这种新型架构设计了一种不同于传统梯度下降法的学习方法，旨在提高学习效率的同时避免过拟合现象。

Method: 多调和级联是一种基于随机函数理论及无差别原则严格推导而来的深层结构，由一系列多调和样条包组成。对于训练过程，作者提出了一种替代梯度下降的方法：不是直接优化系数，而是针对固定节点'星座'处的函数值，在每批样本上求解一个全局线性系统。这种方法允许所有层级同步更新，保留了个体层的概率解释以及与原始模型的理论一致性。

Result: 研究结果表明，在使用提出的多调和级联架构及相应训练方法后，能够在MNIST手写数字识别任务中实现快速学习，并且没有出现过拟合的现象。

Conclusion: 多调和级联作为一种新颖的深度学习架构，不仅能够有效逼近任意复杂的非线性函数，而且通过其独特的训练机制，确保了良好的泛化能力和计算效率。

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [54] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: 本文提出了一种名为YOTO的端到端框架，能够同时识别离散基因子集并进行预测，通过稀疏性强制执行和多任务学习设计，使得所选基因直接参与推断，并且在相关目标之间学习共享表示，从而提高预测性能，促进生物标志物发现。


<details>
  <summary>Details</summary>
Motivation: 从单细胞转录组数据中选择紧凑且信息丰富的基因子集对于生物标志物发现、提高解释性和成本效益分析至关重要。然而，大多数现有的特征选择方法要么作为多阶段管道运行，要么依赖于事后特征归因，这导致选择与预测之间的联系较弱。

Method: YOTO（you only train once）是一种端到端框架，在单一可微架构内联合识别离散基因子集并执行预测任务。该模型通过预测任务直接指导哪些基因被选择，而所学得的子集反过来塑造预测表示。通过多任务学习设计，模型能够在相关目标间学习共享表示，允许部分标记的数据集相互提供信息，并且无需额外训练步骤即可跨任务发现通用基因子集。

Result: 通过对两个代表性的单细胞RNA-seq数据集进行评估，结果显示YOTO持续优于最先进基准方法。

Conclusion: 稀疏性、端到端及多任务基因子集选择方法不仅提高了预测性能，还产生了紧凑且有意义的基因子集，促进了生物标志物发现和单细胞分析领域的进步。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [55] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: 本文提出LaLoRA方法，通过在低秩适应（LoRA）权重上应用拉普拉斯近似来解决模型微调过程中灾难性遗忘的问题。该方法估计每个参数的信心，并限制高曲率方向上的更新，从而保持先前知识的同时实现高效的目标领域学习。实验表明，LaLoRA能够改善学习-遗忘之间的权衡，并且可以通过调整正则化强度直接控制这一过程。


<details>
  <summary>Details</summary>
Motivation: 当前的参数高效微调方法如低秩适应（LoRA），虽然可以快速地将大型预训练模型特化到不同的下游应用中，但往往会引发灾难性遗忘问题，即模型会忘记之前学到的重要领域知识。为了解决这个问题，提出了LaLoRA方法。

Method: LaLoRA是一种权重空间正则化技术，它对Low-Rank Adaptation (LoRA) 权重应用了拉普拉斯近似。这种方法估计了模型对于每个参数的信心水平，并限制了在高曲率方向上的更新，以保持模型之前的领域知识同时允许有效的目标域学习。通过对仅LoRA权重应用拉普拉斯近似，保证了方法的轻量级特性。

Result: 研究者们通过微调一个Llama模型来进行数学推理任务，展示了LaLoRA能够在学习与遗忘之间提供更好的平衡，并且这种平衡可以通过调整方法中的正则化强度直接控制。此外，还探讨了不同损失景观曲率逼近方式对参数信心估计的影响、用于拉普拉斯逼近的数据效果以及超参数稳健性的研究。

Conclusion: LaLoRA作为一种新颖的方法，有效地解决了在进行参数高效微调时遇到的灾难性遗忘问题。通过合理设置正则化强度，可以在保留原有知识的基础上实现新领域的高效学习。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [56] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出了一种可校准的解歧损失（CDL），可以同时提高多实例部分标签学习（MIPL）中的分类准确性和校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多实例部分标签学习方法经常存在校准不佳的问题，这会损害分类器的可靠性。

Method: 本研究提出了一种即插即用的可校准解歧损失（CDL），该损失有两种实现方式：一种基于候选标签集的概率来校准预测，另一种则结合了候选和非候选标签集的概率。

Result: 理论分析表明，与传统解歧损失相比，CDL具有更好的下界和正则化特性。实验结果证明，CDL在基准数据集和真实世界数据集中显著提高了分类和校准性能。

Conclusion: 提出的CDL能有效提升MIPL框架下的分类准确率和校准表现，并且能够无缝集成到现有的MIPL和PLL框架中。

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [57] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: 该论文研究了ID和基于文本的序列推荐（SR）模型之间的互补性，并提出了一种新的SR方法，通过独立训练模型来保持这种互补性，然后通过简单的集成策略加以利用。实验表明，尽管这种方法简单，但它超越了多个竞争性的SR基线，表明要达到最先进的SR性能需要同时使用ID和文本特征，但不需要复杂的融合架构。


<details>
  <summary>Details</summary>
Motivation: 目前的序列推荐（SR）模型倾向于用模态特征来表示项目，部分原因是语言和视觉建模的最新进展。一些工作完全用模态嵌入替换ID嵌入，声称模态嵌入使得ID嵌入变得不必要，因为它们可以匹配甚至超过ID嵌入的表现。然而，对于ID与模态特征之间互补性的理解存在不足。

Method: 作者研究了ID-和基于文本的SR模型之间的互补性，证明这两种模型确实学习到了互补信号。受此启发，他们提出了一种新的SR方法，通过独立模型训练保留ID-文本互补性，再通过一个简单的集成策略来利用它。

Result: 所提方法虽然简单，但在几个有竞争力的SR基线上表现更优，显示出当恰当地结合使用时，ID和文本特征都能提供性能增益。

Conclusion: 为了实现最佳的序列推荐性能，既需要ID也需要文本特征，但并不一定需要复杂的融合架构。

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>


### [58] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: 本文探讨了基于Wasserstein-Fisher-Rao (WFR)几何的重加权机制，通过引入显式的修正项和使用Feynman-Kac表示法实现这些机制，以改进得分基扩散模型在非对数凹目标分布情况下的采样效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于得分的扩散模型在连续生成建模中处于领先地位，但它们对于非凸或多模式景观的混合率会急剧下降。鉴于许多实际生成建模任务涉及高度非对数凹的目标分布，最近的努力集中在开发能够超越经典扩散动力学探索能力的采样方案上。

Method: 文章提出了一种利用信息几何工具增强基于扩散的采样器的方法，通过加入控制质量重加权机制，并且这种视角自然地导致了Wasserstein-Fisher-Rao (WFR)几何的应用。通过引入明确的校正项来制定这种重加权机制，并展示了如何通过加权随机微分方程（采用Feynman-Kac表示）来实现这些机制。

Result: 研究表明，WFR基础上的采样动力学提供了一种初步但严格的调查，阐明了其几何结构和算子理论结构，为未来的理论和算法发展奠定了基础。

Conclusion: 本研究为理解与应用WFR几何于改善复杂目标分布下的采样过程提供了新的见解和技术途径，指出了未来在理论分析和算法设计上的潜在方向。

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [59] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 本文提出了一种正则化随机傅里叶特征（RRFF）方法，结合有限元重构映射（RRFF-FEM），用于从噪声数据中学习算子。该方法使用从多变量学生t分布抽取的随机特征，并采用频率加权Tikhonov正则化来抑制高频噪声。通过基准偏微分方程问题的详细数值实验表明，与未正则化的随机特征模型相比，RRFF和RRFF-FEM对噪声具有鲁棒性，减少了训练时间的同时保持了相对内核和神经算子测试的竞争准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于核的方法虽然可以提供准确且理论上有据可依的逼近，但当训练集较大时可能会变得计算成本过高，并且对于噪声敏感。因此，需要一种新的方法来提高运算效率同时降低对噪声的敏感度。

Method: 提出了一种基于正则化随机傅里叶特征（RRFF）的新方法，其中随机特征是从多变量学生t分布中抽取的。此外，还引入了频率加权Tikhonov正则化技术以减轻高频率噪声的影响。此方法进一步结合了有限元重构映射（FEM），形成RRFF-FEM框架。

Result: 通过在多个基准偏微分方程问题上的数值实验证明，RRFF和RRFF-FEM不仅能够有效抵抗噪声干扰，而且相较于未经正则化的随机特征模型而言，在减少训练时间的同时还能维持较高的准确性。

Conclusion: 所提出的RRFF及RRFF-FEM方法展示了良好的噪声鲁棒性和高效的训练过程，为解决大规模或含噪数据下的算子学习问题提供了新途径。

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>
