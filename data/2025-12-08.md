<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.LG](#cs.LG) [Total: 38]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Stellis: A Strategy Language for Purifying Separation Logic Entailments](https://arxiv.org/abs/2512.05159)
*Zhiyi Wang,Xiwei Wu,Yi Fang,Chengtao Li,Hongyi Zhong,Lihan Xie,Qinxiang Cao,Zhenjiang Hu*

Main category: cs.SE

TL;DR: Stellis, a new strategy language, effectively simplifies separation logic entailments by removing spatial formulas, making them easier to prove. Evaluated on 229 real-world entailments, it successfully automated 95.6% of cases.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of rule-based methods in describing automation strategies for proving separation logic entailments, which are crucial for verification. The existing rule statements are not sufficient for automating the alignment and elimination of corresponding memory layouts in specific scenarios.

Method: A strategy language called Stellis is proposed for purifying separation logic entailments. It features a powerful matching mechanism and flexible action description to encode a wide range of strategies. An algorithm generates a soundness condition for each strategy, and a mechanized reduction soundness theorem helps generate correctness proofs for the automation.

Result: Stellis has been evaluated on a benchmark of 229 entailments from standard linked data structures and the memory module of a microkernel, showing that it can automatically purify 95.6% (219 out of 229) of the entailments with 5 libraries containing 98 strategies. The system is both flexible and effective.

Conclusion: The system, Stellis, is highly effective in automatically purifying 95.6% (219 out of 229) of the entailments using 5 libraries with 98 strategies, as demonstrated by the evaluation on a benchmark of 229 entailments.

Abstract: Automatically proving separation logic entailments is a fundamental challenge in verification. While rule-based methods rely on separation logic rules (lemmas) for automation, these rule statements are insufficient for describing automation strategies, which usually involve the alignment and elimination of corresponding memory layouts in specific scenarios. To overcome this limitation, we propose Stellis, a strategy language for purifying separation logic entailments, i.e., removing all spatial formulas to reduce the entailment to a simpler pure entailment. Stellis features a powerful matching mechanism and a flexible action description, enabling the straightforward encoding of a wide range of strategies. To ensure strategy soundness, we introduce an algorithm that generates a soundness condition for each strategy, thereby reducing the soundness of each strategy to the correctness of its soundness condition. Furthermore, based on a mechanized reduction soundness theorem, our prototype implementation generates correctness proofs for the overall automation. We evaluate our system on a benchmark of 229 entailments collected from verification of standard linked data structures and the memory module of a microkernel, and the evaluation results demonstrate that, with such flexibility and convenience provided, our system is also highly effective, which automatically purifies 95.6% (219 out of 229) of the entailments using 5 libraries with 98 strategies.

</details>


### [2] [Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge](https://arxiv.org/abs/2512.05176)
*Brittany Johnson,Erin Reddick,Angela D. R. Smith*

Main category: cs.SE

TL;DR: 本文提出了一种复制研究，将用于开发韩国国家语言模型对齐基准KorNAT的过程转化为开发CIVIQ（文化智能和价值观推断质量基准），以更好地与社区社会价值观和常识保持一致，为AI技术的文化对齐研究和发展提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）作为强大的技术被广泛采用，但往往与西方白人叙事更为一致，而与其他文化和人群的协作创新不匹配。尽管有了一些致力于开发‘文化知情’LLM的努力，但在支持这些模型的发展和评估方面做得还不够。先前提出的国家级别的一致性基准对于更广泛的代表性来说是不够有效的。

Method: 作者提议进行一项复制研究，该研究借鉴了开发KorNAT（韩国国家LLM一致性基准）的方法论来创建CIVIQ，一个专注于社区社会价值观和共同知识一致性的文化智能与价值推断质量基准。

Result: 通过这项工作，研究人员希望为在美国背景下促进AI技术文化一致性方面的研究与发展奠定基础。

Conclusion: 为了改善AI技术特别是LLM在不同文化背景下的适应性和公平性，需要更多地关注开发及评价能反映多样化文化身份的LLM。CIVIQ提供了一个潜在框架，旨在确保AI技术更好地服务于具有不同文化经验的人群。

Abstract: Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as "general purpose" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of "culturally-informed" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.

</details>


### [3] [A Survey of Bugs in AI-Generated Code](https://arxiv.org/abs/2512.05239)
*Ruofan Gao,Amjed Tahir,Peng Liang,Teo Susnjak,Foutse Khomh*

Main category: cs.SE

TL;DR: 本文系统地分析了现有AI生成代码中的错误和缺陷类型及其分布，为未来模型改进和质量评估提供了参考。


<details>
  <summary>Details</summary>
Motivation: 开发者广泛使用AI代码生成模型以提高生产力和效率，但这些由公开代码训练出的模型可能会产生含有bug和质量问题的代码，导致开发过程中的信任和维护挑战。目前缺乏对这些问题的系统性总结。

Method: 通过系统地分析现有的关于AI生成代码的文献，理解AI生成代码中bug的本质和范围，并对不同模型产生的代码中存在的bug类型和模式进行分类。

Result: 建立了对AI生成代码中错误和缺陷的整体理解，包括bug的类型、模式以及可能的修复和缓解策略。

Conclusion: 本研究为理解和解决AI生成代码中的质量问题提供了重要见解，有助于指导未来的研究方向及模型优化工作。

Abstract: Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.

</details>


### [4] [Learning to Code with Context: A Study-Based Approach](https://arxiv.org/abs/2512.05242)
*Uwe M. Borghoff,Mark Minas,Jannis Schopp*

Main category: cs.SE

TL;DR: 本文探讨了在软件工程项目中整合生成式AI工具的教学方法，通过一个大学编程项目中的用户研究，分析了学生如何使用这些工具、它们最有效的任务类型以及遇到的挑战。同时，研究还评估了一个基于检索增强生成（RAG）技术的本地部署大型语言模型助手，该助手能够提供与项目相关的支持，并对模型行为和常见失败模式进行了定性分析。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的迅速出现，软件开发方式正在发生变化。为了保证学生不仅学习传统的开发方法，还能有意义地和负责任地使用新技术，软件工程教育必须作出相应调整。特别是基于项目的课程为探索将AI辅助融入实际开发实践中提供了有效环境。

Method: 本研究在一个大学编程项目中进行，学生们合作开发计算机游戏。研究调查了参与者在整个软件开发过程的不同阶段如何使用生成式AI工具，确定了这些工具最有效的任务类型，并分析了学生遇到的挑战。此外，研究还考察了一个具有仓库意识的、本地部署的大规模语言模型助理，它被设计用来提供与项目背景相关联的支持。该系统采用检索增强生成(RAG)技术，以相关文档和源代码为基础生成响应，从而允许对模型行为、参数敏感性和常见故障模式进行定性分析。

Result: 研究表明，在特定类型的软件开发任务中，生成式AI工具可以非常有效；然而，学生也遇到了一些挑战，比如理解AI提供的建议或处理AI生成代码的质量问题。对于仓库感知型LLM助理的研究揭示了其在提供上下文相关帮助方面的潜力，同时也指出了需要改进的地方，如提高模型对特定参数变化的鲁棒性等。

Conclusion: 这项工作加深了我们对于教育软件项目中情境感知AI支持的理解，并为未来将基于AI的帮助集成到软件工程课程中提供了信息。

Abstract: The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.

</details>


### [5] [Engagement in Code Review: Emotional, Behavioral, and Cognitive Dimensions in Peer vs. LLM Interactions](https://arxiv.org/abs/2512.05309)
*Adam Alami,Nathan Cassee,Thiago Rocha Silva,Elda Paja,Neil A. Ernst*

Main category: cs.SE

TL;DR: 本研究通过两阶段的定性研究，探讨了软件工程师在大型语言模型辅助代码审查与人类同行主导的代码审查中的参与方式。研究发现，在大型语言模型辅助下进行代码审查时，工程师的情绪调节需求较低，且更倾向于采纳反馈。此外，研究提出了一种将情绪自我调节与行为参与及解决过程联系起来的综合模型，并建议AI作为支持伙伴来减轻认知和情感负担，同时保持人类责任和社会意义。


<details>
  <summary>Details</summary>
Motivation: 理解软件工程师如何参与大型语言模型（LLM）辅助的代码审查对比人类同行领导的审查是一个较少被探索的话题。这项研究旨在填补这一空白，通过分析工程师的情感反应、参与决策以及解决问题的方式，为提高代码审查效率提供见解。

Method: 采用两阶段定性研究方法。第一阶段包括同行评审交流和个人访谈；第二阶段则引入符合工程师偏好的新提示词，进一步探究其特征对反应的影响。基于收集到的数据开发了一个整合账户，将情感自我调节与行为参与及解决方案联系起来。

Result: 识别出几种自我调节策略：重构、对话调节、回避和防御性。参与过程通过社会校准进行；工程师根据关系气候和团队规范调整自己的反应和行为。对于同行主导的审查，解决路径因焦点（个人/双人/团队）和内部意义构建过程而异。而在LLM辅助审查中，情绪成本和自我调节的需求似乎更低。当LLM反馈与工程师的认知预期一致时，参与者报告说处理努力减少，可能有更高的采纳倾向。

Conclusion: 研究表明，LLM辅助的代码审查可以将参与从情绪管理转向认知负荷管理。一个整合模型被提出来说明情感自我调节如何影响行为参与和问题解决过程，并指出在同行主导和LLM辅助代码审查中，情感和认知过程如何影响反馈采纳。最终结论是AI最适合扮演支持角色，以减轻认知和情感负担，同时保留人类责任感及同行审查等社会技术活动的社会意义。

Abstract: Code review is a socio-technical practice, yet how software engineers engage in Large Language Model (LLM)-assisted code reviews compared to human peer-led reviews is less understood. We report a two-phase qualitative study with 20 software engineers to understand this. In Phase I, participants exchanged peer reviews and were interviewed about their affective responses and engagement decisions. In Phase II, we introduced a new prompt matching engineers' preferences and probed how characteristics shaped their reactions. We develop an integrative account linking emotional self-regulation to behavioral engagement and resolution. We identify self-regulation strategies that engineers use to regulate their emotions in response to negative feedback: reframing, dialogic regulation, avoidance, and defensiveness. Engagement proceeds through social calibration; engineers align their responses and behaviors to the relational climate and team norms. Trajectories to resolution, in the case of peer-led review, vary by locus (solo/dyad/team) and an internal sense-making process. With the LLM-assisted review, emotional costs and the need for self-regulation seem lower. When LLM feedback aligned with engineers' cognitive expectations, participants reported reduced processing effort and a potentially higher tendency to adopt. We show that LLM-assisted review redirects engagement from emotion management to cognitive load management. We contribute an integrative model of engagement that links emotional self-regulation to behavioral engagement and resolution, showing how affective and cognitive processes influence feedback adoption in peer-led and LLM-assisted code reviews. We conclude that AI is best positioned as a supportive partner to reduce cognitive and emotional load while preserving human accountability and the social meaning of peer review and similar socio-technical activities.

</details>


### [6] [WhatsCode: Large-Scale GenAI Deployment for Developer Efficiency at WhatsApp](https://arxiv.org/abs/2512.05314)
*Ke Mao,Timotej Kapus,Cons T Åhs,Matteo Marescotti,Daniel Ip,Ákos Hajdu,Sopot Cela,Aparup Banerjee*

Main category: cs.SE

TL;DR: 本文报告了WhatsCode在WhatsApp中的工业部署，该AI开发系统支持多平台代码处理，并在25个月内显著提高了隐私验证覆盖率，生成了大量接受的代码变更。研究还确定了两种稳定的人机协作模式，并强调组织因素对于企业规模AI成功的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管学术文献中关于合规相关的大型工业环境中部署AI辅助开发工具的研究较少，但行业采用却在增长。为了填补这一空白，文章探讨了特定领域AI开发系统WhatsCode在WhatsApp中的实际应用及其对企业的影响。

Method: 通过25个月的时间内对WhatsCode进行观察和分析，包括其从隐私自动化到自主工作流集成的演变过程。同时，研究团队也评估了该系统在自动隐私验证、代码变更接受率以及人机协作模式方面的表现。

Result: WhatsCode显著提升了自动隐私验证覆盖率至3.5倍（从15%提高到53%），并产生了超过3,000次被接受的代码更改。此外，还识别出了两种主要的人机协作模式：一键部署适用于高置信度变更（占60%）；指挥-修订则用于复杂决策（占40%）。

Conclusion: 研究表明，除了技术能力外，所有权模型、采用动态及风险管理等组织因素也是决定企业级AI项目成功与否的关键要素。有效的人员与AI合作而非完全自动化，是实现持续商业影响的关键所在。

Abstract: The deployment of AI-assisted development tools in compliance-relevant, large-scale industrial environments represents significant gaps in academic literature, despite growing industry adoption. We report on the industrial deployment of WhatsCode, a domain-specific AI development system that supports WhatsApp (serving over 2 billion users) and processes millions of lines of code across multiple platforms. Over 25 months (2023-2025), WhatsCode evolved from targeted privacy automation to autonomous agentic workflows integrated with end-to-end feature development and DevOps processes.
  WhatsCode achieved substantial quantifiable impact, improving automated privacy verification coverage 3.5x from 15% to 53%, identifying privacy requirements, and generating over 3,000 accepted code changes with acceptance rates ranging from 9% to 100% across different automation domains. The system committed 692 automated refactor/fix changes, 711 framework adoptions, 141 feature development assists and maintained 86% precision in bug triage. Our study identifies two stable human-AI collaboration patterns that emerged from production deployment: one-click rollout for high-confidence changes (60% of cases) and commandeer-revise for complex decisions (40%). We demonstrate that organizational factors, such as ownership models, adoption dynamics, and risk management, are as decisive as technical capabilities for enterprise-scale AI success. The findings provide evidence-based guidance for large-scale AI tool deployment in compliance-relevant environments, showing that effective human-AI collaboration, not full automation, drives sustainable business impact.

</details>


### [7] [Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering](https://arxiv.org/abs/2512.05350)
*Munazza Zaib,Wei Wang,Dulaji Hidellaarachchi,Isma Farah Siddiqui*

Main category: cs.SE

TL;DR: 该论文提出了一种结合包容性框架与GenderMag流程的方法，专门针对软件工程领域中神经多样性女性所面临的独特挑战。通过文献综述、人物画像及分析过程的推导，以及合作工作坊的应用三个阶段来展开研究，旨在为支持实际变革提供包容性的分析方法。


<details>
  <summary>Details</summary>
Motivation: 神经多样性女性在软件工程领域面临由性别偏见和神经差异交织而成的独特挑战。尽管工作场所越来越认识到神经多样性的重要性，但在软件工程研究中尚无系统性地考察这一群体的研究。未诊断或晚诊断、掩饰行为以及以男性为中心的工作文化加剧了这些障碍，导致压力增加、倦怠感上升以及人才流失。

Method: 本文提出了一种混合方法论方法，将InclusiveMag的包容性框架与GenderMag走查过程相结合，并根据软件工程领域内神经多样性女性的具体情况进行了调整。整个设计分为三个阶段：通过文献回顾确定范围；推导出人物画像和分析流程；并在协作工作坊中应用该方法。

Result: 初步呈现了一个有针对性的文献综述结果，该综述将神经多样性女性在软件工程领域面临的挑战归纳为认知、社会、组织结构以及职业发展等方面的挑战，并探讨了未/晚诊断和掩饰行为如何加剧了排斥现象。

Conclusion: 本研究为后续开发和应用包容性分析方法奠定了基础，旨在支持能够带来实际行动变化的方法论。

Abstract: Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.

</details>


### [8] [Metronome: Differentiated Delay Scheduling for Serverless Functions](https://arxiv.org/abs/2512.05703)
*Zhuangbin Chen,Juzheng Zheng,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文探讨了在无服务器环境中现有延迟调度方法的效果，并基于三个关键观察提出了Metronome框架，该框架能够通过预测机制显著减少函数平均执行时间，同时保证SLA合规性。


<details>
  <summary>Details</summary>
Motivation: 由于无服务器函数的动态和事件驱动特性，优化其调度是一个挑战。尽管数据本地化在传统集群计算系统中已被证明有效，但在其在无服务器平台上的应用仍待探索。

Method: 作者系统地评估了无服务器环境下的现有延迟调度方法，并提出了一种名为Metronome的新框架，它使用在线随机森林回归模型来预测不同节点上函数的执行时间，从而做出明智的延迟决策。

Result: 实验结果显示，Metronome相比基线方法能将函数的平均执行时间降低64.88%-95.83%，同时在并发水平提高的情况下保持性能优势并确保符合SLA要求。

Conclusion: Metronome框架通过引入预测机制有效地解决了无服务器计算中的复杂局部性问题，为单个函数识别最优的局部性感知节点，从而大大减少了平均执行时间且不违反SLA。

Abstract: Function-as-a-Service (FaaS) computing is an emerging cloud computing paradigm for its ease-of-management and elasticity. However, optimizing scheduling for serverless functions remains challenging due to their dynamic and event-driven nature. While data locality has been proven effective in traditional cluster computing systems through delay scheduling, its application in serverless platforms remains largely unexplored. In this paper, we systematically evaluate existing delay scheduling methods in serverless environments and identify three key observations: 1) delay scheduling benefits vary significantly based on function input characteristics; 2) serverless computing exhibits more complex locality patterns than cluster computing systems, encompassing both data locality and infrastructure locality; and 3) heterogeneous function execution times make rule-based delay thresholds ineffective. Based on these insights, we propose Metronome, a differentiated delay scheduling framework that employs predictive mechanisms to identify optimal locality-aware nodes for individual functions. Metronome leverages an online Random Forest Regression model to forecast function execution times across various nodes, enabling informed delay decisions while preventing SLA violations. Our implementation on OpenLambda shows that Metronome significantly outperforms baselines, achieving 64.88%-95.83% reduction in mean execution time for functions, while maintaining performance advantages under increased concurrency levels and ensuring SLA compliance.

</details>


### [9] [Legacy Modernization with AI -- Mainframe modernization](https://arxiv.org/abs/2512.05375)
*Sunil Khemka,Arunava Majumdar*

Main category: cs.SE

TL;DR: 本文探讨了人工智能在辅助遗留系统现代化中的作用，特别是对于大型机系统的现代化。通过采用AI驱动的策略如自动代码重构、智能工具的数据迁移和预测性维护等，企业可以更顺利地迁移到微服务、容器化环境以及混合云平台，从而促进业务创新、减少停机时间并提高系统韧性。


<details>
  <summary>Details</summary>
Motivation: 随着技术的发展，尽管传统的大型机系统仍然可靠，但它们面临高维护成本、技能短缺及与基于云的系统集成困难等问题。为了解决这些问题，并实现向更加灵活、可扩展且智能化架构的转变，有必要引入AI辅助的现代化方法。

Method: 文章介绍了几种AI驱动的现代化策略，包括利用机器学习模型审查遗留代码库、识别效率提升机会、执行自动化测试和部署；同时，AI还能够生成洞察以均衡工作负载和检测异常情况。

Result: 通过这些AI支持的方法，公司不仅能够保留核心业务逻辑，还能加速创新过程、减少系统停机时间，并增强整体系统韧性。

Conclusion: 将AI应用于大型机系统的现代化是推动数字转型和可持续企业增长的关键因素。

Abstract: Artificial Intelligence-assisted legacy modernization is essential in changing the stalwart mainframe systems of the past into flexible, scalable, and smart architecture. While mainframes are generally dependable, they can be difficult to maintain due to their high maintenance costs, the shortage of skills, and the problems in integrating them with cloud-based systems. By adopting AI-driven modernization strategies such as automated code refactoring, migration of data using smart tools, and predictive maintenance, companies can easily move to microservices, containerized environments, and hybrid cloud platforms. Machine learning models have the capability to go through legacy codebases, figure out efficiency opportunities, and carry out automated testing and deployment. Besides that, AI improves the organization's operational efficiency by generating the insights that can be used to level the workload and detect the anomalies. The coupling of the two is not only about saving the core business logic but also about enabling quicker innovation, less downtime, and enhanced system resilience. Therefore, the use of AI in mainframe modernization is a catalyst for digital transformation and enterprise growth that is sustainable over time.

</details>


### [10] [Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation](https://arxiv.org/abs/2512.05383)
*Mara Downing,Matthew Peng,Jacob Granley,Michael Beyeler,Tevfik Bultan*

Main category: cs.SE

TL;DR: 本文提出了一种系统化的定量方法，通过适应于神经刺激领域的模糊测试技术来检测和表征机器学习驱动的神经刺激系统中的不安全刺激模式。该方法能够揭示超出既定安全限制的各种刺激模式，并为下一代神经接口的安全性、监管准备以及伦理保证提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型越来越多地用于生成神经假体设备（如视觉假体）中的电刺激模式，尽管这些模型承诺了精确和个人化的控制，但当模型输出直接传递到神经组织时也引入了新的安全风险。因此，需要一种系统化的方法来检测和表征这些由ML驱动的神经刺激系统中潜在的不安全刺激模式。

Method: 研究者将一种称为覆盖引导模糊测试的自动化软件测试技术应用于神经刺激领域。这种方法通过对模型输入进行扰动并跟踪结果刺激是否违反了关于电荷密度、瞬时电流或电极共激活的生物物理限制来进行压力测试。框架将编码器视为黑盒，并使用覆盖度量指标指导探索，以量化测试用例跨越可能输出和违规类型的空间范围的程度。

Result: 当应用于视网膜和皮层的深度刺激编码器时，该方法系统地揭示了多种超过既定安全限制的刺激方案。两种违规-输出覆盖度量标准识别出数量最多且多样性最大的不安全输出，使得跨架构和训练策略之间的可解释比较成为可能。

Conclusion: 聚焦于违规行为的模糊测试将安全性评估重新定义为一个实证且可重复的过程。通过将安全性从训练启发式转变为部署模型的可测量属性，它为下一代神经接口的安全性基准测试、监管准备以及伦理保证奠定了基础。

Abstract: Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.

</details>


### [11] [Bita: A Conversational Assistant for Fairness Testing](https://arxiv.org/abs/2512.05428)
*Keeryn Johnson,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文介绍了一个名为Bita的对话助手，它旨在帮助软件测试人员检测AI系统中的潜在偏见源，并通过公平性的视角评估测试计划，从而生成以公平性为导向的探索性测试章程。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统中存在偏见可能导致不公平和歧视的结果，而当前的公平性测试工具难以使用且对实际工作流程支持有限。为了解决这个问题，研究者开发了Bita这一易于使用的对话助手来辅助进行公平性测试。

Method: Bita结合了大型语言模型与检索增强生成技术，能够基于精心整理的公平性文献资料库提供回答。该工具设计用于帮助用户识别潜在的偏见来源、评估考虑了公平性的测试计划，并制定相关的探索性测试策略。

Result: 通过在真实世界的AI系统上验证，Bita被证明能够有效地支持公平性测试任务，并提供了结构化且可重复利用的有效性证据。

Conclusion: 本研究为工业实践中实施公平性测试贡献了一种实用、易获取且系统化的工具。

Abstract: Bias in AI systems can lead to unfair and discriminatory outcomes, especially when left untested before deployment. Although fairness testing aims to identify and mitigate such bias, existing tools are often difficult to use, requiring advanced expertise and offering limited support for real-world workflows. To address this, we introduce Bita, a conversational assistant designed to help software testers detect potential sources of bias, evaluate test plans through a fairness lens, and generate fairness-oriented exploratory testing charters. Bita integrates a large language model with retrieval-augmented generation, grounding its responses in curated fairness literature. Our validation demonstrates how Bita supports fairness testing tasks on real-world AI systems, providing structured, reproducible evidence of its utility. In summary, our work contributes a practical tool that operationalizes fairness testing in a way that is accessible, systematic, and directly applicable to industrial practice.

</details>


### [12] [Everything is Context: Agentic File System Abstraction for Context Engineering](https://arxiv.org/abs/2512.05470)
*Xiwei Xu,Robert Mao,Quan Bai,Xuewu Gu,Yechao Li,Liming Zhu*

Main category: cs.SE

TL;DR: The paper introduces a file-system abstraction for context engineering in generative AI, aiming to improve the management of external knowledge and human input. This approach, implemented in the AIGNE framework, supports verifiable, maintainable, and industry-ready GenAI systems, with applications such as an agent with memory and a GitHub assistant.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of context engineering in generative AI, moving beyond model fine-tuning to focus on how systems manage external knowledge, memory, tools, and human inputs to ensure reliable reasoning. The paper aims to unify fragmented practices like prompt engineering and tool integration into a more structured and governed approach.

Method: The method involves proposing a file-system abstraction inspired by the Unix philosophy 'everything is a file', which provides a persistent infrastructure for managing diverse context elements. This is achieved through uniform mounting, metadata, and access control. The architecture is realized within the AIGNE framework, featuring a Context Constructor, Loader, and Evaluator to assemble, deliver, and validate context under token constraints.

Result: The result is a verifiable context-engineering pipeline that facilitates accountable and human-centered collaboration with AI. The effectiveness of the proposed architecture is demonstrated through two examples: an agent with memory and a multi-context processing (MCP) based GitHub assistant. The implementation showcases its applicability in both developer and industrial settings, promoting the development of trustworthy and maintainable GenAI systems.

Conclusion: In conclusion, the paper presents a novel file-system abstraction for context engineering in generative AI, offering a unified and governed approach to managing external knowledge and human input. This solution, operationalized via the AIGNE framework, sets a foundation for developing verifiable, maintainable, and industry-ready GenAI systems, emphasizing the role of humans as curators, verifiers, and co-reasoners.

Abstract: Generative AI (GenAI) has reshaped software system design by introducing foundation models as pre-trained subsystems that redefine architectures and operations. The emerging challenge is no longer model fine-tuning but context engineering-how systems capture, structure, and govern external knowledge, memory, tools, and human input to enable trustworthy reasoning. Existing practices such as prompt engineering, retrieval-augmented generation (RAG), and tool integration remain fragmented, producing transient artefacts that limit traceability and accountability. This paper proposes a file-system abstraction for context engineering, inspired by the Unix notion that 'everything is a file'. The abstraction offers a persistent, governed infrastructure for managing heterogeneous context artefacts through uniform mounting, metadata, and access control. Implemented within the open-source AIGNE framework, the architecture realises a verifiable context-engineering pipeline, comprising the Context Constructor, Loader, and Evaluator, that assembles, delivers, and validates context under token constraints. As GenAI becomes an active collaborator in decision support, humans play a central role as curators, verifiers, and co-reasoners. The proposed architecture establishes a reusable foundation for accountable and human-centred AI co-work, demonstrated through two exemplars: an agent with memory and an MCP-based GitHub assistant. The implementation within the AIGNE framework demonstrates how the architecture can be operationalised in developer and industrial settings, supporting verifiable, maintainable, and industry-ready GenAI systems.

</details>


### [13] [A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models](https://arxiv.org/abs/2512.05498)
*Xiao He,Ru Chen,Zeqing Zhang,Yanling Wang,Qiuyan Dong*

Main category: cs.SE

TL;DR: 本文提出了一种名为iEcoreGen的混合方法，该方法结合了Eclipse建模框架（EMF）和大型语言模型（LLMs），以提高代码生成任务中的效率与准确性。通过将需求分解为操作规范，并利用EMF基于模板的生成器产生初始Java代码，随后使用LLMs完成并修复未实现的方法，iEcoreGen在保持编译成功率的同时提高了通过率。


<details>
  <summary>Details</summary>
Motivation: 模板基础的代码生成提供了正确性保证但对复杂需求不够灵活，而大型语言模型虽然非常灵活却可能生成有缺陷的代码。因此，存在一种需求是开发一种能够结合两者优势的新方法。

Method: iEcoreGen首先利用Eclipse Modeling Framework (EMF) 中定义的Ecore模型作为系统结构蓝图，接着将需求拆解成具体的操作规范，再用EMF自带的基于模板的代码生成工具生成初步的Java代码及文档字符串说明；最后调用大型语言模型来补充和完善那些尚未实现的方法部分。

Result: iEcoreGen在二十个代码生成任务中进行了评估，涉及五种不同的大型语言模型。结果表明，在pass@k指标上它优于仅使用大型语言模型的基础方案，而在compilation@k指标上的表现则与之相当。

Conclusion: 研究表明，通过整合大型语言模型增强的模型驱动开发是一种很有前途的方法，可以更高效地实现软件自动化。此外，消融研究进一步明确了iEcoreGen各组成部分的具体贡献。

Abstract: Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.

</details>


### [14] [Generative AI in Simulation-Based Test Environments for Large-Scale Cyber-Physical Systems: An Industrial Study](https://arxiv.org/abs/2512.05507)
*Masoud Sadrnezhaad,José Antonio Hernández López,Torvald Mårtensson,Daniel Varro*

Main category: cs.SE

TL;DR: 本文探讨了生成式AI在大规模信息物理系统仿真测试中的应用潜力和挑战，通过与六个组织的跨公司研讨会收集实践者的观点。研究提出了三个优先的研究方向：AI生成的场景和环境模型、CI/CD管道中的模拟器和AI、以及生成式AI用于仿真的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着大规模信息物理系统的增长，开发和维护硬件、软件组件及物理环境仿真模型所需的资源越来越多。虽然生成式AI工具可以为软件系统产生可执行的测试用例，并可能减少手动工作量或增加测试覆盖率，但将这些技术应用于此类系统的仿真测试仍较少被探索。

Method: 基于与六个组织合作开展的跨公司研讨会，收集工程师们对于利用生成式AI技术的看法和经验。

Result: 得出了工程师面临的挑战细节，并提出了包含三个高优先级方向的研究议程：(a) 由AI生成的场景和环境模型，(b) 在持续集成/持续部署（CI/CD）管道中使用模拟器和AI，(c) 提升生成式AI在仿真应用中的可信度。

Conclusion: 尽管参与者认识到生成式AI在该领域内有巨大潜力，但也指出了许多未解决的问题。本研究旨在通过详细说明这些问题来指导未来学术界与产业界的合作，以负责任的方式采用生成式AI进行基于仿真的测试。

Abstract: Quality assurance for large-scale cyber-physical systems relies on sophisticated test activities using complex test environments investigated with the help of numerous types of simulators. As these systems grow, extensive resources are required to develop and maintain simulation models of hardware and software components, as well as physical environments. Meanwhile, recent advances in generative AI have led to tools that can produce executable test cases for software systems, offering potential benefits such as reducing manual efforts or increasing test coverage. However, the application of generative AI techniques to simulation-based testing of large-scale cyber-physical systems remains underexplored. To better understand this gap, this study captures practitioners' perspectives on leveraging generative AI, based on a cross-company workshop with six organizations. Our contribution is twofold: (1) detailed, experience-based insights into challenges faced by engineers, and (2) a research agenda comprising three high-priority directions: (a) AI-generated scenarios and environment models, (b) simulators and AI in CI/CD pipelines, and (c) trustworthiness in generative AI for simulation. While participants acknowledged substantial potential, they also highlighted unresolved challenges. By detailing these issues, the paper aims to guide future academia-industry collaboration towards the responsible adoption of generative AI in simulation-based testing.

</details>


### [15] [From Challenge to Change: Design Principles for AI Transformations](https://arxiv.org/abs/2512.05533)
*Theocharis Tavantzis,Stefano Lambiase,Daniel Russo,Robert Feldt*

Main category: cs.SE

TL;DR: 本文提出了一种基于行为软件工程（BSE）的人本框架，以支持软件工程组织在早期采用人工智能时的适应。通过文献综述和访谈数据分析构建了该框架，并确定了九个维度：AI策略设计、AI策略评估、协作、沟通、治理与伦理、领导力、组织文化、组织动态以及技能提升。初步调查结果显示，技能提升和AI策略设计被认为是在早期AI项目中最优先考虑的两个方面。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究注意到了AI整合中的行为和其他非技术因素，但大多数研究仍然侧重于技术问题，对团队如何适应和信任AI提供了有限的见解。因此，有必要开发一个更关注人类因素的框架来帮助软件工程组织更好地适应AI的早期采用。

Method: 采用混合方法论，通过组织变革模型的文献回顾和访谈数据的主题分析建立和完善了框架。此外，还进行了初步的从业者调查（N=105）及两次专家研讨会（N=4），以收集关于框架实用性的反馈。

Result: 形成了包含九大维度的行为软件工程信息框架，其中技能提升和AI策略设计被调查参与者认为是最重要的。结果表明，当前组织倾向于优先考虑程序性元素如策略设计，而以人为本的指导原则则相对不够成熟。

Conclusion: 本研究为早期AI采用过程中的社会技术复杂性提供了一个实用路线图，并强调了未来在软件工程中以人为中心的AI研究方向。

Abstract: The rapid rise of Artificial Intelligence (AI) is reshaping Software Engineering (SE), creating new opportunities while introducing human-centered challenges. Although prior work notes behavioral and other non-technical factors in AI integration, most studies still emphasize technical concerns and offer limited insight into how teams adapt to and trust AI. This paper proposes a Behavioral Software Engineering (BSE)-informed, human-centric framework to support SE organizations during early AI adoption. Using a mixed-methods approach, we built and refined the framework through a literature review of organizational change models and thematic analysis of interview data, producing concrete, actionable steps. The framework comprises nine dimensions: AI Strategy Design, AI Strategy Evaluation, Collaboration, Communication, Governance and Ethics, Leadership, Organizational Culture, Organizational Dynamics, and Up-skilling, each supported by design principles and actions. To gather preliminary practitioner input, we conducted a survey (N=105) and two expert workshops (N=4). Survey results show that Up-skilling (15.2%) and AI Strategy Design (15.1%) received the highest $100-method allocations, underscoring their perceived importance in early AI initiatives. Findings indicate that organizations currently prioritize procedural elements such as strategy design, while human-centered guardrails remain less developed. Workshop feedback reinforced these patterns and emphasized the need to ground the framework in real-world practice. By identifying key behavioral dimensions and offering actionable guidance, this work provides a pragmatic roadmap for navigating the socio-technical complexity of early AI adoption and highlights future research directions for human-centric AI in SE.

</details>


### [16] [Automated Code Review Assignments: An Alternative Perspective of Code Ownership on GitHub](https://arxiv.org/abs/2512.05551)
*Jai Lal Lulla,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: 本研究对超过844,000次拉取请求进行了大规模实证分析，发现CODEOWNERS功能有助于提高代码审查效率和工作流程平滑度，并通过重新分配审查责任来改变仓库的审查动态。


<details>
  <summary>Details</summary>
Motivation: 随着外部威胁如软件供应链攻击增加，确保项目健康与质量的责任机制变得至关重要。GitHub在2017年推出了CODEOWNERS功能以加强问责制并保护代码库的关键部分，但其实际采用和实践情况尚不清楚。

Method: 本研究基于超过844,000个拉取请求、190万条评论以及超过200万次审查的大规模数据集，识别了10,287位代码所有者追踪他们的审查活动。使用回归不连续设计（RDD）分析方法来探究采用CODEOWNERS前后仓库审查动态的变化。

Result: 结果表明，代码所有者倾向于遵守CODEOWNERS文件中规定的规则，表现出与传统所有权指标相似的合作行为，同时随着时间推移促进了更顺畅更快捷的PR工作流程。此外，采纳CODEOWNERS的仓库经历了审查动态变化，审查责任从核心开发者转移到了指定的所有者身上。

Conclusion: CODEOWNERS被定位为一种有潜力但尚未充分利用的方法，用于改善软件治理和韧性。它为开源开发中的安全性、问责性和工作流效率提供了新的视角。

Abstract: Code ownership is central to ensuring accountability and maintaining quality in large-scale software development. Yet, as external threats such as software supply chain attacks on project health and quality assurance increase, mechanisms for assigning and enforcing responsibility have become increasingly critical. In 2017, GitHub introduced the CODEOWNERS feature, which automatically designates reviewers for specific files to strengthen accountability and protect critical parts of the codebase. Despite its potential, little is known about how CODEOWNERS is actually adopted and practiced. We present the first large-scale empirical study of CODEOWNERS usage across over 844,000 pull requests with 1.9 million comments and over 2 million reviews. We identify 10,287 code owners to track their review activities. Results indicate that codeowners tend to adhere the rules specified in the CODEOWNERS file, exhibit similar collaborative behaviours to traditional metrics of ownership, but tend to contribute to a smoother and faster PR workflow over time. Finally, using regression discontinuity design (RDD) analysis, we find that repositories adopting CODEOWNERS experience shifts in review dynamics, as ownership redistributes review responsibilities away from core developers. Our results position CODEOWNERS as a promising yet underutilized mechanism for improving software governance and resilience. We discuss how projects can leverage this alternative ownership method as a perspective to enhance security, accountability, and workflow efficiency in open-source development.

</details>


### [17] [Executing Discrete/Continuous Declarative Process Specifications via Complex Event Processing](https://arxiv.org/abs/2512.05653)
*Stefan Schönig,Leo Poss,Fabrizio Maria Maggi*

Main category: cs.SE

TL;DR: 本文提出了一种基于复杂事件处理(CEP)的执行架构，能够实现实时执行和强制执行混合声明性模型。通过将STL启发式的谓词集成到执行流中，该方法可以在连续传感器行为的基础上主动触发活动并强制执行过程边界。


<details>
  <summary>Details</summary>
Motivation: 传统业务流程管理（BPM）专注于离散事件，未能在信息物理环境中整合关键的连续传感器数据。尽管混合声明式规范利用信号时态逻辑(STL)解决了这一限制，但现有工作仅限于监控和事后一致性检查。

Method: 文章介绍了一种新的基于复杂事件处理(CEP)的执行架构，支持实时执行与实施混合声明性模型。采用三层方法，将受STL启发的谓词融入执行流程中，使得系统能够根据连续传感器的行为来激活活动并施加过程界限。

Result: 所提出的架构成功地填补了从混合规范到操作控制之间的空白，实现了对包含连续值信号的过程的有效实时管理和控制。

Conclusion: 通过引入这种基于CEP的新型执行架构，研究展示了如何有效地将连续传感器数据纳入业务流程管理之中，并且能够在运行时直接作用于这些数据以指导流程执行。

Abstract: Traditional Business Process Management (BPM) focuses on discrete events and fails to incorporate critical continuous sensor data in cyber-physical environments. Hybrid declarative specifications, utilizing Signal Temporal Logic (STL), address this limitation by allowing constraints over both discrete events and real-valued signals. However, existing work has been limited to monitoring and post-hoc conformance checking. This paper introduces a novel Complex Event Processing (CEP)-based execution architecture that enables the real-time execution and enforcement of hybrid declarative models. Our three-layer approach integrates STL-inspired predicates into the execution flow, allowing the system to actively trigger activities and enforce process boundaries based on continuous sensor behavior. This approach bridges the gap between hybrid specification and operational control.

</details>


### [18] [MicroRacer: Detecting Concurrency Bugs for Cloud Service Systems](https://arxiv.org/abs/2512.05716)
*Zhiling Deng,Juepeng Wang,Zhuangbin Chen*

Main category: cs.SE

TL;DR: 提出了MicroRacer，一种非侵入式的自动化框架，用于检测微服务环境中并发错误。通过动态地在运行时对常用库进行工具化，MicroRacer能够收集详细的跟踪数据而无需修改应用程序代码，并利用这些数据来分析先发生关系和资源访问模式，以识别可疑的并发操作并通过三阶段验证过程确认并发错误。实验表明了MicroRacer在准确检测和定位并发问题方面的有效性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的并发错误检测方法往往由于其侵入性以及处理微服务架构复杂性的能力不足而存在局限性。为了克服这些限制，研究者们提出了一种针对云服务系统中并发错误检测的新方法。

Method: MicroRacer通过在运行时刻对广泛使用的库进行动态工具化来收集详细追踪信息，无需改动应用代码。基于此信息，该框架分析了服务系统内常见操作之间的先发关系及资源共享模式，识别出可能存在问题的并发操作，并采用三阶段验证流程对其进行测试和确认。

Result: 实验结果证明了MicroRacer能够高效准确地检测并定位开源微服务基准测试中的并发问题，其中包括了一些工业界已知的bug复现案例。

Conclusion: MicroRacer为微服务环境下的并发错误检测提供了一个有效的解决方案，其非侵入式特性及对架构复杂性的良好适应性使其特别适合于现代云应用服务。

Abstract: Modern cloud applications delivering global services are often built on distributed systems with a microservice architecture. In such systems, end-to-end user requests traverse multiple different services and machines, exhibiting intricate interactions. Consequently, cloud service systems are vulnerable to concurrency bugs, which pose significant challenges to their reliability. Existing methods for concurrency bug detection often fall short due to their intrusive nature and inability to handle the architectural complexities of microservices. To address these limitations, we propose MicroRacer, a non-intrusive and automated framework for detecting concurrency bugs in such environments. By dynamically instrumenting widely-used libraries at runtime, MicroRacer collects detailed trace data without modifying the application code. Such data are utilized to analyze the happened-before relationship and resource access patterns of common operations within service systems. Based on this information, MicroRacer identifies suspicious concurrent operations and employs a three-stage validation process to test and confirm concurrency bugs. Experiments on open-source microservice benchmarks with replicated industrial bugs demonstrate MicroRacer's effectiveness and efficiency in accurately detecting and pinpointing concurrency issues.

</details>


### [19] [Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models](https://arxiv.org/abs/2512.05887)
*Sairam Vaidya,Marcel Böhme,Loris D'Antoni*

Main category: cs.SE

TL;DR: 本文提出了一种基于语法和覆盖率引导的模糊测试方法，用于可扩展编译器。该方法结合了自动提取领域特定语言文法和使用预训练大语言模型自动生成代表性种子输入的技术，从而在不需手动干预的情况下生成适用于多个领域的测试用例。通过工具Germinator实现并评估了这种方法，在六个MLIR项目上显著提高了代码行覆盖率，并发现了88个未知错误（其中40个已确认）。


<details>
  <summary>Details</summary>
Motivation: 随着现代可扩展编译器框架如MLIR的发展，虽然加速了领域特定语言方言的创建，但同时也增加了确保正确性的难度。现有自动化测试生成方法要么需要为每个方言单独构建种子语料库，要么无法有效地找到错误。因此，开发一种既不需要手动适应又能有效针对方言特性的自动化测试生成方法变得十分必要。

Method: 研究者们提出了一种新的基于语法且受覆盖率引导的模糊测试策略，该策略能够自动从方言规范中提取语法，并利用预训练的大规模语言模型来自动生成代表性和多样化的初始输入，无需任何手动输入或训练数据。这些初始输入随后被用来启动覆盖率引导的模糊测试过程。

Result: 通过将所提方法整合进名为Germinator的工具中进行实验表明，相较于基于语法的基本方法，当应用于包含91种方言的六个MLIR项目时，Germinator生成的测试用例可以提高10-120%的代码行覆盖率，并且发现了88个此前未被发现的bug（其中40个已被证实），特别是在那些之前缺乏自动化测试生成器支持的方言中表现尤为突出。

Conclusion: 本研究展示了一种新的、对多种方言都有效的自动化测试方法，不仅提高了测试效率，还增强了对低资源方言的支持。这为未来可扩展编译器及类似系统的质量保证提供了有力工具。

Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 提出了一种新的预训练策略CV-Masking，该策略根据每个特征的内在变异性自适应调整掩码概率，旨在提高电子健康记录中不稳定生物标志物的建模效果。实验表明，CV-Masking在重建、下游预测性能和加速收敛方面优于随机和基于方差的策略。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码自动编码器（MAEs）方法在处理电子健康记录时通常采用统一随机掩码技术，假设所有特征都具有相同的可预测性。然而实际上，不同的实验室检测指标存在显著的波动性差异：一些生物标志物较为稳定，而另一些则波动较大且难以建模。特别是那些波动较大的生物标志物往往与急性病理生理学相关联，需要更复杂的模型来捕捉其复杂的时间模式。

Method: 提出了一种称为CV-Masking的新预训练策略，该策略根据每个特征的变异系数来自适应地调整掩码概率。此外，还结合了只针对数值进行掩码的目标函数，以更好地符合临床工作流程。

Result: 实验结果表明，相比于随机掩码和基于方差的方法，CV-Masking不仅提高了重建质量，还增强了下游任务的预测性能，并加快了模型收敛速度，从而生成更加稳健且具有临床意义的EHR表示。

Conclusion: 通过引入CV-Masking这种考虑到特征波动性的预训练方法，可以有效地提升对电子健康记录中不稳定生物标志物的学习能力，进而支持多种临床应用。

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [21] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 本文系统地评估了用于临床时间序列建模的基于转换器架构的分词方法，揭示了关于时间和值特征重要性的任务依赖性和有时反直觉的发现。研究结果表明，显式时间编码对所评估的下游任务没有提供一致的统计学显著益处；值特征的重要性取决于任务类型；冻结预训练编码器在参数效率上远超可训练编码器，并且大型临床编码器通过冻结嵌入减少了计算开销而表现更好。


<details>
  <summary>Details</summary>
Motivation: 针对电子健康记录处理中的分词策略对比不足的问题，旨在通过系统评价不同分词方法对临床时间序列建模的影响，以探索更有效的模型处理方式。

Method: 采用基于转换器的架构，在MIMIC-IV数据集上通过对四个临床预测任务进行控制消融实验来评估不同的分词方法。

Result: 显式时间编码对下游任务无明显统计学优势；值特征对于死亡率预测有影响但不影响再入院预测；冻结预训练代码编码器相比可训练版本表现更优且参数需求少得多；较大的临床编码器在所有任务中均表现出一致性改进。

Conclusion: 更简单、参数效率更高的方法在许多情况下可以实现强劲性能，但最佳分词策略仍取决于具体任务。

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [22] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: 本文提出了一种新的无监督领域适应方法VaRDASS，通过分层抽样来减少随机设置中的方差问题，并在三个领域迁移数据集上进行了实验验证，结果表明该方法提高了不匹配估计的准确性和目标领域的性能。


<details>
  <summary>Details</summary>
Motivation: 领域迁移是将机器学习模型应用于现实世界的关键挑战之一。虽然无监督领域适应（UDA）旨在通过最小化训练过程中的领域差异来解决这个问题，但在随机环境中，差异估计的高方差可能会抑制这种方法的理论优势。因此，需要一种专门针对UDA设计的随机方差减少技术。

Method: 提出了基于分层抽样的方差减少领域适应方法(VaRDASS)，特别考虑了两种具体的差异度量——相关对齐和最大均值差异(MMD)——并为这些术语推导出了特定的分层目标。此外，还介绍并分析了一种实用的k-means风格优化算法。

Result: 在三个领域迁移数据集上的实验表明，所提出的方法能够提高差异估计准确性以及目标域的表现。

Conclusion: VaRDASS作为一种创新的无监督领域适应方法，通过有效降低随机环境下的方差问题，成功地提升了领域间差异估计精度与目标领域性能。

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [23] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 本文提出了一种新的高效遗忘框架，通过识别并移除对模型输出影响微乎其微的数据子集，在实际例子中实现了高达约50%的计算节省。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中数据隐私问题日益受到关注，能够从训练好的模型中删除特定数据点的能力变得越来越重要。当前最先进的遗忘方法通常将遗忘集合中的所有点同等对待，但本文认为对于模型学习几乎没有影响的数据点是否需要被移除值得探讨。

Method: 通过对语言和视觉任务的影响函数进行比较分析，识别出对模型输出影响很小的训练数据子集，并基于此洞察提出了一个高效的遗忘框架，该框架在执行遗忘前减少了数据集的大小。

Result: 提出的高效遗忘框架能够在真实世界的实例中显著减少计算量（高达约50%）。

Conclusion: 本研究提供了一种新颖的方法来优化数据遗忘过程，不仅提高了效率还减少了资源消耗，为处理数据隐私问题提供了新的视角。

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [24] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出了RSA2C算法，一种基于RKHS-SHAP的归因感知强化学习方法，通过使用状态归因来辅助训练，提高了算法效率、稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释强化学习方法很少利用状态归因来帮助训练，而是将所有状态特征同等对待，忽略了个体状态维度对奖励的异质影响。

Method: RSA2C是一个基于RKHS-SHAP的状态归因感知、核化双时间尺度演员-评论家算法。它包括了演员（Actor）、价值评论家（Value Critic）和优势评论家（Advantage Critic）。这些组件都得到了RKHS的增强，并且使用稀疏化的字典。状态归因是通过RKHS-SHAP从价值评论家中计算得出，然后转换成马哈拉诺比斯门控权重来调节演员梯度和优势评论家目标。

Result: 理论分析导出了在状态扰动下的全局非渐近收敛界，展示了通过扰动误差项体现的稳定性以及通过收敛误差项体现的效率。实证结果表明，在三个标准连续控制环境中的表现验证了该算法的有效性、稳定性和可解释性。

Conclusion: RSA2C算法通过引入状态归因机制改进了传统演员-评论家方法，不仅提高了模型性能，还增强了模型决策过程的透明度。

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [25] [CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators](https://arxiv.org/abs/2512.05297)
*Xianglong Hou,Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: 介绍了一种新的框架——连续流算子(CFO)，用于学习时间依赖偏微分方程(PDE)的连续时间动力学，无需通过ODE求解器反向传播，并且能够接受任意非均匀时间网格上的训练数据。CFO在四个基准测试中展示了优越的长期稳定性和显著的数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统的神经算子替代方案在处理时变PDE时通常采用自回归预测方案，这会导致长时间滚动时误差累积，并要求统一的时间离散化。本研究旨在提出一种不需要标准连续方法（如神经ODE）计算负担的新框架，以学习PDE的连续时间动力学。

Method: CFO框架重新利用了流匹配来直接学习PDE的右侧项，而不需要通过ODE求解器进行反向传播。它将时间样条拟合到轨迹数据上，使用结点处时间导数的有限差分估计来构建概率路径，其速度紧密逼近真实的PDE动力学。随后，一个神经算子通过流匹配被训练来预测这些解析速度场。该方法天然地对时间分辨率不变：训练可以接受任意非均匀时间网格上的轨迹样本，同时通过ODE积分以任意时间分辨率查询解决方案。

Result: 在四个基准（Lorenz, 1D Burgers, 2D扩散-反应, 2D浅水）上，CFO显示出了卓越的长视距稳定性和出色的数据效率。即使只用25%不规则采样的时间点进行训练，CFO也超过了基于完整数据训练的自回归基线，相对误差减少了高达87%。尽管推理时需要数值积分，但CFO仍能实现与自回归基线相当的效率，仅使用后者一半的功能评估次数即可完成，同时还独特地支持逆时间推理和任意时间查询。

Conclusion: CFO提供了一种新颖有效的方法来解决时变PDE问题，不仅改善了长期稳定性，还提高了数据效率。相比于传统方法，CFO允许更加灵活的时间采样策略，在保持高精度的同时大大减少了所需的计算资源。

Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.

</details>


### [26] [The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?](https://arxiv.org/abs/2512.05311)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee*

Main category: cs.LG

TL;DR: 该研究系统地评估了最先进的机器学习模型在区分人类和大型语言模型（LLM）生成的科学想法方面的能力，特别是在连续改写阶段之后。结果表明，在五次连续改写后，检测性能平均下降25.4%。加入研究问题作为上下文信息可以提高检测性能达2.97%。


<details>
  <summary>Details</summary>
Motivation: 随着对大型语言模型（LLM）作为研究助手的依赖不断增加，区分由LLM与人类产生的想法变得至关重要，以理解LLM研究能力的认知细微差别。尽管检测LLM生成文本已经被广泛研究，但区分人类与LLM产生的科学想法仍是一个未被探索的领域。

Method: 采用最先进的机器学习模型来评估它们区分人类和LLM生成的想法的能力，特别是经过一系列连续改写过程后的表现。此外，还测试了将研究问题作为上下文信息加入是否能改善检测性能。

Result: 发现最先进的模型在源归属上面临挑战，尤其是在经过五次连续改写后，其检测性能平均下降了25.4%。通过添加研究问题作为背景信息，检测性能最高可提升2.97%。值得注意的是，当想法被改写成简化、非专家风格时，检测算法遇到了极大困难，这是导致可区分LLM特征消失的主要原因。

Conclusion: 这项工作强调了现有检测方法在面对多次改写以及简化表述时存在的局限性，并指出提供额外上下文信息能够稍微缓解这一问题，但仍需进一步研究以有效区分人类与LLM生成的科学想法。

Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.

</details>


### [27] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 本研究测试了NVIDIA的FourCastNetv2 (FCNv2)天气预报模型在不同噪声水平下的敏感性和鲁棒性，特别是在飓风等极端天气事件中的表现。实验结果表明，该模型在低至中度噪声干扰下能够准确保持飓风特征；即使是在高噪声水平下也能大致维持风暴轨迹和结构，但位置准确性有所下降。此外，无论噪声水平如何，模型都倾向于低估风暴强度和持续时间。对于完全随机的初始条件，模型在几个时间步后生成平滑且连贯的预测，显示出其输出趋向于稳定和平滑。


<details>
  <summary>Details</summary>
Motivation: 了解天气预报模型对输入噪声或不同不确定性的鲁棒性对于评估其输出可靠性至关重要，尤其是在处理如飓风这样的极端天气事件时。

Method: 通过两个实验来评估模型在不同噪声注入水平下的输出：首先，使用来自ECMWF再分析v5（ERA5）数据集的飓风佛罗伦萨（2018年9月13日至16日）作为初始状态，并添加不同程度的高斯噪声，观察这对预测轨迹和风暴强度的影响；其次，让FCNv2从完全随机的初始条件开始运行，观察模型如何响应无意义的输入。

Result: FCNv2在低到中度噪声扰动下能准确保留飓风特性；即使在高水平噪声下，尽管位置精度开始下降，它仍能保持总体风暴路径和结构。不过，在所有噪声水平下，模型都一致地低估了风暴强度与持久性。当给予完全随机的起始条件时，经过少数几个时间步骤后，模型产生了平稳且统一的预测结果，这表明了模型倾向于产生稳定而平滑化的输出。

Conclusion: 研究表明，FCNv2在面对不同级别的噪声干扰时表现出良好的鲁棒性，特别是对于飓风特征的保持方面。然而，模型存在低估风暴强度的问题，这可能是未来改进的方向之一。此外，该方法简单易行，可以方便地应用于其他基于数据驱动的人工智能天气预报模型上。

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [28] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦优化算法，该算法在非凸优化中实现了现有方法中最佳的通信和本地计算复杂度。基于不精确复合梯度法，并结合了精心设计的梯度估计器（RG-SAGA）和特殊子问题求解程序。


<details>
  <summary>Details</summary>
Motivation: 当前不同的联邦优化算法采用了不同的客户端选择策略，而现有的比较这些方法的指标通常不会区分这些策略所带来的不同通信成本。为了解决这个问题，作者们引入了一个简单且自然的联邦优化模型来量化通信和本地计算复杂度。

Method: 提出了一个新的算法，该算法基于不精确复合梯度方法，并使用了一种精心构造的梯度估计器以及一种特殊的辅助子问题解决过程。梯度估计器基于SAGA，一种流行的方差减少梯度估计器。此外，还引入了递归梯度技术作为一种可能改进给定条件无偏梯度估计器误差界限的一般方法。通过将此技术应用于SAGA，获得了具有改进误差界限的新估计器RG-SAGA。

Result: 新提出的算法在非凸优化情况下达到了已知最优的通信与本地复杂度。同时，通过对SAGA进行改进得到的RG-SAGA估计器相比原版具有更好的误差界。

Conclusion: 本研究不仅为联邦学习中的优化问题提供了新的视角，还通过引入更有效的梯度估计技术和客户端选择策略，提升了联邦学习算法的实际性能。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [29] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: 提出了一种名为PATHFINDER的方法，通过使用蒙特卡洛树搜索生成训练路径轨迹、提高训练数据质量和重新制定子查询来改善多跳问答的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于训练方法的多跳问答系统仍然受到大语言模型产生幻觉和错误推理路径的影响，这阻碍了性能提升。

Method: 采用蒙特卡洛树搜索生成训练路径轨迹；通过子答案召回率和LLM作为评判验证过滤错误及冗长的轨迹以提高训练数据质量；以及为处理检索失败情况而重新制定子查询。

Result: 在公开基准数据集上展示了PATHFINDER能够提高多跳问答的表现。

Conclusion: PATHFINDER通过改进训练路径的选择与优化训练数据质量，在多跳问答任务中表现优于现有方法。

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [30] [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)
*Mahesh Kumar Nandwana,Youngwan Lim,Joseph Liu,Alex Yang,Varun Notibala,Nishchaie Khanna*

Main category: cs.LG

TL;DR: 本文介绍了一种名为Roblox Guard 1.0的新型指令微调大型语言模型，旨在通过全面的输入输出调节来增强LLM系统的安全性。该模型基于Llama-3.1-8B-Instruct构建，并在合成和开源安全数据集上进行微调，以提高上下文理解和决策能力。同时发布了一个新的评估基准RobloxGuard-Eval，用于评测LLM防护机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）虽然在训练后阶段进行了安全性调整，但仍可能生成不适当的内容，给用户带来潜在风险。因此，需要一种能够对模型的输入和输出都有效管理的安全机制。

Method: 开发了Roblox Guard 1.0，这是一个基于Llama-3.1-8B-Instruct架构的高级指令微调LLM。该模型使用结合了合成数据与开源安全数据集进行指令微调，加入了链式思维解释和输入反转技术来加强其理解能力和判断力。

Result: Roblox Guard 1.0展示出对于未曾见过的安全分类具有很好的泛化能力，并且在域外安全基准测试中表现出色。此外，还推出了一个名为RobloxGuard-Eval的新基准，它包含可扩展的安全分类法，用来评价LLM保护措施及调节框架的效果。

Conclusion: Roblox Guard 1.0作为一项创新成果，为提高大型语言模型的安全性提供了解决方案，同时发布的RobloxGuard-Eval也为后续研究提供了有价值的评估工具。

Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.

</details>


### [31] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出了一种针对基于大语言模型（LLMs）的硬件代码生成的新颖去学习框架，该框架结合了保持语法的去学习策略和细粒度的楼层感知选择性损失，以有效去除问题知识而不损害代码生成能力。实验表明，此框架支持更大的遗忘集，在保留RTL代码的句法正确性和功能完整性的同时，通常只需要一个训练周期。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在自动代码生成方面表现出强大的潜力，但它们容易记忆专有知识产权、受污染的基准测试和不安全的编码模式，这构成了可靠性挑战。为了解决这些问题，研究提出了新的去学习框架。

Method: 采用一种保持语法的去学习策略来保护硬件代码结构的完整性，并引入了细粒度的楼层感知选择性损失来实现问题知识的精确高效移除。

Result: 通过广泛的实验验证，所提出的框架能够支持高达3倍大的遗忘集，且通常仅需一个训练周期即可完成，同时保证寄存器传输级(RTL)代码的句法正确性和功能性完整得到维护。

Conclusion: 这项工作为实现可靠的大语言模型辅助硬件设计开辟了道路。

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [32] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: 本文提出了一种名为Confounding-Aware Token Rationalization (CATR) 的框架，该框架通过选择文本中稀疏但必要的子集来解决因果推断中由高维文本数据带来的挑战，从而保持了足够的混淆信息以实现无混淆性。实验表明，与现有基线相比，CATR能够提供更准确、稳定且可解释的因果效应估计。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理技术的进步，文本数据在因果推断中的应用日益增加，特别是在调整治疗效果估计中的混淆因素方面。然而，高维度文本虽然可以编码丰富的上下文信息，但也给因果识别和估计带来了独特的挑战，尤其是在观察层面经常违反正性假设（即要求在混淆变量值之间有足够的治疗重叠）时。冗余或虚假的文本特征会增加维度，导致倾向分数极端化、权重不稳定以及效应估计方差膨胀。

Method: 为了解决这些问题，作者提出了Confounding-Aware Token Rationalization (CATR) 框架，该方法旨在通过使用一种设计用来保留足够混淆信息以满足无混淆条件的残差独立诊断，选择出一个稀疏但必需的token子集。通过丢弃无关紧要的文本同时保留关键信号，CATR缓解了观察层面的正性假设违反情况，并稳定了下游因果效应估计器。

Result: 合成数据实验及基于MIMIC-III数据库的真实世界研究显示，与现有基线方法相比，CATR能够产生更加精确、稳定并且具有更好可解释性的因果效应估计结果。

Conclusion: 综上所述，CATR作为一种新颖的方法，在处理包含大量文本数据的因果推断问题时表现出色，能够有效减轻由高维文本引起的问题，并提高因果效应估计的质量。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [33] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: 本研究评估了最先进的蛋白质-配体打分函数在模拟有限已知结构和实验亲和力测量的目标上的泛化能力，发现常用基准测试并不能反映泛化到新目标的真实挑战。研究表明大规模自监督预训练可能有助于缩小这一泛化差距，并探索了利用有限测试目标数据提高打分函数性能的简单方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在分子设计中变得越来越重要，确保可学习的蛋白质-配体打分函数对于新蛋白靶点的可靠性至关重要。尽管许多打分函数在标准基准上表现良好，但它们超出训练数据进行泛化的能力仍然是一个重大挑战。

Method: 通过在模拟对具有少量已知结构和实验亲和力测量的目标进行评估的数据集分割上评估最先进打分函数的泛化能力；探讨大规模自监督预训练是否能够帮助弥合这种泛化差距；同时考察利用有限数量的目标数据来改进打分函数性能的简便方法的效果。

Result: 研究显示，常用的基准并不准确地反映了向新的靶标泛化的真正难度；提供了初步证据表明大规模自我监督预训练可能有助于缓解这一问题；还展示了即使是简单的利用少量测试目标信息的方法也能够有效提升打分函数的表现。

Conclusion: 该研究强调了需要更严格的评估协议，并为开发针对新型蛋白靶点具有良好预测性的打分函数提供了实用建议。

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [34] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的MineROI-Net模型，用于预测比特币挖矿硬件购买后一年内的投资回报情况。该模型在检测不盈利和盈利周期方面表现出高精度，并且提供了一个数据驱动的工具来帮助决策者确定最佳的硬件购买时机，从而降低资本密集型挖矿操作中的财务风险。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿行业已经发展成为一个资本密集型产业，但关于何时购买新的ASIC硬件缺乏指导。鉴于市场波动性大、技术快速过时以及由协议驱动的收入周期等因素，存在一个决策问题上的空白。

Method: 通过将硬件购买决策视为时间序列分类任务，开发了名为MineROI-Net的开源Transformer架构模型，旨在捕捉挖矿盈利能力中多尺度的时间模式。

Result: MineROI-Net在2015年至2024年间发布的20款ASIC矿机的数据集上进行了评估，相较于LSTM基线模型及TSLANet，MineROI-Net实现了83.7%的准确率与83.1%的宏观F1分数。特别是在识别非盈利期（精确度93.6%）和盈利期（精确度98.5%）方面表现突出。

Conclusion: MineROI-Net为比特币挖矿硬件购买决策提供了实用的数据驱动工具，有助于减少资本密集型挖矿运营中的财务风险。

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [35] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为RevoNAD的方法，它通过多轮多专家共识、自适应反射探索以及帕累托导向的进化选择来优化神经架构设计。该方法在多个数据集上取得了最先进的性能，并且通过消融和转移研究进一步证明了其实用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已经能够生成不受手动预定义搜索空间限制的新架构，但这种基于LLM的生成方式仍然面临挑战，比如难以平滑地根据反馈进行架构改进，容易陷入冗余结构或产生不可行的设计。

Method: RevoNAD 方法引入了三个关键组件：1) 多轮多专家共识，将孤立的设计规则转化为有意义的架构线索；2) 自适应反射探索，根据奖励方差调整探索程度，在反馈不确定时探索，在达到稳定后细化；3) 帕累托导向的进化选择，促进同时优化准确率、效率、延迟、置信度和结构多样性的架构。

Result: RevoNAD 在 CIFAR10, CIFAR100, ImageNet16-120, COCO-5K 和 Cityscape 等数据集上实现了最先进的表现。此外，消融和迁移学习研究验证了 RevoNAD 在支持实用可靠且可部署的神经架构设计方面的有效性。

Conclusion: RevoNAD 成功地结合了基于 LLM 的推理与反馈对齐的架构搜索，不仅克服了传统方法中的一些局限性，而且在多种任务上展示了其优越性，为未来的研究提供了新的方向。

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [36] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: 本文提出了一种名为TS-Hint的时间序列基础模型框架，该框架结合了基于注意力机制数据和显著性数据的思考链推理。实验结果表明，在有限的数据设置下，通过少量学习，该模型能够直接从多变量时间序列特征中学习，并且有效提高了半导体制造过程中的材料去除率（MRR）预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动方法在预测半导体制造工艺如化学机械抛光(CMP)过程中的材料去除率(MRR)时，依赖于从时间序列中提取静态特征，这导致了时间动态信息的丢失，并且这些方法需要大量的训练数据。为了解决这些问题，作者提出了一个新的解决方案。

Method: 提出了TS-Hint，这是一个时间序列基础模型(TSFM)框架，它整合了基于注意力机制数据和显著性数据的思考链推理，以提供训练期间的注意提示。

Result: 实验结果显示，TS-Hint模型能够在数据有限的情况下，通过少量学习有效地直接从多变量时间序列特征中学习。

Conclusion: 本研究提出的TS-Hint模型不仅减少了对大量训练数据的需求，而且能更准确地捕捉时间序列中的动态特性，从而提高了MRR预测的准确性。

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [37] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为IdealTSF的框架，该框架通过整合理想的正样本和负样本来改进时间序列预测。它包括预训练、训练和优化三个步骤，并使用对抗扰动的负优化机制。实验表明，这种方法在处理噪声样本或低质量数据时特别有效。


<details>
  <summary>Details</summary>
Motivation: 深度学习在时间序列预测任务中表现出色，但序列数据中的缺失值和异常等问题阻碍了其进一步发展。以往的研究主要集中在从序列数据中提取特征信息或将这些问题数据作为正样本进行知识迁移上。本研究旨在利用这些非理想负样本来提高事件预测能力。

Method: 提出了IdealTSF框架，该框架分为预训练、训练和优化三个阶段。首先通过从负样本数据中抽取知识来预训练模型，接着在训练过程中将序列数据转化为理想正样本，并引入带有对抗干扰的负优化机制。

Result: 广泛的实验证明，负样本数据能够解锁基础注意力架构内的时间序列预测潜力。因此，对于包含噪声样本或低质量数据的应用场景而言，IdealTSF尤为适用。

Conclusion: 通过采用非理想负样本与理想正样本相结合的方法，IdealTSF提供了一种新颖的时间序列预测方案，尤其擅长处理存在噪声或质量问题的数据集。

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [38] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: 本研究通过四个表格分类任务探讨了集成模型如何在保持高准确性的同时控制过拟合，发现集成方法可以通过减少方差来实现这一平衡，尤其是在数据集具有有意义的非线性结构时。对于接近线性的干净数据，集成方法提供的额外好处有限；而对于噪声或高度不平衡的数据集，则需要正则化以避免拟合噪声或多数类模式。


<details>
  <summary>Details</summary>
Motivation: 尽管集成模型通常比单个学习器达到更高的准确度，但它们在维持小泛化差距方面的能力并不总是被很好地理解。

Method: 使用重复分层交叉验证和统计显著性测试，比较了线性模型、单一决策树以及九种集成方法在乳腺癌、心脏病、皮马糖尿病和信用卡欺诈这四个表格分类任务上的表现。

Result: 结果显示，在具有有意义非线性结构的数据集上，基于树的集成可以将测试准确度提高5到7个百分点，并且保持差距低于3%。对于噪声大或高度不平衡的数据集，集成仍然具有竞争力，但需要正则化来避免拟合噪声或多数类别模式。此外，还计算了一些简单的数据集复杂度指标（如线性得分、Fisher比率和噪声估计），这些可以帮助解释何时集成能够有效控制方差。

Conclusion: 总体而言，这项研究为集成模型如何及何时能够在保持低过拟合的情况下维持高准确性提供了清晰的理解，为现实世界中表格应用的模型选择提供了实用指导。

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [39] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 本文比较了几何量子机器学习模型在不同分子几何结构下的性能，发现图嵌入特征对于提高几何数据集的可训练性是有效的途径，并且置换对称嵌入是几何学习中最具泛化性的量子机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 研究者们想要探索在不同分子几何形状下，具备不同对称性质（无对称等变、旋转等变及置换等变）的几何量子机器学习模型的表现差异，以此来确定对于特定分子几何而言，选择哪种模型能够更好地实现泛化。

Method: 通过对比两种分子（直线型LiH分子与三角锥形NH3分子）的数据集上，无对称等变、旋转及置换等变以及图嵌入置换等变的几何量子机器学习模型的性能，并以经典等变模型作为基准进行性能比较。

Result: 研究表明，图嵌入特征对于提升几何数据集的可训练性非常有效；同时，在这些模型之中，置换对称嵌入展现出了最佳的泛化能力。

Conclusion: 根据分子几何形状的不同以及模型表现差异，可以为增强几何学习任务中的模型泛化能力提供选择依据。特别地，图嵌入技术与置换对称性相结合被证明是增强量子机器学习模型泛化性的有效方法。

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [40] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 本文提出了Credal和Interval Deep Evidential Classifications（CDEC与IDEC）两种新方法，用于解决分类任务中的不确定性量化问题。这些方法利用信度集和证据预测分布区间来避免过拟合，并系统地评估认识论和偶然性不确定性。通过在多个数据集上的实验表明，CDEC与IDEC能够在保持竞争力的预测准确性的同时，实现最先进的OoD检测和良好的校准预测区域。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化(UQ)是人工智能领域的一个关键挑战，它深刻影响着决策制定、风险评估和模型可靠性。为了解决分类任务中的UQ问题，论文提出了一种新的方法。

Method: 引入了Credal and Interval Deep Evidential Classifications (CDEC 和 IDEC)，它们分别利用一个信度集（概率的闭合凸集）和一个证据预测分布区间。训练时使用标准反向传播算法以及基于证据理论的损失函数。

Result: 通过在MNIST, CIFAR-10, CIFAR-100及其自然OoD变化上进行广泛的实验，结果表明CDEC和IDEC达到了竞争性的预测准确率，在认识论和总不确定性下实现了最先进的OoD检测，并且提供了扩展可靠的紧密良好校准预测区域。此外，关于集成大小的消融研究进一步表明，CDEC仅用一个小的集成就能获得稳定的不确定性估计。

Conclusion: CDEC和IDEC作为处理分类任务中不确定性量化的创新方法，不仅克服了先前工作的局限性，而且扩展了现有的证据深度学习文献。它们在提供准确预测的同时，还能有效识别超出可接受范围的认识论或偶然性不确定性。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [41] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: 本文提出了一种新的增量分布核$\mathcal{IDK}$-$\mathcal{S}$，用于流式异常检测。该方法结合了离线检测器隔离分布核的优势，并采用轻量级的增量更新机制，在保证检测精度的同时显著降低了计算开销。实验表明，$\mathcal{IDK}$-$\mathcal{S}$在十三个基准测试中实现了比现有最先进方法更高的检测精度和更快的速度。


<details>
  <summary>Details</summary>
Motivation: 数据流上的异常检测面临着需要在不断变化的数据分布中保持高检测准确性的同时确保实时效率的挑战。为了解决这些挑战，提出了一个新的动态表示方法。

Method: 引入$\mathcal{IDK}$-$\mathcal{S}$，一种新颖的增量分布核，适用于流式异常检测。该方法基于核均值嵌入框架创建新的动态表示。它继承了离线检测器隔离分布核的优势，并采用了一种轻量级的增量更新机制来减少与全模型重训练相比的计算开销。

Result: 广泛的实验显示，$\mathcal{IDK}$-$\mathcal{S}$在十三个基准测试中实现了卓越的检测准确度，同时运行速度明显快于现有最先进方法，在许多情况下甚至快一个数量级。

Conclusion: 通过结合离线检测器隔离分布核的优点和实现轻量级增量更新机制，$\mathcal{IDK}$-$\mathcal{S}$不仅提高了检测精度，还大大减少了计算成本，成为流式异常检测领域的一个重要进展。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [42] [On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability](https://arxiv.org/abs/2512.05534)
*Yiming Tang,Harshvardhan Saini,Yizhen Liao,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架来理解稀疏字典学习(SDL)方法，包括稀疏自动编码器、转码器和交叉编码器等。该框架首次将SDL视为一个统一的优化问题，并解释了特征吸收、死神经元及神经元重采样技术等现象。此外，还设计了受控实验以验证理论结果的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不同领域展现出卓越的能力，理解它们如何学习表示和处理信息对于科学进步和可信部署变得越来越重要。尽管稀疏字典学习（SDL）方法如稀疏自动编码器、转码器和交叉编码器，在实践中取得了显著的成功，但缺乏足够的理论基础来支持这些方法。现有理论仅限于具有权重约束的稀疏自动编码器，未能为更广泛的SDL方法提供正式依据。

Method: 开发了一个统一的理论框架，将各种稀疏字典学习(SDL)方法视为一个综合性的优化问题。通过这个框架展示了不同类型的方法是如何实例化的，并对优化景观进行了严格的分析。此外，还设计了控制实验来验证所提出的理论成果。

Result: 提供了对一些经验观察到的现象的第一个理论解释，比如特征吸收、死神经元以及神经元重采样技术的作用机制。同时，通过控制实验进一步证实了理论分析的结果。

Conclusion: 本研究为理解稀疏字典学习方法及其在分离叠加概念方面的作用奠定了坚实的理论基础。这不仅增加了我们对该类方法工作原理的理解，也为未来的研究和发展提供了新的视角。

Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.

</details>


### [43] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: 提出了一种名为SCoNE的新方法，用于解决多视图异常检测中一致邻居表示的问题。该方法直接使用多视图实例表示一致邻居，并具有数据依赖性属性，从而在不同密度区域中形成合适大小的邻域，同时保持线性时间复杂度。实验结果表明，SCoNE比现有方法具有更高的检测准确率和更快的速度。


<details>
  <summary>Details</summary>
Motivation: 当前多视图异常检测方法在捕捉所有视图间的一致邻居方面存在不足：一是无法很好地处理各视图中密度不同的同一邻居，影响检测精度；二是学习过程计算成本高，达到O(N^2)，不适合大规模数据集。

Method: 提出了Spherical Consistent Neighborhoods Ensemble (SCoNE) 方法，其特点包括：(a) 直接利用多视图实例来代表一致邻居，无需像现有方法那样使用中间表示。(b) 邻域具有数据依赖特性，在稀疏区域产生较大的邻域，在密集区域则生成较小的邻域。这样的设计避免了学习步骤，使得时间复杂度降至O(N)。

Result: 实验评估表明，SCoNE不仅拥有更优的异常检测准确性，而且在处理大规模数据集时运行速度远快于现有方法。

Conclusion: 通过引入一种新的基于球形一致邻居集成的方法SCoNE，解决了多视图异常检测中存在的主要问题，实现了高效且精确的异常检测。

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [44] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，利用多变量自适应回归样条（MARS）和N-球采样技术来提高黑盒机器学习模型预测解释的局部保真度。实验表明，该方法相比基线能显著减少37%的均方根误差，提高了局部解释的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒机器学习模型变得越来越复杂，并在高风险领域得到应用，为这些模型的预测提供解释变得至关重要。然而，广泛使用的LIME技术假设局部决策边界是线性的，这导致它不能很好地捕捉非线性关系，从而产生不正确的解释。

Method: 研究者们提出了一种新方法，通过使用多变量自适应回归样条(MARS)来建模能够有效捕获参考模型底层行为的非线性局部边界，同时采用N-球采样技术直接从期望分布中取样，而不是像LIME那样重新加权样本，以此进一步提高了解释的真实性得分。

Result: 通过对三个UCI数据集上不同分类器及变化的核宽度进行评估，实验结果表明，本方法相较于基线方法平均减少了37%的均方根误差，显著提升了局部保真度。

Conclusion: 所提出的方法能够生成更真实的解释，尤其是在处理具有非线性关系的数据时，比现有方法如LIME表现更好。

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [45] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的全局度量——当前策略与先前策略之间的熵比率，以及一种基于此度量的'熵比率裁剪'(ERC)机制，用于稳定大型语言模型后训练中的策略更新，并在多种基准测试中证明了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模语言模型后训练依赖强化学习时出现的分布偏移问题，这种偏移可能导致训练不稳定，具体表现为策略熵和梯度波动。尽管PPO-Clip通过重要性裁剪缓解了这一问题，但它忽略了动作的整体分布变化。

Method: 提出使用当前策略与之前策略间的熵比率作为新指标来量化策略探索的变化；基于此指标引入了'熵比率裁剪'(ERC)机制，该机制对熵比率施加双向约束，从而在全球分布层面上稳定策略更新。

Result: 实验表明，在多个基准上整合ERC到DAPO和GPPO强化学习算法后，能够持续提高性能。

Conclusion: 提出的ERC方法有效地解决了因分布偏移导致的语言模型后训练过程中遇到的稳定性问题，并且在不同场景下均表现出色。

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [46] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 研究了如何通过超参数转移来扩大预条件优化器的规模，发现适当的缩放规则可以显著提高Muon和Shampoo等优化器相较于AdamW的训练速度。


<details>
  <summary>Details</summary>
Motivation: 最近引入的一些利用矩阵级预条件的深度学习优化器在小型实验中显示出相对于当前主流优化器AdamW的潜在加速效果，但它们的成功验证和复制结果却参差不齐。为了更好地理解这些优化器在大规模应用中的有效性，本工作旨在探索如何通过超参数转移实现预条件优化器的规模化。

Method: 研究了对于包括Shampoo、SOAP和Muon在内的多种优化器而言，最优学习率与权重衰减应如何随模型宽度和深度进行缩放，并考虑了诸如阻塞（blocking）和嫁接（grafting）等常用技术的影响。基于μP等先前工作，探讨了如何改善超参数转移以提升预条件优化器的表现。

Result: 发现根据μP调整学习率有助于改善转移效果，但仍可能因有限宽度偏差导致最优学习率漂移，该问题可通过阻塞和显式谱归一化得到缓解。对于计算最优缩放来说，独立权重衰减按1/width比例缩放几乎对所有优化器都是最优选择。应用这些缩放规则后，观察到Muon和Shampoo在训练Llama架构语言模型时相比AdamW分别实现了1.4倍和1.3倍的速度提升。

Conclusion: 研究表明，研究最优超参数转移对于在给定实际调优预算下可靠地比较大规模优化器至关重要。正确的超参数缩放规则能够帮助某些预条件优化器如Muon和Shampoo在大规模模型训练中保持相对于AdamW的优势。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [47] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 本文提出了用于回归和分类流程的模块喷流（Modular Jets），并定义了经验上的海市蜃楼状态和可识别状态，通过证明一个关于两模块线性回归管道的喷流可识别性定理，展示了仅基于风险评估的局限性。此外，还提供了一种算法（MoJet）来估计经验喷流和诊断海市蜃楼现象，并通过线性和深度回归以及管道分类的例子说明了这一框架的应用。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习主要通过保留数据的预测风险来评估模型。这种评估方法可以量化函数在某个分布上的表现，但并没有解决模型内部分解是否由数据和评估设计唯一确定的问题。

Method: 文章引入了‘模块喷流’的概念，这是一种局部线性响应映射，描述了每个模块对输入的小结构扰动如何反应。研究者提出了一种经验上的‘海市蜃楼’状态，其中多种不同的模块化分解会诱导出无法区分的喷流，从而保持观察上的等效性；与之相对的是‘可识别’状态，在这种状态下，观察到的喷流能够根据自然对称性唯一地确定一种分解。研究者在两个模块的线性回归流程中证明了一个喷流可识别性定理。

Result: 结果表明，在温和的秩假设下，且有访问模块级喷流的情况下，内部因式分解是唯一确定的；而仅有基于风险的评估则允许存在大量实现相同输入-输出映射的海市蜃楼分解。研究者还展示了一种用于经验喷流估计和海市蜃楼诊断的算法（MoJet），并通过线性和深度回归以及管道分类实例说明了该框架的应用。

Conclusion: 论文通过对模块喷流的研究，揭示了传统风险评估方法在确定模型内部结构方面的不足，并提出了新的方法来更好地理解和诊断模型的模块化分解。

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [48] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: 本文提出了一种改进的梯度路由技术，称为选择性梯度掩蔽（SGTM），旨在解决大型语言模型中由于错误标记内容带来的风险。通过将特定领域知识限制在模型参数的一个子集中，SGTM能够在存在标签噪声的情况下提供更好的保留/遗忘权衡，并且对对抗性微调表现出强大的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力的提升，它们也带来了双重用途的风险。尽管数据过滤被作为一种预训练时间缓解措施出现，但这种方法面临着挑战：大规模地标记有害数据成本高昂，而且随着更大模型样本效率的提高，即使是少量错误标记的内容也可能导致危险的能力。为了解决与错误标记有害内容相关的风险，研究提出了选择性梯度掩蔽（SGTM）作为梯度路由的一种改进版本，特别关注其对于标签噪声的鲁棒性评估。

Method: SGTM通过零掩蔽选定的梯度来实现，使得目标领域的例子仅更新其专用参数。该方法应用于两个场景：从一个双语合成数据集训练的模型中移除一种语言的知识，以及从用英语维基百科训练的模型中移除生物学知识。

Result: 在两种情况下，与数据过滤和先前提出的梯度路由实例化相比，SGTM在存在标注错误时提供了更好的保留/遗忘平衡。此外，与基于微调的遗忘方法相比，SGTM展示了对对抗性微调的强大抵抗力，需要七倍于常规方法的微调步骤才能在遗忘集合上达到基线性能。

Conclusion: 结果表明，SGTM为现有的安全缓解措施提供了一个有前景的预训练时间补充，特别是在无法避免标签噪声的情况下。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [49] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文详细介绍了用于解决超体积(HV)改进计算成本问题的近似算法的数学和算法细节，填补了现有文献中对该算法严谨描述的空白。


<details>
  <summary>Details</summary>
Motivation: 基于超体积（HV）的贝叶斯优化是多目标决策的标准方法之一，但其获取函数优化的计算成本高昂，尤其是HV改进计算非常昂贵。虽然HV箱分解提供了一种有效处理频繁精确改进计算的方法，但在最坏情况下它具有超多项式内存复杂度的问题。为了克服这个问题，尽管Couckuyt等人(2012)提出使用近似算法，但至今缺乏该算法严格的算法描述。

Method: 本文提供了由Couckuyt等人(2012)提出的近似算法全面的数学和算法细节，以解决在使用HV箱分解时遇到的高内存复杂度问题。

Result: 通过详尽地介绍近似算法的具体内容，为研究者们理解和应用此算法来降低HV改进计算的成本提供了坚实的基础。

Conclusion: 本论文通过提供一个详细的近似算法描述，解决了HV箱分解方法中存在的高内存需求问题，促进了基于HV的贝叶斯优化方法的发展。

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [50] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: 介绍了NEAT，一种基于自回归流模型的新型分子图生成方法，它将分子图视为原子集合处理，无需预设顺序，实现了高效的3D分子结构生成，并且对原子排列具有不变性。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在生成3D分子结构时，假设了标记的顺序，这与文本自然顺序不同，因为给定分子图前缀后下一个标记的预测应该不受原子排列的影响。为了解决这个问题，NEAT提出了一种新的方法来处理分子图，不需要依赖于规范顺序或焦点原子。

Method: NEAT使用邻域引导的、高效的自回归集合变换器，将分子图看作是一组原子，并学习图边界上可接受标记的无序分布。通过这种方式，NEAT能够实现原子级排列不变性的同时保持高计算效率。

Result: NEAT在3D分子生成方面接近最先进性能，同时提供了更高的计算效率和原子级别上的排列不变性，为可扩展的分子设计奠定了实用基础。

Conclusion: NEAT展示了一种新的途径，即不依赖于固定的原子顺序就能有效生成3D分子结构，这为未来的研究提供了一个更加灵活高效的方法论。

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [51] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 该论文提出了一种后训练方法，通过灵活的稀疏正则化在保持模型性能的同时，大幅减少Transformer注意力机制中的连接数，使得模型更加结构化和可解释。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在探索如何在不影响模型性能的前提下简化Transformer模型的注意力机制，使其更加高效、结构化且易于理解。

Method: 采用了一种灵活的稀疏正则化技术结合约束损失目标函数，在高达10亿参数量的模型上应用，以实现将注意力机制中的连接数量减少至原始的约0.3%而不损害预训练效果。

Result: 实验证明了即使显著降低注意力机制的连通性，模型仍能保持原有的预训练损失；同时观察到任务特定电路涉及的组件数量大大减少，连接这些组件的边数最多减少了100倍。

Conclusion: 研究表明，通过引入稀疏性作为结构先验，不仅可以保留Transformer的能力，还能暴露更有序、更具解释性的连接模式，表明当前许多计算可能是多余的，并指出稀疏性可能成为未来开发更结构化与可解释模型的一个指导原则。

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [52] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: 本文介绍了一种基于机器学习的方法来预测适用于太空环境的液体润滑剂蒸气压，通过结合高通量分子动力学模拟和实验数据库的数据训练模型，并根据化学结构与蒸气压之间的关系提出了几种候选分子。


<details>
  <summary>Details</summary>
Motivation: 目前只有少数几种液体润滑剂能够满足太空真空条件下的使用要求，但它们各自存在局限性，这对移动机械组件（MMAs）的设计带来了额外限制。为了发现新的适合太空使用的液体润滑剂，需要一种有效的方法来预测潜在润滑剂的蒸气压。

Method: 采用数据驱动的机器学习方法来预测蒸气压，所用模型通过高通量分子动力学模拟及实验数据库提供的信息进行训练。特别地，这些模型设计时强调了解释性，以便于识别化学结构与蒸气压之间的联系。

Result: 成功开发了能够预测蒸气压的机器学习模型，并基于模型揭示的化学结构-蒸气压关系提出了一些可能适用于未来太空润滑剂应用的候选分子。

Conclusion: 本研究展示了一种利用机器学习技术加速寻找适用于太空环境的新型液体润滑剂的有效途径，为解决MMAs在极端条件下运行时面临的润滑挑战提供了新思路。

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [53] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于神经连贯性的方法，用于在仅有少量未标记目标任务样本的情况下进行模型选择。该方法通过比较源域和目标域的模型激活统计特征，实现了高数据效率，并在多个不同的目标任务上显著提高了泛化能力。此外，还展示了神经连贯性原则在训练数据选择中的有效性。


<details>
  <summary>Details</summary>
Motivation: 对于许多下游任务来说，微调预训练的大规模视觉模型已经成为一种常见做法。然而，在大规模训练运行产生的众多可能模型检查点中，如何最好地确定使用哪一个作为起点仍然是一个开放问题。当目标任务的数据稀缺、未标记且分布外时，这一问题尤为重要。在这种情况下，依赖于分布内验证数据的方法变得不可靠或不适用。

Method: 研究者提出了一个名为神经连贯性的新概念，该概念涉及表征模型对于源域和目标域的激活统计数据，从而允许定义具有高数据效率的模型选择方法。实验中，模型是在ImageNet1K上预训练的，并考察了包括Food-101、PlantNet-300K以及iNaturalist在内的目标领域。此外，还在多种元学习环境中对其进行了评估。

Result: 与现有基线相比，所提出的方法在不同目标领域之间显著提升了泛化能力。进一步地，通过展示其在训练数据选择中的有效性，证明了神经连贯性作为一个强有力的原则的多功能性。

Conclusion: 这项工作介绍了一种新颖的方法来解决小样本条件下难以可靠选择合适预训练模型的问题。提出的神经连贯性不仅能够有效提升跨领域的泛化性能，而且在训练数据选择方面也表现出色。

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [54] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法KQ-SVD，用于直接对注意力矩阵进行最优低秩分解，解决了之前方法在压缩键值缓存时忽略查询和键内积依赖的问题。该方法在LLaMA和Mistral模型上的评估表明了其在保持注意力输出保真度的同时提供了更好的压缩效果。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批量大小的增加，基于Transformer的大语言模型中的键值缓存成为了一个主要的内存瓶颈。现有压缩方法要么只对键单独应用低秩分解，要么尝试联合嵌入查询和键，但这些方法都忽略了注意力机制本质上依赖于查询和键之间的内积这一事实。

Method: 提出了KQ-SVD方法，通过封闭解的形式直接执行注意力矩阵的最佳低秩分解。这种方法旨在解决冗余的真实来源，从而在压缩条件下以更高的保真度保留注意力输出。

Result: 通过对LLaMA和Mistral模型进行广泛评估，发现所提出的方法能够持续提供优于现有技术的投影质量。

Conclusion: KQ-SVD作为一种针对注意力矩阵特性的压缩方案，为处理大型语言模型中的键值缓存问题提供了一种有效手段，在保证计算效率的同时提高了压缩后注意力输出的准确性。

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [55] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: 本文介绍了使用机器学习模型来构建基于年度商业调查（ABS）的合成公共使用微观数据样本（PUMS），并讨论了各种质量度量。通过与2007年企业主调查类似的数据进行计量经济学复制，验证了合成数据的真实性和潜在应用案例。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的提高和大数据的普及，匿名化数据被重新识别的风险增加，这可能违反对调查受访者的保密承诺。因此，需要开发一种方法来创建既保留原始数据关键特性又不包含任何实际个体或企业记录的合成数据。

Method: 采用机器学习模型构建了一个基于年度商业调查(ABS)的合成PUMS，并探讨了几种质量评估指标。此外，还利用与ABS业务数据相似的2007年企业主调查中的两个合成PUMS进行了高影响力分析的计量经济复制。

Result: 尽管当前ABS PUMS正在改进中且结果保密，但通过与真实数据高度相似的合成数据成功地复现了一项重要的经济学分析，证明了合成数据的有效性及其在ABS应用场景下的潜力。

Conclusion: 本研究展示了一种利用数据科学技术生成高质量合成数据的方法，为解决隐私保护问题提供了新的思路，同时也为未来基于ABS等商业调查数据的研究开辟了道路。

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>


### [56] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: 提出了一种名为MaxShapley的有效算法，用于基于检索增强生成（RAG）的生成式搜索引擎中的公平归属。该方法通过分解最大和效用函数，以线性计算成本实现与精确Shapley值相似的归属质量，同时大幅减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型（LLMs）的生成式搜索引擎逐渐取代传统搜索方式，信息提供者的补偿机制也需要相应改变。为了维持这一生态系统，需要有公平的机制来根据内容提供者对生成答案的贡献进行归属和补偿。

Method: 开发了MaxShapley算法，这是一种特别情况下的Shapley值应用，通过利用可分解的最大-总和效用函数来计算归属分数，其计算复杂度为文档数量的线性级别，而非Shapley值所需的指数级成本。

Result: 在三个多跳QA数据集上的评估表明，MaxShapley能够达到与准确Shapley值计算相当的归属质量，而且在保持相同归属准确性的同时，相较于先前最先进的方法，最多可将资源消耗降低8倍。

Conclusion: MaxShapley为生成式搜索管道提供了一种高效且公平的内容归属解决方案，对于确保内容提供者得到适当补偿具有重要意义。

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [57] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过使用α-散度族来逼近一个明确的目标分布，从而在保持大型语言模型（LLMs）多样性的同时进行强化学习调优。这种方法在Lean定理证明基准测试中实现了最先进的性能，在覆盖范围和精度之间取得了良好的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法虽然能有效地调整大型语言模型（LLMs）以解决涉及推理的任务，但这些模型往往在多样性方面损失较大。这是因为强化学习倾向于优化逆KL散度到目标分布，导致模型过于集中在某些高概率区域而忽视其他部分。

Method: 作者首先定义了一个通过过滤掉错误答案同时保留正确答案相对概率得到的显式目标分布。然后从预训练的大型语言模型出发，采用α-散度族对这一目标分布进行逼近。该方法统一了先前的方法，并允许通过对模式寻求与质量覆盖之间的散度进行插值直接控制精确度-多样性权衡。

Result: 在Lean定理证明基准上评估时，所提方法在覆盖范围-精度帕累托前沿线上达到了最先进水平，尤其在覆盖轴上超越了所有以前的方法。

Conclusion: 本研究为如何在增强学习过程中保持大型语言模型输出多样性提供了一种新途径，通过使用α-散度族能够有效平衡模型输出的覆盖范围与精确度。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [58] [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)
*Chengjie Ma,Seungeun Oh,Jihong Park,Seong-Lyun Kim*

Main category: cs.DC

TL;DR: FedGMR, a method for federated learning, progressively increases the density of client sub-models during training to improve the participation of Bandwidth-Constrained Clients. It also introduces a mask-aware aggregation rule for asynchronous and heterogeneous model settings, leading to faster convergence and higher accuracy under high heterogeneity and non-IID conditions.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) faces challenges in heterogeneous environments where Bandwidth-Constrained Clients (BCCs) with limited communication capacity cannot effectively participate, leading to slower convergence and reduced generalization. The motivation is to enable BCCs to contribute more effectively throughout the FL process by addressing their under-parameterization issue as training progresses.

Method: FedGMR, or Federated Learning with Gradual Model Restoration, gradually increases the complexity (density) of each client's sub-model during the training process. This approach also includes a mask-aware aggregation rule designed for asynchronous and model-heterogeneous federated learning (MHFL).

Result: Experiments on FEMNIST, CIFAR-10, and ImageNet-100 show that FedGMR leads to faster convergence and better accuracy, particularly in scenarios with high heterogeneity and non-independent and identically distributed (non-IID) data. The method provides convergence guarantees, demonstrating that the aggregated error is related to the average sub-model density across clients and rounds, and that GMR reduces this error towards the level of full-model FL.

Conclusion: FedGMR addresses the limitations faced by Bandwidth-Constrained Clients in federated learning by dynamically adjusting the sub-model densities and incorporating a specialized aggregation rule, which together result in improved performance and robustness in highly heterogeneous and non-IID settings.

Abstract: Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

</details>


### [59] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: 本研究探讨了基于公交车的边缘服务器在车联网中的可行性，通过上海公交/出租车/电信数据集分析了公交车和基站的覆盖情况，并提出了一种贪心启发式算法来选择有限数量的公交车以最大化需求点的覆盖。仿真结果表明该方法能有效处理动态用户需求。


<details>
  <summary>Details</summary>
Motivation: 固定位置的边缘服务器如路边单元（RSU）或基站虽然可以为车辆提供基本服务覆盖，但其位置和服务能力在部署后是固定的，难以应对时空上的用户动态变化。移动服务器如公交车则有潜力为系统增加计算弹性。

Method: 首先利用上海公交、出租车及电信的真实数据集调查了公交车与基站的覆盖范围；接着构建了一个数学模型并设计了一种简单的贪心启发式算法，旨在从有限预算内挑选出能够最大化需求点覆盖的若干辆公交车；最后通过基于轨迹驱动的仿真实验验证所提公交车选择算法的表现。

Result: 研究表明基于公交车的边缘服务器具有很大的潜力，因为它们覆盖了大量的地理区域和需求点。仿真结果显示，所提出的方案能够在现实约束条件下（如服务器容量和购买数量）有效地处理动态用户需求。

Conclusion: 对于城市区域内的车联网来说，安装在公交车上的边缘服务器不仅可行而且非常有益且有价值。

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [60] [RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering](https://arxiv.org/abs/2512.05119)
*Rongyang Zhang,Yuqing Huang,Chengqiang Lu,Qimeng Wang,Yan Gao,Yi Wu,Yao Hu,Yin Xu,Wei Wang,Hao Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 本文提出了一种新的基准RAG-IGBench，用于评估基于检索增强生成的图文交错生成任务。该基准引入了最新的社交平台内容，并提出了创新的评价指标来衡量文本和图像的质量及其一致性。通过广泛的实验，验证了这些模型的能力和局限性，并证明了所提评价指标与人工评估的高度相关性。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究在视觉自回归模型等方面取得了进展，但生成高质量的图文交错内容仍然具有挑战性。此外，对这种交错序列的评估大部分尚未得到充分探索，现有的基准往往受限于单模态度量标准，无法充分评估结合图像-文本输出的复杂性。为了解决这些问题，作者们开发了RAG-IGBench，这是一个专门设计用于评估基于检索增强生成（RAG-IG）任务的全面基准。

Method: RAG-IGBench整合了多模态大型语言模型(MLLMs)与检索机制，使模型能够访问外部图文信息以生成连贯的多模态内容。它不仅利用了最新公开的社交媒体平台内容，还引入了新颖的评估指标，旨在更准确地衡量文本和图片质量及二者之间的一致性。

Result: 通过使用最先进（包括开源和专有）的MLLMs进行广泛实验后，深入分析了这些模型的能力和限制。同时，通过对RAG-IGBench训练集进行微调的模型表现出了在多个基准测试中的性能提升，进一步证实了数据集质量和实用性。

Conclusion: RAG-IGBench为评估基于检索增强的图文交错生成提供了一个有效且实用的新基准，其创新性的评估指标与人类判断高度一致。经过RAG-IGBench训练的数据集有助于提高现有模型的表现，展示了其在开放领域问答中应用的巨大潜力。

Abstract: In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.

</details>


### [61] [The Effect of Document Summarization on LLM-Based Relevance Judgments](https://arxiv.org/abs/2512.05334)
*Samaneh Mohtadi,Kevin Roitero,Stefano Mizzaro,Gianluca Demartini*

Main category: cs.IR

TL;DR: 本文研究了文本摘要对基于大型语言模型（LLMs）的判断可靠性及其在信息检索（IR）评估中的下游影响。结果显示，基于摘要的判断能够达到与全文判断相似的系统排名稳定性，但同时引入了标签分布和偏见的变化。


<details>
  <summary>Details</summary>
Motivation: 传统的信息检索系统评价依赖于人工标注相关性判断，这一过程耗时且成本高昂。虽然大型语言模型作为自动化评价者显示出与人类标注良好的一致性，但对于如何利用这些模型处理文档以提高效率并保持准确性方面仍需进一步探索。

Method: 本研究使用最先进的大型语言模型对多个TREC集合进行了实验，比较了基于完整文档与不同长度LLM生成摘要的相关性判断。此外，还考察了这些判断与人工标签的一致性、对检索效果评价的影响以及对IR系统排名稳定性的作用。

Result: 研究表明，基于摘要的相关性判断能够在一定程度上维持IR系统的排名稳定性，但同时也带来了标签分布变化及因模型和数据集而异的偏差问题。

Conclusion: 总结技术为实现更高效的大规模IR评估提供了机会，但在采用这种方法时需要考虑其可能对自动判断可靠性造成的影响。

Abstract: Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.

</details>


### [62] [A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems](https://arxiv.org/abs/2512.05411)
*Pranav Pushkar Mishra,Kranti Prakash Yeole,Ramyashree Keshavamurthy,Mokshit Bharat Surana,Fatemeh Sarayloo*

Main category: cs.IR

TL;DR: 本文提出了一种使用大型语言模型进行元数据丰富化的系统框架，以增强检索增强生成（RAG）系统中的文档检索。通过比较三种分块策略，并结合先进的嵌入技术，研究发现元数据丰富的方案在准确率和命中率上均优于仅基于内容的基线方法。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，从庞大且复杂的知识库中有效检索相关信息对于提高运营效率和做出明智决策至关重要。为了改善这一过程，提出了一个利用大型语言模型来丰富元数据的新框架，旨在提高文档检索的质量和速度。

Method: 该研究采用了一个全面而结构化的流程，包括动态生成有意义的元数据、对文档段落进行语义表示，并对比了语义、递归和朴素三种分块策略的效果。此外，还使用了TF-IDF加权嵌入等高级嵌入技术进一步优化结果。

Result: 实验结果显示，元数据丰富的方案比仅依赖于内容的方法表现更好，特别是当采用递归分块加上TF-IDF加权嵌入时，精确率达到82.5%，高于纯语义内容方法的73.3%。朴素分块策略与前缀融合达到了最高的Hit Rate@10，为0.925。

Conclusion: 这些发现证实了元数据丰富化能够提高向量聚类质量并减少检索延迟，是优化RAG系统的有效手段。本研究为企业环境下的高性能、可扩展性文档检索解决方案提供了实用见解。

Abstract: In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.

</details>


### [63] [Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms](https://arxiv.org/abs/2512.05967)
*Francesco Granata,Francesco Poggi,Misael Mongiovì*

Main category: cs.IR

TL;DR: 本研究提出了一种增强的检索增强生成（RAG）架构，通过整合来自实体链接的事实信号来提高意大利教育问答系统的准确性。实验表明，在特定领域中基于互惠排名融合的混合方案显著优于基线和跨编码器方法，而在通用领域数据集上跨编码器方法表现最佳。这些发现强调了领域适应性和混合排名策略对于提高检索增强生成中的事实精确度和可靠性的重要性，并展示了实体感知RAG系统在教育环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语义相似性的RAG系统在许多方面表现出色，但在专业领域内往往无法保证事实准确性，因为术语模糊性可能会影响检索的相关性。为了解决这一问题，该研究旨在通过引入实体链接技术来改善教育问答系统的准确性和可靠性。

Method: 研究提出了一个改进版的RAG架构，它包括一个基于Wikidata的实体链接模块，并实现了三种重新排序策略——一种混合得分加权模型、互惠等级融合以及交叉编码器重排器——以结合语义和基于实体的信息。

Result: 实验结果表明，在特定领域上下文中，基于互惠排名融合的方法相比基线和交叉编码器方法有明显优势；而在更广泛的领域数据集中，交叉编码器方法则取得了最好的成绩。

Conclusion: 研究证实了领域不匹配的影响，并强调了领域适应及采用混合排名策略对提升检索增强生成过程中的事实精度与可靠性的关键作用。此外，还揭示了实体感知型RAG系统在促进自适应且可靠的AI辅导工具开发方面的潜力。

Abstract: In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [Integrating Wearable Data into Process Mining: Event, Case and Activity Enrichment](https://arxiv.org/abs/2512.05203)
*Vinicius Stein Dani,Xixi Lu,Iris Beerepoot*

Main category: cs.DB

TL;DR: 本文探讨了将可穿戴设备数据与事件日志结合的三种方法：作为事件属性、案例属性或引入由数据衍生的新事件，并讨论了这种整合在个人生产力和福祉过程挖掘中面临的技术和概念挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过整合可穿戴设备的数据来丰富事件日志，以促进个人生产率和福祉的过程挖掘。

Method: 提出了三种整合可穿戴设备数据的方法：1) 将其视为事件属性直接关联到单个事件上；2) 作为案例属性使用每天级别的汇总分数；3) 根据可穿戴数据引入新的事件类型如睡眠时段或身体活动。并通过真实世界中的一个人的数据进行了演示，包括智能手表健康数据与数字日历应用程序提取事件之间的匹配。

Result: 展示了如何利用可穿戴技术生成的数据来增强对个人日常活动的理解，并指出了在将这些信息集成到过程挖掘分析时所遇到的实际操作和技术难题。

Conclusion: 结论强调了将可穿戴设备数据融入事件日志的重要性及其对于理解并优化个人生产力和幸福感方面的潜力，同时也指出需要进一步解决的技术和概念性障碍。

Abstract: In this short paper, we explore the enrichment of event logs with data from wearable devices. We discuss three approaches: (1) treating wearable data as event attributes, linking them directly to individual events, (2) treating wearable data as case attributes, using aggregated day-level scores, and (3) introducing new events derived from wearable data, such as sleep episodes or physical activities. To illustrate these approaches, we use real-world data from one person, matching health data from a smartwatch with events extracted from a digital calendar application. Finally, we discuss the technical and conceptual challenges involved in integrating wearable data into process mining for personal productivity and well-being.

</details>


### [65] [Featurized-Decomposition Join: Low-Cost Semantic Joins with Guarantees](https://arxiv.org/abs/2512.05399)
*Sepanta Zeighami,Shreya Shankar,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 提出了一种新的方法Featurized-Decomposition Join (FDJ)，用于执行语义连接，显著降低了使用大语言模型处理文本数据的成本，同时保持了结果的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于嵌入的语义相似性方法在过滤候选对时效果有限，因为它们可能无法可靠地预测连接结果。为了解决这一问题，并有效减少使用大型语言模型进行语义连接时的成本，提出了Featurized-Decomposition Join (FDJ) 方法。

Method: FDJ方法通过自动提取特征并将这些特征组合成合取范式逻辑表达式来有效地排除非匹配项。该方法利用大型语言模型自动提取可靠的特征并组成逻辑表达式，同时提供了输出结果上的统计保证。

Result: 实验表明，在真实世界的数据集上，与现有技术相比，FDJ方法可以将成本降低多达10倍，同时提供相同的质量保证。

Conclusion: Featurized-Decomposition Join (FDJ) 是一种有效减少大型语言模型处理大数据集中文本字段成本的方法，能够显著降低成本而不牺牲结果质量。

Abstract: Large Language Models (LLMs) are being increasingly used within data systems to process large datasets with text fields. A broad class of such tasks involves a semantic join-joining two tables based on a natural language predicate per pair of tuples, evaluated using an LLM. Semantic joins generalize tasks such as entity matching and record categorization, as well as more complex text understanding tasks. A naive implementation is expensive as it requires invoking an LLM for every pair of rows in the cross product. Existing approaches mitigate this cost by first applying embedding-based semantic similarity to filter candidate pairs, deferring to an LLM only when similarity scores are deemed inconclusive. However, these methods yield limited gains in practice, since semantic similarity may not reliably predict the join outcome. We propose Featurized-Decomposition Join (FDJ for short), a novel approach for performing semantic joins that significantly reduces cost while preserving quality. FDJ automatically extracts features and combines them into a logical expression in conjunctive normal form that we call a featurized decomposition to effectively prune out non-matching pairs. A featurized decomposition extracts key information from text records and performs inexpensive comparisons on the extracted features. We show how to use LLMs to automatically extract reliable features and compose them into logical expressions while providing statistical guarantees on the output result-an inherently challenging problem due to dependencies among features. Experiments on real-world datasets show up to 10 times reduction in cost compared with the state-of-the-art while providing the same quality guarantees.

</details>


### [66] [PETGraphDB: A Property Evolution Temporal Graph Data Management System](https://arxiv.org/abs/2512.05417)
*Jinghe Song,Zongyu Zuo,Xuelian Lin,Yang Wang,Shuai Ma*

Main category: cs.DB

TL;DR: 本文提出了一种新的数据管理系统PETGraph，专门用于处理属性演变时态图（Property Evolution Temporal Graph）的数据。通过采用有效的时态属性图数据模型和优化的存储及锁定机制，PETGraph在存储空间需求、事务吞吐量以及查询延迟方面均优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着物联网系统的发展，一类特殊的时态图——属性演变时态图正迅速增长，这类图的特点是节点或边上的属性值频繁变化而图的拓扑结构几乎不变。然而，现有的时态图管理方案并不适用于此类数据，导致了复杂的数据建模过程和低效的查询性能。

Method: 为了解决上述问题，研究人员开发了PETGraph，这是一种专为属性演变时态图设计的数据管理系统。该系统采用了有效时间时态属性图数据模型来简化数据建模，并支持具有ACID特性的事务处理。此外，还设计了一种节省空间的时态属性存储方式和细粒度多层次锁机制以提高查询性能。

Result: 实验结果显示，PETGraph平均只需要当前最佳数据管理解决方案所需存储空间的33%。同时，在HTAP工作负载下，其事务吞吐量比现有最佳方案高出平均58.8倍；而在查询延迟上则快出平均267倍。

Conclusion: PETGraph作为一种创新的数据管理系统，针对属性演变时态图提供了高效的解决方案，不仅减少了存储开销，而且显著提高了事务处理能力和查询效率。

Abstract: Temporal graphs are graphs whose nodes and edges, together with their associated properties, continuously change over time. With the development of Internet of Things (IoT) systems, a subclass of the temporal graph, i.e., Property Evolution Temporal Graph, in which the value of properties on nodes or edges changes frequently while the graph's topology barely changes, is growing rapidly. However, existing temporal graph management solutions are not oriented to the Property Evolution Temporal Graph data, which leads to highly complex data modeling and low-performance query processing of temporal graph queries. To solve these problems, we developed PETGraph, a data management system for Property Evolution Temporal Graph data. PETGraph adopts a valid-time temporal property graph data model to facilitate data modeling, supporting ACID features with transactions. To improve temporal graph query performance, we designed a space-efficient temporal property storage and a fine-granularity multi-level locking mechanism. Experimental results show that PETGraph requires, on average, only 33% of the storage space needed by the current best data management solution. Additionally, it achieves an average of 58.8 times higher transaction throughput in HTAP workloads compared to the best current solutions and outperforms them by an average of 267 times in query latency.

</details>


### [67] [Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments](https://arxiv.org/abs/2512.05453)
*Luc Moreau,Alfred Rossi,Sophie Stalla-Bourdillon*

Main category: cs.DB

TL;DR: 本文提出了一种基于RDF/SPARQL的规则系统Parajudica，用于在多个同时适用的合规框架下评估依赖于上下文的数据合规状态，并通过应用到现有的法律框架和行业标准来展示其效用。


<details>
  <summary>Details</summary>
Motivation: 面对在多个同时适用的合规框架下实施基于策略的数据访问控制(PBAC)的挑战。

Method: 开发了Parajudica，一个开放、模块化且可扩展的基于RDF/SPARQL的规则系统。

Result: 该资源及其伴随的元模型对现有法律框架及行业标准的应用证明了其价值，并为比较框架分析提供了见解。

Conclusion: Parajudica可以应用于合规政策执行、合规监控、数据发现和风险评估等领域。

Abstract: Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment.

</details>


### [68] [Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement](https://arxiv.org/abs/2512.05525)
*Nils Strassenburg,Boris Glavic,Tilmann Rabl*

Main category: cs.DB

TL;DR: 本文提出了即时模型替换（JITR）的概念，旨在通过识别大型语言模型（LLMs）中的重复任务，并将其透明地替换成成本更低的替代模型，以节省资源和能源。JITR保持了使用LLM的简便性，同时显著降低了成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着企业越来越多地依赖大型语言模型（LLMs）来自动化简单重复的任务而非开发定制机器学习模型，虽然这样减少了对模型开发专业知识的需求，但同时也带来了更高的资源和能量消耗问题。为了解决这一矛盾，作者提出了即时模型替换（JITR）的想法，目的是在维持LLMs易用性和低开发工作量的同时，通过自动替换为更节能的小型模型来大幅度节约成本与能源。

Method: JITR方法涉及两个主要步骤：首先，系统需要能够准确识别出对LLM请求中出现的重复任务；其次，在确认存在特定重复任务后，找到或创建一个性能良好且成本较低的替代模型用于该任务。文章指出，在实现JITR的过程中，模型搜索与迁移学习将是关键环节，有助于高效地发现并微调适合处理某一重复任务的模型。

Result: 通过名为Poodle的JITR原型实现，研究人员展示了对于某些示例任务来说，确实可以达到显著的成本节省效果。

Conclusion: 即时模型替换（JITR）作为一种新方法，有望在保持大型语言模型便利性的同时大幅降低其运行成本及环境影响。不过，要全面实现JITR愿景还需克服如何有效识别重复任务以及怎样快速构建或调整适用于这些任务的经济型模型等挑战。

Abstract: Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks.

</details>
