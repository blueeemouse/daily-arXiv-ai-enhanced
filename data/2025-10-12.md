<div id=toc></div>

# Table of Contents

- [cs.MM](#cs.MM) [Total: 2]


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1] [Audio-Visual Separation with Hierarchical Fusion and Representation Alignment](https://arxiv.org/abs/2510.07326)
*Han Hu,Dongheng Lin,Qiming Huang,Yuqi Hou,Hyung Jin Chang,Jianbo Jiao*

Main category: cs.MM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-supervised audio-visual source separation leverages natural correlations
between audio and vision modalities to separate mixed audio signals. In this
work, we first systematically analyse the performance of existing multimodal
fusion methods for audio-visual separation task, demonstrating that the
performance of different fusion strategies is closely linked to the
characteristics of the sound: middle fusion is better suited for handling
short, transient sounds, while late fusion is more effective for capturing
sustained and harmonically rich sounds. We thus propose a hierarchical fusion
strategy that effectively integrates both fusion stages. In addition, training
can be made easier by incorporating high-quality external audio
representations, rather than relying solely on the audio branch to learn them
independently. To explore this, we propose a representation alignment approach
that aligns the latent features of the audio encoder with embeddings extracted
from pre-trained audio models. Extensive experiments on MUSIC, MUSIC-21 and
VGGSound datasets demonstrate that our approach achieves state-of-the-art
results, surpassing existing methods under the self-supervised setting. We
further analyse the impact of representation alignment on audio features,
showing that it reduces modality gap between the audio and visual modalities.

</details>


### [2] [AV-EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Omni-modal LLMS with Audio-visual Cues](https://arxiv.org/abs/2510.07355)
*Krish Patel,Dingkun Zhou,Ajay Kankipati,Akshaj Gupta,Zeyi Austin Li,Mohul Shukla,Vibhor Narang,Sara Kofman,Zongli Ye,Grace Wang,Xiaoyu Shi,Tingle Li,Guan-Ting Lin,Kan Jen Cheng,Huang-Cheng Chou,Jiachen Lian,Gopala Anumanchipalli*

Main category: cs.MM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Emotions conveyed through voice and face shape engagement and context in
human-AI interaction. Despite rapid progress in omni-modal large language
models (LLMs), the holistic evaluation of emotional reasoning with audiovisual
cues remains limited. To address this gap, we introduce AV-EMO-Reasoning, a
benchmark designed to systematically assess emotional coherence in LLMs. The
framework leverages a curated, single- and multi-turn synthetic audiovisual
corpus with a real-world set and is assessed under continuous, categorical, and
perceptual metrics. Experiments with leading LLMs show that visual cues
reliably improve emotional coherence over audio-only baselines. Moreover, LLMs
can leverage audio-visual cues to generate more emotion-aware speech. Models
exhibit complementary strengths across metric families, indicating that
automatic scores capture facets distinct from perceptual judgments. By
releasing a systematic evaluation benchmark, AV-EMO-Reasoning offers a
reproducible standard for evaluating emotion-aware dialogue and advances toward
more natural, adaptive human-AI interaction.

</details>
