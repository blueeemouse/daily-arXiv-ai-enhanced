<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://arxiv.org/abs/2512.19758)
*Wang Bin,Ao Yang,Kedan Li,Aofan Liu,Hui Li,Guibo Luo,Weixiang Huang,Yan Zhuang*

Main category: cs.SE

TL;DR: 本研究提出了一种新的度量——注意力距离，通过利用大型语言模型的上下文分析来计算代码元素之间的注意力分数，并揭示它们内在的联系。在38个实际漏洞复现实验中，该方法比传统方法平均提高了3.43倍的测试效率。


<details>
  <summary>Details</summary>
Motivation: 当前的定向灰盒模糊测试（DGF）方法仅考虑了种子执行路径与目标位置之间的物理距离，而忽略了代码段之间的逻辑关系，这可能导致复杂二进制文件中的冗余或误导性指导，从而削弱DGF的实际效果。

Method: 引入“注意力距离”作为新指标，利用大型语言模型对代码元素间的关联进行上下文分析，以提高模糊测试过程中对于代码内在联系的理解。

Result: 在38个真实世界漏洞复制实验中，使用注意力距离代替物理距离后，在相同AFLGo配置下，测试效率平均提升了3.43倍。相较于最先进的定向模糊器DAFL和WindRanger，该方法分别实现了2.89倍和7.13倍的改进。

Conclusion: 注意力距离作为一种新颖的度量标准，能够显著增强现有定向模糊测试工具的有效性和泛化能力。

Abstract: In the domain of software security testing, Directed Grey-Box Fuzzing (DGF) has garnered widespread attention for its efficient target localization and excellent detection performance. However, existing approaches measure only the physical distance between seed execution paths and target locations, overlooking logical relationships among code segments. This omission can yield redundant or misleading guidance in complex binaries, weakening DGF's real-world effectiveness. To address this, we introduce \textbf{attention distance}, a novel metric that leverages a large language model's contextual analysis to compute attention scores between code elements and reveal their intrinsic connections. Under the same AFLGo configuration -- without altering any fuzzing components other than the distance metric -- replacing physical distances with attention distances across 38 real vulnerability reproduction experiments delivers a \textbf{3.43$\times$} average increase in testing efficiency over the traditional method. Compared to state-of-the-art directed fuzzers DAFL and WindRanger, our approach achieves \textbf{2.89$\times$} and \textbf{7.13$\times$} improvements, respectively. To further validate the generalizability of attention distance, we integrate it into DAFL and WindRanger, where it also consistently enhances their original performance. All related code and datasets are publicly available at https://github.com/TheBinKing/Attention\_Distance.git.

</details>


### [2] [Larger Is Not Always Better: Leveraging Structured Code Diffs for Comment Inconsistency Detection](https://arxiv.org/abs/2512.19883)
*Phong Nguyen,Anh M. T. Bui,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种基于CodeT5+的即时代码-注释不一致性检测方法，通过将代码变更分解为替换、删除和添加等有序修改活动序列来更有效地捕捉这些变更与过时注释之间的关联。实验表明，该方法在F1-Score上比最新模型高出最多13.54%，并且相比经过微调的大语言模型（如DeepSeek-Coder, CodeLlama 和 Qwen2.5-Coder）也有4.18%到10.94%的改进。


<details>
  <summary>Details</summary>
Motivation: 源代码与其附带注释之间的语义一致性对于程序理解、有效调试以及长期维护至关重要。当开发者修改代码但忘记更新相应注释时，就会出现注释不一致的情况，这可能误导未来的维护者并引入错误。现有的代码-注释不一致性检测方法虽然利用了大型语言模型来捕捉代码变更与过时注释间的语义关系，但往往忽略了代码演化的结构复杂性及历史变更活动，并且带来了隐私和资源挑战。

Method: 作者提出了一个基于CodeT5+骨架的即时代码-注释不一致性检测方法。此方法通过将代码变更分解成包括替换、删除和添加在内的有序修改活动序列，从而更有效地捕获这些变更与对应过时注释之间的相关性。

Result: 广泛的实验使用公开基准数据集JITDATA和CCIBENCH进行，结果表明所提方法在F1分数上最高超出最近的最先进模型13.54%，相对于经过微调的大型语言模型（如DeepSeek-Coder、CodeLlama和Qwen2.5-Coder）也取得了从4.18%到10.94%不等的提升。

Conclusion: 本研究提出的即时代码-注释不一致性检测方法，在处理代码变更与注释间的关系方面表现出色，相较于现有解决方案提供了显著性能优势。

Abstract: Ensuring semantic consistency between source code and its accompanying comments is crucial for program comprehension, effective debugging, and long-term maintainability. Comment inconsistency arises when developers modify code but neglect to update the corresponding comments, potentially misleading future maintainers and introducing errors. Recent approaches to code-comment inconsistency (CCI) detection leverage Large Language Models (LLMs) and rely on capturing the semantic relationship between code changes and outdated comments. However, they often ignore the structural complexity of code evolution, including historical change activities, and introduce privacy and resource challenges. In this paper, we propose a Just-In-Time CCI detection approach built upon the CodeT5+ backbone. Our method decomposes code changes into ordered sequences of modification activities such as replacing, deleting, and adding to more effectively capture the correlation between these changes and the corresponding outdated comments. Extensive experiments conducted on publicly available benchmark datasets-JITDATA and CCIBENCH--demonstrate that our proposed approach outperforms recent state-of-the-art models by up to 13.54% in F1-Score and achieves an improvement ranging from 4.18% to 10.94% over fine-tuned LLMs including DeepSeek-Coder, CodeLlama and Qwen2.5-Coder.

</details>


### [3] [Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?](https://arxiv.org/abs/2512.19980)
*Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 研究探索了代码语言模型在神经元层面的内部解释性，发现特定于语言的神经元和跨语言的概念层，并展示了这些发现在代码生成、克隆检测和代码摘要任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管代码语言模型在代码智能任务上表现出色，但其内部可解释性尚未得到充分探索。现有来自NLP领域的神经元可解释性技术对于源代码来说并不理想，因为编程语言具有形式化、层次化和可执行性的特点。

Method: 通过实证研究，对Llama-3.1-8B和Qwen2.5-Coder-32B两种模型使用C++、Java、Python、Go及JavaScript等多语言输入进行了分析，测量了神经元的选择性和各层在生成过程中的贡献度。

Result: 发现了专门针对个别语言的神经元以及支持通用生成的一组普遍存在的神经元；较低层主要编码特定语言的语法，而中层则捕捉到跨语言共享的语义抽象，形成了概念层。

Conclusion: 基于这些发现，在三个任务上展示了其实用价值：由神经元引导的微调以促进代码生成、通过概念层嵌入实现克隆检测、以及概念层指导下的代码摘要迁移学习，均在多语言环境中取得了持续改进的效果。

Abstract: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.

</details>


### [4] [AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration](https://arxiv.org/abs/2512.20159)
*Ruiqi Wang,Xinchen Wang,Cuiyun Gao,Chun Yong Chong,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 为了解决现有代码评估基准的局限性，提出了一种新的基于扰动的框架AXIOM，用于大规模合成代码评估基准。该方法通过规则引导的扰动和多源质量校准两个阶段，旨在创建一个具有均衡得分分布的多样化基准，以更好地反映真实世界的代码生成情况。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）作为代码评价工具时所依赖的基准存在重大局限性，包括过于粗略或主观的评分标准、以及使用不可控的数据合成方法导致的不均衡得分分布问题。这些问题妨碍了对这些模型评价能力的可靠评估。

Method: 提出了名为AXIOM的新框架，采用基于扰动的方法来大规模合成代码评价基准。AXIOM包含两个主要步骤：1) 规则引导的扰动，即利用预定义的一系列扰动规则提示LLM修改高质量程序的功能与代码质量，从而精确控制每个程序的目标分数；2) 多源质量校准，进一步确保生成的程序集具有良好的平衡性和多样性。

Result: AXIOM能够创建出跨越不同质量水平且具有良好平衡性的多样化程序集合，并简化手动注释过程。这有助于更准确地评估LLM生成代码的质量。

Conclusion: AXIOM提供了一种有效的方法来解决当前代码评估基准存在的问题，如评分标准过于简单化或主观性强等问题。它通过引入一种新颖的基于扰动的框架促进了更准确可靠的代码质量评估。

Abstract: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.
  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...

</details>


### [5] [Toward Explaining Large Language Models in Software Engineering Tasks](https://arxiv.org/abs/2512.20328)
*Antonio Vitale,Khai-Nguyen Nguyen,Denys Poshyvanyk,Rocco Oliveto,Simone Scalabrino,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 介绍了一种新的可解释性框架FeatureSHAP，旨在为软件工程任务提供更透明的大型语言模型决策过程。该框架基于Shapley值并通过系统输入扰动和任务特定相似度比较来工作，对于提高从业者对模型输出的理解和信任具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自动化软件工程任务方面取得了显著进展，但其黑盒性质阻碍了它们在高风险和安全关键领域的应用。现有方法缺乏与实践者推理软件工程制品方式相一致的领域特定解释。

Method: 提出了FeatureSHAP，一个完全自动化的、模型无关的可解释性框架，专门针对软件工程任务设计。它利用Shapley值通过系统地改变输入并进行任务特定的相似度比较来将模型输出归因于高级输入特征。

Result: 在代码生成和代码总结两个双模态软件工程任务上评估了FeatureSHAP。结果表明，与基线方法相比，FeatureSHAP能更少地关注不相关的输入特征，并产生更高保真度的解释。此外，一项涉及37名参与者的调查显示，FeatureSHAP有助于从业者更好地理解模型输出并作出更加明智的决定。

Conclusion: FeatureSHAP代表了向实用的软件工程中可解释AI迈出的重要一步，为提高大型语言模型在软件工程中的透明度和可信度提供了有力支持。

Abstract: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.

</details>


### [6] [Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation](https://arxiv.org/abs/2512.20334)
*Yuan Huang,Yukang Zhou,Xiangping Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: 该研究探讨了有缺陷的注释掉的代码（CO代码）如何影响AI编码助手（如GitHub Copilot和Cursor）生成的代码质量，发现存在这种代码时，AI更可能生成更多有缺陷的代码，即便明确指示忽略这些缺陷代码，改善效果也有限。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在代码生成中的快速发展，像GitHub Copilot和Cursor这样的AI辅助编辑器正在改变软件开发实践。然而，先前的研究主要集中在代码上下文如何影响有缺陷代码的生成，而忽视了注释掉的代码中存在的缺陷对AI生成代码的影响。

Method: 本研究通过实验评估了当存在有缺陷的注释掉的代码时，GitHub Copilot和Cursor这两个AI编码助手的表现。

Result: 实验结果显示，在上下文中包含有缺陷的注释掉的代码会导致AI编码助手生成更多缺陷代码，比例高达58.17%。即使给出了忽略这些有缺陷代码的明确指示，减少缺陷的效果也不超过21.84%。

Conclusion: 研究结果强调了提高AI编码助手鲁棒性和安全性措施的需求。

Abstract: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.
  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.

</details>


### [7] [A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems](https://arxiv.org/abs/2512.20345)
*Xiaoxue Ma,Wanwei Zhan,Jiale Chen,Yishu Li,Jacky Keung,Federica Sarro*

Main category: cs.SE

TL;DR: 该研究对专用分布式深度学习框架中的实际问题进行了大规模实证分析，揭示了分布式训练中独特的问题症状、根本原因及修复模式，并提供了可操作的建议。


<details>
  <summary>Details</summary>
Motivation: 尽管通用框架如TensorFlow和PyTorch提供了分布式能力，但要实现高级并行性往往需要大量的手动工作，这表明需要专门针对分布式深度学习设计的框架来解决现有挑战。

Method: 研究者们分析了来自DeepSpeed, Megatron-LM, 和 Colossal-AI三个专业分布式框架中的849个真实世界问题案例，构建了一个包含34种错误症状、28种根本原因以及6种修复模式的分类体系，并明确了不同分布式训练阶段的症状、原因与修复方法之间的映射关系。

Result: 发现有45.1%的错误症状是分布式框架特有的，其中设置失败、内存问题和性能异常最为常见；通信设置阶段超过95%的问题仅在分布式环境中出现；超过60%的问题可以通过版本和依赖管理、分布式功能、API以及通信调优来解决。

Conclusion: 研究结果强调了理解分布式深度学习框架特有问题的重要性，并为开发人员提供了解决这些问题的具体指导。

Abstract: In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized frameworks. This study conducts the first large-scale empirical analysis of practitioner challenges in dedicated distributed frameworks. We examine 849 real-world issues from DeepSpeed, Megatron-LM, and Colossal-AI and construct a taxonomy of 34 bug symptoms, 28 root causes, and 6 fix patterns. Crucially, we establish explicit mappings between symptoms, causes, and fixes across distributed training stages, enabling a systematic understanding of how issues emerge and are resolved. Our results show that 45.1\% of bug symptoms are unique to distributed frameworks, with setup failures, memory issues, and performance anomalies being the most prevalent. Moreover, 95\% of issues in the communication setup stage occur exclusively in distributed contexts. We also find over 60\% of cases can be resolved through version and dependency management, and distributed feature, API, and communication tuning. Based on these findings, we provide actionable implications.

</details>


### [8] [Identifying Appropriately-Sized Services with Deep Reinforcement Learning](https://arxiv.org/abs/2512.20381)
*Syeda Tasnim Fabiha,Saad Shafiq,Wesley Klewerton Guez Assunção,Nenad Medvidović*

Main category: cs.SE

TL;DR: 本文提出了一种基于深度强化学习的方法Rake，用于从实现工件中直接识别适当大小的服务，以解决服务基础架构（SBA）设计中的挑战。Rake不需要特定的文档或项目人员访问，并且支持自定义目标函数来平衡模块化质量和业务能力对齐。在四个开源遗留项目上的应用表明，与两种最先进的技术相比，Rake平均实现了更高的模块化质量以及更强的业务能力对齐。


<details>
  <summary>Details</summary>
Motivation: 服务基础架构(SBA)作为一种现代化遗留系统的方式受到了业界和学术界的关注。然而，确定能够捕捉到系统功能连贯子集的合适规模的服务仍然是一个挑战。现有工作通常依赖于文档可用性、项目人员访问权或者对目标服务数量的事先了解，这些假设在许多实际场景中并不成立。

Method: 本文介绍了一种名为Rake的基于强化学习的技术，该技术利用可用的系统文档和源代码来指导实施方法级别的服务分解。Rake不需要具体的文档或项目人员访问，并且是语言无关的。它还支持可定制的目标函数，以平衡模块化质量和业务能力对齐，即服务覆盖目标业务能力的程度。

Result: Rake被应用于四个开源遗留项目，并与两种最先进技术进行了比较。结果表明，Rake平均达到了7-14%更高的模块化质量及18-22%更强大的业务能力对齐度。进一步的结果显示，仅针对业务上下文进行优化可能会降低紧密耦合系统的分解质量，强调了平衡目标的重要性。

Conclusion: 研究证明了Rake在提高模块化质量和服务对齐度方面的有效性，同时指出在紧密耦合系统中单独优化业务背景可能损害分解质量，强调了采用平衡目标策略的重要性。

Abstract: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [9] [Risk-Aware GPU-Assisted Cardinality Estimation for Cost-Based Query Optimizers](https://arxiv.org/abs/2512.19750)
*Ilsun Chang*

Main category: cs.DB

TL;DR: 提出了一种名为GACE（GPU辅助基数估计）的混合辅助架构，该架构通过仅在风险区间内调用基于GPU的测量来增强优化器，从而在保持稳定区域低开销的同时提高了计划稳定性并减少了问题场景下的尾部延迟。


<details>
  <summary>Details</summary>
Motivation: 现实工作负载经常违反静态统计背后的假设，导致决策稳定性降低和计划翻转率增加。

Method: GACE通过一个能够检测估计不确定性的Risky Gate选择性地仅在风险区间内调用基于GPU的测量，并且有一个GPU测量引擎执行高速探测同时明确考虑测量本身的成本。

Result: GACE设计在稳定区域内保持了较低的开销，同时改善了计划稳定性，并在存在问题的情况下降低了尾部延迟(P99)。

Conclusion: GACE为成本基础优化器提供了一个有效的补充方案，旨在解决由于陈旧统计数据、偏斜、连接相关性、绑定变量中的隐藏分布以及采样偏差所引起的问题。

Abstract: Cardinality estimation is a cornerstone of cost-based optimizers (CBOs), yet real-world workloads often violate the assumptions behind static statistics, degrading decision stability and increasing plan flip rates. We empirically characterize failures caused by stale statistics, skew, join correlations, hidden distributions in bind variables, and sampling bias, and quantify the overhead and break-even points of hardware-accelerated measurement.
  We propose GACE (GPU-Assisted Cardinality Estimation), a hybrid auxiliary architecture that augments rather than replaces the optimizer. GACE selectively invokes GPU-based measurement only in risky intervals via a Risky Gate that detects estimation uncertainty, and a GPU Measurement Engine that performs high-speed probing with explicit cost accounting for the measurement itself. This design preserves low overhead in stable regions while improving plan stability and reducing tail latency (P99) in problematic scenarios.

</details>


### [10] [Automated Training of Learned Database Components with Generative AI](https://arxiv.org/abs/2512.20271)
*Angjela Davitkova,Sebastian Michel*

Main category: cs.DB

TL;DR: 本文探讨了使用生成模型（如GPT）为数据库学习组件合成训练数据的可能性，并初步研究表明这种方法可以有效增强训练数据集，从而提高学习型数据库技术的适应性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在数据库优化中的应用面临着获取高质量训练数据的挑战。为了克服这一难题，研究转向探索利用生成模型来合成适合于学习型数据库组件的训练数据。

Method: 本研究采用了一种初步可行性分析的方法，考察生成模型生产现实查询分布和执行计划的能力，同时讨论了包括数据可扩展性和标注在内的关键挑战及可能的解决方案。

Result: 初步结果表明，生成模型能够有效地补充训练数据集，有助于提升学习型数据库技术对不同工作负载的适应能力。

Conclusion: 通过使用生成模型合成训练数据，可以为学习型数据库组件提供更丰富、多样化的训练材料，进而改善其性能与适应性。

Abstract: The use of deep learning for database optimization has gained significant traction, offering improvements in indexing, cardinality estimation, and query optimization. However, acquiring high-quality training data remains a significant challenge. This paper explores the possibility of using generative models, such as GPT, to synthesize training data for learned database components. We present an initial feasibility study investigating their ability to produce realistic query distributions and execution plans for database workloads. Additionally, we discuss key challenges, such as data scalability and labeling, along with potential solutions. The initial results suggest that generative models can effectively augment training datasets, improving the adaptability of learned database techniques.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Large Language Models for EDA Cloud Job Resource and Lifetime Prediction](https://arxiv.org/abs/2512.19701)
*Yuxuan Yin,Shengke Zhou,Yunjie Zhang,Ajay Mohindra,Boxun Xu,Peng Li*

Main category: cs.LG

TL;DR: 提出了一种新的框架，通过微调大型语言模型（LLMs）来提高EDA行业中云计算资源和作业生命周期预测的准确性，并通过实际云数据集验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 在电子设计自动化（EDA）行业中，云计算的快速增长导致了对资源和作业生命周期预测的迫切需求，以实现最佳调度。然而，传统机器学习方法往往难以处理EDA工作负载的复杂性和异构性。

Method: 本文提出了一种新框架，该框架通过对大型语言模型（LLMs）进行微调来进行文本到文本回归，以此解决上述问题。此外，还引入了科学记数法和前缀填充技术来约束LLM，从而大大提高了输出格式的可靠性；同时发现全注意力微调和推理能够提高滑动窗口注意力机制LLMs的预测准确性。

Result: 实验结果表明，在真实世界的云数据集上，所提出的框架对于EDA领域的性能预测设定了一个新的基准。

Conclusion: 本研究为EDA行业中的云计算提供了更准确的资源与任务生命周期预测解决方案，展示了基于LLM的方法在处理复杂EDA工作负载方面的潜力。

Abstract: The rapid growth of cloud computing in the Electronic Design Automation (EDA) industry has created a critical need for resource and job lifetime prediction to achieve optimal scheduling. Traditional machine learning methods often struggle with the complexity and heterogeneity of EDA workloads, requiring extensive feature engineering and domain expertise. We propose a novel framework that fine-tunes Large Language Models (LLMs) to address this challenge through text-to-text regression. We introduce the scientific notation and prefix filling to constrain the LLM, significantly improving output format reliability. Moreover, we found that full-attention finetuning and inference improves the prediction accuracy of sliding-window-attention LLMs. We demonstrate the effectiveness of our proposed framework on real-world cloud datasets, setting a new baseline for performance prediction in the EDA domain.

</details>


### [12] [Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches](https://arxiv.org/abs/2512.19713)
*Taoran Sheng,Manfred Huber*

Main category: cs.LG

TL;DR: 本文对基于可穿戴设备的人类活动识别（HAR）进行了全面的研究，提出并比较了多种学习方法，旨在减少标签需求的同时保持竞争力的准确性。特别是弱自监督学习框架，在仅使用10%标记数据的情况下展示了显著效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决在人类活动识别中完全监督技术所需的大量标记数据成本高昂的问题，同时探索减少标记需求而不大幅牺牲性能的方法。

Method: 开发并实证比较了六种方法：传统全监督学习、基础无监督学习、约束条件下的弱监督学习、具有知识共享的多任务学习、基于领域专长的自我监督方法以及一种新的结合领域知识和少量标记数据的弱自监督学习框架。

Result: 实验表明，提出的弱监督方法在显著降低监督要求的同时达到了与全监督方法相当的表现；多任务框架通过相关任务之间的知识共享提高了表现；而弱自监督方法仅用10%的标记数据就显示出了极高的效率。

Conclusion: 不同学习范式各有优势，根据标记数据可用性的差异可以定制HAR解决方案。本研究的新弱自监督框架为标记数据有限的实际HAR应用提供了有前景的解决方案。

Abstract: Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.

</details>


### [13] [Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data](https://arxiv.org/abs/2512.19716)
*Behrooz Mamandipoor,Chun-Nan Hsu,Martin Krause,Ulrich H. Schmidt,Rodney A. Gabriel*

Main category: cs.LG

TL;DR: 本研究开发了一种多模态深度学习模型，利用结构化和非结构化临床数据预测重症患者在入院24小时后的院内死亡风险。模型在MIMIC-III、MIMIC-IV、eICU和HiRID数据集上进行训练，并在外部数据集上进行了验证，显示了良好的预测性能。整合临床笔记和影像资料后，模型的表现进一步提升。


<details>
  <summary>Details</summary>
Motivation: 早期预测重症患者的院内死亡率可以帮助临床医生优化治疗方案。

Method: 采用来自MIMIC-III, MIMIC-IV, eICU, 和 HiRID的数据集，构建了一个基于深度学习的多模态模型，该模型使用了入住ICU头24小时内的时间序列成分来预测随后的住院死亡风险。输入包括时间不变变量、时间变化变量、临床笔记以及胸部X光图像。

Result: 仅使用结构化数据点的模型AUROC、AUPRC和Brier得分分别为0.92、0.53和0.19。当加入临床笔记和影像资料时，这些指标分别提高到了0.89、0.48和0.17。

Conclusion: 研究表明结合多种来源的患者信息对于死亡率预测至关重要，同时强调了外部验证的重要性。

Abstract: Early prediction of in-hospital mortality in critically ill patients can aid clinicians in optimizing treatment. The objective was to develop a multimodal deep learning model, using structured and unstructured clinical data, to predict in-hospital mortality risk among critically ill patients after their initial 24 hour intensive care unit (ICU) admission. We used data from MIMIC-III, MIMIC-IV, eICU, and HiRID. A multimodal model was developed on the MIMIC datasets, featuring time series components occurring within the first 24 hours of ICU admission and predicting risk of subsequent inpatient mortality. Inputs included time-invariant variables, time-variant variables, clinical notes, and chest X-ray images. External validation occurred in a temporally separated MIMIC population, HiRID, and eICU datasets. A total of 203,434 ICU admissions from more than 200 hospitals between 2001 to 2022 were included, in which mortality rate ranged from 5.2% to 7.9% across the four datasets. The model integrating structured data points had AUROC, AUPRC, and Brier scores of 0.92, 0.53, and 0.19, respectively. We externally validated the model on eight different institutions within the eICU dataset, demonstrating AUROCs ranging from 0.84-0.92. When including only patients with available clinical notes and imaging data, inclusion of notes and imaging into the model, the AUROC, AUPRC, and Brier score improved from 0.87 to 0.89, 0.43 to 0.48, and 0.37 to 0.17, respectively. Our findings highlight the importance of incorporating multiple sources of patient information for mortality prediction and the importance of external validation.

</details>


### [14] [Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference](https://arxiv.org/abs/2512.19717)
*Zhan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为逆因果聚焦算法（ICFA）的框架，该框架将搜索视为目标条件重加权过程。通过利用现有的提议采样器和特定任务相似性函数来形成集中采样分布，并自适应地控制聚焦强度以避免退化。文章还提供了理论解释、稳定性诊断方法以及两个可重复实验：受限语言生成和稀疏奖励导航。此外，讨论了结构化提示如何实例化近似于语言级别的ICFA，并描述了一种结合提示推理与算法重加权的混合架构。


<details>
  <summary>Details</summary>
Motivation: 在非常大的候选空间中找到罕见但有用的解决方案是语言生成、规划及强化学习领域面临的常见挑战。为了解决这一问题，研究者们提出了逆因果聚焦算法（ICFA），旨在提高搜索效率同时避免陷入局部最优解。

Method: ICFA采用目标条件下的重加权策略，利用已有的提案抽样器和针对具体任务设计的相似度函数构建了一个更集中的抽样分布；并且能够自适应地调整聚焦程度以防止单一化。该方法还包括基于有效样本量的稳定性诊断工具以及何时ICFA可以减少样本需求的简洁理论概述。

Result: 通过受限制的语言生成与稀疏回报导航两项可复制实验验证了ICFA的有效性。结果表明，在这两个场景下，ICFA均能显著降低所需样本数量，从而提高解决问题的效率。此外，研究还展示了结构化提示如何实现一种接近语言层面的ICFA形式，并提出了一种结合了提示推断与算法重加权的混合体系结构。

Conclusion: 逆因果聚焦算法（ICFA）为在大空间内寻找罕见而有价值的解提供了一种有效的方法。它不仅提高了搜索效率，还通过自适应调节防止了过拟合现象的发生。此方法在多个应用场景中表现出色，显示出了良好的泛化能力。

Abstract: Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.

</details>


### [15] [Per-Axis Weight Deltas for Frequent Model Updates](https://arxiv.org/abs/2512.19720)
*Stefan Kuyumdzhiev,Radostin Cholakov*

Main category: cs.LG

TL;DR: 本文提出了一种1比特差分方案来压缩微调后的权重，通过仅存储权重差异的符号和轻量级的每轴FP16缩放因子，从而减少了冷启动延迟和存储开销，同时保持了推理效率。


<details>
  <summary>Details</summary>
Motivation: 由于服务许多任务特定的大规模语言模型变体时经常受到微调检查点大尺寸以及由此产生的冷启动延迟限制，作者希望通过一种更有效的表示方法来解决这一问题。

Method: 提出了一种简单的1比特差分方案，该方案只存储权重差异的符号，并结合从少量校准集中学习到的轻量级每轴（行/列）FP16缩放因子。设计上保持了1比特差分的紧凑性，同时更准确地捕捉了权重维度上的变化。此外，还引入了一个简化加载器，它能以每个模块单次操作的方式传输打包好的差分数据，以此减少冷启动延迟和存储开销。

Result: 该方法相比标量替代方案提高了重建质量，生成的工件比完整的FP16检查点小几倍。而且，这种方法是即插即用型的，只需要最少的校准数据，并且通过避免密集重建来维持推理效率。

Conclusion: 通过采用所提出的1比特差分方案与简化的加载机制相结合的方法，可以有效地降低微调后大规模语言模型的服务成本，包括存储需求和冷启动时间，同时保持良好的推理性能。

Abstract: Serving many task-specialized LLM variants is often limited by the large size of fine-tuned checkpoints and the resulting cold-start latency. Since fine-tuned weights differ from their base model by relatively small structured residuals, a natural approach is to represent them as compressed deltas. We propose a simple 1-bit delta scheme that stores only the sign of the weight difference together with lightweight per-axis (row/column) FP16 scaling factors, learned from a small calibration set. This design preserves the compactness of 1-bit deltas while more accurately capturing variation across weight dimensions, leading to improved reconstruction quality over scalar alternatives. From a systems perspective, a streamlined loader that transfers packed deltas in a single operation per module reduces cold-start latency and storage overhead, with artifacts several times smaller than a full FP16 checkpoint. The method is drop-in, requires minimal calibration data, and maintains inference efficiency by avoiding dense reconstruction. Our experimental setup and source code are available at https://github.com/kuiumdjiev/Per-Axis-Weight-Deltas-for-Frequent-Model-Updates.

</details>


### [16] [Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals](https://arxiv.org/abs/2512.19721)
*Vineet Yadav*

Main category: cs.LG

TL;DR: 本文介绍了一种符号感知的多状态Jaccard/Tanimoto框架，该框架能够处理任意实数和复数值信号，并保持有界度量和半正定核结构。通过将信号表示为带符号状态空间上的原子测度，相似性由这些测度的一般化Jaccard重叠给出。此方法支持科学和金融应用中的相关图、特征工程、相似性图等分析工具。


<details>
  <summary>Details</summary>
Motivation: 为了扩展基于重叠的距离从非负向量和测量到任意实值和复数值信号，同时保留有界度量和半正定核结构，提出了一个符号感知的多状态Jaccard/Tanimoto框架。

Method: 通过将信号嵌入到非负多状态表示中，使用正/负分割来处理实信号，笛卡尔和极坐标分解来处理复信号，以及用户定义的状态划分来进行细化区域分析。对这些嵌入应用Tanimoto构建生成了满足三角不等式的[0,1]距离家族，并定义了可以直接在内核方法和基于图的学习中使用的半正定核。此外，还开发了通过Möbius反演进行联盟分析的方法，该方法将信号幅度分解为具有跨信号联盟精确预算闭合的非负加性贡献。

Result: 所提出的构造提供了一个单一的、机制上可解释的距离，它同时提供了有界度量结构、半正定核、概率语义和透明预算核算，支持科学和金融应用中的相关图、特征工程、相似性图等分析工具。

Conclusion: 这种新的符号感知多状态Jaccard/Tanimoto框架能够在单个框架内为科学和金融领域提供一种综合解决方案，支持多种数据分析需求。

Abstract: We introduce a sign-aware, multistate Jaccard/Tanimoto framework that extends overlap-based distances from nonnegative vectors and measures to arbitrary real- and complex-valued signals while retaining bounded metric and positive-semidefinite kernel structure. Formally, the construction is a set- and measure-theoretic geometry: signals are represented as atomic measures on a signed state space, and similarity is given by a generalized Jaccard overlap of these measures. Each signal is embedded into a nonnegative multistate representation, using positive/negative splits for real signals, Cartesian and polar decompositions for complex signals, and user-defined state partitions for refined regime analysis. Applying the Tanimoto construction to these embeddings yields a family of $[0,1]$ distances that satisfy the triangle inequality and define positive-semidefinite kernels usable directly in kernel methods and graph-based learning. Beyond pairwise distances, we develop coalition analysis via Möbius inversion, which decomposes signal magnitude into nonnegative, additive contributions with exact budget closure across coalitions of signals. Normalizing the same embeddings produces probability measures on coordinate -- state configurations, so that the distance becomes a monotone transform of total variation and admits a regime -- intensity decomposition. The resulting construction yields a single, mechanistically interpretable distance that simultaneously provides bounded metric structure, positive-semidefinite kernels, probabilistic semantics, and transparent budget accounting within one sign-aware framework, supporting correlograms, feature engineering, similarity graphs, and other analytical tools in scientific and financial applications.

</details>


### [17] [Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism](https://arxiv.org/abs/2512.19722)
*Alessandro Casadei,Clemens Grupp,Sreyoshi Bhaduri,Lu Guo,Wilson Fung,Rohit Malshe,Raj Ratan,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: 本研究介绍了一种根据节点特定成本函数的不对称性调整预测的方法，通过动态地将成本不对称性纳入预测误差概率分布中来产生节省。模型能够适应特定站点条件和未建模因素，并且实证结果表明该模型每年可实现510万美元的节省。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过考虑成本函数的不对称性来改进预测准确性，并且在实际应用中能够根据具体情况自我调节以应对诸如校准错误或宏观经济变化等未建模因素的影响。

Method: 提出的方法包括动态整合成本不对称性到预测误差的概率分布中，以及采用一种自我调节机制根据观察到的成本节约情况调整修正幅度。

Result: 实验结果显示，所提模型能够有效适应不同站点的具体条件及未被模型化的因素影响，最终实现了每年约510万美元的成本节约。

Conclusion: 结论指出，通过结合成本不对称性的动态调整预测方法可以显著提高经济效益，在面对复杂多变的实际环境时仍能保持良好表现。

Abstract: This work introduces a methodology to adjust forecasts based on node-specific cost function asymmetry. The proposed model generates savings by dynamically incorporating the cost asymmetry into the forecasting error probability distribution to favor the least expensive scenario. Savings are calculated and a self-regulation mechanism modulates the adjustments magnitude based on the observed savings, enabling the model to adapt to station-specific conditions and unmodeled factors such as calibration errors or shifting macroeconomic dynamics. Finally, empirical results demonstrate the model's ability to achieve \$5.1M annual savings.

</details>


### [18] [End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment](https://arxiv.org/abs/2512.19723)
*Firas Bayram,Bestoun S. Ahmed,Erik Hallin*

Main category: cs.LG

TL;DR: 本文提出了一种将数据质量评估与机器学习模型操作在实时生产环境中高效集成的新颖端到端框架，通过结合动态漂移检测、自适应数据质量指标和MLOps，形成了一个连贯且轻量级的系统。在钢铁制造公司的电渣重熔真空泵过程中验证了该框架，展示了模型性能提高了12%（R2 = 94%），预测延迟减少了四倍。


<details>
  <summary>Details</summary>
Motivation: 现有的方法将数据质量评估和机器学习系统视为孤立的过程，而本文旨在解决理论方法与实际实现之间的关键差距，通过整合这些过程来提高效率并减少计算开销。

Method: 提出了一个集成了动态漂移检测、自适应数据质量度量标准以及MLOps实践的轻量级系统框架。

Result: 在钢铁制造案例研究中，该框架使得模型性能提升了12%，同时预测延迟降低了四倍。通过对数据质量可接受阈值的影响进行探索，为工业应用中平衡数据质量标准和预测表现提供了实用见解。

Conclusion: 本框架代表了MLOps领域的一项重要进步，为动态工业环境下的时间敏感型、数据驱动决策提供了一个强大的解决方案。

Abstract: This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.

</details>


### [19] [Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking](https://arxiv.org/abs/2512.19725)
*Srishti Gupta,Riccardo Balia,Daniele Angioni,Fabio Brau,Maura Pintor,Ambra Demontis,Alessandro Sebastian,Salvatore Mario Carta,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 该论文讨论了持续学习（CL）和分布外检测（OOD）在不断变化的实际应用场景中的重要性，指出现有机器学习模型通常基于独立同分布假设开发，这限制了它们对新条件的适应能力。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型被部署到更多变的实际场景中，保持这些模型随时间推移的可靠性和适应性变得越来越重要。现有方法多基于训练与测试数据独立同分布的假设，但在现实世界中这一假设往往不成立。

Method: 未明确提及具体方法，但强调了持续学习（允许模型从不断演化的数据流中逐步学习而不遗忘过去的知识）和分布外检测（使系统能够识别并对新颖或异常输入作出反应）的重要性。

Result: 文章指出，为了开发出健壮、高效且能适应环境变化的人工智能系统，必须同时解决持续学习和分布外检测两大挑战。

Conclusion: 针对动态变化环境下的机器学习应用，需要发展新的策略来提高模型对于未知情况的处理能力和长期学习效率。

Abstract: Recent years have witnessed significant progress in the development of machine learning models across a wide range of fields, fueled by increased computational resources, large-scale datasets, and the rise of deep learning architectures. From malware detection to enabling autonomous navigation, modern machine learning systems have demonstrated remarkable capabilities. However, as these models are deployed in ever-changing real-world scenarios, their ability to remain reliable and adaptive over time becomes increasingly important. For example, in the real world, new malware families are continuously developed, whereas autonomous driving cars are employed in many different cities and weather conditions. Models trained in fixed settings can not respond effectively to novel conditions encountered post-deployment. In fact, most machine learning models are still developed under the assumption that training and test data are independent and identically distributed (i.i.d.), i.e., sampled from the same underlying (unknown) distribution. While this assumption simplifies model development and evaluation, it does not hold in many real-world applications, where data changes over time and unexpected inputs frequently occur. Retraining models from scratch whenever new data appears is computationally expensive, time-consuming, and impractical in resource-constrained environments. These limitations underscore the need for Continual Learning (CL), which enables models to incrementally learn from evolving data streams without forgetting past knowledge, and Out-of-Distribution (OOD) detection, which allows systems to identify and respond to novel or anomalous inputs. Jointly addressing both challenges is critical to developing robust, efficient, and adaptive AI systems.

</details>


### [20] [Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning](https://arxiv.org/abs/2512.20363)
*Daniel M. Jimenez-Gutierrez,Mehrdad Hassanzadeh,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的个性化联邦学习框架Clust-PSI-PFL，通过使用人口稳定性指数(PSI)量化非独立同分布数据的程度，并形成分布均匀的客户端组，从而提高了全局准确性和客户端公平性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习支持通过将数据保留在客户端设备上来进行隐私保护和分散式的机器学习模型训练。然而，跨客户端的非独立同分布（non-IID）数据会导致更新偏差并降低性能。为了解决这些问题，提出了一个基于聚类的个性化联邦学习框架。

Method: 引入了Clust-PSI-PFL框架，该框架利用人口稳定性指数(PSI)来衡量非IID数据的程度，并且计算了一个加权PSI指标$WPSI^L$，证明它比常见的非IID度量（如Hellinger、Jensen-Shannon和Earth Mover's距离）更加具有信息量。基于PSI特征，通过K-means++算法形成分布均匀的客户端组；最优簇的数量通过系统化的轮廓系数程序确定。

Result: 在六个不同模态的数据集（表格、图像和文本）、两种分割协议（参数为α的Dirichlet和参数S的相似性）以及多种客户端规模下，Clust-PSI-PFL相较于最先进基准方法最高可提升18%的全局准确性，并在严重非IID数据条件下显著提高客户公平性达37%。

Conclusion: 这些结果确立了以PSI为导向的聚类作为一种原则性的轻量级机制，用于在标签偏斜的情况下实现稳健的个性化联邦学习(PFL)。

Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.

</details>


### [21] [Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs](https://arxiv.org/abs/2512.20573)
*Rui Pan,Zhuofu Chen,Ravi Netravali*

Main category: cs.LG

TL;DR: 介绍了一种基于扩散大语言模型(dLLM)的推测解码框架FailFast，该框架通过动态调整推测长度来实现高效解码。FailFast能够在无需微调的情况下为自回归语言模型提供无损加速，并且在多种模型和工作负载上达到了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: Diffusion Large Language Models (dLLMs) 虽然能够快速并行生成令牌，但独立使用时面临效率与质量之间的权衡问题。本研究旨在展示如何通过巧妙应用dLLMs的特点，在推测解码中结合自回归验证器来克服这一挑战。

Method: 提出了一种名为FailFast的dLLM基础推测解码框架。该方法关键在于能够根据具体情况动态调整推测长度：在难以推测的部分迅速失败以减少计算开销；而在容易推测的地方则大胆延长草稿长度来降低验证延迟。

Result: 实验结果显示，FailFast可以在不进行任何微调的情况下加速自回归语言模型，相较于普通解码最高可达4.9倍速度提升，比最佳直接使用的dLLM草稿者快1.7倍，也优于EAGLE-3的表现，后者仅为1.4倍。此外，FailFast还展示了其在不同模型和任务类型上的广泛适用性。

Conclusion: 研究表明，通过适当利用dLLM的特性，可以有效提高推测解码过程中的效率与质量，FailFast作为实现这一目标的具体方案展现出了巨大潜力。

Abstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.

</details>


### [22] [Hard Negative Sample-Augmented DPO Post-Training for Small Language Models](https://arxiv.org/abs/2512.19728)
*Haocheng Lu,Minjun Zhu,Henry Yu*

Main category: cs.LG

TL;DR: 提出了一种轻量级的后训练管道，通过使用MathVerifier来识别和纠正大语言模型在数学推理中的细微错误。这种方法提高了对逻辑不一致但数值上接近正确的解题能力，同时避免了训练大型奖励模型或依赖外部评判者的开销。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在数学推理方面表现不佳，并且常用的后训练方法通常仅将生成的答案二分为正确或错误，这限制了对于链式思维（CoT）推理中结构化失败的理解与改进。此外，基于人类反馈强化学习的方法虽然有效但成本高昂、难以扩展且迭代不稳定。

Method: 1. 从MetaMathQA风格的CoT数据开始进行监督微调（SFT）。
2. 引入一个紧凑型MathVerifier，能够将候选解决方案分解为六个维度的错误概况，并汇总成可解释的错误性和荒谬性分数。
3. 利用验证器信号挖掘几乎正确却存在结构性缺陷的难例，并定义每样本的重要性权重以强调最有信息量的偏好对。
4. 将这些整合到一个离线直接偏好优化(DPO)目标中，通过验证器指导加权公式实现。

Result: 实验表明，在拥有1.5亿参数的Qwen2.5模型上，由验证器指导的加权DPO相比普通的SFT和未加权DPO带来了更针对性的改进，特别是在那些答案数值上接近正确但在逻辑上不一致的问题上。此方法还避免了训练大规模奖励模型或依赖外部裁判带来的额外负担。

Conclusion: 该研究提出的方法为提高大型语言模型在解决数学问题时的表现提供了一条新的途径，特别是针对那些容易被忽视的细小错误。通过引入MathVerifier及加权DPO策略，能够在实际计算资源限制下有效地改善模型性能。

Abstract: Large language models (LLMs) continue to struggle with mathematical reasoning, and common post-training pipelines often reduce each generated solution to a binary outcome: correct or incorrect. This perspective is limiting in practice, as failures in chain-of-thought (CoT) reasoning are frequently structured; solutions may appear convincing while containing subtle logical, algebraic, or numerical flaws. Meanwhile, reinforcement learning from human feedback (RLHF) variants that rely on large reward models or LLM-as-a-judge signals are often expensive, difficult to scale, and unstable to iterate. We propose a lightweight and pragmatic post-training pipeline that targets such structured errors under realistic compute budgets. Starting from supervised fine-tuning (SFT) on MetaMathQA-style CoT data, we introduce a compact MathVerifier that decomposes a candidate solution into a six-dimensional error profile and aggregates it into interpretable wrongness and absurdity scores. These verifier signals serve two roles: (i) mining hard negatives that are near-correct yet structurally flawed, and (ii) defining per-sample importance weights that emphasize the most informative preference pairs. We integrate both into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model show that verifier-guided, weighted DPO yields more targeted improvements than vanilla SFT and unweighted DPO, particularly on problems where solutions are numerically close to correct but logically inconsistent, while avoiding the overhead of training large reward models or relying on external judges.

</details>


### [23] [High-Performance Self-Supervised Learning by Joint Training of Flow Matching](https://arxiv.org/abs/2512.19729)
*Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种基于流匹配的基础模型（FlowFM），通过联合训练表示编码器和条件流匹配生成器，实现了高质量生成与有效识别的结合。实验表明，该方法在减少训练时间的同时，在下游任务中超越了现有最佳的自监督学习方法，并且大幅提高了推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成过程中可以学习到丰富的表示，显示出在自监督学习中的潜力，但它们面临着生成质量与判别性能之间的权衡问题。此外，迭代采样过程也带来了大量的计算和能耗成本，限制了其在工业及边缘AI应用中的实用性。为了解决这些问题，研究者们提出了一个新的解决方案。

Method: 提出的解决方案是Flow Matching-based Foundation Model (FlowFM)，它通过同时训练一个表示编码器和一个条件流匹配生成器来克服上述挑战。这种方法的设计允许模型在保持高质量生成的同时也能实现有效的识别能力。特别是，通过采用流匹配技术学习一个更简单的速度场，使得训练过程既加快又更加稳定，从而提升了表示学习的效率。

Result: 在可穿戴传感器数据上的实验结果显示，相比于基于扩散的方法，FlowFM能够将训练时间减少50.4%。对于下游任务，FlowFM不仅在所有五个测试数据集上都超过了当前最先进的自监督学习方法(SSL-Wearables)的表现，还实现了高达51.0倍的推理加速，同时保持了很高的生成质量。

Conclusion: FlowFM通过创新性地结合表示编码器与条件流匹配生成器，成功解决了传统扩散模型存在的问题，包括生成质量与判别性能之间的矛盾以及高计算成本等。这一方法不仅显著缩短了训练所需时间，还在多个下游任务中表现出色，特别是在推理速度上有大幅度提升，为自监督学习领域提供了新的方向。

Abstract: Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at https://github.com/Okita-Laboratory/jointOptimizationFlowMatching.

</details>


### [24] [ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures](https://arxiv.org/abs/2512.19730)
*Zhonghao Yang,Cheng Luo,Daojing He,Yiming Li,Yu Li*

Main category: cs.LG

TL;DR: 提出了一种新的黑盒神经后门检测方法ArcGen，通过引入对齐层和设计两个对齐损失来提取与架构无关的模型特征，以提高对未知模型架构的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的神经后门检测方法不能很好地泛化到学习阶段未见过的新架构上。

Method: 开发了名为ArcGen的方法，该方法通过添加一个额外的对齐层到特征提取函数中，并设计了两个对齐损失来训练这个函数，目的是减少架构信息对特征的直接影响，并要求具有相似后门行为但不同架构的模型在分布和样本级别上对齐其特征。

Result: 在涉及16,896个模型的大规模评估中，对于未见模型架构，本方法相比现有方法显示出了高达42.5%的检测性能提升（例如AUC）。

Conclusion: ArcGen方法能够有效地改善针对未知模型架构时的后门攻击检测表现。

Abstract: Backdoor attacks pose a significant threat to the security and reliability of deep learning models. To mitigate such attacks, one promising approach is to learn to extract features from the target model and use these features for backdoor detection. However, we discover that existing learning-based neural backdoor detection methods do not generalize well to new architectures not seen during the learning phase. In this paper, we analyze the root cause of this issue and propose a novel black-box neural backdoor detection method called ArcGen. Our method aims to obtain architecture-invariant model features, i.e., aligned features, for effective backdoor detection. Specifically, in contrast to existing methods directly using model outputs as model features, we introduce an additional alignment layer in the feature extraction function to further process these features. This reduces the direct influence of architecture information on the features. Then, we design two alignment losses to train the feature extraction function. These losses explicitly require that features from models with similar backdoor behaviors but different architectures are aligned at both the distribution and sample levels. With these techniques, our method demonstrates up to 42.5% improvements in detection performance (e.g., AUC) on unseen model architectures. This is based on a large-scale evaluation involving 16,896 models trained on diverse datasets, subjected to various backdoor attacks, and utilizing different model architectures. Our code is available at https://github.com/SeRAlab/ArcGen.

</details>


### [25] [Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis](https://arxiv.org/abs/2512.19732)
*Gaurav Kumar Sharma*

Main category: cs.LG

TL;DR: 本研究对JARVIS-DFT带隙数据集进行了系统性分析，移除了可能无意中编码了带结构信息的描述符。通过三阶段建模框架，逐步引入基础物理描述符、工程特征以及组成属性，并使用处理后的数据集进行训练。结果表明，在控制信息泄露的情况下，增加描述符空间并不显著提高预测准确性。SHAP分析显示介电张量组件是主要贡献者。


<details>
  <summary>Details</summary>
Motivation: 研究人员希望通过系统地分析JARVIS-DFT带隙数据集并去除可能包含带结构信息的描述符，来创建一个经过筛选且控制了信息泄露的数据子集，以便更准确地评估模型性能和识别最重要的特征。

Method: 采用三阶段建模方法：首先加入基本物理描述符，然后引入工程化特征，最后添加成分属性。使用树基模型（如随机森林或梯度提升树）进行建模，并利用SHAP值来解析不同特征的重要性。

Result: 树基模型在所有阶段均达到了大约0.88到0.90之间的R²值，表明即使扩大描述符范围，在控制信息泄露的前提下，预测准确性并未得到实质性提升。此外，SHAP分析发现介电张量分量是最主要的影响因素。

Conclusion: 该研究表明，通过构建一个经过精心策划的数据集并在其中实施泄漏控制措施，可以为未来关于带隙预测的研究提供可靠的基准性能指标。同时指出，介电张量成分可能是影响材料带隙的关键因素之一。

Abstract: In this study, we perform a systematic analysis of the JARVIS-DFT bandgap dataset and identify and remove descriptors that may inadvertently encode band-structure information, such as effective masses. This process yields a curated, leakage-controlled subset of 2280 materials. Using this dataset, a three-phase modeling framework is implemented that incrementally incorporates basic physical descriptors, engineered features, and compositional attributes. The results show that tree-based models achieve R2 values of approximately 0.88 to 0.90 across all phases, indicating that expanding the descriptor space does not substantially improve predictive accuracy when leakage is controlled. SHAP analysis consistently identifies the dielectric tensor components as the dominant contributors. This work provides a curated dataset and baseline performance metrics for future leakage-aware bandgap prediction studies.

</details>


### [26] [Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction](https://arxiv.org/abs/2512.19735)
*Gangxiong Zhang,Yongchao Long*

Main category: cs.LG

TL;DR: 本文针对大型语言模型在ICU患者死亡率预测中可能存在的与性别、年龄和种族相关的偏见问题，提出了一种无需再训练的临床适应性提示框架CAse Prompting (CAP)，该框架结合了传统的去偏见提示和基于案例的推理方法。实验结果表明，CAP不仅显著提高了预测准确性（AUROC从0.806提升到0.873，AUPRC从0.497提高至0.694），还大幅减少了性别和种族间的差异超过90%，并且不同人群间特征依赖模式高度一致。


<details>
  <summary>Details</summary>
Motivation: 准确预测重症监护病房（ICU）患者的死亡风险对于临床决策至关重要。尽管大型语言模型（LLMs）在利用结构化医疗数据进行结果预测方面显示出潜力，但它们的预测可能存在与性别、年龄和种族相关的偏见，这限制了其在临床实践中的可信使用。现有的去偏见方法往往以牺牲预测性能为代价，使得同时优化公平性和准确性变得困难。

Method: 研究首先开发了一个多维度偏见评估方案来全面诊断模型。在此基础上，引入了名为CAse Prompting (CAP)的新提示框架，该框架将传统去偏见提示与基于案例的推理相结合。通过引导模型从历史上的类似误预测案例及其正确结果中学习，CAP能够纠正存在偏见的推理模式。

Result: 在MIMIC-IV数据集上的实验显示，CAP显著提高了预测准确性（AUROC从0.806增加到0.873，AUPRC从0.497提高到0.694），同时减少了超过90%的性别和种族相关差异。此外，特征依赖分析进一步指出，在不同的人口统计学群体之间，注意力模式非常一致，相似度得分超过了0.98。

Conclusion: 研究表明，大型语言模型在ICU死亡率预测中确实表现出可测量的偏见；而精心设计的提示框架可以在不重新训练的情况下有效协同优化公平性和性能，为实现公平的临床决策支持提供了一种可转移的范式。

Abstract: Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.

</details>


### [27] [Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics](https://arxiv.org/abs/2512.19740)
*Kousar Raza,Faizan Ali*

Main category: cs.LG

TL;DR: 本文提供了一个关于2025年亚洲杯T20板球锦标赛的综合数据集，包含19场比赛的61个变量，并通过探索性数据分析展示了其在团队表现、边界分布和得分模式研究中的价值。


<details>
  <summary>Details</summary>
Motivation: 为了促进体育分析领域的数据驱动研究，本文构建并公开了一个详细记录了2025年亚洲杯T20板球锦标赛所有比赛信息的数据集。

Method: 创建一个包含从所有19场比赛中提取出的61个变量（如队伍得分、出局数、强力击球统计等）的数据集；对数据集进行探索性数据分析以展示其研究价值。

Result: 成功开发了一个全面的板球赛事数据集，并通过初步分析揭示了有关团队表现指标、边界分布以及得分趋势的一些见解。

Conclusion: 该开放获取的数据集为板球分析研究提供了宝贵的资源，支持了可重复的研究工作，并促进了预测建模和战略决策方面的进一步探索。

Abstract: This paper presents a structured and comprehensive dataset corresponding to the 2025 Asia Cup T20 cricket tournament, designed to facilitate data-driven research in sports analytics. The dataset comprises records from all 19 matches of the tournament and includes 61 variables covering team scores, wickets, powerplay statistics, boundary counts, toss decisions, venues, and player-specific highlights. To demonstrate its analytical value, we conduct an exploratory data analysis focusing on team performance indicators, boundary distributions, and scoring patterns. The dataset is publicly released through Zenodo under a CC-BY 4.0 license to support reproducibility and further research in cricket analytics, predictive modeling, and strategic decision-making. This work contributes an open, machine-readable benchmark dataset for advancing cricket analytics research.

</details>


### [28] [EdgeFlex-Transformer: Transformer Inference for Edge Devices](https://arxiv.org/abs/2512.19741)
*Shoaib Mohammad,Guanqun Song,Ting Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级但有效的多阶段优化流程，旨在压缩和加速视觉变换器（ViTs）以适应资源受限环境的部署。通过激活分析、内存感知剪枝、选择性混合精度执行以及激活感知量化（AWQ），在不需昂贵重训练或特定任务微调的情况下减少了模型的内存占用。实验表明，经过全面优化的模型相比原始FP32基线能够实现峰值内存使用减少76%、延迟降低超过6倍，并保持甚至提高准确性。


<details>
  <summary>Details</summary>
Motivation: 由于边缘设备存在严格的内存、计算能力和延迟限制，将大规模变压器模型部署到这些设备上面临着重大挑战。为了克服这一问题，研究者们希望开发一种解决方案，能够在不牺牲性能的前提下，有效压缩和加速视觉变换器，使其适用于资源有限的环境。

Method: 研究团队设计了一个包含多个步骤的优化流程：首先利用激活统计信息识别低重要性的通道，然后进行结构化剪枝以缩减MLP层规模；接着对选定组件应用FP16转换，并利用激活感知量化技术将剩余模型权重及激活量化为INT8格式，整个过程尽量减少对准确度的影响。

Result: 实验证明，该方法可以显著降低峰值内存消耗达76%，同时将延迟降低了超过6倍，且与原FP32基准相比，模型的准确性得到了维持甚至有所提升。

Conclusion: 本研究展示了一种实用的方法论，用于在边缘平台上实现高效的变压器推理。此外，还探讨了未来可能通过集成动态稀疏性和专家混合架构来进一步提升跨不同任务的表现。

Abstract: Deploying large-scale transformer models on edge devices presents significant challenges due to strict constraints on memory, compute, and latency. In this work, we propose a lightweight yet effective multi-stage optimization pipeline designed to compress and accelerate Vision Transformers (ViTs) for deployment in resource-constrained environments. Our methodology combines activation profiling, memory-aware pruning, selective mixed-precision execution, and activation-aware quantization (AWQ) to reduce the model's memory footprint without requiring costly retraining or task-specific fine-tuning. Starting from a ViT-Huge backbone with 632 million parameters, we first identify low-importance channels using activation statistics collected via forward hooks, followed by structured pruning to shrink the MLP layers under a target memory budget. We further apply FP16 conversion to selected components and leverage AWQ to quantize the remaining model weights and activations to INT8 with minimal accuracy degradation. Our experiments on CIFAR-10 demonstrate that the fully optimized model achieves a 76% reduction in peak memory usage and over 6x lower latency, while retaining or even improving accuracy compared to the original FP32 baseline. This framework offers a practical path toward efficient transformer inference on edge platforms, and opens future avenues for integrating dynamic sparsity and Mixture-of-Experts (MoE) architectures to further scale performance across diverse tasks.

</details>


### [29] [From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning](https://arxiv.org/abs/2512.19743)
*Sasan Sharifipour,Constantino Álvarez Casado,Manuel Lage Cañellas,Miguel Bordallo López*

Main category: cs.LG

TL;DR: 本文提出CUDA-APML，一种稀疏GPU实现方法，通过阈值化可忽略的分配并在COO格式中直接运行自适应softmax、双向对称化和Sinkhorn归一化，实现了近似线性内存扩展，同时在ShapeNet和MM-Fi数据集上与密集APML匹配良好，并将峰值GPU内存减少了99.9%。


<details>
  <summary>Details</summary>
Motivation: 现有的3D点云模型损失函数在几何保真度和计算成本之间存在权衡问题。Chamfer距离虽然高效但允许多对一对应，而Earth Mover距离虽然能更好地反映一对一传输却具有较高的计算成本。APML通过可微分的Sinkhorn迭代和解析得出的温度来逼近传输，但其密集形式在内存使用上呈二次方增长。因此，需要一种既能保持几何精度又能降低计算资源消耗的新方法。

Method: 研究者们开发了CUDA-APML，这是一种基于GPU的稀疏实现方式，它能够识别并剔除不重要的分配，并且能够在坐标（COO）格式下执行自适应softmax、双向对称处理以及Sinkhorn标准化。这种方法旨在提供接近线性的内存扩展特性，同时保持存储支持上的梯度信息，尽管目前实现中的成对距离评估仍需二次运算。

Result: 实验结果显示，在ShapeNet和MM-Fi数据集上，CUDA-APML与传统的密集型APML相比，在很小的容差范围内表现相当，同时极大降低了所需的GPU内存峰值，达到了99.9%的减少。

Conclusion: CUDA-APML为3D点云模型的学习提供了更加高效的选择，通过采用稀疏表示和优化算法，显著降低了内存需求而不牺牲准确性，使得该方法适用于更大规模的数据集或更复杂的模型训练场景。

Abstract: Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml

</details>


### [30] [A K-Means, Ward and DBSCAN repeatability study](https://arxiv.org/abs/2512.19772)
*Anthony Bertrand,Engelbert Mephu Nguifo,Violaine Antoine,David Hill*

Main category: cs.LG

TL;DR: 本研究分析了K-Means、DBSCAN和Ward等流行聚类算法的可重复性，并通过Python库scikit-learn实现示例来检查每种方法的可重复方面。结果揭示了当OpenMP线程数超过两个时，K-Means算法出现不一致的结果。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，可重复性对于确保模型或实验得出相同的科学结论至关重要；对于特定算法而言，具有逐位相同结果的可重复性也是科学诚信的关键，因为它允许调试。

Method: 研究人员将几种非常流行的聚类算法（K-Means、DBSCAN和Ward）分解为它们的基本步骤，并确定每个阶段达到可重复性的条件。使用Python库scikit-learn作为实现例子来考察每种方法的可重复方面。

Result: 研究结果显示，当OpenMP线程数量超过两个时，K-Means算法出现了不一致的结果。

Conclusion: 这项工作旨在提高用户和开发者对这个问题的认识，鼓励进一步调查并可能提出解决方案。

Abstract: Reproducibility is essential in machine learning because it ensures that a model or experiment yields the same scientific conclusion. For specific algorithms repeatability with bitwise identical results is also a key for scientific integrity because it allows debugging. We decomposed several very popular clustering algorithms: K-Means, DBSCAN and Ward into their fundamental steps, and we identify the conditions required to achieve repeatability at each stage. We use an implementation example with the Python library scikit-learn to examine the repeatable aspects of each method. Our results reveal inconsistent results with K-Means when the number of OpenMP threads exceeds two. This work aims to raise awareness of this issue among both users and developers, encouraging further investigation and potential fixes.

</details>


### [31] [Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy](https://arxiv.org/abs/2512.19805)
*Deepit Sapru*

Main category: cs.LG

TL;DR: 本文介绍了一种营销决策框架，通过将异质处理提升转化为受约束的目标策略，在遵守商业规则的同时最大化收入和客户保留。该方法估计条件平均治疗效果（CATE），并通过解决受限分配问题来决定目标对象及提供何种优惠，同时考虑预算或可接受的销售下降等限制。


<details>
  <summary>Details</summary>
Motivation: 为了在遵循商业准则的前提下最大化营销活动的收入与顾客保持率，需要开发一种能够有效评估不同营销干预措施对个体影响差异的方法，并据此制定出最佳的个性化营销策略。

Method: 首先使用提升学习者估算条件平均治疗效应(CATE)，然后通过求解一个带有约束条件的分配问题来确定应针对哪些用户以及部署哪种优惠方案，这些约束条件包括但不限于预算限制或是可接受的最大销售额下降程度。

Result: 不论是应用于维持客户关系的信息推送、活动奖励还是消费门槛设定上，该框架在离线评估中（采用提升AUC、逆倾向评分(IPS)及自归一化IPS(SNIPS)作为评价指标）始终优于基于倾向性得分匹配和静态基线的表现。此外，一次大规模在线A/B测试进一步验证了该策略对于提高收入和完成度的有效性，同时也保证了良好的用户体验。

Conclusion: 提出的方法为营销人员提供了一个可重复使用的操作手册，用于规模化实施因果定向策略、设立保护措施并确保营销活动与关键绩效指标相一致。

Abstract: This paper introduces a marketing decision framework that converts heterogeneous-treatment uplift into constrained targeting strategies to maximize revenue and retention while honoring business guardrails. The approach estimates Conditional Average Treatment Effects (CATE) with uplift learners and then solves a constrained allocation to decide who to target and which offer to deploy under limits such as budget or acceptable sales deterioration. Applied to retention messaging, event rewards, and spend-threshold assignment, the framework consistently outperforms propensity and static baselines in offline evaluations using uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). A production-scale online A/B test further validates strategic lift on revenue and completion while preserving customer-experience constraints. The result is a reusable playbook for marketers to operationalize causal targeting at scale, set guardrails, and align campaigns with strategic KPIs.

</details>


### [32] [Fine-Tuned In-Context Learners for Efficient Adaptation](https://arxiv.org/abs/2512.19879)
*Jorg Bornschein,Clare Lyle,Yazhe Li,Amal Rannen-Triki,Xu Owen He,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文提出了一种结合上下文学习和微调的统一方法，以适应特定下游任务。该方法在任务特定数据上进行微调，并加入上下文示例来模仿k-shot提示结构。这种方法不仅保持了上下文学习的样本效率，还拥有通过微调带来的性能提升。此外，为了在低数据环境下选择超参数，提出了预测性评估方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决在少量数据下仅使用上下文学习或微调大型语言模型时存在的局限性（即上下文学习在数据增多时效果趋于平缓而微调则在数据稀缺时表现不佳），研究者希望探索一种能够同时利用两种方法优势的新途径。

Method: 研究者开发了一个新方法，它将上下文学习直接整合进微调流程中，通过对任务相关数据添加上下文实例来模仿k-shot提示的形式进行模型微调。针对低资源情况下的超参数选择问题，引入了预判评估技术，旨在减少对耗时交叉验证的需求并充分利用所有可用训练数据的同时提供可靠的验证信号。

Result: 实验结果表明，在具体的数据下游任务上，相比单独使用微调或上下文学习的方法，所提出的统一方法始终能匹配甚至显著超越两者的基线表现。

Conclusion: 这项研究表明，通过结合上下文学习与微调的方式可以有效提高大型语言模型对于特定下游任务的适应能力。特别是当面临不同规模的训练数据集时，所提方法能够在保持样本效率的同时实现性能上的改进。

Abstract: When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.

</details>


### [33] [Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling](https://arxiv.org/abs/2512.19905)
*Indranil Halder,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: 本文提出了一种可解析的推理时间缩放模型：具有奖励加权采样的贝叶斯线性回归，通过理论分析和实验验证了在高维情况下推理时间样本数k对泛化误差的影响，并展示了在某些情况下增加推理时间计算优于收集更多数据的优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展表明，在训练时间和推理时间之间重新分配大量计算资源是有利的，但背后的原理尚不清楚。

Method: 使用贝叶斯线性回归与奖励加权采样器构建了一个推理时间缩放的模型，在高维情形下利用确定性等价给出了后验预测均值和方差的封闭形式表达式。同时，当训练数据从教师模型中抽取时，分析了泛化误差。

Result: 对于给定的k个推理时间样本，存在一个最优采样温度；当奖励不太偏离教师模型时，泛化误差随着k的增加而单调减少；然而，优化推理时间选择的具体奖励通常与教师不同。严重的奖励错误指定会导致一个有限的最佳k值，超过这个值更多的采样可能会增加泛化误差。此外，还证明了在“最佳-k”限制下，采用教师作为奖励时，泛化误差按Θ(1/k^2)衰减。

Conclusion: 研究结果为理解大规模语言模型中的推理时间扩展提供了新的见解，并指出在某些条件下，扩大推理时间计算比单纯增加数据量更优。

Abstract: Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the "best-of-$k$" limit with the teacher as reward, we theoretically show that the generalization error decays as $Θ(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.

</details>


### [34] [Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra](https://arxiv.org/abs/2512.19909)
*Maxime Lacour,Pu Ren,Rie Nakata,Nori Nakata,Michael Mahoney*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的方法CGM-FAS，用于非遍历路径效应建模，并在预测准确性、空间模式学习能力以及计算效率方面展示了优于高斯过程方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的非遍历地面运动模型依赖于具有预设相关函数的高斯过程方法，在大规模预测中存在计算限制。研究旨在寻找一种替代方案以提高非遍历路径效应对傅里叶振幅谱（FAS）建模时的效率与准确性。

Method: 通过使用条件变分自编码器架构，开发出名为CGM-FAS的新方法，它能够直接从数据中学习空间模式和频率间相关性，利用地震和站点的地理坐标作为条件变量。

Result: 实验表明，相比最近提出的基于高斯过程的方法，CGM-FAS在旧金山湾区地震数据分析中能够一致地预测非遍历路径效应。此外，该方法无需预设相关函数即可学习空间模式，捕捉频率间的相关性，并实现快速预测。

Conclusion: CGM-FAS为跨多个频率和大空间域的有效非遍历地面运动预测提供了一个有前景的方向。

Abstract: Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.

</details>


### [35] [Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning](https://arxiv.org/abs/2512.19920)
*Jiayun Wu,Jiashuo Liu,Zhiyuan Zeng,Tianyang Zhan,Wenhao Huang*

Main category: cs.LG

TL;DR: 本文针对LLM在关键领域部署时产生事实性错误的问题，提出了一种行为校准方法，通过优化严格的评分规则来鼓励模型在不确定时随机承认不确定性。实验表明，该方法提高了小规模模型的不确定性量化能力，在特定任务上超越了前沿模型。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）在关键领域的应用受到了持续产生事实性错误的影响。尽管扩大规模带来了通用能力上的显著提升，但理论框架指出这种错误不仅仅是随机误差，而是训练目标优先模仿数据分布而非知识诚实性的可预见统计结果。标准RLVR范式使用二元奖励信号，无意中激励了模型成为好的应试者而不是诚实的沟通者，即使正确概率超过零也鼓励猜测。

Method: 本文提出了一个详尽的行为校准研究，旨在通过让模型在不自信时放弃回答或标记出仍然存在不确定性的个别声明，从而将模型行为与准确性对齐。结合最近的进步，作者建议并评估了针对模型输出校准正确概率的严格适当评分规则进行优化的训练干预措施。

Result: 利用Qwen3-4B-Instruct进行实证分析显示，行为校准强化学习允许较小规模的模型在不确定性量化方面超越前沿模型——这是一种与原始预测准确性分离的可转移元技能。在数学推理任务上训练的模型，在具有挑战性的领域内评估（BeyondAIME）中表现出优于GPT-5的对数尺度准确度到幻觉比率增益。此外，在跨领域事实问答（SimpleQA）中，该4B LLM达到了与包括Grok-4和Gemini-2.5-Pro在内的前沿模型相媲美的零样本校准误差，尽管其事实准确性较低。

Conclusion: 研究表明，通过采用行为校准的方法，可以有效地提高LLM处理不确定信息的能力，并减少事实性错误的发生。这不仅为解决现有问题提供了新思路，还展示了即使是参数较少的模型也能在特定任务上达到甚至超越大规模模型的表现。

Abstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.

</details>


### [36] [The Seismic Wavefield Common Task Framework](https://arxiv.org/abs/2512.19927)
*Alexey Yermakov,Yue Zhao,Marine Denolle,Yiyu Ni,Philippe M. Wyder,Judah Goldfeder,Stefano Riva,Jan Williams,David Zoro,Amy Sara Rude,Matteo Tomasetto,Joe Germany,Joseph Bakarji,Georg Maierhofer,Miles Cranmer,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 本文介绍了一个用于地震波场机器学习的通用任务框架(CTF)，旨在通过标准化评估提高科学机器学习的严谨性和可重复性。该框架包括不同规模的数据集和针对预测、重建及泛化能力的任务特定度量，以解决地震学中面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 地震学在状态预测与重建（如地震预警和地面运动预测）以及处理源位置、机制和地球模型参数变化方面面临基本挑战。现有模拟方法由于数据量庞大和数值复杂性而受到限制，实际数据努力则受限于无法充分反映地球复杂性的模型和现场稀疏传感器测量。尽管最近的机器学习(ML)尝试提供了希望，但缺乏适当的特征描述、公平报告和严格比较阻碍了进展。

Method: 为了解决上述问题，作者们引入了一个适用于地震波场机器学习的通用任务框架(CTF)，首先从三个不同的波场数据集开始。该CTF包含了一系列精选的数据集，覆盖全球、地壳和局部尺度，并且具有针对预测、重建以及在现实约束条件下（例如噪声和有限数据）泛化能力的任务特定度量标准。

Result: 研究者展示了两个数据集上的评分过程，展示了多种方法和基础模型在根据模拟和真实世界传感器测量重构地震波场方面的表现。CTF得分揭示了各种方法的优点、局限性及其对特定问题类别的适用性。

Conclusion: 作者们的愿景是用基于隐藏测试集的标准评估来取代临时比较，以此提高科学机器学习中的严谨性和可重复性水平。

Abstract: Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.

</details>


### [37] [LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2512.20002)
*Jiacheng You,Jingcheng Yang,Yuhang Xie,Zhongxuan Wu,Xiucheng Li,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xinyang Chen*

Main category: cs.LG

TL;DR: 本文提出了一种频率感知的预测管道LoFT-LLM，它结合了低频学习与通过大型语言模型(LLM)进行的语义校准，以解决时间序列预测中数据有限和复杂噪声的问题。该方法在金融和能源数据集上的实验表明，无论是在全数据还是少量样本情况下，都显著优于现有基准，在准确性、鲁棒性和可解释性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的时间序列预测（如金融和能源领域）经常遇到由于训练数据有限以及复杂的、带有噪声的时间动态所带来的挑战。现有的深度预测模型通常使用包含大量高频噪声的完整时间窗口来进行监督预测，这往往会掩盖长期趋势。此外，含有丰富领域特定信息的辅助变量往往未被充分利用，尤其是在少量样本设置下。

Method: 提出了LoFT-LLM，一种频率意识预测流程，该流程将低频学习与通过大型语言模型（LLM）实现的语义校准相结合。首先，Patch低频预测模块(PLFM)从局部光谱块中提取稳定的低频趋势；其次，残差学习者对高频变化进行建模；最后，经过微调的LLM通过结构化的自然语言提示整合辅助上下文和领域知识来优化预测结果。

Result: 广泛的实验证明了LoFT-LLM在金融和能源数据集上无论是全数据还是少量样本场景下都显著超越了强大的基线方法，提供了更高的准确性、鲁棒性和可解释性。

Conclusion: LoFT-LLM通过创新地结合低频分析与利用大型语言模型加强语义理解，为解决实际应用中时间序列预测面临的难题提供了一个有效方案。

Abstract: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.

</details>


### [38] [Control Variate Score Matching for Diffusion Models](https://arxiv.org/abs/2512.20003)
*Khaled Kahouli,Romuald Elie,Klaus-Robert Müller,Quentin Berthet,Oliver T. Unke,Arnaud Doucet*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，即控制变量分数恒等式（CVSI），通过结合数据去噪分数恒等式（DSI）和目标分数恒等式（TSI）的优点，在整个噪声谱范围内最小化方差，从而提高无数据采样器学习和推理时扩散采样的样本效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型提供了一个从非标准化概率密度中抽样的强大框架，但是现有的估计方法——数据去噪分数恒等式（DSI）和目标分数恒等式（TSI）——在不同噪声水平下都面临着高方差的问题。为了解决这个问题，并且提高样本效率，作者提出了一个新的方法。

Method: 作者引入了控制变量分数恒等式（CVSI），该方法将DSI和TSI两种估计方法统一在一个有原则的控制变量框架内。他们推导出了一个最优的时间依赖性控制系数，该系数理论上保证在整个噪声谱上实现方差最小化。

Result: 研究表明，CVSI作为一种鲁棒、低方差的插件估计器，显著提高了无数据采样器学习以及推理时间扩散采样的样本效率。

Conclusion: 通过采用CVSI方法，研究人员能够克服传统估计方法在不同噪声水平下的局限性，使得扩散模型在更广泛的场景下变得更加高效实用。

Abstract: Diffusion models offer a robust framework for sampling from unnormalized probability densities, which requires accurately estimating the score of the noise-perturbed target distribution. While the standard Denoising Score Identity (DSI) relies on data samples, access to the target energy function enables an alternative formulation via the Target Score Identity (TSI). However, these estimators face a fundamental variance trade-off: DSI exhibits high variance in low-noise regimes, whereas TSI suffers from high variance at high noise levels. In this work, we reconcile these approaches by unifying both estimators within the principled framework of control variates. We introduce the Control Variate Score Identity (CVSI), deriving an optimal, time-dependent control coefficient that theoretically guarantees variance minimization across the entire noise spectrum. We demonstrate that CVSI serves as a robust, low-variance plug-in estimator that significantly enhances sample efficiency in both data-free sampler learning and inference-time diffusion sampling.

</details>


### [39] [Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance](https://arxiv.org/abs/2512.20006)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: 本文提出了一种新的激活函数OGAB，旨在通过正交性和组感知偏差学习来缓解深度学习分类器中的类别不平衡问题。与现有方法不同，该方法在训练阶段直接处理嵌入学习层面的类别不平衡，无需显式的标签信息或数据预处理/后处理。实验表明，在真实和合成的不平衡数据集上，OGAB相比传统及可学习的激活函数均表现出一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是机器学习和数据挖掘中常见的挑战，它会导致分类器性能不佳。尽管深度学习在特征提取方面表现出色，但在面对不平衡数据时其表现也会下降。为了解决这个问题，作者提出了一个名为OGAB的新激活函数，目的是增强不平衡场景下的特征区分度，而不需要明确地使用标签信息。

Method: OGAB结合了正交变换和组感知偏差机制，前者帮助保持少数类别的信息独立性以防止多数类别在嵌入空间中的主导地位；后者则自动识别数据簇并调整嵌入以提高类别分离度，整个过程不依赖于显式监督。这种方法的独特之处在于它直接在训练过程中、嵌入学习层面上应对类别不平衡问题，从而能够无缝整合到学习流程中。

Result: 研究结果表明，无论是在实际还是合成的不平衡数据集上，所提出的OGAB解决方案相较于传统的以及可学习的激活函数，在性能上有持续的改进。

Conclusion: 通过引入OGAB激活函数，本研究展示了一种新颖有效的方法来解决深度学习领域内因类别不平衡导致的问题，且无需依赖额外的数据预处理或后处理步骤。

Abstract: Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.

</details>


### [40] [PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models](https://arxiv.org/abs/2512.20063)
*Mingue Park,Jisung Hwang,Seungwoo Yoo,Kyeongmin Yeo,Minhyuk Sung*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的预处理步骤PairFlow，用于训练离散流模型（DFMs），以实现无需预训练教师模型的快速采样。通过从源和目标分布的耦合样本中训练DFMs，并采用闭式反演方法高效构建配对样本，PairFlow在极低成本下达到了与两阶段训练相当甚至更好的性能，并且适用于分子数据、二进制图像和RGB图像等多种场景。


<details>
  <summary>Details</summary>
Motivation: 离散流模型（DFMs）作为一类新的生成模型，在处理离散数据方面表现优异，但其迭代性质导致采样速度慢。现有的加速方法大多依赖于微调，这带来了显著的额外训练开销。为了解决这个问题，作者提出了一个轻量级的预处理步骤PairFlow，它不需要预训练教师模型的支持。

Method: PairFlow受ReFlow及其扩展到DFMs的启发，通过从源和目标分布的耦合样本中训练DFMs来工作。该方法的核心是为DFMs提供了一个闭式反演方案，允许有效构建配对的源-目标样本。

Result: 实验结果表明，尽管PairFlow的成本非常低，仅占全模型训练所需计算资源的1.7%，但它能够匹配甚至超越涉及微调的两阶段训练的性能。此外，使用PairFlow框架训练的模型还为后续蒸馏提供了更强的基础模型，在微调后进一步加速了采样过程。

Conclusion: PairFlow作为一种轻量级预处理技术，能够在不牺牲性能的情况下显著加快离散流模型的采样速度，展示了广泛的应用性和有效性。

Abstract: We introduce $\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.

</details>


### [41] [QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption](https://arxiv.org/abs/2512.20084)
*Yanjie Li,Jian Xu,Xueqing Chen,Lina Yu,Shiming Xiang,Weijun Li,Cheng-lin Liu*

Main category: cs.LG

TL;DR: 提出了一种多模态框架QE-Catalytic，它将大型语言模型Qwen与E(3)等变图Transformer Equiformer-V2深度结合，以支持复杂催化表面上的吸附构型属性预测和逆向设计。通过同时利用三维结构和结构化配置文本，并在语言通道中注入“3D几何信息”，QE-Catalytic能够作为高性能的基于文本的预测器工作，同时也自回归地生成CIF文件用于目标能量驱动的结构设计和信息补全。


<details>
  <summary>Details</summary>
Motivation: 现有的基于语言模型的方法虽然可以提供人类可读的文本描述并减少对显式图形的依赖，从而扩大了适用性，但在吸附构型能量预测准确性和区分相同系统不同构型的能力上仍然不足。

Method: 开发了一个名为QE-Catalytic的新框架，该框架将一个大语言模型（Qwen）与一个E(3)等变图Transformer（Equiformer-V2）紧密结合，共同利用三维结构和结构化配置文本进行预测，并且能够自动回归生成CIF文件以支持目标能量驱动的设计。

Result: 实验结果表明，在OC20数据集上，QE-Catalytic将松弛吸附能的平均绝对误差从0.713 eV降低到了0.486 eV，并且在多种评估协议下持续优于CatBERTa和GAP-CATBERTa等基线模型。

Conclusion: QE-Catalytic框架通过整合三维几何信息和文本描述显著提高了吸附构型能量预测的准确性，并且为复杂催化表面的逆向设计提供了新的方法。

Abstract: Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.
  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.

</details>


### [42] [Information-directed sampling for bandits: a primer](https://arxiv.org/abs/2512.20096)
*Annika Hirling,Giorgio Nicoletti,Antonio Celani*

Main category: cs.LG

TL;DR: 本文探讨了信息导向采样（IDS）策略在两状态伯努利老虎机问题中的应用，通过引入修改后的信息量度和调参来扩展IDS框架到折扣无限期设定，并展示了在对称老虎机和单个公平硬币场景中IDS的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析信息导向采样（IDS）策略，在探索与利用之间找到平衡点，特别是在两状态伯努利老虎机这一简化模型中对比启发式策略与最优策略的表现。

Method: 采用信息导向采样方法，针对两状态伯努利老虎机问题进行研究；通过引入调整参数和改进的信息度量方式将IDS框架延伸至折扣无限期环境。

Result: 在对称老虎机情况下，IDS实现了有限累积遗憾；而在包含一个公平硬币的情景下，IDS策略产生的遗憾随时间以对数级别增长，符合经典渐近下界。

Conclusion: 本研究表明，信息导向采样策略能够有效地在特定类型的两状态伯努利老虎机问题中达到接近最优的性能表现。

Abstract: The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.

</details>


### [43] [Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering](https://arxiv.org/abs/2512.20115)
*Yuanhao Chen,Qi Liu,Pengbin Chen,Zhongjian Qiao,Yanjie Li*

Main category: cs.LG

TL;DR: 本文提出了一种高效的样本过滤方法，用于改进离线强化学习中策略约束方法的采样效率和最终性能。通过评估数据集中各段落的得分并选取高分段落来训练算法，实验结果表明该方法优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的策略约束方法在面对包含许多低奖励转换的数据集时会导致次优参考策略的学习，从而导致学习速度慢、样本效率低和性能差。本文旨在通过提出一种简单但有效的样本过滤方法来改善这一状况，提高样本效率和最终性能。

Method: 1. 通过计算数据集中各片段的平均奖励和平均折扣奖励来评估其得分。
2. 提取得分高的转换样本。
3. 使用这些高分转换样本来训练离线RL算法。

Result: 实验结果显示，在一系列离线RL算法和基准任务上验证了所提出的方法，该方法比基线方法表现更好。

Conclusion: 提出的样本过滤方法能够有效提升政策约束离线强化学习中的样本效率与最终表现，为解决低回报转移造成的学习问题提供了新思路。

Abstract: Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.

</details>


### [44] [Learning to Reason in LLMs by Expectation Maximization](https://arxiv.org/abs/2512.20169)
*Junghyun Lee,Branislav Kveton,Sunav Choudhary,Subhojyoti Mukherjee,Anup Rao,Ryan A. Rossi,Alexa Siu*

Main category: cs.LG

TL;DR: 本文提出了一个基于期望最大化（EM）目标的学习推理框架，通过比较几种采样方案，发现提示后验采样（PPS）在提高推理模型准确性方面优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决大型语言模型在进行推理任务时，如何生成合理的解释来支持正确答案的问题。

Method: 将推理形式化为潜在变量模型，并推导出用于学习推理的期望最大化（EM）目标。实验中实现了几种不同的采样方案，包括预算内的拒绝采样、自教推理器（STaR）、以及仅保留STaR合理化阶段的提示后验采样（PPS）。

Result: 在ARC、MMLU和OpenBookQA数据集上使用Llama和Qwen模型进行实验的结果表明，所采用的采样方案对学习到的推理模型准确性有着显著影响。其中，提示后验采样（PPS）尽管简单却表现最佳。

Conclusion: 适当的采样方案对于提高大型语言模型的推理能力至关重要；提示后验采样（PPS）是一种有效的方法。

Abstract: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.

</details>


### [45] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: 本文介绍了一种结合了先进过程基础模型与数据驱动机器学习组件的混合全球网格作物模型（NeuralCrop），该模型在模拟作物产量方面优于现有模型，特别是在干旱等极端条件下，并且在处理训练期间未见条件时表现稳健。


<details>
  <summary>Details</summary>
Motivation: 尽管全球网格作物模型（GGCMs）是评估气候变化对农业生产率影响的主要工具，但它们在模拟复杂生物物理过程方面仍存在显著不确定性。同时，纯机器学习方法虽然在作物产量预测上展示了潜力，但在应对气候条件变化时表现不佳。因此，研究者开发了一个新的混合模型以解决这些问题。

Method: 提出了一种名为NeuralCrop的新模型，它首先通过模仿一个竞争性的GGCM来训练，然后利用观测数据进行微调。此模型结合了高级过程基础GGCM的优势和数据驱动的机器学习成分。

Result: NeuralCrop在站点级和大规模种植区域的表现超过了最先进的GGCMs。特别地，在2000年至2019年间，对于欧洲小麦产区和美国玉米带而言，NeuralCrop能够更准确地再现年际产量异常情况，尤其是在干旱极端条件下。此外，当面对训练过程中未曾遇到的情况时，NeuralCrop依旧能够做出可靠的预测。

Conclusion: 本研究表明，所提出的混合作物建模方法提供了整体上改进的作物建模能力，并且能够在气候变化加剧及极端天气条件下提供更加可靠的产量预测。

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [46] [Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud](https://arxiv.org/abs/2512.20218)
*Jixiao Yang,Jinyu Chen,Zixiao Huang,Chengda Xu,Chi Zhang,Sijia Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为Cost-TrustFL的层次化联邦学习框架，旨在同时优化模型性能与通信成本，并对恶意攻击提供强大的防御。通过使用基于梯度近似Shapley值计算方法来简化声誉评估过程，并采用一种成本意识聚合策略优先考虑云内通信以减少跨云数据传输费用。实验结果表明，在存在30%恶意客户端的情况下，该框架在CIFAR-10和FEMNIST数据集上实现了86.7%的准确率，相较于基线方法减少了32%的通信成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在多云环境中面临着非独立同分布（non-IID）数据、恶意参与者检测以及高昂的跨云通讯成本等挑战。当前针对拜占庭鲁棒性的方法主要关注于提高模型准确性，但忽视了跨云服务商间数据传输所带来的经济影响。

Method: 提出了Cost-TrustFL框架，该框架利用基于梯度的近似Shapley值计算方法降低了声誉评估复杂度；并通过成本感知聚合策略优先处理云内部通信，从而最小化昂贵的跨云数据传输需求。

Result: 在CIFAR-10和FEMNIST两个数据集上的实验显示，当存在30%的恶意客户时，Cost-TrustFL能够达到86.7%的准确率，并且相比基准方案减少了32%的通信开销。此外，该框架对于不同级别的非IID数据分布及攻击强度均表现出稳定的性能。

Conclusion: Cost-TrustFL框架有效地解决了联邦学习中遇到的数据异构性问题、恶意用户威胁以及高昂的跨云通信成本问题，为实际应用中的多云部署提供了实用解决方案。

Abstract: Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.

</details>


### [47] [Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning](https://arxiv.org/abs/2512.20220)
*Kausthubh Manda,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: 本文研究了多任务离线强化学习中多个任务共享低秩动作值函数表示的情况，通过联合学习共享表示和特定任务的价值函数，提高了统计效率和泛化能力。在标准的可实现性和覆盖假设下，我们为学到的价值函数建立了有限样本泛化保证。此外，我们还探讨了当新任务与上游任务共享相同基础表示时，如何重用多任务阶段学到的表示来减少下游学习的有效复杂度。


<details>
  <summary>Details</summary>
Motivation: 在多任务离线强化学习环境中，通过利用任务间的共享结构以提高统计效率和泛化性能。

Method: 采用了一种多任务变体的拟合Q-迭代方法，该方法通过最小化贝尔曼误差，在离线数据上共同学习共享表示和特定于任务的价值函数。

Result: 在标准可实现性和覆盖假设下，对于所学价值函数提供了有限样本泛化保证，并且证明了跨任务汇集数据可以提高估计准确性。此外，研究表明，重用多任务阶段学到的表示能够降低下游学习相对于从头开始学习的有效复杂度。

Conclusion: 结果阐明了共享表征在多任务离线Q学习中的作用，并提供了理论见解，说明了多任务结构何时以及如何改善无模型、基于价值的强化学习中的泛化。

Abstract: We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.

</details>


### [48] [Adaptive Multi-task Learning for Probabilistic Load Forecasting](https://arxiv.org/abs/2512.20232)
*Onintze Zaballa,Verónica Álvarez,Santiago Mazuelas*

Main category: cs.LG

TL;DR: 本文提出了一种自适应多任务学习方法，用于概率负载预测。该方法基于向量值隐马尔可夫模型，并使用递归过程更新模型参数以提供最新的预测结果。实验结果显示，该技术在预测性能和不确定性评估方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 同时对多个实体（如地区、建筑物）进行负载预测对于电力系统的高效、可靠和经济运行至关重要。然而，由于负载需求的内在不确定性、消费模式的动态变化以及实体间的相关性，准确的负载预测是一个具有挑战性的问题。尽管多任务学习已成为一种强大的机器学习方法，能够实现跨多个相关问题的同时学习，但其在负载预测中的应用仍处于探索阶段，并且目前仅限于基于离线学习的方法，无法捕捉到消费模式的变化。

Method: 提出了一个自适应多任务学习方法来解决概率负载预测问题。该方法能够动态地适应消费模式及实体间相关性的变化。此外，它还提供了可靠的针对多个实体的负载概率预测，并评估了负载不确定性。具体来说，这个方法基于向量值隐藏马尔科夫模型，并利用递归流程更新模型参数，从而根据最新参数给出预测。

Result: 通过使用包含多个实体负载需求的数据集对所提方法进行了评估，这些数据集展现了多样性和动态变化的消费模式。实验结果表明，在预测表现和不确定性评价两方面，所展示的技术都超过了现有方法。

Conclusion: 这项研究开发了一种新的自适应多任务学习策略，特别适用于处理具有复杂相互作用和不断演变行为的电力系统负载预测问题。实验证明，与传统方法相比，这种方法不仅提高了预测准确性，还能更有效地评估不确定度。

Abstract: Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.

</details>


### [49] [How I Met Your Bias: Investigating Bias Amplification in Diffusion Models](https://arxiv.org/abs/2512.20233)
*Nathan Roos,Ekaterina Iakovleva,Ani Gjergji,Vito Paolo Pastore,Enzo Tartaglione*

Main category: cs.LG

TL;DR: 该研究首次探讨了采样算法及其超参数如何影响扩散模型中的偏见放大问题。通过在Biased MNIST、Multi-Color MNIST和BFFHQ数据集上训练的模型实验，证明即使固定训练模型，调整采样超参数也能减少或增加偏见。


<details>
  <summary>Details</summary>
Motivation: 虽然先前的研究认为偏见放大是扩散模型固有的特性，但本工作旨在分析采样算法及其超参数对偏见放大现象的具体影响，试图理解为何以及如何这些因素能够改变模型输出中的偏见程度。

Method: 采用控制实验方法，在特定设计的数据集（如Biased MNIST）上训练扩散模型，并系统地变化采样过程中的关键超参数来观察其对最终生成图像中偏见水平的影响。

Result: 研究表明，通过对采样器超参数的选择可以显著影响扩散模型生成结果中的偏见程度；既有可能减少也可以增加偏见，即便是在保持基础模型不变的情况下也是如此。

Conclusion: 选择合适的采样算法及设置恰当的超参数对于控制扩散模型中偏见放大至关重要。这项发现为未来开发更公平的生成模型提供了新的方向。

Abstract: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.

</details>


### [50] [Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion](https://arxiv.org/abs/2512.20249)
*Xuanyu Hu*

Main category: cs.LG

TL;DR: 本文提出了一种名为BrainROI的模型，旨在解决多模态脑解码在跨主体泛化和可解释性方面的挑战。通过设计新的fMRI编码器、引入可解释的提示优化过程以及施加参数化解码约束，该模型在NSD数据集上的大脑-描述评估中取得了领先水平的结果，并且在跨主体设置下显著提高了诸如BLEU-4和CIDEr等指标的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态脑解码的目标是从fMRI等脑活动信号中重建与视觉刺激一致的语义信息并生成自然语言描述。然而，它仍然面临跨主体泛化性和可解释性的关键挑战。

Method: 1. 设计了新的fMRI编码器来处理不同个体间功能脑拓扑结构的异质性问题，采用了多图谱软功能性分割（soft-ROI）作为共享空间，并扩展了MINDLLM中的离散ROI连接策略至体素级门控融合机制（Voxel-gate），同时确保了通过全局标签对齐的一致性ROI映射。
2. 引入了一个可解释的提示优化过程，利用局部部署的Qwen模型迭代生成和选择人类可读的提示，以提高提示设计的稳定性并保持可审计的优化轨迹。
3. 在推理过程中施加参数化的解码约束，进一步提升了生成描述的稳定性和质量。

Result: BrainROI模型在NSD数据集上实现了领先级别的结果，在跨主体设定下，相较于最近最先进的方法及代表性基线，在如BLEU-4和CIDEr这样的度量标准上显示出了明显的改进。

Conclusion: 本研究提出的BrainROI模型有效地解决了多模态脑解码领域中存在的跨主体泛化难题，并增强了系统的可解释性，为未来的研究提供了有价值的方向。

Abstract: Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.

</details>


### [51] [HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training](https://arxiv.org/abs/2512.20272)
*Yuanjian Xu,Yuan Shuai,Jianing Hao,Guang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于GAN的框架HGAN-SDEs，利用神经Hermite函数构建一个结构化且高效的判别器来学习SDE诱导的复杂路径分布。此方法降低了运行时间复杂度并提高了训练稳定性，并在合成和真实世界的系统上展示了优于现有SDE生成模型的样本质量和学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有的用于学习由SDEs（随机微分方程）引起的复杂路径分布的方法存在计算成本高以及对抗性训练不稳定的问题。特别地，尽管神经控制微分方程(CDEs)由于能够建模连续时间动态而被作为判别器使用，但它们的架构导致了较高的计算成本并且加剧了对抗训练的不稳定性。因此，需要一种既能捕捉时间依赖关系又保持计算效率的新方法。

Method: 作者们引入了HGAN-SDEs，这是一种新颖的基于GAN的框架，它利用神经Hermite函数来构造一个既结构化又高效的判别器。Hermite函数为近似路径级动态提供了一个表达力强但轻量级的基础，这使得运行时复杂度降低同时提升了训练稳定性。此外，研究还确立了该框架对于广泛类别的SDE驱动分布具有通用逼近性质，并从理论上描述了其收敛行为。

Result: 通过在合成数据集及真实世界系统上的大量实证评估表明，HGAN-SDEs相较于现有针对SDEs的生成模型，在样本质量和学习效率方面都取得了更优的表现。

Conclusion: HGAN-SDEs作为一种创新的解决方案，有效地解决了当前学习SDEs路径分布存在的挑战，包括计算效率低下与训练过程中的不稳定性问题。它不仅提供了良好的理论基础支持，而且经实验证明在实际应用中也表现出色。

Abstract: Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs

</details>


### [52] [Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity](https://arxiv.org/abs/2512.20291)
*Yuxing Gan,Ziyu Lei*

Main category: cs.LG

TL;DR: 本文提出了一种名为CDSP-MoE的新框架，通过在共享物理子空间内动态实例化专家来解决现有Mixture-of-Experts设计中参数隔离和指令过拟合的问题。基于通用权重子空间假设，该模型利用可学习的拓扑掩码从超级完整的参数骨架中划分出逻辑专家，并通过延迟梯度博弈惩罚共享流形中的干扰连接，促使拓扑自动剪枝冲突路径并发展出可解释的模块结构。实验表明，CDSP-MoE能够在没有人为定义的任务标签情况下实现鲁棒的内容驱动路由，并且即使在缺乏明确指示的情况下也能保持语义专业化。


<details>
  <summary>Details</summary>
Motivation: 当前Mixture-of-Experts架构虽然实现了参数效率，但存在结构性参数隔离导致灾难性遗忘以及对指令过拟合从而影响无指令场景下性能的问题。

Method: 提出了CDSP-MoE框架，该框架通过从孤立的专家容器转向共享物理子空间内的动态专家实例化来应对上述挑战。它基于通用权重子空间假设工作，其中逻辑专家是通过可学习的拓扑掩码从一个超级完整的参数骨架中被‘雕刻’出来的。此外，采用了一种称为延迟梯度博弈的方法作为结构监督信号，以惩罚共享流形中的相互干扰连接，使得系统能够自发地修剪冲突路径，并演化出具有解释性的模块化结构。

Result: 实验结果证明了CDSP-MoE能够实现无需人类定义任务标签的内容驱动路由，并且即使是在在严格的盲推断协议（即没有明确指示）条件下，也能够保持语义上的专长。

Conclusion: CDSP-MoE提供了一种新颖的方法来克服传统Mixture-of-Experts设计中的限制，通过引入一种更加灵活、适应性强并且能够自我优化其内部结构的方式来增强模型的泛化能力和稳定性。

Abstract: Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts

</details>


### [53] [FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning](https://arxiv.org/abs/2512.20329)
*Mrinmay Sen,Subhrajit Nag*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习方法FedDPC，旨在通过减少数据异质性和部分客户端参与的影响来提高训练稳定性和全局模型性能。实验表明FedDPC在图像分类任务上优于现有的联邦学习算法，实现了更快的训练损失减少和更高的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 数据异质性是现代联邦学习中的一个重要挑战，因为它导致局部模型更新出现差异，使得聚合后的全局模型偏离了真正的全局最优解。此外，联邦学习中部分客户端的参与进一步加剧了这个问题，使得局部模型的聚合偏向于参与客户端的数据分布，增加了全局模型更新的额外方差，导致全局模型收敛远离全局目标的最佳点。这些问题导致联邦学习训练过程不稳定，降低了全局模型的表现，并减缓了联邦学习的训练速度。虽然现有文献主要关注解决数据异质性问题，但对于部分客户端参与造成的影响却较少被探讨。

Method: 为了解决上述问题，本研究提出了FedDPC，一种新型的联邦学习方法，通过将每个局部更新投影到前一个全局更新上来控制局部与全局更新中的方差，从而减轻数据异质性和部分客户端参与带来的负面影响。为了加速联邦学习训练过程，FedDPC还采用了自适应缩放技术，在聚合之前对每个局部更新进行调整。

Result: 通过对多个异构分割数据集上的图像分类任务进行广泛实验验证了FedDPC的有效性。结果表明，相比于最新的联邦学习算法，FedDPC能够在通信轮次中实现更快速的训练损失下降以及更高的测试准确性。

Conclusion: FedDPC作为一种创新的联邦学习方法，成功地解决了由数据异质性和部分客户端参与引起的问题，显著提升了联邦学习的效率和全局模型的质量。

Abstract: Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.

</details>


### [54] [Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation](https://arxiv.org/abs/2512.20346)
*Emilia Majerz,Witold Dzwinel,Jacek Kitowski*

Main category: cs.LG

TL;DR: 该论文提出了一种基于物理的机器学习方法，通过引入新的损失函数和基于输出变化的缩放机制来提高对粒子淋浴在探测器输出中空间分布和形态的准确表示，同时减少了罕见伪影对训练的影响。利用规范化流（NFs）在一种教师-学生生成框架下，展示了这种方法不仅优于经典的数据驱动模型同化，而且比ZDC模拟文献中的现有NF实现快421倍。


<details>
  <summary>Details</summary>
Motivation: 将领域知识直接嵌入到学习过程中，以提高模型的准确性和鲁棒性，并加速CERN ALICE实验的零度量能器(ZDC)仿真。

Method: 采用了一种新颖的损失函数和基于输出变异性调整机制的方法，并结合了规范化流(NFs)在一个教师-学生生成框架内工作。

Result: 所提出的方法在精确表示粒子淋浴的空间分布和形态方面表现优异，同时减少了罕见伪影对训练过程的影响。相比传统的数据驱动模型同化技术，该方法显著提高了性能，并且速度上比现有的NF实现快421倍。

Conclusion: 本文介绍的方法通过将物理知识与现代数据驱动技术相结合，在提高模型准确性的同时也大大加快了ZDC仿真速度。

Abstract: Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.

</details>


### [55] [Field-Space Attention for Structure-Preserving Earth System Transformers](https://arxiv.org/abs/2512.20350)
*Maximilian Witte,Johannes Meuer,Étienne Plésiat,Christopher Kadow*

Main category: cs.LG

TL;DR: 本文提出了一种新的地球系统Transformer架构——Field-Space Attention，它直接在物理域中计算注意力，而不是在学习到的潜在空间中。通过保持所有中间表示为球体上的连续场，该模型能够实现可解释的内部状态，并有助于实施科学约束条件。实验表明，与传统的视觉变换器和U-Net基线相比，Field-Space Transformers在全局温度超分辨率上收敛更快更稳定，且需要的参数明显减少。


<details>
  <summary>Details</summary>
Motivation: 为了准确并物理一致地模拟地球系统动态，需要开发一种可以直接作用于连续地理物理场并且保留其基本几何结构的机器学习架构。

Method: 引入了Field-Space Attention机制，这是一种专为地球系统设计的Transformer，它在物理领域而非学习到的潜在空间中计算注意力。此方法使用固定、非学习的多尺度分解技术，并学习输入场的结构保持变形，以促进粗细信息的一致整合。

Result: 当应用于HEALPix网格上的全球温度超分辨率时，Field-Space Transformers比传统的视觉变压器和U-Net基线更加迅速稳定地收敛，同时所需参数显著减少。

Conclusion: Field-Space Attention作为下一代地球系统预测和生成建模框架中的紧凑、可解释且基于物理基础的构建块而显得非常有前景。

Abstract: Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.

</details>


### [56] [BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples](https://arxiv.org/abs/2512.20403)
*Xuan-An Le,Minh-Nam Tran,Son Nguyen*

Main category: cs.LG

TL;DR: BRIDGE框架通过引入中等规模的助教模型来中介大型模型和小型模型之间的知识蒸馏过程，解决了直接蒸馏由于模型容量差距大及API成本高导致的问题。该方法在有限的数据子集上训练助教，并利用其生成合成理由以全数据集训练小模型，实现了性能提升同时大幅降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 为了解决从大型私有模型到小型可部署模型的知识蒸馏过程中遇到的关键容量-预算陷阱问题，即教师与学生之间存在千倍以上的容量差异阻碍了有效直传，同时高昂的API费用限制了大规模数据收集的可能性。

Method: 提出了一种名为BRIDGE（Budget-Aware Reasoning via Intermediate Distillation）的两阶段框架。第一阶段，在一个严格限制的数据子集上，使用零API成本管道选择出平衡熵难度和语义多样性的样本，让中等规模的助教（TA）学习黑盒教师的知识；第二阶段，利用这种不对称性——教师查询成本高而助教推断免费的特点，通过助教生成整个数据集的合成解释来训练小模型学生，并且在转移推理之前对小模型进行了指令调优课程。

Result: 实验表明，在医疗、法律和金融基准测试中，BRIDGE能够使学生模型的表现提高28-41%，与专有教师的能力差距缩小了12-16%，同时仅使用了十分之一的教师查询次数。此外，即使只消耗了5%的资源，BRIDGE也超越了使用全部预算的直接蒸馏基线。

Conclusion: BRIDGE提供了一个有效的方法来克服直接知识蒸馏时遇到的挑战，它不仅提高了小型模型的性能，还显著减少了所需资源，打破了传统意义上的成本-性能边界。

Abstract: Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.

</details>


### [57] [Machine Learning to Predict Digital Frustration from Clickstream Data](https://arxiv.org/abs/2512.20438)
*Jibin Joseph*

Main category: cs.LG

TL;DR: 本研究使用来自真实电子商务网站的点击流数据预测会话是否受挫。定义了基于愤怒爆发、来回导航（U型转弯）、购物车波动、搜索挣扎和长时间漫游的规则，并应用于540万原始点击流事件。从每个会话中构建表格特征并训练标准分类器模型，同时使用完整事件序列来训练区分性LSTM分类器。XGBoost达到了约90%的准确率，ROC AUC为0.9579；而LSTM表现最佳，准确率为91%，ROC AUC为0.9705。研究表明，仅通过前20到30次交互，LSTM就能可靠地预测用户挫败感。


<details>
  <summary>Details</summary>
Motivation: 许多企业依赖于移动应用程序和网站，因此用户在尝试完成任务时遇到的挫败感可能会导致销售损失和投诉。本研究旨在利用点击流数据预测用户会话是否感到挫败，以便企业可以采取措施减少用户的负面体验。

Method: 该研究首先根据愤怒爆发、来回导航（U型转弯）、购物车波动、搜索挣扎以及长时间漫游等行为定义了挫败感。接着，将这些规则应用到了540万个原始点击流事件上。对于每一个会话，研究人员构建了表格形式的特征，并训练了标准分类器模型。此外，还采用了完整的事件序列来训练一个区分性的LSTM分类器。

Result: XGBoost模型实现了大约90%的准确性，ROC AUC得分为0.9579；相比之下，LSTM分类器表现出色，其准确率达到约91%，且ROC AUC得分为0.9705。重要的是，实验表明即使只考虑用户最初的20至30次互动，LSTM也能有效预测出用户可能经历的挫败感。

Conclusion: 通过对实际电子商务平台上的点击流数据分析，本研究表明采用机器学习方法特别是LSTM模型能够高效准确地识别出用户在线上购物过程中遇到挫折的情况。这意味着企业有机会及早发现潜在问题并进行干预，从而改善用户体验、减少流失率。

Abstract: Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.

</details>


### [58] [Explainable time-series forecasting with sampling-free SHAP for Transformers](https://arxiv.org/abs/2512.20514)
*Matthias Hertel,Sebastian Pütz,Ralf Mikut,Veit Hagenmeyer,Benjamin Schäfer*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的可解释时间序列预测模型SHAPformer，该模型能够快速生成准确的解释，并在实际电力负荷数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在许多领域中对于规划和决策至关重要，而可解释性是建立用户信任和满足透明度要求的关键。现有的SHAP方法在处理时间序列时效率低下，且通常假设特征独立。

Method: 本文引入了SHAPformer，一种基于Transformer架构的高效、无需采样的可解释时间序列预测模型。它通过操控注意力机制来基于特征子集进行预测。

Result: SHAPformer能够在不到一秒钟的时间内生成解释，比SHAP排列解释器快几个数量级。在合成数据集上，SHAPformer提供的解释忠实于数据本身；而在真实世界电力负荷数据上的应用表明，它不仅达到了有竞争力的预测性能，还提供了有意义的局部和全局见解。

Conclusion: 研究证明了SHAPformer作为一种快速且准确的可解释时间序列预测工具的有效性，适用于需要透明度及深入洞察力的应用场景。

Abstract: Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.

</details>


### [59] [Performative Policy Gradient: Optimality in Performative Reinforcement Learning](https://arxiv.org/abs/2512.20576)
*Debabrota Basu,Udvas Das,Brahim Driss,Uddalak Mukherjee*

Main category: cs.LG

TL;DR: 本文提出了Performative Policy Gradient算法（PePG），这是首个考虑到RL中表现性的策略梯度算法。该算法在标准的表现性RL环境中表现出优于传统的策略梯度算法和现有的追求稳定性的表现性RL算法。


<details>
  <summary>Details</summary>
Motivation: 后部署的机器学习算法经常会影响它们所处的环境，从而改变标准强化学习(RL)方法忽略的基本动态。尽管在监督学习中设计这种情境下的最优算法已经被研究过，但RL中的对应问题仍然未得到充分探索。

Method: 本文证明了RL中的表现性性能差异引理和策略梯度定理，并引入了Performative Policy Gradient算法(PePG)。PePG是第一个为考虑RL中的表现性而设计的策略梯度算法。作者们证明了，在softmax参数化下，无论是否使用熵正则化，PePG都能收敛到表现性最优策略，即那些能够保持在自身诱导的分布变化下依然最优的策略。

Result: PePG显著扩展了先前在表现性RL方面的工作，这些工作实现了表现性稳定性但没有达到最优性。此外，通过在标准表现性RL环境中的实证分析验证了PePG优于标准策略梯度算法以及旨在实现稳定性的现有表现性RL算法。

Conclusion: 本研究为处理强化学习中的表现性影响提供了一种新的方法，PePG算法不仅考虑到了模型对环境的影响，而且能够找到在这种影响下的最优策略。

Abstract: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.

</details>


### [60] [Improving ML Training Data with Gold-Standard Quality Metrics](https://arxiv.org/abs/2512.20577)
*Leslie Barrett,Michael W. Sherman*

Main category: cs.LG

TL;DR: 本文提出了一种使用统计方法来评估和提高手工标记训练数据质量的方法，通过多次迭代标记记录的一致性和同意度指标能更可靠地反映数据质量。


<details>
  <summary>Details</summary>
Motivation: 尽管数据质量在标记过程中会有很大差异，但关于训练数据质量控制的研究却很少。

Method: 使用统计方法测量标记一致性和同意度，并通过多次迭代标记来记录这些指标的变化。

Result: 同意度指标如果是在多次迭代的标记过程中记录的话，会给出更可靠的结果；随着这些记录中的方差减少，表明数据质量正在提高。此外，还展示了一种无需为每个工作项提供多个标签即可收集高质量训练数据的方法，并指出仅依靠标注者的磨合期可能不足以最小化标注错误。

Conclusion: 该研究强调了通过统计方法监控和改善手工标记训练数据质量的重要性，以及在整个项目中持续关注数据质量而非仅仅依赖于初始培训阶段的价值。

Abstract: Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.

</details>


### [61] [Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning](https://arxiv.org/abs/2512.20605)
*Seijin Kobayashi,Yanick Schimpf,Maximilian Schlegel,Angelika Steger,Maciej Wolczyk,Johannes von Oswald,Nino Scherre,Kaitlin Maile,Guillaume Lajoie,Blake A. Richards,Rif A. Saurous,James Manyika,Blaise Agüera y Arcas,Alexander Meulemans,João Sacramento*

Main category: cs.LG

TL;DR: 研究提出了一种通过在自回归模型的内部表示中行动和探索来克服基于令牌采样动作导致的学习效率低下问题的方法。引入了一个高阶、非因果序列模型，其输出控制基础自回归模型的残差流激活。该方法使得学习从稀疏奖励中受益，并且能够实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 当前大尺度自回归模型在强化学习过程中通过逐个生成新输出来进行探索，这种方式在面对稀疏奖励时可能导致学习非常低效。研究旨在找到一种更有效率的学习途径，特别是当奖励信号稀缺时。

Method: 研究人员开发了一种新的方法，通过在自回归模型的内在表示上进行操作与探索来发现时间抽象的动作。这种方法包括使用一个更高阶、非因果性的序列模型，该模型的输出用于控制基本自回归模型中的残留流激活。

Result: 实验表明，在具有层次结构的任务上，所提出的高阶模型学会了将长激活序列片段压缩到内部控制器上。每个控制器执行一系列有意义的行为动作，这些动作在长时间范围内展开，并伴随着学会的终止条件，从而随着时间推移组合多个控制器可以有效地探索新任务。此外，直接对内部控制器进行强化（称为“内部RL”）使系统能够从标准RL微调失败的情况下学习稀疏奖励。

Conclusion: 研究表明，在自回归模型中生成潜在动作并对其进行强化（即所谓的内部RL）能够显著提高学习效率，特别是在奖励稀少的情境下。这为在基础模型内实现分层强化学习提供了一条有前景的道路。

Abstract: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [An Adaptive Distributed Stencil Abstraction for GPUs](https://arxiv.org/abs/2512.19851)
*Aditya Bhosale,Laxmikant Kale*

Main category: cs.DC

TL;DR: 提出了一种基于CharmTyles框架的自适应分布式抽象，用于多节点GPU上的模板计算。该方法以类似NumPy的语法提供从原型到生产代码转换的便利性，并展示了资源弹性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 科学计算生态系统主要局限于单节点并行，这在高阶原型设计和现代超级计算机的高性能执行之间造成了差距。硬件加速器的普及和对能源效率的需求使得资源适应性变得至关重要，但传统的HPC抽象仍然不够灵活。

Method: 利用基于自适应Charm++运行时的CharmTyles框架开发了一个新的抽象层，专门针对多节点GPU环境中的模板计算。这一抽象层采用了类似于NumPy的语法来减少从实验性质的代码过渡到生产就绪代码所需的工作量。

Result: 通过动态调整运行中应用程序所使用的节点数量，验证了所提抽象层的资源弹性，并对其相关的开销进行了性能分析。此外，还表明了该方法相较于专门的高性能模板领域特定语言(DSL)及通用NumPy替代品，在性能上有显著提高。

Conclusion: 提出的自适应分布式抽象有效地解决了当前科学计算领域面临的一些关键挑战，包括提高资源利用率、增强跨不同规模计算资源的灵活性以及改善整体性能等。

Abstract: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.

</details>


### [63] [Scaling Point-based Differentiable Rendering for Large-scale Reconstruction](https://arxiv.org/abs/2512.20017)
*Hexu Zhao,Xiaoteng Liu,Xiwen Min,Jianhao Huang,Youming Deng,Yanfei Li,Ang Li,Jinyang Li,Aurojit Panda*

Main category: cs.DC

TL;DR: 本文提出了Gaian，一个针对点基可微渲染(PBDR)的通用分布式训练系统，能够优化数据局部性并减少通信开销。在最多128个GPU上测试时，它能减少高达91%的通信量，并将训练吞吐量提高1.50到3.71倍。


<details>
  <summary>Details</summary>
Motivation: 现有的PBDR系统与特定的方法紧密耦合且因数据局部性差而遭受严重的通信开销问题。为了解决这些问题，并实现高分辨率和大规模场景下的高效扩展，研究者开发了Gaian系统。

Method: 研究团队设计了一个名为Gaian的通用分布式训练系统，该系统提供了一个统一的应用程序接口(API)，足以支持现存的各种PBDR方法。通过暴露丰富的数据访问信息，Gaian可以优化数据局部性并降低通信需求。

Result: Gaian实现了4种PBDR算法，并在多达六个数据集和最多128个GPU上进行了评估。结果表明，与现有系统相比，Gaian显著减少了通信量（最高达91%），同时提高了训练吞吐量（提升幅度为1.50至3.71倍）。

Conclusion: Gaian作为一个通用的分布式训练平台，在支持多种PBDR方法的同时，通过改善数据局部性和减少通信来提高性能和资源利用效率，这标志着向更高效的大规模3D场景重建迈进了一步。

Abstract: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.

</details>


### [64] [Population Protocols Revisited: Parity and Beyond](https://arxiv.org/abs/2512.20163)
*Leszek Gąsieniec,Tytus Grodzicki,Tomasz Jurdziński,Jakub Kowalski,Grzegorz Stachowiak*

Main category: cs.DC

TL;DR: 该论文提出了一种新的计算范式，通过结合群体权重、稳健的时钟机制和有效的异常检测与切换机制来解决同余谓词问题，特别是奇偶性计算。所提出的协议使用$O(\log^3 n)$状态并在$O(\log^3 n)$时间内实现静默稳定化。此外，还讨论了权重系统允许的一元和二进制表示之间的隐式转换对其他问题的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管人口协议在分布式计算领域得到了广泛研究，并为诸如领导者选举和多数计算等核心问题提供了高效解决方案，但在同余谓词（如奇偶性计算）方面尚未出现同时时间和空间效率高的协议。本研究旨在填补这一空白。

Method: 本文探索了奇偶性问题，并将解决方案扩展到任意模数$m$下的同余计算。新提出的计算范式包括群体权重、一个稳健的时钟机制以及有效异常检测加上切换机制的设计。

Result: 首次提出了对于奇偶性和同余计算有效的协议，这些协议仅需$O(\log^3 n)$的状态数量，并能在$O(\log^3 n)$时间内达到静默稳定状态。

Conclusion: 通过引入基于群体权重的新计算模型，本文不仅解决了特定的同余谓词问题，而且展示了如何利用一元与二进制表示间隐含转换来处理更多相关挑战。

Abstract: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.

</details>


### [65] [Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs](https://arxiv.org/abs/2512.20210)
*Yinan Ni,Xiao Yang,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 本文提出了一种名为P-LoRA的无服务器推理系统，旨在解决基于LoRA调整的大规模语言模型在无服务器环境中部署时遇到的冷启动延迟和GPU内存碎片化问题。通过引入轻量级LSTM预测器预取热门适配器以及受操作系统虚拟内存启发的分页式适配器内存管理机制，P-LoRA能够显著降低冷启动延迟并保持高GPU利用率。实验表明，在高并发情况下，P-LoRA相比S-LoRA吞吐量提高了1.52倍，同时减少了35%的首字节时间。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算范式为部署大规模语言模型（LLM）提供了弹性扩展和按需付费计费等优势，但在无服务器环境中通过低秩适应（LoRA）提供多个微调后的LLM服务面临着重大挑战：反应式的适配器加载导致显著的冷启动延迟，频繁更换适配器导致严重的GPU内存碎片化。

Method: 提出了Predictive-LoRA (P-LoRA)，这是一种针对基于LoRA的LLM设计的前瞻性且注重碎片化的无服务器推理系统。P-LoRA引入了两个关键创新：(1) 一个轻量级的基于LSTM的流量预测器，可以预测适配器需求并主动从主机内存到GPU预取热门适配器；(2) 受操作系统虚拟内存启发的基于页面的适配器内存管理机制，即使在异构适配器等级下也能保持GPU内存利用率高于87%。

Result: 使用从Azure Functions追踪中得出的类似生产的工作负载评估P-LoRA。实验结果表明，P-LoRA在高并发场景下的吞吐量比S-LoRA高出1.52倍，同时平均Time-To-First-Token (TTFT)降低了35%。

Conclusion: P-LoRA通过其创新的方法有效解决了无服务器环境下部署多微调LLM时遇到的关键问题，包括减少冷启动延迟和优化GPU内存利用效率。

Abstract: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [66] [Towards Analysing Invoices and Receipts with Amazon Textract](https://arxiv.org/abs/2512.19958)
*Sneha Oommen,Gabby Sanchez,Cassandra T. Britto,Di Wang,Jordan Chiou,Maria Spichkova*

Main category: cs.IR

TL;DR: 本研究评估了AWS Textract从不同格式和条件的收据中提取数据的能力，分析了其优缺点，并提出了相应的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 评估AWS Textract在处理多样化的收据时的数据提取性能，以理解其在实际应用中的优势与局限性。

Method: 使用包含多种格式及状况收据的数据集来测试Textract功能，并基于观察结果进行定性分析。

Result: Textract能够稳定地检测出收据总额，但图像质量和布局会影响其准确性；针对发现的问题，研究还提出了一些缓解措施。

Conclusion: 虽然AWS Textract在识别收据总额方面表现出色，但其性能易受图片质量等因素影响；通过实施特定策略可以改善这些限制。

Abstract: This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.

</details>


### [67] [LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews](https://arxiv.org/abs/2512.20022)
*Kian Godhwani,David Benrimoh*

Main category: cs.IR

TL;DR: 本研究开发了OLIVER，一个用于LLM辅助摘要筛选的开源管道，并评估了多个当代大型语言模型在两个非Cochrane系统评价中的表现。结果表明，不同模型的表现差异很大，单个模型配置下的校准能力普遍较弱。而采用演员-评论家框架可以提高分类质量，同时保持计算效率，从而以低成本实现大规模筛选。


<details>
  <summary>Details</summary>
Motivation: 先前的研究集中在早期的大规模语言模型、标准化的Cochrane评价、单一模型设置以及准确性作为主要度量标准上，但关于这些模型的一般性、配置效果和校准能力的研究相对较少。因此，本研究旨在探索现代大规模语言模型在系统评价筛选任务中的应用及性能优化。

Method: 研究人员开发了一个名为OLIVER的工具，它是一个基于优化的大规模语言模型的包含与审核引擎，用于辅助进行文献筛选。该研究还评估了多个最新的大规模语言模型在两个不属于Cochrane数据库的系统评价上的表现，并通过准确性、AUC值和校准度等指标来衡量其在全文筛选阶段和最终纳入阶段的效果。此外，还测试了一种结合两种轻量级模型的演员-评论家筛选框架。

Result: 结果显示，在不同的模型之间性能变化很大。对于较小规模的第一项评价（821篇摘要，63篇最终纳入），一些模型虽然能够达到较高的最终纳入敏感性，但同时也产生了大量的假阳性并且校准不佳。而在较大规模的第二项评价（7741篇摘要，71篇最终纳入）中，大多数模型都表现出很高的特异性，但在恢复真实纳入方面遇到了困难，提示设计影响了召回率。即使总体准确率很高，单个模型配置下校准能力普遍较差。使用演员-评论家筛选方法后，在两项评价中均显著提高了辨别力并大幅减少了校准误差，从而获得了更高的AUC值。

Conclusion: 尽管大规模语言模型有可能加速摘要筛选过程，但它们的表现高度依赖于评价特征、提示方式等因素，且校准能力有限。采用演员-评论家框架可以在保持计算效率的同时提高分类质量和置信度可靠性，从而支持以低成本执行大规模筛选工作。

Abstract: Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.
  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.
  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.
  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.

</details>


### [68] [VSA:Visual-Structural Alignment for UI-to-Code](https://arxiv.org/abs/2512.20034)
*Xian Wu,Ming Zhang,Zhiyu Fang,Fei Li,Bin Wang,Yong Jiang,Hao Zhou*

Main category: cs.IR

TL;DR: 本文提出了一种多阶段范式VSA，通过视觉-结构对齐来合成组织化的前端资产。该方法首先利用空间感知转换器将视觉输入重构为层次树表示，并通过算法模式匹配层识别重复的UI主题并将其封装成模块化模板。这些模板经过模式驱动的合成引擎处理后，生成适合生产环境的类型安全、属性钻取组件。实验结果表明，与最先进的基准相比，我们的框架在代码模块化和架构一致性方面有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前用户界面开发自动化的方法虽然有所进步，但主要产生的是非结构化的、扁平的代码库，这与React或Angular等面向组件的库不兼容。这类输出通常表现出低内聚性和高耦合性，使得长期维护变得复杂。

Method: 1. 使用空间感知转换器将视觉输入重构为层次树表示。
2. 采用算法模式匹配层识别重复出现的UI主题，并将其封装成模块化模板。
3. 利用模式驱动的合成引擎处理这些模板，保证大型语言模型能够生成类型安全、属性钻取的组件，适用于生产环境。

Result: 实验结果显示，所提出的框架在代码模块化和架构一致性方面相对于现有最先进基准有了显著提升，有效地填补了原始像素与可扩展软件工程之间的差距。

Conclusion: 本研究提出的VSA框架为用户界面开发自动化提供了一个新的视角，通过引入视觉-结构对齐技术，实现了从前端设计到高质量、可维护代码的有效转化。

Abstract: The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.

</details>


### [69] [Collaborative Group-Aware Hashing for Fast Recommender Systems](https://arxiv.org/abs/2512.20172)
*Yan Zhang,Li Deng,Lixin Duan,Ivor W. Tsang,Guowu Yang*

Main category: cs.IR

TL;DR: 本文提出了一种协作组感知哈希（CGAH）方法，通过整合内在的组信息来缓解稀疏性问题，从而提高基于哈希的推荐系统的准确性。实验表明，CGAH和CGAH-CF在不同稀疏设置下均优于最新的离散协同过滤方法和内容感知推荐方法。


<details>
  <summary>Details</summary>
Motivation: 快速在线推荐对于大规模数据库的应用至关重要；然而，在稀疏场景中提供准确推荐具有挑战性。尽管哈希技术能够加速在线推荐过程，但现有的基于哈希的推荐系统由于每个比特表示能力有限以及忽略了用户与项目之间的固有关系而导致精度较低，特别是在稀疏环境下表现不佳。

Method: 本文提出了一个名为协作组感知哈希（CGAH）的方法，该方法旨在通过结合内在群组信息来解决稀疏性问题，适用于协同过滤(CGAH-CF)和内容感知推荐(CGAH)两种场景。首先通过对用户和项目的潜在向量进行分类以提取其内在群组亲和力，然后将偏好定义为群组亲和力与哈希码相似性的内积。

Result: 通过在三个公开数据集上进行广泛的实验，结果证明了所提出的CGAH和CGAH-CF方法在不同稀疏度条件下相较于当前最先进的离散协同过滤方法及内容感知推荐方法展现出优越性能。

Conclusion: 研究结论是，利用内在群体信息学习哈希编码的CGAH方法能够生成比其他处理稀疏交互数据的离散方法更有效的哈希编码。这表明CGAH不仅提高了推荐速度，还增强了推荐质量，特别是在面临数据稀疏的情况下。

Abstract: The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.

</details>
