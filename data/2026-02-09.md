<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 13]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Scaling Mobile Chaos Testing with AI-Driven Test Execution](https://arxiv.org/abs/2602.06223)
*Juan Marcano,Ashish Samant,Kai Song,Lingchao Chen,Kaelan Mikowicz,Tim Smyth,Mengdie Zhang,Ali Zamani,Arturo Bravo Rovirosa,Sowjanya Puligadda,Srikanth Prodduturi,Mayank Bansal*

Main category: cs.SE

TL;DR: 本研究提出了一种自动化的移动混沌测试系统，集成了基于大语言模型的移动测试平台DragonCrawl和用于服务级故障注入的uHavoc。自2024年第一季度以来，该系统已在优步的Rider、Driver和Eats应用中执行了超过18万次自动化混沌测试，发现了23个韧性风险，并通过自动化根因分析将调试时间从几小时缩短到几分钟。


<details>
  <summary>Details</summary>
Motivation: 移动应用程序在大规模分布式系统中容易受到后端服务故障的影响，而传统的混沌工程方法由于需要验证的流、位置和故障场景组合爆炸，无法扩展移动测试。

Method: 开发了一个集成DragonCrawl（一个基于LLM的移动测试平台）与uHavoc（一种服务级别的故障注入系统）的自动化移动混沌测试系统。此系统利用适应性AI驱动的测试执行来导航处于降级后端条件下的移动应用程序，从而无需为每个用户流程、城市和故障类型的组合手动编写测试案例。

Result: 自2024年第一季度以来，该系统已经跨越优步Rider、Driver和Eats应用程序中的47个关键流执行了超过180,000次自动化混沌测试，代表了大约39,000小时的手动测试工作量，在这种规模下是不切实际的。识别出23个韧性风险，其中70%是架构依赖违规情况，非关键服务故障降低了核心用户流的质量。十二个问题严重到足以阻止行程请求或食物订单。有两个问题是仅通过移动混沌测试才能检测到的应用程序崩溃，而不是单独的后端测试。

Conclusion: 持续的移动韧性验证在生产规模上是可以实现的。自动化根因分析将调试时间从几小时减少到了几分钟，达到了归因于特定后端服务的移动故障的88%精度@5。

Abstract: Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.

</details>


### [2] [Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study](https://arxiv.org/abs/2602.06671)
*Shijia Dong,Haoruo Zhao,Paul Harvey*

Main category: cs.SE

TL;DR: 本文提出了一种名为AST(NIT)的抽象语法树增强和序列化方法，该方法保留了词法细节，并将结构信息编码为与大语言模型兼容的序列。实验表明，这种序列化的AST可以减少大语言模型输入的长度、缩短训练时间，同时保持高质量的代码摘要。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的代码摘要方法主要依赖于原始代码或仅部分整合了AST信号，未能充分探索完整AST表示对于提升大语言模型性能的潜力。为了改进这一情况，作者提出了一个新的方法来更好地利用AST信息。

Method: 通过开发一种新的AST增强及序列化技术——AST(NIT)，旨在更有效地捕捉源代码的结构特性并将其转换成适合大语言模型处理的形式。

Result: 实验结果证明，使用提出的AST(NIT)方法可以在不牺牲摘要质量的前提下显著降低LLaMA-3.1-8B模型处理Python代码时所需的数据量和训练时间。

Conclusion: 研究展示了如何通过有效利用AST信息来优化基于大语言模型的代码摘要流程，从而实现更加高效且准确的代码理解工具。

Abstract: Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.

</details>


### [3] [Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA](https://arxiv.org/abs/2602.06709)
*Duong Bui,Stefan Grintz,Alexander Berndt,Thomas Bach*

Main category: cs.SE

TL;DR: 本研究评估了基于大型语言模型（LLM）的系统在CI/CD管道失败管理中的自动化能力，特别是在SAP HANA这样的大型工业软件项目中。通过提供不同类型的领域知识，包括管道信息、失败管理指令和历史失败数据，该系统能够准确地识别错误位置并提出精确解决方案。特别是，历史失败数据对于提高系统准确性最为关键，使得系统在92.1%的情况下生成了精确解，并且在提供领域知识时，以97.4%的准确率正确识别错误位置。


<details>
  <summary>Details</summary>
Motivation: CI/CD管道失败的手动管理非常耗时。尽管如此，由于所需的信息是无结构化的，自动处理变得非同小可。鉴于大型语言模型（LLMs）在处理无结构化数据方面的能力，它们为自动化失败管理提供了潜在的解决方案。

Method: 研究者们开发了一个基于LLM的系统来尝试自动化SAP HANA CI/CD管道中的失败管理过程。为了支持LLM生成更精准的解决方案，向其提供了多种类型的领域知识：管道信息、失败管理指导以及来自过往失败的数据。此外，还进行了消融研究以确定哪一种类型的知识对提升解决方案准确性贡献最大。

Result: 结果显示，从过去失败中获得的数据对系统准确性的影响最大，让系统能够在92.1%的情况下给出确切答案。当给予领域特定知识时，系统识别错误发生位置的准确度达到了97.4%，相比之下，在没有这些额外信息辅助下，这一数值仅为84.2%。

Conclusion: 研究表明，当被赋予关于历史失败的相关数据时，大型语言模型展现出了作为CI/CD管道失败管理自动化工具的巨大潜力。

Abstract: CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.

</details>


### [4] [Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience](https://arxiv.org/abs/2602.06831)
*Marco De Luca,Domenico Amalfitano,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 本文提出了一种结构化流程，用于定义适用于工业环境中故障检测工作流的特定上下文软件度量阈值。通过分析三个实际的C语言嵌入式固件项目，并使用Coverity和Understand静态分析工具提取软件度量指标，经统计分析与假设检验后确定了能够区分有缺陷与无缺陷函数的度量标准及其实证阈值。实验验证表明这些阈值在识别易出错函数方面具有高精度。


<details>
  <summary>Details</summary>
Motivation: 在关键安全领域确保嵌入式固件软件质量至关重要，符合功能安全标准（如ISO 26262）需要强有力地保证软件可靠性。尽管基于机器学习的故障预测模型表现出高准确性，但其缺乏可解释性限制了它们在工业环境中的采用。开发者需要可以直接应用于软件质量保证过程并指导缺陷缓解策略的具体见解。

Method: 研究者们提出了一个结构化流程来定义适合集成到工业环境中故障检测工作流里的特定上下文软件度量阈值。通过对由工业合作伙伴提供的三个真实世界C语言嵌入式固件项目的分析，利用Coverity和Understand两种静态分析工具提取软件度量数据。通过统计分析和假设检验找出可以区分有缺陷与无缺陷函数的关键度量指标，并据此推导出经验阈值。

Result: 实验证明所提出的阈值能有效识别易出错函数且具有高精确度。这表明所得到的阈值可以作为故障预测的一种可解释解决方案，符合行业标准和SQA实践。

Conclusion: 该方法为黑盒AI模型提供了一个实用替代方案，允许开发人员系统地评估软件质量、采取预防措施，并将基于度量的故障预测整合进工业开发工作流程中以减少软件故障。

Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.

</details>


### [5] [TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code](https://arxiv.org/abs/2602.06875)
*Jiangping Huang,Wenguang Ye,Weisong Sun,Jian Zhang,Mingyue Zhang,Yang Liu*

Main category: cs.SE

TL;DR: TraceCoder is a multi-agent system that uses diagnostic probes, causal analysis, and historical learning to improve the accuracy and efficiency of repairing code generated by Large Language Models (LLMs). It achieves up to 34.43% improvement in Pass@1 accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often produce code with critical bugs, and current automated repair methods are inefficient due to their reliance on basic pass/fail signals and inability to learn from past failures. This leads to repetitive and ineffective repair cycles, necessitating a more sophisticated approach for error localization and correction.

Method: TraceCoder employs a collaborative multi-agent framework that mimics human expert behavior. It instruments code with diagnostic probes to gather detailed runtime traces, performs causal analysis to pinpoint failure causes, and incorporates a Historical Lesson Learning Mechanism (HLLM) to learn from past errors. A Rollback Mechanism ensures each repair iteration improves upon the previous one, leading to stable convergence towards the correct solution.

Result: Comprehensive experiments demonstrate that TraceCoder can achieve up to a 34.43% relative improvement in Pass@1 accuracy compared to advanced baselines. The iterative repair process alone contributes to a 65.61% relative gain in accuracy. Additionally, TraceCoder outperforms other leading iterative repair methods in both accuracy and cost-efficiency.

Conclusion: The introduction of TraceCoder represents a significant advancement in the automated repair of LLM-generated code, offering substantial improvements in accuracy and cost-efficiency through a novel combination of diagnostic tracing, causal analysis, and historical learning.

Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [NanoNet: Parameter-Efficient Learning with Label-Scarce Supervision for Lightweight Text Mining Model](https://arxiv.org/abs/2602.06093)
*Qianren Mao,Yashuo Luo,Ziqi Qin,Junnan Liu,Weifeng Jiang,Zhijun Chen,Zhuoran Li,Likang Xiao,Chuou Xu,Qili Zhang,Hanwen Hao,Jingzheng Li,Chunghua Lin,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级文本挖掘框架NanoNet，通过在线知识蒸馏生成多个小型模型，并利用相互学习正则化提高它们的性能，从而实现参数高效学习，减少训练成本和监督需求。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督学习策略虽然有效，但计算成本高且容易陷入局部最优解，这促使研究者探索结合有限标签监督、轻量级微调和快速推理小模型三种低成本场景进行文本挖掘任务的可能性。

Method: 提出了NanoNet框架，该框架采用在线知识蒸馏技术来创建多个小规模模型，并通过相互学习正则化增强这些模型的表现力；整个过程中强调了参数效率的学习方法以降低训练开销并最小化对标签数据的需求。

Result: 实验结果表明，NanoNet能够在保证较低训练成本和较少监督信息的前提下，为下游任务提供一个高效的轻量化模型解决方案。

Conclusion: 本研究表明，在面对资源受限环境时，通过设计如NanoNet这样的轻量级学习架构，可以有效地解决文本挖掘中遇到的挑战。

Abstract: The lightweight semi-supervised learning (LSL) strategy provides an effective approach of conserving labeled samples and minimizing model inference costs. Prior research has effectively applied knowledge transfer learning and co-training regularization from large to small models in LSL. However, such training strategies are computationally intensive and prone to local optima, thereby increasing the difficulty of finding the optimal solution. This has prompted us to investigate the feasibility of integrating three low-cost scenarios for text mining tasks: limited labeled supervision, lightweight fine-tuning, and rapid-inference small models. We propose NanoNet, a novel framework for lightweight text mining that implements parameter-efficient learning with limited supervision. It employs online knowledge distillation to generate multiple small models and enhances their performance through mutual learning regularization. The entire process leverages parameter-efficient learning, reducing training costs and minimizing supervision requirements, ultimately yielding a lightweight model for downstream inference.

</details>


### [7] [Toward Faithful and Complete Answer Construction from a Single Document](https://arxiv.org/abs/2602.06103)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: 本文提出了一种基于文档的推理框架EVE，通过结构化、可验证的管道分解高严谨性推理为提取、验证和枚举步骤，从而提高了召回率、精确度和F1分数，解决了大型语言模型在生成文本时无法保证完整性和忠实性的局限。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLMs）虽然能够产生流畅的文本，但其设计倾向于高概率延续而非基于源内容的详尽且忠实的回答，这与核心AI安全原则相冲突。为了解决这一问题，研究者提出了EVE框架。

Method: EVE框架采用一种结构化、可验证的流程，将高严谨性的推理过程分解为提取、验证和枚举三个步骤，与自由形式的提示相比，这种设计旨在确保生成内容的完整性和准确性。

Result: 实验结果表明，EVE的设计能够在召回率、精确度以及F1分数上实现一致且同时的改进，分别最高提升了24%、29%，对应的F1分数增加了31%。此外，EVE还能减轻因长度限制导致的生成截断问题。不过，由于自然语言本身的模糊性，EVE的表现也达到了某种饱和状态，体现了基于语言推理的基本限制。

Conclusion: EVE框架成功地打破了传统单次生成过程中覆盖范围与准确度之间的权衡，并显著提升了文档基础推理任务上的性能指标；然而，它也揭示了基于语言推理方法存在的内在局限。

Abstract: Modern large language models (LLMs) are powerful generators driven by statistical next-token prediction. While effective at producing fluent text, this design biases models toward high-probability continuations rather than exhaustive and faithful answers grounded in source content. As a result, directly applying LLMs lacks systematic mechanisms to ensure both completeness (avoiding omissions) and faithfulness (avoiding unsupported content), which fundamentally conflicts with core AI safety principles. To address this limitation, we present EVE, a structured framework for document-grounded reasoning.
  Unlike free-form prompting, EVE constrains generation to a structured, verifiable pipeline that decomposes high-rigor reasoning into extraction, validation, and enumeration. Empirically, this design enables consistent and simultaneous improvements in recall, precision, and F1-score: recall and precision increase by up to 24\% and 29\%, respectively, with a corresponding 31\% gain in F1-score. This effectively breaks the long-standing trade-off between coverage and accuracy typical of single-pass LLM generation, while also mitigating generation truncation caused by length limitations. At the same time, we emphasize that EVE exhibits performance saturation due to the inherent ambiguity of natural language, reflecting fundamental limits of language-based reasoning.

</details>


### [8] [Pragmatic Curiosity: A Hybrid Learning-Optimization Paradigm via Active Inference](https://arxiv.org/abs/2602.06104)
*Yingke Li,Anjali Parashar,Enlu Zhou,Chuchu Fan*

Main category: cs.LG

TL;DR: 提出了一种名为'实用好奇心'的新方法，该方法结合了性能改进与信息获取，在多种实际任务中表现出色，优于现有的贝叶斯优化和贝叶斯实验设计方法。


<details>
  <summary>Details</summary>
Motivation: 许多工程和科学工作流依赖于昂贵的黑盒评估，需要同时提高性能并减少不确定性。现有方法如贝叶斯优化（BO）和贝叶斯实验设计（BED）主要分别处理目标追求和信息寻求问题，对于学习与优化内在耦合的混合场景提供的指导有限。

Method: 通过主动推理提出“实用好奇心”，这是一种将实用性效用与认知信息增益相结合的学习-优化混合范式，其动作选择基于最小化预期自由能这一单一目标。

Result: 在包括约束系统识别、定向主动搜索以及未知偏好下的复合优化等多种现实世界的混合任务上展示了其实用效果和灵活性，并且在估计准确性、关键区域覆盖度以及最终解决方案质量方面均优于强BO型和BED型基线。

Conclusion: 实用好奇心提供了一个有效的框架来解决那些需要同时进行学习和优化的问题，在各种应用场景下都显示出了优越性。

Abstract: Many engineering and scientific workflows depend on expensive black-box evaluations, requiring decision-making that simultaneously improves performance and reduces uncertainty. Bayesian optimization (BO) and Bayesian experimental design (BED) offer powerful yet largely separate treatments of goal-seeking and information-seeking, providing limited guidance for hybrid settings where learning and optimization are intrinsically coupled. We propose "pragmatic curiosity," a hybrid learning-optimization paradigm derived from active inference, in which actions are selected by minimizing the expected free energy--a single objective that couples pragmatic utility with epistemic information gain. We demonstrate the practical effectiveness and flexibility of pragmatic curiosity on various real-world hybrid tasks, including constrained system identification, targeted active search, and composite optimization with unknown preferences. Across these benchmarks, pragmatic curiosity consistently outperforms strong BO-type and BED-type baselines, delivering higher estimation accuracy, better critical-region coverage, and improved final solution quality.

</details>


### [9] [Compressing LLMs with MoP: Mixture of Pruners](https://arxiv.org/abs/2602.06127)
*Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Leandro Giusti Mugnaini,Keith Ando Ogawa,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoP（Pruner混合）的迭代框架，该框架统一了深度和宽度剪枝两种方法。在LLaMA-2、LLaMA-3以及视觉-语言模型LLaVA-1.5上，MoP展示了超越现有方法的表现，不仅提高了计算效率，还通过文本恢复微调恢复了视觉任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对计算资源的需求很高，促使人们开发减少参数数量并加速推理的方法。尽管模型剪枝是一种有效的策略，但现有的方法通常只关注单个维度——深度或宽度。因此，需要一种能够同时考虑这两个维度的方法来提高压缩效果和计算效率。

Method: 提出了一个名为MoP（Mixture of Pruners）的新框架，它是一个迭代过程，每次迭代都会生成两个分支：一个用于深度剪枝，另一个用于宽度剪枝，并选择其中一个候选方案推进路径。

Result: 实验结果表明，在LLaMA-2和LLaMA-3上，与专注于单一维度的剪枝方法相比，MoP在多种压缩级别下均表现出更高的准确性。此外，当压缩率为40%时，MoP能够将端到端延迟降低39%。对于视觉-语言模型LLaVA-1.5，MoP同样有效提升了计算效率，并且即使是在视觉任务中，仅通过文本数据进行恢复性微调也能恢复性能。

Conclusion: MoP提供了一种新的方式来统一深度与宽度剪枝，从而为大型语言模型提供了更高效的压缩解决方案。这项工作不仅改进了结构化剪枝技术，而且成功地将这些进展应用于实际速度提升及跨模态任务中。

Abstract: The high computational demands of Large Language Models (LLMs) motivate methods that reduce parameter count and accelerate inference. In response, model pruning emerges as an effective strategy, yet current methods typically focus on a single dimension-depth or width. We introduce MoP (Mixture of Pruners), an iterative framework that unifies these dimensions. At each iteration, MoP generates two branches-pruning in depth versus pruning in width-and selects a candidate to advance the path. On LLaMA-2 and LLaMA-3, MoP advances the frontier of structured pruning, exceeding the accuracy of competing methods across a broad set of compression regimes. It also consistently outperforms depth-only and width-only pruning. Furthermore, MoP translates structural pruning into real speedup, reducing end-to-end latency by 39% at 40% compression. Finally, extending MoP to the vision-language model LLaVA-1.5, we notably improve computational efficiency and demonstrate that text-only recovery fine-tuning can restore performance even on visual tasks.

</details>


### [10] [Tempora: Characterising the Time-Contingent Utility of Online Test-Time Adaptation](https://arxiv.org/abs/2602.06136)
*Sudarshan Sreeram,Young D. Kwon,Cecilia Mascolo*

Main category: cs.LG

TL;DR: 本文提出了Tempora框架，用于评估在时间压力下的测试时自适应（TTA）方法。该框架包括模拟部署约束的时间场景、操作化测量的评估协议以及量化准确性-延迟权衡的时间相关效用度量。通过应用Tempora于七个TTA方法上，揭示了不同时间压力下排名的变化情况。


<details>
  <summary>Details</summary>
Motivation: 传统的测试时自适应(TTA)方法评价忽略了实际部署中可能遇到的时间限制问题，即未考虑到准确性与延迟之间的权衡。随着机器学习越来越多地应用于对延迟敏感的应用场景，开发一种能够在时间压力下有效评估TTA方法的新框架变得至关重要。

Method: 设计并实现了一个名为Tempora的评估框架，该框架引入了三种时间相关效用度量：针对具有硬截止日期异步流的离散效用、适用于交互式设置且价值随延迟减少而衰减的连续效用、以及面向预算受限部署的摊销效用。使用这个框架对ImageNet-C数据集上的七种TTA方法进行了共计240次的不同时间条件下的评估。

Result: 研究发现，在常规设定下表现优秀的TTA方法（如ETA）在时间压力条件下并不总是最佳选择；事实上，在41.2%的评估案例中，ETA的表现不如其他方法。此外，最高效用的方法会根据不同的图像损坏类型和时间压力条件发生变化，并不存在一个普遍适用的最佳方案。

Conclusion: Tempora框架首次允许对多种时间约束条件下的TTA方法进行系统性评估，揭示了不同情况下排名反转的原因及条件，为实践者提供了选择合适方法的视角，同时也为研究人员指明了可部署适应性研究的方向。

Abstract: Test-time adaptation (TTA) offers a compelling remedy for machine learning (ML) models that degrade under domain shifts, improving generalisation on-the-fly with only unlabelled samples. This flexibility suits real deployments, yet conventional evaluations unrealistically assume unbounded processing time, overlooking the accuracy-latency trade-off. As ML increasingly underpins latency-sensitive and user-facing use-cases, temporal pressure constrains the viability of adaptable inference; predictions arriving too late to act on are futile. We introduce Tempora, a framework for evaluating TTA under this pressure. It consists of temporal scenarios that model deployment constraints, evaluation protocols that operationalise measurement, and time-contingent utility metrics that quantify the accuracy-latency trade-off. We instantiate the framework with three such metrics: (1) discrete utility for asynchronous streams with hard deadlines, (2) continuous utility for interactive settings where value decays with latency, and (3) amortised utility for budget-constrained deployments. Applying Tempora to seven TTA methods on ImageNet-C across 240 temporal evaluations reveals rank instability: conventional rankings do not predict rankings under temporal pressure; ETA, a state-of-the-art method in the conventional setting, falls short in 41.2% of evaluations. The highest-utility method varies with corruption type and temporal pressure, with no clear winner. By enabling systematic evaluation across diverse temporal constraints for the first time, Tempora reveals when and why rankings invert, offering practitioners a lens for method selection and researchers a target for deployable adaptation.

</details>


### [11] [Optimistic Training and Convergence of Q-Learning -- Extended Version](https://arxiv.org/abs/2602.06146)
*Prashant Mehta,Sean Meyn*

Main category: cs.LG

TL;DR: 本文扩展了Q-learning与线性函数逼近在(ε,κ)-驯服吉布斯策略下的稳定性结果，并通过一维例子说明了在无意识策略下可能不存在或存在多个PBE解，从而表明算法收敛需要更多的结构。


<details>
  <summary>Details</summary>
Motivation: 先前的研究已经证明了在(ε, κ)-驯服吉布斯策略下，带线性函数逼近的Q-learning参数估计是有界的，并且存在投影贝尔曼方程(PBE)的解。然而，解的唯一性和在标准表格或线性MDP设置之外的收敛条件尚未明确。本文旨在进一步探讨这些问题，并澄清以往工作的边界。

Method: 本文采用理论分析的方法，通过构建具体的例子来展示在不同的策略（如无意识策略）下，PBE解的存在性和唯一性问题。同时，作者还讨论了在理想基底情况下，即真实Q函数位于基底张成的空间中时，PBE解的特性。

Result: 研究发现，在某些条件下（例如使用无意识策略训练时），可能存在没有PBE解或者存在多个解的情况，并且这些情况会导致算法不稳定。此外，即使在理想基底的情况下，也有可能对于所有足够小的ε > 0和κ ≥ 1，(ε, κ)-驯服吉布斯策略下的PBE存在两个解。

Conclusion: 该研究表明，为了保证Q-learning及其变体方法能够稳定地收敛，需要比之前假设的更多结构条件。这强调了在设计强化学习算法时考虑恰当的探索策略及状态-动作空间表示的重要性。

Abstract: In recent work it is shown that Q-learning with linear function approximation is stable, in the sense of bounded parameter estimates, under the $(\varepsilon,κ)$-tamed Gibbs policy; $κ$ is inverse temperature, and $\varepsilon>0$ is introduced for additional exploration. Under these assumptions it also follows that there is a solution to the projected Bellman equation (PBE). Left open is uniqueness of the solution, and criteria for convergence outside of the standard tabular or linear MDP settings.
  The present work extends these results to other variants of Q-learning, and clarifies prior work: a one dimensional example shows that under an oblivious policy for training there may be no solution to the PBE, or multiple solutions, and in each case the algorithm is not stable under oblivious training.
  The main contribution is that far more structure is required for convergence. An example is presented for which the basis is ideal, in the sense that the true Q-function is in the span of the basis. However, there are two solutions to the PBE under the greedy policy, and hence also for the $(\varepsilon,κ)$-tamed Gibbs policy for all sufficiently small $\varepsilon>0$ and $κ\ge 1$.

</details>


### [12] [MoSE: Mixture of Slimmable Experts for Efficient and Adaptive Language Models](https://arxiv.org/abs/2602.06154)
*Nurbek Tastan,Stefanos Laskaridis,Karthik Nandakumar,Samuel Horvath*

Main category: cs.LG

TL;DR: 本文提出了一种新的专家混合模型（MoSE），该模型允许每个专家在不同的宽度下执行，从而提供了一个连续的精度-计算权衡谱。实验表明，相比标准的MoE模型，MoSE可以在保持相当性能的同时显著减少FLOPs。


<details>
  <summary>Details</summary>
Motivation: 当前的Mixture-of-Experts (MoE) 模型虽然能够通过稀疏激活专家来高效扩展大型语言模型，但是一旦选择了某个专家，则会完全执行它，导致准确性和计算之间存在较大的不连续性。为了解决这个问题，并使得准确性与计算之间的权衡更加平滑，作者提出了Mixture of Slimmable Experts (MoSE)架构。

Method: 提出了Mixture of Slimmable Experts (MoSE) 架构，在此架构中每个专家都具有可变宽度执行能力的嵌套、可瘦身结构。此外，还介绍了一种结合多宽度训练与标准MoE目标函数的简单稳定训练方法。在推理阶段，探索了多种运行时宽度确定策略，包括一种轻量级测试时训练机制，该机制能够在固定预算下学习如何将路由器置信度/概率映射到专家宽度上。

Result: 基于GPT模型在OpenWebText数据集上的实验表明，相比于传统的MoE模型，所提出的MoSE不仅能在全宽度情况下匹配甚至超越其表现，而且还能持续地改善精度与成本之间的帕累托前沿，即在显著减少浮点运算次数(FLOPs)的前提下达到相近或更好的性能。

Conclusion: 研究展示了通过引入可变宽度执行能力至每个专家内，MoSE架构能够支持一个更为连续且灵活的准确性-计算开销调整范围。这为在不同资源约束条件下寻找最佳模型配置提供了新的可能性。

Abstract: Mixture-of-Experts (MoE) models scale large language models efficiently by sparsely activating experts, but once an expert is selected, it is executed fully. Hence, the trade-off between accuracy and computation in an MoE model typically exhibits large discontinuities. We propose Mixture of Slimmable Experts (MoSE), an MoE architecture in which each expert has a nested, slimmable structure that can be executed at variable widths. This enables conditional computation not only over which experts are activated, but also over how much of each expert is utilized. Consequently, a single pretrained MoSE model can support a more continuous spectrum of accuracy-compute trade-offs at inference time. We present a simple and stable training recipe for slimmable experts under sparse routing, combining multi-width training with standard MoE objectives. During inference, we explore strategies for runtime width determination, including a lightweight test-time training mechanism that learns how to map router confidence/probabilities to expert widths under a fixed budget. Experiments on GPT models trained on OpenWebText demonstrate that MoSE matches or improves upon standard MoE at full width and consistently shifts the Pareto frontier for accuracy vs. cost, achieving comparable performance with significantly fewer FLOPs.

</details>


### [13] [Latent Structure Emergence in Diffusion Models via Confidence-Based Filtering](https://arxiv.org/abs/2602.06155)
*Wei Wei,Yizhou Zeng,Kuntian Chen,Sophie Langer,Mariia Seleznova,Hung-Hsu Chou*

Main category: cs.LG

TL;DR: 本研究探索了扩散模型的潜在空间结构，通过预训练分类器对生成样本的信心分数来观察。研究发现，仅当关注那些能产生高信心度样本的初始噪声种子时，潜在空间才表现出明显的类别可分性。基于信心度的过滤揭示了与类别相关的潜在结构，这为条件生成提供了一种不同于基于引导方法的新途径。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索扩散模型的高维潜在空间是否具有足够的结构来预测生成样本的属性（如类别）。

Method: 采用预训练分类器对生成样本进行分类，并依据其分配的信心分数探究潜在空间中的结构；通过比较不同信心水平下的噪声子集之间的类别可测性以及检查潜在空间中的类别可分离性来进行分析。

Result: 结果显示，在考虑所有噪声实现时，潜在空间似乎大部分是无结构的；但当限定于产生高信心度样本的初始噪声种子时，则显示出明显的类别可分性。

Conclusion: 结论是存在与类别相关的潜在结构，这种结构只有在基于信心度的过滤下才能被观察到；并且基于信心度的过滤可以作为一种替代基于引导的方法实现条件生成。

Abstract: Diffusion models rely on a high-dimensional latent space of initial noise seeds, yet it remains unclear whether this space contains sufficient structure to predict properties of the generated samples, such as their classes. In this work, we investigate the emergence of latent structure through the lens of confidence scores assigned by a pre-trained classifier to generated samples. We show that while the latent space appears largely unstructured when considering all noise realizations, restricting attention to initial noise seeds that produce high-confidence samples reveals pronounced class separability. By comparing class predictability across noise subsets of varying confidence and examining the class separability of the latent space, we find evidence of class-relevant latent structure that becomes observable only under confidence-based filtering. As a practical implication, we discuss how confidence-based filtering enables conditional generation as an alternative to guidance-based methods.

</details>


### [14] [SCONE: A Practical, Constraint-Aware Plug-in for Latent Encoding in Learned DNA Storage](https://arxiv.org/abs/2602.06157)
*Cihan Ruan,Lebin Zhou,Rongduo Han,Linyi Han,Bingqing Zhao,Chenchen Zhu,Wei Jiang,Wei Wang,Nam Ling*

Main category: cs.LG

TL;DR: SCONE 是一种新的DNA数据存储编码方法，通过在DNA碱基上直接进行四进制算术编码来简化神经压缩管道与DNA编码的集成过程。它利用约束感知自适应编码模块在编码过程中动态调整熵编码器的学习概率分布，以满足生物化学约束条件，如GC平衡和同聚物抑制。实验表明，SCONE能够在几乎不增加计算负担的情况下实现接近完美的约束条件满足，为端到端兼容DNA的学习型编解码器提供了一个与潜在变量无关的接口。


<details>
  <summary>Details</summary>
Motivation: 现有的DNA编码技术在整合入神经压缩流程时效率低下，主要因为它们通常将学习到的数据潜在表示转换成DNA序列时采用简单的二进制到四进制转码方式，这忽略了熵模型优化的好处。此外，当前方法还需要额外步骤来纠正不符合生物化学约束的情况，比如保持GC平衡和抑制同聚物形成等。

Method: 提出了SCONE框架，该框架能够在一个步骤中完成潜在压缩和DNA编码。SCONE直接在DNA基础上对潜在空间执行四进制算术编码，并引入了约束感知自适应编码模块，该模块能够在编码期间动态地指导熵编码器学习的概率分布，从而强制执行生物化学约束条件（如GC平衡和同聚物抑制）。这种方法保证了过程完全可逆，并且充分利用了超先验模型所学到的先验知识。

Result: 实验结果表明，SCONE能够以极低的计算开销（<2%延迟）达到近乎完美的生物化学约束条件满足度，为构建端到端兼容DNA存储的学习型编解码器提供了可能。

Conclusion: SCONE 提出了一种有效的方法，用于改善神经压缩管线与DNA编码之间的集成问题，通过直接处理潜在空间中的数据并确保符合关键的生物化学限制，同时保持高效性和可逆性。

Abstract: DNA storage has matured from concept to practical stage, yet its integration with neural compression pipelines remains inefficient. Early DNA encoders applied redundancy-heavy constraint layers atop raw binary data - workable but primitive. Recent neural codecs compress data into learned latent representations with rich statistical structure, yet still convert these latents to DNA via naive binary-to-quaternary transcoding, discarding the entropy model's optimization. This mismatch undermines compression efficiency and complicates the encoding stack. A plug-in module that collapses latent compression and DNA encoding into a single step. SCONE performs quaternary arithmetic coding directly on the latent space in DNA bases. Its Constraint-Aware Adaptive Coding module dynamically steers the entropy encoder's learned probability distribution to enforce biochemical constraints - Guanine-Cytosine (GC) balance and homopolymer suppression - deterministically during encoding, eliminating post-hoc correction. The design preserves full reversibility and exploits the hyperprior model's learned priors without modification. Experiments show SCONE achieves near-perfect constraint satisfaction with negligible computational overhead (<2% latency), establishing a latent-agnostic interface for end-to-end DNA-compatible learned codecs.

</details>


### [15] [$f$-FUM: Federated Unlearning via min--max and $f$-divergence](https://arxiv.org/abs/2602.06187)
*Radmehr Karimian,Amirhossein Bagheri,Meghdad Kurmanji,Nicholas D. Lane,Gholamali Aminian*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦遗忘框架，该框架被表述为一个最小-最大优化问题，旨在在保持其他数据性能的同时最大化包含所有数据训练的模型与不包含特定数据点重新训练的模型之间的f-散度。


<details>
  <summary>Details</summary>
Motivation: 随着法律和道德要求的提高，如“被遗忘的权利”以及减轻数据中毒攻击的需求，联邦学习中对有原则的数据遗忘提出了迫切需求。不同于集中式设置，联邦学习的分布式特性使得移除个别数据贡献变得复杂。

Method: 作者提出了一种新颖的联邦遗忘框架，形式化为一个最小-最大优化问题。其目标是在最大化使用全部数据训练的模型与没有特定数据点重训模型之间的f-散度的同时，尽量减少对保留下来的数据的影响。

Result: 通过实证评估表明，相比直接重新训练的方法，该方法能够显著加快处理速度，并且对实用性影响极小。

Conclusion: 所提出的联邦遗忘框架能够有效地近似联邦环境下数据移除的效果，同时作为插件几乎可以添加到任何联邦学习配置中，而不需要像现有技术那样在服务器端进行模型降级或涉及模型架构及权重。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for collaborative machine learning across decentralized data sources, preserving privacy by keeping data local. However, increasing legal and ethical demands, such as the "right to be forgotten", and the need to mitigate data poisoning attacks have underscored the urgent necessity for principled data unlearning in FL. Unlike centralized settings, the distributed nature of FL complicates the removal of individual data contributions. In this paper, we propose a novel federated unlearning framework formulated as a min-max optimization problem, where the objective is to maximize an $f$-divergence between the model trained with all data and the model retrained without specific data points, while minimizing the degradation on retained data. Our framework could act like a plugin and be added to almost any federated setup, unlike SOTA methods like (\cite{10269017} which requires model degradation in server, or \cite{khalil2025notfederatedunlearningweight} which requires to involve model architecture and model weights). This formulation allows for efficient approximation of data removal effects in a federated setting. We provide empirical evaluations to show that our method achieves significant speedups over naive retraining, with minimal impact on utility.

</details>


### [16] [Degradation of Feature Space in Continual Learning](https://arxiv.org/abs/2602.06586)
*Chiara Lanza,Roberto Pereira,Marco Miozzo,Eduard Angelats,Paolo Dini*

Main category: cs.LG

TL;DR: 本文探讨了在持续学习环境中促进特征空间各向同性是否能提高表示质量。通过实验发现，各向同性的正则化不仅未能改善模型准确性，在某些情况下反而降低了其性能。这表明集中式学习与持续学习之间存在本质差异，各向同性可能不适合作为非平稳学习场景下的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决持续学习中著名的可塑性-稳定性困境，特别是探索是否通过强制执行各向同性来实现更好的平衡，并减少灾难性遗忘现象。

Method: 采用了对比持续学习技术，在CIFAR-10和CIFAR-100数据集上进行实验，以评估促进特征空间各向同性对模型准确性和表现的影响。

Result: 实验结果显示，在持续学习设置下，各向同性的正则化方法未能提升模型的准确性，有时甚至导致性能下降。

Conclusion: 研究表明，虽然各向同性在集中式学习场景中有益，但它可能不适用于处理非平稳数据流的持续学习情况。

Abstract: Centralized training is the standard paradigm in deep learning, enabling models to learn from a unified dataset in a single location. In such setup, isotropic feature distributions naturally arise as a mean to support well-structured and generalizable representations. In contrast, continual learning operates on streaming and non-stationary data, and trains models incrementally, inherently facing the well-known plasticity-stability dilemma. In such settings, learning dynamics tends to yield increasingly anisotropic feature space. This arises a fundamental question: should isotropy be enforced to achieve a better balance between stability and plasticity, and thereby mitigate catastrophic forgetting? In this paper, we investigate whether promoting feature-space isotropy can enhance representation quality in continual learning. Through experiments using contrastive continual learning techniques on CIFAR-10 and CIFAR-100 data, we find that isotropic regularization fails to improve, and can in fact degrade, model accuracy in continual settings. Our results highlight essential differences in feature geometry between centralized and continual learning, suggesting that isotropy, while beneficial in centralized setups, may not constitute an appropriate inductive bias for non-stationary learning scenarios.

</details>


### [17] [Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning](https://arxiv.org/abs/2602.06204)
*Nan Chen,Soledad Villar,Soufiane Hayou*

Main category: cs.LG

TL;DR: 本文提出了Maximal-Update Adaptation（$\mu$A）框架，该框架解释了最优学习率如何随着模型宽度和适配器秩变化而调整，从而在标准配置下产生稳定且非消失的特征更新。实验表明，根据$\mu$A理论确定的学习率可以从LoRA成功转移到完全微调，大大降低了全微调时学习率调整的成本。


<details>
  <summary>Details</summary>
Motivation: 低秩适应（LoRA）是大型模型参数高效微调的标准工具，尽管它占用内存小，但其训练动态却因初始化、适配器秩和学习率等超参数的影响而变得复杂。特别是，不清楚最佳学习率应该如何随适配器秩变化而调整，这使得每当改变秩时都需要重新调整学习率。为了解决这个问题，研究者们引入了一个新的理论框架。

Method: 提出了一种名为Maximal-Update Adaptation（$\mu$A）的新理论框架，该框架受Maximal-Update Parametrization（$\mu$P）启发，用于预训练。通过使用超参数转移技术，分析揭示了根据不同初始化和LoRA缩放因子，最优学习率展示出不同的缩放模式。此外，还识别出了两种状态：一种状态下最优学习率大致与秩无关；另一种则显示学习率与秩成反比关系。

Result: 实验跨越了语言、视觉、视觉-语言、图像生成以及强化学习任务，验证了所提出的缩放规则，并显示出基于LoRA调整的学习率能够可靠地转移到全量微调中。

Conclusion: 通过引入$\mu$A框架，研究人员不仅提供了一种理解最优学习率如何随模型宽度和适配器秩变化的方法，而且还找到了一种配置，允许将针对LoRA优化的学习率直接应用于全量微调，极大地减少了全量微调时所需的学习率调节成本。

Abstract: Low-Rank Adaptation (LoRA) is a standard tool for parameter-efficient finetuning of large models. While it induces a small memory footprint, its training dynamics can be surprisingly complex as they depend on several hyperparameters such as initialization, adapter rank, and learning rate. In particular, it is unclear how the optimal learning rate scales with adapter rank, which forces practitioners to re-tune the learning rate whenever the rank is changed. In this paper, we introduce Maximal-Update Adaptation ($μ$A), a theoretical framework that characterizes how the "optimal" learning rate should scale with model width and adapter rank to produce stable, non-vanishing feature updates under standard configurations. $μ$A is inspired from the Maximal-Update Parametrization ($μ$P) in pretraining. Our analysis leverages techniques from hyperparameter transfer and reveals that the optimal learning rate exhibits different scaling patterns depending on initialization and LoRA scaling factor. Specifically, we identify two regimes: one where the optimal learning rate remains roughly invariant across ranks, and another where it scales inversely with rank. We further identify a configuration that allows learning rate transfer from LoRA to full finetuning, drastically reducing the cost of learning rate tuning for full finetuning. Experiments across language, vision, vision--language, image generation, and reinforcement learning tasks validate our scaling rules and show that learning rates tuned on LoRA transfer reliably to full finetuning.

</details>


### [18] [SR4-Fit: An Interpretable and Informative Classification Algorithm Applied to Prediction of U.S. House of Representatives Elections](https://arxiv.org/abs/2602.06229)
*Shyam Sundar Murali Krishnan,Dean Frederick Hougen*

Main category: cs.LG

TL;DR: 本文提出了一种新的可解释分类算法SR4-Fit，该算法在保持高分类性能的同时解决了现有黑盒模型和基于规则的传统算法的局限性。通过分析美国国会选区的人口特征，SR4-Fit能够以前所未有的准确性和可解释性预测众议院选举结果，并揭示了影响预测结果的人口因素组合。此外，SR4-Fit还在六个额外公开的数据集上得到了验证，展示了其优越的准确性、简洁性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习的增长，对于关键应用需要有可解释的模型，但大多数高性能模型是“黑箱”系统，这掩盖了输入输出之间的关系；同时像RuleFit这样的传统基于规则的算法虽然简单但缺乏预测力且不稳定。为了解决这些问题，研究者开发了SR4-Fit算法。

Method: 研究者们提出了一个名为Sparse Relaxed Regularized Regression Rule-Fit (SR4-Fit)的新颖分类算法。该方法结合了稀疏放松正则化回归技术与规则适应框架，旨在提高模型的可解释性而不牺牲预测表现。

Result: 实验结果显示，SR4-Fit不仅能以非常高的准确率预测众议院选举结果，还能够发现影响预测结果的具体人口因素组合。此外，在包括乳腺癌、Ecoli等在内的六个其他公开数据集上的测试也表明了SR4-Fit相比既有黑盒模型及其它可解释规则基算法如RuleFit，在准确性、简洁性和稳定性方面均有所超越。

Conclusion: SR4-Fit算法成功地解决了模型可解释性和预测能力之间的传统权衡问题，它不仅提供了优秀的预测性能，而且还生成了稳定且易于理解的规则集合。这意味着SR4-Fit可以在不牺牲预测精度的情况下为用户提供更深层次的理解。

Abstract: The growth of machine learning demands interpretable models for critical applications, yet most high-performing models are ``black-box'' systems that obscure input-output relationships, while traditional rule-based algorithms like RuleFit suffer from a lack of predictive power and instability despite their simplicity. This motivated our development of Sparse Relaxed Regularized Regression Rule-Fit (SR4-Fit), a novel interpretable classification algorithm that addresses these limitations while maintaining superior classification performance. Using demographic characteristics of U.S. congressional districts from the Census Bureau's American Community Survey, we demonstrate that SR4-Fit can predict House election party outcomes with unprecedented accuracy and interpretability. Our results show that while the majority party remains the strongest predictor, SR4-Fit has revealed intrinsic combinations of demographic factors that affect prediction outcomes that were unable to be interpreted in black-box algorithms such as random forests. The SR4-Fit algorithm surpasses both black-box models and existing interpretable rule-based algorithms such as RuleFit with respect to accuracy, simplicity, and robustness, generating stable and interpretable rule sets while maintaining superior predictive performance, thus addressing the traditional trade-off between model interpretability and predictive capability in electoral forecasting. To further validate SR4-Fit's performance, we also apply it to six additional publicly available classification datasets, like the breast cancer, Ecoli, page blocks, Pima Indians, vehicle, and yeast datasets, and find similar results.

</details>


### [19] [RuleSmith: Multi-Agent LLMs for Automated Game Balancing](https://arxiv.org/abs/2602.06232)
*Ziyao Zeng,Chen Liu,Tianyu Liu,Hao Wang,Xiatao Sun,Fengyu Yang,Xiaofeng Liu,Zhiwen Fan*

Main category: cs.LG

TL;DR: RuleSmith是首个利用多智能体大语言模型推理能力实现游戏自动平衡的框架，通过结合游戏引擎、多智能体大语言模型自对弈及贝叶斯优化技术，在CivMini游戏中成功展示了其在高效搜索参数空间以达成高度平衡配置的能力。


<details>
  <summary>Details</summary>
Motivation: 游戏平衡是一个长期存在的挑战，需要反复的游戏测试、专家直觉以及大量的手动调整。为解决这一问题，提出了RuleSmith框架，旨在通过自动化过程简化游戏平衡的复杂性。

Method: RuleSmith通过整合游戏引擎、多智能体LLM自对弈和操作于多维规则空间上的贝叶斯优化来实现游戏平衡。特别地，它采用基于获取函数的自适应采样与离散投影技术，使得有潜力的参数组合能够获得更多评估机会，而探索性质较强的则减少评估次数，从而有效探索参数景观。

Result: 实验结果表明，RuleSmith能够收敛到高度平衡的游戏配置，并提供可以直接应用于下游游戏系统的可解释规则调整建议。

Conclusion: 研究表明，LLM模拟可以作为复杂多智能体环境中设计和平衡自动化的有力替代方案，为游戏开发者提供了新的工具。

Abstract: Game balancing is a longstanding challenge requiring repeated playtesting, expert intuition, and extensive manual tuning. We introduce RuleSmith, the first framework that achieves automated game balancing by leveraging the reasoning capabilities of multi-agent LLMs. It couples a game engine, multi-agent LLMs self-play, and Bayesian optimization operating over a multi-dimensional rule space. As a proof of concept, we instantiate RuleSmith on CivMini, a simplified civilization-style game containing heterogeneous factions, economy systems, production rules, and combat mechanics, all governed by tunable parameters. LLM agents interpret textual rulebooks and game states to generate actions, to conduct fast evaluation of balance metrics such as win-rate disparities. To search the parameter landscape efficiently, we integrate Bayesian optimization with acquisition-based adaptive sampling and discrete projection: promising candidates receive more evaluation games for accurate assessment, while exploratory candidates receive fewer games for efficient exploration. Experiments show that RuleSmith converges to highly balanced configurations and provides interpretable rule adjustments that can be directly applied to downstream game systems. Our results illustrate that LLM simulation can serve as a powerful surrogate for automating design and balancing in complex multi-agent environments.

</details>


### [20] [Provably avoiding over-optimization in Direct Preference Optimization without knowing the data distribution](https://arxiv.org/abs/2602.06239)
*Adam Barla,Emanuele Nevali,Luca Viano,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出了PEPO算法，一种基于悲观集合偏好的单步直接偏好优化方法，旨在解决偏好学习中的过度优化问题。它不需要了解数据生成分布或学习显式奖励模型，通过在不同数据子集上训练一组偏好优化策略并以最坏情况构建来实现悲观性，从而促进模型间的一致性。该方法在表格环境中仅依赖于单一策略集中系数即可达到样本复杂度保证，并在实践中表现出色，同时保持了DPO风格训练的简单性和实用性。


<details>
  <summary>Details</summary>
Motivation: 为了解决偏好学习中存在的过度优化问题，提出了一种不需要知道数据生成分布也不需要学习明确奖励模型的新方法。

Method: 开发了名为PEPO的方法，它通过在不相交的数据子集上训练偏好优化策略的集合，并采用倾向于跨模型一致性的最坏情况构造法来聚合这些策略，以此实现悲观主义。

Result: PEPO在表格环境下表现出了只依赖于单一策略集中系数的样本复杂度保证，避免了影响易过优化算法（如DPO）保证的所有策略集中性问题。此外，它还在实践性能上给出了令人信服的结果，同时保留了DPO风格训练的简便性和实际应用性。

Conclusion: PEPO作为一种新颖的直接偏好优化算法，在理论上和实践中都展现出了对处理偏好学习中过度优化问题的有效性，同时保持了实施上的简洁与高效。

Abstract: We introduce PEPO (Pessimistic Ensemble based Preference Optimization), a single-step Direct Preference Optimization (DPO)-like algorithm to mitigate the well-known over-optimization issue in preference learning without requiring the knowledge of the data-generating distribution or learning an explicit reward model. PEPO achieves pessimism via an ensemble of preference-optimized policies trained on disjoint data subsets and then aggregates them through a worst case construction that favors the agreement across models. In the tabular setting, PEPO achieves sample complexity guarantees depending only on a single-policy concentrability coefficient, thus avoiding the all-policy concentrability which affects the guarantees of algorithms prone to over-optimization, such as DPO. The theoretical findings are corroborated by a convincing practical performance, while retaining the simplicity and the practicality of DPO-style training.

</details>


### [21] [REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop](https://arxiv.org/abs/2602.06248)
*Patryk Rybak,Paweł Batorski,Paul Swoboda,Przemysław Spurek*

Main category: cs.LG

TL;DR: 本文提出了一种名为REBEL的进化方法，用于生成对抗性提示，以测试从训练模型中移除的数据是否真的被删除。实验表明，REBEL能够从看似已经遗忘的数据中恢复出知识，揭示了当前的机器去学习方法可能只提供了表面层次的保护。在TOFU和WMDP基准测试子集上的评估显示，REBEL在攻击成功率方面优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 目前针对大型语言模型（LLMs）的数据去除方法的有效性尚不清楚，标准评估指标往往将表面信息抑制误认为是真正知识的移除，并且无法检测到通过更复杂的提示策略仍可提取的剩余知识。因此，需要一种新的方法来有效探测已‘忘记’的数据是否真的被移除。

Method: 引入了REBEL，这是一种为对抗性提示生成而设计的进化方法，旨在探究看似已被遗忘的数据是否仍然可以被恢复。该方法通过进化算法生成能够触发模型中潜在记忆的提示。

Result: 实验结果表明，在TOFU和WMDP基准测试子集上，REBEL在不同的数据去除算法中都能持续表现出色，其攻击成功率分别达到了60%和93%，远高于静态基线。

Conclusion: 研究发现当前的数据去除技术可能仅提供了一个表层的安全感，而REBEL则能有效地揭示这些表面上已经被移除的信息实际上仍旧存在于模型之中。

Abstract: Machine unlearning for LLMs aims to remove sensitive or copyrighted data from trained models. However, the true efficacy of current unlearning methods remains uncertain. Standard evaluation metrics rely on benign queries that often mistake superficial information suppression for genuine knowledge removal. Such metrics fail to detect residual knowledge that more sophisticated prompting strategies could still extract. We introduce REBEL, an evolutionary approach for adversarial prompt generation designed to probe whether unlearned data can still be recovered. Our experiments demonstrate that REBEL successfully elicits ``forgotten'' knowledge from models that seemed to be forgotten in standard unlearning benchmarks, revealing that current unlearning methods may provide only a superficial layer of protection. We validate our framework on subsets of the TOFU and WMDP benchmarks, evaluating performance across a diverse suite of unlearning algorithms. Our experiments show that REBEL consistently outperforms static baselines, recovering ``forgotten'' knowledge with Attack Success Rates (ASRs) reaching up to 60% on TOFU and 93% on WMDP. We will make all code publicly available upon acceptance. Code is available at https://github.com/patryk-rybak/REBEL/

</details>


### [22] [Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions](https://arxiv.org/abs/2602.06256)
*Navita Goyal,Hal Daumé*

Main category: cs.LG

TL;DR: 本文提出了一个框架来评估模型引导的特异性，包括一般性、控制性和鲁棒性三个维度，并通过减少过度拒绝和忠实幻觉两个安全关键用例研究了模型引导的效果。研究表明，虽然引导方法在有效性和保持一般及控制特异性方面表现良好，但在保持鲁棒性特异性上却失败了。


<details>
  <summary>Details</summary>
Motivation: 尽管模型引导的有效性已被广泛研究，但对其是否仅改变预期属性而未对目标属性相关行为产生意外变化的评价仍然有限。因此，作者提出了一种新的框架来区分特异性的三个维度：一般性（保持流畅度和无关能力）、控制性（保持相关控制属性）以及鲁棒性（在分布变化下保持控制属性）。

Method: 文章设计了一个框架用于评估模型引导时的特异性，涵盖了一般性、控制性与鲁棒性三个角度。随后，选取了减少‘过度拒绝’和‘忠实幻觉’作为两个安全性至关重要的应用场景进行研究。

Result: 研究发现，在减少过度拒绝的情况下，所有引导方法都能有效降低过度拒绝率而不损害一般能力和对于有害查询的拒绝；然而，它们显著增加了被绕过的风险。这表明，现有的标准效能和特异性检查不足以全面评估引导方法的安全性，因为缺乏鲁棒性评估可能会导致看似可靠的引导实际上削弱了模型的安全性。

Conclusion: 本研究首次系统地评估了模型引导中的特异性问题，强调了仅仅依赖于传统的效能和特异性测试是不够的，特别是在没有考虑鲁棒性的情况下，可能导致误判引导方法的安全可靠性。

Abstract: Model steering, which involves intervening on hidden representations at inference time, has emerged as a lightweight alternative to finetuning for precisely controlling large language models. While steering efficacy has been widely studied, evaluations of whether interventions alter only the intended property remain limited, especially with respect to unintended changes in behaviors related to the target property. We call this notion specificity. We propose a framework that distinguishes three dimensions of specificity: general (preserving fluency and unrelated abilities), control (preserving related control properties), and robustness (preserving control properties under distribution shifts). We study two safety-critical use cases: steering models to reduce overrefusal and faithfulness hallucinations, and show that while steering achieves high efficacy and largely maintains general and control specificity, it consistently fails to preserve robustness specificity. In the case of overrefusal steering, for example, all steering methods reduce overrefusal without harming general abilities and refusal on harmful queries; however, they substantially increase vulnerability to jailbreaks. Our work provides the first systematic evaluation of specificity in model steering, showing that standard efficacy and specificity checks are insufficient, because without robustness evaluation, steering methods may appear reliable even when they compromise model safety.

</details>


### [23] [On Randomized Algorithms in Online Strategic Classification](https://arxiv.org/abs/2602.06257)
*Chase Hutton,Adam Melrod,Han Shao*

Main category: cs.LG

TL;DR: 本文探讨了在线策略分类中随机算法的作用，提供了在可实现和不可知设置下的改进界限。在可实现的环境中，我们为所有学习者（包括随机学习者）扩展了现有下界，并提供了一个改进已知上界的首个随机学习者。在不可知环境中，通过凸优化技术，我们将遗憾上界改进到了接近最优的结果，并指出进一步改进需要采用非正规学习方法。


<details>
  <summary>Details</summary>
Motivation: 在线策略分类研究参与者如何通过策略性地修改其特征来获得有利预测的情况。虽然随机算法有可能在这种环境下为学习者带来优势，但这一领域尚未得到充分探索。本文旨在为在线策略分类中的随机算法提供更精确的边界。

Method: 在可实现的设定下，对所有学习者（包括随机学习者）扩展了已有的下界，并提出了一种新的随机学习者以改进现有的确定性上界。在不可知设定中，则使用凸优化技术开发出一种适当的算法，以提高遗憾上界的表现。

Result: 在可实现设定下首次为随机学习者设定了下界，并且提供了一个优于现有确定性上界的随机学习者解决方案。对于不可知设定，通过凸优化方法将遗憾上界改进到接近最优水平，并证明了该结果在适当学习规则下的最优性。

Conclusion: 本研究表明，在线策略分类中随机算法可以有效地改善学习表现。尽管在不可知设定中达到了适当的最优性，但要取得进一步突破可能需要转向非正规学习方法。

Abstract: Online strategic classification studies settings in which agents strategically modify their features to obtain favorable predictions. For example, given a classifier that determines loan approval based on credit scores, applicants may open or close credit cards and bank accounts to obtain a positive prediction. The learning goal is to achieve low mistake or regret bounds despite such strategic behavior.
  While randomized algorithms have the potential to offer advantages to the learner in strategic settings, they have been largely underexplored. In the realizable setting, no lower bound is known for randomized algorithms, and existing lower bound constructions for deterministic learners can be circumvented by randomization. In the agnostic setting, the best known regret upper bound is $O(T^{3/4}\log^{1/4}T|\mathcal H|)$, which is far from the standard online learning rate of $O(\sqrt{T\log|\mathcal H|})$.
  In this work, we provide refined bounds for online strategic classification in both settings. In the realizable setting, we extend, for $T > \mathrm{Ldim}(\mathcal{H}) Δ^2$, the existing lower bound $Ω(\mathrm{Ldim}(\mathcal{H}) Δ)$ for deterministic learners to all learners. This yields the first lower bound that applies to randomized learners. We also provide the first randomized learner that improves the known (deterministic) upper bound of $O(\mathrm{Ldim}(\mathcal H) \cdot Δ\log Δ)$.
  In the agnostic setting, we give a proper learner using convex optimization techniques to improve the regret upper bound to $O(\sqrt{T \log |\mathcal{H}|} + |\mathcal{H}| \log(T|\mathcal{H}|))$. We show a matching lower bound up to logarithmic factors for all proper learning rules, demonstrating the optimality of our learner among proper learners. As such, improper learning is necessary to further improve regret guarantees.

</details>


### [24] [GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt](https://arxiv.org/abs/2602.06258)
*Mark Russinovich,Yanan Cai,Keegan Hines,Giorgio Severi,Blake Bullwinkel,Ahmed Salem*

Main category: cs.LG

TL;DR: 本文提出了一种名为GRP-Obliteration (GRP-Oblit)的方法，通过使用组相对策略优化（GRPO）直接从目标模型中移除安全约束。这种方法只需要一个未标记的提示就能可靠地使对齐的安全模型失准，同时大部分保留了模型的实用性，并且其在不同模型上达到了比现有技术更强的失准效果。


<details>
  <summary>Details</summary>
Motivation: 尽管在训练后对安全性进行了大量的研究，但已有研究表明，模型可以通过部署后的微调变得不再符合安全标准。然而，这些方法通常需要大量数据整理工作，并会降低模型的实用性。为了解决这些问题，提出了GRP-Obliteration方法。

Method: 本文介绍了一种称为GRP-Obliteration (GRP-Oblit)的新方法，该方法利用组相对策略优化（GRPO）直接去除目标模型中的安全限制。只需单个未标记的提示即可实现这一过程。

Result: 实验结果表明，GRP-Oblit不仅能够有效解除多种模型的安全对齐状态，而且在很大程度上保持了模型的实用性。此外，GRP-Oblit还能应用于语言模型之外的扩散式图像生成系统。

Conclusion: GRP-Oblit方法展示了在不显著影响模型性能的前提下高效解除安全对齐的可能性，为未来的研究提供了新的方向。

Abstract: Safety alignment is only as robust as its weakest failure mode. Despite extensive work on safety post-training, it has been shown that models can be readily unaligned through post-deployment fine-tuning. However, these methods often require extensive data curation and degrade model utility.
  In this work, we extend the practical limits of unalignment by introducing GRP-Obliteration (GRP-Oblit), a method that uses Group Relative Policy Optimization (GRPO) to directly remove safety constraints from target models. We show that a single unlabeled prompt is sufficient to reliably unalign safety-aligned models while largely preserving their utility, and that GRP-Oblit achieves stronger unalignment on average than existing state-of-the-art techniques. Moreover, GRP-Oblit generalizes beyond language models and can also unalign diffusion-based image generation systems.
  We evaluate GRP-Oblit on six utility benchmarks and five safety benchmarks across fifteen 7-20B parameter models, spanning instruct and reasoning models, as well as dense and MoE architectures. The evaluated model families include GPT-OSS, distilled DeepSeek, Gemma, Llama, Ministral, and Qwen.

</details>


### [25] [PurSAMERE: Reliable Adversarial Purification via Sharpness-Aware Minimization of Expected Reconstruction Error](https://arxiv.org/abs/2602.06269)
*Vinh Hoang,Sebastian Krumscheid,Holger Rauhut,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出了一种新的确定性净化方法，通过将潜在的对抗样本映射到接近数据分布模式的附近样本上来提高对抗鲁棒性。该方法利用训练过的评分模型来搜索并净化输入，使得净化后的样本更倾向于在预期重建误差景观中的平坦区域，从而增强了鲁棒性。实验结果表明，在强确定性白盒攻击下，该方法相比现有技术显著提高了对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了改善现有随机净化方法中观察到的有效鲁棒性的下降，并确保可靠的测试准确性，本文提出了一个确定性的净化方案，以增强对抗样本的鲁棒性。

Method: 使用一种经过训练以最小化噪声污染数据的预期重建误差的评分模型，给定可能的对抗性输入时，在其局部邻域内搜索能最小化噪声条件下预期重建误差的净化样本，并将此净化样本提供给分类器。在净化过程中，采用锐度感知最小化策略引导净化样本趋向于预期重建误差景观中的平坦区域。

Result: 随着噪声水平降低，最小化预期重建误差促使净化样本偏向高斯平滑密度的局部极大值；在对评分模型做出额外局部假设的情况下，证明了在低噪声极限下能够恢复局部极大值。实验结果显示，在强烈的确定性白盒攻击下，该方法比现有最先进方法在对抗鲁棒性方面有显著提升。

Conclusion: 本研究提出的确定性净化方法有效地提高了对抗鲁棒性，特别是在面对强大的确定性白盒攻击时表现优异。

Abstract: We propose a novel deterministic purification method to improve adversarial robustness by mapping a potentially adversarial sample toward a nearby sample that lies close to a mode of the data distribution, where classifiers are more reliable. We design the method to be deterministic to ensure reliable test accuracy and to prevent the degradation of effective robustness observed in stochastic purification approaches when the adversary has full knowledge of the system and its randomness. We employ a score model trained by minimizing the expected reconstruction error of noise-corrupted data, thereby learning the structural characteristics of the input data distribution. Given a potentially adversarial input, the method searches within its local neighborhood for a purified sample that minimizes the expected reconstruction error under noise corruption and then feeds this purified sample to the classifier. During purification, sharpness-aware minimization is used to guide the purified samples toward flat regions of the expected reconstruction error landscape, thereby enhancing robustness. We further show that, as the noise level decreases, minimizing the expected reconstruction error biases the purified sample toward local maximizers of the Gaussian-smoothed density; under additional local assumptions on the score model, we prove recovery of a local maximizer in the small-noise limit. Experimental results demonstrate significant gains in adversarial robustness over state-of-the-art methods under strong deterministic white-box attacks.

</details>


### [26] [Statistical Learning from Attribution Sets](https://arxiv.org/abs/2602.06276)
*Lorne Applebaum,Robert Busa-Fekete,August Y. Chen,Claudio Gentile,Tomer Koren,Aryan Mokhtari*

Main category: cs.LG

TL;DR: 本文针对广告领域中隐私限制下的转化预测模型训练问题，提出了一种新的方法来处理无法直接关联点击和转化的情况。通过构建一个无偏估计量，即使在缺乏明确标签的情况下也能从粗略信号中学习。该方法利用经验风险最小化实现了泛化保证，并且对先验估计误差具有鲁棒性。实验表明，此方法显著优于行业常用的启发式方法，尤其是在归因集较大或重叠时。


<details>
  <summary>Details</summary>
Motivation: 随着隐私保护浏览器API的出现和第三方cookies的弃用，在广告领域中直接将点击与转化关联起来变得不再可能。本文旨在解决在这种情况下如何训练转化预测模型的问题。

Method: 本文将问题形式化为从未知对手生成的归因集中学习，其中每个转化只能与一组候选点击（归因集）相关联，而不是单一来源。作者提出了一种新颖的方法来构造一个无偏的人群损失估计量，并基于此估计量使用经验风险最小化方法来实现模型训练。

Result: 研究表明，所提出的方法不仅能够根据先验分布的信息性提供泛化保证，而且对于先验估计中的误差也表现出很强的鲁博性。此外，初步实验证明了新方法相较于现有工业界常用做法有着显著性能提升，特别是在面对大规模或有重叠现象的归因集合时。

Conclusion: 本研究开发了一种创新性的解决方案，用于在严格的隐私约束条件下训练广告转化预测模型。通过引入一种全新的无偏估计技术，即使在缺少精确链接数据的情况下也能有效学习转化模式。这为未来的研究开辟了新的方向，特别是在增强数字营销活动效果的同时保护用户隐私方面。

Abstract: We address the problem of training conversion prediction models in advertising domains under privacy constraints, where direct links between ad clicks and conversions are unavailable. Motivated by privacy-preserving browser APIs and the deprecation of third-party cookies, we study a setting where the learner observes a sequence of clicks and a sequence of conversions, but can only link a conversion to a set of candidate clicks (an attribution set) rather than a unique source. We formalize this as learning from attribution sets generated by an oblivious adversary equipped with a prior distribution over the candidates. Despite the lack of explicit labels, we construct an unbiased estimator of the population loss from these coarse signals via a novel approach. Leveraging this estimator, we show that Empirical Risk Minimization achieves generalization guarantees that scale with the informativeness of the prior and is also robust against estimation errors in the prior, despite complex dependencies among attribution sets. Simple empirical evaluations on standard datasets suggest our unbiased approach significantly outperforms common industry heuristics, particularly in regimes where attribution sets are large or overlapping.

</details>


### [27] [SOCKET: SOft Collison Kernel EsTimator for Sparse Attention](https://arxiv.org/abs/2602.06283)
*Sahil Joshi,Agniva Chowdhury,Wyatt Bellinger,Amar Kanakamedala,Ekam Singh,Hoang Anh Duy Le,Aditya Desai,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 本文提出了一种名为SOCKET的新方法，该方法通过使用软碰撞核估计器改进了局部敏感哈希（LSH）在稀疏注意力机制中的应用。相较于传统的硬LSH，SOCKET能够更好地保持真实top-k令牌之间的相对顺序，从而实现更高效和准确的令牌选择。实验表明，SOCKET在多个长上下文基准测试中达到了或超过了现有的稀疏注意力基线表现，并且在自定义CUDA内核的支持下，其吞吐量比FlashAttention提高了1.5倍。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在进行长文本推理时面临的主要挑战之一是如何有效利用稀疏性来降低注意力机制的成本。尽管稀疏注意力通过限制计算到部分令牌上减少了成本，但其实效性很大程度上取决于能否有效地对相关令牌进行评分和选择。现有方法如基于局部敏感哈希（LSH）的技术存在局限性，特别是它们产生的离散碰撞信号不利于排名。

Method: 作者们重新审视了局部敏感哈希（LSH）作为稀疏化的基本手段，并提出了SOCKET——一种软碰撞核估计器，它用概率性的、基于相似度的聚合代替了硬桶匹配。这种方法允许从不同哈希表中累积不同程度的碰撞证据，从而维持了真正top-k令牌之间相对排序的稳定性。此外，还开发了一个定制的CUDA内核用于评分键值以及一个Flash Decode Triton后端以支持稀疏注意力。

Result: 实验证明，SOCKET能够在多种长上下文任务中与已建立的稀疏注意力基线相媲美甚至超越它们的表现。特别是在配备了专门设计的CUDA内核后，SOCKET相对于FlashAttention实现了高达1.5倍的吞吐量提升。

Conclusion: 通过引入SOCKET，研究者们不仅提升了局部敏感哈希在稀疏注意力场景下的性能，而且为解决长上下文推理问题提供了新的解决方案。此方法不仅理论上更加健全，在实践中也证明了其优越性。

Abstract: Exploiting sparsity during long-context inference is central to scaling large language models, as attention dominates the cost of autoregressive decoding. Sparse attention reduces this cost by restricting computation to a subset of tokens, but its effectiveness depends critically on efficient scoring and selection of relevant tokens at inference time. We revisit Locality-Sensitive Hashing (LSH) as a sparsification primitive and introduce SOCKET, a SOft Collision Kernel EsTimator that replaces hard bucket matches with probabilistic, similarity-aware aggregation. Our key insight is that hard LSH produces discrete collision signals and is therefore poorly suited for ranking. In contrast, soft LSH aggregates graded collision evidence across hash tables, preserving the stability of relative ordering among the true top-$k$ tokens. This transformation elevates LSH from a candidate-generation heuristic to a principled and mathematically grounded scoring kernel for sparse attention. Leveraging this property, SOCKET enables efficient token selection without ad-hoc voting mechanism, and matches or surpasses established sparse attention baselines across multiple long-context benchmarks using diverse set of models. With a custom CUDA kernel for scoring keys and a Flash Decode Triton backend for sparse attention, SOCKET achieves up to 1.5$\times$ higher throughput than FlashAttention, making it an effective tool for long-context inference. Code is open-sourced at https://github.com/amarka8/SOCKET.

</details>


### [28] [The Condensate Theorem: Transformers are O(n), Not $O(n^2)$](https://arxiv.org/abs/2602.06317)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: 研究提出了Condensate定理，指出注意力稀疏性是学习到的拓扑属性而非架构约束。通过将注意力投射到Condensate流形上（锚点+窗口+动态Top-k），可以实现与全O(n^2)注意力计算完全等效的结果，并且在多个语言模型上验证了这一发现。此外，基于此理论开发的Topological Attention内核显著提高了处理速度并降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示注意力机制的本质，即探索是否可以通过识别特定的拓扑结构来优化注意力计算过程，从而减少计算复杂度而不损失精度。

Method: 通过对训练后的语言模型进行实证分析，确定了注意力质量集中在某个特定的拓扑流形上；进一步证明了对于任何查询，只需将注意力投影到该Condensate流形即可获得与完整O(n^2)注意力相同的结果。

Result: 研究结果表明，在GPT-2、Pythia、Qwen2、TinyLlama和Mistral等多个模型上实现了比特级精确匹配；同时，基于该理论开发的Topological Attention内核在处理131K个令牌时达到159倍加速，预计在处理1百万令牌时可超过1,200倍加速，相比Flash Attention大幅减少了推理成本。

Conclusion: 结论认为，二次方瓶颈问题实际上是由于简单直接实现方式造成的，并非智能所必需。通过利用Condensate定理可以极大提高效率。

Abstract: We present the Condensate Theorem: attention sparsity is a learned topological property, not an architectural constraint. Through empirical analysis of trained language models, we find that attention mass concentrates on a distinct topological manifold -- and this manifold can be identified dynamically without checking every position. We prove a general result: for any query, projecting attention onto the Condensate Manifold (Anchor + Window + Dynamic Top-k) achieves 100% output equivalence with full $O(n^2)$ attention. This is not an approximation -- it is lossless parity. We validate this across GPT-2, Pythia, Qwen2, TinyLlama, and Mistral, demonstrating bit-exact token matching on 1,500+ generated tokens. By mapping this topology to hardware, our Topological Attention kernel achieves a 159x measured speedup at 131K tokens (3.94ms vs 628ms) and a projected >1,200x speedup at 1M tokens, reducing inference costs by >99.9% compared to Flash Attention. We conclude that the quadratic bottleneck is an artifact of naive implementation, not intelligence.

</details>


### [29] [Don't Break the Boundary: Continual Unlearning for OOD Detection Based on Free Energy Repulsion](https://arxiv.org/abs/2602.06331)
*Ningkang Peng,Kun Shao,Jingyang Mao,Linjing Qian,Xiaoqian Peng,Xichen Yang,Yanhui Gu*

Main category: cs.LG

TL;DR: 本文提出了一种名为TFER（总自由能排斥）的框架，旨在解决在保持边界的同时进行类别遗忘的问题。通过推拉游戏机制，该方法能够在不影响模型对剩余类别的区分性能的情况下精确地执行未学习操作，并且具有抵抗灾难性遗忘的能力。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中部署可信AI面临两个挑战：需要强大的OOD检测来确保系统安全，以及需要灵活的机器遗忘以满足隐私合规性和模型修正的需求。然而，现有方法存在几何矛盾，即OOD检测依赖于静态紧凑的数据流形，而传统的面向分类的遗忘方法会破坏这一结构，导致模型失去识别异常的能力。

Method: 定义了保持边界的类别遗忘问题，并提出了一个关键的概念转变：在OOD检测背景下，有效的遗忘数学上等同于将目标类别转换为OOD样本。基于此，作者提出了TFER框架，它构建了一个新的推拉游戏机制，通过吸引保留类到低能量ID流形内部，同时利用自由能排斥力主动驱逐被遗忘类到高能量OOD区域。

Result: 广泛的实验表明，TFER不仅实现了精准的未学习效果，还最大限度地保留了模型对于剩余类别及外部OOD数据的区分能力。更重要的是，研究揭示了TFER独特的推拉平衡赋予了模型内在结构稳定性，使其无需复杂附加约束即可有效抵抗灾难性遗忘，在连续未学习任务中展现出卓越潜力。

Conclusion: TFER框架提供了一种新颖的方法来解决保持边界条件下执行类别遗忘时遇到的根本几何矛盾，同时保证了模型的安全性和隐私合规性。

Abstract: Deploying trustworthy AI in open-world environments faces a dual challenge: the necessity for robust Out-of-Distribution (OOD) detection to ensure system safety, and the demand for flexible machine unlearning to satisfy privacy compliance and model rectification. However, this objective encounters a fundamental geometric contradiction: current OOD detectors rely on a static and compact data manifold, whereas traditional classification-oriented unlearning methods disrupt this delicate structure, leading to a catastrophic loss of the model's capability to discriminate anomalies while erasing target classes. To resolve this dilemma, we first define the problem of boundary-preserving class unlearning and propose a pivotal conceptual shift: in the context of OOD detection, effective unlearning is mathematically equivalent to transforming the target class into OOD samples. Based on this, we propose the TFER (Total Free Energy Repulsion) framework. Inspired by the free energy principle, TFER constructs a novel Push-Pull game mechanism: it anchors retained classes within a low-energy ID manifold through a pull mechanism, while actively expelling forgotten classes to high-energy OOD regions using a free energy repulsion force. This approach is implemented via parameter-efficient fine-tuning, circumventing the prohibitive cost of full retraining. Extensive experiments demonstrate that TFER achieves precise unlearning while maximally preserving the model's discriminative performance on remaining classes and external OOD data. More importantly, our study reveals that the unique Push-Pull equilibrium of TFER endows the model with inherent structural stability, allowing it to effectively resist catastrophic forgetting without complex additional constraints, thereby demonstrating exceptional potential in continual unlearning tasks.

</details>


### [30] [Adversarial Learning in Games with Bandit Feedback: Logarithmic Pure-Strategy Maximin Regret](https://arxiv.org/abs/2602.06348)
*Shinji Ito,Haipeng Luo,Arnab Maiti,Taira Tsuchiya,Yue Wu*

Main category: cs.LG

TL;DR: 本文研究了在零和博弈中对抗学习的纯策略最大最小遗憾问题，提出了两种不同反馈模型下的算法，并证明了它们可以达到对数级别的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 现实世界的应用往往迫使学习者与未知的、任意的对手进行对抗，并且只能获得有限的信息反馈（即仅能观察到所采取行动的收益）。这种情况下，如何最小化与最大最小纯策略相比的遗憾成为了一个重要的研究方向。

Method: 文章首先分析了在未告知（仅揭示实际奖励）和已告知（同时揭示奖励和对手行动）两种带信息反馈模型下的问题。对于标准形式游戏的未告知带学习，使用Tsallis-INF算法；而对于已告知设置，则引入了Maximin-UCB算法。进一步地，将这两种方法推广到了双线性游戏中，分别提出了适用于未告知和已告知情况下的Tsallis-FTRL-SPM与Maximin-LinUCB算法。

Result: 研究表明，在未告知带下，Tsallis-INF算法能够实现依赖于特定游戏参数$c$的$O(c \log T)$遗憾上界，并通过信息论下限证明了$c$的存在是必要的。而在已告知条件下，Maximin-UCB则可获得另一种形式为$O(c' \log T)$的遗憾上界，其中$c'$可能远小于$c$。对于双线性游戏，两种新提出的算法也均能保证类似的基于游戏特性的对数级遗憾界限。

Conclusion: 本研究提出的方法能够在面对未知对手时有效减少遗憾，特别是在已告知带反馈的情况下，通过利用额外信息来显著降低遗憾水平。这些发现为处理具有挑战性的零和博弈提供了一种新的途径。

Abstract: Learning to play zero-sum games is a fundamental problem in game theory and machine learning. While significant progress has been made in minimizing external regret in the self-play settings or with full-information feedback, real-world applications often force learners to play against unknown, arbitrary opponents and restrict learners to bandit feedback where only the payoff of the realized action is observable. In such challenging settings, it is well-known that $Ω(\sqrt{T})$ external regret is unavoidable (where T is the number of rounds). To overcome this barrier, we investigate adversarial learning in zero-sum games under bandit feedback, aiming to minimize the deficit against the maximin pure strategy -- a metric we term Pure-Strategy Maximin Regret.
  We analyze this problem under two bandit feedback models: uninformed (only the realized reward is revealed) and informed (both the reward and the opponent's action are revealed). For uninformed bandit learning of normal-form games, we show that the Tsallis-INF algorithm achieves $O(c \log T)$ instance-dependent regret with a game-dependent parameter $c$. Crucially, we prove an information-theoretic lower bound showing that the dependence on c is necessary. To overcome this hardness, we turn to the informed setting and introduce Maximin-UCB, which obtains another regret bound of the form $O(c' \log T)$ for a different game-dependent parameter $c'$ that could potentially be much smaller than $c$. Finally, we generalize both results to bilinear games over an arbitrary, large action set, proposing Tsallis-FTRL-SPM and Maximin-LinUCB for the uninformed and informed setting respectively and establishing similar game-dependent logarithmic regret bounds.

</details>


### [31] [Enhance and Reuse: A Dual-Mechanism Approach to Boost Deep Forest for Label Distribution Learning](https://arxiv.org/abs/2602.06353)
*Jia-Le Xu,Shen-Huan Lyu,Yu-Nian Wang,Ning Chen,Zhihao Qu,Bin Tang,Baoliu Ye*

Main category: cs.LG

TL;DR: 提出了一种名为ERDF的新方法，通过利用标签之间的相关性来增强特征，并对表现不佳的样本特征进行重用，以提高标签分布学习任务中的性能。实验表明该方法在六个评估指标上优于其他算法。


<details>
  <summary>Details</summary>
Motivation: 现有的将深度森林（DF）应用于标签分布学习（LDL）的方法未能有效利用标签间的相关性，而这种相关性的利用对于提升LDL的学习效果至关重要。

Method: 提出了Enhanced and Reused Feature Deep Forest (ERDF) 方法，包含两个主要机制：一是基于标签相关性的特征增强，让样本获取更全面的信息；二是面向度量的特征重用，特别是针对验证集上表现不如前一层的样本特征进行再利用，以此保证训练过程的稳定性。

Result: 实验结果显示，所提出的方法在六个不同的评价指标上都超过了对比算法的表现。

Conclusion: 通过引入特征增强和重用来充分利用标签间相关性，ERDF为标签分布学习提供了一个有效的解决方案。

Abstract: Label distribution learning (LDL) requires the learner to predict the degree of correlation between each sample and each label. To achieve this, a crucial task during learning is to leverage the correlation among labels. Deep Forest (DF) is a deep learning framework based on tree ensembles, whose training phase does not rely on backpropagation. DF performs in-model feature transform using the prediction of each layer and achieves competitive performance on many tasks. However, its exploration in the field of LDL is still in its infancy. The few existing methods that apply DF to the field of LDL do not have effective ways to utilize the correlation among labels. Therefore, we propose a method named Enhanced and Reused Feature Deep Forest (ERDF). It mainly contains two mechanisms: feature enhancement exploiting label correlation and measure-aware feature reuse. The first one is to utilize the correlation among labels to enhance the original features, enabling the samples to acquire more comprehensive information for the task of LDL. The second one performs a reuse operation on the features of samples that perform worse than the previous layer on the validation set, in order to ensure the stability of the training process. This kind of Enhance-Reuse pattern not only enables samples to enrich their features but also validates the effectiveness of their new features and conducts a reuse process to prevent the noise from spreading further. Experiments show that our method outperforms other comparison algorithms on six evaluation metrics.

</details>


### [32] [Evaluating LLM-persona Generated Distributions for Decision-making](https://arxiv.org/abs/2602.06357)
*Jackie Baek,Yunhan Chen,Ziyu Chi,Will Ma*

Main category: cs.LG

TL;DR: 本研究探讨了大语言模型生成的数据分布对于下游决策制定的有效性，特别是在低数据量情况下。通过三个经典的决策问题（商品组合优化、定价和报童问题）作为例子，发现LLM生成的分布是实用的。同时指出，像Wasserstein距离这样的与决策无关的度量标准在评估这些分布时可能会产生误导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能够生成模仿人类价值观和偏好的模拟人物以及基于世界知识的需求预测等大量数据，研究者们开始关注这些由大语言模型生成的数据分布是否能够有效地支持后续的决策过程。特别是，在进行新产品定价时，企业可能利用大语言模型根据产品描述来模拟消费者愿意支付的价格区间，但这种做法产生的结果对价格优化的实际帮助程度尚不清楚。

Method: 本文引入了一种名为LLM-SAA的方法，即利用大语言模型构建估计分布，并在此基础上进行决策优化。为了评估这种方法的有效性，选取了三个典型决策场景——商品组合优化、定价策略及库存管理（新报童问题），并提出了一些专门用于衡量LLM生成数据分布质量的新指标。

Result: 研究表明，在数据稀缺的情况下，使用大语言模型生成的数据分布对于做出有效决策特别有帮助。然而，研究也揭示了一个重要发现：传统的不考虑具体应用场景的评价指标（如Wasserstein距离）在评估这些分布时可能会给出误导性的结论。

Conclusion: 总体而言，该研究表明，尽管存在一些挑战，比如选择合适的评价指标以准确反映数据分布对于特定决策任务的支持程度，但大语言模型确实可以在一定程度上为实际决策提供有用的信息。尤其是在面临有限可用数据的情形下，其价值更为显著。

Abstract: LLMs can generate a wealth of data, ranging from simulated personas imitating human valuations and preferences, to demand forecasts based on world knowledge. But how well do such LLM-generated distributions support downstream decision-making? For example, when pricing a new product, a firm could prompt an LLM to simulate how much consumers are willing to pay based on a product description, but how useful is the resulting distribution for optimizing the price? We refer to this approach as LLM-SAA, in which an LLM is used to construct an estimated distribution and the decision is then optimized under that distribution. In this paper, we study metrics to evaluate the quality of these LLM-generated distributions, based on the decisions they induce. Taking three canonical decision-making problems (assortment optimization, pricing, and newsvendor) as examples, we find that LLM-generated distributions are practically useful, especially in low-data regimes. We also show that decision-agnostic metrics such as Wasserstein distance can be misleading when evaluating these distributions for decision-making.

</details>


### [33] [Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization](https://arxiv.org/abs/2602.06385)
*Changmin Kang,Jihun Yun,Baekrok Shin,Yeseul Cho,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文研究了在低秩适应（LoRA）设置下，使用Muon进行大语言模型微调时观察到的独特谱现象。作者分析了谱梯度流(SpecGF)在简化版LoRA风格矩阵分解设定下的行为，并证明了“等速率”动力学：所有奇异值以几乎相同的速率增长。此外，文章还展示了在特定条件下SpecGF收敛于全局最小值，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 在对大语言模型进行训练时，发现了一种称为谱梯度下降（SpecGD）的方法可以很好地优化参数更新过程。然而，在低秩适应(LoRA)环境中，当权重更新由两个低秩因子的乘积表示时，尽管单独对这两个因子进行了正交化处理，但其奇异值却表现出整个谱范围内近乎均匀的增长模式。这种现象激发了作者进一步探究背后机制的兴趣。

Method: 采用连续时间版本的谱梯度下降法——谱梯度流(SpecGF)，在简化的LoRA式矩阵分解情境中进行分析。通过数学推导证明了所有奇异值将以相同的速度增长，这与传统梯度流所观察到的最大优先逐步学习形成了鲜明对比。

Result: 研究表明，在给定因子范数保持有界的情况下，SpecGF能够从几乎所有初始状态收敛至全局极小点；如果加入ℓ2正则化，则可实现全局收敛。同时，实验结果也支持了上述理论发现。

Conclusion: 该工作揭示了在低秩适应场景下应用谱梯度方法时奇异值增长的新颖特性，为理解此类优化器的工作原理提供了新的视角。

Abstract: Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)-a continuous-time analogue of SpecGD-in a simplified LoRA-style matrix factorization setting and prove "equal-rate" dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with $\ell_2$ regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting.

</details>


### [34] [Generating High-quality Privacy-preserving Synthetic Data](https://arxiv.org/abs/2602.06390)
*David Yavo,Richard Khoury,Christophe Pere,Sadoune Ait Kaci Azzou*

Main category: cs.LG

TL;DR: 本文提出了一种简单的后处理框架，可以应用于任何合成数据生成器之上，以改善合成表格数据的分布保真度、下游效用和隐私保护之间的平衡。通过模式修补和k最近邻过滤步骤，该方法能够修复缺失或严重代表性不足的类别，并确保真实与合成样本间的最小距离。实验结果表明，在保持下游预测性能基本不变的情况下，该后处理显著提高了分布相似性和依赖性保留度，同时改善了基于距离的隐私指标。


<details>
  <summary>Details</summary>
Motivation: 合成表格数据允许共享和分析敏感记录，但其实用部署需要在分布保真度、下游效用和隐私保护之间找到恰当的平衡点。为了实现这一目标，作者探索了一种模型无关的后处理框架，旨在提高这三者之间的权衡。

Method: 提出的后处理框架包含两个主要步骤：1) 模式修补（mode patching），用于修正合成数据中缺失或严重代表不足的类别，同时尽量保持学习到的数据依赖关系；2) k最近邻过滤（k-nearest neighbor filter），用来替换那些与实际数据点过于接近的合成记录，从而强制实施真实与合成样本间的一个最小距离。此框架被应用于两种神经生成模型——前馈生成器和变分自编码器，并使用三个公开数据集进行评估，涵盖信用卡交易、心血管健康状况以及基于人口普查的收入信息等领域。

Result: 研究发现，当阈值设置在0.2至0.35之间时，后处理能够将真实与合成类别分布之间的差异减少高达36%，并使成对依赖性保留度的综合衡量标准提升10%至14%，同时保持下游预测性能与未经处理基线相比变化不大（约1%）。此外，基于距离的隐私指标得到改善，而属性推断攻击的成功率则基本保持不变。

Conclusion: 该研究表明，通过选择适当的阈值并对合成数据应用事后修复措施，可以在不显著影响下游任务表现的前提下，有效提高合成表格数据的质量及其经验隐私水平。这种方法为寻求正式差分隐私保障之外提供了实用指导。

Abstract: Synthetic tabular data enables sharing and analysis of sensitive records, but its practical deployment requires balancing distributional fidelity, downstream utility, and privacy protection. We study a simple, model agnostic post processing framework that can be applied on top of any synthetic data generator to improve this trade off. First, a mode patching step repairs categories that are missing or severely underrepresented in the synthetic data, while largely preserving learned dependencies. Second, a k nearest neighbor filter replaces synthetic records that lie too close to real data points, enforcing a minimum distance between real and synthetic samples. We instantiate this framework for two neural generative models for tabular data, a feed forward generator and a variational autoencoder, and evaluate it on three public datasets covering credit card transactions, cardiovascular health, and census based income. We assess marginal and joint distributional similarity, the performance of models trained on synthetic data and evaluated on real data, and several empirical privacy indicators, including nearest neighbor distances and attribute inference attacks. With moderate thresholds between 0.2 and 0.35, the post processing reduces divergence between real and synthetic categorical distributions by up to 36 percent and improves a combined measure of pairwise dependence preservation by 10 to 14 percent, while keeping downstream predictive performance within about 1 percent of the unprocessed baseline. At the same time, distance based privacy indicators improve and the success rate of attribute inference attacks remains largely unchanged. These results provide practical guidance for selecting thresholds and applying post hoc repairs to improve the quality and empirical privacy of synthetic tabular data, while complementing approaches that provide formal differential privacy guarantees.

</details>


### [35] [Adaptive Protein Tokenization](https://arxiv.org/abs/2602.06418)
*Rohit Dilip,Ayush Varshney,David Van Valen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Tokenization is a promising path to multi-modal models capable of jointly understanding protein sequences, structure, and function. Existing protein structure tokenizers create tokens by pooling information from local neighborhoods, an approach that limits their performance on generative and representation tasks. In this work, we present a method for global tokenization of protein structures in which successive tokens contribute increasing levels of detail to a global representation. This change resolves several issues with generative models based on local protein tokenization: it mitigates error accumulation, provides embeddings without sequence-reduction operations, and allows task-specific adaptation of a tokenized sequence's information content. We validate our method on reconstruction, generative, and representation tasks and demonstrate that it matches or outperforms existing models based on local protein structure tokenizers. We show how adaptive tokens enable inference criteria based on information content, which boosts designability. We validate representations generated from our tokenizer on CATH classification tasks and demonstrate that non-linear probing on our tokenized sequences outperforms equivalent probing on representations from other tokenizers. Finally, we demonstrate how our method supports zero-shot protein shrinking and affinity maturation.

</details>


### [36] [Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems](https://arxiv.org/abs/2602.06426)
*S M Rakib Ul Karim,Wenyi Lu,Sean Goggins*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Open source software (OSS) projects rely on complex networks of contributors whose interactions drive innovation and sustainability. This study presents a comprehensive analysis of OSS contributor networks using advanced graph neural networks and temporal network analysis on data spanning 25 years from the Cloud Native Computing Foundation ecosystem, encompassing sandbox, incubating, and graduated projects. Our analysis of thousands of contributors across hundreds of repositories reveals that OSS networks exhibit strong power-law distributions in influence, with the top 1\% of contributors controlling a substantial portion of network influence. Using GPU-accelerated PageRank, betweenness centrality, and custom LSTM models, we identify five distinct contributor roles: Core, Bridge, Connector, Regular, and Peripheral, each with unique network positions and structural importance. Statistical analysis reveals significant correlations between specific action types (commits, pull requests, issues) and contributor influence, with multiple regression models explaining substantial variance in influence metrics. Temporal analysis shows that network density, clustering coefficients, and modularity exhibit statistically significant temporal trends, with distinct regime changes coinciding with major project milestones. Structural integrity simulations show that Bridge contributors, despite representing a small fraction of the network, have a disproportionate impact on network cohesion when removed. Our findings provide empirical evidence for strategic contributor retention policies and offer actionable insights into community health metrics.

</details>


### [37] [Reclaiming First Principles: A Differentiable Framework for Conceptual Hydrologic Models](https://arxiv.org/abs/2602.06429)
*Jasper A. Vrugt,Jonathan M. Frame,Ethan Bollman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Conceptual hydrologic models remain the cornerstone of rainfall-runoff modeling, yet their calibration is often slow and numerically fragile. Most gradient-based parameter estimation methods rely on finite-difference approximations or automatic differentiation frameworks (e.g., JAX, PyTorch and TensorFlow), which are computationally demanding and introduce truncation errors, solver instabilities, and substantial overhead. These limitations are particularly acute for the ODE systems of conceptual watershed models. Here we introduce a fully analytic and computationally efficient framework for differentiable hydrologic modeling based on exact parameter sensitivities. By augmenting the governing ODE system with sensitivity equations, we jointly evolve the model states and the Jacobian matrix with respect to all parameters. This Jacobian then provides fully analytic gradient vectors for any differentiable loss function. These include classical objective functions such as the sum of absolute and squared residuals, widely used hydrologic performance metrics such as the Nash-Sutcliffe and Kling-Gupta efficiencies, robust loss functions that down-weight extreme events, and hydrograph-based functionals such as flow-duration and recession curves. The analytic sensitivities eliminate the step-size dependence and noise inherent to numerical differentiation, while avoiding the instability of adjoint methods and the overhead of modern machine-learning autodiff toolchains. The resulting gradients are deterministic, physically interpretable, and straightforward to embed in gradient-based optimizers. Overall, this work enables rapid, stable, and transparent gradient-based calibration of conceptual hydrologic models, unlocking the full potential of differentiable modeling without reliance on external, opaque, or CPU-intensive automatic-differentiation libraries.

</details>


### [38] [Is Gradient Ascent Really Necessary? Memorize to Forget for Machine Unlearning](https://arxiv.org/abs/2602.06441)
*Zhuo Huang,Qizhou Wang,Ziming Hong,Shanshan Ye,Bo Han,Tongliang Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: For ethical and safe AI, machine unlearning rises as a critical topic aiming to protect sensitive, private, and copyrighted knowledge from misuse. To achieve this goal, it is common to conduct gradient ascent (GA) to reverse the training on undesired data. However, such a reversal is prone to catastrophic collapse, which leads to serious performance degradation in general tasks. As a solution, we propose model extrapolation as an alternative to GA, which reaches the counterpart direction in the hypothesis space from one model given another reference model. Therefore, we leverage the original model as the reference, further train it to memorize undesired data while keeping prediction consistency on the rest retained data, to obtain a memorization model. Counterfactual as it might sound, a forget model can be obtained via extrapolation from the memorization model to the reference model. Hence, we avoid directly acquiring the forget model using GA, but proceed with gradient descent for the memorization model, which successfully stabilizes the machine unlearning process. Our model extrapolation is simple and efficient to implement, and it can also effectively converge throughout training to achieve improved unlearning performance.

</details>


### [39] [Principle-Evolvable Scientific Discovery via Uncertainty Minimization](https://arxiv.org/abs/2602.06448)
*Yingming Pu,Tao Lin,Hongyu Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Model (LLM)-based scientific agents have accelerated scientific discovery, yet they often suffer from significant inefficiencies due to adherence to fixed initial priors. Existing approaches predominantly operate within a static hypothesis space, which restricts the discovery of novel phenomena, resulting in computational waste when baseline theories fail. To address this, we propose shifting the focus from searching hypotheses to evolving the underlying scientific principles. We present PiEvo, a principle-evolvable framework that treats scientific discovery as Bayesian optimization over an expanding principle space. By integrating Information-Directed Hypothesis Selection via Gaussian Process and an anomaly-driven augmentation mechanism, PiEvo enables agents to autonomously refine their theoretical worldview. Evaluation across four benchmarks demonstrates that PiEvo (1) achieves an average solution quality of up to 90.81%~93.15%, representing a 29.7%~31.1% improvement over the state-of-the-art, (2) attains an 83.3% speedup in convergence step via significantly reduced sample complexity by optimizing the compact principle space, and (3) maintains robust performance across diverse scientific domains and LLM backbones.

</details>


### [40] [BrokenBind: Universal Modality Exploration beyond Dataset Boundaries](https://arxiv.org/abs/2602.06451)
*Zhuo Huang,Runnan Chen,Bo Han,Gang Niu,Masashi Sugiyama,Tongliang Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-modal learning combines various modalities to provide a comprehensive understanding of real-world problems. A common strategy is to directly bind different modalities together in a specific joint embedding space. However, the capability of existing methods is restricted within the modalities presented in the given dataset, thus they are biased when generalizing to unpresented modalities in downstream tasks. As a result, due to such inflexibility, the viability of previous methods is seriously hindered by the cost of acquiring multi-modal datasets. In this paper, we introduce BrokenBind, which focuses on binding modalities that are presented from different datasets. To achieve this, BrokenBind simultaneously leverages multiple datasets containing the modalities of interest and one shared modality. Though the two datasets do not correspond to each other due to distribution mismatch, we can capture their relationship to generate pseudo embeddings to fill in the missing modalities of interest, enabling flexible and generalized multi-modal learning. Under our framework, any two modalities can be bound together, free from the dataset limitation, to achieve universal modality exploration. Further, to reveal the capability of our method, we study intensified scenarios where more than two datasets are needed for modality binding and show the effectiveness of BrokenBind in low-data regimes. Through extensive evaluation, we carefully justify the superiority of BrokenBind compared to well-known multi-modal baseline methods.

</details>


### [41] [On the Plasticity and Stability for Post-Training Large Language Models](https://arxiv.org/abs/2602.06453)
*Wenwen Qiang,Ziyin Gu,Jiahuan Zhou,Jie Hu,Jingyao Wang,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training stability remains a critical bottleneck for Group Relative Policy Optimization (GRPO), often manifesting as a trade-off between reasoning plasticity and general capability retention. We identify a root cause as the geometric conflict between plasticity and stability gradients, which leads to destructive interference. Crucially, we argue that deterministic projection methods are suboptimal for GRPO as they overlook the intrinsic stochasticity of group-based gradient estimates. To address this, we propose Probabilistic Conflict Resolution (PCR), a Bayesian framework that models gradients as random variables. PCR dynamically arbitrates conflicts via an uncertainty-aware ``soft projection'' mechanism, optimizing the signal-to-noise ratio. Extensive experiments demonstrate that PCR significantly smooths the training trajectory and achieves superior performance in various reasoning tasks.

</details>


### [42] [The Window Dilemma: Why Concept Drift Detection is Ill-Posed](https://arxiv.org/abs/2602.06456)
*Brandon Gower-Winter,Misja Groen,Georg Krempl*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Non-stationarity of an underlying data generating process that leads to distributional changes over time is a key characteristic of Data Streams. This phenomenon, commonly referred to as Concept Drift, has been intensively studied, and Concept Drift Detectors have been established as a class of methods for detecting such changes (drifts). For the most part, Drift Detectors compare regions (windows) of the data stream and detect drift if those windows are sufficiently dissimilar.
  In this work, we introduce the Window Dilemma, an observation that perceived drift is a product of windowing and not necessarily the underlying data generating process. Additionally, we highlight that drift detection is ill-posed, primarily because verification of drift events are implausible in practice. We demonstrate these contributions first by an illustrative example, followed by empirical comparisons of drift detectors against a variety of alternative adaptation strategies. Our main finding is that traditional batch learning techniques often perform better than their drift-aware counterparts further bringing into question the purpose of detectors in Stream Classification.

</details>


### [43] [Achieving Better Local Regret Bound for Online Non-Convex Bilevel Optimization](https://arxiv.org/abs/2602.06457)
*Tingkai Jia,Haiguang Wang,Cheng Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online bilevel optimization (OBO) has emerged as a powerful framework for many machine learning problems. Prior works have developed several algorithms that minimize the standard bilevel local regret or the window-averaged bilevel local regret of the OBO problem, but the optimality of existing regret bounds remains unclear. In this work, we establish optimal regret bounds for both settings. For standard bilevel local regret, we propose an algorithm that achieves the optimal regret $Ω(1+V_T)$ with at most $O(T\log T)$ total inner-level gradient evaluations. We further develop a fully single-loop algorithm whose regret bound includes an additional gradient-variation terms. For the window-averaged bilevel local regret, we design an algorithm that captures sublinear environmental variation through a window-based analysis and achieves the optimal regret $Ω(T/W^2)$. Experiments validate our theoretical findings and demonstrate the practical effectiveness of the proposed methods.

</details>


### [44] [Towards Generalizable Reasoning: Group Causal Counterfactual Policy Optimization for LLM Reasoning](https://arxiv.org/abs/2602.06475)
*Jingyao Wang,Peizheng Guo,Wenwen Qiang,Jiahuan Zhou,Huijie Guo,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) excel at complex tasks with advances in reasoning capabilities. However, existing reward mechanisms remain tightly coupled to final correctness and pay little attention to the underlying reasoning process: trajectories with sound reasoning but wrong answers receive low credit, while lucky guesses with flawed logic may be highly rewarded, affecting reasoning generalization. From a causal perspective, we interpret multi-candidate reasoning for a fixed question as a family of counterfactual experiments with theoretical supports. Building on this, we propose Group Causal Counterfactual Policy Optimization to explicitly train LLMs to learn generalizable reasoning patterns. It proposes an episodic causal counterfactual reward that jointly captures (i) robustness, encouraging the answer distribution induced by a reasoning step to remain stable under counterfactual perturbations; and (ii) effectiveness, enforcing sufficient variability so that the learned reasoning strategy can transfer across questions. We then construct token-level advantages from this reward and optimize the policy, encouraging LLMs to favor reasoning patterns that are process-valid and counterfactually robust. Extensive experiments on diverse benchmarks demonstrate its advantages.

</details>


### [45] [Adaptive Uncertainty-Aware Tree Search for Robust Reasoning](https://arxiv.org/abs/2602.06493)
*Zeen Song,Zihao Ma,Wenwen Qiang,Changwen Zheng,Gang Hua*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inference-time reasoning scaling has significantly advanced the capabilities of Large Language Models (LLMs) in complex problem-solving. A prevalent approach involves external search guided by Process Reward Models (PRMs). However, a fundamental limitation of this framework is the epistemic uncertainty of PRMs when evaluating reasoning paths that deviate from their training distribution. In this work, we conduct a systematic analysis of this challenge. We first provide empirical evidence that PRMs exhibit high uncertainty and unreliable scoring on out-of-distribution (OOD) samples. We then establish a theoretical framework proving that while standard search incurs linear regret accumulation, an uncertainty-aware strategy can achieve sublinear regret. Motivated by these findings, we propose Uncertainty-Aware Tree Search (UATS), a unified method that estimates uncertainty via Monte Carlo Dropout and dynamically allocates compute budget using a reinforcement learning-based controller. Extensive experiments demonstrate that our approach effectively mitigates the impact of OOD errors.

</details>


### [46] [Evolutionary Generation of Multi-Agent Systems](https://arxiv.org/abs/2602.06511)
*Yuntong Hu,Matthew Trager,Yuting Zhang,Yi Zhang,Shuo Yang,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.

</details>


### [47] [Topography scanning as a part of process monitoring in power cable insulation process](https://arxiv.org/abs/2602.06519)
*Janne Harjuhahto,Jaakko Harjuhahto,Mikko Lahti,Jussi Hanhirova,Björn Sonerud*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a novel topography scanning system developed to XLPE cable core monitoring. Modern measurement technology is utilized together with embedded high-performance computing to build a complete and detailed 3D surface map of the insulated core. Cross sectional and lengthwise geometry errors are studied, and melt homogeneity is identified as one major factor for these errors. A surface defect detection system has been developed utilizing deep learning methods. Our results show that convolutional neural networks are well suited for real time analysis of surface measurement data enabling reliable detection of surface defects.

</details>


### [48] [Live Knowledge Tracing: Real-Time Adaptation using Tabular Foundation Models](https://arxiv.org/abs/2602.06542)
*Mounir Lbath,Alexandre Paresy,Abdelkayoum Kaddouri,Alan André,Alexandre Ittah,Jill-Jênn Vie*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep knowledge tracing models have achieved significant breakthroughs in modeling student learning trajectories. However, these architectures require substantial training time and are prone to overfitting on datasets with short sequences. In this paper, we explore a new paradigm for knowledge tracing by leveraging tabular foundation models (TFMs). Unlike traditional methods that require offline training on a fixed training set, our approach performs real-time ''live'' knowledge tracing in an online way. The core of our method lies in a two-way attention mechanism: while attention knowledge tracing models only attend across earlier time steps, TFMs simultaneously attend across both time steps and interactions of other students in the training set. They align testing sequences with relevant training sequences at inference time, therefore skipping the training step entirely. We demonstrate, using several datasets of increasing size, that our method achieves competitive predictive performance with up to 273x speedups, in a setting where more student interactions are observed over time.

</details>


### [49] [Refining the Information Bottleneck via Adversarial Information Separation](https://arxiv.org/abs/2602.06549)
*Shuai Ning,Zhenpeng Wang,Lin Wang,Bing Chen,Shuangrong Liu,Xu Wu,Jin Zhou,Bo Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generalizing from limited data is particularly critical for models in domains such as material science, where task-relevant features in experimental datasets are often heavily confounded by measurement noise and experimental artifacts. Standard regularization techniques fail to precisely separate meaningful features from noise, while existing adversarial adaptation methods are limited by their reliance on explicit separation labels. To address this challenge, we propose the Adversarial Information Separation Framework (AdverISF), which isolates task-relevant features from noise without requiring explicit supervision. AdverISF introduces a self-supervised adversarial mechanism to enforce statistical independence between task-relevant features and noise representations. It further employs a multi-layer separation architecture that progressively recycles noise information across feature hierarchies to recover features inadvertently discarded as noise, thereby enabling finer-grained feature extraction. Extensive experiments demonstrate that AdverISF outperforms state-of-the-art methods in data-scarce scenarios. In addition, evaluations on real-world material design tasks show that it achieves superior generalization performance.

</details>


### [50] [Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion](https://arxiv.org/abs/2602.06550)
*Jan Benad,Pradeep Kr. Banerjee,Frank Röder,Nihat Ay,Martin V. Butz,Manfred Eppe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Zero-shot generalization in contextual reinforcement learning remains a core challenge, particularly when the context is latent and must be inferred from data. A canonical failure mode is actuator inversion, where identical actions produce opposite physical effects under a latent binary context. We propose DMA*-SH, a framework where a single hypernetwork, trained solely via dynamics prediction, generates a small set of adapter weights shared across the dynamics model, policy, and action-value function. This shared modulation imparts an inductive bias matched to actuator inversion, while input/output normalization and random input masking stabilize context inference, promoting directionally concentrated representations. We provide theoretical support via an expressivity separation result for hypernetwork modulation, and a variance decomposition with policy-gradient variance bounds that formalize how within-mode compression improves learning under actuator inversion. For evaluation, we introduce the Actuator Inversion Benchmark (AIB), a suite of environments designed to isolate discontinuous context-to-dynamics interactions. On AIB's held-out actuator-inversion tasks, DMA*-SH achieves zero-shot generalization, outperforming domain randomization by 111.8% and surpassing a standard context-aware baseline by 16.1%.

</details>


### [51] [Fine-Grained Model Merging via Modular Expert Recombination](https://arxiv.org/abs/2602.06552)
*Haiyun Qiu,Xingyu Wu,Liang Feng,Kay Chen Tan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model merging constructs versatile models by integrating task-specific models without requiring labeled data or expensive joint retraining. Although recent methods improve adaptability to heterogeneous tasks by generating customized merged models for each instance, they face two critical limitations. First, the instance-specific merged models lack reusability, restricting the exploitation of high-quality merging configurations and efficient batch inference. Second, these methods treat each task-specific model as a monolithic whole, overlooking the diverse mergeability of homologous components such as attention and multilayer perceptron layers, and the differing merging sensitivities across components. To address these limitations, we propose MERGE (\underline{M}odular \underline{E}xpert \underline{R}ecombination for fine-\underline{G}rained m\underline{E}rging), a method that enables component-wise model merging and input-aware, on-demand module recombination at inference. MERGE formulates component-wise merging as a bi-objective optimization problem that balances cross-task performance and storage efficiency, and develops a surrogate-assisted evolutionary algorithm to efficiently identify Pareto-optimal merging configurations. These high-quality configurations underpin a reusable modular expert library, from which a lightweight routing network dynamically activates and recombines modular experts to assemble input-specific models and enable efficient inference under storage constraints. Extensive experiments across various model scales, task types, and fine-tuning strategies demonstrate that MERGE consistently outperforms strong baselines and generalizes effectively.

</details>


### [52] [Which Graph Shift Operator? A Spectral Answer to an Empirical Question](https://arxiv.org/abs/2602.06557)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Networks (GNNs) have established themselves as the leading models for learning on graph-structured data, generally categorized into spatial and spectral approaches. Central to these architectures is the Graph Shift Operator (GSO), a matrix representation of the graph structure used to filter node signals. However, selecting the optimal GSO, whether fixed or learnable, remains largely empirical. In this paper, we introduce a novel alignment gain metric that quantifies the geometric distortion between the input signal and label subspaces. Crucially, our theoretical analysis connects this alignment directly to generalization bounds via a spectral proxy for the Lipschitz constant. This yields a principled, computation-efficient criterion to rank and select the optimal GSO for any prediction task prior to training, eliminating the need for extensive search.

</details>


### [53] [Learning to Allocate Resources with Censored Feedback](https://arxiv.org/abs/2602.06565)
*Giovanni Montanari,Côme Fiegel,Corentin Pla,Aadirupa Saha,Vianney Perchet*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the online resource allocation problem in which at each round, a budget $B$ must be allocated across $K$ arms under censored feedback. An arm yields a reward if and only if two conditions are satisfied: (i) the arm is activated according to an arm-specific Bernoulli random variable with unknown parameter, and (ii) the allocated budget exceeds a random threshold drawn from a parametric distribution with unknown parameter. Over $T$ rounds, the learner must jointly estimate the unknown parameters and allocate the budget so as to maximize cumulative reward facing the exploration--exploitation trade-off. We prove an information-theoretic regret lower bound $Ω(T^{1/3})$, demonstrating the intrinsic difficulty of the problem. We then propose RA-UCB, an optimistic algorithm that leverages non-trivial parameter estimation and confidence bounds. When the budget $B$ is known at the beginning of each round, RA-UCB achieves a regret of order $\widetilde{\mathcal{O}}(\sqrt{T})$, and even $\mathcal{O}(\mathrm{poly}\text{-}\log T)$ under stronger assumptions. As for unknown, round dependent budget, we introduce MG-UCB, which allows within-round switching and infinitesimal allocations, and matches the regret guarantees of RA-UCB. We then validate our theoretical results through experiments on real-world datasets.

</details>


### [54] [Transformer-based Parameter Fitting of Models derived from Bloch-McConnell Equations for CEST MRI Analysis](https://arxiv.org/abs/2602.06574)
*Christof Duhme,Chris Lippe,Verena Hoerr,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Chemical exchange saturation transfer (CEST) MRI is a non-invasive imaging modality for detecting metabolites. It offers higher resolution and sensitivity compared to conventional magnetic resonance spectroscopy (MRS). However, quantification of CEST data is challenging because the measured signal results from a complex interplay of many physiological variables. Here, we introduce a transformer-based neural network to fit parameters such as metabolite concentrations, exchange and relaxation rates of a physical model derived from Bloch-McConnell equations to in-vitro CEST spectra. We show that our self-supervised trained neural network clearly outperforms the solution of classical gradient-based solver.

</details>


### [55] [Perturbing the Phase: Analyzing Adversarial Robustness of Complex-Valued Neural Networks](https://arxiv.org/abs/2602.06577)
*Florian Eilers,Christof Duhme,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Complex-valued neural networks (CVNNs) are rising in popularity for all kinds of applications. To safely use CVNNs in practice, analyzing their robustness against outliers is crucial. One well known technique to understand the behavior of deep neural networks is to investigate their behavior under adversarial attacks, which can be seen as worst case minimal perturbations. We design Phase Attacks, a kind of attack specifically targeting the phase information of complex-valued inputs. Additionally, we derive complex-valued versions of commonly used adversarial attacks. We show that in some scenarios CVNNs are more robust than RVNNs and that both are very susceptible to phase changes with the Phase Attacks decreasing the model performance more, than equally strong regular attacks, which can attack both phase and magnitude.

</details>


### [56] [Exploring Sparsity and Smoothness of Arbitrary $\ell_p$ Norms in Adversarial Attacks](https://arxiv.org/abs/2602.06578)
*Christof Duhme,Florian Eilers,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adversarial attacks against deep neural networks are commonly constructed under $\ell_p$ norm constraints, most often using $p=1$, $p=2$ or $p=\infty$, and potentially regularized for specific demands such as sparsity or smoothness. These choices are typically made without a systematic investigation of how the norm parameter \( p \) influences the structural and perceptual properties of adversarial perturbations. In this work, we study how the choice of \( p \) affects sparsity and smoothness of adversarial attacks generated under \( \ell_p \) norm constraints for values of $p \in [1,2]$. To enable a quantitative analysis, we adopt two established sparsity measures from the literature and introduce three smoothness measures. In particular, we propose a general framework for deriving smoothness measures based on smoothing operations and additionally introduce a smoothness measure based on first-order Taylor approximations. Using these measures, we conduct a comprehensive empirical evaluation across multiple real-world image datasets and a diverse set of model architectures, including both convolutional and transformer-based networks. We show that the choice of $\ell_1$ or $\ell_2$ is suboptimal in most cases and the optimal $p$ value is dependent on the specific task. In our experiments, using $\ell_p$ norms with $p\in [1.3, 1.5]$ yields the best trade-off between sparse and smooth attacks. These findings highlight the importance of principled norm selection when designing and evaluating adversarial attacks.

</details>


### [57] [Target noise: A pre-training based neural network initialization for efficient high resolution learning](https://arxiv.org/abs/2602.06585)
*Shaowen Wang,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Weight initialization plays a crucial role in the optimization behavior and convergence efficiency of neural networks. Most existing initialization methods, such as Xavier and Kaiming initializations, rely on random sampling and do not exploit information from the optimization process itself. We propose a simple, yet effective, initialization strategy based on self-supervised pre-training using random noise as the target. Instead of directly training the network from random weights, we first pre-train it to fit random noise, which leads to a structured and non-random parameter configuration. We show that this noise-driven pre-training significantly improves convergence speed in subsequent tasks, without requiring additional data or changes to the network architecture. The proposed method is particularly effective for implicit neural representations (INRs) and Deep Image Prior (DIP)-style networks, which are known to exhibit a strong low-frequency bias during optimization. After noise-based pre-training, the network is able to capture high-frequency components much earlier in training, leading to faster and more stable convergence. Although random noise contains no semantic information, it serves as an effective self-supervised signal (considering its white spectrum nature) for shaping the initialization of neural networks. Overall, this work demonstrates that noise-based pre-training offers a lightweight and general alternative to traditional random initialization, enabling more efficient optimization of deep neural networks.

</details>


### [58] [DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters](https://arxiv.org/abs/2602.06597)
*Haoran Zhang,Haixuan Liu,Yong Liu,Yunzhong Qiu,Yuxuan Wang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.

</details>


### [59] [The hidden risks of temporal resampling in clinical reinforcement learning](https://arxiv.org/abs/2602.06603)
*Thomas Frost,Hrisheekesh Vaidya,Steve Harris*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Offline reinforcement learning (ORL) has shown potential for improving decision-making in healthcare. However, contemporary research typically aggregates patient data into fixed time intervals, simplifying their mapping to standard ORL frameworks. The impact of these temporal manipulations on model safety and efficacy remains poorly understood. In this work, using both a gridworld navigation task and the UVA/Padova clinical diabetes simulator, we demonstrate that temporal resampling significantly degrades the performance of offline reinforcement learning algorithms during live deployment. We propose three mechanisms that drive this failure: (i) the generation of counterfactual trajectories, (ii) the distortion of temporal expectations, and (iii) the compounding of generalisation errors. Crucially, we find that standard off-policy evaluation metrics can fail to detect these drops in performance. Our findings reveal a fundamental risk in current healthcare ORL pipelines and emphasise the need for methods that explicitly handle the irregular timing of clinical decision-making.

</details>


### [60] [The challenge of generating and evolving real-life like synthetic test data without accessing real-world raw data -- a Systematic Review](https://arxiv.org/abs/2602.06609)
*Maj-Annika Tammisto,Faiz Ali Shah,Daniel Rodriguez,Dietmar Pfahl*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Background: High-level system testing of applications that use data from e-Government services as input requires test data that is real-life-like but where the privacy of personal information is guaranteed. Applications with such strong requirement include information exchange between countries, medicine, banking, etc. This review aims to synthesize the current state-of-the-practice in this domain.
  Objectives: The objective of this Systematic Review is to identify existing approaches for creating and evolving synthetic test data without using real-life raw data.
  Methods: We followed well-known methodologies for conducting systematic literature reviews, including the ones from Kitchenham as well as guidelines for analysing the limitations of our review and its threats to validity.
  Results: A variety of methods and tools exist for creating privacy-preserving test data. Our search found 1,013 publications in IEEE Xplore, ACM Digital Library, and SCOPUS. We extracted data from 75 of those publications and identified 37 approaches that answer our research question partly. A common prerequisite for using these methods and tools is direct access to real-life data for data anonymization or synthetic test data generation. Nine existing synthetic test data generation approaches were identified that were closest to answering our research question. Nevertheless, further work would be needed to add the ability to evolve synthetic test data to the existing approaches.
  Conclusions: None of the publications really covered our requirements completely, only partially. Synthetic test data evolution is a field that has not received much attention from researchers but needs to be explored in Digital Government Solutions, especially since new legal regulations are being placed in force in many countries.

</details>


### [61] [Adaptive-CaRe: Adaptive Causal Regularization for Robust Outcome Prediction](https://arxiv.org/abs/2602.06611)
*Nithya Bhasker,Fiona R. Kolbinger,Susu Hu,Gitta Kutyniok,Stefanie Speidel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate prediction of outcomes is crucial for clinical decision-making and personalized patient care. Supervised machine learning algorithms, which are commonly used for outcome prediction in the medical domain, optimize for predictive accuracy, which can result in models latching onto spurious correlations instead of robust predictors. Causal structure learning methods on the other hand have the potential to provide robust predictors for the target, but can be too conservative because of algorithmic and data assumptions, resulting in loss of diagnostic precision. Therefore, we propose a novel model-agnostic regularization strategy, Adaptive-CaRe, for generalized outcome prediction in the medical domain. Adaptive-CaRe strikes a balance between both predictive value and causal robustness by incorporating a penalty that is proportional to the difference between the estimated statistical contribution and estimated causal contribution of the input features for model predictions. Our experiments on synthetic data establish the efficacy of the proposed Adaptive-CaRe regularizer in finding robust predictors for the target while maintaining competitive predictive accuracy. With experiments on a standard causal benchmark, we provide a blueprint for navigating the trade-off between predictive accuracy and causal robustness by tweaking the regularization strength, $λ$. Validation using real-world dataset confirms that the results translate to practical, real-domain settings. Therefore, Adaptive-CaRe provides a simple yet effective solution to the long-standing trade-off between predictive accuracy and causal robustness in the medical domain. Future work would involve studying alternate causal structure learning frameworks and complex classification models to provide deeper insights at a larger scale.

</details>


### [62] [Temperature Scaling Attack Disrupting Model Confidence in Federated Learning](https://arxiv.org/abs/2602.06638)
*Kichang Lee,Jaeho Jin,JaeYeon Park,Songkuk Kim,JeongGil Ko*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Predictive confidence serves as a foundational control signal in mission-critical systems, directly governing risk-aware logic such as escalation, abstention, and conservative fallback. While prior federated learning attacks predominantly target accuracy or implant backdoors, we identify confidence calibration as a distinct attack objective. We present the Temperature Scaling Attack (TSA), a training-time attack that degrades calibration while preserving accuracy. By injecting temperature scaling with learning rate-temperature coupling during local training, malicious updates maintain benign-like optimization behavior, evading accuracy-based monitoring and similarity-based detection. We provide a convergence analysis under non-IID settings, showing that this coupling preserves standard convergence bounds while systematically distorting confidence. Across three benchmarks, TSA substantially shifts calibration (e.g., 145% error increase on CIFAR-100) with <2 accuracy change, and remains effective under robust aggregation and post-hoc calibration defenses. Case studies further show that confidence manipulation can cause up to 7.2x increases in missed critical cases (healthcare) or false alarms (autonomous driving), even when accuracy is unchanged. Overall, our results establish calibration integrity as a critical attack surface in federated learning.

</details>


### [63] [Pruning at Initialisation through the lens of Graphon Limit: Convergence, Expressivity, and Generalisation](https://arxiv.org/abs/2602.06675)
*Hoang Pham,The-Anh Ta,Long Tran-Thanh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Pruning at Initialisation methods discover sparse, trainable subnetworks before training, but their theoretical mechanisms remain elusive. Existing analyses are often limited to finite-width statistics, lacking a rigorous characterisation of the global sparsity patterns that emerge as networks grow large. In this work, we connect discrete pruning heuristics to graph limit theory via graphons, establishing the graphon limit of PaI masks. We introduce a Factorised Saliency Model that encompasses popular pruning criteria and prove that, under regularity conditions, the discrete masks generated by these algorithms converge to deterministic bipartite graphons. This limit framework establishes a novel topological taxonomy for sparse networks: while unstructured methods (e.g., Random, Magnitude) converge to homogeneous graphons representing uniform connectivity, data-driven methods (e.g., SNIP, GraSP) converge to heterogeneous graphons that encode implicit feature selection. Leveraging this continuous characterisation, we derive two fundamental theoretical results: (i) a Universal Approximation Theorem for sparse networks that depends only on the intrinsic dimension of active coordinate subspaces; and (ii) a Graphon-NTK generalisation bound demonstrating how the limit graphon modulates the kernel geometry to align with informative features. Our results transform the study of sparse neural networks from combinatorial graph problems into a rigorous framework of continuous operators, offering a new mechanism for analysing expressivity and generalisation in sparse neural networks.

</details>


### [64] [Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts](https://arxiv.org/abs/2602.06689)
*Victor Armegioiu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autoregressive generative PDE solvers can be accurate one step ahead yet drift over long rollouts, especially in coarse-to-fine regimes where each step must regenerate unresolved fine scales. This is the regime of diffusion and flow-matching generators: although their internal dynamics are Markovian, rollout stability is governed by per-step \emph{conditional law} errors. Using the Mori--Zwanzig projection formalism, we show that eliminating unresolved variables yields an exact resolved evolution with a Markov term, a memory term, and an orthogonal forcing, exposing a structural limitation of memoryless closures. Motivated by this, we introduce memory-conditioned diffusion/flow-matching with a compact online state injected into denoising via latent features. Via disintegration, memory induces a structured conditional tail prior for unresolved scales and reduces the transport needed to populate missing frequencies. We prove Wasserstein stability of the resulting conditional kernel. We then derive discrete Grönwall rollout bounds that separate memory approximation from conditional generation error. Experiments on compressible flows with shocks and multiscale mixing show improved accuracy and markedly more stable long-horizon rollouts, with better fine-scale spectral and statistical fidelity.

</details>


### [65] [NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models](https://arxiv.org/abs/2602.06694)
*Hyochan Chong,Dongkyu Kim,Changdong Kim,Minseop Choi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8$\times$ in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.

</details>


### [66] [Diffeomorphism-Equivariant Neural Networks](https://arxiv.org/abs/2602.06695)
*Josephine Elisabeth Oettinger,Zakhar Shumaylov,Johannes Bostelmann,Jan Lellmann,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Incorporating group symmetries via equivariance into neural networks has emerged as a robust approach for overcoming the efficiency and data demands of modern deep learning. While most existing approaches, such as group convolutions and averaging-based methods, focus on compact, finite, or low-dimensional groups with linear actions, this work explores how equivariance can be extended to infinite-dimensional groups. We propose a strategy designed to induce diffeomorphism equivariance in pre-trained neural networks via energy-based canonicalisation. Formulating equivariance as an optimisation problem allows us to access the rich toolbox of already established differentiable image registration methods. Empirical results on segmentation and classification tasks confirm that our approach achieves approximate equivariance and generalises to unseen transformations without relying on extensive data augmentation or retraining.

</details>


### [67] [Explaining Grokking in Transformers through the Lens of Inductive Bias](https://arxiv.org/abs/2602.06702)
*Jaisidh Singh,Diganta Misra,Antonio Orvieto*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate grokking in transformers through the lens of inductive bias: dispositions arising from architecture or optimization that let the network prefer one solution over another. We first show that architectural choices such as the position of Layer Normalization (LN) strongly modulates grokking speed. This modulation is explained by isolating how LN on specific pathways shapes shortcut-learning and attention entropy. Subsequently, we study how different optimization settings modulate grokking, inducing distinct interpretations of previously proposed controls such as readout scale. Particularly, we find that using readout scale as a control for lazy training can be confounded by learning rate and weight decay in our setting. Accordingly, we show that features evolve continuously throughout training, suggesting grokking in transformers can be more nuanced than a lazy-to-rich transition of the learning regime. Finally, we show how generalization predictably emerges with feature compressibility in grokking, across different modulators of inductive bias. Our code is released at https://tinyurl.com/y52u3cad.

</details>


### [68] [SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers](https://arxiv.org/abs/2602.06706)
*Shentong Mo,Lanqing Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina have introduced flow-matching to improve sampling efficiency, the potential of tokenization for structural compression and acceleration remains largely unexplored in the protein domain. In this work, we present SaDiT, a novel framework that accelerates protein backbone generation by integrating SaProt Tokenization with a Diffusion Transformer (DiT) architecture. SaDiT leverages a discrete latent space to represent protein geometry, significantly reducing the complexity of the generation process while maintaining theoretical SE(3) equivalence. To further enhance efficiency, we introduce an IPA Token Cache mechanism that optimizes the Invariant Point Attention (IPA) layers by reusing computed token states during iterative sampling. Experimental results demonstrate that SaDiT outperforms state-of-the-art models, including RFDiffusion and Proteina, in both computational speed and structural viability. We evaluate our model across unconditional backbone generation and fold-class conditional generation tasks, where SaDiT shows superior ability to capture complex topological features with high designability.

</details>


### [69] [F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare](https://arxiv.org/abs/2602.06717)
*Daniil Plyusov,Alexey Gorbatovski,Boris Shaposhnikov,Viacheslav Sinii,Alexey Malakhov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.

</details>


### [70] [Pairwise is Not Enough: Hypergraph Neural Networks for Multi-Agent Pathfinding](https://arxiv.org/abs/2602.06733)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Pietro Lio,Amanda Prorok*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-Agent Path Finding (MAPF) is a representative multi-agent coordination problem, where multiple agents are required to navigate to their respective goals without collisions. Solving MAPF optimally is known to be NP-hard, leading to the adoption of learning-based approaches to alleviate the online computational burden. Prevailing approaches, such as Graph Neural Networks (GNNs), are typically constrained to pairwise message passing between agents. However, this limitation leads to suboptimal behaviours and critical issues, such as attention dilution, particularly in dense environments where group (i.e. beyond just two agents) coordination is most critical. Despite the importance of such higher-order interactions, existing approaches have not been able to fully explore them. To address this representational bottleneck, we introduce HMAGAT (Hypergraph Multi-Agent Attention Network), a novel architecture that leverages attentional mechanisms over directed hypergraphs to explicitly capture group dynamics. Empirically, HMAGAT establishes a new state-of-the-art among learning-based MAPF solvers: e.g., despite having just 1M parameters and being trained on 100$\times$ less data, it outperforms the current SoTA 85M parameter model. Through detailed analysis of HMAGAT's attention values, we demonstrate how hypergraph representations mitigate the attention dilution inherent in GNNs and capture complex interactions where pairwise methods fail. Our results illustrate that appropriate inductive biases are often more critical than the training data size or sheer parameter count for multi-agent problems.

</details>


### [71] [Optimal Abstractions for Verifying Properties of Kolmogorov-Arnold Networks (KANs)](https://arxiv.org/abs/2602.06737)
*Noah Schwartz,Chandra Kanth Nagesh,Sriram Sankaranarayanan,Ramneet Kaur,Tuhin Sahai,Susmit Jha*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a novel approach for verifying properties of Kolmogorov-Arnold Networks (KANs), a class of neural networks characterized by nonlinear, univariate activation functions typically implemented as piecewise polynomial splines or Gaussian processes. Our method creates mathematical ``abstractions'' by replacing each KAN unit with a piecewise affine (PWA) function, providing both local and global error estimates between the original network and its approximation. These abstractions enable property verification by encoding the problem as a Mixed Integer Linear Program (MILP), determining whether outputs satisfy specified properties when inputs belong to a given set. A critical challenge lies in balancing the number of pieces in the PWA approximation: too many pieces add binary variables that make verification computationally intractable, while too few pieces create excessive error margins that yield uninformative bounds. Our key contribution is a systematic framework that exploits KAN structure to find optimal abstractions. By combining dynamic programming at the unit level with a knapsack optimization across the network, we minimize the total number of pieces while guaranteeing specified error bounds. This approach determines the optimal approximation strategy for each unit while maintaining overall accuracy requirements. Empirical evaluation across multiple KAN benchmarks demonstrates that the upfront analysis costs of our method are justified by superior verification results.

</details>


### [72] [Disentanglement by means of action-induced representations](https://arxiv.org/abs/2602.06741)
*Gorka Muñoz-Gil,Hendrik Poulsen Nautrup,Arunava Majumder,Paulin de Schoulepnikoff,Florian Fürrutter,Marius Krumm,Hans J. Briegel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning interpretable representations with variational autoencoders (VAEs) is a major goal of representation learning. The main challenge lies in obtaining disentangled representations, where each latent dimension corresponds to a distinct generative factor. This difficulty is fundamentally tied to the inability to perform nonlinear independent component analysis. Here, we introduce the framework of action-induced representations (AIRs) which models representations of physical systems given experiments (or actions) that can be performed on them. We show that, in this framework, we can provably disentangle degrees of freedom w.r.t. their action dependence. We further introduce a variational AIR architecture (VAIR) that can extract AIRs and therefore achieve provable disentanglement where standard VAEs fail. Beyond state representation, VAIR also captures the action dependence of the underlying generative factors, directly linking experiments to the degrees of freedom they influence.

</details>


### [73] [Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities](https://arxiv.org/abs/2602.06769)
*Marco Bagatella,Thomas Rupf,Georg Martius,Andreas Krause*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advancements in zero-shot reinforcement learning (RL) have facilitated the extraction of diverse behaviors from unlabeled, offline data sources. In particular, forward-backward algorithms (FB) can retrieve a family of policies that can approximately solve any standard RL problem (with additive rewards, linear in the occupancy measure), given sufficient capacity. While retaining zero-shot properties, we tackle the greater problem class of RL with general utilities, in which the objective is an arbitrary differentiable function of the occupancy measure. This setting is strictly more expressive, capturing tasks such as distribution matching or pure exploration, which may not be reduced to additive rewards. We show that this additional complexity can be captured by a novel, maximum entropy (soft) variant of the forward-backward algorithm, which recovers a family of stochastic policies from offline data. When coupled with zero-order search over compact policy embeddings, this algorithm can sidestep iterative optimization schemes, and optimizes general utilities directly at test-time. Across both didactic and high-dimensional experiments, we demonstrate that our method retains favorable properties of FB algorithms, while also extending their range to more general RL problems.

</details>


### [74] [AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models](https://arxiv.org/abs/2602.06771)
*Fengpeng Li,Kemou Li,Qizhou Wang,Bo Han,Jiantao Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Concept erasure helps stop diffusion models (DMs) from generating harmful content; but current methods face robustness retention trade off. Robustness means the model fine-tuned by concept erasure methods resists reactivation of erased concepts, even under semantically related prompts. Retention means unrelated concepts are preserved so the model's overall utility stays intact. Both are critical for concept erasure in practice, yet addressing them simultaneously is challenging, as existing works typically improve one factor while sacrificing the other. Prior work typically strengthens one while degrading the other, e.g., mapping a single erased prompt to a fixed safe target leaves class level remnants exploitable by prompt attacks, whereas retention-oriented schemes underperform against adaptive adversaries. This paper introduces Adversarial Erasure with Gradient Informed Synergy (AEGIS), a retention-data-free framework that advances both robustness and retention.

</details>


### [75] [Calibrating Generative AI to Produce Realistic Essays for Data Augmentation](https://arxiv.org/abs/2602.06772)
*Edward W. Wolfe,Justin O. Barber*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and produce realistic text for augmenting ASE training datasets. We created simulated versions of student essays, and human raters assigned scores to them and rated the realism of the generated text. The results of the study indicate that the predict next prompting strategy produces the highest level of agreement between human raters regarding simulated essay scores, predict next and sentence strategies best preserve the rated quality of the original essay in the simulated essays, and predict next and 25 examples strategies produce the most realistic text as judged by human raters.

</details>


### [76] [On the Convergence of Multicalibration Gradient Boosting](https://arxiv.org/abs/2602.06773)
*Daniel Haimovich,Fridolin Linder,Lorenzo Perini,Niek Tax,Milan Vojnovic*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multicalibration gradient boosting has recently emerged as a scalable method that empirically produces approximately multicalibrated predictors and has been deployed at web scale. Despite this empirical success, its convergence properties are not well understood. In this paper, we bridge the gap by providing convergence guarantees for multicalibration gradient boosting in regression with squared-error loss. We show that the magnitude of successive prediction updates decays at $O(1/\sqrt{T})$, which implies the same convergence rate bound for the multicalibration error over rounds. Under additional smoothness assumptions on the weak learners, this rate improves to linear convergence. We further analyze adaptive variants, showing local quadratic convergence of the training loss, and we study rescaling schemes that preserve convergence. Experiments on real-world datasets support our theory and clarify the regimes in which the method achieves fast convergence and strong multicalibration.

</details>


### [77] [Robust Online Learning](https://arxiv.org/abs/2602.06775)
*Sajad Ashkezari*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of learning robust classifiers where the classifier will receive a perturbed input. Unlike robust PAC learning studied in prior work, here the clean data and its label are also adversarially chosen. We formulate this setting as an online learning problem and consider both the realizable and agnostic learnability of hypothesis classes. We define a new dimension of classes and show it controls the mistake bounds in the realizable setting and the regret bounds in the agnostic setting. In contrast to the dimension that characterizes learnability in the PAC setting, our dimension is rather simple and resembles the Littlestone dimension. We generalize our dimension to multiclass hypothesis classes and prove similar results in the realizable case. Finally, we study the case where the learner does not know the set of allowed perturbations for each point and only has some prior on them.

</details>


### [78] [Weisfeiler and Lehman Go Categorical](https://arxiv.org/abs/2602.06787)
*Seongjin Choi,Gahee Kim,Se-Young Yun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While lifting map has significantly enhanced the expressivity of graph neural networks, extending this paradigm to hypergraphs remains fragmented. To address this, we introduce the categorical Weisfeiler-Lehman framework, which formalizes lifting as a functorial mapping from an arbitrary data category to the unifying category of graded posets. When applied to hypergraphs, this perspective allows us to systematically derive Hypergraph Isomorphism Networks, a family of neural architectures where the message passing topology is strictly determined by the choice of functor. We introduce two distinct functors from the category of hypergraphs: an incidence functor and a symmetric simplicial complex functor. While the incidence architecture structurally mirrors standard bipartite schemes, our functorial derivation enforces a richer information flow over the resulting poset, capturing complex intersection geometries often missed by existing methods. We theoretically characterize the expressivity of these models, proving that both the incidence-based and symmetric simplicial approaches subsume the expressive power of the standard Hypergraph Weisfeiler-Lehman test. Extensive experiments on real-world benchmarks validate these theoretical findings.

</details>


### [79] [Displacement-Resistant Extensions of DPO with Nonconvex $f$-Divergences](https://arxiv.org/abs/2602.06788)
*Idan Pipano,Shoham Sabach,Kavosh Asadi,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: DPO and related algorithms align language models by directly optimizing the RLHF objective: find a policy that maximizes the Bradley-Terry reward while staying close to a reference policy through a KL divergence penalty. Previous work showed that this approach could be further generalized: the original problem remains tractable even if the KL divergence is replaced by a family of $f$-divergence with a convex generating function $f$. Our first contribution is to show that convexity of $f$ is not essential. Instead, we identify a more general condition, referred to as DPO-inducing, that precisely characterizes when the RLHF problem remains tractable. Our next contribution is to establish a second condition on $f$ that is necessary to prevent probability displacement, a known empirical phenomenon in which the probabilities of the winner and the loser responses approach zero. We refer to any $f$ that satisfies this condition as displacement-resistant. We finally focus on a specific DPO-inducing and displacement-resistant $f$, leading to our novel SquaredPO loss. Compared to DPO, this new loss offers stronger theoretical guarantees while performing competitively in practice.

</details>


### [80] [Rare Event Analysis of Large Language Models](https://arxiv.org/abs/2602.06791)
*Jake McAllister Dorman,Edward Gillman,Dominic C. Rose,Jamie F. Mair,Juan P. Garrahan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.

</details>


### [81] [FlowDA: Accurate, Low-Latency Weather Data Assimilation via Flow Matching](https://arxiv.org/abs/2602.06800)
*Ran Cheng,Lailai Zhu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data assimilation (DA) is a fundamental component of modern weather prediction, yet it remains a major computational bottleneck in machine learning (ML)-based forecasting pipelines due to reliance on traditional variational methods. Recent generative ML-based DA methods offer a promising alternative but typically require many sampling steps and suffer from error accumulation under long-horizon auto-regressive rollouts with cycling assimilation. We propose FlowDA, a low-latency weather-scale generative DA framework based on flow matching. FlowDA conditions on observations through a SetConv-based embedding and fine-tunes the Aurora foundation model to deliver accurate, efficient, and robust analyses. Experiments across observation rates decreasing from $3.9\%$ to $0.1\%$ demonstrate superior performance of FlowDA over strong baselines with similar tunable-parameter size. FlowDA further shows robustness to observational noise and stable performance in long-horizon auto-regressive cycling DA. Overall, FlowDA points to an efficient and scalable direction for data-driven DA.

</details>


### [82] [On the Identifiability of Steering Vectors in Large Language Models](https://arxiv.org/abs/2602.06801)
*Sohan Venkatesh,Ashish Mahendran Kurapath*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Activation steering methods, such as persona vectors, are widely used to control large language model behavior and increasingly interpreted as revealing meaningful internal representations. This interpretation implicitly assumes steering directions are identifiable and uniquely recoverable from input-output behavior. We formalize steering as an intervention on internal representations and prove that, under realistic modeling and data conditions, steering vectors are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Empirically, we validate this across multiple models and semantic traits, showing orthogonal perturbations achieve near-equivalent efficacy with negligible effect sizes. However, identifiability is recoverable under structural assumptions including statistical independence, sparsity constraints, multi-environment validation or cross-layer consistency. These findings reveal fundamental interpretability limits and clarify structural assumptions required for reliable safety-critical control.

</details>


### [83] [Calibrating Tabular Anomaly Detection via Optimal Transport](https://arxiv.org/abs/2602.06810)
*Hangting Ye,He Zhao. Wei Fan,Xiaozhuang Song,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: CTAD, a model-agnostic post-processing framework, enhances tabular anomaly detection by calibrating any TAD detector through sample-specific calibration using two complementary data distributions and Optimal Transport distance. It consistently improves performance across 34 diverse datasets and 7 representative detectors, including state-of-the-art deep learning methods, without requiring additional tuning.


<details>
  <summary>Details</summary>
Motivation: The heterogeneity of tabular data poses challenges for anomaly detection, as existing methods make implicit assumptions about anomaly patterns that may not be universally applicable. This leads to inconsistent performance across different datasets. The motivation is to develop a generalizable solution that can enhance the performance of any tabular anomaly detection method.

Method: CTAD (Calibrating Tabular Anomaly Detection) is a post-processing framework that characterizes normal data using an empirical distribution from random sampling and a structural distribution from K-means centroids. It measures the disruption caused by adding a test sample to these distributions using the Optimal Transport (OT) distance. A lower OT distance indicates a normal sample, while a higher OT distance suggests an anomaly. CTAD uses this measure to calibrate and improve the scores of any given TAD detector.

Result: Experiments on 34 diverse tabular datasets with 7 representative TAD detectors show that CTAD significantly and consistently improves anomaly detection performance. It effectively enhances even the most advanced deep learning-based TAD methods and demonstrates robustness across various hyperparameter settings, making it practical for deployment without extra tuning.

Conclusion: CTAD provides a robust and versatile approach to improving tabular anomaly detection, capable of enhancing a wide range of existing methods across different types of data and scenarios. Its effectiveness is supported by both theoretical analysis and empirical evidence.

Abstract: Tabular anomaly detection (TAD) remains challenging due to the heterogeneity of tabular data: features lack natural relationships, vary widely in distribution and scale, and exhibit diverse types. Consequently, each TAD method makes implicit assumptions about anomaly patterns that work well on some datasets but fail on others, and no method consistently outperforms across diverse scenarios. We present CTAD (Calibrating Tabular Anomaly Detection), a model-agnostic post-processing framework that enhances any existing TAD detector through sample-specific calibration. Our approach characterizes normal data via two complementary distributions, i.e., an empirical distribution from random sampling and a structural distribution from K-means centroids, and measures how adding a test sample disrupts their compatibility using Optimal Transport (OT) distance. Normal samples maintain low disruption while anomalies cause high disruption, providing a calibration signal to amplify detection. We prove that OT distance has a lower bound proportional to the test sample's distance from centroids, and establish that anomalies systematically receive higher calibration scores than normals in expectation, explaining why the method generalizes across datasets. Extensive experiments on 34 diverse tabular datasets with 7 representative detectors spanning all major TAD categories (density estimation, classification, reconstruction, and isolation-based methods) demonstrate that CTAD consistently improves performance with statistical significance. Remarkably, CTAD enhances even state-of-the-art deep learning methods and shows robust performance across diverse hyperparameter settings, requiring no additional tuning for practical deployment.

</details>


### [84] [AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models](https://arxiv.org/abs/2602.06825)
*Yuming Li,Qingyu Li,Chengyu Bai,Xiangyang Luo,Zeyue Xue,Wenyu Qin,Meng Wang,Yikai Wang,Shanghang Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments.
  To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy (ΔEntropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs.
  Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses ΔEntropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps.
  By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.

</details>


### [85] [Learning Deep Hybrid Models with Sharpness-Aware Minimization](https://arxiv.org/abs/2602.06837)
*Naoya Takeishi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hybrid modeling, the combination of machine learning models and scientific mathematical models, enables flexible and robust data-driven prediction with partial interpretability. However, effectively the scientific models may be ignored in prediction due to the flexibility of the machine learning model, making the idea of hybrid modeling pointless. Typically some regularization is applied to hybrid model learning to avoid such a failure case, but the formulation of the regularizer strongly depends on model architectures and domain knowledge. In this paper, we propose to focus on the flatness of loss minima in learning hybrid models, aiming to make the model as simple as possible. We employ the idea of sharpness-aware minimization and adapt it to the hybrid modeling setting. Numerical experiments show that the SAM-based method works well across different choices of models and datasets.

</details>


### [86] [Improved Sampling Schedules for Discrete Diffusion Models](https://arxiv.org/abs/2602.06849)
*Alberto Foresti,Mustapha Bounoua,Giulio Franzese,Luca Ambrogioni,Pietro Michiardi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Discrete diffusion models have emerged as a powerful paradigm for generative modeling on sequence data; however, the information-theoretic principles governing their reverse processes remain significantly less understood than those of their continuous counterparts. In this work, we bridge this gap by analyzing the reverse process dynamics through the lens of thermodynamic entropy production. We propose the entropy production rate as a rigorous proxy for quantifying information generation, deriving as a byproduct a bound on the Wasserstein distance between intermediate states and the data distribution. Leveraging these insights, we introduce two novel sampling schedules that are uniformly spaced with respect to their corresponding physics-inspired metrics: the Entropic Discrete Schedule (EDS), which is defined by maintaining a constant rate of information gain, and the Wasserstein Discrete Schedule (WDS), which is defined by taking equal steps in terms of the Wasserstein distance. We empirically demonstrate that our proposed schedules significantly outperform state-of-the-art strategies across diverse application domains, including synthetic data, music notation, vision and language modeling, consistently achieving superior performance at a lower computational budget.

</details>


### [87] [Designing a Robust, Bounded, and Smooth Loss Function for Improved Supervised Learning](https://arxiv.org/abs/2602.06858)
*Soumi Mahato,Lineesh M. C*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The loss function is crucial to machine learning, especially in supervised learning frameworks. It is a fundamental component that controls the behavior and general efficacy of learning algorithms. However, despite their widespread use, traditional loss functions have significant drawbacks when dealing with high-dimensional and outlier-sensitive datasets, which frequently results in reduced performance and slower convergence during training. In this work, we develop a robust, bounded, and smooth (RoBoS-NN) loss function to resolve the aforementioned hindrances. The generalization ability of the loss function has also been theoretically analyzed to rigorously justify its robustness. Moreover, we implement RoboS-NN loss in the framework of a neural network (NN) to forecast time series and present a new robust algorithm named $\mathcal{L}_{\text{RoBoS}}$-NN. To assess the potential of $\mathcal{L}_{\text{RoBoS}}$-NN, we conduct experiments on multiple real-world datasets. In addition, we infuse outliers into data sets to evaluate the performance of $\mathcal{L}_{\text{RoBoS}}$-NN in more challenging scenarios. Numerical results show that $\mathcal{L}_{\text{RoBoS}}$-NN outperforms the other benchmark models in terms of accuracy measures.

</details>


### [88] [Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts](https://arxiv.org/abs/2602.06859)
*Xinyu Zhao,Qingyun Sun,Jiayi Luo,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse anomaly patterns, substantially limiting their cross-domain generalization. In this work, we reveal that anomaly detectability is highly dependent on the underlying geometric properties and that embedding graphs from different domains into a single static curvature space can distort the structural signatures of anomalies. To address the challenge that a single curvature space cannot capture geometry-dependent graph anomaly patterns, we propose GAD-MoRE, a novel framework for zero-shot Generalizable Graph Anomaly Detection with a Mixture of Riemannian Experts architecture. Specifically, to ensure that each anomaly pattern is modeled in the Riemannian space where it is most detectable, GAD-MoRE employs a set of specialized Riemannian expert networks, each operating in a distinct curvature space. To align raw node features with curvature-specific anomaly characteristics, we introduce an anomaly-aware multi-curvature feature alignment module that projects inputs into parallel Riemannian spaces, enabling the capture of diverse geometric characteristics. Finally, to facilitate better generalization beyond seen patterns, we design a memory-based dynamic router that adaptively assigns each input to the most compatible expert based on historical reconstruction performance on similar anomalies. Extensive experiments in the zero-shot setting demonstrate that GAD-MoRE significantly outperforms state-of-the-art generalist GAD baselines, and even surpasses strong competitors that are few-shot fine-tuned with labeled data from the target domain.

</details>


### [89] [T-STAR: A Context-Aware Transformer Framework for Short-Term Probabilistic Demand Forecasting in Dock-Based Shared Micro-Mobility](https://arxiv.org/abs/2602.06866)
*Jingyi Cheng,Gonçalo Homem de Almeida Correia,Oded Cats,Shadi Sharif Azadeh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reliable short-term demand forecasting is essential for managing shared micro-mobility services and ensuring responsive, user-centered operations. This study introduces T-STAR (Two-stage Spatial and Temporal Adaptive contextual Representation), a novel transformer-based probabilistic framework designed to forecast station-level bike-sharing demand at a 15-minute resolution. T-STAR addresses key challenges in high-resolution forecasting by disentangling consistent demand patterns from short-term fluctuations through a hierarchical two-stage structure. The first stage captures coarse-grained hourly demand patterns, while the second stage improves prediction accuracy by incorporating high-frequency, localized inputs, including recent fluctuations and real-time demand variations in connected metro services, to account for temporal shifts in short-term demand. Time series transformer models are employed in both stages to generate probabilistic predictions. Extensive experiments using Washington D.C.'s Capital Bikeshare data demonstrate that T-STAR outperforms existing methods in both deterministic and probabilistic accuracy. The model exhibits strong spatial and temporal robustness across stations and time periods. A zero-shot forecasting experiment further highlights T-STAR's ability to transfer to previously unseen service areas without retraining. These results underscore the framework's potential to deliver granular, reliable, and uncertainty-aware short-term demand forecasts, which enable seamless integration to support multimodal trip planning for travelers and enhance real-time operations in shared micro-mobility services.

</details>


### [90] [Decoupling Variance and Scale-Invariant Updates in Adaptive Gradient Descent for Unified Vector and Matrix Optimization](https://arxiv.org/abs/2602.06880)
*Zitao Song,Cedar Site Bai,Zhe Zhang,Brian Bullins,David F. Gleich*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adaptive methods like Adam have become the $\textit{de facto}$ standard for large-scale vector and Euclidean optimization due to their coordinate-wise adaptation with a second-order nature. More recently, matrix-based spectral optimizers like Muon (Jordan et al., 2024b) show the power of treating weight matrices as matrices rather than long vectors. Linking these is hard because many natural generalizations are not feasible to implement, and we also cannot simply move the Adam adaptation to the matrix spectrum. To address this, we reformulate the AdaGrad update and decompose it into a variance adaptation term and a scale-invariant term. This decoupling produces $\textbf{DeVA}$ ($\textbf{De}$coupled $\textbf{V}$ariance $\textbf{A}$daptation), a framework that bridges between vector-based variance adaptation and matrix spectral optimization, enabling a seamless transition from Adam to adaptive spectral descent. Extensive experiments across language modeling and image classification demonstrate that DeVA consistently outperforms state-of-the-art methods such as Muon and SOAP (Vyas et al., 2024), reducing token usage by around 6.6\%. Theoretically, we show that the variance adaptation term effectively improves the blockwise smoothness, facilitating faster convergence. Our implementation is available at https://github.com/Tsedao/Decoupled-Variance-Adaptation

</details>


### [91] [Vision Transformer Finetuning Benefits from Non-Smooth Components](https://arxiv.org/abs/2602.06883)
*Ambroise Odonnat,Laetitia Chapel,Romain Tavenard,Ievgen Redko*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.

</details>


### [92] [A Cycle-Consistent Graph Surrogate for Full-Cycle Left Ventricular Myocardial Biomechanics](https://arxiv.org/abs/2602.06884)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Image-based patient-specific simulation of left ventricular (LV) mechanics is valuable for understanding cardiac function and supporting clinical intervention planning, but conventional finite-element analysis (FEA) is computationally intensive. Current graph-based surrogates do not have full-cycle prediction capabilities, and physics-informed neural networks often struggle to converge on complex cardiac geometries. We present CardioGraphFENet (CGFENet), a unified graph-based surrogate for rapid full-cycle estimation of LV myocardial biomechanics, supervised by a large FEA simulation dataset. The proposed model integrates (i) a global--local graph encoder to capture mesh features with weak-form-inspired global coupling, (ii) a gated recurrent unit-based temporal encoder conditioned on the target volume-time signal to model cycle-coherent dynamics, and (iii) a cycle-consistent bidirectional formulation for both loading and inverse unloading within a single framework. These strategies enable high fidelity with respect to traditional FEA ground truths and produce physiologically plausible pressure-volume loops that match FEA results when coupled with a lumped-parameter model. In particular, the cycle-consistency strategy enables a significant reduction in FEA supervision with only minimal loss in accuracy.

</details>


### [93] [Sample Complexity of Causal Identification with Temporal Heterogeneity](https://arxiv.org/abs/2602.06899)
*Ameya Rathod,Sujay Belsare,Salvik Krishna Nautiyal,Dhruv Laad,Ponnurangam Kumaraguru*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recovering a unique causal graph from observational data is an ill-posed problem because multiple generating mechanisms can lead to the same observational distribution. This problem becomes solvable only by exploiting specific structural or distributional assumptions. While recent work has separately utilized time-series dynamics or multi-environment heterogeneity to constrain this problem, we integrate both as complementary sources of heterogeneity. This integration yields unified necessary identifiability conditions and enables a rigorous analysis of the statistical limits of recovery under thin versus heavy-tailed noise. In particular, temporal structure is shown to effectively substitute for missing environmental diversity, possibly achieving identifiability even under insufficient heterogeneity. Extending this analysis to heavy-tailed (Student's t) distributions, we demonstrate that while geometric identifiability conditions remain invariant, the sample complexity diverges significantly from the Gaussian baseline. Explicit information-theoretic bounds quantify this cost of robustness, establishing the fundamental limits of covariance-based causal graph recovery methods in realistic non-stationary systems. This work shifts the focus from whether causal structure is identifiable to whether it is statistically recoverable in practice.

</details>


### [94] [Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design](https://arxiv.org/abs/2602.06900)
*Samuel Klein,Willie Neiswanger,Daniel Ratner,Michael Kagan,Sean Gasiorowski*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bayesian optimal experimental design (BOED) seeks to maximize the expected information gain (EIG) of experiments. This requires a likelihood estimate, which in many settings is intractable. Simulation-based inference (SBI) provides powerful tools for this regime. However, existing work explicitly connecting SBI and BOED is restricted to a single contrastive EIG bound. We show that the EIG admits multiple formulations which can directly leverage modern SBI density estimators, encompassing neural posterior, likelihood, and ratio estimation. Building on this perspective, we define a novel EIG estimator using neural likelihood estimation. Further, we identify optimization as a key bottleneck of gradient based EIG maximization and show that a simple multi-start parallel gradient ascent procedure can substantially improve reliability and performance. With these innovations, our SBI-based BOED methods are able to match or outperform by up to $22\%$ existing state-of-the-art approaches across standard BOED benchmarks.

</details>


### [95] [Parameter-free Dynamic Regret: Time-varying Movement Costs, Delayed Feedback, and Memory](https://arxiv.org/abs/2602.06902)
*Emmanuel Esposito,Andrew Jacobsen,Hao Qiu,Mengxiao Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we study dynamic regret in unconstrained online convex optimization (OCO) with movement costs. Specifically, we generalize the standard setting by allowing the movement cost coefficients $λ_t$ to vary arbitrarily over time. Our main contribution is a novel algorithm that establishes the first comparator-adaptive dynamic regret bound for this setting, guaranteeing $\widetilde{\mathcal{O}}(\sqrt{(1+P_T)(T+\sum_t λ_t)})$ regret, where $P_T$ is the path length of the comparator sequence over $T$ rounds. This recovers the optimal guarantees for both static and dynamic regret in standard OCO as a special case where $λ_t=0$ for all rounds. To demonstrate the versatility of our results, we consider two applications: OCO with delayed feedback and OCO with time-varying memory. We show that both problems can be translated into time-varying movement costs, establishing a novel reduction specifically for the delayed feedback setting that is of independent interest. A crucial observation is that the first-order dependence on movement costs in our regret bound plays a key role in enabling optimal comparator-adaptive dynamic regret guarantees in both settings.

</details>


### [96] [A first realization of reinforcement learning-based closed-loop EEG-TMS](https://arxiv.org/abs/2602.06907)
*Dania Humaidan,Jiahua Xu,Jing Chen,Christoph Zrenner,David Emanuel Vetter,Laura Marzetti,Paolo Belardinelli,Timo Roine,Risto J. Ilmoniemi,Gian Luca Romani,Ulf Zieman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Background: Transcranial magnetic stimulation (TMS) is a powerful tool to investigate neurophysiology of the human brain and treat brain disorders. Traditionally, therapeutic TMS has been applied in a one-size-fits-all approach, disregarding inter- and intra-individual differences. Brain state-dependent EEG-TMS, such as coupling TMS with a pre-specified phase of the sensorimotor mu-rhythm, enables the induction of differential neuroplastic effects depending on the targeted phase. But this approach is still user-dependent as it requires defining an a-priori target phase. Objectives: To present a first realization of a machine-learning-based, closed-loop real-time EEG-TMS setup to identify user-independently the individual mu-rhythm phase associated with high- vs. low-corticospinal excitability states. Methods: We applied EEG-TMS to 25 participants targeting the supplementary motor area-primary motor cortex network and used a reinforcement learning algorithm to identify the mu-rhythm phase associated with high- vs. low corticospinal excitability. We employed linear mixed effects models and Bayesian analysis to determine effects of reinforced learning on corticospinal excitability indexed by motor evoked potential amplitude, and functional connectivity indexed by the imaginary part of resting-state EEG coherence. Results: Reinforcement learning effectively identified the mu-rhythm phase associated with high- vs. low-excitability states, and their repetitive stimulation resulted in long-term increases vs. decreases in functional connectivity in the stimulated sensorimotor network. Conclusions: We demonstrated for the first time the feasibility of closed-loop EEG-TMS in humans, a critical step towards individualized treatment of brain disorders.

</details>


### [97] [Revisiting the Generic Transformer: Deconstructing a Strong Baseline for Time Series Foundation Models](https://arxiv.org/abs/2602.06909)
*Yunshi Wen,Wesley M. Gifford,Chandra Reddy,Lam M. Nguyen,Jayant Kalagnanam,Anak Agung Julius*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The recent surge in Time Series Foundation Models has rapidly advanced the field, yet the heterogeneous training setups across studies make it difficult to attribute improvements to architectural innovations versus data engineering. In this work, we investigate the potential of a standard patch Transformer, demonstrating that this generic architecture achieves state-of-the-art zero-shot forecasting performance using a straightforward training protocol. We conduct a comprehensive ablation study that covers model scaling, data composition, and training techniques to isolate the essential ingredients for high performance. Our findings identify the key drivers of performance, while confirming that the generic architecture itself demonstrates excellent scalability. By strictly controlling these variables, we provide comprehensive empirical results on model scaling across multiple dimensions. We release our open-source model and detailed findings to establish a transparent, reproducible baseline for future research.

</details>


### [98] [From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers](https://arxiv.org/abs/2602.06923)
*Ziming Liu,Sophia Sanborn,Surya Ganguli,Andreas Tolias*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on "world models" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous "AI Physicist" approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively "bake in" the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.

</details>


### [99] [Robustness Beyond Known Groups with Low-rank Adaptation](https://arxiv.org/abs/2602.06924)
*Abinitha Gourabathina,Hyewon Jeong,Teya Bergamaschi,Marzyeh Ghassemi,Collin Stultz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning models trained to optimize average accuracy often exhibit systematic failures on particular subpopulations. In real world settings, the subpopulations most affected by such disparities are frequently unlabeled or unknown, thereby motivating the development of methods that are performant on sensitive subgroups without being pre-specified. However, existing group-robust methods typically assume prior knowledge of relevant subgroups, using group annotations for training or model selection. We propose Low-rank Error Informed Adaptation (LEIA), a simple two-stage method that improves group robustness by identifying a low-dimensional subspace in the representation space where model errors concentrate. LEIA restricts adaptation to this error-informed subspace via a low-rank adjustment to the classifier logits, directly targeting latent failure modes without modifying the backbone or requiring group labels. Using five real-world datasets, we analyze group robustness under three settings: (1) truly no knowledge of subgroup relevance, (2) partial knowledge of subgroup relevance, and (3) full knowledge of subgroup relevance. Across all settings, LEIA consistently improves worst-group performance while remaining fast, parameter-efficient, and robust to hyperparameter choice.

</details>


### [100] [Continuous-time reinforcement learning: ellipticity enables model-free value function approximation](https://arxiv.org/abs/2602.06930)
*Wenlong Mou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study off-policy reinforcement learning for controlling continuous-time Markov diffusion processes with discrete-time observations and actions. We consider model-free algorithms with function approximation that learn value and advantage functions directly from data, without unrealistic structural assumptions on the dynamics.
  Leveraging the ellipticity of the diffusions, we establish a new class of Hilbert-space positive definiteness and boundedness properties for the Bellman operators. Based on these properties, we propose the Sobolev-prox fitted $q$-learning algorithm, which learns value and advantage functions by iteratively solving least-squares regression problems. We derive oracle inequalities for the estimation error, governed by (i) the best approximation error of the function classes, (ii) their localized complexity, (iii) exponentially decaying optimization error, and (iv) numerical discretization error. These results identify ellipticity as a key structural property that renders reinforcement learning with function approximation for Markov diffusions no harder than supervised learning.

</details>


### [101] [When RL Meets Adaptive Speculative Training: A Unified Training-Serving System](https://arxiv.org/abs/2602.06932)
*Junxiong Wang,Fengxiang Bie,Jisen Li,Zhongzhu Zhou,Zelei Shao,Yubo Wang,Yinghui Liu,Qingyang Wu,Avner May,Sri Yanamandra,Yineng Zhang,Ce Zhang,Tri Dao,Percy Liang,Ben Athiwaratkun,Shuaiwen Leon Song,Chenfeng Xu,Xiaoxia Wu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.
  To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).

</details>


### [102] [Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics](https://arxiv.org/abs/2602.06939)
*Zuyuan Zhang,Sizhe Tang,Tian Lan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.

</details>


### [103] [From Core to Detail: Unsupervised Disentanglement with Entropy-Ordered Flows](https://arxiv.org/abs/2602.06940)
*Daniel Galperin,Ullrich Köthe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning unsupervised representations that are both semantically meaningful and stable across runs remains a central challenge in modern representation learning. We introduce entropy-ordered flows (EOFlows), a normalizing-flow framework that orders latent dimensions by their explained entropy, analogously to PCA's explained variance. This ordering enables adaptive injective flows: after training, one may retain only the top C latent variables to form a compact core representation while the remaining variables capture fine-grained detail and noise, with C chosen flexibly at inference time rather than fixed during training. EOFlows build on insights from Independent Mechanism Analysis, Principal Component Flows and Manifold Entropic Metrics. We combine likelihood-based training with local Jacobian regularization and noise augmentation into a method that scales well to high-dimensional data such as images. Experiments on the CelebA dataset show that our method uncovers a rich set of semantically interpretable features, allowing for high compression and strong denoising.

</details>


### [104] [Endogenous Resistance to Activation Steering in Language Models](https://arxiv.org/abs/2602.06941)
*Alex McKenzie,Keenan Pepper,Stijn Servaes,Martin Leitgab,Murat Cubuktepe,Mike Vaiana,Diogo de Lucena,Judd Rosenblatt,Michael S. A. Graziano*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.

</details>


### [105] [Improving Credit Card Fraud Detection with an Optimized Explainable Boosting Machine](https://arxiv.org/abs/2602.06955)
*Reza E. Fazel,Arash Bakhtiary,Siavash A. Bigdeli*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Addressing class imbalance is a central challenge in credit card fraud detection, as it directly impacts predictive reliability in real-world financial systems. To overcome this, the study proposes an enhanced workflow based on the Explainable Boosting Machine (EBM)-a transparent, state-of-the-art implementation of the GA2M algorithm-optimized through systematic hyperparameter tuning, feature selection, and preprocessing refinement. Rather than relying on conventional sampling techniques that may introduce bias or cause information loss, the optimized EBM achieves an effective balance between accuracy and interpretability, enabling precise detection of fraudulent transactions while providing actionable insights into feature importance and interaction effects. Furthermore, the Taguchi method is employed to optimize both the sequence of data scalers and model hyperparameters, ensuring robust, reproducible, and systematically validated performance improvements. Experimental evaluation on benchmark credit card data yields an ROC-AUC of 0.983, surpassing prior EBM baselines (0.975) and outperforming Logistic Regression, Random Forest, XGBoost, and Decision Tree models. These results highlight the potential of interpretable machine learning and data-driven optimization for advancing trustworthy fraud analytics in financial systems.

</details>


### [106] [Learning a Generative Meta-Model of LLM Activations](https://arxiv.org/abs/2602.06964)
*Grace Luo,Jiahai Feng,Trevor Darrell,Alec Radford,Jacob Steinhardt*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating "meta-models" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [107] [Machine Learning Practitioners' Views on Data Quality in Light of EU Regulatory Requirements: A European Online Survey](https://arxiv.org/abs/2602.06594)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 本文提出了一种将既定的数据质量维度与特定的欧盟监管要求相结合的实际框架，并通过在线调查研究了180多名欧盟数据从业人员在确保符合监管要求的机器学习系统中的数据质量时所采取的方法、主要挑战和未满足的需求。研究结果揭示了当前实践与监管期望之间的关键差距，强调需要更集成的数据质量工具和技术与法律从业者之间更好的协作。


<details>
  <summary>Details</summary>
Motivation: 理解数据质量如何与机器学习(ML)系统中的监管要求相一致对于在不断发展的欧盟监管环境中航行的从业者来说是一个关键挑战。为了应对这一挑战，有必要建立一个实用框架来对齐数据质量和具体的欧盟法规需求，同时识别实践中存在的差距和未被满足的需求。

Method: 首先提出了一个实际框架，该框架将已有的数据质量维度与特定的欧盟法规要求对齐。其次，通过对超过180名基于欧盟的数据从业者进行综合性的在线调查，探讨他们在确保符合法规要求的ML系统中数据质量的方法、关键挑战及未满足的需求。

Result: 研究发现揭示了现有做法与监管预期之间的关键差异，强调了从业者对于更加整合的数据质量工具以及技术与法律从业者之间更好合作的需求。

Conclusion: 这些见解为连接技术专长与监管合规性提供了建议，最终促进了负责任且值得信赖的ML部署。

Abstract: Understanding how data quality aligns with regulatory requirements in machine learning (ML) systems presents a critical challenge for practitioners navigating the evolving EU regulatory landscape. To address this, we first propose a practical framework aligning established data quality dimensions with specific EU regulatory requirements. Second, we conducted a comprehensive online survey with over 180 EU-based data practitioners, investigating their approaches, key challenges, and unmet needs when ensuring data quality in ML systems that align with regulatory requirements. Our findings highlight crucial gaps between current practices and regulatory expectations, underscoring practitioners' need for more integrated data quality tools and better collaboration between technical and legal practitioners. These insights inform recommendations for bridging technical expertise and regulatory compliance, ultimately fostering responsible and trustworthy ML deployments.

</details>


### [108] [Filtered Approximate Nearest Neighbor Search Cost Estimation](https://arxiv.org/abs/2602.06721)
*Wenxuan Xia,Mingyu Yang,Wentao Li,Wei Wang*

Main category: cs.DB

TL;DR: 本文提出了一种新的成本估算框架E2E，用于带有过滤条件的近似k最近邻搜索，并在实际数据集上展示了其相较于现有方法在检索效率上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 带有过滤条件的高维向量相似性查询在学术界和工业界都受到了广泛关注，尤其是在结合了标签或数值范围约束的嵌入（如图像或文本）查询中。然而，由于组合过滤器引起的搜索成本高度可变，优化这类查询仍然具有挑战性。

Method: 提出了名为E2E的成本估算框架，该框架能够明确捕捉查询向量分布与属性值选择性之间的相关性，从而提供更高的估计精度。通过利用这些估计来改进搜索终止条件，进而实现性能的重大提升。

Result: 实验结果表明，在真实世界的数据集上，所提出的方法相比最先进基线，在保持高搜索准确度的同时，检索效率提高了2到3倍。

Conclusion: 本研究提出的E2E框架为解决带过滤条件的AKNN搜索问题提供了有效手段，尤其在早期终止等下游优化任务中表现突出。

Abstract: Hybrid queries combining high-dimensional vector similarity with structured attribute filtering have garnered significant attention across both academia and industry. A critical instance of this paradigm is filtered Approximate k Nearest Neighbor (AKNN) search, where embeddings (e.g., image or text) are queried alongside constraints such as labels or numerical range. While essential for rich retrieval, optimizing these queries remains challenging due to the highly variable search cost induced by combined filters. In this paper, we propose a novel cost estimation framework, E2E, for filtered AKNN search and demonstrate its utility in downstream optimization tasks, specifically early termination. Unlike existing approaches, our model explicitly captures the correlation between the query vector distribution and attribute-value selectivity, yielding significantly higher estimation accuracy. By leveraging these estimates to refine search termination conditions, we achieve substantial performance gains. Experimental results on real-world datasets demonstrate that our approach improves retrieval efficiency by 2x-3x over state-of-the-art baselines while maintaining high search accuracy.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [109] [Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space](https://arxiv.org/abs/2602.06056)
*Zihang Wang,Siyue Zhang,Yilun Zhao,Jingyi Yang,Tingyu Song,Anh Tuan Luu,Chen Zhao*

Main category: cs.MM

TL;DR: 本研究首次系统地探讨了将多模态大扩散语言模型（Multimodal dLLMs）转换为嵌入模型的可行性，并通过分类、视觉问答和信息检索三类任务评估了其性能。结果显示，与自回归视觉-语言模型相比，多模态dLLM的嵌入表现普遍较差，特别是由于图像-文本对齐不足导致的。


<details>
  <summary>Details</summary>
Motivation: 随着大型基础模型的发展，基于大型语言模型（LLMs）、视觉语言模型（VLMs）及多模态LLMs的嵌入模型也得到了快速发展。最近出现的大扩散语言模型（dLLMs）及其多模态版本作为自回归模型的竞争替代方案，在双向注意力机制和平行生成方面提供了优势。因此，自然引发了一个关键但未被探索的问题：多模态dLLMs是否可以作为有效的多模态嵌入模型？

Method: 为了回答这个问题，研究人员进行了首个关于将多模态dLLMs转变为嵌入模型的系统性研究。他们选取了最先进的多模态dLLMs和自回归VLMs，在包括分类、视觉问答以及信息检索在内的三类嵌入任务上进行比较评估。

Result: 结果表明，总体而言，多模态dLLM嵌入的表现不如自回归VLMs。其中，更强的扩散模型LaViDa在分类任务上落后3.5个百分点，在视觉问答上落后2.5个百分点，在检索任务上则落后4.4个百分点；而另一个扩散模型MMaDA在所有任务上的表现差距超过20个百分点。进一步分析揭示了扩散模型中图像-文本对齐不足的问题，这解释了它们在嵌入性能上的局限性。

Conclusion: 尽管多模态dLLMs展现了作为新型嵌入模型的潜力，但当前阶段它们在多个任务上的表现仍不及传统的自回归VLMs。研究发现，改善图像-文本对齐可能是提高多模态dLLMs嵌入性能的关键。

Abstract: Embedding models are a fundamental component of modern AI systems such as semantic search and retrieval-augmented generation. Recent advances in large foundation models have substantially accelerated the development of embedding models, including those based on Large Language Models (LLMs), Vision Language Models (VLMs), and Multimodal LLMs. More recently, Large Diffusion Language Models (dLLMs) and Multimodal dLLMs have emerged as competitive alternatives to autoregressive models, offering advantages such as bidirectional attention and parallel generation. This progress naturally raises a critical yet unexplored question: can Multimodal dLLMs serve as effective multimodal embedding models? To answer this, we present the first systematic study of converting Multimodal dLLMs into embedding models. We evaluate state-of-the-art Multimodal dLLMs and Autoregressive VLMs across three categories of embedding tasks: classification, visual question answering, and information retrieval. Our results show that Multimodal dLLM embeddings generally underperform their autoregressive VLM counterparts. The stronger diffusion-based model, LaViDa, lags by only 3.5 points on classification, 2.5 points on VQA, and 4.4 points on retrieval tasks, whereas the other diffusion-based model, MMaDA, exhibits substantially larger performance gaps, exceeding 20 points across all tasks. Further analysis reveals insufficient image-text alignment in diffusion-based models, accounting for the observed limitations in their embedding performance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [110] [MuCo: Multi-turn Contrastive Learning for Multimodal Embedding Model](https://arxiv.org/abs/2602.06393)
*Geonmo Gu,Byeongho Heo,Jaemyung Yu,Jaehui Hwang,Taekyung Kim,Sangmin Lee,HeeJae Jun,Yoohoon Kang,Sangdoo Yun,Dongyoon Han*

Main category: cs.IR

TL;DR: 本文提出了一种名为多轮对比学习（MuCo）的新框架，该框架利用了多模态大语言模型的对话特性，在单次前向传递中处理与同一图像相关的多个查询-目标对。实验表明，MuCo在MMEB和M-BEIR基准测试上达到了最先进的检索性能，并显著提高了训练效率和跨模态表示的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于对比学习构建的通用多模态嵌入模型通常采用“单轮”形式处理每个查询-目标对，这导致计算效率低下且忽略了多个查询之间的潜在上下文关系。

Method: 提出了多轮对比学习（MuCo），一种受对话启发的框架，它允许同时提取一组多个查询和目标嵌入，这些嵌入基于共享上下文表示进行条件化，从而增加有效批量大小并提高整体训练效率。

Result: 实验展示了MuCo使用新策划的500万条多模态多轮数据集(M3T)，在MMEB和M-BEIR基准上实现了最先进水平的检索表现，同时大幅提升了训练效率以及跨模态间的表征一致性。

Conclusion: 通过引入MuCo框架，研究人员能够更有效地处理多模态数据，不仅提高了模型的检索性能，还增强了跨不同模态间的数据表示一致性。

Abstract: Universal Multimodal embedding models built on Multimodal Large Language Models (MLLMs) have traditionally employed contrastive learning, which aligns representations of query-target pairs across different modalities. Yet, despite its empirical success, they are primarily built on a "single-turn" formulation where each query-target pair is treated as an independent data point. This paradigm leads to computational inefficiency when scaling, as it requires a separate forward pass for each pair and overlooks potential contextual relationships between multiple queries that can relate to the same context. In this work, we introduce Multi-Turn Contrastive Learning (MuCo), a dialogue-inspired framework that revisits this process. MuCo leverages the conversational nature of MLLMs to process multiple, related query-target pairs associated with a single image within a single forward pass. This allows us to extract a set of multiple query and target embeddings simultaneously, conditioned on a shared context representation, amplifying the effective batch size and overall training efficiency. Experiments exhibit MuCo with a newly curated 5M multimodal multi-turn dataset (M3T), which yields state-of-the-art retrieval performance on MMEB and M-BEIR benchmarks, while markedly enhancing both training efficiency and representation coherence across modalities. Code and M3T are available at https://github.com/naver-ai/muco

</details>


### [111] [TokenMixer-Large: Scaling Up Large Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2602.06563)
*Yuchen Jiang,Jie Zhu,Xintian Han,Hui Lu,Kunmin Bai,Mingyu Yang,Shikang Wu,Ruihao Zhang,Wenlin Zhao,Shipeng Bai,Sijin Zhou,Huizhi Yang,Tianyi Liu,Wenda Liu,Ziyan Gong,Haoran Ding,Zheng Chai,Deping Xie,Zhe Chen,Yuchao Zheng,Peng Xu*

Main category: cs.IR

TL;DR: 本文提出了TokenMixer-Large，通过解决原始TokenMixer架构的设计限制，包括次优残差设计、深层模型中的梯度更新不足、MoE稀疏化不全以及可扩展性探索有限等问题，成功将参数规模扩展至70亿和150亿，并在字节跳动的多个场景中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 先前的工作引入了TokenMixer架构，该架构被证明在排名阶段有效，但其本身存在一些设计上的局限性。为了解决这些问题并进一步提高模型的效率和可扩展性，研究人员开发了TokenMixer-Large。

Method: TokenMixer-Large采用了混合与复原操作、层间残差、辅助损失以及一种新的Sparse-Pertoken MoE架构来解决原有架构的核心问题。

Result: TokenMixer-Large能够分别在线上流量和线下实验中将参数扩展到70亿和150亿，同时在字节跳动的多个应用场景下实现了显著的离线和在线性能提升。

Conclusion: 通过一系列改进措施，TokenMixer-Large不仅解决了原始架构中存在的多个问题，而且展示了出色的可扩展性和性能表现，在实际应用中取得了很好的效果。

Abstract: In recent years, the study of scaling laws for large recommendation models has gradually gained attention. Works such as Wukong, HiFormer, and DHEN have attempted to increase the complexity of interaction structures in ranking models and validate scaling laws between performance and parameters/FLOPs by stacking multiple layers. However, their experimental scale remains relatively limited. Our previous work introduced the TokenMixer architecture, an efficient variant of the standard Transformer where the self-attention mechanism is replaced by a simple reshape operation, and the feed-forward network is adapted to a pertoken FFN. The effectiveness of this architecture was demonstrated in the ranking stage by the model presented in the RankMixer paper. However, this foundational TokenMixer architecture itself has several design limitations. In this paper, we propose TokenMixer-Large, which systematically addresses these core issues: sub-optimal residual design, insufficient gradient updates in deep models, incomplete MoE sparsification, and limited exploration of scalability. By leveraging a mixing-and-reverting operation, inter-layer residuals, the auxiliary loss and a novel Sparse-Pertoken MoE architecture, TokenMixer-Large successfully scales its parameters to 7-billion and 15-billion on online traffic and offline experiments, respectively. Currently deployed in multiple scenarios at ByteDance, TokenMixer -Large has achieved significant offline and online performance gains.

</details>


### [112] [R2LED: Equipping Retrieval and Refinement in Lifelong User Modeling with Semantic IDs for CTR Prediction](https://arxiv.org/abs/2602.06622)
*Qidong Liu,Gengnan Wang,Zhichen Liu,Moranxin Wang,Zijian Zhang,Xiao Han,Ni Zhang,Tao Qin,Chen Li*

Main category: cs.IR

TL;DR: 提出了一种新的 lifelong user modeling 方法 R2LED，通过引入 Multi-route Mixed Retrieval 和 Bi-level Fusion Refinement 来解决现有方法中检索噪声和语义理解不足的问题。实验结果表明该方法在性能和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 lifelong user modeling 方法存在由于数据分布偏斜导致的检索噪声问题以及精细化阶段缺乏语义理解的问题。虽然语义增强（如 LLMs 建模或语义嵌入）为这两个挑战提供了潜在解决方案，但这些方法面临着推理成本过高或表示粒度不足的问题。

Method: 提出了一个名为 R2LED 的新范式，它利用了语义身份 (SID) 的多粒度性和轻量级优点来改进长期用户建模中的检索和精细化过程。具体来说，包括一个多路径混合检索机制，用于从多个角度捕捉用户兴趣，并有效减少候选检索时的噪音；以及一种双层融合精细化设计，通过目标感知交叉注意力实现路径级别的融合，利用门控机制完成 SID 级别的融合，以连接语义空间与协作空间。

Result: 在两个公开数据集上的综合实验结果显示，所提方法在表现和效率方面都优于现有方法。

Conclusion: R2LED 通过结合多粒度语义身份信息有效地解决了 lifelong user modeling 中存在的检索噪声及语义理解欠缺问题，并且在性能和效率上取得了显著进步。

Abstract: Lifelong user modeling, which leverages users' long-term behavior sequences for CTR prediction, has been widely applied in personalized services. Existing methods generally adopted a two-stage "retrieval-refinement" strategy to balance effectiveness and efficiency. However, they still suffer from (i) noisy retrieval due to skewed data distribution and (ii) lack of semantic understanding in refinement. While semantic enhancement, e.g., LLMs modeling or semantic embeddings, offers potential solutions to these two challenges, these approaches face impractical inference costs or insufficient representation granularity. Obsorbing multi-granularity and lightness merits of semantic identity (SID), we propose a novel paradigm that equips retrieval and refinement in Lifelong User Modeling with SEmantic IDs (R2LED) to address these issues. First, we introduce a Multi-route Mixed Retrieval for the retrieval stage. On the one hand, it captures users' interests from various granularities by several parallel recall routes. On the other hand, a mixed retrieval mechanism is proposed to efficiently retrieve candidates from both collaborative and semantic views, reducing noise. Then, for refinement, we design a Bi-level Fusion Refinement, including a target-aware cross-attention for route-level fusion and a gate mechanism for SID-level fusion. It can bridge the gap between semantic and collaborative spaces, exerting the merits of SID. The comprehensive experimental results on two public datasets demonstrate the superiority of our method in both performance and efficiency. To facilitate the reproduction, we have released the code online https://github.com/abananbao/R2LED.

</details>


### [113] [Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan](https://arxiv.org/abs/2602.06654)
*Boyu Chen,Tai Guo,Weiyu Cui,Yuqing Li,Xingxing Wang,Chuan Shi,Cheng Yang*

Main category: cs.IR

TL;DR: 本文提出了一种分阶段预训练策略，以解决多模态检索模型中某些模态主导训练过程而其他模态被忽视的问题，并通过设计生成和判别任务来更好地利用压缩高维多模态嵌入的语义ID。实验结果表明，该方法相比主流基线在R@5、R@10等指标上有所提升，在美团平台上的在线A/B测试也验证了其实用性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的方法通常采用查询与项目之间的双塔架构，并执行内部和跨塔任务的联合优化。但这种联合优化往往导致某些模态在训练过程中占主导地位，同时忽略了其他模态。此外，不同模态间不一致的训练速度容易引发一次迭代问题。为了解决这些问题，作者提出了新的方法。

Method: 提出一种分阶段预训练策略，使模型能够在每个阶段专注于特定任务，有效关注并利用多模态特征，并允许对每个阶段的训练过程进行灵活控制以避免一次迭代问题。此外，为了更好地利用能够压缩高维多模态嵌入的语义ID（SIDs），设计了生成式和判别式任务来帮助模型理解SIDs、查询以及项目特征间的关联，从而提高整体性能。

Result: 大规模真实世界美团数据上的广泛实验证明，该方法相比于主流基线，在R@5、R@10和R@20指标上分别提高了3.80%、2.64%和2.17%，在N@5、N@10和N@20上分别提高了5.10%、4.22%和2.09%。美团平台上的在线A/B测试显示，该方法实现了收入增长1.12%，点击率增加1.02%。

Conclusion: 所提出的方法在多模态检索场景下表现出色，不仅解决了训练过程中存在的模态不平衡问题，还通过引入SID相关的生成与判别任务提升了模型的整体表现。实际应用效果良好，证明了其有效性和优越性。

Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint optimization of intra-tower and inter-tower tasks. However, we observe that joint optimization often leads to certain modalities dominating the training process, while other modalities are neglected. In addition, inconsistent training speeds across modalities can easily result in the one-epoch problem. To address these challenges, we propose a staged pretraining strategy, which guides the model to focus on specialized tasks at each stage, enabling it to effectively attend to and utilize multimodal features, and allowing flexible control over the training process at each stage to avoid the one-epoch problem. Furthermore, to better utilize the semantic IDs that compress high-dimensional multimodal embeddings, we design both generative and discriminative tasks to help the model understand the associations between SIDs, queries, and item features, thereby improving overall performance. Extensive experiments on large-scale real-world Meituan data demonstrate that our method achieves improvements of 3.80%, 2.64%, and 2.17% on R@5, R@10, and R@20, and 5.10%, 4.22%, and 2.09% on N@5, N@10, and N@20 compared to mainstream baselines. Online A/B testing on the Meituan platform shows that our approach achieves a 1.12% increase in revenue and a 1.02% increase in click-through rate, validating the effectiveness and superiority of our method in practical applications.

</details>


### [114] [On the Efficiency of Sequentially Aware Recommender Systems: Cotten4Rec](https://arxiv.org/abs/2602.06935)
*Shankar Veludandi,Gulrukh Kurdistan,Uzma Mushtaque*

Main category: cs.IR

TL;DR: 提出了一种新的基于线性时间余弦相似度注意力的序列推荐模型Cotten4Rec，通过减少中间缓存和内核启动开销，显著降低了资源使用量，并在基准数据集上验证了该模型能够在几乎不牺牲推荐准确性的前提下大幅减少内存占用和运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的序列推荐方法（如BERT4Rec）虽然能够有效捕捉用户历史行为模式，但因Softmax注意力机制涉及大量中间计算而导致计算开销较大。为了降低计算成本同时保持推荐效果，研究者开发了Cotten4Rec模型。

Method: 采用线性时间复杂度的余弦相似度注意力机制代替传统的Softmax注意力，并通过优化单个CUDA内核实现来最小化中间缓冲区和内核启动开销，从而提高效率。

Result: 实验结果表明，在三个基准数据集上，与BERT4Rec及线性注意力基线LinRec相比，Cotten4Rec在保证推荐准确性的同时大幅度减少了内存消耗和运行时长。

Conclusion: Cotten4Rec作为一种高效的序列推荐方案，为大规模实际应用提供了具有竞争力的选择，特别是在计算资源有限的情况下。

Abstract: Sequential recommendation (SR) models predict a user's next interaction by modeling their historical behaviors. Transformer-based SR methods, notably BERT4Rec, effectively capture these patterns but incur significant computational overhead due to extensive intermediate computations associated with Softmax-based attention. We propose Cotten4Rec, a novel SR model utilizing linear-time cosine similarity attention, implemented through a single optimized compute unified device architecture (CUDA) kernel. By minimizing intermediate buffers and kernel-launch overhead, Cotten4Rec substantially reduces resource usage compared to BERT4Rec and the linear-attention baseline, LinRec, especially for datasets with moderate sequence lengths and vocabulary sizes. Evaluations across three benchmark datasets confirm that Cotten4Rec achieves considerable reductions in memory and runtime with minimal compromise in recommendation accuracy, demonstrating Cotten4Rec's viability as an efficient alternative for practical, large-scale sequential recommendation scenarios where computational resources are critical.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [115] [Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing](https://arxiv.org/abs/2602.06057)
*Satyam Kumar,Saurabh Jha*

Main category: cs.DC

TL;DR: 该研究提出了QEIL框架，旨在通过量化推理时间的缩放规律和跨CPU、GPU及NPU加速器的异构编排来优化边缘设备上的大型语言模型推理效率。研究展示了在保持准确度的同时显著提高了能源效率、降低了能耗和延迟，并且确认了推理时间缩放定律的普适性和与架构无关性。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的边缘设备上执行大型语言模型推理时面临的挑战，特别是现有解决方案对云或数据中心基础设施的高度依赖问题。

Method: 通过开发一个名为QEIL（Quantifying Edge Intelligence via Inference time Scaling Laws）的框架，利用与架构无关的定理描述推理效率如何随模型大小、样本预算以及设备级限制变化，并结合三个优化维度：异构工作负载分布、硬件感知路由及性能-能量权衡量化指标。

Result: 广泛评估表明，对于从1.25亿到26亿参数不等的五个模型家族，均观察到了一致性的改进，包括pass@k覆盖率提高7至10.5个百分点，能耗减少35.6%至78.2%，平均功耗降低68%，延迟改善15.8%，且无准确性损失。

Conclusion: 研究表明，通过应用QEIL框架及其相关技术，在资源受限的边缘设备上实现了高效的大规模语言模型推理，验证了异构边缘编排是面向能源受限智能系统的最优策略。

Abstract: Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.

</details>


### [116] [Mapping Gemma3 onto an Edge Dataflow Architecture](https://arxiv.org/abs/2602.06063)
*Shouyu Du,Miaoxiang Yu,Zhiheng Ni,Jillian Cai,Qing Yang,Tao Wei,Zhenyu Xu*

Main category: cs.DC

TL;DR: 本文首次在边缘数据流架构（AMD Ryzen AI NPU）上实现了Gemma3大型语言和视觉模型的端到端部署。通过引入一系列硬件感知技术，包括高效的解量化引擎、优化的矩阵乘法内核以及新的注意力机制等，实现了相较于iGPU高达5.2倍的预填充速度和4.8倍的解码速度提升，并且能效比也显著提高。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示现代NPU能够在边缘设备上提供实用且低功耗的大规模语言模型(LLM)与视觉模型(VLM)推理能力，同时为将基于Transformer的模型映射到分块数据流加速器上提供了通用蓝图。

Method: 论文提出了一组针对硬件优化的技术，包括：1）用于预填充阶段的高效解量化引擎及优化后的分块矩阵乘法内核；2）FlowQKV，一种分段流水线式的注意力机制；3）FusedDQP，在解码过程中将解量化和投影融合成单一内核执行；4）FlowKV，重新构建注意力机制以维持高内存带宽利用率；5）采用紧凑型Q4NX 4位量化格式。

Result: 所提方法相比iGPU实现了最高5.2倍的预填充速度提升和4.8倍的解码速度提升；对比CPU，则分别达到了33.5倍和2.2倍的速度优势。此外，功率效率方面相对于iGPU提高了67.2倍，而与CPU相比更是提升了222.9倍。

Conclusion: 本研究表明，通过适当的硬件感知优化手段，当今的NPU能够支持边缘设备上的高性能LLM和VLM推理任务，同时也为未来类似应用场景下的模型部署提供了有价值的参考。

Abstract: We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\times$ faster prefill and $4.8\times$ faster decoding versus the iGPU, and $33.5\times$ and $2.2\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\times$ and $222.9\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.

</details>


### [117] [iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems](https://arxiv.org/abs/2602.06064)
*Yi-Xiang Hu,Yuke Wang,Feng Wu,Zirui Huang,Shuli Zeng,Xiang-Yang Li*

Main category: cs.DC

TL;DR: 提出了一种基于强化学习的迭代调度框架iScheduler，用于解决资源投资问题(RIP)，并通过实验展示了其在降低求解时间和资源成本方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法如混合整数规划和约束规划在处理大规模实例时变得非常慢，并且动态更新需要在严格的延迟预算下修订计划表。

Method: iScheduler将RIP求解建模为马尔可夫决策过程，并通过分解子问题进行顺序过程选择来构建计划表。该框架能够通过重用未更改的过程计划表并仅重新安排受影响的过程来加速优化和支持重新配置。

Result: 实验表明，与强大的商业基线相比，iScheduler可以达到有竞争力的资源成本，同时将可行性时间减少高达43倍。

Conclusion: iScheduler提供了一个有效的方法来处理共享可再生资源下的任务调度问题，特别是在面对大规模实例和动态变化时。

Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\times$ against strong commercial baselines.

</details>


### [118] [HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference](https://arxiv.org/abs/2602.06069)
*Dinesh Gopalan,Ratul Ali*

Main category: cs.DC

TL;DR: 本文提出了一种混合量化和剪枝（HQP）框架，旨在分布式边缘云环境中实现模型加速，同时保持质量。通过敏感性感知的结构剪枝算法与8位后训练量化相结合，该方法在保证精度损失不超过1.5%的前提下，实现了高达3.12倍的推理速度提升和55%的模型大小减少。


<details>
  <summary>Details</summary>
Motivation: 随着对高保真度、实时推断的需求不断增加，在分布式边缘-云环境中需要进行积极的模型优化来应对严重的延迟和能耗限制。

Method: 介绍了一种新的集成方法——混合量化与剪枝（HQP）框架，它采用基于Fisher信息矩阵高效近似计算得出的动态权重敏感度指标指导冗余过滤器的迭代移除过程，并且只有当满足最大允许准确率下降条件时才会执行8位后训练量化。

Result: 在不同NVIDIA Jetson边缘平台上使用MobileNetV3和ResNet-18等资源效率高的架构进行了全面评估，结果显示HQP框架能够达到最高3.12倍的推理加速以及55%的模型尺寸缩减，同时严格控制准确率下降不超过1.5%。

Conclusion: 与传统的单一目标压缩技术相比，HQP框架被证明是部署超低延迟AI于资源受限边缘基础设施中的更优解，具有硬件无关性优势。

Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.

</details>


### [119] [FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs](https://arxiv.org/abs/2602.06071)
*Rajat Vadiraj Dwaraknath,Sungyoon Kim,Mert Pilanci*

Main category: cs.DC

TL;DR: 本文提出了一种新的稀疏草图BlockPerm-SJLT及其对应的优化CUDA内核FlashSketch，旨在解决随机稀疏性与GPU高效实现之间的矛盾。通过引入可调参数来平衡GPU效率和草图鲁棒性，并在多种任务中实现了约1.7倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏草图如稀疏Johnson-Lindenstrauss变换虽然减少了线性代数运算中的算术成本，但其随机稀疏性导致了不规则的内存访问模式，不利于现代GPU上的高效实现。为了解决这一问题，研究者采取了草图-内核协同设计的方法。

Method: 设计了一种名为BlockPerm-SJLT的新稀疏草图族，该草图的稀疏结构被特别选择以支持FlashSketch，这是一个针对这些草图进行了优化的CUDA内核。此外，还提供了一个可调节参数，用于显式地在GPU效率和草图鲁棒性之间进行权衡。

Result: BlockPerm-SJLT在无意识子空间嵌入（OSE）框架下得到了理论保证，并分析了可调参数对草图质量的影响。实验结果表明，FlashSketch在各种基准测试及GraSS等端到端机器学习数据归因管道上都表现出了优越性能，在不同场景和任务中相对于现有最佳GPU草图实现了大约1.7倍的速度提升。

Conclusion: 本研究通过提出BlockPerm-SJLT和FlashSketch成功缓解了随机稀疏性与GPU高效执行间的冲突，同时保持了良好的草图近似特性。这为未来利用GPU加速处理大规模数据集提供了新思路。

Abstract: Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.

</details>


### [120] [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072)
*Rui Ning,Wei Zhang,Fan Lai*

Main category: cs.DC

TL;DR: PackInfer, a new kernel-level attention framework, optimizes the inference process for large language models (LLMs) by improving GPU utilization and reducing memory fragmentation, leading to 13.0-20.1% lower inference latency and 20% higher throughput compared to FlashAttention.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the inefficiencies in the current LLM serving process, where batching requests with different sequence lengths causes computation and I/O imbalance, resulting in underutilized GPU resources and increased processing time for certain tasks.

Method: The method involves developing PackInfer, which reorganizes batched requests into balanced execution groups, packs multiple requests into unified kernel launches, eliminates redundant computations, and uses I/O-aware grouping to co-locate shared-prefix requests while reorganizing key-value (KV) caches to reduce memory fragmentation and data movement.

Result: The result of implementing PackInfer shows a significant improvement in inference performance, with a reduction in latency by 13.0-20.1% and an increase in throughput by 20% when compared to the state-of-the-art FlashAttention on real-world workloads.

Conclusion: PackInfer demonstrates a more efficient way to handle batched inference for LLMs, optimizing both computational and I/O operations, and significantly improving the overall performance of LLM serving systems.

Abstract: Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.

</details>


### [121] [Experimental Analysis of Server-Side Caching for Web Performance](https://arxiv.org/abs/2602.06074)
*Mohammad Umar,Bharat Tripathi*

Main category: cs.DC

TL;DR: 本研究通过实验比较了无缓存和内存缓存两种服务器端Web应用程序配置的性能，结果表明使用内存缓存能够显著减少响应时间，对于教育环境和小型Web应用来说是一个简单且有效的性能优化方法。


<details>
  <summary>Details</summary>
Motivation: 尽管缓存在Web性能优化文献中被广泛探讨，但对于小规模Web应用中简单内存缓存效果的实验性工作仍显不足。本文旨在填补这一研究空白。

Method: 采用轻量级Web服务器框架进行性能评估，通过在相同环境条件下重复发送HTTP请求来测量响应时间，并对比分析有无内存缓存情况下的差异。

Result: 实验结果显示，使用内存缓存后请求的响应时间有了显著降低。

Conclusion: 简单的服务器端缓存策略能有效提升Web应用性能，特别适合于重视简易性和可重现性的教育场景及小规模Web应用。

Abstract: Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.

</details>


### [122] [Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)
*Liangyu Wang,Siqi Zhang,Junjie Wang,Yiming Dong,Bo Zheng,Zihan Qiu,Shengkun Tang,Di Wang,Rui Men,Dayiheng Liu*

Main category: cs.DC

TL;DR: Canzona框架通过解耦逻辑优化器分配和物理参数分布，解决了大规模语言模型在分布式框架中使用矩阵优化器时遇到的更新冲突问题。它引入了静态分区策略和异步计算流水线技术，从而提高了数据并行性和张量并行性下的训练效率，在Qwen3模型家族上实现了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）规模的增长，对于高效收敛性的需求推动了对基于矩阵的优化器（如Shampoo、Muon、SOAP等）的兴趣。然而，这些优化器要求整体更新的特点与像Megatron这样的分布式框架中的张量碎片化现象相冲突，现有的解决方法要么存在计算冗余问题，要么无法在不违反有效通信原语几何约束的情况下解决这一矛盾。

Method: 提出了Canzona框架，该框架统一、异步且负载均衡，能够将逻辑上的优化器指派与物理上的参数分布解耦。针对数据并行性，采用alpha平衡静态分区策略来保持原子性的同时消除负载不平衡；对于张量并行性，则设计了一种利用微组调度的异步计算流水线以批量处理碎片化的更新并隐藏重建开销。

Result: 通过对Qwen3模型系列（最高达320亿参数）在256个GPU上的广泛评估显示，所提出的方法不仅保留了现有并行架构的效率，并且在端到端迭代时间和优化器步骤延迟方面分别达到了1.57倍和5.8倍的加速效果。

Conclusion: Canzona框架有效地解决了大规模语言模型在分布式环境中应用矩阵优化器时面临的问题，提供了一个既高效又灵活的解决方案，为未来更大规模模型的训练提供了新的可能性。

Abstract: The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.

</details>


### [123] [LAAFD: LLM-based Agents for Accelerated FPGA Design](https://arxiv.org/abs/2602.06085)
*Maxim Moraru,Kamalavasan Kamalakkannan,Jered Dominguez-Trujillo,Patrick Diehl,Atanu Barai,Julien Loiseau,Zachary Kent Baker,Howard Pritchard,Galen M Shipman*

Main category: cs.DC

TL;DR: 本文介绍了一种名为LAAFD的工作流，它利用大型语言模型将通用C++代码转换为优化后的Vitis HLS内核，从而实现对FPGA加速计算的自动化和高效化。实验表明，在HPC中常见的15个内核上，LAAFD相比手工调优的基线在性能上达到了99.9%的几何平均值，并且对于模板工作负载，LAAFD能够与最先进的领域特定语言（DSL）基础的HLS代码生成器SODA相匹敌，同时产生更易读的内核代码。


<details>
  <summary>Details</summary>
Motivation: 尽管FPGA为加速计算提供了高性能、低延迟和能效的优势，但其在科学及边缘计算场景中的应用受到限制，主要原因是需要专门的硬件知识。虽然高层次综合（HLS）提高了相对于硬件描述语言（HDLs）的生产力，但要达到竞争性的设计仍然需要进行硬件意识优化以及精心的数据流设计。

Method: 提出了LAAFD，一种使用大型语言模型来将通用C++代码转化为针对Vitis HLS优化过的内核的工作流。LAAFD自动执行关键转换：深度流水线技术、向量化处理以及数据流划分，并通过HLS协同仿真与合成反馈闭环来验证正确性的同时逐步改善执行周期时间。

Result: 在代表HPC中常见计算模式的一组15个内核上，LAAFD实现了与手工调整的Vitis HLS基线相比99.9%的几何平均性能。对于模板工作负载，LAAFD的表现与SODA（一种基于DSL的状态最先进HLS代码生成器）相当，同时还产生了更加易于理解的内核代码。

Conclusion: 研究结果表明，LAAFD显著降低了利用FPGA进行加速所需的专家门槛，同时并未牺牲效率。

Abstract: FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.

</details>


### [124] [BouquetFL: Emulating diverse participant hardware in Federated Learning](https://arxiv.org/abs/2602.06498)
*Arno Geimer*

Main category: cs.DC

TL;DR: 本文提出BouquetFL框架，旨在单一物理机器上模拟异构客户端硬件，以解决联邦学习研究中忽视参与者间潜在硬件异质性的问题。该工具为研究者提供了一种在现实硬件多样性条件下进行受控实验的便捷方法，无需使用多个物理设备，从而使得实验实践更接近实际部署条件。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许多方协作训练共享的机器学习模型，而无需交换信息。然而，现有的大部分FL研究都是基于中心机器上的仿真进行的，未充分考虑到参与方之间可能存在的硬件异质性问题。为了填补这一方法论上的空白，本论文开发了BouquetFL框架。

Method: 通过程序化地模仿不同硬件配置来限制资源，BouquetFL能够在单个物理机上模拟出多样化的客户端硬件环境。此外，它还提供了从常见消费级和小型实验室设备中得出的各种配置文件，以及一个基于真实世界硬件流行度构建的自定义硬件采样器，让用户可以根据自己的偏好来配置联盟。

Result: BouquetFL成功地为研究人员提供了一个平台，可以在不依赖于多台物理设备的情况下探索系统异质性对FL的影响。这不仅简化了实验过程，也使实验设置更加贴近真实的部署场景。

Conclusion: BouquetFL框架为联邦学习领域引入了一种新的方法，用于模拟高度异构的联盟，支持在单一物理机器上实现对多样化硬件环境下的控制实验。这对于希望在更贴近实际部署条件的研究人员来说特别有用。

Abstract: In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.

</details>


### [125] [FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training](https://arxiv.org/abs/2602.06499)
*Gyeongseo Park,Eungyeong Lee,Song-woo Sok,Myung-Hoon Cha,Kwangwon Koh,Baik-Song An,Hongyeon Kim,Ki-Dong Kang*

Main category: cs.DC

TL;DR: FCDP, a novel approach for training large models on commodity hardware, reduces inter-node communication by 50% and selectively communicates only trainable parameters during parameter-efficient fine-tuning, achieving up to 100x higher throughput than ZeRO-3.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges faced by researchers using commodity hardware when training large models, specifically the severe inter-node all-gather bottlenecks. Existing solutions either trade memory capacity for reduced communication or offload to host memory, which can lead to out-of-memory issues or decreased throughput due to PCIe overhead.

Method: FCDP leverages host memory as a fast caching layer rather than an overflow tier, caches forward-pass parameters in host memory, and reuses them via fast intra-node all-gather during the backward pass. For parameter-efficient fine-tuning (PEFT), FCDP further optimizes by communicating only the trainable parameters, thus maximizing the use of caching and reducing inter-node traffic significantly.

Result: In a commodity cluster setup, FCDP demonstrates up to 100x higher throughput compared to ZeRO-3 and 51x higher than ZeRO++, while still maintaining the maximum batch size possible with ZeRO-3. This indicates that FCDP effectively mitigates the inter-node communication bottleneck and enhances the efficiency of model training on limited hardware resources.

Conclusion: FCDP provides a more efficient method for training large-scale models on commodity hardware, significantly improving throughput and reducing inter-node communication, making it a promising solution for researchers without access to high-bandwidth interconnects.

Abstract: Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.

</details>


### [126] [DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving](https://arxiv.org/abs/2602.06502)
*Ying Yuan,Pengfei Zuo,Bo Wang,Zhangyu Chen,Zhipeng Tan,Zhou Yu*

Main category: cs.DC

TL;DR: 提出了一种名为DualMap的双映射调度策略，用于分布式大语言模型服务中同时实现缓存亲和性和负载均衡。通过为每个请求基于其提示符使用两个独立哈希函数映射到两个候选实例，并根据系统当前状态智能选择更好的候选者，从而提高具有相同前缀的请求共置的可能性，同时均匀分散不同的前缀。此外，结合了SLO感知路由、热点感知重平衡及轻量级双重哈希环扩展三项技术以增强对动态和偏斜工作负载的鲁棒性。实验表明，与最先进方法相比，在相同的TTFT SLO约束下，DualMap可将有效请求容量提高至多2.25倍。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型（LLM）服务中，跨请求重用提示符的KV缓存对于减少首次文本生成时间（TTFT）和服务成本至关重要。然而，旨在最大化KV缓存重用的缓存亲和性调度与追求均匀分布请求的负载均衡调度之间存在冲突。现有调度器无法在一个单一映射空间内同时达成这两个目标，因此需要一种能够兼顾两者的新方法。

Method: 设计了一种称为DualMap的双映射调度策略。该策略首先利用两个独立的哈希函数基于请求提示将每个请求映射到两个候选实例上，然后依据当前系统状态智能地从中挑选出更优的选择。为了使DualMap能够在真实世界中高度动态且倾斜的工作负载下保持健壮性，研究还引入了三种技术：1) SLO感知请求路由；2) 热点感知重平衡；3) 轻量级双哈希环扩展。

Result: 实验证明，DualMap相较于现有最先进技术，在保证相同TTFT SLO的前提下，能够将系统的有效请求处理能力提升高达2.25倍。

Conclusion: DualMap作为一种新颖的双映射调度方案，成功解决了LLM服务领域中存在的缓存亲和性与负载均衡之间的权衡问题，为提高系统性能提供了一个有效的解决方案。

Abstract: In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\times$ under the same TTFT SLO constraints compared with SOTA work.

</details>


### [127] [Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms](https://arxiv.org/abs/2602.06555)
*Lanpei Li,Massimo Coppola,Malio Li,Valerio Besozzi,Jack Bell,Vincenzo Lomonaco*

Main category: cs.DC

TL;DR: 本文提出了一种在无服务器平台上动态管理结构化并行处理框架的方法，通过结合可重用的农场模板和基于Gymnasium的监控控制层，实现了对Farm模式的有效管理。研究了AI驱动的动态扩展对于管理无服务器函数的并行度的有效性，并通过强化学习策略与基于简单模型的响应式管理基线对比，展示了基于AI的管理方法能够更好地适应平台特定限制，提高服务质量的同时保持资源使用的效率和稳定的扩展行为。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为无服务器及连续环境带来类似高性能计算（HPC）的表现力与韧性，同时保留骨架编程带来的可编程性优势。特别地，作者们关注于著名的Farm模式在开源OpenFaaS平台上的实现情况，将工作池的自动缩放视为一个服务质量意识下的资源管理问题来处理。

Method: 提出了一个框架，该框架结合了一个可重复使用的农场模板以及一个基于Gymnasium的监控和控制层，后者向反应性和基于学习的控制器公开队列、时间和服务质量指标。此外，还探讨了AI驱动的动态缩放在管理农场并行度方面的有效性，特别是在OpenFaaS上无服务器功能的可扩展性方面。

Result: 实验结果表明，与从简单农场性能模型得出的响应式管理基准相比，两种强化学习(RL)策略在改善服务质量的同时，还能维持有效的资源利用和稳定的扩展行为。这说明基于AI的管理方式能够比纯模型驱动的服务质量指导更有效地应对平台特有的局限性。

Conclusion: 本研究表明，采用AI技术进行动态管理和自动调整可以显著提升无服务器环境中并行处理任务时的服务质量和资源利用率，同时保持系统稳定。

Abstract: We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.

</details>
