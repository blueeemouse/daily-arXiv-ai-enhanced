<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.DC](#cs.DC) [Total: 20]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 86]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair](https://arxiv.org/abs/2512.22216)
*Shaunak Samant*

Main category: cs.SE

TL;DR: This study explores the effectiveness of a small transformer model, CodeT5-small, in repairing real-world Java bugs. Despite achieving high syntactic validity, the model fails to generate correct repairs and often reproduces the buggy code.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate if a small transformer model can effectively repair real-world Java bugs and whether syntactic correctness can be a reliable indicator of semantic correctness.

Method: The method involves fine-tuning CodeT5-small on 52,364 Java bug-fix pairs from CodeXGLUE, followed by evaluating its token-level performance and syntactic validity through AST parsing.

Result: The result shows that while the model generates syntactically valid Java code in about 94% of cases, it does not produce any correct repairs when assessed with an exact-match criterion, and in around 80% of the cases, the model simply repeats the original buggy code.

Conclusion: The conclusion drawn is that although the model demonstrates a high level of grammatical correctness, it is ineffective for generating semantically correct bug fixes, suggesting that syntactic correctness is not a sufficient proxy for semantic correctness.

Abstract: Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.
  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.

</details>


### [2] [Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing](https://arxiv.org/abs/2512.22250)
*Bo Yang,Yinfen Xia,Weisong Sun,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出了一种新的基于变体测试的SQL幻觉检测方法SQLHD，该方法不需要标准答案，通过结构感知和逻辑感知的变体关系来检测文本到SQL任务中的幻觉。实验结果显示，在F1分数方面，该方法表现优秀，范围从69.36%到82.76%，并且在识别文本到SQL任务中的幻觉方面优于大语言模型自我评估方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文本转SQL生成中展现了强大的泛化能力和适应性，但有时会产生不切实际或不合逻辑的内容（即幻觉），导致错误的SQL查询并对下游应用产生负面影响。现有的文本转SQL错误检测方法对于LLMs存在显著限制，主要是由于缺乏真实数据。为了解决这一挑战，提出了新的解决方案。

Method: 提出了一种名为SQLHD的新幻觉检测方法，基于变形测试（MT），无需依赖标准答案。该方法将检测任务分为两个连续阶段：通过八个结构感知的变形关系（MRs）进行模式链接幻觉检测，以及通过九个逻辑感知的MRs进行逻辑合成幻觉检测。每个阶段分别调用LLM生成模式映射或SQL工件；后续输出与源输出通过相应的MRs交叉检查，任何违反都被标记为幻觉，而无需真实的SQL。

Result: 实验结果表明，该方法在F1分数方面的性能优越，范围从69.36%到82.76%。此外，SQLHD在识别Text-to-SQL任务中的幻觉方面表现出色，优于LLM自评估方法。

Conclusion: SQLHD作为一种创新的方法，解决了现有技术在处理LLMs生成的幻觉时遇到的数据稀缺问题，并且在准确性和效率上都展现出了明显优势，为提高Text-to-SQL转换质量提供了有效途径。

Abstract: In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\% to 82.76\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.

</details>


### [3] [AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents](https://arxiv.org/abs/2512.22387)
*Bhanu Prakash Vangala,Ali Adibifar,Tanu Malik,Ashish Gehani*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLMs）生成的代码在仅有操作系统包和模型指定依赖项的干净环境中是否可以成功执行，发现只有68.3%的项目能够直接运行，不同编程语言之间存在显著差异，并且实际运行时依赖项比声明的多出13.5倍。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型作为编码助手的兴起，它们对软件开发速度有潜在加速作用，但这些自动生成代码的可重复性问题尚未得到充分探索。本研究旨在填补这一空白，通过实证研究来评估由这类模型产生的代码，在仅安装了操作系统自带包以及模型自身指定的依赖项情况下能否成功执行。

Method: 选择了三个最先进的基于LLM的编码助手（Claude Code、OpenAI Codex 和 Gemini），针对Python、JavaScript 和 Java 三种编程语言，根据100个标准化提示生成共300个项目。引入了一个三层依赖框架（区分声称依赖、工作依赖和运行时依赖）来量化执行可重复性。

Result: 结果表明，仅有68.3%的项目能够在开箱即用的情况下执行，不同编程语言之间存在很大差异（Python为89.2%，Java仅为44.0%）。此外，从声明到实际运行时依赖项平均扩展了13.5倍，揭示了显著隐藏的依赖关系。

Conclusion: 尽管LLM在促进软件开发方面展现出巨大潜力，但其生成代码的可执行性和可重复性仍面临挑战，特别是在处理依赖关系方面。需要进一步的工作来提高LLM生成代码的质量和可靠性。

Abstract: The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.

</details>


### [4] [Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding](https://arxiv.org/abs/2512.22418)
*Yi-Hung Chou,Boyuan Jiang,Yi Wen Chen,Mingyue Weng,Victoria Jackson,Thomas Zimmermann,James A. Jones*

Main category: cs.SE

TL;DR: 研究通过分析20个vibe-coding视频，揭示了开发者在使用大型语言模型辅助编程时的行为模式和面临的挑战。不同开发者对AI生成代码的依赖程度不同，但所有人都需要应对代码生成的不确定性，并且开发者的专业背景和对AI的信任度影响着他们的提示策略、评估实践以及信任水平。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型被认为能够极大提高软件开发效率，但对于实际中开发者如何定义并实施这种基于提示而非传统编码方式的“氛围编码”知之甚少。本研究旨在探索这一新兴现象背后的实践细节。

Method: 采用扎根理论研究方法，分析了包括7个实时编码会话（约16小时，254个提示）和13个意见视频（约5小时）在内的20个vibe-coding视频，并进一步分析了活动持续时间和提示意图。

Result: 研究发现了一种行为谱系：一些氛围编码者几乎完全依赖AI而不检查代码，而另一些则审查并调整生成的输出。所有参与者都必须面对生成过程中的随机性问题，调试与优化常被描述为‘掷骰子’。此外，根据氛围编码者的专业知识及其对AI的信任度形成的差异心理模型影响着提示策略、评价实践及信任层次。

Conclusion: 这些发现为软件工程未来的研究开辟了新方向，并指出在工具设计与教育培训方面存在实际机会。

Abstract: Large language models (LLMs) are reshaping software engineering by enabling "vibe coding," in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as "rolling the dice." Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.

</details>


### [5] [GraphLocator: Graph-guided Causal Reasoning for Issue Localization](https://arxiv.org/abs/2512.22469)
*Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin*

Main category: cs.SE

TL;DR: GraphLocator, a novel approach for issue localization in software repositories, utilizes a causal issue graph (CIG) to address symptom-to-cause and one-to-many mismatches. It achieves significant improvements over baseline methods in terms of recall and precision.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to tackle the fundamental yet challenging task of issue localization in automated software engineering, specifically addressing the semantic gap between natural language issue descriptions and source code implementations, which leads to symptom-to-cause and one-to-many mismatches.

Method: The method proposed in this paper is GraphLocator, an innovative approach that leverages causal structure discovering and dynamic issue disentangling to mitigate the aforementioned mismatches. The core of GraphLocator is the Causal Issue Graph (CIG), which models sub-issues and their associated code entities as vertices and captures causal dependencies through edges. The process involves two key phases: locating symptom vertices and dynamically expanding the CIG by reasoning over neighboring vertices.

Result: Experimental results on three real-world datasets show that GraphLocator outperforms baseline methods, with notable improvements in function-level recall (+19.49%) and precision (+11.89%). Additionally, it demonstrates superior performance in both types of mismatch scenarios, achieving recall and precision enhancements. The CIG generated also significantly boosts the downstream resolving task's performance, marking a 28.74% relative improvement.

Conclusion: In conclusion, GraphLocator effectively addresses the challenges of issue localization in software engineering by bridging the semantic gap between issue descriptions and code, thus offering a more accurate and efficient solution compared to existing approaches.

Abstract: The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.

</details>


### [6] [Isolating Compiler Faults via Multiple Pairs of Adversarial Compilation Configurations](https://arxiv.org/abs/2512.22538)
*Qingyang Li,Yibiao Yang,Maolin Sun,Jiangchang Wu,Qingkai Shi,Yuming Zhou*

Main category: cs.SE

TL;DR: 提出了一种新的方法MultiConf，通过构建多个对抗性编译配置对来自动隔离编译器故障，并使用基于谱的故障定位公式来排名可疑的编译器源文件。在60个真实GCC编译器错误的基准测试中，MultiConf相比现有技术在效果和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 由于现代编译器基础设施的复杂性和规模，识别并修复编译器中的错误变得非常困难。为了更有效地定位这些错误到具体的源代码文件，提出了本研究的方法。

Method: 研究介绍了一种名为MultiConf的新方法，它通过创建多对仅在少数细粒度选项上有所不同的对抗性编译配置（失败配置与其对应的通过配置）来自动分离出编译器故障。接着，采用基于谱的故障定位(SBFL)公式给编译器源文件打分，根据每对配置独立产生的排序结果，再通过加权投票方案汇总得到最终的可疑度排序，从而实现更精准可靠的故障定位。

Result: 在包含60个真实世界GCC编译器bug的基准测试中，MultiConf的表现优于现有的编译器故障定位技术，在效果和效率方面均有显著提高；尤其在Top-1文件级别成功定位了27个bug，比当前最优方法Odfl(20)和Basic(21)分别提高了35.0%和28.6%。

Conclusion: 实验结果表明，MultiConf是一种有效的编译器故障定位方法，能够以更高的准确率和效率确定引起问题的源代码文件。

Abstract: Compilers are fundamental to modern software development, making the effective identification and resolution of compiler faults essential. However, localizing these faults to specific source files remains highly challenging due to the complexity and scale of modern compiler infrastructures. In this study, we propose MultiConf, a novel approach that automatically isolates compiler faults by constructing multiple pairs of adversarial compilation configurations. Each adversarial compilation configuration pair consists of a failing configuration and its corresponding passing configuration, which differ in only a small number of fine-grained options. MultiConf generates failing configurations through a lightweight construction process and derives the corresponding passing configurations by selectively disabling bug-related fine-grained options. We then employ a Spectrum-Based Fault Localization (SBFL) formula to rank the suspiciousness of compiler source files. Each adversarial configuration pair independently produces a ranking, which is subsequently aggregated using a weighted voting scheme to derive a final suspiciousness ranking, enabling more accurate and robust fault localization. We evaluate MultiConf on a benchmark of 60 real-world GCC compiler bugs. The results demonstrate that MultiConf significantly outperforms existing compiler fault localization techniques in both effectiveness and efficiency. In particular, MultiConf successfully localizes 27 out of 60 bugs at the Top-1 file level, representing improvements of 35.0% and 28.6% over the two state-of-the-art approaches, Odfl(20) and Basic(21), respectively.

</details>


### [7] [Rethinking the Capability of Fine-Tuned Language Models for Automated Vulnerability Repair](https://arxiv.org/abs/2512.22633)
*Woorim Han,Yeongjun Kwak,Miseon Yu,Kyeongmin Kim,Younghan Lee,Hyungon Moon,Yunheung Paek*

Main category: cs.SE

TL;DR: 本文研究了基于学习的自动漏洞修复(AVR)技术的有效性，发现现有模型存在过拟合问题且评估方法不完善。为此，作者通过语义保持转换测试集、重新划分数据集以及引入新的评测基准L-AVRBench来更准确地评价AVR模型的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于微调语言模型的自动漏洞修复技术在生成补丁方面展现出潜力，但其修复未知漏洞的能力仍有待验证。此外，现有模型容易过拟合到训练集，并且常用的基于匹配度量标准可能无法全面反映修复效果，因为可能存在多种有效的修复方式。

Method: 1. 对测试集进行语义保持变换，检验模型是否真正学到了鲁棒的漏洞修复模式。
2. 重新划分训练集、验证集和测试集，确保它们互斥，以此评估模型泛化能力。
3. 引入L-AVRBench，一个专为基于学习的AVR设计的测试基准，旨在克服基于匹配度量标准的局限性，更好地考察AVR模型的实际修复能力。

Result: 研究表明，目前先进的AVR模型确实存在一定程度上的过拟合现象；同时，基于匹配的传统评估指标未能充分体现出模型对于不同但同样有效的修复方案的适应性。通过提出的方法与新基准L-AVRBench，能够更加客观公正地衡量AVR系统的性能。

Conclusion: 该工作揭示了现有基于学习的AVR系统存在的问题，并提出了一套改进的评估框架，包括数据处理策略及新的评测基准，以促进该领域向更准确地衡量模型真实修复能力的方向发展。

Abstract: Learning-based automated vulnerability repair (AVR) techniques that utilize fine-tuned language models have shown promise in generating vulnerability patches. However, questions remain about their ability to repair unseen vulnerabilities. Our empirical study reveals that state-of-the-art models often overfit to the training set and are evaluated using training, validation, and test sets that are not mutually exclusive. Furthermore, relying on match-based metrics that compare generated patches to reference fixes at the token level has some limitations, failing to account for the possibility of various valid ways to patch the vulnerability. In this paper, we examine the capabilities of state-of-the-art fine-tuned AVR models and the adequacy of match-based evaluation metrics in three ways. First, we apply semantic-preserving transformations to test sets in order to determine whether models truly learn robust vulnerability-repair patterns or simply rely on spurious features. Second, we re-split the training, validation, and test sets to be mutually exclusive and evaluate the models on the revised test set to assess their generalization capabilities. Third, we introduce L-AVRBench, a test-based benchmark tailored for learning-based AVR, to overcome the limitations of match-based metrics and examine the AVR models' true repair capabilities.

</details>


### [8] [CFIghter: Automated Control-Flow Integrity Enablement and Evaluation for Legacy C/C++ Systems](https://arxiv.org/abs/2512.22701)
*Sabine Houy,Bruno Kreyssig,Alexandre Bartel*

Main category: cs.SE

TL;DR: CFIghter 是一个全自动系统，能够通过检测、分类和修复测试套件中暴露的意外策略违规来在实际项目中启用严格的基于类型的控制流完整性（CFI）。它结合了全程序分析与引导运行时监控，并仅在必要处迭代应用最小必要的调整，直到所有测试通过或剩余失败被认为无法解决。在四个 GNU 项目的评估中，CFIghter 在保持大多数代码库严格基于类型 CFI 的同时，无需手动源代码更改，主要依靠自动生成的可见性调整和局部执行范围。


<details>
  <summary>Details</summary>
Motivation: 尽管基于编译器的控制流完整性（CFI）提供了强大的前向边保护，但在由于可见性不匹配、类型不一致以及非预期的行为故障等问题，在大型C/C++软件中的部署仍然具有挑战性。

Method: CFIghter 结合了全程序分析与引导运行时监控的方法，自动检测、分类并修复由测试套件揭示的意外CFI政策违规。该系统通过迭代地只对需要的地方应用最小必要的调整至CFI实施，直到所有测试成功或未解决的问题被认定为不可解为止。

Result: 在四个GNU项目的评估中，CFIghter能够解决所有与可见性相关的构建错误，并自动修复了大型多库util-linux代码库中95.8%的意外CFI违规行为，同时在超过89%的间接控制流位置上保持了严格的实施。对于所有研究对象而言，CFIghter能够在不需要手动修改源代码的情况下，通过自动生成的可见性调整和必要时的局部执行范围保留大部分代码库的严格基于类型的CFI。

Conclusion: 结果表明，自动化兼容性修复使得严格的编译器CFI可以在成熟的模块化C软件中实际部署。

Abstract: Compiler-based Control-Flow Integrity (CFI) offers strong forward-edge protection but remains challenging to deploy in large C/C++ software due to visibility mismatches, type inconsistencies, and unintended behavioral failures. We present CFIghter, the first fully automated system that enables strict, type-based CFI in real-world projects by detecting, classifying, and repairing unintended policy violations exposed by the test suite. CFIghter integrates whole-program analysis with guided runtime monitoring and iteratively applies the minimal necessary adjustments to CFI enforcement only where required, stopping once all tests pass or remaining failures are deemed unresolvable. We evaluate CFIghter on four GNU projects. It resolves all visibility-related build errors and automatically repairs 95.8% of unintended CFI violations in the large, multi-library util-linux codebase, while retaining strict enforcement at over 89% of indirect control-flow sites. Across all subjects, CFIghter preserves strict type-based CFI for the majority of the codebase without requiring manual source-code changes, relying only on automatically generated visibility adjustments and localized enforcement scopes where necessary. These results show that automated compatibility repair makes strict compiler CFI practically deployable in mature, modular C software.

</details>


### [9] [FasterPy: An LLM-based Code Execution Efficiency Optimization Framework](https://arxiv.org/abs/2512.22827)
*Yue Wu,Minghao Han,Ruiyin Li,Peng Liang,Amjed Tahir,Zengyang Li,Qiong Feng,Mojtaba Shahin*

Main category: cs.SE

TL;DR: 提出了一种名为FasterPy的框架，该框架利用大型语言模型（LLM）来优化Python代码的执行效率。通过结合检索增强生成（RAG）和低秩适应（LoRA），并在基于现有性能改进代码对的知识库支持下，FasterPy在PIE基准测试中多个指标上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法需要手动设计和维护特定性能缺陷的规则，这既耗时又限制了适用性；而基于机器学习或深度学习的方法虽然有潜力，但通常依赖于特定程序表示和精心制作的训练数据集，开发成本高且难以扩展。随着大型语言模型的发展，其在代码生成方面的卓越能力为自动代码优化开辟了新的途径。

Method: FasterPy框架采用了检索增强生成(RAG)技术，并由一个从现有的性能改进代码对及其相关性能度量构建的知识库支持。同时，它利用低秩适应(LoRA)技术来进一步提高代码优化的表现。

Result: 实验结果表明，在Performance Improving Code Edits (PIE)基准上，FasterPy方法在多个指标上超过了现有模型。

Conclusion: FasterPy作为一种低成本高效的框架，成功地将大型语言模型应用于Python代码的自动优化中，显示出比现有解决方案更好的性能。

Abstract: Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.

</details>


### [10] [Towards the analysis of team members well-being](https://arxiv.org/abs/2512.22845)
*Zan Xu,Sari Nurfauziyyah,Anastasia Romanova,Kaamesh G S,Yiqun Gao,Maria Spichkova*

Main category: cs.SE

TL;DR: 该论文介绍了关于团队成员幸福感分析的项目结果以及在此项目中开发的原型，强调了成员感到被赞赏和认可对其幸福感的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究软件开发团队成员的幸福感对于提高工作效率和个人健康至关重要。

Method: 通过一个项目来分析团队成员的幸福感，并开发了一个原型工具。

Result: 项目提供了对团队成员幸福感的理解，并开发出一个有助于提升团队成员幸福感的原型工具。

Conclusion: 确保团队成员感到自己的贡献得到认可是提高其幸福感的关键因素之一。

Abstract: Many recent research studies have focused on the well-being of software development team members, as this aspect may be critical not only for productivity and performance at work but also for the physical health and personal life of employees. Many studies agree that an important factor of team member well-being is whether team members feel appreciated and acknowledged for their contributions. This paper presents the results of a project on the team well-being analysis as well as the prototype developed within the project.

</details>


### [11] [Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI](https://arxiv.org/abs/2512.23033)
*Fuyad Hasan Bhoyan,Prashanta Sarker,Parsia Noor Ethila,Md. Emon Hossain,Md Kaviul Hossain,Md Humaion Kabir Mehedi*

Main category: cs.SE

TL;DR: 本研究开发了一种基于AI的诊断软件，利用混合深度学习模型MobResTaNet直接从超声图像中对十种类别（包括九种胆囊疾病类型和正常）进行分类。该系统通过可解释的人工智能(XAI)可视化提供实时预测，支持透明的临床决策，并达到了高达99.85%的准确率，同时仅有2.24M参数量。此外，该软件被部署为网页和移动应用程序，旨在提供高效、便捷且值得信赖的诊疗支持。


<details>
  <summary>Details</summary>
Motivation: 早期和准确地检测胆囊疾病非常重要，但超声图像解读具有挑战性。为了解决这一问题，研究旨在开发一种能够直接从超声图像中识别多种胆囊疾病类型的AI驱动诊断工具。

Method: 研究人员开发了一个名为MobResTaNet的混合深度学习模型，专门用于从超声图像中分类出十种不同的状态，包括九种特定的胆囊疾病类型及正常情况。该模型设计注重于保持高准确性的同时减少参数数量，从而实现快速而准确的诊断。

Result: 该AI系统在测试中展示了卓越的表现，最高达到99.85%的准确度，而且仅需使用2.24百万个参数即可完成任务。此外，通过运用XAI技术，该软件还能生成有助于理解诊断依据的可视化结果。

Conclusion: 这项工作成功地创建了一个高性能、低复杂度的AI解决方案来辅助胆囊疾病的诊断过程。其易于访问的web和移动端部署形式，使得医疗专业人员可以在任何地方获取到可靠的支持信息，促进更加精准有效的患者护理。

Abstract: Early and accurate detection of gallbladder diseases is crucial, yet ultrasound interpretation is challenging. To address this, an AI-driven diagnostic software integrates our hybrid deep learning model MobResTaNet to classify ten categories, nine gallbladder disease types and normal directly from ultrasound images. The system delivers interpretable, real-time predictions via Explainable AI (XAI) visualizations, supporting transparent clinical decision-making. It achieves up to 99.85% accuracy with only 2.24M parameters. Deployed as web and mobile applications using HTML, CSS, JavaScript, Bootstrap, and Flutter, the software provides efficient, accessible, and trustworthy diagnostic support at the point of care

</details>


### [12] [An Empirical Study of Generative AI Adoption in Software Engineering](https://arxiv.org/abs/2512.23327)
*Görkem Giray,Onur Demirörs,Marcos Kalinowski,Daniel Mendez*

Main category: cs.SE

TL;DR: 该研究旨在概述GenAI在软件工程（SE）中的采用情况，探讨其带来的好处、挑战、工具和方法的制度化以及对SE专业人士和社区的长期影响。结果显示GenAI工具广泛应用于日常SE工作中，特别是在实施、验证和个人辅助等方面，显著提高了工作效率和质量。然而，也存在输出不准确、提示工程难题等挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管GenAI工具在软件工程领域越来越受欢迎，并为多项活动提供支持，但关于其实用性、好处、挑战及其更广泛的组织和社会影响方面仍缺乏实证证据。

Method: 本研究通过调查的方法收集数据，以了解GenAI工具在软件工程中的应用状况、相关的好处与挑战、以及这些工具和技术是如何被制度化的。

Result: 发现GenAI工具已经被广泛集成到日常的软件工程工作中，尤其是在实现、验证与确认、个人帮助及维护相关任务中发挥了重要作用。实践者报告了显著的好处，如周期时间减少、质量提升和支持知识工作等。同时，也面临着诸如结果不正确或不可靠、提示工程难度大等问题。工具和技术的制度化普遍存在，但在培训和治理方面的重视程度较低。

Conclusion: 虽然GenAI为软件工程师带来了效率和生产力上的提升，但同时也引入了一系列新的挑战。从业者预计GenAI将重新定义而不是取代他们的角色，对于就业市场收缩和技术需求变化表示了一定程度的关注。

Abstract: Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.

</details>


### [13] [Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?](https://arxiv.org/abs/2512.23385)
*The Anh Nguyen,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: 本研究通过分析Hugging Face和GitHub上的开发者报告的问题和解决方案，对AI供应链中的安全问题进行了实证调查。采用关键词匹配与优化的distilBERT分类器相结合的方法，构建了一个包含312,868条安全讨论的数据集，并基于此数据集归纳出32种安全问题及24种解决办法，涵盖系统软件、外部工具生态系统、模型以及数据四个主题领域。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）模型和应用的快速增长，安全环境变得越来越复杂。AI项目的开发者不仅要面对传统的软件供应链问题，还要应对新型的、特定于AI的安全威胁。然而，对于实践中常见的安全问题及其解决方法知之甚少。这一差距阻碍了为AI供应链每个组件开发有效安全措施的进程。

Method: 研究者们通过对Hugging Face和GitHub上开发者所报告的问题及解决方案进行经验性调查来填补上述空白。为此，他们开发了一套结合了关键词匹配与经过最佳微调的distilBERT分类器的工作流程，用于识别安全相关讨论。该工作流程生成了一个包含312,868次安全讨论的数据集。

Result: 从数据集中抽取了753篇帖子进行主题分析后，研究人员揭示了一个细粒度的分类法，包括32种安全问题和24种解决方案，这些内容被归类到四个主要类别：(1) 系统与软件；(2) 外部工具与生态系统；(3) 模型；(4) 数据。许多安全问题源于AI组件的复杂依赖性和黑盒性质。值得注意的是，与模型和数据相关的挑战往往缺乏具体的解决方案。

Conclusion: 本研究提供的洞见可以为开发者和研究人员提供基于证据的指导，以应对贯穿整个AI供应链的真实世界安全威胁。

Abstract: The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.

</details>


### [14] [An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes](https://arxiv.org/abs/2512.23415)
*Vinoth Punniyamoorthy,Bikesh Kumar,Sumit Saha,Lokesh Butra,Mayilsamy Palanigounder,Akash Kumar Agarwal,Kabilan Kannan*

Main category: cs.SE

TL;DR: 本文探讨了如何利用AIOps原则改进Kubernetes的自动扩展机制，提出了一种新的多信号自动扩展框架，该框架结合了SLO感知和成本意识控制与轻量级需求预测。实验结果表明，与默认及调优后的Kubernetes自动扩展基线相比，所提方法能够减少最多31%的SLO违规持续时间、提高24%的扩展响应时间以及降低18%的基础架构成本，同时保持稳定且可审计的控制行为。


<details>
  <summary>Details</summary>
Motivation: 生产环境中由于反应性扩展行为、有限的应用程序级别信号使用以及不透明的控制逻辑，导致服务级别目标（SLO）违规和成本效率低下问题频发。

Method: 提出了一个基于差距分析现有自动扩展方法的研究，并设计了一个安全且可解释的多信号自动扩展框架，该框架整合了SLO意识和成本敏感控制与轻量级需求预测技术。

Result: 实验评估表明，对于代表性的微服务和事件驱动工作负载，所提议的方法相较于Kubernetes默认设置及经过调整的自动扩展基准，在减少SLO违规时长、提升扩展响应速度以及降低基础设施开支方面均表现出显著优势。

Conclusion: AIOps驱动的以SLO为中心的自动扩展可以极大地提高基于Kubernetes云平台的可靠性、效率和操作可信度。

Abstract: Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.

</details>


### [15] [Embedding Quality Assurance in project-based learning](https://arxiv.org/abs/2512.23488)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 本文基于作者在软件工程课程中超过十年教授软件质量方面的经验，特别是在敏捷/Scrum环境下的教学经验，分享了他们的心得体会，并提出了关于如何在基于项目的敏捷/Scrum学习环境中嵌入质量保证话题的若干建议。


<details>
  <summary>Details</summary>
Motivation: 作者希望分享他们在软件工程（SE）课程中讲授软件质量方面特别是敏捷/Scrum设置下的多年教学经验。

Method: 通过回顾和总结多年来在最终年软件开发项目及SE项目管理课程中的实践经验。

Result: 总结出了一系列关于如何在采用敏捷/Scrum方法的基于项目的学习中更好地融入质量保证主题的教学建议。

Conclusion: 根据多年的教学经验，在敏捷/Scrum环境下进行软件工程项目时，将质量保证作为核心内容之一是可行且有效的；同时提供了具体的实施建议。

Abstract: In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.

</details>


### [16] [Adaptable Teastore with Energy Consumption Awareness: A Case Study](https://arxiv.org/abs/2512.23498)
*Henrique De Medeiros,Denisse Muñante,Sophie Chabridon,César Perdigão Batista,Denis Conan*

Main category: cs.SE

TL;DR: 本文提出了一种名为EnCoMSAS的工具，用于监测自适应软件系统在运行时的能量消耗情况。通过使用Adaptable TeaStore作为案例研究，实验结果表明EnCoMSAS能够有效地收集软件应用的能量消耗数据，并且其对整个自适应系统生态能量消耗的影响相对较小。


<details>
  <summary>Details</summary>
Motivation: 随着全球能源消费的增长，数据中心成为主要贡献者之一，尤其是应用程序向云端迁移的趋势加剧了这一现象。尽管已经有一些努力将能效作为软件应用动态调整的主要目标，但在如何为这些运行时动态调整的自适应软件系统配备有效的能量消耗监控工具方面仍存在差距，同时这种监控工具对整个自适应系统生态环境总体能量消耗的影响程度尚未得到充分探索。

Method: 为了解决上述问题，研究人员开发了EnCoMSAS（Energy Consumption Monitoring for Self-Adaptive Systems），一种可以收集分布式软件应用（例如部署于云上的应用）所消耗能量的工具。为了评估EnCoMSAS的有效性及其对自适应系统生态环境总能量消耗的影响，采用了一个名为Adaptable TeaStore的案例研究进行实证研究。首先使Adaptable TeaStore具备EnCoMSAS功能，然后在不同工作负载条件下执行该应用模拟用户交互，最后利用EnCoMSAS来收集并评估Adaptable TeaStore推荐服务算法的能量消耗情况。

Result: 结果显示，EnCoMSAS能够在运行时有效收集软件应用的能量消耗信息以支持动态调整。观察到CPU使用率与EnCoMSAS收集到的能量消耗之间存在相关性，这证明了所收集能量测量值的有效性。此外还发现，除了算法复杂度外，部署环境特性也会影响能量消耗。最后，关于EnCoMSAS对于整个SAS生态系统整体能量消耗影响的研究显示，相对于TeaStore应用微服务集合而言，这种影响是比较小的。

Conclusion: EnCoMSAS被证明是监测自适应软件系统能量消耗的有效工具，它有助于实现更加节能的应用程序设计和运行时动态调整策略。此外，本研究表明，在考虑软件应用的能耗时，不仅需要关注算法本身的效率，还需考虑实际部署环境带来的影响。

Abstract: [Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.
  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.
  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.
  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.

</details>


### [17] [Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving](https://arxiv.org/abs/2512.23511)
*Xinyi Zheng,Ningke Li,Xiaokun Luan,Kailong Wang,Ling Shi,Meng Sun,Haoyu Wang*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.
  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.

</details>


### [18] [Model-based Development for Autonomous Driving Software Considering Parallelization](https://arxiv.org/abs/2512.23575)
*Kenshin Obi,Takumi Onozawa,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种使用基于模型的开发（MBD）过程来并行化自动驾驶软件的方法，该方法扩展了现有的基于模型的并行器（MBP）方法，从而减少了执行时间，并且特别适合于实现自动驾驶软件的实时性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车作为解决各种社会问题的方案之一而受到关注，其软件需要实现实时性能以应对各种功能和复杂环境。因此，有必要找到一种有效的方法来提高软件处理速度。

Method: 本文提出的方法是利用基于模型的开发（MBD）流程来对自动驾驶软件进行并行化处理，并扩展了现有的基于模型的并行器（MBP）方法来简化复杂处理的实现。

Result: 通过实施所提出的方法，成功地减少了执行时间。评估结果表明，这种方法对于开发具有实时性能需求的自动驾驶软件是非常合适的。

Conclusion: 提出的并行化方法被证明能够有效地减少自动驾驶软件的执行时间，有助于其实现实时性能，为自动驾驶技术的发展提供了新的途径。

Abstract: In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.

</details>


### [19] [Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes](https://arxiv.org/abs/2512.23605)
*Kenshin Obi,Ryo Yoshinaka,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型开发（MBD）的框架，专门用于解决ROS 2在多输入场景下的并行化问题。通过将兼容ROS 2的Simulink模型分类为事件驱动型和定时器驱动型来实现针对性的并行处理。实验结果表明该方法能够有效减少执行时间。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统复杂性和规模的增长，特别是在自动驾驶领域，软件与硬件解决方案如ROS 2和多核处理器被广泛应用。传统手动程序并行化面临保持数据完整性和避免死锁等并发问题挑战；同时，基于模型开发虽然能自动化这一过程，但在集成现代框架如ROS 2于多输入情境时遇到困难。

Method: 提出了一种新的基于模型开发(MBD)框架，该框架将兼容ROS 2的Simulink模型分为事件驱动型和定时器驱动型两大类，以促进更有效的并行化处理。

Result: 评估结果显示，在应用了所提出的框架后，并行化所有模式都显示出了执行时间上的减少，证明了该方法的有效性。

Conclusion: 新提出的MBD框架不仅解决了ROS 2在多输入情况下的并行化难题，还通过扩展传统MBD并行化方式支持了基于ROS 2模型的多输入并行代码生成，从而提高了效率。

Abstract: In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: 本文介绍了一个名为GPU-Virt-Bench的基准测试框架，用于评估不同GPU虚拟化系统在多租户环境下的性能，包括软件虚拟化方案和NVIDIA MIG技术。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速工作负载（尤其是在人工智能和大型语言模型推理中）的需求激增，云和容器环境中对高效GPU资源共享提出了前所未有的需求。尽管NVIDIA的MIG技术提供了硬件级隔离，但其仅限于高端数据中心GPU使用。此外，现有的软件虚拟化解决方案缺乏标准化的评估方法。

Method: 研究者们提出了一种全面的基准测试框架——GPU-Virt-Bench，该框架通过56个性能指标来评估GPU虚拟化系统，这些指标被组织成10个类别。

Result: GPU-Virt-Bench能够系统地比较软件虚拟化方法与理想的MIG行为之间的差异，并为在多租户环境下部署GPU资源的专业人士提供实用见解。通过对HAMi-core、BUD-FCSP以及模拟的MIG基线进行评估，展示了此框架对于生产部署决策的重要性。

Conclusion: 本文提出的GPU-Virt-Bench框架有助于更深入理解各种GPU虚拟化技术的性能特点，支持做出更加明智的技术选型决定。

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [21] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: 提出了一种名为HybridFlow的资源自适应推理框架，通过任务分解和平行执行以及基于资源意识的任务路由，在减少推理时间和令牌使用的同时保持了竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘-云协作方法通常采用粗粒度的任务分配策略，导致计算冗余和资源利用效率低下。为了克服这一问题，并提高大型语言模型在资源受限边缘设备上的实时部署能力，提出了HybridFlow。

Method: HybridFlow分为两个阶段运作：1）任务分解和平行执行，动态地将复杂查询分割成相互依赖的子任务，这些子任务一旦其依赖项被解决即可开始执行；2）基于资源意识的子任务路由，通过学习得到的路由器根据预测的效用增益和实时预算状态自适应地将每个子任务分配给边缘或云端模型。

Result: 通过GPQA、MMLU-Pro、AIME和LiveBench-Reasoning等综合评估显示，HybridFlow能够有效减少端到端推理时间及总体令牌使用量，同时保持了较高的准确率。

Conclusion: HybridFlow提供了一个有效的解决方案来提升大型语言模型在边缘设备上的性能表现，通过优化任务分配策略显著减少了推理延迟与资源消耗，为实现实时应用提供了可能。

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [22] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: 提出了一种名为Cleave的新范式，通过选择性混合张量并行方法精细地划分训练操作，并结合以参数服务器为中心的训练框架，解决了边缘设备内存限制和通信瓶颈问题，从而实现了与云相当的大模型高效训练。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型开发的生态系统高度集中且仅限于大规模云数据中心运营商：训练基础模型成本高昂，需要巨大的计算资源。跨边缘设备的去中心化基础模型训练，利用其空闲计算能力，提供了一种民主化的替代方案。然而，现有的边缘训练方法存在性能不足、扩展性差、超出设备内存容量以及通信开销过大的问题。此外，它们还不能很好地处理设备异构性和动态变化。

Method: Cleave，一种新的范式，通过新颖的选择性混合张量并行方法精细地划分训练操作。该方法结合了以参数服务器为中心的训练框架，能够应对设备内存限制并避免通信瓶颈，使得在边缘设备上进行大规模模型训练成为可能。同时，采用成本优化模型来指导设备选择和训练工作负载分布，有效考虑了设备异构性和波动性。

Result: 评估显示，Cleave能够有效地将训练规模扩大到更大的模型和数千台设备上，支持的设备数量比基线边缘训练方法多出8倍。相较于最先进的边缘训练方法，在每批训练时间上提高了最多10倍，并且能更高效地处理设备故障，恢复速度至少比先前的方法快100倍。

Conclusion: Cleave为实现大规模模型在边缘设备上的高效训练提供了一个有效的解决方案，不仅克服了现有边缘训练方法所面临的挑战，还在性能、可扩展性和容错性方面取得了显著进步。

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [23] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: 本文提出了一种端到端的大语言模型框架，该框架通过性能反馈来优化GPU内核而无需构建整个应用程序。它能自动完成代码至最小可执行程序，并在应用外部进行多轮迭代优化与评估。该方法集成了自动错误修复和性能模式继承功能，以保证正确性、重用有效策略并减少搜索成本。实验结果表明，该方法在不同平台上实现了显著的加速比，且具有跨平台可移植性和低成本的优点。


<details>
  <summary>Details</summary>
Motivation: 在高性能计算中，热点GPU内核是主要瓶颈，而专家手动调优既昂贵又难以移植。大语言模型方法通常假设可以廉价地编译和执行内核，但在需要高昂成本才能完成完整构建和运行的大规模应用中并不适用。因此，作者提出了一个能够在不构建整个应用程序的情况下优化这些内核的新方法。

Method: 研究者开发了一个带有性能反馈机制的端到端大语言模型框架，用于从独立提取出来的热点内核开始，自动将代码补全为最小可执行程序（MEP），然后在应用程序外进行多轮迭代优化与评估。此外，该框架还引入了自动错误修复和性能模式继承技术，旨在修复错误的同时保持正确性、重复利用有效的平铺/内存/同步策略以及降低搜索成本。

Result: 实验基于NVIDIA GPU和采用AMD授权架构的Haiguang DCU平台进行，使用了PolyBench、AMD APP SDK及来自大规模超级计算应用中的热点内核作为测试对象。结果显示，该方法在不同测试集上分别达到了平均5.05倍（NVIDIA上的PolyBench）、7.77倍（DCU上的PolyBench）、1.77倍（AMD APP SDK）以及在三个特定热点内核上1.25倍的速度提升，超越了直接使用大语言模型进行优化的效果。

Conclusion: 所提出的方法不需要完整的源码依赖，支持跨平台移植，并且能够实现实际可行且低成本的GPU内核优化。

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [24] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: 本文提出了一种自适应GPU资源分配框架，旨在解决多智能体系统在无服务器GPU平台上部署时面临的资源分配挑战。该框架能够基于工作负载特性、智能体优先级和最小资源需求动态分配GPU资源，从而实现85%的延迟降低，并且在吞吐量上与静态分配相当。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统通过协作智能解决了复杂的推理任务，但将其高效部署于无服务器GPU平台面临显著的资源分配难题，包括异构智能体工作负载、变化的计算需求以及成本效益扩展的需求。

Method: 采用复杂度为O(N)的算法进行实时调整，依据工作负载特点、智能体优先级及最低资源要求动态分配GPU资源，以应对轻量级协调者与重量级专家之间异构计算需求、毫秒级重分配的工作负载波动以及无服务器环境下的容量限制等关键挑战。

Result: 通过模拟四个异构智能体的真实多智能体工作流程表明，与静态平均和循环调度策略相比，自适应分配方法在延迟、成本和GPU利用率指标上表现更优。

Conclusion: 该自适应GPU资源分配框架为在无服务器GPU基础设施上部署具有成本效益的多智能体AI系统提供了一个实用解决方案。

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [25] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: AiiDAlab平台通过直观的用户界面简化了复杂的计算工作流程，使科学家能够专注于研究而不是计算细节，并确保了可重复性。该平台已经从材料科学扩展到量子化学、大气建模、电池研究和大规模设施的实验数据分析等领域，并且正在与电子实验室笔记本（ELN）集成以支持开放研究数据（ORD）的生成。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的不断提高，需要强大的自动化研究工作流来执行和协调超级计算机上的大量相互依赖的模拟。然而，这些工作流的执行通常需要在设置计算输入、解释输出以及处理远程机器上并行代码执行的复杂性方面具备技术专长。为了解决这些问题，开发了AiiDAlab平台，旨在让研究人员能通过一个直观的网页界面访问复杂的计算工作流，从而减少对计算细节的关注。

Method: AiiDAlab提供了一个基于Web的用户友好界面，用于创建和管理跨多个学科的计算工作流。它利用底层AiiDA引擎自动跟踪整个仿真过程，确保结果的可再现性。此外，AiiDAlab还根据用户反馈不断优化新用户的引导过程，简化计算资源的接入途径，并提供了处理大数据集的强大机制。最近，AiiDAlab开始与电子实验室笔记本（ELN）整合，以加强遵循FAIR原则的支持，帮助数据驱动型科研人员轻松生成可重复使用的开放研究数据。

Result: AiiDAlab不仅被广泛应用于材料科学领域，也逐渐成为了量子化学、大气建模、电池研究乃至大型科研设施中的实验数据分析等多个领域的强大工具。同时，在教育场景中也有活跃的应用。

Conclusion: AiiDAlab作为一个促进科学研究发现的多功能平台，成功地降低了科学家们面对复杂计算任务时的技术门槛，让他们能够更专注于核心研究问题。通过持续改进用户体验和功能拓展，AiiDAlab正日益成为连接不同学科领域内科研活动的关键桥梁。

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [26] [BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs](https://arxiv.org/abs/2512.22174)
*Muhammad Zeeshan Karamat,Sadman Saif,Christiana Chamon Garcia*

Main category: cs.DC

TL;DR: 本研究提出了一种名为BitFlipScope的软件框架，旨在识别由于硬件老化、宇宙辐射或恶意攻击导致的位翻转故障所影响的转换器架构内的区域。该框架在有无干净参考模型两种情况下都能有效定位故障，并支持轻量级性能恢复，为在易受硬件故障和对抗性环境中的大型语言模型（LLMs）提供可靠的部署方案。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地应用于实际和安全关键场景中，它们变得越来越容易受到由硬件退化、宇宙辐射或诸如Rowhammer之类的故意故障注入攻击引起的位翻转故障的影响。这些故障会无声地破坏内部参数，可能导致不可预测甚至危险的模型行为。为了能够诊断出劣化的原因、采取针对性的纠正措施或是在不依赖于昂贵的微调或完全重新训练的情况下恢复模型功能，定位这些损坏至关重要。

Method: 本文介绍了BitFlipScope，这是一种可扩展的基于软件的框架，用于在两种部署场景下识别变压器架构内受故障影响的区域。当存在一个干净的参考模型时，BitFlipScope通过输出、隐藏状态以及内部激活的差异分析来检测指示腐败的异常行为，从而确定或定位故障。当没有参考模型时，则利用残差路径扰动和损失敏感度分析直接从被污染的模型推断出受故障影响的区域。

Result: 无论在哪种设置下，该框架不仅能够实现有效的故障诊断，而且还支持无需微调的轻量级性能恢复，为恢复被污染的模型提供了一个实用的途径。

Conclusion: BitFlipScope作为一个重要的进步，朝向在硬件易损及对抗环境中部署值得信赖且具有容错能力的大规模语言模型迈出了重要一步。

Abstract: Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.

</details>


### [27] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: 该论文提出了一种名为MatKV的方案，旨在通过预计算和存储检索增强生成(RAG)对象的关键-值向量来提高RAG推理效率。实验表明，与GPU上的完全KV计算相比，MatKV能将推理时间和能耗减少一半，并且对问答任务的准确性影响不大。此外，MatKV还支持额外优化，如同时加载已物化的KVs以减少延迟，以及利用低端GPU进行解码而不显著牺牲速度。


<details>
  <summary>Details</summary>
Motivation: 观察到基于大型语言模型(LLM)的生成式AI两个主要趋势：推理成本及能耗超过训练；检索增强生成(RAG)变得普遍。在处理长输入时，RAG中的预填充阶段非常耗能且耗时。因此，提高RAG推理中预填充阶段的效率至关重要。

Method: 提出了MatKV方案，它预计算RAG对象（如文档）的关键-值向量(KVs)，并将它们物质化到低成本但快速、节能的闪存存储中，在推理时重用这些KVs而不是使用昂贵且耗能的GPU重新计算。

Result: 实验结果证实，与GPU上完全KV计算相比，MatKV能够将RAG工作负载的推理时间和功耗减少一半，而不会严重影响问答任务的准确性。此外，MatKV还允许通过两种方式实现进一步优化：一是GPU可以在加载下一个实例的已物化KVs的同时解码文本，从而减少加载延迟；二是由于解码速度对GPU性能的敏感度低于KV计算，一旦已物化KVs被加载到GPU内存中，就可以利用低端GPU进行解码而不严重牺牲速度。

Conclusion: MatKV方案展示了其在降低成本、提高能效方面的作用，使大规模生成式AI应用更加经济高效、节能，并且适用于更广泛的任务和硬件环境。

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [28] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: This paper introduces SPUMA, a GPU-accelerated version of OPENFOAM, demonstrating improved performance and energy efficiency on pre-exascale clusters with AMD and NVIDIA GPUs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of programmability in high-performance computing (HPC) for computational fluid dynamics (CFD) on hybrid clusters, particularly with the use of GPUs in open-source software, the authors developed SPUMA, a fully GPU-ported version of the OPENFOAM CFD package, to improve performance and energy efficiency.

Method: The researchers implemented SPUMA, a full GPU version of OPENFOAM, using a portable programming model and a memory pool manager that takes advantage of the unified memory in modern GPUs. They conducted numerical tests on two pre-exascale clusters, LUMI and Leonardo, which are equipped with AMD MI250X and NVIDIA A100 GPUs, respectively. The tests included an analysis of memory usage, kernel execution time, the effect of the memory pool, and energy consumption.

Result: SPUMA showed strong scalability up to 65% efficiency at 8 million cells per GPU on both LUMI and Leonardo. Weak scalability was 75-85% efficient on 20 GPUs. When using the NVIDIA AmgX solver, efficiency remained above 90%. The energy consumption was reduced by up to 82% compared to CPU-based simulations. It was also found that one A100 GPU on Leonardo could match the performance of 200-300 Intel Sapphire Rapids cores under certain conditions.

Conclusion: The SPUMA, a full GPU porting of OPENFOAM, demonstrates strong and weak scalability on pre-exascale clusters with AMD and NVIDIA GPUs. The use of the memory pool manager and unified memory feature improves performance and energy efficiency, especially when using the NVIDIA AmgX linear algebra solver. Energy consumption is significantly reduced compared to CPU simulations.

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [29] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: 本文提出了一种可扩展的云原生架构，用于智能处理PMU数据，该架构结合了人工智能与边缘及云计算，以应对传统集中式处理架构在面对大规模PMU部署时遇到的延迟、可扩展性和可靠性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着PMU部署规模的增长，传统集中式处理架构难以有效处理PMU数据的大容量和高速度，特别是在具有动态运行条件的现代电网中。为了解决这些问题，需要开发一种新的架构来支持低延迟摄入、实时异常检测以及高级分析。

Method: 所提出的框架利用分布式流处理、容器化微服务和弹性资源编排技术，并整合了用于时间序列分析的机器学习模型，以提高电网可观测性和预测能力。同时，还开发了分析模型来评估系统的延迟、吞吐量和可靠性。

Result: 研究表明，该架构能够在扩展到大规模PMU部署的同时实现亚秒级响应时间。此外，通过嵌入安全和隐私机制，确保其适用于关键基础设施环境。

Conclusion: 本研究提供了一个强大且灵活的基础，旨在促进下一代智能电网分析的发展。

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [30] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: Pick and Spin是一个基于Kubernetes的框架，旨在简化大型语言模型（LLM）的自托管过程。它通过统一的Helm部署系统、自适应缩放自动化以及混合路由模块来提高GPU利用率、降低成本并确保可靠性。实验结果显示，与静态部署相比，Pick and Spin在成功率、延迟降低和每查询GPU成本方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着组织对于隐私保护、成本控制及定制化需求的增长，自行托管大型语言模型变得越来越有吸引力。然而，在实际操作中面临着GPU使用效率低、工作负载分配不合理以及稳定性差等问题。为了解决这些挑战，本文提出了一套名为'Pick and Spin'的新框架。

Method: Pick and Spin架构基于Kubernetes构建，包含了基于Helm的统一部署解决方案、能够根据需要自动调整至零规模以节省资源的机制，以及一个结合了关键词启发式方法与轻量级DistilBERT分类器的混合路由组件。该框架被设计用于智能地平衡成本、延迟与准确性之间的关系。

Result: 通过对Llama-3 (90B)、Gemma-3 (27B)、Qwen-3 (235B) 和 DeepSeek-R1 (685B) 四种不同规模的语言模型进行测试，并在八个公开基准数据集上采用五种推理策略及两种路由变体进行了共计31,019条提示词处理与163,720次推理运行后发现：Pick and Spin相较于传统静态部署方案，在提升成功率达到21.6%的同时，还能将延迟减少30%，并且每查询的GPU成本也降低了33%。

Conclusion: Pick and Spin框架展示了其在提高大规模语言模型自托管部署效率方面的潜力，不仅有效解决了GPU利用率低下、工作负载分配不均等现实问题，同时也显著提升了系统的整体性能与经济性。

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [31] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: 提出了一种名为Nightjar的学习型算法，用于自适应推测性推理，该算法可以根据请求负载动态选择最佳推测长度，甚至在无益时禁用推测解码。实验表明，与标准推测解码相比，Nightjar可实现高达14.8%的吞吐量提升和20.2%的延迟降低，对于实时服务具有稳健的效率。


<details>
  <summary>Details</summary>
Motivation: 当前投机解码（SD）方法在低负载、内存受限系统中提高了吞吐量，但在高负载、计算受限环境中由于验证开销而性能下降。现有SD实现使用固定的投机长度，不能适应动态请求速率，在实际服务场景中造成了显著的性能瓶颈。

Method: 提出了Nightjar，这是一种新的基于学习的算法，用于自适应投机性推理，它能够根据不同的批处理大小调整到请求负载，并且当投机解码没有带来好处时可以禁用它。

Result: 实验结果表明，与标准投机解码相比，Nightjar实现了高达14.8%的吞吐量增加以及20.2%的延迟减少。

Conclusion: Nightjar为实时服务提供了更加高效和灵活的投机解码方案，通过动态调整推测长度来优化不同条件下的性能。

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [32] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: 本文提出了一种名为RobustRL的新系统，旨在通过角色故障隔离和恢复策略来提高大规模GPU集群上强化学习后训练过程中的容错能力和效率。相比现有的ByteRobust方法，在相同的失败注入频率下，RobustRL能够实现超过80%的有效训练时间比，并且端到端训练时间加快了8.4%-17.4%。


<details>
  <summary>Details</summary>
Motivation: 针对现有LLMs（大型语言模型）的容错框架要么专注于训练要么是推理，未能充分探索异步执行中优化潜力的问题，特别是在RL（强化学习）后训练过程中同时存在训练与推理工作负载的情况下。研究指出，当前的方法在遇到故障时需要重启整个RL任务，导致了不必要的开销。因此，作者提出了一个基于角色的故障隔离方法，以减少单个机器故障对整个系统的影响。

Method: RobustRL系统采用了三个核心步骤：(1) 检测 - 实现了角色感知监控，用于区分实际故障与角色特定行为，避免误报及延迟检测；(2) 重启 - 对于训练器，采用非中断恢复方式，其中rollouts保持状态并继续轨迹生成，而训练器则通过rollout热备快速恢复；对于rollout，则进行独立的机器替换而不中断RL任务；(3) 重新连接 - 使用动态、基于UCX（统一通信X）点对点通信代替静态集体通信，使得恢复的角色间能够立即同步权重。

Result: 在一个拥有256个GPU的集群上，使用Qwen3-8B-Math工作负载并且在10%的故障注入频率下进行测试，RobustRL达到了超过80%的有效训练时间比率，相较于ByteRobust的60%，并且在端到端训练时间上快了8.4%-17.4%。

Conclusion: RobustRL作为首个全面处理GPU机器错误的鲁棒系统，为RL后训练提供了显著的性能提升。它通过有效管理不同角色间的故障隔离与恢复流程，减少了由单个组件故障引起的整体停机时间，从而提高了系统的稳定性和训练效率。

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [33] [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)
*Wei Gao,Yuheng Zhao,Tianyuan Wu,Shaopan Xiong,Weixun Wang,Dakai An,Lunxi Cao,Dilxat Muhtar,Zichen Liu,Haizhou Zhao,Ju Huang,Siran Yang,Yongbin Li,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.
  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\(\times\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.

</details>


### [34] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [35] [OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads](https://arxiv.org/abs/2512.22743)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: OptiNIC, a domain-specific RDMA transport, improves the performance of distributed machine learning workloads by adopting a best-effort, out-of-order data delivery model that eliminates retransmissions and strict in-order requirements, leading to significant improvements in time-to-accuracy, throughput, and fault resilience.


<details>
  <summary>Details</summary>
Motivation: 随着分布式机器学习工作负载扩展到数千个通过高速互连连接的GPU，集合通信中的尾部延迟已成为主要瓶颈。现有的RDMA传输（如RoCE、IRN、SRNIC和Falcon）依赖于重传来确保数据的可靠性和顺序交付，但这对于机器学习来说引入了不必要的复杂性和延迟，因为即便是罕见的数据包延迟也可能导致整个模型流水线停滞。

Method: 提出了OptiNIC，这是一种特定领域的RDMA传输，它基于机器学习对部分或缺失数据的容忍度重新审视了传统的可靠性保证。OptiNIC从网卡中消除了重传和顺序交付的需求，实现了尽力而为的非顺序传输模式。与传统RDMA只有在数据完全送达后才发出完成信号不同，OptiNIC引入了自适应超时机制来触发前进，即使在数据可能丢失或延迟的情况下也是如此。同时，OptiNIC保留了标准的拥塞控制机制，并将损失恢复转移到了ML管道本身，例如通过Hadamard变换和纠删码实现。

Result: 评估表明，在两个公有云（即Hyperstack和CloudLab）上，OptiNIC分别将训练和推理的时间准确性提高了2倍和吞吐量提高了1.6倍。此外，OptiNIC还将99百分位延迟降低了3.5倍，减少了BRAM使用量至原来的2.7分之一，并几乎使网卡对故障的恢复能力翻了一番，从而为分布式机器学习工作负载提供了一个具有弹性的、针对尾部优化的RDMA传输方案。

Conclusion: OptiNIC通过采用一种新的传输方法成功地解决了分布式机器学习环境中面临的挑战，该方法允许以更高效的方式处理数据丢失和延迟问题，从而显著改善了性能指标如时间准确性、吞吐量以及系统对故障的抵抗力。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.
  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).
  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.

</details>


### [36] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: 本文提出了一种名为Argus的框架，这是首个考虑输出令牌长度和设备多样性的分布式边缘-云LLM推理框架。它通过LAS模块预测令牌长度，并通过LOO模块优化任务卸载，以实现长期体验质量的优化。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案大多忽略了动态、随机和异构环境的本质，通常忽视了可变输出令牌长度和设备多样性的影响。

Method: Argus框架包括一个长度感知语义（LAS）模块，用于使用经过微调的语言模型预测输入提示的输出令牌长度；以及Lyapunov引导的卸载优化（LOO）模块，该模块将长期质量体验优化公式化，明确考虑了LLM预填充和解码成本。此外，还引入了一种新的带有阻尼和拥塞控制的迭代卸载算法（IODCC），来有效解决在时变约束下的整数非线性规划问题。

Result: 广泛的理论和实证评估表明，Argus在高度动态和异构的环境中实现了稳健的表现和优越的效率。

Conclusion: Argus提供了一种有效的解决方案，能够适应不断变化的条件，同时优化了在边缘-云系统中部署大型语言模型的成本与性能。

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


### [37] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: 本文讨论了如何通过软件架构方法、云原生Operator模式和传统编程语言技术来实现TeaStore应用中自适应控制逻辑的解耦，并分析了这些不同方法在细粒度表达式适应与系统级控制之间的权衡，提出可以将这些方法结合成一个多层架构以支持自适应微服务。


<details>
  <summary>Details</summary>
Motivation: 作者认为Adaptable TeaStore规范的实现应该考虑到自适应的关键属性：系统一致性（跨副本协调适应）、规划（执行适应直到满足适当条件）以及模块化（干净地集成适应逻辑）。文章旨在探讨如何利用多种技术手段来解耦自适应控制逻辑，同时保持这些关键属性。

Method: 本文采用了一种实施讨论的方式，考察了软件架构方法、云原生Operator模式以及传统的编程语言技术等手段如何能够帮助实现TeaStore应用中的自适应控制逻辑与核心业务逻辑之间的分离。

Result: 研究表明，不同的方法在提供细粒度适应能力和实现全系统控制方面存在各自的优缺点；并且，这些方法并非相互排斥而是可以被组合起来形成一个多层次架构，从而更有效地重用适应策略。

Conclusion: 为了构建支持自适应特性的微服务架构，可以考虑综合运用软件架构设计、云原生技术和编程技巧等多种方式。通过合理搭配使用这些方法，能够在保持系统整体一致性和可管理性的同时，提高自适应逻辑的灵活性和复用性。

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>


### [38] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: 本文提出了一种新的哈希方法——局部会合哈希（LRH），它在保持环状结构的同时，通过限制最高随机权重选择至C个相邻物理节点的本地缓存窗口内，实现了负载均衡。与多探针一致性哈希相比，LRH不仅减少了最大与平均负载比，还显著提高了处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性哈希方案要么需要大量的虚拟节点来减少峰值与平均负载比率，要么采用多探针方法牺牲内存访问效率以换取更好的负载平衡。本文旨在提出一种既能保持高效负载分布又能减少内存访问开销的新方法。

Method: 提出了局部会合哈希（LRH）技术，该技术通过预先计算不同的下一个偏移量，在一个由C个不同邻近物理节点组成的本地缓存窗口中精确地枚举候选者，并选择最高随机权重的胜者。此外，LRH能够通过固定候选过滤器仅重新映射那些原胜者不可用的键，从而避免了额外的变动。

Result: 实验结果表明，在N=5000, V=256 (|R|=1.28M), K=50M和C=8的情况下，LRH将最大/平均负载从1.2785降低到1.0947，并达到了约60.05 Mkeys/s的速度，比使用8次探测的多探针一致性哈希快约6.8倍(后者为8.80 Mkeys/s)，同时接近其负载平衡效果(Max/Avg 1.0697)。

Conclusion: 局部会合哈希（LRH）提供了一种有效的方法来提高分布式系统中的一致性哈希性能，同时维持良好的负载均衡。相比于传统的多探针一致性哈希，LRH显示出更高的查询效率和更低的最大/平均负载比。

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [39] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: 本文探讨了如何在DevOps生命周期的发布阶段，将现有的离线性能优化框架应用于微服务应用的CPU和内存资源分配配置参数优化。通过TeaStore微服务应用程序评估该框架，并统计比较不同优化算法，为决策提供支持。结果显示，预先筛选因子有助于在有限采样预算下找到最优资源配置；而当目标是近似最优配置时，不进行筛选直接运行贝叶斯优化更为合适。


<details>
  <summary>Details</summary>
Motivation: 微服务应用在发布阶段的资源分配配置参数（如CPU和内存）优化尚未得到充分研究，大多数研究集中在运维阶段的智能调度与自动扩展上。然而，基于CPU使用情况的容器水平自动扩展可能会导致内存分配不当的问题，除非在部署前对两种资源进行了细致调整。

Method: 采用TeaStore微服务应用来评估性能优化框架，并且从统计学角度对比了几种不同的优化算法，以帮助理解它们之间关于采样成本与达到最优资源配置距离之间的权衡。

Result: 研究表明，在寻找具有可接受采样预算的最佳资源配置时，提前进行因素筛选是有益的；如果目的是统计地比较不同算法，则必须实施筛选以使收集所有数据点变得可行；若寻求接近最优的配置，则建议直接执行无筛选条件下的贝叶斯优化。

Conclusion: 本文提出的方法能够有效指导微服务应用在发布阶段进行资源分配配置参数的优化选择，同时指出根据不同目标选择适当策略的重要性。

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [40] [OrchANN: A Unified I/O Orchestration Framework for Skewed Out-of-Core Vector Search](https://arxiv.org/abs/2512.22838)
*Chengying Huan,Lizheng Chen,Zhengyi Yang,Shaonan Ma,Rong Gu,Renjie Yao,Zhibin Wang,Mingxing Zhang,Fang Xi,Jie Tao,Gang Zhang,Guihai Chen,Chen Tian*

Main category: cs.DB

TL;DR: OrchANN, an out-of-core ANNS engine, optimizes I/O performance for billion-scale datasets by using a tailored I/O orchestration model. It selects cluster-specific local indexes, maintains a dynamic in-memory navigation graph, and applies multi-level pruning to reduce SSD accesses, thereby significantly improving QPS and latency over existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing out-of-core ANNS systems struggle with skewed semantic embeddings, leading to inefficient I/O operations, inflated query costs, and suboptimal performance. The motivation is to address these inefficiencies through a more adaptive and efficient approach that can handle large-scale data while reducing the number of SSD accesses and improving query processing speed and latency.

Method: OrchANN introduces an I/O orchestration model that unifies I/O management across the route-access-verify pipeline. This involves selecting heterogeneous local indexes per cluster, maintaining a query-aware in-memory navigation graph, and applying multi-level geometric bounds-based pruning to filter clusters and vectors before accessing SSDs. These methods work together to adapt to skewed data distributions and minimize unnecessary I/O operations.

Result: Across five standard datasets under strict out-of-core constraints, OrchANN surpasses four baselines (DiskANN, Starling, SPANN, and PipeANN) in terms of both queries per second (QPS) and latency. It achieves up to 17.2 times higher QPS and 25.0 times lower latency compared to competing systems, all while not compromising on accuracy.

Conclusion: The proposed OrchANN system demonstrates significant improvements in handling billion-scale approximate nearest neighbor search problems, particularly in environments characterized by skewed data and out-of-core constraints. By optimizing I/O operations, it offers a practical solution for enhancing the efficiency and performance of ANNS at scale.

Abstract: Approximate nearest neighbor search (ANNS) at billion scale is fundamentally an out-of-core problem: vectors and indexes live on SSD, so performance is dominated by I/O rather than compute. Under skewed semantic embeddings, existing out-of-core systems break down: a uniform local index mismatches cluster scales, static routing misguides queries and inflates the number of probed partitions, and pruning is incomplete at the cluster level and lossy at the vector level, triggering "fetch-to-discard" reranking on raw vectors.
  We present OrchANN, an out-of-core ANNS engine that uses an I/O orchestration model for unified I/O governance along the route-access-verify pipeline. OrchANN selects a heterogeneous local index per cluster via offline auto-profiling, maintains a query-aware in-memory navigation graph that adapts to skewed workloads, and applies multi-level pruning with geometric bounds to filter both clusters and vectors before issuing SSD reads. Across five standard datasets under strict out-of-core constraints, OrchANN outperforms four baselines including DiskANN, Starling, SPANN, and PipeANN in both QPS and latency while reducing SSD accesses. Furthermore, OrchANN delivers up to 17.2x higher QPS and 25.0x lower latency than competing systems without sacrificing accuracy.

</details>


### [41] [MonoM: Enhancing Monotonicity in Learned Cardinality Estimators](https://arxiv.org/abs/2512.22122)
*Lyu Yi,Weiqi Feng,Yuanbiao Wang,Yuhong Kan*

Main category: cs.DB

TL;DR: 本文探讨了基于学习的基数估计模型（特别是MSCN算法）违反单调性约束的问题，并提出了一种名为MonoM的新度量标准和一种新的单调训练框架，包括一个工作负载生成器和添加到损失函数中的正则化项。实验结果表明，该方法不仅提高了对单调性的遵守程度，还提高了基数估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于学习的基数估计技术在准确性上优于传统方法，但它们在生产系统中被采用的一个重要障碍是容易违反诸如单调性等基本逻辑原则。

Method: 提出了一个新的度量标准MonoM来量化基数估计器在给定查询工作负载下遵守单调性的程度；开发了一个单调训练框架，包含能够生成直接可比较查询的工作负载生成器以及加到损失函数中的新颖正则化项。

Result: 实验结果显示，提出的单调训练算法不仅增强了对单调性的遵循，同时也通过减少过拟合和提高模型泛化能力提升了基数估计的准确性。

Conclusion: 通过引入MonoM度量标准及改进的训练策略，可以有效解决基于学习的基数估计模型违反单调性的问题，同时进一步提升其准确性和实用性。

Abstract: Cardinality estimation is a key component of database query optimization. Recent studies have demonstrated that learned cardinality estimation techniques can surpass traditional methods in accuracy. However, a significant barrier to their adoption in production systems is their tendency to violate fundamental logical principles such as monotonicity. In this paper, we explore how learned models specifically MSCN, a query driven deep learning algorithm can breach monotonicity constraints. To address this, we propose a metric called MonoM, which quantitatively measures how well a cardinality estimator adheres to monotonicity across a given query workload. We also propose a monotonic training framework which includes a workload generator that produces directly comparable queries (one query's predicates are strictly more relaxed than another's, enabling monotonicity inference without actual execution) and a novel regularization term added to the loss function. Experimental results show that our monotonic training algorithm not only enhances monotonicity adherence but also improves cardinality estimation accuracy. This improvement is attributed to the regularization term, which reduces overfitting and improves model generalization.

</details>


### [42] [Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries](https://arxiv.org/abs/2512.22364)
*Saurabh Deochake,Debajyoti Mukhopadhyay*

Main category: cs.DB

TL;DR: 本文首次系统地评估了大型语言模型生成的SQL查询在云端数据仓库中的计算成本，发现推理模型处理的数据量比标准模型少44.5%，但正确性相当；执行时间和查询成本之间的相关性较弱；模型间存在高达3.4倍的成本差异。同时识别出常见的效率低下模式，并为企业环境提供了成本敏感型部署指南。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL系统依赖于大型语言模型（LLMs）已能在标准基准测试中取得高准确率，但现有如有效效率分数(VES)等效率指标主要衡量的是执行时间而非云数据仓库的实际消耗成本。因此，有必要对由LLM生成的SQL查询所导致的云计算成本进行首次系统性评估。

Method: 研究者选择了六种最先进LLMs，在Google BigQuery上使用StackOverflow数据集(230GB)进行了180次查询执行实验，测量了字节处理量、槽位利用率以及估计成本。

Result: 研究表明：推理模型相比标准模型能减少44.5%的字节处理量而保持相似的正确性水平(96.7%-100%)；执行时间与查询成本间的相关度较低(r=0.16)，表明速度优化并不意味着成本优化；不同模型间最大成本差异可达3.4倍，其中一些标准模型单次查询可能消耗超过36GB的数据。此外还发现了诸如缺少分区过滤器和不必要的全表扫描等常见低效模式。

Conclusion: 本研究揭示了通过大型语言模型生成的SQL查询在实际应用中可能带来的显著云计算成本差异，并强调了在成本敏感的企业环境中采用适当优化策略的重要性。

Abstract: Text-to-SQL systems powered by Large Language Models (LLMs) achieve high accuracy on standard benchmarks, yet existing efficiency metrics such as the Valid Efficiency Score (VES) measure execution time rather than the consumption-based costs of cloud data warehouses. This paper presents the first systematic evaluation of cloud compute costs for LLM-generated SQL queries. We evaluate six state-of-the-art LLMs across 180 query executions on Google BigQuery using the StackOverflow dataset (230GB), measuring bytes processed, slot utilization, and estimated cost. Our analysis yields three key findings: (1) reasoning models process 44.5% fewer bytes than standard models while maintaining equivalent correctness (96.7%-100%); (2) execution time correlates weakly with query cost (r=0.16), indicating that speed optimization does not imply cost optimization; and (3) models exhibit up to 3.4x cost variance, with standard models producing outliers exceeding 36GB per query. We identify prevalent inefficiency patterns including missing partition filters and unnecessary full-table scans, and provide deployment guidelines for cost-sensitive enterprise environments.

</details>


### [43] [Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning](https://arxiv.org/abs/2512.22742)
*Hanze Meng,Jianhao Cao,Rachel Pottinger*

Main category: cs.DB

TL;DR: 本文提出了一种参数高效的列类型注释(CTA)框架，通过低秩适应(LoRA)训练增强提示数据的模型，从而减少了对提示变化的敏感性，并大幅减少了需要训练的参数数量，实现了跨数据集和模板的稳健性能。实验结果表明，使用该策略微调的模型在推理过程中对不同提示模式保持稳定表现，并且比仅用单一提示模板微调的模型具有更高的加权F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器的语言模型在细调后可以达到很高的准确率，但其应用范围受限于领域内设置；生成式大型语言模型虽然尝试了将CTA作为多项选择任务来解决这个问题，但它们的表现对提示词的选择非常敏感，同时标注F1分数仍然较低。完全细调大型语言模型虽然是一种自然扩展方法，但因其规模导致计算成本过高而变得不切实际，而且同样未能消除对提示词的敏感度问题。

Method: 提出了一个基于低秩适应（LoRA）的参数高效框架，通过对提示增强的数据进行模型训练以减少对提示变动的敏感度，并显著降低了所需训练参数的数量。

Result: 实验结果显示，采用该策略微调后的模型能够在多种提示模式下保持稳定的性能，并获得比仅针对单一提示模板微调更高的加权F1得分。

Conclusion: 本研究提出的参数高效训练及增强策略为开发实用且适应性强的CTA系统提供了有效途径。

Abstract: Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but their applicability is limited to in-domain settings, as distribution shifts in tables or label spaces require costly re-training from scratch. Recent work has explored prompting generative large language models (LLMs) by framing CTA as a multiple-choice task, but these approaches face two key challenges: (1) model performance is highly sensitive to subtle changes in prompt wording and structure, and (2) annotation F1 scores remain modest. A natural extension is to fine-tune large language models. However, fully fine-tuning these models incurs prohibitive computational costs due to their scale, and the sensitivity to prompts is not eliminated. In this paper, we present a parameter-efficient framework for CTA that trains models over prompt-augmented data via Low-Rank Adaptation (LoRA). Our approach mitigates sensitivity to prompt variations while drastically reducing the number of necessary trainable parameters, achieving robust performance across datasets and templates. Experimental results on recent benchmarks demonstrate that models fine-tuned with our prompt augmentation strategy maintain stable performance across diverse prompt patterns during inference and yield higher weighted F1 scores than those fine-tuned on a single prompt template. These results highlight the effectiveness of parameter-efficient training and augmentation strategies in developing practical and adaptable CTA systems.

</details>


### [44] [Evolution of Buffer Management in Database Systems: From Classical Algorithms to Machine Learning and Disaggregated Memory](https://arxiv.org/abs/2512.22995)
*Prudhvi Gadupudi,Suman Saha*

Main category: cs.DB

TL;DR: 本文全面回顾了缓冲区管理的演变，从基础算法到机器学习增强策略和分布式内存架构，并探讨了操作系统与数据库管理系统架构的分歧、生产系统实现以及新兴趋势。最后提出了结合机器学习和内核扩展机制的研究方向，以支持现代数据库系统中的异构内存层次结构的自适应跨层缓冲区管理。


<details>
  <summary>Details</summary>
Motivation: 鉴于缓冲区管理对数据库和操作系统性能的重要性，本研究旨在提供一个跨越四十年研究的缓冲区管理演变的全面概述，包括分析历史上的操作系统-数据库管理系统架构差异、生产系统实现情况及当前趋势。

Method: 通过分析来自SIGMOD、VLDB、OSDI、FAST等顶级会议的50多篇重要论文，系统地考察了从LRU-K、2Q、LIRS、ARC等基础算法到最新的机器学习辅助策略和内存解聚架构的发展历程。

Result: 确定了关键的架构模式、性能权衡点以及开放的研究挑战，并提出了一种将机器学习与内核可扩展性机制相结合的新研究方向，以便为现代数据库系统中的异构内存层级提供自适应的跨层缓冲区管理解决方案。

Conclusion: 未来的研究应着重于开发能够整合机器学习与内核扩展性的方法，从而在现代数据库环境中实现更高效、更灵活的缓冲区管理方案。

Abstract: Buffer management remains a critical component of database and operating system performance, serving as the primary mechanism for bridging the persistent latency gap between CPU processing speeds and storage access times. This paper provides a comprehensive survey of buffer management evolution spanning four decades of research. We systematically analyze the progression from foundational algorithms like LRU-K, 2Q, LIRS, and ARC to contemporary machine learning-augmented policies and disaggregated memory architectures. Our survey examines the historical OS-DBMS architectural divergence, production system implementations in PostgreSQL, Oracle, and Linux, and emerging trends including eBPF-based kernel extensibility, NVM-aware tiering strategies, and RDMA-enabled memory disaggregation. Through analysis of over 50 seminal papers from leading conferences (SIGMOD, VLDB, OSDI, FAST), we identify key architectural patterns, performance trade-offs, and open research challenges. We conclude by outlining a research direction that integrates machine learning with kernel extensibility mechanisms to enable adaptive, cross-layer buffer management for heterogeneous memory hierarchies in modern database systems.

</details>


### [45] [ChronoConnect: Tracking Pathways Along Highly Dynamic Vertices in Temporal Graphs](https://arxiv.org/abs/2512.23289)
*Jiacheng Ding,Cong Guo,Xiaofei Zhang*

Main category: cs.DB

TL;DR: ChronoConnect是一个新颖的系统，能够跟踪时态图中的时态路径，特别有助于理解信息传播的关键路径。它支持多种时序遍历算法的配置和执行，利用并行处理应对图数据增长，并提供交互式用户界面来可视化图形和探索查询结果。


<details>
  <summary>Details</summary>
Motivation: 随着时态图数据的激增，对图演变过程中信息传播模式进行分析的需求日益增加。现有的基于静态快照的图分析系统难以有效捕捉沿时间维度的信息流。

Method: 开发了名为ChronoConnect的新系统，该系统允许用户方便地配置和执行各种时序遍历算法，以在时间限制下高效分析信息扩散过程。此外，ChronoConnect采用并行处理技术来应对不断增长的图数据规模。

Result: 通过实现追踪时态图中高度动态顶点路径的算法，展示了ChronoConnect的有效性和性能提升。同时，提供了用于图可视化和查询结果探索的交互式用户界面。

Conclusion: ChronoConnect有望成为一个强大的工具，帮助用户研究信息是如何在时态图上扩散的。

Abstract: With the proliferation of temporal graph data, there is a growing demand for analyzing information propagation patterns during graph evolution. Existing graph analysis systems, mostly based on static snapshots, struggle to effectively capture information flows along the temporal dimension. To address this challenge, we introduce ChronoConnect, a novel system that enables tracking temporal pathways in temporal graph, especially beneficial to downstream mining tasks, e.g., understanding what are the critical pathways in propagating information towards a specific group of vertices. Built on ChronoConnect, users can conveniently configure and execute a variety of temporal traversal algorithms to efficiently analyze information diffusion processes under time constraints. Moreover, ChronoConnect utilizes parallel processing to tackle the explosive size-growth of evolving graphs. We showcase the effectiveness and enhanced performance of ChronoConnect through the implementation of algorithms that track pathways along highly dynamic vertices in temporal graphs. Furthermore, we offer an interactive user interface for graph visualization and query result exploration. We envision ChronoConnect to become a powerful tool for users to examine how information spreads over a temporal graph.

</details>


### [46] [Flexible Keyword-Aware Top-$k$ Route Search](https://arxiv.org/abs/2512.23319)
*Ziqiang Yu,Xiaohui Yu,Yueting Chen,Wei Liu,Anbang Song,Bolong Zheng*

Main category: cs.DB

TL;DR: 本文提出了一种关键词感知的Top-k路线查询(KATR)，旨在为用户提供更灵活、全面的路线规划服务，以满足包括兴趣点(POI)访问顺序、旅行距离预算以及个性化POI评分在内的多样化用户偏好。通过探索与限定范式有效处理KATR查询，实验证明了该方法在不同场景下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LLMs)的兴起，游客越来越倾向于使用它们来进行基于关键词输入的路线规划，而非依赖传统的手动地图服务。然而，LLMs虽然能提供合理的建议，却难以生成考虑到详细用户需求的最佳计划。因此，需要一种能够接受一系列关键词并返回最符合用户请求的前k条最佳路线的路线规划API。

Method: 本文介绍了一种名为Keyword-Aware Top-$k$ Routes (KATR)查询的方法，它提供了更加灵活且全面的语义来适应各种用户的偏好，比如灵活的兴趣点访问顺序、灵活的旅行距离预算和个人化的兴趣点评分等。为了有效地处理KATR查询，提出了一个探索与限定（explore-and-bound）范式，该范式能够根据从全局到局部级别的估计得分边界来消除冗余候选方案。

Result: 广泛的实验表明，在不同场景下，所提出的方法相比现有方法展现出更优的表现。

Conclusion: 提出的KATR查询及其处理策略为解决基于关键词的兴趣点路线规划问题提供了一个有效的新途径，特别是在考虑用户具体偏好时。

Abstract: With the rise of Large Language Models (LLMs), tourists increasingly use it for route planning by entering keywords for attractions, instead of relying on traditional manual map services. LLMs provide generally reasonable suggestions, but often fail to generate optimal plans that account for detailed user requirements, given the vast number of potential POIs and possible routes based on POI combinations within a real-world road network. In this case, a route-planning API could serve as an external tool, accepting a sequence of keywords and returning the top-$k$ best routes tailored to user requests. To address this need, this paper introduces the Keyword-Aware Top-$k$ Routes (KATR) query that provides a more flexible and comprehensive semantic to route planning that caters to various user's preferences including flexible POI visiting order, flexible travel distance budget, and personalized POI ratings. Subsequently, we propose an explore-and-bound paradigm to efficiently process KATR queries by eliminating redundant candidates based on estimated score bounds from global to local levels. Extensive experiments demonstrate our approach's superior performance over existing methods across different scenarios.

</details>


### [47] [Database Theory in Action: From Inexpressibility to Efficiency in GQL's Order-Constrained Paths](https://arxiv.org/abs/2512.23330)
*Hadar Rotschield,Liat Peterfreund*

Main category: cs.DB

TL;DR: 本文提出了一种构造性转换方法，通过将递增边条件编译到输入图中来克服GQL模式匹配的限制。实验表明，这种方法不仅恢复了表达能力，还带来了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的GQL标准在进行模式匹配时无法检查路径上边值是否递增。

Method: 提出一种将递增边条件直接编译进输入图中的构造性翻译方法。

Result: 在Neo4j的Cypher实现中，该方法比直接表达路径约束的方式运行得更快，并且避免了超时问题。

Conclusion: 理论上驱动的翻译不仅能填补表达能力上的差距，还能带来实际的性能改进。

Abstract: Pattern matching of core GQL, the new ISO standard for querying property graphs, cannot check whether edge values are increasing along a path, as established in recent work. We present a construc- tive translation that overcomes this limitation by compiling the increasing-edges condition into the input graph. Remarkably, the benefit of this construction goes beyond restoring expressiveness. In our proof-of-concept implementation in Neo4j's Cypher, where such path constraints are expressible but costly, our compiled version runs faster and avoids timeouts. This illustrates how a theoretically motivated translation can not only close an expressiveness gap but also bring practical performance gains.

</details>


### [48] [SPER: Accelerating Progressive Entity Resolution via Stochastic Bipartite Maximization](https://arxiv.org/abs/2512.23491)
*Dimitrios Karapiperis,George Papadakis,Themis Palpanas,Vassilios Verykios*

Main category: cs.DB

TL;DR: 本文提出了一种名为SPER（随机渐进实体解析）的新框架，通过将优先级排序问题转换为采样问题来解决高速数据流中实体解析的可扩展性问题。实验表明，相比现有方法，SPER在保持相当召回率和精确度的同时实现了显著的速度提升(3到6倍)。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，传统的批量实体解析方法由于数据量和处理速度的限制而变得不切实际，需要能够以有限计算资源最大化召回率的进步式实体解析方法。然而，现有的进步式方法依赖于确定性的排序来优先考虑候选对，这导致了过高的复杂性和初始化成本，无法很好地适应高吞吐量的数据流。

Method: 提出了SPER框架，该框架将优先级选择视为一个采样问题而不是排名问题。通过采用连续随机二分图最大化策略代替全局排序，SPER能够在严格线性时间内挑选出高价值的匹配对，从而实现高效处理。

Result: 通过对八个真实世界数据集进行广泛的测试显示，与最先进基准相比，SPER在保持相近召回率和准确率的情况下实现了显著的速度提升（3至6倍）。

Conclusion: 研究证明了SPER作为一种新颖的方法，在处理大规模高速数据流时具有明显优势，能够有效提高实体解析任务的效率而不牺牲结果质量。

Abstract: Entity Resolution (ER) is a critical data cleaning task for identifying records that refer to the same real-world entity. In the era of Big Data, traditional batch ER is often infeasible due to volume and velocity constraints, necessitating Progressive ER methods that maximize recall within a limited computational budget. However, existing progressive approaches fail to scale to high-velocity streams because they rely on deterministic sorting to prioritize candidate pairs, a process that incurs prohibitive super-linear complexity and heavy initialization costs. To address this scalability wall, we introduce SPER (Stochastic Progressive ER), a novel framework that redefines prioritization as a sampling problem rather than a ranking problem. By replacing global sorting with a continuous stochastic bipartite maximization strategy, SPER acts as a probabilistic high-pass filter that selects high-utility pairs in strictly linear time. Extensive experiments on eight real-world datasets demonstrate that SPER achieves significant speedups (3x to 6x) over state-of-the-art baselines while maintaining comparable recall and precision.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [49] [Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM](https://arxiv.org/abs/2512.22690)
*Poojan Vanani,Darsh Patel,Danyal Khorami,Siva Munaganuru,Pavan Reddy,Varun Reddy,Bhargav Raghunath,Ishrat Lallmamode,Romir Patel,Assegid Kidané,Tejaswi Gowda*

Main category: cs.MM

TL;DR: Mesquite, an open-source, low-cost inertial motion-capture system, combines 15 IMU sensors with a smartphone for real-time, cross-platform motion tracking. It achieves high accuracy at a significantly lower cost compared to commercial systems, making motion capture more accessible for various applications.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the high costs and complexity associated with traditional motion capture systems, which limit their use to specialized laboratories. The authors aim to create a more accessible, low-cost, and easy-to-deploy solution that can be utilized in a wider range of applications, including entertainment, biomechanics, healthcare, and virtual reality.

Method: The method involves developing Mesquite, an open-source, low-cost inertial motion-capture system. This system uses a network of 15 IMU (Inertial Measurement Unit) sensor nodes worn on the body, combined with an Android smartphone for position tracking. Data from the IMUs are streamed via a low-power wireless link to a central USB dongle, and a browser-based application is used for visualization and recording. The system leverages modern web technologies such as WebGL, WebXR, WebSerial, and WebSockets for its operation.

Result: Mesquite demonstrates a mean joint-angle error of 2-5 degrees, while being approximately 5% of the cost of a commercial optical system. It operates at 30 frames per second, with end-to-end latency under 15ms, and maintains a packet delivery rate of at least 99.7% in typical indoor settings. The system is designed to run entirely within a web browser, supporting cross-platform usage.

Conclusion: By employing IoT principles, edge processing, and a web-native stack, Mesquite effectively reduces the barriers to entry for motion capture technology. Its open-source nature, along with the release of hardware designs, firmware, and software, further encourages adoption and development across multiple fields, enhancing accessibility and lowering costs for a broad spectrum of applications.

Abstract: Motion capture remains costly and complex to deploy, limiting use outside specialized laboratories. We present Mesquite, an open-source, low-cost inertial motion-capture system that combines a body-worn network of 15 IMU sensor nodes with a hip-worn Android smartphone for position tracking. A low-power wireless link streams quaternion orientations to a central USB dongle and a browser-based application for real-time visualization and recording. Built on modern web technologies -- WebGL for rendering, WebXR for SLAM, WebSerial and WebSockets for device and network I/O, and Progressive Web Apps for packaging -- the system runs cross-platform entirely in the browser. In benchmarks against a commercial optical system, Mesquite achieves mean joint-angle error of 2-5 degrees while operating at approximately 5% of the cost. The system sustains 30 frames per second with end-to-end latency under 15ms and a packet delivery rate of at least 99.7% in standard indoor environments. By leveraging IoT principles, edge processing, and a web-native stack, Mesquite lowers the barrier to motion capture for applications in entertainment, biomechanics, healthcare monitoring, human-computer interaction, and virtual reality. We release hardware designs, firmware, and software under an open-source license (GNU GPL).

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [50] [OxygenREC: An Instruction-Following Generative Framework for E-commerce Recommendation](https://arxiv.org/abs/2512.22386)
*Xuegang Hao,Ming Zhang,Alex Li,Xiangyu Qian,Zhi Ma,Yanlong Zang,Shijie Yang,Zhongxuan Han,Xiaolong Ma,Jinguang Liu,Zhen Li,Zhida Jiang,Shusheng Wang,Ning Tang,Yanchen Qiao,Chenxiang Yang,Chen Sun,Jincheng Yuan,Chunhua Peng,Heng Hu,Peijun Yang,Baopeng Yuan,Caiyun Qiu,Zhaolong Xing,Haofei Yuan,Haipeng Zhang,Yuzhang Guo,Weijie Ding,Jiahua Gao,Hao Huang,Zhen Chen,Tongxuan Liu,Pinghua Gong*

Main category: cs.IR

TL;DR: OxygenREC, a novel recommendation system, integrates Fast-Slow Thinking to achieve deep reasoning and meet the strict latency and multi-scenario requirements of real-world applications. It uses a near-line LLM for slow thinking and a high-efficiency backbone for fast thinking, along with a semantic alignment mechanism and SA-GCPO to enhance performance and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the shortcomings of traditional recommendation systems and existing Generative Recommendation (GR) methods, which struggle with multi-stage optimization inconsistency, reliance on inductive patterns, and challenges in multi-scenario scalability. The paper also aims to leverage the strong reasoning capabilities of LLMs while mitigating their high latency and computational costs.

Method: The method proposed in this paper is OxygenREC, an industrial recommendation system that employs a Fast-Slow Thinking architecture. This involves using a near-line LLM pipeline for slow thinking to generate Contextual Reasoning Instructions, and a high-efficiency encoder-decoder backbone for fast thinking to support real-time generation. Additionally, it introduces a semantic alignment mechanism, Instruction-Guided Retrieval (IGR), and a Query-to-Item (Q2I) loss to ensure the effectiveness of reasoning instructions. To handle multi-scenario scalability, the system transforms scenario information into controllable instructions and applies unified reward mapping and Soft Adaptive Group Clip Policy Optimization (SA-GCPO).

Result: The results of this work are not explicitly stated in the abstract but imply that OxygenREC can effectively deliver deep reasoning within strict latency constraints and across multiple scenarios, potentially improving resource utilization and reducing maintenance costs compared to existing solutions. The approach aims to align policies with diverse business objectives through a train-once-deploy-everywhere paradigm.

Conclusion: The conclusion suggests that by integrating Fast-Slow Thinking and innovative mechanisms such as IGR and SA-GCPO, OxygenREC offers a promising solution to the challenges faced by current recommendation systems, particularly in terms of deep reasoning, latency, and multi-scenario scalability.

Abstract: Traditional recommendation systems suffer from inconsistency in multi-stage optimization objectives. Generative Recommendation (GR) mitigates them through an end-to-end framework; however, existing methods still rely on matching mechanisms based on inductive patterns. Although responsive, they lack the ability to uncover complex user intents that require deductive reasoning based on world knowledge. Meanwhile, LLMs show strong deep reasoning capabilities, but their latency and computational costs remain challenging for industrial applications. More critically, there are performance bottlenecks in multi-scenario scalability: as shown in Figure 1, existing solutions require independent training and deployment for each scenario, leading to low resource utilization and high maintenance costs-a challenge unaddressed in GR literature. To address these, we present OxygenREC, an industrial recommendation system that leverages Fast-Slow Thinking to deliver deep reasoning with strict latency and multi-scenario requirements of real-world environments. First, we adopt a Fast-Slow Thinking architecture. Slow thinking uses a near-line LLM pipeline to synthesize Contextual Reasoning Instructions, while fast thinking employs a high-efficiency encoder--decoder backbone for real-time generation. Second, to ensure reasoning instructions effectively enhance recommendation generation, we introduce a semantic alignment mechanism with Instruction-Guided Retrieval (IGR) to filter intent-relevant historical behaviors and use a Query-to-Item (Q2I) loss for instruction-item consistency. Finally, to resolve multi-scenario scalability, we transform scenario information into controllable instructions, using unified reward mapping and Soft Adaptive Group Clip Policy Optimization (SA-GCPO) to align policies with diverse business objectives, realizing a train-once-deploy-everywhere paradigm.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders](https://arxiv.org/abs/2512.22150)
*Hans Jarett J. Ong,Brian Godwin S. Lim,Dominic Dayta,Renzo Roel P. Tan,Kazushi Ikeda*

Main category: cs.LG

TL;DR: 提出了一种新的无监督学习方法LANCA，通过引入加性噪声模型（ANM）作为归纳偏置来解决潜在因果变量的识别问题。该方法在合成物理基准和逼真的环境测试中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 无监督表示学习旨在恢复潜在生成因素，但依赖统计独立性的标准方法往往无法捕捉因果依赖关系。关键挑战在于可识别性：从观察数据中解缠因果变量是不可能的，除非有监督、辅助信号或强归纳偏置。

Method: 提出了Latent Additive Noise Model Causal Autoencoder (LANCA)，它将加性噪声模型(ANM)作为一种强归纳偏置用于无监督发现。LANCA使用确定性Wasserstein自编码器(WAE)结合一个可微分的ANM层，将残差独立性从被动假设转变为明确优化目标。

Result: LANCA在合成物理基准（如Pendulum, Flow）和逼真环境（如CANDLE）上的表现优于最先进基线，并且对于复杂背景场景引起的虚假关联表现出更好的鲁棒性。

Conclusion: 尽管ANM约束在一般混合情况下不能保证唯一可识别性，但它通过限制允许的变换范围解决了组件级不确定性问题。LANCA为无监督环境下潜在因果变量的发现提供了有效途径。

Abstract: Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.

</details>


### [52] [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)
*S. Dai,G. Sun,F. Li,X. Tang,Q. Wang,Y. Cong*

Main category: cs.LG

TL;DR: 本文提出了一种名为联邦多任务聚类（FMTC）的新框架，旨在为异构客户端学习个性化的聚类模型，同时以隐私保护的方式协作利用其共享的底层结构。通过客户端个性化聚类模块和服务端张量相关性模块，该框架解决了现有联邦学习方法对不可靠伪标签依赖以及未能捕捉到异构客户端间潜在关联的问题。实验表明，FMTC框架在多个真实世界数据集上显著优于各种基线和最先进的联邦聚类算法。


<details>
  <summary>Details</summary>
Motivation: 当前谱聚类算法主要针对集中式环境设计，在现代分散环境下不适用；现有的联邦学习方法由于依赖于不可靠的伪标签而导致泛化性能较差，并且未能捕捉异构客户端间的潜在关联。

Method: 提出了Federated Multi-Task Clustering (FMTC) 框架，包括客户端个性化聚类模块（用于学习参数化映射模型支持鲁棒的样本外推理）和服务端张量相关性模块（通过将所有客户端模型组织成统一张量并应用低秩正则化来发现它们的共同子空间）。使用交替方向乘子法(ADMM)开发了一个高效的、保护隐私的分布式算法来解决联合优化问题。

Result: 在多个真实世界数据集上的广泛实验表明，所提出的FMTC框架显著优于各种基线和最先进的联邦聚类算法。

Conclusion: FMTC框架能够有效克服现有联邦学习方案中的局限性，不仅提高了个性化模型的质量还增强了整体系统的通用性与隐私保护能力。

Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.

</details>


### [53] [Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection](https://arxiv.org/abs/2512.22179)
*Rajeeb Thapa Chhetri,Zhixiong Chen,Saurab Thapa*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A fundamental limitation of supervised deep learning in high-dimensional tabular domains is "Generalization Collapse": models learn precise decision boundaries for known distributions but fail catastrophically when facing Out-of-Distribution (OOD) data. We hypothesize that this failure stems from the lack of topological constraints in the latent space, resulting in diffuse manifolds where novel anomalies remain statistically indistinguishable from benign data. To address this, we propose Latent Sculpting, a hierarchical two-stage representation learning framework. Stage 1 utilizes a hybrid 1D-CNN and Transformer Encoder trained with a novel Dual-Centroid Compactness Loss (DCCL) to actively "sculpt" benign traffic into a low-entropy, hyperspherical cluster. Unlike standard contrastive losses that rely on triplet mining, DCCL optimizes global cluster centroids to enforce absolute manifold density. Stage 2 conditions a Masked Autoregressive Flow (MAF) on this pre-structured manifold to learn an exact density estimate. We evaluate this methodology on the rigorous CIC-IDS-2017 benchmark, treating it as a proxy for complex, non-stationary data streams. Empirical results demonstrate that explicit manifold sculpting is a prerequisite for robust zero-shot generalization. While supervised baselines suffered catastrophic performance collapse on unseen distribution shifts (F1 approx 0.30) and the strongest unsupervised baseline achieved only 0.76, our framework achieved an F1-Score of 0.87 on strictly zero-shot anomalies. Notably, we report an 88.89% detection rate on "Infiltration" scenarios--a complex distributional shift where state-of-the-art supervised models achieved 0.00% accuracy. These findings suggest that decoupling structure learning from density estimation provides a scalable path toward generalized anomaly detection.

</details>


### [54] [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)
*Ying Zhu,Jiaxin Wan,Xiaoran Liu,Siyanag He,Qiqi Wang,Xu Guo,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出了一种名为DiRL的有效后训练框架，该框架结合了FlexAttention加速的分块训练与LMDeploy优化的推理，并基于此提出了首个针对扩散语言模型（dLLM）的无偏组相对策略优化（GRPO）实现DiPO。通过在高质量数学数据上训练DiRL-8B-Instruct模型，其在多个基准测试中超越了同类模型，特别是在数学表现上达到了领先水平。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型（dLLMs）虽然在预训练潜力和加速推理速度方面得到了验证，但其后训练领域仍显不足。现有方法存在计算效率低下以及训练与推理之间目标不匹配的问题，这极大地限制了这些模型在诸如数学等复杂推理任务上的表现。

Method: 为了解决上述问题，研究者们开发了一个名为DiRL的新框架，它将利用FlexAttention技术进行加速的分块训练与经过LMDeploy优化后的推理紧密结合在一起。此外，他们还提出了DiPO——一种专为dLLMs设计的无偏差组相对策略优化算法。

Result: 通过使用高质量的数学数据集对DiRL-8B-Instruct模型进行了训练，实验结果表明该模型不仅在dLLMs类别中实现了最先进的数学处理能力，而且还在多项基准测试中超过了Qwen2.5系列中的相似规模模型。

Conclusion: 本研究表明，通过引入有效的后训练机制如DiRL及其相关组件DiPO，可以显著提升扩散语言模型在特定任务上的性能，特别是对于需要复杂逻辑推理的任务如数学问题解答而言。

Abstract: Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.

</details>


### [55] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: 提出了一种名为Masters的掩码渐进强化学习蒸馏框架，通过首先掩盖教师模型中非主导权重来减少不必要的复杂性，然后在训练过程中逐步恢复教师的能力，以实现从大规模视觉-语言模型到小型学生模型的知识转移。


<details>
  <summary>Details</summary>
Motivation: 由于大型视觉-语言模型（VLMs）的规模庞大，导致它们在移动或边缘设备上部署不切实际，因此需要一种方法能够有效地将知识从大型教师模型转移到更紧凑的学生模型上。然而，大小之间的差距使得直接的知识转移变得困难，学生模型难以复制教师复杂的高维表示，从而导致学习不稳定和性能下降。

Method: Masters框架首先对教师模型中的非主要权重进行掩蔽处理，以减少不必要的复杂性；接着，在训练过程中逐渐增加教师模型的能力，让学生模型能够平滑且稳定地从教师那里学到更多样化的表达。此外，该框架还结合了离线强化学习阶段，使用准确性和蒸馏两个互补奖励来进一步优化知识转移过程。

Result: 实验结果表明，通过Masters框架，学生模型能够在不需要经历思考回答过程的情况下，利用预生成的回答作为丰富而高效的指导，从而达到强大的表现。

Conclusion: Masters提供了一个有效的解决方案，通过掩码渐进式强化学习框架促进了从大型视觉-语言模型向小型学生模型的知识迁移，提高了后者的学习稳定性与最终性能。

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [56] [EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs](https://arxiv.org/abs/2512.22240)
*Chama Bensmail*

Main category: cs.LG

TL;DR: 本文介绍了一种名为EvoXplain的诊断框架，它能够衡量模型解释在重复训练过程中的稳定性，并发现即使是高精度模型也可能依赖不同的内部逻辑来达到相同的预测结果。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习模型通常通过预测性能进行评估，但当两个模型都能达到高精度时，它们是否依赖相同的内部逻辑或通过不同的机制达成同样的结果是一个被忽视的问题。

Method: EvoXplain框架将解释视为从随机优化过程中抽取的样本，不聚合预测或构建集成模型，而是检查这些样本是否形成一个单一的一致性解释或分成多个不同的解释模式。该方法应用于乳腺癌和COMPAS数据集上，使用了逻辑回归和随机森林两种广泛部署的模型类别。

Result: 研究发现，即使所有模型都达到了很高的预测准确性，它们的解释也经常表现出明显的多模态特征。甚至像逻辑回归这样通常被认为稳定的模型，在同一数据分割下反复训练时也能产生多个分离良好的解释盆地。这种差异不能简单地用超参数变化或性能权衡来解释。

Conclusion: EvoXplain框架使得解释不稳定性的可见性和可量化成为可能，揭示了单个实例或平均解释可能会掩盖存在多种潜在机制的情况。更广泛地说，EvoXplain重新定义了解释性为模型类在重复实例化下的属性，而不仅仅是单个已训练模型的特性。

Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.

</details>


### [57] [Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening](https://arxiv.org/abs/2512.22242)
*Shaurya Gaur,Michel Vitale,Alessa Hering,Johan Kwisthout,Colin Jacobs,Lena Philipp,Fennie van der Graaf*

Main category: cs.LG

TL;DR: 本研究评估了两种深度学习肺癌风险评估模型（Sybil肺癌风险模型和Venkadesh21结节风险评估器）以及PanCan2b逻辑回归模型在不同人口统计学子群体中的性能差异，发现存在显著的性别和种族间表现差异，这些差异无法通过临床混淆因素解释，因此可能被认为是不公平偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管AI模型在从低剂量CT扫描估计肺癌风险方面显示出潜力，但它们在不同人群中的表现如何仍不清楚。考虑到肺癌高危人群的多样性，有必要对这些模型进行公平性评估以确保其广泛实施时不会加剧现有的放射科人力资源紧张问题。

Method: 使用JustEFAB框架考虑混杂因素和伦理上重要的偏差，评估两个基于美国国家肺癌筛查试验(NLST)数据训练的深度学习风险评估模型：Sybil肺癌风险模型与Venkadesh21结节风险评估器，及英国胸科学会推荐的PanCan2b逻辑回归模型。评估指标包括AUROC、敏感性和特异性，并探索了临床风险因素潜在的混淆影响。

Result: Sybil模型对于女性的表现优于男性（AUROC分别为0.88和0.81），而Venkadesh21模型在黑人参与者中比白人表现出更低的敏感度（90%特异性下分别为0.39和0.69）。这些差异不能由已知的临床混淆因素来解释。

Conclusion: 研究结果强调了提高并监测模型在代表性不足子群体中表现的重要性，同时指出需要进一步研究算法公平性问题，特别是在肺癌筛查领域。

Abstract: Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p < .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.

</details>


### [58] [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)
*Bhaktipriya Radharapu,Eshika Saxena,Kenneth Li,Chenxi Whitehouse,Adina Williams,Nicola Cancedda*

Main category: cs.LG

TL;DR: 本文提出了一种基于Brier分数损失训练的线性探针方法，用于从推理法官的隐藏状态中提供校准的不确定性估计，无需额外的模型训练。该方法在客观任务和主观人类偏好判断上均表现出色，计算成本节省约10倍，并且在未见评估领域上具有较强的泛化能力。不过，在较简单的数据集上，这种方法产生的估计较为保守。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的评判者成为行业应用的关键部分，高效地获取良好校准的不确定性估计对于生产部署至关重要。然而，现有技术如口头信心表达和多次生成方法往往要么校准不佳，要么计算成本高昂。

Method: 作者引入了使用Brier分数为基础的损失函数训练的线性探针，以从评判者的隐藏状态中提供经过校准的不确定性估计，此过程不需要对模型进行额外训练。

Result: 实验结果表明，与现有方法相比，探针法实现了更优的校准效果，大约节省了10倍的计算资源，能够稳健地推广到未曾见过的评估领域，并提高了高置信度预测的准确性。但在较简单的数据集上，探针提供的估计偏保守，可能更适合需要优先考虑低误报率的安全关键部署场景。

Conclusion: 总体而言，这项工作展示了基于可解释性的不确定性估计为生产环境中的LLM评判者提供了一个实用且可扩展的即插即用解决方案。

Abstract: As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.

</details>


### [59] [Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models](https://arxiv.org/abs/2512.22249)
*Zheng Xing,Weibing Zhao*

Main category: cs.LG

TL;DR: 该论文提出了一种结合时态视觉语义（TVS）与子空间聚类的方法，通过利用大语言模型（LLM）从连续帧中提取文本运动信息，并将这些信息整合到子空间聚类框架中。此外，还引入了一个反馈机制来持续优化基于分割输出的子空间嵌入。实验结果表明，所提方法在四个基准人体运动数据集上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统的人体运动分割方法忽略了时态语义探索的作用。本文旨在通过使用从人类运动序列中提取的时态视觉语义（TVS），并利用大型语言模型（LLM）的图像到文本能力，来提高子空间聚类性能。

Method: 1. 通过LLM从连续帧中抽取文本运动信息。
2. 利用LLM判断连续帧是否描绘相同动作，并基于此学习时态邻近信息。
3. 开发一种整合了TVS的子空间聚类方法，其中包含了促使每个帧与其时间邻居共享相似子空间嵌入的时间正则化项。
4. 根据具有时间约束的子空间嵌入进行分割，促进每帧与其时间邻居的分组。
5. 引入一个启用反馈的框架，根据分割输出不断优化子空间嵌入。

Result: 实验结果显示，所提出的方法在四个基准人体运动数据集上的表现优于当前最先进的方法。

Conclusion: 通过结合时态视觉语义和子空间聚类，以及采用反馈机制优化子空间嵌入，可以有效提升人体运动分割的效果。

Abstract: Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.

</details>


### [60] [Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data](https://arxiv.org/abs/2512.22259)
*Daniil Burakov,Ivan Petrov,Dmitrii Khelimskii,Ivan Bessonov,Mikhail Lazarev*

Main category: cs.LG

TL;DR: 研究开发了一种基于真实和合成数据预测PCI术后心脏死亡风险的模型，并通过特征重要性分析确定了影响死亡率的关键因素。使用数据增强提高了少数类别的识别率，同时保持了模型的整体性能。


<details>
  <summary>Details</summary>
Motivation: 旨在建立一个能够评估接受PCI治疗患者长期预后（特别是心脏死亡）风险的预测模型，并识别对死亡影响最大的因素。

Method: 分析了2044名接受PCI治疗分叉病变患者的临床数据，应用了多种机器学习模型来预测三年后的死亡情况。为了处理类别不平衡问题，生成并添加了500个合成样本到训练集中。还进行了特征重要性分析及移除非信息特征后的模型表现测试。

Result: 所有模型在没有过采样的情况下都达到了很高的总体准确率(0.92-0.93)，但几乎完全忽略了少数类别。增加合成样本后，模型对于少数类别的召回率有所提高，同时仅轻微降低了AUROC值。特征重要性分析表明，年龄、射血分数、外周动脉疾病以及脑血管疾病是最重要的四个影响因素。

Conclusion: 简单的数据增强可以揭示并减少不平衡临床预测中的脆弱性，仅利用表格记录即可实现。此外，建议常规报告概率质量及压力测试结果作为主要指标的一部分。

Abstract: Patient status, angiographic and procedural characteristics encode crucial signals for predicting long-term outcomes after percutaneous coronary intervention (PCI). The aim of the study was to develop a predictive model for assessing the risk of cardiac death based on the real and synthetic data of patients undergoing PCI and to identify the factors that have the greatest impact on mortality. We analyzed 2,044 patients, who underwent a PCI for bifurcation lesions. The primary outcome was cardiac death at 3-year follow-up. Several machine learning models were applied to predict three-year mortality after PCI. To address class imbalance and improve the representation of the minority class, an additional 500 synthetic samples were generated and added to the training set. To evaluate the contribution of individual features to model performance, we applied permutation feature importance. An additional experiment was conducted to evaluate how the model's predictions would change after removing non-informative features from the training and test datasets. Without oversampling, all models achieve high overall accuracy (0.92-0.93), yet they almost completely ignore the minority class. Across models, augmentation consistently increases minority-class recall with minimal loss of AUROC, improves probability quality, and yields more clinically reasonable risk estimates on the constructed severe profiles. According to feature importance analysis, four features emerged as the most influential: Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease. These results show that straightforward augmentation with realistic and extreme cases can expose, quantify, and reduce brittleness in imbalanced clinical prediction using only tabular records, and motivate routine reporting of probability quality and stress tests alongside headline metrics.

</details>


### [61] [The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning](https://arxiv.org/abs/2512.22261)
*Rahul D Ray*

Main category: cs.LG

TL;DR: 研究通过系统性消融实验，分析了物理约束在光栅耦合器谱生成模型中的作用，揭示了能量守恒约束的数学冗余性和Fabry-Perot振荡对带宽变化的影响，并指出标准噪声添加流程可能引入非物理负吸收值。结果表明适当移除某些物理约束可以提高机器学习模型对于特定参数预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 在科学领域中，当实际数据稀缺时，基于物理约束的数据生成对于机器学习至关重要；然而，现有方法往往过度约束模型而不区分哪些物理组件是必要的。本研究旨在通过系统性的消融研究来识别这些必要组件。

Method: 采用了一种受物理启发的光栅耦合器谱生成器，该生成器能够将五个几何参数映射到100点的光谱响应上。通过选择性地去除显式的能量守恒强制、Fabry-Perot振荡、带宽变化和噪声等元素，研究人员进行了详尽的分析。

Result: 研究发现了一个物理约束悖论：当基础方程物理一致时，显式的能量守恒强制在数学上是多余的，有无此约束的版本能达到相同的守恒精度（平均误差约为7×10^-9）。相反地，去除Fabry-Perot振荡显著减少了半最大带宽扩散（从132.3 nm降至37.4 nm），减少了72%。此外，标准的加噪再归一化流程被发现会引入0.5%的非物理负吸收值。

Conclusion: 这项工作为物理信息数据集的设计提供了实用指导，并强调了利用机器学习性能作为评估约束相关性的诊断工具的价值。特别是，它展示了通过移除特定物理约束（如Fabry-Perot振荡）可以改善对某些参数（例如带宽）预测准确性的潜力。

Abstract: Physics-constrained data generation is essential for machine learning in scientific domains where real data are scarce; however, existing approaches often over-constrain models without identifying which physical components are necessary. We present a systematic ablation study of a physics-informed grating coupler spectrum generator that maps five geometric parameters to 100-point spectral responses. By selectively removing explicit energy conservation enforcement, Fabry-Perot oscillations, bandwidth variation, and noise, we uncover a physics constraint paradox: explicit energy conservation enforcement is mathematically redundant when the underlying equations are physically consistent, with constrained and unconstrained variants achieving identical conservation accuracy (mean error approximately 7 x 10^-9). In contrast, Fabry-Perot oscillations dominate threshold-based bandwidth variability, accounting for a 72 percent reduction in half-maximum bandwidth spread when removed (with bandwidth spread reduced from 132.3 nm to 37.4 nm). We further identify a subtle pitfall: standard noise-addition-plus-renormalization pipelines introduce 0.5 percent unphysical negative absorption values. The generator operates at 200 samples per second, enabling high-throughput data generation and remaining orders of magnitude faster than typical full-wave solvers reported in the literature. Finally, downstream machine learning evaluation reveals a clear physics-learnability trade-off: while central wavelength prediction remains unaffected, removing Fabry-Perot oscillations improves bandwidth prediction accuracy by 31.3 percent in R-squared and reduces RMSE by 73.8 percent. These findings provide actionable guidance for physics-informed dataset design and highlight machine learning performance as a diagnostic tool for assessing constraint relevance.

</details>


### [62] [Valori: A Deterministic Memory Substrate for AI Systems](https://arxiv.org/abs/2512.22280)
*Varshith Gudur*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为Valori的确定性AI内存基底，它使用定点算术(Q16.16)替代浮点运算，并将内存建模为可重播的状态机，以确保跨平台的比特级一致性内存状态、快照和搜索结果。研究强调了对于可信AI系统而言，确定性内存是必要的基础组件。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统依赖于存储和检索向量嵌入的过程，这通常基于浮点运算完成。尽管这种方法对近似相似性搜索有效，但它引入了根本性的非确定性问题：相同的模型、输入和代码在不同硬件架构（如x86与ARM）上可能产生不同的内存状态及检索结果。这种非确定性阻碍了过程重现性和安全部署，导致静默数据分歧，影响受监管行业的事后验证和审计追踪完整性。

Method: 提出了一种名为Valori的新方法，通过用定点算术(Q16.16)替换浮点内存操作，并将内存处理视为一种可回放的状态机来实现AI系统的确定性。Valori确保跨平台的内存状态、快照以及搜索结果完全一致。

Result: 研究表明，在索引或检索之前就已经出现了非确定性现象，并展示了Valori如何在内存边界处强制执行确定性。实验结果表明，对于构建值得信赖的人工智能系统来说，采用确定性内存机制是一项必不可少的技术手段。

Conclusion: Valori作为一项创新技术，成功解决了现有AI系统中存在的非确定性问题，为开发更加可靠且可验证的人工智能应用提供了新的可能性。此外，公开发布的参考实现有助于促进该领域内进一步的研究与发展。

Abstract: Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).

</details>


### [63] [Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization](https://arxiv.org/abs/2512.22279)
*Amgad Ahmed Ali Ibrahim,Hein Htet,Ryoji Asahi*

Main category: cs.LG

TL;DR: 本文提出了一种分层堆叠优化框架SoDip，用于解决辐射诱导接枝(RIG)过程中由于基膜形态变化导致的可重复性问题。该框架结合了文本处理、多模态特征交互建模、不确定性量化以及贝叶斯优化等技术，以提高RIG过程中的可重复性和性能一致性。


<details>
  <summary>Details</summary>
Motivation: 辐射诱导接枝(RIG)技术在制备离子交换膜、CO2分离膜和电池电解质等方面具有重要作用，但其结果的再现性受到基膜形态差异的影响而受限。这种形态上的微小变化能够影响单体扩散、自由基分布及Trommsdorff效应，进而造成空间接枝梯度与性能不一致的问题。

Method: 开发了一个名为SoDip的分层堆叠优化框架，它包括：(1)使用DeepSeek-R1解码器仅变压器来编码文本过程描述符；(2)采用TabNet和XGBoost模型处理多模态特征间的相互作用；(3)通过结合高斯过程回归（GPR）与狄利克雷过程混合模型(DPMM)，实现不确定性量化与异方差性分析；(4)利用贝叶斯优化有效探索高维合成空间。

Result: 通过对数百个RIG研究的数据集进行交叉验证测试，发现SoDip相比GPR有约33%的改进，并且提供了校准后的置信区间，有助于识别低再现性区域。其架构设计允许整合不同质量的稀疏文本和数值输入，在表现上超越了先前的方法。

Conclusion: 所提出的SoDip框架为实现更可重现、形态感知的设计奠定了基础，在接枝聚合物研究领域展现出巨大潜力。

Abstract: Radiation-induced grafting (RIG) enables precise functionalization of polymer films for ion-exchange membranes, CO2-separation membranes, and battery electrolytes by generating radicals on robust substrates to graft desired monomers. However, reproducibility remains limited due to unreported variability in base-film morphology (crystallinity, grain orientation, free volume), which governs monomer diffusion, radical distribution, and the Trommsdorff effect, leading to spatial graft gradients and performance inconsistencies. We present a hierarchical stacking optimization framework with a Dirichlet's Process (SoDip), a hierarchical data-driven framework integrating: (1) a decoder-only Transformer (DeepSeek-R1) to encode textual process descriptors (irradiation source, grafting type, substrate manufacturer); (2) TabNet and XGBoost for modelling multimodal feature interactions; (3) Gaussian Process Regression (GPR) with Dirichlet Process Mixture Models (DPMM) for uncertainty quantification and heteroscedasticity; and (4) Bayesian Optimization for efficient exploration of high-dimensional synthesis space. A diverse dataset was curated using ChemDataExtractor 2.0 and WebPlotDigitizer, incorporating numerical and textual variables across hundreds of RIG studies. In cross-validation, SoDip achieved ~33% improvement over GPR while providing calibrated confidence intervals that identify low-reproducibility regimes. Its stacked architecture integrates sparse textual and numerical inputs of varying quality, outperforming prior models and establishing a foundation for reproducible, morphology-aware design in graft polymerization research.

</details>


### [64] [FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs](https://arxiv.org/abs/2512.23235)
*Zihao Zhou,Shusen Yang,Fangyuan Zhao,Xuebin Ren*

Main category: cs.LG

TL;DR: 本文提出了FairGFL算法，通过可解释的加权聚合方法和精心设计的正则化器来解决图联邦学习中由于子图重叠不平衡导致的不公平问题，在保持模型实用性的同时增强了跨客户端公平性。实验结果表明，与四个代表性基线算法相比，FairGFL在模型实用性及公平性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 图联邦学习允许从分布式子图中协作提取高阶信息，同时保护原始数据隐私。但是，当不同客户端之间存在数据重叠时，尤其是这种重叠不平衡分布的情况下，可能引发不公平问题。先前的研究主要关注了重叠数据在缓解数据异质性方面的积极影响，而忽略了其潜在的负面影响。因此，本研究旨在揭示并解决由不平衡重叠子图引起的不公平性问题。

Method: 提出了一种名为FairGFL的新颖算法，该算法采用可解释的加权聚合策略以提高客户间的公平性，并利用保护隐私的方法估计它们的重叠比例。此外，通过将一个特别设计的正则项集成到联合复合损失函数中，FairGFL能够在改善模型效用与公平性之间的平衡。

Result: 通过对四个基准图数据集进行广泛的实验评估，结果显示FairGFL在模型效用和公平性方面均优于四种代表性基准算法。

Conclusion: FairGFL提供了一种有效的方法来处理图联邦学习框架下的不公平问题，特别是在面临数据重叠不平衡的情形下。它不仅提高了参与方之间的公平性，还保持了良好的模型性能。

Abstract: Graph federated learning enables the collaborative extraction of high-order information from distributed subgraphs while preserving the privacy of raw data. However, graph data often exhibits overlap among different clients. Previous research has demonstrated certain benefits of overlapping data in mitigating data heterogeneity. However, the negative effects have not been explored, particularly in cases where the overlaps are imbalanced across clients. In this paper, we uncover the unfairness issue arising from imbalanced overlapping subgraphs through both empirical observations and theoretical reasoning. To address this issue, we propose FairGFL (FAIRness-aware subGraph Federated Learning), a novel algorithm that enhances cross-client fairness while maintaining model utility in a privacy-preserving manner. Specifically, FairGFL incorporates an interpretable weighted aggregation approach to enhance fairness across clients, leveraging privacy-preserving estimation of their overlapping ratios. Furthermore, FairGFL improves the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function. Through extensive experiments on four benchmark graph datasets, we demonstrate that FairGFL outperforms four representative baseline algorithms in terms of both model utility and fairness.

</details>


### [65] [Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation](https://arxiv.org/abs/2512.22287)
*Zikun Guoa,Adeyinka. P. Adedigbaa,Rammohan Mallipeddi*

Main category: cs.LG

TL;DR: 本文提出了一种集群聚合GAN框架，专门用于生成非侵入式负载监测算法所需的合成电器数据。该方法根据电器的行为特征将其分配到不同的分支处理，从而提高了输出的真实性和多样性，并增强了训练的稳定性。实验表明，此框架在真实感、多样性和训练稳定性方面均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于GAN的方法在生成负载模式时忽视了间歇性和连续性电器之间的行为差异所导致的不稳定训练和有限输出保真度问题，提出了一个能够根据电器行为特性进行分类处理的混合生成方法。

Method: Cluster Aggregated GAN框架，其中间歇性电器通过聚类模块被分组并为每个组分配专用生成器；连续性电器则遵循另一个采用LSTM为基础的生成器路径，旨在捕捉渐进的时间演变同时保持训练稳定。

Result: 广泛的实验证明了提出的框架在衡量现实性、多样性和训练稳定性等指标上始终优于基线方法，并且将聚类作为主动生成组件显著提高了可解释性和可扩展性。

Conclusion: 本研究提出的集群聚合GAN框架为非侵入式负载监测研究中的合成负载生成提供了一种有效的方法。

Abstract: Synthetic appliance data are essential for developing non-intrusive load monitoring algorithms and enabling privacy preserving energy research, yet the scarcity of labeled datasets remains a significant barrier. Recent GAN-based methods have demonstrated the feasibility of synthesizing load patterns, but most existing approaches treat all devices uniformly within a single model, neglecting the behavioral differences between intermittent and continuous appliances and resulting in unstable training and limited output fidelity. To address these limitations, we propose the Cluster Aggregated GAN framework, a hybrid generative approach that routes each appliance to a specialized branch based on its behavioral characteristics. For intermittent appliances, a clustering module groups similar activation patterns and allocates dedicated generators for each cluster, ensuring that both common and rare operational modes receive adequate modeling capacity. Continuous appliances follow a separate branch that employs an LSTM-based generator to capture gradual temporal evolution while maintaining training stability through sequence compression. Extensive experiments on the UVIC smart plug dataset demonstrate that the proposed framework consistently outperforms baseline methods across metrics measuring realism, diversity, and training stability, and that integrating clustering as an active generative component substantially improves both interpretability and scalability. These findings establish the proposed framework as an effective approach for synthetic load generation in non-intrusive load monitoring research.

</details>


### [66] [Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model](https://arxiv.org/abs/2512.22288)
*Renping Zhou,Zanlin Ni,Tianyi Chen,Zeyu Liu,Yang Yue,Yulin Wang,Yuxuan Wang,Jingshu Liu,Gao Huang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Co-GRPO的方法，通过将Masked Diffusion Models (MDMs)的生成过程重新表述为一个统一的马尔可夫决策过程(MDP)，联合优化模型参数和推理调度参数，从而在训练和推理之间实现了更紧密的一致性。实验结果表明该方法有效提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: Masked Diffusion Models (MDMs) 在视觉、语言及跨模态生成方面展现了巨大潜力，但其训练与推理过程存在显著差异：训练时采用简化的单步BERT式目标函数，而推理则是一个多步骤迭代过程。这种不匹配导致了训练过程中未能对推理调度进行优化。

Method: 提出了Co-GRPO方法，它将MDM的生成视为一个综合考虑模型本身与推理调度的马尔科夫决策过程(Markov Decision Process, MDP)。通过轨迹级别的组相对策略优化(Group Relative Policy Optimization)，Co-GRPO能够在共享奖励下协同优化模型参数与调度参数，避免了多步生成过程中昂贵的反向传播需求。

Result: 四个基准测试（ImageReward, HPS, GenEval, 和 DPG-Bench）的结果证明了所提方法的有效性，显示出了在生成质量上的显著提升。

Conclusion: Co-GRPO提供了一种新的方式来缩小Masked Diffusion Models (MDMs) 训练与推理之间的差距，通过共同优化模型和推理调度，在无需高成本反向传播的情况下提高了生成任务的表现。

Abstract: Recently, Masked Diffusion Models (MDMs) have shown promising potential across vision, language, and cross-modal generation. However, a notable discrepancy exists between their training and inference procedures. In particular, MDM inference is a multi-step, iterative process governed not only by the model itself but also by various schedules that dictate the token-decoding trajectory (e.g., how many tokens to decode at each step). In contrast, MDMs are typically trained using a simplified, single-step BERT-style objective that masks a subset of tokens and predicts all of them simultaneously. This step-level simplification fundamentally disconnects the training paradigm from the trajectory-level nature of inference, leaving the inference schedules never optimized during training. In this paper, we introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process (MDP) that jointly incorporates both the model and the inference schedule. By applying Group Relative Policy Optimization at the trajectory level, Co-GRPO cooperatively optimizes model parameters and schedule parameters under a shared reward, without requiring costly backpropagation through the multi-step generation process. This holistic optimization aligns training with inference more thoroughly and substantially improves generation quality. Empirical results across four benchmarks-ImageReward, HPS, GenEval, and DPG-Bench-demonstrate the effectiveness of our approach. For more details, please refer to our project page: https://co-grpo.github.io/ .

</details>


### [67] [When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing](https://arxiv.org/abs/2512.22290)
*Arunkumar V,Nivethitha S,Sharan Srinivas,Gangadharan G. R*

Main category: cs.LG

TL;DR: 研究探讨了在算法承担管理角色的情况下，以人为核心的管理模式是否能够生存。通过对464名零工工作者的调查数据分析发现，支持性的人力资源实践可以提高员工福祉，但在当算法监管存在且难以解读时，这种做法与绩效之间的联系会减弱。只有当监管透明并可解释时，这种关系才会再次加强。此研究表明简单的线性模型可能无法捕捉到真实模式，并提出清晰的规则和可靠的补救措施可以使强监管变得可行。方法上，本文展示了如何使用双重机器学习来估计组织研究中的条件间接效应，而无需将数据强制转换为线性形式。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索在算法越来越多地承担管理职责的背景下，以人为本的管理方式能否继续有效。特别是关注于员工对算法系统的反应往往不是线性的这一现象，传统工具可能无法准确捕捉这些动态变化。

Method: 采用双重机器学习框架来估计一个调节中介模型，不施加限制性的函数形式。该方法基于464名零工工作者的调查数据进行分析。

Result: 研究发现了一种明显的非单调模式：支持性人力资源实践虽能改善员工福利，但其与工作表现之间的联系在算法监管模糊不清时会减弱；当监管变得透明且易于理解后，二者之间的正向关系又会增强。

Conclusion: 简单线性设定可能会忽略或错误解读实际存在的复杂模式。对于平台设计者而言，这意味着部分定义的控制会导致混乱，而明确的规则加上可信的追索机制则能使强有力的监管更加可行。从方法学角度来看，论文展示了双重机器学习技术在组织研究中用于评估条件间接效应而不必强迫数据符合线性形状的应用潜力。

Abstract: A central question for the future of work is whether person centered management can survive when algorithms take on managerial roles. Standard tools often miss what is happening because worker responses to algorithmic systems are rarely linear. We use a Double Machine Learning framework to estimate a moderated mediation model without imposing restrictive functional forms. Using survey data from 464 gig workers, we find a clear nonmonotonic pattern. Supportive HR practices improve worker wellbeing, but their link to performance weakens in a murky middle where algorithmic oversight is present yet hard to interpret. The relationship strengthens again when oversight is transparent and explainable. These results show why simple linear specifications can miss the pattern and sometimes suggest the opposite conclusion. For platform design, the message is practical: control that is only partly defined creates confusion, but clear rules and credible recourse can make strong oversight workable. Methodologically, the paper shows how Double Machine Learning can be used to estimate conditional indirect effects in organizational research without forcing the data into a linear shape.

</details>


### [68] [Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against](https://arxiv.org/abs/2512.22293)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 警告框架的内容在训练数据中并不能教会语言模型避免被警告的行为。实验显示，接收到警告的模型与直接接收内容的模型在复制标记内容的比率上没有统计学差异（76.7%对比83.3%）。研究发现这归因于描述某行为和执行该行为激活了重叠的潜在特征，且提示和推理时的引导无法解决这个问题；只有通过训练时的功能消融才能有所改善。


<details>
  <summary>Details</summary>
Motivation: 研究者想要探究是否通过警告框架（如“不要使用-此代码存在漏洞”）可以有效地教会语言模型避免特定不当行为。

Method: 通过实验比较了接触警告信息的语言模型与直接给予有问题内容的语言模型在复制有风险内容上的表现，并采用稀疏自动编码器分析来理解背后的原因。此外，还探讨了通过提示、推理时指导以及训练时功能消融等方法的效果。

Result: 结果表明，无论是给模型展示警告还是直接提供问题代码，两种情况下模型复制问题代码的比例没有显著差别。进一步分析揭示这是因为描述行为与执行行为之间存在潜在特征重叠的问题。

Conclusion: 基于统计共现性而非实际意图的学习方式主导了当前架构下的模型学习过程。这意味着模型更多地学会了在某个上下文后通常会跟随什么，而不是为什么这些内容会出现在那里。

Abstract: Warning-framed content in training data (e.g., "DO NOT USE - this code is vulnerable") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: "describing X" and "performing X" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call "stealth slip", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.

</details>


### [69] [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)
*Zehao Chen,Tianxiang Ai,Yifei Li,Gongxun Li,Yuyang Wei,Wang Zhou,Guanghui Li,Bin Yu,Zhijun Chen,Hailong Sun,Fuzhen Zhuang,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.LG

TL;DR: 本文提出了一种新的集成微调框架LLMBoost，通过利用大型语言模型（LLMs）的中间状态来提升性能。该方法包括跨模型注意力机制、链式训练范式以及近似并行推理范式三大创新点，并在常识推理和算术推理任务上证明了其有效性，同时减少了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的集成学习方法通常将模型视为黑盒处理，只结合输入或最终输出而忽略了模型内部丰富的表示形式及其相互作用。为了解决这一问题，作者提出了LLMBoost，旨在通过显式利用这些中间状态来打破这种障碍。

Method: LLMBoost框架包含三个主要创新：1) 跨模型注意力机制允许后续模型访问并融合前序模型的隐藏状态；2) 链式训练范式采用错误抑制目标逐步微调连接的模型；3) 近似并行推理范式设计层间管道传输隐藏状态以接近单模型解码效率。此外，还建立了理论基础，证明了在有限纠正假设下顺序集成可以保证单调改进。

Result: 实验结果表明，在常识推理和算数推理任务中，LLMBoost能够一致地提高准确性同时减少推理延迟。

Conclusion: 通过引入一种新颖的方法来利用LLMs中的中间状态，LLMBoost不仅增强了模型的整体性能，还在保持较低计算成本的情况下提高了准确性和效率。

Abstract: Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.

</details>


### [70] [Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making](https://arxiv.org/abs/2512.22313)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 本文提出了一种名为乐观可行搜索（OFS）的方法，用于在线学习带有一个维度阈值策略的闭环保策系统，并满足人口平等和服务率限制。OFS通过维护每个候选阈值的奖励和约束残差置信边界来工作，在每一轮中选择一个看起来可行且最大化乐观奖励的阈值。实验表明，OFS在合成与半合成基准测试中均表现出色，相比无约束和原始-对偶bandit基线，它能够实现更高的回报并减少累积约束违规。


<details>
  <summary>Details</summary>
Motivation: 决策系统（如贷款、筛选或再犯风险评估）在公平性和服务限制下运行时，会引发反馈效应：决策改变了未来出现的人群，导致数据非稳态并可能加剧差异性。研究者们希望开发一种方法，能够在这样的环境中进行在线学习，同时满足人口平等以及可选的服务率约束条件。

Method: 提出了乐观可行搜索(Optimistic Feasible Search, OFS)方法，该方法基于网格维持每个候选阈值的奖励及约束残差置信区间。每轮中，OFS会选择一个根据置信区间看似可行且能最大化乐观奖励的阈值；如果没有阈值看似可行，则选择最小化乐观约束违规的阈值。这种方法特别适用于低维度、可解释性强的政策类，其中离散化是自然的选择。

Result: OFS方法在(i)具有稳定收缩动力学特性的合成闭环保策基准测试，(ii)以德国信用评分和COMPAS为基础构建的两个半合成闭环保策基准测试上进行了评估。结果表明，在所有测试环境中，OFS相较于无约束和原始-对偶bandit基线能够获得更高收益的同时减小累计约束违反量，并且接近于使用相同扫描程序下的最佳可行固定阈值表现。

Conclusion: 乐观可行搜索(OFS)方法被证明是在满足人口平等及服务率约束条件下，对于在线学习一维阈值策略非常有效。实验显示OFS不仅提高了收益还减少了累积约束违规情况，显示出其相对于其他方法的优势。

Abstract: Closed-loop decision-making systems (e.g., lending, screening, or recidivism risk assessment) often operate under fairness and service constraints while inducing feedback effects: decisions change who appears in the future, yielding non-stationary data and potentially amplifying disparities. We study online learning of a one-dimensional threshold policy from bandit feedback under demographic parity (DP) and, optionally, service-rate constraints. The learner observes only a scalar score each round and selects a threshold; reward and constraint residuals are revealed only for the chosen threshold.
  We propose Optimistic Feasible Search (OFS), a simple grid-based method that maintains confidence bounds for reward and constraint residuals for each candidate threshold. At each round, OFS selects a threshold that appears feasible under confidence bounds and, among those, maximizes optimistic reward; if no threshold appears feasible, OFS selects the threshold minimizing optimistic constraint violation. This design directly targets feasible high-utility thresholds and is particularly effective for low-dimensional, interpretable policy classes where discretization is natural.
  We evaluate OFS on (i) a synthetic closed-loop benchmark with stable contraction dynamics and (ii) two semi-synthetic closed-loop benchmarks grounded in German Credit and COMPAS, constructed by training a score model and feeding group-dependent acceptance decisions back into population composition. Across all environments, OFS achieves higher reward with smaller cumulative constraint violation than unconstrained and primal-dual bandit baselines, and is near-oracle relative to the best feasible fixed threshold under the same sweep procedure. Experiments are reproducible and organized with double-blind-friendly relative outputs.

</details>


### [71] [LangPrecip: Language-Aware Multimodal Precipitation Nowcasting](https://arxiv.org/abs/2512.22317)
*Xudong Ling,Tianxi Huang,Qian Dong,Tao He,Chaorong Li,Guiduo Duan*

Main category: cs.LG

TL;DR: 提出了一种语言感知的多模态降水临近预报框架（LangPrecip），通过将气象文本作为降水演变的语义运动约束，改进了对快速演变和极端天气事件的预报准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法主要依赖于视觉条件，这使得未来运动的约束较弱且具有不确定性。为了提高短期降水预报尤其是快速演变和极端天气事件的准确性，提出了结合文本信息的新方法。

Method: 引入了LangPrecip框架，该框架将现在天气预报视为在Rectified Flow范式下的语义约束轨迹生成问题，实现了文本与雷达信息在潜在空间中的有效整合。此外，还发布了包含16万对雷达序列和运动描述的大规模多模态数据集LangPrecip-160k。

Result: 实验结果表明，在瑞典和MRMS数据集上，新方法相较于现有最先进方法在大雨CSI指标上取得了超过60%和19%的提升，特别是在80分钟的提前量下表现尤为突出。

Conclusion: 本研究提出的LangPrecip框架能够显著改善短期降水预测的效果，特别是在处理快速变化及极端天气情况时。

Abstract: Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \% and 19\% gains in heavy-rainfall CSI at an 80-minute lead time.

</details>


### [72] [Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough](https://arxiv.org/abs/2512.22318)
*Chorok Lee*

Main category: cs.LG

TL;DR: 本文指出了概率知识图谱嵌入的一个基本限制，即实体的不确定性是关系无关的，并提出了一种结合语义和结构不确定性的新方法CAGP，该方法在时间OOD检测上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的概率知识图谱嵌入技术中，实体的方差（用以量化认知不确定性）未能根据不同关系上下文调整，导致对新兴实体与新颖关系上下文这两种不同的分布外现象处理不佳。

Method: 通过形式化不确定性分解为互补部分：来自实体嵌入方差的语义不确定性（用于检测新兴实体）以及来自实体-关系共现的结构不确定性（用于检测新颖上下文）。基于此理论分析，提出了CAGP方法，它通过学习权重来结合这两种类型的不确定性。

Result: CAGP方法在多个基准测试的时间OOD检测任务上实现了0.94-0.99 AUROC的成绩，相对于关系无关基线提高了60-80%。此外，在选择性预测方面，当回答率为85%时，错误率减少了43%。

Conclusion: 研究证明了仅依赖于实体级别统计而不考虑关系上下文的任何不确定性估计器在新上下文中的OOD检测几乎随机；而将语义和结构不确定性信号结合起来的方法可以更有效地处理不同类型的分布外情况。

Abstract: Probabilistic knowledge graph embeddings represent entities as distributions, using learned variances to quantify epistemic uncertainty. We identify a fundamental limitation: these variances are relation-agnostic, meaning an entity receives identical uncertainty regardless of relational context. This conflates two distinct out-of-distribution phenomena that behave oppositely: emerging entities (rare, poorly-learned) and novel relational contexts (familiar entities in unobserved relationships). We prove an impossibility result: any uncertainty estimator using only entity-level statistics independent of relation context achieves near-random OOD detection on novel contexts. We empirically validate this on three datasets, finding 100 percent of novel-context triples have frequency-matched in-distribution counterparts. This explains why existing probabilistic methods achieve 0.99 AUROC on random corruptions but only 0.52-0.64 on temporal distribution shift. We formalize uncertainty decomposition into complementary components: semantic uncertainty from entity embedding variance (detecting emerging entities) and structural uncertainty from entity-relation co-occurrence (detecting novel contexts). Our main theoretical result proves these signals are non-redundant, and that any convex combination strictly dominates either signal alone. Our method (CAGP) combines semantic and structural uncertainty via learned weights, achieving 0.94-0.99 AUROC on temporal OOD detection across multiple benchmarks, a 60-80 percent relative improvement over relation-agnostic baselines. Empirical validation confirms complete frequency overlap on three datasets (FB15k-237, WN18RR, YAGO3-10). On selective prediction, our method reduces errors by 43 percent at 85 percent answer rate.

</details>


### [73] [Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers](https://arxiv.org/abs/2512.22326)
*Sravan Karthick T*

Main category: cs.LG

TL;DR: 本文通过将来自18个主要经济体的全球M2流动性作为外生变量，并使用TimeXer架构，提出了一种基于流动性的预测模型（TimeXer-Exog），在对比包括LSTM、N-BEATS等在内的多种基准模型后，发现该模型在长期比特币价格预测上表现更优，特别是在70天的预测范围内，其均方误差比单变量TimeXer基线模型低超过89%。


<details>
  <summary>Details</summary>
Motivation: 比特币价格预测因其极端波动性和非平稳性而对传统单变量时间序列模型构成了挑战，尤其是在长预测区间内。本文旨在通过引入全球经济体的M2流动性作为一个具有12周滞后结构的领先外生变量来填补这一研究空白，以期提高比特币价格的长期预测准确性。

Method: 采用名为TimeXer的架构，构建了一个基于流动性条件的预测模型（TimeXer-Exog），并与包括LSTM、N-BEATS、PatchTST及标准单变量TimeXer在内的最先进基准进行了比较。研究利用了从2020年1月至2025年8月的每日比特币价格数据进行实验。

Result: 实验证明，在70天的预测范围内，提出的TimeXer-Exog模型达到了1.08e8的均方误差(MSE)，相较于单变量TimeXer基线模型提高了超过89%的表现。结果表明，基于全球流动性的深度学习模型条件化对于改善比特币价格长期预测有显著效果。

Conclusion: 通过将宏观经济状况特别是全球M2流动性纳入考虑范围，可以显著稳定并提高比特币价格的长期预测性能。这为未来开发更加准确可靠的加密货币价格预测模型提供了新思路。

Abstract: Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.

</details>


### [74] [The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2512.22337)
*Matthew Riemer,Erik Miehling,Miao Liu,Djallel Bouneffouf,Murray Campbell*

Main category: cs.LG

TL;DR: 尽管LoRA等参数高效微调方法仅修改一小部分参数，但它们可能严重影响模型性能。研究发现，基于LoRA的监督微调即使在小数据集上训练也会显著降低模型能力；然而，通过引入正则化近似重放方法，并结合来自与预训练相似但不同的开放访问语料库的数据进行下一个令牌预测，可以几乎消除这个问题，同时保持模型的一般知识和对新任务的适应性，且仅增加少量计算开销。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决使用如LoRA这样的参数高效微调技术时遇到的问题，特别是当这些技术被应用于指令调整实验中可能会导致模型能力灾难性下降的情况。

Method: 提出了一种正则化近似重放方法，该方法通过对初始模型KL散度进行惩罚，并插入来自与预训练所用数据集相似但不同的开放访问语料库的数据来进行下一个令牌预测，以改善基于LoRA的监督微调表现。

Result: 实验表明，所提出的方法能够有效防止模型能力的大幅下降，同时保持了模型对于新任务的学习灵活性，仅需承担适度的额外计算成本。

Conclusion: 通过简单的调整训练过程，可以极大地缓解甚至消除基于LoRA的微调给模型带来的负面影响，从而实现模型性能的有效维护。

Abstract: Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.

</details>


### [75] [Causality-Inspired Safe Residual Correction for Multivariate Time Series](https://arxiv.org/abs/2512.22428)
*Jianxiang Xie,Yuncheng Hua*

Main category: cs.LG

TL;DR: 提出了一种名为CRC（因果启发的安全残差校正）的框架，旨在解决多变量预测器在部署时性能下降的问题。通过解耦自我和交叉变量动态，并采用严格的四重安全机制来防止有害更新，实验证明CRC能够提高准确性并确保非常高的非退化率。


<details>
  <summary>Details</summary>
Motivation: 现有的后处理残差校正方法虽然可以提高平均准确度，但可能会过度纠正可靠的预测，导致在未见过的情景中出现局部失败。因此，需要一种既能修正系统性错误又不会导致性能下降的新方法。

Method: CRC框架结合了因果启发的编码器来揭示方向感知结构，以及一个混合校正器来建模残差误差。该框架设计了一个四重安全保障机制以防止有害更新。

Result: 实验结果表明，CRC不仅提高了预测的准确性，而且通过其核心安全机制保证了极高的非退化率，适用于安全可靠的部署。

Conclusion: CRC提供了一种有效且安全的方法来改进现代多变量预报器的表现，特别是在避免因过度校正而导致的性能下降方面表现突出。

Abstract: While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also "help in the wrong way" by overcorrecting reliable predictions and causing local failures in unseen scenarios.
  To address this critical "safety gap," we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.
  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.

</details>


### [76] [AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing](https://arxiv.org/abs/2512.22455)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Feiye Huo,Yerui Sun,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出了AFA-LoRA，一种新的训练策略，通过引入退火激活函数来增强LoRA的非线性表达能力，同时保持其无缝合并特性。实验结果表明，AFA-LoRA缩小了LoRA与全参数训练之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前流行的低秩适应（LoRA）方法由于其线性的适应过程限制了表达能力，存在线性训练与非线性训练之间表达力的差距。

Method: 开发了一种名为AFA-LoRA的新训练策略，该策略采用了一个退火激活函数，能够在训练过程中从非线性转换为线性变换，从而让适配器在初期拥有更强的表现力，并最终收敛到一个可合并的线性形式。

Result: AFA-LoRA在监督微调、强化学习和推测解码上的应用结果显示，它有效减少了LoRA与全参数训练之间的性能差异。

Conclusion: 这项工作提供了一种更加强大且实用的参数高效适应范式，通过增加非线性表达能力的同时保持了LoRA原有的优势。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.

</details>


### [77] [AMBIT: Augmenting Mobility Baselines with Interpretable Trees](https://arxiv.org/abs/2512.22466)
*Qizhi Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为AMBIT的灰盒框架，该框架通过可解释的树模型增强了物理移动基线。研究首先对经典空间交互模型进行了全面审计，并基于此构建了基于梯度提升树和SHAP分析的残差学习器，证明了这些方法在保持可解释性的同时接近强树型预测器的准确性。


<details>
  <summary>Details</summary>
Motivation: 起讫点（OD）流预测是GIS和城市分析中的核心任务，但在实际部署中面临着高准确性和清晰可解释性的矛盾需求。为了同时满足这两个方面的要求，本文开发了AMBIT框架，旨在提高预测精度的同时不牺牲结果的可解释性。

Method: 1. 对一系列经典的空间交互模型在纽约市长达一年、每小时更新的出租车OD数据集上进行评估。
2. 选定PPML引力模型作为最强的物理基线，并使用全OD边界校准约束变体以改善其表现。
3. 在物理基线上利用梯度提升树建立残差学习器，并结合SHAP分析来增强模型的可解释性与性能。

Result: 1. 大多数物理模型在此时间分辨率下表现出脆弱性；PPML引力模型是最强大的物理基线，而经过完全OD边界校准的约束版本有所改进但仍较弱。
2. 结合物理基础的残差学习器不仅能够达到强大树模型预测器的准确性水平，而且还保留了易于理解的结构。
3. POI锚定残差在整个空间泛化过程中始终具有竞争力且最为稳健。

Conclusion: 通过将物理移动基线与可解释的树模型相结合，AMBIT框架为OD流量预测提供了一个既能保证较高准确性又能维持良好可解释性的解决方案。此外，本研究还提供了可重复使用的管道、丰富的诊断工具以及面向城市决策制定的空间误差分析。

Abstract: Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.

</details>


### [78] [Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting](https://arxiv.org/abs/2512.22478)
*Chuantao Li,Zhi Li,Jiahao Xu,Jie Li,Sheng Li*

Main category: cs.LG

TL;DR: 本文提出了一种多类不平衡学习的协同优化Boosting模型，通过集成密度因子和置信因子设计了抗噪权重更新机制和动态采样策略，实现了不平衡学习与模型训练的协同优化。实验表明该模型在20个公共不平衡数据集上显著优于八种最先进基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未探索不平衡学习与模型训练之间的协同优化问题，这限制了性能的进一步提升。

Method: 提出了一种结合密度因子和置信因子的多类不平衡学习协同优化Boosting模型，包括抗噪声权重更新机制和动态采样策略的设计。

Result: 在20个公开的不平衡数据集上的广泛实验表明，所提出的模型显著优于八种最先进的基线方法。

Conclusion: 通过提出一种新的Boosting模型，本研究成功地实现了不平衡学习与模型训练之间的协同优化，从而提高了处理不平衡数据集时的分类性能。

Abstract: Numerous studies attempt to mitigate classification bias caused by class imbalance. However, existing studies have yet to explore the collaborative optimization of imbalanced learning and model training. This constraint hinders further performance improvements. To bridge this gap, this study proposes a collaborative optimization Boosting model of multiclass imbalanced learning. This model is simple but effective by integrating the density factor and the confidence factor, this study designs a noise-resistant weight update mechanism and a dynamic sampling strategy. Rather than functioning as independent components, these modules are tightly integrated to orchestrate weight updates, sample region partitioning, and region-guided sampling. Thus, this study achieves the collaborative optimization of imbalanced learning and model training. Extensive experiments on 20 public imbalanced datasets demonstrate that the proposed model significantly outperforms eight state-of-the-art baselines. The code for the proposed model is available at: https://github.com/ChuantaoLi/DARG.

</details>


### [79] [Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals](https://arxiv.org/abs/2512.22508)
*Lucky Susanto,Anasta Pranawijayana,Cortino Sukotjo,Soni Prasad,Derry Wijaya*

Main category: cs.LG

TL;DR: 研究探索了通过分析元数据和幻觉信号来预测大型语言模型在义齿修复学考试中答案正确性的可行性，发现基于元数据的方法可以提高准确性，并且提示策略对模型内部行为及元数据的预测价值有显著影响。


<details>
  <summary>Details</summary>
Motivation: 由于在医疗保健和医学教育等高风险领域中使用大型语言模型时存在生成错误信息的风险，因此研究如何预测这些模型回答的正确性变得至关重要。

Method: 该研究使用了一种通用模型（GPT-4o）和一种以推理为中心的模型（OSS-120B），在多选题义齿修复学考试上进行实验。通过分析三种不同提示策略下的元数据和幻觉信号构建正确性预测器。

Result: 基于元数据的方法能够将准确性提高至多7.14%，并达到83.12%的精确度。虽然实际发生的幻觉是不正确的强烈指标，但仅凭元数据信号无法可靠地预测幻觉。此外，尽管提示策略不影响总体准确性，但它们显著改变了模型内部的行为以及元数据的预测效用。

Conclusion: 本研究表明，利用元数据分析来开发语言模型的可靠性信号是有希望的方向；然而，当前所探讨的方法还不足以应对关键、高风险的应用部署。

Abstract: Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.

</details>


### [80] [TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting](https://arxiv.org/abs/2512.22550)
*Jaebin Lee,Hankook Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为TimePerceiver的统一编码-解码预测框架，它与有效的训练策略紧密结合。该模型能够处理多样化的时序预测目标，并且在广泛的基准数据集上显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测中，先前的工作主要集中在编码器设计上，而往往将预测和训练视为独立或次要的问题。为了弥补这一不足，作者提出了一个同时考虑输入编码、预测（即解码）以及模型训练的整体框架。

Method: 首先，将预测任务泛化为包含多种时序预测目标，如外推、内插和填充。为此设计了一个新颖的编码-解码架构，能够灵活地感知并适应沿时间轴任意位置的输入和目标段落。编码部分引入了一组潜在瓶颈表示，可以与所有输入段交互以共同捕捉时序和跨通道依赖关系；解码部分则利用对应于目标时间戳的学习查询来有效检索相关信息。

Result: 大量实验表明，在广泛的数据集范围内，所提出的框架始终且显著优于之前的最先进基线模型。

Conclusion: TimePerceiver作为一个统一的时间序列预测框架，在处理多样化预测任务时表现出色，代表了当前最先进的技术水平。

Abstract: In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.

</details>


### [81] [On Admissible Rank-based Input Normalization Operators](https://arxiv.org/abs/2512.22587)
*Taeyun Kim*

Main category: cs.LG

TL;DR: 本文探讨了基于排名的输入归一化方法在现代机器学习中的重要性，指出广泛使用的可微排序和排名操作符在严格单调变换、小批量组成变化及微小输入扰动下存在内在不稳定性。为此，作者提出了三个公理来形式化基于排名的输入归一化所需的最小不变性和稳定性属性，并构建了一个满足这些条件的最小操作符，从而明确区分了有效的基于排名的归一化操作符与现有的基于连续松弛的排序方法的设计空间。


<details>
  <summary>Details</summary>
Motivation: 研究发现现有广泛应用的不同可微排序和排名操作符在面对严格单调变换、小批量样本构成变动以及极小输入扰动时表现出固有的不稳定性。这种不稳定性源自于这些操作符的设计结构而非具体实现选择。为解决这一问题，文章旨在确立一套能够确保基于排名的输入归一化过程稳定性的基本原则。

Method: 通过提出并证明三个公理，定义了基于排名的输入归一化所必需的最小不变性和稳定性属性。进一步地，依据这些理论框架构造了一个同时满足特征级排名表示与单调且Lipschitz连续标量化映射要求的操作符。

Result: 研究表明，任何符合所提公理的操作符都必须分解为特征级排名表示和一个既单调又Lipschitz连续的标量化映射两部分。实验结果表明，在实际场景下，由此产生的约束条件是有效且非平凡的。此外，还清晰界定了有效基于排名归一化算子的设计范围，并将其与现存基于连续松弛的排序方法区分开来。

Conclusion: 该工作通过确立关于基于排名的输入归一化的三个关键公理，不仅揭示了现有技术存在的根本缺陷，同时也为开发更加鲁棒可靠的归一化算法提供了理论基础。

Abstract: Rank-based input normalization is a workhorse of modern machine learning, prized for its robustness to scale, monotone transformations, and batch-to-batch variation. In many real systems, the ordering of feature values matters far more than their raw magnitudes - yet the structural conditions that a rank-based normalization operator must satisfy to remain stable under these invariances have never been formally pinned down.
  We show that widely used differentiable sorting and ranking operators fundamentally fail these criteria. Because they rely on value gaps and batch-level pairwise interactions, they are intrinsically unstable under strictly monotone transformations, shifts in mini-batch composition, and even tiny input perturbations. Crucially, these failures stem from the operators' structural design, not from incidental implementation choices.
  To address this, we propose three axioms that formalize the minimal invariance and stability properties required of rank-based input normalization. We prove that any operator satisfying these axioms must factor into (i) a feature-wise rank representation and (ii) a scalarization map that is both monotone and Lipschitz-continuous. We then construct a minimal operator that meets these criteria and empirically show that the resulting constraints are non-trivial in realistic setups. Together, our results sharply delineate the design space of valid rank-based normalization operators and formally separate them from existing continuous-relaxation-based sorting methods.

</details>


### [82] [Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining](https://arxiv.org/abs/2512.22589)
*Jewel Rana Palit,Vijayalakshmi K Kumarasamy,Osama A. Osman*

Main category: cs.LG

TL;DR: 本研究分析了来自NHTSA的2500多条自动驾驶车辆（AV）事故记录，通过K-means聚类和关联规则挖掘揭示了不同自动化水平下的事故动态与成因之间的关系，为开发者、安全监管机构及政策制定者提供了减少事故风险的指导。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车在混合交通环境中的安全性和操作可靠性问题日益凸显，现有研究主要依赖于规模较小且以加州为中心的数据集，并且对不同SAE自动化级别下的事故趋势理解有限。因此，需要一个更全面的研究来探索这些事故背后的动态因素。

Method: 采用了一个两阶段的数据挖掘框架：首先使用K-means聚类算法基于时间、空间和环境因素将事故记录分为4个不同的行为集群；然后，在每个集群内应用关联规则挖掘(ARM)技术提取可解释的多变量关系，包括照明条件、路面状况、车辆动态以及环境条件等与事故模式的关系。

Result: 研究成功地识别出了四个具有独特特性的事故行为集群，并通过关联规则挖掘发现了影响事故发生的多个关键因素及其相互间的关系。

Conclusion: 这项研究的结果为自动驾驶车辆开发商、安全监管机构和决策者提供了重要的见解，有助于制定更加有效的策略来降低事故风险并促进自动驾驶技术的安全部署。

Abstract: Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.

</details>


### [83] [Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification](https://arxiv.org/abs/2512.22597)
*Guikun Xu,Xiaohan Yi,Peilin Zhao,Yatao Bian*

Main category: cs.LG

TL;DR: 本文介绍了一种名为EnFlow的新框架，它结合了流匹配和显式学习的能量模型，通过能量引导的采样方案来生成低能量构象集合，并识别分子图中的基态构象。实验表明，EnFlow在生成度量方面有所改进，并且与现有方法相比减少了基态预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的方法在生成多样化的构象时缺乏可靠的能量校准，而确定性预测器则只能针对单一结构，无法表示构象集合的变异性。因此，需要一种能够同时提高构象保真度并准确识别基态构象的新方法。

Method: EnFlow框架将流匹配（FM）与显式学习的能量模型相结合，定义了一个沿非高斯FM路径的能量引导采样方案。通过在采样过程中加入能量梯度指导，该方法可以将轨迹导向更低能量区域，特别是在少数步骤的情况下显著提高了构象保真度。

Result: 广泛的实验表明，在GEOM-QM9和GEOM-Drugs数据集上，EnFlow仅用1-2步ODE即可改善生成度量，并且与最先进方法相比，降低了基态预测错误。

Conclusion: EnFlow提供了一种统一的方法来生成低能量构象集合，并通过能量引导采样方案提高了基态构象识别的准确性。

Abstract: Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.

</details>


### [84] [Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback](https://arxiv.org/abs/2512.22623)
*Tomas Ortega,Chun-Yin Huang,Xiaoxiao Li,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 本文提出两种新的框架，即压缩聚合反馈（CAFe）和服务器引导的压缩聚合反馈（CAFe-S），以实现无客户端状态或控制变量的偏置压缩技术。这些方法在联邦学习中能够有效减少通信开销，并且在非凸设定下优于现有的分布式压缩梯度下降算法。实验结果证明了所提方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 分布式学习特别是联邦学习，在客户端到服务器的上行传输过程中面临着显著的通信成本瓶颈，这通常受到边缘不对称带宽限制的影响。虽然偏置压缩技术在实践中有效，但需要错误反馈机制来提供理论保证并确保在激进压缩情况下的收敛性。然而，标准的错误反馈依赖于特定于客户端的控制变量，这不仅侵犯用户隐私，而且与大规模联邦学习中常见的无状态客户端不兼容。

Method: 文章提出了两种新框架：1. 压缩聚合反馈（Compressed Aggregate Feedback, CAFe），它使用前一轮全局聚合更新作为所有客户端共享的控制变量；2. 服务器引导的压缩聚合反馈（Server-Guided Compressed Aggregate Feedback, CAFe-S），当服务器拥有小规模私有数据集时，该框架通过生成一个服务器引导候选更新来充当更精确的预测器。采用分布式梯度下降(DGD)作为代表算法进行分析。

Result: 从理论上证明了在非凸设定及有界梯度差异条件下，CAFe相较于带有偏置压缩的分布式压缩梯度下降(DCGD)具有优越性。此外还证明了随着服务器数据代表性增加，CAFe-S能收敛至一个稳定点且收敛速度得到提升。实际联邦学习场景中的实验结果验证了所提出方法相比现有压缩方案的优势。

Conclusion: 提出的CAFe和CAFe-S框架为解决联邦学习中因通信成本高而产生的问题提供了有效的解决方案，同时保护了用户隐私。这两种方法不需要客户端存储任何状态信息或使用个人控制变量即可工作，从而提高了联邦学习系统的效率与实用性。

Abstract: Distributed learning, particularly Federated Learning (FL), faces a significant bottleneck in the communication cost, particularly the uplink transmission of client-to-server updates, which is often constrained by asymmetric bandwidth limits at the edge. Biased compression techniques are effective in practice, but require error feedback mechanisms to provide theoretical guarantees and to ensure convergence when compression is aggressive. Standard error feedback, however, relies on client-specific control variates, which violates user privacy and is incompatible with stateless clients common in large-scale FL. This paper proposes two novel frameworks that enable biased compression without client-side state or control variates. The first, Compressed Aggregate Feedback (CAFe), uses the globally aggregated update from the previous round as a shared control variate for all clients. The second, Server-Guided Compressed Aggregate Feedback (CAFe-S), extends this idea to scenarios where the server possesses a small private dataset; it generates a server-guided candidate update to be used as a more accurate predictor. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. We further prove that CAFe-S converges to a stationary point, with a rate that improves as the server's data become more representative. Experimental results in FL scenarios validate the superiority of our approaches over existing compression schemes.

</details>


### [85] [What Matters in Deep Learning for Time Series Forecasting?](https://arxiv.org/abs/2512.22702)
*Valentina Moretti,Andrea Cini,Ivan Marisca,Cesare Alippi*

Main category: cs.LG

TL;DR: 本文探讨了深度学习时间序列预测模型的设计原则及其对性能的影响，强调了局部性和全局性等概念的重要性，并指出简单的、设计良好的预测架构通常可以达到最先进的水平。同时提出了一种辅助的预测模型卡片，帮助基于关键设计选择来表征现有的和新的预测架构。


<details>
  <summary>Details</summary>
Motivation: 面对大量新提出的深度学习架构以及经常相互矛盾的经验结果，评估哪些组件对最终性能有显著贡献变得困难。文章旨在通过讨论设计维度和权衡来理解当前的时间序列预测深度学习架构设计空间，从而解释观察到的结果。

Method: 分析现有深度学习架构中的设计原则，特别是局部性和全局性的应用；评估这些方面对于实现准确结果的重要性；讨论现有架构中被忽视的实现细节如何根本改变预测方法类别并严重影响观察到的经验结果；提出一种辅助预报模型卡片以根据关键设计选择来表征模型。

Result: 发现考虑局部性和全局性等因素比采用特定序列建模层更为重要；简单而设计良好的预测架构往往能够匹敌最先进水平；提出了用于更好地理解和描述预报架构的辅助预报模型卡片。

Conclusion: 需要重新思考当前存在问题的基准测试实践，并且在设计架构时应专注于预测问题的基础方面。

Abstract: Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. This paper discusses the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and the need to focus on the foundational aspects of the forecasting problem when designing architectures. As a step in this direction, we propose an auxiliary forecasting model card, whose fields serve to characterize existing and new forecasting architectures based on key design choices.

</details>


### [86] [When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction](https://arxiv.org/abs/2512.22740)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: 研究通过54,028个合金样本测试多任务学习（MTL）在同时预测电导率、维氏硬度和非晶形成能力方面的效果。结果表明，MTL显著降低了回归性能但提高了分类性能。建议对需要精确回归的任务使用独立模型，而对召回率至关重要的分类任务则可以采用MTL。


<details>
  <summary>Details</summary>
Motivation: 探索多任务学习是否能够利用材料属性之间共享的基础物理原理来改善预测表现。

Method: 使用了54,028个合金样本进行实验，比较了单一任务模型与标准及结构化的多任务学习方法之间的差异。

Result: 对于回归任务，如预测电导率和维氏硬度，MTL的表现显著下降；而对于分类任务，比如非晶形成能力的预测，MTL则表现出了一定的优势。进一步分析发现，不同任务间权重接近零，表明这些材料属性相对独立。

Conclusion: 多任务学习并不适合所有类型的预测任务，特别是当面对数据极度不平衡时可能导致负面迁移效应从而降低回归性能。因此推荐针对需要高精度回归的情况开发独立模型，而在重视召回率的分类场景中考虑使用MTL。

Abstract: Multi-task learning (MTL) assumes related material properties share underlying physics that can be leveraged for better predictions. We test this by simultaneously predicting electrical resistivity, Vickers hardness, and amorphous-forming ability using 54,028 alloy samples. We compare single-task models against standard and structured MTL. Results reveal a striking dichotomy: MTL significantly degrades regression performance (resistivity $R^2$: 0.897 $\to$ 0.844; hardness $R^2$: 0.832 $\to$ 0.694, $p < 0.01$) but improves classification (amorphous F1: 0.703 $\to$ 0.744, $p < 0.05$; recall +17%). Analysis shows near-zero inter-task weights, indicating property independence. Regression failure is attributed to negative transfer caused by severe data imbalance (52k vs. 800 samples). We recommend independent models for precise regression, while reserving MTL for classification tasks where recall is critical.

</details>


### [87] [A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants](https://arxiv.org/abs/2512.22758)
*Eswarasanthosh Kumar Mamillapalli,Nishtha Sharma*

Main category: cs.LG

TL;DR: 本研究提出了一种微宏观机器学习框架，结合个体层面的数据和宏观环境因素来预测儿童肥胖问题。XGBoost模型表现最佳，并构建了一个综合环境脆弱性指数(EnvScore)，揭示了环境负担高的州与全国预测的个体层面肥胖风险分布之间存在显著地理相似性。该工作为公共卫生信息学提供了可扩展的多层次建模流程，具有扩展到因果建模、干预规划和实时分析的强大潜力。


<details>
  <summary>Details</summary>
Motivation: 鉴于传统流行病学研究通常独立分析个体层面、家庭层面及环境层面的风险因素，限制了对结构性环境条件如何与个体特征相互作用以影响健康结果的理解。

Method: 采用一种微宏观机器学习框架，整合了NHANES提供的个体层面人体测量和经济社会数据以及从USDA和EPA数据集中提取的食物获取、空气质量和社会经济脆弱性等宏观层次结构环境特征。训练了包括逻辑回归、随机森林、XGBoost和LightGBM在内的四种机器学习模型来预测肥胖情况。

Result: XGBoost模型在预测肥胖方面表现最好。此外，还构建了一个基于USDA和EPA指标标准化后的复合环境脆弱性指数（EnvScore）。多级比较显示，在环境负担较重的州与全国范围内预测的个体层面肥胖风险分布之间存在着很强的地理相似性。

Conclusion: 这项研究表明，通过整合多尺度数据集来识别由环境驱动的肥胖风险差异是可行的。它为公共卫生信息学贡献了一个可扩展的数据驱动型多层次建模流程，展示出向因果建模、干预计划制定及实时数据分析扩展的强大潜力。

Abstract: Childhood obesity remains a major public health challenge in the United States, strongly influenced by a combination of individual-level, household-level, and environmental-level risk factors. Traditional epidemiological studies typically analyze these levels independently, limiting insights into how structural environmental conditions interact with individual-level characteristics to influence health outcomes. In this study, we introduce a micro-macro machine learning framework that integrates (1) individual-level anthropometric and socioeconomic data from NHANES and (2) macro-level structural environment features, including food access, air quality, and socioeconomic vulnerability extracted from USDA and EPA datasets. Four machine learning models Logistic Regression, Random Forest, XGBoost, and LightGBM were trained to predict obesity using NHANES microdata. XGBoost achieved the strongest performance. A composite environmental vulnerability index (EnvScore) was constructed using normalized indicators from USDA and EPA at the state level. Multi-level comparison revealed strong geographic similarity between states with high environmental burden and the nationally predicted micro-level obesity risk distribution. This demonstrates the feasibility of integrating multi-scale datasets to identify environment-driven disparities in obesity risk. This work contributes a scalable, data-driven, multi-level modeling pipeline suitable for public health informatics, demonstrating strong potential for expansion into causal modeling, intervention planning, and real-time analytics.

</details>


### [88] [Understanding the Mechanisms of Fast Hyperparameter Transfer](https://arxiv.org/abs/2512.22768)
*Nikhil Ghosh,Denny Wu,Alberto Bietti*

Main category: cs.LG

TL;DR: 本文探讨了深度学习模型中如何通过规模感知超参数实现从小规模到大规模模型的高效迁移，并提出了一个概念框架来理解这种跨规模迁移。研究发现，快速迁移等同于有用的迁移，对于计算最优网格搜索来说更有效率。同时，文章还提出了一种假设，认为优化轨迹可以分解为宽度稳定和宽度敏感两个部分，以解释实际观察到的快速迁移现象。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模的增长，标准超参数优化变得异常昂贵。因此，需要找到一种方法能够直接将小规模模型上找到的最佳超参数迁移到大规模模型上，且保证性能损失最小。

Method: 开发了一个通用的概念框架来推理跨规模的超参数迁移，定义了当由迁移引起的次优性消失速度比有限规模下的性能差距更快时，则该迁移被认为是快速的。此外，还通过合成设置展示了在某些情况下使用Maximal Update Parameterization（μP）进行迁移可提供明显的计算优势，而在其他情况下则不然。最后，提出了关于优化轨迹分解的假设，并提供了实证证据支持这一观点。

Result: 证明了对于计算最优网格搜索而言，快速迁移与有用迁移是等价的，这意味着迁移在渐近意义上比直接调整更为计算高效。并且，针对实践中观察到的快速迁移现象，研究者提出的假设得到了不同场景下包括大型语言模型预训练在内的实证支持。

Conclusion: 本文的研究加深了对超参数迁移策略背后原理的理解，特别是关于如何利用规模感知超参数有效地从较小规模转移到较大大规模模型上。这不仅有助于减少大模型训练中的计算成本，也为未来进一步优化超参数选择提供了新的视角。

Abstract: The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($μ$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $μ$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.

</details>


### [89] [Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization](https://arxiv.org/abs/2512.22774)
*Truong Son Nguyen*

Main category: cs.LG

TL;DR: 本文介绍了一种受量子力学启发的统一机器学习框架Schrödinger AI，它由三个紧密耦合的部分组成，并展示了在语义流形出现、动态推理适应环境变化以及模算术任务上的操作泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受到量子力学的启发，旨在提供一种基于物理原理的方法来替代传统的交叉熵训练和变压器注意力机制，从而实现鲁棒的泛化性、可解释的语义以及涌现的拓扑结构。

Method: 提出了一个名为Schrödinger AI的系统，该系统包括：时间无关波能求解器用于感知与分类；时间相关动力学求解器管理语义波函数随时间演化；低秩算子微积分通过学习类似量子的转换算子来执行符号变换。

Result: Schrödinger AI显示了没有明确监督下反映人类概念类别关系的新兴语义流形；能够根据环境变化进行调整的动态推理能力；以及在模算术任务上远超训练长度的操作精确泛化。

Conclusion: 结果表明，Schrödinger AI为机器学习开辟了一个新的基础方向，即把学习视为发现并导航潜在的语义能量景观的过程。

Abstract: We introduce \textbf{Schrödinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology.
  Empirically, Schrödinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.

</details>


### [90] [Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning](https://arxiv.org/abs/2512.22777)
*Kasra Jalaldoust,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文基于因果可迁移性理论设计了一种零样本组合泛化算法Circuit-TR，该算法利用因果图和领域间机制共享的差异预言器来学习源数据中的模块，并在目标领域中进行预测。此外，还提出了一个无需显式因果结构的有监督领域适应方案，并通过有限的目标数据来操作。


<details>
  <summary>Details</summary>
Motivation: 跨领域的泛化只有在对未见过的目标领域相对于源领域施加某种结构约束的情况下才有可能实现。因此，研究旨在通过引入因果关系图以及领域间机制共享信息，开发一种能够实现零样本组合泛化能力的新方法。

Method: 提出了一种名为Circuit-TR的算法，它依赖于以因果图形式存在的定性领域知识和用于领域间机制共享的差异预言器。该算法首先从源数据中学习一组模块（即局部预测器），然后如果因果结构允许的话，将这些模块迁移到目标域并组成一个新的电路来进行预测。

Result: 研究表明，通过电路可迁移性可以为少量样本学习任务提供新的理解视角，并且与已有的电路大小复杂度概念建立了联系。控制模拟实验也证实了理论结果的有效性。

Conclusion: Circuit-TR算法及其相关理论为处理少量样本学习问题提供了新的途径，同时揭示了少量样本泛化能力和电路规模复杂度之间的联系。

Abstract: Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.

</details>


### [91] [SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance](https://arxiv.org/abs/2512.22792)
*Shuai Chen,Chen Wang,Ziran Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为SNM-Net的通用深度学习框架，用于解决电子鼻系统在开放集气体识别中遇到的问题。通过几何解耦机制和马氏距离评分机制，该框架能够适应不同架构，并在Vergara数据集上展示了接近理论最优的表现。


<details>
  <summary>Details</summary>
Motivation: 电子鼻系统在开放集气体识别中面临特征分布偏移和未知干扰导致的决策失败问题。现有方法主要依赖欧几里得距离，未能充分考虑各向异性气体特征分布及信号强度变化。

Method: 提出了SNM-Net，一种包含级联批归一化与L2归一化的几何解耦机制，将高维特征映射到单位超球面上以消除信号强度波动；引入了基于类别统计信息构建自适应椭球决策边界的马氏距离作为评分机制。

Result: 实验表明，在Vergara数据集上，Transformer+SNM配置达到了几乎理论最优性能，AUROC为0.9977，未知气体检测率为99.57%（FPR 5%时TPR）。相比Class Anchor Clustering方法，AUROC提高了3.0%，标准差降低了91.0%。

Conclusion: SNM-Net框架有效解决了精度与稳定性之间的权衡问题，为工业电子鼻部署提供了坚实的技术基础。

Abstract: Electronic nose (E-nose) systems face dual challenges in open-set gas recognition: feature distribution shifts caused by signal drift and decision failures induced by unknown interference. Existing methods predominantly rely on Euclidean distance, failing to adequately account for anisotropic gas feature distributions and dynamic signal intensity variations. To address these issues, this study proposes SNM-Net, a universal deep learning framework for open-set gas recognition. The core innovation lies in a geometric decoupling mechanism achieved through cascaded batch normalization and L2 normalization, which projects high-dimensional features onto a unit hypersphere to eliminate signal intensity fluctuations. Additionally, Mahalanobis distance is introduced as the scoring mechanism, utilizing class-wise statistics to construct adaptive ellipsoidal decision boundaries. SNM-Net is architecture-agnostic and seamlessly integrates with CNN, RNN, and Transformer backbones. Systematic experiments on the Vergara dataset demonstrate that the Transformer+SNM configuration attains near-theoretical performance, achieving an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). This performance significantly outperforms state-of-the-art methods, showing a 3.0% improvement in AUROC and a 91.0% reduction in standard deviation compared to Class Anchor Clustering. The framework exhibits exceptional robustness across sensor positions with standard deviations below 0.0028. This work effectively resolves the trade-off between accuracy and stability, providing a solid technical foundation for industrial E-nose deployment.

</details>


### [92] [ReDiF: Reinforced Distillation for Few Step Diffusion](https://arxiv.org/abs/2512.22802)
*Amirhossein Tighkhorshid,Zahra Dehghanian,Gholamali Aminian,Chengchun Shi,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的扩散模型蒸馏框架，通过将学生模型训练视为一个策略优化问题，利用从与教师模型输出对齐中得出的奖励信号进行训练。这种方法允许学生探索多条去噪路径，并以更少的推理步骤和计算资源实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型采样速度慢的问题，通过创建较小尺寸或较少步骤的模型来近似高步数教师的行为。

Method: 提出一种基于强化学习的蒸馏框架，将蒸馏过程视为策略优化问题，学生模型根据与教师模型输出对齐产生的奖励信号进行训练。该方法让学生能够探索多条去噪路径，并向数据分布的高概率区域采取更长、更优化的步伐。

Result: 实验结果表明，与现有的蒸馏技术相比，该方法能够以显著减少的推理步骤和计算资源达到更好的性能。

Conclusion: 本框架为高效扩散学习提供了一个通用的优化范式，适用于任何类型的扩散模型以及合适的奖励函数。

Abstract: Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.

</details>


### [93] [MoR: Mixture Of Representations For Mixed-Precision Training](https://arxiv.org/abs/2512.22804)
*Bor-Yiing Su,Peter Dykas,Mike Chrzanowski,Jatin Chhugani*

Main category: cs.LG

TL;DR: 本文介绍了一种新的量化框架MoR，该框架能够根据张量的数值特性动态选择不同的表示方法。通过在每个张量或子张量级别上动态选择FP8和BF16表示法，这种方法能够在保持模型质量的同时实现最先进的结果，并且高达98.38%的张量可以被量化为FP8格式。


<details>
  <summary>Details</summary>
Motivation: 混合精度训练是扩展深度学习模型的重要技术，但要成功进行混合精度训练需要找到并应用合适的训练方法组合。为此，研究者们提出了一种新颖的、基于每个张量及子张量级别的量化框架，旨在通过动态分析来选择最适配的表示形式以优化模型性能。

Method: 提出了名为Mixture-of-Representations (MoR) 的新框架，它能基于张量的数值属性，在不同表示之间进行动态选择。研究中具体实现了在每个张量及其子张量层次上动态切换FP8与BF16表示的具体算法。

Result: 初步研究表明，使用该方法可达到业界领先的结果，其中98.38%的张量能够被量化成FP8格式。此外，该方法还展示了无需细粒度划分即可获得与现有方法相当的FP8精度，表明了其在低精度训练中的潜在优势。

Conclusion: 这种动态、属性感知的量化方法不仅有助于保持模型的质量，而且可能普遍提高低精度训练的鲁棒性。同时，它也可以与其他训练方法结合使用，进一步提升更低精度数格式（如NVFP4）的应用效率。

Abstract: Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.

</details>


### [94] [Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models](https://arxiv.org/abs/2512.22814)
*Scott A. Martin,Noah Brenowitz,Dale Durran,Michael Pritchard*

Main category: cs.LG

TL;DR: 介绍了一种长距离蒸馏方法，通过使用由短时间步长自回归“教师”模型生成的大量合成训练数据集来训练长时间步长的概率“学生”模型，从而直接进行长期天气预报。该方法提高了S2S预测技能，并且表明AI生成的合成训练数据可用于扩展长期预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的AI天气模型在做长期天气预报时存在误差累积和样本量不足的问题，而一步生成长期预报的概率模型则面临过拟合挑战。因此，研究者提出了一种新方法以解决这些问题并提高长期预报的准确性。

Method: 采用长距离蒸馏法，利用Deep Learning Earth System Model作为教师模型生成超过10,000年的模拟气候数据，以此训练能够在不同时间尺度上进行预测的学生模型。

Result: 在理想模型实验中，蒸馏后的模型优于气候学表现，并接近其自回归教师的表现；在现实世界应用中，经过ERA5微调后，它们达到了与ECMWF集合预报相当的S2S预报技巧。

Conclusion: 研究表明，通过增加由AI生成的合成训练数据量，可以有效提升长期天气预报的能力。这为未来改进天气预报技术提供了新的方向。

Abstract: Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic "student" model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep autoregressive "teacher" model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000 years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration that AI-generated synthetic training data can be used to scale long-range forecast skill.

</details>


### [95] [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 提出了一种基于时间方差驱动课程的学生-教师学习范式，以加速目标条件下的强化学习。该方法通过动态优先考虑策略置信度分数中时间方差最高的目标来提供自适应和集中的学习信号，从而促进持续有效的学习进程。


<details>
  <summary>Details</summary>
Motivation: 在多目标设置下，统一的目标选择通常会导致样本效率低下。受生物系统中观察到的自适应和结构化学习过程的启发，研究者旨在通过引入一种新的学生-教师学习范式来提高目标条件下强化学习的效率。

Method: 设计了一个学生-教师框架，其中教师模块根据状态-动作值（Q）函数参数化的策略置信度得分的时间方差来动态地优先处理具有最高不确定性目标。该方法与现有RL框架无缝集成，并且不依赖于特定算法。

Result: 通过对11个不同的机器人操纵和迷宫导航任务进行评估，结果显示该方法相比当前最先进的课程学习和目标选择方法，在性能上取得了持续显著的改进。

Conclusion: 时间方差驱动的课程学习方法能够有效地加速目标条件下的强化学习过程，为解决复杂环境下的多目标任务提供了新的思路。

Abstract: Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.

</details>


### [96] [Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral](https://arxiv.org/abs/2512.22886)
*Anqi Mao*

Main category: cs.LG

TL;DR: 本论文探讨了多专家转介学习问题及其相关的带有弃权的学习问题，旨在解决大型语言模型的幻觉和高推理成本问题。研究中引入了新的替代损失函数，并为多分类、回归以及单阶段和两阶段场景设计了有效算法，证明了这些方法在保持一致性的同时能提高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现出色，但存在产生幻觉和推理成本高的问题。通过利用多个专家的力量，可以将不确定的输入转交给更强大的专家处理以增强可靠性，同时将简单查询导向更小、精简的模型来提升效率。这促使了对多专家转介学习的研究。

Method: 本文首先分析了带弃权学习（作为转介的一种特殊情况）中的基于分数和预测-拒绝两种形式，并为此引入了新的替代损失函数家族。接着，针对一般性的多专家转介分类问题，为单阶段及两阶段情况设计了新的替代损失函数。最后，提出了一个适用于连续标签空间的新框架——带转介的回归，该框架支持多种专家和成本结构。

Result: 对于带弃权学习，提出的算法在CIFAR-10、CIFAR-100和SVHN数据集上展示了优越的表现。在多专家转介方面，新设计的损失函数被证实具有强H-一致性界限，尤其是在两阶段情形下，当成本函数为常数时，所提方法更为有效。此外，在带转介的回归框架内开发的新算法也显示出良好的实际效果。

Conclusion: 这项研究全面探讨了学习过程中使用多专家转介的问题，不仅解决了两个已知的开放问题，还通过引入新的替代损失函数促进了算法性能的提升，从而在保证一致性的前提下提高了系统处理复杂任务的能力。

Abstract: Large language models (LLMs) have achieved remarkable performance but face critical challenges: hallucinations and high inference costs. Leveraging multiple experts offers a solution: deferring uncertain inputs to more capable experts improves reliability, while routing simpler queries to smaller, distilled models enhances efficiency. This motivates the problem of learning with multiple-expert deferral. This thesis presents a comprehensive study of this problem and the related problem of learning with abstention, supported by strong consistency guarantees.
  First, for learning with abstention (a special case of deferral), we analyze score-based and predictor-rejector formulations in multi-class classification. We introduce new families of surrogate losses and prove strong non-asymptotic, hypothesis set-specific consistency guarantees, resolving two existing open questions. We analyze both single-stage and practical two-stage settings, with experiments on CIFAR-10, CIFAR-100, and SVHN demonstrating the superior performance of our algorithms.
  Second, we address general multi-expert deferral in classification. We design new surrogate losses for both single-stage and two-stage scenarios and prove they benefit from strong $H$-consistency bounds. For the two-stage scenario, we show that our surrogate losses are realizable $H$-consistent for constant cost functions, leading to effective new algorithms.
  Finally, we introduce a novel framework for regression with deferral to address continuous label spaces. Our versatile framework accommodates multiple experts and various cost structures, supporting both single-stage and two-stage methods. It subsumes recent work on regression with abstention. We propose new surrogate losses with proven $H$-consistency and demonstrate the empirical effectiveness of the resulting algorithms.

</details>


### [97] [MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning](https://arxiv.org/abs/2512.22904)
*Jin Wu,Chanjin Zheng*

Main category: cs.LG

TL;DR: 提出了一种基于持续学习的元学习框架MetaCD，用于认知诊断，能够缓解长尾分布问题，并适应数据中的动态变化，实验表明其在准确性和泛化性上优于其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在处理学生、题目和技能之间复杂交互时，受到长尾分布和数据中动态变化的限制，影响了性能。

Method: 提出了一个基于持续学习的元学习框架（MetaCD），利用元学习找到最优初始化状态来缓解长尾问题，并通过参数保护机制使模型能够适应新技能或任务的变化。

Result: 在五个真实世界数据集上的综合实验显示，MetaCD在准确性和泛化能力方面都超过了其他基线方法。

Conclusion: MetaCD提供了一种有效的方法来解决认知诊断中的长尾分布和动态变化挑战，提高了模型对于单一任务的可塑性以及连续任务上的稳定性和泛化性。

Abstract: Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.

</details>


### [98] [Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning](https://arxiv.org/abs/2512.22910)
*Ünver Çiftçi*

Main category: cs.LG

TL;DR: Sat-EnQ, a two-phase deep Q-learning framework, initially focuses on achieving 'good enough' performance to reduce early training instability before transitioning to more aggressive optimization. This method significantly reduces variance, prevents catastrophic overestimation, and enhances robustness under environmental noise.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the instability of deep Q-learning algorithms, particularly during the early stages of training, by introducing a two-phase learning approach that balances between satisficing (being good enough) and optimizing, inspired by bounded rationality and developmental learning principles.

Method: Sat-EnQ consists of two phases: In Phase 1, an ensemble of lightweight Q-networks is trained with a satisficing objective that controls value growth through a dynamic baseline, promoting diversity and low variance in estimates. In Phase 2, these networks are distilled into a single, larger network and fine-tuned using standard Double DQN techniques.

Result: Empirically, Sat-EnQ achieves a 3.8x reduction in variance, completely eliminates catastrophic failures compared to 50% for DQN, maintains 79% performance under environmental noise, and requires 2.5x less computational resources than bootstrapped ensembles. Theoretical analysis supports that satisficing helps in inducing bounded updates and reducing target variance.

Conclusion: The paper concludes that by first ensuring the model is 'good enough' before pushing for optimization, Sat-EnQ offers a promising direction towards achieving more stable and robust reinforcement learning outcomes.

Abstract: Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.

</details>


### [99] [Multiple Token Divergence: Measuring and Steering In-Context Computation Density](https://arxiv.org/abs/2512.22944)
*Vincent Herrmann,Eric Alcaide,Michael Wand,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量语言模型上下文计算努力的方法，称为多令牌散度（MTD），并引入了发散导向解码方法来控制生成文本的计算特性。实验表明，MTD在区分复杂任务与简单任务上比先前的方法更有效，并且与数学推理基准中的问题难度呈正相关。


<details>
  <summary>Details</summary>
Motivation: 现有的衡量语言模型上下文计算努力的指标，如下一个令牌损失，无法捕捉到推理复杂性；而基于潜在状态可压缩性的方法可能具有侵入性和不稳定性。因此，需要一种简单有效的新方法来度量这一努力。

Method: 提出了多令牌散度（MTD）作为计算努力的一种新度量标准，定义为模型完整输出分布与浅层辅助预测头之间的KL散度。此外，还介绍了一种名为“发散导向”的新解码方法，用于调控生成文本的计算特征。

Result: 实验证明，MTD在区分复杂任务和简单任务方面优于现有方法，在数学推理基准测试中与问题难度呈正相关关系。较低的MTD值通常伴随着更加准确的推理过程。

Conclusion: MTD提供了一个实用且轻量级的工具，用于分析和指导语言模型的计算动态。

Abstract: Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.

</details>


### [100] [APO: Alpha-Divergence Preference Optimization](https://arxiv.org/abs/2512.22953)
*Wang Zixian*

Main category: cs.LG

TL;DR: 本文提出了一种名为Alpha-Divergence Preference Optimization (APO)的新框架，该框架能够在同一锚定几何结构中连续地在前向KL和反向KL行为之间插值。通过实验表明，APO在保持训练稳定性的同时达到了与GRPO和GSPO基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的对齐实践中主要存在两种差异机制：监督微调和许多蒸馏式目标隐式地最小化前向KL散度，导致稳定的模式覆盖更新但往往未能充分利用高回报模式；而PPO风格的人类反馈在线强化学习则更接近于反向KL散度，虽然能够寻找改进模式但却冒着模式崩溃的风险。为了结合两者的优势并克服其局限性，提出了新的方法。

Method: 介绍了一种基于Csiszar alpha-散度的新型锚定框架——Alpha-Divergence Preference Optimization (APO)，该框架可以在相同的锚定几何内连续地从前向KL到反向KL行为进行插值。此外，还推导了统一的梯度动态过程，并提出了一个实用的奖励与信心保护alpha调度策略，以确保只有当策略既得到改善又自信校准时才从覆盖转为利用。

Result: 实验结果显示，在Qwen3-1.7B模型上使用数学级别3任务时，APO不仅表现出色，与GRPO和GSPO基线相比具有竞争力，同时还能保持良好的训练稳定性。

Conclusion: 通过引入APO框架，研究者们成功地结合了前向KL和反向KL的优点，提供了一种既能探索又能有效利用高回报模式的方法，同时保持了训练过程中的稳定性。

Abstract: Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.

</details>


### [101] [A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging](https://arxiv.org/abs/2512.22976)
*Amirali Vakili,Salar Jahanshiri,Armin Salimi-Badr*

Main category: cs.LG

TL;DR: 本研究提出了一种上下文感知且可解释的单通道EEG睡眠分期框架，特别关注N1阶段的检测。通过结合多尺度特征提取与时间建模，采用类别加权损失函数和数据增强来解决数据不平衡问题，并在SleepEDF数据集上实现了整体准确率89.72%及宏观平均F1分数85.46%，特别是对N1阶段的F1分数达到了61.7%，优于先前的方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于全球范围内睡眠障碍的普遍性，自动睡眠分期成为医疗健康领域的重要任务。尽管单通道脑电图（EEG）信号为此提供了一个实用而广泛可用的基础，但现有方法面临诸如类别不平衡、接收域建模有限以及解释性不足等问题。此外，N1阶段的检测尤其具有挑战性。

Method: 该研究开发了一种新的上下文感知且可解释的模型，专门针对单通道EEG睡眠分期设计。该模型融合了紧凑型多尺度特征提取与时间序列建模技术，旨在捕捉局部与长距离依赖关系。为应对数据不平衡问题，特别是在N1阶段，研究者应用了类别加权损失函数及数据增强技术。EEG信号被分割成子时段块，最终预测结果通过对各块softmax概率取平均值得出，从而增强了上下文表示能力和鲁棒性。

Result: 所提出的框架在SleepEDF数据集上达到了89.72%的整体准确率和85.46%的宏观平均F1分数。对于极具挑战性的N1阶段，其F1分数更是达到了61.7%，显著优于之前的研究成果。

Conclusion: 实验结果表明，所提方法不仅提高了睡眠分期的表现，同时保持了良好的可解释性和实际临床应用潜力。

Abstract: Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.

</details>


### [102] [Fusion or Confusion? Multimodal Complexity Is Not All You Need](https://arxiv.org/abs/2512.22991)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: 本研究通过大规模实证研究，重新实现了19种高影响力方法，在标准化条件下对它们进行了评估，并提出了一种简单的多模态学习基线模型SimBaMM。研究表明，在严格的超参数调整下，更复杂的架构并不能可靠地优于SimBaMM。此外，还提供了一个实用性可靠性检查表以促进未来评价的可比性、稳健性和可信度。


<details>
  <summary>Details</summary>
Motivation: 该论文动机在于质疑复杂多模态特定方法能够提高性能这一假设的有效性，旨在通过广泛实验验证简单与复杂架构在多模态学习中的实际表现差异。

Method: 研究者们重新实现了19种具有高影响力的多模态学习方法，并在包含多达23种模态的九个不同数据集上进行了评估。同时，他们提出了一个名为SimBaMM的简单基线模型，基于一种直接的后期融合Transformer架构。所有方法均在相同的实验条件下进行了严格的超参数调优。

Result: 结果显示，在统一且严格控制的实验环境下，更复杂的架构并不总是优于提出的简单基线SimBaMM。统计分析表明，许多复杂的方法表现与SimBaMM相当，而且经常无法可靠地超越经过良好调优的单模态基线，尤其是在原研究中常见的小数据量情况下。

Conclusion: 研究结论建议，未来的多模态学习研究应该更加注重方法论上的严谨性，而非一味追求架构的新颖性。

Abstract: Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions, evaluating them across nine diverse datasets with up to 23 modalities, and testing their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a straightforward late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analysis indicates that more complex methods perform comparably to SimBaMM and frequently do not reliably outperform well-tuned unimodal baselines, especially in the small-data regime considered in many original studies. To support our findings, we include a case study of a recent multimodal learning method highlighting the methodological shortcomings in the literature. In addition, we provide a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.

</details>


### [103] [Merge before Forget: A Single LoRA Continual Learning via Continual Merging](https://arxiv.org/abs/2512.23017)
*Fuli Qiao,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 提出了一种新的连续学习方法，通过正交初始化和顺序合并LoRA更新到一个统一的LoRA中，以解决现有低秩适应（LoRA）连续学习技术中的计算内存增长、存储空间有限以及任务间潜在干扰的问题。该方法利用了之前学习到的LoRA的正交基提取进行新任务的学习初始化，并采用时间感知缩放机制来平衡在持续合并过程中新旧知识的关系。


<details>
  <summary>Details</summary>
Motivation: 当前的低秩适应（LoRA）连续学习技术虽然能够缓解灾难性遗忘并支持新任务的学习，但它们忽略了随着任务数量增加而增长的计算内存需求和有限的存储空间问题。此外，由于缺乏有效的LoRA合并机制，这些方法还可能遭遇任务间的干扰。因此，需要一种新的方法来解决这些问题。

Method: 本研究提出的方法包括对先前学习到的LoRA进行正交基提取以初始化新任务的学习过程，并利用内在的不对称属性通过时间感知缩放机制来调整连续合并期间新旧知识的比例。这种方法旨在保持与任务数量无关的恒定内存复杂度，同时最小化过去与新任务之间的干扰，并通过自适应缩放改进性能。

Result: 通过理论分析和使用多种Llama模型在不同连续学习基准上的广泛实验表明，所提出的方法不仅有效而且高效，在保持资源限制的同时提高了处理多个任务的能力。

Conclusion: 提出的新连续学习方法成功地解决了传统LoRA连续学习技术中存在的问题，包括计算内存随任务增长、存储空间限制及任务间干扰等。通过正交初始化和顺序合并LoRA更新，结合时间感知缩放机制，实现了更优的任务表现。

Abstract: Parameter-efficient continual learning has emerged as a promising approach for large language models (LLMs) to mitigate catastrophic forgetting while enabling adaptation to new tasks. Current Low-Rank Adaptation (LoRA) continual learning techniques often retain and freeze previously learned LoRAs or generate data representations to overcome forgetting, typically utilizing these to support new LoRAs learn new tasks. However, these methods not only ignore growing computational memory with tasks and limited storage space but also suffer from potential task interference due to the lack of effective LoRA merging mechanisms. In this paper, we propose a novel continual learning method that orthogonally initializes and sequentially merges LoRAs updates into a single unified LoRA. Our method leverages orthogonal basis extraction from previously learned LoRA to initialize the learning of new tasks, further exploits the intrinsic asymmetry property of LoRA components by using a time-aware scaling mechanism to balance new and old knowledge during continual merging. Our approach maintains constant memory complexity with respect to the number of tasks, minimizes interference between past and new tasks via orthogonal basis initialization, and improves performance over asymmetric LoRA merging via adaptive scaling. We provide theoretical analysis to justify our design and conduct extensive experiments across diverse continual learning benchmarks using various Llama models, demonstrating the effectiveness and efficiency of our method.

</details>


### [104] [Mechanistic Analysis of Circuit Preservation in Federated Learning](https://arxiv.org/abs/2512.23043)
*Muhammad Haseeb,Salaar Masood,Muhammad Abdullah Sohail*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting client updates leads to circuit collapse, the destructive interference of functional, sparse sub-networks responsible for specific class predictions. By training inherently interpretable, weight-sparse neural networks within an FL framework, we identify and track these circuits across clients and communication rounds. Using Intersection-over-Union (IoU) to quantify circuit preservation, we provide the first mechanistic evidence that Non-IID data distributions cause structurally distinct local circuits to diverge, leading to their degradation in the global model. Our findings reframe the problem of statistical drift in FL as a concrete, observable failure of mechanistic preservation, paving the way for more targeted solutions.

</details>


### [105] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [106] [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)
*Boyang Zhang,Xiaobing Chen,Songyang Zhang,Shuai Zhang,Xiangwei Zhou,Mingxuan Sun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe expert load imbalance that degrades model performance. To this end, we propose \textbf{FLEX-MoE}, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. Specifically, our approach introduces client-expert fitness scores that quantify the expert suitability for local datasets through training feedback, and employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide. Unlike existing greedy methods that focus solely on personalization while ignoring load imbalance, our FLEX-MoE is capable of addressing the expert utilization skew, which is particularly severe in FL settings with heterogeneous data. Our comprehensive experiments on three different datasets demonstrate the superior performance of the proposed FLEX-MoE, together with its ability to maintain balanced expert utilization across diverse resource-constrained scenarios.

</details>


### [107] [Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models](https://arxiv.org/abs/2512.23073)
*Mingyuan Zhang,Yue Bai,Yifan Wang,Yiyang Huang,Yun Fu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explorations in fine-tuning Vision-Language Models (VLMs), such as Low-Rank Adaptation (LoRA) from Parameter Efficient Fine-Tuning (PEFT), have made impressive progress. However, most approaches rely on explicit weight updates, overlooking the extensive representational structures already encoded in pre-trained models that remain underutilized. Recent works have demonstrated that Mask Fine-Tuning (MFT) can be a powerful and efficient post-training paradigm for language models. Instead of updating weights, MFT assigns learnable gating scores to each weight, allowing the model to reorganize its internal subnetworks for downstream task adaptation. In this paper, we rethink fine-tuning for VLMs from a structural reparameterization perspective grounded in MFT. We apply MFT to the language and projector components of VLMs with different language backbones and compare against strong PEFT baselines. Experiments show that MFT consistently surpasses LoRA variants and even full fine-tuning, achieving high performance without altering the frozen backbone. Our findings reveal that effective adaptation can emerge not only from updating weights but also from reestablishing connections among the model's existing knowledge. Code available at: https://github.com/Ming-K9/MFT-VLM

</details>


### [108] [Multimodal Functional Maximum Correlation for Emotion Recognition](https://arxiv.org/abs/2512.23076)
*Deyang Zheng,Tianyi Zhang,Wenming Zheng,Shujian Yu*

Main category: cs.LG

TL;DR: 提出了一种新的自监督学习框架MFMC，用于改善情感计算中的多模态表示学习问题。该方法通过最大化高阶多模态依赖性来直接捕捉多模态间的相互作用，无需依赖成对对比损失。实验表明，MFMC在多个公开基准测试中表现优异或具有竞争力，特别是在跨被试变异性方面表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 情绪状态表现为中枢和自主系统间协调但异质的生理反应，给情感计算中的多模态表示学习带来了挑战。此外，情感注释的稀缺性和主观性推动了自监督学习（SSL）的应用。然而，现有大多数SSL方法基于成对对齐目标，这不足以描述超过两种模态之间的依赖关系，并且无法捕捉由大脑与自主响应协调产生的高阶交互作用。

Method: 本文提出了Multimodal Functional Maximum Correlation (MFMC)，这是一种通过双重总相关（DTC）目标最大化高阶多模态依赖性的原则性SSL框架。利用紧密的夹逼界并通过基于功能最大相关分析（FMCA）的追踪替代物进行优化，MFMC能够直接捕获多模态联合交互，而不需要依靠成对对比损失。

Result: 实验结果显示，在三个公开的情感计算基准上，无论是在受试者内还是受试者间评估协议下，MFMC均达到了最先进的或有竞争力的表现。尤其值得注意的是，仅使用EDA信号时，MFMC将CEAP-360VR数据集上的受试者内准确率从78.9%提高到了86.8%，受试者间准确率从27.5%提升至33.1%。此外，在MAHNOB-HCI数据集最具挑战性的EEG受试者间分割任务上，MFMC的表现与最佳方法相差不到0.8个百分点。

Conclusion: 本研究提出的MFMC框架为解决情感计算领域内多模态表示学习面临的挑战提供了一个有效的方法。它不仅能够更好地捕捉不同生理信号间的复杂互动，还展示了对于个体差异的良好适应能力。

Abstract: Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.
  To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.
  Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.

</details>


### [109] [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)
*Yingru Li,Jiawei Xu,Jiacai Liu,Yuxuan Tong,Ziniu Li,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种解决大型语言模型强化学习中训练-推理不匹配问题的方法，通过动态修剪词汇表来排除极低概率的词汇，从而实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）的强化学习过程中，存在着由于高吞吐量推理引擎和数值精确训练系统对同一参数产生不同概率分布而导致的训练-推理不匹配问题。特别是对于低概率词汇而言，这种不匹配会导致序列级上的系统性偏差积累，影响梯度估计的稳定性。

Method: 本文建议不是事后修正，而是直接约束RL目标函数到一个经过动态修剪后的“安全”词汇表上，该词汇表排除了极端尾部的概率非常低的词汇。通过这种方式，用较小且可控的优化偏差替换了大且系统性的偏差。

Result: 实验表明，所提出的方法能够实现稳定的训练过程；理论上，作者还界定了词汇修剪引入的优化偏差。

Conclusion: 本研究提供了一种有效缓解因训练-推理不匹配引起的问题的新方法，即通过动态调整词汇表来提高强化学习算法应用于大型语言模型时的稳定性与效率。

Abstract: Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.

</details>


### [110] [A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms](https://arxiv.org/abs/2512.23097)
*Yingru Li,Ziniu Li,Jiacai Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种结合模仿学习和强化学习的大语言模型微调统一框架，通过分析包含轨迹级KL散度和任务奖励的复合目标梯度，将其自然分解为两个部分：(1) 可以解析计算的用于词元级别模仿的密集梯度；(2) 用于长时序奖励优化的蒙特卡洛估计稀疏梯度。密集梯度具有闭式logit层面公式，支持高效的GPU实现。


<details>
  <summary>Details</summary>
Motivation: 为了提高大语言模型（LLM）在特定任务上的性能，作者提出了一种新的微调方法，旨在结合模仿学习和强化学习的优势来优化模型。

Method: 本文介绍了一个集成模仿学习与强化学习的大语言模型微调框架。通过对一个结合了轨迹级KL散度及任务奖励的目标函数进行梯度分析，研究者们推导出了由两部分组成的自然分解方案：一部分是可精确计算的、针对词元级模仿的密集梯度；另一部分则是采用蒙特卡洛估计法获得的、专注于长期回报最大化的稀疏梯度。其中，密集梯度拥有可以直接应用于logit层的具体公式表达形式，从而便于在GPU上高效执行。

Result: 研究表明，所提出的框架能够有效提升大语言模型在特定任务上的表现，特别是在处理需要长期规划的任务时展现出显著优势。此外，由于密集梯度可以解析计算并直接应用于模型训练过程中，因此整个过程可以在GPU上快速完成。

Conclusion: 这项工作展示了一种新颖且有效的策略，通过整合模仿学习与强化学习来改进大语言模型的微调过程。提出的基于梯度分解的方法不仅提高了模型性能，还保证了计算效率。

Abstract: We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.

</details>


### [111] [How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure](https://arxiv.org/abs/2512.23109)
*Paul M. Thompson*

Main category: cs.LG

TL;DR: 本研究探讨了在有限样本条件下，生成模型和视觉-语言模型（VLMs）在科学和医学决策支持中如何实现跨输入、类别或子群体的一致准确性和良好校准。通过分析基于提示或语义嵌入变化的分类器家族，并假设模型输出平滑依赖于低维语义表示，研究表明，在满足Lipschitz稳定性条件下，VLM诱导的分类器对于准确性和校准函数能够得到有意义的非渐近保证。这些结果对数据量有限的生物医学建模具有重要启示，指出了当前数据集大小是否足以支持一致可靠的预测以及为什么平均校准指标可能忽略最坏情况下的校准问题。


<details>
  <summary>Details</summary>
Motivation: 尽管现代生成模型和视觉-语言模型在有适量数据的情况下表现出色，但它们在所有输入、类别或子群体上是否具有一致准确且良好校准的表现仍不清楚。这对于生物医学领域尤为重要，因为罕见状况或特定群体可能会导致即使总体损失较低时也存在较大误差。

Method: 采用有限样本视角来研究这一问题，专注于通过改变提示或受限表示空间内的语义嵌入获得的分类器族。假设模型输出平滑地依赖于一个低维语义表示，这得到了文本和联合图像-文本嵌入中的谱结构的支持。利用经典的统一收敛工具，为VLM诱导的分类器在准确性与校准方面提供了非渐近性保证。

Result: 主要发现是在关于提示嵌入的Lipschitz稳定性条件下，VLM诱导的分类器对于准确性和校准函数可以达到有意义的非渐近保证。样本复杂度取决于内在/有效维度而非环境嵌入维度，并进一步导出了与谱相关的边界，明确显示了特征值衰减如何影响数据需求。

Conclusion: 该研究为在有限样本下使用生成模型及视觉-语言模型进行生物医学建模提供了理论基础，表明现有数据集规模可能不足以支持所有情况下的一致可靠预测，并强调了仅依赖平均校准度量可能忽视最坏情况下校准不佳的问题。

Abstract: Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.
  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.
  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.

</details>


### [112] [SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals](https://arxiv.org/abs/2512.23131)
*Yankang Li,Changsheng Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合通道注意力机制和残差连接的多层感知机架构（SE-MLP），用于快速预测穿透加速度特征值。通过对比实验，该模型展现了更高的预测精度、泛化能力和稳定性，并且在数值模拟和实际测试中验证了其工程适用性。


<details>
  <summary>Details</summary>
Motivation: 准确识别穿透过程依赖于穿透加速度的先验特征值，但这些特征值通常需要长时间的仿真周期和高昂的计算成本来获取。为了解决这个问题，提出了新的方法以实现快速预测。

Method: 采用了一种名为挤压与激励多层感知机（SE-MLP）的新架构，它将通道注意力机制与残差连接相结合，使用不同工作条件下的物理参数作为输入，输出分层加速特性，从而建立物理参数与穿透特性之间的非线性映射关系。

Result: 与传统MLP、XGBoost及Transformer模型相比，SE-MLP展现出更好的预测准确性、泛化能力以及稳定性。消融研究进一步证实了通道注意力模块和残差结构对性能提升的重要贡献。此外，数值模拟和范围恢复测试表明，预测值与测量值之间的差异保持在可接受的工程容许范围内。

Conclusion: 提出的SE-MLP方法不仅提高了穿透加速度特征值预测的速度和精度，还证明了其实用性和工程应用潜力，为快速生成穿透引信所需的先验特征值提供了实践基础。

Abstract: Accurate identification of the penetration process relies heavily on prior feature values of penetration acceleration. However, these feature values are typically obtained through long simulation cycles and expensive computations. To overcome this limitation, this paper proposes a multi-layer Perceptron architecture, termed squeeze and excitation multi-layer perceptron (SE-MLP), which integrates a channel attention mechanism with residual connections to enable rapid prediction of acceleration feature values. Using physical parameters under different working conditions as inputs, the model outputs layer-wise acceleration features, thereby establishing a nonlinear mapping between physical parameters and penetration characteristics. Comparative experiments against conventional MLP, XGBoost, and Transformer models demonstrate that SE-MLP achieves superior prediction accuracy, generalization, and stability. Ablation studies further confirm that both the channel attention module and residual structure contribute significantly to performance gains. Numerical simulations and range recovery tests show that the discrepancies between predicted and measured acceleration peaks and pulse widths remain within acceptable engineering tolerances. These results validate the feasibility and engineering applicability of the proposed method and provide a practical basis for rapidly generating prior feature values for penetration fuzes.

</details>


### [113] [A Weak Signal Learning Dataset and Its Baseline Method](https://arxiv.org/abs/2512.23160)
*Xianqi Liu,Xiangru Li,Lefeng He,Ziyu Fang*

Main category: cs.LG

TL;DR: 本文构建了首个针对弱信号特征学习的专门数据集，并提出了一种适用于低信噪比、分布偏斜及双重不平衡情况的PDVFN模型，该模型在处理弱信号、高噪声和极端类别不平衡方面表现出更高的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 弱信号学习（WSL）在故障诊断、医学成像和自动驾驶等多个领域中是一个普遍挑战，其中关键信息往往被噪声和干扰掩盖，使得特征识别变得困难。即使是在强信号丰富的任务中，提高模型性能的关键也常在于有效提取弱信号。然而，缺乏专门的数据集长期以来限制了这一领域的研究发展。

Method: 为了解决上述问题，作者们构建了一个包含13,158个光谱样本的专业数据集用于弱信号特征学习。此外，他们还提出了一种双视图表示方法（向量+时频图）以及一个名为PDVFN的新模型，后者专为应对低信噪比、分布偏差和双重不平衡设计。PDVFN能够并行抽取局部序列特征与全局频域结构，遵循局部增强、序列建模、噪声抑制、多尺度捕捉、频率提取及全局感知等原则。

Result: 实验表明，所提出的方法在处理低信噪比环境下的弱信号、高噪声以及极端类别不平衡方面展现出了更优的准确性与鲁棒性。

Conclusion: 通过提供专用数据集、基准模型，本研究为未来的WSL研究奠定了基础，特别是在天文学光谱学等WSL任务中提出了新解决方案。

Abstract: Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improving model performance often lies in effectively extracting weak signals. However, the lack of dedicated datasets has long constrained research. To address this, we construct the first specialized dataset for weak signal feature learning, containing 13,158 spectral samples. It features low SNR dominance (over 55% samples with SNR below 50) and extreme class imbalance (class ratio up to 29:1), providing a challenging benchmark for classification and regression in weak signal scenarios. We also propose a dual-view representation (vector + time-frequency map) and a PDVFN model tailored to low SNR, distribution skew, and dual imbalance. PDVFN extracts local sequential features and global frequency-domain structures in parallel, following principles of local enhancement, sequential modeling, noise suppression, multi-scale capture, frequency extraction, and global perception. This multi-source complementarity enhances representation for low-SNR and imbalanced data, offering a novel solution for WSL tasks like astronomical spectroscopy. Experiments show our method achieves higher accuracy and robustness in handling weak signals, high noise, and extreme class imbalance, especially in low SNR and imbalanced scenarios. This study provides a dedicated dataset, a baseline model, and establishes a foundation for future WSL research.

</details>


### [114] [Diffusion-based Decentralized Federated Multi-Task Representation Learning](https://arxiv.org/abs/2512.23161)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: 本文提出了一种基于分散式投影梯度下降的多任务表示学习算法，针对的是多个线性回归模型共享一个共同的低维线性表示的问题。该算法在扩散式的分散和联邦方式下恢复低秩特征矩阵，并提供了样本复杂性和迭代复杂性的理论保证。此外，分析了算法的时间和通信复杂性，并通过数值模拟验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 尽管表征学习已被广泛研究，但去中心化的方法仍相对较少被探索。为了在数据稀缺环境中从各种不同但相关的任务中获得特征提取器或表示方法，提出了一个适用于多任务线性回归问题的新算法。

Method: 采用交替投影梯度下降和最小化算法，在一种基于扩散的去中心化和联邦方式下来恢复低秩特征矩阵。

Result: 为所提出的算法获得了可构造的、可证明的保证，包括对所需样本复杂性的下限和迭代复杂性的上限。同时，表明该算法在时间和通信上是高效的。数值仿真也证实了算法的有效性，并与基准算法进行了比较。

Conclusion: 本文开发了一种新的去中心化算法，用于多任务表示学习，它能够以高效的方式处理数据稀缺环境下的问题，并且在性能上优于一些基准算法。

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.

</details>


### [115] [Evaluating Parameter Efficient Methods for RLVR](https://arxiv.org/abs/2512.23165)
*Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu*

Main category: cs.LG

TL;DR: 本文系统地评估了多种参数高效微调(PEFT)方法在基于可验证奖励的强化学习(RLVR)范式下的表现，发现结构变体如DoRA、AdaLoRA和MiSS优于标准LoRA，并揭示了SVD信息初始化策略中存在的谱塌陷现象以及极端参数减少对推理能力的负面影响。


<details>
  <summary>Details</summary>
Motivation: 当前，在使用可验证反馈来提高语言模型推理能力的RLVR框架内，尽管像LoRA这样的方法被广泛采用，但最适合RLVR的PEFT架构尚未确定。

Method: 通过在数学推理基准上对DeepSeek-R1-Distill系列中的超过12种PEFT方法进行全面评估，并进行了消融研究与扩展实验以验证发现。

Result: 结构变体（如DoRA, AdaLoRA, MiSS）比标准LoRA表现更好；SVD信息初始化策略中观察到了谱塌陷现象，这导致了主要成分更新与RL优化之间的根本性不匹配；极端参数减少（例如VeRA, Rank-1）严重限制了推理能力。

Conclusion: 本研究表明需要更多探索适合RL的参数高效方法，并为未来的研究提供了明确指南。

Abstract: We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.

</details>


### [116] [Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR](https://arxiv.org/abs/2512.23177)
*Will Sebelik-Lassiter,Evan Schubert,Muhammad Alliyu,Quentin Robbins,Excel Olatunji,Mustafa Barry*

Main category: cs.LG

TL;DR: 本研究通过应用机器学习辅助算法自动识别声带并区分正常声带图像与声带麻痹(VCP)图像，提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 鉴于声带超声(VCUS)检查技术虽然侵入性较小且更易被接受，但其准确性依赖于操作者。本研究旨在通过机器学习方法提高VCUS的诊断准确性。

Method: 从30名志愿者处获取了VCUS视频，并将其分割为静止帧且裁剪成统一大小。使用健康和模拟VCP图像作为训练数据，分别用于声带分割模型和VCP分类模型的开发。

Result: 声带分割模型达到了96%的验证准确率，而最佳分类模型（VIPRnet）则达到了99%的验证准确率。

Conclusion: 机器学习辅助分析在提高VCUS诊断准确性方面展现出了巨大潜力，优于依赖于操作者的传统人工解读方式。

Abstract: Intro: Vocal cord ultrasound (VCUS) has emerged as a less invasive and better tolerated examination technique, but its accuracy is operator dependent. This research aims to apply a machine learning-assisted algorithm to automatically identify the vocal cords and distinguish normal vocal cord images from vocal cord paralysis (VCP). Methods: VCUS videos were acquired from 30 volunteers, which were split into still frames and cropped to a uniform size. Healthy and simulated VCP images were used as training data for vocal cord segmentation and VCP classification models. Results: The vocal cord segmentation model achieved a validation accuracy of 96%, while the best classification model (VIPRnet) achieved a validation accuracy of 99%. Conclusion: Machine learning-assisted analysis of VCUS shows great promise in improving diagnostic accuracy over operator-dependent human interpretation.

</details>


### [117] [A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization](https://arxiv.org/abs/2512.23190)
*Yi-Han Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种名为LightONS的在线牛顿步法(ONS)变体，旨在解决在线eXp-凹优化(OXO)中的计算瓶颈问题。LightONS在保持最优$O(d \log T)$遗憾的同时，将总运行时间减少到$O(d^2 T + d^\omega\sqrt{T \log T})$。此外，它还为随机eXp-凹优化(SXO)提供了一个解决方案，其运行时间为$\tilde{O}(d^3/\varepsilon)$，解决了COLT'13提出的开放问题。


<details>
  <summary>Details</summary>
Motivation: 在线eXp-凹优化（OXO）是在线学习中的一个基本问题，而标准算法——在线牛顿步（ONS）虽然在统计最优性和计算实用性之间取得了平衡，但在每轮迭代中由于马氏投影导致了计算瓶颈。这使得整个运行时间可能达到$\tilde{O}(d^\omega T)$，特别是当迭代频繁接近域边界时。对于随机eXp-凹优化（SXO），使用ONS通过在线到批量转换来部署也需要较长的运行时间。因此，研究者们寻求一种更高效的算法以降低计算成本。

Method: 本文介绍了一种简化的在线牛顿步法版本——LightONS，该方法通过引入滞后机制延迟昂贵的马氏投影直到必要时刻，从而减少了整体运行时间至$O(d^2 T + d^\omega\sqrt{T \log T})$，同时保持了$O(d \log T)$的最佳遗憾率。这一改进得益于参数自由在线学习领域转换技术的应用。

Result: 实验结果表明，LightONS不仅能够有效地作为ONS的替代方案应用于更广泛的场景中，包括但不限于遗憾最小化、梯度范数自适应遗憾、参数化随机多臂赌博机以及内存高效在线学习等领域，而且它还成功地回答了由Koren [2013]在COLT'13上提出的关于寻找运行时间低于$\tilde{O}(d^{\omega+1}/\varepsilon)$的SXO算法的开放问题。

Conclusion: 通过引入LightONS算法，本文成功地降低了在线eXp-凹优化与随机eXp-凹优化任务中的计算复杂度，同时保持了理论上的最优性能。这为处理大规模数据集或高维空间中的在线学习问题提供了新的思路和工具。

Abstract: Online eXp-concave Optimization (OXO) is a fundamental problem in online learning. The standard algorithm, Online Newton Step (ONS), balances statistical optimality and computational practicality, guaranteeing an optimal regret of $O(d \log T)$, where $d$ is the dimension and $T$ is the time horizon. ONS faces a computational bottleneck due to the Mahalanobis projections at each round. This step costs $Ω(d^ω)$ arithmetic operations for bounded domains, even for the unit ball, where $ω\in (2,3]$ is the matrix-multiplication exponent. As a result, the total runtime can reach $\tilde{O}(d^ωT)$, particularly when iterates frequently oscillate near the domain boundary. For Stochastic eXp-concave Optimization (SXO), computational cost is also a challenge. Deploying ONS with online-to-batch conversion for SXO requires $T = \tilde{O}(d/ε)$ rounds to achieve an excess risk of $ε$, and thereby necessitates an $\tilde{O}(d^{ω+1}/ε)$ runtime. A COLT'13 open problem posed by Koren [2013] asks for an SXO algorithm with runtime less than $\tilde{O}(d^{ω+1}/ε)$.
  This paper proposes a simple variant of ONS, LightONS, which reduces the total runtime to $O(d^2 T + d^ω\sqrt{T \log T})$ while preserving the optimal $O(d \log T)$ regret. LightONS implies an SXO method with runtime $\tilde{O}(d^3/ε)$, thereby answering the open problem. Importantly, LightONS preserves the elegant structure of ONS by leveraging domain-conversion techniques from parameter-free online learning to introduce a hysteresis mechanism that delays expensive Mahalanobis projections until necessary. This design enables LightONS to serve as an efficient plug-in replacement of ONS in broader scenarios, even beyond regret minimization, including gradient-norm adaptive regret, parametric stochastic bandits, and memory-efficient online learning.

</details>


### [118] [PGOT: A Physics-Geometry Operator Transformer for Complex PDEs](https://arxiv.org/abs/2512.23192)
*Zhuo Zhang,Xi Yang,Yuan Zhao,Canqun Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer模型，称为物理-几何操作变换器（PGOT），旨在解决大型非结构化网格中复杂几何形状的偏微分方程建模问题。通过引入保持频谱的几何注意力机制和自适应计算路径选择策略，PGOT能够更准确地保留多尺度几何特征，并在多个标准基准测试以及实际工业应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的高效架构通常采用特征维度缩减策略，但这会导致几何混叠现象，进而丢失重要的物理边界信息。为了解决这一问题，研究者们开发了PGOT，它通过显式的几何感知来重建物理特征学习。

Method: 提出了Spectrum-Preserving Geometric Attention (SpecGeo-Attention)机制，利用“物理切片-几何注入”方法结合多尺度几何编码，在保持线性计算复杂度O(N)的同时明确保存多尺度几何特性；此外，基于空间坐标动态调整计算至低阶线性或高阶非线性路径，以实现空间自适应且高精度的物理场建模。

Result: PGOT在四个标准基准上达到了最先进的性能水平，并且在包括机翼设计与汽车设计在内的大规模工业任务中表现优异。

Conclusion: 本研究表明，通过引入几何意识和自适应路径选择策略，可以显著提高使用Transformers进行PDE建模时对复杂几何结构处理的能力。

Abstract: While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.

</details>


### [119] [PFed-Signal: An ADR Prediction Model based on Federated Learning](https://arxiv.org/abs/2512.23262)
*Tao Li,Peilin Li,Kui Lu,Yilei Wang,Junliang Shang,Guangshun Li,Huiyu Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦学习的ADR信号预测模型PFed-signal，通过欧氏距离消除FAERS中的偏差数据，从而提高ADR预测准确性。实验结果表明，该方法在准确率、F1分数、召回率和AUC等方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于FAERS记录预测的药物不良反应（ADRs）可能由于数据偏差导致在线诊断误导。尽管已有方法如优化报告比值比（ROR）或比例报告比（PRR）尝试解决这一问题，但它们依赖于统计方法，无法彻底排除偏差数据，因此影响了信号预测的准确性。

Method: 提出了名为PFed-signal的ADR信号预测模型，首先通过Pfed-Split方法根据ADR将原始数据集分割成子集；接着引入ADR-signal模型，包括基于联邦学习的偏差数据识别技术和基于Transformer架构的ADR预测模型。其中，偏差数据识别技术利用欧氏距离来识别并剔除偏差数据，生成干净的数据集用于训练后续的预测模型。

Result: 实验结果显示，在经过清理后的数据集上计算得到的ROR和PRR指标优于传统方法。此外，PFed-Signal模型在准确率(0.887)、F1分数(0.890)、召回率(0.913)以及AUC(0.957)方面均表现出色，超过了基准模型的表现。

Conclusion: 提出的PFed-signal模型能够有效减少FAERS中偏差数据对ADR预测的影响，提高了预测精度和其他评价指标，为更准确地进行药物安全性评估提供了一个新的解决方案。

Abstract: The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.

</details>


### [120] [On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases](https://arxiv.org/abs/2512.23265)
*Alexander Korotin,Gudmund Pammer*

Main category: cs.LG

TL;DR: 本文研究了具有有限指数矩的分布之间的流匹配（FM）逆问题，该问题受到现代生成式AI应用的启发，例如流匹配模型的蒸馏。在两种情况下建立了唯一解：一维设置和高斯情况。一般多维问题仍需未来的研究。


<details>
  <summary>Details</summary>
Motivation: 受现代生成式AI应用如流匹配模型蒸馏的启发，研究者对具有有限指数矩的分布间的流匹配逆问题进行了探讨。

Method: 文章通过理论分析，在一维环境和高斯条件下证明了解的唯一性。

Result: 研究表明在一维设置以及高斯情形下，流匹配逆问题存在唯一解。

Conclusion: 尽管在一维与高斯场景中已证明流匹配逆问题有唯一解，但多维度下的通用解决方案仍待进一步探索。

Abstract: This paper studies the inverse problem of flow matching (FM) between distributions with finite exponential moment, a problem motivated by modern generative AI applications such as the distillation of flow matching models. Uniqueness of the solution is established in two cases - the one-dimensional setting and the Gaussian case. The general multidimensional problem remains open for future studies.

</details>


### [121] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了多模型协作定律，该定律基于集成模型的总参数预算预测其性能极限。实验结果表明，与单一模型相比，多模型系统在总参数数量上遵循幂律缩放，并表现出更显著的改进趋势和更低的理论损失下限。此外，异构模型家族的集成比单个模型家族内的集成实现了更好的性能缩放，这表明模型多样性是协作收益的主要驱动力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）通过增加模型参数和数据量获得了性能提升，但任何单一LLM的能力都是有限的。通过多个LLM之间的复杂交互作用可以使其集体表现超越任何单个模型。然而，目前缺乏一个统一的理论框架来解释多模型协作时的性能缩放规律。

Method: 研究者提出了一种名为“多模型协作定律”的缩放法则，它根据集成模型的汇总参数预算预测LLM集合的表现限制。为了量化多模型协作的内在上限，采用了方法无关的形式化描述，并假设了一个理想化的集成预言机，在这个预言机中每个样本的总交叉熵损失由模型池中任一模型的最小损失决定。

Result: 实验结果显示，相对于总的参数数量，多模型系统遵循幂律缩放规则，相较于单模型缩放而言展现出更加明显的改善趋势以及更低的理论损失底线。另外，来自不同模型家族的组合比起同一家族内部形成的组合能够实现更好的性能扩展性，这意味着模型多样性是促进合作增益的关键因素。

Conclusion: 这些发现表明，模型间的协作代表了扩大LLMs智能边界的一个关键方向。

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [122] [ISOPO: Proximal policy gradients without pi-old](https://arxiv.org/abs/2512.23353)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: 本文介绍了一种名为ISOPO的新方法，该方法能够在单次梯度步骤中有效近似自然策略梯度，相比现有的近端策略优化方法更高效。


<details>
  <summary>Details</summary>
Motivation: 为了提高现有策略优化方法（如GRPO或CISPO）的效率，这些方法通常需要通过多次梯度步骤来近似相对于参考策略的自然梯度步骤。

Method: 提出的方法称为Isometric Policy Optimization (ISOPO)，它通过在Fisher度量中对每个序列的对数概率梯度进行归一化然后与优势结合来工作。此外，还有一种变体使用神经切线核在每一层转换微批次的优势，并且这种方法可以在单个后向传递中分层应用。

Result: ISOPO可以以几乎可以忽略不计的计算开销实现，与普通的REINFORCE相比。

Conclusion: ISOPO提供了一种更加高效的策略优化途径，在单次梯度更新中即可达到良好的性能。

Abstract: This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.

</details>


### [123] [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)
*Yilun Luo,HuaQing Zheng,Haoqian Meng,Wenyuan Liu,Peng Zhang*

Main category: cs.LG

TL;DR: 本文介绍了华为的openPangu-Embedded模型通过低比特量化技术在Ascend NPUs上实现高效Chain-of-Thought推理的方法，同时保持了较高的模型准确性。


<details>
  <summary>Details</summary>
Motivation: 为了应对在Ascend NPUs上部署时由于采用三种不同的Chain-of-Thought（CoT）推理模式而带来的内存占用和延迟增加的问题，寻找一种可以减少这些开销同时保持模型性能的方法。

Method: 通过引入统一的低位推理框架支持INT8 (W8A8) 和 W4A8 量化，特别是针对Atlas A2上的openPangu-Embedded模型进行了优化。

Result: INT8量化在所有三种CoT模式下都能保持超过90%的FP16基线准确率，并且在Atlas A2上实现了1.5倍的预填充加速；W4A8虽然对准确性有一定影响但显著降低了内存消耗。

Conclusion: 低比特量化为Ascend NPUs上开放Pangu嵌入式模型的有效CoT推理提供了一种解决方案，在保证较高模型保真度的同时提高了效率。

Abstract: Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

</details>


### [124] [On the Sample Complexity of Learning for Blind Inverse Problems](https://arxiv.org/abs/2512.23405)
*Nathan Buskulic,Luca Calatroni,Lorenzo Rosasco,Silvia Villa*

Main category: cs.LG

TL;DR: 本文通过线性最小均方估计器（LMMSEs）框架，对盲反问题中的学习进行了深入的理论分析，并提供了最优估计量的闭式表达。文章还证明了在特定条件下与Tikhonov正则化形式等价，并且推导了有限样本误差界来表征所学估计量的表现。


<details>
  <summary>Details</summary>
Motivation: 盲反问题出现在许多前向算子部分或完全未知的实验设置中，现有的非盲方法难以直接应用；虽然数据驱动方法在解决这些问题上表现出色，但缺乏可解释性和坚实的理论支持，限制了其在如成像逆问题等领域的可靠性。

Method: 采用线性最小均方估计器(LMMSE)简化框架，进行深度理论分析，包括得出最佳估计量的封闭形式解、与适当选取的Tikhonov正则化方案建立等价关系及证明在一定源条件假设下的收敛结果。

Result: 得出了关于噪声水平、问题适定性以及可用样本数量如何影响学习到的估计器性能的严格有限样本错误界限。这些界限明确量化了算子随机性的影响，并揭示了当这种随机性消失时相应的收敛率。

Conclusion: 本研究为盲反问题的学习提供了一个更清晰的理解视角，通过LMMSE框架下的理论分析和数值实验验证了所提出的收敛行为预测。

Abstract: Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.

</details>


### [125] [Stochastic Siamese MAE Pretraining for Longitudinal Medical Images](https://arxiv.org/abs/2512.23441)
*Taha Emre,Arunava Chakravarty,Thomas Pinetz,Dmitrii Lachinov,Martin J. Menten,Hendrik Scholl,Sobha Sivaprasad,Daniel Rueckert,Andrew Lotery,Stefan Sacu,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.LG

TL;DR: 本文提出了一种名为STAMP的新框架，该框架通过在Masked Autoencoding (MAE)中引入时间信息来学习疾病进展的非确定性时间动态。实验表明，使用STAMP预训练的ViT模型在预测年龄相关性黄斑变性和阿尔茨海默病进展方面优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 当前先进的自监督学习方法如Masked Autoencoding (MAE)，虽然具有强大的表示学习能力，但缺乏对时间信息的捕捉，这对于理解纵向医学数据集中的疾病进展至关重要。

Method: 提出了STAMP（Stochastic Temporal Autoencoder with Masked Pretraining），一种Siamese MAE框架，它通过基于两输入体积间的时间差进行条件设置的过程来编码时间信息。STAMP通过将MAE重建损失重构为条件变分推理目标的方式随机地学习时间动态。

Result: 在两个OCT和一个MRI数据集上评估了STAMP，这些数据集包含每位患者的多次访问记录。结果表明，使用STAMP预训练的ViT模型在不同阶段的年龄相关性黄斑变性和阿尔茨海默病进展预测方面优于现有时间感知的MAE方法及基础模型。

Conclusion: STAMP作为一种新颖的方法，能够有效地从纵向医学图像数据中学习到疾病的非确定性时间动态，并且在实际应用中对于疾病进展预测表现出优越性能。

Abstract: Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.

</details>


### [126] [Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion](https://arxiv.org/abs/2512.23448)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出了一种名为动态子空间组合（DSC）的新框架，用于近似依赖于上下文的权重，通过状态依赖的稀疏扩展共享基底库来实现。该方法相比标准混合低秩自适应（Mixture-of-LoRAs）减少了参数复杂度和内存流量，并提供了对动态更新严格的最坏情况界限。


<details>
  <summary>Details</summary>
Motivation: 为了解决专家混合（MoE）模型在容量扩展时遇到的表示崩溃和梯度不稳定性问题。

Method: 通过引入一种基于状态依赖的、从共享基底库中稀疏扩展的技术——动态子空间组合（DSC），该技术将权重更新建模为星形域内的残差轨迹，并采用幅度门控单纯形插值保证了身份处的连续性。与传统的独立检索秩-r矩阵的方法不同，DSC从解耦的单位范数基向量构建组合秩-K近似。

Result: DSC方法成功地将参数复杂度从O(Mrd)降低到O(Md)，并将内存流量减少至O(Kd)，同时利用框架理论正则化和谱约束提供了关于动态更新的严格最坏情况边界。

Conclusion: DSC作为一种新颖的方法，有效地解决了MoE模型中存在的问题，通过优化权重更新过程减少了计算资源的需求，并确保了模型性能的稳定性。

Abstract: Mixture of Experts (MoE) models scale capacity but often suffer from representation collapse and gradient instability. We propose Dynamic Subspace Composition (DSC), a framework that approximates context-dependent weights via a state-dependent, sparse expansion of a shared basis bank. Formally, DSC models the weight update as a residual trajectory within a Star- Shaped Domain, employing a Magnitude-Gated Simplex Interpolation to ensure continuity at the identity. Unlike standard Mixture-of-LoRAs, which incurs O(M rd) parameter complexity by retrieving independent rank-r matrices, DSC constructs a compositional rank-K approximation from decoupled unit-norm basis vectors. This reduces parameter complexity to O(M d) and memory traffic to O(Kd), while Frame-Theoretic regularization and spectral constraints provide rigorous worst-case bounds on the dynamic update. The code is available at https://github. com/VladimerKhasia/DSC

</details>


### [127] [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)
*Zhuo Li,Pengyu Cheng,Zhechao Yu,Feifei Tong,Anningzhe Gao,Tsung-Hui Chang,Xiang Wan,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于信息理论的去偏方法DIR，用于缓解奖励模型中的复杂和多样化的诱导偏差。通过最大化RM分数与人类偏好对之间的互信息，同时最小化RM输出与偏好输入的有偏属性之间的互信息，DIR能够处理非线性相关的更复杂的偏差类型，从而提高RLHF性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型（RMs）在从人类反馈中进行强化学习（RLHF）时面临着低质量训练数据的问题，这导致了过拟合和奖励黑客等挑战。特别是，响应长度成为了一个不可避免的诱导偏差。现有的一些去偏方法要么只针对单一类型的偏差，要么仅使用简单的线性相关性来建模问题。因此，需要一种可以处理更复杂、更多样化偏差的方法。

Method: 提出了一个名为DIR的新方法，该方法基于信息瓶颈原理，旨在最大化奖励模型评分与人类偏好对之间的互信息，同时最小化奖励模型输出与偏好输入中存在偏差属性间的互信息。这种方法能够应对非线性关联的复杂偏差情况。

Result: 实验结果表明，DIR不仅有效地减轻了目标诱导偏差（如响应长度、谄媚性和格式），还提高了不同基准测试上的RLHF表现，展现出更好的泛化能力。

Conclusion: 通过引入DIR这一新方法，我们为奖励模型提供了一种有效手段以减少复杂多样的诱导偏差，并且此方法在提升RLHF性能方面显示出了显著效果。

Abstract: Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.

</details>


### [128] [FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence](https://arxiv.org/abs/2512.23485)
*Guoan Wan,Tianyu Chen,Fangzheng Feng,Haoyi Zhou,Runhua Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的微调方法FRoD，通过结合层次联合分解和旋转自由度来提高参数效率微调方法的表现力和效率，使得在视觉、推理和语言理解等20个基准测试中，FRoD仅使用1.72%的可训练参数就能达到与全模型微调相同的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法虽然减少了计算和内存成本，但由于其固有的低秩约束，通常存在收敛缓慢和适应能力有限的问题，这限制了它们捕捉完成多样化任务所需复杂模式的能力。

Method: 提出了FRoD，一种结合了层次联合分解与旋转自由度的新微调方法。该方法通过跨层提取全局共享基，并对缩放因子注入稀疏可学习扰动以实现灵活的全秩更新，从而增强表达能力和效率。

Result: FRoD在覆盖视觉、推理及语言理解领域的20项基准测试中表现优异，在保持相同训练预算的情况下，仅需调整1.72%的参数即可达到与全模型微调相媲美的准确性。

Conclusion: FRoD作为一种新颖的微调方法，成功解决了现有PEFT方法面临的局限性，实现了更快速且稳健的收敛，同时极大地提高了参数利用效率。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.

</details>


### [129] [Trustworthy Machine Learning under Distribution Shifts](https://arxiv.org/abs/2512.23524)
*Zhuo Huang*

Main category: cs.LG

TL;DR: 本文探讨了在分布偏移下可信机器学习的问题，旨在提高AI的鲁棒性、多功能性、责任性和可靠性。研究针对扰动偏移、领域偏移和模态偏移三种常见情况，并从鲁棒性、可解释性和适应性三方面严格考察可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能（AI）模型在视觉识别、语言对齐等方面表现出色，甚至初步发展出了通用智能，但分布偏移问题一直是限制机器学习系统可靠性和泛用性的关键障碍。此外，分布偏移下的泛化能力还会引发对AI的信任问题。

Method: 作者的研究集中在分布偏移条件下的可信机器学习，具体研究了三种常见的分布偏移：扰动偏移、领域偏移和模态偏移。对于所有这些场景，通过鲁棒性、可解释性和适应性三个方面来仔细探究系统的可信度。

Result: 基于上述研究维度，提出了有效的解决方案并提供了基础见解，同时力求增强机器学习中的关键问题，如效率、适应性和安全性。

Conclusion: 该研究致力于解决由于分布偏移导致的AI信任问题，通过增强鲁棒性、可解释性和适应性，以期提升AI的整体性能与社会应用价值。

Abstract: Machine Learning (ML) has been a foundational topic in artificial intelligence (AI), providing both theoretical groundwork and practical tools for its exciting advancements. From ResNet for visual recognition to Transformer for vision-language alignment, the AI models have achieved superior capability to humans. Furthermore, the scaling law has enabled AI to initially develop general intelligence, as demonstrated by Large Language Models (LLMs). To this stage, AI has had an enormous influence on society and yet still keeps shaping the future for humanity. However, distribution shift remains a persistent ``Achilles' heel'', fundamentally limiting the reliability and general usefulness of ML systems. Moreover, generalization under distribution shift would also cause trust issues for AIs. Motivated by these challenges, my research focuses on \textit{Trustworthy Machine Learning under Distribution Shifts}, with the goal of expanding AI's robustness, versatility, as well as its responsibility and reliability. We carefully study the three common distribution shifts into: (1) Perturbation Shift, (2) Domain Shift, and (3) Modality Shift. For all scenarios, we also rigorously investigate trustworthiness via three aspects: (1) Robustness, (2) Explainability, and (3) Adaptability. Based on these dimensions, we propose effective solutions and fundamental insights, meanwhile aiming to enhance the critical ML problems, such as efficiency, adaptability, and safety.

</details>


### [130] [EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition](https://arxiv.org/abs/2512.23526)
*Maryam Mirzaei,Farzaneh Shayegh,Hamed Narimani*

Main category: cs.LG

TL;DR: 提出了一种名为EGDA的框架，通过同时对齐全局和类特定分布并保持EEG数据的内在结构，来减少跨会话变异性，从而提高情绪识别的准确性。在SEED-IV数据集上的实验表明，EGDA在三个迁移任务中均表现出色，并且Gamma频段以及中央顶叶和前额叶脑区被认定为情绪识别的关键因素。


<details>
  <summary>Details</summary>
Motivation: 由于记录会话间的差异给模型泛化带来了挑战，因此需要一种方法来减少这些差异以提高基于EEG的情绪识别系统的性能。

Method: 提出了EGDA框架，该框架旨在通过联合对齐全局（边缘）和类特定（条件）分布同时利用图正则化保持EEG数据的内在结构，来减少跨会话的变化。

Result: EGDA在SEED-IV数据集上实现了强大的跨会话表现，在三个迁移任务中分别达到了81.22%、80.15%和83.27%的准确率，并超越了多个基线方法。此外，分析显示Gamma频率带是最具区分性的，并且中央顶叶和前额叶大脑区域对于可靠的情绪识别至关重要。

Conclusion: EGDA框架能够有效地减少跨会话变化，提高了基于EEG的情绪识别系统的表现，并且确定了Gamma频段及某些大脑区域在情绪识别中的关键作用。

Abstract: Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.

</details>


### [131] [VL-RouterBench: A Benchmark for Vision-Language Model Routing](https://arxiv.org/abs/2512.23562)
*Zhehao Huang,Baijiong Lin,Jingyuan Zhang,Jingying Wang,Yuhang Liu,Ning Lu,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: 本文提出了VL-RouterBench，一个用于系统评估视觉-语言模型路由系统能力的基准。该基准覆盖了14个数据集、17个模型，并评估了10种路由方法和基线，发现当前最佳路由器与理想状态仍有差距，表明在路由架构方面还有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的多模型路由技术缺乏系统且可重复的基准来评估视觉-语言模型(VLMs)的表现。

Method: 通过构建包含14个数据集、17个开源及API模型的VL-RouterBench基准测试，基于原始推理和评分日志创建质量与成本矩阵，并通过平均准确率、平均成本及吞吐量综合衡量性能，采用归一化成本和准确率的调和平均值作为排名分数。

Result: 评估了10种路由方法和基线，观察到了显著的路由性能提升；然而，即使是最优的现有路由器也明显不如理想的Oracle表现，这表明通过更精细的视觉线索和文本结构建模可以在路由器架构上取得进一步改进。

Conclusion: VL-RouterBench为视觉-语言模型路由系统的评估提供了一个全面而系统的基准，揭示了当前方法与理想状态之间的差距，并指出了未来研究的方向。此外，作者承诺将公开所有数据构建和评估工具链以促进比较性、可重复性和实际部署。

Abstract: Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.

</details>


### [132] [Distribution-Free Process Monitoring with Conformal Prediction](https://arxiv.org/abs/2512.23602)
*Christopher Burger*

Main category: cs.LG

TL;DR: 本文提出了一种将无分布、模型无关的保形预测与传统统计过程控制相结合的混合框架，以提高复杂制造环境下的质量控制可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的统计过程控制（SPC）对于质量管理至关重要，但在现代复杂的制造环境中，由于其依赖于经常被违反的统计假设，导致监控不可靠。

Method: 作者提出了一个混合框架，该框架通过整合保形预测来增强SPC，包括两个新的应用：保形增强控制图和保形增强过程监控。前者可视化过程不确定性并启用如'不确定性峰值'这样的主动信号；后者则将多变量控制重新定义为使用直观p值图表的形式异常检测问题。

Result: 这种新方法提供了一种更稳健且统计上更为严谨的质量控制手段，同时保持了经典方法的可解释性和易用性。

Conclusion: 本研究介绍的方法为在保持传统方法易于理解和使用的同时，解决现代制造业中遇到的质量控制挑战提供了可能性。

Abstract: Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.

</details>


### [133] [Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning](https://arxiv.org/abs/2512.23617)
*Deniz Akdemir*

Main category: cs.LG

TL;DR: 本文提出了一种基于Le Cam统计实验理论的决策论框架，通过引入Le Cam失真（由缺陷距离δ(E1, E2)量化）来替代对称不变性，实现有方向性的模拟性。该方法在基因组学、视觉和强化学习等五个实验中展示了优越的表现，特别是在避免负迁移方面，为医疗影像、自主系统和精准医学等领域提供了首个风险可控的迁移学习框架。


<details>
  <summary>Details</summary>
Motivation: 当前主流的无监督领域适应(UDA)方法通过强制特征不变性来使源域和目标域表示一致，但当两个领域的信息量不同时，这种方法会导致信息破坏，从而引起可能在关键安全应用中有灾难性影响的‘负迁移’。因此，需要一种新的方法来解决这一问题。

Method: 提出了一种基于Le Cam统计实验理论的决策论框架，利用构造近似法取代对称不变性，转而追求方向性可模拟性，并引入了Le Cam失真作为迁移风险的一个严格上限度量。通过学习一个能够从源模拟到目标的核，此方法允许在不损害源质量的情况下进行知识迁移。

Result: 在五个不同类型的实验（包括HLA基因组学、CIFAR-10图像分类以及强化学习中的控制任务）中，Le Cam失真方法展现出了优异的结果：在HLA基因组学中几乎完美地估计了频率；于CIFAR-10图像分类任务上保持了81.2%的准确率，而基于不变性的方法则下降到了34.7%；在RL控制任务里实现了安全策略转移，而其他基于不变性的方法却遭遇了灾难性崩溃。

Conclusion: Le Cam Distortion提供了一个新的风险控制迁移学习框架，特别适用于那些不允许出现负迁移的关键领域如医学成像、自动驾驶系统及精准医疗。

Abstract: Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.

</details>


### [134] [Random Controlled Differential Equations](https://arxiv.org/abs/2512.23670)
*Francesco Piatti,Thomas Cass,William F. Turner*

Main category: cs.LG

TL;DR: 提出了一种结合随机特征与控制微分方程的时间序列学习框架，包括RF-CDEs和R-RDEs两种变体，在多种时间序列基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了提高时间序列学习的效率并保留强归纳偏置，同时减少训练成本。

Method: 开发了基于随机特征与控制微分方程相结合的新框架，具体表现为Random Fourier CDEs (RF-CDEs)和Random Rough DEs (R-RDEs)两种模型。前者通过随机傅里叶特征映射输入信号，后者直接作用于粗糙路径输入上，并利用对数签名捕捉高阶时间交互。

Result: 在一系列时间序列基准测试中，这两种方法展示了竞争性或最先进的性能。

Conclusion: 该研究为连续时间深度架构提供了一个统一视角，同时保持了使用随机特征带来的效率优势。

Abstract: We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.

</details>


### [135] [End-to-End Test-Time Training for Long Context](https://arxiv.org/abs/2512.23675)
*Arnuv Tandon,Karan Dalal,Xinhao Li,Daniel Koceja,Marcel Rød,Sam Buchanan,Xiaolong Wang,Jure Leskovec,Sanmi Koyejo,Tatsunori Hashimoto,Carlos Guestrin,Jed McCaleb,Yejin Choi,Yu Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种将长上下文语言建模视为持续学习问题的方法，而非单纯依赖架构设计。通过使用标准的Transformer加上滑动窗口注意力机制，并在测试时通过下一个令牌预测继续学习，同时利用元学习改进模型初始化。这种端到端（E2E）方法，在不同上下文长度下的扩展性与全注意力机制的Transformer相当，但具有恒定的推理延迟，使得对于128K上下文长度的情况，其速度比全注意力机制快2.7倍。


<details>
  <summary>Details</summary>
Motivation: 研究者旨在解决长上下文语言建模问题，希望通过一种新的视角——将其视作一个持续学习的问题而不是仅依靠改变模型架构来实现性能提升。

Method: 采用了标准的Transformer架构结合滑动窗口注意力机制；模型能够在测试阶段通过对给定上下文进行下一个令牌预测而继续学习，从而将读取到的信息压缩进权重中；此外，通过训练时的元学习来优化模型对测试阶段学习的初始化设置。

Result: 所提出的方法(TTT-E2E)在30亿参数规模、使用1640亿个令牌训练的情况下，相对于其他方案如Mamba 2和Gated DeltaNet，展示了与采用完全注意力机制的Transformer相同的随上下文长度变化的扩展特性；并且，类似于RNN，TTT-E2E无论上下文多长都保持恒定的推理延迟，在处理128K长度的上下文时比全注意力机制快了2.7倍。

Conclusion: 通过将长上下文语言建模重新定义为一个持续学习问题，本研究提出了一种新颖且有效的解决方案，不仅保持了良好的扩展性，还显著提高了处理极长文本序列时的速度。

Abstract: We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.

</details>


### [136] [Training AI Co-Scientists Using Rubric Rewards](https://arxiv.org/abs/2512.23707)
*Shashwat Goel,Rishi Hazra,Dulhan Jayalath,Timon Willi,Parag Jain,William F. Shen,Ilias Leontiadis,Francesco Barbieri,Yoram Bachrach,Jonas Geiping,Chenxi Whitehouse*

Main category: cs.LG

TL;DR: 本文研究了如何利用大量现有研究论文来训练语言模型，以生成更符合要求的研究计划。通过自动从多个领域的论文中提取研究目标和特定目标的评分标准，然后使用强化学习和自我评分的方式训练模型。实验结果表明，该方法在机器学习、医学研究等领域均表现出色，且无需外部人类监督即可实现改进。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型难以生成完全符合所有约束条件和隐含要求的研究计划。因此，本研究旨在探索一种新的方法，即利用现有的大量研究论文来训练能够更好地辅助研究人员制定研究计划的语言模型。

Method: 1. 从不同领域的论文中自动提取研究目标及针对这些目标的评分标准，构建一个可扩展、多样化的训练语料库。
2. 使用强化学习结合自我评分机制对模型进行训练，在此过程中，初始策略的一个冻结副本充当评分者角色，而评分标准则创建了一个生成器-验证者之间的差距，使得模型可以在没有外部人类监督的情况下得到改善。
3. 通过与领域内专家合作开展研究，特别是在机器学习研究目标方面进行了长达225小时的研究，以评估模型的表现。

Result: 1. 在机器学习研究目标上，经过微调后的Qwen3-30B-A3B模型相比初始模型获得了70%专家偏好的提升，并且84%自动生成的目标特异性评分标准得到了专家的认可。
2. 当将该方法应用于医学研究领域以及最新的arXiv预印本时，也观察到了12%-22%相对性能提升，并显示出显著的跨领域泛化能力。
3. 证明了即使是在执行反馈不可行的问题设置下（如医学研究），所提出的方法依然有效。

Conclusion: 这项工作展示了一种可扩展的自动化训练方法对于提高通用AI协同科学家能力方面的潜力。它不仅提高了模型生成研究计划的质量，还展示了良好的跨领域适应性。

Abstract: AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.

</details>
