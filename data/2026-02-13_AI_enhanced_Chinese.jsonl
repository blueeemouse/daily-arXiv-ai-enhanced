{"id": "2602.11235", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11235", "abs": "https://arxiv.org/abs/2602.11235", "authors": ["Xin Song", "Zhilin Guan", "Ruidong Han", "Binghao Tang", "Tianwen Chen", "Bing Li", "Zihao Li", "Han Zhang", "Fei Jiang", "Chaolin Xie", "Chi Ma", "Chunyang Jiang", "Chunzhen Jing", "Dengxuan Li", "Fengyi Li", "Lei Yu", "Mengyao Sun", "Pu Wang", "Qing Wang", "Rui Fan", "Shangyu Chen", "Shifeng Du", "Siyuan Bai", "Wei Lin", "Wentao Zhu", "Zhou Han", "Zhuo Chen", "Zikang Xu"], "title": "MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan", "comment": null, "summary": "Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u63a8\u8350\u7cfb\u7edf\u6846\u67b6MTFM\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8de8\u57df\u548c\u591a\u573a\u666f\u65b9\u6cd5\u5728\u8d44\u6e90\u9700\u6c42\u548c\u8f93\u5165\u5bf9\u9f50\u4e0a\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5c06\u8de8\u57df\u6570\u636e\u8f6c\u6362\u4e3a\u5f02\u6784token\u3001\u7528\u6237\u7ea7\u6837\u672c\u805a\u5408\u4ee5\u53ca\u5f15\u5165\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\u7b49\u624b\u6bb5\uff0c\u5728\u4e0d\u4f9d\u8d56\u9884\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u6355\u83b7\u591a\u573a\u666f\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u8de8\u57df\uff08CDR\uff09\u548c\u591a\u573a\u666f\uff08MSR\uff09\u63a8\u8350\u7cfb\u7edf\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u8d44\u6e90\u5e76\u8981\u6c42\u4e25\u683c\u7684\u8f93\u5165\u5bf9\u9f50\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u53ef\u6269\u5c55\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\u3002", "method": "1. MTFM\u4f7f\u7528\u4e00\u79cd\u65e0\u9700\u9884\u5148\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u5c06\u8de8\u57df\u6570\u636e\u8f6c\u5316\u4e3a\u5f02\u6784token\u3002\n2. \u91c7\u7528\u591a\u573a\u666f\u7528\u6237\u7ea7\u6837\u672c\u805a\u5408\u6280\u672f\u6765\u51cf\u5c11\u5b9e\u4f8b\u603b\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\n3. \u5f15\u5165Grouped-Query Attention\u53ca\u5b9a\u5236\u5316\u7684Hybrid Target Attention\u4ee5\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\n4. \u5b9e\u65bd\u591a\u9879\u7cfb\u7edf\u7ea7\u4f18\u5316\u63aa\u65bd\uff0c\u5982\u5185\u6838\u878d\u5408\u548c\u6d88\u9664CPU-GPU\u963b\u585e\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u8bad\u7ec3\u4e0e\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u5747\u9a8c\u8bc1\u4e86MTFM\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u901a\u8fc7\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u53ca\u5229\u7528\u66f4\u591a\u7684\u591a\u573a\u666f\u8bad\u7ec3\u6570\u636e\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "MTFM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8de8\u57df\u548c\u591a\u573a\u666f\u95ee\u9898\uff0c\u5b83\u4e0d\u4ec5\u51cf\u5c11\u4e86\u8d44\u6e90\u9700\u6c42\u8fd8\u63d0\u9ad8\u4e86\u5904\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.11453", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11453", "abs": "https://arxiv.org/abs/2602.11453", "authors": ["Sajad Ebrahimi", "Bhaskar Mitra", "Negar Arabzadeh", "Ye Yuan", "Haolun Wu", "Fattane Zarrinkalam", "Ebrahim Bagheri"], "title": "From Noise to Order: Learning to Rank via Denoising Diffusion", "comment": null, "summary": "In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u7684\u6df1\u5ea6\u751f\u6210\u65b9\u6cd5DiffusionRank\uff0c\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u5b66\u4e60\u6392\u5e8f\uff08LTR\uff09\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5efa\u6a21\u7279\u5f81\u5411\u91cf\u548c\u76f8\u5173\u6027\u6807\u7b7e\u4e4b\u95f4\u7684\u5b8c\u6574\u8054\u5408\u5206\u5e03\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u7684\u5224\u522b\u5f0f\u65b9\u6cd5\u76f8\u6bd4\uff0cDiffusionRank\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u5b66\u4e60\u6392\u5e8f\uff08LTR\uff09\u65b9\u6cd5\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5224\u522b\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u80fd\u6839\u636e\u67e5\u8be2-\u6587\u6863\u5bf9\u7684\u67d0\u4e9b\u7279\u5f81\u8868\u793a\u6765\u5efa\u6a21\u6587\u6863\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6982\u7387\u3002\u4f5c\u8005\u5047\u8bbe\uff0c\u5728\u751f\u6210\u5f0f\u8bbe\u7f6e\u4e0b\u80fd\u591f\u89e3\u91ca\u6574\u4e2a\u6570\u636e\u5206\u5e03\u7684\u5019\u9009\u89e3\u51b3\u65b9\u6848\u4f1a\u4ea7\u751f\u66f4\u7a33\u5065\u7684\u6392\u5e8f\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86DiffusionRank\uff0c\u8fd9\u662f\u4e00\u79cd\u6269\u5c55\u4e86TabDiff\uff08\u4e00\u79cd\u73b0\u6709\u7684\u9488\u5bf9\u8868\u683c\u6570\u636e\u96c6\u7684\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4ee5\u521b\u5efa\u7ecf\u5178\u5224\u522b\u5f0f\u9010\u70b9\u548c\u6210\u5bf9LTR\u76ee\u6807\u7684\u751f\u6210\u7b49\u4ef7\u7269\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cDiffusionRank\u6a21\u578b\u76f8\u5bf9\u4e8e\u5176\u5224\u522b\u5f0f\u5bf9\u5e94\u7248\u672c\u663e\u793a\u51fa\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u6307\u51fa\uff0c\u5728\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u8fdb\u884c\u4fe1\u606f\u68c0\u7d22\u7684\u5b66\u4e60\u6392\u5e8f\u65b9\u9762\u5b58\u5728\u4e30\u5bcc\u7684\u672a\u6765\u7814\u7a76\u63a2\u7d22\u7a7a\u95f4\u3002"}}
{"id": "2602.11518", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11518", "abs": "https://arxiv.org/abs/2602.11518", "authors": ["Yupeng Li", "Ben Chen", "Mingyue Cheng", "Zhiding Liu", "Xuxin Zhang", "Chenyi Lei", "Wenwu Ou"], "title": "KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance", "comment": null, "summary": "E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aKuaiSearch\u7684\u7535\u5546\u641c\u7d22\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u57fa\u4e8e\u5feb\u624b\u5e73\u53f0\u771f\u5b9e\u7528\u6237\u641c\u7d22\u4e92\u52a8\u6784\u5efa\uff0c\u8986\u76d6\u51b7\u542f\u52a8\u7528\u6237\u548c\u957f\u5c3e\u5546\u54c1\uff0c\u5e76\u7cfb\u7edf\u5730\u6db5\u76d6\u4e86\u53ec\u56de\u3001\u6392\u5e8f\u548c\u76f8\u5173\u6027\u5224\u65ad\u4e09\u4e2a\u5173\u952e\u641c\u7d22\u9636\u6bb5\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u641c\u7d22\u5728\u8fde\u63a5\u7528\u6237\u9700\u6c42\u4e0e\u5927\u91cf\u5546\u54c1\u5e93\u5b58\u65b9\u9762\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u9762\u4e34\u7740\u67e5\u8be2\u6a21\u7cca\u3001\u5546\u54c1\u6587\u672c\u8bed\u4e49\u5f31\u53ca\u7528\u6237\u504f\u597d\u591a\u6837\u7b49\u6311\u6218\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u8bb8\u591a\u5c40\u9650\u6027\uff0c\u5982\u67e5\u8be2\u6784\u9020\u8fc7\u4e8e\u7b80\u5316\u3001\u6392\u9664\u4e86\u51b7\u542f\u52a8\u7528\u6237\u548c\u957f\u5c3e\u5546\u54c1\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u57fa\u4e8eLLM\u7684\u7535\u5b50\u5546\u52a1\u641c\u7d22\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5e76\u53d1\u5e03KuaiSearch\u6570\u636e\u96c6\u6765\u5e94\u5bf9\u4e0a\u8ff0\u6311\u6218\uff0c\u8be5\u6570\u636e\u96c6\u57fa\u4e8e\u5feb\u624b\u5e73\u53f0\u7684\u771f\u5b9e\u7528\u6237\u641c\u7d22\u4ea4\u4e92\uff0c\u4fdd\u7559\u4e86\u771f\u5b9e\u7684\u7528\u6237\u67e5\u8be2\u548c\u81ea\u7136\u8bed\u8a00\u5f62\u5f0f\u7684\u5546\u54c1\u6587\u672c\uff0c\u540c\u65f6\u6db5\u76d6\u4e86\u641c\u7d22\u6d41\u7a0b\u4e2d\u7684\u53ec\u56de\u3001\u6392\u5e8f\u4ee5\u53ca\u76f8\u5173\u6027\u5224\u65ad\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\u3002", "result": "\u5bf9KuaiSearch\u8fdb\u884c\u4e86\u591a\u89d2\u5ea6\u7efc\u5408\u5206\u6790\uff0c\u5e76\u5728\u591a\u4e2a\u4ee3\u8868\u6027\u641c\u7d22\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86\u57fa\u51c6\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKuaiSearch\u4e3a\u5b9e\u9645\u7535\u5b50\u5546\u52a1\u641c\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u57fa\u77f3\u3002", "conclusion": "KuaiSearch\u4f5c\u4e3a\u76ee\u524d\u53ef\u7528\u7684\u6700\u5927\u7535\u5b50\u5546\u52a1\u641c\u7d22\u6570\u636e\u96c6\u4e4b\u4e00\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u4e3b\u8981\u95ee\u9898\uff0c\u4e3a\u63a8\u52a8\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7535\u5b50\u5546\u52a1\u641c\u7d22\u6280\u672f\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2602.11562", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11562", "abs": "https://arxiv.org/abs/2602.11562", "authors": ["Tianhe Lin", "Ziwei Xiong", "Baoyuan Ou", "Yingjie Qin", "Lai Xu", "Xiaocheng Zhong", "Yao Hu", "Zhiyong Wang", "Tao Zhou", "Yubin Xu", "Di Wu"], "title": "LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling", "comment": "9 pages", "summary": "Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict \"Latency Wall\", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.", "AI": {"tldr": "LASER, developed by Xiaohongshu, optimizes both system and algorithmic efficiency to model ultra-long user behavior sequences for recommendation systems, overcoming high I/O latency and computational complexity. It employs SeqVault for efficient data retrieval and Segmented Target Attention (STA) with a Global Stacked Target Attention (GSTA) module for effective sequence compression, achieving notable performance improvements in both offline evaluations and large-scale online A/B testing.", "motivation": "The motivation is to overcome the 'Latency Wall' in real-time industrial environments when modeling ultra-long user behavior sequences, which is crucial for capturing evolving and lifelong user interests in recommendation systems. The primary challenges are the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms.", "method": "The method involves a full-stack optimization framework called LASER, featuring two main innovations: 1) System Efficiency - SeqVault, a unified schema-aware serving infrastructure that uses a hybrid DRAM-SSD indexing strategy to reduce retrieval latency and CPU usage; 2) Algorithmic Efficiency - Segmented Target Attention (STA) mechanism, which utilizes a sigmoid-based gating strategy to filter out irrelevant items, followed by a lightweight Global Stacked Target Attention (GSTA) module to refine these segments and capture cross-segment dependencies without high computational costs.", "result": "Extensive offline evaluations show that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.", "conclusion": "LASER, through its innovative approach to both system and algorithmic efficiency, successfully addresses the key challenges in deploying models for ultra-long user behavior sequences in real-time industrial settings, leading to improved performance and substantial business benefits."}}
{"id": "2602.11164", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11164", "abs": "https://arxiv.org/abs/2602.11164", "authors": ["Weiting Liu", "Han Wu", "Yufei Kuang", "Xiongwei Han", "Tao Zhong", "Jianfeng Feng", "Wenlian Lu"], "title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective", "comment": null, "summary": "Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\\textbf{m}ated opt\\textbf{i}mization modeli\\textbf{n}g via a localizable error-\\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \\textbf{D}ynamic Supervised \\textbf{F}ine-Tuning \\textbf{P}olicy \\textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9519\u8bef\u9a71\u52a8\u5b66\u4e60\u6846\u67b6MIND\uff0c\u901a\u8fc7\u5c40\u90e8\u5316\u9519\u8bef\u9a71\u52a8\u7684\u89c6\u89d2\u6765\u81ea\u5b9a\u4e49\u6574\u4e2a\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ece\u6570\u636e\u5408\u6210\u5230\u540e\u8bad\u7ec3\u9636\u6bb5\u3002\u8fd9\u79cd\u65b9\u6cd5\u9488\u5bf9\u73b0\u6709\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u65b9\u6cd5\u4e2d\u7a00\u758f\u6027\u95ee\u9898\u548c\u96be\u4ee5\u89e3\u51b3\u7684\u95ee\u9898\u5956\u52b1\u7a00\u758f\u6027\u7684\u9650\u5236\uff0c\u5b9e\u9a8c\u8868\u660eMIND\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6240\u6709\u81ea\u52a8\u4f18\u5316\u5efa\u6a21\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u867d\u7136\u5728\u8f85\u52a9\u590d\u6742\u4eba\u7c7b\u51b3\u7b56\u65b9\u9762\u663e\u793a\u51fa\u4e86\u6f5c\u529b\uff0c\u4f46\u5176\u6548\u80fd\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u4e0d\u8db3\u5229\u7528\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u5c40\u9650\uff1a\u7279\u5b9a\u9519\u8bef\u95ee\u9898\u7684\u7a00\u758f\u6027\u4ee5\u53ca\u4e0e\u96be\u9898\u76f8\u5173\u7684\u7a00\u758f\u5956\u52b1\u3002\u8fd9\u4e9b\u5c40\u9650\u5bfc\u81f4\u4e86\u9886\u57df\u7279\u5b9a\u540e\u8bad\u7ec3\u4e2d\u7684\u6b21\u4f18\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aMIND\uff08\u901a\u8fc7\u5c40\u90e8\u5316\u9519\u8bef\u9a71\u52a8\u89c6\u89d2\u5b9e\u73b0\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\uff09\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u89c2\u5bdf\u5230\u4f18\u5316\u5efa\u6a21\u4e2d\u9519\u8bef\u4f20\u64ad\u7684\u72ec\u7279\u53ef\u5b9a\u4f4d\u6a21\u5f0f\uff0c\u5e76\u4e14\u5f15\u5165\u4e86\u52a8\u6001\u76d1\u7763\u5fae\u8c03\u7b56\u7565\u4f18\u5316(DFPO)\u6765\u5904\u7406\u96be\u95ee\u9898\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMIND\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u65b9\u6cd5\u3002", "conclusion": "MIND\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u9519\u8bef\u9a71\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u514b\u670d\u73b0\u6709\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u65b9\u6cd5\u4e2d\u7684\u5173\u952e\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u4f18\u5316\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.11209", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11209", "abs": "https://arxiv.org/abs/2602.11209", "authors": ["Ziyi Yang", "Kalit Inani", "Keshav Kabra", "Vima Gupta", "Anand Padmanabha Iyer"], "title": "SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code", "comment": "11 pages, 6 figures, 4 tables", "summary": "While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6d4b\u8bd5\u6846\u67b6SAFuzz\uff0c\u5229\u7528LLM\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u6a21\u7cca\u6d4b\u8bd5\u6765\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u7b97\u6cd5\u6f0f\u6d1e\u3002\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cSAFuzz\u5728\u63d0\u9ad8\u6f0f\u6d1e\u8bc6\u522b\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u65f6\u95f4\u6210\u672c\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u7684\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u65f6\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9519\u8bef\u68c0\u6d4b\u7387\u3002", "motivation": "\u968f\u7740AI\u7f16\u7801\u52a9\u624b\u52a0\u901f\u4e86\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\uff0c\u5f53\u524d\u7684\u6d4b\u8bd5\u6846\u67b6\u96be\u4ee5\u8ddf\u4e0a\u7531\u6b64\u4ea7\u751f\u7684\u5927\u91cfAI\u751f\u6210\u4ee3\u7801\u7684\u6b65\u4f10\u3002\u4f20\u7edf\u7684\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\u5f80\u5f80\u5747\u5300\u5206\u914d\u8d44\u6e90\uff0c\u7f3a\u4e4f\u5bf9\u7b97\u6cd5\u6f0f\u6d1e\u6a21\u5f0f\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5bfc\u81f4\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u9057\u6f0f\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aSAFuzz\u7684\u6df7\u5408\u6d4b\u8bd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u884c\u4e3a\u591a\u6837\u5316\u3001\u5e26\u6709\u95ee\u9898\u7279\u5b9a\u9884\u8a00\u673a\u7684\u6355\u83b7\u751f\u6210\u5668\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9884\u6d4b\u5668\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u548c\u52a8\u6001\u63d0\u524d\u505c\u6b62\u3002", "result": "\u5728CSES\u7b97\u6cd5\u95ee\u9898\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5GreenFuzz, SAFuzz\u5c06\u6f0f\u6d1e\u533a\u5206\u7cbe\u5ea6\u4ece77.9%\u63d0\u5347\u5230\u4e8685.7%\uff0c\u540c\u65f6\u65f6\u95f4\u6210\u672c\u964d\u4f4e\u4e861.71\u500d\u3002\u6b64\u5916\uff0c\u5f53\u4e0e\u73b0\u6709\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u76f8\u7ed3\u5408\u65f6\uff0c\u9519\u8bef\u68c0\u6d4b\u53ec\u56de\u7387\u4ece67.3%\u589e\u52a0\u523079.5%\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165SAFuzz\uff0c\u7814\u7a76\u8005\u4eec\u5c55\u793a\u4e86\u4e00\u79cd\u6709\u6548\u589e\u5f3aAI\u751f\u6210\u4ee3\u7801\u5b89\u5168\u6027\u5206\u6790\u80fd\u529b\u7684\u65b0\u9014\u5f84\uff0c\u5176\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u53d1\u73b0\u7b97\u6cd5\u4e2d\u6f5c\u5728\u5b89\u5168\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8fd8\u4f18\u5316\u4e86\u6574\u4f53\u6d4b\u8bd5\u6d41\u7a0b\u7684\u6548\u7387\u3002"}}
{"id": "2602.11362", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.11362", "abs": "https://arxiv.org/abs/2602.11362", "authors": ["Reginald Frank", "Soujanya Ponnapalli", "Octavio Lomeli", "Neil Giridharan", "Marcos K Aguilera", "Natacha Crooks"], "title": "Real Life Is Uncertain. Consensus Should Be Too!", "comment": "HotOS '25: Proceedings of the 2025 Workshop on Hot Topics in Operating Systems", "summary": "Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \\textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u6027\u6545\u969c\u6a21\u578b\uff0c\u7528\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684f-threshold\u6545\u969c\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u5229\u7528\u5355\u4e2a\u673a\u5668\u7684\u6545\u969c\u66f2\u7ebf\u6765\u7ed5\u8fc7\u591a\u6570\u6cd5\u5b9a\u4ea4\u96c6\u7b49\u4f20\u7edf\u74f6\u9888\uff0c\u4ece\u800c\u6784\u5efa\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u3001\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u4e14\u53ef\u6301\u7eed\u7684\u7cfb\u7edf\u3002", "motivation": "\u4f5c\u8005\u8ba4\u4e3a\u5f53\u524d\u5171\u8bc6\u534f\u8bae\u6240\u4f9d\u8d56\u7684f-threshold\u6545\u969c\u6a21\u578b\u8fc7\u4e8e\u7b80\u5316\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u60c5\u51b5\uff0c\u5e76\u9650\u5236\u4e86\u4f18\u5316\u6210\u672c\u6216\u6027\u80fd\u7684\u673a\u4f1a\u3002\u56e0\u6b64\uff0c\u4ed6\u4eec\u63d0\u8bae\u91c7\u7528\u4e00\u79cd\u80fd\u591f\u6355\u6349\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u590d\u6742\u800c\u7ec6\u5fae\u7684\u6545\u969c\u7279\u6027\u7684\u6982\u7387\u6027\u6545\u969c\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u6982\u7387\u6027\u5171\u8bc6\u534f\u8bae\uff0c\u8fd9\u4e9b\u534f\u8bae\u53ef\u4ee5\u660e\u786e\u5730\u5229\u7528\u6bcf\u4e2a\u673a\u5668\u7684\u5177\u4f53'\u6545\u969c\u66f2\u7ebf'\uff0c\u5e76\u63a2\u7d22\u907f\u514d\u4f20\u7edf\u74f6\u9888\u5982\u5927\u591a\u6570\u6cd5\u5b9a\u4ea4\u96c6\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u671b\u4f7f\u57fa\u4e8e\u6b64\u65b0\u578b\u6545\u969c\u6a21\u578b\u8bbe\u8ba1\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u53d8\u5f97\u66f4\u52a0\u53ef\u9760\u3001\u6548\u7387\u66f4\u9ad8\u3001\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u53ef\u6301\u7eed\u6027\u3002", "conclusion": "\u8f6c\u5411\u6982\u7387\u6027\u6545\u969c\u6a21\u578b\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u66f4\u52a0\u7075\u6d3b\u3001\u9002\u5e94\u6027\u5f3a\u4ee5\u53ca\u5728\u6210\u672c\u4e0e\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.11443", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11443", "abs": "https://arxiv.org/abs/2602.11443", "authors": ["Abylay Amanbayev", "Brian Tsan", "Tri Dang", "Florin Rusu"], "title": "Filtered Approximate Nearest Neighbor Search in Vector Databases: System Design and Performance Analysis", "comment": "The artifacts are available at: https://github.com/aabylay/ANN-benchmark-HQ", "summary": "Retrieval-Augmented Generation (RAG) applications increasingly rely on Filtered Approximate Nearest Neighbor Search (FANNS) to combine semantic retrieval with metadata constraints. While algorithmic innovations for FANNS have been proposed, there remains a lack of understanding regarding how generic filtering strategies perform within Vector Databases. In this work, we systematize the taxonomy of filtering strategies and evaluate their integration into FAISS, Milvus, and pgvector. To provide a robust benchmarking framework, we introduce a new relational dataset, \\textit{MoReVec}, consisting of two tables, featuring 768-dimensional text embeddings and a rich schema of metadata attributes. We further propose the \\textit{Global-Local Selectivity (GLS)} correlation metric to quantify the relationship between filters and query vectors.\n  Our experiments reveal that algorithmic adaptations within the engine often override raw index performance. Specifically, we find that: (1) \\textit{Milvus} achieves superior recall stability through hybrid approximate/exact execution; (2) \\textit{pgvector}'s cost-based query optimizer frequently selects suboptimal execution plans, favoring approximate index scans even when exact sequential scans would yield perfect recall at comparable latency; and (3) partition-based indexes (IVFFlat) outperform graph-based indexes (HNSW) for low-selectivity queries. To facilitate this analysis, we extend the widely-used \\textit{ANN-Benchmarks} to support filtered vector search and make it available online. Finally, we synthesize our findings into a set of practical guidelines for selecting index types and configuring query optimizers for hybrid search workloads.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5730\u5206\u7c7b\u4e86\u8fc7\u6ee4\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728FAISS\u3001Milvus\u548cpgvector\u4e2d\u7684\u96c6\u6210\u6548\u679c\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u5173\u7cfb\u578b\u6570\u636e\u96c6MoReVec\u53ca\u63d0\u51faGlobal-Local Selectivity (GLS)\u76f8\u5173\u6027\u5ea6\u91cf\u6807\u51c6\u6765\u91cf\u5316\u8fc7\u6ee4\u5668\u4e0e\u67e5\u8be2\u5411\u91cf\u95f4\u7684\u5173\u7cfb\u3002\u7814\u7a76\u53d1\u73b0\u5f15\u64ce\u5185\u7684\u7b97\u6cd5\u8c03\u6574\u5f80\u5f80\u80fd\u8d85\u8d8a\u539f\u59cb\u7d22\u5f15\u6027\u80fd\uff0c\u5e76\u636e\u6b64\u4e3a\u6df7\u5408\u641c\u7d22\u8d1f\u8f7d\u4e0b\u7684\u7d22\u5f15\u7c7b\u578b\u9009\u62e9\u548c\u67e5\u8be2\u4f18\u5316\u5668\u914d\u7f6e\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u5e94\u7528\u5bf9\u7ed3\u5408\u8bed\u4e49\u68c0\u7d22\u4e0e\u5143\u6570\u636e\u7ea6\u675f\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5bf9\u4e8e\u77e2\u91cf\u6570\u636e\u5e93\u4e2d\u901a\u7528\u8fc7\u6ee4\u7b56\u7565\u8868\u73b0\u7684\u7406\u89e3\u4ecd\u663e\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u5206\u6790\u4e0d\u540c\u8fc7\u6ee4\u7b56\u7565\u5728\u4e3b\u6d41\u77e2\u91cf\u6570\u636e\u5e93\u4e2d\u7684\u96c6\u6210\u60c5\u51b5\u53ca\u5176\u6027\u80fd\u5f71\u54cd\u3002", "method": "\u672c\u6587\u9996\u5148\u5bf9\u8fc7\u6ee4\u7b56\u7565\u8fdb\u884c\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\uff0c\u63a5\u7740\u5728FAISS\u3001Milvus\u4ee5\u53capgvector\u4e09\u4e2a\u5e73\u53f0\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u7b56\u7565\u7684\u96c6\u6210\u6548\u679c\u3002\u4e3a\u6b64\uff0c\u8fd8\u7279\u522b\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMoReVec\u7684\u65b0\u5173\u7cfb\u578b\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86Global-Local Selectivity(GLS)\u6307\u6807\u4ee5\u8861\u91cf\u8fc7\u6ee4\u5668\u4e0e\u67e5\u8be2\u5411\u91cf\u4e4b\u95f4\u7684\u5173\u8054\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f15\u64ce\u5185\u90e8\u7684\u7b97\u6cd5\u9002\u5e94\u6027\u7ecf\u5e38\u80fd\u591f\u8d85\u8d8a\u5355\u7eaf\u4f9d\u8d56\u7d22\u5f15\u7684\u8868\u73b0\u3002\u5177\u4f53\u6765\u8bf4\uff1a1. Milvus\u901a\u8fc7\u6df7\u5408\u8fd1\u4f3c/\u7cbe\u786e\u6267\u884c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ec\u56de\u7a33\u5b9a\u6027\uff1b2. pgvector\u7684\u6210\u672c\u57fa\u7840\u67e5\u8be2\u4f18\u5316\u5668\u503e\u5411\u4e8e\u9009\u62e9\u6b21\u4f18\u6267\u884c\u8ba1\u5212\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5373\u4f7f\u987a\u5e8f\u626b\u63cf\u53ef\u8fbe\u5230\u5b8c\u7f8e\u53ec\u56de\u7387\u4e14\u5ef6\u8fdf\u76f8\u5f53\uff1b3. \u5bf9\u4e8e\u4f4e\u9009\u62e9\u6027\u67e5\u8be2\uff0c\u57fa\u4e8e\u5206\u533a\u7684\u7d22\u5f15\uff08\u5982IVFFlat\uff09\u6bd4\u57fa\u4e8e\u56fe\u7684\u7d22\u5f15\uff08\u5982HNSW\uff09\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u7d22\u5f15\u5728\u5904\u7406\u5e26\u6709\u8fc7\u6ee4\u6761\u4ef6\u7684\u6700\u8fd1\u90bb\u641c\u7d22\u4efb\u52a1\u65f6\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u6839\u636e\u7279\u5b9a\u5e94\u7528\u573a\u666f\u5408\u7406\u9009\u62e9\u7d22\u5f15\u7c7b\u578b\u53ca\u8c03\u4f18\u67e5\u8be2\u4f18\u5316\u5668\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u6269\u5c55\u4e86ANN-Benchmarks\u652f\u6301\u8fc7\u6ee4\u5411\u91cf\u641c\u7d22\u529f\u80fd\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u8be5\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdb\u9886\u57df\u5185\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2602.11581", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11581", "abs": "https://arxiv.org/abs/2602.11581", "authors": ["Yiteng Tu", "Shuo Miao", "Weihang Su", "Yiqun Liu", "Qingyao Ai"], "title": "Analytical Search", "comment": null, "summary": "Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.\n  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5206\u6790\u641c\u7d22\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u641c\u7d22\u8303\u5f0f\uff0c\u65e8\u5728\u6ee1\u8db3\u591a\u6837\u5316\u7684\u5206\u6790\u4fe1\u606f\u9700\u6c42\u3002\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u96c6\u6210\u7406\u89e3\u67e5\u8be2\u3001\u53ec\u56de\u5bfc\u5411\u68c0\u7d22\u3001\u878d\u5408\u65f6\u8003\u8651\u63a8\u7406\u4ee5\u53ca\u81ea\u9002\u5e94\u9a8c\u8bc1\u7684\u7edf\u4e00\u7cfb\u7edf\u6846\u67b6\uff0c\u5206\u6790\u641c\u7d22\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u5177\u6709\u9ad8\u5ea6\u95ee\u8d23\u8981\u6c42\u7684\u5206\u6790\u67e5\u8be2\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u4fe1\u606f\u68c0\u7d22\u8303\u5f0f\u96be\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u7aef\u5230\u7aef\u89e3\u51b3\u5206\u6790\u6027\u95ee\u9898\u7684\u9700\u6c42\uff0c\u5982\u8d8b\u52bf\u5206\u6790\u548c\u56e0\u679c\u5f71\u54cd\u8bc4\u4f30\u7b49\u8de8\u9886\u57df\u4efb\u52a1\u3002\u5b83\u4eec\u8981\u4e48\u4fa7\u91cd\u4e8e\u67e5\u627e\u4fe1\u606f\u800c\u975e\u89e3\u51b3\u95ee\u9898\uff0c\u8981\u4e48\u5c06\u6240\u6709\u5185\u5bb9\u89c6\u4e3a\u7b80\u5355\u7684\u95ee\u7b54\u5904\u7406\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u3001\u8bc1\u636e\u4f7f\u7528\u53ca\u53ef\u9a8c\u8bc1\u6027\u7684\u5145\u5206\u63a7\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u65b9\u5f0f\u6765\u652f\u6301\u8fd9\u4e9b\u6709\u591a\u79cd\u7528\u9014\u6982\u5ff5\u4e14\u9ad8\u95ee\u8d23\u8981\u6c42\u7684\u5206\u6790\u67e5\u8be2\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5206\u6790\u641c\u7d22\u201d\u7684\u65b0\u641c\u7d22\u8303\u5f0f\uff0c\u5b83\u5c06\u641c\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ee5\u8bc1\u636e\u4e3a\u57fa\u7840\u3001\u9762\u5411\u8fc7\u7a0b\u7684\u5206\u6790\u5de5\u4f5c\u6d41\uff0c\u5e76\u660e\u786e\u5efa\u6a21\u5206\u6790\u610f\u56fe\u3001\u6536\u96c6\u8bc1\u636e\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u53ca\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u6b65\u9aa4\u63a8\u65ad\u5f97\u51fa\u53ef\u9a8c\u8bc1\u7ed3\u8bba\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u67e5\u8be2\u7406\u89e3\u3001\u53ec\u56de\u5bfc\u5411\u68c0\u7d22\u3001\u6ce8\u91cd\u63a8\u7406\u7684\u878d\u5408\u4e0e\u81ea\u9002\u5e94\u9a8c\u8bc1\u7b49\u529f\u80fd\u3002", "result": "\u6587\u7ae0\u6210\u529f\u5730\u5b9a\u4f4d\u4e86\u5206\u6790\u641c\u7d22\u76f8\u5bf9\u4e8e\u73b0\u6709\u8303\u5f0f\u7684\u72ec\u7279\u4e4b\u5904\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u52a9\u4e8e\u5f00\u53d1\u4e0b\u4e00\u4ee3\u641c\u7d22\u5f15\u64ce\u7684\u7efc\u5408\u7cfb\u7edf\u67b6\u6784\uff0c\u8fd9\u4e9b\u5f15\u64ce\u80fd\u591f\u66f4\u6709\u6548\u5730\u652f\u6301\u7528\u6237\u7684\u5206\u6790\u4fe1\u606f\u9700\u6c42\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5206\u6790\u641c\u7d22\u5728\u6982\u5ff5\u4e0a\u7684\u91cd\u8981\u6027\u548c\u5b9e\u8df5\u4ef7\u503c\uff0c\u5e76\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u52aa\u529b\uff0c\u4ee5\u63a8\u52a8\u652f\u6301\u5206\u6790\u4fe1\u606f\u9700\u6c42\u7684\u65b0\u4e00\u4ee3\u641c\u7d22\u5f15\u64ce\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.11184", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11184", "abs": "https://arxiv.org/abs/2602.11184", "authors": ["Zukang Xu", "Zhixiong Zhao", "Xing Hu", "Zhixuan Chen", "Dawei Yang"], "title": "KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models", "comment": "Accepted by ICLR 2026", "summary": "Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5411\u91cf\u91cf\u5316\u6846\u67b6KBVQ-MoE\uff0c\u7528\u4e8e\u6539\u5584\u57fa\u4e8eMoE\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u95ee\u9898\u3002\u901a\u8fc7\u8f93\u5165\u9a71\u52a8\u7684\u5197\u4f59\u6d88\u9664\u548c\u504f\u7f6e\u6821\u6b63\u8f93\u51fa\u7a33\u5b9a\u5316\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "Mixture of Experts (MoE) \u6a21\u578b\u867d\u7136\u5728\u63d0\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u7531\u4e8e\u5176\u5de8\u5927\u7684\u53c2\u6570\u5927\u5c0f\u548c\u5185\u5b58\u9700\u6c42\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002\u76f4\u63a5\u5c06\u5411\u91cf\u91cf\u5316(VQ)\u5e94\u7528\u4e8eMoE\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u5305\u62ec\u4e13\u5bb6\u95f4\u5b58\u5728\u5197\u4f59\u8868\u793a\u4ee5\u53ca\u7531\u4e13\u5bb6\u805a\u5408\u5bfc\u81f4\u7684\u7d2f\u79ef\u8f93\u51fa\u504f\u5dee\u88ab\u653e\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aKBVQ-MoE\u7684\u65b0\u6846\u67b6\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e24\u79cd\u6280\u672f\uff1a1\uff09\u5229\u7528Karhunen-Loeve\u53d8\u6362(KLT)\u5f15\u5bfc\u7684\u5947\u5f02\u503c\u5206\u89e3(SVD)\u8fdb\u884c\u8f93\u5165\u9a71\u52a8\u7684\u5197\u4f59\u6d88\u9664\uff0c\u63d0\u53d6\u4e3b\u8981\u6743\u91cd\u5206\u91cf\u5e76\u5728\u4e13\u5bb6\u4e4b\u95f4\u5171\u4eab\uff1b2\uff09\u53ea\u5bf9\u4e13\u5bb6\u7279\u5b9a\uff08\u975e\u5197\u4f59\uff09\u8868\u793a\u5e94\u7528\u5411\u91cf\u91cf\u5316\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u7ea7\u4eff\u5c04\u8865\u507f\u6765\u6821\u6b63\u91cf\u5316\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKBVQ-MoE\u76f8\u6bd4\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002\u4f8b\u5982\uff0c\u5728Qwen1.5-MoE-A2.7B\u6a21\u578b\u4e0a\u4f7f\u75283\u6bd4\u7279\u91cf\u5316\u8fbe\u5230\u4e8667.99\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u51e0\u4e4e\u4e0eFP16\u57fa\u7ebf\u768468.07\u76f8\u540c\u3002", "conclusion": "KBVQ-MoE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u5c11MoE\u6a21\u578b\u7684\u53c2\u6570\u5927\u5c0f\u548c\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u548c\u5176\u4ed6\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2602.11573", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.11573", "abs": "https://arxiv.org/abs/2602.11573", "authors": ["Wenyang Zhou", "Jiadong Xie", "Yingfan Liu", "Zhihao Yin", "Jeffrey Xu Yu", "Hui Li", "Zhangqian Mu", "Xiaotian Qiao", "Jiangtao Cui"], "title": "Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases", "comment": null, "summary": "k-approximate nearest neighbor search (k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process.In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend multiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFastPGT\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u7528\u4e8e\u8c03\u6574\u90bb\u8fd1\u56fe\uff08PG\uff09\u6784\u5efa\u53c2\u6570\u3002\u901a\u8fc7\u540c\u65f6\u6784\u5efa\u591a\u4e2aPG\uff0cFastPGT\u52a0\u901f\u4e86\u53c2\u6570\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u91cd\u590d\u8ba1\u7b97\uff0c\u5e76\u4e14\u5728\u4e0d\u727a\u7272\u8c03\u4f18\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5VDTuner\u5b9e\u73b0\u4e86\u6700\u9ad82.37\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "k-\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22(k-ANNS)\u5728\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4\u4e2d\u662f\u4e00\u4e2a\u8de8\u9886\u57df\u7684\u57fa\u672c\u95ee\u9898\uff0c\u968f\u7740\u5411\u91cf\u6570\u636e\u5e93\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u53d1\u5c55\u53d7\u5230\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u57fa\u4e8e\u90bb\u8fd1\u56fe(PG)\u7684\u65b9\u6cd5\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u6784\u9020\u53c2\u6570\u7684\u5f71\u54cd\u5f88\u5927\u3002\u7136\u800c\uff0c\u76ee\u524d\u8fd8\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u9488\u5bf9\u8fd9\u4e00\u53c2\u6570\u8c03\u6574\u7684\u8fc7\u7a0b\u8fdb\u884c\u8003\u8651\u548c\u4f18\u5316\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86FastPGT\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u8c03\u6574PG\u6784\u9020\u53c2\u6570\u7684\u9ad8\u6548\u6846\u67b6\u3002FastPGT\u901a\u8fc7\u540c\u65f6\u6784\u5efa\u591a\u4e2aPG\u6765\u52a0\u901f\u53c2\u6570\u4f30\u8ba1\uff0c\u4ece\u800c\u51cf\u5c11\u91cd\u590d\u8ba1\u7b97\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8005\u4eec\u8fd8\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u6700\u4f18\u8c03\u53c2\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u4e00\u6b21\u6027\u63a8\u8350\u591a\u4e2a\u53c2\u6570\uff0c\u5e76\u5229\u7528\u6211\u4eec\u540c\u65f6\u6784\u5efa\u591a\u4e2aPG\u7684\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u53c2\u6570\u8fdb\u884c\u6709\u6548\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5bf9\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFastPGT\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5VDTuner\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.37\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u800c\u4e14\u6ca1\u6709\u964d\u4f4e\u8c03\u53c2\u7684\u8d28\u91cf\u3002", "conclusion": "FastPGT\u4e3a\u89e3\u51b3\u90bb\u8fd1\u56fe(PG)\u6784\u5efa\u8fc7\u7a0b\u4e2d\u53c2\u6570\u8c03\u6574\u6210\u672c\u9ad8\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u540c\u65f6\u6784\u5efa\u5e76\u8bc4\u4f30\u591a\u4e2aPG\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u6570\u8c03\u4f18\u6548\u7387\u3002"}}
{"id": "2602.11605", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11605", "abs": "https://arxiv.org/abs/2602.11605", "authors": ["Yixiao Chen", "Yuan Wang", "Yue Liu", "Qiyao Wang", "Ke Cheng", "Xin Xu", "Juntong Yan", "Shuojin Yang", "Menghao Guo", "Jun Zhang", "Huan Yu", "Jie Jiang"], "title": "Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation", "comment": "12 pages, 6figures", "summary": "Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.", "AI": {"tldr": "Rec2PM\u6846\u67b6\u901a\u8fc7\u5c06\u957f\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u504f\u597d\u8bb0\u5fc6\u4ee4\u724c\u6765\u89e3\u51b3\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u5728\u5904\u7406\u957f\u671f\u5e8f\u5217\u65f6\u9047\u5230\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u968f\u673a\u4ea4\u4e92\u566a\u58f0\u7d2f\u79ef\u7684\u95ee\u9898\u3002\u5b83\u91c7\u7528\u81ea\u53c2\u7167\u6559\u5e08\u5f3a\u5236\u7b56\u7565\uff0c\u652f\u6301\u5e76\u884c\u5316\u8bad\u7ec3\u548c\u8fed\u4ee3\u66f4\u65b0\uff0c\u5e76\u4e14\u4ee5\u4ee4\u724c\u5d4c\u5165\u5f62\u5f0f\u8868\u793a\u8bb0\u5fc6\u63d0\u9ad8\u4e86\u5b58\u50a8\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u5168\u5e8f\u5217\u6a21\u578b\uff0cRec2PM\u51cf\u5c11\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\uff08GenRec\uff09\u901a\u5e38\u901a\u8fc7\u5168\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u7528\u6237\u884c\u4e3a\uff0c\u4f46\u5f53\u6269\u5c55\u5230\u7ec8\u8eab\u5e8f\u5217\u65f6\u4f1a\u53d7\u5230\u8fc7\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u4ee5\u53ca\u6765\u81ea\u968f\u673a\u4ea4\u4e92\u7684\u566a\u97f3\u79ef\u7d2f\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Rec2PM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u957f\u65f6\u95f4\u7684\u7528\u6237\u4ea4\u4e92\u8bb0\u5f55\u538b\u7f29\u6210\u7b80\u77ed\u7684\u504f\u597d\u8bb0\u5fc6\u4ee4\u724c\u3002\u4e0e\u4f20\u7edf\u7684\u9012\u5f52\u65b9\u6cd5\u4e0d\u540c\uff0cRec2PM\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u53c2\u7167\u6559\u5e08\u5f3a\u8feb\u7b56\u7565\uff1a\u5229\u7528\u5386\u53f2\u7684\u6574\u4f53\u89c6\u56fe\u751f\u6210\u53c2\u8003\u8bb0\u5fc6\uff0c\u4f5c\u4e3a\u5e76\u884c\u9012\u5f52\u66f4\u65b0\u7684\u76d1\u7763\u76ee\u6807\u3002\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u5728\u4fdd\u6301\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u66f4\u65b0\u80fd\u529b\u7684\u540c\u65f6\u8fdb\u884c\u5b8c\u5168\u5e76\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5c06\u8bb0\u5fc6\u8868\u793a\u4e3a\u4ee4\u724c\u5d4c\u5165\u800c\u4e0d\u662f\u5927\u91cf\u7684KV\u7f13\u5b58\uff0cRec2PM\u5b9e\u73b0\u4e86\u6781\u9ad8\u7684\u5b58\u50a8\u6548\u7387\u3002", "result": "\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5168\u5e8f\u5217\u6a21\u578b\u76f8\u6bd4\uff0cRec2PM\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528\u91cf\uff0c\u540c\u65f6\u8fbe\u5230\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u5206\u6790\u8fd8\u663e\u793a\uff0c\u504f\u597d\u8bb0\u5fc6\u529f\u80fd\u4f5c\u4e3a\u4e00\u4e2a\u53bb\u566a\u4fe1\u606f\u74f6\u9888\uff0c\u80fd\u591f\u6709\u6548\u5730\u8fc7\u6ee4\u6389\u4ea4\u4e92\u4e2d\u7684\u566a\u97f3\uff0c\u6355\u6349\u5230\u7a33\u5b9a\u7684\u957f\u671f\u5174\u8da3\u3002", "conclusion": "Rec2PM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5904\u7406\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u957f\u671f\u7528\u6237\u5e8f\u5217\u5e26\u6765\u7684\u6311\u6218\uff0c\u5305\u62ec\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u6570\u636e\u566a\u58f0\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u504f\u597d\u8bb0\u5fc6\u4ee4\u724c\u53ca\u81ea\u53c2\u7167\u5b66\u4e60\u673a\u5236\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u4e0e\u63a8\u65ad\u7684\u901f\u5ea6\u548c\u6548\u7387\uff0c\u4e5f\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u7528\u6237\u957f\u671f\u5174\u8da3\u7684\u7406\u89e3\u51c6\u786e\u5ea6\u3002"}}
{"id": "2602.11185", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11185", "abs": "https://arxiv.org/abs/2602.11185", "authors": ["Zhendong Huang", "Hengjie Cao", "Fang Dong", "Ruijun Huang", "Mengyi Chen", "Yifeng Yang", "Xin Zhang", "Anrui Chen", "Mingzhi Dong", "Yujiang Wang", "Jinlong Hou", "Qin Lv", "Robert P. Dick", "Yuan Cheng", "Fan Yang", "Tun Lu", "Li Shang"], "title": "Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy", "comment": null, "summary": "Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spike occupying only about 1.5% of directions yet dominating optimizer statistics. This dominance suppresses tail learning by contracting tail updates through second moment normalization and tightening the globally stable learning rate bound. Motivated by this analysis, we propose Spectra, a spike aware optimizer that suppresses the dominant low rank spike subspace without amplifying the noise sensitive spectral tail. Spectra tracks the spike subspace via cached, warm started power iteration and applies low rank spectral shaping with negligible overhead and substantially reduced optimizer state memory. On LLaMA3 8B trained on 50B tokens, Spectra reaches the same target loss 30% faster than AdamW, reduces per step end to end overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves average downstream accuracy by 1.62%. Compared to Muon, Spectra is 5.1x faster in optimizer processing time, achieves a lower final loss, and improves average accuracy by 0.66%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpectra\u7684\u4f18\u5316\u5668\uff0c\u5b83\u80fd\u591f\u6291\u5236\u4e3b\u5bfc\u7684\u4f4e\u79e9\u5c16\u5cf0\u5b50\u7a7a\u95f4\u800c\u4e0d\u653e\u5927\u5bf9\u566a\u58f0\u654f\u611f\u7684\u9891\u8c31\u5c3e\u90e8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0eAdamW\u76f8\u6bd4\uff0cSpectra\u5728\u8bad\u7ec3LLaMA3 8B\u65f6\u8fbe\u5230\u76f8\u540c\u76ee\u6807\u635f\u5931\u7684\u901f\u5ea6\u5feb\u4e8630%\uff0c\u6bcf\u6b65\u7aef\u5230\u7aef\u5f00\u9500\u51cf\u5c11\u4e860.7%\uff0c\u4f18\u5316\u5668\u72b6\u6001\u5185\u5b58\u51cf\u5c11\u4e8649.25%\uff0c\u5e73\u5747\u4e0b\u6e38\u51c6\u786e\u7387\u63d0\u9ad8\u4e861.62%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u4fe1\u53f7\u9ad8\u5ea6\u5404\u5411\u5f02\u6027\uff1a\u91cd\u590d\u7684\u8bed\u8a00\u7ed3\u6784\u5c06\u80fd\u91cf\u96c6\u4e2d\u5728\u5c11\u6570\u4e3b\u5bfc\u9891\u8c31\u65b9\u5411\u4e0a\uff0c\u800c\u4e0a\u4e0b\u6587\u7279\u5b9a\u4fe1\u606f\u5219\u5206\u5e03\u5728\u4e00\u4e2a\u957f\u5c3e\u4e2d\u3002\u8fd9\u79cd\u5c16\u5cf0-\u5c3e\u90e8\u5206\u79bb\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u5b58\u5728\uff0c\u5176\u4e2d\u5c16\u5cf0\u4ec5\u5360\u636e\u7ea61.5%\u7684\u65b9\u5411\u5374\u4e3b\u5bfc\u4e86\u4f18\u5316\u5668\u7edf\u8ba1\uff0c\u8fd9\u9650\u5236\u4e86\u5c3e\u90e8\u7684\u5b66\u4e60\u3002", "method": "\u53d7\u4e0a\u8ff0\u5206\u6790\u542f\u53d1\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86Spectra\uff0c\u8fd9\u662f\u4e00\u79cd\u80fd\u591f\u8bc6\u522b\u5c16\u5cf0\u7684\u4f18\u5316\u5668\uff0c\u65e8\u5728\u6291\u5236\u4e3b\u5bfc\u7684\u4f4e\u79e9\u5c16\u5cf0\u5b50\u7a7a\u95f4\u800c\u4e0d\u589e\u52a0\u5bf9\u566a\u58f0\u654f\u611f\u7684\u9891\u8c31\u5c3e\u90e8\u7684\u5f71\u54cd\u3002Spectra\u901a\u8fc7\u7f13\u5b58\u548c\u9884\u70ed\u542f\u52a8\u7684\u5e42\u8fed\u4ee3\u8ffd\u8e2a\u5c16\u5cf0\u5b50\u7a7a\u95f4\uff0c\u5e76\u4ee5\u6781\u5c0f\u7684\u989d\u5916\u5f00\u9500\u548c\u663e\u8457\u51cf\u5c11\u7684\u4f18\u5316\u5668\u72b6\u6001\u5185\u5b58\u6765\u5e94\u7528\u4f4e\u79e9\u9891\u8c31\u5851\u5f62\u3002", "result": "\u5728\u4f7f\u752850B tokens\u8bad\u7ec3LLaMA3 8B\u65f6\uff0cSpectra\u6bd4AdamW\u66f4\u5feb\u5730\u8fbe\u5230\u76f8\u540c\u7684\u76ee\u6807\u635f\u5931\uff08\u5feb30%\uff09\uff0c\u540c\u65f6\u6bcf\u6b65\u7684\u7aef\u5230\u7aef\u5f00\u9500\u964d\u4f4e\u4e860.7%\uff0c\u4f18\u5316\u5668\u72b6\u6001\u5185\u5b58\u51cf\u5c11\u4e8649.25%\uff0c\u5e76\u4e14\u5e73\u5747\u4e0b\u6e38\u51c6\u786e\u6027\u63d0\u9ad8\u4e861.62%\u3002\u4e0eMuon\u76f8\u6bd4\uff0cSpectra\u5728\u4f18\u5316\u5668\u5904\u7406\u65f6\u95f4\u4e0a\u5feb\u4e865.1\u500d\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u6700\u7ec8\u635f\u5931\uff0c\u5e76\u4e14\u5e73\u5747\u51c6\u786e\u6027\u63d0\u9ad8\u4e860.66%\u3002", "conclusion": "Spectra\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u52a0\u901f\u8bad\u7ec3\u3001\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u4ee5\u53ca\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u5f3a\u70c8\u5404\u5411\u5f02\u6027\u68af\u5ea6\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u65f6\u3002"}}
{"id": "2602.11223", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.11223", "abs": "https://arxiv.org/abs/2602.11223", "authors": ["Micheal P. Papazoglou", "Bernd J. Kr\u00e4mer", "Mira Raheem", "Amal Elgammal"], "title": "Patient Digital Twins for Chronic Care: Technical Hurdles, Lessons Learned, and the Road Ahead", "comment": "Feature Article, Patient Medical Digital Twins, Under Review in IEEE SOftware", "summary": "Chronic diseases constitute the principal burden of morbidity, mortality, and healthcare costs worldwide, yet current health systems remain fragmented and predominantly reactive. Patient Medical Digital Twins (PMDTs) offer a paradigm shift: holistic, continuously updated digital counterparts of patients that integrate clinical, genomic, lifestyle, and quality-of-life data. We report early implementations of PMDTs via ontology-driven modeling and federated analytics pilots. Insights from the QUALITOP oncology study and a distributed AI platform confirm both feasibility and challenges: aligning with HL7 FHIR and OMOP standards, embedding privacy governance, scaling federated queries, and designing intuitive clinician interfaces. We also highlight technical gains, such as automated reasoning over multimodal blueprints and predictive analytics for patient outcomes. By reflecting on these experiences, we outline actionable insights for software engineers and identify opportunities, such as DSLs and model-driven engineering, to advance PMDTs toward trustworthy, adaptive chronic care ecosystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u60a3\u8005\u533b\u7597\u6570\u5b57\u5b6a\u751f\uff08PMDT\uff09\u4f5c\u4e3a\u6162\u6027\u75c5\u7ba1\u7406\u7684\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7efc\u5408\u4e34\u5e8a\u3001\u57fa\u56e0\u7ec4\u3001\u751f\u6d3b\u65b9\u5f0f\u53ca\u751f\u6d3b\u8d28\u91cf\u6570\u636e\u5b9e\u73b0\u3002\u57fa\u4e8eQUALITOP\u80bf\u7624\u5b66\u7814\u7a76\u548c\u5206\u5e03\u5f0fAI\u5e73\u53f0\u7684\u521d\u6b65\u5b9e\u65bd\u8868\u660e\uff0c\u5c3d\u7ba1\u5b58\u5728\u6807\u51c6\u5316\u5bf9\u63a5\u3001\u9690\u79c1\u6cbb\u7406\u7b49\u6311\u6218\uff0c\u4f46PMDT\u5728\u4e2a\u6027\u5316\u533b\u7597\u9884\u6d4b\u5206\u6790\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u5065\u5eb7\u7cfb\u7edf\u5bf9\u4e8e\u6162\u6027\u75be\u75c5\u7ba1\u7406\u5b58\u5728\u788e\u7247\u5316\u548c\u53cd\u5e94\u6027\u7684\u95ee\u9898\uff0c\u800c\u60a3\u8005\u533b\u7597\u6570\u5b57\u5b6a\u751f\uff08PMDT\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u8bba\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u5404\u7c7b\u60a3\u8005\u76f8\u5173\u6570\u636e\u6765\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8e\u672c\u4f53\u8bba\u5efa\u6a21\u7684\u65b9\u6cd5\u4ee5\u53ca\u8054\u90a6\u5206\u6790\u8bd5\u70b9\u9879\u76ee\uff0c\u7ed3\u5408\u4e86QUALITOP\u80bf\u7624\u5b66\u6848\u4f8b\u7814\u7a76\u4e0e\u4e00\u4e2a\u5206\u5e03\u5f0f\u4eba\u5de5\u667a\u80fd\u5e73\u53f0\u7684\u5e94\u7528\u5b9e\u4f8b\u6765\u8fdb\u884c\u63a2\u7d22\u3002", "result": "\u7ed3\u679c\u663e\u793a\u51fa\u5229\u7528PMDT\u8fdb\u884c\u81ea\u52a8\u5316\u63a8\u7406\u548c\u9884\u6d4b\u5206\u6790\u7684\u6280\u672f\u4f18\u52bf\uff0c\u5e76\u4e14\u6307\u51fa\u4e86\u5728\u9075\u5faaHL7 FHIR\u548cOMOP\u6807\u51c6\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u5230\u4f4d\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f3a\u8c03\u4e86\u6269\u5927\u8054\u90a6\u67e5\u8be2\u89c4\u6a21\u548c\u5f00\u53d1\u7528\u6237\u53cb\u597d\u578b\u4e34\u5e8a\u754c\u9762\u7684\u9700\u6c42\u3002", "conclusion": "\u901a\u8fc7\u53cd\u601d\u8fd9\u4e9b\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u7814\u7a76\u4eba\u5458\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSLs\uff09\u548c\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u7b49\u6280\u672f\u5728\u672a\u6765\u53d1\u5c55\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\uff0c\u4ee5\u63a8\u52a8PMDT\u6210\u4e3a\u53ef\u4fe1\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6162\u6027\u62a4\u7406\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2602.11756", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.11756", "abs": "https://arxiv.org/abs/2602.11756", "authors": ["Luigi Asprino", "Enrico Daga"], "title": "Towards a theory of Fa\u00e7ade-X data access: satisfiability of SPARQL basic graph patterns", "comment": null, "summary": "Data integration is the primary use case for knowledge graphs. However, integrated data are not typically graphs but come in different formats, for example, CSV, XML, or a relational database. Fa\u00e7ade-X is a recently proposed method for providing direct access to an open-ended set of data formats. The method includes a meta-model that specialises RDF to fit general data structures. This model allows to express SPARQL queries targeting data sources with those structures. Previous work formalised Fa\u00e7ade-X and demonstrated how it can theoretically represent any format expressible with a context-free grammar, as well as the relational model. A reference implementation, SPARQL Anything, demonstrates the feasibility of the approach in practice. It is noteworthy that Fa\u00e7ade-X utilises a fraction of RDF, and, consequently, not all SPARQL queries yield a solution (i.e. are satisfiable) when evaluated over a Fa\u00e7ade-X graph. In this article, we consolidate Fa\u00e7ade-X, and we study the satisfiability of basic graph patterns. The theory is accompanied by an algorithm for deciding the satisfiability of basic graph patterns on Fa\u00e7ade-X data sources. Furthermore, we provide extensive experiments with a proof-of-concept implementation, demonstrating practical feasibility, including with real-world queries. Our results pave the way for studying query execution strategies for Fa\u00e7ade-X data access with SPARQL and supporting developers to build more efficient data integration systems for knowledge graphs.", "AI": {"tldr": "This paper consolidates Fa\u00e7ade-X, a method for direct access to various data formats through a specialized RDF meta-model, and studies the satisfiability of SPARQL basic graph patterns. It provides an algorithm for deciding satisfiability and demonstrates practical feasibility via experiments, paving the way for improved query execution strategies and more efficient data integration for knowledge graphs.", "motivation": "The motivation is to improve the efficiency and effectiveness of data integration for knowledge graphs by addressing the limitations of Fa\u00e7ade-X in handling SPARQL queries, specifically focusing on the satisfiability of basic graph patterns over Fa\u00e7ade-X graphs, which are representations of non-graph data sources such as CSV, XML, or relational databases.", "method": "The authors consolidate the Fa\u00e7ade-X approach, formalize the concept of satisfiability for basic graph patterns within this framework, and develop an algorithm to determine if a given SPARQL query is satisfiable when applied to Fa\u00e7ade-X data sources. They also conduct extensive experiments using a proof-of-concept implementation to test the practicality of their theory with real-world queries.", "result": "The results show that the proposed algorithm can effectively decide the satisfiability of basic graph patterns on Fa\u00e7ade-X data sources, and the experimental evaluation confirms the practical feasibility of the approach. This work sets the foundation for further research into query execution strategies for Fa\u00e7ade-X and supports the development of more efficient data integration systems for knowledge graphs.", "conclusion": "The conclusion suggests that the study successfully addresses the satisfiability of SPARQL queries over Fa\u00e7ade-X data, providing a solid theoretical and practical basis for future improvements in data integration for knowledge graphs, and indicating potential for optimizing query execution and system performance."}}
{"id": "2602.11622", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11622", "abs": "https://arxiv.org/abs/2602.11622", "authors": ["Haiyang Jiang", "Tong Chen", "Xinyi Gao", "Guansong Pang", "Quoc Viet Hung Nguyen", "Hongzhi Yin"], "title": "Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts", "comment": null, "summary": "Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MoE\u6846\u67b6EvoFG\uff0c\u901a\u8fc7\u8fdb\u5316\u7279\u5f81\u751f\u6210\u65b9\u6848\u548c\u8bb0\u5fc6\u589e\u5f3a\u8def\u7531\u5668\u6765\u89e3\u51b3\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8def\u7531\u6311\u6218\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u4e2aGNN\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u8868\u8fbe\u591a\u6837\u5316\u7684\u5f02\u5e38\u673a\u5236\uff0c\u800c\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08MoE\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u6574\u5408\u4e0d\u540cGNN\u4e13\u5bb6\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5176\u5728\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u53d7\u5230\u5206\u5e03\u504f\u79fb\u7684\u4e25\u91cd\u9650\u5236\uff0c\u5bfc\u81f4\u5173\u952e\u7684\u8def\u7531\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aEvoFG\u7684\u65b0MoE\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u751f\u6210\u5668\u548cShapley\u503c\u5f15\u5bfc\u8bc4\u4f30\u7684\u8fdb\u5316\u7279\u5f81\u751f\u6210\u65b9\u6848\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5177\u6709\u4e0d\u53d8\u5b66\u4e60\u76ee\u6807\u7684\u8bb0\u5fc6\u589e\u5f3a\u8def\u7531\u5668\uff0c\u4ee5\u5e94\u5bf9\u8de8\u56fe\u8282\u70b9\u8bed\u4e49\u5dee\u5f02\u5927\u548c\u5f02\u5e38\u56fe\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aEvoFG\u6301\u7eed\u4f18\u4e8e\u6700\u65b0\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u4e14\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684EvoFG\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u8def\u7531\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.11686", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11686", "abs": "https://arxiv.org/abs/2602.11686", "authors": ["Xinyi Liu", "Yujie Wang", "Fangcheng Fu", "Xuefeng Xiao", "Huixia Li", "Jiashi Li", "Bin Cui"], "title": "LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training", "comment": "19 pages, 12 figures, the paper will be presented at ASPLOS 2026", "summary": "Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.\n  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MoE\u8bad\u7ec3\u6846\u67b6LAER-MoE\uff0c\u901a\u8fc7\u5168\u5207\u5206\u4e13\u5bb6\u5e76\u884c\uff08FSEP\uff09\u548c\u7cbe\u7ec6\u8c03\u5ea6\u901a\u4fe1\u64cd\u4f5c\u6765\u4f18\u5316\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u5728A100\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\u6700\u9ad8\u53ef\u8fbe1.69\u500d\u52a0\u901f\u3002", "motivation": "\u5f53\u524dMixture-of-Experts (MoE)\u6a21\u578b\u7684\u4e13\u5bb6\u5e76\u884c\u8bad\u7ec3\u4e2d\u5b58\u5728\u663e\u8457\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u90e8\u5206\u8fc7\u8f7d\u4e13\u5bb6\u6210\u4e3a\u8bad\u7ec3\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86LAER-MoE\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u4e00\u79cd\u65b0\u9896\u7684\u5e76\u884c\u6a21\u5f0f\u2014\u2014\u5168\u5207\u5206\u4e13\u5bb6\u5e76\u884c\uff08Fully Sharded Expert Parallel, FSEP\uff09\uff0c\u5b9e\u73b0\u5bf9\u6bcf\u4e2a\u4e13\u5bb6\u53c2\u6570\u6309\u8bbe\u5907\u6570\u91cf\u8fdb\u884c\u5b8c\u5168\u5212\u5206\uff0c\u5e76\u901a\u8fc7All-to-All\u901a\u4fe1\u6062\u590d\u90e8\u5206\u4e13\u5bb6\u3002\u540c\u65f6\uff0c\u8fdb\u884c\u4e86\u7ec6\u7c92\u5ea6\u7684\u901a\u4fe1\u64cd\u4f5c\u8c03\u5ea6\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5f00\u53d1\u4e86\u8d1f\u8f7d\u5e73\u8861\u89c4\u5212\u5668\u6765\u5236\u5b9a\u4e13\u5bb6\u91cd\u65b0\u5e03\u5c40\u7b56\u7565\u53ca\u4ee4\u724c\u8def\u7531\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728A100\u96c6\u7fa4\u4e0a\uff0c\u8be5\u7cfb\u7edf\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u7cfb\u7edf\u76f8\u6bd4\u53ef\u8fbe\u5230\u6700\u9ad81.69\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "LAER-MoE\u4e3a\u89e3\u51b3MoE\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5e76\u884c\u673a\u5236\u548c\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002"}}
{"id": "2602.11890", "categories": ["cs.DB", "cs.CG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.11890", "abs": "https://arxiv.org/abs/2602.11890", "authors": ["Giannis Spiliopoulos", "Alexandros Troupiotis-Kapeliaris", "Kostas Patroumpas", "Nikolaos Liapis", "Dimitrios Skoutas", "Dimitris Zissis", "Nikos Bikakis"], "title": "Data-Driven Trajectory Imputation for Vessel Mobility Analysis", "comment": "International Conference on Extending Database Technology (EDBT 2026)", "summary": "Modeling vessel activity at sea is critical for a wide range of applications, including route planning, transportation logistics, maritime safety, and environmental monitoring. Over the past two decades, the Automatic Identification System (AIS) has enabled real-time monitoring of hundreds of thousands of vessels, generating huge amounts of data daily. One major challenge in using AIS data is the presence of large gaps in vessel trajectories, often caused by coverage limitations or intentional transmission interruptions. These gaps can significantly degrade data quality, resulting in inaccurate or incomplete analysis. State-of-the-art imputation approaches have mainly been devised to tackle gaps in vehicle trajectories, even when the underlying road network is not considered. But the motion patterns of sailing vessels differ substantially, e.g., smooth turns, maneuvering near ports, or navigating in adverse weather conditions. In this application paper, we propose HABIT, a lightweight, configurable H3 Aggregation-Based Imputation framework for vessel Trajectories. This data-driven framework provides a valuable means to impute missing trajectory segments by extracting, analyzing, and indexing motion patterns from historical AIS data. Our empirical study over AIS data across various timeframes, densities, and vessel types reveals that HABIT produces maritime trajectory imputations performing comparably to baseline methods in terms of accuracy, while performing better in terms of latency while accounting for vessel characteristics and their motion patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHABIT\u7684\u8f7b\u91cf\u7ea7\u3001\u53ef\u914d\u7f6e\u7684\u57fa\u4e8eH3\u805a\u5408\u7684\u8239\u53ea\u8f68\u8ff9\u586b\u8865\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3AIS\u6570\u636e\u4e2d\u56e0\u8986\u76d6\u9650\u5236\u6216\u6545\u610f\u4e2d\u65ad\u5bfc\u81f4\u7684\u8f68\u8ff9\u7a7a\u767d\u95ee\u9898\u3002\u901a\u8fc7\u4ece\u5386\u53f2AIS\u6570\u636e\u4e2d\u63d0\u53d6\u3001\u5206\u6790\u548c\u7d22\u5f15\u8fd0\u52a8\u6a21\u5f0f\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5728\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u8003\u8651\u5230\u4e86\u8239\u53ea\u7279\u6027\u548c\u5176\u8fd0\u52a8\u6a21\u5f0f\u3002", "motivation": "\u7531\u4e8e\u8986\u76d6\u9650\u5236\u6216\u6709\u610f\u56fe\u7684\u4f20\u8f93\u4e2d\u65ad\uff0cAIS\u6570\u636e\u4e2d\u5b58\u5728\u5927\u91cf\u8239\u53ea\u8f68\u8ff9\u7a7a\u767d\uff0c\u8fd9\u4f1a\u663e\u8457\u964d\u4f4e\u6570\u636e\u8d28\u91cf\uff0c\u5bfc\u81f4\u5206\u6790\u4e0d\u51c6\u786e\u6216\u4e0d\u5b8c\u6574\u3002\u73b0\u6709\u6700\u5148\u8fdb\u7684\u586b\u8865\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8f66\u8f86\u8f68\u8ff9\u8bbe\u8ba1\uff0c\u4f46\u8239\u53ea\uff08\u7279\u522b\u662f\u822a\u884c\u4e2d\u7684\u8239\u53ea\uff09\u7684\u8fd0\u52a8\u6a21\u5f0f\u4e0e\u4e4b\u5927\u4e0d\u76f8\u540c\uff0c\u4f8b\u5982\u5e73\u6ed1\u8f6c\u5f2f\u3001\u9760\u8fd1\u6e2f\u53e3\u64cd\u4f5c\u6216\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u822a\u884c\u7b49\u7279\u6027\u672a\u88ab\u5145\u5206\u8003\u8651\u3002", "method": "\u63d0\u51fa\u4e86HABIT\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eH3\u805a\u5408\u7684\u8f7b\u91cf\u7ea7\u3001\u53ef\u914d\u7f6e\u7684\u8239\u53ea\u8f68\u8ff9\u586b\u8865\u65b9\u6848\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4ece\u5386\u53f2AIS\u6570\u636e\u4e2d\u62bd\u53d6\u3001\u5206\u6790\u5e76\u7d22\u5f15\u8fd0\u52a8\u6a21\u5f0f\u6765\u586b\u8865\u7f3a\u5931\u7684\u8f68\u8ff9\u6bb5\u843d\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e0d\u540c\u65f6\u95f4\u6bb5\u3001\u5bc6\u5ea6\u4ee5\u53ca\u8239\u53ea\u7c7b\u578b\u4e0b\uff0cHABIT\u751f\u6210\u7684\u6d77\u4e0a\u8f68\u8ff9\u586b\u8865\u7ed3\u679c\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u4e0a\u53ef\u4ee5\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u800c\u4e14\u5728\u5ef6\u8fdf\u8868\u73b0\u4e0a\u66f4\u4f18\uff0c\u540c\u65f6\u8fd8\u80fd\u8003\u8651\u5230\u8239\u53ea\u7279\u5f81\u53ca\u5176\u8fd0\u52a8\u6a21\u5f0f\u3002", "conclusion": "HABIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u586b\u8865AIS\u6570\u636e\u4e2d\u7f3a\u5931\u7684\u8f68\u8ff9\u90e8\u5206\uff0c\u901a\u8fc7\u5229\u7528\u5386\u53f2\u6570\u636e\u4e2d\u7684\u79fb\u52a8\u6a21\u5f0f\u8fdb\u884c\u9884\u6d4b\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u8fd8\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\uff0c\u5e76\u4e14\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7c7b\u578b\u8239\u53ea\u7684\u4e0d\u540c\u8fd0\u52a8\u7279\u70b9\u3002"}}
{"id": "2602.11664", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11664", "abs": "https://arxiv.org/abs/2602.11664", "authors": ["Huimin Yan", "Longfei Xu", "Junjie Sun", "Zheng Liu", "Wei Luo", "Kaikui Liu", "Xiangxiang Chu"], "title": "IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation", "comment": null, "summary": "Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: \"when to depart\", \"how to travel\", \"where to go\", and \"what needs arise via the route\". However, current research is limited by fragmented datasets that focus merely on next POI recommendation (\"where to go\"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.", "AI": {"tldr": "This paper introduces IntTravel, a large-scale public dataset for integrated travel recommendation, and proposes an end-to-end, decoder-only generative framework to improve multi-task recommendation. The solution has been deployed on Amap, significantly increasing the CTR.", "motivation": "The motivation is to address the limitations of current research in next POI recommendation, which only focuses on 'where to go' but neglects other important aspects such as departure time, travel mode, and situational requirements along the journey. Additionally, the limited scale of existing datasets hinders accurate performance evaluation.", "method": "The method involves creating IntTravel, a large-scale public dataset that includes 4.1 billion interactions from 163 million users with 7.3 million POIs. On top of this, an end-to-end, decoder-only generative framework for multi-task recommendation is introduced, incorporating information preservation, selection, and factorization to balance task collaboration with specialized differentiation.", "result": "The results show substantial performance gains from the proposed framework, with state-of-the-art performance across both the IntTravel dataset and an additional non-travel benchmark. When deployed on Amap, it led to a 1.09% increase in CTR.", "conclusion": "The conclusion is that by introducing a comprehensive dataset and a generative framework, the research effectively addresses the gaps in next POI recommendation, leading to improved user experience and higher CTR when implemented in real-world applications."}}
{"id": "2602.11411", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11411", "abs": "https://arxiv.org/abs/2602.11411", "authors": ["Yang Liu", "Armstrong Foundjem", "Xingfang Wu", "Heng Li", "Foutse Khomh"], "title": "Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data", "comment": null, "summary": "Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.\n  Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.\n  Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.\n  Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\\% - 6\\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\\% - 3\\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.\n  Conclusion \\& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u6270\u52a8\u6570\u636e\u96c6\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u7f16\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u8fd9\u4f1a\u8f7b\u5fae\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u80fd\u663e\u8457\u589e\u5f3a\u5176\u5bf9\u6297\u8f93\u5165\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u591a\u6837\u5316\u8f93\u5165\u65f6\u80fd\u591f\u62b5\u6297\u6f5c\u5728\u6f0f\u6d1e\uff0c\u907f\u514d\u4ea7\u751f\u9519\u8bef\u6216\u4e0d\u5b89\u5168\u7684\u4ee3\u7801\u8f93\u51fa\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u5fae\u8c03\u6270\u52a8\u6570\u636e\u96c6\u6765\u63d0\u5347\u8fd9\u4e9b\u6a21\u578b\u5bf9\u4e8e\u5bf9\u6297\u6027\u8f93\u5165\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7814\u7a76\u8005\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u901a\u8fc7\u5b57\u7b26\u7ea7\u3001\u8bcd\u7ea7\u548c\u53e5\u5b50\u7ea7\u6270\u52a8\u7684\u6570\u636e\u96c6\u5bf9LLMs\u8fdb\u884c\u5fae\u8c03\u7684\u6548\u679c\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u57fa\u51c6\u6a21\u578b\u53ca\u672a\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u8fc7\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5229\u7528\u6270\u52a8\u6570\u636e\u96c6\u5fae\u8c03LLMs\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff08RD\u901a\u5e38\u4e0b\u964d\u7ea64%-6%\uff09\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u9c81\u68d2\u6027\u8f83\u5f31\u7684\u6a21\u578b\u3002\u4e0d\u8fc7\uff0c\u8fd9\u6837\u505a\u901a\u5e38\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u7565\u6709\u4e0b\u964d\uff08pass@1\u901a\u5e38\u4e0b\u964d\u7ea61%-3%\uff09\uff0c\u5c3d\u7ba1\u6709\u65f6\u4e5f\u80fd\u89c2\u5bdf\u5230\u6027\u80fd\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u7528\u6270\u52a8\u6570\u636e\u5bf9LLMs\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u7a81\u51fa\u4e86\u5728\u7f16\u7801\u5e94\u7528\u4e2d\u5e73\u8861LLMs\u9c81\u68d2\u6027\u548c\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.11741", "categories": ["cs.DC", "cs.DB", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11741", "abs": "https://arxiv.org/abs/2602.11741", "authors": ["Bo Guan"], "title": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions", "comment": "27 pages, 8 figures, 2 tables", "summary": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528\u7684\u5206\u5e03\u5f0f\u901f\u7387\u9650\u5236\u7cfb\u7edf\u67b6\u6784\uff0c\u4f7f\u7528Redis\u5185\u5b58\u7f13\u5b58\u6570\u636e\u5e93\u53ca\u5176\u6709\u5e8f\u96c6\u6570\u636e\u7ed3\u6784\u6765\u5b9e\u73b0\u9ad8\u6548\u7387\u548c\u4f4e\u5ef6\u8fdf\u7684\u952e\u503c\u5bf9\u6570\u636e\u96c6\u64cd\u4f5c\uff0c\u5e76\u4fdd\u6301\u7cbe\u5ea6\u3002\u6587\u7ae0\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u91cf\u5316\u4e86\u6240\u9009\u6eda\u52a8\u7a97\u53e3\u7b97\u6cd5\u4e0e\u4ee4\u724c\u6876\u548c\u56fa\u5b9a\u7a97\u53e3\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5728\u51c6\u786e\u6027\u548c\u5185\u5b58\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u670d\u52a1\u5668\u7aefLua\u811a\u672c\u5982\u4f55\u901a\u8fc7\u5c06\u6e05\u7406\u3001\u8ba1\u6570\u548c\u63d2\u5165\u6346\u7ed1\u6210\u5355\u4e2a\u539f\u5b50\u64cd\u4f5c\u6765\u6d88\u9664\u5e76\u53d1\u73af\u5883\u4e2d\u7684\u7ade\u4e89\u6761\u4ef6\uff0c\u4ee5\u53ca\u4e09\u5c42\u67b6\u6784\u7684\u8bbe\u8ba1\uff0c\u7528\u4e8e\u7ba1\u7406\u9650\u6d41\u89c4\u5219\u7684\u5b58\u50a8\u548c\u66f4\u65b0\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u540c\u65f6\u5177\u5907\u51c6\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u901f\u7387\u9650\u5236\u5668\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u5728\u7b97\u6cd5\u7cbe\u5ea6\u3001\u53ef\u7528\u6027\u3001\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u9519\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u91c7\u7528Redis\u5185\u5b58\u7f13\u5b58\u6570\u636e\u5e93\u53ca\u5176\u4e2d\u7684Sorted Set\u6570\u636e\u7ed3\u6784\uff0c\u5229\u7528\u5176O(log(N))\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u64cd\u4f5c\u63d0\u4f9b\u9ad8\u6548\u4e14\u4f4e\u5ef6\u65f6\u7684\u6570\u636e\u5904\u7406\u80fd\u529b\uff1b\u901a\u8fc7\u670d\u52a1\u7aefLua\u811a\u672c\u6765\u786e\u4fdd\u6e05\u7406\u3001\u8ba1\u6570\u548c\u63d2\u5165\u7b49\u64cd\u4f5c\u4ee5\u539f\u5b50\u5f62\u5f0f\u6267\u884c\uff0c\u4ece\u800c\u907f\u514d\u5e76\u53d1\u73af\u5883\u4e0b\u7684\u7ade\u6001\u6761\u4ef6\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u67b6\u6784\u7528\u4e8e\u7ba1\u7406\u548c\u66f4\u65b0\u9650\u6d41\u89c4\u5219\u3002", "result": "\u5b9e\u73b0\u4e86\u80fd\u591f\u6ee1\u8db3\u751f\u4ea7\u7ea7\u8981\u6c42\u7684\u5206\u5e03\u5f0f\u901f\u7387\u9650\u5236\u7cfb\u7edf\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4e0d\u4ec5\u652f\u6301\u9ad8\u6548\u7684\u952e\u503c\u5bf9\u6570\u636e\u5904\u7406\u8fd8\u80fd\u591f\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u5f00\u9500\uff1b\u901a\u8fc7Lua\u811a\u672c\u6709\u6548\u89e3\u51b3\u4e86\u5e76\u53d1\u573a\u666f\u4e0b\u7684\u7ade\u6001\u95ee\u9898\uff1b\u5e76\u4e14\u901a\u8fc7\u5bf9CAP\u5b9a\u7406\u4e2dAP\uff08\u53ef\u7528\u6027\u548c\u5206\u533a\u5bb9\u5fcd\u6027\uff09\u7684\u9009\u62e9\u4f5c\u4e3a\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u7684\u4e00\u79cd\u5b9e\u9645\u6298\u8877\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u65e2\u7cbe\u786e\u53c8\u5177\u6709\u9ad8\u5ea6\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u5206\u5e03\u5f0f\u901f\u7387\u9650\u5236\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002\u5b83\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\u548c\u5de5\u5177\u6709\u6548\u5730\u5e73\u8861\u4e86\u51c6\u786e\u6027\u4e0e\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2602.11949", "categories": ["cs.DB", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.11949", "abs": "https://arxiv.org/abs/2602.11949", "authors": ["Victor Marsault", "Antoine Meyer"], "title": "Designing and Comparing RPQ Semantics", "comment": "30 pages, 1 figure", "summary": "Modern property graph database query languages such as Cypher, PGQL, GSQL, and the standard GQL draw inspiration from the formalism of regular path queries (RPQs). In order to output walks explicitly, they depart from the classical and well-studied homomorphism semantics. However, it then becomes difficult to present results to users because RPQs may match infinitely many walks. The aforementioned languages use ad-hoc criteria to select a finite subset of those matches. For instance, Cypher uses trail semantics, discarding walks with repeated edges; PGQL and GSQL use shortest walk semantics, retaining only the walks of minimal length among all matched walks; and GQL allows users to choose from several semantics. Even though there is academic research on these semantics, it focuses almost exclusively on evaluation efficiency.\n  In an attempt to better understand, choose and design RPQ semantics, we present a framework to categorize and compare them according to other criteria. We formalize several possible properties, pertaining to the study of RPQ semantics seen as mathematical functions mapping a database and a query to a finite set of walks. We show that some properties are mutually exclusive, or cannot be met. We also give several new RPQ semantics as examples. Some of them may provide ideas for the design of new semantics for future graph database query languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u5176\u4ed6\u6807\u51c6\u5bf9\u6b63\u5219\u8def\u5f84\u67e5\u8be2\uff08RPQ\uff09\u8bed\u4e49\u8fdb\u884c\u5206\u7c7b\u548c\u6bd4\u8f83\u3002\u901a\u8fc7\u5c06RPQ\u8bed\u4e49\u5f62\u5f0f\u5316\u4e3a\u4ece\u6570\u636e\u5e93\u548c\u67e5\u8be2\u6620\u5c04\u5230\u6709\u9650\u884c\u8d70\u96c6\u7684\u6570\u5b66\u51fd\u6570\uff0c\u5b9a\u4e49\u4e86\u82e5\u5e72\u53ef\u80fd\u7684\u5c5e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u67d0\u4e9b\u5c5e\u6027\u662f\u76f8\u4e92\u6392\u65a5\u6216\u65e0\u6cd5\u6ee1\u8db3\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u7ed9\u51fa\u4e86\u51e0\u4e2a\u65b0\u7684RPQ\u8bed\u4e49\u4f5c\u4e3a\u793a\u4f8b\uff0c\u4ee5\u671f\u5bf9\u672a\u6765\u56fe\u6570\u636e\u5e93\u67e5\u8be2\u8bed\u8a00\u4e2d\u65b0\u7684\u8bed\u4e49\u8bbe\u8ba1\u63d0\u4f9b\u7075\u611f\u3002", "motivation": "\u73b0\u4ee3\u5c5e\u6027\u56fe\u6570\u636e\u5e93\u67e5\u8be2\u8bed\u8a00\u5982Cypher\u3001PGQL\u3001GSQL\u4ee5\u53ca\u6807\u51c6GQL\u53d7\u5230\u6b63\u5219\u8def\u5f84\u67e5\u8be2(RPQs)\u7684\u5f62\u5f0f\u4e3b\u4e49\u542f\u53d1\u3002\u4e3a\u4e86\u660e\u786e\u8f93\u51fa\u8def\u5f84\uff0c\u5b83\u4eec\u504f\u79bb\u4e86\u7ecf\u5178\u7684\u540c\u6001\u8bed\u4e49\u3002\u7136\u800c\uff0c\u7531\u4e8eRPQs\u53ef\u80fd\u4f1a\u5339\u914d\u65e0\u9650\u591a\u7684\u8def\u5f84\uff0c\u5411\u7528\u6237\u5c55\u793a\u7ed3\u679c\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u8bed\u8a00\u4f7f\u7528\u7279\u5b9a\u6807\u51c6\u6765\u9009\u62e9\u6240\u6709\u5339\u914d\u4e2d\u7684\u4e00\u4e2a\u6709\u9650\u5b50\u96c6\u3002\u5c3d\u7ba1\u6709\u5173\u4e8e\u8fd9\u4e9b\u8bed\u4e49\u7684\u7814\u7a76\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u5728\u8bc4\u4f30\u6548\u7387\u4e0a\u3002\u672c\u6587\u8bd5\u56fe\u66f4\u597d\u5730\u7406\u89e3\u3001\u9009\u62e9\u5e76\u8bbe\u8ba1RPQ\u8bed\u4e49\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u5176\u4ed6\u6807\u51c6\u6765\u5206\u7c7b\u548c\u6bd4\u8f83RPQ\u8bed\u4e49\u3002\u4f5c\u8005\u5f62\u5f0f\u5316\u4e86\u591a\u4e2a\u6f5c\u5728\u5c5e\u6027\uff0c\u8fd9\u4e9b\u5c5e\u6027\u4e0e\u88ab\u89c6\u4e3a\u5c06\u6570\u636e\u5e93\u548c\u67e5\u8be2\u6620\u5c04\u5230\u6709\u9650\u884c\u8d70\u96c6\u5408\u7684\u6570\u5b66\u51fd\u6570\u7684RPQ\u8bed\u4e49\u7814\u7a76\u76f8\u5173\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e00\u4e9b\u5c5e\u6027\u662f\u4e92\u76f8\u6392\u65a5\u7684\u6216\u8005\u6839\u672c\u65e0\u6cd5\u6ee1\u8db3\u3002\u540c\u65f6\uff0c\u6587\u4e2d\u4e5f\u4ecb\u7ecd\u4e86\u51e0\u79cd\u65b0\u7684RPQ\u8bed\u4e49\u4f5c\u4e3a\u4f8b\u5b50\u3002", "conclusion": "\u901a\u8fc7\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u53ca\u65b0RPQ\u8bed\u4e49\u793a\u4f8b\uff0c\u53ef\u4ee5\u4e3a\u672a\u6765\u56fe\u6570\u636e\u5e93\u67e5\u8be2\u8bed\u8a00\u4e2d\u65b0\u7684\u8bed\u4e49\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2602.11680", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11680", "abs": "https://arxiv.org/abs/2602.11680", "authors": ["Yihang Li", "Zhuo Liu", "Wei Wei"], "title": "EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation", "comment": "10 pages, 3 figures, 5 tables, accepted by WSDM 2026", "summary": "Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEpicCBR\u7684\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u51b7\u542f\u52a8\u6346\u7ed1\u63a8\u8350\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u7cbe\u786e\u6316\u6398\u548c\u5229\u7528\u9879\u76ee\u5173\u7cfb\u6765\u6784\u5efa\u7528\u6237\u753b\u50cf\uff0c\u5e76\u901a\u8fc7\u5386\u53f2\u6346\u7ed1\u4fe1\u606f\u548c\u7528\u6237\u504f\u597d\u6765\u8868\u5f81\u65b0\u6346\u7ed1\u7684\u7279\u70b9\u3002\u5b9e\u9a8c\u8868\u660e\uff0cEpicCBR\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u6346\u7ed1\u63a8\u8350\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4e8e\u89c2\u5bdf\u5230\u7684\u7528\u6237-\u6346\u7ed1\u4e92\u52a8\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u4e0d\u65ad\u51fa\u73b0\u7684\u65b0\u6346\u7ed1\u7684\u63a2\u7d22\u3002\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u5c06\u6bcf\u4e2a\u6346\u7ed1\u89c6\u4e3a\u72ec\u7acb\u5b9e\u4f8b\uff0c\u800c\u5ffd\u89c6\u4e86\u5145\u5206\u5229\u7528\u70ed\u95e8\u9879\u76ee\u4e0a\u7684\u7528\u6237-\u9879\u76ee(UI)\u548c\u6346\u7ed1-\u9879\u76ee(BI)\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEpicCBR\u7684\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u51b7\u542f\u52a8\u6346\u7ed1\u63a8\u8350\u95ee\u9898\u3002\u8be5\u6846\u67b6\u7cbe\u51c6\u5730\u6316\u6398\u5e76\u5229\u7528\u5546\u54c1\u95f4\u7684\u5173\u7cfb\u4ee5\u6784\u5efa\u7528\u6237\u753b\u50cf\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d7\u6b22\u8fce\u7a0b\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5386\u53f2\u6346\u7ed1\u4fe1\u606f\u53ca\u7528\u6237\u504f\u597d\u6765\u523b\u753b\u65b0\u6346\u7ed1\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u80fd\u591f\u6574\u5408\u591a\u79cd\u4e0d\u540c\u60c5\u5883\u7684\u591a\u89c6\u56fe\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u786e\u4fdd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eEpicCBR\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u7684\u4f18\u52bf\uff08\u6700\u9ad8\u53ef\u8fbe387%\uff09\uff0c\u5c24\u5176\u662f\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u6027\u3002", "conclusion": "EpicCBR\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u51b7\u542f\u52a8\u6346\u7ed1\u63a8\u8350\u65b9\u6cd5\uff0c\u5728\u5904\u7406\u65b0\u5174\u6346\u7ed1\u63a8\u8350\u65b9\u9762\u5c55\u73b0\u4e86\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.11435", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11435", "abs": "https://arxiv.org/abs/2602.11435", "authors": ["Haolin Li", "Michael Coblenz"], "title": "A Grounded Theory of Debugging in Professional Software Engineering Practice", "comment": "Accepted by FSE'26", "summary": "Debugging is a central yet complex activity in software engineering. Prior studies have documented debugging strategies and tool usage, but little theory explains how experienced developers reason about bugs in large, real-world codebases. We conducted a qualitative study using a grounded theory approach. We observed seven professional developers and five professional live-coding streamers working on 17 debugging tasks in their own codebases, capturing diverse contexts of debugging. We theorize debugging as a structured, iterative diagnostic process in which programmers update a mental model of the system to guide information gathering. Developers gather information by alternating between navigation and execution strategies, employing forward and backward tracing modes of reasoning and adapting these approaches according to codebase context, complexity, and familiarity. Developers also gather external resources to complement code-based evidence, with their experience enabling them to systematically construct a mental model. We contribute a grounded theory of professional debugging that surfaces the human-centered dimensions of the practice, with implications for tool design and software engineering education.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u89c2\u5bdf\u4e86\u4e13\u4e1a\u5f00\u53d1\u8005\u548c\u76f4\u64ad\u7f16\u7801\u8005\u5728\u5b9e\u9645\u4ee3\u7801\u5e93\u4e2d\u5904\u7406\u8c03\u8bd5\u4efb\u52a1\u7684\u8fc7\u7a0b\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fed\u4ee3\u8bca\u65ad\u8fc7\u7a0b\u7684\u8c03\u8bd5\u7406\u8bba\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u8005\u5982\u4f55\u6839\u636e\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u3001\u590d\u6742\u6027\u548c\u719f\u6089\u5ea6\u8c03\u6574\u5176\u5bfc\u822a\u548c\u6267\u884c\u7b56\u7565\uff0c\u5e76\u5229\u7528\u5916\u90e8\u8d44\u6e90\u6765\u6784\u5efa\u7cfb\u7edf\u7684\u5fc3\u7406\u6a21\u578b\u3002\u8fd9\u4e00\u7406\u8bba\u4e3a\u5de5\u5177\u8bbe\u8ba1\u548c\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u63d0\u4f9b\u4e86\u5173\u4e8e\u8c03\u8bd5\u5b9e\u8df5\u7684\u4eba\u4e3a\u4e2d\u5fc3\u89c6\u89d2\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8bb0\u5f55\u4e86\u8c03\u8bd5\u7b56\u7565\u548c\u5de5\u5177\u4f7f\u7528\u60c5\u51b5\uff0c\u4f46\u5bf9\u4e8e\u6709\u7ecf\u9a8c\u7684\u5f00\u53d1\u4eba\u5458\u5982\u4f55\u5728\u5927\u578b\u771f\u5b9e\u4e16\u754c\u7684\u4ee3\u7801\u5e93\u4e2d\u63a8\u7406\u9519\u8bef\uff0c\u5c1a\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e13\u4e1a\u5f00\u53d1\u8005\u5728\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u4f55\u601d\u8003\u548c\u884c\u52a8\u3002", "method": "\u91c7\u7528\u624e\u6839\u7406\u8bba\u65b9\u6cd5\u8fdb\u884c\u4e86\u4e00\u9879\u5b9a\u6027\u7814\u7a76\uff0c\u89c2\u5bdf\u4e867\u540d\u804c\u4e1a\u5f00\u53d1\u8005\u53ca5\u540d\u804c\u4e1a\u76f4\u64ad\u7f16\u7801\u8005\u5728\u5176\u4e2a\u4eba\u4ee3\u7801\u5e93\u4e0a\u5b8c\u621017\u4e2a\u8c03\u8bd5\u4efb\u52a1\u7684\u60c5\u51b5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u8c03\u8bd5\u89c6\u4e3a\u7ed3\u6784\u5316\u3001\u8fed\u4ee3\u5f0f\u8bca\u65ad\u8fc7\u7a0b\u7684\u7406\u8bba\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u7a0b\u5e8f\u5458\u66f4\u65b0\u7cfb\u7edf\u7684\u5fc3\u667a\u6a21\u578b\u4ee5\u6307\u5bfc\u4fe1\u606f\u6536\u96c6\u3002\u5f00\u53d1\u8005\u901a\u8fc7\u4ea4\u66ff\u4f7f\u7528\u5bfc\u822a\u4e0e\u6267\u884c\u7b56\u7565\uff0c\u91c7\u7528\u524d\u5411\u548c\u540e\u5411\u8ffd\u8e2a\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u6839\u636e\u4ee3\u7801\u5e93\u80cc\u666f\u3001\u590d\u6742\u5ea6\u548c\u4e2a\u4eba\u719f\u6089\u7a0b\u5ea6\u8c03\u6574\u8fd9\u4e9b\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u4f1a\u641c\u96c6\u5916\u90e8\u8d44\u6e90\u4f5c\u4e3a\u4ee3\u7801\u8bc1\u636e\u7684\u8865\u5145\uff0c\u800c\u4ed6\u4eec\u7684\u7ecf\u9a8c\u4f7f\u4ed6\u4eec\u80fd\u591f\u7cfb\u7edf\u5730\u6784\u5efa\u5fc3\u667a\u6a21\u578b\u3002", "conclusion": "\u8d21\u732e\u4e86\u4e00\u4e2a\u5173\u4e8e\u4e13\u4e1a\u8c03\u8bd5\u7684\u624e\u6839\u7406\u8bba\uff0c\u63ed\u793a\u4e86\u5b9e\u8df5\u4e2d\u4ee5\u4eba\u4e3a\u672c\u7684\u7ef4\u5ea6\uff0c\u8fd9\u5bf9\u5de5\u5177\u8bbe\u8ba1\u548c\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.11998", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.11998", "abs": "https://arxiv.org/abs/2602.11998", "authors": ["Ramakant kumar"], "title": "An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization", "comment": null, "summary": "Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62cd\u5356\u7684\u673a\u5236AUC-RAC\uff0c\u7528\u4e8e\u5728\u7269\u8054\u7f51\u8bbe\u5907\u73af\u5883\u4e2d\u6709\u6548\u5378\u8f7d\u591a\u4e2a\u672c\u5730\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u8ba1\u7b97\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5229\u7528Docker\u96c6\u7fa4\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u57fa\u4e8e\u62cd\u5356\u7684\u7ade\u4ef7\u8fc7\u7a0b\u4f18\u5316\u4e86\u591a\u4e2a\u7cfb\u7edf\u95f4\u8ba1\u7b97\u4efb\u52a1\u7684\u5206\u914d\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7269\u8054\u7f51\u8bbe\u5907\u7684\u4efb\u52a1\u5378\u8f7d\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1\u3002", "motivation": "\u5728\u57fa\u4e8e\u4e91\u7684\u5bb9\u5668\u5316\u4e2d\u4e3a\u7269\u8054\u7f51\u6267\u884c\u4efb\u52a1\u65f6\uff0c\u8d44\u6e90\u7ba1\u7406\u548c\u6210\u672c\u4f18\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u66f4\u9ad8\u6548\u5730\u8fdb\u884c\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u5378\u8f7d\u4e0e\u5904\u7406\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002", "method": "\u63d0\u51fa\u4e86AUC-RAC\uff0c\u4e00\u79cd\u57fa\u4e8e\u62cd\u5356\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u4e0b\u6587\u4e2d\u591a\u4e2a\u672c\u5730\u670d\u52a1\u5668\u4e4b\u95f4\u8ba1\u7b97\u4efb\u52a1\u7684\u6709\u6548\u5378\u8f7d\u3002\u8be5\u65b9\u6848\u7ed3\u5408\u4e86Docker\u96c6\u7fa4\uff08\u7531\u7ba1\u7406\u8282\u70b9MN\u548c\u5de5\u4f5c\u8282\u70b9WNs\u7ec4\u6210\uff09\u4ee5\u53caDocker\u5bb9\u5668\u5316\u6280\u672f\uff0c\u901a\u8fc7\u8ba9\u5404\u8282\u70b9\u53c2\u4e0e\u57fa\u4e8e\u62cd\u5356\u7684\u7ade\u6807\u8fc7\u7a0b\u6765\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0c\u901a\u8fc7\u4fc3\u8fdb\u672c\u5730\u670d\u52a1\u5668\u95f4\u7684\u534f\u4f5c\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u6539\u8fdb\u7684\u4efb\u52a1\u5378\u8f7d\u53ca\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1\u3002", "conclusion": "AUC-RAC\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5728\u63d0\u9ad8\u7269\u8054\u7f51\u73af\u5883\u4e0b\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u6548\u7387\u65b9\u9762\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u5145\u8db3\u6027\u7684\u8003\u91cf\u4e0b\u4f18\u5316\u4e86\u4efb\u52a1\u5728\u591a\u7cfb\u7edf\u4e2d\u7684\u5206\u914d\u3002"}}
{"id": "2602.12064", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.12064", "abs": "https://arxiv.org/abs/2602.12064", "authors": ["Yafeng Nan", "Haifeng Sun", "Zirui Zhuang", "Qi Qi", "Guojun Chu", "Jianxin Liao", "Dan Pei", "Jingyu Wang"], "title": "DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning", "comment": "Accepted by SIGMOD 2026", "summary": "In the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIVER\u7684\u65b0\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u5f0f\u503c\u94fe\u63a5\u81ea\u52a8\u8fdb\u884c\u8bc1\u636e\u63a8\u7406\uff0c\u4ece\u800c\u5728\u6ca1\u6709\u4e13\u5bb6\u5e2e\u52a9\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684Text-to-SQL\u8f6c\u6362\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDIVER\u663e\u8457\u63d0\u9ad8\u4e86\u5404\u79cdText-to-SQL\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u52a8\u6001\u6570\u636e\u5e93\u503c\u65f6\u7279\u522b\u6709\u6548\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684Text-to-SQL\u6a21\u578b\u867d\u7136\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u7f16\u5199\u7684\u8bc1\u636e\u6765\u6f84\u6e05\u6a21\u5f0f\u548c\u503c\u94fe\u63a5\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7f3a\u4e4f\u4e13\u5bb6\u652f\u6301\u7684\u60c5\u51b5\u4e0b\u4f1a\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5728\u65e0\u4e13\u5bb6\u8f85\u52a9\u4e0b\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u8bc1\u636e\u5e76\u5b8c\u6210\u51c6\u786eSQL\u751f\u6210\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aDIVER\u7684\u7cfb\u7edf\uff0c\u5b83\u5229\u7528\u4e00\u5957\u517c\u5bb9\u5de5\u5177\u7bb1\u6765\u63a2\u7d22\u6570\u636e\u5e93\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u5de5\u4f5c\u7a7a\u95f4\uff08CoTF\uff0c\u601d\u7ef4\u4e0e\u4e8b\u5b9e\u94fe\uff09\u57fa\u4e8e\u63a2\u6d4b\u7ed3\u679c\u53cd\u601d\u5e76\u9009\u62e9\u65b0\u5de5\u5177\u8fdb\u884c\u65b0\u4e00\u8f6e\u63a2\u6d4b\u3002\u6b64\u8fc7\u7a0b\u8fed\u4ee3\u6267\u884c\uff0c\u4ee5\u8bc6\u522b\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u53d1\u73b0\u7684\u6a21\u5f0f\u548c\u503c\u94fe\u63a5\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDIER\u7cfb\u7edf\u80fd\u5c06\u4e0d\u540cText-to-SQL\u6a21\u578b\u7684\u6267\u884c\u51c6\u786e\u7387\u63d0\u9ad8\u6700\u591a10.82%\uff0c\u6709\u6548\u6548\u7387\u5f97\u5206\u63d0\u5347\u81f316.09%\uff1b\u540c\u65f6\uff0c\u5728\u9762\u5bf9\u5927\u89c4\u6a21\u53ca\u52a8\u6001\u53d8\u5316\u7684\u6570\u636e\u5e93\u503c\u65f6\uff0c\u5176\u52a8\u6001\u4ea4\u4e92\u5f0f\u503c\u94fe\u63a5\u529f\u80fd\u663e\u8457\u589e\u5f3a\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u94fe\u63a5\u51c6\u786e\u6027\u3002", "conclusion": "DIVER\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4e13\u5bb6\u5e72\u9884\u5373\u53ef\u589e\u5f3aText-to-SQL\u4efb\u52a1\u9c81\u68d2\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u9700\u8981\u5904\u7406\u590d\u6742\u6216\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u96c6\u7684\u5e94\u7528\u800c\u8a00\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2602.11719", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11719", "abs": "https://arxiv.org/abs/2602.11719", "authors": ["Chenxiao Fan", "Chongming Gao", "Yaxin Gong", "Haoyan Liu", "Fuli Feng", "Xiangnan He"], "title": "Uncertainty-aware Generative Recommendation", "comment": null, "summary": "Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u8350\u65b9\u6cd5\u2014\u2014\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u751f\u6210\u63a8\u8350(UGR)\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u7684\u6982\u5ff5\u6765\u4f18\u5316\u63a8\u8350\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u5e76\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e8c\u5143\u7ed3\u679c\u7684\u6b63\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u6a21\u578b\u5185\u5728\u7684\u751f\u6210\u4fe1\u5fc3\u3001\u6837\u672c\u5b66\u4e60\u96be\u5ea6\u7684\u53d8\u5316\u4ee5\u53ca\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u5fc3\u8868\u8fbe\uff0c\u5bfc\u81f4\u8bad\u7ec3\u52a8\u6001\u4e0d\u7a33\u5b9a\u548c\u51b3\u7b56\u98ce\u9669\u65e0\u6cd5\u91cf\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u751f\u6210\u63a8\u8350(UGR)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e09\u4e2a\u673a\u5236\uff1a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u52a0\u6743\u5956\u52b1\u4ee5\u60e9\u7f5a\u81ea\u4fe1\u9519\u8bef\uff1b\u96be\u5ea6\u611f\u77e5\u4f18\u5316\u52a8\u6001\u9632\u6b62\u8fc7\u65e9\u6536\u655b\uff1b\u4ee5\u53ca\u663e\u5f0f\u4fe1\u5fc3\u5bf9\u9f50\u4f7f\u6a21\u578b\u5177\u5907\u4fe1\u5fc3\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUGR\u4e0d\u4ec5\u5728\u63a8\u8350\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u800c\u4e14\u4ece\u6839\u672c\u4e0a\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u907f\u514d\u4e86\u6807\u51c6\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6240\u5b66\u5f97\u7684\u4fe1\u5fc3\u8fd8\u80fd\u652f\u6301\u53ef\u9760\u7684\u4e0b\u6e38\u98ce\u9669\u611f\u77e5\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u81ea\u9002\u5e94\u4f18\u5316\u7684\u5173\u952e\u4fe1\u53f7\uff0cUGR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6539\u5584\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u4e0e\u6548\u80fd\u3002"}}
{"id": "2602.11192", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11192", "abs": "https://arxiv.org/abs/2602.11192", "authors": ["Arian Raje", "Anupam Nayak", "Gauri Joshi"], "title": "MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models", "comment": null, "summary": "Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained settings as all of the parameters must still be loaded into GPU memory. Prior works aim to address this memory bottleneck by offloading certain experts into CPU memory and porting them to GPU memory only when they are activated. In practice, these methods suffer from the significant I/O latency incurred by expert transfer. We present MELINOE, a method that fine-tunes an MoE model to more strongly prefer activating a smaller number of experts per sequence. Caching these preferred experts in GPU memory reduces expert churn and CPU-GPU transfer overhead. MELINOE increases throughput by $1.2-3\\times$ over efficient baselines and up to $14.7\\times$ over transfer-heavy baselines while retaining or even improving the performance of the model on a downstream task, making it a reliable method for improving MoE inference efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMELINOE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03MoE\u6a21\u578b\u4ee5\u66f4\u504f\u597d\u6fc0\u6d3b\u8f83\u5c11\u7684\u4e13\u5bb6\uff0c\u4ece\u800c\u51cf\u5c11GPU\u548cCPU\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u6df7\u5408\u4e13\u5bb6(MoE)\u6a21\u578b\u80fd\u591f\u6709\u6548\u964d\u4f4e\u6bcf\u4e2atoken\u6fc0\u6d3b\u53c2\u6570\u7684\u6570\u91cf\uff0c\u4f46\u7531\u4e8e\u5176\u5e9e\u5927\u7684\u603b\u4f53\u53c2\u6570\u91cf\u53ca\u6a21\u578b\u5927\u5c0f\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u4f7f\u7528\u53d7\u5230\u9650\u5236\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5c1d\u8bd5\u5c06\u90e8\u5206\u4e13\u5bb6\u8fc1\u79fb\u5230CPU\u5185\u5b58\u4e2d\uff0c\u4ec5\u5728\u9700\u8981\u65f6\u624d\u52a0\u8f7d\u5230GPU\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5e26\u6765\u4e86\u663e\u8457\u7684I/O\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "MELINOE\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9MoE\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6765\u4f7f\u5176\u66f4\u52a0\u503e\u5411\u4e8e\u4e3a\u6bcf\u4e2a\u5e8f\u5217\u6fc0\u6d3b\u8f83\u5c11\u6570\u91cf\u7684\u4e13\u5bb6\u3002\u901a\u8fc7\u5728GPU\u5185\u5b58\u4e2d\u7f13\u5b58\u8fd9\u4e9b\u4f18\u9009\u7684\u4e13\u5bb6\uff0c\u53ef\u4ee5\u51cf\u5c11\u4e13\u5bb6\u66f4\u6362\u9891\u7387\u4ee5\u53caCPU-GPU\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\u5f00\u9500\u3002", "result": "\u4e0e\u9ad8\u6548\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cMELINOE\u63d0\u9ad8\u4e861.2\u52303\u500d\u7684\u541e\u5410\u91cf\uff1b\u4e0e\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u4f20\u8f93\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6700\u9ad8\u53ef\u63d0\u5347\u81f314.7\u500d\u7684\u541e\u5410\u91cf\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u8fd8\u4fdd\u6301\u4e86\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "MELINOE\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u9014\u5f84\u6765\u589e\u5f3aMoE\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002"}}
{"id": "2602.12070", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.12070", "abs": "https://arxiv.org/abs/2602.12070", "authors": ["Zixi Cai", "Kuowen Chen", "Shengquan Du", "Tsvi Kopelowitz", "Seth Pettie", "Ben Plosk"], "title": "Contention Resolution, With and Without a Global Clock", "comment": null, "summary": "In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\\left(\\left(n\\log\\log n\\log^{(3)} n\\log^{(4)} n\\cdots \\log^{(\\log^* n)} n\\right)\\cdot 2^{\\log^* n}\\right) \\le n(\\log\\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $\u0398(n \\log n/\\log\\log n)$ whereas the With-High-Probability latency is $\u0398(n\\log^2 n/\\log\\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\\log^2 n/(\\log\\log n)^2)$ and With-High-Probability latency $n\\log^{O(1)} n$ simultaneously.", "AI": {"tldr": "The study introduces a novel Contention Resolution protocol and explores the complexity differences when assuming a global clock, revealing a fundamental trade-off between expected and high-probability latency measures.", "motivation": "The motivation behind this research is to explore the impact of having a global clock on the Contention Resolution problem, which has traditionally been studied under the assumption that parties only have access to their local time since wake-up. The authors aim to show how the presence of a global clock can enrich the problem and lead to new algorithmic techniques.", "method": "The researchers designed a new Contention Resolution protocol with a specific latency bound. They also analyzed the complexity gap between randomized protocols in GlobalClock and LocalClock settings, and compared the expected latency and high-probability latency for memoryless protocols.", "result": "The primary results include a new Contention Resolution protocol with a guaranteed latency, a demonstrated complexity gap between different types of protocols, and a proof that it is impossible to create a protocol that is optimal under both In-Expectation and With-High-Probability metrics.", "conclusion": "The paper concludes by demonstrating that it is impossible to design a Contention Resolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics, and even more, it is not possible to simultaneously achieve an In-Expectation latency of o(n log^2 n / (log log n)^2) and a With-High-Probability latency of n log^O(1) n."}}
{"id": "2602.11836", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11836", "abs": "https://arxiv.org/abs/2602.11836", "authors": ["Alishbah Bashir", "Fatima Qaiser", "Ijaz Hussain"], "title": "ULTRA:Urdu Language Transformer-based Recommendation Architecture", "comment": null, "summary": "Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aULTRA\u7684\u8bed\u4e49\u63a8\u8350\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4e4c\u5c14\u90fd\u8bed\u8fd9\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u4e2a\u6027\u5316\u65b0\u95fb\u68c0\u7d22\u4e2d\u7684\u5185\u5bb9\u63a8\u8350\u95ee\u9898\u3002\u901a\u8fc7\u91c7\u7528\u53cc\u5d4c\u5165\u67b6\u6784\u548c\u57fa\u4e8e\u67e5\u8be2\u957f\u5ea6\u7684\u8def\u7531\u673a\u5236\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u7528\u6237\u67e5\u8be2\u7c7b\u578b\u52a8\u6001\u9009\u62e9\u5408\u9002\u7684\u8bed\u4e49\u5904\u7406\u7ba1\u9053\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u8350\u7684\u76f8\u5173\u6027\u548c\u9002\u5e94\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5355\u4e00\u7ba1\u9053\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u67e5\u8be2\u4e0a\u90fd\u80fd\u663e\u8457\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u4e8690%\u4ee5\u4e0a\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5728\u4e2a\u6027\u5316\u7684\u65b0\u95fb\u68c0\u7d22\u9886\u57df\u7f3a\u4e4f\u6709\u6548\u7684\u8bed\u4e49\u5185\u5bb9\u63a8\u8350\u7cfb\u7edf\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8bcd\u6c47\u5339\u914d\u6216\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u96be\u4ee5\u6355\u6349\u8bed\u4e49\u610f\u56fe\uff0c\u5e76\u4e14\u5728\u5e94\u5bf9\u4e0d\u540c\u957f\u5ea6\u7684\u67e5\u8be2\u548c\u4fe1\u606f\u9700\u6c42\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86ULTRA\uff08\u4e4c\u5c14\u90fd\u8bed\u8bed\u8a00\u8f6c\u6362\u5668\u57fa\u7840\u63a8\u8350\u67b6\u6784\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u8bed\u4e49\u63a8\u8350\u6846\u67b6\u3002\u5b83\u5f15\u5165\u4e86\u5177\u6709\u67e5\u8be2\u957f\u5ea6\u611f\u77e5\u8def\u7531\u673a\u5236\u7684\u53cc\u5d4c\u5165\u67b6\u6784\uff0c\u80fd\u591f\u533a\u5206\u77ed\u5c0f\u3001\u610f\u56fe\u660e\u786e\u7684\u67e5\u8be2\u548c\u8f83\u957f\u3001\u5bcc\u542b\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u67e5\u8be2\u3002\u57fa\u4e8e\u4e00\u4e2a\u9608\u503c\u9a71\u52a8\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7528\u6237\u67e5\u8be2\u88ab\u5bfc\u5411\u4e13\u95e8\u4f18\u5316\u8fc7\u7684\u8bed\u4e49\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u9002\u5e94\u6807\u9898/\u5934\u6761\u7ea7\u522b\u6216\u5168\u6587/\u6587\u6863\u7ea7\u522b\u7684\u8868\u793a\u3002", "result": "\u901a\u8fc7\u5bf9\u5927\u89c4\u6a21\u4e4c\u5c14\u90fd\u8bed\u6587\u672c\u5e93\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u6240\u63d0\u51fa\u7684\u67b6\u6784\u80fd\u591f\u5728\u591a\u79cd\u67e5\u8be2\u7c7b\u578b\u4e0b\u6301\u7eed\u63d0\u5347\u63a8\u8350\u76f8\u5173\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u5355\u4e00\u5904\u7406\u6d41\u7a0b\u57fa\u51c6\u76f8\u6bd4\uff0c\u63a8\u8350\u51c6\u786e\u5ea6\u63d0\u9ad8\u4e86\u8d85\u8fc790%\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u786e\u7acb\u4e86ULTRA\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u4e00\u79cd\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684\u6587\u672c\u63a8\u8350\u67b6\u6784\u7684\u5730\u4f4d\uff0c\u4e3a\u8fd9\u7c7b\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u68c0\u7d22\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2602.11194", "categories": ["cs.LG", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.11194", "abs": "https://arxiv.org/abs/2602.11194", "authors": ["Mahta Movasat", "Ingrid Tomac"], "title": "Predicting the post-wildfire mudflow onset using machine learning models on multi-parameter experimental data", "comment": null, "summary": "Post-wildfire mudflows are increasingly hazardous due to the prevalence of wildfires, including those on the wildland-urban interface. Upon burning, soil on the surface or immediately beneath becomes hydrophobic, a phenomenon that occurs predominantly on sand-based hillslopes. Rainwater and eroded soil blanket the downslope, leading to catastrophic debris flows. Soil hydrophobicity enhances erosion, resulting in post-wildfire debris flows that differ from natural mudflows in intensity, duration, and destructiveness. Thus, it is crucial to understand the timing and conditions of debris-flow onset, driven by the coupled effects of critical parameters: varying rain intensities (RI), slope gradients, water-entry values, and grain sizes (D50). Machine Learning (ML) techniques have become increasingly valuable in geotechnical engineering due to their ability to model complex systems without predefined assumptions. This study applies multiple ML algorithms: multiple linear regression (MLR), logistic regression (LR), support vector classifier (SVC), K-means clustering, and principal component analysis (PCA) to predict and classify outcomes from laboratory experiments that model field conditions using a rain device on various soils in sloped flumes. While MLR effectively predicted total discharge, erosion predictions were less accurate, especially for coarse sand. LR and SVC achieved good accuracy in classifying failure outcomes, supported by clustering and dimensionality reduction. Sensitivity analysis revealed that fine sand is highly susceptible to erosion, particularly under low-intensity, long-duration rainfall. Results also show that the first 10 minutes of high-intensity rain are most critical for discharge and failure. These findings highlight the potential of ML for post-wildfire hazard assessment and emergency response planning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u591a\u5143\u7ebf\u6027\u56de\u5f52\u3001\u903b\u8f91\u56de\u5f52\u7b49\uff09\u5206\u6790\u4e86\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u4e0b\u6a21\u62df\u7684\u964d\u96e8\u5bf9\u4e0d\u540c\u571f\u58e4\u659c\u5761\u7684\u5f71\u54cd\uff0c\u4ee5\u9884\u6d4b\u548c\u5206\u7c7b\u6ce5\u6d41\u7684\u53d1\u751f\u3002\u7814\u7a76\u53d1\u73b0\u7ec6\u6c99\u5728\u4f4e\u5f3a\u5ea6\u957f\u65f6\u95f4\u964d\u96e8\u4e0b\u7279\u522b\u5bb9\u6613\u53d7\u5230\u4fb5\u8680\uff0c\u4e14\u9ad8\u5f3a\u5ea6\u964d\u96e8\u7684\u524d10\u5206\u949f\u5bf9\u4e8e\u6ce5\u6d41\u7684\u4ea7\u751f\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u91ce\u706b\u540e\u707e\u5bb3\u8bc4\u4f30\u53ca\u5e94\u6025\u54cd\u5e94\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u91ce\u706b\u9891\u53d1\u5bfc\u81f4\u7684\u5730\u8868\u6216\u8fd1\u5730\u8868\u571f\u58e4\u758f\u6c34\u6027\u589e\u52a0\uff0c\u8fdb\u800c\u5f15\u53d1\u707e\u96be\u6027\u7684\u6ce5\u77f3\u6d41\u73b0\u8c61\u53d8\u5f97\u8d8a\u6765\u8d8a\u5371\u9669\u3002\u4e3a\u4e86\u7406\u89e3\u7531\u5173\u952e\u53c2\u6570\u5171\u540c\u4f5c\u7528\u9a71\u52a8\u7684\u6ce5\u77f3\u6d41\u53d1\u751f\u7684\u65f6\u95f4\u548c\u6761\u4ef6\uff0c\u5305\u62ec\u53d8\u5316\u7684\u964d\u96e8\u5f3a\u5ea6\u3001\u5761\u5ea6\u68af\u5ea6\u3001\u6c34\u5206\u5165\u6e17\u503c\u548c\u9897\u7c92\u5927\u5c0f\u7b49\u56e0\u7d20\uff0c\u8fdb\u884c\u8fd9\u9879\u7814\u7a76\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u96e8\u91cf\u8bbe\u5907\u5728\u503e\u659c\u7684\u6c9f\u69fd\u4e2d\u5bf9\u5404\u79cd\u571f\u58e4\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6a21\u4eff\u91ce\u5916\u6761\u4ef6\uff0c\u5e76\u5e94\u7528\u591a\u4e2a\u673a\u5668\u5b66\u4e60(ML)\u7b97\u6cd5\uff1a\u591a\u5143\u7ebf\u6027\u56de\u5f52(MLR)\u3001\u903b\u8f91\u56de\u5f52(LR)\u3001\u652f\u6301\u5411\u91cf\u5206\u7c7b\u5668(SVC)\u3001K-\u5747\u503c\u805a\u7c7b\u4ee5\u53ca\u4e3b\u6210\u5206\u5206\u6790(PCA)\uff0c\u6765\u9884\u6d4b\u603b\u6392\u653e\u91cf\u3001\u4fb5\u8680\u60c5\u51b5\u5e76\u5bf9\u5931\u8d25\u7ed3\u679c\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u591a\u5143\u7ebf\u6027\u56de\u5f52\u80fd\u591f\u6709\u6548\u9884\u6d4b\u603b\u6392\u653e\u91cf\uff0c\u4f46\u5728\u9884\u6d4b\u7c97\u7802\u4fb5\u8680\u65b9\u9762\u4e0d\u591f\u51c6\u786e\uff1b\u903b\u8f91\u56de\u5f52\u548c\u652f\u6301\u5411\u91cf\u5206\u7c7b\u5668\u5728\u5206\u7c7b\u5931\u8d25\u7ed3\u679c\u4e0a\u8fbe\u5230\u4e86\u826f\u597d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f97\u5230\u4e86\u805a\u7c7b\u548c\u964d\u7ef4\u7684\u652f\u6301\u3002\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff0c\u7ec6\u6c99\u5728\u4f4e\u5f3a\u5ea6\u3001\u957f\u6301\u7eed\u65f6\u95f4\u7684\u964d\u96e8\u4e0b\u6781\u6613\u53d7\u5230\u4fb5\u8680\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\u9ad8\u5f3a\u5ea6\u964d\u96e8\u6700\u521d\u768410\u5206\u949f\u5bf9\u4e8e\u6392\u653e\u91cf\u548c\u5931\u8d25\u6700\u4e3a\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u7406\u89e3\u548c\u9884\u6d4b\u91ce\u706b\u540e\u6ce5\u77f3\u6d41\u884c\u4e3a\u65b9\u9762\u7684\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u707e\u5bb3\u8bc4\u4f30\u548c\u7d27\u6025\u54cd\u5e94\u8ba1\u5212\u5236\u5b9a\u65b9\u9762\u5c55\u793a\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.11487", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11487", "abs": "https://arxiv.org/abs/2602.11487", "authors": ["Asmar Muqeet", "Shaukat Ali", "Paolo Arcaini"], "title": "Search-Based Quantum Program Testing via Commuting Pauli String", "comment": null, "summary": "Quantum software testing is important for reliable quantum software engineering. Despite recent advances, existing quantum software testing approaches rely on simple test inputs and statistical oracles, costly program specifications, and limited validation on real quantum computers. To address these challenges, we propose SB-QOPS, a search-based quantum program testing approach via commuting Pauli strings. SB-QOPS, as a direct extension to a previously proposed QOPS approach, redefines test cases in terms of Pauli strings and introduces a measurement-centric oracle that exploits their commutation properties, enabling effective testing of quantum programs while reducing the need for full program specifications. By systematically exploring the search space through an expectation-value-based fitness function, SB-QOPS improves test budget utilization and increases the likelihood of uncovering subtle faults. We conduct a large-scale empirical evaluation on quantum circuits of up to 29 qubits on real quantum computers and emulators. We assess three search strategies: Genetic Algorithm, Hill Climbing, and the (1+1) Evolutionary Algorithm, and evaluate SB-QOPS under both simulated and real noisy conditions. Experiments span three quantum computing platforms: IBM, IQM, and Quantinuum. Results show that SB-QOPS significantly outperforms QOPS, achieving a fault-detection score of 100% for circuits up to 29 qubits, and demonstrating portability across quantum platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u91cf\u5b50\u7a0b\u5e8f\u6d4b\u8bd5\u65b9\u6cd5SB-QOPS\uff0c\u901a\u8fc7\u53ef\u4ea4\u6362\u6ce1\u5229\u4e32\u91cd\u65b0\u5b9a\u4e49\u4e86\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u4ee5\u6d4b\u91cf\u4e3a\u4e2d\u5fc3\u7684\u9884\u8a00\u673a\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u9884\u7b97\u7684\u5229\u7528\u6548\u7387\uff0c\u5e76\u589e\u52a0\u4e86\u53d1\u73b0\u7ec6\u5fae\u6545\u969c\u7684\u53ef\u80fd\u6027\u3002\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u771f\u5b9e\u91cf\u5b50\u8ba1\u7b97\u673a\u548c\u6a21\u62df\u5668\u4e0a\uff0c\u5bf9\u4e8e\u591a\u8fbe29\u4e2a\u91cf\u5b50\u4f4d\u7684\u91cf\u5b50\u7535\u8def\uff0cSB-QOPS\u5728\u4e09\u79cd\u4e0d\u540c\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u4e0a\u7684\u6545\u969c\u68c0\u6d4b\u5f97\u5206\u4e3a100%\u3002", "motivation": "\u73b0\u6709\u7684\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u6d4b\u8bd5\u8f93\u5165\u548c\u7edf\u8ba1\u9884\u8a00\u673a\u3001\u6602\u8d35\u7684\u7a0b\u5e8f\u89c4\u8303\u4ee5\u53ca\u5728\u5b9e\u9645\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6709\u9650\u7684\u9a8c\u8bc1\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86SB-QOPS\u3002", "method": "SB-QOPS\u6269\u5c55\u4e86\u4e4b\u524d\u63d0\u51fa\u7684QOPS\u65b9\u6cd5\uff0c\u5c06\u6d4b\u8bd5\u7528\u4f8b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6ce1\u5229\u4e32\u5f62\u5f0f\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5229\u7528\u5b83\u4eec\u4ea4\u6362\u5c5e\u6027\u7684\u6d4b\u91cf\u4e2d\u5fc3\u9884\u8a00\u673a\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u63a2\u7d22\u671f\u671b\u503c\u4e3a\u57fa\u7840\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u6240\u5b9a\u4e49\u7684\u641c\u7d22\u7a7a\u95f4\u6765\u63d0\u9ad8\u6d4b\u8bd5\u9884\u7b97\u4f7f\u7528\u7387\u5e76\u589e\u52a0\u63ed\u793a\u6f5c\u5728\u9519\u8bef\u7684\u673a\u4f1a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSB-QOPS\u663e\u8457\u4f18\u4e8eQOPS\uff0c\u5728IBM\u3001IQM\u548cQuantinuum\u4e09\u4e2a\u4e0d\u540c\u7684\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u4e0a\uff0c\u5bf9\u6700\u591a\u5305\u542b29\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7535\u8def\u5b9e\u73b0\u4e86100%\u7684\u6545\u969c\u68c0\u6d4b\u7387\u3002", "conclusion": "SB-QOPS\u4e3a\u91cf\u5b50\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u80fd\u591f\u51cf\u5c11\u5bf9\u5b8c\u6574\u7a0b\u5e8f\u89c4\u8303\u7684\u9700\u6c42\u540c\u65f6\u63d0\u9ad8\u6d4b\u8bd5\u6548\u7387\u4e0e\u8d28\u91cf\u3002"}}
{"id": "2602.12151", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.12151", "abs": "https://arxiv.org/abs/2602.12151", "authors": ["Youhe Jiang", "Fangcheng Fu", "Taiyi Wang", "Guoliang He", "Eiko Yoneki"], "title": "OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration", "comment": null, "summary": "Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\\times$ (average: 1.5$\\times$) compared to state-of-the-art serving systems.", "AI": {"tldr": "OServe\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\u548c\u6709\u6548\u7684\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u5207\u6362\u65b9\u6cd5\u6765\u89e3\u51b3\u5de5\u4f5c\u8d1f\u8f7d\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u670d\u52a1\u7cfb\u7edf\u76f8\u6bd4\uff0cOServe\u53ef\u4ee5\u5c06\u6027\u80fd\u63d0\u9ad8\u81f3\u591a2\u500d\uff08\u5e73\u57471.5\u500d\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u901a\u5e38\u5047\u8bbe\u5de5\u4f5c\u8d1f\u8f7d\u5728\u7a7a\u95f4\u4e0a\u662f\u5747\u5300\u7684\uff0c\u5728\u65f6\u95f4\u4e0a\u662f\u7a33\u5b9a\u7684\uff0c\u5e76\u91c7\u7528\u540c\u6784\u9759\u6001\u6a21\u578b\u90e8\u7f72\u3002\u7136\u800c\u5b9e\u9645\u4e0a\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u5f02\u8d28\u6027\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86OServe\u7cfb\u7edf\u3002", "method": "OServe\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6839\u636e\u5b9e\u65f6\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u4f18\u5316\u5f02\u6784\u6a21\u578b\u90e8\u7f72\u3002\u6b64\u5916\uff0cOServe\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u5207\u6362\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u9884\u6d4b\u7684\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u8fc1\u79fb\u6a21\u578b\u90e8\u7f72\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u8ffd\u8e2a\u6570\u636e\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u670d\u52a1\u7cfb\u7edf\uff0cOServe\u80fd\u591f\u5b9e\u73b0\u6700\u9ad8\u8fbe2\u500d\u3001\u5e73\u57471.5\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\u53ca\u81ea\u9002\u5e94\u5207\u6362\u673a\u5236\uff0cOServe\u6210\u529f\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u65f6\u7a7a\u5f02\u8d28\u6027\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2602.11841", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11841", "abs": "https://arxiv.org/abs/2602.11841", "authors": ["Moncef Garouani", "Josiane Mothe"], "title": "Improving Neural Retrieval with Attribution-Guided Query Rewriting", "comment": null, "summary": "Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u6307\u5bfc\u7684\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u68c0\u7d22\u5668\u4e2d\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u6807\u8bb0\u5c5e\u6027\u6765\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u67e5\u8be2\u91cd\u5199\uff0c\u4ee5\u6f84\u6e05\u542b\u7cca\u6216\u8bef\u5bfc\u6027\u7684\u67e5\u8be2\u90e8\u5206\u540c\u65f6\u4fdd\u6301\u539f\u610f\u3002\u5728BEIR\u96c6\u5408\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9690\u542b\u6216\u6a21\u7cca\u4fe1\u606f\u9700\u6c42\u65f6\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u68c0\u7d22\u5668\u867d\u7136\u6709\u6548\u4f46\u5bb9\u6613\u53d7\u5230\u4e0d\u660e\u786e\u6216\u6a21\u68f1\u4e24\u53ef\u67e5\u8be2\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5373\u4f7f\u5b58\u5728\u76f8\u5173\u6587\u6863\u4e5f\u4f1a\u53d1\u751f\u9519\u8bef\u6392\u540d\u3002\u800c\u5f53\u524d\u89e3\u51b3\u6b64\u8106\u5f31\u6027\u7684\u65b9\u6cd5\u8981\u4e48\u662f\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u72ec\u7acb\u5730\u91cd\u5199\u67e5\u8be2\u800c\u4e0d\u8003\u8651\u68c0\u7d22\u53cd\u9988\uff0c\u8981\u4e48\u662f\u4f7f\u7528\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u8bef\u5bfc\u6027\u8bcd\u8bed\u4f46\u4ec5\u7528\u4e8e\u4e8b\u540e\u5206\u6790\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u9996\u5148\u9488\u5bf9\u6bcf\u4e2a\u67e5\u8be2\u4ece\u68c0\u7d22\u5668\u4e2d\u8ba1\u7b97\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684\u8bcd\u7ea7\u5c5e\u6027\u5206\u6570\uff1b\u7136\u540e\u5c06\u8fd9\u4e9b\u5206\u6570\u4f5c\u4e3a\u8f6f\u6307\u5f15\uff0c\u5728\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u63d0\u793a\u4e2d\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u67e5\u8be2\u4e2d\u8f83\u5f31\u6216\u5177\u6709\u8bef\u5bfc\u6027\u7684\u90e8\u5206\u8fdb\u884c\u6f84\u6e05\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u610f\u56fe\u4e0d\u53d8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u91cd\u5199\u65b9\u6cd5\u76f8\u6bd4\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u5728\u68c0\u7d22\u6548\u679c\u4e0a\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9690\u542b\u6216\u6a21\u7cca\u7684\u4fe1\u606f\u9700\u6c42\u65f6\u83b7\u5f97\u4e86\u66f4\u5927\u7684\u6536\u76ca\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u5668\u53cd\u9988\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u67e5\u8be2\u91cd\u5199\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u56e0\u67e5\u8be2\u8868\u8ff0\u4e0d\u6e05\u800c\u5bfc\u81f4\u7684\u68c0\u7d22\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2602.11200", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.11200", "abs": "https://arxiv.org/abs/2602.11200", "authors": ["Guozhen Zhu", "Yuqian Hu", "Sakila Jayaweera", "Weihang Gao", "Wei-Hsiang Wang", "Jiaxuan Zhang", "Beibei Wang", "Chenshu Wu", "K. J. Ray Liu"], "title": "AM-FM: A Foundation Model for Ambient Intelligence Through WiFi", "comment": null, "summary": "Ambient intelligence, continuously understanding human presence, activity, and physiology in physical spaces, is fundamental to smart environments, health monitoring, and human-computer interaction. WiFi infrastructure provides a ubiquitous, always-on, privacy-preserving substrate for this capability across billions of IoT devices. Yet this potential remains largely untapped, as wireless sensing has typically relied on task-specific models that require substantial labeled data and limit practical deployment. We present AM-FM, the first foundation model for ambient intelligence and sensing through WiFi. AM-FM is pre-trained on 9.2 million unlabeled Channel State Information (CSI) samples collected over 439 days from 20 commercial device types deployed worldwide, learning general-purpose representations via contrastive learning, masked reconstruction, and physics-informed objectives tailored to wireless signals. Evaluated on public benchmarks spanning nine downstream tasks, AM-FM shows strong cross-task performance with improved data efficiency, demonstrating that foundation models can enable scalable ambient intelligence using existing wireless infrastructure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AM-FM\uff0c\u9996\u4e2a\u901a\u8fc7WiFi\u5b9e\u73b0\u73af\u5883\u667a\u80fd\u548c\u611f\u77e5\u7684\u57fa\u7840\u6a21\u578b\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u4ece\u5168\u7403\u90e8\u7f72\u768420\u79cd\u5546\u7528\u8bbe\u5907\u7c7b\u578b\u6536\u96c6\u7684920\u4e07\u4e2a\u672a\u6807\u8bb0CSI\u6837\u672c\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u4e5d\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u6027\u80fd\u548c\u6539\u8fdb\u7684\u6570\u636e\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1WiFi\u57fa\u7840\u8bbe\u65bd\u4e3a\u6570\u5341\u4ebf\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u4e86\u65e0\u5904\u4e0d\u5728\u3001\u59cb\u7ec8\u5728\u7ebf\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u80fd\u529b\uff0c\u4f46\u65e0\u7ebf\u611f\u77e5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u4eec\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5229\u7528\u73b0\u6709\u65e0\u7ebf\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u53ef\u6269\u5c55\u73af\u5883\u667a\u80fd\u7684\u57fa\u7840\u6a21\u578b\u3002", "method": "AM-FM\u662f\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u3001\u63a9\u7801\u91cd\u5efa\u4ee5\u53ca\u9488\u5bf9\u65e0\u7ebf\u4fe1\u53f7\u5b9a\u5236\u7684\u7269\u7406\u4fe1\u606f\u76ee\u6807\uff0c\u5728\u6765\u81ea\u5168\u7403\u90e8\u7f72\u768420\u79cd\u5546\u7528\u8bbe\u5907\u7c7b\u578b\u7684920\u4e07\u672a\u6807\u8bb0CSI\u6837\u672c\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u3002\u8fd9\u4e9b\u6837\u672c\u662f\u5728439\u5929\u5185\u6536\u96c6\u7684\u3002", "result": "AM-FM\u5728\u6db5\u76d6\u4e5d\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u5982AM-FM\uff0c\u53ef\u4ee5\u5229\u7528\u73b0\u6709\u7684\u65e0\u7ebf\u57fa\u7840\u8bbe\u65bd\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u73af\u5883\u667a\u80fd\u3002"}}
{"id": "2602.11776", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.11776", "abs": "https://arxiv.org/abs/2602.11776", "authors": ["Cl\u00e1udio Correia", "Alberto E. A. Ferreira", "Lucas Martins", "Miguel P. Bento", "Sofia Guerreiro", "Ricardo Ribeiro Pereira", "Ana Sofia Gomes", "Jacopo Bono", "Hugo Ferreira", "Pedro Bizarro"], "title": "MUSE: Multi-Tenant Model Serving With Seamless Model Updates", "comment": "Currently under review for KDD 2026 (Applied Data Science)", "summary": "In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMUSE\u7684\u6a21\u578b\u670d\u52a1\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5c06\u6a21\u578b\u5206\u6570\u4e0e\u5ba2\u6237\u51b3\u7b56\u8fb9\u754c\u5206\u79bb\u6765\u5b9e\u73b0\u65e0\u7f1d\u6a21\u578b\u66f4\u65b0\u3002MUSE\u9488\u5bf9\u591a\u79df\u6237\u73af\u5883\u8bbe\u8ba1\uff0c\u901a\u8fc7\u52a8\u6001\u610f\u56fe\u8def\u7531\u548c\u4e24\u7ea7\u5206\u6570\u8f6c\u6362\u4f18\u5316\u57fa\u7840\u8bbe\u65bd\u590d\u7528\uff0c\u786e\u4fdd\u6a21\u578b\u8f93\u51fa\u6620\u5c04\u5230\u4e00\u4e2a\u7a33\u5b9a\u7684\u53c2\u8003\u5206\u5e03\u3002\u5728Feedzai\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\uff0cMUSE\u5904\u7406\u4e86\u5927\u91cf\u4e8b\u4ef6\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u6a21\u578b\u4e0a\u7ebf\u65f6\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u9002\u5e94\u6027\uff0c\u8282\u7701\u4e86\u6570\u767e\u4e07\u7f8e\u5143\u7684\u6b3a\u8bc8\u635f\u5931\u548c\u8fd0\u8425\u6210\u672c\u3002", "motivation": "\u5728\u4e8c\u5206\u7c7b\u7cfb\u7edf\u4e2d\uff0c\u51b3\u7b56\u9608\u503c\u7528\u4e8e\u5c06\u6a21\u578b\u8bc4\u5206\u8f6c\u5316\u4e3a\u5177\u4f53\u884c\u52a8\u3002\u9009\u62e9\u5408\u9002\u7684\u9608\u503c\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u57fa\u7840\u6a21\u578b\u8bc4\u5206\u7684\u5177\u4f53\u5206\u5e03\uff0c\u8fd8\u53d6\u51b3\u4e8e\u6bcf\u4e2a\u4f7f\u7528\u8be5\u6a21\u578b\u5ba2\u6237\u7684\u7279\u5b9a\u4e1a\u52a1\u51b3\u7b56\u3002\u7136\u800c\uff0c\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u6539\u53d8\u8bc4\u5206\u5206\u5e03\uff0c\u4f7f\u73b0\u6709\u9608\u503c\u5931\u6548\u3002\u5728\u591a\u79df\u6237\u201c\u8bc4\u5206\u5373\u670d\u52a1\u201d\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u51b3\u7b56\u754c\u9650\u4f4d\u4e8e\u5ba2\u6237\u7aef\u7ba1\u7406\u7684\u57fa\u7840\u67b6\u6784\u5185\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e00\u4e2a\u4e25\u91cd\u74f6\u9888\uff1a\u91cd\u65b0\u6821\u51c6\u9700\u8981\u534f\u8c03\u6570\u767e\u4e2a\u5ba2\u6237\u4e4b\u95f4\u7684\u9608\u503c\u66f4\u65b0\uff0c\u6d88\u8017\u4e86\u5927\u91cf\u4eba\u529b\u8d44\u6e90\u5e76\u5bfc\u81f4\u6a21\u578b\u505c\u6ede\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86MUSE\uff08Model Update with Seamless Experience\uff09\uff0c\u4e00\u79cd\u6a21\u578b\u670d\u52a1\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5c06\u6a21\u578b\u8bc4\u5206\u4e0e\u5ba2\u6237\u7aef\u51b3\u7b56\u8fb9\u754c\u89e3\u8026\u6765\u652f\u6301\u65e0\u7f1d\u6a21\u578b\u66f4\u65b0\u3002MUSE\u5229\u7528\u52a8\u6001\u610f\u56fe\u5bfc\u5411\u8def\u7531\u5171\u4eab\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e24\u5c42\u8bc4\u5206\u8f6c\u6362\u673a\u5236\uff0c\u5c06\u6a21\u578b\u8f93\u51fa\u6620\u5c04\u5230\u4e00\u4e2a\u7a33\u5b9a\u3001\u53c2\u7167\u7684\u5206\u5e03\u4e0a\u3002", "result": "MUSE\u5df2\u5728Feedzai\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u6bcf\u79d2\u5904\u7406\u8d85\u8fc7\u4e00\u5343\u4e2a\u4e8b\u4ef6\uff0c\u5728\u8fc7\u53bb12\u4e2a\u6708\u91cc\u5904\u7406\u4e86\u8d85\u8fc7550\u4ebf\u4e2a\u4e8b\u4ef6\uff0c\u8986\u76d6\u6570\u5341\u4e2a\u79df\u6237\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ef\u7528\u6027\u548c\u4f4e\u5ef6\u8fdf\u627f\u8bfa\u3002\u901a\u8fc7\u5c06\u6a21\u578b\u524d\u7f6e\u65f6\u95f4\u4ece\u51e0\u5468\u51cf\u5c11\u5230\u51e0\u5206\u949f\uff0cMUSE\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u53d8\u5316\u653b\u51fb\u7684\u62b5\u5fa1\u80fd\u529b\uff0c\u4ece\u800c\u907f\u514d\u4e86\u6570\u767e\u4e07\u7f8e\u5143\u7684\u6b3a\u8bc8\u635f\u5931\u53ca\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "MUSE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u79df\u6237\u73af\u5883\u4e0b\u6a21\u578b\u66f4\u65b0\u65f6\u9047\u5230\u7684\u95ee\u9898\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u56e0\u91cd\u65b0\u8bad\u7ec3\u5f15\u8d77\u7684\u8bc4\u5206\u5206\u5e03\u504f\u79fb\u4ee5\u53ca\u8de8\u591a\u4e2a\u5ba2\u6237\u7aef\u534f\u8c03\u9608\u503c\u66f4\u65b0\u6240\u9700\u7684\u4eba\u529b\u8d44\u6e90\u3002\u5176\u6210\u529f\u5e94\u7528\u8bc1\u660e\u4e86MUSE\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u589e\u5f3a\u9762\u5bf9\u4e0d\u65ad\u6f14\u53d8\u5a01\u80c1\u65f6\u7684\u5b89\u5168\u9632\u62a4\u6c34\u5e73\u3002"}}
{"id": "2602.11204", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11204", "abs": "https://arxiv.org/abs/2602.11204", "authors": ["Zhuxin Lei", "Ziyuan Yang", "Yi Zhang"], "title": "Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders", "comment": null, "summary": "The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downstream models. While several defense methods have been explored recently, they rely primarily on task-specific adversarial fine-tuning, which inevitably limits generalizability and causes catastrophic forgetting and deteriorates benign performance. Different with previous works, we propose a more rigorous defense goal that requires only a single tuning for diverse downstream tasks to defend against DAEs and preserve benign performance. To achieve this defense goal, we introduce Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD), which is inspired by the inherent sensitivity of neural networks to data characteristics. Specifically, ZePAD is a dual-branch structure, which consists of a Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch) that uses two adversarially fine-tuned encoders to strengthen adversarial resistance. The Benign Memory Preservation Branch (BMP-Branch) is trained on local data to ensure adversarial robustness does not compromise benign performance. Surprisingly, we find that ZePAD can directly detect DAEs by evaluating branch confidence, without introducing any adversarial exsample identification task during training. Notably, by enriching feature diversity, our method enables a single adversarial fine-tuning to defend against DAEs across downstream tasks, thereby achieving persistent robustness. Extensive experiments on 11 SSL methods and 6 datasets validate its effectiveness. In certain cases, it achieves a 29.20% improvement in benign performance and a 73.86% gain in adversarial robustness, highlighting its zero-sacrifice property.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZePAD\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9632\u5fa1\u4e0b\u6e38\u4efb\u52a1\u65e0\u5173\u7684\u5bf9\u6297\u6027\u793a\u4f8b\uff08DAEs\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u826f\u6027\u6570\u636e\u7684\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53cc\u5206\u652f\u7ed3\u6784\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u4e00\u6b21\u5fae\u8c03\u5373\u53ef\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0cZePAD\u5728\u63d0\u9ad8\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u540c\u65f6\u4e5f\u663e\u8457\u63d0\u5347\u4e86\u826f\u6027\u6837\u672c\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5bb9\u6613\u53d7\u5230\u4e0b\u6e38\u4efb\u52a1\u65e0\u5173\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u9632\u5fa1\u624b\u6bb5\u591a\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u5bf9\u6297\u6027\u5fae\u8c03\uff0c\u8fd9\u4e0d\u4ec5\u9650\u5236\u4e86\u901a\u7528\u6027\u8fd8\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u548c\u826f\u6027\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u6297\u6b64\u7c7b\u653b\u51fb\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u5176\u5728\u6b63\u5e38\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "ZePAD\u91c7\u7528\u53cc\u5206\u652f\u8bbe\u8ba1\uff1a\u4e00\u4e2aMulti-Pattern Adversarial Enhancement Branch\u4f7f\u7528\u4e24\u4e2a\u5bf9\u6297\u6027\u5fae\u8c03\u540e\u7684\u7f16\u7801\u5668\u52a0\u5f3a\u62b5\u6297\u80fd\u529b\uff1b\u53e6\u4e00\u4e2aBenign Memory Preservation Branch\u5219\u4e13\u6ce8\u4e8e\u672c\u5730\u6570\u636e\u4ee5\u786e\u4fdd\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u63d0\u5347\u4e0d\u4f1a\u635f\u5bb3\u5230\u826f\u6027\u6837\u672c\u5904\u7406\u80fd\u529b\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u589e\u52a0\u7279\u5f81\u591a\u6837\u6027\uff0c\u4f7f\u5f97\u5355\u6b21\u5bf9\u6297\u6027\u5fae\u8c03\u8db3\u4ee5\u5e94\u5bf9\u8de8\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u65f6\u9047\u5230\u7684DAE\u6311\u6218\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86ZePAD\u7684\u6709\u6548\u6027\uff0c\u572811\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u548c6\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cZePAD\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u5c06\u826f\u6027\u6027\u80fd\u63d0\u9ad829.20%\uff0c\u540c\u65f6\u5c06\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u9ad873.86%\u3002", "conclusion": "ZePAD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u65e0\u5173\u5bf9\u6297\u6027\u793a\u4f8b\u95ee\u9898\u7684\u65b9\u6848\uff0c\u5b83\u80fd\u591f\u5728\u4e0d\u727a\u7272\u826f\u6027\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u589e\u5f3a\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u7279\u5f81\u591a\u6837\u6027\uff0cZePAD\u5141\u8bb8\u5355\u4e00\u8c03\u6574\u5373\u670d\u52a1\u4e8e\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.11671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11671", "abs": "https://arxiv.org/abs/2602.11671", "authors": ["Minh Le-Anh", "Huyen Nguyen", "Khanh An Tran", "Nam Le Hai", "Linh Ngo Van", "Nghi D. Q. Bui", "Bach Le"], "title": "Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond", "comment": "Accepted to FSE 2026", "summary": "Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.", "AI": {"tldr": "Hydra, a novel repository-level code generation framework, improves upon existing Retrieval-Augmented Generation (RAG) methods by incorporating a structure-aware indexing strategy and a dependency-aware retriever, leading to state-of-the-art performance on challenging benchmarks.", "motivation": "The motivation behind this paper is to enhance the effectiveness of large language models for code (CodeLLMs) in repository-level settings, where they face challenges due to cross-file dependencies and structural context. The authors aim to address the limitations of current RAG approaches that often fail to maintain the coherence between code units and miss out on functionally relevant dependencies.", "method": "The method introduced in this paper is called Hydra, which is a repository-level code generation framework. It consists of three main components: 1) A structure-aware indexing strategy that maps repositories into hierarchical trees, thereby preserving the code's structure and dependencies. 2) A lightweight dependency-aware retriever (DAR) designed to identify and fetch true dependencies needed by a target function. 3) A hybrid retrieval mechanism that integrates DAR with similarity-based retrieval to offer both necessary building blocks and practical usage examples.", "result": "Experiments conducted on the DevEval and RepoExec benchmarks, which require the implementation of functions from real-world repositories with complex contexts, demonstrate that Hydra achieves state-of-the-art performance. Specifically, it surpasses the strongest baseline by more than 5% in Pass@1 accuracy and enables smaller CodeLLMs to perform as well as or better than much larger models using conventional retrievers.", "conclusion": "In conclusion, the paper presents Hydra, an innovative approach to repository-level code generation that significantly outperforms existing methods. By focusing on the structural and dependency aspects of code, Hydra not only improves the accuracy of generated code but also allows for more efficient use of computational resources through the successful application of smaller models."}}
{"id": "2602.11941", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11941", "abs": "https://arxiv.org/abs/2602.11941", "authors": ["Benjamin Clavi\u00e9", "Atoof Shakir", "Jonah Turner", "Sean Lee", "Aamir Shakir", "Makoto P. Kato"], "title": "IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval", "comment": null, "summary": "Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \\textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86IncompeBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u97f3\u4e50\u68c0\u7d22\u6027\u80fd\u7684\u65b0\u57fa\u51c6\u3002\u5b83\u5305\u542b1,574\u4e2a\u9ad8\u8d28\u91cf\u97f3\u4e50\u7247\u6bb5\u3001500\u4e2a\u591a\u6837\u5316\u67e5\u8be2\u548c\u8d85\u8fc7125,000\u4e2a\u5355\u72ec\u7684\u76f8\u5173\u6027\u5224\u65ad\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\u521b\u5efa\u4e86\u8fd9\u4e9b\u6ce8\u91ca\u4ee5\u786e\u4fdd\u9ad8\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u97f3\u4e50\u68c0\u7d22\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u591a\u9636\u6bb5\u7ba1\u9053\u521b\u5efa\u6ce8\u91ca\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aIncompeBench\u7684\u65b0\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5305\u62ec\u5927\u91cf\u8bb8\u53ef\u6388\u6743\u7684\u9ad8\u8d28\u91cf\u97f3\u4e50\u7247\u6bb5\u3001\u591a\u6837\u5316\u7684\u67e5\u8be2\u4ee5\u53ca\u5927\u91cf\u7684\u4e2a\u4f53\u76f8\u5173\u6027\u5224\u65ad\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6IncompeBench\uff0c\u4e13\u95e8\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4ef7\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7684\u8868\u73b0\u3002", "conclusion": "IncompeBench\u4e3a\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u6709\u529b\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2602.12041", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12041", "abs": "https://arxiv.org/abs/2602.12041", "authors": ["Heng Yu", "Xiangjun Zhou", "Jie Xia", "Heng Zhao", "Anxin Wu", "Yu Zhao", "Dongying Kong"], "title": "Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems", "comment": "11 pages, 3 figures", "summary": "Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLCC\u7684\u7279\u5f81\u4ea4\u4e92\u67b6\u6784\uff0c\u901a\u8fc7\u5c42\u6b21\u538b\u7f29\u548c\u52a8\u6001\u7ec4\u5408\u6765\u7ec4\u7ec7\u7279\u5f81\u4ea4\u53c9\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u9ad8\u9636\u7279\u5f81\u4f9d\u8d56\u6027\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86MC-MLCC\uff0c\u4e00\u79cd\u591a\u901a\u9053\u6269\u5c55\u65b9\u6cd5\uff0c\u5c06\u7279\u5f81\u4ea4\u4e92\u5206\u89e3\u4e3a\u5e76\u884c\u5b50\u7a7a\u95f4\uff0c\u4f7f\u5f97\u5728\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u589e\u957f\u3002\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u51fa\u7684\u6a21\u578b\u76f8\u6bd4\u5f3a\u57fa\u7ebfDLRM\u98ce\u683c\u6a21\u578b\uff0c\u5728AUC\u4e0a\u6700\u9ad8\u53ef\u63d0\u53470.52\uff0c\u540c\u65f6\u51cf\u5c11\u9ad8\u8fbe26\u500d\u7684\u6a21\u578b\u53c2\u6570\u4e0eFLOPs\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u4ee3\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\u6784\u5efa\uff0c\u5176\u4e2d\u4ea4\u4e92\u9aa8\u5e72\u5bf9\u4e8e\u9884\u6d4b\u6027\u80fd\u548c\u7cfb\u7edf\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4ea4\u4e92\u6a21\u5757\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5f3a\u5927\u7684\u4ea4\u4e92\u80fd\u529b\u3001\u9ad8\u6548\u7684\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5bfc\u81f4\u5728\u4e25\u683c\u7684\u751f\u4ea7\u9650\u5236\u4e0b\u6269\u5927\u6a21\u578b\u89c4\u6a21\u65f6\u6295\u8d44\u56de\u62a5\u7387\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86MLCC\uff0c\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u7279\u5f81\u4ea4\u4e92\u67b6\u6784\uff0c\u901a\u8fc7\u5c42\u6b21\u538b\u7f29\u548c\u52a8\u6001\u7ec4\u6210\u6765\u7ec4\u7ec7\u7279\u5f81\u4ea4\u53c9\uff1b\u5f15\u5165\u4e86MC-MLCC\uff0c\u4e00\u4e2a\u591a\u91cd\u901a\u9053\u6269\u5c55\u7248\u672c\uff0c\u5b83\u5c06\u7279\u5f81\u4ea4\u4e92\u5206\u89e3\u6210\u5e76\u884c\u5b50\u7a7a\u95f4\uff0c\u5141\u8bb8\u6709\u6548\u5730\u6c34\u5e73\u6269\u5c55\uff0c\u5e76\u6539\u5584\u4e86\u8868\u793a\u5bb9\u91cf\uff0c\u540c\u65f6\u6781\u5927\u5730\u51cf\u5c11\u4e86\u53c2\u6570\u589e\u957f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e00\u4e2a\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u6a21\u578b\u76f8\u5bf9\u4e8e\u5f3a\u5927\u7684DLRM\u98ce\u683c\u57fa\u7ebf\u6a21\u578b\u6700\u591a\u53ef\u4ee5\u63d0\u9ad80.52 AUC\uff0c\u540c\u65f6\u5728\u7c7b\u4f3c\u6027\u80fd\u4e0b\u6700\u591a\u51cf\u5c1126\u500d\u7684\u6a21\u578b\u53c2\u6570\u548cFLOPs\u3002\u5168\u9762\u7684\u6269\u5c55\u5206\u6790\u5c55\u793a\u4e86\u5728\u5d4c\u5165\u7ef4\u5ea6\u3001\u5934\u90e8\u6570\u91cf\u53ca\u901a\u9053\u6570\u65b9\u9762\u7684\u7a33\u5b9a\u4e14\u53ef\u9884\u6d4b\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u57fa\u4e8e\u901a\u9053\u7684\u6269\u5c55\u6bd4\u4f20\u7edf\u7684\u5d4c\u5165\u81a8\u80c0\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u597d\u7684\u6548\u7387\u3002\u6700\u540e\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u5e7f\u544a\u5e73\u53f0\u4e0a\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u7684\u65b0\u6a21\u578b\uff08MLCC\u53ca\u5176\u591a\u901a\u9053\u7248\u672cMC-MLCC\uff09\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u70b9\u51fb\u7387\u548c\u8f6c\u5316\u7387\u9884\u6d4b\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u800c\u4e14\u8fd8\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u5ea6\u964d\u4f4e\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u3002"}}
{"id": "2602.11208", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11208", "abs": "https://arxiv.org/abs/2602.11208", "authors": ["Xin Ju", "Nok Hei", "Fung", "Yuyan Zhang", "Carl Jacquemyn", "Matthew Jackson", "Randolph Settgast", "Sally M. Benson", "Gege Wen"], "title": "Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems", "comment": null, "summary": "The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \\textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b97\u5b50\u2014\u2014\u81ea\u9002\u5e94\u7269\u7406\u53d8\u6362\u5668\uff08APT\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5730\u4e0b\u7cfb\u7edf\u6a21\u62df\u4e2d\u7684\u8ba1\u7b97\u96be\u9898\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u7684\u5c40\u90e8\u5f02\u8d28\u7279\u5f81\u548c\u957f\u8ddd\u79bb\u7269\u7406\u5f71\u54cd\u3002APT\u5728\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7f51\u683c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u9996\u6b21\u76f4\u63a5\u4ece\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u6a21\u62df\u4e2d\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u7684\u80fd\u529b\u3002", "motivation": "\u5730\u7403\u5730\u4e0b\u662f\u73b0\u4ee3\u793e\u4f1a\u7684\u91cd\u8981\u57fa\u77f3\uff0c\u63d0\u4f9b\u8bf8\u5982\u78b3\u6c22\u5316\u5408\u7269\u3001\u5730\u70ed\u548c\u77ff\u7269\u7b49\u91cd\u8981\u80fd\u6e90\u8d44\u6e90\uff0c\u540c\u65f6\u4e5f\u662f\u4e8c\u6c27\u5316\u78b3\u5c01\u5b58\u7684\u4e3b\u8981\u50a8\u5e93\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5730\u8d28\u5f02\u8d28\u6027\u3001\u5bf9\u9ad8\u5206\u8fa8\u7387\u7684\u9700\u6c42\u4ee5\u53ca\u4e0d\u540c\u4f20\u64ad\u65f6\u95f4\u5c3a\u5ea6\u7269\u7406\u8fc7\u7a0b\u7684\u7d27\u5bc6\u8026\u5408\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5168\u7269\u7406\u6570\u503c\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\u3002", "method": "\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u7269\u7406\u53d8\u6362\u5668\uff08APT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0e\u51e0\u4f55\u3001\u7f51\u683c\u548c\u7269\u7406\u65e0\u5173\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u5b83\u901a\u8fc7\u56fe\u7f16\u7801\u5668\u63d0\u53d6\u9ad8\u5206\u8fa8\u7387\u5c40\u90e8\u5f02\u8d28\u7279\u5f81\uff0c\u5e76\u5229\u7528\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u6765\u89e3\u51b3\u957f\u8ddd\u79bb\u7269\u7406\u6548\u5e94\u95ee\u9898\u3002", "result": "APT\u5728\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7f51\u683c\u4e0a\u7684\u5730\u4e0b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u67b6\u6784\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8d85\u5206\u8fa8\u7387\u80fd\u529b\u3002\u6b64\u5916\uff0cAPT\u662f\u9996\u4e2a\u53ef\u4ee5\u76f4\u63a5\u4ece\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u6a21\u62df\u4e2d\u5b66\u4e60\u7684\u67b6\u6784\uff0c\u5e76\u4e14\u5c55\u793a\u51fa\u4e86\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u7684\u80fd\u529b\u3002", "conclusion": "APT\u4e3a\u5927\u89c4\u6a21\u5730\u4e0b\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5730\u4e0b\u7cfb\u7edf\u6a21\u62df\u9762\u4e34\u7684\u8ba1\u7b97\u6311\u6218\u3002"}}
{"id": "2602.11724", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11724", "abs": "https://arxiv.org/abs/2602.11724", "authors": ["Xiwen Teoh", "Yun Lin", "Duc-Minh Nguyen", "Ruofei Ren", "Wenjie Zhang", "Jin Song Dong"], "title": "WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements", "comment": null, "summary": "Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.\n  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.", "AI": {"tldr": "This paper introduces WebTestPilot, an LLM-based agent for end-to-end web testing that addresses challenges of implicit and probabilistic inference by symbolizing GUI elements and translating natural language specifications into steps with inferred conditions. It achieves a 99% task completion rate, 96% precision, and 96% recall in bug detection, outperforming the best baseline.", "motivation": "The motivation is to address the technical challenges faced by visual language model (VLM) agents in end-to-end web testing, specifically the implicit oracle inference challenge and the probabilistic inference challenge, which can lead to uncertainty in distinguishing between hallucinations and real application bugs. The goal is to create a more reliable and accurate testing tool that can handle these issues effectively.", "method": "WebTestPilot uses a symbolization layer to detect and represent critical GUI elements as symbols and translates natural language requirements into a sequence of steps, each equipped with pre- and post-conditions over the symbols. This approach allows for the validation of implicit requirements and the capturing of data, temporal, and causal dependencies. Additionally, a benchmark of bug-injected web applications was created to evaluate the performance of the NL-to-E2E testing approach.", "result": "WebTestPilot achieved a 99% task completion rate, 96% precision, and 96% recall in bug detection, significantly outperforming the best baseline with +70% in precision and +27% in recall. The system demonstrated generalizability across various natural language inputs and model scales.", "conclusion": "WebTestPilot demonstrates significant improvements in the reliability and accuracy of end-to-end web testing using natural language requirements, by overcoming the challenges associated with implicit and probabilistic inferences. The results suggest that it is a robust solution for automating web testing while maintaining high standards of precision and recall."}}
{"id": "2602.11212", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11212", "abs": "https://arxiv.org/abs/2602.11212", "authors": ["Yunchong Song", "Jushi Kai", "Liming Lu", "Kaixi Qiu", "Zhouhan Lin"], "title": "Towards Compressive and Scalable Recurrent Memory", "comment": null, "summary": "Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent approaches introduce recurrent memory to extend context beyond the current window, yet these often face a fundamental trade-off between theoretical principles and practical scalability. To address this, we introduce Elastic Memory, a novel memory architecture grounded in the HiPPO framework for online function approximation. Elastic Memory treats historical sequence as samples from continuous signals, applying optimal online compression to encode them into a fixed-size memory state. For retrieval, we propose a flexible \\textit{polynomial sampling} mechanism that reconstructs a history summary from this compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at 4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bb0\u5fc6\u67b6\u6784\u2014\u2014\u5f39\u6027\u8bb0\u5fc6\uff08Elastic Memory\uff09\uff0c\u57fa\u4e8eHiPPO\u6846\u67b6\u8fdb\u884c\u5728\u7ebf\u51fd\u6570\u903c\u8fd1\uff0c\u65e8\u5728\u89e3\u51b3Transformer\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u7684\u4e8c\u6b21\u74f6\u9888\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u5386\u53f2\u5e8f\u5217\u89c6\u4e3a\u8fde\u7eed\u4fe1\u53f7\u7684\u6837\u672c\uff0c\u5e76\u91c7\u7528\u6700\u4f18\u5728\u7ebf\u538b\u7f29\u65b9\u6cd5\u5c06\u5176\u7f16\u7801\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u8bb0\u5fc6\u72b6\u6001\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u7406\u8bba\u539f\u5219\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u96c6\u4e0a\uff0cElastic Memory\u4e0d\u4ec5\u5728\u5185\u5b58\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u800c\u4e14\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5c1d\u8bd5\u901a\u8fc7\u5f15\u5165\u5faa\u73af\u8bb0\u5fc6\u6765\u6269\u5c55\u5f53\u524d\u7a97\u53e3\u4ee5\u5916\u7684\u4e0a\u4e0b\u6587\u65f6\uff0c\u901a\u5e38\u9762\u4e34\u7406\u8bba\u539f\u5219\u4e0e\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eHiPPO\u6846\u67b6\u7684\u65b0\u8bb0\u5fc6\u67b6\u6784\u2014\u2014\u5f39\u6027\u8bb0\u5fc6\uff08Elastic Memory\uff09\u3002", "method": "Elastic Memory\u89c6\u5386\u53f2\u5e8f\u5217\u4e3a\u6765\u81ea\u8fde\u7eed\u4fe1\u53f7\u7684\u6837\u672c\uff0c\u5229\u7528\u6700\u4f18\u5728\u7ebf\u538b\u7f29\u6280\u672f\u5c06\u5176\u7f16\u7801\u8fdb\u56fa\u5b9a\u5927\u5c0f\u7684\u8bb0\u5fc6\u72b6\u6001\u4e2d\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u591a\u9879\u5f0f\u91c7\u6837\u673a\u5236\uff0c\u7528\u4e8e\u4ece\u538b\u7f29\u72b6\u6001\u91cd\u5efa\u5386\u53f2\u6458\u8981\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684\u957f\u4e0a\u4e0b\u6587\uff0832k+\uff09\u6570\u636e\u96c6\u4e0a\uff0cElastic Memory\u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u3002\u5f53\u53c2\u6570\u6570\u91cf\u76f8\u540c\u65f6\uff0c\u5176\u5185\u5b58\u4f7f\u7528\u91cf\u4ec5\u4e3aMemorizing Transformer\u76841/16\uff0c\u5e76\u4e14\u5373\u4f7fMelodi\u62e5\u6709\u989d\u591630%\u7684\u53c2\u6570\u91cf\uff0cElastic Memory\u4f9d\u7136\u8868\u73b0\u66f4\u4f73\u3002\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\uff0cElastic Memory\u4e0d\u4ec5\u4fdd\u6301\u4e86\u76f8\u5bf9\u4e8e\u6240\u6709\u57fa\u7ebf\u7684\u4f18\u52bf\uff0c\u800c\u4e14\u901f\u5ea6\u8fdc\u8d85\u56db\u500d\u89c4\u6a21\u4e0b\u7684Melodi\u3002", "conclusion": "Elastic Memory\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86Transformer\u5904\u7406\u957f\u6587\u672c\u65f6\u9047\u5230\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u826f\u597d\u7684\u7406\u8bba\u57fa\u7840\u548c\u51fa\u8272\u7684\u5b9e\u8df5\u6027\u80fd\u3002"}}
{"id": "2602.11746", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11746", "abs": "https://arxiv.org/abs/2602.11746", "authors": ["Nafiz Imtiaz Khan", "Vladimir Filkov"], "title": "Leveraging Language Models to Discover Evidence-Based Actions for OSS Sustainability", "comment": null, "summary": "When successful, Open Source Software (OSS) projects create enormous value, but most never reach a sustainable state. Recent work has produced accurate models that forecast OSS sustainability, yet these models rarely tell maintainers what to do: their features are often high-level socio-technical signals that are not directly actionable. Decades of empirical software engineering research have accumulated a large but underused body of evidence on concrete practices that improve project health.\n  We close this gap by using LLMs as evidence miners over the SE literature. We design a RAG-pipeline and a two-layer prompting strategy that extract researched actionables (ReACTs): concise, evidence-linked recommendations mapping to specific OSS practices. In the first layer, we systematically explore open LLMs and prompting techniques, selecting the best-performing combination to derive candidate ReACTs from 829 ICSE and FSE papers. In the second layer, we apply follow-up prompting to filter hallucinations, extract impact and evidence, and assess soundness and precision.\n  Our pipeline yields 1,922 ReACTs, of which 1,312 pass strict quality criteria and are organized into practice-oriented categories connectable to project signals from tools like APEX. The result is a reproducible, scalable approach turning scattered research findings into structured, evidence-based actions guiding OSS projects toward sustainability.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b(SE)\u6587\u732e\u4e2d\u7684\u8bc1\u636e\u6316\u6398\u5de5\u5177\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cdRAG\u7ba1\u9053\u548c\u53cc\u5c42\u63d0\u793a\u7b56\u7565\uff0c\u4ee5\u4ece829\u7bc7ICSE\u548cFSE\u8bba\u6587\u4e2d\u63d0\u53d6\u7814\u7a76\u884c\u52a8\u9879(ReACTs)\uff0c\u5373\u7b80\u6d01\u4e14\u4e0e\u5177\u4f53\u5f00\u6e90\u8f6f\u4ef6(OSS)\u5b9e\u8df5\u76f8\u5173\u7684\u5efa\u8bae\u3002\u6700\u7ec8\u83b7\u5f97\u4e861,312\u6761\u7b26\u5408\u4e25\u683c\u8d28\u91cf\u6807\u51c6\u7684ReACTs\uff0c\u5e76\u88ab\u7ec4\u7ec7\u6210\u53ef\u4ee5\u4e0e\u9879\u76ee\u4fe1\u53f7\u5de5\u5177\uff08\u5982APEX\uff09\u76f8\u8fde\u63a5\u7684\u9762\u5411\u5b9e\u8df5\u7c7b\u522b\uff0c\u4e3aOSS\u9879\u76ee\u671d\u5411\u53ef\u6301\u7eed\u6027\u53d1\u5c55\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u5de5\u4f5c\u5df2\u7ecf\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5f00\u6e90\u8f6f\u4ef6(OSS)\u9879\u76ee\u7684\u53ef\u6301\u7eed\u6027\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u7f3a\u4e4f\u76f4\u63a5\u53ef\u64cd\u4f5c\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u7279\u70b9\u901a\u5e38\u662f\u9ad8\u5c42\u6b21\u7684\u793e\u4f1a\u6280\u672f\u4fe1\u53f7\u3002\u540c\u65f6\uff0c\u6570\u5341\u5e74\u7684\u7ecf\u9a8c\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u79ef\u7d2f\u4e86\u5927\u91cf\u5173\u4e8e\u6539\u5584\u9879\u76ee\u5065\u5eb7\u7684\u5b9e\u9645\u505a\u6cd5\u7684\u8bc1\u636e\uff0c\u4f46\u8fd9\u4e9b\u8bc1\u636e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2aRAG\u7ba1\u9053\u548c\u4e24\u5c42\u63d0\u793a\u7b56\u7565\u6765\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u6587\u732e\u4e2d\u63d0\u53d6\u7814\u7a76\u884c\u52a8\u9879(ReACTs)\u3002\u7b2c\u4e00\u5c42\u6d89\u53ca\u7cfb\u7edf\u5730\u63a2\u7d22\u5f00\u653e\u5f0fLLM\u53ca\u63d0\u793a\u6280\u672f\uff0c\u9009\u62e9\u6700\u4f73\u7ec4\u5408\u6765\u4ece\u8bba\u6587\u4e2d\u5f97\u51fa\u5019\u9009ReACTs\uff1b\u7b2c\u4e8c\u5c42\u5219\u901a\u8fc7\u540e\u7eed\u63d0\u793a\u8fc7\u6ee4\u9519\u8bef\u4fe1\u606f\u3001\u63d0\u53d6\u5f71\u54cd\u548c\u8bc1\u636e\uff0c\u5e76\u8bc4\u4f30\u5408\u7406\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u603b\u51711,922\u4e2aReACTs\uff0c\u5176\u4e2d1,312\u4e2a\u6ee1\u8db3\u4e25\u683c\u7684\u54c1\u8d28\u8981\u6c42\uff0c\u5e76\u88ab\u5f52\u7c7b\u6574\u7406\u6210\u4e0e\u7279\u5b9aOSS\u5b9e\u8df5\u76f8\u5173\u8054\u7684\u9762\u5411\u5b9e\u8df5\u7c7b\u522b\u3002\u8fd9\u4e9b\u7c7b\u522b\u80fd\u591f\u4e0e\u50cfAPEX\u8fd9\u6837\u7684\u5de5\u5177\u6240\u63d0\u4f9b\u7684\u9879\u76ee\u4fe1\u53f7\u76f8\u8fde\u63a5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5c06\u5206\u6563\u7684\u7814\u7a76\u6210\u679c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u884c\u52a8\u6307\u5357\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\uff0c\u65e8\u5728\u5e2e\u52a9\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u5b9e\u73b0\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2602.12187", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12187", "abs": "https://arxiv.org/abs/2602.12187", "authors": ["Sunghwan Kim", "Wooseok Jeong", "Serin Kim", "Sangam Lee", "Dongha Lee"], "title": "SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization", "comment": "Work in Progress", "summary": "Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.", "AI": {"tldr": "A new environment, SAGEO Arena, is introduced for the evaluation and optimization of Search-Augmented Generative Engine Optimization (SAGEO), addressing the limitations of current benchmarks by incorporating a full generative search pipeline over a large-scale, structurally rich web document corpus. The findings highlight the impracticality of existing approaches under realistic conditions and the importance of structural information in improving performance.", "motivation": "The motivation stems from the lack of a comprehensive evaluation environment for Search-Augmented Generative Engine Optimization (SAGEO) that can provide end-to-end visibility into optimization strategies, as well as the absence of consideration for the structural information of web documents in existing benchmarks, which are critical for the practical performance of SAGE systems.", "method": "The method involves the creation of SAGEO Arena, an environment that integrates a complete generative search pipeline including retrieval, reranking, and generation stages, using a large-scale corpus of web documents with rich structural details. This setup allows for the joint targeting of SEO and GEO, enabling a more realistic assessment of SAGEO techniques.", "result": "Results show that without proper adaptation, current SAGEO methods do not perform well in a full pipeline context and often have negative impacts on retrieval and reranking. The inclusion of structural information, such as schema markup, is found to be beneficial in enhancing the effectiveness of SAGEO, suggesting the need for stage-specific optimizations.", "conclusion": "SAGEO Arena provides a much-needed platform for evaluating and optimizing SAGEO in a setting that closely mirrors real-world conditions, revealing the necessity of considering both the generative and retrieval aspects of SAGE systems, and the value of utilizing structural elements within web documents."}}
{"id": "2602.11215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11215", "abs": "https://arxiv.org/abs/2602.11215", "authors": ["Lintao Wang", "Zhuqiang Lu", "Yilin Zhu", "Kun Hu", "Zhenfei Yin", "Shixiang Tang", "Zhiyong Wang", "Wanli Ouyang", "Xinzhu Ma"], "title": "Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning", "comment": null, "summary": "While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domain knowledge synergy. In this work, we present the first systematic study of multi-disciplinary LLM fine-tuning, constructing a five-discipline corpus and analyzing learning patterns of full fine-tuning, LoRA, LoRA-MoE, and LoRA compositions. Particularly, our study shows that multi-disciplinary learning is substantially more variable than single-discipline training and distills four consistent empirical laws: (1) Balance-then-Diversity: low-resource disciplines degrade performance unless mitigated via diversity-aware upsampling; (2) Merge-then-Align: restoring instruction-following ability is critical for cross-discipline synergy; (3) Optimize-then-Scale: parameter scaling offers limited gains without prior design optimization; and (4) Share-then-Specialize: asymmetric LoRA-MoE yields robust gains with minimal trainable parameters via shared low-rank projection. Together, these laws form a practical recipe for principled multi-discipline fine-tuning and provide actionable guidance for developing generalizable scientific LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86\u8de8\u5b66\u79d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5fae\u8c03\uff0c\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u6db5\u76d6\u4e94\u4e2a\u5b66\u79d1\u7684\u6570\u636e\u96c6\u6765\u5206\u6790\u5168\u5fae\u8c03\u3001LoRA\u3001LoRA-MoE\u4ee5\u53caLoRA\u7ec4\u5408\u7684\u5b66\u4e60\u6a21\u5f0f\u3002\u7814\u7a76\u63ed\u793a\u4e86\u591a\u5b66\u79d1\u5b66\u4e60\u76f8\u6bd4\u5355\u4e00\u5b66\u79d1\u8bad\u7ec3\u66f4\u52a0\u591a\u53d8\uff0c\u5e76\u63d0\u70bc\u51fa\u56db\u4e2a\u4e00\u81f4\u7684\u7ecf\u9a8c\u6cd5\u5219\uff1a\u5e73\u8861\u540e\u6c42\u591a\u6837\u3001\u5408\u5e76\u540e\u5bf9\u9f50\u3001\u4f18\u5316\u540e\u6269\u5c55\u3001\u5171\u4eab\u540e\u7279\u5316\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u79d1\u5b66LLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1\u8de8\u9886\u57df\u77e5\u8bc6\u534f\u540c\u6709\u671b\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u7528\u8303\u56f4\uff0c\u4f46\u5bf9\u4e8e\u8fd9\u7c7b\u6a21\u578b\u5728\u591a\u5b66\u79d1\u80cc\u666f\u4e0b\u7684\u5b66\u4e60\u52a8\u6001\u4e86\u89e3\u6709\u9650\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u52a8\u673a\u5728\u4e8e\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5982\u4f55\u6709\u6548\u5229\u7528\u8de8\u5b66\u79d1\u6570\u636e\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u4e0d\u540c\u5b66\u79d1\u9886\u57df\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u57fa\u4e8e\u6b64\u5bf9\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u5168\u91cf\u5fae\u8c03\u3001LoRA\u53ca\u5176\u53d8\u4f53\uff09\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u8de8\u5b66\u79d1\u4efb\u52a1\u65f6\u8868\u73b0\u7684\u89c2\u5bdf\u4e0e\u5206\u6790\uff0c\u8bd5\u56fe\u627e\u51fa\u6700\u6709\u6548\u7684\u591a\u5b66\u79d1\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u5b66\u79d1\u5b66\u4e60\u8fc7\u7a0b\u6bd4\u5355\u5b66\u79d1\u66f4\u590d\u6742\u4e14\u53d8\u5316\u66f4\u5927\uff1b\u540c\u65f6\u603b\u7ed3\u51fa\u4e86\u56db\u6761\u7ecf\u9a8c\u6cd5\u5219\uff0c\u5373\u5148\u786e\u4fdd\u5747\u8861\u518d\u589e\u52a0\u591a\u6837\u6027\u3001\u5148\u6574\u5408\u4fe1\u606f\u518d\u8c03\u6574\u4e00\u81f4\u6027\u3001\u5148\u4f18\u5316\u8bbe\u8ba1\u518d\u8003\u8651\u89c4\u6a21\u6269\u5927\u3001\u5148\u4fc3\u8fdb\u8d44\u6e90\u5171\u4eab\u518d\u9f13\u52b1\u4e13\u4e1a\u5316\u53d1\u5c55\u3002\u8fd9\u4e9b\u89c4\u5f8b\u4e3a\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u5b66\u79d1\u9002\u5e94\u6027\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86\u591a\u5b66\u79d1\u80cc\u666f\u4e0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u72ec\u7279\u6311\u6218\uff0c\u8fd8\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6307\u5bfc\u539f\u5219\uff0c\u65e8\u5728\u63a8\u52a8\u80fd\u591f\u8de8\u8d8a\u591a\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u901a\u7528\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.12278", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12278", "abs": "https://arxiv.org/abs/2602.12278", "authors": ["David Jiahao Fu", "Lam Thanh Do", "Jiayu Li", "Kevin Chen-Chuan Chang"], "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers", "comment": null, "summary": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u957f\u6587\u6863\u68c0\u7d22\u6a21\u578bAttentionRetriever\uff0c\u8be5\u6a21\u578b\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8e\u5b9e\u4f53\u7684\u68c0\u7d22\u6765\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5d4c\u5165\uff0c\u5e76\u786e\u5b9a\u68c0\u7d22\u8303\u56f4\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAttentionRetriever\u5728\u957f\u6587\u6863\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u68c0\u7d22\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u76f8\u540c\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u867d\u7136\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6d89\u53ca\u957f\u6587\u6863\u7684\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e9b\u68c0\u7d22\u6a21\u578b\u5e76\u672a\u4e13\u95e8\u9488\u5bf9\u957f\u6587\u6863\u68c0\u7d22\u8bbe\u8ba1\uff0c\u672a\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u8bf8\u5982\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u56e0\u679c\u4f9d\u8d56\u6027\u4ee5\u53ca\u68c0\u7d22\u8303\u56f4\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86AttentionRetriever\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u957f\u6587\u6863\u68c0\u7d22\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8e\u5b9e\u4f53\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u4ee5\u521b\u5efa\u80fd\u591f\u7406\u89e3\u4e0a\u4e0b\u6587\u7684\u6587\u6863\u5d4c\u5165\uff0c\u5e76\u660e\u786e\u68c0\u7d22\u65f6\u9700\u8981\u8986\u76d6\u7684\u8303\u56f4\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cAttentionRetriever\u4e0d\u4ec5\u5728\u591a\u4e2a\u957f\u6587\u6863\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u5f53\u524d\u5176\u4ed6\u68c0\u7d22\u6a21\u578b\uff0c\u800c\u4e14\u5176\u8fd0\u884c\u6548\u7387\u4e5f\u8fbe\u5230\u4e86\u4e0e\u5bc6\u96c6\u578b\u68c0\u7d22\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528AttentionRetriever\u53ef\u4ee5\u6709\u6548\u6539\u5584\u957f\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u540c\u65f6\u7ef4\u6301\u9ad8\u6548\u8fd0\u4f5c\uff0c\u4e3a\u5904\u7406\u957f\u6587\u6863\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9009\u62e9\u3002"}}
{"id": "2602.11216", "categories": ["cs.LG", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2602.11216", "abs": "https://arxiv.org/abs/2602.11216", "authors": ["Panagiotis Antoniadis", "Beatrice Pavesi", "Simon Olsson", "Ole Winther"], "title": "Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators", "comment": "24 pages, 12 figures and 7 tables", "summary": "Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u4fe1\u606f\u6e90\uff0c\u5982\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\uff0c\u63d0\u9ad8\u4e86\u53ef\u8f6c\u79fb\u9690\u5f0f\u8f6c\u79fb\u7b97\u5b50\uff08TITO\uff09\u5728\u5206\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u63d0\u51fa\u7684PLaTITO\u65b9\u6cd5\u5728\u975e\u5206\u5e03\u86cb\u767d\u8d28\u7cfb\u7edf\u7684\u5e73\u8861\u91c7\u6837\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u4e86\u7ed3\u6784\u5d4c\u5165\u3001\u6e29\u5ea6\u7b49\u989d\u5916\u6761\u4ef6\u4fe1\u53f7\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5206\u5b50\u52a8\u529b\u5b66\u7531\u4e8e\u751f\u6210\u72ec\u7acb\u6837\u672c\u6240\u9700\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u800c\u53d7\u5230\u9650\u5236\u3002\u867d\u7136\u751f\u6210\u5f0f\u5206\u5b50\u52a8\u529b\u5b66\uff08GenMD\uff09\u80fd\u591f\u6709\u6548\u91c7\u6837\uff0c\u4f46\u5176\u8de8\u5206\u5b50\u7cfb\u7edf\u7684\u8fc1\u79fb\u6027\u901a\u5e38\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u8f85\u52a9\u4fe1\u606f\u6e90\u6765\u63d0\u9ad8\u7528\u4e8e\u5206\u5b50\u52a8\u529b\u5b66\u7684\u53ef\u8f6c\u79fb\u9690\u5f0f\u8f6c\u79fb\u7b97\u5b50\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aPLaTITO\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u7c97\u7c92\u5ea6TITO\u6a21\u578b\u4ee5\u53ca\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08pLM\uff09\u5d4c\u5165\u6765\u6539\u8fdb\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u8bf8\u5982\u7ed3\u6784\u5d4c\u5165\u3001\u6e29\u5ea6\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u884d\u751f\u7684\u5d4c\u5165\u7b49\u9644\u52a0\u6761\u4ef6\u4fe1\u53f7\u5bf9\u4e8e\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7c97\u7c92\u5ea6TITO\u6a21\u578b\u76f8\u6bd4Boltzmann Emulators\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff1b\u540c\u65f6\u52a0\u5165pLM\u5d4c\u5165\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5916\u90e8\u5206\u5e03\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002PLaTITO\u5728\u5305\u62ec\u5feb\u901f\u6298\u53e0\u86cb\u767d\u5728\u5185\u7684\u975e\u5206\u5e03\u86cb\u767d\u8d28\u7cfb\u7edf\u4e0a\u7684\u5747\u8861\u91c7\u6837\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6210\u7ee9\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8f85\u52a9\u4fe1\u606f\u8d44\u6e90\uff0c\u7279\u522b\u662f\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u53ef\u8f6c\u79fb\u9690\u5f0f\u8f6c\u79fb\u7b97\u5b50\u5728\u5206\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u6570\u636e\u6548\u7387\u4e0e\u6cdb\u5316\u6027\u80fd\u3002\u8fd9\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u5206\u5b50\u6a21\u62df\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.11887", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11887", "abs": "https://arxiv.org/abs/2602.11887", "authors": ["Javier Ron", "Martin Monperrus"], "title": "Verifiable Provenance of Software Artifacts with Zero-Knowledge Compilation", "comment": null, "summary": "Verifying that a compiled binary originates from its claimed source code is a fundamental security requirement, called source code provenance. Achieving verifiable source code provenance in practice remains challenging. The most popular technique, called reproducible builds, requires difficult matching and reexecution of build toolchains and environments. We propose a novel approach to verifiable provenance based on compiling software with zero-knowledge virtual machines (zkVMs). By executing a compiler within a zkVM, our system produces both the compiled output and a cryptographic proof attesting that the compilation was performed on the claimed source code with the claimed compiler. We implement a proof-of-concept implementation using the RISC Zero zkVM and the ChibiCC C compiler, and evaluate it on 200 synthetic programs as well as 31 OpenSSL and 21 libsodium source files. Our results show that zk-compilation is applicable to real-world software and provides strong security guarantees: all adversarial tests targeting compiler substitution, source tampering, output manipulation, and replay attacks are successfully blocked.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u77e5\u8bc6\u865a\u62df\u673a(zkVM)\u7684\u53ef\u9a8c\u8bc1\u6e90\u4ee3\u7801\u6765\u6e90\u7684\u65b0\u65b9\u6cd5\u3002\u901a\u8fc7\u5728zkVM\u4e2d\u6267\u884c\u7f16\u8bd1\u5668\uff0c\u7cfb\u7edf\u751f\u6210\u7f16\u8bd1\u8f93\u51fa\u548c\u4e00\u4e2a\u52a0\u5bc6\u8bc1\u660e\uff0c\u786e\u4fdd\u7f16\u8bd1\u8fc7\u7a0b\u4f7f\u7528\u4e86\u6307\u5b9a\u7684\u6e90\u4ee3\u7801\u548c\u7f16\u8bd1\u5668\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9632\u6b62\u591a\u79cd\u653b\u51fb\u65b9\u5f0f\uff0c\u5e76\u9002\u7528\u4e8e\u5b9e\u9645\u8f6f\u4ef6\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u9a8c\u8bc1\u6e90\u4ee3\u7801\u6765\u6e90\u6280\u672f\u5982\u53ef\u91cd\u590d\u6784\u5efa\u9700\u8981\u590d\u6742\u7684\u5de5\u5177\u94fe\u5339\u914d\u4e0e\u73af\u5883\u91cd\u73b0\uff0c\u5b9e\u65bd\u8d77\u6765\u9887\u5177\u6311\u6218\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u96be\u9898\u5e76\u63d0\u4f9b\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u5229\u7528\u96f6\u77e5\u8bc6\u865a\u62df\u673a\u8fdb\u884c\u7f16\u8bd1\u7684\u65b0\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06\u7f16\u8bd1\u8fc7\u7a0b\u7f6e\u4e8e\u96f6\u77e5\u8bc6\u865a\u62df\u673a\u5185\u90e8\u6267\u884c\uff0c\u4ece\u800c\u540c\u65f6\u4ea7\u751f\u7f16\u8bd1\u540e\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ca\u4e00\u4e2a\u52a0\u5bc6\u8bc1\u660e\u6765\u8bc1\u5b9e\u7f16\u8bd1\u786e\u5b9e\u4f9d\u636e\u7ed9\u5b9a\u7684\u6e90\u4ee3\u7801\u548c\u7f16\u8bd1\u5668\u5b8c\u6210\u3002\u7814\u7a76\u56e2\u961f\u91c7\u7528RISC Zero zkVM\u4e0eChibiCC C\u7f16\u8bd1\u5668\u8fdb\u884c\u4e86\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u3002", "result": "\u5bf9200\u4e2a\u5408\u6210\u7a0b\u5e8f\u4ee5\u53ca\u6765\u81eaOpenSSL\u548clibsodium\u9879\u76ee\u7684\u517152\u4e2a\u6e90\u6587\u4ef6\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6240\u6709\u9488\u5bf9\u7f16\u8bd1\u5668\u66ff\u6362\u3001\u6e90\u7801\u7be1\u6539\u3001\u8f93\u51fa\u66f4\u6539\u548c\u91cd\u653e\u653b\u51fb\u7684\u6076\u610f\u6d4b\u8bd5\u5747\u88ab\u6210\u529f\u963b\u6b62\u3002", "conclusion": "\u57fa\u4e8ezkVM\u7684\u7f16\u8bd1\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e14\u5728\u62b5\u5fa1\u5404\u79cd\u5b89\u5168\u5a01\u80c1\u65b9\u9762\u5c55\u73b0\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.11217", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11217", "abs": "https://arxiv.org/abs/2602.11217", "authors": ["Simin Fan", "Dimitris Paparas", "Natasha Noy", "Binbin Xiong", "Noveen Sachdeva", "Berivan Isik"], "title": "The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning", "comment": null, "summary": "Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5982\u4f55\u4ece\u9884\u8bad\u7ec3\u8f6c\u79fb\u5230\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u76f8\u5173\u6027\u534f\u8bae\u5e94\u7528\u4e8e\u4e0d\u540c\u6570\u636e\u6df7\u5408\u548c\u6a21\u578b\u89c4\u6a21\u7684\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6307\u6807\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u8de8\u80fd\u529b\u7c7b\u522b\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u89c4\u6a21\u7684\u8f6c\u79fb\u53ef\u9760\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u57fa\u51c6\u9009\u62e9\u3001\u6570\u636e\u7ba1\u7406\u548c\u9ad8\u6548\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u5982\u4f55\u4ece\u9884\u8bad\u7ec3\u9636\u6bb5\u8f6c\u79fb\u5230\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u5bf9\u4e8e\u6709\u6548\u7684\u6a21\u578b\u5f00\u53d1\u548c\u6570\u636e\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u4e00\u7cfb\u5217\u76f8\u5173\u6027\u534f\u8bae\u5e94\u7528\u5230\u8de8\u8d8a\u591a\u79cd\u6570\u636e\u6df7\u5408\u4e0e\u6a21\u578b\u89c4\u6a21\u7684\u51c6\u786e\u6027\u53ca\u7f6e\u4fe1\u5ea6\u8861\u91cf\u6807\u51c6\u4e0a\uff0c\u6765\u89e3\u51b3\u56db\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u9884\u8bad\u7ec3\u671f\u95f4\u5efa\u7acb\u7684\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6392\u540d\u5728SFT\u540e\u4fdd\u6301\u7684\u7a0b\u5ea6\uff1b\u54ea\u4e9b\u57fa\u51c6\u4f5c\u4e3a\u7a33\u5065\u7684\u8de8\u9636\u6bb5\u9884\u6d4b\u5668\u662f\u53ef\u9760\u7684\u6216\u4e0d\u53ef\u9760\u7684\uff1b\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u53d8\u5316\uff0c\u8f6c\u79fb\u52a8\u6001\u5982\u4f55\u53d8\u5316\uff1b\u4ee5\u53ca\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u4f5c\u4e3a\u6821\u51c6\u8d28\u91cf\u7684\u4e00\u79cd\u5ea6\u91cf\uff0c\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u95f4\u662f\u5426\u4e5f\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8de8\u80fd\u529b\u9886\u57df\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u89c4\u6a21\u7684\u8fc1\u79fb\u53ef\u9760\u6027\u5dee\u5f02\u5de8\u5927\u2014\u2014\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u5c55\u73b0\u51fa\u72ec\u7279\u4e14\u6709\u65f6\u76f8\u53cd\u7684\u6269\u5c55\u52a8\u6001\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u51b3\u7b56\u4e0e\u4e0b\u6e38\u7ed3\u679c\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u4e3a\u57fa\u51c6\u9009\u53d6\u3001\u6570\u636e\u6574\u7406\u53ca\u9ad8\u6548\u7684\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5357\u3002"}}
{"id": "2602.11904", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11904", "abs": "https://arxiv.org/abs/2602.11904", "authors": ["Weixing Zhang", "Bowen Jiang", "Yuhong Fu", "Anne Koziolek", "Regina Hebig", "Daniel Str\u00fcber"], "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation", "comment": null, "summary": "Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Claude Sonnet 4.5\u548cGPT-5.2\uff09\u5728\u6587\u672cDSLs\u7684\u8bed\u6cd5\u4e0e\u5b9e\u4f8b\u5171\u540c\u8fdb\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5c0f\u89c4\u6a21\u6848\u4f8b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u8f83\u5927\u89c4\u6a21\u5b9e\u4f8b\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u8bed\u8a00\u4e3a\u4e86\u6dfb\u52a0\u65b0\u529f\u80fd\u7b49\u539f\u56e0\u800c\u4e0d\u65ad\u6f14\u5316\uff0c\u539f\u672c\u7b26\u5408\u8fd9\u4e9b\u8bed\u6cd5\u7684\u6587\u672c\u5b9e\u4f8b\u53ef\u80fd\u4f1a\u53d8\u5f97\u8fc7\u65f6\u3002\u867d\u7136\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u63d0\u4f9b\u4e86\u8bb8\u591a\u6280\u672f\u6765\u4f7f\u6a21\u578b\u4e0e\u5143\u6a21\u578b\u53d8\u66f4\u540c\u6b65\u6f14\u53d8\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u4e0d\u9002\u7528\u4e8e\u6587\u672cDSL\uff0c\u5e76\u4e14\u53ef\u80fd\u4e22\u5931\u8bf8\u5982\u5e03\u5c40\u548c\u6ce8\u91ca\u7b49\u5bf9\u4eba\u7c7b\u91cd\u8981\u7684\u4fe1\u606f\u3002", "method": "\u4f7f\u7528Claude Sonnet 4.5\u548cGPT-5.2\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5341\u4e2a\u6848\u4f8b\u8bed\u8a00\u4e0a\u8fdb\u884c\u4e86\u5341\u6b21\u8fd0\u884c\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u6b63\u786e\u6027\u548c\u4eba\u7c7b\u76f8\u5173\u4fe1\u606f\u4fdd\u5b58\u60c5\u51b5\u3002", "result": "\u5c0f\u578b\u6848\u4f8b\u4e2d\u8868\u73b0\u5f3a\u52b2\uff08\u5bf9\u4e8e\u9700\u8981\u4fee\u6539\u5c11\u4e8e20\u884c\u7684\u5b9e\u4f8b\uff0c\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u5747\u226594%\uff09\uff0c\u4f46\u968f\u7740\u89c4\u6a21\u589e\u5927\u6027\u80fd\u4e0b\u964d\uff1aClaude\u572840\u884c\u65f6\u4fdd\u630185%\u53ec\u56de\u7387\uff0c\u800cGPT\u65e0\u6cd5\u5904\u7406\u6700\u5927\u5b9e\u4f8b\u3002\u54cd\u5e94\u65f6\u95f4\u968f\u5b9e\u4f8b\u5927\u5c0f\u663e\u8457\u589e\u52a0\uff0c\u4e14\u8bed\u6cd5\u6f14\u5316\u590d\u6742\u5ea6\u53ca\u5220\u9664\u7c92\u5ea6\u6bd4\u53d8\u66f4\u7c7b\u578b\u66f4\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5171\u6f14\u8fdb\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u662f\u6709\u6548\u7684\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5f53\u524d\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.11219", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11219", "abs": "https://arxiv.org/abs/2602.11219", "authors": ["Tanmoy Mukherjee", "Marius Kloft", "Pierre Marquis", "Zied Bouraoui"], "title": "Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty", "comment": null, "summary": "Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically strongly correlated, so changes in predictive spread simultaneously affect both components and blur their semantics. We propose a credal-set formulation in which uncertainty is represented as a set of predictive distributions, so that epistemic and aleatoric uncertainty correspond to distinct geometric properties: the size of the set versus the noise within its elements. We instantiate this idea in a Variational Credal Concept Bottleneck Model with two disjoint uncertainty heads trained by disjoint objectives and non-overlapping gradient paths, yielding separation by construction rather than post hoc decomposition. Across multi-annotator benchmarks, our approach reduces the correlation between epistemic and aleatoric uncertainty by over an order of magnitude compared to standard methods, while improving the alignment of epistemic uncertainty with prediction error and aleatoric uncertainty with ground-truth ambiguity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u5ea6\u96c6\u7684\u65b9\u6cd5\u6765\u533a\u5206\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e2d\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u76ee\u6807\u548c\u4e0d\u91cd\u53e0\u7684\u68af\u5ea6\u8def\u5f84\u8bad\u7ec3\u6a21\u578b\uff0c\u4ece\u800c\u5728\u6784\u9020\u4e0a\u5b9e\u73b0\u5206\u79bb\uff0c\u800c\u4e0d\u662f\u4e8b\u540e\u5206\u89e3\u3002\u8fd9\u79cd\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u4e0d\u786e\u5b9a\u6027\u4e0e\u9884\u6d4b\u8bef\u5dee\u53ca\u6570\u636e\u771f\u5b9e\u6a21\u7cca\u6027\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u65b9\u6cd5\u4ece\u76f8\u540c\u7684\u9884\u6d4b\u5206\u5e03\u4e2d\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u8fd9\u4e9b\u4f30\u8ba1\u503c\u901a\u5e38\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f7f\u5f97\u9884\u6d4b\u8303\u56f4\u7684\u53d8\u5316\u540c\u65f6\u5f71\u54cd\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\uff0c\u6df7\u6dc6\u4e86\u5b83\u4eec\u7684\u542b\u4e49\u3002\u4e3a\u4e86\u63d0\u9ad8\u51b3\u7b56\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u51c6\u786e\u5730\u533a\u5206\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u4fe1\u5ea6\u96c6\u516c\u5f0f\uff0c\u5176\u4e2d\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u4e3a\u4e00\u7ec4\u9884\u6d4b\u5206\u5e03\uff1b\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u5bf9\u5e94\u4e8e\u4e0d\u540c\u7684\u51e0\u4f55\u5c5e\u6027\uff1a\u96c6\u5408\u7684\u5927\u5c0f\u4e0e\u5143\u7d20\u5185\u7684\u566a\u58f0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u53d8\u5206\u4fe1\u5ea6\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u4e92\u4e0d\u76f8\u4ea4\u7684\u4e0d\u786e\u5b9a\u6027\u5934\uff0c\u901a\u8fc7\u4e0d\u540c\u7684\u76ee\u6807\u51fd\u6570\u548c\u975e\u91cd\u53e0\u7684\u68af\u5ea6\u8def\u5f84\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u6807\u6ce8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u964d\u4f4e\u4e86\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u6539\u5584\u4e86\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0e\u9884\u6d4b\u9519\u8bef\u4e4b\u95f4\u4ee5\u53ca\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4e0e\u5b9e\u9645\u6570\u636e\u6a21\u7cca\u6027\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u5206\u79bb\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e2d\u7684\u8ba4\u77e5\u6210\u5206\u548c\u5076\u7136\u6210\u5206\uff0c\u63d0\u4f9b\u66f4\u52a0\u6e05\u6670\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2602.11911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11911", "abs": "https://arxiv.org/abs/2602.11911", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Gabriele Bavota"], "title": "Improving Code Generation via Small Language Model-as-a-judge", "comment": "Accepted to the 48th International Conference on Software Engineering (ICSE 2026)", "summary": "Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bad\u7ec3\u4e86\u591a\u4e2a\u6700\u65b0\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4f5c\u4e3a\u4ee3\u7801\u6b63\u786e\u6027\u8bc4\u5224\u8005\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u5b9e\u73b0\u7684\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u4e0d\u4f7f\u7528\u57fa\u4e8e\u6267\u884c\u7684\u4fe1\u606f\uff0c\u73b0\u4ee3SLMs\u7684\u8868\u73b0\u4e5f\u4f18\u4e8eRankEF\uff1b\u5f53\u7528\u4f5c\u4ee3\u7801\u6392\u5e8f\u5668\u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u6bd4RankEF\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u589e\u76ca\uff0c\u5e76\u4e14\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u4e0e\u59275\u523025\u500d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ade\u4e89\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u4e8e\u4e0d\u592a\u5e38\u89c1\u6216\u9886\u57df\u7279\u5b9a\u7684\u8bed\u8a00\u6765\u8bf4\uff0c\u5176\u8868\u73b0\u53ef\u80fd\u4e0d\u4f73\uff0c\u4fc3\u4f7f\u4f01\u4e1a\u5f00\u53d1\u5185\u90e8\u4ee3\u7801\u751f\u6210\u5668\u3002\u867d\u7136\u5f00\u6e90\u6a21\u578b\u53ef\u4ee5\u4e3a\u6b64\u76ee\u7684\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u53ea\u6709\u5177\u6709\u6570\u5341\u4ebf\u53c2\u6570\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u624d\u80fd\u4e0e\u5546\u4e1a\u5de5\u5177\u76f8\u5339\u654c\uff0c\u8fd9\u9700\u8981\u6602\u8d35\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u6210\u672c\u3002\u6700\u8fd1\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5019\u9009\u89e3\u51b3\u65b9\u6848\u5e76\u5229\u7528\u53e6\u4e00\u4e2a\u5c0f\u578b\u6a21\u578b\u9009\u62e9\u6700\u6709\u53ef\u80fd\u6b63\u786e\u7684\u90a3\u4e2a\u6765\u652f\u6301\u4ee3\u7801\u751f\u6210\u3002Sun\u7b49\u4eba\u63d0\u51fa\u7684RankEF\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u4f8b\u5b50\uff0c\u4f46\u5b83\u6ca1\u6709\u8bc4\u4f30T5\u6392\u540d\u5668\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5373\u5b83\u5c06\u6b63\u786e\u5b9e\u73b0\u8bef\u5224\u4e3a\u4e0d\u6b63\u786e\u6216\u53cd\u4e4b\u7684\u6982\u7387\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5176\u4ed6\u4efb\u52a1\uff08\u5982\u81ea\u52a8\u4ee3\u7801\u5ba1\u67e5\uff09\u4e2d\u7684\u4ee3\u7801\u6b63\u786e\u6027\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u7684\u5b9e\u9a8c\u6d89\u53ca\u7684\u662f\u76f8\u5bf9\u8f83\u65e7\u7684\u6a21\u578b\uff0c\u4f7f\u5f97\u4e0d\u6e05\u695a\u8fd9\u79cd\u65b9\u6cd5\u662f\u5426\u4ecd\u7136\u6709\u52a9\u4e8e\u516c\u53f8\u4ee5\u8f83\u4f4e\u6210\u672c\u8bad\u7ec3\u51fa\u4e0e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u7684\u81ea\u5df1\u7684\u4ee3\u7801\u751f\u6210\u5668\u3002", "method": "\u7814\u7a76\u4eba\u5458\u9996\u5148\u8bad\u7ec3\u4e86\u6570\u4e2a\u6700\u65b0\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7801\u6b63\u786e\u6027\u8bc4\u5224\u8005\u3002\u63a5\u7740\uff0c\u4ed6\u4eec\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u4ee3\u7801\u5b9e\u73b0\u4e0a\u7684\u80fd\u529b\u3002\u6700\u540e\uff0c\u7814\u7a76\u8fd8\u63a2\u7d22\u4e86\u5f53\u8fd9\u4e9b\u6a21\u578b\u88ab\u7528\u4f5c\u4ee3\u7801\u6392\u5e8f\u5668\u65f6\uff0c\u76f8\u6bd4\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\uff08\u5982RankEF\uff09\u4ee5\u53ca\u66f4\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u6027\u80fd\u65b9\u9762\u7684\u63d0\u5347\u60c5\u51b5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u4ee3\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u4e0d\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u6267\u884c\u7684\u4fe1\u606f\u4e5f\u80fd\u8d85\u8d8aRankEF\u7684\u8868\u73b0\u3002\u5f53\u8fd9\u4e9b\u6a21\u578b\u5e94\u7528\u4e8e\u4ee3\u7801\u6392\u5e8f\u65f6\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6bd4RankEF\u66f4\u9ad8\u7684\u6027\u80fd\u589e\u957f\uff0c\u800c\u4e14\u8fd8\u80fd\u4ee5\u8fdc\u4f4e\u4e8e\u6210\u672c\u7684\u65b9\u5f0f\u4e0e\u89c4\u6a21\u5927\u5f97\u591a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u8bad\u7ec3\uff0c\u8f83\u65b0\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u4ee5\u8f83\u4f4e\u6210\u672c\u83b7\u5f97\u63a5\u8fd1\u9876\u7ea7\u6027\u80fd\u7684\u4f01\u4e1a\u800c\u8a00\u3002\u8fd9\u4e3a\u5f00\u53d1\u9ad8\u6548\u7684\u4ee3\u7801\u751f\u6210\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.11220", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11220", "abs": "https://arxiv.org/abs/2602.11220", "authors": ["Jiacheng Wang", "Ping Jian", "Zhen Yang", "Zirong Chen", "Keren Liao", "Zhongbin Guo"], "title": "Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT", "comment": null, "summary": "Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u6570\u636e\u4e0e\u6a21\u578b\u5148\u524d\u8bad\u7ec3\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u65f6\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u53ef\u80fd\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u91cd\u5199\u7b56\u7565\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u4f18\u5316\u4ee5\u5956\u52b1\u53cd\u9988\u4e3a\u5bfc\u5411\u7684\u91cd\u5199\u5206\u5e03\uff0c\u5728\u4fdd\u6301\u591a\u6837\u6027\u548c\u4efb\u52a1\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u66f4\u597d\u5730\u5339\u914d\u6a21\u578b\u7684\u95ee\u7b54\u5f0f\u751f\u6210\u5206\u5e03\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u51cf\u5c11\u975e\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u574712.34%\u7684\u9057\u5fd8\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u6807\u51c6SFT\u76f8\u5f53\u7684\u4e0b\u6e38\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u7528\u4e8e\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u9700\u8981\u7ecf\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002\u5982\u679c\u4e0b\u6e38\u6570\u636e\u4e0e\u6a21\u578b\u9884\u8bad\u7ec3\u9636\u6bb5\u6240\u7528\u7684\u6570\u636e\u5206\u5e03\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff0c\u5219\u53ef\u80fd\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6cd5\u5982\u6570\u636e\u91cd\u5199\u867d\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u6a21\u677f\u4e14\u96be\u4ee5\u4fdd\u8bc1\u4e0e\u6a21\u578b\u81ea\u7136\u751f\u6210\u98ce\u683c\u7684\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u591a\u6837\u6027\u635f\u5931\u3002", "method": "\u672c\u6587\u5c06\u6570\u636e\u91cd\u5199\u89c6\u4e3a\u4e00\u79cd\u7b56\u7565\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u91cd\u5199\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u63d0\u9ad8\u91cd\u5199\u540e\u6570\u636e\u4e0e\u6a21\u578bQA\u5f0f\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684\u5339\u914d\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u591a\u6837\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u81ea\u52a8\u53ef\u8bc4\u4f30\u4f46\u96be\u4ee5\u7aef\u5230\u7aef\u4f18\u5316\u7684\u76ee\u6807\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u8c03\u6574\u91cd\u5199\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u7efc\u5408\u8003\u8651\u4e86\u5206\u5e03\u5bf9\u9f50\u3001\u591a\u6837\u6027\u548c\u4efb\u52a1\u4e00\u81f4\u6027\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u56e0\u5206\u5e03\u504f\u79fb\u5f15\u8d77\u7684\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\uff0c\u5728\u975e\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u9057\u5fd8\u51cf\u5c11\u4e86\u7ea612.34%\uff0c\u800c\u4e14\u5728\u76ee\u6807\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e5f\u4e0e\u4f20\u7edf\u7684SFT\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u91cd\u5199\u6846\u67b6\uff0c\u5b83\u6709\u52a9\u4e8e\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0e\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u5177\u6709\u663e\u8457\u4e0d\u540c\u5206\u5e03\u7684\u65b0\u6570\u636e\u96c6\u65f6\u3002"}}
{"id": "2602.11925", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11925", "abs": "https://arxiv.org/abs/2602.11925", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Gabriele Bavota"], "title": "Studying Quality Improvements Recommended via Manual and Automated Code Review", "comment": "Accepted at the 34th International Conference on Program Comprehension (ICPC 2026)", "summary": "Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u6bd4\u4e86\u4eba\u7c7b\u8bc4\u5ba1\u5458\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff08\u4ee5ChatGPT-4\u4e3a\u4ee3\u8868\uff09\u5728\u5efa\u8bae\u4ee3\u7801\u8d28\u91cf\u6539\u8fdb\u65b9\u9762\u7684\u5f02\u540c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136ChatGPT\u63a8\u8350\u7684\u4ee3\u7801\u53d8\u66f4\u6570\u91cf\u662f\u4eba\u7c7b\u8bc4\u5ba1\u5458\u7684\u5927\u7ea62.4\u500d\uff0c\u4f46\u5b83\u4ec5\u80fd\u8bc6\u522b\u51fa\u4eba\u7c7b\u6307\u51fa\u7684\u8d28\u91cf\u95ee\u9898\u4e2d\u768410%\u3002\u7136\u800c\uff0cChatGPT\u989d\u5916\u63d0\u4f9b\u7684\u8bc4\u8bba\u4e2d\u6709\u7ea640%\u6307\u5411\u4e86\u6709\u610f\u4e49\u7684\u8d28\u91cf\u95ee\u9898\u3002\u8fd9\u8868\u660e\uff0c\u5f53\u524d\u57fa\u4e8eDL\u7684\u4ee3\u7801\u5ba1\u67e5\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u7c7b\u624b\u52a8\u5ba1\u67e5\u4e4b\u5916\u7684\u4e00\u4e2a\u8865\u5145\u8d28\u91cf\u68c0\u67e5\u624b\u6bb5\uff0c\u4f46\u5c1a\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u5ba1\u67e5\u6216\u8282\u7701\u5ba1\u67e5\u65f6\u95f4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u4e2d\u80fd\u5426\u50cf\u4eba\u7c7b\u8bc4\u5ba1\u5458\u4e00\u6837\u6709\u6548\u5730\u63a8\u8350\u4ee3\u7801\u8d28\u91cf\u6539\u8fdb\u63aa\u65bd\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u6316\u6398\u7684\u7814\u7a76\u65b9\u6cd5\u6536\u96c6\u5e76\u624b\u52a8\u68c0\u67e5\u4e86739\u6761\u7531\u4eba\u7c7b\u8bc4\u5ba1\u5458\u63d0\u51fa\u7684\u5173\u4e8e240\u4e2aPR\uff08Pull Request\uff09\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u5efa\u8bae\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u968f\u540e\u4f7f\u7528ChatGPT-4\u5bf9\u76f8\u540c\u7684PR\u8fdb\u884c\u4ee3\u7801\u5ba1\u67e5\uff0c\u5e76\u5c06\u5176\u63a8\u8350\u7684\u8d28\u91cf\u6539\u8fdb\u4e0e\u4eba\u7c7b\u8bc4\u5ba1\u5458\u7684\u5efa\u8bae\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1ChatGPT\u5e73\u5747\u63a8\u8350\u7684\u4ee3\u7801\u53d8\u66f4\u6570\u91cf\u6bd4\u4eba\u7c7b\u591a\u7ea62.4\u500d\uff0c\u4f46\u5b83\u53ea\u80fd\u53d1\u73b0\u4eba\u7c7b\u62a5\u544a\u7684\u8d28\u91cf\u95ee\u9898\u4e2d\u768410%\u3002\u4e0d\u8fc7\uff0cChatGPT\u751f\u6210\u7684\u989d\u5916\u8bc4\u8bba\u4e2d\u5927\u7ea6\u670940%\u786e\u5b9e\u6307\u51fa\u4e86\u6709\u4ef7\u503c\u7684\u8d28\u91cf\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u662f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4ee3\u7801\u5ba1\u67e5\u6280\u672f\u76ee\u524d\u53ef\u4f5c\u4e3a\u4eba\u7c7b\u624b\u52a8\u5ba1\u67e5\u8fc7\u7a0b\u7684\u4e00\u4e2a\u8865\u5145\u8d28\u91cf\u68c0\u67e5\u624b\u6bb5\uff0c\u4f46\u8fd8\u4e0d\u8db3\u4ee5\u4ee3\u66ff\u4eba\u7c7b\u8bc4\u5ba1\u5458\u7684\u89d2\u8272\uff0c\u4e5f\u4e0d\u5e94\u88ab\u89c6\u4e3a\u51cf\u5c11\u4ee3\u7801\u5ba1\u67e5\u65f6\u95f4\u7684\u65b9\u5f0f\u3002"}}
{"id": "2602.11234", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.11234", "abs": "https://arxiv.org/abs/2602.11234", "authors": ["Ankita Paul", "Wenyi Wang"], "title": "Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks", "comment": null, "summary": "Accurate prognosis for Glioblastoma (GBM) using deep learning (DL) is hindered by extreme spatial and structural heterogeneity. Moreover, inconsistent MRI acquisition protocols across institutions hinder generalizability of models. Conventional transformer and DL pipelines often fail to capture the multi-scale morphological diversity such as fragmented necrotic cores, infiltrating margins, and disjoint enhancing components leading to scanner-specific artifacts and poor cross-site prognosis. We propose TopoGBM, a learning framework designed to capture heterogeneity-preserved, scanner-robust representations from multi-parametric 3D MRI. Central to our approach is a 3D convolutional autoencoder regularized by a topological regularization that preserves the complex, non-Euclidean invariants of the tumor's manifold within a compressed latent space. By enforcing these topological priors, TopoGBM explicitly models the high-variance structural signatures characteristic of aggressive GBM. Evaluated across heterogeneous cohorts (UPENN, UCSF, RHUH) and external validation on TCGA, TopoGBM achieves better performance (C-index 0.67 test, 0.58 validation), outperforming baselines that degrade under domain shift. Mechanistic interpretability analysis reveals that reconstruction residuals are highly localized to pathologically heterogeneous zones, with tumor-restricted and healthy tissue error significantly low (Test: 0.03, Validation: 0.09). Furthermore, occlusion-based attribution localizes approximately 50% of the prognostic signal to the tumor and the diverse peritumoral microenvironment advocating clinical reliability of the unsupervised learning method. Our findings demonstrate that incorporating topological priors enables the learning of morphology-faithful embeddings that capture tumor heterogeneity while maintaining cross-institutional robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTopoGBM\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc73D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u5e76\u7ed3\u5408\u62d3\u6251\u6b63\u5219\u5316\u6765\u6355\u6349\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624(GBM)\u7684\u5f02\u8d28\u6027\u548c\u7ed3\u6784\u591a\u6837\u6027\uff0c\u63d0\u9ad8\u4e86\u8de8\u673a\u6784MRI\u6570\u636e\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u961f\u5217\u4e2d\u5c55\u793a\u4e86\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u51c6\u786e\u5730\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5bf9\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\uff08GBM\uff09\u8fdb\u884c\u9884\u540e\u53d7\u5230\u7a7a\u95f4\u548c\u7ed3\u6784\u9ad8\u5ea6\u5f02\u8d28\u6027\u4ee5\u53ca\u4e0d\u540c\u673a\u6784\u95f4MRI\u91c7\u96c6\u534f\u8bae\u4e0d\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002\u4f20\u7edf\u7684\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u5145\u5206\u6355\u6349\u80bf\u7624\u7684\u591a\u5c3a\u5ea6\u5f62\u6001\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u4e86\u9488\u5bf9\u7279\u5b9a\u626b\u63cf\u4eea\u7684\u4f2a\u5f71\u548c\u8f83\u5dee\u7684\u8de8\u7ad9\u70b9\u9884\u540e\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86TopoGBM\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u75283D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u5e76\u901a\u8fc7\u62d3\u6251\u6b63\u5219\u5316\u4fdd\u6301\u80bf\u7624\u590d\u6742\u975e\u6b27\u51e0\u91cc\u5f97\u4e0d\u53d8\u91cf\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u5185\u4fdd\u7559\u4e86\u80bf\u7624\u7684\u7ed3\u6784\u7279\u5f81\u3002\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u8fd9\u4e9b\u62d3\u6251\u5148\u9a8c\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u660e\u786e\u5efa\u6a21\u9ad8\u53d8\u5f02\u7ed3\u6784\u7b7e\u540d\uff0c\u8fd9\u5728\u4fb5\u88ad\u6027GBM\u4e2d\u5c24\u4e3a\u5e38\u89c1\u3002", "result": "\u5728\u4e0d\u540c\u961f\u5217\uff08UPENN, UCSF, RHUH\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e14\u5728TCGA\u4e0a\u7684\u5916\u90e8\u9a8c\u8bc1\u8868\u660e\uff0cTopoGBM\u8fbe\u5230\u4e86\u66f4\u597d\u7684\u6027\u80fd(C-index\u6d4b\u8bd50.67\uff0c\u9a8c\u8bc10.58)\uff0c\u8d85\u8fc7\u4e86\u5728\u9886\u57df\u8fc1\u79fb\u4e0b\u8868\u73b0\u4e0b\u964d\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u673a\u5236\u89e3\u91ca\u6027\u5206\u6790\u663e\u793a\uff0c\u91cd\u5efa\u6b8b\u5dee\u9ad8\u5ea6\u96c6\u4e2d\u5728\u75c5\u7406\u5f02\u8d28\u533a\u57df\uff0c\u800c\u80bf\u7624\u53ca\u5065\u5eb7\u7ec4\u7ec7\u8bef\u5dee\u663e\u8457\u8f83\u4f4e\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u906e\u6321\u7684\u5f52\u56e0\u5b9a\u4f4d\u5927\u7ea650%\u7684\u9884\u540e\u4fe1\u53f7\u5230\u80bf\u7624\u53ca\u5176\u591a\u6837\u5316\u7684\u5468\u56f4\u5fae\u73af\u5883\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u5148\u9a8c\u77e5\u8bc6\uff0c\u53ef\u4ee5\u5b66\u4e60\u5230\u5fe0\u5b9e\u4e8e\u5f62\u6001\u5b66\u7684\u5d4c\u5165\u8868\u793a\uff0c\u65e2\u6355\u6349\u5230\u4e86\u80bf\u7624\u5f02\u8d28\u6027\u4e5f\u4fdd\u6301\u4e86\u8de8\u673a\u6784\u95f4\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.11237", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11237", "abs": "https://arxiv.org/abs/2602.11237", "authors": ["Mujeeb Ur Rehman", "Imran Rehan", "Sohail Khalid"], "title": "AI-Driven Clinical Decision Support System for Enhanced Diabetes Diagnosis and Management", "comment": null, "summary": "Identifying type 2 diabetes mellitus can be challenging, particularly for primary care physicians. Clinical decision support systems incorporating artificial intelligence (AI-CDSS) can assist medical professionals in diagnosing type 2 diabetes with high accuracy. This study aimed to assess an AI-CDSS specifically developed for the diagnosis of type 2 diabetes by employing a hybrid approach that integrates expert-driven insights with machine learning techniques. The AI-CDSS was developed (training dataset: n = 650) and tested (test dataset: n = 648) using a dataset of 1298 patients with and without type 2 diabetes. To generate predictions, the algorithm utilized key features such as body mass index, plasma fasting glucose, and hemoglobin A1C. Furthermore, a clinical pilot study involving 105 patients was conducted to assess the diagnostic accuracy of the system in comparison to non-endocrinology specialists. The AI-CDSS showed a high degree of accuracy, with 99.8% accuracy in predicting diabetes, 99.3% in predicting prediabetes, 99.2% in identifying at-risk individuals, and 98.8% in predicting no diabetes. The test dataset revealed a 98.8% agreement between endocrinology specialists and the AI-CDSS. Type 2 diabetes was identified in 45% of 105 individuals in the pilot study. Compared with diabetes specialists, the AI-CDSS scored a 98.5% concordance rate, greatly exceeding that of nonendocrinology specialists, who had an 85% agreement rate. These findings indicate that the AI-CDSS has the potential to be a useful tool for accurately identifying type 2 diabetes, especially in situations in which diabetes specialists are not readily available.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u5e76\u6d4b\u8bd5\u4e86\u4e00\u79cd\u7ed3\u5408\u4e13\u5bb6\u89c1\u89e3\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u4eba\u5de5\u667a\u80fd\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08AI-CDSS\uff09\uff0c\u7528\u4e8e2\u578b\u7cd6\u5c3f\u75c5\u7684\u8bca\u65ad\u3002\u8be5\u7cfb\u7edf\u5728\u9884\u6d4b\u7cd6\u5c3f\u75c5\u3001\u7cd6\u5c3f\u75c5\u524d\u671f\u4ee5\u53ca\u65e0\u7cd6\u5c3f\u75c5\u72b6\u6001\u65b9\u9762\u5c55\u73b0\u4e86\u6781\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u975e\u5185\u5206\u6ccc\u79d1\u4e13\u5bb6\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u9274\u4e8e2\u578b\u7cd6\u5c3f\u75c5\u8bc6\u522b\u5bf9\u521d\u7ea7\u4fdd\u5065\u533b\u751f\u6765\u8bf4\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u4e13\u95e8\u9488\u5bf92\u578b\u7cd6\u5c3f\u75c5\u8bca\u65ad\u7684AI-CDSS\u6765\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\u5f00\u53d1\u4e86AI-CDSS\uff0c\u5e76\u4f7f\u7528\u5305\u542b1298\u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff08n=650\uff09\u548c\u6d4b\u8bd5\uff08n=648\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u6d89\u53ca105\u540d\u60a3\u8005\u7684\u4e34\u5e8a\u8bd5\u70b9\u7814\u7a76\u4ee5\u6bd4\u8f83AI-CDSS\u4e0e\u975e\u5185\u5206\u6ccc\u79d1\u4e13\u5bb6\u4e4b\u95f4\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "result": "AI-CDSS\u5728\u9884\u6d4b\u7cd6\u5c3f\u75c5\u3001\u7cd6\u5c3f\u75c5\u524d\u671f\u53ca\u65e0\u7cd6\u5c3f\u75c5\u72b6\u51b5\u65f6\u8868\u73b0\u51fa\u975e\u5e38\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5206\u522b\u4e3a99.8%\u300199.3%\u300199.2% \u548c 98.8%\u3002\u4e0e\u5185\u5206\u6ccc\u5b66\u4e13\u5bb6\u76f8\u6bd4\uff0c\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8698.8%\u7684\u4e00\u81f4\u6027\uff1b\u800c\u5728\u8bd5\u70b9\u7814\u7a76\u4e2d\uff0c\u4e0e\u7cd6\u5c3f\u75c5\u4e13\u5bb6\u76f8\u6bd4AI-CDSS\u7684\u4e00\u81f4\u7387\u4e3a98.5%\uff0c\u8fdc\u9ad8\u4e8e\u975e\u5185\u5206\u6ccc\u79d1\u4e13\u5bb6\u768485%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cAI-CDSS\u80fd\u591f\u4f5c\u4e3a\u51c6\u786e\u8bc6\u522b2\u578b\u7cd6\u5c3f\u75c5\u7684\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u7cd6\u5c3f\u75c5\u4e13\u79d1\u533b\u751f\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2602.12038", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12038", "abs": "https://arxiv.org/abs/2602.12038", "authors": ["Yuejun Guo", "Qiang Hu", "Qiang Tang", "Yves Le Traon"], "title": "An Empirical Study of the Imbalance Issue in Software Vulnerability Detection", "comment": "This paper was accepted by the 28th European Symposium on Research in Computer Security (ESORICS), 2023", "summary": "Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.", "AI": {"tldr": "This paper investigates the impact of data imbalance on the performance of deep learning models used for detecting software vulnerabilities, finding that while certain techniques help with specific metrics, none addresses all issues, and suggests areas for further research.", "motivation": "The motivation is to understand why deep learning (DL) based vulnerability detection shows variable performance across different datasets and to explore if the imbalance issue, where vulnerable code instances are far fewer than non-vulnerable ones, is a key factor behind this variability.", "method": "A comprehensive empirical study was conducted using nine open-source datasets and two state-of-the-art deep learning models to validate the conjecture that the imbalance issue is central to the variability of model performance in vulnerability detection. The study also evaluated how existing solutions for imbalance issues perform in this context.", "result": "The results show that the imbalance issue indeed plays a critical role in the performance of DL models for vulnerability detection. Additionally, it was found that Focal loss improves precision, mean false error and class-balanced loss improve recall, and random over-sampling enhances the F1-measure, but no solution performs well across all metrics.", "conclusion": "The study confirms that the imbalance issue is a core problem in deep learning-based vulnerability detection, and different existing solutions for imbalance perform variably across datasets and evaluation metrics. No single solution excels in all metrics, and external influences on these solutions are explored to provide insights for developing new solutions."}}
{"id": "2602.12058", "categories": ["cs.SE", "cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.12058", "abs": "https://arxiv.org/abs/2602.12058", "authors": ["Zhiyong Chen", "Jialun Cao", "Chang Xu", "Shing-Chi Cheung"], "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair", "comment": "Accepted by FM 2026 Research Track (Tool)", "summary": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.", "AI": {"tldr": "ModelWisdom \u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u73af\u5883\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u9ad8 TLA+ \u6a21\u578b\u68c0\u67e5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u3002\u5b83\u63d0\u4f9b\u4e86\u6a21\u578b\u53ef\u89c6\u5316\u3001\u56fe\u4f18\u5316\u3001\u6a21\u578b\u6458\u8981\u548c\u6a21\u578b\u4fee\u590d\u7b49\u529f\u80fd\uff0c\u4ee5\u5c06\u539f\u59cb\u6a21\u578b\u68c0\u67e5\u5668\u8f93\u51fa\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u3001\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u800c\u6539\u5584\u7406\u89e3\u5e76\u51cf\u5c11\u8c03\u8bd5\u5de5\u4f5c\u91cf\u3002", "motivation": "\u5c3d\u7ba1TLA+\u4e2d\u7684\u6a21\u578b\u68c0\u67e5\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u4f46\u4ece\u4e1a\u8005\u5728\u89e3\u91ca\u53cd\u4f8b\u3001\u7406\u89e3\u5927\u89c4\u6a21\u72b6\u6001\u8f6c\u6362\u56fe\u4ee5\u53ca\u4fee\u590d\u6545\u969c\u6a21\u578b\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u8fd9\u4e9b\u56f0\u96be\u6e90\u4e8e\u539f\u59cb\u6a21\u578b\u68c0\u67e5\u5668\u8f93\u51fa\u7684\u6709\u9650\u89e3\u91ca\u6027\u4ee5\u53ca\u8ffd\u8e2a\u8fdd\u89c4\u884c\u4e3a\u56de\u6e90\u89c4\u8303\u6240\u9700\u7684\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u3002", "method": "ModelWisdom \u63d0\u4f9b\u4e86\u4ee5\u4e0b\u529f\u80fd\uff1a(i) \u6a21\u578b\u53ef\u89c6\u5316\uff0c\u5305\u62ec\u7528\u989c\u8272\u7a81\u51fa\u663e\u793a\u8fdd\u89c4\u60c5\u51b5\u3001\u4ece\u8f6c\u6362\u5230TLA+\u4ee3\u7801\u7684\u70b9\u51fb\u94fe\u63a5\u4ee5\u53ca\u8fdd\u89c4\u72b6\u6001\u4e0e\u635f\u574f\u5c5e\u6027\u4e4b\u95f4\u7684\u6620\u5c04\uff1b(ii) \u56fe\u5f62\u4f18\u5316\uff0c\u5305\u62ec\u57fa\u4e8e\u6811\u7684\u7ed3\u6784\u5316\u548c\u8282\u70b9/\u8fb9\u7f18\u6298\u53e0\u4ee5\u7ba1\u7406\u5927\u578b\u6a21\u578b\uff1b(iii) \u6a21\u578b\u6458\u8981\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u603b\u7ed3\u5e76\u89e3\u91ca\u5b50\u56fe\uff0c\u5e76\u6267\u884c\u9884\u5904\u7406\u548c\u90e8\u5206\u89e3\u91ca\uff1b(iv) \u6a21\u578b\u4fee\u590d\uff0c\u63d0\u53d6\u9519\u8bef\u4fe1\u606f\u5e76\u652f\u6301\u8fed\u4ee3\u8c03\u8bd5\u3002", "result": "ModelWisdom \u80fd\u591f\u5c06\u539f\u59cb\u6a21\u578b\u68c0\u67e5\u5668\u8f93\u51fa\u8f6c\u5316\u4e3a\u66f4\u52a0\u4e92\u52a8\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8fd9\u6709\u52a9\u4e8e\u6539\u5584\u5bf9\u975e\u5e73\u51e1TLA+\u89c4\u8303\u7684\u7406\u89e3\uff0c\u5e76\u51cf\u5c11\u4e86\u8c03\u8bd5\u6240\u9700\u7684\u5de5\u4f5c\u91cf\u3002", "conclusion": "ModelWisdom \u901a\u8fc7\u63d0\u4f9b\u4e00\u7cfb\u5217\u5de5\u5177\u548c\u670d\u52a1\u663e\u8457\u63d0\u9ad8\u4e86TLA+\u6a21\u578b\u68c0\u67e5\u8fc7\u7a0b\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u9700\u8981\u5904\u7406\u590d\u6742\u6a21\u578b\u53ca\u76f8\u5e94\u53cd\u4f8b\u7684\u60c5\u51b5\u3002"}}
{"id": "2602.11246", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.11246", "abs": "https://arxiv.org/abs/2602.11246", "authors": ["Nikhil Garg", "Jon Kleinberg", "Kenny Peng"], "title": "How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?", "comment": null, "summary": "We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are linearly embedded in neuron activations) and linear accessibility (features can be linearly decoded). We then ask: How many neurons $d$ suffice to both linearly represent and linearly access $m$ features? Classical results in compressed sensing imply that for $k$-sparse inputs, $d = O(k\\log (m/k))$ suffices if we allow non-linear decoding algorithms (Candes and Tao, 2006; Candes et al., 2006; Donoho, 2006). However, the additional requirement of linear decoding takes the problem out of the classical compressed sensing, into linear compressed sensing.\n  Our main theoretical result establishes nearly-matching upper and lower bounds for linear compressed sensing. We prove that $d = \u03a9_\u03b5(\\frac{k^2}{\\log k}\\log (m/k))$ is required while $d = O_\u03b5(k^2\\log m)$ suffices. The lower bound establishes a quantitative gap between classical and linear compressed setting, illustrating how linear accessibility is a meaningfully stronger hypothesis than linear representation alone. The upper bound confirms that neurons can store an exponential number of features under the LRH, giving theoretical evidence for the \"superposition hypothesis\" (Elhage et al., 2022).\n  The upper bound proof uses standard random constructions of matrices with approximately orthogonal columns. The lower bound proof uses rank bounds for near-identity matrices (Alon, 2003) together with Tur\u00e1n's theorem (bounding the number of edges in clique-free graphs). We also show how our results do and do not constrain the geometry of feature representations and extend our results to allow decoders with an activation function and bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u6846\u67b6\u6765\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u662f\u5426\u4ee5\u7ebf\u6027\u65b9\u5f0f\u5b58\u50a8\u7279\u5f81\uff0c\u5e76\u5c06\u8fd9\u4e00\u5047\u8bbe\u5206\u4e3a\u7ebf\u6027\u8868\u793a\u548c\u7ebf\u6027\u8bbf\u95ee\u4e24\u4e2a\u65b9\u9762\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u4f5c\u8005\u4e3a\u7ebf\u6027\u538b\u7f29\u611f\u77e5\u95ee\u9898\u5efa\u7acb\u4e86\u51e0\u4e4e\u5339\u914d\u7684\u4e0a\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u795e\u7ecf\u5143\u53ef\u4ee5\u5b58\u50a8\u6307\u6570\u6570\u91cf\u7684\u7279\u5f81\uff0c\u4ece\u800c\u652f\u6301\u4e86'\u53e0\u52a0\u5047\u8bbe'\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u4e2d\u6240\u8c13\u7684\u7ebf\u6027\u8868\u793a\u5047\u8bbe\uff08LRH\uff09\uff0c\u5373\u8bed\u8a00\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u662f\u4ee5\u7ebf\u6027\u5f62\u5f0f\u5b58\u50a8\u7279\u5f81\u7684\u3002\u8be5\u7814\u7a76\u8bd5\u56fe\u89e3\u7b54\u9700\u8981\u591a\u5c11\u4e2a\u795e\u7ecf\u5143\u624d\u80fd\u540c\u65f6\u5b9e\u73b0\u5bf9\u7279\u5b9a\u6570\u91cf\u7279\u5f81\u7684\u7ebf\u6027\u8868\u793a\u4e0e\u7ebf\u6027\u8bbf\u95ee\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6570\u5b66\u5206\u6790\u65b9\u6cd5\uff0c\u7279\u522b\u5730\uff0c\u5229\u7528\u4e86\u7ecf\u5178\u538b\u7f29\u611f\u77e5\u9886\u57df\u7684\u7ed3\u679c\u4f5c\u4e3a\u57fa\u7840\uff0c\u5e76\u9488\u5bf9\u7ebf\u6027\u89e3\u7801\u8981\u6c42\u63d0\u51fa\u4e86\u65b0\u7684\u7406\u8bba\u8fb9\u754c\u3002\u4e0a\u754c\u8bc1\u660e\u4f9d\u8d56\u4e8e\u6807\u51c6\u968f\u673a\u77e9\u9635\u6784\u9020\u6280\u672f\uff1b\u4e0b\u754c\u5219\u501f\u52a9Alon (2003) \u5173\u4e8e\u8fd1\u5355\u4f4d\u77e9\u9635\u7684\u79e9\u754c\u9650\u4ee5\u53caTur\u00e1n\u5b9a\u7406\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e86\u7ebf\u6027\u538b\u7f29\u611f\u77e5\u6240\u9700\u7684\u795e\u7ecf\u5143\u6570\u76eed\u7684\u8fd1\u4f3c\u4e0a\u4e0b\u754c\uff1a\u4e0b\u754c\u4e3a\u03a9\u03b5(k^2/log k * log(m/k))\uff0c\u800c\u4e0a\u754c\u4e3aO\u03b5(k^2log m)\u3002\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u7ebf\u6027\u53ef\u8bbf\u95ee\u6027\u6bd4\u5355\u7eaf\u7ebf\u6027\u8868\u793a\u66f4\u5f3a\u7684\u4e8b\u5b9e\uff0c\u5e76\u4e14\u8868\u660e\u6839\u636eLRH\uff0c\u795e\u7ecf\u5143\u786e\u5b9e\u80fd\u591f\u5b58\u50a8\u5927\u91cf\u7684\u7279\u5f81\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8ek-\u7a00\u758f\u8f93\u5165\uff0c\u5728\u5141\u8bb8\u975e\u7ebf\u6027\u89e3\u7801\u7b97\u6cd5\u7684\u60c5\u51b5\u4e0b\uff0c\u6240\u9700\u795e\u7ecf\u5143\u6570\u76ee\u8fdc\u5c11\u4e8e\u5f53\u989d\u5916\u8981\u6c42\u7ebf\u6027\u89e3\u7801\u65f6\u7684\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u5176\u7ed3\u679c\u5982\u4f55\u5f71\u54cd\u7279\u5f81\u8868\u793a\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u6269\u5c55\u81f3\u8003\u8651\u5e26\u6fc0\u6d3b\u51fd\u6570\u53ca\u504f\u7f6e\u9879\u7684\u89e3\u7801\u5668\u60c5\u5f62\u3002"}}
{"id": "2602.12079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12079", "abs": "https://arxiv.org/abs/2602.12079", "authors": ["Alessandro Aneggi", "Vincenzo Stoico", "Andrea Janes"], "title": "Performance Antipatterns: Angel or Devil for Power Consumption?", "comment": null, "summary": "Performance antipatterns are known to degrade the responsiveness of microservice-based systems, but their impact on energy consumption remains largely unexplored. This paper empirically investigates whether widely studied performance antipatterns defined by Smith and Williams also negatively influence power usage. We implement ten antipatterns as isolated microservices and evaluate them under controlled load conditions, collecting synchronized measurements of performance, CPU and DRAM power consumption, and resource utilization across 30 repeated runs per antipattern. The results show that while all antipatterns degrade performance as expected, only a subset exhibit a statistically significant relationship between response time and increased power consumption. Specifically, several antipatterns reach CPU saturation, capping power draw regardless of rising response time, whereas others (\\eg Unnecessary Processing, The Ramp) demonstrate energy-performance coupling indicative of inefficiency. Our results show that, while all injected performance antipatterns increase response time as expected, only a subset also behaves as clear energy antipatterns, with several cases reaching a nearly constant CPU power level where additional slowdowns mainly translate into longer execution time rather than higher instantaneous power consumption. The study provides a systematic foundation for identifying performance antipatterns that also behave as energy antipatterns and offers actionable insights for designing more energy-efficient microservices architectures.", "AI": {"tldr": "\u672c\u7814\u7a76\u5b9e\u8bc1\u8c03\u67e5\u4e86\u5df2\u77e5\u7684\u6027\u80fd\u53cd\u6a21\u5f0f\u662f\u5426\u4e5f\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u7684\u80fd\u8017\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u901a\u8fc7\u5b9e\u65bd\u5341\u79cd\u53cd\u6a21\u5f0f\u5e76\u8fdb\u884c\u53d7\u63a7\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u867d\u7136\u6240\u6709\u53cd\u6a21\u5f0f\u90fd\u964d\u4f4e\u4e86\u6027\u80fd\uff0c\u4f46\u53ea\u6709\u90e8\u5206\u4e0e\u7535\u529b\u6d88\u8017\u663e\u8457\u589e\u52a0\u6709\u5173\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u77e5\u6027\u80fd\u53cd\u6a21\u5f0f\u4f1a\u964d\u4f4e\u57fa\u4e8e\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u54cd\u5e94\u6027\uff0c\u4f46\u5bf9\u4e8e\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u80fd\u6e90\u6d88\u8017\u7684\u7814\u7a76\u4ecd\u8f83\u5c11\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u76ee\u7684\u662f\u63a2\u8ba8Smith\u548cWilliams\u5b9a\u4e49\u7684\u4e00\u4e9b\u5e7f\u6cdb\u7814\u7a76\u7684\u6027\u80fd\u53cd\u6a21\u5f0f\u662f\u5426\u540c\u6837\u5bf9\u7535\u529b\u4f7f\u7528\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5c06\u5341\u4e2a\u4e0d\u540c\u7684\u6027\u80fd\u53cd\u6a21\u5f0f\u5b9e\u73b0\u4e3a\u72ec\u7acb\u7684\u5fae\u670d\u52a1\uff0c\u5e76\u5728\u53d7\u63a7\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5bf9\u4e8e\u6bcf\u4e00\u79cd\u53cd\u6a21\u5f0f\uff0c\u4ed6\u4eec\u6536\u96c6\u4e86\u5173\u4e8e\u6027\u80fd\u3001CPU\u548cDRAM\u529f\u8017\u4ee5\u53ca\u8d44\u6e90\u5229\u7528\u7387\u7684\u6570\u636e\uff0c\u6bcf\u4e2a\u53cd\u6a21\u5f0f\u91cd\u590d\u5b9e\u9a8c30\u6b21\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u6240\u6709\u7684\u6027\u80fd\u53cd\u6a21\u5f0f\u5982\u9884\u671f\u90a3\u6837\u6076\u5316\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u4f46\u662f\u53ea\u6709\u4e00\u90e8\u5206\u663e\u793a\u51fa\u54cd\u5e94\u65f6\u95f4\u589e\u957f\u4e0e\u529f\u7387\u6d88\u8017\u4e0a\u5347\u4e4b\u95f4\u5b58\u5728\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u5173\u7cfb\u3002\u7279\u522b\u5730\uff0c\u4e00\u4e9b\u53cd\u6a21\u5f0f\u8fbe\u5230\u4e86CPU\u9971\u548c\u70b9\uff0c\u6b64\u65f6\u5373\u4fbf\u54cd\u5e94\u65f6\u95f4\u7ee7\u7eed\u589e\u957f\uff0c\u529f\u7387\u5438\u6536\u4e5f\u4e0d\u4f1a\u518d\u589e\u52a0\uff1b\u800c\u5176\u4ed6\u51e0\u79cd\u53cd\u6a21\u5f0f\uff08\u4f8b\u5982\u4e0d\u5fc5\u8981\u7684\u5904\u7406\u3001\u5761\u9053\uff09\u5219\u663e\u793a\u51fa\u80fd\u91cf-\u6027\u80fd\u8026\u5408\u73b0\u8c61\uff0c\u8868\u660e\u5176\u6548\u7387\u4f4e\u4e0b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u6240\u6709\u88ab\u6ce8\u5165\u7684\u6027\u80fd\u53cd\u6a21\u5f0f\u90fd\u4f1a\u5bfc\u81f4\u54cd\u5e94\u65f6\u95f4\u5ef6\u957f\uff0c\u4f46\u53ea\u6709\u5c11\u6570\u540c\u65f6\u8868\u73b0\u4e3a\u660e\u663e\u7684\u80fd\u6e90\u53cd\u6a21\u5f0f\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u968f\u7740\u989d\u5916\u5ef6\u8fdf\u4e3b\u8981\u8f6c\u5316\u4e3a\u66f4\u957f\u7684\u6267\u884c\u65f6\u95f4\u800c\u4e0d\u662f\u66f4\u9ad8\u7684\u77ac\u65f6\u529f\u8017\uff0cCPU\u529f\u7387\u6c34\u5e73\u51e0\u4e4e\u4fdd\u6301\u4e0d\u53d8\u3002\u8be5\u7814\u7a76\u4e3a\u8bc6\u522b\u65e2\u662f\u6027\u80fd\u53c8\u662f\u80fd\u6e90\u53cd\u6a21\u5f0f\u7684\u60c5\u51b5\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u57fa\u7840\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u52a0\u8282\u80fd\u7684\u5fae\u670d\u52a1\u67b6\u6784\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.11287", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11287", "abs": "https://arxiv.org/abs/2602.11287", "authors": ["Yuanyong Luo", "Jing Huang", "Yu Cheng", "Ziwei Yu", "Kaihua Zhang", "Kehong Hong", "Xinda Ma", "Xin Wang", "Anping Tong", "Guipeng Hu", "Yun Xu", "Mehran Taghian", "Peng Wu", "Guanglin Li", "Yunke Peng", "Tianchi Hu", "Minqi Chen", "Michael Bi Mi", "Hu Liu", "Xiping Zhou", "Junsong Wang", "Qiang Lin", "Heng Liao"], "title": "HiFloat4 Format for Language Model Inference", "comment": "8 pages, 4 figures", "summary": "This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e13\u4e3a\u6df1\u5ea6\u5b66\u4e60\u8bbe\u8ba1\u7684\u5757\u6d6e\u70b9\u6570\u636e\u683c\u5f0fHiFloat4\uff08HiF4\uff09\uff0c\u6bcf\u4e2aHiF4\u5355\u5143\u5305\u542b64\u4e2a4\u4f4d\u5143\u7d20\u548c32\u4f4d\u5171\u4eab\u7f29\u653e\u5143\u6570\u636e\uff0c\u5e73\u5747\u6bcf\u4e2a\u503c\u4f7f\u75284.5\u4f4d\u3002\u8be5\u683c\u5f0f\u901a\u8fc7\u4e09\u7ea7\u7f29\u653e\u5c42\u6b21\u7ed3\u6784\u63d0\u9ad8\u4e86\u8868\u793a\u7a7a\u95f4\u5229\u7528\u7387\uff0c\u5e76\u4e14\u5728\u51e0\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u63a8\u7406\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684NVFP4\u683c\u5f0f\u76f8\u6bd4\uff0cHiF4\u5728\u591a\u79cd\u6a21\u578b\u548c\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u51c6\u786e\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u3001\u66f4\u9ad8\u6548\u7684\u6570\u636e\u8868\u793a\u683c\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u786c\u4ef6\u6548\u7387\u548c\u8ba1\u7b97\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiFloat4\u7684\u65b0\u6570\u636e\u683c\u5f0f\uff0c\u5b83\u5c06\u591a\u4e2a4\u4f4d\u6570\u503c\u4e0e\u5171\u4eab\u7684\u7f29\u653e\u4fe1\u606f\u6253\u5305\u5728\u4e00\u8d77\uff0c\u540c\u65f6\u91c7\u7528\u4e09\u7ea7\u7f29\u653e\u673a\u5236\u6765\u4f18\u5316\u52a8\u6001\u8303\u56f4\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u53ca\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52a1\u4e0a\uff0cHiF4\u683c\u5f0f\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6848NVFP4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "HiF4\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u683c\u5f0f\uff0c\u4e0d\u4ec5\u51cf\u5c11\u4e86\u786c\u4ef6\u9762\u79ef\u548c\u529f\u8017\u9700\u6c42\uff0c\u800c\u4e14\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u8ba1\u7b97\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u3002"}}
{"id": "2602.12081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12081", "abs": "https://arxiv.org/abs/2602.12081", "authors": ["Alessandro Aneggi", "Xiaozhou Li", "Andrea Janes"], "title": "PPTAM$\u03b7$: Energy Aware CI/CD Pipeline for Container Based Applications", "comment": null, "summary": "Modern container-based microservices evolve through rapid deployment cycles, but CI/CD pipelines still rarely measure energy consumption, even though prior work shows that design patterns, code smells and refactorings affect energy efficiency. We present PPTAM$\u03b7$, an automated pipeline that integrates power and energy measurement into GitLab CI for containerised API systems, coordinating load generation, container monitoring and hardware power probes to collect comparable metrics at each commit. The pipeline makes energy visible to developers, supports version comparison for test engineers and enables trend analysis for researchers. We evaluate PPTAM$\u03b7$ on a JWT-authenticated API across four commits, collecting performance and energy metrics and summarising the architecture, measurement methodology and validation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPPTAM\u03b7\u7684\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5b83\u5c06\u529f\u7387\u548c\u80fd\u91cf\u6d4b\u91cf\u96c6\u6210\u5230GitLab CI\u4e2d\uff0c\u7528\u4e8e\u5bb9\u5668\u5316\u7684API\u7cfb\u7edf\uff0c\u4f7f\u5f97\u5f00\u53d1\u8005\u80fd\u591f\u770b\u5230\u6bcf\u6b21\u63d0\u4ea4\u7684\u80fd\u91cf\u6d88\u8017\u60c5\u51b5\uff0c\u652f\u6301\u6d4b\u8bd5\u5de5\u7a0b\u5e08\u8fdb\u884c\u7248\u672c\u6bd4\u8f83\uff0c\u5e76\u4e14\u8ba9\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u8fdb\u884c\u8d8b\u52bf\u5206\u6790\u3002", "motivation": "\u5c3d\u7ba1\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\u8bbe\u8ba1\u6a21\u5f0f\u3001\u4ee3\u7801\u5f02\u5473\u4ee5\u53ca\u91cd\u6784\u4f1a\u5f71\u54cd\u80fd\u6e90\u6548\u7387\uff0c\u4f46CI/CD\u6d41\u6c34\u7ebf\u5f88\u5c11\u8861\u91cf\u80fd\u6e90\u6d88\u8017\u3002\u4e3a\u4e86\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\uff0c\u4f5c\u8005\u4eec\u5f00\u53d1\u4e86PPTAM\u03b7\uff0c\u65e8\u5728\u63d0\u9ad8\u5fae\u670d\u52a1\u67b6\u6784\u4e0b\u7684\u80fd\u6e90\u53ef\u89c1\u6027\u3002", "method": "PPTAM\u03b7\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u6d41\u7a0b\uff0c\u5b83\u901a\u8fc7\u534f\u8c03\u8d1f\u8f7d\u751f\u6210\u3001\u5bb9\u5668\u76d1\u63a7\u4e0e\u786c\u4ef6\u529f\u8017\u63a2\u9488\u6765\u6536\u96c6\u6bcf\u6b21\u63d0\u4ea4\u65f6\u53ef\u6bd4\u8f83\u7684\u5ea6\u91cf\u6307\u6807\u3002\u8be5\u6d41\u7a0b\u57fa\u4e8eGitLab CI\u6784\u5efa\uff0c\u9002\u7528\u4e8e\u5bb9\u5668\u5316API\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u5bf9\u4e00\u4e2aJWT\u8ba4\u8bc1API\u5728\u56db\u4e2a\u4e0d\u540c\u63d0\u4ea4\u4e0a\u7684\u8bc4\u4f30\uff0cPPTAM\u03b7\u6210\u529f\u5730\u6536\u96c6\u5230\u4e86\u6027\u80fd\u548c\u80fd\u8017\u6307\u6807\uff0c\u5e76\u5bf9\u67b6\u6784\u3001\u6d4b\u91cf\u65b9\u6cd5\u5b66\u53ca\u9a8c\u8bc1\u8fc7\u7a0b\u8fdb\u884c\u4e86\u603b\u7ed3\u3002", "conclusion": "PPTAM\u03b7\u4e3a\u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u4e86\u89c2\u5bdf\u6bcf\u4e2a\u7248\u672c\u80fd\u91cf\u6d88\u8017\u7684\u80fd\u529b\uff0c\u5e2e\u52a9\u6d4b\u8bd5\u5de5\u7a0b\u5e08\u5bf9\u6bd4\u4e0d\u540c\u7248\u672c\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u4e14\u4fbf\u4e8e\u7814\u7a76\u8005\u8fdb\u884c\u957f\u671f\u7684\u8d8b\u52bf\u5206\u6790\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u66f4\u52a0\u8282\u80fd\u7684\u670d\u52a1\u5f00\u53d1\u5b9e\u8df5\u3002"}}
{"id": "2602.11320", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11320", "abs": "https://arxiv.org/abs/2602.11320", "authors": ["Jamie Mahowald", "Brian Bell", "Alex Ho", "Michael Geyer"], "title": "Efficient Analysis of the Distilled Neural Tangent Kernel", "comment": "27 pages, 9 figures", "summary": "Neural tangent kernel (NTK) methods are computationally limited by the need to evaluate large Jacobians across many data points. Existing approaches reduce this cost primarily through projecting and sketching the Jacobian. We show that NTK computation can also be reduced by compressing the data dimension itself using NTK-tuned dataset distillation. We demonstrate that the neural tangent space spanned by the input data can be induced by dataset distillation, yielding a 20-100$\\times$ reduction in required Jacobian calculations. We further show that per-class NTK matrices have low effective rank that is preserved by this reduction. Building on these insights, we propose the distilled neural tangent kernel (DNTK), which combines NTK-tuned dataset distillation with state-of-the-art projection methods to reduce up NTK computational complexity by up to five orders of magnitude while preserving kernel structure and predictive performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u84b8\u998f\u795e\u7ecf\u5207\u7ebf\u6838\uff08DNTK\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u9488\u5bf9NTK\u8c03\u6574\u7684\u6570\u636e\u96c6\u84b8\u998f\u548c\u6700\u5148\u8fdb\u7684\u6295\u5f71\u65b9\u6cd5\u6765\u51cf\u5c11NTK\u8ba1\u7b97\u7684\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u6838\u7ed3\u6784\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u5207\u7ebf\u6838(NTK)\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u53d7\u5230\u9700\u8981\u8de8\u591a\u4e2a\u6570\u636e\u70b9\u8bc4\u4f30\u5927\u578b\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u6295\u5c04\u548c\u8349\u56fe\u5316\u96c5\u53ef\u6bd4\u77e9\u9635\u6765\u964d\u4f4e\u8fd9\u79cd\u6210\u672c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u9488\u5bf9NTK\u8c03\u6574\u7684\u6570\u636e\u96c6\u84b8\u998f\u6765\u538b\u7f29\u6570\u636e\u7ef4\u5ea6\u672c\u8eab\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u51cf\u5c11NTK\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u672c\u6587\u9996\u5148\u5c55\u793a\u4e86\u53ef\u4ee5\u901a\u8fc7\u6570\u636e\u96c6\u84b8\u998f\u8bf1\u5bfc\u8f93\u5165\u6570\u636e\u6240\u8de8\u8d8a\u7684\u795e\u7ecf\u5207\u7ebf\u7a7a\u95f4\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u8ba1\u7b97\u91cf\u3002\u7136\u540e\uff0c\u4f5c\u8005\u6307\u51fa\u6bcf\u4e2a\u7c7b\u522b\u7684NTK\u77e9\u9635\u5177\u6709\u4f4e\u7684\u6709\u6548\u79e9\uff0c\u5e76\u4e14\u8fd9\u79cd\u6027\u8d28\u5728\u6570\u636e\u964d\u7ef4\u8fc7\u7a0b\u4e2d\u5f97\u4ee5\u4fdd\u7559\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e86\u84b8\u998f\u795e\u7ecf\u5207\u7ebf\u6838(DNTK)\uff0c\u5b83\u5c06\u9488\u5bf9NTK\u8c03\u4f18\u7684\u6570\u636e\u96c6\u84b8\u998f\u4e0e\u6700\u5148\u8fdb\u7684\u6295\u5f71\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86NTK\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u6301\u5185\u6838\u7ed3\u6784\u548c\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06NTK\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u591a\u8fbe\u4e94\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u7684\u65b9\u6cd5\uff0c\u5373\u84b8\u998f\u795e\u7ecf\u5207\u7ebf\u6838(DNTK)\uff0c\u4e3a\u51cf\u5c11\u795e\u7ecf\u5207\u7ebf\u6838\u65b9\u6cd5\u4e2d\u7684\u8ba1\u7b97\u8d1f\u62c5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u9014\u5f84\uff0c\u540c\u65f6\u80fd\u591f\u4fdd\u6301\u826f\u597d\u7684\u9884\u6d4b\u8868\u73b0\u3002"}}
{"id": "2602.11322", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.11322", "abs": "https://arxiv.org/abs/2602.11322", "authors": ["Jason Dury"], "title": "Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence", "comment": "20 pages, 6 figures, for associated Git: https://github.com/EridosAI/PAM-Benchmark", "summary": "Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through temporal co-occurrence. We propose Predictive Associative Memory (PAM), an architecture in which a JEPA-style predictor, trained on temporal co-occurrence within a continuous experience stream, learns to navigate the associative structure of an embedding space. We introduce an Inward JEPA that operates over stored experience (predicting associatively reachable past states) as the complement to the standard Outward JEPA that operates over incoming sensory data (predicting future states). We evaluate PAM as an associative recall system -- testing faithfulness of recall for experienced associations -- rather than as a retrieval system evaluated on generalisation to unseen associations. On a synthetic benchmark, the predictor's top retrieval is a true temporal associate 97% of the time (Association Precision@1 = 0.970); it achieves cross-boundary Recall@20 = 0.421 where cosine similarity scores zero; and it separates experienced-together from never-experienced-together states with a discrimination AUC of 0.916 (cosine: 0.789). Even restricted to cross-room pairs where embedding similarity is uninformative, the predictor achieves AUC = 0.849 (cosine: 0.503, chance). A temporal shuffle control confirms the signal is genuine temporal co-occurrence structure, not embedding geometry: shuffling collapses cross-boundary recall by 90%, replicated across training seeds. All results are stable across seeds (SD < 0.006) and query selections (SD $\\leq$ 0.012).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPAM\uff08Predictive Associative Memory\uff09\u7684\u65b0\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7JEPA\u98ce\u683c\u7684\u9884\u6d4b\u5668\u5b66\u4e60\u8fde\u7eed\u4f53\u9a8c\u6d41\u4e2d\u7684\u65f6\u95f4\u5171\u73b0\u5173\u7cfb\uff0c\u4ee5\u5bfc\u822a\u5d4c\u5165\u7a7a\u95f4\u7684\u5173\u8054\u7ed3\u6784\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPAM\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u80fd\u591f\u6709\u6548\u8bc6\u522b\u7ecf\u9a8c\u6027\u5173\u8054\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7cfb\u7edf\u4e2d\u7684\u8bb0\u5fc6\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86\u751f\u7269\u8bb0\u5fc6\u7684\u4e00\u4e2a\u57fa\u672c\u7279\u6027\uff1a\u901a\u8fc7\u65f6\u95f4\u5171\u73b0\u8fdb\u884c\u5173\u8054\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u8bb0\u5fc6\u67b6\u6784\uff0c\u5b83\u80fd\u66f4\u597d\u5730\u6a21\u4eff\u751f\u7269\u8bb0\u5fc6\u7684\u65f6\u95f4\u5173\u8054\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86PAM\u67b6\u6784\uff0c\u5305\u542b\u4e00\u4e2aJEPA\u98ce\u683c\u7684\u9884\u6d4b\u5668\uff0c\u7528\u4e8e\u5b66\u4e60\u8fde\u7eed\u4f53\u9a8c\u6d41\u5185\u7684\u65f6\u95f4\u5171\u73b0\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86Inward JEPA\u6765\u5904\u7406\u5b58\u50a8\u7684\u7ecf\u9a8c\u4ee5\u53caOutward JEPA\u6765\u5904\u7406\u4f20\u5165\u7684\u611f\u89c9\u6570\u636e\u3002\u8bc4\u4f30\u65f6\u5c06PAM\u89c6\u4e3a\u8054\u60f3\u56de\u5fc6\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u5df2\u4f53\u9a8c\u8fc7\u7684\u5173\u8054\u800c\u975e\u5bf9\u672a\u89c1\u8fc7\u7684\u5173\u8054\u7684\u4e00\u822c\u5316\u80fd\u529b\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cPAM\u7684\u8868\u73b0\u975e\u5e38\u51fa\u8272\uff0c\u5bf9\u4e8e\u771f\u5b9e\u65f6\u95f4\u5173\u8054\u7684\u8bb0\u5fc6\u51c6\u786e\u7387\u8fbe\u5230\u4e8697%\uff1b\u8de8\u8fb9\u754c\u53ec\u56de\u7387\u4e3a0.421\uff0c\u800c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5f97\u5206\u4e3a\u96f6\uff1b\u533a\u5206\u5171\u540c\u7ecf\u5386\u548c\u4ece\u672a\u5171\u540c\u7ecf\u5386\u7684\u72b6\u6001\u65f6\uff0cAUC\u4e3a0.916\uff08\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e3a0.789\uff09\u3002\u5373\u4f7f\u662f\u5728\u623f\u95f4\u95f4\u914d\u5bf9\u8fd9\u6837\u7684\u9650\u5236\u6761\u4ef6\u4e0b\uff0cPAM\u4f9d\u65e7\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u6a21\u62df\u751f\u7269\u8bb0\u5fc6\u4e2d\u7684\u65f6\u95f4\u5173\u8054\u7279\u5f81\uff0c\u5e76\u4e14\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u3002\u8fd9\u8868\u660e\uff0c\u901a\u8fc7\u5b66\u4e60\u65f6\u95f4\u5171\u73b0\u6a21\u5f0f\u53ef\u4ee5\u6709\u6548\u5730\u589e\u5f3a\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2602.12256", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12256", "abs": "https://arxiv.org/abs/2602.12256", "authors": ["Alex Chudic", "G\u00fcl \u00c7al\u0131kl\u0131"], "title": "Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting", "comment": "13 pages, 3 figures, accepted to ICPC 2026 (34th International Conference on Program Comprehension)", "summary": "Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\uff08few-shot prompting\uff09\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u4e0d\u540c\u6765\u6e90\u7684\u6d4b\u8bd5\u5de5\u4ef6\uff08\u5982\u4eba\u5de5\u7f16\u5199\u3001SBST\u6216LLM\u751f\u6210\u7684\uff09\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u4eba\u5de5\u7f16\u5199\u7684\u4f8b\u5b50\u4f5c\u4e3a\u63d0\u793a\u53ef\u4ee5\u4ea7\u751f\u6700\u4f73\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\uff0c\u5e76\u4e14\u57fa\u4e8e\u95ee\u9898\u63cf\u8ff0\u548c\u4ee3\u7801\u76f8\u4f3c\u6027\u7684\u7efc\u5408\u9009\u62e9\u65b9\u6cd5\u6700\u6709\u6548\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u9762\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u5c11\u91cf\u793a\u4f8b\u5b66\u4e60\u6f5c\u529b\u4ecd\u5f85\u63a2\u7d22\u3002\u8003\u8651\u5230\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u6df7\u5408\u5b58\u5728\u4eba\u5de5\u7f16\u5199\u7684\u6d4b\u8bd5\u3001\u4f20\u7edf\u5de5\u5177\u751f\u6210\u7684\u6d4b\u8bd5\u4ee5\u53caLLM\u751f\u6210\u7684\u6d4b\u8bd5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u4e0d\u540c\u6765\u6e90\u7684\u6d4b\u8bd5\u5de5\u4ef6\u5982\u4f55\u5f71\u54cdLLM\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u7684\u8d28\u91cf\u53ca\u5176\u5bf9\u73b0\u6709\u6d4b\u8bd5\u5957\u4ef6\u6539\u8fdb\u7684\u8d21\u732e\u3002", "method": "\u5b9e\u9a8c\u91c7\u7528\u4e86HumanEval\u548cClassEval\u6570\u636e\u96c6\uff0c\u5229\u7528\u96c6\u6210\u5230GitHub Copilot\u4e2d\u7684GPT-4o\u8fdb\u884c\u3002\u8bc4\u4f30\u4e0d\u4ec5\u6db5\u76d6\u4e86\u6b63\u786e\u6027\u548c\u8986\u76d6\u7387\uff0c\u8fd8\u8003\u5bdf\u4e86\u53ef\u8bfb\u6027\u3001\u8ba4\u77e5\u590d\u6742\u5ea6\u53ca\u7ef4\u62a4\u6027\u7b49\u6307\u6807\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e86\u51e0\u79cd\u57fa\u4e8e\u68c0\u7d22\u7684\u65b9\u6cd5\u6765\u9009\u53d6\u76f8\u5173\u793a\u4f8b\u7528\u4e8e\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u4f7f\u7528\u4eba\u5de5\u7f16\u5199\u7684\u793a\u4f8b\u65f6\uff0c\u751f\u6210\u7684\u5355\u5143\u6d4b\u8bd5\u5177\u6709\u6700\u9ad8\u7684\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\uff1b\u800c\u57fa\u4e8e\u95ee\u9898\u63cf\u8ff0\u4e0e\u4ee3\u7801\u4e4b\u95f4\u7ec4\u5408\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\uff0c\u5728\u6311\u9009\u793a\u4f8b\u4ee5\u5f62\u6210\u9ad8\u6548\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\u65b9\u9762\u8868\u73b0\u6700\u4e3a\u51fa\u8272\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6311\u9009\u7684\u4f8b\u5b50\u8fdb\u884c\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\uff0cLLMs\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u8fd9\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u5957\u4ef6\u7684\u6709\u6548\u6027\uff0c\u4e5f\u589e\u5f3a\u4e86\u4eba\u7c7b\u4e0eAI\u5171\u540c\u5f00\u53d1\u73af\u5883\u4e0b\u7684\u4ee3\u7801\u7406\u89e3\u548c\u7ef4\u62a4\u80fd\u529b\u3002"}}
{"id": "2602.11350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11350", "abs": "https://arxiv.org/abs/2602.11350", "authors": ["Tomer Meir", "Ori Linial", "Danny Eytan", "Uri Shalit"], "title": "Structured Hybrid Mechanistic Models for Robust Estimation of Time-Dependent Intervention Outcomes", "comment": null, "summary": "Estimating intervention effects in dynamical systems is crucial for outcome optimization. In medicine, such interventions arise in physiological regulation (e.g., cardiovascular system under fluid administration) and pharmacokinetics, among others. Propofol administration is an anesthetic intervention, where the challenge is to estimate the optimal dose required to achieve a target brain concentration for anesthesia, given patient characteristics, while avoiding under- or over-dosing. The pharmacokinetic state is characterized by drug concentrations across tissues, and its dynamics are governed by prior states, patient covariates, drug clearance, and drug administration. While data-driven models can capture complex dynamics, they often fail in out-of-distribution (OOD) regimes. Mechanistic models on the other hand are typically robust, but might be oversimplified. We propose a hybrid mechanistic-data-driven approach to estimate time-dependent intervention outcomes. Our approach decomposes the dynamical system's transition operator into parametric and nonparametric components, further distinguishing between intervention-related and unrelated dynamics. This structure leverages mechanistic anchors while learning residual patterns from data. For scenarios where mechanistic parameters are unknown, we introduce a two-stage procedure: first, pre-training an encoder on simulated data, and subsequently learning corrections from observed data. Two regimes with incomplete mechanistic knowledge are considered: periodic pendulum and Propofol bolus injections. Results demonstrate that our hybrid approach outperforms purely data-driven and mechanistic approaches, particularly OOD. This work highlights the potential of hybrid mechanistic-data-driven models for robust intervention optimization in complex, real-world dynamical systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u673a\u5236-\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u65f6\u95f4\u4f9d\u8d56\u6027\u5e72\u9884\u7684\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u5c06\u52a8\u6001\u7cfb\u7edf\u7684\u8f6c\u79fb\u7b97\u5b50\u5206\u89e3\u4e3a\u53c2\u6570\u548c\u975e\u53c2\u6570\u7ec4\u4ef6\uff0c\u5e76\u8fdb\u4e00\u6b65\u533a\u5206\u4e0e\u5e72\u9884\u76f8\u5173\u7684\u548c\u4e0d\u76f8\u5173\u7684\u52a8\u6001\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u5728\u5904\u7406\u5206\u5e03\u5916(OOD)\u60c5\u51b5\u65f6\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u548c\u673a\u5236\u6a21\u578b\u3002", "motivation": "\u5728\u533b\u5b66\u4e2d\uff0c\u5bf9\u751f\u7406\u8c03\u8282\uff08\u5982\u6db2\u4f53\u7ed9\u836f\u4e0b\u7684\u5fc3\u8840\u7ba1\u7cfb\u7edf\uff09\u548c\u836f\u4ee3\u52a8\u529b\u5b66\u7b49\u8fdb\u884c\u5e72\u9884\u6548\u679c\u7684\u4f30\u8ba1\u5bf9\u4e8e\u4f18\u5316\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u867d\u7136\u80fd\u6355\u6349\u590d\u6742\u7684\u52a8\u6001\uff0c\u4f46\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u5e38\u5e38\u8868\u73b0\u4e0d\u4f73\uff1b\u800c\u673a\u5236\u6a21\u578b\u867d\u7136\u901a\u5e38\u6bd4\u8f83\u7a33\u5065\uff0c\u4f46\u53ef\u80fd\u8fc7\u4e8e\u7b80\u5316\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u52a8\u6001\u7cfb\u7edf\u7684\u8f6c\u79fb\u7b97\u5b50\u5206\u89e3\u4e3a\u53c2\u6570\u5316\u548c\u975e\u53c2\u6570\u5316\u6210\u5206\u6765\u5b9e\u73b0\uff0c\u540c\u65f6\u533a\u5206\u51fa\u4e0e\u5e72\u9884\u76f8\u5173\u53ca\u65e0\u5173\u7684\u52a8\u529b\u5b66\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u90a3\u4e9b\u672a\u77e5\u673a\u5236\u53c2\u6570\u7684\u60c5\u51b5\uff0c\u6587\u4e2d\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7a0b\u5e8f\uff1a\u9996\u5148\u57fa\u4e8e\u6a21\u62df\u6570\u636e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u7136\u540e\u4ece\u89c2\u5bdf\u6570\u636e\u4e2d\u5b66\u4e60\u4fee\u6b63\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e24\u79cd\u5177\u6709\u4e0d\u5b8c\u5168\u673a\u5236\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u2014\u2014\u5468\u671f\u6446\u52a8\u548c\u4e19\u6cca\u915a\u63a8\u6ce8\u2014\u2014\u6240\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u6216\u673a\u5236\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u5206\u5e03\u5916\u7684\u6570\u636e\u65f6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u6df7\u5408\u673a\u5236-\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u9c81\u68d2\u5e72\u9884\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2602.11378", "categories": ["cs.LG", "cs.CE", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.11378", "abs": "https://arxiv.org/abs/2602.11378", "authors": ["Amirpasha Hedayat", "Alberto Padovan", "Karthik Duraisamy"], "title": "Toward Adaptive Non-Intrusive Reduced-Order Models: Design and Challenges", "comment": null, "summary": "Projection-based Reduced Order Models (ROMs) are often deployed as static surrogates, which limits their practical utility once a system leaves the training manifold. We formalize and study adaptive non-intrusive ROMs that update both the latent subspace and the reduced dynamics online. Building on ideas from static non-intrusive ROMs, specifically, Operator Inference (OpInf) and the recently-introduced Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM), we propose three formulations: Adaptive OpInf (sequential basis/operator refits), Adaptive NiTROM (joint Riemannian optimization of encoder/decoder and polynomial dynamics), and a hybrid that initializes NiTROM with an OpInf update. We describe the online data window, adaptation window, and computational budget, and analyze cost scaling. On a transiently perturbed lid-driven cavity flow, static Galerkin/OpInf/NiTROM drift or destabilize when forecasting beyond training. In contrast, Adaptive OpInf robustly suppresses amplitude drift with modest cost; Adaptive NiTROM is shown to attain near-exact energy tracking under frequent updates but is sensitive to its initialization and optimization depth; the hybrid is most reliable under regime changes and minimal offline data, yielding physically coherent fields and bounded energy. We argue that predictive claims for ROMs must be cost-aware and transparent, with clear separation of training/adaptation/deployment regimes and explicit reporting of online budgets and full-order model queries. This work provides a practical template for building self-correcting, non-intrusive ROMs that remain effective as the dynamics evolve well beyond the initial manifold.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u4e09\u79cd\u81ea\u9002\u5e94\u975e\u4fb5\u5165\u5f0f\u964d\u9636\u6a21\u578b\uff08ROM\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u5728\u7ebf\u66f4\u65b0\u6f5c\u5728\u5b50\u7a7a\u95f4\u548c\u7b80\u5316\u52a8\u529b\u5b66\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8d85\u51fa\u521d\u59cb\u8bad\u7ec3\u8303\u56f4\u65f6\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6295\u5f71\u7684\u964d\u9636\u6a21\u578b\u4f5c\u4e3a\u9759\u6001\u66ff\u4ee3\u54c1\u4f7f\u7528\u65f6\uff0c\u5728\u7cfb\u7edf\u79bb\u5f00\u8bad\u7ec3\u6d41\u5f62\u540e\u5176\u5b9e\u7528\u6027\u53d7\u5230\u9650\u5236\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u80fd\u591f\u5728\u7ebf\u66f4\u65b0\u6f5c\u5728\u5b50\u7a7a\u95f4\u548c\u7b80\u5316\u52a8\u529b\u5b66\u7684\u81ea\u9002\u5e94\u975e\u4fb5\u5165\u5f0fROM\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u9759\u6001\u975e\u4fb5\u5165\u5f0fROM\u7684\u60f3\u6cd5\uff0c\u7279\u522b\u662f\u64cd\u4f5c\u63a8\u65ad(OpInf)\u548c\u6700\u8fd1\u5f15\u5165\u7684\u975e\u4fb5\u5165\u5f0f\u8f68\u8ff9\u4f18\u5316ROM(NiTROM)\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e09\u79cd\u5f62\u5f0f\uff1a\u81ea\u9002\u5e94OpInf\uff08\u5e8f\u5217\u57fa\u7840/\u64cd\u4f5c\u91cd\u65b0\u62df\u5408\uff09\u3001\u81ea\u9002\u5e94NiTROM\uff08\u7f16\u7801\u5668/\u89e3\u7801\u5668\u4e0e\u591a\u9879\u5f0f\u52a8\u529b\u5b66\u7684\u8054\u5408\u9ece\u66fc\u4f18\u5316\uff09\uff0c\u4ee5\u53ca\u4e00\u79cd\u5c06NiTROM\u521d\u59cb\u5316\u4e3aOpInf\u66f4\u65b0\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u5bf9\u4e8e\u77ac\u6001\u6270\u52a8\u7684\u9876\u76d6\u9a71\u52a8\u8154\u6d41\u95ee\u9898\uff0c\u5f53\u9884\u6d4b\u8d85\u51fa\u8bad\u7ec3\u8303\u56f4\u65f6\uff0c\u9759\u6001Galerkin/OpInf/NiTROM\u4f1a\u51fa\u73b0\u6f02\u79fb\u6216\u4e0d\u7a33\u5b9a\u73b0\u8c61\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u81ea\u9002\u5e94OpInf\u80fd\u591f\u4ee5\u8f83\u4f4e\u6210\u672c\u6709\u6548\u5730\u6291\u5236\u632f\u5e45\u6f02\u79fb\uff1b\u81ea\u9002\u5e94NiTROM\u5728\u9891\u7e41\u66f4\u65b0\u4e0b\u51e0\u4e4e\u53ef\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u80fd\u91cf\u8ddf\u8e2a\uff0c\u4f46\u5bf9\u521d\u59cb\u5316\u548c\u4f18\u5316\u6df1\u5ea6\u654f\u611f\uff1b\u800c\u6df7\u5408\u65b9\u6cd5\u5728\u673a\u5236\u53d8\u5316\u53ca\u6700\u5c0f\u79bb\u7ebf\u6570\u636e\u6761\u4ef6\u4e0b\u6700\u4e3a\u53ef\u9760\uff0c\u4ea7\u751f\u7269\u7406\u4e0a\u4e00\u81f4\u7684\u573a\u548c\u6709\u754c\u80fd\u91cf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6784\u5efa\u81ea\u6211\u7ea0\u6b63\u3001\u975e\u4fb5\u5165\u5f0f\u7684ROM\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u5373\u4f7f\u5728\u52a8\u6001\u53d1\u5c55\u8fdc\u8d85\u521d\u59cb\u6d41\u5f62\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u6307\u51fa\u5bf9\u4e8eROM\u7684\u9884\u6d4b\u58f0\u660e\u9700\u8981\u8003\u8651\u6210\u672c\u610f\u8bc6\u548c\u900f\u660e\u5ea6\uff0c\u660e\u786e\u533a\u5206\u8bad\u7ec3/\u9002\u5e94/\u90e8\u7f72\u9636\u6bb5\uff0c\u5e76\u6e05\u6670\u62a5\u544a\u5728\u7ebf\u9884\u7b97\u548c\u5168\u9636\u6a21\u578b\u67e5\u8be2\u60c5\u51b5\u3002"}}
{"id": "2602.11387", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11387", "abs": "https://arxiv.org/abs/2602.11387", "authors": ["Anirudh Satheesh", "Ziyi Chen", "Furong Huang", "Heng Huang"], "title": "Provably Efficient Algorithms for S- and Non-Rectangular Robust MDPs with General Parameterization", "comment": "30 pages", "summary": "We study robust Markov decision processes (RMDPs) with general policy parameterization under s-rectangular and non-rectangular uncertainty sets. Prior work is largely limited to tabular policies, and hence either lacks sample complexity guarantees or incurs high computational cost. Our method reduces the average reward RMDPs to entropy-regularized discounted robust MDPs, restoring strong duality and enabling tractable equilibrium computation. We prove novel Lipschitz and Lipschitz-smoothness properties for general policy parameterizations that extends to infinite state spaces. To address infinite-horizon gradient estimation, we introduce a multilevel Monte Carlo gradient estimator with $\\tilde{\\mathcal{O}}(\u03b5^{-2})$ sample complexity, a factor of $\\mathcal{O}(\u03b5^{-2})$ improvement over prior work. Building on this, we design a projected gradient descent algorithm for s-rectangular uncertainty ($\\mathcal{O}(\u03b5^{-5})$) and a Frank--Wolfe algorithm for non-rectangular uncertainty ($\\mathcal{O}(\u03b5^{-4})$ discounted, $\\mathcal{O}(\u03b5^{-10.5})$ average reward), significantly improving prior results in both the discounted setting and average reward setting. Our work is the first one to provide sample complexity guarantees for RMDPs with general policy parameterization beyond $(s, a)$-rectangularity. It also provides the first such guarantees in the average reward setting and improves existing bounds for discounted robust MDPs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728s-\u77e9\u5f62\u548c\u975e\u77e9\u5f62\u4e0d\u786e\u5b9a\u96c6\u4e0b\u5177\u6709\u4e00\u822c\u7b56\u7565\u53c2\u6570\u5316\u7684\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08RMDPs\uff09\u3002\u901a\u8fc7\u5c06\u5e73\u5747\u5956\u52b1RMDPs\u7b80\u5316\u4e3a\u71b5\u6b63\u5219\u5316\u6298\u6263\u9c81\u68d2MDPs\uff0c\u89e3\u51b3\u4e86\u5f3a\u5bf9\u5076\u6027\u95ee\u9898\uff0c\u5e76\u4f7f\u5f97\u5747\u8861\u8ba1\u7b97\u53d8\u5f97\u53ef\u884c\u3002\u5bf9\u4e8e\u65e0\u9650\u65f6\u57df\u68af\u5ea6\u4f30\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u5176\u6837\u672c\u590d\u6742\u5ea6\u4e3a$\\tilde{\\mathcal{O}}(\u03b5^{-2})$\u3002\u57fa\u4e8e\u6b64\uff0c\u9488\u5bf9s-\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u5bf9\u4e8e\u975e\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u8bbe\u8ba1\u4e86\u4e00\u4e2aFrank-Wolfe\u7b97\u6cd5\u3002\u8fd9\u662f\u9996\u4e2a\u4e3a\u8d85\u8d8a$(s, a)$-\u77e9\u5f62\u6027\u7684RMDPs\u63d0\u4f9b\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u7684\u5de5\u4f5c\uff0c\u5728\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\u4e2d\u63d0\u4f9b\u4e86\u9996\u4f8b\u6b64\u7c7b\u4fdd\u8bc1\uff0c\u5e76\u6539\u8fdb\u4e86\u6298\u6263\u9c81\u68d2MDPs\u7684\u73b0\u6709\u754c\u9650\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u8868\u683c\u578b\u7b56\u7565\uff0c\u8fd9\u5bfc\u81f4\u8981\u4e48\u7f3a\u4e4f\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5904\u7406\u5177\u6709\u66f4\u4e00\u822c\u7b56\u7565\u53c2\u6570\u5316\u7684\u9c81\u68d2MDP\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65e0\u9650\u72b6\u6001\u7a7a\u95f4\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f5c\u8005\u4eec\u901a\u8fc7\u5c06\u5e73\u5747\u5956\u52b1RMDPs\u8f6c\u5316\u4e3a\u71b5\u6b63\u5219\u5316\u6298\u6263\u9c81\u68d2MDPs\u6765\u6062\u590d\u5f3a\u5bf9\u5076\u6027\u5e76\u5141\u8bb8\u8fdb\u884c\u6613\u4e8e\u7ba1\u7406\u7684\u5e73\u8861\u8ba1\u7b97\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63d0\u51fa\u4e86\u65b0\u7684Lipschitz\u548c\u5e73\u6ed1\u6027\u8d28\u4ee5\u9002\u7528\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b56\u7565\u53c2\u6570\u5316\u5f62\u5f0f\u3002\u4e3a\u4e86\u5e94\u5bf9\u65e0\u9650\u65f6\u57df\u4e0b\u7684\u68af\u5ea6\u4f30\u8ba1\u6311\u6218\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u68af\u5ea6\u4f30\u8ba1\u6280\u672f\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5f00\u53d1\u4e86\u4e13\u95e8\u9488\u5bf9s-\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u60c5\u51b5\u7684\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4ee5\u53ca\u5904\u7406\u975e\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u7684Frank-Wolfe\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u68af\u5ea6\u4f30\u8ba1\u5668\u76f8\u8f83\u4e8e\u4e4b\u524d\u7684\u5de5\u4f5c\uff0c\u5728\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u63d0\u9ad8\u4e86$\\mathcal{O}(\u03b5^{-2})$\u500d\u3002\u6b64\u5916\uff0c\u9488\u5bf9s-\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u5206\u522b\u8bbe\u8ba1\u7684\u7b97\u6cd5\u4e5f\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff1as-\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fbe\u5230$\\mathcal{O}(\u03b5^{-5})$\u3001\u975e\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fbe\u5230$\\mathcal{O}(\u03b5^{-4})$\u6298\u6263\u5956\u52b1\u4e0e$\\mathcal{O}(\u03b5^{-10.5})$\u5e73\u5747\u5956\u52b1\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u4e3a\u8d85\u51fa$(s, a)$-\u77e9\u5f62\u6027\u7684\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u5e76\u4e14\u5728\u5e73\u5747\u5956\u52b1\u8bbe\u5b9a\u4e0b\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002\u540c\u65f6\uff0c\u5b83\u4e5f\u4e3a\u6298\u6263\u9c81\u68d2MDPs\u5e26\u6765\u4e86\u6539\u8fdb\u7684\u8fb9\u754c\u6761\u4ef6\u3002"}}
{"id": "2602.11388", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11388", "abs": "https://arxiv.org/abs/2602.11388", "authors": ["Dibyanayan Bandyopadhyay", "Asif Ekbal"], "title": "Sparse Semantic Dimension as a Generalization Certificate for LLMs", "comment": "Work in progress (17 pages)", "summary": "Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive \"feature sharpness\" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable \"feature explosion\" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u590d\u6742\u6027\u5ea6\u91cf\u2014\u2014\u7a00\u758f\u8bed\u4e49\u7ef4\u5ea6\uff08SSD\uff09\uff0c\u7528\u6765\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u4f55\u80fd\u591f\u6709\u6548\u6cdb\u5316\u3002\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u7814\u7a76\u8005\u53d1\u73b0\u6a21\u578b\u53c2\u6570\u867d\u7136\u5e9e\u5927\uff0c\u4f46\u6fc0\u6d3b\u72b6\u6001\u5b9e\u9645\u4e0a\u4f4d\u4e8e\u4e00\u4e2a\u4f4e\u7ef4\u4e14\u7a00\u758f\u7684\u6d41\u5f62\u4e0a\u3002\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bad\u7ec3\u5f97\u5230\u7684\u7279\u5f81\u8bcd\u6c47\u8868\uff0c\u4f5c\u8005\u4eec\u5c55\u793a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u66f4\u591a\u5730\u4f9d\u8d56\u4e8e\u5b57\u5178\u7684\u7a00\u758f\u6027\u800c\u975e\u603b\u53c2\u6570\u6570\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u5927\u7684\u6a21\u578b\u5982Gemma-2B\u76f8\u8f83\u4e8eGPT-2 Small\uff0c\u5728\u8bc6\u522b\u6d3b\u8dc3\u6d41\u5f62\u65f6\u9700\u8981\u66f4\u5c11\u7684\u6821\u51c6\u6837\u672c\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u538b\u7f29\u6027\u548c\u72ec\u7279\u7684\u8bed\u4e49\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u6b64\u6846\u67b6\u8fd8\u53ef\u7528\u4f5c\u5b89\u5168\u76d1\u63a7\u5de5\u5177\uff0c\u901a\u8fc7\u68c0\u6d4b\u5f02\u5e38\u8f93\u5165\u5bfc\u81f4\u7684\u7279\u5f81\u6fc0\u589e\u6765\u6307\u793a\u77e5\u8bc6\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u6807\u51c6\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u9884\u6d4b\uff0c\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53c2\u6570\u6570\u91cf\u8fdc\u8d85\u8bad\u7ec3\u6807\u8bb0\u6570\uff0c\u8fd9\u4e9b\u6a21\u578b\u5e94\u8be5\u4f1a\u8fc7\u62df\u5408\u3002\u7136\u800c\u5b9e\u8df5\u4e2d\uff0c\u5b83\u4eec\u5374\u80fd\u7a33\u5065\u5730\u6cdb\u5316\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u8fd9\u79cd\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u8868\u793a\u51e0\u4f55\u7ed3\u6784\u7684\u65b0\u7406\u8bba\u6765\u89e3\u91ca\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u7a00\u758f\u8bed\u4e49\u7ef4\u5ea6\uff08SSD\uff09\u4f5c\u4e3a\u590d\u6742\u6027\u5ea6\u91cf\u6307\u6807\uff0c\u5b83\u662f\u4ece\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u7684\u6d3b\u8dc3\u7279\u5f81\u8bcd\u6c47\u4e2d\u5bfc\u51fa\u7684\u3002\u7814\u7a76\u8005\u5c06LLM\u548cSAE\u89c6\u4e3a\u56fa\u5b9a\u4e0d\u53d8\u7684oracle\uff0c\u5229\u7528\u8fd9\u4e2a\u6846\u67b6\u6765\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0e\u7279\u5f81\u5b57\u5178\u7a00\u758f\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u800c\u975e\u5355\u7eaf\u8003\u8651\u603b\u53c2\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5bf9\u4e8e\u73b0\u5b9e\u6837\u672c\u89c4\u6a21\u4e0b\u7684GPT-2 Small\u548cGemma-2B\u6a21\u578b\u63d0\u4f9b\u4e86\u975e\u7a7a\u6d1e\u7684\u8bc1\u4e66\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5c3d\u7ba1Gemma-2B\u6bd4GPT-2\u5927\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4f46\u5b83\u5728\u786e\u5b9a\u5176\u6d3b\u8dc3\u6d41\u5f62\u65f6\u6240\u9700\u6821\u51c6\u6837\u672c\u663e\u8457\u51cf\u5c11\uff0c\u8fd9\u8868\u660e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u5b66\u4e60\u5230\u4e86\u66f4\u52a0\u53ef\u538b\u7f29\u4e14\u72ec\u7279\u7684\u8bed\u4e49\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u5f53\u8f93\u5165\u8d85\u51fa\u5206\u5e03\u8303\u56f4\u65f6\uff0c\u8be5\u6846\u67b6\u8fd8\u80fd\u4f5c\u4e3a\u53ef\u9760\u7684\u5b89\u5168\u76d1\u6d4b\u624b\u6bb5\uff0c\u901a\u8fc7\u68c0\u6d4b\u5230\u7684\u2018\u7279\u5f81\u7206\u70b8\u2019\u73b0\u8c61\u6765\u53cd\u6620\u6a21\u578b\u7684\u77e5\u8bc6\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u4e0e\u5176\u5185\u90e8\u8868\u793a\u7684\u7a00\u758f\u6027\u548c\u51e0\u4f55\u7ed3\u6784\u6709\u5173\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u53c2\u6570\u6570\u91cf\u3002\u901a\u8fc7\u5f15\u5165\u7a00\u758f\u8bed\u4e49\u7ef4\u5ea6\u8fd9\u4e00\u65b0\u6982\u5ff5\uff0c\u6211\u4eec\u4e0d\u4ec5\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u6a21\u578b\u6cdb\u5316\u7684\u539f\u56e0\uff0c\u800c\u4e14\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u6f5c\u5728\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\u3002"}}
{"id": "2602.11395", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11395", "abs": "https://arxiv.org/abs/2602.11395", "authors": ["Qingsong Wang", "Mikhail Belkin", "Yusu Wang"], "title": "General and Efficient Steering of Unconditional Diffusion", "comment": null, "summary": "Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusion {without gradient guidance during inference}, enabling fast controllable generation. Our approach is built on two observations about diffusion model structure: Noise Alignment: even in early, highly corrupted stages, coarse semantic steering is possible using a lightweight, offline-computed guidance signal, avoiding any per-step or per-sample gradients. Transferable concept vectors: a concept direction in activation space once learned transfers across both {timesteps} and {samples}; the same fixed steering vector learned near low noise level remains effective when injected at intermediate noise levels for every generation trajectory, providing refined conditional control with efficiency. Such concept directions can be efficiently and reliably identified via Recursive Feature Machine (RFM), a light-weight backpropagation-free feature learning method. Experiments on CIFAR-10, ImageNet, and CelebA demonstrate improved accuracy/quality over gradient-based guidance, while achieving significant inference speedups.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u68af\u5ea6\u6307\u5bfc\u5373\u53ef\u9ad8\u6548\u5f15\u5bfc\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u5bf9\u9f50\u548c\u53ef\u8f6c\u79fb\u6982\u5ff5\u5411\u91cf\u5b9e\u73b0\u4e86\u5feb\u901f\u53ef\u63a7\u7684\u751f\u6210\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f15\u5bfc\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u6bcf\u6b65\u8ba1\u7b97\u68af\u5ea6\u5bfc\u81f4\u7684\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "method": "\u5229\u7528\u566a\u58f0\u5bf9\u9f50\u548c\u53ef\u8f6c\u79fb\u6982\u5ff5\u5411\u91cf\u4e24\u4e2a\u89c2\u5bdf\u7ed3\u679c\uff0c\u7ed3\u5408\u9012\u5f52\u7279\u5f81\u673a\uff08RFM\uff09\u6765\u8bc6\u522b\u6982\u5ff5\u65b9\u5411\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6761\u4ef6\u63a7\u5236\u3002", "result": "\u5728CIFAR-10\u3001ImageNet\u548cCelebA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u68af\u5ea6\u7684\u6307\u5bfc\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8d28\u91cf\uff0c\u5e76\u4e14\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u65b9\u5f0f\u6765\u6307\u5bfc\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2602.11399", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.11399", "abs": "https://arxiv.org/abs/2602.11399", "authors": ["Chongyi Zheng", "Royina Karegoudra Jayanth", "Benjamin Eysenbach"], "title": "Can We Really Learn One Representation to Optimize All Rewards?", "comment": null, "summary": "As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u524d\u5411\u540e\u5411\uff08FB\uff09\u8868\u793a\u5b66\u4e60\u7684\u8bad\u7ec3\u76ee\u6807\u548c\u5b66\u4e60\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u79f0\u4e3a\u4e00\u6b65\u524d\u5411\u540e\u5411\u8868\u793a\u5b66\u4e60\uff08one-step FB\uff09\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6267\u884c\u7b56\u7565\u6539\u8fdb\u6b65\u9aa4\uff0c\u5e76\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u9886\u57df\u5f00\u59cb\u5229\u7528\u5927\u578b\u6a21\u578b\u4f5c\u4e3a\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u7684\u57fa\u7840\uff0c\u793e\u533a\u5bf9\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u6700\u4f73\u57fa\u7840\u5f62\u5f0f\u5b58\u5728\u4e89\u8bae\u3002\u5148\u524d\u7684\u5de5\u4f5c\u5c1d\u8bd5\u901a\u8fc7\u4e00\u79cd\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\u6765\u5b9e\u73b0\u5bf9\u4efb\u610f\u5956\u52b1\u7684\u6700\u4f18\u63a7\u5236\uff0c\u800c\u65e0\u9700\u8fdb\u4e00\u6b65\u5fae\u8c03\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u88ab\u79f0\u4e3a\u524d\u5411\u540e\u5411\uff08FB\uff09\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\u5176\u8bad\u7ec3\u76ee\u6807\u53ca\u5b66\u4e60\u673a\u5236\u4ecd\u7136\u4e0d\u591f\u6e05\u6670\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9FB\u8868\u793a\u5b66\u4e60\u7684\u5b58\u5728\u6761\u4ef6\u3001\u4f18\u5316\u76ee\u6807\u53ca\u5176\u5b9e\u9645\u6536\u655b\u60c5\u51b5\u8fdb\u884c\u5206\u6790\uff0c\u5c06\u5176\u4e0e\u6392\u5e8f\u5339\u914d\u3001\u62df\u5408Q\u8bc4\u4f30\u4ee5\u53ca\u6536\u7f29\u6620\u5c04\u7b49\u6982\u5ff5\u76f8\u8054\u7cfb\u3002\u57fa\u4e8e\u6b64\u5206\u6790\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7248\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u2014\u2014\u4e00\u6b65\u524d\u5411\u540e\u5411\u8868\u793a\u5b66\u4e60(one-step FB)\uff0c\u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u8fdb\u884c\u4e00\u6b21\u7b56\u7565\u63d0\u5347\u800c\u975e\u76f4\u63a5\u652f\u6301\u6700\u4f18\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6559\u5b66\u8bbe\u7f6e\u4ee5\u53ca10\u4e2a\u57fa\u4e8e\u72b6\u6001\u548c\u56fe\u50cf\u7684\u8fde\u7eed\u63a7\u5236\u9886\u57df\u5185\uff0cone-step FB\u76f8\u6bd4\u539f\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u5c0f5\u4e2a\u6570\u91cf\u7ea7\u7684\u8bef\u5dee\uff0c\u5e76\u4e14\u5e73\u5747\u63d0\u5347\u4e8624%\u7684\u96f6\u6837\u672c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u6f84\u6e05FB\u8868\u793a\u5b66\u4e60\u80cc\u540e\u7684\u539f\u7406\u5e76\u63d0\u51fa\u7b80\u5316\u7248\u672cone-step FB\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u672a\u89c1\u4efb\u52a1\u4e0a\u6a21\u578b\u7684\u8868\u73b0\u3002"}}
{"id": "2602.11410", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11410", "abs": "https://arxiv.org/abs/2602.11410", "authors": ["David Pardoe", "Neil Daftary", "Miro Furtado", "Aditya Aiyer", "Yu Wang", "Liuqing Li", "Tao Song", "Lars Hertel", "Young Jin Yun", "Senthil Radhakrishnan", "Zhiwei Wang", "Tommy Li", "Khai Tran", "Ananth Nagarajan", "Ali Naqvi", "Yue Zhang", "Renpeng Fang", "Avi Romascanu", "Arjun Kulothungun", "Deepak Kumar", "Praneeth Boda", "Fedor Borisyuk", "Ruoyan Wang"], "title": "CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer", "comment": null, "summary": "Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89e3\u7801\u5668\u4ec5\u53d8\u538b\u5668\u6a21\u578bCADET\uff0c\u7528\u4e8e\u5e7f\u544a\u70b9\u51fb\u7387\u9884\u6d4b\uff0c\u5e76\u5728LinkedIn\u4e0a\u90e8\u7f72\u3002\u901a\u8fc7\u5f15\u5165\u591a\u79cd\u521b\u65b0\u673a\u5236\uff0c\u5982\u57fa\u4e8e\u4e0a\u4e0b\u6587\u6761\u4ef6\u7684\u89e3\u7801\u67b6\u6784\u3001\u81ea\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u7b49\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u751f\u4ea7\u6a21\u578b\u66f4\u9ad8\u7684CTR\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u5728\u5185\u5bb9\u63a8\u8350\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u5c06\u8fd9\u4e9b\u67b6\u6784\u5e94\u7528\u4e8e\u5e7f\u544a\u70b9\u51fb\u7387\uff08CTR\uff09\u9884\u6d4b\u4ecd\u7136\u5b58\u5728\u72ec\u7279\u6311\u6218\uff0c\u6bd4\u5982\u5904\u7406\u8bc4\u5206\u540e\u7684\u4e0a\u4e0b\u6587\u4fe1\u53f7\u3001\u4fdd\u6301\u79bb\u7ebf-\u5728\u7ebf\u4e00\u81f4\u6027\u4ee5\u53ca\u6269\u5c55\u81f3\u5de5\u4e1a\u5de5\u4f5c\u8d1f\u8f7d\u7b49\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86CADET\uff08Context-Conditioned Ads Decoder-Only Transformer\uff09\uff0c\u4e00\u79cd\u4e13\u4e3a\u5e7f\u544aCTR\u9884\u6d4b\u8bbe\u8ba1\u7684\u7aef\u5230\u7aef\u89e3\u7801\u5668\u4ec5\u53d8\u538b\u5668\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e86\u51e0\u4e2a\u5173\u952e\u521b\u65b0\u70b9\uff1a1) \u4e00\u4e2a\u80fd\u591f\u660e\u786e\u5efa\u6a21\u8bf8\u5982\u5e7f\u544a\u4f4d\u7f6e\u8fd9\u7c7b\u8bc4\u5206\u540e\u4fe1\u53f7\u7684\u57fa\u4e8e\u4e0a\u4e0b\u6587\u6761\u4ef6\u7684\u89e3\u7801\u67b6\u6784\uff1b2) \u4e00\u79cd\u53ef\u4ee5\u5728\u8868\u793a\u548c\u4ea4\u4e92\u5c42\u9762\u4e0a\u81ea\u9002\u5e94\u8c03\u8282\u4fe1\u606f\u6d41\u7684\u81ea\u95e8\u63a7\u6ce8\u610f\u673a\u5236\uff1b3) \u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u6233\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u53d8\u4f53\uff0c\u80fd\u591f\u6355\u6349\u4ece\u79d2\u5230\u6708\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u65f6\u95f4\u5173\u7cfb\uff1b4) \u9632\u6b62\u6a21\u578b\u5b66\u4e60\u4e0d\u53ef\u7528\u4f1a\u8bdd\u4e8b\u4ef6\u4f9d\u8d56\u6027\u7684\u4f1a\u8bdd\u5c4f\u853d\u7b56\u7565\uff1b5) \u5305\u62ec\u5f20\u91cf\u6253\u5305\u3001\u5e8f\u5217\u5206\u5757\u53ca\u5b9a\u5236Flash Attention\u5185\u6838\u5728\u5185\u7684\u751f\u4ea7\u5de5\u7a0b\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u9ad8\u6548\u8bad\u7ec3\u548c\u670d\u52a1\u3002", "result": "\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cCADET\u76f8\u6bd4\u73b0\u6709\u7684\u751f\u4ea7\u57fa\u51c6\u6a21\u578bLiRank\uff08DCNv2\u548c\u987a\u5e8f\u7f16\u7801\u5668\u7684\u6df7\u5408\u96c6\u6210\uff09\u5b9e\u73b0\u4e8611.04%\u7684CTR\u63d0\u5347\u3002\u8be5\u7cfb\u7edf\u5df2\u5728LinkedIn\u7684\u5e7f\u544a\u5e73\u53f0\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u670d\u52a1\u4e8e\u4e3b\u9875\u8d5e\u52a9\u66f4\u65b0\u7684\u4e3b\u8981\u6d41\u91cf\u3002", "conclusion": "CADET\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u4e00\u7cfb\u5217\u521b\u65b0\u6027\u6539\u8fdb\uff0c\u5728\u89e3\u51b3\u5e7f\u544aCTR\u9884\u6d4b\u9762\u4e34\u7684\u7279\u5b9a\u6311\u6218\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11448", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.11448", "abs": "https://arxiv.org/abs/2602.11448", "authors": ["Nghia Nguyen", "Tianjiao Ding", "Ren\u00e9 Vidal"], "title": "Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification", "comment": null, "summary": "Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \\& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6HCEP\uff0c\u8be5\u6846\u67b6\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bf1\u5bfc\u6982\u5ff5\u5d4c\u5165\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u5206\u5c42\u7a00\u758f\u7f16\u7801\u6765\u6062\u590d\u56fe\u50cf\u4e2d\u5b58\u5728\u7684\u6982\u5ff5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHCEP\u5728\u6982\u5ff5\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u5206\u7c7b\u51c6\u786e\u5ea6\u3002\u6b64\u5916\uff0c\u5728\u6837\u672c\u6570\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0cHCEP\u8fbe\u5230\u4e86\u66f4\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u5ea6\u548c\u6982\u5ff5\u6062\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7a00\u758f\u6982\u5ff5\u6062\u590d\u7684\u65b9\u6cd5\u5ffd\u7565\u4e86\u6982\u5ff5\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u53ef\u80fd\u5bfc\u81f4\u9884\u6d4b\u6b63\u786e\u4f46\u89e3\u91ca\u4e0e\u5c42\u6b21\u7ed3\u6784\u4e0d\u7b26\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8003\u8651\u6982\u5ff5\u95f4\u5c42\u7ea7\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\u6765\u6539\u8fdb\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86Hierarchical Concept Embedding & Pursuit (HCEP)\u6846\u67b6\uff0c\u5b83\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bf1\u5bfc\u51fa\u6982\u5ff5\u5d4c\u5165\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u5206\u5c42\u7a00\u758f\u7f16\u7801\u6280\u672f\u6765\u4ece\u56fe\u50cf\u4e2d\u6062\u590d\u76f8\u5173\u6982\u5ff5\u3002\u6b64\u65b9\u6cd5\u5047\u8bbe\u7ed9\u5b9a\u56fe\u50cf\u6b63\u786e\u7684\u6982\u5ff5\u6784\u6210\u4e86\u5c42\u6b21\u4e2d\u7684\u6839\u8def\u5f84\uff0c\u5e76\u636e\u6b64\u63a8\u5bfc\u51fa\u4e86\u8bc6\u522b\u8fd9\u4e9b\u6982\u5ff5\u6240\u9700\u6ee1\u8db3\u7684\u7406\u60f3\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5206\u5c42\u7a00\u758f\u7f16\u7801\u80fd\u53ef\u9760\u5730\u6062\u590d\u5c42\u6b21\u5316\u7684\u6982\u5ff5\u5d4c\u5165\uff0c\u800c\u666e\u901a\u7684\u7a00\u758f\u7f16\u7801\u5219\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9\u3002HCEP\u4e0d\u4ec5\u5728\u6982\u5ff5\u7cbe\u5ea6\u548c\u53ec\u56de\u4e0a\u8d85\u8d8a\u4e86\u57fa\u51c6\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u6837\u672c\u91cf\u8f83\u5c11\u65f6\u4e5f\u80fd\u8fbe\u5230\u66f4\u4f18\u7684\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6982\u5ff5\u6062\u590d\u8868\u73b0\u3002", "conclusion": "\u5c06\u5c42\u6b21\u7ed3\u6784\u5f15\u5165\u7a00\u758f\u7f16\u7801\u53ef\u4ee5\u4ea7\u751f\u66f4\u52a0\u53ef\u9760\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u3002HCEP\u8bc1\u660e\u4e86\u901a\u8fc7\u7ed3\u5408\u6982\u5ff5\u95f4\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u89e3\u91ca\u7684\u8d28\u91cf\u3002"}}
{"id": "2602.11465", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11465", "abs": "https://arxiv.org/abs/2602.11465", "authors": ["Jared Levy", "Aarti Lalwani", "Elijah Wyckoff", "Kenneth J. Loh", "Sara P. Gombatto", "Rose Yu", "Emilia Farcas"], "title": "Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning", "comment": null, "summary": "Back pain is a pervasive issue affecting a significant portion of the population, often worsened by certain movements of the lower back. Assessing these movements is important for helping clinicians prescribe appropriate physical therapy. However, it can be difficult to monitor patients' movements remotely outside the clinic. High-fidelity data from motion capture sensors can be used to classify different movements, but these sensors are costly and impractical for use in free-living environments. Motion Tape (MT), a new fabric-based wearable sensor, addresses these issues by being low cost and portable. Despite these advantages, novelty and variability in sensor stability make the MT dataset small scale and inherent to noise. In this work, we propose the Motion-Tape Augmentation Inference Model (MT-AIM), a deep learning classification pipeline trained on MT data. In order to address the challenges of limited sample size and noise present within the MT dataset, MT-AIM leverages conditional generative models to generate synthetic MT data of a desired movement, as well as predicting joint kinematics as additional features. This combination of synthetic data generation and feature augmentation enables MT-AIM to achieve state-of-the-art accuracy in classifying lower back movements, bridging the gap between physiological sensing and movement analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMT-AIM\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u6d41\u7a0b\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u548c\u7279\u5f81\u589e\u5f3a\u6765\u89e3\u51b3\u57fa\u4e8eMotion Tape\uff08\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u4fbf\u643a\u5f0f\u7684\u7ec7\u7269\u53ef\u7a7f\u6234\u4f20\u611f\u5668\uff09\u6570\u636e\u96c6\u6837\u672c\u91cf\u5c0f\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4ece\u800c\u51c6\u786e\u5206\u7c7b\u4e0b\u80cc\u90e8\u8fd0\u52a8\u3002", "motivation": "\u80cc\u75db\u662f\u4e00\u4e2a\u666e\u904d\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u7740\u5927\u91cf\u4eba\u7fa4\uff0c\u800c\u67d0\u4e9b\u4e0b\u80cc\u90e8\u7684\u52a8\u4f5c\u5f80\u5f80\u4f1a\u52a0\u91cd\u8fd9\u79cd\u75bc\u75db\u3002\u4e3a\u4e86\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u5f00\u5177\u5408\u9002\u7684\u7269\u7406\u6cbb\u7597\u65b9\u6848\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u52a8\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u8bca\u6240\u4e4b\u5916\u8fdc\u7a0b\u76d1\u63a7\u60a3\u8005\u7684\u52a8\u4f5c\u5374\u5341\u5206\u56f0\u96be\u3002\u867d\u7136\u9ad8\u4fdd\u771f\u5ea6\u7684\u52a8\u4f5c\u6355\u6349\u4f20\u611f\u5668\u53ef\u7528\u4e8e\u5206\u7c7b\u4e0d\u540c\u7684\u52a8\u4f5c\uff0c\u4f46\u5b83\u4eec\u6210\u672c\u9ad8\u6602\u4e14\u5728\u65e5\u5e38\u751f\u6d3b\u73af\u5883\u4e2d\u4f7f\u7528\u4e0d\u4fbf\u3002\u4e3a\u6b64\uff0c\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7ec7\u7269\u7684\u53ef\u7a7f\u6234\u4f20\u611f\u5668Motion Tape\uff08MT\uff09\u5e94\u8fd0\u800c\u751f\uff0c\u5b83\u5177\u6709\u4f4e\u6210\u672c\u548c\u4fbf\u643a\u6027\u7684\u4f18\u70b9\u3002\u4e0d\u8fc7\uff0c\u7531\u4e8e\u4f20\u611f\u5668\u65b0\u9896\u6027\u548c\u7a33\u5b9a\u6027\u53d8\u5316\u5bfc\u81f4\u7684\u6570\u636e\u96c6\u89c4\u6a21\u8f83\u5c0f\u4ee5\u53ca\u56fa\u6709\u7684\u566a\u97f3\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86Motion-Tape Augmentation Inference Model (MT-AIM)\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3aMT\u6570\u636e\u8bad\u7ec3\u8bbe\u8ba1\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u6d41\u7a0b\u3002\u9488\u5bf9MT\u6570\u636e\u96c6\u4e2d\u6837\u672c\u91cf\u6709\u9650\u53ca\u5b58\u5728\u7684\u566a\u58f0\u95ee\u9898\uff0cMT-AIM\u5229\u7528\u6761\u4ef6\u751f\u6210\u6a21\u578b\u6765\u4ea7\u751f\u6307\u5b9a\u52a8\u4f5c\u6240\u9700\u7684\u5408\u6210MT\u6570\u636e\uff0c\u5e76\u9884\u6d4b\u5173\u8282\u8fd0\u52a8\u5b66\u4f5c\u4e3a\u989d\u5916\u7279\u5f81\u3002", "result": "\u7ed3\u5408\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u4e0e\u7279\u5f81\u589e\u5f3a\u7684\u65b9\u6cd5\u540e\uff0cMT-AIM\u80fd\u591f\u5728\u5206\u7c7b\u4e0b\u80cc\u90e8\u8fd0\u52a8\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u51c6\u786e\u6027\u3002", "conclusion": "MT-AIM\u901a\u8fc7\u514b\u670d\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u548c\u566a\u58f0\u5e26\u6765\u7684\u6311\u6218\uff0c\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u57fa\u4e8eMotion Tape\u4f20\u611f\u5668\u6570\u636e\u5bf9\u4e0b\u80cc\u90e8\u8fd0\u52a8\u8fdb\u884c\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4fc3\u8fdb\u4e86\u751f\u7406\u611f\u77e5\u4e0e\u8fd0\u52a8\u5206\u6790\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2602.11482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11482", "abs": "https://arxiv.org/abs/2602.11482", "authors": ["Kazuki Haishima", "Kyohei Suzuki", "Konstantinos Slavakis"], "title": "External Division of Two Bregman Proximity Operators for Poisson Inverse Problems", "comment": null, "summary": "This paper presents a novel method for recovering sparse vectors from linear models corrupted by Poisson noise. The contribution is twofold. First, an operator defined via the external division of two Bregman proximity operators is introduced to promote sparse solutions while mitigating the estimation bias induced by classical $\\ell_1$-norm regularization. This operator is then embedded into the already established NoLips algorithm, replacing the standard Bregman proximity operator in a plug-and-play manner. Second, the geometric structure of the proposed external-division operator is elucidated through two complementary reformulations, which provide clear interpretations in terms of the primal and dual spaces of the Poisson inverse problem. Numerical tests show that the proposed method exhibits more stable convergence behavior than conventional Kullback-Leibler (KL)-based approaches and achieves significantly superior performance on synthetic data and an image restoration problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u88ab\u6cca\u677e\u566a\u58f0\u7834\u574f\u7684\u7ebf\u6027\u6a21\u578b\u4e2d\u6062\u590d\u7a00\u758f\u5411\u91cf\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u4e24\u4e2aBregman\u90bb\u8fd1\u7b97\u5b50\u5916\u90e8\u9664\u6cd5\u5b9a\u4e49\u7684\u65b0\u7b97\u5b50\u6765\u4fc3\u8fdb\u7a00\u758f\u89e3\uff0c\u5e76\u51cf\u5c11\u7531\u7ecf\u5178\u21131-\u8303\u6570\u6b63\u5219\u5316\u5f15\u8d77\u7684\u4f30\u8ba1\u504f\u5dee\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u884c\u4e3a\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u548c\u56fe\u50cf\u6062\u590d\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5f53\u7ebf\u6027\u6a21\u578b\u53d7\u5230\u6cca\u677e\u566a\u58f0\u5f71\u54cd\u65f6\uff0c\u5982\u4f55\u6709\u6548\u5730\u6062\u590d\u7a00\u758f\u5411\u91cf\u7684\u95ee\u9898\u3002\u7279\u522b\u5730\uff0c\u5b83\u5c1d\u8bd5\u6539\u8fdb\u7531\u4e8e\u4f7f\u7528\u7ecf\u5178\u21131-\u8303\u6570\u6b63\u5219\u5316\u800c\u5bfc\u81f4\u7684\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a\u4e00\u662f\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65b0\u7b97\u5b50\uff0c\u8be5\u7b97\u5b50\u901a\u8fc7\u4e24\u4e2aBregman\u90bb\u8fd1\u7b97\u5b50\u7684\u5916\u90e8\u9664\u6cd5\u5f97\u5230\uff0c\u4ee5\u4fc3\u8fdb\u7a00\u758f\u89e3\u5e76\u51cf\u5c11\u4f30\u8ba1\u504f\u5dee\uff1b\u4e8c\u662f\u5c06\u8fd9\u4e2a\u65b0\u7b97\u5b50\u5d4c\u5165\u5230\u5df2\u6709\u7684NoLips\u7b97\u6cd5\u4e2d\uff0c\u66ff\u6362\u6807\u51c6\u7684Bregman\u90bb\u8fd1\u7b97\u5b50\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u8be5\u5916\u90e8\u9664\u6cd5\u7b97\u5b50\u51e0\u4f55\u7ed3\u6784\u7684\u89e3\u91ca\uff0c\u5e2e\u52a9\u7406\u89e3\u5176\u5728\u6cca\u677e\u9006\u95ee\u9898\u539f\u7a7a\u95f4\u4e0e\u5bf9\u5076\u7a7a\u95f4\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u6570\u503c\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u57fa\u4e8eKullback-Leibler (KL) \u7684\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u52a0\u7a33\u5b9a\u7684\u6536\u655b\u7279\u6027\uff0c\u5e76\u4e14\u5728\u5408\u6210\u6570\u636e\u96c6\u4ee5\u53ca\u5b9e\u9645\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e0a\u83b7\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u57fa\u4e8eBregman\u90bb\u8fd1\u7b97\u5b50\u7684\u5916\u90e8\u9664\u6cd5\u7b97\u5b50\uff0c\u672c\u6587\u4e3a\u4ece\u6cca\u677e\u566a\u58f0\u6c61\u67d3\u7684\u6570\u636e\u4e2d\u6062\u590d\u7a00\u758f\u5411\u91cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u65b9\u6cd5\u3002\u6b64\u65b9\u6cd5\u4e0d\u4ec5\u51cf\u5c11\u4e86\u4f30\u8ba1\u504f\u5dee\uff0c\u800c\u4e14\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e0b\u5747\u663e\u793a\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.11500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11500", "abs": "https://arxiv.org/abs/2602.11500", "authors": ["Diptarka Chakraborty", "Kushagra Chatterjee", "Debarati Das", "Tien-Long Nguyen"], "title": "A Generic Framework for Fair Consensus Clustering in Streams", "comment": "Accepted in AAMAS 2026", "summary": "Consensus clustering seeks to combine multiple clusterings of the same dataset, potentially derived by considering various non-sensitive attributes by different agents in a multi-agent environment, into a single partitioning that best reflects the overall structure of the underlying dataset. Recent work by Chakraborty et al, introduced a fair variant under proportionate fairness and obtained a constant-factor approximation by naively selecting the best closest fair input clustering; however, their offline approach requires storing all input clusterings, which is prohibitively expensive for most large-scale applications.\n  In this paper, we initiate the study of fair consensus clustering in the streaming model, where input clusterings arrive sequentially and memory is limited. We design the first constant-factor algorithm that processes the stream while storing only a logarithmic number of inputs. En route, we introduce a new generic algorithmic framework that integrates closest fair clustering with cluster fitting, yielding improved approximation guarantees not only in the streaming setting but also when revisited offline. Furthermore, the framework is fairness-agnostic: it applies to any fairness definition for which an approximately close fair clustering can be computed efficiently. Finally, we extend our methods to the more general k-median consensus clustering problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u6d41\u6a21\u578b\u4e2d\u5904\u7406\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u7684\u5e38\u6570\u56e0\u5b50\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4ec5\u9700\u5b58\u50a8\u5bf9\u6570\u6570\u91cf\u7684\u8f93\u5165\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u800c\u4e14\u5bf9\u4e8e\u4efb\u4f55\u53ef\u4ee5\u6709\u6548\u8ba1\u7b97\u8fd1\u4f3c\u6700\u8fd1\u516c\u5e73\u805a\u7c7b\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u90fd\u662f\u9002\u7528\u7684\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u867d\u7136\u5728\u6bd4\u4f8b\u516c\u5e73\u4e0b\u5f15\u5165\u4e86\u516c\u5e73\u53d8\u4f53\u5e76\u83b7\u5f97\u4e86\u6052\u5b9a\u56e0\u5b50\u903c\u8fd1\uff0c\u4f46\u5176\u79bb\u7ebf\u65b9\u6cd5\u9700\u8981\u5b58\u50a8\u6240\u6709\u8f93\u5165\u7684\u805a\u7c7b\uff0c\u8fd9\u5bf9\u5927\u591a\u6570\u5927\u89c4\u6a21\u5e94\u7528\u6765\u8bf4\u6210\u672c\u8fc7\u9ad8\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u6709\u9650\u5185\u5b58\u6761\u4ef6\u4e0b\u987a\u5e8f\u5230\u8fbe\u7684\u8f93\u5165\u805a\u7c7b\u7684\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u901a\u7528\u7b97\u6cd5\u6846\u67b6\uff0c\u5c06\u6700\u63a5\u8fd1\u7684\u516c\u5e73\u805a\u7c7b\u4e0e\u805a\u7c7b\u62df\u5408\u76f8\u7ed3\u5408\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9996\u4e2a\u80fd\u591f\u5904\u7406\u6d41\u6a21\u578b\u4e2d\u7684\u6570\u636e\u4e14\u53ea\u9700\u5b58\u50a8\u5c11\u91cf\u8f93\u5165\u7684\u5e38\u6570\u56e0\u5b50\u7b97\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u5904\u7406\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u95ee\u9898\uff0c\u540c\u65f6\u53ea\u5b58\u50a8\u5bf9\u6570\u6570\u91cf\u7684\u8f93\u5165\u3002\u6b64\u6846\u67b6\u8fd8\u80fd\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684k-\u4e2d\u4f4d\u6570\u5171\u8bc6\u805a\u7c7b\u95ee\u9898\uff0c\u5e76\u4e14\u5bf9\u4efb\u4f55\u80fd\u6709\u6548\u8ba1\u7b97\u8fd1\u4f3c\u6700\u8fd1\u516c\u5e73\u805a\u7c7b\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u5747\u9002\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u53ca\u6d41\u6570\u636e\u5904\u7406\u573a\u666f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u8fd1\u4f3c\u5ea6\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2602.11505", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11505", "abs": "https://arxiv.org/abs/2602.11505", "authors": ["Jiangkai Xiong", "Kalyan Talluri", "Hanzhao Wang"], "title": "Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice", "comment": null, "summary": "Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4f01\u4e1a\u96be\u4ee5\u76f4\u63a5\u89c2\u5bdf\u6d88\u8d39\u8005\u662f\u5426\u9009\u62e9\u7ade\u4e89\u5bf9\u624b\u6216\u5b8c\u5168\u4e0d\u8d2d\u4e70\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6821\u51c6\u5916\u90e8\u9009\u9879\u7684\u6982\u7387\u9884\u6d4b\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u4ec5\u6709\u4ea4\u6613\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u4ece\u8d2d\u4e70\u6570\u636e\u4e2d\u5f97\u5230\u6709\u6548\u7684\u65e0\u8d2d\u4e70\u4f30\u8ba1\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u5728\u4e0d\u540c\u6821\u51c6\u6761\u4ef6\u4e0b\u5982\u4f55\u6539\u5584\u65e0\u8d2d\u4e70\u4e8b\u4ef6\u7684\u4f30\u8ba1\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u6539\u8fdb\u5bf9\u540e\u7eed\u51b3\u7b56\uff08\u5982\u5546\u54c1\u7ec4\u5408\u4f18\u5316\uff09\u7684\u5f71\u54cd\u3002", "motivation": "\u4f01\u4e1a\u901a\u5e38\u65e0\u6cd5\u76f4\u63a5\u89c2\u5bdf\u5230\u6d88\u8d39\u8005\u662f\u5426\u9009\u62e9\u4e86\u7ade\u4e89\u5bf9\u624b\u7684\u4ea7\u54c1\u3001\u51b3\u5b9a\u4e0d\u8d2d\u4e70\u6216\u662f\u5f7b\u5e95\u8003\u8651\u4e86\u4f01\u4e1a\u7684\u62a5\u4ef7\u3002\u8fd9\u79cd\u7f3a\u5931\u7684\u5916\u90e8\u9009\u9879\u4fe1\u606f\u4f7f\u5f97\u5e02\u573a\u5927\u5c0f\u548c\u504f\u597d\u4f30\u8ba1\u53d8\u5f97\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u53ea\u6709\u4ea4\u6613\u6570\u636e\u88ab\u8bb0\u5f55\u65f6\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u4e8e\u8f85\u52a9\u5e02\u573a\u4efd\u989d\u3001\u6c47\u603b\u6570\u636e\u6216\u8de8\u5e02\u573a\u6570\u636e\u3002\u7136\u800c\uff0c\u5f53\u4e00\u4e2a\u9ed1\u76d2\u8f85\u52a9\u9884\u6d4b\u5668\u63d0\u4f9b\u7684\u5916\u90e8\u9009\u9879\u6982\u7387\u53ef\u80fd\u56e0\u4e3a\u8bad\u7ec3\u73af\u5883\u4e0e\u5b9e\u9645\u5e94\u7528\u73af\u5883\u7684\u4e0d\u540c\u800c\u5b58\u5728\u504f\u5dee\u65f6\uff0c\u5982\u4f55\u5229\u7528\u8fd9\u6837\u7684\u9884\u6d4b\u8fdb\u884c\u6709\u6548\u6821\u51c6\u6210\u4e3a\u4e86\u7814\u7a76\u7684\u91cd\u70b9\u3002", "method": "\u8be5\u7814\u7a76\u9996\u5148\u5728logit\u7a7a\u95f4\u4e0b\u7684\u4eff\u5c04\u5931\u51c6\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u56de\u5f52\u65b9\u6cd5\u6765\u8bc6\u522b\u5916\u90e8\u9009\u9879\u6548\u7528\u53c2\u6570\uff0c\u5e76\u4f7f\u7528\u7126\u70b9\u73af\u5883\u4e2d\u7684\u4ec5\u8d2d\u4e70\u6570\u636e\u4e00\u81f4\u5730\u6062\u590d\u65e0\u8d2d\u4e70\u6982\u7387\uff0c\u800c\u65e0\u9700\u4e3a\u65e0\u8d2d\u4e70\u4e8b\u4ef6\u6536\u96c6\u65b0\u6807\u7b7e\u3002\u5176\u6b21\uff0c\u5728\u8f83\u5f31\u7684\u8fd1\u4f3c\u5355\u8c03\u6761\u4ef6\u4e0b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6392\u540d\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u51fa\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u9650\uff0c\u660e\u786e\u533a\u5206\u8f85\u52a9\u9884\u6d4b\u5668\u8d28\u91cf\u548c\u7b2c\u4e00\u9636\u6bb5\u89c2\u6d4b\u5185\u96c6\u9009\u62e9\u4e0a\u7684\u6548\u7528\u5b66\u4e60\u9519\u8bef\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5c06\u4f30\u8ba1\u8bef\u5dee\u8f6c\u5316\u4e3a\u4e0b\u6e38\u51b3\u7b56\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5546\u54c1\u7ec4\u5408\u4f18\u5316\u800c\u8a00\uff0c\u91cf\u5316\u4e86\u6821\u51c6\u51c6\u786e\u6027\u5982\u4f55\u5f71\u54cd\u6536\u5165\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u65e0\u8d2d\u4e70\u4e8b\u4ef6\u4f30\u8ba1\u4ee5\u53ca\u968f\u540e\u7684\u5546\u54c1\u7ec4\u5408\u51b3\u7b56\u65b9\u9762\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u6587\u4e2d\u8fd8\u8ba8\u8bba\u4e86\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u8f85\u52a9\u9884\u6d4b\u5668\u6765\u8fdb\u884c\u7a33\u5065\u805a\u5408\u6269\u5c55\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u5957\u80fd\u591f\u5c06\u4e0d\u5b8c\u7f8e\u7684\u9884\u6d4b\u8f6c\u53d8\u4e3a\u7edf\u8ba1\u4e0a\u6709\u6548\u7684\u65e0\u8d2d\u4e70\u4f30\u8ba1\u7684\u65b9\u6cd5\u8bba\u4f53\u7cfb\uff0c\u8fd9\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63d0\u9ad8\u65e0\u8d2d\u4e70\u884c\u4e3a\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4e5f\u4e3a\u4f01\u4e1a\u57fa\u4e8e\u66f4\u51c6\u786e\u7684\u5e02\u573a\u9700\u6c42\u7406\u89e3\u505a\u51fa\u66f4\u597d\u7684\u5546\u4e1a\u51b3\u7b56\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2602.11523", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11523", "abs": "https://arxiv.org/abs/2602.11523", "authors": ["Li He", "Qiang Qu", "He Zhao", "Stephen Wan", "Dadong Wang", "Lina Yao", "Tongliang Liu"], "title": "Unifying Stable Optimization and Reference Regularization in RLHF", "comment": "ICLR 2026", "summary": "Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \\textbf{reward hacking} and \\textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($\u03c0_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($\u03c0_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $\u03c0_0$ and $\u03c0_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u5e73\u8861\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u548c\u4fdd\u6301\u7b56\u7565\u66f4\u65b0\u7a33\u5b9a\u8fd9\u4e24\u4e2a\u76ee\u6807\uff0c\u901a\u8fc7\u52a0\u6743\u76d1\u7763\u5fae\u8c03\u635f\u5931\u5b9e\u73b0\u66f4\u597d\u7684\u6743\u8861\uff0c\u4ece\u800c\u5728\u5bf9\u9f50\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684RLHF\u548c\u5728\u7ebf\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6848\u5206\u522b\u91c7\u7528\u72ec\u7acb\u7684\u6b63\u5219\u5316\u7b56\u7565\u6765\u89e3\u51b3\u5956\u52b1\u9ed1\u5ba2\u548c\u7a33\u5b9a\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u540c\u65f6\u9488\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b$\u03c0_0$\u548c\u5f53\u524d\u7b56\u7565$\u03c0_t$\u8fdb\u884c\u6b63\u5219\u5316\u65f6\u5b58\u5728\u7684\u9690\u542b\u6743\u8861\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u660e\u786e\u5730\u5e73\u8861\u4e86\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u4e0e\u7ef4\u6301\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u7684\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u4f18\u8d8a\u6743\u8861\u6548\u679c\u7684\u52a0\u6743\u76d1\u7763\u5fae\u8c03\u635f\u5931\u51fd\u6570\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8eRLHF\u548c\u5728\u7ebf\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5bf9\u9f50\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u7edf\u4e00\u6b63\u5219\u5316\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5316\u4e86\u5b9e\u65bd\u590d\u6742\u6027\uff0c\u800c\u4e14\u5728\u589e\u5f3a\u5bf9\u9f50\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u8bc1\u4e86\u7a33\u5b9a\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.11530", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11530", "abs": "https://arxiv.org/abs/2602.11530", "authors": ["Eunyeong Cho", "Jehyeon Bang", "Ranggi Hwang", "Minsoo Rhu"], "title": "PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models", "comment": "Accepted for publication at the 32nd IEEE International Symposium on High-Performance Computer Architecture (HPCA-32), 2026", "summary": "The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.", "AI": {"tldr": "PASCAL, a phase-aware scheduling algorithm, optimizes the serving of reasoning-based LLMs by prioritizing the reasoning phase to reduce Time-To-First-Token (TTFT) and using preemption and token pacing during the answering phase to ensure Quality-of-Experience (QoE). It effectively reduces tail TTFT by up to 72% while maintaining service level objectives for the answering phase.", "motivation": "\u968f\u7740\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5229\u7528Chain-of-Thought\uff08CoT\uff09\u63a8\u7406\u7684\u51fa\u73b0\uff0c\u4e3a\u670d\u52a1\u7aef\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ef6\u957f\u4e86\u63a8\u7406\u9636\u6bb5\u4ece\u800c\u5ef6\u8fdf\u4e86\u7528\u6237\u53ef\u89c1\u7684\u8f93\u51fa\uff0c\u5e76\u589e\u52a0\u4e86\u9996\u6b21\u8f93\u51fa\u4ee4\u724c\u7684\u65f6\u95f4\uff08Time-To-First-Token, TTFT\uff09\u3002\u73b0\u6709\u7684LLM\u670d\u52a1\u6846\u67b6\u65e0\u6cd5\u533a\u5206\u63a8\u7406\u548c\u56de\u7b54\u9636\u6bb5\uff0c\u5728GPU\u5185\u5b58\u9650\u5236\u4e0b\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86PASCAL\uff0c\u8fd9\u662f\u4e00\u79cd\u9636\u6bb5\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5148\u5904\u7406\u63a8\u7406\u9636\u6bb5\u6765\u51cf\u5c11TTFT\uff0c\u540c\u65f6\u5728\u56de\u7b54\u9636\u6bb5\u4f7f\u7528\u53d7\u63a7\u62a2\u5360\u548c\u4ee4\u724c\u8282\u6d41\u6765\u4fdd\u6301\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u3002\u8be5\u5c42\u6b21\u7ed3\u6784\u8c03\u5ea6\u5668\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u653e\u7f6e\u4e0e\u5b9e\u4f8b\u5185\u6267\u884c\uff0c\u5e76\u5141\u8bb8\u5728\u9636\u6bb5\u8fb9\u754c\u5904\u52a8\u6001\u8fc1\u79fb\u4ee5\u5e73\u8861\u8d1f\u8f7d\u5e76\u51cf\u5c11\u5e72\u6270\u3002", "result": "\u5728\u4f7f\u7528DeepSeek-R1-Distill-Qwen-32B\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPASCAL\u6700\u591a\u51cf\u5c11\u4e8672%\u7684\u5c3e\u90e8TTFT\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56de\u7b54\u9636\u6bb5\u7684\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u8fbe\u6210\u7387\u3002", "conclusion": "\u9636\u6bb5\u611f\u77e5\u8c03\u5ea6\u5bf9\u4e8e\u57fa\u4e8e\u63a8\u7406\u7684LLM\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0cPASCAL\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u548c\u670d\u52a1\u9636\u6bb5\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002"}}
{"id": "2602.11533", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11533", "abs": "https://arxiv.org/abs/2602.11533", "authors": ["Zhihang Yuan", "Zhiyuan Liu", "Mahesh K. Marina"], "title": "AltTS: A Dual-Path Framework with Alternating Optimization for Multivariate Time Series Forecasting", "comment": "Preprint", "summary": "Multivariate time series forecasting involves two qualitatively distinct factors: (i) stable within-series autoregressive (AR) dynamics, and (ii) intermittent cross-dimension interactions that can become spurious over long horizons. We argue that fitting a single model to capture both effects creates an optimization conflict: the high-variance updates needed for cross-dimension modeling can corrupt the gradients that support autoregression, resulting in brittle training and degraded long-horizon accuracy. To address this, we propose ALTTS, a dual-path framework that explicitly decouples autoregression and cross-relation (CR) modeling. In ALTTS, the AR path is instantiated with a linear predictor, while the CR path uses a Transformer equipped with Cross-Relation Self-Attention (CRSA); the two branches are coordinated via alternating optimization to isolate gradient noise and reduce cross-block interference. Extensive experiments on multiple benchmarks show that ALTTS consistently outperforms prior methods, with the most pronounced improvements on long-horizon forecasting. Overall, our results suggest that carefully designed optimization strategies, rather than ever more complex architectures, can be a key driver of progress in multivariate time series forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aALTTS\u7684\u53cc\u8def\u5f84\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u81ea\u56de\u5f52(AR)\u548c\u4ea4\u53c9\u5173\u7cfb(CR)\u5efa\u6a21\u663e\u5f0f\u89e3\u8026\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u6765\u9694\u79bb\u68af\u5ea6\u566a\u58f0\u5e76\u51cf\u5c11\u8de8\u5757\u5e72\u6270\uff0c\u4ece\u800c\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7279\u522b\u662f\u5728\u957f\u671f\u9884\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6d89\u53ca\u5230\u4e24\u4e2a\u5b9a\u6027\u4e0d\u540c\u7684\u56e0\u7d20\uff1a\uff08i\uff09\u7cfb\u5217\u5185\u90e8\u7a33\u5b9a\u7684\u81ea\u56de\u5f52(AR)\u52a8\u6001\uff1b\uff08ii\uff09\u95f4\u6b47\u6027\u7684\u8de8\u7ef4\u5ea6\u4ea4\u4e92\u4f5c\u7528\uff0c\u5728\u957f\u9884\u6d4b\u8303\u56f4\u5185\u53ef\u80fd\u4f1a\u53d8\u5f97\u865a\u5047\u3002\u8bd5\u56fe\u7528\u5355\u4e00\u6a21\u578b\u540c\u65f6\u6355\u6349\u8fd9\u4e24\u79cd\u6548\u5e94\u4f1a\u5bfc\u81f4\u4f18\u5316\u51b2\u7a81\uff0c\u5f71\u54cd\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86ALTTS\uff0c\u4e00\u4e2a\u660e\u786e\u5206\u79bb\u81ea\u56de\u5f52\u548c\u4ea4\u53c9\u5173\u7cfb(CR)\u5efa\u6a21\u7684\u53cc\u8def\u5f84\u6846\u67b6\u3002\u5176\u4e2d\uff0cAR\u8def\u5f84\u4f7f\u7528\u7ebf\u6027\u9884\u6d4b\u5668\u5b9e\u73b0\uff0c\u800cCR\u8def\u5f84\u5219\u91c7\u7528\u914d\u5907\u4e86\u4ea4\u53c9\u5173\u7cfb\u81ea\u6211\u6ce8\u610f\u673a\u5236(CRSA)\u7684Transformer\u3002\u8fd9\u4e24\u6761\u5206\u652f\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u534f\u8c03\uff0c\u4ee5\u9694\u79bb\u68af\u5ea6\u566a\u58f0\u5e76\u51cf\u5c11\u8de8\u5757\u5e72\u6270\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cALTTS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4f18\u5316\u7b56\u7565\uff0c\u800c\u4e0d\u662f\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u6210\u4e3a\u63a8\u52a8\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8fdb\u5c55\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2602.11534", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11534", "abs": "https://arxiv.org/abs/2602.11534", "authors": ["Jingkun Liu", "Yisong Yue", "Max Welling", "Yue Song"], "title": "Krause Synchronization Transformers", "comment": "Project page: https://jingkun-liu.github.io/krause-sync-transformers/", "summary": "Self-attention in Transformers relies on globally normalized softmax weights, causing all tokens to compete for influence at every layer. When composed across depth, this interaction pattern induces strong synchronization dynamics that favor convergence toward a dominant mode, a behavior associated with representation collapse and attention sink phenomena. We introduce Krause Attention, a principled attention mechanism inspired by bounded-confidence consensus dynamics. Krause Attention replaces similarity-based global aggregation with distance-based, localized, and selectively sparse interactions, promoting structured local synchronization instead of global mixing. We relate this behavior to recent theory modeling Transformer dynamics as interacting particle systems, and show how bounded-confidence interactions naturally moderate attention concentration and alleviate attention sinks. Restricting interactions to local neighborhoods also reduces runtime complexity from quadratic to linear in sequence length. Experiments across vision (ViT on CIFAR/ImageNet), autoregressive generation (MNIST/CIFAR-10), and large language models (Llama/Qwen) demonstrate consistent gains with substantially reduced computation, highlighting bounded-confidence dynamics as a scalable and effective inductive bias for attention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u2014\u2014Krause Attention\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u8ddd\u79bb\u7684\u5c40\u90e8\u548c\u9009\u62e9\u6027\u7a00\u758f\u4ea4\u4e92\u6765\u66ff\u4ee3\u4f20\u7edf\u7684\u5168\u5c40\u805a\u5408\u65b9\u5f0f\uff0c\u4ece\u800c\u4fc3\u8fdb\u7ed3\u6784\u5316\u7684\u5c40\u90e8\u540c\u6b65\u800c\u975e\u5168\u5c40\u6df7\u5408\u3002\u8fd9\u79cd\u673a\u5236\u4e0d\u4ec5\u6709\u52a9\u4e8e\u7f13\u89e3\u6ce8\u610f\u529b\u96c6\u4e2d\u95ee\u9898\uff0c\u8fd8\u80fd\u5c06\u8fd0\u884c\u65f6\u95f4\u590d\u6742\u5ea6\u4ece\u5e8f\u5217\u957f\u5ea6\u7684\u5e73\u65b9\u964d\u4f4e\u5230\u7ebf\u6027\u7ea7\u522b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u3001\u81ea\u56de\u5f52\u751f\u6210\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u4f18\u52bf\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u3002", "motivation": "\u4f5c\u8005\u6ce8\u610f\u5230\uff0cTransformer\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4f9d\u8d56\u4e8e\u5168\u5c40\u5f52\u4e00\u5316\u7684softmax\u6743\u91cd\uff0c\u5bfc\u81f4\u6240\u6709token\u5728\u6bcf\u4e00\u5c42\u90fd\u7ade\u4e89\u5f71\u54cd\u529b\u3002\u8fd9\u79cd\u8de8\u6df1\u5ea6\u7ec4\u6210\u7684\u4e92\u52a8\u6a21\u5f0f\u4f1a\u5bfc\u81f4\u5f3a\u70c8\u7684\u540c\u6b65\u52a8\u529b\u5b66\uff0c\u503e\u5411\u4e8e\u6536\u655b\u5230\u4e00\u4e2a\u4e3b\u5bfc\u6a21\u5f0f\uff0c\u8fd9\u4e0e\u8868\u793a\u5d29\u6e83\u548c\u6ce8\u610f\u529b\u6c89\u9677\u73b0\u8c61\u6709\u5173\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Krause Attention\u673a\u5236\u3002", "method": "Krause Attention\u53d7\u5230\u6709\u754c\u4fe1\u5fc3\u5171\u8bc6\u52a8\u529b\u5b66\u542f\u53d1\uff0c\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u5c40\u90e8\u5316\u4e14\u9009\u62e9\u6027\u7a00\u758f\u7684\u4ea4\u4e92\u4ee3\u66ff\u4e86\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u5168\u5c40\u805a\u5408\u3002\u8fd9\u6837\u7684\u8bbe\u8ba1\u4fc3\u8fdb\u4e86\u7ed3\u6784\u5316\u7684\u5c40\u90e8\u540c\u6b65\u800c\u4e0d\u662f\u5168\u5c40\u6df7\u5408\uff0c\u5e76\u4e14\u81ea\u7136\u5730\u8c03\u8282\u4e86\u6ce8\u610f\u529b\u96c6\u4e2d\u5ea6\uff0c\u51cf\u8f7b\u4e86\u6ce8\u610f\u529b\u6c89\u9677\u95ee\u9898\u3002\u6b64\u5916\uff0c\u9650\u5236\u5728\u5c40\u90e8\u90bb\u57df\u5185\u7684\u4ea4\u4e92\u8fd8\u5c06\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u4ece\u5e8f\u5217\u957f\u5ea6\u7684\u4e8c\u6b21\u65b9\u51cf\u5c11\u5230\u4e86\u7ebf\u6027\u7ea7\u522b\u3002", "result": "\u5b9e\u9a8c\u6db5\u76d6\u4e86\u89c6\u89c9\uff08ViT\u5728CIFAR/ImageNet\u4e0a\u7684\u8868\u73b0\uff09\u3001\u81ea\u56de\u5f52\u751f\u6210\uff08MNIST/CIFAR-10\uff09\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama/Qwen\uff09\uff0c\u7ed3\u679c\u8868\u660eKrause Attention\u80fd\u591f\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5927\u5e45\u5ea6\u51cf\u5c11\u4e86\u6240\u9700\u7684\u8ba1\u7b97\u91cf\u3002", "conclusion": "Krause Attention\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5bf9\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u6765\u8bf4\u662f\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u6709\u754c\u4fe1\u5fc3\u52a8\u6001\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5b58\u5728\u7684\u95ee\u9898\u3002"}}
{"id": "2602.11549", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11549", "abs": "https://arxiv.org/abs/2602.11549", "authors": ["Yuanfu Wang", "Zhixuan Liu", "Xiangtian Li", "Chaochao Lu", "Chao Yang"], "title": "Native Reasoning Models: Training Language Models to Reason on Unverifiable Data", "comment": "Accepted at ICLR 2026", "summary": "The prevailing paradigm for training large reasoning models--combining Supervised Fine-Tuning (SFT) with Reinforcement Learning with Verifiable Rewards (RLVR)--is fundamentally constrained by its reliance on high-quality, human-annotated reasoning data and external verifiers. This dependency incurs significant data-collection costs, risks embedding human cognitive biases, and confines the reinforcement learning stage to objectively assessable domains like mathematics and coding, leaving a wide range of unverifiable tasks beyond its scope. To overcome these limitations, we introduce NRT (Native Reasoning Training), a novel framework that cultivates complex reasoning by having the model generate its own reasoning traces using only standard question-answer pairs, thereby obviating the need for expert-written demonstrations. NRT reframes the training problem by treating the reasoning process as a latent variable. It employs a unified training objective that models reasoning as an optimization problem, intrinsically rewarding paths that increase the model's likelihood of producing the ground-truth answer. This unified perspective allows us to analyze intrinsic failure modes of prior methods, such as policy collapse, and systematically design more robust reward aggregation functions, creating a self-reinforcing feedback loop where the model learns to think in ways that resolve its own uncertainty. Empirical evaluation on Llama and Mistral model families demonstrates that NRT achieves state-of-the-art performance among verifier-free methods, significantly outperforming standard SFT baselines and prior verifier-free RL methods. Our approach yields particularly strong performance gains in complex reasoning domains and exhibits high robustness to policy collapse, offering a general, scalable path toward building more powerful and broadly applicable reasoning systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6NRT\uff08\u539f\u751f\u63a8\u7406\u8bad\u7ec3\uff09\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4ec5\u4f7f\u7528\u6807\u51c6\u95ee\u7b54\u5bf9\u8ba9\u6a21\u578b\u751f\u6210\u81ea\u5df1\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u6446\u8131\u4e86\u5bf9\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u548c\u5916\u90e8\u9a8c\u8bc1\u5668\u7684\u4f9d\u8d56\u3002\u5b9e\u9a8c\u8868\u660e\uff0cNRT\u5728\u590d\u6742\u63a8\u7406\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u907f\u514d\u7b56\u7565\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u4eba\u5de5\u6807\u6ce8\u63a8\u7406\u6570\u636e\u548c\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u8fd9\u4e0d\u4ec5\u589e\u52a0\u4e86\u6570\u636e\u6536\u96c6\u6210\u672c\uff0c\u8fd8\u53ef\u80fd\u5d4c\u5165\u4eba\u7c7b\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u5e76\u5c06\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u9650\u5236\u5728\u6570\u5b66\u3001\u7f16\u7801\u7b49\u5ba2\u89c2\u53ef\u8bc4\u4f30\u9886\u57df\u5185\uff0c\u65e0\u6cd5\u8986\u76d6\u5927\u91cf\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u3002", "method": "NRT (Native Reasoning Training)\u6846\u67b6\u5c06\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u4e00\u4e2a\u6f5c\u5728\u53d8\u91cf\u6765\u5904\u7406\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5185\u5728\u5956\u52b1\u90a3\u4e9b\u589e\u52a0\u6a21\u578b\u4ea7\u751f\u6b63\u786e\u7b54\u6848\u53ef\u80fd\u6027\u7684\u8def\u5f84\u3002\u8fd9\u4e00\u65b9\u6cd5\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u5206\u6790\u5148\u524d\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u56fa\u6709\u5931\u8d25\u6a21\u5f0f\uff0c\u5982\u653f\u7b56\u5d29\u6e83\uff0c\u5e76\u7cfb\u7edf\u5730\u8bbe\u8ba1\u66f4\u52a0\u7a33\u5065\u7684\u5956\u52b1\u805a\u5408\u51fd\u6570\uff0c\u5f62\u6210\u4e00\u79cd\u81ea\u6211\u5f3a\u5316\u53cd\u9988\u5faa\u73af\uff0c\u4f7f\u6a21\u578b\u5b66\u4f1a\u4ee5\u89e3\u51b3\u81ea\u8eab\u4e0d\u786e\u5b9a\u6027\u7684\u601d\u8003\u65b9\u5f0f\u3002", "result": "\u57fa\u4e8eLlama\u548cMistral\u6a21\u578b\u5bb6\u65cf\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cNRT\u5728\u65e0\u9a8c\u8bc1\u5668\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u57fa\u7ebf\u4ee5\u53ca\u5176\u4ed6\u65e0\u9a8c\u8bc1\u5668\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cNRT\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u663e\u793a\u51fa\u5bf9\u6297\u7b56\u7565\u5d29\u6e83\u7684\u9ad8\u5ea6\u9c81\u68d2\u6027\u3002", "conclusion": "NRT\u63d0\u4f9b\u4e86\u4e00\u6761\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u9053\u8def\uff0c\u7528\u4e8e\u6784\u5efa\u66f4\u5f3a\u5927\u4e14\u5e94\u7528\u8303\u56f4\u66f4\u5e7f\u7684\u63a8\u7406\u7cfb\u7edf\u3002"}}
{"id": "2602.11550", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11550", "abs": "https://arxiv.org/abs/2602.11550", "authors": ["Sisuo Lyu", "Siru Zhong", "Tiegang Chen", "Weilin Ruan", "Qingxiang Liu", "Taiqiang Lv", "Qingsong Wen", "Raymond Chi-Wing Wong", "Yuxuan Liang"], "title": "TS-Memory: Plug-and-Play Memory for Time Series Foundation Models", "comment": null, "summary": "Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires costly multi-domain maintenance, while Non-Parametric Retrieval improves forecasts but incurs high inference latency due to datastore search. We propose Parametric Memory Distillation and implement it as TS-Memory, a lightweight memory adapter that augments frozen TSFMs. TS-Memory is trained in two stages. First, we construct an offline, leakage-safe kNN teacher that synthesizes confidence-aware quantile targets from retrieved futures. Second, we distill this retrieval-induced distributional correction into a lightweight memory adapter via confidence-gated supervision. During inference, TS-Memory fuses memory and backbone predictions with constant-time overhead, enabling retrieval-free deployment. Experiments across diverse TSFMs and benchmarks demonstrate consistent improvements in both point and probabilistic forecasting over representative adaptation methods, with efficiency comparable to the frozen backbone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTS-Memory\u7684\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u9002\u914d\u5668\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\u5c06\u68c0\u7d22\u5f15\u8d77\u7684\u5206\u5e03\u6821\u6b63\u84b8\u998f\u5230\u9002\u914d\u5668\u4e2d\uff0c\u4ece\u800c\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u4e0b\u6e38\u9886\u57df\u65f6\u9762\u4e34\u53c2\u6570\u8c03\u6574\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u548c\u975e\u53c2\u6570\u68c0\u7d22\u9020\u6210\u9ad8\u63a8\u7406\u5ef6\u8fdf\u7684\u95ee\u9898\u3002", "method": "Parametric Memory Distillation\uff0c\u5177\u4f53\u5b9e\u73b0\u4e3aTS-Memory\uff0c\u5305\u62ec\u6784\u5efa\u4e00\u4e2a\u79bb\u7ebf\u3001\u9632\u6cc4\u6f0fkNN\u6559\u5e08\u4ee5\u5408\u6210\u7f6e\u4fe1\u5ea6\u611f\u77e5\u5206\u4f4d\u6570\u76ee\u6807\uff0c\u4ee5\u53ca\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u76d1\u7763\u5c06\u6b64\u6821\u6b63\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u9002\u914d\u5668\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTS-Memory\u5728\u4e0d\u540cTSFMs\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u70b9\u9884\u6d4b\u548c\u6982\u7387\u9884\u6d4b\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u6548\u7387\u4e0e\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "TS-Memory\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8TSFMs\u5728\u9762\u5bf9\u5206\u5e03\u504f\u79fb\u65f6\u5bf9\u4e0b\u6e38\u9886\u57df\u7684\u9002\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u3002"}}
{"id": "2602.11557", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11557", "abs": "https://arxiv.org/abs/2602.11557", "authors": ["Jichu Li", "Xuan Tang", "Difan Zou"], "title": "The Implicit Bias of Steepest Descent with Mini-batch Stochastic Gradient", "comment": null, "summary": "A variety of widely used optimization methods like SignSGD and Muon can be interpreted as instances of steepest descent under different norm-induced geometries. In this work, we study the implicit bias of mini-batch stochastic steepest descent in multi-class classification, characterizing how batch size, momentum, and variance reduction shape the limiting max-margin behavior and convergence rates under general entry-wise and Schatten-$p$ norms. We show that without momentum, convergence only occurs with large batches, yielding a batch-dependent margin gap but the full-batch convergence rate. In contrast, momentum enables small-batch convergence through a batch-momentum trade-off, though it slows convergence. This approach provides fully explicit, dimension-free rates that improve upon prior results. Moreover, we prove that variance reduction can recover the exact full-batch implicit bias for any batch size, albeit at a slower convergence rate. Finally, we further investigate the batch-size-one steepest descent without momentum, and reveal its convergence to a fundamentally different bias via a concrete data example, which reveals a key limitation of purely stochastic updates. Overall, our unified analysis clarifies when stochastic optimization aligns with full-batch behavior, and paves the way for perform deeper explorations of the training behavior of stochastic gradient steepest descent algorithms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5c0f\u6279\u91cf\u968f\u673a\u6700\u901f\u4e0b\u964d\u6cd5\u7684\u9690\u5f0f\u504f\u5dee\u5982\u4f55\u53d7\u5230\u6279\u91cf\u5927\u5c0f\u3001\u52a8\u91cf\u548c\u65b9\u5dee\u51cf\u5c11\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6ca1\u6709\u52a8\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u53ea\u6709\u5927\u6279\u6b21\u624d\u80fd\u6536\u655b\uff1b\u800c\u52a8\u91cf\u53ef\u4ee5\u5b9e\u73b0\u5c0f\u6279\u6b21\u6536\u655b\u4f46\u4f1a\u51cf\u6162\u6536\u655b\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u65b9\u5dee\u51cf\u5c11\u80fd\u591f\u5728\u4efb\u4f55\u6279\u6b21\u5927\u5c0f\u4e0b\u6062\u590d\u5b8c\u6574\u7684\u6279\u6b21\u9690\u5f0f\u504f\u5dee\uff0c\u5c3d\u7ba1\u6536\u655b\u901f\u5ea6\u8f83\u6162\u3002\u901a\u8fc7\u7edf\u4e00\u5206\u6790\uff0c\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u968f\u673a\u4f18\u5316\u4f55\u65f6\u4e0e\u5168\u6279\u91cf\u884c\u4e3a\u4e00\u81f4\u63d0\u4f9b\u4e86\u6e05\u6670\u89c6\u89d2\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u968f\u673a\u68af\u5ea6\u6700\u901f\u4e0b\u964d\u7b97\u6cd5\u7684\u8bad\u7ec3\u884c\u4e3a\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u4e0d\u540c\u8303\u6570\u8bf1\u5bfc\u51e0\u4f55\u4e0b\u89e3\u91ca\u7684\u4e00\u7cfb\u5217\u5e7f\u6cdb\u4f7f\u7528\u7684\u4f18\u5316\u65b9\u6cd5\uff08\u5982SignSGD\u548cMuon\uff09\u5728\u591a\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u9690\u5f0f\u504f\u5dee\u7279\u6027\uff0c\u7279\u522b\u662f\u6279\u91cf\u5927\u5c0f\u3001\u52a8\u91cf\u4ee5\u53ca\u65b9\u5dee\u51cf\u5c11\u5bf9\u6700\u7ec8\u6700\u5927\u8fb9\u754c\u884c\u4e3a\u53ca\u6536\u655b\u901f\u7387\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u624b\u6bb5\uff0c\u7814\u7a76\u4e86\u5728\u6ca1\u6709\u52a8\u91cf\u60c5\u51b5\u4e0b\u4ec5\u5927\u6279\u6b21\u80fd\u5bfc\u81f4\u6536\u655b\u7684\u73b0\u8c61\uff0c\u540c\u65f6\u4e5f\u63a2\u8ba8\u4e86\u52a8\u91cf\u5982\u4f55\u5141\u8bb8\u5c0f\u6279\u6b21\u6536\u655b\u4f46\u4ee5\u727a\u7272\u90e8\u5206\u6536\u655b\u901f\u5ea6\u4e3a\u4ee3\u4ef7\u7684\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u65b9\u5dee\u51cf\u5c11\u6280\u672f\u53ef\u4ee5\u5728\u4efb\u4f55\u6279\u6b21\u89c4\u6a21\u4e0b\u6062\u590d\u5b8c\u5168\u6279\u6b21\u5904\u7406\u7684\u9690\u5f0f\u504f\u5dee\uff0c\u867d\u7136\u8fd9\u6837\u505a\u4f1a\u964d\u4f4e\u6536\u655b\u901f\u7387\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u52a8\u91cf\u65f6\u9700\u8981\u8f83\u5927\u7684\u6279\u6b21\u624d\u80fd\u4fdd\u8bc1\u6536\u655b\u6027\uff1b\u5f15\u5165\u52a8\u91cf\u540e\uff0c\u5373\u4f7f\u4f7f\u7528\u8f83\u5c0f\u7684\u6279\u6b21\u4e5f\u80fd\u5b9e\u73b0\u6536\u655b\uff0c\u4f46\u6536\u655b\u901f\u5ea6\u6709\u6240\u51cf\u6162\u3002\u65b9\u5dee\u51cf\u5c11\u7b56\u7565\u80fd\u591f\u5bf9\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u6279\u6b21\u91cd\u73b0\u5168\u6279\u6b21\u5904\u7406\u4e0b\u7684\u9690\u5f0f\u504f\u5dee\uff0c\u4e0d\u8fc7\u8fd9\u540c\u6837\u4f34\u968f\u7740\u6536\u655b\u901f\u7387\u7684\u4e0b\u964d\u3002\u7279\u522b\u5730\uff0c\u5f53\u6279\u6b21\u5927\u5c0f\u4e3a1\u4e14\u4e0d\u4f7f\u7528\u52a8\u91cf\u65f6\uff0c\u89c2\u5bdf\u5230\u4e86\u4e00\u79cd\u672c\u8d28\u4e0a\u4e0d\u540c\u7684\u6536\u655b\u504f\u79fb\u3002", "conclusion": "\u6574\u4f53\u800c\u8a00\uff0c\u8fd9\u9879\u5de5\u4f5c\u7684\u7efc\u5408\u5206\u6790\u63ed\u793a\u4e86\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u80fd\u591f\u4e0e\u5168\u6279\u91cf\u5904\u7406\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u5e76\u4e3a\u66f4\u6df1\u5165\u5730\u63a2\u7a76\u968f\u673a\u68af\u5ea6\u6700\u901f\u4e0b\u964d\u7b97\u6cd5\u7684\u8bad\u7ec3\u52a8\u6001\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.11558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11558", "abs": "https://arxiv.org/abs/2602.11558", "authors": ["Fanqi Shen", "Enhong Yang", "Jiahe Li", "Junru Hong", "Xiaoran Pan", "Zhizhang Yuan", "Meng Li", "Yang Yang"], "title": "Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal", "comment": null, "summary": "Brain Foundation Models (BFMs) are transforming neuroscience by enabling scalable and transferable learning from neural signals, advancing both clinical diagnostics and cutting-edge neuroscience exploration. Their emergence is powered by large-scale clinical recordings, particularly electroencephalography (EEG) and intracranial EEG, which provide rich temporal and spatial representations of brain dynamics. However, despite their rapid proliferation, the field lacks a unified understanding of existing methodologies and a standardized evaluation framework. To fill this gap, we map the benchmark design space along two axes: (i) from the model perspective, we organize BFMs under a self-supervised learning (SSL) taxonomy; and (ii) from the dataset perspective, we summarize common downstream tasks and curate representative public datasets across clinical and human-centric neurotechnology applications. Building on this consolidation, we introduce Brain4FMs, an open evaluation platform with plug-and-play interfaces that integrates 15 representative BFMs and 18 public datasets. It enables standardized comparisons and analysis of how pretraining data, SSL strategies, and architectures affect generalization and downstream performance, guiding more accurate and transferable BFMs. The code is available at https://anonymous.4open.science/r/Brain4FMs-85B8.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u8111\u57fa\u7840\u6a21\u578b\uff08BFMs\uff09\u5728\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBrain4FMs\u7684\u5f00\u653e\u8bc4\u4f30\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u6574\u5408\u4e8615\u4e2a\u4ee3\u8868\u6027\u7684BFMs\u548c18\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4ee5\u6807\u51c6\u5316\u6bd4\u8f83\u548c\u5206\u6790\u9884\u8bad\u7ec3\u6570\u636e\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u53ca\u67b6\u6784\u5bf9\u6cdb\u5316\u6027\u548c\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u8111\u57fa\u7840\u6a21\u578b\uff08BFMs\uff09\u5728\u4e34\u5e8a\u8bca\u65ad\u548c\u524d\u6cbf\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9886\u57df\u5185\u7f3a\u4e4f\u7edf\u4e00\u7684\u65b9\u6cd5\u8bba\u7406\u89e3\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6587\u7ae0\u65e8\u5728\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u5f00\u653e\u7684\u8bc4\u4f30\u5e73\u53f0\u6765\u586b\u8865\u7a7a\u767d\u3002", "method": "\u6587\u7ae0\u9996\u5148\u4ece\u6a21\u578b\u89c6\u89d2\u51fa\u53d1\uff0c\u5c06BFMs\u5f52\u7c7b\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5206\u7c7b\u4f53\u7cfb\u4e0b\uff1b\u5176\u6b21\u4ece\u6570\u636e\u96c6\u89c6\u89d2\uff0c\u603b\u7ed3\u4e86\u5e38\u89c1\u7684\u4e0b\u6e38\u4efb\u52a1\u5e76\u6574\u7406\u4e86\u4e34\u5e8a\u4e0e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u795e\u7ecf\u6280\u672f\u5e94\u7528\u4e2d\u5177\u6709\u4ee3\u8868\u6027\u7684\u516c\u5f00\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u6b64\u6574\u5408\u5de5\u4f5c\uff0c\u5f00\u53d1\u4e86Brain4FMs\u5e73\u53f0\uff0c\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u63a5\u53e3\uff0c\u96c6\u6210\u4e8615\u4e2aBFMs\u548c18\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u3002", "result": "Brain4FMs\u5e73\u53f0\u652f\u6301\u6807\u51c6\u5316\u6bd4\u8f83\u4e0d\u540c\u9884\u8bad\u7ec3\u6570\u636e\u3001SSL\u7b56\u7565\u4ee5\u53ca\u67b6\u6784\u5bf9\u4e8e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u53ca\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u66f4\u51c6\u786e\u4e14\u53ef\u8fc1\u79fb\u7684BFM\u7684\u53d1\u5c55\u3002", "conclusion": "\u901a\u8fc7Brain4FMs\u5e73\u53f0\u7684\u63a8\u51fa\uff0c\u7814\u7a76\u8005\u4eec\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3BFMs\u7684\u8bbe\u8ba1\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u5176\u6027\u80fd\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u671d\u5411\u66f4\u52a0\u6807\u51c6\u5316\u548c\u9ad8\u6548\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.11584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11584", "abs": "https://arxiv.org/abs/2602.11584", "authors": ["Yujie Gu", "Richeng Jin", "Zhaoyang Zhang", "Huaiyu Dai"], "title": "Gradient Compression May Hurt Generalization: A Remedy by Synthetic Data Guided Sharpness Aware Minimization", "comment": null, "summary": "It is commonly believed that gradient compression in federated learning (FL) enjoys significant improvement in communication efficiency with negligible performance degradation. In this paper, we find that gradient compression induces sharper loss landscapes in federated learning, particularly under non-IID data distributions, which suggests hindered generalization capability. The recently emerging Sharpness Aware Minimization (SAM) effectively searches for a flat minima by incorporating a gradient ascent step (i.e., perturbing the model with gradients) before the celebrated stochastic gradient descent. Nonetheless, the direct application of SAM in FL suffers from inaccurate estimation of the global perturbation due to data heterogeneity. Existing approaches propose to utilize the model update from the previous communication round as a rough estimate. However, its effectiveness is hindered when model update compression is incorporated. In this paper, we propose FedSynSAM, which leverages the global model trajectory to construct synthetic data and facilitates an accurate estimation of the global perturbation. The convergence of the proposed algorithm is established, and extensive experiments are conducted to validate its effectiveness.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u538b\u7f29\u4f1a\u5bfc\u81f4\u635f\u5931\u666f\u89c2\u53d8\u5f97\u66f4\u5c16\u9510\uff0c\u7279\u522b\u662f\u5728\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86FedSynSAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5168\u5c40\u6a21\u578b\u8f68\u8ff9\u6765\u6784\u5efa\u5408\u6210\u6570\u636e\uff0c\u4ece\u800c\u51c6\u786e\u4f30\u8ba1\u5168\u5c40\u6270\u52a8\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u6307\u51fa\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u4f7f\u7528\u68af\u5ea6\u538b\u7f29\u867d\u7136\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\uff0c\u4f46\u4f1a\u4f7f\u5f97\u635f\u5931\u666f\u89c2\u53d8\u5f97\u66f4\u4e3a\u5c16\u9510\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u6570\u636e\u65f6\uff0c\u8fd9\u53ef\u80fd\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u800c\u76f4\u63a5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5e94\u7528Sharpness Aware Minimization (SAM)\u65b9\u6cd5\u7531\u4e8e\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\u9762\u4e34\u5168\u7403\u6270\u52a8\u4f30\u8ba1\u4e0d\u51c6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedSynSAM\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5168\u5c40\u6a21\u578b\u8f68\u8ff9\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u8fdb\u800c\u4fc3\u8fdb\u5bf9\u5168\u5c40\u6270\u52a8\u7684\u51c6\u786e\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u5206\u6790\u4e0a\u786e\u7acb\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86FedSynSAM\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedSynSAM\u4e3a\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u68af\u5ea6\u538b\u7f29\u5bfc\u81f4\u7684\u635f\u5931\u666f\u89c2\u5c16\u9510\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5904\u7406\u975eIID\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u6311\u6218\u3002"}}
{"id": "2602.11615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11615", "abs": "https://arxiv.org/abs/2602.11615", "authors": ["Naveen Sahi", "Jeremy Dohmann", "Armen Aghajanyan", "Akshat Shrivastava"], "title": "SkillRater: Untangling Capabilities in Multimodal Data", "comment": null, "summary": "Data curation methods typically assign samples a single quality score. We argue this scalar framing is fundamentally limited: when training requires multiple distinct capabilities, a monolithic scorer cannot maximize useful signals for all of them simultaneously. Quality is better understood as multidimensional, with each dimension corresponding to a capability the model must acquire. We introduce SkillRater, a framework that decomposes data filtering into specialized raters - one per capability, each trained via meta-learning on a disjoint validation objective - and composes their scores through a progressive selection rule: at each training stage, a sample is retained if any rater ranks it above a threshold that tightens over time, preserving diversity early while concentrating on high-value samples late. We validate this approach on vision language models, decomposing quality into three capability dimensions: visual understanding, OCR, and STEM reasoning. At 2B parameters, SkillRater improves over unfiltered baselines by 5.63% on visual understanding, 2.00% on OCR, and 3.53% on STEM on held out benchmarks. The learned rater signals are near orthogonal, confirming that the decomposition captures genuinely independent quality dimensions and explaining why it outperforms both unfiltered training and monolithic learned filtering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSkillRater\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5c06\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u5206\u89e3\u4e3a\u591a\u4e2a\u7ef4\u5ea6\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u5bf9\u5e94\u6a21\u578b\u9700\u8981\u83b7\u5f97\u7684\u4e00\u79cd\u80fd\u529b\u3002\u901a\u8fc7\u8fd9\u79cd\u591a\u7ef4\u5ea6\u7684\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u672a\u8fc7\u6ee4\u7684\u57fa\u7ebf\u548c\u5355\u4e00\u8bc4\u5206\u65b9\u6cd5\uff0cSkillRater\u5728\u89c6\u89c9\u7406\u89e3\u3001OCR\u4ee5\u53caSTEM\u63a8\u7406\u7b49\u4e0d\u540c\u80fd\u529b\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u7ba1\u7406\u65b9\u6cd5\u901a\u5e38\u4ec5\u7ed9\u6837\u672c\u5206\u914d\u4e00\u4e2a\u8d28\u91cf\u5206\u6570\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5f53\u8bad\u7ec3\u9700\u8981\u591a\u79cd\u4e0d\u540c\u7684\u80fd\u529b\u65f6\uff0c\u5355\u4e00\u7684\u8bc4\u5206\u673a\u5236\u65e0\u6cd5\u540c\u65f6\u6700\u5927\u5316\u6240\u6709\u8fd9\u4e9b\u80fd\u529b\u6240\u9700\u7684\u6709\u7528\u4fe1\u53f7\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8d28\u91cf\u5e94\u8be5\u662f\u591a\u7ef4\u7684\u6982\u5ff5\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u90fd\u5bf9\u5e94\u7740\u6a21\u578b\u5fc5\u987b\u638c\u63e1\u7684\u4e00\u79cd\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86SkillRater\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u6570\u636e\u7b5b\u9009\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u8bc4\u4ef7\u8005\u2014\u2014\u6bcf\u79cd\u80fd\u529b\u4e00\u4e2a\uff0c\u6bcf\u4e2a\u90fd\u662f\u901a\u8fc7\u5143\u5b66\u4e60\u5728\u4e0d\u76f8\u4ea4\u7684\u9a8c\u8bc1\u76ee\u6807\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684\u2014\u2014\u5e76\u901a\u8fc7\u4e00\u79cd\u6e10\u8fdb\u7684\u9009\u62e9\u89c4\u5219\u6765\u7efc\u5408\u4ed6\u4eec\u7684\u8bc4\u5206\uff1a\u5728\u6bcf\u4e2a\u8bad\u7ec3\u9636\u6bb5\uff0c\u5982\u679c\u4efb\u4f55\u4e00\u4e2a\u8bc4\u4ef7\u8005\u7684\u8bc4\u5206\u9ad8\u4e8e\u968f\u65f6\u95f4\u9010\u6e10\u63d0\u9ad8\u7684\u9608\u503c\uff0c\u5219\u4fdd\u7559\u8be5\u6837\u672c\uff0c\u8fd9\u6837\u65e2\u4fdd\u6301\u4e86\u65e9\u671f\u7684\u591a\u6837\u6027\u53c8\u5728\u540e\u671f\u96c6\u4e2d\u4e8e\u9ad8\u4ef7\u503c\u6837\u672c\u3002", "result": "\u572820\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u4e0e\u672a\u7ecf\u7b5b\u9009\u7684\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\uff0cSkillRater\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u63d0\u9ad8\u4e865.63%\uff0cOCR\u65b9\u9762\u63d0\u9ad8\u4e862.00%\uff0cSTEM\u63a8\u7406\u65b9\u9762\u63d0\u9ad8\u4e863.53%\u3002\u6240\u5b66\u5f97\u7684\u8bc4\u4ef7\u4fe1\u53f7\u51e0\u4e4e\u662f\u6b63\u4ea4\u7684\uff0c\u8fd9\u8bc1\u5b9e\u4e86\u5206\u89e3\u786e\u5b9e\u6355\u6349\u5230\u4e86\u771f\u6b63\u72ec\u7acb\u7684\u8d28\u91cf\u7ef4\u5ea6\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5b83\u4f18\u4e8e\u672a\u7ecf\u8fc7\u6ee4\u7684\u8bad\u7ec3\u548c\u5355\u4e00\u5b66\u4e60\u8fc7\u7684\u8fc7\u6ee4\u65b9\u6cd5\u3002", "conclusion": "SkillRater\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6839\u636e\u6a21\u578b\u6240\u9700\u7684\u4e0d\u540c\u80fd\u529b\u5bf9\u6570\u636e\u8fdb\u884c\u591a\u7ef4\u5ea6\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.11618", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.11618", "abs": "https://arxiv.org/abs/2602.11618", "authors": ["Tatsuya Sagawa", "Ryosuke Kojima"], "title": "How Well Do Large-Scale Chemical Language Models Transfer to Downstream Tasks?", "comment": null, "summary": "Chemical Language Models (CLMs) pre-trained on large scale molecular data are widely used for molecular property prediction. However, the common belief that increasing training resources such as model size, dataset size, and training compute improves both pretraining loss and downstream task performance has not been systematically validated in the chemical domain. In this work, we evaluate this assumption by pretraining CLMs while scaling training resources and measuring transfer performance across diverse molecular property prediction (MPP) tasks. We find that while pretraining loss consistently decreases with increased training resources, downstream task performance shows limited improvement. Moreover, alternative metrics based on the Hessian or loss landscape also fail to estimate downstream performance in CLMs. We further identify conditions under which downstream performance saturates or degrades despite continued improvements in pretraining metrics, and analyze the underlying task dependent failure modes through parameter space visualizations. These results expose a gap between pretraining based evaluation and downstream performance, and emphasize the need for model selection and evaluation strategies that explicitly account for downstream task characteristics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5316\u5b66\u9886\u57df\u4e2d\uff0c\u589e\u52a0\u8bad\u7ec3\u8d44\u6e90\uff08\u5982\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8ba1\u7b97\u80fd\u529b\uff09\u662f\u5426\u80fd\u540c\u65f6\u63d0\u9ad8\u9884\u8bad\u7ec3\u635f\u5931\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u9884\u8bad\u7ec3\u635f\u5931\u968f\u7740\u8bad\u7ec3\u8d44\u6e90\u7684\u589e\u52a0\u800c\u6301\u7eed\u4e0b\u964d\uff0c\u4f46\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u4ec5\u663e\u793a\u51fa\u6709\u9650\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u57fa\u4e8eHessian\u6216\u635f\u5931\u666f\u89c2\u7684\u66ff\u4ee3\u6307\u6807\u4e5f\u65e0\u6cd5\u51c6\u786e\u4f30\u8ba1\u4e0b\u6e38\u8868\u73b0\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u9884\u8bad\u7ec3\u6307\u6807\u6709\u6240\u6539\u5584\uff0c\u4e0b\u6e38\u6027\u80fd\u4e5f\u53ef\u80fd\u8fbe\u5230\u9971\u548c\u751a\u81f3\u9000\u5316\u3002", "motivation": "\u5c3d\u7ba1\u666e\u904d\u8ba4\u4e3a\u589e\u52a0\u8bad\u7ec3\u8d44\u6e90\u53ef\u4ee5\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u679c\u53ca\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5728\u5316\u5b66\u9886\u57df\u8fd9\u4e00\u5047\u8bbe\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u6027\u9a8c\u8bc1\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8c03\u6574\u8bad\u7ec3\u8d44\u6e90\u5e76\u6d4b\u91cf\u8de8\u591a\u79cd\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u6027\u80fd\u6765\u68c0\u9a8c\u8fd9\u4e00\u89c2\u70b9\u3002", "method": "\u7814\u7a76\u8005\u4eec\u901a\u8fc7\u6539\u53d8\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u8d44\u6e90\uff0c\u5305\u62ec\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u96c6\u5927\u5c0f\u4ee5\u53ca\u8bad\u7ec3\u7b97\u529b\uff0c\u5e76\u5bf9\u4e0d\u540c\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u8bc4\u4f30\u3002\u9664\u4e86\u89c2\u5bdf\u9884\u8bad\u7ec3\u635f\u5931\u7684\u53d8\u5316\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u4f7f\u7528Hessian\u77e9\u9635\u6216\u635f\u5931\u5730\u5f62\u56fe\u4f5c\u4e3a\u66ff\u4ee3\u5ea6\u91cf\u7684\u53ef\u80fd\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u968f\u7740\u8bad\u7ec3\u8d44\u6e90\u7684\u589e\u957f\uff0c\u9884\u8bad\u7ec3\u635f\u5931\u786e\u5b9e\u5448\u73b0\u4e0b\u964d\u8d8b\u52bf\uff1b\u7136\u800c\uff0c\u5bf9\u4e8e\u4e0b\u6e38\u4efb\u52a1\u6765\u8bf4\uff0c\u5176\u6027\u80fd\u6539\u5584\u5374\u5341\u5206\u6709\u9650\u3002\u800c\u4e14\uff0c\u5c1d\u8bd5\u5229\u7528Hessian\u77e9\u9635\u6216\u8005\u635f\u5931\u5730\u5f62\u56fe\u7b49\u5176\u4ed6\u5ea6\u91cf\u65b9\u5f0f\u6765\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e5f\u672a\u80fd\u6210\u529f\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5f53\u9884\u8bad\u7ec3\u6307\u6807\u7ee7\u7eed\u4f18\u5316\u65f6\uff0c\u67d0\u4e9b\u6761\u4ef6\u4e0b\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u53cd\u800c\u505c\u6ede\u4e0d\u524d\u751a\u81f3\u6076\u5316\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u8bc4\u4ef7\u53ef\u80fd\u4e0d\u8db3\u4ee5\u53cd\u6620\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u80fd\u591f\u66f4\u76f4\u63a5\u5730\u8003\u8651\u4e0b\u6e38\u4efb\u52a1\u7279\u6027\u7684\u6a21\u578b\u9009\u62e9\u4e0e\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2602.11623", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11623", "abs": "https://arxiv.org/abs/2602.11623", "authors": ["Weida Li", "Yaoliang Yu", "Bryan Kian Hsiang Low"], "title": "TreeGrad-Ranker: Feature Ranking via $O(L)$-Time Gradients for Decision Trees", "comment": null, "summary": "We revisit the use of probabilistic values, which include the well-known Shapley and Banzhaf values, to rank features for explaining the local predicted values of decision trees. The quality of feature rankings is typically assessed with the insertion and deletion metrics. Empirically, we observe that co-optimizing these two metrics is closely related to a joint optimization that selects a subset of features to maximize the local predicted value while minimizing it for the complement. However, we theoretically show that probabilistic values are generally unreliable for solving this joint optimization. Therefore, we explore deriving feature rankings by directly optimizing the joint objective. As the backbone, we propose TreeGrad, which computes the gradients of the multilinear extension of the joint objective in $O(L)$ time for decision trees with $L$ leaves; these gradients include weighted Banzhaf values. Building upon TreeGrad, we introduce TreeGrad-Ranker, which aggregates the gradients while optimizing the joint objective to produce feature rankings, and TreeGrad-Shap, a numerically stable algorithm for computing Beta Shapley values with integral parameters. In particular, the feature scores computed by TreeGrad-Ranker satisfy all the axioms uniquely characterizing probabilistic values, except for linearity, which itself leads to the established unreliability. Empirically, we demonstrate that the numerical error of Linear TreeShap can be up to $10^{15}$ times larger than that of TreeGrad-Shap when computing the Shapley value. As a by-product, we also develop TreeProb, which generalizes Linear TreeShap to support all probabilistic values. In our experiments, TreeGrad-Ranker performs significantly better on both insertion and deletion metrics. Our code is available at https://github.com/watml/TreeGrad.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u4f7f\u7528\u6982\u7387\u503c\uff08\u5305\u62ecShapley\u503c\u548cBanzhaf\u503c\uff09\u5bf9\u51b3\u7b56\u6811\u7684\u5c40\u90e8\u9884\u6d4b\u503c\u8fdb\u884c\u7279\u5f81\u6392\u5e8f\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86TreeGrad\u7b97\u6cd5\u6765\u76f4\u63a5\u4f18\u5316\u8054\u5408\u76ee\u6807\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u4f18\u7684\u7279\u5f81\u6392\u540d\u3002", "motivation": "\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u6539\u8fdb\u7528\u4e8e\u89e3\u91ca\u51b3\u7b56\u6811\u5c40\u90e8\u9884\u6d4b\u503c\u7684\u7279\u5f81\u6392\u5e8f\u65b9\u6cd5\u3002\u5c3d\u7ba1\u4f20\u7edf\u4e0a\u4f7f\u7528Shapley\u548cBanzhaf\u503c\u7b49\u6982\u7387\u503c\u6765\u8fdb\u884c\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u540c\u65f6\u4f18\u5316\u63d2\u5165\u548c\u5220\u9664\u6307\u6807\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5TreeGrad\uff0c\u5b83\u80fd\u591f\u8ba1\u7b97\u8054\u5408\u76ee\u6807\u7684\u591a\u7ebf\u6027\u6269\u5c55\u68af\u5ea6\uff0c\u8fdb\u800c\u76f4\u63a5\u4f18\u5316\u8fd9\u4e2a\u76ee\u6807\u3002\u57fa\u4e8eTreeGrad\u5f00\u53d1\u4e86\u4e24\u4e2a\u5de5\u5177\uff1aTreeGrad-Ranker\u7528\u4e8e\u751f\u6210\u7279\u5f81\u6392\u540d\uff0c\u800cTreeGrad-Shap\u5219\u7528\u4e8e\u8ba1\u7b97Beta Shapley\u503c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTreeGrad-Ranker\u5728\u63d2\u5165\u548c\u5220\u9664\u6307\u6807\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0cTreeGrad-Shap\u76f8\u6bd4Linear TreeShap\u5728\u6570\u503c\u7a33\u5b9a\u6027\u65b9\u9762\u4e5f\u6709\u6781\u5927\u63d0\u5347\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684TreeGrad\u53ca\u5176\u884d\u751f\u65b9\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6846\u67b6\u6765\u89e3\u51b3\u7279\u5f81\u6392\u5e8f\u95ee\u9898\uff0c\u800c\u4e14\u901a\u8fc7\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.11629", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11629", "abs": "https://arxiv.org/abs/2602.11629", "authors": ["Dongxiao He", "Wenxuan Sun", "Yongqi Huang", "Jitao Zhao", "Di Jin"], "title": "GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks", "comment": "16 pages, 8 figures", "summary": "Graph Prompt Learning (GPL) has recently emerged as a promising paradigm for downstream adaptation of pre-trained graph models, mitigating the misalignment between pre-training objectives and downstream tasks. Recently, the focus of GPL has shifted from in-domain to cross-domain scenarios, which is closer to the real world applications, where the pre-training source and downstream target often differ substantially in data distribution. However, why GPLs remain effective under such domain shifts is still unexplored. Empirically, we observe that representative GPL methods are competitive with two simple baselines in cross-domain settings: full fine-tuning (FT) and linear probing (LP), motivating us to explore a deeper understanding of the prompting mechanism. We provide a theoretical analysis demonstrating that jointly leveraging these two complementary branches yields a smaller estimation error than using either branch alone, formally proving that cross-domain GPL benefits from the integration between pre-trained knowledge and task-specific adaptation. Based on this insight, we propose GP2F, a dual-branch GPL method that explicitly instantiates the two extremes: (1) a frozen branch that retains pre-trained knowledge, and (2) an adapted branch with lightweight adapters for task-specific adaptation. We then perform adaptive fusion under topology constraints via a contrastive loss and a topology-consistent loss. Extensive experiments on cross-domain few-shot node and graph classification demonstrate that our method outperforms existing methods.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u56fe\u63d0\u793a\u5b66\u4e60\uff08GPL\uff09\u5728\u8de8\u9886\u57df\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652fGPL\u65b9\u6cd5GP2F\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u548c\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u8c03\u6574\uff0c\u4ece\u800c\u5728\u8de8\u9886\u57df\u5c0f\u6837\u672c\u8282\u70b9\u548c\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1GPL\u5728\u4ece\u9884\u8bad\u7ec3\u76ee\u6807\u5230\u4e0b\u6e38\u4efb\u52a1\u7684\u9002\u5e94\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u8de8\u9886\u57df\u573a\u666f\u4e0b\uff0c\u4f46\u5176\u5728\u9886\u57df\u8fc1\u79fb\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u6027\u7684\u539f\u56e0\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u7814\u7a76\u8005\u89c2\u5bdf\u5230\uff0c\u5728\u8de8\u9886\u57df\u8bbe\u7f6e\u4e2d\uff0cGPL\u65b9\u6cd5\u4e0e\u5168\u5fae\u8c03\uff08FT\uff09\u548c\u7ebf\u6027\u63a2\u6d4b\uff08LP\uff09\u4e24\u79cd\u7b80\u5355\u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8fd9\u4fc3\u4f7f\u4ed6\u4eec\u6df1\u5165\u7406\u89e3\u63d0\u793a\u673a\u5236\u80cc\u540e\u7684\u539f\u56e0\u3002", "method": "\u672c\u6587\u9996\u5148\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u7ed3\u5408\u4f7f\u7528\u4e24\u4e2a\u4e92\u8865\u5206\u652f\uff08\u5373\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u5206\u652f\u548c\u7528\u4e8e\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u7684\u5206\u652f\uff09\u53ef\u4ee5\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u5206\u652f\u83b7\u5f97\u66f4\u5c0f\u7684\u4f30\u8ba1\u8bef\u5dee\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86GP2F\u65b9\u6cd5\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u51bb\u7ed3\u5206\u652f\u6765\u4fdd\u7559\u9884\u8bad\u7ec3\u7684\u77e5\u8bc6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5e26\u6709\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u7684\u9002\u5e94\u5206\u652f\u6765\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u8c03\u6574\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u548c\u62d3\u6251\u4e00\u81f4\u6027\u635f\u5931\u5b9e\u73b0\u4e86\u5728\u62d3\u6251\u7ea6\u675f\u4e0b\u7684\u81ea\u9002\u5e94\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8de8\u9886\u57df\u7684\u5c11\u91cf\u6837\u672c\u8282\u70b9\u5206\u7c7b\u548c\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5GP2F\u76f8\u8f83\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u77e5\u8bc6\u4e0e\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8GPL\u5728\u8de8\u9886\u57df\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u63d0\u51fa\u7684GP2F\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u7406\u89e3GPL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\uff0c\u4e5f\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11633", "abs": "https://arxiv.org/abs/2602.11633", "authors": ["Jianhua Wang", "Yinlin Su"], "title": "TIP: Resisting Gradient Inversion via Targeted Interpretable Perturbation in Federated Learning", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while preserving data locality; however, the exchange of gradients renders the system vulnerable to Gradient Inversion Attacks (GIAs), allowing adversaries to reconstruct private training data with high fidelity. Existing defenses, such as Differential Privacy (DP), typically employ indiscriminate noise injection across all parameters, which severely degrades model utility and convergence stability. To address those limitation, we proposes Targeted Interpretable Perturbation (TIP), a novel defense framework that integrates model interpretability with frequency domain analysis. Unlike conventional methods that treat parameters uniformly, TIP introduces a dual-targeting strategy. First, leveraging Gradient-weighted Class Activation Mapping (Grad-CAM) to quantify channel sensitivity, we dynamically identify critical convolution channels that encode primary semantic features. Second, we transform these selected kernels into the frequency domain via the Discrete Fourier Transform and selectively inject calibrated perturbations into the high-frequency spectrum. By selectively perturbing high-frequency components, TIP effectively destroys the fine-grained details necessary for image reconstruction while preserving the low-frequency information crucial for model accuracy. Extensive experiments on benchmark datasets demonstrate that TIP renders reconstructed images visually unrecognizable against state-of-the-art GIAs, while maintaining global model accuracy comparable to non-private baselines, significantly outperforming existing DP-based defenses in the privacy-utility trade-off and interpretability. Code is available in https://github.com/2766733506/asldkfjssdf_arxiv", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u6846\u67b6TIP\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0e\u9891\u57df\u5206\u6790\u6765\u5bf9\u6297\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u3002\u8be5\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u5bf9\u9ad8\u9891\u5206\u91cf\u6ce8\u5165\u6270\u52a8\uff0c\u4ece\u800c\u7834\u574f\u56fe\u50cf\u91cd\u5efa\u6240\u9700\u7684\u7ec6\u8282\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTIP\u5728\u9690\u79c1-\u5b9e\u7528\u6027\u6743\u8861\u53ca\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7684\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u53ef\u4ee5\u5b9e\u73b0\u5728\u4fdd\u7559\u6570\u636e\u672c\u5730\u6027\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u534f\u4f5c\u8bad\u7ec3\uff0c\u4f46\u5176\u68af\u5ea6\u4ea4\u6362\u673a\u5236\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\uff08GIA\uff09\uff0c\u653b\u51fb\u8005\u80fd\u591f\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u91cd\u6784\u79c1\u4eba\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u7684\u9632\u5fa1\u624b\u6bb5\u5982\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u901a\u5e38\u901a\u8fc7\u5bf9\u6240\u6709\u53c2\u6570\u65e0\u5dee\u522b\u5730\u52a0\u5165\u566a\u58f0\u6765\u5b9e\u73b0\uff0c\u4f46\u8fd9\u4e25\u91cd\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u6536\u655b\u7a33\u5b9a\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86TIP\u3002", "method": "TIP\u6846\u67b6\u6574\u5408\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0e\u9891\u7387\u57df\u5206\u6790\u6280\u672f\u3002\u9996\u5148\u4f7f\u7528Grad-CAM\u91cf\u5316\u901a\u9053\u654f\u611f\u5ea6\uff0c\u52a8\u6001\u8bc6\u522b\u7f16\u7801\u4e3b\u8981\u8bed\u4e49\u7279\u5f81\u7684\u5173\u952e\u5377\u79ef\u901a\u9053\uff1b\u63a5\u7740\u5c06\u8fd9\u4e9b\u9009\u5b9a\u7684\u6838\u8f6c\u6362\u5230\u9891\u7387\u57df\uff0c\u5e76\u6709\u9009\u62e9\u6027\u5730\u5411\u9ad8\u9891\u8c31\u90e8\u5206\u6ce8\u5165\u6821\u51c6\u540e\u7684\u6270\u52a8\u3002\u8fd9\u6837\u7684\u8bbe\u8ba1\u65e8\u5728\u901a\u8fc7\u5e72\u6270\u9ad8\u9891\u6210\u5206\u6765\u7834\u574f\u7528\u4e8e\u56fe\u50cf\u91cd\u5efa\u7684\u7cbe\u7ec6\u7ec6\u8282\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6a21\u578b\u51c6\u786e\u5ea6\u81f3\u5173\u91cd\u8981\u7684\u4f4e\u9891\u4fe1\u606f\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u6d4b\u8bd5\u663e\u793a\uff0cTIP\u80fd\u4f7f\u9488\u5bf9\u6700\u5148\u8fdbGIA\u7684\u91cd\u5efa\u56fe\u7247\u53d8\u5f97\u89c6\u89c9\u4e0a\u65e0\u6cd5\u8fa8\u8ba4\uff0c\u4e0e\u6b64\u540c\u65f6\u7ef4\u6301\u4e0e\u975e\u79c1\u6709\u57fa\u7ebf\u76f8\u5f53\u7684\u6574\u4f53\u6a21\u578b\u7cbe\u5ea6\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u4e0e\u5b9e\u7528\u4ef7\u503c\u4e4b\u95f4\u7684\u5e73\u8861\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eDP\u7684\u9632\u5fa1\u7b56\u7565\u3002", "conclusion": "TIP\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u800c\u5f15\u53d1\u7684\u6570\u636e\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u5b83\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u62b5\u6297\u6b64\u7c7b\u653b\u51fb\u7684\u80fd\u529b\uff0c\u8fd8\u4fdd\u8bc1\u4e86\u8f83\u9ad8\u7684\u6a21\u578b\u6027\u80fd\u548c\u66f4\u597d\u7684\u9690\u79c1-\u5b9e\u7528\u6027\u6743\u8861\u3002"}}
{"id": "2602.11662", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11662", "abs": "https://arxiv.org/abs/2602.11662", "authors": ["Yang Yang"], "title": "UMAP Is Spectral Clustering on the Fuzzy Nearest-Neighbor Graph", "comment": null, "summary": "UMAP (Uniform Manifold Approximation and Projection) is among the most widely used algorithms for non linear dimensionality reduction and data visualisation. Despite its popularity, and despite being presented through the lens of algebraic topology, the exact relationship between UMAP and classical spectral methods has remained informal. In this work, we prove that UMAP performs spectral clustering on the fuzzy k nearest neighbour graph. Our proof proceeds in three steps: (1) we show that UMAP's stochastic optimisation with negative sampling is a contrastive learning objective on the similarity graph; (2) we invoke the result of HaoChen et al. [8], establishing that contrastive learning on a similarity graph is equivalent to spectral clustering; and (3) we verify that UMAP's spectral initialisation computes the exact linear solution to this spectral problem. The equivalence is exact for Gaussian kernels, and holds as a first order approximation for UMAP's default Cauchy type kernel. Our result unifies UMAP, contrastive learning, and spectral clustering under a single framework, and provides theoretical grounding for several empirical observations about UMAP's behaviour.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86UMAP\u7b97\u6cd5\u5b9e\u9645\u4e0a\u662f\u5728\u6a21\u7ccak\u6700\u8fd1\u90bb\u56fe\u4e0a\u6267\u884c\u8c31\u805a\u7c7b\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u6b65\u9aa4\u5efa\u7acb\u4e86UMAP\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u53ca\u8c31\u805a\u7c7b\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u4e3aUMAP\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "\u5c3d\u7ba1UMAP\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u975e\u7ebf\u6027\u964d\u7ef4\u548c\u6570\u636e\u53ef\u89c6\u5316\u7b97\u6cd5\uff0c\u4f46\u5176\u4e0e\u7ecf\u5178\u8c31\u65b9\u6cd5\u4e4b\u95f4\u7684\u7cbe\u786e\u5173\u7cfb\u4e00\u76f4\u672a\u88ab\u6b63\u5f0f\u5b9a\u4e49\u3002", "method": "1. \u8bc1\u660eUMAP\u7684\u968f\u673a\u4f18\u5316\u5e26\u8d1f\u91c7\u6837\u662f\u76f8\u4f3c\u5ea6\u56fe\u4e0a\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff1b2. \u5f15\u7528HaoChen\u7b49\u4eba\u7684\u7814\u7a76\u7ed3\u679c\uff0c\u8bf4\u660e\u76f8\u4f3c\u5ea6\u56fe\u4e0a\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b49\u540c\u4e8e\u8c31\u805a\u7c7b\uff1b3. \u9a8c\u8bc1UMAP\u7684\u8c31\u521d\u59cb\u5316\u8ba1\u7b97\u4e86\u8be5\u8c31\u95ee\u9898\u7684\u786e\u5207\u7ebf\u6027\u89e3\u3002", "result": "\u5bf9\u4e8e\u9ad8\u65af\u6838\u6765\u8bf4\uff0c\u8fd9\u79cd\u7b49\u4ef7\u5173\u7cfb\u662f\u5b8c\u5168\u6210\u7acb\u7684\uff0c\u800c\u5bf9\u4e8eUMAP\u9ed8\u8ba4\u7684\u67ef\u897f\u578b\u6838\uff0c\u5219\u4f5c\u4e3a\u4e00\u9636\u8fd1\u4f3c\u6210\u7acb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u7edf\u4e00\u4e86UMAP\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u8c31\u805a\u7c7b\u5230\u4e00\u4e2a\u6846\u67b6\u4e0b\uff0c\u4e3aUMAP\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u4e9b\u7ecf\u9a8c\u89c2\u5bdf\u80cc\u540e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.11665", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.11665", "abs": "https://arxiv.org/abs/2602.11665", "authors": ["Tingkai Jia", "Cheng Chen"], "title": "Fully First-Order Algorithms for Online Bilevel Optimization", "comment": null, "summary": "In this work, we study non-convex-strongly-convex online bilevel optimization (OBO). Existing OBO algorithms are mainly based on hypergradient descent, which requires access to a Hessian-vector product (HVP) oracle and potentially incurs high computational costs. By reformulating the original OBO problem as a single-level online problem with inequality constraints and constructing a sequence of Lagrangian function, we eliminate the need for HVPs arising from implicit differentiation. Specifically, we propose a fully first-order algorithm for OBO, and provide theoretical guarantees showing that it achieves regret of $O(1 + V_T + H_{2,T})$. Furthermore, we develop an improved variant with an adaptive inner-iteration scheme, which removes the dependence on the drift variation of the inner-level optimal solution and achieves regret of $O(\\sqrt{T} + V_T)$. This regret have the advatange when $V_{T}\\ge O(\\sqrt{T})$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u4e00\u9636\u5728\u7ebf\u53cc\u5c42\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u539f\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e26\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7684\u5355\u5c42\u5728\u7ebf\u95ee\u9898\u5e76\u6784\u9020\u4e00\u7cfb\u5217\u62c9\u683c\u6717\u65e5\u51fd\u6570\u6765\u907f\u514d\u4f7f\u7528Hessian-vector\u4ea7\u54c1\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u6539\u8fdb\u7248\u672c\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u5185\u8fed\u4ee3\u65b9\u6848\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u53cc\u5c42\u4f18\u5316(OBO)\u7b97\u6cd5\u4e3b\u8981\u57fa\u4e8e\u8d85\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u8bbf\u95eeHessian-vector\u4e58\u79ef(HVP)oracle\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8f83\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6d88\u9664\u9690\u5f0f\u5fae\u5206\u4ea7\u751f\u7684\u5bf9HVP\u7684\u9700\u6c42\u6765\u964d\u4f4e\u8fd9\u79cd\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u5c06\u539f\u59cbOBO\u95ee\u9898\u91cd\u6784\u6210\u4e00\u4e2a\u5177\u6709\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7684\u5355\u5c42\u5728\u7ebf\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u7cfb\u5217\u62c9\u683c\u6717\u65e5\u51fd\u6570\u7684\u65b9\u6cd5\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u4e00\u9636\u4fe1\u606f\u7684OBO\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u5176\u8fbe\u5230$O(1 + V_T + H_{2,T})$\u9057\u61be\u754c\u3002\u53e6\u5916\uff0c\u53d1\u5c55\u4e86\u4e00\u4e2a\u6539\u8fdb\u7248\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u5185\u90e8\u8fed\u4ee3\u673a\u5236\uff0c\u53bb\u9664\u4e86\u5bf9\u4e8e\u5185\u90e8\u6700\u4f18\u89e3\u6f02\u79fb\u53d8\u5316\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u8fbe\u5230\u4e86$O(\\sqrt{T} + V_T)$\u7684\u9057\u61be\u754c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u88ab\u8bc1\u660e\u53ef\u4ee5\u8fbe\u5230$O(1 + V_T + H_{2,T})$\u7684\u9057\u61be\u754c\uff1b\u800c\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u5219\u80fd\u5b9e\u73b0$O(\\sqrt{T} + V_T)$\u7684\u9057\u61be\u754c\uff0c\u5728$V_{T}\\ge O(\\sqrt{T})$\u65f6\u663e\u793a\u51fa\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u7684\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u975e\u51f8-\u5f3a\u51f8\u5728\u7ebf\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u4e2d\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u907f\u514d\u4f7f\u7528\u4e8c\u9636\u68af\u5ea6\u4fe1\u606f\uff08\u5373Hessian-vector\u4e58\u79ef\uff09\u8fbe\u5230\u4e86\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002\u7279\u522b\u662f\u5f53\u5185\u5c42\u6700\u4f18\u89e3\u7684\u53d8\u5316\u4e0d\u5927\u4e8e\u7279\u5b9a\u9608\u503c\u65f6\uff0c\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2602.11668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11668", "abs": "https://arxiv.org/abs/2602.11668", "authors": ["David Fuentes-Jim\u00e9nez", "Sara Garc\u00eda-de-Villa", "David Casillas-P\u00e9rez", "Pablo Flor\u00eda", "Francisco-Manuel Melgarejo-Meseguer"], "title": "Explainable Machine-Learning based Detection of Knee Injuries in Runners", "comment": null, "summary": "Running is a widely practiced activity but shows a high incidence of knee injuries, especially Patellofemoral Pain Syndrome (PFPS) and Iliotibial Band Syndrome (ITBS). Identifying gait patterns linked to these injuries can improve clinical decision-making, which requires precise systems capable of capturing and analyzing temporal kinematic data.\n  This study uses optical motion capture systems to enhance detection of injury-related running patterns. We analyze a public dataset of 839 treadmill recordings from healthy and injured runners to evaluate how effectively these systems capture dynamic parameters relevant to injury classification. The focus is on the stance phase, using joint and segment angle time series and discrete point values.\n  Three classification tasks are addressed: healthy vs. injured, healthy vs. PFPS, and healthy vs. ITBS. We examine different feature spaces, from traditional point-based metrics to full stance-phase time series and hybrid representations. Multiple models are tested, including classical algorithms (K-Nearest Neighbors, Gaussian Processes, Decision Trees) and deep learning architectures (CNNs, LSTMs).\n  Performance is evaluated with accuracy, precision, recall, and F1-score. Explainability tools such as Shapley values, saliency maps, and Grad-CAM are used to interpret model behavior. Results show that combining time series with point values substantially improves detection. Deep learning models outperform classical ones, with CNNs achieving the highest accuracy: 77.9% for PFPS, 73.8% for ITBS, and 71.43% for the combined injury class.\n  These findings highlight the potential of motion capture systems coupled with advanced machine learning to identify knee injury-related running patterns.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u5206\u6790\u4e86839\u4efd\u8dd1\u6b65\u673a\u8bb0\u5f55\uff0c\u4ee5\u8bc6\u522b\u4e0e\u819d\u76d6\u53d7\u4f24\u76f8\u5173\u7684\u6b65\u6001\u6a21\u5f0f\u3002\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u548c\u70b9\u503c\u6570\u636e\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662fCNN\uff09\u5728\u8bc6\u522b\u9acc\u80a1\u75bc\u75db\u7efc\u5408\u75c7(PFPS)\u548c\u9ac2\u80eb\u5e26\u7efc\u5408\u75c7(ITBS)\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523077.9%\u300173.8%\uff0c\u5bf9\u4e8e\u5408\u5e76\u4f24\u5bb3\u7c7b\u522b\u5219\u4e3a71.43%\u3002", "motivation": "\u8dd1\u6b65\u65f6\u819d\u5173\u8282\u53d7\u4f24\u7684\u9ad8\u53d1\u6027\uff0c\u7279\u522b\u662f\u9acc\u80a1\u75bc\u75db\u7efc\u5408\u75c7(PFPS)\u548c\u9ac2\u80eb\u5e26\u7efc\u5408\u75c7(ITBS)\uff0c\u9700\u8981\u901a\u8fc7\u7cbe\u786e\u7684\u7cfb\u7edf\u6765\u6355\u6349\u5e76\u5206\u6790\u65f6\u95f4\u52a8\u6001\u6570\u636e\uff0c\u4ece\u800c\u8bc6\u522b\u4e0e\u8fd9\u4e9b\u635f\u4f24\u76f8\u5173\u7684\u6b65\u6001\u6a21\u5f0f\uff0c\u4ee5\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u5229\u7528\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u5bf9\u5305\u542b\u5065\u5eb7\u8dd1\u8005\u53ca\u53d7\u4f24\u8dd1\u8005\u7684839\u4efd\u8dd1\u6b65\u673a\u5f55\u50cf\u8fdb\u884c\u5206\u6790\uff0c\u91cd\u70b9\u653e\u5728\u652f\u6491\u9636\u6bb5\uff0c\u91c7\u7528\u5173\u8282\u548c\u8282\u6bb5\u89d2\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u53ca\u79bb\u6563\u70b9\u503c\u3002\u5b9e\u9a8c\u4e2d\u8003\u5bdf\u4e86\u4ece\u4f20\u7edf\u57fa\u4e8e\u70b9\u7684\u5ea6\u91cf\u5230\u5168\u652f\u6491\u9636\u6bb5\u65f6\u95f4\u5e8f\u5217\u4ee5\u53ca\u6df7\u5408\u8868\u793a\u7684\u4e0d\u540c\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5305\u62ec\u7ecf\u5178\u7b97\u6cd5\uff08K-\u6700\u8fd1\u90bb\u3001\u9ad8\u65af\u8fc7\u7a0b\u3001\u51b3\u7b56\u6811\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08CNNs\u3001LSTMs\uff09\u5728\u5185\u7684\u591a\u79cd\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u4e0e\u70b9\u503c\u76f8\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff0c\u5176\u4e2dCNN\u5728PFPS\u3001ITBS\u4ee5\u53ca\u5408\u5e76\u4f24\u5bb3\u7c7b\u522b\u7684\u5206\u7c7b\u4e0a\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\uff1a\u5206\u522b\u4e3a77.9%\u300173.8%\u548c71.43%\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u5728\u8bc6\u522b\u4e0e\u819d\u4f24\u76f8\u5173\u7684\u8dd1\u6b65\u6a21\u5f0f\u65b9\u9762\u5177\u6709\u5f88\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.11685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11685", "abs": "https://arxiv.org/abs/2602.11685", "authors": ["Joey Zhong", "Hao Zhang", "Clare Southern", "Jeremy Yang", "Thomas Wang", "Kate Jung", "Shu Zhang", "Denis Yarats", "Johnny Ho", "Jerry Ma"], "title": "DRACO: a Cross-Domain Benchmark for Deep Research Accuracy, Completeness, and Objectivity", "comment": null, "summary": "We present DRACO (Deep Research Accuracy, Completeness, and Objectivity), a benchmark of complex deep research tasks. These tasks, which span 10 domains and draw on information sources from 40 countries, originate from anonymized real-world usage patterns within a large-scale deep research system. Tasks are sampled from a de-identified dataset of Perplexity Deep Research requests, then filtered and augmented to ensure that the tasks are anonymized, open-ended and complex, objectively evaluable, and representative of the broad scope of real-world deep research use cases. Outputs are graded against task-specific rubrics along four dimensions: factual accuracy (accuracy), breadth and depth of analysis (including completeness), presentation quality (including objectivity), and citation quality. DRACO is publicly available at https://hf.co/datasets/perplexity-ai/draco.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86DRACO\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d610\u4e2a\u9886\u57df\u548c40\u4e2a\u56fd\u5bb6\u7684\u4fe1\u606f\u6e90\u3002\u8be5\u6570\u636e\u96c6\u57fa\u4e8e\u533f\u540d\u7684\u771f\u5b9e\u4e16\u754c\u4f7f\u7528\u6a21\u5f0f\u6784\u5efa\uff0c\u4efb\u52a1\u7ecf\u8fc7\u7b5b\u9009\u548c\u589e\u5f3a\u4ee5\u786e\u4fdd\u5176\u533f\u540d\u6027\u3001\u5f00\u653e\u6027\u548c\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u53ef\u4ee5\u5ba2\u89c2\u8bc4\u4f30\u3002\u8bc4\u4f30\u7ef4\u5ea6\u5305\u62ec\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u5e7f\u5ea6\u4e0e\u6df1\u5ea6\u3001\u5448\u73b0\u8d28\u91cf\u4ee5\u53ca\u5f15\u7528\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u4ee3\u8868\u771f\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u6df1\u5ea6\u7814\u7a76\u7528\u4f8b\u5e7f\u6cdb\u8303\u56f4\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u540c\u65f6\u786e\u4fdd\u8fd9\u4e9b\u6848\u4f8b\u662f\u533f\u540d\u5904\u7406\u3001\u5f00\u653e\u6027\u95ee\u9898\u5e76\u4e14\u8db3\u591f\u590d\u6742\uff0c\u4ee5\u4fbf\u4e8e\u5ba2\u89c2\u8bc4\u4ef7\u3002", "method": "\u4ece\u5927\u89c4\u6a21\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u4e2d\u7684\u533f\u540d\u5b9e\u9645\u4f7f\u7528\u6a21\u5f0f\u62bd\u53d6\u4efb\u52a1\u6837\u672c\uff0c\u7136\u540e\u5bf9\u8fd9\u4e9b\u6837\u672c\u8fdb\u884c\u8fc7\u6ee4\u548c\u589e\u8865\uff0c\u4fdd\u8bc1\u5b83\u4eec\u65e2\u533f\u540d\u53c8\u5177\u6709\u5f00\u653e\u6027\u548c\u590d\u6742\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u88ab\u5ba2\u89c2\u5730\u8bc4\u4ef7\u3002\u6bcf\u9879\u4efb\u52a1\u90fd\u6839\u636e\u7279\u5b9a\u7684\u4efb\u52a1\u8bc4\u5206\u6807\u51c6\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u6253\u5206\uff1a\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u7684\u5e7f\u5ea6\u4e0e\u6df1\u5ea6\uff08\u5305\u542b\u5b8c\u6574\u6027\uff09\u3001\u5c55\u793a\u8d28\u91cf\uff08\u5305\u542b\u5ba2\u89c2\u6027\uff09\u4ee5\u53ca\u5f15\u7528\u8d28\u91cf\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aDRACO\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5b83\u5305\u542b\u4e86\u8de8\u8d8a10\u4e2a\u4e0d\u540c\u9886\u57df\u548c\u6765\u81ea40\u4e2a\u56fd\u5bb6\u4fe1\u606f\u6e90\u7684\u4efb\u52a1\u3002\u8fd9\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u66f4\u597d\u5730\u7406\u89e3\u548c\u6539\u8fdb\u590d\u6742\u7684\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u8fd9\u6837\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6DRACO\uff0c\u7814\u7a76\u4eba\u5458\u73b0\u5728\u6709\u4e86\u4e00\u4e2a\u5f3a\u6709\u529b\u7684\u65b0\u5de5\u5177\u6765\u8861\u91cf\u548c\u63d0\u9ad8\u4ed6\u4eec\u5728\u6267\u884c\u590d\u6742\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u65f6\u7684\u8868\u73b0\u3002"}}
{"id": "2602.11690", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11690", "abs": "https://arxiv.org/abs/2602.11690", "authors": ["Oliver Zahn", "Matt Beton", "Simran Chana"], "title": "ANML: Attribution-Native Machine Learning with Guaranteed Robustness", "comment": "27 pages, 6 figures", "summary": "Frontier AI systems increasingly train on specialized expert data, from clinical records to proprietary research to curated datasets, yet current training pipelines treat all samples identically. A Nobel laureate's contribution receives the same weight as an unverified submission. We introduce ANML (Attribution-Native Machine Learning), a framework that weights training samples by four quality factors: gradient-based consistency (q), verification status (v), contributor reputation (r), and temporal relevance (T). By combining what the model observes (gradient signals) with what the system knows about data provenance (external signals), ANML produces per-contributor quality weights that simultaneously improve model performance and enable downstream attribution. Across 5 datasets (178-32,561 samples), ANML achieves 33-72% error reduction over gradient-only baselines. Quality-weighted training is data-efficient: 20% high-quality data outperforms 100% uniformly weighted data by 47%. A Two-Stage Adaptive gating mechanism guarantees that ANML never underperforms the best available baseline, including under strategic joint attacks combining credential faking with gradient alignment. When per-sample detection fails against subtle corruption, contributor-level attribution provides 1.3-5.3x greater improvement than sample-level methods, with the advantage growing as corruption becomes harder to detect.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6ANML\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u8d28\u91cf\u56e0\u7d20\u5bf9\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u52a0\u6743\u5904\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0cANML\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u4ec5\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u51cf\u5c11\u4e8633-72%\u7684\u9519\u8bef\u7387\uff0c\u5e76\u4e14\u5373\u4f7f\u5728\u9762\u5bf9\u590d\u6742\u7684\u8054\u5408\u653b\u51fb\u65f6\u4e5f\u80fd\u591f\u4fdd\u6301\u4e0d\u4f4e\u4e8e\u6700\u4f73\u57fa\u7ebf\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u4f7f\u7528\u4e13\u5bb6\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u6ca1\u6709\u533a\u5206\u4e0d\u540c\u8d28\u91cf\u7684\u6570\u636e\u6837\u672c\u7684\u91cd\u8981\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u8003\u8651\u6570\u636e\u6765\u6e90\u548c\u8d21\u732e\u8005\u4fe1\u8a89\u7b49\u56e0\u7d20\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u5f52\u56e0\u539f\u751f\u673a\u5668\u5b66\u4e60(ANML)\u6846\u67b6\uff0c\u5b83\u6839\u636e\u56db\u4e2a\u8d28\u91cf\u56e0\u7d20\u2014\u2014\u57fa\u4e8e\u68af\u5ea6\u7684\u4e00\u81f4\u6027(q)\u3001\u9a8c\u8bc1\u72b6\u6001(v)\u3001\u8d21\u732e\u8005\u58f0\u8a89(r)\u4ee5\u53ca\u65f6\u95f4\u76f8\u5173\u6027(T)\u6765\u7ed9\u8bad\u7ec3\u6837\u672c\u5206\u914d\u6743\u91cd\u3002\u6b64\u5916\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u4ee5\u786e\u4fddANML\u4e0d\u4f1a\u4f4e\u4e8e\u4efb\u4f55\u53ef\u7528\u7684\u6700\u4f73\u57fa\u7ebf\u8868\u73b0\u3002", "result": "ANML\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u76f8\u8f83\u4e8e\u4ec5\u4f9d\u8d56\u68af\u5ea6\u4fe1\u53f7\u57fa\u51c6\u9ad8\u8fbe33-72%\u7684\u8bef\u5dee\u51cf\u5c11\u3002\u9ad8\u8d28\u91cf\u6570\u636e\u536020%\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e100%\u5747\u5300\u52a0\u6743\u6570\u636e47%\u3002\u5f53\u5355\u72ec\u6837\u672c\u68c0\u6d4b\u5931\u8d25\u65f6\uff0c\u8d21\u732e\u8005\u7ea7\u522b\u7684\u5f52\u56e0\u63d0\u4f9b\u4e86\u6bd4\u6837\u672c\u7ea7\u522b\u65b9\u6cd5\u9ad8\u51fa1.3\u81f35.3\u500d\u7684\u6539\u8fdb\u3002", "conclusion": "ANML\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5229\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u652f\u6301\u4e0b\u6e38\u5f52\u5c5e\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff0c\u800c\u4e14\u5728\u9762\u5bf9\u590d\u6742\u653b\u51fb\u65f6\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.11698", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11698", "abs": "https://arxiv.org/abs/2602.11698", "authors": ["Chengting Yu", "Xiaobo Shu", "Yadao Wang", "Yizhen Zhang", "Haoyi Wu", "You Wu", "Rujiao Long", "Ziheng Chen", "Yuchi Xu", "Wenbo Su", "Bo Zheng"], "title": "SpiralFormer: Looped Transformers Can Learn Hierarchical Dependencies via Multi-Resolution Recursion", "comment": null, "summary": "Recursive (looped) Transformers decouple computational depth from parameter depth by repeatedly applying shared layers, providing an explicit architectural primitive for iterative refinement and latent reasoning. However, early looped Transformers often underperform non-recursive baselines of equal compute. While recent literature has introduced more effective recursion mechanisms to mitigate this gap, existing architectures still operate at a fixed, full-token resolution, neglecting the potential efficiency of computing over compressed latent representations. In this paper, we propose SpiralFormer, a looped Transformer that executes recurrence under a multi-resolution recursion schedule. We provide probing evidence that multi-resolution recursion enables the model to learn hierarchical dependencies by inducing iteration-wise functional specialization across different scales. Empirically, SpiralFormer achieves better parameter and compute efficiency than both looped and non-looped baselines across model scales from 160M to 1.4B, establishing sequence resolution as a potential axis for scaling recursive architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpiralFormer\u7684\u5faa\u73afTransformer\u6a21\u578b\uff0c\u5b83\u91c7\u7528\u591a\u5206\u8fa8\u7387\u9012\u5f52\u8c03\u5ea6\u6765\u6267\u884c\u8fed\u4ee3\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u8de8\u4e0d\u540c\u5c3a\u5ea6\u7684\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5728\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u5faa\u73af\u548c\u975e\u5faa\u73af\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u65e9\u671f\u7684\u5faa\u73afTransformer\u901a\u5e38\u5728\u76f8\u540c\u8ba1\u7b97\u91cf\u4e0b\u8868\u73b0\u4e0d\u5982\u975e\u5faa\u73af\u57fa\u7ebf\u6a21\u578b\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u5f15\u5165\u4e86\u66f4\u6709\u6548\u7684\u9012\u5f52\u673a\u5236\u4ee5\u51cf\u5c11\u8fd9\u79cd\u5dee\u8ddd\uff0c\u4f46\u73b0\u6709\u67b6\u6784\u4ecd\u7136\u4ee5\u56fa\u5b9a\u7684\u5168\u4ee4\u724c\u5206\u8fa8\u7387\u8fd0\u884c\uff0c\u5ffd\u89c6\u4e86\u5728\u538b\u7f29\u6f5c\u5728\u8868\u793a\u4e0a\u8fdb\u884c\u8ba1\u7b97\u7684\u53ef\u80fd\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86SpiralFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u5206\u8fa8\u7387\u9012\u5f52\u8c03\u5ea6\u6267\u884c\u7684\u5faa\u73afTransformer\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u540c\u7684\u5c3a\u5ea6\u4e0a\u8bf1\u5bfc\u51fa\u8fed\u4ee3\u7ea7\u522b\u7684\u529f\u80fd\u4e13\u4e1a\u5316\uff0c\u4ece\u800c\u5b66\u4e60\u5230\u5c42\u6b21\u4f9d\u8d56\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSpiralFormer\u5728\u4ece160M\u52301.4B\u7684\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\uff0c\u90fd\u6bd4\u5faa\u73af\u4e0e\u975e\u5faa\u73af\u57fa\u7ebf\u6a21\u578b\u5c55\u73b0\u51fa\u4e86\u66f4\u597d\u7684\u53c2\u6570\u53ca\u8ba1\u7b97\u6548\u7387\u3002\u8fd9\u8868\u660e\u5e8f\u5217\u5206\u8fa8\u7387\u53ef\u80fd\u662f\u6269\u5c55\u9012\u5f52\u67b6\u6784\u7684\u4e00\u4e2a\u6f5c\u5728\u65b9\u5411\u3002", "conclusion": "SpiralFormer\u901a\u8fc7\u5f15\u5165\u591a\u5206\u8fa8\u7387\u9012\u5f52\u673a\u5236\uff0c\u5728\u63d0\u9ad8\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u4e5f\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u9012\u5f52\u7ed3\u6784\u6765\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u4e2d\u7684\u5c42\u6b21\u4fe1\u606f\uff0c\u4e3a\u9012\u5f52\u67b6\u6784\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.11700", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11700", "abs": "https://arxiv.org/abs/2602.11700", "authors": ["Yongyao Wang", "Ziqi Miao", "Lu Yang", "Haonan Jia", "Wenting Yan", "Chen Qian", "Lijun Li"], "title": "TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction", "comment": "13 pages", "summary": "Tabular prediction can benefit from in-table rows as few-shot evidence, yet existing tabular models typically perform instance-wise inference and LLM-based prompting is often brittle. Models do not consistently leverage relevant rows, and noisy context can degrade performance. To address this challenge, we propose TabSieve, a select-then-predict framework that makes evidence usage explicit and auditable. Given a table and a query row, TabSieve first selects a small set of informative rows as evidence and then predicts the missing target conditioned on the selected evidence. To enable this capability, we construct TabSieve-SFT-40K by synthesizing high-quality reasoning trajectories from 331 real tables using a strong teacher model with strict filtering. Furthermore, we introduce TAB-GRPO, a reinforcement learning recipe that jointly optimizes evidence selection and prediction correctness with separate rewards, and stabilizes mixed regression and classification training via dynamic task-advantage balancing. Experiments on a held-out benchmark of 75 classification and 52 regression tables show that TabSieve consistently improves performance across shot budgets, with average gains of 2.92% on classification and 4.45% on regression over the second-best baseline. Further analysis indicates that TabSieve concentrates more attention on the selected evidence, which improves robustness to noisy context.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTabSieve\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u9996\u5148\u9009\u62e9\u5c11\u91cf\u4fe1\u606f\u884c\u4f5c\u4e3a\u8bc1\u636e\uff0c\u7136\u540e\u57fa\u4e8e\u6240\u9009\u8bc1\u636e\u9884\u6d4b\u7f3a\u5931\u76ee\u6807\uff0c\u4ece\u800c\u6539\u8fdb\u4e86\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u3002\u6b64\u5916\u8fd8\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u8bad\u7ec3\u6570\u636e\u96c6TabSieve-SFT-40K\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5TAB-GRPO\u6765\u4f18\u5316\u8bc1\u636e\u9009\u62e9\u4e0e\u9884\u6d4b\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTabSieve\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u683c\u6a21\u578b\u901a\u5e38\u8fdb\u884c\u5b9e\u4f8b\u7ea7\u63a8\u7406\uff0c\u5e76\u4e14\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u5f80\u5f80\u4e0d\u591f\u7a33\u5b9a\u3002\u8fd9\u4e9b\u6a21\u578b\u4e0d\u80fd\u4e00\u81f4\u5730\u5229\u7528\u76f8\u5173\u884c\uff0c\u800c\u5608\u6742\u7684\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u660e\u786e\u4e14\u53ef\u5ba1\u8ba1\u7684\u8bc1\u636e\u4f7f\u7528\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aTabSieve\u7684\u9009\u62e9-\u9884\u6d4b\u6846\u67b6\uff0c\u5b83\u9996\u5148\u4ece\u7ed9\u5b9a\u7684\u8868\u4e2d\u9009\u53d6\u4e00\u5c0f\u90e8\u5206\u4fe1\u606f\u91cf\u5927\u7684\u884c\u4f5c\u4e3a\u8bc1\u636e\uff0c\u63a5\u7740\u57fa\u4e8e\u9009\u51fa\u7684\u8bc1\u636e\u6765\u9884\u6d4b\u67e5\u8be2\u884c\u4e2d\u7684\u7f3a\u5931\u76ee\u6807\u503c\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u4e00\u529f\u80fd\uff0c\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6TabSieve-SFT-40K\uff0c\u5e76\u5f15\u5165\u4e86TAB-GRPO\u8fd9\u4e00\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u540e\u8005\u80fd\u591f\u540c\u65f6\u4f18\u5316\u8bc1\u636e\u9009\u62e9\u548c\u9884\u6d4b\u51c6\u786e\u6027\u7684\u5956\u52b1\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u4f18\u52bf\u5e73\u8861\u6765\u7a33\u5b9a\u6df7\u5408\u56de\u5f52\u548c\u5206\u7c7b\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u5305\u542b75\u4e2a\u5206\u7c7b\u8868\u548c52\u4e2a\u56de\u5f52\u8868\u7684\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u6b21\u4f18\u57fa\u7ebf\u76f8\u6bd4\uff0cTabSieve\u5728\u6240\u6709\u6837\u672c\u9884\u7b97\u6761\u4ef6\u4e0b\u90fd\u80fd\u6301\u7eed\u63d0\u9ad8\u8868\u73b0\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e862.92%\uff0c\u5728\u56de\u5f52\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u5347\u4e864.45%\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0cTabSieve\u66f4\u96c6\u4e2d\u5173\u6ce8\u4e8e\u9009\u5b9a\u7684\u8bc1\u636e\u4e0a\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u9ad8\u5bf9\u566a\u58f0\u73af\u5883\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "TabSieve\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u6539\u5584\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u901a\u8fc7\u66f4\u597d\u5730\u5229\u7528\u8868\u5185\u5c11\u6570\u793a\u4f8b\u4f5c\u4e3a\u8bc1\u636e\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u4e8e\u566a\u97f3\u6570\u636e\u7684\u62b5\u6297\u529b\u3002"}}
{"id": "2602.11712", "categories": ["cs.LG", "cs.CE", "nlin.CD", "physics.data-an", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.11712", "abs": "https://arxiv.org/abs/2602.11712", "authors": ["Luigi Simeone"], "title": "Potential-energy gating for robust state estimation in bistable stochastic systems", "comment": "20 pages, 8 figures", "summary": "We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52bf\u80fd\u95e8\u63a7\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u53cc\u9631\u968f\u673a\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u7684\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\u3002\u901a\u8fc7\u5c06\u89c2\u6d4b\u566a\u58f0\u534f\u65b9\u5dee\u4e0e\u5df2\u77e5\u6216\u5047\u8bbe\u7684\u52bf\u80fd\u51fd\u6570\u5c40\u90e8\u503c\u8054\u7cfb\u8d77\u6765\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u72b6\u6001\u63a5\u8fd1\u52bf\u80fd\u6781\u5c0f\u503c\u7684\u7a0b\u5ea6\u6765\u8c03\u6574\u5bf9\u89c2\u6d4b\u7684\u4fe1\u4efb\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6b64\u65b9\u6cd5\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u90fd\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7b49\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u5728\u5904\u7406\u6240\u6709\u72b6\u6001\u7a7a\u95f4\u533a\u57df\u65f6\u91c7\u7528\u7edf\u4e00\u7684\u65b9\u5f0f\uff0c\u800c\u7ea6\u675f\u6ee4\u6ce2\u5668\u5219\u901a\u8fc7\u5bf9\u72b6\u6001\u65bd\u52a0\u786c\u6027\u8fb9\u754c\u6765\u9650\u5236\u72b6\u6001\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u52bf\u80fd\u666f\u89c2\u7684\u7cfb\u7edf\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u52bf\u80fd\u95e8\u63a7\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u8003\u8651\u7269\u7406\u673a\u5236\u6765\u6539\u8fdb\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u7684\u52bf\u80fd\u95e8\u63a7\u65b9\u6cd5\u901a\u8fc7\u8c03\u8282\u89c2\u6d4b\u566a\u58f0\u534f\u65b9\u5dee\u5b9e\u73b0\uff0c\u5177\u4f53\u800c\u8a00\u662f\u4f9d\u636e\u5f53\u524d\u72b6\u6001\u4e0b\u7684\u52bf\u80fd\u503c\u52a8\u6001\u8c03\u6574\u5bf9\u89c2\u6d4b\u7684\u4fe1\u4efb\u7a0b\u5ea6\u3002\u8be5\u6280\u672f\u88ab\u96c6\u6210\u5230\u51e0\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4ee5\u53ca\u7c92\u5b50\u6ee4\u6ce2\u5668\u4e2d\uff0c\u5e76\u4ec5\u9700\u5f15\u5165\u4e24\u4e2a\u989d\u5916\u8d85\u53c2\u6570\u5373\u53ef\u5b9e\u65bd\u3002", "result": "\u5728Ginzburg-Landau\u53cc\u9631\u8fc7\u7a0b\u7684\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5373\u4f7f\u5b58\u572810%\u7684\u5f02\u5e38\u503c\u6c61\u67d3\u60c5\u51b5\u4e0b\uff0c\u76f8\u8f83\u4e8e\u6807\u51c6\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u6240\u63d0\u65b9\u6cd5\u5b9e\u73b0\u4e8657-80%\u7684RMSE\uff08\u5747\u65b9\u6839\u8bef\u5dee\uff09\u964d\u4f4e\u3002\u6b64\u5916\uff0c\u5728\u5e94\u7528\u4e8eNGRIP delta-18O\u51b0\u82af\u8bb0\u5f55\u7684\u6570\u636e\u5206\u6790\u65f6\uff0c\u4e5f\u5c55\u793a\u4e86\u5176\u5728\u4f30\u8ba1\u975e\u5bf9\u79f0\u53c2\u6570\u03b3\u53ca\u89e3\u91ca\u8fc7\u6ee4\u5668\u6027\u80fd\u6539\u8fdb\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u52bf\u80fd\u95e8\u63a7\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u9762\u5bf9\u5f02\u5e38\u503c\u65f6\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u800c\u4e14\u5bf9\u4e8e\u6f5c\u5728\u53c2\u6570\u9519\u8bef\u6307\u5b9a\u7684\u60c5\u51b5\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u5bb9\u5fcd\u5ea6\u3002\u8fd9\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\u4e0a\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.11715", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11715", "abs": "https://arxiv.org/abs/2602.11715", "authors": ["Haolei Bai", "Lingcheng Kong", "Xueyi Chen", "Jianmian Wang", "Zhiqiang Tao", "Huan Wang"], "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels", "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u7cbe\u9009\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08BiC-RL\uff09\u548c\u4e00\u4e2a\u9488\u5bf9CUDA\u5185\u6838\u751f\u6210\u4f18\u5316\u7684\u6570\u636e\u96c6CuKe\uff0c\u7528\u4e8e\u8bad\u7ec3\u540d\u4e3aDICE\u7684\u4e00\u7cfb\u5217\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDICE\u5728CUDA\u5185\u6838\u751f\u6210\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u540c\u7c7b\u89c4\u6a21\u7684\u81ea\u56de\u5f52\u548c\u6269\u6563\u5927\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728CUDA\u5185\u6838\u751f\u6210\u4e0a\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u9886\u57df\u9ad8\u5ea6\u4e13\u4e1a\u5316\u4ee5\u53ca\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4e25\u91cd\u7f3a\u4e4f\u3002", "method": "\u5f00\u53d1\u4e86CuKe\uff0c\u4e00\u4e2a\u4e13\u4e3a\u9ad8\u6027\u80fdCUDA\u5185\u6838\u800c\u4f18\u5316\u7684\u589e\u5f3a\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542bCUDA\u5185\u6838\u586b\u5145\u9636\u6bb5\u548c\u7aef\u5230\u7aefCUDA\u5185\u6838\u751f\u6210\u9636\u6bb5\u7684\u53cc\u9636\u6bb5\u7cbe\u9009\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08BiC-RL\uff09\u3002\u57fa\u4e8e\u6b64\u8bad\u7ec3\u6846\u67b6\uff0c\u7814\u7a76\u8005\u4eec\u63a8\u51fa\u4e86DICE\uff0c\u4e00\u7cfb\u5217\u4e13\u95e8\u8bbe\u8ba1\u7528\u6765\u751f\u6210CUDA\u5185\u6838\u7684\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u901a\u8fc7KernelBench\u5e73\u53f0\u8fdb\u884c\u7684\u5e7f\u6cdb\u6d4b\u8bd5\u663e\u793a\uff0cDICE\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u4e0b\u7684\u5176\u4ed6\u81ea\u56de\u5f52\u53ca\u6269\u6563\u578b\u5927\u6a21\u578b\uff0c\u5728CUDA\u5185\u6838\u751f\u6210\u9886\u57df\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u4e0e\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6269\u6563\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u5982CUDA\u5185\u6838\u751f\u6210\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.11726", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11726", "abs": "https://arxiv.org/abs/2602.11726", "authors": ["Shervin Ghasemlou"], "title": "Dopamine: Brain Modes, Not Brains", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods such as \\lora{} adapt large pretrained models by adding small weight-space updates. While effective, weight deltas are hard to interpret mechanistically, and they do not directly expose \\emph{which} internal computations are reused versus bypassed for a new task. We explore an alternative view inspired by neuromodulation: adaptation as a change in \\emph{mode} -- selecting and rescaling existing computations -- rather than rewriting the underlying weights. We propose \\methodname{}, a simple activation-space PEFT technique that freezes base weights and learns per-neuron \\emph{thresholds} and \\emph{gains}. During training, a smooth gate decides whether a neuron's activation participates; at inference the gate can be hardened to yield explicit conditional computation and neuron-level attributions.\n  As a proof of concept, we study ``mode specialization'' on MNIST (0$^\\circ$) versus rotated MNIST (45$^\\circ$). We pretrain a small MLP on a 50/50 mixture (foundation), freeze its weights, and then specialize to the rotated mode using \\methodname{}. Across seeds, \\methodname{} improves rotated accuracy over the frozen baseline while using only a few hundred trainable parameters per layer, and exhibits partial activation sparsity (a minority of units strongly active). Compared to \\lora{}, \\methodname{} trades some accuracy for substantially fewer trainable parameters and a more interpretable ``which-neurons-fire'' mechanism. We discuss limitations, including reduced expressivity when the frozen base lacks features needed for the target mode.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u6bcf\u4e2a\u795e\u7ecf\u5143\u7684\u9608\u503c\u548c\u589e\u76ca\u6765\u9009\u62e9\u5e76\u91cd\u65b0\u7f29\u653e\u73b0\u6709\u7684\u8ba1\u7b97\uff0c\u800c\u4e0d\u662f\u91cd\u5199\u57fa\u7840\u6743\u91cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728MNIST\u53ca\u5176\u65cb\u8f6c\u7248\u672c\u4e0a\u6709\u6548\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u96be\u4ee5\u4ece\u673a\u5236\u4e0a\u89e3\u91ca\uff0c\u5e76\u4e14\u4e0d\u76f4\u63a5\u663e\u793a\u54ea\u4e9b\u5185\u90e8\u8ba1\u7b97\u88ab\u91cd\u7528\u6216\u7ed5\u8fc7\u3002\u53d7\u795e\u7ecf\u8c03\u8282\u542f\u53d1\uff0c\u4f5c\u8005\u63a2\u7d22\u4e86\u53e6\u4e00\u79cd\u9002\u5e94\u65b9\u5f0f\u2014\u2014\u6539\u53d8\u6a21\u5f0f\uff0c\u5373\u9009\u62e9\u548c\u8c03\u6574\u73b0\u6709\u8ba1\u7b97\u800c\u975e\u4fee\u6539\u5e95\u5c42\u6743\u91cd\u3002", "method": "\u63d0\u51fa\u4e86\\methodname{}\uff0c\u4e00\u79cd\u7b80\u5355\u7684\u6fc0\u6d3b\u7a7a\u95f4PEFT\u6280\u672f\uff0c\u51bb\u7ed3\u57fa\u7840\u6743\u91cd\u5e76\u5b66\u4e60\u6bcf\u4e2a\u795e\u7ecf\u5143\u7684\u9608\u503c\u548c\u589e\u76ca\u3002\u8bad\u7ec3\u65f6\uff0c\u5e73\u6ed1\u95e8\u63a7\u51b3\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u662f\u5426\u53c2\u4e0e\uff1b\u63a8\u7406\u65f6\uff0c\u95e8\u63a7\u53ef\u4ee5\u786c\u5316\u4ee5\u4ea7\u751f\u663e\u5f0f\u7684\u6761\u4ef6\u8ba1\u7b97\u548c\u795e\u7ecf\u5143\u7ea7\u5f52\u56e0\u3002", "result": "\u5728MNIST\u4e0e\u517645\u5ea6\u65cb\u8f6c\u7248\u672c\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0c\\methodname{}\u76f8\u6bd4\u51bb\u7ed3\u57fa\u7ebf\u63d0\u9ad8\u4e86\u65cb\u8f6c\u7cbe\u5ea6\uff0c\u6bcf\u5c42\u4ec5\u4f7f\u7528\u51e0\u767e\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5e76\u8868\u73b0\u51fa\u90e8\u5206\u6fc0\u6d3b\u7a00\u758f\u6027\u3002\u4e0e\\lora{}\u76f8\u6bd4\uff0c\\methodname{}\u727a\u7272\u4e86\u4e00\u4e9b\u51c6\u786e\u6027\uff0c\u4f46\u5927\u5e45\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u6613\u7406\u89e3\u7684\u201c\u54ea\u4e9b\u795e\u7ecf\u5143\u6fc0\u53d1\u201d\u673a\u5236\u3002", "conclusion": "\u5c3d\u7ba1\\methodname{}\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u7531\u4e8e\u51bb\u7ed3\u7684\u57fa\u7840\u7f3a\u4e4f\u76ee\u6807\u6a21\u5f0f\u6240\u9700\u7279\u5f81\u800c\u8868\u8fbe\u529b\u6709\u9650\uff0c\u4f46\u5b83\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u66f4\u52a0\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.11738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11738", "abs": "https://arxiv.org/abs/2602.11738", "authors": ["Ilya Kuleshov", "Alexander Marusov", "Alexey Zaytsev"], "title": "U-Former ODE: Fast Probabilistic Forecasting of Irregular Time Series", "comment": null, "summary": "Probabilistic forecasting of irregularly sampled time series is crucial in domains such as healthcare and finance, yet it remains a formidable challenge. Existing Neural Controlled Differential Equation (Neural CDE) approaches, while effective at modelling continuous dynamics, suffer from slow, inherently sequential computation, which restricts scalability and limits access to global context. We introduce UFO (U-Former ODE), a novel architecture that seamlessly integrates the parallelizable, multiscale feature extraction of U-Nets, the powerful global modelling of Transformers, and the continuous-time dynamics of Neural CDEs. By constructing a fully causal, parallelizable model, UFO achieves a global receptive field while retaining strong sensitivity to local temporal dynamics. Extensive experiments on five standard benchmarks -- covering both regularly and irregularly sampled time series -- demonstrate that UFO consistently outperforms ten state-of-the-art neural baselines in predictive accuracy. Moreover, UFO delivers up to 15$\\times$ faster inference compared to conventional Neural CDEs, with consistently strong performance on long and highly multivariate sequences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784UFO\uff08U-Former ODE\uff09\uff0c\u5b83\u7ed3\u5408\u4e86U-Nets\u7684\u5e76\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001Transformers\u7684\u5f3a\u5927\u5168\u5c40\u5efa\u6a21\u4ee5\u53caNeural CDEs\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u7279\u6027\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u6982\u7387\u9884\u6d4b\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u663e\u793aUFO\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u5341\u79cd\u6700\u65b0\u7684\u795e\u7ecf\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u6bd4\u4f20\u7edf\u7684Neural CDEs\u5feb15\u500d\u3002", "motivation": "\u5728\u533b\u7597\u4fdd\u5065\u548c\u91d1\u878d\u7b49\u9886\u57df\uff0c\u5bf9\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u6982\u7387\u9884\u6d4b\u975e\u5e38\u91cd\u8981\u4f46\u540c\u65f6\u4e5f\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u591f\u5f88\u597d\u5730\u6a21\u62df\u8fde\u7eed\u52a8\u6001\u8fc7\u7a0b\uff0c\u4f46\u7531\u4e8e\u5176\u8ba1\u7b97\u672c\u8d28\u4e0a\u662f\u987a\u5e8f\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u53ef\u6269\u5c55\u6027\u5e76\u4e14\u96be\u4ee5\u83b7\u5f97\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86UFO\uff08U-Former ODE\uff09\u8fd9\u4e00\u65b0\u9896\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5c06U-Nets\u7684\u53ef\u5e76\u884c\u5316\u591a\u5c3a\u5ea6\u7279\u5f81\u62bd\u53d6\u80fd\u529b\u3001Transformers\u5f3a\u5927\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\u4e0e\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b(Neural CDEs)\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5b8c\u5168\u56e0\u679c\u5173\u7cfb\u4e14\u53ef\u5e76\u884c\u5316\u7684\u6a21\u578b\uff0cUFO\u80fd\u591f\u5728\u4fdd\u6301\u5bf9\u5c40\u90e8\u65f6\u95f4\u52a8\u6001\u654f\u611f\u7684\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u611f\u53d7\u91ce\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6db5\u76d6\u5e38\u89c4\u548c\u975e\u5e38\u89c4\u91c7\u6837\u7684\u4e94\u9879\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUFO\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5341\u4e2a\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u57fa\u7ebf\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4e0e\u4f20\u7edfNeural CDEs\u76f8\u6bd4\uff0cUFO\u63d0\u4f9b\u4e86\u9ad8\u8fbe15\u500d\u7684\u66f4\u5feb\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u4e14\u5728\u957f\u5e8f\u5217\u53ca\u9ad8\u5ea6\u591a\u53d8\u91cf\u5e8f\u5217\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "UFO\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u6548\u7387\uff0c\u4e3a\u89e3\u51b3\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u7684\u6982\u7387\u9884\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11779", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11779", "abs": "https://arxiv.org/abs/2602.11779", "authors": ["Haoran Dang", "Cuiling Lan", "Hai Wan", "Xibin Zhao", "Yan Lu"], "title": "Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning", "comment": "Accepted at ICLR 2026. 10 pages (main text) + supplementary material, 6 figures", "summary": "Temperature is a crucial hyperparameter in large language models (LLMs), controlling the trade-off between exploration and exploitation during text generation. High temperatures encourage diverse but noisy outputs, while low temperatures produce focused outputs but may cause premature convergence. Yet static or heuristic temperature schedules fail to adapt to the dynamic demands of reinforcement learning (RL) throughout training, often limiting policy improvement. We propose Temperature Adaptive Meta Policy Optimization (TAMPO), a new framework that recasts temperature control as a learnable meta-policy. TAMPO operates through a hierarchical two-loop process. In the inner loop, the LLM policy is updated (e.g., using GRPO) with trajectories sampled at the temperature selected by the meta-policy. In the outer loop, meta-policy updates the distribution over candidate temperatures by rewarding those that maximize the likelihood of high-advantage trajectories. This trajectory-guided, reward-driven mechanism enables online adaptation without additional rollouts, directly aligning exploration with policy improvement. On five mathematical reasoning benchmarks, TAMPO outperforms baselines using fixed or heuristic temperatures, establishing temperature as an effective learnable meta-policy for adaptive exploration in LLM reinforcement learning. Accepted at ICLR 2026.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6TAMPO\uff0c\u901a\u8fc7\u5c06\u6e29\u5ea6\u63a7\u5236\u89c6\u4e3a\u53ef\u5b66\u4e60\u7684\u5143\u7b56\u7565\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002TAMPO\u91c7\u7528\u53cc\u5c42\u5faa\u73af\u8fc7\u7a0b\uff0c\u5185\u5faa\u73af\u4e2d\u4f7f\u7528\u5143\u7b56\u7565\u9009\u5b9a\u7684\u6e29\u5ea6\u66f4\u65b0\u7b56\u7565\uff0c\u5916\u5faa\u73af\u5219\u6839\u636e\u8f68\u8ff9\u4f18\u52bf\u8c03\u6574\u6e29\u5ea6\u5206\u5e03\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTAMPO\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4f18\u4e8e\u4f7f\u7528\u56fa\u5b9a\u6216\u542f\u53d1\u5f0f\u6e29\u5ea6\u8bbe\u5b9a\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u9759\u6001\u6216\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u6e29\u5ea6\u8c03\u5ea6\u65b9\u6848\u65e0\u6cd5\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\uff0c\u9650\u5236\u4e86\u7b56\u7565\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u5e73\u8861\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Temperature Adaptive Meta Policy Optimization (TAMPO)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u628a\u6e29\u5ea6\u63a7\u5236\u5f53\u4f5c\u4e00\u4e2a\u53ef\u4ee5\u5b66\u4e60\u7684\u5143\u7b56\u7565\u3002TAMPO\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a\u5185\u5faa\u73af\u548c\u5916\u5faa\u73af\u3002\u5185\u5faa\u73af\u91cc\uff0c\u6309\u7167\u5143\u7b56\u7565\u9009\u62e9\u7684\u6e29\u5ea6\u503c\u66f4\u65b0LLM\u7b56\u7565\uff1b\u5916\u5faa\u73af\uff0c\u5219\u4f9d\u636e\u9ad8\u4f18\u52bf\u8f68\u8ff9\u7684\u53ef\u80fd\u6027\u5956\u52b1\u7279\u5b9a\u6e29\u5ea6\u503c\uff0c\u4ece\u800c\u8c03\u6574\u5019\u9009\u6e29\u5ea6\u503c\u7684\u6982\u7387\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cTAMPO\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u4f7f\u7528\u56fa\u5b9a\u6216\u542f\u53d1\u5f0f\u6e29\u5ea6\u8bbe\u7f6e\u7684\u65b9\u6cd5\u3002", "conclusion": "TAMPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u5982\u4f55\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u81ea\u9002\u5e94\u5730\u8c03\u8282\u6e29\u5ea6\u53c2\u6570\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u63a2\u7d22\u4e0e\u7b56\u7565\u63d0\u5347\u3002"}}
{"id": "2602.11785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11785", "abs": "https://arxiv.org/abs/2602.11785", "authors": ["Ainhize Barrainkua", "Santiago Mazuelas", "Novi Quadrianto", "Jose A. Lozano"], "title": "Safe Fairness Guarantees Without Demographics in Classification: Spectral Uncertainty Set Perspective", "comment": null, "summary": "As automated classification systems become increasingly prevalent, concerns have emerged over their potential to reinforce and amplify existing societal biases. In the light of this issue, many methods have been proposed to enhance the fairness guarantees of classifiers. Most of the existing interventions assume access to group information for all instances, a requirement rarely met in practice. Fairness without access to demographic information has often been approached through robust optimization techniques,which target worst-case outcomes over a set of plausible distributions known as the uncertainty set. However, their effectiveness is strongly influenced by the chosen uncertainty set. In fact, existing approaches often overemphasize outliers or overly pessimistic scenarios, compromising both overall performance and fairness. To overcome these limitations, we introduce SPECTRE, a minimax-fair method that adjusts the spectrum of a simple Fourier feature mapping and constrains the extent to which the worst-case distribution can deviate from the empirical distribution. We perform extensive experiments on the American Community Survey datasets involving 20 states. The safeness of SPECTRE comes as it provides the highest average values on fairness guarantees together with the smallest interquartile range in comparison to state-of-the-art approaches, even compared to those with access to demographic group information. In addition, we provide a theoretical analysis that derives computable bounds on the worst-case error for both individual groups and the overall population, as well as characterizes the worst-case distributions responsible for these extremal performances", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPECTRE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u7b80\u5355\u5085\u91cc\u53f6\u7279\u5f81\u6620\u5c04\u7684\u9891\u8c31\u5e76\u9650\u5236\u6700\u574f\u60c5\u51b5\u5206\u5e03\u4e0e\u7ecf\u9a8c\u5206\u5e03\u4e4b\u95f4\u7684\u504f\u5dee\u7a0b\u5ea6\u6765\u89e3\u51b3\u5728\u6ca1\u6709\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u516c\u5e73\u6027\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u4e0e\u90a3\u4e9b\u53ef\u4ee5\u8bbf\u95ee\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4fe1\u606f\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cSPECTRE\u4e5f\u80fd\u63d0\u4f9b\u6700\u9ad8\u7684\u5e73\u5747\u516c\u5e73\u4fdd\u8bc1\u503c\u4ee5\u53ca\u6700\u5c0f\u7684\u56db\u5206\u4f4d\u8ddd\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u5316\u5206\u7c7b\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u62c5\u5fc3\u5b83\u4eec\u53ef\u80fd\u5f3a\u5316\u548c\u653e\u5927\u73b0\u6709\u7684\u793e\u4f1a\u504f\u89c1\u3002\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u65b9\u6cd5\u88ab\u63d0\u51fa\u6765\u63d0\u9ad8\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\u4fdd\u8bc1\uff0c\u4f46\u5927\u591a\u6570\u73b0\u6709\u5e72\u9884\u63aa\u65bd\u5047\u5b9a\u80fd\u591f\u83b7\u53d6\u6240\u6709\u5b9e\u4f8b\u7684\u7fa4\u4f53\u4fe1\u606f\uff0c\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5f88\u5c11\u6ee1\u8db3\u3002\u6b64\u5916\uff0c\u5728\u6ca1\u6709\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8ffd\u6c42\u516c\u5e73\u6027\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u9c81\u68d2\u4f18\u5316\u6280\u672f\uff0c\u800c\u8fd9\u4e9b\u6280\u672f\u7684\u6709\u6548\u6027\u53d7\u5230\u6240\u9009\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u5f80\u5f80\u8fc7\u5ea6\u5f3a\u8c03\u5f02\u5e38\u503c\u6216\u8fc7\u4e8e\u60b2\u89c2\u7684\u60c5\u51b5\uff0c\u4ece\u800c\u635f\u5bb3\u4e86\u6574\u4f53\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86SPECTRE\uff0c\u8fd9\u662f\u4e00\u79cd\u6781\u5c0f\u6781\u5927\u516c\u5e73\u6cd5\uff0c\u5b83\u901a\u8fc7\u8c03\u8282\u4e00\u4e2a\u7b80\u5355\u7684\u5085\u91cc\u53f6\u7279\u5f81\u6620\u5c04\u7684\u9891\u8c31\uff0c\u5e76\u7ea6\u675f\u6700\u574f\u60c5\u51b5\u4e0b\u5206\u5e03\u504f\u79bb\u7ecf\u9a8c\u5206\u5e03\u7684\u7a0b\u5ea6\u3002", "result": "\u901a\u8fc7\u5bf9\u7f8e\u56fd\u793e\u533a\u8c03\u67e5\u6570\u636e\u96c6\uff08\u6d89\u53ca20\u4e2a\u5dde\uff09\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0cSPECTRE\u63d0\u4f9b\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u516c\u5e73\u6027\u4fdd\u8bc1\u503c\u53ca\u6700\u5c0f\u7684\u56db\u5206\u4f4d\u8ddd\uff0c\u751a\u81f3\u6bd4\u90a3\u4e9b\u80fd\u8bbf\u95ee\u5230\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4fe1\u606f\u7684\u65b9\u6cd5\u8868\u73b0\u8fd8\u8981\u597d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5f97\u51fa\u4e86\u4e2a\u4f53\u7fa4\u4f53\u548c\u603b\u4f53\u6700\u574f\u60c5\u51b5\u8bef\u5dee\u7684\u53ef\u8ba1\u7b97\u8fb9\u754c\uff0c\u5e76\u63cf\u8ff0\u4e86\u5bfc\u81f4\u8fd9\u4e9b\u6781\u7aef\u8868\u73b0\u7684\u6700\u574f\u60c5\u51b5\u5206\u5e03\u7684\u7279\u70b9\u3002", "conclusion": "SPECTRE\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u65e0\u987b\u4f9d\u8d56\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u5373\u53ef\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u516c\u5e73\u6027\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u516c\u5e73\u6027\u7684\u540c\u65f6\u4e5f\u7ef4\u6301\u4e86\u8f83\u597d\u7684\u6027\u80fd\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.11786", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11786", "abs": "https://arxiv.org/abs/2602.11786", "authors": ["Keita Broadwater"], "title": "Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing", "comment": "24 pages, 9 figures. Submitted to TMLR", "summary": "Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a0\u901f\u63d0\u793a\u538b\u529b\u6d4b\u8bd5\uff08APST\uff09\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u590d\u63a8\u7406\u76f8\u540c\u6216\u51e0\u4e4e\u76f8\u540c\u7684\u63d0\u793a\u65f6\u7684\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u53d1\u73b0\u6f5c\u5728\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u4f2f\u52aa\u5229\u548c\u4e8c\u9879\u5f0f\u6a21\u578b\u91cf\u5316\u6bcf\u6b21\u63a8\u7406\u7684\u5931\u8d25\u6982\u7387\uff0c\u4ece\u800c\u8865\u5145\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u6301\u7eed\u4f7f\u7528\u573a\u666f\u4e0b\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u901a\u8fc7\u8de8\u591a\u79cd\u4efb\u52a1\u7684\u5e7f\u5ea6\u5bfc\u5411\u8bc4\u4f30\u6765\u8861\u91cf\u5b89\u5168\u98ce\u9669\u3002\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u66f4\u591a\u9047\u5230\u7684\u662f\u7531\u4e8e\u5bf9\u76f8\u540c\u6216\u975e\u5e38\u76f8\u4f3c\u7684\u63d0\u793a\u8fdb\u884c\u91cd\u590d\u63a8\u7406\u800c\u5bfc\u81f4\u7684\u64cd\u4f5c\u5931\u8d25\uff0c\u800c\u975e\u5e7f\u6cdb\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0c\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u5c24\u4e3a\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u5728\u6301\u7eed\u4f7f\u7528\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86\u52a0\u901f\u63d0\u793a\u538b\u529b\u6d4b\u8bd5\uff08APST\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u5230\u53ef\u9760\u6027\u5de5\u7a0b\u542f\u53d1\u7684\u6df1\u5ea6\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u63a7\u5236\u64cd\u4f5c\u6761\u4ef6\u4e0b\uff08\u5982\u89e3\u7801\u6e29\u5ea6\uff09\u76f8\u540c\u63d0\u793a\u7684\u53cd\u590d\u91c7\u6837\uff0c\u63ed\u793a\u5305\u62ec\u5e7b\u89c9\u3001\u62d2\u7edd\u4e0d\u4e00\u81f4\u4ee5\u53ca\u4e0d\u5b89\u5168\u5b8c\u6210\u5728\u5185\u7684\u6f5c\u5728\u6545\u969c\u6a21\u5f0f\u3002APST\u5c06\u6545\u969c\u89c6\u4e3a\u72ec\u7acb\u63a8\u7406\u4e8b\u4ef6\u968f\u673a\u7ed3\u679c\u7684\u4e00\u90e8\u5206\uff0c\u5e76\u91c7\u7528\u4f2f\u52aa\u5229\u548c\u4e8c\u9879\u5f0f\u6a21\u578b\u6765\u6b63\u5f0f\u5b9a\u4e49\u5b89\u5168\u6545\u969c\uff0c\u4ee5\u4f30\u8ba1\u6bcf\u6b21\u63a8\u7406\u7684\u5931\u8d25\u6982\u7387\u3002", "result": "\u5f53\u5c06APST\u5e94\u7528\u4e8e\u591a\u4e2a\u57fa\u4e8eAIR-BENCH\u884d\u751f\u7684\u5b89\u5168\u63d0\u793a\u8fdb\u884c\u8bc4\u6d4b\u7684\u6307\u4ee4\u8c03\u4f18LLM\u65f6\uff0c\u7814\u7a76\u53d1\u73b0\u5177\u6709\u7c7b\u4f3c\u57fa\u51c6\u5bf9\u9f50\u5206\u6570\u7684\u6a21\u578b\uff0c\u5728\u91cd\u590d\u91c7\u6837\u5c24\u5176\u662f\u968f\u7740\u6e29\u5ea6\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u540c\u7684\u5b9e\u9645\u5931\u8d25\u7387\u3002\u8fd9\u8868\u660e\u6d45\u5c42\u5355\u6837\u672c\u8bc4\u4f30\u53ef\u80fd\u63a9\u76d6\u4e86\u5728\u6301\u7eed\u4f7f\u7528\u60c5\u51b5\u4e0b\u6709\u610f\u4e49\u7684\u53ef\u9760\u6027\u5dee\u5f02\u3002", "conclusion": "APST\u4e3a\u8bc4\u4f30LLM\u5728\u91cd\u590d\u63a8\u7406\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u4e0e\u9762\u5411\u90e8\u7f72\u7684\u98ce\u9669\u8bc4\u4f30\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.11805", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11805", "abs": "https://arxiv.org/abs/2602.11805", "authors": ["Ziyi Zhao", "Qingchuan Li", "Yuxuan Xu"], "title": "From Path Signatures to Sequential Modeling: Incremental Signature Contributions for Offline RL", "comment": null, "summary": "Path signatures embed trajectories into tensor algebra and constitute a universal, non-parametric representation of paths; however, in the standard form, they collapse temporal structure into a single global object, which limits their suitability for decision-making problems that require step-wise reactivity. We propose the Incremental Signature Contribution (ISC) method, which decomposes truncated path signatures into a temporally ordered sequence of elements in the tensor-algebra space, corresponding to incremental contributions induced by last path increments. This reconstruction preserves the algebraic structure and expressivity of signatures, while making their internal temporal evolution explicit, enabling processing signature-based representations via sequential modeling approaches. In contrast to full signatures, ISC is inherently sensitive to instantaneous trajectory updates, which is critical for sensitive and stability-requiring control dynamics. Building on this representation, we introduce ISC-Transformer (ISCT), an offline reinforcement learning model that integrates ISC into a standard Transformer architecture without further architectural modification. We evaluate ISCT on HalfCheetah, Walker2d, Hopper, and Maze2d, including settings with delayed rewards and downgraded datasets. The results demonstrate that ISC method provides a theoretically grounded and practically effective alternative to path processing for temporally sensitive control tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u91cf\u7b7e\u540d\u8d21\u732e\uff08ISC\uff09\u65b9\u6cd5\uff0c\u5c06\u8def\u5f84\u7b7e\u540d\u5206\u89e3\u4e3a\u5f20\u91cf\u4ee3\u6570\u7a7a\u95f4\u4e2d\u7684\u65f6\u95f4\u6709\u5e8f\u5e8f\u5217\uff0c\u4ece\u800c\u4fdd\u7559\u4e86\u7b7e\u540d\u7684\u4ee3\u6570\u7ed3\u6784\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u4f7f\u5185\u90e8\u65f6\u95f4\u6f14\u53d8\u663e\u5f0f\u5316\u3002\u57fa\u4e8e\u6b64\u8868\u793a\uff0c\u4f5c\u8005\u5f15\u5165\u4e86ISC-Transformer (ISCT)\uff0c\u8fd9\u662f\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u6807\u51c6Transformer\u67b6\u6784\u4e2d\u96c6\u6210\u4e86ISC\u800c\u65e0\u9700\u8fdb\u4e00\u6b65\u7684\u67b6\u6784\u4fee\u6539\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cISC\u65b9\u6cd5\u4e3a\u65f6\u95f4\u654f\u611f\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u4e14\u5b9e\u7528\u6709\u6548\u7684\u8def\u5f84\u5904\u7406\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u7684\u8def\u5f84\u7b7e\u540d\u65b9\u6cd5\u867d\u7136\u80fd\u591f\u63d0\u4f9b\u975e\u53c2\u6570\u5316\u7684\u8def\u5f84\u8868\u793a\uff0c\u4f46\u5b83\u4eec\u5c06\u65f6\u95f4\u7ed3\u6784\u6298\u53e0\u6210\u4e00\u4e2a\u5168\u5c40\u5bf9\u8c61\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u9010\u6b65\u53cd\u5e94\u7684\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u589e\u91cf\u7b7e\u540d\u8d21\u732e(ISC)\u65b9\u6cd5\uff0c\u65e8\u5728\u4fdd\u6301\u7b7e\u540d\u8868\u8fbe\u529b\u7684\u540c\u65f6\uff0c\u8ba9\u7b7e\u540d\u7684\u5185\u90e8\u65f6\u95f4\u6f14\u5316\u53d8\u5f97\u660e\u786e\u3002", "method": "\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u589e\u91cf\u7b7e\u540d\u8d21\u732e(ISC)\u65b9\u6cd5\uff0c\u5b83\u5c06\u622a\u65ad\u8def\u5f84\u7b7e\u540d\u5206\u89e3\u4e3a\u6309\u7167\u65f6\u95f4\u987a\u5e8f\u6392\u5217\u7684\u4e00\u7cfb\u5217\u5143\u7d20\uff0c\u8fd9\u4e9b\u5143\u7d20\u5bf9\u5e94\u4e8e\u6700\u540e\u8def\u5f84\u589e\u91cf\u6240\u5f15\u8d77\u7684\u589e\u91cf\u8d21\u732e\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u57fa\u4e8eISC\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aISC-Transformer (ISCT)\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06ISC\u76f4\u63a5\u96c6\u6210\u5230\u4e86\u6807\u51c6\u7684Transformer\u67b6\u6784\u4e2d\uff0c\u6ca1\u6709\u8fdb\u884c\u989d\u5916\u7684\u67b6\u6784\u8c03\u6574\u3002", "result": "\u901a\u8fc7\u5728HalfCheetah\u3001Walker2d\u3001Hopper\u4ee5\u53caMaze2d\u7b49\u73af\u5883\u4e0b\u7684\u6d4b\u8bd5\uff0c\u5305\u62ec\u90a3\u4e9b\u5177\u6709\u5ef6\u8fdf\u5956\u52b1\u548c\u964d\u7ea7\u6570\u636e\u96c6\u7684\u60c5\u51b5\uff0c\u7ed3\u679c\u663e\u793aISC\u65b9\u6cd5\u5bf9\u4e8e\u65f6\u95f4\u654f\u611f\u578b\u63a7\u5236\u4efb\u52a1\u65e2\u5177\u5907\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u53c8\u8868\u73b0\u51fa\u5b9e\u9645\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u589e\u91cf\u7b7e\u540d\u8d21\u732e\uff08ISC\uff09\u65b9\u6cd5\u53ca\u5176\u5e94\u7528\u5230\u7684ISC-Transformer\u6a21\u578b\u6210\u529f\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u8def\u5f84\u7b7e\u540d\u65b9\u6cd5\u5728\u65f6\u95f4\u654f\u611f\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8fd9\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11825", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.11825", "abs": "https://arxiv.org/abs/2602.11825", "authors": ["Fei Jiang", "Jiyang Xia", "Junjie Yu", "Mingfei Sun", "Hugh Coe", "David Topping", "Dantong Liu", "Zhenhui Jessie Li", "Zhonghua Zheng"], "title": "CAAL: Confidence-Aware Active Learning for Heteroscedastic Atmospheric Regression", "comment": "17 pages in total", "summary": "Quantifying the impacts of air pollution on health and climate relies on key atmospheric particle properties such as toxicity and hygroscopicity. However, these properties typically require complex observational techniques or expensive particle-resolved numerical simulations, limiting the availability of labeled data. We therefore estimate these hard-to-measure particle properties from routinely available observations (e.g., air pollutant concentrations and meteorological conditions). Because routine observations only indirectly reflect particle composition and structure, the mapping from routine observations to particle properties is noisy and input-dependent, yielding a heteroscedastic regression setting. With a limited and costly labeling budget, the central challenge is to select which samples to measure or simulate. While active learning is a natural approach, most acquisition strategies rely on predictive uncertainty. Under heteroscedastic noise, this signal conflates reducible epistemic uncertainty with irreducible aleatoric uncertainty, causing limited budgets to be wasted in noise-dominated regions. To address this challenge, we propose a confidence-aware active learning framework (CAAL) for efficient and robust sample selection in heteroscedastic settings. CAAL consists of two components: a decoupled uncertainty-aware training objective that separately optimises the predictive mean and noise level to stabilise uncertainty estimation, and a confidence-aware acquisition function that dynamically weights epistemic uncertainty using predicted aleatoric uncertainty as a reliability signal. Experiments on particle-resolved numerical simulations and real atmospheric observations show that CAAL consistently outperforms standard AL baselines. The proposed framework provides a practical and general solution for the efficient expansion of high-cost atmospheric particle property databases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff08CAAL\uff09\uff0c\u7528\u4e8e\u5728\u5f02\u65b9\u5dee\u8bbe\u7f6e\u4e2d\u9ad8\u6548\u7a33\u5065\u5730\u9009\u62e9\u6837\u672c\uff0c\u4ee5\u4f30\u8ba1\u5927\u6c14\u9897\u7c92\u7269\u96be\u4ee5\u6d4b\u91cf\u7684\u6027\u8d28\u3002\u5b9e\u9a8c\u8868\u660eCAAL\u4f18\u4e8e\u6807\u51c6\u7684\u4e3b\u52a8\u5b66\u4e60\u57fa\u7ebf\uff0c\u5e76\u4e3a\u9ad8\u6210\u672c\u7684\u5927\u6c14\u9897\u7c92\u7269\u6027\u8d28\u6570\u636e\u5e93\u7684\u6709\u6548\u6269\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u91cf\u5316\u7a7a\u6c14\u6c61\u67d3\u5bf9\u5065\u5eb7\u548c\u6c14\u5019\u7684\u5f71\u54cd\u4f9d\u8d56\u4e8e\u5982\u6bd2\u6027\u3001\u5438\u6e7f\u6027\u7b49\u5173\u952e\u7684\u5927\u6c14\u9897\u7c92\u7269\u5c5e\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5c5e\u6027\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u89c2\u6d4b\u6280\u672f\u6216\u6602\u8d35\u7684\u7c92\u5b50\u89e3\u6790\u6570\u503c\u6a21\u62df\uff0c\u9650\u5236\u4e86\u6807\u8bb0\u6570\u636e\u7684\u53ef\u7528\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u4ece\u5e38\u89c4\u53ef\u83b7\u5f97\u7684\u89c2\u6d4b\u6570\u636e\u4e2d\u4f30\u8ba1\u8fd9\u4e9b\u96be\u4ee5\u6d4b\u91cf\u7684\u9897\u7c92\u7269\u5c5e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCAAL\u7684\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u89e3\u8026\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bad\u7ec3\u76ee\u6807\uff0c\u5b83\u5206\u522b\u4f18\u5316\u9884\u6d4b\u5747\u503c\u548c\u566a\u58f0\u6c34\u5e73\u4ee5\u7a33\u5b9a\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff1b\u4ee5\u53ca\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u611f\u77e5\u83b7\u53d6\u51fd\u6570\uff0c\u5b83\u4f7f\u7528\u9884\u6d4b\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u53ef\u9760\u6027\u4fe1\u53f7\u6765\u52a8\u6001\u52a0\u6743\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u7c92\u5b50\u89e3\u6790\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9645\u5927\u6c14\u89c2\u6d4b\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u6807\u51c6\u4e3b\u52a8\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0cCAAL\u80fd\u591f\u6301\u7eed\u8868\u73b0\u51fa\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684CAAL\u6846\u67b6\u4e0d\u4ec5\u4e3a\u5728\u5f02\u65b9\u5dee\u6761\u4ef6\u4e0b\u8fdb\u884c\u6709\u6548\u800c\u7a33\u5065\u7684\u6837\u672c\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u800c\u4e14\u4e5f\u7ed9\u9ad8\u6210\u672c\u7684\u5927\u6c14\u9897\u7c92\u7269\u5c5e\u6027\u6570\u636e\u5e93\u7684\u6269\u5145\u5e26\u6765\u4e86\u5b9e\u7528\u4e14\u5e7f\u6cdb\u7684\u89e3\u51b3\u65b9\u6cd5\u3002"}}
{"id": "2602.11861", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11861", "abs": "https://arxiv.org/abs/2602.11861", "authors": ["S\u00fcmeyye Meryem Ta\u015fy\u00fcrek", "Enis M\u00fccahid \u0130skender", "Hacer Yalim Keles"], "title": "A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production", "comment": "9 pages, 2 figures, 8 tables", "summary": "Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u7684\u6846\u67b6A$^{2}$V-SLP\uff0c\u7528\u4e8e\u5b66\u4e60\u624b\u8bed\u751f\u6210\u4e2d\u89e3\u7f20\u7684\u53d1\u97f3\u5668\u5b98\u7ea7\u522b\u7684\u6f5c\u5728\u5206\u5e03\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u975e\u81ea\u56de\u5f52Transformer\u9884\u6d4b\u6f5c\u5728\u5747\u503c\u548c\u5bf9\u6570\u65b9\u5dee\uff0c\u5e76\u7ed3\u5408\u4e86\u8bcd\u4e49\u6ce8\u610f\u673a\u5236\u6765\u52a0\u5f3a\u8bed\u8a00\u8f93\u5165\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b8c\u5168\u4e0d\u4f9d\u8d56\u8bcd\u4e49\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u65b0\u7684\u53cd\u5411\u7ffb\u8bd1\u6027\u80fd\u548c\u63d0\u9ad8\u4e86\u52a8\u4f5c\u7684\u771f\u5b9e\u611f\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u7684\u624b\u8bed\u751f\u6210\u6a21\u578b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u73b0\u6709\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u786e\u5b9a\u6027\u6f5c\u5728\u5d4c\u5165\u5bfc\u81f4\u7684\u95ee\u9898\uff08\u5982\u6f5c\u5728\u5d29\u6e83\uff09\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u53d8\u5206\u6846\u67b6\uff0c\u4ed6\u4eec\u5e0c\u671b\u80fd\u591f\u5728\u4fdd\u6301\u53d1\u97f3\u5668\u5b98\u7ea7\u522b\u8868\u793a\u7684\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u5bf9\u624b\u8bed\u5e8f\u5217\u751f\u6210\u7684\u8d28\u91cf\u53ca\u771f\u5b9e\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86A$^{2}$V-SLP\uff0c\u4e00\u4e2a\u8003\u8651\u5bf9\u9f50\u7684\u53d8\u5206\u6846\u67b6\uff0c\u5229\u7528VAE\u4ece\u771f\u5b9e\u7684\u624b\u8bed\u59ff\u52bf\u5e8f\u5217\u4e2d\u63d0\u53d6\u7279\u5b9a\u53d1\u97f3\u5668\u5b98\u7684\u5e73\u5747\u503c\u548c\u65b9\u5dee\u5411\u91cf\u4f5c\u4e3a\u5206\u5e03\u76d1\u7763\u4fe1\u606f\u8bad\u7ec3\u975e\u81ea\u56de\u5f52Transformer\u3002\u7ed9\u5b9a\u6587\u672c\u5d4c\u5165\u540e\uff0cTransformer\u53ef\u4ee5\u9884\u6d4b\u6f5c\u5728\u7684\u5747\u503c\u548c\u5bf9\u6570\u65b9\u5dee\uff1b\u7136\u540e\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u901a\u8fc7\u968f\u673a\u91c7\u6837\u7531VAE\u89e3\u7801\u5668\u91cd\u5efa\u6700\u7ec8\u7684\u624b\u8bed\u59ff\u52bf\u5e8f\u5217\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2a\u8bcd\u4e49\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u589e\u5f3a\u8bed\u8a00\u8f93\u5165\u4e0e\u5177\u4f53\u52a8\u4f5c\u95f4\u7684\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u786e\u5b9a\u6027\u7684\u6f5c\u5728\u56de\u5f52\u65b9\u6cd5\uff0c\u6240\u63d0\u51fa\u7684A$^{2}$V-SLP\u6846\u67b6\u4e0d\u4ec5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u53cd\u5411\u7ffb\u8bd1\u8868\u73b0\uff0c\u800c\u4e14\u5728\u5b8c\u5168\u4e0d\u9700\u8981\u4f9d\u8d56\u8bcd\u4e49\u7684\u60c5\u51b5\u4e0b\u4e5f\u63d0\u5347\u4e86\u8fd0\u52a8\u7684\u771f\u5b9e\u611f\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u5206\u5e03\u5f0f\u7684\u6f5c\u5728\u5efa\u6a21\u65b9\u5f0f\u800c\u975e\u786e\u5b9a\u6027\u7684\u5d4c\u5165\uff0cA$^{2}$V-SLP\u80fd\u591f\u66f4\u597d\u5730\u7ef4\u6301\u53d1\u97f3\u5668\u5b98\u7ea7\u522b\u7684\u8868\u793a\uff0c\u907f\u514d\u6f5c\u5728\u5d29\u6e83\u95ee\u9898\uff0c\u4ece\u800c\u5728\u624b\u8bed\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002"}}
{"id": "2602.11863", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11863", "abs": "https://arxiv.org/abs/2602.11863", "authors": ["Elif Akata", "Konstantinos Voudouris", "Vincent Fortuin", "Eric Schulz"], "title": "In-Context Function Learning in Large Language Models", "comment": null, "summary": "Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u89c6\u89d2\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u65f6\u4ece\u5c11\u91cf\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u73b0\u8c61\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u968f\u7740\u6f14\u793a\u6570\u91cf\u589e\u52a0\uff0cLLM\u7684\u5b66\u4e60\u66f2\u7ebf\u63a5\u8fd1GP\u4e0b\u754c\uff0c\u5e76\u4e14\u5176\u5f52\u7eb3\u504f\u597d\u66f4\u503e\u5411\u4e8e\u8f83\u4e0d\u5e73\u6ed1\u7684GP\u6838\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u53ef\u4ee5\u6709\u6548\u8c03\u6574\u8fd9\u4e9b\u5f52\u7eb3\u504f\u597d\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4ece\u66f4\u5e73\u6ed1\u7684GP\u6838\u91c7\u6837\u7684\u51fd\u6570\u5b66\u4e60\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u4ec5\u6709\u5c11\u6570\u793a\u4f8b\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u884c\u4e3a\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u8005\u6709\u591a\u76f8\u4f3c\u3002", "method": "\u6784\u5efa\u4e86\u63a7\u5236\u5b9e\u9a8c\uff0c\u5176\u4e2d\u6a21\u578b\u89c2\u5bdf\u5230\u4ece\u5df2\u77e5GP\u5148\u9a8c\u62bd\u53d6\u7684\u591a\u53d8\u91cf\u6807\u91cf\u503c\u51fd\u6570\u6837\u672c\u5e8f\u5217\u3002\u8bc4\u4f30\u4e86\u9884\u6d4b\u8bef\u5dee\u4e0e\u6f14\u793a\u6b21\u6570\u7684\u5173\u7cfb\uff0c\u5e76\u4e0e\u4e24\u4e2a\u57fa\u51c6\u8fdb\u884c\u4e86\u6bd4\u8f83\uff1a\u4e00\u4e2a\u57fa\u4e8e\u7ecf\u9a8c\u7684GP\u56de\u5f52\u5b66\u4e60\u8005\u4f5c\u4e3a\u53ef\u8fbe\u5230\u8bef\u5dee\u7684\u4e0b\u754c\uff1b\u53e6\u4e00\u4e2a\u662f1-\u6700\u8fd1\u90bb\u89c4\u5219\u7684\u9884\u671f\u8bef\u5dee\uff0c\u4f5c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u4e0a\u754c\u3002", "result": "\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u5b66\u4e60\u66f2\u7ebf\u53d7\u51fd\u6570\u751f\u6210\u6838\u7684\u5f71\u54cd\u5f88\u5927\uff0c\u5e76\u4e14\u968f\u7740\u6f14\u793a\u6570\u91cf\u7684\u589e\u52a0\u9010\u6e10\u903c\u8fd1GP\u4e0b\u754c\u3002LLM\u7684\u9884\u6d4b\u6700\u6709\u53ef\u80fd\u53d1\u751f\u5728\u4e0d\u592a\u5e73\u6ed1\u7684GP\u6838\u6761\u4ef6\u4e0b\u3002\u540c\u65f6\uff0c\u540e\u8bad\u7ec3\u65b9\u6cd5\u5982\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u80fd\u591f\u6709\u6548\u5730\u6539\u53d8\u8fd9\u4e9b\u5f52\u7eb3\u504f\u89c1\uff0c\u63d0\u9ad8\u5bf9\u4e8e\u4ece\u5e73\u6ed1\u6838GP\u91c7\u6837\u51fd\u6570\u7684\u5b66\u4e60\u6548\u7387\u3002", "conclusion": "\u6574\u4f53\u800c\u8a00\uff0c\u8be5\u6846\u67b6\u91cf\u5316\u4e86LLM\u50cfGP\u5b66\u4e60\u8005\u4e00\u6837\u884c\u4e8b\u7684\u7a0b\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5de5\u5177\u6765\u6307\u5bfc\u5176\u5f52\u7eb3\u504f\u89c1\u4ee5\u9002\u5e94\u8fde\u7eed\u51fd\u6570\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2602.11882", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.11882", "abs": "https://arxiv.org/abs/2602.11882", "authors": ["Suraj Ranganath", "Anish Patnaik", "Vaishak Menon"], "title": "Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning", "comment": "Workshop submission", "summary": "Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u4e25\u683c\u7684\u7cbe\u5ea6\u9884\u7b97\u4e0b\uff0c\u4f4e\u6bd4\u7279\u89c4\u5212\u884c\u4e3a\u4e3b\u8981\u53d7\u603b\u6bd4\u7279\u5bbd\u5ea6\u8fd8\u662f\u6a21\u5757\u95f4\u6bd4\u7279\u5206\u914d\u7684\u5f71\u54cd\u3002\u901a\u8fc7DINO-WM\u5728Wall\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\uff0c\u53d1\u73b08\u6bd4\u7279\u548c6\u6bd4\u7279\u8bbe\u7f6e\u63a5\u8fd1FP16\u8868\u73b0\uff0c3\u6bd4\u7279\u8bbe\u7f6e\u6027\u80fd\u5d29\u6e83\uff0c\u800c4\u6bd4\u7279\u8bbe\u7f6e\u5bf9\u5206\u914d\u654f\u611f\u3002\u8fd9\u4e9b\u7ed3\u679c\u652f\u6301\u4e86\u57fa\u4e8e\u6a21\u5757\u548c\u9884\u7b97\u7684\u91cf\u5316\u7b56\u7565\u4f5c\u4e3a\u9ad8\u6548\u7a7a\u95f4\u63a8\u7406\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u5728\u6709\u9650\u7cbe\u5ea6\u9884\u7b97\u4e0b\uff0c\u9ad8\u6548\u7684\u7a7a\u95ee\u63a8\u7406\u6a21\u578b\u5982\u4f55\u4fdd\u6301\u53ef\u9760\u6027\uff0c\u5e76\u786e\u5b9a\u4f4e\u6bd4\u7279\u89c4\u5212\u884c\u4e3a\u662f\u7531\u603b\u7684\u6bd4\u7279\u5bbd\u5ea6\u51b3\u5b9a\u8fd8\u662f\u7531\u6a21\u5757\u95f4\u7684\u6bd4\u7279\u5206\u914d\u51b3\u5b9a\u3002", "method": "\u4f7f\u7528DINO-WM\u6a21\u578b\u5728Wall\u89c4\u5212\u4efb\u52a1\u4e0a\u6267\u884c\u914d\u5bf9\u76ee\u6807\u6df7\u5408\u4f4d\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u53d8\u4f53\uff08\u7edf\u4e00\u3001\u6df7\u5408\u3001\u4e0d\u5bf9\u79f0\u53ca\u5c42\u95f4\uff09\u5728\u4e24\u4e2a\u89c4\u5212\u5668\u9884\u7b97\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u89c2\u5bdf\u5230\u4e00\u4e2a\u6301\u7eed\u7684\u4e09\u9636\u6bb5\u6a21\u5f0f\uff1a8\u6bd4\u7279\u548c6\u6bd4\u7279\u8bbe\u5b9a\u4e0eFP16\u975e\u5e38\u63a5\u8fd1\uff1b3\u6bd4\u7279\u8bbe\u5b9a\u5219\u5b8c\u5168\u5931\u6548\uff1b4\u6bd4\u7279\u8bbe\u5b9a\u5bf9\u4e8e\u5206\u914d\u5341\u5206\u654f\u611f\u3002\u7279\u522b\u662f\u5728\u8fc7\u6e21\u533a\u57df\uff0c\u4fdd\u6301\u7f16\u7801\u5668\u7cbe\u5ea6\u53ef\u4ee5\u63d0\u9ad8\u76f8\u5bf9\u4e8e\u5747\u5300\u91cf\u5316\u7684\u89c4\u5212\u6548\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u7279\u5b9a\u7684\u6bd4\u7279\u8bbe\u5b9a\u8303\u56f4\u5185\uff0c\u6bd4\u7279\u5206\u914d\u65b9\u5f0f\u5bf9\u89c4\u5212\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8fd9\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u57fa\u4e8e\u6a21\u5757\u548c\u9884\u7b97\u610f\u8bc6\u7684\u91cf\u5316\u7b56\u7565\u6765\u4fc3\u8fdb\u9ad8\u6548\u7684\u7a7a\u95f4\u63a8\u7406\u3002"}}
{"id": "2602.11893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11893", "abs": "https://arxiv.org/abs/2602.11893", "authors": ["Roberto Molinaro", "Niall Siegenheim", "Henry Martin", "Mark Frey", "Niels Poulsen", "Philipp Seitz", "Marvin Vincent Gabler"], "title": "Universal Diffusion-Based Probabilistic Downscaling", "comment": null, "summary": "We introduce a universal diffusion-based downscaling framework that lifts deterministic low-resolution weather forecasts into probabilistic high-resolution predictions without any model-specific fine-tuning. A single conditional diffusion model is trained on paired coarse-resolution inputs (~25 km resolution) and high-resolution regional reanalysis targets (~5 km resolution), and is applied in a fully zero-shot manner to deterministic forecasts from heterogeneous upstream weather models. Focusing on near-surface variables, we evaluate probabilistic forecasts against independent in situ station observations over lead times up to 90 h. Across a diverse set of AI-based and numerical weather prediction (NWP) systems, the ensemble mean of the downscaled forecasts consistently improves upon each model's own raw deterministic forecast, and substantially larger gains are observed in probabilistic skill as measured by CRPS. These results demonstrate that diffusion-based downscaling provides a scalable, model-agnostic probabilistic interface for enhancing spatial resolution and uncertainty representation in operational weather forecasting pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u901a\u7528\u964d\u5c3a\u5ea6\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u786e\u5b9a\u6027\u7684\u4f4e\u5206\u8fa8\u7387\u5929\u6c14\u9884\u62a5\u63d0\u5347\u4e3a\u6982\u7387\u6027\u7684\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u8be5\u65b9\u6cd5\u5bf9\u63d0\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u6709\u663e\u8457\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5929\u6c14\u9884\u62a5\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u8868\u793a\uff0c\u540c\u65f6\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u4e0a\u6e38\u5929\u6c14\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u9700\u8981\u4efb\u4f55\u6a21\u578b\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u4ece\u4f4e\u5206\u8fa8\u7387\u5230\u9ad8\u5206\u8fa8\u7387\u5929\u6c14\u9884\u6d4b\u8f6c\u6362\u7684\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u914d\u5bf9\u7c97\u5206\u8fa8\u7387\u8f93\u5165\uff08\u7ea625\u516c\u91cc\u5206\u8fa8\u7387\uff09\u4e0e\u9ad8\u5206\u8fa8\u7387\u533a\u57df\u518d\u5206\u6790\u76ee\u6807\uff08\u7ea65\u516c\u91cc\u5206\u8fa8\u7387\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4ee5\u5b8c\u5168\u96f6\u6837\u672c\u65b9\u5f0f\u5e94\u7528\u4e8e\u6765\u81ea\u5f02\u6784\u4e0a\u6e38\u5929\u6c14\u6a21\u578b\u7684\u786e\u5b9a\u6027\u9884\u62a5\u4e0a\u3002", "result": "\u5728\u4e0d\u540c\u7684\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u548c\u6570\u503c\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u4e2d\uff0c\u964d\u5c3a\u5ea6\u540e\u7684\u9884\u62a5\u96c6\u5408\u5e73\u5747\u503c\u59cb\u7ec8\u4f18\u4e8e\u6bcf\u4e2a\u6a21\u578b\u81ea\u8eab\u7684\u539f\u59cb\u786e\u5b9a\u6027\u9884\u62a5\uff0c\u5728\u6982\u7387\u6280\u80fd\u65b9\u9762\u4e5f\u6709\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728CRPS\u6d4b\u91cf\u4e0b\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u7684\u964d\u5c3a\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6a21\u578b\u65e0\u5173\u7684\u6982\u7387\u63a5\u53e3\uff0c\u80fd\u591f\u589e\u5f3a\u64cd\u4f5c\u6027\u5929\u6c14\u9884\u62a5\u6d41\u7a0b\u4e2d\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u3002"}}
{"id": "2602.11902", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11902", "abs": "https://arxiv.org/abs/2602.11902", "authors": ["Suqin Yuan", "Xingrui Yu", "Jiyang Zheng", "Lei Feng", "Dadong Wang", "Ivor Tsang", "Tongliang Liu"], "title": "Mitigating Mismatch within Reference-based Preference Optimization", "comment": "Accepted by ICLR 2026", "summary": "Direct Preference Optimization (DPO) has become the de facto standard for offline preference alignment of large language models, but its reliance on a reference policy introduces a critical tension. DPO weighs each update relative to a reference, which stabilizes the training by regularizing the updates within a trusted region. This reliance becomes problematic for pessimistic pairs, where the reference model prefers the rejected response. For these pairs, DPO prematurely attenuates the gradient as soon as the policy margin ($\u0394_\u03b8$) merely beats the reference margin ($\u0394_{\\mathrm{ref}}$) even if the policy is still wrong ($\u0394_\u03b8<0$). We name this failure premature satisfaction, which is a concrete form of the training-inference mismatch. Reference-free objectives remove this mismatch by optimizing the absolute margin, but at the cost of discarding the stabilizing signal of the reference. We mitigate this tension with Hybrid-DPO (HyPO), a drop-in modification to DPO that applies reference conditionally: HyPO behaves exactly like DPO when the reference is optimistic or neutral, and it treats the reference as neutral when it is pessimistic by replacing $\u0394_\u03b8-\u0394_{\\mathrm{ref}}$ with $\u0394_\u03b8-\\max\\{0,\u0394_{\\mathrm{ref}}\\}$. This one-line change strictly strengthens per-example learning signals on pessimistic pairs while preserving DPO's objective form and computational cost. By conditionally debiasing the pessimistic reference signal, HyPO mitigates premature satisfaction; empirically, across preference alignment, HyPO improves inference-aligned metrics and achieves higher pairwise win rates. Our results provide evidence that direct preference alignment could be enhanced by conditionally debiasing the reference signal, rather than discarding it.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHybrid-DPO (HyPO)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u6027\u5730\u5e94\u7528\u53c2\u8003\u7b56\u7565\u6765\u89e3\u51b3\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u4e2d\u5b58\u5728\u7684\u8fc7\u65e9\u6ee1\u8db3\u95ee\u9898\u3002HyPO\u5728\u4fdd\u6301DPO\u76ee\u6807\u5f62\u5f0f\u548c\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u52a0\u5f3a\u4e86\u5bf9\u60b2\u89c2\u6837\u672c\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u914d\u5bf9\u80dc\u7387\u7b49\u6307\u6807\u3002", "motivation": "\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u867d\u7136\u5df2\u6210\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u79bb\u7ebf\u504f\u597d\u6821\u51c6\u7684\u4e8b\u5b9e\u6807\u51c6\uff0c\u4f46\u5176\u4f9d\u8d56\u4e8e\u53c2\u8003\u7b56\u7565\u7684\u505a\u6cd5\u5b58\u5728\u4e00\u4e2a\u95ee\u9898\uff1a\u5bf9\u4e8e\u60b2\u89c2\u6837\u672c\u5bf9\uff0c\u5373\u4f7f\u7b56\u7565\u4ecd\u6709\u8bef\uff0c\u53ea\u8981\u653f\u7b56\u8fb9\u9645\u8d85\u8fc7\u53c2\u8003\u8fb9\u9645\uff0cDPO\u5c31\u4f1a\u63d0\u524d\u51cf\u5c11\u68af\u5ea6\u66f4\u65b0\u3002\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u201c\u8fc7\u65e9\u6ee1\u8db3\u201d\uff0c\u5bfc\u81f4\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86Hybrid-DPO (HyPO)\uff0c\u4e00\u79cd\u9488\u5bf9DPO\u7684\u7b80\u5355\u4fee\u6539\u65b9\u6848\u3002HyPO\u6839\u636e\u53c2\u8003\u7b56\u7565\u7684\u6001\u5ea6\u6709\u6761\u4ef6\u5730\u5de5\u4f5c\uff1a\u5f53\u53c2\u8003\u7b56\u7565\u662f\u4e50\u89c2\u6216\u4e2d\u7acb\u65f6\uff0c\u5b83\u8868\u73b0\u5f97\u50cfDPO\uff1b\u800c\u5f53\u53c2\u8003\u7b56\u7565\u60b2\u89c2\u65f6\uff0c\u5219\u5c06\\(\u0394_\u03b8-\u0394_{\\mathrm{ref}}\\)\u66ff\u6362\u4e3a\\(\u0394_\u03b8-\\max\\{0,\u0394_{\\mathrm{ref}}\\}\\)\uff0c\u4ece\u800c\u89c6\u4e3a\u53c2\u8003\u7b56\u7565\u4e3a\u4e2d\u7acb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u504f\u597d\u6821\u51c6\u4efb\u52a1\u4e0a\uff0cHyPO\u80fd\u591f\u63d0\u9ad8\u4e0e\u63a8\u7406\u4e00\u81f4\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u5e76\u4e14\u8fbe\u5230\u4e86\u66f4\u9ad8\u7684\u4e24\u4e24\u6bd4\u8f83\u83b7\u80dc\u7387\u3002\u8fd9\u610f\u5473\u7740\u901a\u8fc7\u6761\u4ef6\u6027\u5730\u53bb\u504f\u53c2\u8003\u4fe1\u53f7\u800c\u975e\u5b8c\u5168\u820d\u5f03\u4e4b\uff0c\u53ef\u4ee5\u76f4\u63a5\u589e\u5f3a\u504f\u597d\u6821\u51c6\u7684\u6548\u679c\u3002", "conclusion": "HyPO\u901a\u8fc7\u6539\u8fdbDPO\u4e2d\u7684\u53c2\u8003\u7b56\u7565\u4f7f\u7528\u65b9\u5f0f\uff0c\u5728\u4fdd\u6301\u539f\u65b9\u6cd5\u4f18\u70b9\u7684\u540c\u65f6\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u65e9\u6ee1\u8db3\u95ee\u9898\uff0c\u8fdb\u800c\u63d0\u5347\u4e86\u504f\u597d\u6821\u51c6\u4efb\u52a1\u7684\u8868\u73b0\u3002"}}
{"id": "2602.11920", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.11920", "abs": "https://arxiv.org/abs/2602.11920", "authors": ["Marco Bressan", "Nataly Brukhim", "Nicolo Cesa-Bianchi", "Emmanuel Esposito", "Yishay Mansour", "Shay Moran", "Maximilian Thiessen"], "title": "Learning Conditional Averages", "comment": null, "summary": "We introduce the problem of learning conditional averages in the PAC framework. The learner receives a sample labeled by an unknown target concept from a known concept class, as in standard PAC learning. However, instead of learning the target concept itself, the goal is to predict, for each instance, the average label over its neighborhood -- an arbitrary subset of points that contains the instance. In the degenerate case where all neighborhoods are singletons, the problem reduces exactly to classic PAC learning. More generally, it extends PAC learning to a setting that captures learning tasks arising in several domains, including explainability, fairness, and recommendation systems. Our main contribution is a complete characterization of when conditional averages are learnable, together with sample complexity bounds that are tight up to logarithmic factors. The characterization hinges on the joint finiteness of two novel combinatorial parameters, which depend on both the concept class and the neighborhood system, and are closely related to the independence number of the associated neighborhood graph.", "AI": {"tldr": "\u672c\u6587\u5728PAC\u6846\u67b6\u4e0b\u5f15\u5165\u4e86\u5b66\u4e60\u6761\u4ef6\u5e73\u5747\u503c\u7684\u95ee\u9898\uff0c\u76ee\u6807\u662f\u9884\u6d4b\u6bcf\u4e2a\u5b9e\u4f8b\u5728\u5176\u90bb\u57df\u5185\u7684\u5e73\u5747\u6807\u7b7e\u3002\u6587\u7ae0\u7ed9\u51fa\u4e86\u5f53\u6761\u4ef6\u5e73\u5747\u503c\u53ef\u5b66\u4e60\u65f6\u7684\u5b8c\u6574\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u4e86\u6837\u672c\u590d\u6742\u5ea6\u8fb9\u754c\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728PAC\u6846\u67b6\u4e0b\u7684\u6761\u4ef6\u5e73\u5747\u503c\u5b66\u4e60\u95ee\u9898\uff0c\u8fd9\u4e00\u95ee\u9898\u6bd4\u4f20\u7edf\u7684PAC\u5b66\u4e60\u66f4\u5177\u4e00\u822c\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e94\u7528\u4e8e\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u4e2d\u51fa\u73b0\u7684\u5b66\u4e60\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5f15\u5165\u4e24\u4e2a\u65b0\u7684\u7ec4\u5408\u53c2\u6570\uff0c\u5bf9\u6982\u5ff5\u7c7b\u548c\u90bb\u57df\u7cfb\u7edf\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u8fdb\u800c\u4e3a\u6761\u4ef6\u5e73\u5747\u503c\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u53ef\u5b66\u4e60\u6027\u8868\u5f81\u3002\u8fd9\u4e24\u4e2a\u53c2\u6570\u4e0e\u76f8\u5173\u90bb\u57df\u56fe\u7684\u72ec\u7acb\u6570\u5bc6\u5207\u76f8\u5173\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u8fd9\u4e24\u4e2a\u65b0\u63d0\u51fa\u7684\u7ec4\u5408\u53c2\u6570\u540c\u65f6\u6709\u9650\u65f6\uff0c\u6761\u4ef6\u5e73\u5747\u503c\u662f\u53ef\u4ee5\u88ab\u5b66\u4e60\u7684\uff0c\u5e76\u4e14\u7ed9\u51fa\u4e86\u7d27\u81f4\u5230\u5bf9\u6570\u56e0\u5b50\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "conclusion": "\u672c\u6587\u6269\u5c55\u4e86PAC\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u4ee5\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u4ee5\u53ca\u63a8\u8350\u7cfb\u7edf\uff0c\u5e76\u4e14\u5bf9\u6761\u4ef6\u5e73\u5747\u503c\u5b66\u4e60\u7684\u53ef\u80fd\u6027\u7ed9\u51fa\u4e86\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.11937", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11937", "abs": "https://arxiv.org/abs/2602.11937", "authors": ["Akhiad Bercovich", "Nir Ailon", "Vladimir Anisimov", "Tomer Asida", "Nave Assaf", "Mohammad Dabbah", "Ido Galil", "Amnon Geifman", "Yonatan Geifman", "Izhak Golan", "Roi Koren", "Itay Levy", "Zach Moshe", "Pavlo Molchanov", "Najeeb Nabwani", "Mostofa Patwari", "Omri Puny", "Tomer Ronen", "Itamar Schen", "Elad Segal", "Ido Shahaf", "Oren Tropp", "Ran Zilberstein", "Ran El-Yaniv"], "title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration", "comment": null, "summary": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528Puzzle\u6846\u67b6\u5bf9gpt-oss-120B\u8fdb\u884c\u540e\u8bad\u7ec3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86gpt-oss-puzzle-88B\u6a21\u578b\u3002\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u591a\u79cd\u6280\u672f\u6765\u4f18\u5316\u63a8\u7406\u6548\u7387\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5305\u62ec\u5f02\u6784MoE\u4e13\u5bb6\u526a\u679d\u3001\u7a97\u53e3\u6ce8\u610f\u529b\u66ff\u6362\u5168\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u3001FP8 KV\u7f13\u5b58\u91cf\u5316\u7b49\u3002\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u914d\u7f6e\u4e0b\uff0cgpt-oss-puzzle-88B\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u6216\u7a0d\u5fae\u8d85\u8fc7\u4e86\u539f\u59cb\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u57fa\u4e8e\u957f\u63a8\u7406\u8def\u5f84\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u670d\u52a1\u6548\u7387\u800c\u4e0d\u727a\u7272\u7b54\u6848\u8d28\u91cf\uff0c\u7814\u7a76\u8005\u4eec\u5bfb\u6c42\u901a\u8fc7\u67b6\u6784\u4f18\u5316\u51cf\u5c11\u63a8\u7406\u6210\u672c\u3002", "method": "\u91c7\u7528\u540d\u4e3aPuzzle\u7684\u540e\u8bad\u7ec3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u6846\u67b6\u5bf9gpt-oss-120B\u8fdb\u884c\u4f18\u5316\uff0c\u751f\u6210\u4e86gpt-oss-puzzle-88B\u3002\u6b64\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u4e86\u5f02\u6784\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u526a\u679d\u3001\u9009\u62e9\u6027\u5730\u7528\u7a97\u53e3\u6ce8\u610f\u529b\u4ee3\u66ff\u5168\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u3001\u4ee5\u53ca\u4f7f\u7528\u6821\u51c6\u6bd4\u4f8b\u7684FP8 KV\u7f13\u5b58\u91cf\u5316\u7b49\u6280\u672f\u3002\u6b64\u5916\uff0c\u8fd8\u8fd0\u7528\u4e86\u540e\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u6765\u6062\u590d\u7cbe\u5ea6\u3002", "result": "\u65b0\u6a21\u578bgpt-oss-puzzle-88B\u76f8\u6bd4\u539f\u7248\uff0c\u5728\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4f8b\u5982\u5728\u5355\u4e2aNVIDIA H100 GPU\u4e0a\u541e\u5410\u91cf\u63d0\u5347\u4e862.82\u500d\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u80fd\u591f\u5728\u4fdd\u6301\u751a\u81f3\u7565\u5fae\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u5ea6\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5e94\u7528Puzzle\u6846\u67b6\u53ca\u4e00\u7cfb\u5217\u4f18\u5316\u6280\u672f\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u800c\u4e0d\u635f\u5bb3\u5176\u6027\u80fd\uff0c\u8fd9\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u9009\u62e9\u3002"}}
{"id": "2602.11940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11940", "abs": "https://arxiv.org/abs/2602.11940", "authors": ["Ruixian Su", "Yukun Bao", "Xinze Zhang"], "title": "Temporally Unified Adversarial Perturbations for Time Series Forecasting", "comment": null, "summary": "While deep learning models have achieved remarkable success in time series forecasting, their vulnerability to adversarial examples remains a critical security concern. However, existing attack methods in the forecasting field typically ignore the temporal consistency inherent in time series data, leading to divergent and contradictory perturbation values for the same timestamp across overlapping samples. This temporally inconsistent perturbations problem renders adversarial attacks impractical for real-world data manipulation. To address this, we introduce Temporally Unified Adversarial Perturbations (TUAPs), which enforce a temporal unification constraint to ensure identical perturbations for each timestamp across all overlapping samples. Moreover, we propose a novel Timestamp-wise Gradient Accumulation Method (TGAM) that provides a modular and efficient approach to effectively generate TUAPs by aggregating local gradient information from overlapping samples. By integrating TGAM with momentum-based attack algorithms, we ensure strict temporal consistency while fully utilizing series-level gradient information to explore the adversarial perturbation space. Comprehensive experiments on three benchmark datasets and four representative state-of-the-art models demonstrate that our proposed method significantly outperforms baselines in both white-box and black-box transfer attack scenarios under TUAP constraints. Moreover, our method also exhibits superior transfer attack performance even without TUAP constraints, demonstrating its effectiveness and superiority in generating adversarial perturbations for time series forecasting models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6270\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u5373\u65f6\u95f4\u7edf\u4e00\u7684\u5bf9\u6297\u6270\u52a8\uff08TUAPs\uff09\u548c\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u6233\u68af\u5ea6\u7d2f\u79ef\u7684\u65b9\u6cd5\uff08TGAM\uff09\uff0c\u4ee5\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u4e2d\u5bf9\u6297\u653b\u51fb\u5ffd\u89c6\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u767d\u76d2\u548c\u9ed1\u76d2\u8fc1\u79fb\u653b\u51fb\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5373\u4f7f\u6ca1\u6709TUAP\u9650\u5236\u4e5f\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u76f8\u540c\u65f6\u95f4\u6233\u4e0a\u7684\u6270\u52a8\u503c\u5728\u91cd\u53e0\u6837\u672c\u95f4\u51fa\u73b0\u5206\u6b67\u548c\u77db\u76fe\uff0c\u4f7f\u5f97\u5bf9\u6297\u653b\u51fb\u5728\u5b9e\u9645\u6570\u636e\u64cd\u4f5c\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u4e86Temporally Unified Adversarial Perturbations (TUAPs) \u548c Timestamp-wise Gradient Accumulation Method (TGAM)\u3002\u5176\u4e2d\uff0cTUAPs\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u65f6\u95f4\u7edf\u4e00\u7ea6\u675f\u6765\u786e\u4fdd\u6240\u6709\u91cd\u53e0\u6837\u672c\u4e2d\u6bcf\u4e2a\u65f6\u95f4\u6233\u7684\u6270\u52a8\u4e00\u81f4\uff1b\u800cTGAM\u5219\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u6765\u81ea\u91cd\u53e0\u6837\u672c\u7684\u5c40\u90e8\u68af\u5ea6\u4fe1\u606f\u6709\u6548\u5730\u751f\u6210TUAPs\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u56db\u79cd\u4ee3\u8868\u6027\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u65e0\u8bba\u662f\u5728\u767d\u76d2\u8fd8\u662f\u9ed1\u76d2\u8fc1\u79fb\u653b\u51fb\u573a\u666f\u4e0b\u90fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u4e0d\u53d7TUAP\u7ea6\u675f\uff0c\u8be5\u65b9\u6cd5\u4e5f\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u8fc1\u79fb\u653b\u51fb\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684TUAPs\u4e0eTGAM\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u5185\u5bf9\u6297\u653b\u51fb\u4e2d\u5b58\u5728\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u6548\u679c\uff0c\u8fd8\u589e\u5f3a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.11944", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.11944", "abs": "https://arxiv.org/abs/2602.11944", "authors": ["Karolin Frohnapfel", "Mara Seyfert", "Sebastian Bordt", "Ulrike von Luxburg", "Kristof Meding"], "title": "Using predictive multiplicity to measure individual performance within the AI Act", "comment": null, "summary": "When building AI systems for decision support, one often encounters the phenomenon of predictive multiplicity: a single best model does not exist; instead, one can construct many models with similar overall accuracy that differ in their predictions for individual cases. Especially when decisions have a direct impact on humans, this can be highly unsatisfactory. For a person subject to high disagreement between models, one could as well have chosen a different model of similar overall accuracy that would have decided the person's case differently. We argue that this arbitrariness conflicts with the EU AI Act, which requires providers of high-risk AI systems to report performance not only at the dataset level but also for specific persons. The goal of this paper is to put predictive multiplicity in context with the EU AI Act's provisions on accuracy and to subsequently derive concrete suggestions on how to evaluate and report predictive multiplicity in practice. Specifically: (1) We argue that incorporating information about predictive multiplicity can serve compliance with the EU AI Act's accuracy provisions for providers. (2) Based on this legal analysis, we suggest individual conflict ratios and $\u03b4$-ambiguity as tools to quantify the disagreement between models on individual cases and to help detect individuals subject to conflicting predictions. (3) Based on computational insights, we derive easy-to-implement rules on how model providers could evaluate predictive multiplicity in practice. (4) Ultimately, we suggest that information about predictive multiplicity should be made available to deployers under the AI Act, enabling them to judge whether system outputs for specific individuals are reliable enough for their use case.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u6784\u5efa\u51b3\u7b56\u652f\u6301AI\u7cfb\u7edf\u65f6\u9047\u5230\u7684\u9884\u6d4b\u591a\u91cd\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u5e94\u5982\u4f55\u6839\u636e\u6b27\u76dfAI\u6cd5\u6848\u7684\u89c4\u5b9a\u6765\u8bc4\u4f30\u548c\u62a5\u544a\u8fd9\u4e00\u73b0\u8c61\u3002\u63d0\u51fa\u4e86\u4f7f\u7528\u4e2a\u4f53\u51b2\u7a81\u6bd4\u7387\u548c\u03b4-\u6a21\u7cca\u5ea6\u4f5c\u4e3a\u5de5\u5177\u91cf\u5316\u6a21\u578b\u95f4\u5bf9\u4e8e\u4e2a\u4f53\u6848\u4f8b\u7684\u5206\u6b67\uff0c\u4ee5\u53ca\u5efa\u8bae\u8ba9\u90e8\u7f72\u8005\u80fd\u591f\u83b7\u5f97\u5173\u4e8e\u9884\u6d4b\u591a\u91cd\u6027\u7684\u4fe1\u606f\u4ee5\u5224\u65ad\u7cfb\u7edf\u8f93\u51fa\u5bf9\u7279\u5b9a\u4e2a\u4f53\u662f\u5426\u8db3\u591f\u53ef\u9760\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u89e3\u51b3AI\u7cfb\u7edf\u4e2d\u9884\u6d4b\u591a\u91cd\u6027\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u76f4\u63a5\u5f71\u54cd\u4eba\u7c7b\u51b3\u7b56\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u4e0e\u6b27\u76dfAI\u6cd5\u6848\u4e2d\u8981\u6c42\u63d0\u4f9b\u9ad8\u98ce\u9669AI\u7cfb\u7edf\u6027\u80fd\u62a5\u544a\u7684\u89c4\u5b9a\u76f8\u51b2\u7a81\u3002", "method": "\u901a\u8fc7\u6cd5\u5f8b\u5206\u6790\u8bba\u8bc1\u5c06\u9884\u6d4b\u591a\u91cd\u6027\u4fe1\u606f\u7eb3\u5165\u6709\u52a9\u4e8e\u7b26\u5408\u6b27\u76dfAI\u6cd5\u6848\u51c6\u786e\u6027\u89c4\u5b9a\uff1b\u63d0\u51fa\u7528\u4e2a\u4f53\u51b2\u7a81\u6bd4\u7387\u548c\u03b4-\u6a21\u7cca\u5ea6\u7b49\u5de5\u5177\u91cf\u5316\u6a21\u578b\u95f4\u5bf9\u4e2a\u522b\u6848\u4f8b\u7684\u5206\u6b67\uff1b\u57fa\u4e8e\u8ba1\u7b97\u6d1e\u5bdf\u63d0\u4f9b\u4e86\u6613\u4e8e\u5b9e\u65bd\u7684\u89c4\u5219\u6765\u8bc4\u4f30\u5b9e\u8df5\u4e2d\u9884\u6d4b\u591a\u91cd\u6027\u3002", "result": "\u53d1\u73b0\u8003\u8651\u9884\u6d4b\u591a\u91cd\u6027\u53ef\u4ee5\u5e2e\u52a9AI\u7cfb\u7edf\u7684\u63d0\u4f9b\u8005\u66f4\u597d\u5730\u9075\u5b88\u6b27\u76dfAI\u6cd5\u6848\u4e2d\u7684\u51c6\u786e\u6027\u6761\u6b3e\uff1b\u63d0\u51fa\u7684\u5177\u4f53\u65b9\u6cd5\u5982\u4e2a\u4f53\u51b2\u7a81\u6bd4\u7387\u548c\u03b4-\u6a21\u7cca\u5ea6\u53ef\u6709\u6548\u8bc6\u522b\u53d7\u5230\u4e0d\u540c\u6a21\u578b\u9884\u6d4b\u5f71\u54cd\u8f83\u5927\u7684\u4e2a\u4f53\uff1b\u4e3a\u6a21\u578b\u63d0\u4f9b\u8005\u63d0\u4f9b\u4e86\u4e00\u5957\u7b80\u5355\u5b9e\u7528\u7684\u8bc4\u4f30\u6307\u5357\u3002", "conclusion": "\u8ba4\u4e3a\u5411AI\u7cfb\u7edf\u7684\u90e8\u7f72\u8005\u63d0\u4f9b\u6709\u5173\u9884\u6d4b\u591a\u91cd\u6027\u7684\u4fe1\u606f\u662f\u5fc5\u8981\u7684\uff0c\u8fd9\u6709\u52a9\u4e8e\u4ed6\u4eec\u8bc4\u4f30\u9488\u5bf9\u7279\u5b9a\u4e2a\u4eba\u7684\u7cfb\u7edf\u8f93\u51fa\u662f\u5426\u8db3\u591f\u53ef\u9760\uff0c\u4ece\u800c\u786e\u4fdd\u9075\u5faa\u6b27\u76dfAI\u6cd5\u6848\u7684\u8981\u6c42\u3002"}}
{"id": "2602.11945", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11945", "abs": "https://arxiv.org/abs/2602.11945", "authors": ["Hongliang Zhang", "Jiguo Yu", "Guijuan Wang", "Wenshuo Ma", "Tianqing He", "Baobao Chai", "Chunqiang Hu"], "title": "Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios", "comment": null, "summary": "Federated Learning (FL) enables multiple nodes to collaboratively train a model without sharing raw data. However, FL systems are usually deployed in heterogeneous scenarios, where nodes differ in both data distributions and participation frequencies, which undermines the FL performance. To tackle the above issue, this paper proposes PMFL, a performance-enhanced model-contrastive federated learning framework using historical training information. Specifically, on the node side, we design a novel model-contrastive term into the node optimization objective by incorporating historical local models to capture stable contrastive points, thereby improving the consistency of model updates in heterogeneous data distributions.\n  On the server side, we utilize the cumulative participation count of each node to adaptively adjust its aggregation weight, thereby correcting the bias in the global objective caused by different node participation frequencies. Furthermore, the updated global model incorporates historical global models to reduce its fluctuations in performance between adjacent rounds. Extensive experiments demonstrate that PMFL achieves superior performance compared with existing FL methods in heterogeneous scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5386\u53f2\u8bad\u7ec3\u4fe1\u606f\u7684\u6027\u80fd\u589e\u5f3a\u6a21\u578b\u5bf9\u6bd4\u8054\u90a6\u5b66\u4e60\u6846\u67b6PMFL\uff0c\u901a\u8fc7\u5728\u8282\u70b9\u4fa7\u5f15\u5165\u65b0\u9896\u7684\u6a21\u578b\u5bf9\u6bd4\u9879\u5e76\u5728\u670d\u52a1\u5668\u4fa7\u6839\u636e\u6bcf\u4e2a\u8282\u70b9\u7684\u7d2f\u79ef\u53c2\u4e0e\u6b21\u6570\u81ea\u9002\u5e94\u8c03\u6574\u5176\u805a\u5408\u6743\u91cd\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u901a\u5e38\u90e8\u7f72\u5728\u5f02\u6784\u573a\u666f\u4e2d\uff0c\u8282\u70b9\u95f4\u7684\u6570\u636e\u5206\u5e03\u548c\u53c2\u4e0e\u9891\u7387\u4e0d\u540c\uff0c\u8fd9\u4f1a\u524a\u5f31\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86PMFL\u6846\u67b6\u6765\u63d0\u9ad8\u6a21\u578b\u66f4\u65b0\u7684\u4e00\u81f4\u6027\u548c\u5168\u5c40\u76ee\u6807\u7684\u51c6\u786e\u6027\u3002", "method": "1. \u5728\u8282\u70b9\u7aef\uff0c\u901a\u8fc7\u6574\u5408\u5386\u53f2\u672c\u5730\u6a21\u578b\u5230\u8282\u70b9\u4f18\u5316\u76ee\u6807\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\u5bf9\u6bd4\u9879\uff0c\u4ee5\u6355\u6349\u7a33\u5b9a\u7684\u5bf9\u6bd4\u70b9\uff0c\u4ece\u800c\u6539\u5584\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u6a21\u578b\u66f4\u65b0\u7684\u4e00\u81f4\u6027\u3002\n2. \u5728\u670d\u52a1\u7aef\uff0c\u5229\u7528\u6bcf\u4e2a\u8282\u70b9\u7684\u7d2f\u79ef\u53c2\u4e0e\u8ba1\u6570\u6765\u81ea\u9002\u5e94\u5730\u8c03\u6574\u5176\u805a\u5408\u6743\u91cd\uff0c\u4ee5\u6b64\u4fee\u6b63\u7531\u4e8e\u8282\u70b9\u53c2\u4e0e\u9891\u7387\u4e0d\u540c\u6240\u5f15\u8d77\u7684\u5168\u5c40\u76ee\u6807\u504f\u5dee\u3002\n3. \u66f4\u65b0\u540e\u7684\u5168\u5c40\u6a21\u578b\u878d\u5408\u4e86\u5386\u53f2\u5168\u5c40\u6a21\u578b\uff0c\u51cf\u5c11\u4e86\u76f8\u90bb\u8f6e\u6b21\u4e4b\u95f4\u6027\u80fd\u6ce2\u52a8\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5f02\u6784\u573a\u666f\u4e0b\uff0c\u4e0e\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0cPMFL\u80fd\u591f\u5b9e\u73b0\u66f4\u4f18\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5386\u53f2\u8bad\u7ec3\u4fe1\u606f\u7684\u6a21\u578b\u5bf9\u6bd4\u7b56\u7565\u4ee5\u53ca\u52a8\u6001\u8c03\u6574\u8282\u70b9\u805a\u5408\u6743\u91cd\u7684\u65b9\u6cd5\uff0cPMFL\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u6570\u636e\u5206\u5e03\u4e0d\u5747\u53ca\u53c2\u4e0e\u9891\u7387\u5dee\u5f02\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2602.11958", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11958", "abs": "https://arxiv.org/abs/2602.11958", "authors": ["Kaicheng Xiao", "Haotian Li", "Liran Dong", "Guoliang Xing"], "title": "RAM-Net: Expressive Linear Attention with Selectively Addressable Memory", "comment": null, "summary": "While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAM-Net\u7684\u65b0\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u5c06\u8f93\u5165\u6620\u5c04\u5230\u9ad8\u7ef4\u7a00\u758f\u5411\u91cf\u4f5c\u4e3a\u663e\u5f0f\u5730\u5740\u6765\u9009\u62e9\u6027\u5730\u8bbf\u95ee\u5927\u5bb9\u91cf\u5185\u5b58\u72b6\u6001\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u7684\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5168\u6ce8\u610f\u529b\u673a\u5236\u7684\u8868\u73b0\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cRAM-Net\u5728\u7ec6\u7c92\u5ea6\u957f\u8ddd\u79bb\u68c0\u7d22\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u548c\u96f6\u6837\u672c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\u867d\u7136\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5c06\u65e0\u754c\u7684\u5386\u53f2\u538b\u7f29\u5230\u56fa\u5b9a\u5927\u5c0f\u7684\u8bb0\u5fc6\u4e2d\u4f1a\u9650\u5236\u8868\u8fbe\u80fd\u529b\u548c\u9020\u6210\u4fe1\u606f\u635f\u5931\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u4eec\u5f00\u53d1\u4e86RAM-Net\uff0c\u65e8\u5728\u7ed3\u5408\u5168\u6ce8\u610f\u529b\u673a\u5236\u7684\u8868\u73b0\u529b\u4e0e\u7ebf\u6027\u6a21\u578b\u7684\u5185\u5b58\u6548\u7387\u4f18\u52bf\u3002", "method": "RAM-Net\u7684\u6838\u5fc3\u662f\u5c06\u8f93\u5165\u8f6c\u6362\u6210\u9ad8\u7ef4\u7a00\u758f\u5411\u91cf\uff0c\u8fd9\u4e9b\u5411\u91cf\u5145\u5f53\u8bbf\u95ee\u5de8\u5927\u8bb0\u5fc6\u72b6\u6001\u7684\u660e\u786e\u5730\u5740\u3002\u8fd9\u6837\u7684\u8bbe\u8ba1\u5141\u8bb8\u72b6\u6001\u5927\u5c0f\u4ee5\u6307\u6570\u7ea7\u589e\u957f\u800c\u65e0\u9700\u989d\u5916\u53c2\u6570\uff0c\u51cf\u5c11\u4e86\u4fe1\u53f7\u5e72\u6270\u5e76\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u56fa\u6709\u7684\u7a00\u758f\u6027\u4fdd\u8bc1\u4e86\u6781\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u56e0\u4e3a\u72b6\u6001\u66f4\u65b0\u4ec5\u9650\u4e8e\u5c11\u6570\u51e0\u4e2a\u6761\u76ee\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cRAM-Net\u4e0d\u4ec5\u5728\u7ec6\u81f4\u7684\u957f\u7a0b\u68c0\u7d22\u4efb\u52a1\u4e0a\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u57fa\u51c6\uff0c\u5728\u6807\u51c6\u7684\u8bed\u8a00\u5efa\u6a21\u4ee5\u53ca\u96f6\u6b21\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u5f97\u5f88\u6709\u7ade\u4e89\u529b\u3002\u8fd9\u8bc1\u5b9e\u4e86RAM-Net\u80fd\u591f\u4ee5\u663e\u8457\u51cf\u5c11\u7684\u8ba1\u7b97\u5f00\u9500\u6355\u6349\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "RAM-Net\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u6210\u529f\u5730\u5728\u4e0d\u727a\u7272\u5185\u5b58\u6548\u7387\u7684\u524d\u63d0\u4e0b\u589e\u5f3a\u4e86\u6a21\u578b\u5904\u7406\u590d\u6742\u4f9d\u8d56\u7684\u80fd\u529b\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u7ebf\u6027\u6ce8\u610f\u673a\u5236\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.11965", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11965", "abs": "https://arxiv.org/abs/2602.11965", "authors": ["Yiheng Yao", "Zekun Cai", "Xinyuan Song", "Hiroki Hill Kobayashi", "Xuan Song", "Ryosuke Shibasaki", "Liang Zhao"], "title": "Manifold-Aware Temporal Domain Generalization for Large Language Models", "comment": "14 pages, 2 figures", "summary": "Temporal distribution shifts are pervasive in real-world deployments of Large Language Models (LLMs), where data evolves continuously over time. While Temporal Domain Generalization (TDG) seeks to model such structured evolution, existing approaches characterize model adaptation in the full parameter space. This formulation becomes computationally infeasible for modern LLMs. This paper introduces a geometric reformulation of TDG under parameter-efficient fine-tuning. We establish that the low-dimensional temporal structure underlying model evolution can be preserved under parameter-efficient reparameterization, enabling temporal modeling without operating in the ambient parameter space. Building on this principle, we propose Manifold-aware Temporal LoRA (MaT-LoRA), which constrains temporal updates to a shared low-dimensional manifold within a low-rank adaptation subspace, and models its evolution through a structured temporal core. This reparameterization dramatically reduces temporal modeling complexity while retaining expressive power. Extensive experiments on synthetic and real-world datasets, including scientific documents, news publishers, and review ratings, demonstrate that MaT-LoRA achieves superior temporal generalization performance with practical scalability for LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMaT-LoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f4e\u79e9\u9002\u5e94\u5b50\u7a7a\u95f4\u5185\u7ea6\u675f\u65f6\u95f4\u66f4\u65b0\u81f3\u5171\u4eab\u7684\u4f4e\u7ef4\u6d41\u5f62\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u65f6\u95f4\u6838\u5fc3\u5efa\u6a21\u5176\u6f14\u53d8\u8fc7\u7a0b\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5168\u53c2\u6570\u7a7a\u95f4\u4e0b\u7684\u65f6\u95f4\u57df\u6cdb\u5316\uff08TDG\uff09\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u964d\u4f4e\u4e86\u65f6\u95f4\u5efa\u6a21\u590d\u6742\u5ea6\uff0c\u8fd8\u4fdd\u6301\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u65f6\u95f4\u6cdb\u5316\u6027\u80fd\u53ca\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u90e8\u7f72\u65f6\u9762\u4e34\u7740\u968f\u65f6\u95f4\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002\u867d\u7136\u65f6\u95f4\u57df\u6cdb\u5316\uff08TDG\uff09\u8bd5\u56fe\u89e3\u51b3\u8fd9\u79cd\u7ed3\u6784\u5316\u6f14\u53d8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5728\u5168\u53c2\u6570\u7a7a\u95f4\u4e0b\u8fdb\u884c\u6a21\u578b\u8c03\u6574\uff0c\u8fd9\u4f7f\u5f97\u5bf9\u4e8e\u73b0\u4ee3LLMs\u6765\u8bf4\u53d8\u5f97\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u65f6\u95f4\u57df\u6cdb\u5316\uff08TDG\uff09\u51e0\u4f55\u91cd\u6784\u65b9\u6cd5\u3002\u901a\u8fc7\u8bc1\u660e\u6a21\u578b\u6f14\u53d8\u80cc\u540e\u7684\u4f4e\u7ef4\u65f6\u95f4\u7ed3\u6784\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u91cd\u65b0\u53c2\u6570\u5316\u5f97\u4ee5\u4fdd\u7559\uff0c\u4f7f\u5f97\u65e0\u9700\u5728\u539f\u59cb\u53c2\u6570\u7a7a\u95f4\u4e2d\u4e5f\u80fd\u5b9e\u73b0\u65f6\u95f4\u5efa\u6a21\u3002\u57fa\u4e8e\u8fd9\u4e00\u539f\u7406\uff0c\u63d0\u51fa\u4e86Manifold-aware Temporal LoRA (MaT-LoRA)\uff0c\u5b83\u5c06\u65f6\u95f4\u66f4\u65b0\u9650\u5236\u5728\u4e00\u4e2a\u5171\u4eab\u7684\u4f4e\u7ef4\u6d41\u5f62\u5185\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u65f6\u95f4\u6838\u5fc3\u6765\u6a21\u62df\u5176\u6f14\u53d8\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u9488\u5bf9\u5408\u6210\u6570\u636e\u8fd8\u662f\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u79d1\u5b66\u6587\u6863\u3001\u65b0\u95fb\u51fa\u7248\u5546\u548c\u8bc4\u8bba\u8bc4\u5206\u7b49\u6570\u636e\u96c6\uff0cMaT-LoRA\u90fd\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u65f6\u95f4\u6cdb\u5316\u8868\u73b0\uff0c\u540c\u65f6\u5bf9LLMs\u800c\u8a00\u5177\u6709\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MaT-LoRA\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u65f6\u95f4\u5efa\u6a21\u7684\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u8868\u8fbe\u529b\uff0c\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u5bf9\u7684\u65f6\u95f4\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11995", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11995", "abs": "https://arxiv.org/abs/2602.11995", "authors": ["Yifei Jin", "Xin Zheng", "Lei Guo"], "title": "Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret", "comment": "9 pages, 3 figures", "summary": "In large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a8\u91cf\u6700\u5c0f\u5747\u65b9\uff08MLMS\uff09\u7b97\u6cd5\u5728\u65f6\u53d8\u968f\u673a\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u5feb\u901f\u9002\u5e94\u6027\u548c\u9c81\u68d2\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u573a\u666f\u4e2d\uff0c\u6570\u636e\u901a\u5e38\u4ee5\u7531\u590d\u6742\u7cfb\u7edf\u751f\u6210\u7684\u987a\u5e8f\u6d41\u5f62\u5f0f\u5230\u8fbe\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u8868\u73b0\u51fa\u6f02\u79fb\u5206\u5e03\u548c\u65f6\u53d8\u7cfb\u7edf\u53c2\u6570\u3002\u8fd9\u79cd\u975e\u5e73\u7a33\u6027\u6311\u6218\u4e86\u4f20\u7edf\u7684\u72ec\u7acb\u540c\u5206\u5e03\u6837\u672c\u5047\u8bbe\uff0c\u9700\u8981\u80fd\u591f\u5b9e\u65f6\u66f4\u65b0\u800c\u65e0\u9700\u6602\u8d35\u518d\u8bad\u7ec3\u7684\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u52a8\u91cf\u6700\u5c0f\u5747\u65b9\uff08MLMS\uff09\u7b97\u6cd5\u4f5c\u4e3a\u81ea\u9002\u5e94\u8bc6\u522b\u5de5\u5177\uff0c\u5e76\u5bf9\u5176\u5728\u4e0d\u540c\u5b9e\u9645\u6761\u4ef6\u4e0b\u7684\u65f6\u53d8\u968f\u673a\u7ebf\u6027\u7cfb\u7edf\u7684\u8ddf\u8e2a\u6027\u80fd\u548c\u9057\u61be\u754c\u8fdb\u884c\u4e86\u7406\u8bba\u63a8\u5bfc\u3002\u4e0e\u7ecf\u5178LMS\u7b97\u6cd5\u76f8\u6bd4\uff0cMLMS\u7531\u4e8e\u5f15\u5165\u4e86\u52a8\u91cf\u800c\u589e\u52a0\u4e86\u4e00\u4e2a\u52a8\u6001\u72b6\u6001\uff0c\u5bfc\u81f4\u4e86\u66f4\u590d\u6742\u7684\u7a33\u5b9a\u6027\u5206\u6790\u95ee\u9898\u3002", "result": "\u5408\u6210\u6570\u636e\u6d41\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6d41\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMLMS\u5b9e\u73b0\u4e86\u5feb\u901f\u9002\u5e94\u548c\u9c81\u68d2\u8ddf\u8e2a\uff0c\u7279\u522b\u662f\u5728\u975e\u5e73\u7a33\u8bbe\u7f6e\u4e0b\uff0c\u8fd9\u4e0e\u6211\u4eec\u7684\u7406\u8bba\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "MLMS\u7b97\u6cd5\u5c55\u793a\u51fa\u5728\u73b0\u4ee3\u6d41\u5904\u7406\u548c\u5728\u7ebf\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u975e\u5e73\u7a33\u6570\u636e\u6d41\u65b9\u9762\u3002"}}
{"id": "2602.12014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12014", "abs": "https://arxiv.org/abs/2602.12014", "authors": ["Gongxi Zhu", "Hanlin Gu", "Lixin Fan", "Qiang Yang", "Yuxing Han"], "title": "FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client", "comment": "Accepted by AAAI 2026 as Oral", "summary": "One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or incur high communication costs and introduce unavoidable privacy risks. We reformulate this problem as a reinforcement learning style evaluation process and propose FedGRPO, a privacy preserving framework comprising two modules. The first module performs competence-based expert selection by building a lightweight confidence graph from auxiliary data to identify the most suitable clients for each question. The second module leverages the \"Group Relative\" concept from the Group Relative Policy Optimization (GRPO) framework by packaging each question together with its solution rationale into candidate policies, dispatching these policies to a selected subset of expert clients, and aggregating solely the resulting scalar reward signals via a federated group-relative loss function. By exchanging reward values instead of data or model updates, FedGRPO reduces privacy risk and communication overhead while enabling parallel evaluation across heterogeneous devices. Empirical results on diverse domain tasks demonstrate that FedGRPO achieves superior downstream accuracy and communication efficiency compared to conventional FedFMs baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedGRPO\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u90a6\u57fa\u7840\u6a21\u578b\uff08FedFMs\uff09\u4e2d\u901a\u8fc7\u5ba2\u6237\u7aef\u6570\u636e\u589e\u5f3a\u670d\u52a1\u5668\u7aef\u5927\u6a21\u578b\u6027\u80fd\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u8f7b\u91cf\u7ea7\u4fe1\u5fc3\u56fe\u6765\u9009\u62e9\u5408\u9002\u7684\u5ba2\u6237\u7aef\uff0c\u5e76\u5c06\u95ee\u9898\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u6253\u5305\u6210\u5019\u9009\u7b56\u7565\u53d1\u9001\u7ed9\u9009\u5b9a\u7684\u4e13\u5bb6\u5ba2\u6237\u7aef\uff0c\u4ec5\u6536\u96c6\u7ed3\u679c\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u9690\u79c1\u98ce\u9669\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u57fa\u7840\u6a21\u578b(FedFMs)\u4e2d\u5b58\u5728\u7684\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u672c\u5730\u8bad\u7ec3\uff0c\u8981\u4e48\u5bfc\u81f4\u9ad8\u901a\u4fe1\u6210\u672c\u5e76\u5e26\u6765\u4e0d\u53ef\u907f\u514d\u7684\u9690\u79c1\u98ce\u9669\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u4ee5\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "FedGRPO\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u6a21\u5757\uff1a\u7b2c\u4e00\u4e2a\u6a21\u5757\u57fa\u4e8e\u8f85\u52a9\u6570\u636e\u5efa\u7acb\u8f7b\u91cf\u7ea7\u7684\u4fe1\u5fc3\u56fe\u6765\u8fdb\u884c\u80fd\u529b\u4e3a\u57fa\u7840\u7684\u4e13\u5bb6\u9009\u62e9\uff1b\u7b2c\u4e8c\u4e2a\u6a21\u5757\u91c7\u7528\u201c\u7ec4\u76f8\u5bf9\u201d\u6982\u5ff5\u4eceGroup Relative Policy Optimization (GRPO)\u6846\u67b6\u4e2d\uff0c\u5c06\u6bcf\u4e2a\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848\u539f\u7406\u4e00\u8d77\u6253\u5305\u4e3a\u5019\u9009\u7b56\u7565\uff0c\u7136\u540e\u5206\u53d1\u7ed9\u88ab\u9009\u4e2d\u7684\u4e13\u5bb6\u5ba2\u6237\u7aef\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u7ec4\u76f8\u5bf9\u635f\u5931\u51fd\u6570\u4ec5\u805a\u5408\u5f97\u5230\u7684\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u591a\u6837\u5316\u7684\u9886\u57df\u4efb\u52a1\u4e0a\uff0cFedGRPO\u76f8\u6bd4\u4f20\u7edfFedFMs\u57fa\u7ebf\u5728\u4e0b\u6e38\u51c6\u786e\u6027\u4ee5\u53ca\u901a\u4fe1\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "FedGRPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u8868\u73b0\uff0c\u540c\u65f6\u51cf\u5c11\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\u548c\u901a\u4fe1\u6210\u672c\u3002"}}
{"id": "2602.12045", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12045", "abs": "https://arxiv.org/abs/2602.12045", "authors": ["Jed A. Duersch", "Elohan Veillon", "Astrid Klipfel", "Adlane Sayede", "Zied Bouraoui"], "title": "Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling", "comment": null, "summary": "The discovery of new crystalline materials calls for generative models that handle periodic boundary conditions, crystallographic symmetries, and physical constraints, while scaling to large and structurally diverse unit cells. We propose a reciprocal-space generative pipeline that represents crystals through a truncated Fourier transform of the species-resolved unit-cell density, rather than modeling atomic coordinates directly. This representation is periodicity-native, admits simple algebraic actions of space-group symmetries, and naturally supports variable atomic multiplicities during generation, addressing a common limitation of particle-based approaches. Using only nine Fourier basis functions per spatial dimension, our approach reconstructs unit cells containing up to 108 atoms per chemical species. We instantiate this pipeline with a transformer variational autoencoder over complex-valued Fourier coefficients, and a latent diffusion model that generates in the compressed latent space. We evaluate reconstruction and latent diffusion on the LeMaterial benchmark and compare unconditional generation against coordinate-based baselines in the small-cell regime ($\\leq 16$ atoms per unit cell).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5012\u6613\u7a7a\u95f4\u7684\u6676\u4f53\u6750\u6599\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u8868\u793a\u6676\u4f53\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u53d8\u538b\u5668\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u751f\u6210\u65b0\u7684\u6676\u4f53\u5355\u5143\u3002", "motivation": "\u53d1\u73b0\u65b0\u7684\u6676\u4f53\u6750\u6599\u9700\u8981\u80fd\u591f\u5904\u7406\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u3001\u6676\u4f53\u5b66\u5bf9\u79f0\u6027\u548c\u7269\u7406\u7ea6\u675f\u7684\u751f\u6210\u6a21\u578b\uff0c\u540c\u65f6\u8981\u80fd\u591f\u6269\u5c55\u5230\u5927\u4e14\u7ed3\u6784\u591a\u6837\u7684\u5355\u5143\u683c\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u57fa\u4e8e\u5012\u6613\u7a7a\u95f4\u7684\u751f\u6210\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u901a\u8fc7\u7269\u79cd\u5206\u8fa8\u5355\u4f4d\u7ec6\u80de\u5bc6\u5ea6\u7684\u622a\u65ad\u5085\u91cc\u53f6\u53d8\u6362\u6765\u8868\u793a\u6676\u4f53\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5efa\u6a21\u539f\u5b50\u5750\u6807\u3002\u6b64\u8868\u793a\u6cd5\u662f\u56fa\u6709\u5468\u671f\u6027\u7684\uff0c\u5141\u8bb8\u7b80\u5355\u7684\u7a7a\u95f4\u7fa4\u5bf9\u79f0\u6027\u4ee3\u6570\u64cd\u4f5c\uff0c\u5e76\u81ea\u7136\u652f\u6301\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u53d8\u7684\u539f\u5b50\u591a\u91cd\u6027\u3002", "result": "\u4f7f\u7528\u6bcf\u7a7a\u95f4\u7ef4\u5ea6\u4ec5\u4e5d\u4e2a\u5085\u91cc\u53f6\u57fa\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u91cd\u5efa\u5305\u542b\u591a\u8fbe108\u4e2a\u6bcf\u4e2a\u5316\u5b66\u7269\u79cd\u539f\u5b50\u7684\u5355\u5143\u683c\u3002\u5728LeMaterial\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u91cd\u6784\u548c\u6f5c\u5728\u6269\u6563\uff0c\u5e76\u4e0e\u57fa\u4e8e\u5750\u6807\u7684\u57fa\u7ebf\u5728\u5c0f\u5355\u5143\u683c\u4f53\u7cfb\uff08\u6bcf\u5355\u4f4d\u6676\u80de$\\leq 16$\u4e2a\u539f\u5b50\uff09\u4e2d\u8fdb\u884c\u4e86\u65e0\u6761\u4ef6\u751f\u6210\u6bd4\u8f83\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u751f\u6210\u5177\u6709\u590d\u6742\u7ed3\u6784\u7684\u65b0\u6676\u4f53\u6750\u6599\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u9014\u5f84\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u800c\u7ed3\u6784\u591a\u6837\u5316\u7684\u5355\u4f4d\u6676\u80de\uff0c\u540c\u65f6\u514b\u670d\u4e86\u57fa\u4e8e\u7c92\u5b50\u65b9\u6cd5\u7684\u4e00\u4e9b\u9650\u5236\u3002"}}
{"id": "2602.12049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12049", "abs": "https://arxiv.org/abs/2602.12049", "authors": ["Ryo Mikasa", "Shun-ichiro Hayashi", "Daichi Mukunoki", "Tetsuya Hoshino", "Takahiro Katagiri"], "title": "Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u6267\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\uff0c\u5e76\u76f4\u63a5\u5c06\u6d4b\u91cf\u5230\u7684\u8fd0\u884c\u65f6\u6027\u80fd\uff08GFLOPS\uff09\u4f5c\u4e3a\u5956\u52b1\u53cd\u9988\u7ed9\u6a21\u578b\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u5206\u9636\u6bb5\u8d28\u91cf\u591a\u6837\u6027\u7b97\u6cd5\u6765\u9010\u6b65\u6539\u53d8\u6bcf\u4e2a\u95ee\u9898\u5141\u8bb8\u7684\u4f18\u5316\u6280\u672f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u591a\u89d2\u5ea6\u5b66\u4e60\u4ee3\u7801\u4f18\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u6027\u80fd\u53cd\u9988\u4e0e\u5206\u9636\u6bb5\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u63d0\u9ad8LLM\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u751f\u6210\u4ee3\u7801\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u5e76\u4e0d\u603b\u662f\u5f97\u5230\u4fdd\u8bc1\uff0c\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u5185\u9c9c\u6709\u5c1d\u8bd5\u4f7f\u7528\u8fd0\u884c\u65f6\u6027\u80fd\u4f5c\u4e3a\u5956\u52b1\u8bad\u7ec3\u8fd9\u4e9b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u6267\u884c\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\uff0c\u5e76\u4ee5\u5b9e\u9645\u6d4b\u91cf\u7684\u8fd0\u884c\u65f6\u6027\u80fd\uff08GFLOPS\uff09\u4e3a\u5956\u52b1\u4fe1\u53f7\uff1b\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aStaged Quality-Diversity (SQD) \u7684\u65b0\u7b97\u6cd5\uff0c\u5b83\u80fd\u591f\u6839\u636e\u5177\u4f53\u95ee\u9898\u9010\u6b65\u8c03\u6574\u6240\u5141\u8bb8\u4f7f\u7528\u7684\u4f18\u5316\u6280\u672f\uff1b\u6784\u5efa\u4e86\u4e00\u4e2a\u8fde\u63a5GPU\u8bad\u7ec3\u96c6\u7fa4\u548cCPU\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7fa4\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5e76\u5229\u7528Group Relative Policy Optimization (GRPO) \u5bf9Qwen2.5 Coder 14B\u6a21\u578b\u8fdb\u884c\u53cc\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\u4efb\u52a1\u7684\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u6027\u80fd\u53cd\u9988\u4e0e\u5206\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u786e\u5b9e\u80fd\u591f\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u573a\u666f\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u8fd0\u884c\u65f6\u6027\u80fd\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u5e76\u7ed3\u5408\u5206\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u6765\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u7684\u4ee3\u7801\u751f\u6210\u8868\u73b0\u3002"}}
{"id": "2602.12080", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12080", "abs": "https://arxiv.org/abs/2602.12080", "authors": ["Hyunsung Kim", "Kunhee Lee", "Sangwoo Seo", "Sang-Ki Ko", "Jinsung Yoon", "Chanyoung Park"], "title": "PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories", "comment": null, "summary": "Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PathCRF\u6846\u67b6\uff0c\u5229\u7528\u7403\u5458\u8ddf\u8e2a\u6570\u636e\u6765\u68c0\u6d4b\u8db3\u7403\u6bd4\u8d5b\u4e2d\u7684\u6301\u7403\u4e8b\u4ef6\u3002\u901a\u8fc7\u5c06\u7403\u5458\u8f68\u8ff9\u5efa\u6a21\u4e3a\u5b8c\u5168\u8fde\u63a5\u7684\u52a8\u6001\u56fe\uff0c\u5e76\u91c7\u7528\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5730\u8bc6\u522b\u8bf8\u5982\u63a7\u7403\u6216\u4f20\u7403\u7b49\u4e8b\u4ef6\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u4e8b\u4ef6\u6807\u6ce8\u7684\u9700\u6c42\u3002", "motivation": "\u5c3d\u7ba1AI\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u8db3\u7403\u8d5b\u4e8b\u7684\u6570\u636e\u6536\u96c6\u4ecd\u7136\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002\u73b0\u6709\u7814\u7a76\u867d\u7136\u63a2\u7d22\u4e86\u4f7f\u7528\u7403\u5458\u548c\u7403\u7684\u8f68\u8ff9\u8fdb\u884c\u81ea\u52a8\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u4f46\u7531\u4e8e\u9ad8\u6602\u7684\u57fa\u7840\u5efa\u8bbe\u548c\u8fd0\u8425\u6210\u672c\uff0c\u7403\u7684\u8ffd\u8e2a\u4e5f\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\u3002\u8fd9\u5bfc\u81f4\u5168\u9762\u7684\u6570\u636e\u6536\u96c6\u4e3b\u8981\u9650\u4e8e\u9876\u7ea7\u6bd4\u8d5b\uff0c\u9650\u5236\u4e86\u6570\u636e\u5206\u6790\u5728\u8fd9\u4e00\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPathCRF\u7684\u6846\u67b6\uff0c\u5b83\u4ec5\u4f7f\u7528\u7403\u5458\u8ddf\u8e2a\u6570\u636e\u6765\u68c0\u6d4b\u8db3\u7403\u6bd4\u8d5b\u4e2d\u7684\u6301\u7403\u4e8b\u4ef6\u3002\u8be5\u65b9\u6cd5\u5c06\u7403\u5458\u8f68\u8ff9\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5168\u8fde\u63a5\u7684\u52a8\u6001\u56fe\uff0c\u5e76\u4e14\u5b9a\u4e49\u4e8b\u4ef6\u68c0\u6d4b\u95ee\u9898\u4e3a\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9009\u62e9\u4e00\u4e2a\u4ee3\u8868\u5f53\u524d\u63a7\u7403\u72b6\u6001\u7684\u8fb9\u3002\u4e3a\u4e86\u4fdd\u8bc1\u7ed3\u679c\u8fb9\u5e8f\u5217\u7684\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u91c7\u7528\u4e86\u6761\u4ef6\u968f\u673a\u573a(CRF)\u7981\u6b62\u8fde\u7eed\u8fb9\u4e4b\u95f4\u7684\u4e0d\u53ef\u80fd\u8f6c\u6362\u3002\u53d1\u5c04\u5206\u6570\u548c\u8f6c\u79fb\u5206\u6570\u90fd\u662f\u4ece\u57fa\u4e8e\u96c6\u5408\u6ce8\u610f\u529b\u673a\u5236\u9aa8\u5e72\u67b6\u6784\u751f\u6210\u7684\u8fb9\u5d4c\u5165\u4e2d\u52a8\u6001\u8ba1\u7b97\u5f97\u51fa\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7Viterbi\u89e3\u7801\u83b7\u5f97\u6700\u53ef\u80fd\u7684\u8fb9\u5e8f\u5217\uff0c\u5e76\u4e14\u5f53\u76f8\u90bb\u65f6\u95f4\u6b65\u4e4b\u95f4\u9009\u5b9a\u7684\u8fb9\u53d1\u751f\u53d8\u5316\u65f6\u5c31\u68c0\u6d4b\u5230\u50cf\u63a7\u7403\u6216\u4f20\u7403\u8fd9\u6837\u7684\u4e8b\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPathCRF\u80fd\u591f\u4ea7\u751f\u51c6\u786e\u4e14\u903b\u8f91\u4e00\u81f4\u7684\u63a7\u7403\u8def\u5f84\uff0c\u4f7f\u5f97\u53ef\u9760\u7684\u4e0b\u6e38\u5206\u6790\u6210\u4e3a\u53ef\u80fd\uff0c\u540c\u65f6\u5927\u5927\u964d\u4f4e\u4e86\u5bf9\u4eba\u5de5\u4e8b\u4ef6\u6807\u6ce8\u7684\u9700\u6c42\u3002", "conclusion": "PathCRF\u6846\u67b6\u4e3a\u8db3\u7403\u6bd4\u8d5b\u4e2d\u6301\u7403\u4e8b\u4ef6\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u5185\u66f4\u52a0\u5e7f\u6cdb\u7684\u6570\u636e\u9a71\u52a8\u5206\u6790\u5e94\u7528\u3002"}}
{"id": "2602.12082", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.12082", "abs": "https://arxiv.org/abs/2602.12082", "authors": ["Jihao Andreas Lin", "Sebastian Ament", "Louis C. Tiao", "David Eriksson", "Maximilian Balandat", "Eytan Bakshy"], "title": "Empirical Gaussian Processes", "comment": null, "summary": "Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Empirical GPs\uff0c\u4e00\u79cd\u4ece\u5386\u53f2\u89c2\u6d4b\u6570\u636e\u4e2d\u4f30\u8ba1\u5747\u503c\u548c\u534f\u65b9\u5dee\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6a21\u578b\u4e2d\u6838\u51fd\u6570\u9009\u62e9\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u7406\u8bba\u4e0a\u6536\u655b\u4e8e\u4e0e\u771f\u5b9e\u6570\u636e\u751f\u6210\u8fc7\u7a0bKL\u6563\u5ea6\u6700\u5c0f\u7684GP\uff0c\u5e76\u5728\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u6a21\u578b\u5728\u5b9e\u8df5\u4e2d\u6709\u6548\u6027\u53d7\u9650\u4e8e\u624b\u5de5\u6311\u9009\u7684\u6838\u51fd\u6570\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u4e0d\u4ec5\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u4e14\u5bf9\u5047\u8bbe\u7a7a\u95f4\u65bd\u52a0\u4e86\u8f83\u5f3a\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u9002\u5e94\u6570\u636e\u7684\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aEmpirical GPs\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5386\u53f2\u89c2\u5bdf\u8bed\u6599\u5e93\u4e2d\u7ecf\u9a8c\u5730\u4f30\u8ba1\u5747\u503c\u548c\u534f\u65b9\u5dee\u51fd\u6570\u6765\u6784\u5efa\u7075\u6d3b\u7684\u6570\u636e\u9a71\u52a8GP\u5148\u9a8c\uff0c\u800c\u975e\u4f9d\u8d56\u4e8e\u6807\u51c6\u53c2\u6570\u5316\u6838\u3002\u6b64\u5916\uff0c\u8fd8\u4e3a\u4ece\u72ec\u7acb\u6570\u636e\u96c6\u4e2d\u5b66\u4e60GP\u5148\u9a8c\u7684\u95ee\u9898\u5236\u5b9a\u4e86\u4f3c\u7136\u4f30\u8ba1\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e00\u4e2a\u5177\u6709\u5c01\u95ed\u5f62\u5f0f\u66f4\u65b0\u7684\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\uff0c\u652f\u6301\u5904\u7406\u8de8\u6570\u636e\u96c6\u7684\u5f02\u8d28\u89c2\u6d4b\u4f4d\u7f6e\u3002", "result": "Empirical GPs\u80fd\u591f\u5728\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6536\u655b\u5230\u4e0e\u5b9e\u9645\u6570\u636e\u4ea7\u751f\u8fc7\u7a0b\u6700\u63a5\u8fd1\uff08\u5728KL\u6563\u5ea6\u610f\u4e49\u4e0a\uff09\u7684GP\u3002", "conclusion": "Empirical GPs\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6765\u6784\u5efa\u66f4\u52a0\u7075\u6d3b\u4e14\u9002\u5e94\u6027\u5f3a\u7684GP\u5148\u9a8c\uff0c\u4ece\u800c\u514b\u670d\u4e86\u4f20\u7edfGP\u6a21\u578b\u7684\u4e00\u4e9b\u5c40\u9650\u6027\u3002"}}
{"id": "2602.12107", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.12107", "abs": "https://arxiv.org/abs/2602.12107", "authors": ["Haolin Liu", "Braham Snyder", "Chen-Yu Wei"], "title": "On the Complexity of Offline Reinforcement Learning with $Q^\\star$-Approximation and Partial Coverage", "comment": null, "summary": "We study offline reinforcement learning under $Q^\\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: \"Are $Q^\\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?\"\n  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\\star$ estimation procedures, modularizing and generalizing existing approaches.\n  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $\u03b5^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $\u03b5^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\\star$-realizability and Bellman completeness beyond the tabular case.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728$Q^\\star$\u8fd1\u4f3c\u548c\u90e8\u5206\u8986\u76d6\u4e0b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u65b0\u7684\u51b3\u7b56-\u4f30\u8ba1\u5206\u89e3\u6846\u67b6\u6765\u8868\u5f81\u7ed9\u5b9a$Q^\\star$\u51fd\u6570\u7c7b\u7684\u5185\u5728\u590d\u6742\u6027\uff0c\u5e76\u4e14\u5728\u90e8\u5206\u8986\u76d6\u7684\u60c5\u51b5\u4e0b\u4e3a\u8f6f$Q$-\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a$\\epsilon^{-2}$\u6837\u672c\u590d\u6742\u5ea6\u7ed3\u679c\u3002", "motivation": "\u63a2\u8ba8$Q^\\star$\u53ef\u5b9e\u73b0\u6027\u548c\u8d1d\u5c14\u66fc\u5b8c\u5907\u6027\u662f\u5426\u8db3\u4ee5\u4fdd\u8bc1\u5728\u90e8\u5206\u8986\u76d6\u4e0b\u8fdb\u884c\u6837\u672c\u9ad8\u6548\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u540c\u65f6\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\u63d0\u51fa\u4e86\u65b0\u7684\u7406\u8bba\u8d21\u732e\u548c\u6280\u672f\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51b3\u7b56-\u4f30\u8ba1\u5206\u89e3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u4e0e\u591a\u79cd$Q^\\star$\u4f30\u8ba1\u7a0b\u5e8f\u7ed3\u5408\u4f7f\u7528\uff1b\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4e8c\u9636\u6027\u80fd\u5dee\u5f02\u5f15\u7406\u4ee5\u6539\u5584\u6837\u672c\u590d\u6742\u5ea6\u7684\u7ed3\u679c\u3002", "result": "\u5f97\u51fa\u4e86\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86\u4ec5\u9760$Q^\\star$\u53ef\u5b9e\u73b0\u6027\u548c\u8d1d\u5c14\u66fc\u5b8c\u5907\u6027\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u5b66\u4e60\uff1b\u5bf9\u8f6f$Q$-\u5b66\u4e60\u7ed9\u51fa\u4e86\u7b2c\u4e00\u4e2a$\\epsilon^{-2}$\u6837\u672c\u590d\u6742\u5ea6\u7ed3\u679c\uff1b\u53bb\u9664\u4e86\u5f53$Q^\\star$\u7684\u4ef7\u503c\u5dee\u8ddd\u672a\u77e5\u65f6\u989d\u5916\u5728\u7ebf\u4ea4\u4e92\u7684\u9700\u6c42\uff1b\u9996\u6b21\u5206\u6790\u4e86\u975e\u8868\u683c\u60c5\u51b5\u4e0b\u7684CQL\u3002", "conclusion": "\u672c\u6587\u7684\u5de5\u4f5c\u4e0d\u4ec5\u56de\u7b54\u4e86\u5173\u4e8e\u79bb\u7ebfRL\u4e2d$Q^\\star$\u53ef\u5b9e\u73b0\u6027\u548c\u8d1d\u5c14\u66fc\u5b8c\u5907\u6027\u7684\u5f00\u653e\u95ee\u9898\uff0c\u800c\u4e14\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u548c\u6280\u672f\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u6211\u4eec\u5bf9\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6311\u6218\u7684\u7406\u89e3\u3002"}}
{"id": "2602.12123", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12123", "abs": "https://arxiv.org/abs/2602.12123", "authors": ["Xubin Wang", "Weijia Jia"], "title": "Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning", "comment": null, "summary": "Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.\n  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.\n  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u76d1\u7763\u5143\u5b66\u4e60\u65b9\u6cd5Meta-Sel\uff0c\u7528\u4e8e\u610f\u56fe\u5206\u7c7b\u4e2d\u7684\u6f14\u793a\u9009\u62e9\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u4e24\u4e2a\u4f4e\u6210\u672c\u5143\u7279\u5f81\uff08TF-IDF\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u957f\u5ea6\u517c\u5bb9\u6027\u6bd4\u7387\uff09\u7684\u6821\u51c6\u903b\u8f91\u56de\u5f52\u6a21\u578b\u6765\u5b9e\u73b0\u3002\u5728\u63a8\u7406\u65f6\uff0cMeta-Sel\u80fd\u591f\u5bf9\u6574\u4e2a\u5019\u9009\u6c60\u8fdb\u884c\u4e00\u6b21\u6027\u5411\u91cf\u5316\u8bc4\u5206\uff0c\u5e76\u8fd4\u56de\u524dk\u4e2a\u6f14\u793a\u793a\u4f8b\uff0c\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u3001\u5728\u7ebf\u63a2\u7d22\u6216\u989d\u5916\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e8612\u79cd\u4e0d\u540c\u65b9\u6cd5\u5728\u56db\u4e2a\u610f\u56fe\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0cMeta-Sel\u5728\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5bf9\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\u7279\u522b\u6709\u6548\u3002", "motivation": "\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\uff0c\u6f14\u793a\u9009\u62e9\u662f\u4e00\u4e2a\u5b9e\u9645\u74f6\u9888\uff1a\u5f53\u63d0\u793a\u9884\u7b97\u6709\u9650\u65f6\uff0c\u6240\u5305\u542b\u7684\u5c11\u6570\u793a\u4f8b\u4f1a\u663e\u8457\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u4f46\u540c\u65f6\u9009\u62e9\u8fc7\u7a0b\u5fc5\u987b\u8db3\u591f\u9ad8\u6548\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u5019\u9009\u6c60\u3002", "method": "Meta-Sel\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u76d1\u7763\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u4ece\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u5feb\u901f\u4e14\u53ef\u89e3\u91ca\u7684\uff08\u5019\u9009\uff0c\u67e5\u8be2\uff09\u5bf9\u6253\u5206\u51fd\u6570\u3002\u901a\u8fc7\u4ece\u8bad\u7ec3\u96c6\u4e2d\u91c7\u6837\u5bf9\u5e76\u4f7f\u7528\u7c7b\u522b\u4e00\u81f4\u6027\u4f5c\u4e3a\u76d1\u7763\u6784\u5efa\u5143\u6570\u636e\u96c6\uff0c\u7136\u540e\u5728\u4e00\u4e2a\u57fa\u4e8eTF-IDF\u4f59\u5f26\u76f8\u4f3c\u6027\u548c\u957f\u5ea6\u517c\u5bb9\u6027\u6bd4\u7387\u7684\u4f4e\u6210\u672c\u5143\u7279\u5f81\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u6821\u51c6\u903b\u8f91\u56de\u5f52\u5668\u3002", "result": "Meta-Sel\u80fd\u591f\u5728\u4e0d\u9700\u8981\u6a21\u578b\u5fae\u8c03\u3001\u5728\u7ebf\u63a2\u7d22\u6216\u989d\u5916\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6574\u4e2a\u5019\u9009\u6c60\u6267\u884c\u4e00\u6b21\u5411\u91cf\u5316\u8bc4\u5206\uff0c\u5e76\u8fd4\u56de\u6700\u4f73k\u4e2a\u793a\u4f8b\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u786e\u5b9a\u6027\u7684\u6392\u540d\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u529f\u80fd\u6743\u91cd\u4f7f\u9009\u62e9\u673a\u5236\u6613\u4e8e\u5ba1\u6838\u3002\u5e7f\u6cdb\u7684\u7ecf\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u5728\u56db\u4e2a\u610f\u56fe\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5341\u4e8c\u79cd\u65b9\u6cd5\u5bf9\u6bd4\u4e2d\uff0cMeta-Sel\u8868\u73b0\u6301\u7eed\u4f4d\u4e8e\u524d\u5217\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u6548\u679c\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "Meta-Sel\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u6f14\u793a\u9009\u62e9\u65b9\u6848\uff0c\u5728\u63d0\u5347\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u4e3a\u5c0f\u578b\u6a21\u578b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u9009\u62e9\u652f\u6301\u3002"}}
{"id": "2602.12125", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12125", "abs": "https://arxiv.org/abs/2602.12125", "authors": ["Wenkai Yang", "Weijie Liu", "Ruobing Xie", "Kai Yang", "Saiyong Yang", "Yankai Lin"], "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation", "comment": "Work in progress. Github repo: https://github.com/RUCBM/G-OPD", "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u5728\u7ebf\u7b56\u7565\u84b8\u998f(OPD)\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u5e7f\u4e49\u5728\u7ebf\u7b56\u7565\u84b8\u998f(G-OPD)\uff0c\u901a\u8fc7\u5f15\u5165\u7075\u6d3b\u7684\u53c2\u8003\u6a21\u578b\u548c\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u6765\u63a7\u5236\u5956\u52b1\u9879\u4e0eKL\u6b63\u5219\u5316\u7684\u76f8\u5bf9\u6743\u91cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u5927\u4e8e1\u65f6(ExOPD)\uff0c\u5b66\u751f\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6OPD\uff0c\u5e76\u4e14\u5728\u5f3a\u5230\u5f31\u7684\u77e5\u8bc6\u84b8\u998f\u8bbe\u7f6e\u4e0b\uff0c\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u524d\u7684\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u53c2\u8003\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u7b56\u7565\u84b8\u998f(OPD)\u5df2\u88ab\u8bc1\u660e\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u8868\u73b0\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u53ca\u5176\u76f8\u5bf9\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u4f18\u52bf\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u8ba8\u3002\u6b64\u5916\uff0c\u5982\u4f55\u66f4\u6709\u6548\u5730\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u5b66\u751f\u6a21\u578b\u4e5f\u662f\u4e00\u4e2a\u503c\u5f97\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u4ece\u7406\u8bba\u4e0a\u5c06OPD\u5b9a\u4e49\u4e3a\u4e00\u79cd\u5177\u6709\u7279\u5b9a\u7ea6\u675f\u6761\u4ef6\u7684\u5bc6\u96c6KL\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u3002\u57fa\u4e8e\u6b64\u7406\u89e3\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6G-OPD\uff0c\u5b83\u5141\u8bb8\u8c03\u6574\u5956\u52b1\u51fd\u6570\u4e0eKL\u6b63\u5219\u5316\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5e76\u652f\u6301\u4efb\u610f\u9009\u62e9\u53c2\u8003\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u91c7\u7528\u5956\u52b1\u5916\u63a8(\u5373\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u5927\u4e8e1)\u7684\u65b9\u6cd5(ExOPD)\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edfOPD\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5728\u5f3a\u81f3\u5f31\u6a21\u578b\u8f6c\u6362\u573a\u666f\u4e0b\uff0c\u82e5\u9009\u53d6\u6559\u5e08\u6a21\u578b\u672a\u7ecf\u5f3a\u5316\u5b66\u4e60\u8c03\u6574\u7684\u57fa\u7840\u7248\u672c\u4f5c\u4e3a\u53c2\u8003\uff0c\u5219\u53ef\u83b7\u5f97\u66f4\u4f73\u7684\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u52a0\u6df1\u4e86\u5bf9OPD\u673a\u5236\u7684\u7406\u89e3\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5G-OPD\u7528\u4e8e\u6539\u8fdb\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u3002\u7279\u522b\u662f\u53d1\u73b0\u9002\u5f53\u7684\u5956\u52b1\u7f29\u653e\u53ca\u53c2\u8003\u6a21\u578b\u9009\u62e9\u5bf9\u4e8e\u589e\u5f3a\u5b66\u751f\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.12139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12139", "abs": "https://arxiv.org/abs/2602.12139", "authors": ["Yashas Shende", "Aritra Das", "Reva Laxmi Chauhan", "Arghya Pathak", "Debayan Gupta"], "title": "Oscillators Are All You Need: Irregular Time Series Modelling via Damped Harmonic Oscillators with Closed-Form Solutions", "comment": null, "summary": "Transformers excel at time series modelling through attention mechanisms that capture long-term temporal patterns. However, they assume uniform time intervals and therefore struggle with irregular time series. Neural Ordinary Differential Equations (NODEs) effectively handle irregular time series by modelling hidden states as continuously evolving trajectories. ContiFormers arxiv:2402.10635 combine NODEs with Transformers, but inherit the computational bottleneck of the former by using heavy numerical solvers. This bottleneck can be removed by using a closed-form solution for the given dynamical system - but this is known to be intractable in general! We obviate this by replacing NODEs with a novel linear damped harmonic oscillator analogy - which has a known closed-form solution. We model keys and values as damped, driven oscillators and expand the query in a sinusoidal basis up to a suitable number of modes. This analogy naturally captures the query-key coupling that is fundamental to any transformer architecture by modelling attention as a resonance phenomenon. Our closed-form solution eliminates the computational overhead of numerical ODE solvers while preserving expressivity. We prove that this oscillator-based parameterisation maintains the universal approximation property of continuous-time attention; specifically, any discrete attention matrix realisable by ContiFormer's continuous keys can be approximated arbitrarily well by our fixed oscillator modes. Our approach delivers both theoretical guarantees and scalability, achieving state-of-the-art performance on irregular time series benchmarks while being orders of magnitude faster.", "AI": {"tldr": "Researchers developed a new method that combines the strengths of Transformers and Neural ODEs for handling irregular time series, by using a linear damped harmonic oscillator analogy with a known closed-form solution, which improves computational efficiency without sacrificing performance.", "motivation": "The motivation is to address the limitations of Transformers in dealing with irregular time series and the computational inefficiency of Neural ODEs, by introducing an approach that leverages a closed-form solution based on a linear damped harmonic oscillator analogy.", "method": "The method involves modeling keys and values as damped, driven oscillators and expanding the query in a sinusoidal basis. The use of a closed-form solution for the oscillator-based system allows for efficient computation while maintaining the expressivity and universal approximation property of continuous-time attention mechanisms.", "result": "The result is a scalable and computationally efficient method that achieves state-of-the-art performance on benchmarks for irregular time series, significantly outpacing existing methods in terms of speed.", "conclusion": "The conclusion is that the proposed oscillator-based parameterization not only resolves the computational bottleneck associated with numerical ODE solvers but also provides theoretical guarantees and superior performance for irregular time series modeling."}}
{"id": "2602.12147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12147", "abs": "https://arxiv.org/abs/2602.12147", "authors": ["Zhongzheng Qiao", "Sheng Pan", "Anni Wang", "Viktoriya Zhukova", "Yong Liu", "Xudong Jiang", "Qingsong Wen", "Mingsheng Long", "Ming Jin", "Chenghao Liu"], "title": "It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks", "comment": "The source code will be released on GitHub shortly", "summary": "Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.", "AI": {"tldr": "\u63d0\u51fa\u4e86TIME\uff0c\u4e00\u4e2a\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u7684\u65b0\u4e00\u4ee3\u4efb\u52a1\u4e2d\u5fc3\u57fa\u51c6\uff0c\u5305\u62ec50\u4e2a\u65b0\u6570\u636e\u96c6\u548c98\u4e2a\u9884\u6d4b\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u5728\u6570\u636e\u7ec4\u6210\u3001\u6570\u636e\u5b8c\u6574\u6027\u3001\u4efb\u52a1\u8868\u8ff0\u548c\u5206\u6790\u89c6\u89d2\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u8bc4\u4ef7\u57fa\u51c6\u5b58\u5728\u51e0\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u6570\u636e\u6765\u6e90\u53d7\u9650\u4e14\u591a\u4e3a\u65e7\u6709\u8d44\u6e90\u91cd\u590d\u4f7f\u7528\uff1b\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u4e0d\u8db3\uff1b\u4efb\u52a1\u8bbe\u5b9a\u4e0e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u8131\u8282\uff1b\u4ee5\u53ca\u5206\u6790\u89d2\u5ea6\u8fc7\u4e8e\u50f5\u5316\uff0c\u96be\u4ee5\u63ed\u793a\u901a\u7528\u89c1\u89e3\u3002", "method": "\u5f00\u53d1\u4e86TIME\uff0c\u4e00\u4e2a\u65b0\u7684\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b50\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u53ca98\u4e2a\u9884\u6d4b\u4efb\u52a1\uff0c\u4e13\u4e3a\u4e25\u683c\u7684\u96f6\u6837\u672cTSFM\u8bc4\u4f30\u800c\u8bbe\u8ba1\u3002\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u77e5\u8bc6\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u4eba\u673a\u534f\u540c\u57fa\u51c6\u6784\u5efa\u6d41\u7a0b\u6765\u786e\u4fdd\u9ad8\u6570\u636e\u5b8c\u6574\u6027\u548c\u91cd\u65b0\u5b9a\u4e49\u4efb\u52a1\u8bbe\u5b9a\u3002\u6b64\u5916\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u5f0f\u7ea7\u522b\u8bc4\u4f30\u89c6\u89d2\uff0c\u5229\u7528\u7ed3\u6784\u5316\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u6765\u63cf\u8ff0\u5185\u5728\u7684\u65f6\u95f4\u5c5e\u6027\u3002", "result": "\u5bf912\u4e2a\u4ee3\u8868\u6027TSFMs\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u591a\u5c42\u6b21\u6392\u884c\u699c\u4ee5\u4fbf\u6df1\u5165\u5206\u6790\u548c\u53ef\u89c6\u5316\u68c0\u67e5\u3002", "conclusion": "TIME\u4f5c\u4e3a\u4e00\u4e2a\u4e0b\u4e00\u4ee3\u4efb\u52a1\u4e2d\u5fc3\u57fa\u51c6\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u9c9c\u7684\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7684\u4efb\u52a1\u8bbe\u5b9a\u65b9\u5f0f\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8bc4\u6d4b\u4e2d\u5b58\u5728\u7684\u591a\u4e2a\u95ee\u9898\uff0c\u4e3a\u7406\u89e3\u4e0d\u540c\u6a21\u5f0f\u4e0b\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.12162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12162", "abs": "https://arxiv.org/abs/2602.12162", "authors": ["Muhammad bin Javaid", "Hasham Hussain", "Ashima Khanna", "Berke Kisin", "Jonathan Pirnay", "Alexander Mitsos", "Dominik G. Grimm", "Martin Grohe"], "title": "Amortized Molecular Optimization via Group Relative Policy Optimization", "comment": "23 pages, 5 figures", "summary": "Molecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as \"Instance Optimizers'', expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5b50\u8bbe\u8ba1\u65b9\u6cd5GRXForm\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684Graph Transformer\u6a21\u578b\u548cGroup Relative Policy Optimization\uff08GRPO\uff09\u6280\u672f\u4f18\u5316\u5206\u5b50\u7ed3\u6784\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u8d77\u59cb\u7ed3\u6784\u65f6\u9047\u5230\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u65e0\u9700\u63a8\u7406\u65f6\u95f4\u7684oracle\u8c03\u7528\u6216\u7cbe\u70bc\u8fc7\u7a0b\u5c31\u80fd\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u53d6\u5f97\u4e0e\u9886\u5148\u5b9e\u4f8b\u4f18\u5316\u5668\u76f8\u7ade\u4e89\u7684\u6210\u7ee9\u3002", "motivation": "\u5f53\u524d\u5206\u5b50\u8bbe\u8ba1\u9886\u57df\u7684\u65b9\u6cd5\u5728\u9762\u5bf9\u4e0d\u540c\u7684\u8d77\u59cb\u7ed3\u6784\u65f6\u8868\u73b0\u51fa\u8f83\u5927\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u5404\u81ea\u8d77\u59cb\u7ed3\u6784\u5f02\u8d28\u6027\u7684\u96be\u5ea6\u5bfc\u81f4\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u4e14\u63d0\u9ad8\u5bf9\u672a\u89c1\u8fc7\u7ed3\u6784\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRXForm\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684Graph Transformer\u6a21\u578b\u5de5\u4f5c\uff0c\u901a\u8fc7\u9010\u6b65\u6dfb\u52a0\u539f\u5b50\u548c\u952e\u6765\u4f18\u5316\u5206\u5b50\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86Group Relative Policy Optimization (GRPO) \u6280\u672f\u7528\u4e8e\u76ee\u6807\u5bfc\u5411\u7684\u5fae\u8c03\uff0c\u901a\u8fc7\u76f8\u5bf9\u4e8e\u8d77\u59cb\u7ed3\u6784\u6807\u51c6\u5316\u5956\u52b1\u6765\u51cf\u5c11\u65b9\u5dee\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cGRXForm\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u4e8e\u63a8\u7406\u65f6\u7684oracle\u8c03\u7528\u6216\u989d\u5916\u7cbe\u70bc\u6b65\u9aa4\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u5206\u5e03\u5916\u7684\u5206\u5b50\u9aa8\u67b6\u8fdb\u884c\u6709\u6548\u6cdb\u5316\uff0c\u5e76\u5728\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u4e0e\u9876\u7ea7\u5b9e\u4f8b\u4f18\u5316\u5668\u76f8\u5f53\u7684\u8868\u73b0\u3002", "conclusion": "GRXForm\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u9014\u5f84\u6765\u89e3\u51b3\u5206\u5b50\u8bbe\u8ba1\u4e2d\u7684\u9ad8\u65b9\u5dee\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u79cd\u8d77\u59cb\u7ed3\u6784\u4e0b\u4fdd\u6301\u826f\u597d\u6027\u80fd\u7684\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.12180", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.12180", "abs": "https://arxiv.org/abs/2602.12180", "authors": ["Yurong Chen", "Yu He", "Michael I. Jordan", "Fan Yao"], "title": "How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics", "comment": null, "summary": "Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u504f\u597d\u5bf9\u9f50\u6846\u67b6\u4e2d\u91c7\u6837\u548c\u53c2\u8003\u7b56\u7565\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u9002\u5f53\u7684\u5b9e\u4f8b\u4f9d\u8d56\u6027\u91c7\u6837\u53ef\u4ee5\u63d0\u4f9b\u66f4\u5f3a\u7684\u6392\u5e8f\u4fdd\u8bc1\uff0c\u5e76\u4e14\u5206\u6790\u4e86\u8fed\u4ee3\u5bf9\u9f50\u52a8\u6001\u53ef\u80fd\u4ea7\u751f\u7684\u6301\u7eed\u632f\u8361\u6216\u71b5\u5d29\u6e83\u73b0\u8c61\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u7684\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u4f46\u5173\u4e8e\u91c7\u6837\u548c\u53c2\u8003\u7b56\u7565\u9009\u62e9\u5bf9\u5176\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5728\u7406\u8bba\u4e0a\u4ecd\u4e0d\u6e05\u6670\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u4f7f\u7528\u7684\u504f\u597d\u5bf9\u9f50\u6846\u67b6\u2014\u2014\u8eab\u4efd\u504f\u597d\u4f18\u5316\uff08Identity Preference Optimization\uff09\uff0c\u7814\u7a76\u4e86\u91c7\u6837\u548c\u53c2\u8003\u7b56\u7565\u9009\u62e9\u7684\u6548\u679c\u3002\u8fd8\u5206\u6790\u4e86\u4e00\u4e2a\u8fed\u4ee3\u5bf9\u9f50\u8fc7\u7a0b\uff0c\u5176\u4e2d\u5b66\u5230\u7684\u7b56\u7565\u4f1a\u53cd\u9988\u5230\u672a\u6765\u7684\u91c7\u6837\u548c\u53c2\u8003\u7b56\u7565\u4e2d\u3002", "result": "\u8bc1\u660e\u4e86\u9002\u5f53\u7684\u5b9e\u4f8b\u4f9d\u8d56\u6027\u91c7\u6837\u80fd\u591f\u63d0\u4f9b\u66f4\u5f3a\u7684\u6392\u5e8f\u4fdd\u8bc1\uff1b\u800c\u6709\u504f\u7684\u5728\u7ebf\u7b56\u7565\u91c7\u6837\u5728\u7ed3\u6784\u5316\u504f\u597d\u4e0b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u96c6\u4e2d\u3002\u5bf9\u4e8e\u67d0\u4e9b\u53c2\u6570\u9009\u62e9\uff0c\u6240\u7814\u7a76\u7684\u52a8\u529b\u5b66\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u8868\u73b0\u51fa\u6301\u7eed\u632f\u8361\u6216\u71b5\u5d29\u6e83\u7684\u73b0\u8c61\uff0c\u5e76\u4e14\u786e\u5b9a\u4e86\u80fd\u786e\u4fdd\u7a33\u5b9a\u6027\u7684\u6761\u4ef6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u7684\u5173\u952e\u56e0\u7d20\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e5f\u9002\u7528\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b49\u66f4\u5e7f\u6cdb\u7684\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u4e86\u4e0a\u8ff0\u7406\u8bba\u5206\u6790\u3002"}}
{"id": "2602.12189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12189", "abs": "https://arxiv.org/abs/2602.12189", "authors": ["Habib Irani", "Bikram De", "Vangelis Metsis"], "title": "WaveFormer: Wavelet Embedding Transformer for Biomedical Signals", "comment": null, "summary": "Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWaveFormer\u7684\u65b0\u578b\u53d8\u538b\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u5d4c\u5165\u6784\u5efa\u548c\u4f4d\u7f6e\u7f16\u7801\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\u96c6\u6210\u5c0f\u6ce2\u5206\u89e3\u6765\u6539\u5584\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u957f\u5e8f\u5217\u3001\u590d\u6742\u65f6\u95f4\u52a8\u6001\u548c\u591a\u5c3a\u5ea6\u9891\u7387\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u516b\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7ade\u4e89\u529b\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u5206\u7c7b\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u957f\u5e8f\u5217\u3001\u590d\u6742\u7684\u65f6\u95f4\u52a8\u6001\u4ee5\u53ca\u591a\u5c3a\u5ea6\u9891\u7387\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u7279\u70b9\u4f7f\u5f97\u6807\u51c6\u7684Transformer\u67b6\u6784\u96be\u4ee5\u6709\u6548\u6355\u6349\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u6765\u66f4\u597d\u5730\u5904\u7406\u8fd9\u7c7b\u4fe1\u53f7\u3002", "method": "WaveFormer\u662f\u4e00\u79cd\u6539\u8fdb\u7684Transformer\u67b6\u6784\uff0c\u5728\u5d4c\u5165\u6784\u9020\u548c\u4f4d\u7f6e\u7f16\u7801\u4e24\u4e2a\u91cd\u8981\u73af\u8282\u878d\u5165\u4e86\u5c0f\u6ce2\u5206\u89e3\u6280\u672f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u591a\u901a\u9053\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09\u63d0\u53d6\u9891\u7387\u7279\u5f81\u5f62\u6210\u5305\u542b\u65f6\u57df\u4e0e\u9891\u57df\u4fe1\u606f\u7684\u4ee4\u724c\uff1b\u540c\u65f6\u91c7\u7528\u5355\u901a\u9053DWT\u5206\u6790\u9002\u5e94\u4fe1\u53f7\u7279\u5b9a\u65f6\u95f4\u7ed3\u6784\u7684\u4f4d\u7f6e\u7f16\u7801\u3002", "result": "\u901a\u8fc7\u5bf9\u6db5\u76d6\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u53ca\u8111\u4fe1\u53f7\u5206\u6790\u7684\u516b\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u4e2d\u5e8f\u5217\u957f\u5ea6\u4ece50\u52303000\u4e2a\u65f6\u95f4\u6b65\u4e0d\u7b49\uff0c\u4fe1\u9053\u6570\u91cf\u4ece1\u81f3144\u53d8\u5316\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eWaveFormer\u901a\u8fc7\u5168\u9762\u7684\u9891\u7387\u611f\u77e5\u5904\u7406\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u8868\u73b0\u3002", "conclusion": "WaveFormer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5c06\u9891\u57df\u77e5\u8bc6\u6574\u5408\u8fdb\u57fa\u4e8eTransformer\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u7684\u539f\u5219\u6027\u6846\u67b6\uff0c\u4e3a\u5904\u7406\u5177\u6709\u590d\u6742\u65f6\u95f4\u52a8\u6001\u548c\u591a\u5c3a\u5ea6\u9891\u7387\u7279\u6027\u7684\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.12204", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12204", "abs": "https://arxiv.org/abs/2602.12204", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction", "comment": null, "summary": "Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \\emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \\textbf{88\\%} of attention operations retrieve information already predictable from the model's hidden state, and this redundancy does \\emph{not} decrease during training. Motivated by this observation, we introduce \\textbf{\\ours{}} (\\textbf{C}onsolidation-based \\textbf{R}outing for \\textbf{A}daptive \\textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \\ours{} exhibits \\emph{decreasing attention utilization} over training, achieving a \\textbf{37.8$\\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \\emph{impossible} without consolidation: any static routing scheme requires $\u03a9(f \\cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \\ours{} achieves \\textbf{100\\% retrieval accuracy} at 1.6\\% attention compute (vs.\\ 68\\% for baselines), and consolidated patterns transfer to unseen tasks with \\textbf{48--52\\%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($\u03b3= 0.43$ vs.\\ $\u03b3_{\\text{human}} \\approx 0.4$--$0.5$). Code and benchmarks are available at [anonymized].", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-2\u6a21\u578b\u4e2d88%\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u662f\u591a\u4f59\u7684\uff0c\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86CRAM\u673a\u5236\uff0c\u8be5\u673a\u5236\u80fd\u591f\u51cf\u5c11\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u6ce8\u610f\u529b\u7684\u9700\u6c42\uff0c\u5e76\u5728SRCD\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u4eba\u5458\u89c2\u5bdf\u5230\u73b0\u6709\u6df7\u5408\u67b6\u6784\u4e2d\u7684\u6ce8\u610f\u529b\u4f7f\u7528\u6a21\u5f0f\u8981\u4e48\u5747\u5300\u5e94\u7528\u8981\u4e48\u5f62\u6210\u9759\u6001\u7a00\u758f\u6a21\u5f0f\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u90fd\u5ffd\u89c6\u4e86\u968f\u7740\u91cd\u590d\u6a21\u5f0f\u53d8\u5f97\u719f\u6089\uff0c\u5bf9\u6ce8\u610f\u529b\u7684\u9700\u6c42\u5e94\u8be5\u968f\u65f6\u95f4\u51cf\u5c11\u8fd9\u4e00\u70b9\u3002\u901a\u8fc7\u5206\u6790GPT-2\u6a21\u578b\uff0c\u4ed6\u4eec\u8fdb\u4e00\u6b65\u53d1\u73b0\u5927\u91cf\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u5b9e\u9645\u4e0a\u662f\u5728\u5904\u7406\u5df2\u7ecf\u53ef\u4ee5\u4ece\u6a21\u578b\u9690\u85cf\u72b6\u6001\u9884\u6d4b\u51fa\u6765\u7684\u4fe1\u606f\uff0c\u8fd9\u8868\u660e\u5b58\u5728\u6539\u8fdb\u7684\u7a7a\u95f4\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aCRAM\uff08\u57fa\u4e8e\u6574\u5408\u7684\u81ea\u9002\u5e94\u8bb0\u5fc6\u8def\u7531\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u751f\u7269\u7684\u8bb0\u5fc6\u5de9\u56fa\u8fc7\u7a0b\u3002CRAM\u65e8\u5728\u9010\u6e10\u5c06\u60c5\u666f\u68c0\u7d22\u63d0\u70bc\u4e3a\u53c2\u6570\u5316\u7684\u8bed\u4e49\u8bb0\u5fc6\uff0c\u4ece\u800c\u968f\u7740\u65f6\u95f4\u63a8\u79fb\u51cf\u5c11\u5bf9\u5916\u90e8\u6ce8\u610f\u529b\u8d44\u6e90\u7684\u9700\u6c42\u3002\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cCRAM\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u6ce8\u610f\u529b\u4f7f\u7528\u7684\u9010\u6b65\u4e0b\u964d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5927\u7ea63000\u6b65\u4e4b\u540e\uff0cCRAM\u80fd\u591f\u901a\u8fc7\u4e00\u4e2a\u660e\u663e\u7684\u9636\u6bb5\u8f6c\u53d8\u8fbe\u523037.8\u500d\u7684\u6ce8\u610f\u529b\u51cf\u5c11\u3002\u6b64\u5916\uff0c\u5728\u63d0\u51fa\u7684SRCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRAM\u4ec5\u4ee51.6%\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u91cf\u5c31\u8fbe\u5230\u4e86100%\u7684\u68c0\u7d22\u51c6\u786e\u7387\uff08\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u7ebf\u65b9\u6cd5\u9700\u898168%\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7ecf\u8fc7\u5b66\u4e60\u7684\u5de9\u56fa\u52a8\u6001\u4e0e\u4eba\u7c7b\u4ece\u60c5\u8282\u8bb0\u5fc6\u5230\u8bed\u4e49\u8bb0\u5fc6\u8f6c\u6362\u66f2\u7ebf\u5b9a\u91cf\u5339\u914d\u826f\u597d\u3002", "conclusion": "CRAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u964d\u4f4e\u6ce8\u610f\u529b\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u6027\u80fd\u7684\u65b0\u9014\u5f84\u3002\u5b83\u4e0d\u4ec5\u8bc1\u660e\u4e86\u81ea\u5df1\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u8fd8\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u672a\u7ecf\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5c06\u5df2\u5de9\u56fa\u7684\u6a21\u5f0f\u8f6c\u79fb\u5230\u65b0\u4efb\u52a1\u4e0a\u5e76\u51cf\u5c11\u7ea648-52%\u7684\u6ce8\u610f\u529b\u9700\u6c42\u3002"}}
{"id": "2602.12218", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12218", "abs": "https://arxiv.org/abs/2602.12218", "authors": ["Christian Intern\u00f2", "Jumpei Yamaguchi", "Loren Amdahl-Culleton", "Markus Olhofer", "David Klindt", "Barbara Hammer"], "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics", "comment": null, "summary": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $\u03c1> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($\u03c1\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u7684\u8bc4\u4f30\u534f\u8baePhyIP\uff0c\u7528\u4e8e\u6d4b\u8bd5\u7269\u7406\u91cf\u662f\u5426\u53ef\u4ee5\u4ece\u51bb\u7ed3\u7684\u8868\u793a\u4e2d\u7ebf\u6027\u89e3\u7801\u3002\u7814\u7a76\u53d1\u73b0\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u8f68\u9053\u529b\u5b66\u4e2d\uff0c\u5f53\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u8fbe\u5230\u4f4e\u9519\u8bef\u7387\u65f6\uff0c\u6f5c\u5728\u7ed3\u6784\u53d8\u5f97\u7ebf\u6027\u53ef\u8bbf\u95ee\u3002\u4e0e\u57fa\u4e8e\u9002\u5e94\u7684\u8bc4\u4f30\u76f8\u6bd4\uff0cPhyIP\u5728OOD\u6d4b\u8bd5\u4e2d\u80fd\u66f4\u597d\u5730\u6062\u590d\u5185\u90e8\u80fd\u91cf\u548c\u725b\u987f\u53cd\u5e73\u65b9\u6bd4\u4f8b\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u6a21\u578b\u662f\u5426\u5c06\u7269\u7406\u5b9a\u5f8b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u5185\u5316\uff0c\u800c\u4e0d\u662f\u5229\u7528\u7edf\u8ba1\u6377\u5f84\uff0c\u8fd9\u4e00\u70b9\u4ecd\u7136\u96be\u4ee5\u786e\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002\u6807\u51c6\u8bc4\u4f30\u901a\u5e38\u901a\u8fc7\u4e0b\u6e38\u9002\u5e94\uff08\u4f8b\u5982\u5fae\u8c03\u6216\u9ad8\u5bb9\u91cf\u63a2\u9488\uff09\u6765\u6d4b\u8bd5\u6f5c\u5728\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u5e72\u9884\u53ef\u4ee5\u6539\u53d8\u6b63\u5728\u6d4b\u91cf\u7684\u8868\u793a\uff0c\u4ece\u800c\u6df7\u6dc6\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u671f\u95f4\u5b66\u5230\u7684\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aPhyIP\u7684\u975e\u4fb5\u5165\u6027\u8bc4\u4f30\u534f\u8bae\uff0c\u8be5\u65b9\u6cd5\u6d4b\u8bd5\u7269\u7406\u91cf\u662f\u5426\u80fd\u591f\u4ece\u56fa\u5b9a\u7684\u8868\u793a\u4e2d\u88ab\u7ebf\u6027\u89e3\u7801\uff0c\u5e76\u4e14\u53d7\u5230\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u7684\u542f\u53d1\u3002", "result": "\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u8f68\u9053\u529b\u5b66\u9886\u57df\uff0c\u5f53\u81ea\u76d1\u7763\u5b66\u4e60\u8fbe\u5230\u8f83\u4f4e\u8bef\u5dee\u65f6\uff0c\u6f5c\u53d8\u91cf\u7ed3\u6784\u53d8\u5f97\u53ef\u901a\u8fc7\u7ebf\u6027\u65b9\u5f0f\u83b7\u53d6\u3002PhyIP\u80fd\u591f\u5728OOD\u6d4b\u8bd5\u4e2d\u6062\u590d\u51fa\u5185\u90e8\u80fd\u91cf\u548c\u725b\u987f\u9006\u5e73\u65b9\u5f8b\u5173\u7cfb\uff08\u5982\u03c1>0.90\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u4e8e\u8c03\u6574\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u7834\u574f\u8fd9\u79cd\u7ed3\u6784\uff08\u5982\u03c1\u22480.05\uff09\u3002", "conclusion": "\u57fa\u4e8e\u9002\u5e94\u7684\u8bc4\u4f30\u53ef\u80fd\u4f1a\u63a9\u76d6\u6f5c\u5728\u7ed3\u6784\uff0c\u800c\u4f4e\u5bb9\u91cf\u63a2\u9488\u5219\u63d0\u4f9b\u4e86\u5bf9\u7269\u7406\u4e16\u754c\u6a21\u578b\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u3002"}}
{"id": "2602.12222", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12222", "abs": "https://arxiv.org/abs/2602.12222", "authors": ["Miaosen Zhang", "Yishan Liu", "Shuxia Lin", "Xu Yang", "Qi Dai", "Chong Luo", "Weihao Jiang", "Peng Hou", "Anxiang Zeng", "Xin Geng", "Baining Guo"], "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training", "comment": null, "summary": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5206\u5e03\u5224\u522b\u7406\u8bba\uff08DDT\uff09\u548c\u4e24\u79cd\u4e92\u8865\u6280\u672f\u2014\u2014\u5206\u5e03\u5185\u5fae\u8c03\uff08IDFT\uff09\u4e0e\u63d0\u793a\u89e3\u7801\uff0c\u6765\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u4e0d\u5982\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u4fdd\u6301\u4e86SFT\u7684\u6548\u7387\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u79bb\u7ebfRL\u7b97\u6cd5\u76f8\u5f53\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u901a\u5e38\u4e0d\u5982\u5f3a\u5316\u5b66\u4e60\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u65b0\u578b\u6846\u67b6\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f7f\u5f97\u5373\u4f7f\u5728\u65e0\u6cd5\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u83b7\u5f97\u826f\u597d\u7684\u6cdb\u5316\u8868\u73b0\u3002", "method": "\u9996\u5148\u63d0\u51fa\u4e86\u5206\u5e03\u5224\u522b\u7406\u8bba(DDT)\uff0c\u7528\u4e8e\u89e3\u91ca\u5e76\u91cf\u5316\u6570\u636e\u4e0e\u6a21\u578b\u8bf1\u5bfc\u5206\u5e03\u4e4b\u95f4\u7684\u5bf9\u9f50\u60c5\u51b5\uff1b\u57fa\u4e8eDDT\uff0c\u8fdb\u4e00\u6b65\u53d1\u5c55\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u6280\u672f\uff1a\u4e00\u79cd\u662f\u5728\u635f\u5931\u5c42\u9762\u4e0a\u589e\u5f3aSFT\u6cdb\u5316\u80fd\u529b\u7684\u5206\u5e03\u5185\u5fae\u8c03(IDFT)\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u80fd\u5728\u6570\u636e\u5c42\u9762\u4e0a\u91cd\u65b0\u8c03\u6574\u8bad\u7ec3\u8bed\u6599\u5e93\u4ee5\u5339\u914d\u6a21\u578b\u5206\u5e03\u7684\u63d0\u793a\u89e3\u7801\u6280\u672f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u80fd\u591f\u8fbe\u5230\u4e0e\u77e5\u540d\u79bb\u7ebfRL\u7b97\u6cd5\uff08\u5982DPO\u548cSimPO\uff09\u76f8\u5ab2\u7f8e\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8fd8\u4fdd\u7559\u4e86SFT\u6d41\u7a0b\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u90a3\u4e9b\u4e0d\u9002\u5408\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u9009\u62e9\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u63d0\u5347\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.12233", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12233", "abs": "https://arxiv.org/abs/2602.12233", "authors": ["Daan Roos", "Oscar Davis", "Floor Eijkelboom", "Michael Bronstein", "Max Welling", "\u0130smail \u0130lkan Ceylan", "Luca Ambrogioni", "Jan-Willem van de Meent"], "title": "Categorical Flow Maps", "comment": null, "summary": "We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCategorical Flow Maps\u7684\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u52a0\u901f\u7c7b\u522b\u6570\u636e\u7684\u5c11\u91cf\u6b65\u9aa4\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6700\u8fd1\u7684\u6d41\u5339\u914d\u53d8\u5206\u516c\u5f0f\u548c\u6269\u6563\u53ca\u6d41\u6a21\u578b\u4e2d\u52a0\u901f\u63a8\u7406\u7684\u8d8b\u52bf\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u671d\u5411\u5355\u7eaf\u5f62\u7684\u6d41\u6620\u5c04\uff0c\u4ece\u800c\u81ea\u7136\u5730\u7ea6\u675f\u4e86\u6a21\u578b\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u8f68\u8ff9\u662f\u8fde\u7eed\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528\u73b0\u6709\u7684\u84b8\u998f\u6280\u672f\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e14\u5728\u6d4b\u8bd5\u65f6\u53ef\u4ee5\u76f4\u63a5\u91cd\u7528\u73b0\u6709\u7684\u6307\u5bfc\u548c\u91cd\u65b0\u52a0\u6743\u6280\u672f\u6765\u5f15\u5bfc\u91c7\u6837\u8fbe\u5230\u4e0b\u6e38\u76ee\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u56fe\u50cf\u3001\u5206\u5b50\u56fe\u548c\u6587\u672c\u4e0a\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u5c11\u91cf\u6b65\u9aa4\u751f\u6210\u6548\u679c\uff0c\u5373\u4f7f\u662f\u5728\u5355\u6b65\u751f\u6210\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u52a0\u901f\u7c7b\u522b\u6570\u636e\u7684\u5c11\u91cf\u6b65\u9aa4\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Categorical Flow Maps\u65b9\u6cd5\u3002\u6b64\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5f53\u5e94\u7528\u5230\u5982\u56fe\u50cf\u3001\u5206\u5b50\u56fe\u4ee5\u53ca\u6587\u672c\u7b49\u590d\u6742\u6570\u636e\u7c7b\u578b\u65f6\u3002", "method": "Categorical Flow Maps\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e86\u6700\u8fd1\u63d0\u51fa\u7684\u6d41\u5339\u914d\u53d8\u5206\u516c\u5f0f\uff0c\u5e76\u7ed3\u5408\u4e86\u5355\u7eaf\u5f62\u4f5c\u4e3a\u76ee\u6807\u7a7a\u95f4\u4ee5\u786e\u4fdd\u6982\u7387\u8d28\u91cf\u88ab\u6b63\u786e\u8f6c\u79fb\u81f3\u9884\u6d4b\u7ec8\u70b9\u3002\u6b64\u5916\uff0c\u501f\u52a9\u4e8e\u8fde\u7eed\u800c\u975e\u79bb\u6563\u7684\u8f68\u8ff9\u8868\u793a\uff0c\u4f7f\u5f97\u8be5\u65b9\u6cd5\u80fd\u591f\u517c\u5bb9\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u548c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7ec8\u70b9\u4e00\u81f4\u6027\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCategorical Flow Maps\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u7c7b\u522b\u6570\u636e\uff08\u5305\u62ec\u56fe\u50cf\u3001\u5206\u5b50\u56fe\u548c\u6587\u672c\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u5c11\u91cf\u6b65\u9aa4\u751f\u6210\u6027\u80fd\uff0c\u5c24\u5176\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5355\u6b65\u751f\u6210\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u80fd\u529b\u3002", "conclusion": "Categorical Flow Maps\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u624b\u6bb5\u6765\u52a0\u901f\u7c7b\u522b\u6570\u636e\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u751f\u6210\u8d28\u91cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\u96c6\uff0c\u800c\u4e14\u8fd8\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u4e0e\u73b0\u6709\u7684\u6307\u5bfc\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u4e3a\u5b9e\u73b0\u7279\u5b9a\u76ee\u7684\u800c\u8c03\u6574\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2602.12237", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12237", "abs": "https://arxiv.org/abs/2602.12237", "authors": ["Mayee F. Chen", "Tyler Murray", "David Heineman", "Matt Jordan", "Hannaneh Hajishirzi", "Christopher R\u00e9", "Luca Soldaini", "Kyle Lo"], "title": "Olmix: A Framework for Data Mixing Throughout LM Development", "comment": null, "summary": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOlmix\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u6df7\u5408\u65b9\u6cd5\u5728\u5b9e\u9645\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e2d\u9047\u5230\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u5bf9\u5f00\u53d1\u6df7\u5408\u65b9\u6cd5\u7684\u914d\u7f6e\u7a7a\u95f4\u7406\u89e3\u4e0d\u8db3\uff1b\u4e8c\u662f\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5047\u8bbe\u9886\u57df\u96c6\u56fa\u5b9a\u4e0d\u53d8\uff0c\u800c\u5b9e\u9645\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9886\u57df\u96c6\u4f1a\u4e0d\u65ad\u53d8\u5316\u3002Olmix\u901a\u8fc7\u5168\u9762\u7684\u7ecf\u9a8c\u7814\u7a76\u786e\u5b9a\u4e86\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u53ef\u4ee5\u6784\u6210\u6709\u6548\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u6df7\u5408\u91cd\u7528\u673a\u5236\u6765\u9ad8\u6548\u5730\u5904\u7406\u9886\u57df\u96c6\u66f4\u65b0\u540e\u6df7\u5408\u6bd4\u4f8b\u7684\u91cd\u65b0\u8ba1\u7b97\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6a21\u62df\u771f\u5b9e\u4e16\u754cLM\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u4e94\u6b21\u9886\u57df\u96c6\u66f4\u65b0\u5e8f\u5217\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5b8c\u5168\u91cd\u65b0\u8ba1\u7b97\u8282\u7701\u4e8674%\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e76\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u63d0\u9ad8\u4e8611.6%\u3002", "motivation": "\u5f53\u524d\u7684\u6570\u636e\u6df7\u5408\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u5b9e\u9645\u7684\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u4e8e\u5f00\u53d1\u6df7\u5408\u65b9\u6cd5\u6240\u9700\u8003\u8651\u7684\u8bbe\u8ba1\u9009\u9879\u7f3a\u4e4f\u5145\u5206\u7684\u7406\u89e3\u6216\u5171\u8bc6\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u56fa\u5b9a\u7684\u9886\u57df\u96c6\u5408\u6765\u8fdb\u884c\uff0c\u8fd9\u4e0e\u73b0\u5b9e\u4e2d\u9886\u57df\u96c6\u5408\u968f\u65f6\u95f4\u53d8\u5316\u7684\u60c5\u51b5\u4e0d\u7b26\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u6570\u636e\u6df7\u5408\u6548\u7387\uff0c\u63d0\u51fa\u4e86Olmix\u6846\u67b6\u3002", "method": "Olmix\u6846\u67b6\u901a\u8fc7\u5bf9\u4e0d\u540c\u8bbe\u8ba1\u65b9\u6848\u8fdb\u884c\u5168\u9762\u7684\u7ecf\u9a8c\u6027\u7814\u7a76\u6765\u63a2\u7d22\u6709\u6548\u6df7\u5408\u65b9\u6cd5\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u5f15\u5165\u4e86\u2018\u6df7\u5408\u91cd\u7528\u2019\u673a\u5236\u3002\u8fd9\u79cd\u673a\u5236\u5141\u8bb8\u5728\u9886\u57df\u96c6\u53d1\u751f\u53d8\u5316\u65f6\u4ec5\u91cd\u65b0\u8ba1\u7b97\u53d7\u5f71\u54cd\u9886\u57df\u7684\u6bd4\u7387\uff0c\u800c\u4e0d\u662f\u6bcf\u6b21\u90fd\u4ece\u5934\u5f00\u59cb\u6574\u4e2a\u8fc7\u7a0b\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u7684\u8ba1\u7b97\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6a21\u62df\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u8fde\u7eed\u4e94\u6b21\u9886\u57df\u96c6\u53d8\u66f4\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528Olmix\u6846\u67b6\u7684\u6df7\u5408\u91cd\u7528\u6280\u672f\u80fd\u591f\u8fbe\u5230\u4e0e\u6bcf\u6b21\u5b8c\u6574\u91cd\u65b0\u8ba1\u7b97\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e8674%\u3002\u6b64\u5916\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u4e0a\u4e5f\u89c2\u5bdf\u5230\u4e8611.6%\u7684\u6539\u8fdb\u3002", "conclusion": "Olmix\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u52a0\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u9645\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u6570\u636e\u6df7\u5408\u6311\u6218\u3002\u5b83\u4e0d\u4ec5\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f18\u5316\u6df7\u5408\u65b9\u6cd5\u7684\u8bbe\u8ba1\uff0c\u800c\u4e14\u901a\u8fc7\u5176\u521b\u65b0\u6027\u7684\u6df7\u5408\u91cd\u7528\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5927\u5927\u51cf\u5c11\u4e86\u8ba1\u7b97\u9700\u6c42\u3002"}}
{"id": "2602.12245", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12245", "abs": "https://arxiv.org/abs/2602.12245", "authors": ["Anthony Kobanda", "Waris Radji"], "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces", "comment": null, "summary": "Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPAs\uff09\u4e0e\u51c6\u5ea6\u91cf\u5f3a\u5316\u5b66\u4e60\uff08QRL\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u901a\u8fc7\u5185\u5728\u80fd\u91cf\u51fd\u6570\u7684\u6982\u5ff5\uff0c\u8868\u660e\u5728\u76ee\u6807\u5230\u8fbe\u63a7\u5236\u4e2d\uff0c\u6700\u4f18\u6210\u672c\u5230\u51fd\u6570\u5177\u6709\u5185\u5728\u5f62\u5f0f\uff1b\u53cd\u4e4b\uff0c\u8bad\u7ec3\u7528\u4e8e\u5efa\u6a21\u5185\u5728\u80fd\u91cf\u7684JEPAs\u5c5e\u4e8eQRL\u6240\u9488\u5bf9\u7684\u51c6\u5ea6\u91cf\u4ef7\u503c\u7c7b\u522b\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u5bf9\u79f0\u6709\u9650\u80fd\u91cf\u5728\u5355\u5411\u53ef\u8fbe\u6027\u4e0a\u7684\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5f53\u65b9\u5411\u6027\u91cd\u8981\u65f6\u91c7\u7528\u4e0d\u5bf9\u79f0\uff08\u51c6\u5ea6\u91cf\uff09\u80fd\u91cf\u7684\u7406\u7531\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5efa\u7acb\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPAs\uff09\u548c\u51c6\u5ea6\u91cf\u5f3a\u5316\u5b66\u4e60\uff08QRL\uff09\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u63a2\u7d22\u5b83\u4eec\u5982\u4f55\u901a\u8fc7\u5185\u5728\u80fd\u91cf\u51fd\u6570\u76f8\u4e92\u5173\u8054\uff0c\u5e76\u7406\u89e3\u4e3a\u4f55\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u9700\u8981\u8003\u8651\u80fd\u91cf\u6216\u8ddd\u79bb\u5ea6\u91cf\u7684\u975e\u5bf9\u79f0\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9a\u4e49\u4e00\u7c7bJEPA\u80fd\u91cf\u51fd\u6570\u2014\u2014\u5185\u5728\uff08\u6700\u5c0f\u4f5c\u7528\uff09\u80fd\u91cf\uff0c\u4f5c\u4e3a\u4e24\u4e2a\u72b6\u6001\u4e4b\u95f4\u53ef\u63a5\u53d7\u8f68\u8ff9\u4e0a\u7d2f\u79ef\u5c40\u90e8\u52aa\u529b\u7684\u4e0b\u786e\u754c\uff0c\u6765\u8fde\u63a5JEPAs\u4e0eQRL\u7684\u89c2\u70b9\u3002\u57fa\u4e8e\u6e29\u548c\u7684\u95ed\u5408\u6027\u548c\u53ef\u52a0\u6027\u5047\u8bbe\uff0c\u8bc1\u660e\u4efb\u4f55\u5185\u5728\u80fd\u91cf\u90fd\u662f\u4e00\u4e2a\u51c6\u5ea6\u91cf\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u76ee\u6807\u8fbe\u5230\u63a7\u5236\u4e2d\uff0c\u6700\u4f73\u6210\u672c\u5230\u51fd\u6570\u6070\u597d\u91c7\u53d6\u4e86\u8fd9\u79cd\u5185\u5728\u5f62\u5f0f\u3002\u76f8\u53cd\u5730\uff0c\u4e3a\u5efa\u6a21\u5185\u5728\u80fd\u91cf\u800c\u8bad\u7ec3\u7684JEPAs\u843d\u5165\u4e86QRL\u6240\u7784\u51c6\u7684\u51c6\u5ea6\u91cf\u4ef7\u503c\u7c7b\u522b\u3002\u540c\u65f6\u6307\u51fa\u4e86\u5bf9\u79f0\u6709\u9650\u80fd\u91cf\u5bf9\u4e8e\u5355\u5411\u53ef\u8fbe\u6027\u7684\u7ed3\u6784\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u652f\u6301\u4e86\u5f53\u65b9\u5411\u6027\u8d77\u5173\u952e\u4f5c\u7528\u65f6\u91c7\u7528\u4e0d\u5bf9\u79f0\uff08\u51c6\u5ea6\u91cf\uff09\u80fd\u91cf\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u5f15\u5165\u5185\u5728\u80fd\u91cf\u7684\u6982\u5ff5\uff0c\u80fd\u591f\u6709\u6548\u94fe\u63a5JEPAs\u4e0eQRL\u9886\u57df\u5185\u7684\u7814\u7a76\u5de5\u4f5c\uff0c\u5f3a\u8c03\u4e86\u5728\u5904\u7406\u5177\u6709\u65b9\u5411\u6027\u7279\u5f81\u7684\u95ee\u9898\u65f6\u91c7\u7528\u51c6\u5ea6\u91cf\u800c\u975e\u5ea6\u91cf\u7684\u91cd\u8981\u6027\u3002"}}
