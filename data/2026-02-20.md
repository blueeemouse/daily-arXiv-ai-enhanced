<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 72]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [MMCAformer: Macro-Micro Cross-Attention Transformer for Traffic Speed Prediction with Microscopic Connected Vehicle Driving Behavior](https://arxiv.org/abs/2602.16730)
*Lei Han,Mohamed Abdel-Aty,Younggun Kim,Yang-Jun Joo,Zubayer Islam*

Main category: cs.LG

TL;DR: 本文提出了一种Macro-Micro Cross-Attention Transformer (MMCAformer) 模型，该模型结合了基于联网车辆数据的微观驾驶行为特征与宏观交通流特性来进行速度预测。实验结果表明，相比仅使用宏观特征的方法，引入微观驾驶行为特征不仅提高了预测精度（例如RMSE、MAE和MAPE分别降低了9.0%、6.9%和10.2%），还减少了模型预测的不确定性。


<details>
  <summary>Details</summary>
Motivation: 准确的速度预测对于主动交通管理至关重要，以提高交通效率和安全性。虽然现有的研究主要依赖于聚合的宏观交通流量数据来预测未来的交通趋势，但道路交通动态也受到个体微观人类驾驶行为的影响。最近的联网车辆(CV)数据提供了丰富的驾驶行为特征，为将这些行为洞察整合到速度预测中提供了新的机会。

Method: 提出了Macro-Micro Cross-Attention Transformer (MMCAformer)，利用自注意力机制学习宏观交通流动中的内在依赖关系，并通过交叉注意力捕捉宏观交通状态与微观驾驶行为之间的时空交互作用。此外，MMCAformer采用Student-t负对数似然损失进行优化，以提供点级速度预测并估计不确定性。

Result: 在佛罗里达州四条高速公路上的实验显示，相比于基线方法，所提出的MMCAformer表现出优越性能。与只使用宏观特征相比，引入微观驾驶行为特征不仅增强了预测准确性（例如整体RMSE、MAE和MAPE分别减少9.0%、6.9%和10.2%），而且缩小了模型预测的不确定性（例如平均预测区间在四条高速公路上减少了10.1-24.0%）。结果显示，急刹车和加速频率成为最具影响力的因素，在拥堵、低速交通条件下这种改进更为明显。

Conclusion: 通过结合微观驾驶行为特征与宏观交通流信息，MMCAformer能够更准确地预测速度，并且在交通拥堵时表现尤为突出。这表明，将驾驶员行为纳入考虑可以显著提升交通预测的质量。

Abstract: Accurate speed prediction is crucial for proactive traffic management to enhance traffic efficiency and safety. Existing studies have primarily relied on aggregated, macroscopic traffic flow data to predict future traffic trends, whereas road traffic dynamics are also influenced by individual, microscopic human driving behaviors. Recent Connected Vehicle (CV) data provide rich driving behavior features, offering new opportunities to incorporate these behavioral insights into speed prediction. To this end, we propose the Macro-Micro Cross-Attention Transformer (MMCAformer) to integrate CV data-based micro driving behavior features with macro traffic features for speed prediction. Specifically, MMCAformer employs self-attention to learn intrinsic dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. MMCAformer is optimized with a Student-t negative log-likelihood loss to provide point-wise speed prediction and estimate uncertainty. Experiments on four Florida freeways demonstrate the superior performance of the proposed MMCAformer compared to baselines. Compared with only using macro features, introducing micro driving behavior features not only enhances prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) but also shrinks model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across the four freeways). Results reveal that hard braking and acceleration frequencies emerge as the most influential features. Such improvements are more pronounced under congested, low-speed traffic conditions.

</details>


### [2] [A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets](https://arxiv.org/abs/2602.16735)
*Saud Alghumayjan,Ming Yi,Bolun Xu*

Main category: cs.LG

TL;DR: 本研究提出了一种基于大型语言模型（LLMs）的少量样本分类框架，用于预测次日实时电价是否会飙升。通过将系统状态信息整合为自然语言提示并输入到LLMs中，该方法在历史数据有限的情况下表现优于传统的监督学习模型，如支持向量机和XGBoost，显示出LLMs作为数据高效工具在分类电价飙升方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索一种新的方法来更有效地利用有限的数据集对电价飙升进行预测，特别是在数据稀缺的情况下。

Method: 采用的方法是构建一个基于大型语言模型（LLMs）的少量样本学习框架，它将电力需求、可再生能源发电、天气预报及近期电价等系统状态信息转换成一组统计特征，并以自然语言提示的形式与一般指令一起提供给LLM。

Result: 实验结果显示，在德克萨斯州电力市场的历史数据上，该少量样本学习方法的表现与支持向量机和XGBoost等监督机器学习模型相当；而且当可用的历史数据较少时，该方法的表现优于后两者。

Conclusion: 结论指出，大型语言模型作为一种数据高效的工具，在处理数据稀缺条件下对电价飙升进行分类方面具有巨大潜力。

Abstract: This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.

</details>


### [3] [Real-time Secondary Crash Likelihood Prediction Excluding Post Primary Crash Features](https://arxiv.org/abs/2602.16739)
*Lei Han,Mohamed Abdel-Aty,Zubayer Islam,Chenzhu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种混合次级碰撞可能性预测框架，该框架不依赖于碰撞后特征，而是基于实时交通流和环境特征进行预测。通过集成六种机器学习算法，并采用投票机制结合三个模型的输出，实验证明该方法在佛罗里达州高速公路上的表现优于以往研究，能够以较低的误报率准确识别91%的次级碰撞事件。


<details>
  <summary>Details</summary>
Motivation: 现有次级碰撞可能性预测方法主要依赖于碰撞类型、严重程度等碰撞后特征，这些信息在实际中难以实时获取，限制了其应用范围。为了克服这一局限性，本研究旨在开发一种无需依赖碰撞后特性的预测框架。

Method: 设计了一个动态时空窗口来从主碰撞位置及其上游路段提取实时交通流与环境特征；构建了包含一个主碰撞模型和两个用于评估不同比较场景下碰撞点及上游段交通状况的次级碰撞模型在内的框架；采用集成学习策略整合了六种机器学习算法，并通过投票机制综合三个模型的结果。

Result: 实验结果表明，在佛罗里达州高速公路的应用中，所提出的混合框架能够正确识别91%的次级碰撞事件，且误报率仅为0.20；ROC曲线下面积从单个模型的0.654、0.744、0.902提升到了混合模型下的0.952，显著优于之前的研究成果。

Conclusion: 提出的混合次级碰撞可能性预测框架能够在没有使用碰撞后特征的情况下实现对次级碰撞的有效预测，展示了比现有方法更高的准确性与更低的误报率，为交通管理部门提供了一种实用工具。

Abstract: Secondary crash likelihood prediction is a critical component of an active traffic management system to mitigate congestion and adverse impacts caused by secondary crashes. However, existing approaches mainly rely on post-crash features (e.g., crash type and severity) that are rarely available in real time, limiting their practical applicability. To address this limitation, we propose a hybrid secondary crash likelihood prediction framework that does not depend on post-crash features. A dynamic spatiotemporal window is designed to extract real-time traffic flow and environmental features from primary crash locations and their upstream segments. The framework includes three models: a primary crash model to estimate the likelihood of secondary crash occurrence, and two secondary crash models to evaluate traffic conditions at crash and upstream segments under different comparative scenarios. An ensemble learning strategy integrating six machine learning algorithms is developed to enhance predictive performance, and a voting-based mechanism combines the outputs of the three models. Experiments on Florida freeways demonstrate that the proposed hybrid framework correctly identifies 91% of secondary crashes with a low false alarm rate of 0.20. The Area Under the ROC Curve improves from 0.654, 0.744, and 0.902 for the individual models to 0.952 for the hybrid model, outperforming previous studies.

</details>


### [4] [DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning](https://arxiv.org/abs/2602.16742)
*Haoxiang Sun,Lizhen Xu,Bing Zhao,Wotao Yin,Wei Wang,Boyu Yang,Rui Wang,Hu Wei*

Main category: cs.LG

TL;DR: 本文介绍了一个名为DeepVision-103K的新数据集，用于增强大型多模态模型在视觉反思和推理方面的能力。该数据集涵盖了广泛的K12数学主题、丰富的知识点以及多样化的视觉元素，并且训练出的模型在多模态数学基准测试中表现出色，同时也能很好地泛化到一般的多模态推理任务上。


<details>
  <summary>Details</summary>
Motivation: 现有的用于强化学习与可验证奖励（RLVR）的数据集主要基于小规模的手工构建或先前资源的重组，这限制了数据的多样性与覆盖范围，进而制约了模型性能的进一步提升。为了解决这一问题，作者们提出了一个新的综合性数据集DeepVision-103K。

Method: 通过创建一个包含多样化K12数学话题、广泛的知识点及丰富视觉元素的大规模数据集DeepVision-103K来解决现有数据集多样性不足的问题。

Result: 使用DeepVision-103K训练的模型不仅在多模态数学基准测试中表现优异，而且能够很好地推广至更广泛的多模态推理任务中。此外，进一步分析显示这些模型在视觉感知、反思与推理能力上有所提高。

Conclusion: DeepVision-103K作为一个专为RLVR训练设计的全面性数据集，有效地促进了大型多模态模型在视觉反思与推理能力上的进步。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: \href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.

</details>


### [5] [PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency](https://arxiv.org/abs/2602.16745)
*Zhangyi Liu,Huaizhi Qu,Xiaowei Yin,He Sun,Yanjun Han,Tianlong Chen,Zhun Deng*

Main category: cs.LG

TL;DR: 提出了PETS方法，通过优化框架研究推理轨迹分配问题，旨在提高测试时的样本效率。在线下和线上环境中，PETS均能有效减少采样预算同时保持高一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间缩放技术虽然能够通过聚合随机推理路径来提升模型性能，但在有限预算条件下实现高效的自一致性仍是一个未解决的问题。

Method: 开发了PETS（原则性且高效的测试时间自一致性）方法，引入自我一致性比率作为新的度量标准，并通过将推理轨迹建模为工作者连接到众包领域，在线下环境使用基于多数投票的分配算法，在线流式环境中则提出了一种新颖的方法以适应问题难度并保持计算效率。

Result: 实验表明，PETS在GPQA上实现了完美的自一致性，并且与均匀分配相比，在离线设置中减少了高达75%、在线设置中减少了55%的采样预算。

Conclusion: PETS提供了一个理论基础强且计算上高效的方法来解决测试时的样本高效自一致性问题，无论是在预先知道所有问题的离线场景还是问题顺序到达的在线场景中都表现出了优越性。

Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.

</details>


### [6] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文通过几何分析研究了Transformer在模算术任务中训练时从记忆到泛化的延迟转变（即Grokking现象），发现主要在低维执行子空间内进行，并且在此子空间正交方向上的曲率增长是泛化发生前的重要标志。


<details>
  <summary>Details</summary>
Motivation: 为了解释小型算法任务中，模型从简单记忆转向真正理解（Grokking）这一延迟转变背后的机制。

Method: 采用PCA对注意力权重轨迹进行了分析；测量了梯度步之间非交换性的缺陷值，并将其投影到了学习得到的子空间上；进行了因果干预实验来验证沿着学习子空间移动对于Grokking的重要性以及增加曲率是否足够促使Grokking。

Result: 发现训练过程主要在一个低维执行子空间内进行，该子空间的一个主成分可以解释轨迹方差的68-83%；正交于执行子空间的方向上观察到曲率显著增长，而这种增长通常发生在泛化之前；仅沿着学习子空间移动是Grokking所必需的，但单独增加曲率并不足以引发Grokking。

Conclusion: Grokking反映了从一个特征为低维限制和横向曲率累积的亚稳态区域逃逸的过程。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [7] [LiveClin: A Live Clinical Benchmark without Leakage](https://arxiv.org/abs/2602.16747)
*Xidong Wang,Shuqi Guo,Yue Shen,Junying Chen,Jian Wang,Jinjie Gu,Ping Zhang,Lei Liu,Benyou Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为LiveClin的实时基准，旨在解决医疗大语言模型评估中的数据污染和知识过时问题。通过使用当代同行评审的病例报告构建，并每半年更新一次，确保了临床相关性和防止数据污染。经26个模型测试，最高准确率仅为35.7%，表明真实世界场景下的挑战性。


<details>
  <summary>Details</summary>
Motivation: 由于数据污染和知识过时问题，现有的静态基准在评估医疗大语言模型（LLM）时存在可靠性不足的问题。为了更贴近实际临床实践，需要一个能够反映最新医学进展且避免数据污染的新基准。

Method: 开发了一个名为LiveClin的实时基准，该基准基于最新的同行评审案例报告构建，并定期更新以保持信息的时效性；利用经过验证的人工智能-人类工作流程将真实的患者案例转化为复杂的多模态评估情景，涵盖整个临床路径；对26个不同模型进行了评估。

Result: LiveClin目前包含1,407份案例报告和6,605个问题；最佳表现模型的案例准确率为35.7%；与人类专家相比，主任医师取得了最高的准确性，随后是主治医师，两者都超过了大多数模型的表现。

Conclusion: LiveClin提供了一个持续发展、基于临床的框架，有助于指导医疗大语言模型的发展，使其更加可靠并具有实际应用价值。

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

</details>


### [8] [Attending to Routers Aids Indoor Wireless Localization](https://arxiv.org/abs/2602.16762)
*Ayush Roy,Tahsin Fuad Hassan,Roshan Ayyalasomayajula,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 本研究通过引入注意力机制到路由器上，改进了基于Wi-Fi信号的无线定位算法，使得每个路由器的信息在聚合时根据其重要性被赋予不同的权重，从而提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有大多数基于机器学习的使用Wi-Fi信号进行无线定位的方法没有适当考虑不同路由器信息的重要性，导致收敛效果不佳且准确性降低。受传统加权三角测量方法启发，本文旨在通过为各个路由器分配不同的权重来优化这一过程。

Method: 本文提出了一种新的方法——对路由器施加注意力，即在标准机器学习定位架构中加入注意力层，以强调每个路由器对于最终定位结果的相关性和贡献度。

Result: 实验表明，在开源数据集上的评估显示，与基准架构相比，该方法能够将定位精度提高超过30%。

Conclusion: 通过对路由器引入注意力机制，可以显著提升基于Wi-Fi信号的无线定位系统的性能，证明了该方法的有效性。

Abstract: Modern machine learning-based wireless localization using Wi-Fi signals continues to face significant challenges in achieving groundbreaking performance across diverse environments. A major limitation is that most existing algorithms do not appropriately weight the information from different routers during aggregation, resulting in suboptimal convergence and reduced accuracy. Motivated by traditional weighted triangulation methods, this paper introduces the concept of attention to routers, ensuring that each router's contribution is weighted differently when aggregating information from multiple routers for triangulation. We demonstrate, by incorporating attention layers into a standard machine learning localization architecture, that emphasizing the relevance of each router can substantially improve overall performance. We have also shown through evaluation over the open-sourced datasets and demonstrate that Attention to Routers outperforms the benchmark architecture by over 30% in accuracy.

</details>


### [9] [Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning](https://arxiv.org/abs/2602.17614)
*Obaidullah Zaland,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文提出了一种k-匿名差分隐私U形联邦分裂学习(KD-UFSL)方法，该方法利用微聚合和差分隐私等隐私增强技术来最小化从客户端传输到服务器的中间数据(即粉碎数据)的数据泄漏。实验表明，KD-UFSL在增加实际与重建图像之间的均方误差的同时，也降低了它们之间的结构相似性，并且提高了隐私性同时保持了全局模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 在大数据场景中，分散式训练虽然可以实现跨客户端的机器学习模型训练而无需集中数据，但给客户端设备带来了计算负担。U形联邦分裂学习（UFSL）能够将部分客户端计算卸载到服务器上，但客户向服务器共享的中间表示容易暴露客户的私有数据。为了减少通过中间数据表示暴露客户数据的风险，需要一种既能保护隐私又能维持模型效用的学习方法。

Method: 提出了k-匿名差分隐私U形联邦分裂学习(KD-UFSL)，结合使用微聚合和差分隐私技术以减少由粉碎数据转移至服务器时可能引发的数据泄露问题。

Result: 实验结果证明，KD-UFSL方法能够提高实际与重建图片间的平均平方误差高达50%，并降低两者之间结构相似度达40%。此外，在四个基准测试数据集上进行的实验还显示，该方法在改善隐私的同时保持了全球模型的有效性。

Conclusion: KD-UFSL为大规模大数据应用提供了一个平衡隐私与实用性的解决方案，适合于需要同时考虑隐私保护和模型性能的大规模应用场景。

Abstract: Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.

</details>


### [10] [Omitted Variable Bias in Language Models Under Distribution Shift](https://arxiv.org/abs/2602.16784)
*Victoria Lin,Louis-Philippe Morency,Eli Ben-Michael*

Main category: cs.LG

TL;DR: 本文探讨了语言模型在遇到训练数据分布之外的数据时的脆弱性，区分了可观察和不可观察的分布变化，并提出了一种框架来处理由于未观测变量导致的遗漏变量偏差问题。通过实验验证，该方法能够提供更合理的评估指标，提高模型在未知分布下的表现，并且允许对未观测变量强度进行推断。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型虽然在多种任务上表现出色，但在面对与训练数据分布不同的数据时仍然显得非常脆弱。现有应对分布变化的方法只解决了可以观察到的部分，而未能解决那些不可见的因素所带来的影响。这些未被注意到的变化可能导致评估和优化过程中的偏差。

Method: 作者提出了一种新的框架，用于将未观察到变量的影响强度映射到语言模型在分布变化下最差情况泛化性能的界限上。此外，在有目标分布标签的情况下，还可以利用此框架来推断未观察到变量的强度。

Result: 实验证明，直接使用这种方法提供的边界来进行语言模型的评估与优化，不仅能给出更加准确的超出分布性能度量，而且相比传统调整方法还能进一步提升真实环境下的超出分布性能。

Conclusion: 针对语言模型中由未观测变量引起的遗漏变量偏差问题，本文提出的框架为理解和改善模型在不同分布下的泛化能力提供了有效途径。

Abstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.

</details>


### [11] [Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning](https://arxiv.org/abs/2602.17625)
*Obaidullah Zaland,Zulfiqar Ahmad Khan,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文提出了一种名为One-Shot Incremental Federated Learning (OSI-FL)的框架，它通过单轮通信中传递类别特定嵌入来应对通信开销和灾难性遗忘的双重挑战，并采用Selective Sample Retention (SSR)方法保留最具信息量的样本以减少遗忘。实验结果表明，OSI-FL在类增量和域增量场景下均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习（FL）假设数据流是静态的，并且在多轮次中学习协作模型，这使得在有限通信场景下处理增量数据变得困难。此外，随着新任务的到来，重新训练模型会导致灾难性遗忘问题。为了解决这些问题，提出了一个新的框架。

Method: 提出了One-Shot Incremental Federated Learning (OSI-FL)框架，该框架允许客户端通过单轮通信向服务器发送由冻结视觉语言模型生成的类别特定嵌入。服务器使用预训练扩散模型合成与客户端数据分布相似的新数据，并利用这些合成样本来训练模型。为了进一步减轻灾难性遗忘的问题，引入了Selective Sample Retention (SSR)机制，该机制根据样本损失识别并保留每个类别和任务对中最具有信息价值的顶部p个样本。

Result: 实验结果显示，在三个基准数据集上，无论是对于类增量还是域增量的情况，OSI-FL的表现都优于包括传统FL方法在内的基线方法。

Conclusion: OSI-FL成功地解决了联邦学习中通信成本高以及面对增量数据时容易发生灾难性遗忘的问题，为未来的研究提供了新的方向。

Abstract: Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.

</details>


### [12] [Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency](https://arxiv.org/abs/2602.16787)
*Victoria Lin,Xinnuo Xu,Rachel Lawrence,Risa Ueno,Amit Sharma,Javier Gonzalez,Niranjani Prasad*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的推理时间方法，即双重反事实一致性（DCC），用于衡量和指导大型语言模型（LLMs）的因果推理能力。DCC 不需要标记的反事实数据，能够验证模型执行因果干预和反事实预测的能力。研究展示了 DCC 在多种领先 LLMs 中提高推理任务性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在推理基准测试中表现出色，但当面对反事实问题时显得脆弱，这表明它们在因果推理方面存在弱点。虽然有研究表明带有标签的反事实任务可以作为评估 LLMs 因果推理能力的有效基准，但生成足以覆盖广泛潜在反事实空间的数据规模有限。因此，研究旨在开发一种不需要大量标注数据的方法来衡量和改进 LLMs 的因果推理能力。

Method: 引入了双重反事实一致性（DCC）这一概念，这是一种轻量级的、在推理阶段使用的方法，无需依赖标记的反事实数据即可测量并指导LLMs进行因果推理的能力。该方法侧重于验证模型是否能够有效地实施因果干预以及准确地做出反事实预测。

Result: 通过对一系列领先的LLMs进行不同类型的推理任务及干预措施下的测试，证实了DCC能有效评价这些模型的因果推理水平。此外，还证明了DCC可作为一种无需训练的测试时间拒绝采样标准，并且直接促进了多个模型家族在推理任务上的表现提升。

Conclusion: 本研究表明，通过采用DCC方法可以在没有额外训练或大量标注数据的情况下，有效增强大型语言模型处理因果关系相关问题的能力。

Abstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate the effectiveness of DCC as a training-free test-time rejection sampling criterion and show that it can directly improve performance on reasoning tasks across multiple model families.

</details>


### [13] [Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models](https://arxiv.org/abs/2602.16793)
*Xingyu Dang,Rohit Agarwal,Rodrigo Porto,Anirudh Goyal,Liam H Fowl,Sanjeev Arora*

Main category: cs.LG

TL;DR: 本研究提出了一种新的推理流程，该流程在国际数学奥林匹克竞赛（IMO）样式的数学问题上达到了同类最佳的表现，同时大幅度降低了推理成本。通过解决评估者在解题-评分管道中的认知陷阱问题，即迭代改进收敛到一个解题器和内部评分系统都认为基本正确的错误解决方案，此方法使用了猜想提取技术，从生成的解决方案中分离出候选引理，并在新的环境中独立验证它们及其否定。在IMO-ProofBench Advanced测试集上，使用Gemini 3.0 Pro模型实现了67.1%的成绩，每题平均成本约为31美元，这是公开模型中最好的表现之一，且比下一个最佳公开可访问管道的成功率高出一倍多。


<details>
  <summary>Details</summary>
Motivation: 过去一年里，定制和未发布的数学推理模型已经在国际数学奥林匹克竞赛（IMO）上达到了金牌水平的表现。尽管后来报告称利用大规模推理在公开可用模型上也能达到类似表现，但其成本极其高昂（例如，每题需花费3000美元）。因此，本研究旨在开发一种既高效又经济的新方法来解决IMO风格的数学问题。

Method: 本研究的方法基于对解题-评分管道中评估失败情况的认知井效应的理解，即迭代改进过程可能最终收敛至一个被解题器及管道内部评分机制误认为正确但实际上错误的答案。为克服这一挑战，研究人员提出了猜想提取策略：从已产生的解答中抽取候选引理，并将其与对应的否定一起，在一个全新的上下文中独立进行验证（即上下文脱离）。整个流程采用通用现成模型完成。

Result: 实验结果表明，在IMO-ProofBench Advanced数据集上应用Gemini 3.0 Pro模型时，所提出的管道能够达到67.1%的性能指标，平均每题处理费用大约为31美元。这不仅代表了当时PB-Adv基准测试中公开与非公开模型的最佳表现，而且相对于次优公开访问管道来说，成功比率翻了一番还多，同时成本仅为后者的一小部分。

Conclusion: 本研究展示了一种新颖高效的推理流程，能够在显著降低计算成本的同时，大幅提高解决IMO级别数学难题的能力。这标志着向更广泛地利用先进AI技术促进数学教育和个人学习迈出的重要一步。

Abstract: In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only general-purpose off-the-shelf models. Our method relies on insights about grader failure in solver-grader pipelines, which we call the Cognitive Well (iterative refinement converging to a wrong solution that the solver as well as the pipeline's internal grader consider to be basically correct). Our pipeline addresses these failure modes through conjecture extraction, wherein candidate lemmas are isolated from generated solutions and independently verified alongside their negations in a fresh environment (context detachment). On IMO-ProofBench Advanced (PB-Adv), our pipeline achieves 67.1 percent performance using Gemini 3.0 Pro with an average cost per question of approximately 31 USD. At the time of evaluation, this represented the state-of-the-art on PB-Adv among both public and unreleased models, and more than doubles the success rate of the next best publicly accessible pipeline, all at a fraction of the cost.

</details>


### [14] [Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning](https://arxiv.org/abs/2602.16796)
*Zifan Wang,Riccardo De Santi,Xiaoyu Mo,Michael M. Zavlanos,Andreas Krause,Karl H. Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件风险价值(CVaR)的尾部感知流微调算法(TFFT)，旨在优化预训练扩散和流模型在下游任务中的表现。通过关注高奖励尾部的新颖样本探索与低奖励尾部的最坏情况控制，TFFT提供了一种高效且具成本效益的方法来调整模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有的熵正则化方法主要关注于最大化预期回报，但缺乏对尾部行为进行调控的机制。然而，在实际应用中，无论是为了提高可靠性还是促进新发现，控制尾部分布都是非常重要的。因此，需要一种能够有效调整尾部分布的微调算法。

Method: 提出了Tail-aware Flow Fine-Tuning (TFFT)算法，该算法基于条件风险价值(CVaR)理论，通过将CVaR分解为两阶段过程——一维阈值优化步骤及单个通过特定伪奖励执行的熵正则化微调过程——来实现高效的尾部控制。这种方法避免了非线性优化的需求，并保持了与传统期望微调方法相当的计算成本。

Result: TFFT在说明性实验、高维度文本到图像生成以及分子设计等多个领域展示了其有效性。表明了该方法不仅能够有效地寻找高回报尾部的新颖样本，同时也能够很好地控制低回报尾部的情况，从而提高了整体模型性能。

Conclusion: Tail-aware Flow Fine-Tuning (TFFT)作为一种新颖且有效的分布微调算法，通过引入CVaR的概念实现了对模型尾部分布的有效控制，为改善预训练模型在不同应用场景下的表现提供了新的思路。

Abstract: Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a principled and efficient distributional fine-tuning algorithm based on the Conditional Value-at-Risk (CVaR). We address two distinct tail-shaping goals: right-CVaR for seeking novel samples in the high-reward tail and left-CVaR for controlling worst-case samples in the low-reward tail. Unlike prior approaches that rely on non-linear optimization, we leverage the variational dual formulation of CVaR to decompose it into a decoupled two-stage procedure: a lightweight one-dimensional threshold optimization step, and a single entropy-regularized fine-tuning process via a specific pseudo-reward. This decomposition achieves CVaR fine-tuning efficiently with computational cost comparable to standard expected fine-tuning methods. We demonstrate the effectiveness of TFFT across illustrative experiments, high-dimensional text-to-image generation, and molecular design.

</details>


### [15] [TopoFlow: Physics-guided Neural Networks for high-resolution air quality prediction](https://arxiv.org/abs/2602.16821)
*Ammar Kheder,Helmi Toropainen,Wenqing Peng,Samuel Antão,Jia Chen,Zhi-Song Liu,Michael Boy*

Main category: cs.LG

TL;DR: TopoFlow, a physics-guided neural network, incorporates topography and wind direction to predict air quality with high resolution. It uses topography-aware attention and wind-guided patch reordering, achieving a 71-80% improvement over current forecasting systems for PM2.5 prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy of air quality predictions by integrating physical processes, specifically the effects of topography and wind direction, into the learning model. This addresses the limitations of existing models in handling complex terrain and wind dynamics, which are crucial for pollutant transport and dispersion.

Method: TopoFlow employs a vision transformer architecture enhanced with two mechanisms: topography-aware attention that considers terrain-induced flow patterns, and wind-guided patch reordering that aligns spatial representations with the prevailing wind directions. The model was trained on six years of high-resolution reanalysis data from over 1,400 surface monitoring stations in China.

Result: TopoFlow achieved a PM2.5 RMSE of 9.71 ug/m3, marking a 71-80% improvement over operational forecasting systems and a 13% enhancement over state-of-the-art AI baselines. The model also reliably discriminates between clean and polluted conditions, maintaining forecast errors below China's 24-hour air quality threshold of 75 ug/m3 (GB 3095-2012).

Conclusion: The integration of physical knowledge regarding topography and wind direction into neural networks significantly advances air quality prediction, as demonstrated by TopoFlow's consistent performance improvements across major pollutants and various forecast lead times.

Abstract: We propose TopoFlow (Topography-aware pollutant Flow learning), a physics-guided neural network for efficient, high-resolution air quality prediction. To explicitly embed physical processes into the learning framework, we identify two critical factors governing pollutant dynamics: topography and wind direction. Complex terrain can channel, block, and trap pollutants, while wind acts as a primary driver of their transport and dispersion. Building on these insights, TopoFlow leverages a vision transformer architecture with two novel mechanisms: topography-aware attention, which explicitly models terrain-induced flow patterns, and wind-guided patch reordering, which aligns spatial representations with prevailing wind directions. Trained on six years of high-resolution reanalysis data assimilating observations from over 1,400 surface monitoring stations across China, TopoFlow achieves a PM2.5 RMSE of 9.71 ug/m3, representing a 71-80% improvement over operational forecasting systems and a 13% improvement over state-of-the-art AI baselines. Forecast errors remain well below China's 24-hour air quality threshold of 75 ug/m3 (GB 3095-2012), enabling reliable discrimination between clean and polluted conditions. These performance gains are consistent across all four major pollutants and forecast lead times from 12 to 96 hours, demonstrating that principled integration of physical knowledge into neural networks can fundamentally advance air quality prediction.

</details>


### [16] [HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind](https://arxiv.org/abs/2602.16826)
*Nigel Doering,Rahath Malladi,Arshia Sangwan,David Danks,Tauhidur Rahman*

Main category: cs.LG

TL;DR: 本研究提出了一种名为HiVAE的分层变分架构，旨在将心智理论(ToM)推理扩展到更真实的时空领域。通过在具有3,185个节点的校园导航任务上测试，该架构表现出了显著的性能改进。不过，也指出了一个关键限制：虽然这种分层结构提高了预测准确性，但所学到的潜在表示缺乏与实际心理状态的具体联系。为此，研究者们提出了自我监督对齐策略，并寻求社区对于如何更好地实现这些潜在表示与真实心理状态之间的关联的意见。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中在易于人类理解的小规模网格世界空间中应用心智理论(ToM)，而这项工作的动机是开发一种能够将ToM推理能力扩展至更加复杂且贴近现实世界的时空环境下的新方法。

Method: 研究者设计了HiVAE（分层变分自动编码器），它受到人类认知中的信念-欲望-意图结构启发，构建了一个三层VAE层次结构来改善ToM推理。

Result: 实验结果表明，在一项涉及3,185个节点的校园导航任务中，HiVAE相比其他方法取得了显著的性能提升；然而，同时也发现所学习到的潜在表征未能明确地与实际的心理状态相联系。

Conclusion: 尽管HiVAE展示了其在提高ToM推理方面的能力，尤其是在处理大规模时空数据时，但其潜在表示缺乏与实际心理状态的直接对应关系。因此，研究人员提出了一些自我监督的对齐策略，并希望得到更多关于如何解决这一问题的反馈。

Abstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.

</details>


### [17] [VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study](https://arxiv.org/abs/2602.16833)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 提出了一种名为Verbalized Action Masking (VAM)的方法，用于在大型语言模型的强化学习中提高探索效率。通过在提示中口头化行动掩码，并基于此执行迭代动作空间剪枝策略，该方法在国际象棋任务上表现出了比强基线更好的学习效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLMs）在强化学习(RL)后训练阶段遇到的探索瓶颈问题，特别是在面对稀疏反馈和大动作空间时容易陷入重复行为的问题。

Method: 提出了Verbalized Action Masking (VAM)技术，它将动作掩码口头化并在提示中实施，确保模型仅从被掩码的动作集中输出动作；并且基于这种方法引入了迭代式动作空间削减：如果目标动作没有被采样，则从掩码中移除有效的已采样动作，并在减少后的候选集合下重新采样，直至抽到目标或达到预定次数为止。

Result: 在国际象棋的研究案例中，无论是在与引擎对手对弈生成状态的引擎玩法模式还是使用带有验证者评分的固定数据集进行训练的模式下，VAM相比强大的基线都展现出了更高的学习效率及最终表现。

Conclusion: VAM通过提供一种实用的可控探索机制，在提高大型语言模型强化学习后训练阶段的表现方面显示出了其有效性。

Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.

</details>


### [18] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 本文通过引入考虑残差连接的累积注意力传播理论，解决了之前理论预测与实际观察到的Transformer模型位置偏置现象之间的差异。证明了因果Transformer在有限深度下会产生U形的位置偏置，即注意力集中在序列的开头和结尾处，为'Lost-in-the-Middle'现象提供了架构层面的解释。


<details>
  <summary>Details</summary>
Motivation: 先前对于注意力传播的理论分析预测，在无限深度且采用因果掩码的情况下，注意力会不可避免地集中于第一个token上。然而，实际情况并非如此。本文旨在解决这一理论预测与实践经验不符的问题，并探索Transformer模型中位置偏置的确切来源。

Method: 提出了一个考虑到残差连接的累积注意力传播新理论；通过数学建模分析了在现实条件下（即有限层数）残差连接如何防止注意力完全坍缩到首个token上；进一步证明了因果关系下的Transformer模型会在有限深度内形成一种U型的位置偏好模式。

Result: 发现残差连接是阻止注意力完全坍缩的关键因素；揭示了因果Transformer确实倾向于在序列的起始端和末端分配更多注意力权重，从而形成U形分布特征；为‘迷失中间’效应提供了一个基于模型结构的合理解释。

Conclusion: 本研究不仅阐明了Transformer架构中位置偏置现象背后的机制，还强调了残差连接在保持模型性能方面的重要性。这些发现加深了我们对Transformer工作原理的理解，并可能指导未来更高效架构的设计。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [19] [Training Large Reasoning Models Efficiently via Progressive Thought Encoding](https://arxiv.org/abs/2602.16839)
*Zeliang Zhang,Xiaodong Liu,Hao Cheng,Hao Sun,Chenliang Xu,Jianfeng Gao*

Main category: cs.LG

TL;DR: 本文提出了一种参数高效的微调方法——渐进思想编码，该方法使大型推理模型能够在固定大小的缓存下有效进行推理。通过将中间推理逐步编码为固定大小的向量表示，减少了内存使用同时保持推理过程中内存占用不变。实验表明，与LoRA微调和其他未微调的大规模推理模型相比，本方法在多个数学基准测试上实现了显著的性能提升，并提高了RL训练效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型（LRMs）在解决复杂问题时表现出色，但在效率方面面临一个关键障碍：基于结果奖励的强化学习（RL）训练需要长时间展开，其中自回归解码占据了大量时间和内存。虽然滑动窗口缓存策略可以限制内存使用，但会破坏长上下文推理并降低性能。

Method: 引入了渐进思想编码，这是一种参数高效的微调方法，允许LRMs在固定大小的缓存条件下有效推理。通过将中间推理过程逐步编码为固定大小的向量表示，避免了在整个缓存展开过程中反向传播的需求，从而降低了内存使用，并且在推理期间保持恒定的内存消耗。

Result: 在三个模型上进行了实验，包括Qwen2.5-3B-Instruct、Qwen2.5-7B-Instruct和DeepSeek-R1-Distill-Llama-8B，在六个广泛使用的具有挑战性的数学基准测试中显示了一致的进步：与基于LoRA的微调相比，我们的方法平均提高了19.3%，与没有微调的LRMs相比提高了29.9%；在相同的严格缓存预算下，AIME2024/2025上的准确率最高提升了23.4%。

Conclusion: 这些结果证明了渐进思想编码不仅提高了推理准确性，还使得LRMs在实际内存约束条件下的RL训练更加高效和可扩展。

Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.

</details>


### [20] [What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor](https://arxiv.org/abs/2602.16842)
*Rachitesh Kumar,Omar Mouchtaki*

Main category: cs.LG

TL;DR: 本文研究了在需求数据被库存水平截断的情况下，离线数据驱动的报童问题。通过提供一种计算经典数据驱动库存策略的确切最坏情况遗憾的方法，并证明该问题可以简化为有限维优化问题，从而对各种样本量和截断水平下的政策表现进行精确刻画。特别是分析了Kaplan-Meier策略，表明即使在严重截断情况下，少量有针对性的高库存探索也能显著改善最坏情况保证，实现接近最优的表现。而基于销售即需求启发式的策略则可能随着截断数据积累遭受严重的性能下降。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决当需求数据被库存水平截断时的数据驱动报童问题，与之前工作中的完全观察需求不同，在此设置下仅能观察到销售额度；当库存充足时销售额等于需求，否则等于可得库存。这一问题的核心在于如何利用这些不完全信息来优化库存管理决策。

Method: 作者提出了一种通用过程来计算所有需求分布上评估的经典数据驱动库存策略的确切最坏情况遗憾。主要技术成果表明，这个无限维非凸优化问题可以减少到一个有限维的问题，这使得能够对任何样本大小和截断水平下的策略性能进行精确描述。此外，还特别分析了Kaplan-Meier策略及基于销售即需求假设的策略表现。

Result: 研究表明，尽管需求截断从根本上限制了从被动销售数据中所能学到的信息量，但只要在较高库存水平上进行少量有针对性的探索就可以大幅提高最坏情况下的保证，即使在严重截断的情况下也能实现接近最优的表现。相比之下，如果POS系统未记录缺货事件且仅报告实际销售，则基于销售即需求假设的策略可能会随着截断数据的累积而经历严重的性能退化。

Conclusion: 本文强调了销售点信息系统质量对于离线学习能力的重要性。即使是在需求数据被截断的情形下，通过适当的设计（如：针对高库存水平的探索），仍有可能获得接近最优的结果。然而，如果简单地将销售等同于需求，则可能导致显著的性能损失。

Abstract: We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.

</details>


### [21] [Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling](https://arxiv.org/abs/2602.16864)
*Daniel Durstewitz,Christoph Jürgen Hemmer,Florian Hess,Charlotte Ricarda Doll,Lukas Eisenmann*

Main category: cs.LG

TL;DR: The paper argues for the integration of a dynamical systems (DS) perspective into time series (TS) forecasting and analysis, suggesting that DS reconstruction (DSR) methods can provide more accurate long-term forecasts and deeper theoretical insights into TS generation. It reviews key concepts in DS theory and DSR, illustrating how these can improve TS modeling with reduced computational and memory requirements, and concludes with specific recommendations for applying DSR to TS.


<details>
  <summary>Details</summary>
Motivation: 鉴于时间序列建模领域从早期的统计方法发展到当前的基础模型趋势，文章提出需要采用动力系统（DS）的视角来推动时间序列预测和分析达到新的水平。基于DS的方法不仅能够提供理论上最优的短期预测，还能帮助预测系统的长期统计数据，这对于许多实际场景来说可能是更重要的量度。此外，DS理论提供了关于时间序列生成机制背后的独立于领域的理论见解，这有助于理解任何时间序列模型性能的上限、在未见情景下的泛化能力以及可能的控制策略。

Method: 文章首先回顾了动力系统理论与动力系统重构（DSR）中的核心概念、方法、测量及模型。随后讨论了如何通过来自DS领域的见解以关键方式推进时间序列建模，使预测更加准确的同时大大降低计算和内存需求。

Result: 结果表明，将DSR方法应用于时间序列建模可以显著提高预测准确性，并且相比传统方法，具有更低的计算成本和内存占用。

Conclusion: 文章总结了利用DSR改进时间序列建模的具体建议，强调了结合动力系统观点对于提升预测精度及减少资源消耗的重要性。

Abstract: Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models. With a lot of hype and industrial demand in this field, it is not always clear how much progress there really is. To advance TS forecasting and analysis to the next level, here we argue that the field needs a dynamical systems (DS) perspective. TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts. This is the promise of DS reconstruction (DSR), a class of ML/AI approaches that aim to infer surrogate models of the underlying DS from data. But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints. We conclude with a number of specific suggestions for translating insights from DSR into TS modeling.

</details>


### [22] [ML-driven detection and reduction of ballast information in multi-modal datasets](https://arxiv.org/abs/2602.16876)
*Yaroslav Solovko*

Main category: cs.LG

TL;DR: 本研究提出了一种通用的多模态框架，用于检测和减少结构化、半结构化、非结构化和稀疏数据类型中的冗余或低效信息。通过引入一种新的压舱物分数来整合多种技术信号，形成了跨模式修剪策略。实验结果表明，可以剪除特征空间中高达70%以上的部分，同时最小化甚至改善分类性能，并大幅减少训练时间和内存占用。


<details>
  <summary>Details</summary>
Motivation: 现代数据集经常包含增加维度、存储需求和计算成本而不提供有意义分析价值的冗余或低效信息（称为压舱物）。为了提高机器学习流程的效率和性能，有必要开发一种方法来识别并消除这些不必要的特征。

Method: 该研究应用了熵、互信息、Lasso、SHAP、PCA、主题建模以及嵌入式分析等技术于不同类型的数据集上，以识别和去除压舱物特征。并且提出了一个新的压舱物评分，将上述各种技术的结果整合到一个统一的跨模式修剪策略中。

Result: 实验结果显示，在稀疏或半结构化数据中，通常超过70%的特征空间可以被削减，同时对分类表现的影响极小甚至有所提升，此外还显著减少了训练时间与内存使用量。

Conclusion: 这项工作揭示了不同类型的压舱物（例如统计型、语义型、基础设施型），并为构建更精简高效的机器学习管道提供了实用指南。

Abstract: Modern datasets often contain ballast as redundant or low-utility information that increases dimensionality, storage requirements, and computational cost without contributing meaningful analytical value. This study introduces a generalized, multimodal framework for ballast detection and reduction across structured, semi-structured, unstructured, and sparse data types. Using diverse datasets, entropy, mutual information, Lasso, SHAP, PCA, topic modelling, and embedding analysis are applied to identify and eliminate ballast features. A novel Ballast Score is proposed to integrate these signals into a unified, cross-modal pruning strategy. Experimental results demonstrate that significant portions of the feature space as often exceeding 70% in sparse or semi-structured data, can be pruned with minimal or even improved classification performance, along with substantial reductions in training time and memory footprint. The framework reveals distinct ballast typologies (e.g. statistical, semantic, infrastructural), and offers practical guidance for leaner, more efficient machine learning pipelines.

</details>


### [23] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 本研究通过结合变量选择和多变量分析，利用低成本且可调整的变量，在Python中构建了一个针对巴西中老年人的痴呆症分类模型。使用来自ELSI-Brazil的数据，对9,412名参与者进行了横断面设计的研究。随机森林（RF）模型的表现优于逻辑回归模型，并确定了多种痴呆风险因素及保护性因素。


<details>
  <summary>Details</summary>
Motivation: 为了使用低成本且具有潜在可调节性的变量来建立一个针对巴西中老年人群体的痴呆症分类模型，以促进资源的有效分配和痴呆症预防。

Method: 采用观察性研究方法，基于横断面设计，旨在估计发展成痴呆症的可能性。该研究使用了巴西老龄化纵向研究(ELSI-Brazil)的数据，涉及9,412名参与者。运用随机森林(RF)与多变量逻辑回归分析来估计痴呆症的风险。

Result: 痴呆症的患病率为9.6%。文盲、年龄≥90岁、低体重、握力弱、自报黑皮肤、缺乏体育活动、听力损失以及抑郁症状等因素与较高的痴呆风险相关；而较高教育水平、生活满意度高及就业状态则为保护性因素。随机森林模型在性能上优于逻辑回归模型，其ROC曲线下面积达到0.776，敏感度为0.708，特异度为0.702，F1分数为0.311，G均值为0.705，准确率为0.703。

Conclusion: 研究结果强调了痴呆症的多维性质及其识别脆弱个体时可访问因素的重要性。加强关注脑健康促进的公共政策对于巴西初级保健服务中资源的有效配置及痴呆症预防有着重要贡献。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [24] [A Unified Framework for Locality in Scalable MARL](https://arxiv.org/abs/2602.16966)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的策略诱导的相互依赖矩阵分解方法，揭示了即使在环境强烈耦合的情况下，平滑策略也能诱导局部性，并推导出一个更严格的谱条件来保证指数衰减性质。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）面临维度灾难挑战，现有解决方案基于环境最坏情况下的边界条件，未能捕捉策略本身的正则化效果。

Method: 通过分解策略诱导的相互依赖矩阵$H^π$，将环境对状态和动作的敏感度与策略对状态的敏感度解耦，进而分析局部性和最优性之间的权衡，并推导出一般谱条件$ρ(E^s+E^aΠ(π)) < 1$以确保价值函数的指数衰减性质。

Result: 发现了平滑策略可以在环境强动作耦合的情况下诱导局部性；提出了比之前基于范数条件更为严格的谱条件；利用该理论分析了一个具有直接与谱半径相关保证的局部块坐标策略改进框架。

Conclusion: 本研究工作为理解多智能体系统中的局部现象提供了新视角，并且为设计有效算法指出了潜在方向。

Abstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment's sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy's sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.

</details>


### [25] [Early-Warning Signals of Grokking via Loss-Landscape Geometry](https://arxiv.org/abs/2602.16967)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 研究发现，对于序列学习基准任务（SCAN和Dyck-1），非交换梯度更新的曲率测度——换位缺陷，在泛化之前显著增加，并且通过干预实验表明增强非交换性可以加速理解过程，而抑制正交梯度流则会延迟或阻止它。这表明换位缺陷是转换器中延迟泛化的可靠早期预警信号。


<details>
  <summary>Details</summary>
Motivation: 探讨在模块化算术以外的任务中，长时间训练后从记忆到泛化的突然转变（即理解）是否也遵循相似机制。

Method: 选取了两个序列学习基准测试：SCAN组合泛化与Dyck-1深度预测，使用不同学习率进行实验，并采用权重空间PCA分析、因果干预等方法来研究换位缺陷的作用。

Result: 换位缺陷在泛化前明显上升，且其上升时间遵循超线性幂律；放大非交换性能够加速理解过程，而抑制正交梯度流则会延迟或阻止该过程；换位缺陷作为早期预警信号比光谱集中更普遍适用。

Conclusion: 换位缺陷被认定为一种可靠的、架构无关的、与原因相关的早期预警信号，适用于变换器中的延迟泛化现象。

Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.

</details>


### [26] [Fail-Closed Alignment for Large Language Models](https://arxiv.org/abs/2602.16977)
*Zachary Coalson,Beth Sohler,Aiden Gabriel,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文提出了一种新的对齐原则——fail-closed alignment，用于提高大型语言模型的安全性。通过迭代地识别并消除之前学到的拒绝方向，促使模型沿着新的独立子空间重建安全性，从而在面对部分失效时仍能保持有效的拒绝机制。实验结果表明，使用该方法训练的模型能够编码多个因果上独立的拒绝方向，这些方向无法被同时抑制，从而增强了模型的整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）中的拒绝机制存在结构性弱点，在单一主要特征被抑制时可能导致整个对齐机制崩溃，产生不安全的内容生成。基于此问题，研究提出了一个新的设计原则——fail-closed alignment，旨在即使在部分功能失效的情况下也能保证拒绝机制的有效性。

Method: 提出了一种渐进式对齐框架，通过反复识别和移除已学习到的拒绝路径，强制模型沿着新的、相互独立的子空间重新构建其安全性。这种方法意在创建冗余且独立的因果路径来维持模型的安全性能。

Result: 在四个越狱攻击测试中，所提出的方法展现了最强的整体鲁棒性，同时减少了过度拒绝的情况，并保持了良好的生成质量，且额外计算开销较小。进一步的机制分析证实了通过本方法训练出的模型确实包含了多个因果上独立的拒绝方向，这使得基于提示的越狱攻击难以同时压制所有这些方向。

Conclusion: 研究表明，采用fail-closed alignment作为设计原则可以有效增强大型语言模型的安全性和鲁棒性，为开发更加可靠的人工智能系统提供了坚实的理论基础。

Abstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.

</details>


### [27] [Discovering Universal Activation Directions for PII Leakage in Language Models](https://arxiv.org/abs/2602.16980)
*Leo Marchyok,Zachary Coalson,Sungho Keum,Sooel Son,Sanghyun Hong*

Main category: cs.LG

TL;DR: 介绍了一种新的框架UniLeak，它能够识别出模型隐藏状态中普遍存在的激活方向，这些方向能够在推理时线性地增加生成个人可识别信息(PII)的可能性。这种机制无需访问训练数据或真实PII，仅依赖于模型自动生成的文本，并且在多个模型和数据集上测试显示，沿着这些通用方向操作显著增加了PII泄露的风险。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索隐私敏感行为（如个人可识别信息PII泄露）如何在现代语言模型的隐藏状态中被表示及调节。鉴于此领域了解尚浅，提出了一种新方法来更好地理解并控制这类风险。

Method: 开发了名为UniLeak的机制解释框架，该框架能够识别模型残差流中的通用激活方向。通过仅利用模型自产文本，而不需要训练数据或真实的PII信息，就能找到那些能在推理时通过线性添加持续提高跨提示生成PII可能性的方向。

Result: 实验表明，在不同模型与数据集之间，沿已识别出的通用方向进行调整可以大大增加PII泄露的机会，相较于现有的基于提示的提取方法效果更佳。此外，还发现了一个关于PII泄露的新视角：即模型表示中存在的潜在信号叠加现象，这既可能放大风险也可能有助于减轻风险。

Conclusion: 本研究表明，通过识别并操控特定模型内的通用激活方向，可以有效地调控PII泄露风险。这一发现不仅揭示了语言模型内部处理隐私敏感信息的一种新方式，同时也为减少此类风险提供了潜在途径。

Abstract: Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.

</details>


### [28] [Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding](https://arxiv.org/abs/2602.16994)
*Rahul Thomas,Teo Kitanovski,Micah Goldblum,Arka Pal*

Main category: cs.LG

TL;DR: 本文系统地评估了不同验证策略在多路径推测解码中的表现，发现遍历验证方法始终优于基于最优传输（OT）的方法。为改进OT方法的表现，提出了一种延迟树扩展技术，并开发了一个动态神经选择器来估计基于OT的验证方法的预期块效率，从而使得OT方法如SpecInfer首次超越了遍历验证方法，在多种模型、数据集和采样设置下平均吞吐量提高了5%。


<details>
  <summary>Details</summary>
Motivation: 先前的工作提出了各种独立同分布展开情况下的验证算法，但它们在匹配设置下的相对性能尚不清楚。研究旨在探索不同的验证策略如何影响多路径推测解码的效果，并寻求改进现有方法以提高解码效率。

Method: 首先对跨模型家族、任务和采样机制的验证策略进行了系统性评估。基于分析结果，提出了延迟树扩展技术和一个能够根据草稿与目标特征估计基于最优传输方法预期块效率的动态神经选择器。

Result: 遍历验证方法在多数情况下表现最佳，而基于最优传输的方法则在草稿树根部附近表现出较高的多令牌接受率。通过引入延迟树扩展及动态神经选择器，基于最优传输的方法如SpecInfer实现了相对于遍历验证方法高达5%的平均吞吐量提升。

Conclusion: 通过延迟树扩展以及动态调整决策点的神经选择器，可以显著改善基于最优传输的验证方法的表现，使其在多种条件下超越传统遍历验证方法。

Abstract: Multi-path speculative decoding accelerates lossless sampling from a target model by using a cheaper draft model to generate a draft tree of tokens, and then applies a verification algorithm that accepts a subset of these. While prior work has proposed various verification algorithms for i.i.d rollouts, their relative performance under matched settings remains unclear. In this work, we firstly present a systematic evaluation of verification strategies across model families, tasks, and sampling regimes, and find that Traversal Verification dominates consistently, with OT-based methods lagging far behind. Our analysis uncovers that this occurs because OT-based methods achieve high multi-token acceptance near the root of the draft tree, while multi-token gains are most impactful deeper in the draft tree, where draft and target distributions diverge. Based on this insight, we propose delayed tree expansion, which drafts a partial single path, delaying the i.i.d. branching point. We show that delayed tree expansion preserves the target distribution and improves on root-node i.i.d rollouts. Further, we develop a dynamic neural selector that estimates the expected block efficiency of optimal-transport-based verification methods from draft and target features, enabling context-dependent expansion decisions. Our neural selector allows OT-based methods like SpecInfer to outperform Traversal Verification for the first time, achieving 5% higher average throughput across a wide range of models, datasets, and sampling settings.

</details>


### [29] [Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17009)
*Nikunj Gupta,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 本文提出了一种称为行动图策略（AGP）的方法，用于多智能体强化学习中建模智能体之间可选动作的依赖关系。通过构建协调上下文，使得智能体能够基于全局动作依赖性做出决策。实验表明，AGP在具有部分可观测性和反协调惩罚的经典协调任务上取得了80-95%的成功率，并且在不同的多智能体环境中持续优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，协调行动是合作最基本的形式。成功的分散决策不仅取决于个体的好动作，还在于选择跨智能体间兼容的动作以同步行为、避免冲突并满足全局约束条件。为此，需要一种方法来有效建模智能体之间可选动作的相互依赖性，从而实现更优的联合行动。

Method: 提出了行动图策略（Action Graph Policies, AGP），该方法通过建模智能体可用行动选项间的依赖关系，构造所谓的“协调上下文”，让智能体可以根据全球行动依赖性来调整自己的决定。

Result: 理论分析表明，与完全独立的策略相比，AGP诱导出一个更具表现力的联合策略，并且能够实现比集中式价值分解方法中的贪婪执行更为优化的协调联合行动。实验证明，在具有部分可观测性和反协调惩罚的经典协调任务上，AGP达到了80-95%的成功率；而在这些任务上，其他MARL方法仅能达到10-25%的成功率。此外，AGP还在多种多智能体环境中持续优于基准方法。

Conclusion: AGP为解决多智能体强化学习中的协调问题提供了一个新途径，通过引入对智能体间动作依赖性的建模，显著提高了在复杂环境下的协调性能和任务成功率。

Abstract: Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.

</details>


### [30] [Malliavin Calculus as Stochastic Backpropogation](https://arxiv.org/abs/2602.17013)
*Kevin D. Oden*

Main category: cs.LG

TL;DR: 该论文建立了路径重参数化和得分函数（Malliavin）梯度估计器之间的严格联系，并基于此等价性引入了一个统一且具有方差感知能力的混合估计器，该估计器利用了两种梯度的实证协方差结构自适应地结合两者。研究结果在VAE上实现了9%的方差减少，在强耦合合成问题上最多减少了35%的方差。此外，探索性策略梯度实验表明，非平稳优化场景给混合方法带来了挑战。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过建立路径重参数化与Malliavin梯度估计器间的联系来提供一个更加统一和可解释的随机梯度估计框架，从而解决现有方法中存在的方差问题并提升估计效率。

Method: 首先证明了路径重参数化和Malliavin梯度估计器都源于Malliavin积分部分恒等式；然后基于这种等价关系开发了一个新的混合估计器，它能够根据经验协方差自适应地组合这两种梯度估计方法。

Result: 提出的混合估计器在不同任务中显著降低了方差，特别是在VAEs(CIFAR-10)上的方差减少了9%，以及在某些合成问题上最高可达35%。但同时也发现，在非平稳优化环境中，这种方法面临着一定挑战。

Conclusion: 本工作将Malliavin微积分定位为概念上统一、实践上易于理解的随机梯度估计框架，明确了混合方法何时能带来实际好处及其内在局限性所在。

Abstract: We establish a rigorous connection between pathwise (reparameterization) and score-function (Malliavin) gradient estimators by showing that both arise from the Malliavin integration-by-parts identity. Building on this equivalence, we introduce a unified and variance-aware hybrid estimator that adaptively combines pathwise and Malliavin gradients using their empirical covariance structure. The resulting formulation provides a principled understanding of stochastic backpropagation and achieves minimum variance among all unbiased linear combinations, with closed-form finite-sample convergence bounds. We demonstrate 9% variance reduction on VAEs (CIFAR-10) and up to 35% on strongly-coupled synthetic problems. Exploratory policy gradient experiments reveal that non-stationary optimization landscapes present challenges for the hybrid approach, highlighting important directions for future work. Overall, this work positions Malliavin calculus as a conceptually unifying and practically interpretable framework for stochastic gradient estimation, clarifying when hybrid approaches provide tangible benefits and when they face inherent limitations.

</details>


### [31] [WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning](https://arxiv.org/abs/2602.17025)
*Gagan Mundada,Zihan Huang,Rohan Surana,Sheldon Yu,Jennifer Yuntong Zhang,Xintong Li,Tong Yu,Lina Yao,Jingbo Shang,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: 本文提出了弱监督组相对策略优化(WS-GRPO)方法，通过将终端奖励转换为对部分轨迹的正确性感知指导来提高推理效率。该方法训练了一个偏好模型，从结果的正确性中学习何时继续或停止推理，从而减少了冗余思考同时保持了准确性。实验结果显示，WS-GRPO能够在显著减少推理长度的同时与GRPO基线保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的组相对策略优化（GRPO）方法在训练语言模型进行复杂推理时有效，但其目标定义相对于一组采样轨迹，这可能导致低效推理和过度思考的问题。此外，控制这种行为很困难，因为难以准确校准长度惩罚，并且直接指示何时继续或停止的监督通常只存在于最终答案的正确性上。

Method: 提出了一种名为弱监督组相对策略优化（WS-GRPO）的新方法，它通过将终端奖励转变为基于正确性的、覆盖部分轨迹的指导信号来改善推理过程中的滚动效率问题。该方法利用仅基于结果正确性的数据来训练一个偏好模型，进而产生前缀级别的信号，指出何时额外的延续是有益的。

Result: 理论分析加上实验证明，在多个推理基准测试中，WS-GRPO能够大幅减少滚动输出长度，同时保持与使用GRPO的方法相当的性能。

Conclusion: 通过引入WS-GRPO，研究者们找到了一种更高效地训练语言模型以执行复杂推理任务的方式，这种方法不仅减少了不必要的计算量，同时也维持了模型的表现力。

Abstract: Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We propose Weakly Supervised GRPO (WS-GRPO), which improves rollout efficiency by converting terminal rewards into correctness-aware guidance over partial trajectories. Unlike global length penalties that are hard to calibrate, WS-GRPO trains a preference model from outcome-only correctness to produce prefix-level signals that indicate when additional continuation is beneficial. Thus, WS-GRPO supplies outcome-derived continue/stop guidance, reducing redundant deliberation while maintaining accuracy. We provide theoretical results and empirically show on reasoning benchmarks that WS-GRPO substantially reduces rollout length while remaining competitive with GRPO baselines.

</details>


### [32] [Transforming Behavioral Neuroscience Discovery with In-Context Learning and AI-Enhanced Tensor Methods](https://arxiv.org/abs/2602.17027)
*Paimon Goulart,Jordan Steinhauser,Dawon Ahn,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文介绍了一种AI增强的科研发现流程，特别针对行为神经科学领域中恐惧泛化的研究。通过利用'情境学习'(ICL)这一新兴范式以及对张量分解模型的创新性改进，该流程能够自动化部分实验数据处理和模式识别过程，从而提高工作效率并保证了研究成果的质量。


<details>
  <summary>Details</summary>
Motivation: 传统科学研究流程复杂、僵化且耗时长，从数据准备到分析解读都需大量人力投入。本研究旨在通过AI技术简化这一流程，使领域专家能更专注于理解研究结果而非繁琐的数据处理工作。

Method: 采用“情境学习”(ICL)作为接口，让领域专家无需深入了解AI模型训练即可实现部分流程自动化；同时对张量分解模型进行了创新性改进以更好地处理异构数据。

Result: 实验证明所提出的管道在性能上优于当前领域内的标准做法及其他非ICL范式的机器学习基线方法，并且得到了团队内领域专家的认可。

Conclusion: 通过引入AI技术特别是ICL与改良后的张量分解模型，可以显著加速行为神经科学研究中的数据分析过程，同时保持甚至提升研究质量。

Abstract: Scientific discovery pipelines typically involve complex, rigid, and time-consuming processes, from data preparation to analyzing and interpreting findings. Recent advances in AI have the potential to transform such pipelines in a way that domain experts can focus on interpreting and understanding findings, rather than debugging rigid pipelines or manually annotating data. As part of an active collaboration between data science/AI researchers and behavioral neuroscientists, we showcase an example AI-enhanced pipeline, specifically designed to transform and accelerate the way that the domain experts in the team are able to gain insights out of experimental data. The application at hand is in the domain of behavioral neuroscience, studying fear generalization in mice, an important problem whose progress can advance our understanding of clinically significant and often debilitating conditions such as PTSD (Post-Traumatic Stress Disorder). We identify the emerging paradigm of "In-Context Learning" (ICL) as a suitable interface for domain experts to automate parts of their pipeline without the need for or familiarity with AI model training and fine-tuning, and showcase its remarkable efficacy in data preparation and pattern interpretation. Also, we introduce novel AI-enhancements to tensor decomposition model, which allows for more seamless pattern discovery from the heterogeneous data in our application. We thoroughly evaluate our proposed pipeline experimentally, showcasing its superior performance compared to what is standard practice in the domain, as well as against reasonable ML baselines that do not fall under the ICL paradigm, to ensure that we are not compromising performance in our quest for a seamless and easy-to-use interface for domain experts. Finally, we demonstrate effective discovery, with results validated by the domain experts in the team.

</details>


### [33] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: 介绍了一种新的索引机制MPZCH，它基于线性探测来有效减少嵌入碰撞，并且在保证生产规模效率的同时通常能够完全消除这些碰撞。该方法通过辅助张量和高性能CUDA内核实现了可配置的探测和主动驱逐策略，从而提高推荐系统的个性化质量。


<details>
  <summary>Details</summary>
Motivation: 随着唯一ID数量的增长，传统的基于哈希的索引方法会遇到导致模型性能和个人化质量下降的冲突问题。

Method: 提出了Multi-Probe Zero Collision Hash (MPZCH)，一种基于线性探测的新索引机制，它利用辅助张量和高性能CUDA内核来执行可配置的探测及积极的淘汰策略，从而避免了旧嵌入继承的问题，确保新特征可以从零开始有效地学习。

Result: 在线实验严格证明，MPZCH对于用户嵌入实现了零碰撞，并显著提高了项目嵌入的新鲜度和质量，同时保持了与现有方法相当的训练吞吐量和推理延迟。

Conclusion: MPZCH是一种有效的解决方案，能够解决大型推荐系统中由于高基数分类特征引起的嵌入表碰撞问题，改善了个性化推荐的质量。此外，该方案已经作为开源库TorchRec的一部分发布给更广泛的社区使用。

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [34] [AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation](https://arxiv.org/abs/2602.17071)
*Rong Fu,Muge Qi,Chunlei Meng,Shuo Yin,Kun Liu,Zhaolu Kang,Simon Fong*

Main category: cs.LG

TL;DR: AdvSynGNN is a robust architecture for node-level representation learning in graph neural networks, which uses multi-resolution structural synthesis, contrastive objectives, and an adversarial propagation engine to handle structural noise and non-homophilous topologies, leading to improved predictive accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to address the performance degradation of graph neural networks (GNNs) when dealing with structural noise or non-homophilous (heterophilic) topologies. The authors aim to create a more resilient GNN model that can maintain high performance across various graph structures, including those where nodes are not necessarily connected to similar nodes.

Method: The method introduced is AdvSynGNN, which includes a transformer backbone that adapts to heterophily by adjusting attention mechanisms based on learned topological signals. It also features an adversarial propagation engine with a generative component that suggests connectivity changes and a discriminator that enforces global coherence. Additionally, it employs a residual correction scheme for label refinement, guided by per-node confidence metrics.

Result: Empirical evaluations show that AdvSynGNN optimizes predictive accuracy across different graph distributions while keeping computational efficiency. The framework is designed to be robust against structural noise and capable of handling non-homophilous topologies, thereby improving the overall performance of GNNs in diverse scenarios.

Conclusion: The study concludes that the AdvSynGNN system effectively addresses the challenges faced by GNNs in the presence of structural noise and non-homophilous topologies. It provides practical implementation protocols for deploying the system in large-scale environments, ensuring robustness and efficiency.

Abstract: Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.

</details>


### [35] [Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum](https://arxiv.org/abs/2602.17080)
*Minxin Zhang,Yuxuan Liu,Hayden Scheaffer*

Main category: cs.LG

TL;DR: 本文提出了一种新的优化器NAMO及其对角扩展NAMO-D，首次将正交化动量与基于范数的Adam型噪声适应原则性地结合起来。实验表明，在预训练GPT-2模型时，两者相比AdamW和Muon基线均有更好的表现。


<details>
  <summary>Details</summary>
Motivation: 为了在随机优化中结合一个在确定性情况下表现良好的更新方向以及一种能够适应随机扰动的机制，同时改进现有的优化方法如Adam和Muon，提高大型语言模型训练中的性能。

Method: 通过开发新的优化器NAMO及它的对角扩展版本NAMO-D，其中NAMO使用单个自适应步长来调整正交化动量保持其正交性，而NAMO-D则通过对正交化动量右乘以一个具有钳制条目的对角矩阵实现神经元级别的噪声适应。

Result: NAMO和NAMO-D在确定性设置下达到最优收敛速度，并且在随机环境中它们的收敛保证能适应随机梯度的噪声水平。实验显示，二者在预训练GPT-2模型上优于AdamW和Muon基线，特别是NAMO-D通过额外的钳制超参数进一步提高了性能。

Conclusion: NAMO和NAMO-D提供了一种有效的方法来集成正交化动量与基于范数的噪声适应，从而在训练大型语言模型时表现出色。NAMO-D特别适合于需要精细噪声适应的任务。

Abstract: Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.

</details>


### [36] [MeGU: Machine-Guided Unlearning with Target Feature Disentanglement](https://arxiv.org/abs/2602.17088)
*Haoyu Wang,Zhuo Huang,Xiaolong Wang,Bo Han,Zhiwei Lin,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器引导的遗忘框架（MeGU），通过概念感知的重新对齐来指导遗忘过程。利用多模态大型语言模型为目标样本分配语义上有意义的扰动标签，同时引入正负特征噪声对以显式解缠目标概念影响，从而在保持共享语义结构的同时选择性地破坏特定于目标的表示。


<details>
  <summary>Details</summary>
Motivation: 现有数据遗忘方法通常面临一个基本权衡：积极抹除目标数据的影响往往会导致模型在保留数据上的实用性下降，而保守策略则会留下目标信息残余。此外，预训练期间学习到的内在表征属性中，语义类概念在特征模式层面是纠缠在一起的，这从根本上限制了现有遗忘范式的有效性。

Method: 提出了机器引导的遗忘（MeGU）框架，该框架通过概念感知的重新对齐来指导遗忘过程。具体来说，使用多模态大型语言模型（MLLMs）为需要被遗忘的目标样本分配具有语义意义的扰动标签，并且基于MLLM估计的类间概念相似度构建了一个轻量级转换矩阵。此外，MeGU还引入了正负特征噪声对，用于明确分离目标概念的影响。在微调过程中，负噪声抑制了与目标相关的特征模式，而正噪声则强化了剩余的相关特征，并将它们与扰动概念对齐。

Result: MeGU能够实现受控和选择性的遗忘，有效缓解了遗忘不足和过度遗忘的问题。

Conclusion: 通过采用MeGU框架，可以更有效地处理‘被遗忘的权利’问题，在移除特定数据痕迹的同时保持模型对于其他数据的有效性和准确性。

Abstract: The growing concern over training data privacy has elevated the "Right to be Forgotten" into a critical requirement, thereby raising the demand for effective Machine Unlearning. However, existing unlearning approaches commonly suffer from a fundamental trade-off: aggressively erasing the influence of target data often degrades model utility on retained data, while conservative strategies leave residual target information intact. In this work, the intrinsic representation properties learned during model pretraining are analyzed. It is demonstrated that semantic class concepts are entangled at the feature-pattern level, sharing associated features while preserving concept-specific discriminative components. This entanglement fundamentally limits the effectiveness of existing unlearning paradigms. Motivated by this insight, we propose Machine-Guided Unlearning (MeGU), a novel framework that guides unlearning through concept-aware re-alignment. Specifically, Multi-modal Large Language Models (MLLMs) are leveraged to explicitly determine re-alignment directions for target samples by assigning semantically meaningful perturbing labels. To improve efficiency, inter-class conceptual similarities estimated by the MLLM are encoded into a lightweight transition matrix. Furthermore, MeGU introduces a positive-negative feature noise pair to explicitly disentangle target concept influence. During finetuning, the negative noise suppresses target-specific feature patterns, while the positive noise reinforces remaining associated features and aligns them with perturbing concepts. This coordinated design enables selective disruption of target-specific representations while preserving shared semantic structures. As a result, MeGU enables controlled and selective forgetting, effectively mitigating both under-unlearning and over-unlearning.

</details>


### [37] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: 研究发现流匹配在低维潜在空间中适用于快速采样随机闭合模型，比迭代扩散方法快两个数量级。同时，通过比较隐式正则化和两种显式正则化（保持度量和几何感知约束）的效果，表明无论是显式还是隐式正则化的潜在空间都能够继承原始复杂动力系统低维流形的关键拓扑信息，从而可以在不需要大量训练数据的情况下学习随机闭合模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能够为生成AI任务提供高质量样本，并维持样本多样性以促进模式覆盖，但其采样速度较慢被视为主要劣势。本研究旨在探索如何利用流匹配技术在低维潜在空间中加速采样过程，同时保证物理保真度。

Method: 采用基于传输的生成模型对比实验，具体是在2D Kolmogorov流的数值示例上进行。此外，还评估了联合训练方案提供的隐式正则化与两种显式正则化——保持度量(MP)和几何感知(GA)约束——之间的效果差异。

Result: 结果表明，在低维潜在空间中使用流匹配可以实现单步采样，比迭代扩散方法快至多两个数量级。而且，无论是通过隐式还是显式正则化的潜在空间都保留了原始复杂动力系统低维流形的关键拓扑信息，这使得即使没有大量的训练数据也能学习到有效的随机闭合模型。

Conclusion: 该研究表明，对于需要快速采样的应用场景，如学习随机闭合模型时，利用流匹配技术在适当正则化的低维潜在空间中操作是一个有效策略。这种方法不仅提高了采样效率，还保证了所学模型的物理准确性。

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


### [38] [FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment](https://arxiv.org/abs/2602.17095)
*Chuiyang Meng,Ming Tang,Vincent W. S. Wong*

Main category: cs.LG

TL;DR: 本文提出了一种名为FLoRG的联邦微调框架，该框架使用单个低秩矩阵进行微调，并聚集其Gram矩阵，以解决低秩适应（LoRA）在联邦学习中带来的聚合误差和分解漂移问题。实验表明，FLoRG不仅提高了下游任务准确性，还大幅减少了通信开销。


<details>
  <summary>Details</summary>
Motivation: 针对现有参数高效微调技术如低秩适应（LoRA）在联邦学习环境下进行微调时遇到的问题——即由于分别聚合两个低秩矩阵所引起的错误以及即使聚合了两个低秩矩阵乘积后仍需通过非唯一性矩阵分解来恢复因子导致的分解漂移现象，提出了新的解决方案。

Method: 提出FLoRG框架，它采用单一低秩矩阵代替传统的双低秩矩阵设计来进行模型微调，并通过聚集此低秩矩阵的Gram矩阵而非直接聚集低秩矩阵本身来消除因分散式聚合而产生的误差。同时，引入Procrustes对齐方法最小化连续微调轮次间因矩阵分解非唯一性而导致的漂移问题，确保更新的一致性。

Result: 理论分析显示，采用Procrustes对齐法可以得到更紧的收敛界限；实验结果证明，在多个大型语言模型微调基准测试上，FLoRG相比五种最先进基线方案，在提高下游任务准确率的同时，还能将通信开销降低至多2041倍。

Conclusion: FLoRG为联邦学习场景下的参数高效微调提供了一个有效的方法，能够显著提升模型性能并减少资源消耗。

Abstract: Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\times$.

</details>


### [39] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 本文介绍了一个无服务器的MLOps框架，用于管理从数据摄入到重新训练的完整机器学习生命周期。通过实际应用案例——HS编码预测，展示了该框架的有效性，其中Text-CNN模型达到了98%的准确率。此外，该解决方案还确保了可重复性、可审计性和服务级别协议遵守，并且优先考虑成本效益和确定性的分类。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提出一种无服务器MLOps框架，以简化机器学习（ML）项目从开发到部署的整个过程。特别地，通过一个工业实施例子——HS代码预测，说明了该框架在处理关键合规任务中的实用性。

Method: 采用了事件驱动的管道和服务来构建一个无服务器架构，支持多种推理模式。对于HS代码预测问题，使用了自定义文本嵌入编码器加上几种深度学习架构进行实验比较，最终选择Text-CNN作为最佳模型。

Result: 所提出的方案不仅在HS代码预测上取得了98%的高准确度，而且在整个过程中保证了良好的可再现性、审计能力和SLA遵循。同时，通过自动化的A/B测试机制实现了动态模型选择与安全推广。

Conclusion: 本研究提供了一种基于无服务器架构实现机器学习操作的可复制蓝图，帮助企业优化性能的同时也关注经济成本。虽然文中提到的架构主要针对非Transformer类模型进行了优化，但其设计具有足够的灵活性以适应未来可能引入的新技术。

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [40] [i-PhysGaussian: Implicit Physical Simulation for 3D Gaussian Splatting](https://arxiv.org/abs/2602.17117)
*Yicheng Cao,Zhuo Huang,Yu Yao,Yiming Ying,Daoyi Dong,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出了一种结合3D高斯点绘和隐式物质点方法的新型物理模拟框架i-PhysGaussian，通过最小化动量平衡残差实现更稳定的模拟效果，尤其在处理高刚度材料或准静态运动等复杂场景时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3D重建的模拟器通常依赖于显式的、逐步更新的方法，这种方法对时间步长敏感，在面对如高刚度材料或准静态移动这样的复杂情况时，准确性会快速下降。为了解决这些问题，并提高模拟精度与稳定性，研究者开发了新的解决方案。

Method: 本研究引入了i-PhysGaussian框架，该框架将3D高斯点绘(3DGS)与隐式物质点法(MPM)积分器相结合。与传统的显式方法不同，i-PhysGaussian通过使用GMRES求解器进行隐式的Newton类型优化来最小化动量平衡残差，从而得到最终状态。

Result: 实验结果表明，相比于显式基线方法，i-PhysGaussian能够在高达20倍大的时间步长下保持稳定，即使是在复杂的动态转换中也能保持结构的一致性和平滑的动作。

Conclusion: i-PhysGaussian框架提供了一种有效解决现有物理模拟器局限性的新途径，特别是在处理需要更高精度和稳定性的复杂场景时展现出显著优势。

Abstract: Physical simulation predicts future states of objects based on material properties and external loads, enabling blueprints for both Industry and Engineering to conduct risk management. Current 3D reconstruction-based simulators typically rely on explicit, step-wise updates, which are sensitive to step time and suffer from rapid accuracy degradation under complicated scenarios, such as high-stiffness materials or quasi-static movement. To address this, we introduce i-PhysGaussian, a framework that couples 3D Gaussian Splatting (3DGS) with an implicit Material Point Method (MPM) integrator. Unlike explicit methods, our solution obtains an end-of-step state by minimizing a momentum-balance residual through implicit Newton-type optimization with a GMRES solver. This formulation significantly reduces time-step sensitivity and ensures physical consistency. Our results demonstrate that i-PhysGaussian maintains stability at up to 20x larger time steps than explicit baselines, preserving structural coherence and smooth motion even in complex dynamic transitions.

</details>


### [41] [TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series](https://arxiv.org/abs/2602.17122)
*Xihao Piao,Zheng Chen,Lingwei Zhu,Yushun Dong,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文提出了一种时间不变频率算子(TIFO)，通过在整个数据集中学习频谱上的平稳性感知权重来解决非平稳时间序列预测中的分布偏移问题。TIFO能够在多种预测模型中无缝集成，并且实验显示它在28个预测设置中有18次排名第一、6次排名第二，同时在ETTm2数据集上平均MSE提高了33.3%至55.3%，并且相比基准方法降低了60%-70%的计算成本。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列预测面临训练和测试数据由不同分布产生的分布偏移问题。现有方法试图通过例如从每个样本中去除低阶矩来减轻这种依赖关系，但这些解决方案未能捕捉到样本间的基本时变结构，也未对复杂的时间结构进行建模。

Method: 本文提出了时间不变频率算子（TIFO），该方法通过考虑所有可能的时间结构，在频域内解决了分布偏移的问题。TIFO能够学习整个数据集频谱上的平稳性敏感权重，从而突出平稳频率成分并抑制非平稳成分，进而缓解时间序列中的分布偏移问题。

Result: 实验表明，所提出的方法在28个预测场景中取得了18次第一和6次第二的成绩。特别地，在ETTm2数据集上，平均MSE分别提升了33.3%和55.3%。此外，与基线方法相比，TIFO还减少了60%-70%的计算成本。

Conclusion: TIFO作为解决非平稳时间序列预测中分布偏移问题的一种有效方法，不仅能够显著提高预测准确性，还能大幅降低计算成本，展现出了强大的可扩展性。

Abstract: Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.

</details>


### [42] [VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation](https://arxiv.org/abs/2602.17133)
*Linwei Zhai,Han Ding,Mingzhi Lin,Cui Zhao,Fei Wang,Ge Wang,Wang Zhi,Wei Xi*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型VP-VAE，它通过消除训练期间对显式码本的需求来解耦表示学习和离散化过程。VP-VAE使用Metropolis-Hastings采样生成的潜变量扰动代替非可微分量化器，从而实现了无码本的稳定训练，并且对于推理时的量化误差更加鲁棒。此外，基于近似均匀潜在变量的假设，还导出了FSP（有限标量扰动），作为VP-VAE的一个轻量级变体，为FSQ风格的固定量化器提供统一理论解释及实际改进。实验表明，VP-VAE与FSP在图像和音频基准测试中提高了重建保真度，达到更平衡的令牌使用率，同时避免了由于联合码本训练引起的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的VQ-VAE在训练过程中经常遇到不稳定性和“码本崩溃”的问题，这是因为表示学习与离散码本优化之间存在内在联系。为了解决这些问题，作者们旨在开发一种新的方法，能够将表示学习与离散化过程分离，从而提高模型训练的稳定性和效果。

Method: VP-VAE通过引入分布一致且尺度自适应的潜变量扰动来替代非可微分量化器，这些扰动是通过Metropolis--Hastings采样产生的。该方法允许在没有显式码本的情况下进行稳定训练，并增强了模型对推理阶段量化错误的鲁棒性。另外，文章还提出了FSP（有限标量扰动）作为VP-VAE的一种简化版本，适用于近似均匀分布的潜变量情况。

Result: 实验结果表明，在图像和音频数据集上，VP-VAE和FSP相比传统方法能够显著提高重建质量，并且在令牌使用方面表现得更为均衡。此外，新方法有效避免了由联合码本训练导致的不稳定性问题。

Conclusion: VP-VAE及其变体FSP为解决VQ-VAE中存在的训练不稳定性和码本崩溃问题提供了有效的解决方案。通过将表示学习从离散化过程中分离出来，这两种方法不仅提高了模型训练的稳定性，还增强了其性能。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and "codebook collapse" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.

</details>


### [43] [TimeOmni-VL: Unified Models for Time Series Understanding and Generation](https://arxiv.org/abs/2602.17149)
*Tong Guan,Sheng Pan,Johan Barthelemy,Zhao Li,Yujun Cai,Cesare Alippi,Ming Jin,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出了TimeOmni-VL，这是一个以视觉为中心的框架，通过保真度的双向映射和理解导向的生成方法，统一了时间序列的理解与生成。实验表明这种方法在提高语义理解和数值精度方面取得了显著成效，为多模态时间序列建模开辟了新的前沿。


<details>
  <summary>Details</summary>
Motivation: 最近的时间序列建模在数值生成和语义理解之间存在明显的分歧。生成模型往往依赖于表面模式匹配，而面向理解的模型则难以提供高保真的数值输出。尽管统一的多模态模型(UMMs)已经在视觉领域解决了这一问题，但它们在时间序列中的潜力尚未被发掘。

Method: 提出了一种名为TimeOmni-VL的新框架，该框架通过两个关键创新来统合时间序列的理解与生成：（1）开发了时间序列与图像之间的保真度保留双向映射（Bi-TSI），改进了TS2I和I2TS转换过程，确保近乎无损的转换。（2）实施了基于理解的生成策略。此外，还引入了一个新的数据集TSUMM-Suite，它包含了六个基于时间序列分析的理解任务以及两项生成任务。

Result: 实验结果证明，这种统一的方法不仅极大地提高了对时间序列数据的语义理解能力，同时也增强了数值预测的准确性。

Conclusion: TimeOmni-VL作为首个利用时间序列理解作为显式控制信号以实现高保真生成的系统，为多模态时间序列建模设立了新的标准。

Abstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.

</details>


### [44] [Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization](https://arxiv.org/abs/2602.17155)
*Yicheng Lang,Changsheng Wang,Yihua Zhang,Mingyi Hong,Zheng Zhang,Wotao Yin,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的零阶优化方法ZO-Muon，通过结合投影子空间视图和Muon风格的谱优化来提高零阶优化的效率和准确性。实验表明，该方法在大型语言模型和视觉转换器上的收敛速度明显加快，并且在查询/运行时效率方面也有所改善。


<details>
  <summary>Details</summary>
Motivation: 零阶（ZO）优化提供了一种无需梯度的方法，通过函数评估的有限差分估计梯度，在避免反向传播的情况下对大规模模型进行微调，从而成为一种内存高效的范式。然而，ZO优化在准确性和查询效率之间存在根本性的矛盾。

Method: 提出了一个统一框架，即子空间梯度正交化，该框架结合了两个互补原则：(i) 利用模型更新内在低秩结构减少梯度估计方差的基于投影的子空间视图；(ii) 应用梯度正交化的Muon风格谱优化从嘈杂的ZO梯度中提取信息丰富的谱结构。这些发现形成了一个新的方法ZO-Muon，它可被解释为ZO设置下的低秩Muon优化器。

Result: 广泛的实验显示，ZO-Muon显著加速了收敛过程，并在精度与查询/运行时间效率上实现了双赢改进。特别是，与流行的MeZO基线相比，对于LLM微调达到相同的SST-2性能，ZO-Muon仅需要24.7%的查询量，并在CIFAR-100上的ViT-B微调提高了25.1%的准确性。

Conclusion: 本研究证明了通过融合两种互补原理——投影式子空间视角及Muon样式的谱优化——可以大幅度提升ZO优化的表现。所提出的ZO-Muon方法不仅能够加速收敛，还在准确率与查询/运行效率上取得了显著进步。

Abstract: Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as a low-rank Muon optimizer in the ZO setting. Extensive experiments on large language models (LLMs) and vision transformers (ViTs) demonstrate that ZO-Muon significantly accelerates convergence and achieves a win-win improvement in accuracy and query/runtime efficiency. Notably, compared to the popular MeZO baseline, ZO-Muon requires only 24.7% of the queries to reach the same SST-2 performance for LLM fine-tuning, and improves accuracy by 25.1% on ViT-B fine-tuning on CIFAR-100.

</details>


### [45] [In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks](https://arxiv.org/abs/2602.17171)
*Ayush Goel,Arjun Kohli,Sarvagya Somvanshi*

Main category: cs.LG

TL;DR: 本文通过实证研究比较了transformers和线性注意力模型在上下文学习中的表现，特别是针对线性回归任务时的学习质量、收敛性和泛化行为，并探讨了模型深度对上下文学习性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索并对比transformers与线性注意力模型在处理简单函数类（如线性回归）时的上下文学习能力差异及其特性。

Method: 采用Garg等人提出的经典线性回归任务作为基准，评估两种注意机制在该任务上的学习质量(MSE)、收敛性及泛化行为，并分析模型深度增加对于上下文学习性能的影响。

Result: 结果显示，在给定设置下，虽然线性注意力模型相对于二次方注意力有其相似之处，但也存在一定的局限性。

Conclusion: 研究表明，尽管线性注意力模型在线性回归任务中表现出了一定的有效性，但相比于传统的二次方注意力机制，它仍然面临着一些挑战。

Abstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.

</details>


### [46] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: 介绍了一个名为softdtw-cuda-torch的开源PyTorch库，该库能够在GPU上计算Soft Dynamic Time Warping (SoftDTW)，并解决了现有GPU实现中的三个关键限制：序列长度上限为1024、小平滑参数下的数值不稳定性以及成对距离张量导致的过高的GPU内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的GPU上的SoftDTW实现存在几个问题：只能处理长度不超过1024的序列、当平滑参数较小时后向传递过程中出现数值不稳定、以及由于存储成对距离矩阵而导致的GPU内存使用过高。本研究旨在解决这些问题。

Method: 通过引入(1)分块对角线核执行来消除序列长度限制，(2)采用对数空间反向传递防止浮点溢出，(3)融合的距离计算模式以消除O(BNM)中间距离张量，从而达到高达98%的内存减少。

Result: 开发了softdtw-cuda-torch库，它支持任意长度的序列、完全集成于PyTorch autograd，并能进行Soft-DTW Barycenter计算。与之前的工作相比，该库实现了显著的内存节省。

Conclusion: softdtw-cuda-torch是一个改进的PyTorch库，用于在GPU上高效地执行SoftDTW，解决了先前实现中遇到的主要挑战。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [47] [Structured Prototype-Guided Adaptation for EEG Foundation Models](https://arxiv.org/abs/2602.17251)
*Jingying Ma,Feng Wu,Yucheng Xing,Qika Lin,Tianyu Liu,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: 提出了一种名为SCOPE的新框架，用于在有限标签的跨主体设置下对EEG基础模型进行微调，通过构建可靠的外部监督和轻量级适配器来解决由于有限监督和模型参数空间之间的结构不匹配导致的泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的EEG基础模型（EFMs）在全微调下表现良好，但在实际临床环境中常见的受试者级别监督受限时，其泛化能力较差。研究发现，这一问题不仅源于监督的限制，还因为EFM的高度可塑性参数空间与噪声大、有限的监督之间存在结构上的不匹配。

Method: 提出了SCOPE，一种结构化信心感知原型引导适应框架，分为两个阶段：第一阶段通过学习几何正则化的任务先验，在结果嵌入上构建平衡的类级原型，并从它们的一致性中生成信心感知伪标签以过滤未标记数据上的不可靠信号；第二阶段引入ProAdapter，它通过基于结构化原型的轻量级适配器来调整冻结的EEG基础模型。

Result: 在三个EEG任务和五个基础模型骨干上进行的实验表明，SCOPE在标签有限的跨主体环境下始终表现出色且高效。

Conclusion: SCOPE为EEG基础模型在有限监督条件下的微调提供了一个有效的方法，能够提高模型在不同个体间的表现一致性。

Abstract: Electroencephalography (EEG) foundation models (EFMs) have achieved strong performance under full fine-tuning but exhibit poor generalization when subject-level supervision is limited, a common constraint in real-world clinical settings. We show that this failure stems not merely from limited supervision, but from a structural mismatch between noisy, limited supervision and the highly plastic parameter space of EFMs. To address this challenge, we propose SCOPE, a Structured COnfidence-aware Prototype-guided adaptation framework for EFM fine-tuning. SCOPE follows a two-stage pipeline. In the first stage, we construct reliable external supervision by learning geometry-regularized task priors, constructing balanced class-level prototypes over the resulting embeddings, and producing confidence-aware pseudo-labels from their agreement to filter unreliable signals on unlabeled data. In the second stage, we introduce ProAdapter, which adapts frozen EEG foundation models via a lightweight adapter conditioned on the structured prototypes. Experiments across three EEG tasks and five foundation model backbones demonstrate that SCOPE consistently achieves strong performance and efficiency under label-limited cross-subject settings.

</details>


### [48] [Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems](https://arxiv.org/abs/2602.17263)
*Alexander Klemps,Denis Ilia,Pradeep Kr. Banerjee,Ye Chen,Henrik Tünnermann,Nihat Ay*

Main category: cs.LG

TL;DR: 研究提出了一种基于Wasserstein自编码器的生成建模框架，用于学习脉冲整形和下游束流动力学之间的可微潜空间接口。该方法减少了对昂贵的脉冲传播模拟的依赖，同时促进了束流动力学的模拟和分析。


<details>
  <summary>Details</summary>
Motivation: 在自由电子激光器的光电注入器中控制纵向激光脉冲形状是优化电子束质量的有效手段，但由于暴力脉冲传播模拟的成本高昂，系统地探索设计空间受到了限制。因此，需要一种更有效的方法来减少对这些成本高昂模拟的依赖。

Method: 采用基于Wasserstein自编码器的生成模型框架，建立脉冲整形与下游束流动力学之间的一个可微分潜变量接口。通过这种方法，能够学习到一个连续且可解释的潜空间，并保持高保真度的重建。

Result: 实证结果表明，所学到的潜空间是连续且可解释的，同时能保持高保真度重建。不同类型的脉冲可以通过线性插值平滑过渡。此外，该模型能够从模拟数据推广到真实实验脉冲测量，准确重构脉冲并将它们一致地嵌入到已学习的流形中。

Conclusion: 本研究所提出的基于Wasserstein自编码器的生成建模框架为脉冲整形提供了一个高效的学习工具，减少了对昂贵脉冲传播模拟的需求，并有助于进行后续的束流动力学模拟和分析工作。

Abstract: Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gaussian Mixture Models reveals a well behaved latent geometry, enabling smooth transitions between distinct pulse types via linear interpolation. The model generalizes from simulated data to real experimental pulse measurements, accurately reconstructing pulses and embedding them consistently into the learned manifold. Overall, the approach reduces reliance on expensive pulse-propagation simulations and facilitates downstream beam dynamics simulation and analysis.

</details>


### [49] [Unified Latents (UL): How to train your latents](https://arxiv.org/abs/2602.17270)
*Jonathan Heek,Emiel Hoogeboom,Thomas Mensink,Tim Salimans*

Main category: cs.LG

TL;DR: 本文提出了一种名为Unified Latents (UL)的框架，通过扩散先验联合正则化学习潜在表示，并由扩散模型解码。该方法在ImageNet-512上达到了竞争性的FID分数1.4和高质量的重建质量（PSNR），同时比基于Stable Diffusion潜变量训练的模型需要更少的训练FLOPs。此外，在Kinetics-600数据集上设置了新的最先进的FVD得分1.3。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在开发一种能够有效学习潜在表示的方法，这些表示不仅具有良好的生成能力，而且还能保持高重建质量，同时尽量减少训练计算成本。

Method: 提出了Unified Latents (UL)框架，其中潜在表示受到扩散先验的共同正则化，并通过扩散模型进行解码。特别地，通过将编码器输出噪声与先验的最小噪声水平关联起来，形成一个简洁的训练目标，从而为潜在比特率提供了一个紧致的上限。

Result: 实验结果显示，在ImageNet-512数据集上，所提方法达到了竞争性的FID分数1.4，同时保持了较高的重建质量(PSNR)，并且相比于基于Stable Diffusion潜变量训练的模型，所需训练FLOPs更少。另外，在Kinetics-600视频数据集上，实现了当前最佳的FVD分数1.3。

Conclusion: Unified Latents (UL)作为一种新颖的学习潜在表示的方法，展示了其在图像及视频生成任务中的优越性能，特别是在保证生成质量的同时显著降低了训练所需的计算资源。

Abstract: We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.

</details>


### [50] [RLGT: A reinforcement learning framework for extremal graph theory](https://arxiv.org/abs/2602.17276)
*Ivan Damnjanović,Uroš Milivojević,Irena Đorđević,Dragan Stevanović*

Main category: cs.LG

TL;DR: 介绍了用于图论的新型强化学习框架RLGT，该框架系统化了先前的工作，并支持无向和有向图，无论是否有环以及具有任意数量的边颜色。


<details>
  <summary>Details</summary>
Motivation: 受到Wagner提出的深度交叉熵强化学习方法应用于极值图论问题的启发，研究者们希望改进和扩展这一框架，以解决更多图论中的组合优化问题。

Method: 提出了一个名为RLGT的新框架，它对以前的工作进行了系统化，并且能够处理无向图和有向图、带或不带环的情况，同时支持多色边。

Result: RLGT框架提供了一种有效的图表示方式，旨在通过优化的计算性能和清晰模块化的设计促进基于强化学习的极值图论未来的研究。

Conclusion: RLGT框架是一个创新的强化学习工具，它为图论特别是极值图论提供了强大的支持，有望在未来的研究中发挥重要作用。

Abstract: Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for certain Ramsey numbers, and contributions were made to the Turán-type extremal problem in which the forbidden structures are cycles of length three and four. Here, we present Reinforcement Learning for Graph Theory (RLGT), a novel RL framework that systematizes the previous work and provides support for both undirected and directed graphs, with or without loops, and with an arbitrary number of edge colors. The framework efficiently represents graphs and aims to facilitate future RL-based research in extremal graph theory through optimized computational performance and a clean and modular design.

</details>


### [51] [Efficient privacy loss accounting for subsampling and random allocation](https://arxiv.org/abs/2602.17284)
*Vitaly Feldman,Moshe Shenfeld*

Main category: cs.LG

TL;DR: 本文研究了随机分配采样方案的隐私放大特性，证明了其在差分隐私算法中的隐私损失分布可以被有效计算。特别地，当应用于高斯机制时，随机分配至少与泊松子采样一样好，并且更适合于通过DP-SGD进行训练。此外，还开发了新的工具来支持基于隐私损失分布实现概念的一般隐私损失核算。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，在不同场景下应用的一种特定采样方案——从一系列步骤中均匀随机选择k步使用用户的资料数据，相比标准泊松采样具有实用性优势。然而，对该方案的理论分析存在两个主要缺陷：首先，实际应用中因分析过程中的近似处理导致隐私参数不够精确；其次，当前计算出的隐私参数（如曲棍球棒或Rényi散度）在用于隐私损失核算时引入了额外开销。本研究旨在解决这些问题，提供更准确、高效的隐私损失评估方法。

Method: 作者提出了一种有效计算随机分配给任意差分隐私算法带来的隐私损失分布的方法。特别是针对高斯机制，展示了随机分配在隐私-效用权衡方面至少等同于甚至优于泊松子采样，并且对于使用DP-SGD进行训练更加合适。此外，他们还开发了基于隐私损失分布实现的新工具，以改进一般情况下的隐私损失核算流程。

Result: 研究表明，随机分配的隐私损失分布能够被有效地计算出来；当该方法应用于高斯机制时，它展现出与泊松子采样相当甚至更好的隐私-效用平衡；新提出的PLD实现概念允许将精确的隐私损失核算扩展到以前需要手动进行噪声机制特异性分析的子采样情形。

Conclusion: 本文通过提出一种有效计算随机分配隐私损失分布的方法及其在高斯机制中的应用，为差分隐私优化提供了新的见解。结果表明，随机分配不仅在理论上接近甚至优于现有的泊松采样方法，而且通过引入新的隐私损失核算工具，使得整个过程变得更加高效和通用。

Abstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used in privacy loss accounting.
  In this work, we demonstrate that the privacy loss distribution (PLD) of random allocation applied to any differentially private algorithm can be computed efficiently. When applied to the Gaussian mechanism, our results demonstrate that the privacy-utility trade-off for random allocation is at least as good as that of Poisson subsampling. In particular, random allocation is better suited for training via DP-SGD. To support these computations, our work develops new tools for general privacy loss accounting based on a notion of PLD realization. This notion allows us to extend accurate privacy loss accounting to subsampling which previously required manual noise-mechanism-specific analysis.

</details>


### [52] [SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework](https://arxiv.org/abs/2602.17330)
*Rong Fu,Zijian Zhang,Wenxin Zhang,Kun Liu,Jiekai Wu,Xianda Li,Simon Fong*

Main category: cs.LG

TL;DR: SubQuad是一个端到端的管道，它结合了抗原感知、近亚二次检索与GPU加速亲和力内核、学习多模态融合以及公平约束聚类，解决了大规模比较分析适应性免疫库时遇到的成本问题和数据集不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在人群规模上进行适应性免疫库的比较分析受到两个实际瓶颈的阻碍：几乎二次方的成对亲和力评估成本以及数据集失衡，这掩盖了临床上重要的少数克隆型。为了解决这些挑战，研究者开发了SubQuad系统。

Method: SubQuad通过结合抗原感知的近亚二次检索、GPU加速的亲和力内核、学习多模态融合以及公平约束聚类来解决上述问题。该系统使用紧凑的MinHash预过滤以显著减少候选比较次数，一个可微分门控模块根据每对基础自适应地加权互补对齐和嵌入通道，并且有一个自动化校准例程来确保稀有抗原特异性子群的比例代表。

Result: 对于大型病毒和肿瘤库，SubQuad在吞吐量和峰值内存使用方面实现了测量增益，同时保持或提高了召回率@k、簇纯度和子群公平性。

Conclusion: 通过共同设计索引、相似性融合和公平意识目标，SubQuad提供了一个可扩展、偏见感知的平台，用于库挖掘及下游转化任务，例如疫苗靶点优先级排序和生物标志物发现。

Abstract: Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.

</details>


### [53] [2Mamba2Furious: Linear in Complexity, Competitive in Accuracy](https://arxiv.org/abs/2602.17363)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: 本研究通过简化并改进Mamba-2线性注意力机制，提出了2Mamba方法，在保持接近Softmax注意力准确率的同时，大幅提高了长上下文长度下的内存效率。


<details>
  <summary>Details</summary>
Motivation: 线性注意力转换器因其高效性成为Softmax注意力的一个强有力替代方案，但其表达力较弱且准确性较低。为缩小Softmax注意力与线性注意力之间的准确性差距，研究人员对一种非常强大的线性注意力变体Mamba-2进行了调整。

Method: 首先将Mamba-2简化为其最基本和最重要的组件，并评估哪些具体选择使其最为准确；基于简化的Mamba-2S，进一步改进A-mask并增加隐藏状态的阶数，开发出称为2Mamba的方法。

Result: 开发出的2Mamba方法几乎与Softmax注意力一样准确，但在处理长上下文时内存效率显著提高。此外，还探讨了使Mamba-2超越Softmax注意力准确性的因素。

Conclusion: 通过简化Mamba-2并对其关键组件进行改进，成功地开发出了既高效又高准确度的新方法2Mamba，它在长序列任务中表现出色。

Abstract: Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments

</details>


### [54] [A feature-stable and explainable machine learning framework for trustworthy decision-making under incomplete clinical data](https://arxiv.org/abs/2602.17364)
*Justyna Andrys-Olek,Paulina Tworek,Luca Gherardini,Mark W. Ruddock,Mary Jo Kurt,Peter Fitzgerald,Jose Sousa*

Main category: cs.LG

TL;DR: 研究提出了一种名为CACTUS的可解释机器学习框架，旨在解决小规模、异质性及不完整临床数据集中的鲁棒性差、解释性有限和特征不稳定的问题。通过与随机森林等方法比较，在实际血尿队列中，CACTUS展示了在预测性能上具有竞争力的同时保持了较高水平的特征稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型虽然在生物医学数据上有广泛应用，但由于鲁棒性差、解释性有限以及在真实数据扰动下（如缺失值）特征的不稳定性，其在高风险领域的应用仍受到限制。特别是当关键特征随数据完整性变化而波动时，即使模型具有高预测性能也可能无法赢得信任，从而影响可重复性和下游决策。

Method: 提出了一个名为CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures) 的可解释机器学习框架，该框架整合了特征抽象、可解释分类和系统化的特征稳定性分析，用于量化随着数据质量下降时持续信息特征的一致性保存情况。

Result: 在包含568名接受膀胱癌评估患者的现实世界血尿队列中，将CACTUS与包括随机森林和梯度提升方法在内的广泛使用的机器学习方法进行了基准测试，并在控制引入的随机缺失数据水平下进行。结果显示，随着缺失程度增加，CACTUS不仅达到了竞争性的或更优的预测性能，而且保持了显著更高的顶级特征稳定性。

Conclusion: 特征稳定性提供了传统性能指标之外的信息，并且对于评估应用于生物医学数据的机器学习模型的信任度至关重要。通过明确量化对缺失数据的鲁棒性并优先考虑可解释、稳定的特征，CACTUS为可信的数据驱动决策支持提供了一个通用框架。

Abstract: Machine learning models are increasingly applied to biomedical data, yet their adoption in high stakes domains remains limited by poor robustness, limited interpretability, and instability of learned features under realistic data perturbations, such as missingness. In particular, models that achieve high predictive performance may still fail to inspire trust if their key features fluctuate when data completeness changes, undermining reproducibility and downstream decision-making. Here, we present CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures), an explainable machine learning framework explicitly designed to address these challenges in small, heterogeneous, and incomplete clinical datasets. CACTUS integrates feature abstraction, interpretable classification, and systematic feature stability analysis to quantify how consistently informative features are preserved as data quality degrades. Using a real-world haematuria cohort comprising 568 patients evaluated for bladder cancer, we benchmark CACTUS against widely used machine learning approaches, including random forests and gradient boosting methods, under controlled levels of randomly introduced missing data. We demonstrate that CACTUS achieves competitive or superior predictive performance while maintaining markedly higher stability of top-ranked features as missingness increases, including in sex-stratified analyses. Our results show that feature stability provides information complementary to conventional performance metrics and is essential for assessing the trustworthiness of machine learning models applied to biomedical data. By explicitly quantifying robustness to missing data and prioritising interpretable, stable features, CACTUS offers a generalizable framework for trustworthy data-driven decision support.

</details>


### [55] [Variational Grey-Box Dynamics Matching](https://arxiv.org/abs/2602.17477)
*Gurjeet Sangra Singh,Frantzeska Lavda,Giangiacomo Mercatali,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 该论文提出了一种新的灰盒方法，将不完全的物理模型直接整合到生成模型中，通过仅从观测轨迹学习动力学，并避免了神经ODE的可扩展性和稳定性问题。实验表明，该方法在保持物理模型可解释性的同时，表现优于或等同于完全数据驱动的方法和先前的灰盒基线。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型如流匹配和扩散模型在学习复杂分布和动态系统方面表现出巨大潜力，但通常作为黑盒操作，忽视了潜在的物理特性。相反地，基于ODE/PDE描述的物理模拟模型虽然可解释，但由于缺少或未知的项而无法完全描述现实世界的现象。为解决这一问题，本文旨在开发一种能够结合不完全物理模型与生成模型的新方法，从而同时保留物理上的可解释性并增强模型的学习能力。

Method: 研究者们提出了一种新颖的灰盒方法，它能够在无需真实物理参数的情况下，仅从观测轨迹中学习动力学，且整个过程无需进行模拟。该方法的核心在于利用流匹配框架内的一种结构化变分分布建模方式，通过两个潜编码来实现：一个用于建模缺失的随机性和多模态速度，另一个则以物理信息先验的方式编码物理参数为潜变量。此外，还介绍了该框架如何适应处理二阶动力学。

Result: 实验结果表明，在代表性的ODE/PDE问题上，所提方法的表现与完全数据驱动的方法及之前的灰盒基准相比，要么相当要么更优，同时保持了物理模型的可解释性。

Conclusion: 这项工作展示了一种创新的灰盒方法，成功地将不完整的物理知识融入到了深度生成模型之中，不仅提升了模型对于复杂动态系统的理解能力，而且保持了模型的可解释性。这为未来结合物理原理与机器学习技术的研究提供了新的视角。

Abstract: Deep generative models such as flow matching and diffusion models have shown great potential in learning complex distributions and dynamical systems, but often act as black-boxes, neglecting underlying physics. In contrast, physics-based simulation models described by ODEs/PDEs remain interpretable, but may have missing or unknown terms, unable to fully describe real-world observations. We bridge this gap with a novel grey-box method that integrates incomplete physics models directly into generative models. Our approach learns dynamics from observational trajectories alone, without ground-truth physics parameters, in a simulation-free manner that avoids scalability and stability issues of Neural ODEs. The core of our method lies in modelling a structured variational distribution within the flow matching framework, by using two latent encodings: one to model the missing stochasticity and multi-modal velocity, and a second to encode physics parameters as a latent variable with a physics-informed prior. Furthermore, we present an adaptation of the framework to handle second-order dynamics. Our experiments on representative ODE/PDE problems show that our method performs on par with or superior to fully data-driven approaches and previous grey-box baselines, while preserving the interpretability of the physics model. Our code is available at https://github.com/DMML-Geneva/VGB-DM.

</details>


### [56] [Linear Convergence in Games with Delayed Feedback via Extra Prediction](https://arxiv.org/abs/2602.17486)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 本研究通过在无约束双线性博弈中引入加权乐观梯度下降-上升（WOGDA）算法，证明了其在线性反馈延迟下的收敛速度。特别地，研究发现额外乐观（预测更远未来的奖励）可以容忍更大的步长，并显著加速收敛速率至$\exp(-\u0398(t/(m^2\\log m)))$。实验结果与理论一致，表明额外乐观是应对由反馈延迟引起性能下降的有效对策。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多智能体学习不可避免地存在反馈延迟问题，这已知会严重降低性能，而带有延迟反馈的收敛率即使对于双线性博弈也仍然不清楚。因此，研究旨在探索一种能够对抗由于反馈延迟导致性能下降的方法。

Method: 提出了加权乐观梯度下降-上升(WOGDA)算法，该算法通过额外乐观来预测未来奖励。此外，还从理论上分析了WOGDA算法作为更远未来奖励基础上更新的Extra Proximal Point (EPP)的一种近似形式的行为表现。

Result: 研究表明，标准乐观（即预测下一步奖励）在$t$次迭代后以$\exp(-\u0398(t/m^5))$的速度向均衡点线性收敛；而采用额外乐观则可接受更大的步长并显著加快收敛速度达到$\exp(-\u0398(t/(m^2\\log m)))$。实验观察到的结果与理论分析相符。

Conclusion: 额外乐观是一种有前途的方法，可用于缓解因反馈延迟造成的性能退化问题。

Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-Θ(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-Θ(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.

</details>


### [57] [LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights](https://arxiv.org/abs/2602.17510)
*Kasun Dewage,Marianna Pensky,Suranadi De Silva,Shankadeep Mondal*

Main category: cs.LG

TL;DR: CRAFT是一种参数高效的微调方法，通过对预训练的注意力权重矩阵进行Tucker张量分解，并仅在得到的冻结Tucker因子上训练小型平方适应矩阵，实现了与现有方法相当的性能，同时大幅减少了所需参数数量。


<details>
  <summary>Details</summary>
Motivation: 为了提高模型微调过程中的参数效率，减少需要训练的新参数数量，同时保持模型性能。

Method: 通过将跨层堆叠的预训练注意力权重矩阵组织成3D张量，并对其进行全Tucker分解（使用HOSVD），然后冻结所有生成的因素矩阵，再对每个因素矩阵应用轻量级可训练变换来调整模型。

Result: 实验表明，在GLUE基准测试中使用RoBERTa-base和RoBERTa-large时，CRAFT能够以固定Tucker秩的情况下，独立于模型维度和深度，仅需41K个Tucker适应参数就达到与其他方法相竞争的表现。

Conclusion: CRAFT提供了一种有效的方法来减少微调大型语言模型所需的参数数量，同时保持良好的性能，这为更高效地利用计算资源提供了可能。

Abstract: We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $ΔW$ across layers before applying Tucker decomposition. Separately, methods like PiSSA apply SVD to pre-trained weights but operate independently per layer. CRAFT bridges these two lines of work: it performs full Tucker decomposition via Higher-Order SVD (HOSVD) directly on pre-trained weights organized as cross-layer 3D tensors, freezes all resulting factors, and adapts the model through lightweight trainable transformations applied to each factor matrix. Experiments on the GLUE benchmark using RoBERTa-base and RoBERTa-large demonstrate that CRAFT achieves competitive performance with existing methods while requiring only 41K Tucker adaptation parameters--a count independent of model dimension and depth at fixed Tucker ranks.

</details>


### [58] [Variational inference via radial transport](https://arxiv.org/abs/2602.17525)
*Luca Ghafourpour,Sinho Chewi,Alessio Figalli,Aram-Alexandre Pooladian*

Main category: cs.LG

TL;DR: 本文提出了一种新的变分推理(radVI)算法，该算法通过优化径向剖面来改善高斯分布近似复杂分布时的覆盖效果。此方法可以作为现有多种变分推理方案如高斯均场变分推理和拉普拉斯近似的有效补充，并且提供了理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 在变分推理中，使用高斯分布去近似一个高维分布π往往不能准确捕捉到π的径向轮廓，从而导致较差的覆盖效果。为了解决这个问题，研究者们从优化径向轮廓的角度出发，提出了新的方法。

Method: 提出了一种名为radVI的新算法，该算法旨在通过优化目标分布的径向轮廓来改进现有的变分推理技术。它基于Wasserstein空间上的优化进展以及类似Caffarelli (2000)风格的径向传输映射的新规律性属性提供支持。

Result: 研究表明，radVI算法能够以较低的成本有效地增强许多现存变分推理方案的表现，包括但不限于高斯（均场）变分推理与拉普拉斯逼近方法。此外，还为所提算法给出了理论上的收敛性保障。

Conclusion: 通过引入radVI算法，在不显著增加计算成本的前提下提高了变分推理对复杂分布的近似精度，特别是对于那些具有非典型径向特性的分布来说尤为有效。

Abstract: In variational inference (VI), the practitioner approximates a high-dimensional distribution $π$ with a simple surrogate one, often a (product) Gaussian distribution. However, in many cases of practical interest, Gaussian distributions might not capture the correct radial profile of $π$, resulting in poor coverage. In this work, we approach the VI problem from the perspective of optimizing over these radial profiles. Our algorithm radVI is a cheap, effective add-on to many existing VI schemes, such as Gaussian (mean-field) VI and Laplace approximation. We provide theoretical convergence guarantees for our algorithm, owing to recent developments in optimization over the Wasserstein space--the space of probability distributions endowed with the Wasserstein distance--and new regularity properties of radial transport maps in the style of Caffarelli (2000).

</details>


### [59] [The Anxiety of Influence: Bloom Filters in Transformer Attention Heads](https://arxiv.org/abs/2602.17526)
*Peter Balogh*

Main category: cs.LG

TL;DR: 研究发现，某些Transformer注意力头在四个语言模型中作为成员测试器工作，能够以高精度检测上下文中是否出现过某个标记。这些成员测试头形成了一种多分辨率系统，不仅对重复的名字有反应，还对任何类型的重复标记有响应，并且它们对于处理重复和新奇的标记都有贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索Transformer架构中的特定注意力头如何专门用于检测当前处理的标记是否之前已在上下文中出现过。通过分析GPT-2系列及Pythia-160M模型内的注意力头行为，研究人员希望揭示这些“成员测试”头的工作机制及其在模型中的作用。

Method: 通过对GPT-2小型、中型、大型以及Pythia-160M四个语言模型的研究，研究人员确定了几个专注于成员测试功能的注意力头。他们进一步分析了这些头的具体表现形式，包括误报率与上下文标记数量间的关系等，并通过消融实验验证了这些头的功能。

Result: 研究结果表明，在GPT-2小型模型中发现了三个真正的成员测试注意力头，其中两个表现出极低的误报率；另一个则遵循经典布隆过滤器的容量曲线。此外，原本被归类为布隆过滤器的一个注意力头（L3H0）经过控制混淆因素后被重新分类为一般的前缀注意力头。这些成员测试头显示出比仅关注重复标记的头更高的泛化能力，并且在处理重复和新颖标记时都发挥了作用。

Conclusion: 本研究表明，一些Transformer注意力头确实执行着类似成员测试的任务，它们不仅能够准确地识别出上下文中已存在的标记，而且对于不同类型的重复标记也具有良好的响应性。这表明，这类注意力头在语言模型中扮演着重要的角色，既有助于重复信息的处理也有助于新信息的学习。

Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question "has this token appeared before in the context?" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\% even at 180 unique context tokens -- well above the $d_\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \approx 5$ bits, saturating by $n \approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.

</details>


### [60] [Position: Evaluation of ECG Representations Must Be Fixed](https://arxiv.org/abs/2602.17531)
*Zachary Berger,Daniel Prakah-Asante,John Guttag,Collin M. Stultz*

Main category: cs.LG

TL;DR: 本文提出当前12导联心电图表示学习的基准测试实践需要改进，以确保进展可靠且符合临床有意义的目标。建议扩展下游评估以包括结构性心脏病和患者水平预测等，并指出在多标签、不平衡设置中应用最佳评估实践时，文献中关于哪种表示方法表现最佳的结论会发生变化。此外，随机初始化编码器在线性评估中的表现与最先进的预训练相当，这支持了使用随机编码器作为合理基线模型的观点。


<details>
  <summary>Details</summary>
Motivation: 现有的12导联心电图表示学习领域主要集中在几个公开的多标签基准上，这些基准主要关注心律失常和波形形态标记，而忽略了心电图所包含的更广泛的临床信息。因此，作者认为当前的基准测试实践需要调整，以便更好地反映临床实际需求，并促进该领域的健康发展。

Method: 通过扩大对结构性心脏病及患者层面预测等额外临床目标的评估范围，并针对多标签、数据不平衡的情况提出最佳评估实践建议。同时，通过对三种代表性的心电图预训练方法在六个不同评估场景下的实证研究来验证其观点。

Result: 当采用推荐的最佳评估实践时，对于哪些表征方法最有效的现有结论发生了改变。令人惊讶的是，在许多任务上，一个带有线性评估的随机初始化编码器能够与最先进的预训练技术相媲美。这意味着可以考虑将随机编码器作为合理的基准模型。

Conclusion: 为了使12导联心电图表示学习的研究进步更加可靠并紧密联系临床意义，有必要拓宽评估范围至更多样的临床终点。此外，研究还表明即使是简单的随机初始化编码器也能提供强大的性能，这为未来的工作提供了新的视角。

Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting.

</details>


### [61] [MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning](https://arxiv.org/abs/2602.17550)
*Xiaoliang Fu,Jiaye Lin,Yangyi Fang,Binbin Zheng,Chaowen Hu,Zekai Shao,Cong Qin,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架MASPO，旨在解决现有RLVR算法在大型语言模型优化中的三个关键挑战。实验表明，MASPO相比基线方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的基于验证奖励的强化学习(RLVR)算法，如GRPO，在处理大型语言模型时存在效率低下、概率质量不敏感以及信号可靠性不对称的问题。

Method: 提出了Mass-Adaptive Soft Policy Optimization (MASPO)，一个统一框架，通过引入可微软高斯门控机制来最大化梯度利用效率；使用质量自适应限制器平衡探索与利用；并通过非对称风险控制器调整更新幅度以匹配信号置信度。

Result: 广泛的评估显示，MASPO作为一种鲁棒且全面的RLVR解决方案，其性能明显优于强大的基准方法。

Conclusion: MASPO提供了一个有效解决大型语言模型中遇到的特定挑战的方法，并且在实践中表现出色。

Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.

</details>


### [62] [A Theoretical Framework for Modular Learning of Robust Generative Models](https://arxiv.org/abs/2602.17554)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 该论文提出了一种模块化生成模型的理论框架，通过预训练专家和门控机制相结合的方式，旨在解决大规模语言模型训练资源密集以及数据集权重调整过于依赖启发式方法的问题。研究者证明了存在一种鲁棒性的门控函数可以最小化与最坏情况下的数据混合的差异，并且这种模块化方法在理论上可以优于基于聚合数据重新训练的模型。此外，还引入了一种可扩展的随机原始对偶算法和结构蒸馏方法来提高推理效率。实验结果表明，提出的模块化架构能够有效缓解梯度冲突问题，并且相对于单一基线模型表现出更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对大规模生成模型训练成本高及数据集权重调整高度依赖于经验规则的问题，探讨是否可以通过组合小型、领域特定的专家模型来达到与单一大型模型相媲美的性能，并且这一过程对于任何数据混合都足够稳健，从而消除掉经验性的调优需求。

Method: 构建了一个理论框架用于模块化的生成建模，其中一组预训练好的专家模型通过某种门控机制被结合在一起。定义了标准化门控函数空间G1，并将问题形式化为一个极小极大博弈，以找到一个能最小化与最糟糕的数据混合之间差异的鲁棒门控。利用Kakutani不动点定理证明了这种鲁棒门控的存在性，并展示了模块化作为一种强有力的正则化手段的作用。另外，提出了一个可扩展的随机原对偶算法和结构蒸馏方法。

Result: 实验证明所提出的模块化架构能够有效地减轻梯度冲突，并且相对于单一体系基线而言，在合成数据集和真实世界数据集中均表现出了更佳的鲁棒性和超越性能。

Conclusion: 这项工作不仅提供了一种新的方式来构建和训练大型语言模型，使其更加高效和灵活，而且还在理论上证明了这种方法的有效性。通过引入模块化思想，不仅可以减少训练所需资源，同时还能提高模型面对不同数据分布时的表现稳定性。

Abstract: Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.

</details>


### [63] [Revisiting Weight Regularization for Low-Rank Continual Learning](https://arxiv.org/abs/2602.17559)
*Yaoyue Zheng,Yin Zhang,Joost van de Weijer,Gido M van de Ven,Shaoyi Du,Xuetao Zhang,Zhiqiang Tian*

Main category: cs.LG

TL;DR: 本文提出了一种名为EWC-LoRA的新方法，该方法通过在低秩持续学习中应用权重正则化（如EWC）来缓解任务干扰，从而为基于预训练模型的参数高效持续学习提供了一个计算和内存效率高的解决方案。


<details>
  <summary>Details</summary>
Motivation: 虽然参数高效持续学习(PECL)已经成为一种有前景的范式，但权重正则化技术在这一新范式中的应用仍较少被探索。为了进一步减轻任务间的干扰，作者们希望重新审视如何在低秩持续学习环境中有效地运用权重正则化策略。

Method: 提出了EWC-LoRA方法，它通过EWC对共享低秩更新进行正则化处理，以此来控制任务间干扰问题。此法利用了低秩表示来估计整个维度空间上的参数重要性，并且保持了存储需求和推理成本不随任务数量增加而变化的特点。

Result: 广泛的实验表明，与现有的低秩持续学习方法相比，EWC-LoRA能够在稳定性-可塑性权衡方面表现出更优性能。这证明了即使是在低秩参数化条件下，权重正则化依然是缓解任务干扰的有效机制。

Conclusion: 研究结果强调了即使采用低秩参数化方案时，权重正则化依旧能够有效减少任务干扰，同时EWC-LoRA作为一种新颖的方法，在实现参数高效持续学习方面展现出了显著优势。

Abstract: Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space. This design offers a practical, computational- and memory-efficient solution for CL with PTMs, and provides insights that may inform the broader application of regularization techniques within PECL. Extensive experiments on various benchmarks demonstrate the effectiveness of EWC-LoRA, achieving a stability-plasticity trade-off superior to existing low-rank CL approaches. These results indicate that, even under low-rank parameterizations, weight regularization remains an effective mechanism for mitigating task interference. Code is available at: https://github.com/yaoyz96/low-rank-cl.

</details>


### [64] [Be Wary of Your Time Series Preprocessing](https://arxiv.org/abs/2602.17568)
*Sofiane Ennadir,Tianze Wang,Oleg Smirnov,Sahar Asadi,Lele Cao*

Main category: cs.LG

TL;DR: 本文首次从理论上分析了不同的归一化策略（实例基础和全局缩放）对基于Transformer架构的时间序列表示学习表达能力的影响，提出了一种新的时间序列表达能力框架，并通过理论推导和实证验证了标准化和最小-最大缩放两种常用方法的理论边界。研究结果表明，没有一种归一化方法能够始终优于其他方法，在某些情况下省略归一化步骤反而能获得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然归一化和缩放是时间序列建模中的基本预处理步骤，但它们在基于Transformer模型中的作用尚未从理论角度充分探索。因此，有必要深入研究不同归一化策略如何影响这类模型的表现力，以提高时间序列学习任务的效果。

Method: 1. 提出了一个专门针对时间序列设计的新表达力框架，用于量化模型区分相似与不相似输入的能力。
2. 为两种广泛使用的归一化方法——标准缩放和最小-最大缩放——推导出理论界限。
3. 在分类和预测基准上使用多个基于Transformer的模型进行实验验证。

Result: 研究表明，归一化策略的选择可以显著影响模型的表征能力，这取决于具体的任务和数据特征。没有任何一种归一化方法能够在所有场景下都表现出色；有时候，完全不应用归一化反而会得到更好的结果。

Conclusion: 本研究强调了预处理步骤对于时间序列学习的重要性，并指出需要开发更加原则性的、针对特定任务和数据集定制的归一化策略。

Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.

</details>


### [65] [Towards Anytime-Valid Statistical Watermarking](https://arxiv.org/abs/2602.17608)
*Baihe Huang,Eric Xu,Kannan Ramchandran,Jiantao Jiao,Michael I. Jordan*

Main category: cs.LG

TL;DR: 本文提出了Anchored E-Watermarking框架，这是首个基于e值的水印技术，它统一了最优采样与任意时间有效的推断，解决了现有方法在选择采样分布上的随意性以及固定范围假设检验的问题。通过模拟和基准测试验证，该框架能够显著提高样本效率，将检测所需的平均令牌预算相对减少了13-15%。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的普及，区分机器生成内容与人类文本的需求日益增长。尽管统计水印技术展现出了潜力，但现有方法存在两大关键限制：缺乏选择采样分布的原则性方法，以及依赖于不允许提前有效停止的固定期限假设检验。

Method: 研究者们开发了一种名为Anchored E-Watermarking的新框架，该框架是首次基于e值设计的水印方案，旨在结合最佳采样策略与任意时刻都有效的推理过程。此方法通过使用锚定分布来近似目标模型，从而确定关于最坏情况对数增长率的最佳e值，并推导出最优预期停止时间。

Result: 通过仿真分析及标准基准测试表明，提出的方法能够大幅提升样本利用效率，在保持检测准确性的同时，相比目前最先进的基线方法而言，所需用于检测的平均令牌数降低了13至15个百分点。

Conclusion: Anchored E-Watermarking框架为识别机器生成文本提供了一个更高效、灵活的解决方案，它不仅克服了传统方法中存在的问题，还通过优化采样策略实现了资源的有效节约。

Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.

</details>


### [66] [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs](https://arxiv.org/abs/2602.17616)
*Luke Huang,Zhuoyang Zhang,Qinghao Hu,Shang Yang,Song Han*

Main category: cs.LG

TL;DR: 该论文提出了一种名为VCPO的新方法，旨在通过控制策略梯度的方差来提高异步强化学习训练中REINFORCE/GRPO类算法的稳定性。实验证明VCPO在数学、一般推理和工具使用任务中的异步训练上显著提高了鲁棒性，并且与同步性能相匹配的同时将长上下文多轮训练时间减少了2.5倍。


<details>
  <summary>Details</summary>
Motivation: 研究者们注意到，在广泛采用的无批评家策略梯度方法（如REINFORCE和GRPO）中，高异步性会导致策略梯度估计器出现更高的方差：基于过时的序列进行训练会产生重尾的重要性比率，使得一小部分样本主导更新过程。这导致了梯度噪声增大以及相对于匹配的在线策略训练来说学习变得不稳定。此外，他们发现有效样本大小(ESS)和不稳定的梯度范数能够可靠地预测崩溃现象。

Method: 为了解决上述问题，本文提出了一个名为Variance Controlled Policy Optimization (VCPO)的方法。VCPO包含两个主要组成部分：(i) 根据有效样本大小调整学习率以减弱不可靠更新的影响；(ii) 应用一种针对离线策略环境下的封闭形式最小方差基线，避免使用辅助价值模型并保持较低的额外开销。

Result: 实验结果显示，VCPO在包括数学、一般推理及工具使用在内的多个任务上对于异步训练表现出显著增强的鲁棒性，超越了一系列涵盖掩码/剪辑稳定器和算法变体的基准方法。特别是，这种方法能够在达到与同步训练相同效果的同时，将长时间多轮对话场景下的训练时间减少至原来的40%。

Conclusion: 研究表明，通过显式控制策略梯度方差是实现大规模可靠异步强化学习的关键。VCPO提供了一个通用的解决方案来稳定REINFORCE/GRPO风格算法，从而使得它们更适合应用于需要高效利用计算资源的实际场景。

Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.

</details>


### [67] [When to Trust the Cheap Check: Weak and Strong Verification for Reasoning](https://arxiv.org/abs/2602.17633)
*Shayan Kiyani,Sima Noorani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: 本文探讨了大语言模型（LLMs）推理过程中弱验证与强验证的使用策略。提出了一种在线算法，能够在不对查询流、语言模型或弱验证器做出假设的情况下，控制接受和拒绝错误。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的推理越来越多地嵌入到一个更广泛的验证循环中，如何平衡内部快速但不完全可靠的弱验证与外部可靠但资源密集型的强验证成为了一个亟待解决的问题。

Method: 通过定义弱-强验证策略来决定何时基于弱验证接受或拒绝结果，以及何时需要依赖于强验证。引入了衡量错误接受、错误拒绝及强验证频率的指标，并展示了最优策略具有双阈值结构。基于这些发现，开发了一种在线算法以控制接受和拒绝错误。

Result: 研究表明，校准度和清晰度决定了弱验证器的价值；所提出的在线算法能够在没有任何预设条件下有效管理接受和拒绝错误。

Conclusion: 该研究为理解和优化大语言模型在实际应用中的验证过程提供了新的视角，特别是关于如何高效结合弱验证与强验证方法以提高整体性能。

Abstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.

</details>


### [68] [Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting](https://arxiv.org/abs/2602.17634)
*Xinghong Fu,Yanhong Li,Georgios Papaioannou,Yoon Kim*

Main category: cs.LG

TL;DR: 本文介绍了一种学习高效基础模型的方法，用于零样本时间序列预测。这些模型比大型Transformer模型小很多倍，但通过结合长卷积和线性RNN层（特别是DeltaNet层）能达到类似的性能，并且更加经济实用。此外，还提出了几种数据增强和推理策略来进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模Transformer在其他模态如语言和视觉上展示了通过扩展提高性能的重要性，但在时间序列领域，构建具有数亿参数的基础模型虽然表现出色，却非常低效且成本高昂。因此，研究旨在探索一种更高效、规模更小但仍能保持高性能的时间序列基础模型解决方案。

Method: 提出了一种简单的方法，利用混合模型交替使用长卷积与线性RNN层（特别是DeltaNet层），以实现与较大的基于Transformer的模型相媲美的性能，同时模型大小仅为后者的百分之一。另外，研究还探讨了几种数据增强及推理策略，以进一步优化模型表现。

Result: 实验证明了小型混合模型能够在显著减小模型尺寸的同时匹配大型Transformer模型的性能，且效率更高。提出的Reverso模型家族在推进性能-效率帕累托前沿方面取得了重要进展。

Conclusion: 研究表明，对于零样本时间序列预测任务而言，不需要依赖于大规模的Transformer架构；通过采用特定设计的小型混合模型可以有效地达到相似甚至更好的结果，同时极大地降低了实际应用中的成本与资源需求。

Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.

</details>


### [69] [A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning](https://arxiv.org/abs/2602.17642)
*Dhruv Talwar,Harsh Desai,Wendong Yin,Goutam Mohanty,Rafael Reveles*

Main category: cs.LG

TL;DR: 本文介绍了一种名为A.R.I.S.的低成本便携式电子废物分拣系统，通过结合YOLOx模型实现实时分类金属、塑料和电路板材料，提高了回收效率和分类纯度。


<details>
  <summary>Details</summary>
Motivation: 传统电子回收过程因材料分离和识别能力不足而导致资源损失严重，限制了材料回收率。

Method: 开发了A.R.I.S.（自动化回收识别系统），一种低成本且便携式的碎电子废料分选器，利用YOLOx模型实时分类金属、塑料和电路板，并达到低推理延迟与高检测精度。

Result: 实验评估显示整体精度为90%，平均精度均值(mAP)为82.2%，以及84%的分选纯净度。

Conclusion: 通过将深度学习与现有分拣方法相结合，A.R.I.S.提高了材料回收效率并降低了采用先进回收技术的门槛，支持产品生命周期延长、换购及回收计划等更广泛的举措以减少供应链中的环境影响。

Abstract: Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.

</details>


### [70] [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting](https://arxiv.org/abs/2602.17645)
*Xiaohan Zhao,Zhaoyi Li,Yaxin Luo,Jiacheng Cui,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 本文提出了一种改进的黑盒对抗攻击方法M-Attack-V2，通过引入多裁剪对齐（MCA）和辅助目标对齐（ATA）等技术来减少梯度方差、平滑目标流形，并显著提高了针对前沿大型视觉语言模型（LVLMs）的成功攻击率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于转移的黑盒对抗攻击方法在处理大型视觉语言模型时存在高方差、几乎正交的梯度问题，这导致了局部对齐不稳定和优化困难。这些问题主要归因于ViT对于翻译敏感以及源与目标裁剪之间的结构不对称性。

Method: 作者提出了M-Attack-V2，一个基于M-Attack升级版的方法。它包括两个关键组件：1) 多裁剪对齐(Multi-Crop Alignment, MCA)，通过平均来自多个独立采样局部视图的梯度以减少方差；2) 辅助目标对齐(Auxiliary Target Alignment, ATA)，用语义相关分布的小型辅助集替换激进的目标增强，从而生成更平滑、更低方差的目标流形。此外，还重新解释了动量作为Patch Momentum，回放历史裁剪梯度；结合精炼的补丁大小集成(PE+)，进一步加强可传递方向。

Result: M-Attack-V2在多种先进大型视觉语言模型上实现了显著更高的成功攻击率，比如将Claude-4.0上的成功率从8%提升到了30%，Gemini-2.5-Pro从83%到97%，GPT-5则从98%达到了100%，超过了之前所有针对LVLMs的黑盒攻击表现。

Conclusion: M-Attack-V2作为一种简单且模块化的改进方案，在减少梯度噪声、提高局部匹配稳定性方面表现出色，有效提升了对最新大型视觉语言模型进行黑盒对抗攻击的成功率。

Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.

</details>


### [71] [Multi-Round Human-AI Collaboration with User-Specified Requirements](https://arxiv.org/abs/2602.17646)
*Sima Noorani,Shayan Kiyani,Hamed Hassani,George Pappas*

Main category: cs.LG

TL;DR: 本文提出了一种基于人类中心视角的框架，通过确保AI不削弱人类优势（反事实伤害原则）和在人类易出错的地方增加价值（互补性原则）来提高多轮对话AI决策的质量。该框架允许用户定义规则，并引入了一个在线、无分布算法以保证用户指定约束条件下的合作动态。实验结果表明，即使在非平稳交互下，该方法也能保持规定的反事实伤害和互补性违规率，并且调整这些约束可以预测性地改变最终的人类准确性，从而在不需要建模或限制人类行为的情况下提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越依赖于多轮对话AI进行重要决策，需要有原则性的框架来确保这种互动可靠地改善决策质量。本文从以人为本的角度出发，提出了两个原则：反事实伤害（确保AI不会削弱人类的优势）和互补性（确保AI能在人类容易出错的地方增加价值）。

Method: 本文首先将这两个概念通过用户自定义规则的形式化表达，让用户能够针对特定任务明确定义什么是伤害与互补性。接着，介绍了一种在线且无需事先知道数据分布的算法，该算法能够在合作过程中强制执行用户设定的约束条件，并且对样本数量有限的情况给出了性能保证。

Result: 通过对两种互动场景——医学诊断任务上的LLM模拟协作以及一项图形推理任务中的人群外包研究——的评估，展示了即使在交互动态变化的情况下，所提出的在线过程也能够维持预定的反事实伤害及互补性违规比率。此外，调整这些约束条件会导致下游人类准确性的可预见变动。

Conclusion: 研究表明，通过控制反事实伤害和互补性原则，可以在不直接建模或限制人类行为的前提下，有效地引导多轮次协作朝着更好的决策质量发展。

Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.

</details>


### [72] [MARS: Margin-Aware Reward-Modeling with Self-Refinement](https://arxiv.org/abs/2602.17658)
*Payel Bhattacharjee,Osvaldo Simeone,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文提出了一种名为MARS的自适应、边界感知的数据增强和采样策略，专门针对奖励模型中的模糊和失败模式。通过在低边界（即模型最不确定的）偏好对上集中进行数据增强，并通过困难样本增强来迭代优化训练分布，从而提高奖励模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据增强方法通常在表示或语义层面操作，忽略了奖励模型估计难度的问题。由于训练可靠的奖励模型依赖于昂贵且有限的人类标注偏好数据，因此需要一种新的数据增强方法来解决这个问题。

Method: 提出了MARS框架，该框架是一种自适应的、边界感知的数据增强与采样策略，特别关注奖励模型中不确定性最高的低边界偏好对，并通过困难样本增强技术迭代地改进训练数据分布。

Result: 理论分析表明，所提出的策略能够增加损失函数的平均曲率，进而提高信息量并改善条件；实验结果也显示，与均匀增强相比，MARS在鲁棒奖励建模方面具有持续的优势。

Conclusion: MARS提供了一种有效的方法来应对奖励模型训练过程中遇到的模糊性和失败情况，为实现更可靠和鲁棒的奖励模型提供了新的途径。

Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [73] [Read-Modify-Writable Snapshots from Read/Write operations](https://arxiv.org/abs/2602.16903)
*Armando Castañeda,Braulio Ramses Hernández Martínez*

Main category: cs.DC

TL;DR: 本文探讨了仅使用读/写操作实现RMWable快照的可能性，并提出了两种基于读/写操作的RMWable快照算法，分别适用于进程数量已知且有限的标准并发共享内存模型和具有无限并发性的变体模型。


<details>
  <summary>Details</summary>
Motivation: 在异步并发共享内存系统中，虽然已经提出了允许进程执行比简单读写更复杂操作的RMWable快照算法，但这些算法通常依赖于如比较并交换或加载链接/存储条件等强大的底层操作。因此，研究者们希望探索是否可以仅通过简单的读/写低级操作来实现RMWable快照。

Method: 研究者开发出了两种仅基于读/写操作的RMWable快照算法：一种针对的是标准的并发共享内存模型，在该模型中参与者的数量是有限且事先已知的；另一种则面向一个具有无界并发性的标准模型变种，其中尽管存在无限多的进程，但在任何时刻只有有限数目的进程参与到执行过程中。

Result: 成功地提出了两种新的RMWable快照算法，证明了即使是在限制到只使用读/写这类相对弱的操作的情况下，依然能够支持RMWable快照特性。

Conclusion: 这项研究表明，即便不依赖于更为复杂的底层原语（如比较并交换），也能够在不同类型的并发环境下实现RMWable快照。

Abstract: In the context of asynchronous concurrent shared-memory systems, a snapshot algorithm allows failure-prone processes to concurrently and atomically write on the entries of a shared array MEM , and also atomically read the whole array. Recently, Read-Modify-Writable (RMWable) snapshot was proposed, a variant of snapshot that allows processes to perform operations more complex than just read and write, specifically, each entry MEM[k] is an arbitrary readable object. The known RMWable snapshot algorithms heavily rely on powerful low-level operations such as compare&swap or load-link/store-conditional to correctly produce snapshots of MEM. Following the large body of research devoted to understand the limits of what can be solved using the simple read/write low-level operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, we explore if RMWable snapshots are possible using only read/write operations. We present two read/write RMWable snapshot algorithms, the first one in the standard concurrent shared-memory model where the number of processes n is finite and known in advance, and the second one in a variant of the standard model with unbounded concurrency, where there are infinitely many processes, but at any moment only finitely many processes participate in an execution.

</details>


### [74] [Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation](https://arxiv.org/abs/2602.16936)
*Zikai Zhang,Rui Hu,Jiahao Xu*

Main category: cs.DC

TL;DR: 提出了Fed-PLoRA，一种轻量级异构联邦微调框架，通过引入并行单秩适应(PLoRA)和Select-N-Fold策略来解决客户资源异质性导致的初始化和聚合噪声问题，从而在保持数据隐私的同时提高大语言模型（LLMs）在下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习（FL）方法在处理拥有不同资源的分布式客户端时，由于采用不同的LoRA等级，会产生大量的初始化和聚合噪声，影响系统整体性能。为了解决这个问题，并且同时保护数据隐私，避免原始数据共享，研究者们开发了Fed-PLoRA框架。

Method: Fed-PLoRA框架采用了两种关键技术：一是Parallel One-Rank Adaptation (PLoRA)，它使用多个并行的一秩模块替代传统的多秩LoRA模块；二是Select-N-Fold策略，在本地训练前将未经训练的PLoRA模块折叠到预训练权重中，以此来适应具有不同资源能力的客户端。

Result: 广泛的实验表明，Fed-PLoRA在各种大型语言模型（LLMs）微调任务上均优于现有方法，不仅提高了准确性还增强了效率。此外，对Fed-PLoRA进行了统一分析，展示了其如何解决当前最先进方法存在的限制。

Conclusion: Fed-PLoRA提供了一种有效的方法来克服联邦学习环境中由于客户端资源异质性带来的挑战，通过减少初始化与聚合过程中的噪音，实现了更好的性能表现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. The code is available at https://github.com/TNI-playground/Fed-PLoRA.

</details>


### [75] [Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads](https://arxiv.org/abs/2602.17318)
*Patrick Zojer,Jonas Posner,Taylan Özden*

Main category: cs.DC

TL;DR: 本研究评估了在高性能计算集群中采用可塑性作业调度策略的益处，通过动态调整运行时资源分配来优化资源利用率。基于Cori、Eagle和Theta超级计算机的实际工作负载跟踪模拟显示，即使只有20%的可塑性作业也能显著提高系统效率和用户满意度的关键指标。


<details>
  <summary>Details</summary>
Motivation: 传统刚性的作业调度方法导致了资源利用率低下以及作业等待时间增加的问题，因此需要探索新的调度策略以提高HPC系统的整体性能。

Method: 使用ElastiSim软件模拟不同比例（从0到100%）的可塑性作业，并评估五种不同的作业调度策略的效果，其中包括一种尽量保持可塑性作业在其首选资源配置的新策略。

Result: 相比于完全刚性的作业负载，采用可塑性作业能够显著改善所有关键指标：作业周转时间减少了37-67%，作业完成时间减少了16-65%，作业等待时间减少了73-99%，节点利用率提高了5-52%。即使在仅20%的作业为可塑性的情况下，这些改进仍然十分显著。

Conclusion: 研究结果表明，作业特性（如运行时间和节点需求）、可塑性比例以及调度策略之间存在重要关联。这证实了可塑性对于解决当前HPC实践中存在的低效问题具有巨大潜力，并且即使少量引入也可带来实质性的优势，鼓励将其整合进HPC资源管理中。

Abstract: Optimizing resource utilization in high-performance computing (HPC) clusters is essential for maximizing both system efficiency and user satisfaction. However, traditional rigid job scheduling often results in underutilized resources and increased job waiting times.
  This work evaluates the benefits of resource elasticity, where the job scheduler dynamically adjusts the resource allocation of malleable jobs at runtime. Using real workload traces from the Cori, Eagle, and Theta supercomputers, we simulate varying proportions (0-100%) of malleable jobs with the ElastiSim software.
  We evaluate five job scheduling strategies, including a novel one that maintains malleable jobs at their preferred resource allocation when possible. Results show that, compared to fully rigid workloads, malleable jobs yield significant improvements across all key metrics. Considering the best-performing scheduling strategy for each supercomputer, job turnaround times decrease by 37-67%, job makespan by 16-65%, job wait times by 73-99%, and node utilization improves by 5-52%. Although improvements vary, gains remain substantial even at 20% malleable jobs.
  This work highlights important correlations between workload characteristics (e.g., job runtimes and node requirements), malleability proportions, and scheduling strategies. These findings confirm the potential of malleability to address inefficiencies in current HPC practices and demonstrate that even limited adoption can provide substantial advantages, encouraging its integration into HPC resource management.

</details>


### [76] [Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction](https://arxiv.org/abs/2602.17610)
*Nicolau Manubens Gil*

Main category: cs.DC

TL;DR: 本研究评估了DAOS和Ceph两种对象存储系统对于ECMWF业务数值天气预报（NWP）以及HPC和AI应用的适用性和性能。通过开发新的软件级适配器，使ECMWF的NWP能够利用这些系统，并进行了广泛的I/O基准测试。结果显示，尽管两者都表现优异，但DAOS在可扩展性和灵活性方面优于Ceph和Lustre，为大规模I/O操作提供了更好的支持。


<details>
  <summary>Details</summary>
Motivation: 随着科学界与工业界的雄心不断增长，高性能计算（HPC）及人工智能（AI）应用如业务数值天气预报需要尽可能快速地处理并存储越来越多的数据量。虽然POSIX分布式文件系统与NVMe SSD是当前常见的HPC存储配置，但在过去十年中涌现或流行起来的新存储解决方案可能解决某些I/O工作负载下POSIX文件系统在规模上表现出的性能限制问题。

Method: 研究人员主要针对两个对象存储系统——即DAOS和Ceph——对ECMWF运营中的数值天气预报以及其他HPC和AI应用程序的适用性与性能进行了评估。为此开发了新的软件层面适配器以支持ECMWF NWP使用这些系统，并且在几台计算机系统上进行了广泛的I/O基准测试，将所评价的对象存储系统的性能与相同硬件条件下部署的Lustre文件系统的性能进行了对比。

Result: 研究表明，尽管DAOS和Ceph都展示了出色的性能，但相比Ceph和Lustre，DAOS展现出更优的可扩展性和灵活性，更适合于希望在大规模环境下执行I/O操作的应用程序。这表明，未来几年内HPC中心可能会更多采用DAOS等对象存储技术，但这并不意味着会完全转向非POSIX类型的I/O方法。

Conclusion: DAOS和Ceph两种对象存储系统都证明了它们能够有效支持HPC和AI应用的需求，特别是在处理大规模数据集时。其中，DAOS因其卓越的可扩展性和灵活性而显得尤为突出，预示着对象存储技术在未来HPC设施中的广泛应用潜力。

Abstract: Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.
  This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community.
  DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O.

</details>


### [77] [TopoSZp: Lightweight Topology-Aware Error-controlled Compression for Scientific Data](https://arxiv.org/abs/2602.17552)
*Tripti Agarwal,Sheng Di,Xin Liang,Zhaoyuan Su,Yuxiao Li,Ganesh Gopalakrishnan,Hanqi Guo,Franck Cappello*

Main category: cs.DC

TL;DR: TopoSZp是一个轻量级的、拓扑感知的、误差控制的有损压缩器，它能有效保存关键点及其关系，同时保持高性能的压缩和解压缩速度。


<details>
  <summary>Details</summary>
Motivation: 现有的高效压缩器如SZ和ZFP虽然提供了强有力的数值误差保证，但往往无法保留对科学分析至关重要的拓扑结构（例如最小值、最大值和鞍点）。而现存的拓扑感知压缩器虽然解决了这一局限性，却带来了巨大的计算开销。因此，开发一个既能保留关键拓扑特征又能保持高性能的压缩解决方案是非常必要的。

Method: 基于高吞吐量的SZp压缩器，TopoSZp集成了有效的关键点检测、局部排序保护以及有针对性的鞍点细化技术，在一个宽松但严格实施的误差范围内运作。

Result: 实验结果表明，与现有的拓扑感知压缩器相比，TopoSZp在真实世界科学数据集上实现了3到100倍更少的非保留关键点，没有引入假阳性或不正确的关键点类型，并且压缩速度快了100到10000倍，解压缩速度快了10到500倍，同时保持了竞争力的压缩比。

Conclusion: 通过提出TopoSZp，研究者们成功地为大规模HPC模拟产生的海量数据提供了一个既能够有效保存重要拓扑特征又拥有出色性能表现的压缩解决方案。

Abstract: Error-bounded lossy compression is essential for managing the massive data volumes produced by large-scale HPC simulations. While state-of-the-art compressors such as SZ and ZFP provide strong numerical error guarantees, they often fail to preserve topological structures (example, minima, maxima, and saddle points) that are critical for scientific analysis. Existing topology-aware compressors address this limitation but incur substantial computational overhead. We present TopoSZp, a lightweight, topology-aware, error-controlled lossy compressor that preserves critical points and their relationships while maintaining high compression and decompression performance. Built on the high-throughput SZp compressor, TopoSZp integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement, all within a relaxed but strictly enforced error bound. Experimental results on real-world scientific datasets show that TopoSZp achieves 3 to 100 times fewer non-preserved critical points, introduces no false positives or incorrect critical point types, and delivers 100 to 10000 times faster compression and 10 to 500 times faster decompression compared to existing topology-aware compressors, while maintaining competitive compression ratios.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [78] [Guided Exploration of Sequential Rules](https://arxiv.org/abs/2602.16717)
*Wensheng Gan,Gengsen Huang,Junyu Ren,Philip S. Yu*

Main category: cs.DB

TL;DR: 本文提出了一种高效发现以用户为中心的序列规则的新方法，通过设计紧密且可泛化的上限来剪枝不具前景的项，并提出了针对频率和效用两种常见评估指标的挖掘算法。实验表明，该方法在运行时间和内存使用方面优于现有技术，同时能够根据灵活的相似性设置发现简洁的序列规则集。


<details>
  <summary>Details</summary>
Motivation: 现有的序列规则发现过程计算密集，而且生成许多对用户无关的规则，这不仅增加了额外的计算开销，还使下游分析复杂化。为了解决这些问题，研究旨在开发一种更高效、更贴合用户需求的序列规则发现方法。

Method: 首先处理原始数据库判断目标查询规则是否存在；设计了紧密且具有广泛适用性的上限值来剪枝不具潜力的项目并避免不必要的扩展；引入新的技术与剪枝策略用于高效生成目标序列规则；为频率和效用这两种常见的评价指标设计了相应的挖掘算法；提出了两种规则相似度度量方法帮助识别最相关的序列规则。

Result: 所提算法在运行时间与内存消耗上均优于当前最佳方案，并能在灵活性较高的相似性设定下发现一套简明扼要的序列规则。此外，定向序列规则搜索可以处理带有个性化特征的序列数据实现模式发现。

Conclusion: 提出的解决方案成功解决了几个关键挑战，并能应用于两种常见的挖掘任务中，有效提高了序列规则发现的效率与相关性。

Abstract: In pattern mining, sequential rules provide a formal framework to capture the temporal relationships and inferential dependencies between items. However, the discovery process is computationally intensive. To obtain mining results efficiently and flexibly, many methods have been proposed that rely on specific evaluation metrics (i.e., ensuring results meet minimum threshold requirements). A key issue with these methods, however, is that they generate many sequential rules that are irrelevant to users. Such rules not only incur additional computational overhead but also complicate downstream analysis. In this paper, we investigate how to efficiently discover user-centric sequential rules. The original database is first processed to determine whether a target query rule is present. To prune unpromising items and avoid unnecessary expansions, we design tight and generalizable upper bounds. We introduce a novel method for efficiently generating target sequential rules using the proposed techniques and pruning strategies. In addition, we propose the corresponding mining algorithms for two common evaluation metrics: frequency and utility. We also design two rule similarity metrics to help discover the most relevant sequential rules. Extensive experiments demonstrate that our algorithms outperform state-of-the-art approaches in terms of runtime and memory usage, while discovering a concise set of sequential rules under flexible similarity settings. Targeted sequential rule search can handle sequence data with personalized features and achieve pattern discovery. The proposed solution addresses several challenges and can be applied to two common mining tasks.

</details>


### [79] [UPER: Efficient Utility-driven Partially-ordered Episode Rule Mining](https://arxiv.org/abs/2602.16718)
*Hong Lin,Wensheng Gan,Junyu Ren,Philip S. Yu*

Main category: cs.DB

TL;DR: 本文提出了一种名为UPER的算法，用于发现高实用性的部分有序事件规则（POERs），并引入了NoList数据结构和几种剪枝策略来减少候选规则的数量。


<details>
  <summary>Details</summary>
Motivation: 现有的事件规则挖掘对于事件顺序有严格的要求，限制了其实用性。因此，研究者们提出了部分有序事件规则挖掘（POERM）来放宽这些约束。然而，为了发现更有价值的规则，考虑将效用概念引入POERM是必要的。

Method: 定义了POERs的效用，并开发了一个名为UPER的新算法来识别高实用性的部分有序事件规则。此外，使用NoList这种数据结构来存储必要信息，并且提出了包括WEUP、REUCSP和REEUP在内的几种剪枝策略以优化过程。

Result: 通过在多个数据集上进行实验，证明了所提方法的有效性，能够有效地找到具有高实用性的部分有序事件规则。

Conclusion: 该研究为部分有序事件规则挖掘提供了一种新的方法，通过引入效用度量和有效的剪枝策略，提高了规则挖掘的质量和效率。

Abstract: Episode mining is a fundamental problem in analyzing a sequence of numerous events. For discovering strong relationships between events in a complex event sequence, episode rule mining is proposed. However, both the episode and episode rules have strict requirements for the order of events. Hence, partially-ordered episode rule mining (POERM) is designed to loosen the constraints on the ordering, i.e., events in the antecedents and consequents of the rule can be unordered, and POERM has been applied to real-life event prediction. In this paper, we consider the utility of POERM, intending to discover more valuable rules. We define the utility of POERs and propose an algorithm called UPER to discover high-utility partially-ordered episode rules. In addition, we adopt a data structure named NoList to store the necessary information, analyze the expansion of POERs in detail, and propose several pruning strategies (namely WEUP, REUCSP, and REEUP) to reduce the number of candidate rules. Finally, we conduct experiments on several datasets to demonstrate the effectivene

</details>


### [80] [GPU-Accelerated Algorithms for Graph Vector Search: Taxonomy, Empirical Study, and Research Directions](https://arxiv.org/abs/2602.16719)
*Yaowen Liu,Xuejia Chen,Anxin Tian,Haoyang Li,Qinbin Li,Xin Zhang,Alexander Zhou,Chen Jason Zhang,Qing Li,Lei Chen*

Main category: cs.DB

TL;DR: 本文对GPU加速的基于图的向量搜索算法进行了全面调查和实验研究，旨在为大规模数据挖掘和机器学习应用中的近似最近邻搜索提供优化指导。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模的增长，近似最近邻搜索（ANNS）对于许多大规模数据挖掘和机器学习应用至关重要，而高效的检索越来越依赖于GPU加速。尽管基于图的方法代表了近似最近邻搜索领域的最新技术，但缺乏对其在现代GPU架构上进行优化以及在实际场景中端到端有效性的系统性理解。

Method: 通过建立详细的GPU优化策略分类法，并明确算法任务与GPU内硬件执行单元之间的映射关系。对六种领先的算法在八个大规模基准数据集上进行了全面评估，考察了图索引构建和查询搜索性能。

Result: 分析显示，距离计算仍然是主要的计算瓶颈，而在大规模情况下，主机CPU与GPU之间的数据传输成为影响实际延迟的主要因素。此外，还强调了不同系统设计之间在可扩展性和内存使用方面的关键权衡。

Conclusion: 研究结果为设计可扩展且稳健的GPU驱动近似最近邻搜索系统提供了明确指南，并为知识发现和数据挖掘社区提供了一个全面的基准。

Abstract: Approximate Nearest Neighbor Search (ANNS) underpins many large-scale data mining and machine learning applications, with efficient retrieval increasingly hinging on GPU acceleration as dataset sizes grow. Although graph-based approaches represent the state of the art in approximate nearest neighbor search, there is a lack of systematic understanding regarding their optimization for modern GPU architectures and their end-to-end effectiveness in practical scenarios. In this work, we present a comprehensive survey and experimental study of GPU-accelerated graph-based vector search algorithms. We establish a detailed taxonomy of GPU optimization strategies and clarify the mapping between algorithmic tasks and hardware execution units within GPUs. Through a thorough evaluation of six leading algorithms on eight large-scale benchmark datasets, we assess both graph index construction and query search performance. Our analysis reveals that distance computation remains the primary computational bottleneck, while data transfer between the host CPU and GPU emerges as the dominant factor influencing real-world latency at large scale. We also highlight key trade-offs in scalability and memory usage across different system designs. Our findings offer clear guidelines for designing scalable and robust GPU-powered approximate nearest neighbor search systems, and provide a comprehensive benchmark for the knowledge discovery and data mining community.

</details>


### [81] [Do GPUs Really Need New Tabular File Formats?](https://arxiv.org/abs/2602.17335)
*Jigao Luo,Qi Chen,Carsten Binnig*

Main category: cs.DB

TL;DR: 本研究探讨了Parquet文件格式在GPU加速数据处理中的性能问题，通过采用针对GPU优化的配置，显著提高了读取带宽，最高可达125GB/s，而无需更改Parquet规范本身。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速的数据处理变得越来越普遍，基于CPU设计默认设置生成的Parquet文件无法充分利用GPU并行性，导致GPU扫描成为性能瓶颈。

Method: 系统地研究了不同Parquet配置对GPU扫描性能的影响，并证明了通过应用针对GPU的配置可以大幅度提高性能。

Result: 研究表明，使用GPU感知配置能够显著增加有效读取带宽，最高速度达到了125GB/s。

Conclusion: Parquet在GPU上的低性能并非由格式本身引起，而是由于配置选择不当所致；通过适当的配置调整，可以极大改善其在GPU环境下的表现。

Abstract: Parquet is the de facto columnar file format in modern analytical systems, yet its configuration guidelines have largely been shaped by CPU-centric execution models. As GPU-accelerated data processing becomes increasingly prevalent, Parquet files generated with CPU-oriented defaults can severely underutilize GPU parallelism, turning GPU scans into a performance bottleneck.
  In this work, we systematically study how Parquet configurations affect GPU scan performance. We show that Parquet's poor GPU performance is not inherent to the format itself but rather a consequence of suboptimal configuration choices. By applying GPU-aware configurations, we increase effective read bandwidth up to 125 GB/s without modifying the Parquet specification.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [82] [A Construction-Phase Digital Twin Framework for Quality Assurance and Decision Support in Civil Infrastructure Projects](https://arxiv.org/abs/2602.16748)
*Md Asiful Islam,Shanto Jouerder,Md Sabit As Sami,Afia Jahin Prema*

Main category: cs.SE

TL;DR: 本研究提出了一种施工阶段的数字孪生框架，旨在支持施工过程中元素级别的质量保证和基于准备情况的决策。该框架通过整合检查记录、材料生产和放置数据、早期感应以及预测强度模型等数据流来表示每个施工元素的质量状态演变，并在标准龄期测试结果可用之前支持结构化的放行或暂停决策。


<details>
  <summary>Details</summary>
Motivation: 当前施工中的质量保证通常依赖于完工后数天或数周才能获得的检验记录和实验室测试结果，这限制了早期干预并增加了返工、进度影响及文件碎片化的风险。为解决这些问题，提出了一个能够提供更早、数据驱动的质量评估方法的框架。

Method: 通过连接检查记录、材料生产和放置数据、早期感应技术以及预测强度模型到单独的施工元素上，创建了一个能够代表每个元素质量状态变化的系统。

Result: 该框架不仅提高了可追溯性，还允许在传统测试结果出来之前进行更早的数据信息质量评估，从而补充现有工作流程而不取代已建立的检验和测试程序。

Conclusion: 所提出的框架为从延迟的、文档驱动的审查向主动的、元素级决策支持转变提供了结构化途径，有助于提高施工过程中的质量保证效率。

Abstract: Quality assurance (QA) during construction often relies on inspection records and laboratory test results that become available days or weeks after work is completed. On large highway and bridge projects, this delay limits early intervention and increases the risk of rework, schedule impacts, and fragmented documentation. This study presents a construction-phase digital twin framework designed to support element-level QA and readiness-based decision making during active construction. The framework links inspection records, material production and placement data, early-age sensing, and predictive strength models to individual construction elements. By integrating these data streams, the system represents the evolving quality state of each element and supports structured release or hold decisions before standard-age test results are available. The approach does not replace established inspection and testing procedures. Instead, it supplements existing workflows by improving traceability and enabling earlier, data-informed quality assessments. Practical considerations related to data integration, contractual constraints, and implementation challenges are also discussed. The proposed framework provides a structured pathway for transitioning construction QA from delayed, document-driven review toward proactive, element-level decision support during construction.

</details>


### [83] [Exploring LLMs for User Story Extraction from Mockups](https://arxiv.org/abs/2602.16997)
*Diego Firmenich,Leandro Antonelli,Bruno Pazos,Fabricio Lozada,Leonardo Morales*

Main category: cs.SE

TL;DR: 本研究探讨了结合高保真模型和大型语言模型（LLMs）从模型中自动生成用户故事的能力，结果表明加入扩展词汇表（LEL）显著提高了生成的用户故事的准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 为了促进终端用户的参与并定义他们的需求，本文探索了将用户故事与高保真原型图相结合，并利用大型语言模型自动从这些原型图中生成用户故事的方法。

Method: 通过案例研究分析了大型语言模型直接从高保真原型图抽取用户故事的能力，并且比较了在提示中包含和不包含扩展词汇表（LEL）的情况下的效果差异。

Result: 研究表明，在提示中加入扩展词汇表（LEL）能够显著提高所生成用户故事的准确度与合适程度。

Conclusion: 将AI整合到需求工程中的这一方法标志着一个进步，有可能改善用户与开发者之间的沟通。

Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.

</details>


### [84] [Not Only for Developers: Exploring Plugin Maintenance for Knowledge-Centric Communities](https://arxiv.org/abs/2602.17018)
*Giovanni Rosa,David Moreno-Lumbreras,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 本文研究了Obsidian平台上的插件生态系统，尽管其社区主要关注写作、组织和创造力而非开发，但仍形成了显著的插件生态。通过对396个代表性插件进行仓库挖掘和基于大语言模型的主题建模，识别出六个与知识管理和工具相关的话题，并分析了这些插件的Pull Requests，揭示即使在混合型社区中插件生态系统也能发展出可识别的工程结构，为未来关于非开发者生态系统的健康与可持续性研究指明了三个方向及六个研究问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨在一个以写作、组织和创作为核心但不以开发人员为主的社区里，如何形成并维护一个庞大的插件生态系统。

Method: 采用仓库挖掘技术和基于大语言模型的主题建模方法，对选取的396个插件样本进行了分析；同时，通过分析这些插件的Pull Requests来了解软件演进的情况。

Result: 确定了六个与知识管理和工具相关的主题：动态编辑与组织、界面与布局、创意写作与生产力、知识同步解决方案、链接与脚本工具以及工作流增强工具。此外，观察到大量软件演化活动发生在该生态系统内。

Conclusion: 即便是在非开发者主导的混合型社区中，插件生态系统也能够发展出明显的工程结构特征。这一发现激发了针对这类非传统开发者生态系统的健康度与可持续性的进一步研究兴趣。

Abstract: The adoption of third-party libraries has become integral to modern software development, leading to large ecosystems such as PyPI, NPM, and Maven, where contributors typically share the technical expertise to sustain extensions. In communities that are not exclusively composed of developers, however, maintaining plugin ecosystems can present different challenges. In this early results paper, we study Obsidian, a knowledge--centric platform whose community is focused on writing, organization, and creativity--has built a substantial plugin ecosystem despite not being developer--centric. We investigate what kinds of plugins exist within this hybrid ecosystem and establish a foundation for understanding how they are maintained. Using repository mining and LLM-based topic modeling on a representative sample of 396 plugins, we identify six topics related to knowledge management and tooling, which is (i) dynamic editing and organization, (ii) interface and layouts, (iii) creative writing and productivity, (iv) knowledge sync solutions, (v) linking and script tools, and (vi) workflow enhancements tools. Furthermore, analysis of the Pull Requests from these plugins show that much software evolution has been performed on these ecosystem. These findings suggest that even in mixed communities, plugin ecosystems can develop recognizable engineering structures, motivating future work that highlight three different research directions with six research questions related to the health and sustainability of these non-developer ecosystems.

</details>


### [85] [Multi-Ecosystem Modeling of OSS Project Sustainability](https://arxiv.org/abs/2602.17112)
*Arjun Ashok,Nafiz Imtiaz Khan,Swati Singhvi,Stefan Stanciulescu,Zhouhao Wang,Vladimir Filkov*

Main category: cs.SE

TL;DR: 本研究通过实证和定量分析，探讨了Apache、Eclipse和OSGeo基金会孵化项目的可持续性问题，并且也考察了GitHub上非基金会的开源项目。基于项目社会技术追踪概况开发了特定于基金会的可持续性模型及项目分类方法，证明这些模型不仅在基金会内部而且跨基金会都能有效预测可持续性结果。此外，该框架还被应用于GitHub上非基金会的项目，并结合可操作的恢复策略对失败案例进行了研究。


<details>
  <summary>Details</summary>
Motivation: 许多开源软件（OSS）项目加入到如Apache、Eclipse和OSGeo等基金会中以获得治理建议、孵化支持和社区建设机制来帮助实现其短期计划并改善长期前景。然而，由于各基金会在政策、资金模式和支持策略上的差异以及加入基金会的项目在其生命周期阶段和需求方面存在多样性，因此确定合适的项目-基金会匹配度及制定项目特异性可持续发展计划变得具有挑战性。

Method: 采用实证研究与定量分析相结合的方法，针对Apache、Eclipse和OSGeo三个基金会中的孵化项目以及GitHub上不属于任何基金会的OSS项目进行了研究。根据项目的社会技术追踪情况建立了特定于每个基金会的可持续性模型及一种项目分类方法，并展示了它们在不同基金会间有效性。

Result: 研究表明，所提出的带有项目分类的模型能够有效地预测各个基金会内外项目的可持续性结果。此外，这种方法还可以推广到基金会外的GitHub项目。通过对以往工作的可执行恢复策略的应用，为失败的孵化器项目提供了案例研究。

Conclusion: 研究强调了社会技术框架在描述和解决软件项目可持续性问题方面的价值。

Abstract: Many OSS projects join foundations such as Apache, Eclipse, and OSGeo, to aid their immediate plans and improve long-term prospects by getting governance advice, incubation support, and community-building mechanisms. But foundations differ in their policies, funding models, and support strategies. Moreover, since projects joining these foundations are diverse, coming at different lifecycle stages and having different needs, it can be challenging to decide on the appropriate project-foundation match and on the project-specific plan for sustainability.
  Here, we present an empirical study and quantitative analysis of the sustainability of incubator projects in the Apache, Eclipse, and OSGeo foundations, and, additionally, of OSS projects from GitHub outside of foundations. We develop foundation-specific sustainability models and a project triage, based on projects' sociotechnical trace profiles, and demonstrate their effectiveness across the foundations. Our results show that our models with triage can effectively forecast sustainability outcomes not only within but across foundations. In addition, the generalizability of the framework allows us to apply the approach to GitHub projects outside the foundations. We complement our findings with actionable recovery strategies from previous work and apply them to case studies of failed incubator projects. Our study highlights the value of sociotechnical frameworks in characterizing and addressing software project sustainability issues.

</details>


### [86] [Quantifying Competitive Relationships Among Open-Source Software Projects](https://arxiv.org/abs/2602.17131)
*Yuki Takei,Toshiaki Aoki,Chaiyong Ragkhitwetsagul*

Main category: cs.SE

TL;DR: 研究提出了一种名为MIAO的新方法，用于量化开源软件项目间的竞争关系，并通过实证分析展示了其在预测项目终止方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 开源软件（OSS）领域内的竞争对其生存的影响尚不清楚，存在失去竞争优势给竞争对手的风险。为了应对这种情况，本研究旨在开发一种新方法来量化这些竞争关系。

Method: 该研究提出了一个名为'Mutual Impact Analysis of OSS (MIAO)'的新自动化方法，利用结构向量自回归模型和脉冲响应函数（通常用于宏观经济分析中）来分析OSS项目之间的相互作用。

Result: 通过对187个OSS项目组进行挖掘和分析，MIAO能够以高达81%的准确率识别出因竞争影响而被迫停止开发的项目，并且生成的特征支持了提前一年预测停止情况的实验，准确率达到77%。

Conclusion: MIAO可以成为OSS项目维护者了解OSS生态系统动态及预测项目兴衰趋势的宝贵工具。

Abstract: Throughout the history of software, evolution has occurred in cycles of rise and fall driven by competition, and open-source software (OSS) is no exception. This cycle is accelerating, particularly in rapidly evolving domains such as web development and deep learning. However, the impact of competitive relationships among OSS projects on their survival remains unclear, and there are risks of losing a competitive edge to rivals. To address this, this study proposes a new automated method called ``Mutual Impact Analysis of OSS (MIAO)'' to quantify these competitive relationships. The proposed method employs a structural vector autoregressive model and impulse response functions, normally used in macroeconomic analysis, to analyze the interactions among OSS projects. In an empirical analysis involving mining and analyzing 187 OSS project groups, MIAO identified projects that were forced to cease development owing to competitive influences with up to 81\% accuracy, and the resulting features supported predictive experiments that anticipate cessation one year ahead with up to 77\% accuracy. This suggests that MIAO could be a valuable tool for OSS project maintainers to understand the dynamics of OSS ecosystems and predict the rise and fall of OSS projects.

</details>


### [87] [Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering](https://arxiv.org/abs/2602.17183)
*Kishan Maharaj,Nandakishore Menon,Ashita Saxena,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 本研究通过控制消融实验来测试大型语言模型在长代码上下文中的鲁棒性，包括答案格式、干扰项和上下文规模的敏感性。结果表明，在选择项打乱和开放式问题设置下性能显著下降，并且在存在不相关线索时表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型越来越多地被用来辅助需要对长代码进行推理的软件工程任务，但它们在不同输入条件下的鲁棒性仍不清楚。因此，这项研究旨在通过一系列受控实验来评估这些模型在处理长代码文本时的表现。

Method: 研究者扩展了LongCodeBench Python数据集，新增了COBOL和Java的问题-答案集合，并在三种设定下评估了最先进的模型：(i) 打乱的选择题选项 (ii) 开放式问题 (iii) 包含相关与恶意无关信息的'大海捞针'式上下文环境。

Result: 结果显示，在面对打乱顺序的选择题选项以及开放式问题时，模型的表现有明显下滑；同时，在遇到不相关的提示信息时，模型的行为显得很脆弱。

Conclusion: 当前针对长代码背景下的评价方法存在局限性，而本研究提供的更广泛的基准可以帮助更好地评估现代及遗留系统中代码推理的能力。

Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.

</details>


### [88] [Disjunction Composition of BDD Transition Systems for Model-Based Testing](https://arxiv.org/abs/2602.17237)
*Tannaz Zameni,Petra van den Bos,Arend Rensink*

Main category: cs.SE

TL;DR: 本研究提出了一种基于行为驱动开发（BDD）的模型测试生成的组合方法，通过定义析取组合来合并表示系统可选行为的BDD转换系统，并证明了这种方法能够保持原始场景集的测试能力。此外，还通过工业案例研究展示了析取组合的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了在行为驱动开发中提高模型测试的有效性，特别是当需要处理描述系统不同但可能的行为选项时，提出了一种新的组合方法来更好地集成这些行为模型并确保整体测试能力不受影响。

Method: 研究者们形式化定义了‘析取组合’的概念，用于结合代表系统替代行为的BDD转换系统。通过使用BDD转换系统的符号语义来证明这种组合方式可以保持原有场景集合的测试效能。

Result: 成功地定义了如何通过‘析取组合’来整合多个BDD转换系统而不损失原有的测试覆盖率；并且通过一个实际的工业应用案例验证了该方法的有效性和实用性。

Conclusion: 引入的析取组合为BDD中的模型测试提供了一种强有力的新手段，能够在不牺牲单独行为模型测试能力的前提下实现对多种行为模式的一体化建模与测试。

Abstract: We introduce a compositional approach to model-based test generation in Behavior-Driven Development (BDD). BDD is an agile methodology in which system behavior is specified through textual scenarios that, in our approach, are translated into transition systems used for model-based testing. This paper formally defines disjunction composition, to combine BDD transition systems that represent alternative system behaviors. Disjunction composition allows for modeling and testing the integrated behavior while ensuring that the testing power of the original set of scenarios is preserved. This is proved using a symbolic semantics for BDD transition systems, with the property that the symbolic equivalence of two BDD transition systems guarantees that they fail the same test cases. Also, we demonstrate the potential of disjunction composition by applying the composition in an industrial case study.

</details>


### [89] [Socio-Technical Well-Being of Quantum Software Communities: An Overview on Community Smells](https://arxiv.org/abs/2602.17320)
*Stefano Lambiase,Manuel De Stefano,Fabio Palomba,Filomena Ferrucci,Andrea De Lucia*

Main category: cs.SE

TL;DR: 本文研究了开源量子计算社区中的社会技术反模式（称为社区气味），这些反模式可能对产品品质和社区健康产生负面影响。通过一项横断面研究，文章旨在分析量子社区的社会技术状况，并为减轻与社区气味相关的风险提供基础性知识，以确保开源量子项目的长期可持续性。


<details>
  <summary>Details</summary>
Motivation: 尽管量子技术前景广阔，但开源社区在开发量子支持系统时面临重要的社会技术挑战，包括可能导致技术和代码质量下降的社会技术反模式。目前对于这些因素如何影响量子开源社区的研究较少。

Method: 采用横断面研究方法来分析量子开源社区的社会技术福祉。

Result: 通过了解社会技术动态，可以建立基础性知识，以缓解与社区气味有关的风险，保证开源量子计划的长期可持续性。

Conclusion: 理解量子开源社区中社会技术互动的重要性，以及识别和处理社区气味，对于确保项目质量和社区健康发展至关重要。

Abstract: Quantum computing has gained significant attention due to its potential to solve computational problems beyond the capabilities of classical computers. With major corporations and academic institutions investing in quantum hardware and software, there has been a rise in the development of quantum-enabled systems, particularly within open-source communities. However, despite the promising nature of quantum technologies, these communities face critical socio-technical challenges, including the emergence of socio-technical anti-patterns known as community smells. These anti-patterns, prevalent in open-source environments, have the potential to negatively impact both product quality and community health by introducing technical debt and amplifying architectural and code smells. Despite the importance of these socio-technical factors, there remains a scarcity of research investigating their influence within quantum open-source communities. This work aims to address this gap by providing a first step in analyzing the socio-technical well-being of quantum communities through a cross-sectional study. By understanding the socio-technical dynamics at play, it is expected that foundational knowledge can be established to mitigate the risks associated with community smells and ensure the long-term sustainability of open-source quantum initiatives.

</details>


### [90] [The Runtime Dimension of Ethics in Self-Adaptive Systems](https://arxiv.org/abs/2602.17426)
*Marco Autili,Gianluca Filippone,Mashal Afzal Memon,Patrizio Pelliccione*

Main category: cs.SE

TL;DR: 本文提倡从静态的伦理规则转向运行时伦理推理，以适应自适应系统中个体和群体间不同的、可变的且可能冲突的伦理偏好。提出需要在利益相关者之间进行明确的基于伦理的协商来管理多个伦理权衡，并确定了关键挑战及研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前方法通常将伦理编码为固定的、基于规则的约束或设计时嵌入的单一选定伦理理论，这忽略了人类-系统交互设置的一个基本属性：伦理偏好因人而异、随情境变化，并可能存在冲突。然而，这些偏好仍然需要保持在法律和监管定义的硬性伦理范围内（例如，安全与合规约束）。

Method: 文章没有具体描述一种方法论，而是提出了一个概念框架，主张采用运行时伦理推理的方式，将伦理偏好视为必须根据利益相关者和情境的变化持续更新的运行时需求。

Result: 指出了实现伦理自适应系统的关键挑战，包括伦理不确定性、伦理价值观之间的冲突（包括人类、社会和环境驱动因素）以及多维度/多方/多驱动因素协商等问题，并概述了朝向伦理自适应系统的研究方向和问题。

Conclusion: 为了构建能够处理多样化、动态化并且潜在冲突的伦理偏好的自适应系统，我们需要开发新的方法来进行运行时伦理推理，并通过显式的伦理基础协商来解决多方面之间的伦理权衡问题。

Abstract: Self-adaptive systems increasingly operate in close interaction with humans, often sharing the same physical or virtual environments and making decisions with ethical implications at runtime. Current approaches typically encode ethics as fixed, rule-based constraints or as a single chosen ethical theory embedded at design time. This overlooks a fundamental property of human-system interaction settings: ethical preferences vary across individuals and groups, evolve with context, and may conflict, while still needing to remain within a legally and regulatorily defined hard-ethics envelope (e.g., safety and compliance constraints). This paper advocates a shift from static ethical rules to runtime ethical reasoning for self-adaptive systems, where ethical preferences are treated as runtime requirements that must be elicited, represented, and continuously revised as stakeholders and situations change. We argue that satisfying such requirements demands explicit ethics-based negotiation to manage ethical trade-offs among multiple humans who interact with, are represented by, or are affected by a system. We identify key challenges, ethical uncertainty, conflicts among ethical values (including human, societal, and environmental drivers), and multi-dimensional/multi-party/multi-driver negotiation, and outline research directions and questions toward ethically self-adaptive systems.

</details>


### [91] [Towards a Software Reference Architecture for Natural Language Processing Tools in Requirements Engineering](https://arxiv.org/abs/2602.17498)
*Julian Frattini,Quim Motger*

Main category: cs.SE

TL;DR: 本文提出了一种从单一的NLP4RE工具转向可重用、可互操作模块生态系统的愿景，并为此制定了一个软件参考架构（SRA）的研发路线图，以促进此类工具的开发、复用及长期维护。


<details>
  <summary>Details</summary>
Motivation: 当前NLP工具在支持需求工程任务时存在功能重复建设、缺乏互操作性以及维护不足等问题，导致不必要的开发工作量增加，阻碍了工具比较与基准测试，同时增加了文档编制难度，降低了NLP4RE工具的长期可持续性。

Method: 作者们首先提出了向可重用、可互操作模块生态系统过渡的愿景；接着，按照标准方法论框架详细规划了实现这一愿景所需的软件参考架构（SRA）研发路线图；最后，通过开展利益相关者驱动的重点小组会议来收集NLP4RE工具的一般系统需求。

Result: 研究活动成功地识别出36个关键系统需求，进一步证明了专门设计SRA的必要性。

Conclusion: 提出的愿景、路线图和初步贡献为改进NLP4RE工具的开发、复用及其长期维护奠定了基础。

Abstract: Natural Language Processing (NLP) tools support requirements engineering (RE) tasks like requirements elicitation, classification, and validation. However, they are often developed from scratch despite functional overlaps, and abandoned after publication. This lack of interoperability and maintenance incurs unnecessary development effort, impedes tool comparison and benchmarking, complicates documentation, and diminishes the long-term sustainability of NLP4RE tools. To address these issues, we postulate a vision to transition from monolithic NLP4RE tools to an ecosystem of reusable, interoperable modules. We outline a research roadmap towards a software reference architecture (SRA) to realize this vision, elaborated following a standard methodological framework for SRA development. As an initial step, we conducted a stakeholder-driven focus group session to elicit generic system requirements for NLP4RE tools. This activity resulted in 36 key system requirements, further motivating the need for a dedicated SRA. Overall, the proposed vision, roadmap, and initial contribution pave the way towards improved development, reuse, and long-term maintenance of NLP4RE tools.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [92] [RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution](https://arxiv.org/abs/2602.16932)
*Jinming Nian,Fangchen Li,Dae Hoon Park,Yi Fang*

Main category: cs.IR

TL;DR: 本文介绍了一种名为RankEvolve的方法，该方法基于大型语言模型和进化搜索来自动发现改进的词汇检索算法。从BM25和带有Dirichlet平滑的查询似然两种种子程序开始，所生成的新算法在多个信息检索数据集上表现良好，并显示出对未来基准测试的良好迁移性。


<details>
  <summary>Details</summary>
Motivation: 尽管BM25和带有Dirichlet平滑的查询似然等检索算法作为第一阶段排序器仍然强大且高效，但它们的改进主要依赖于参数调优和人为直觉。研究者们探索了大型语言模型是否能在评估器和进化搜索指导下自动发现更优的词汇检索算法。

Method: 提出了RankEvolve，一种基于AlphaEvolve的程序进化设置，在这个设置中候选排名算法被表示为可执行代码，并通过迭代变异、重组以及根据12个来自BEIR和BRIGHT的信息检索数据集上的检索性能进行选择。起始点是两个种子程序：BM25与带Dirichlet平滑的查询似然。

Result: 演化出的算法新颖有效，并且对完整的BEIR、BRIGHT基准以及TREC DL 19和20表现出良好的迁移能力。

Conclusion: 结果表明，由评估器引导的大规模语言模型程序进化是一条通往自动发现新排名算法实用路径。

Abstract: Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.

</details>


### [93] [Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval](https://arxiv.org/abs/2602.16974)
*Yongjie Zhou,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 本文重现了文档切分的先前研究，并提出了一个系统框架，该框架沿着两个关键维度统一现有策略：(1)分割方法，包括基于结构的方法（固定大小、基于句子和基于段落）以及语义知情和大模型引导的方法；(2)嵌入范式，决定了相对于嵌入的切分时机（预嵌入切分与上下文化切分）。综合评估表明，最佳切分策略是任务依赖性的。


<details>
  <summary>Details</summary>
Motivation: 文档切分是密集检索系统中的一个关键预处理步骤，但关于切分策略的设计空间了解不足。最近的研究提出了几种并行的方法，包括大模型引导的方法（例如DenseX和LumberChunker）以及上下文化策略（例如Late Chunking），这些方法在分段前生成嵌入以保留上下文信息。然而，这些方法独立出现，并且是在基准测试中进行评估的，其重叠部分很少，这使得直接比较变得困难。

Method: 本文通过重现以前的研究，并提出一个将现有策略沿两个主要维度统一起来的系统框架来解决这个问题：(1)分割方法，包括基于结构的方法（如固定大小、基于句子和基于段落）以及语义知情和LLM指导的方法；(2)嵌入模式，确定相对于嵌入的切分时间（预嵌入切分与上下文化切分）。

Result: 全面评估显示，最优的切分策略取决于任务：对于跨文档检索，简单的基于结构的方法优于LLM指导的替代方案，而LumberChunker则在文档内检索中表现最佳。上下文化的切分提高了跨文档的有效性，但降低了文档内检索的表现。还发现块大小与文档内效果有中等程度的相关性，而与跨文档有效性相关性较弱，这表明分割方法之间的差异并非完全由块大小驱动。

Conclusion: 本研究强调了选择适当文档切分策略的重要性，这取决于具体的检索任务。此外，它还提供了对不同切分方法及其对检索性能影响的新见解。

Abstract: Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal overlap, making direct comparisons difficult.
  This paper reproduces prior studies in document chunking and presents a systematic framework that unifies existing strategies along two key dimensions: (1) segmentation methods, including structure-based methods (fixed-size, sentence-based, and paragraph-based) as well as semantically-informed and LLM-guided methods; and (2) embedding paradigms, which determine the timing of chunking relative to embedding (pre-embedding chunking vs. contextualized chunking). Our reproduction evaluates these approaches in two distinct retrieval settings established in previous work: in-document retrieval (needle-in-a-haystack) and in-corpus retrieval (the standard information retrieval task).
  Our comprehensive evaluation reveals that optimal chunking strategies are task-dependent: simple structure-based methods outperform LLM-guided alternatives for in-corpus retrieval, while LumberChunker performs best for in-document retrieval. Contextualized chunking improves in-corpus effectiveness but degrades in-document retrieval. We also find that chunk size correlates moderately with in-document but weakly with in-corpus effectiveness, suggesting segmentation method differences are not purely driven by chunk size. Our code and evaluation benchmarks are publicly available at (Anonymoused).

</details>


### [94] [Bending the Scaling Law Curve in Large-Scale Recommendation Systems](https://arxiv.org/abs/2602.16986)
*Qin Ding,Kevin Course,Linjian Ma,Jianhui Sun,Rouchen Liu,Zhao Zhu,Chunxing Yin,Wei Li,Dai Li,Yu Shi,Xuan Cao,Ze Yang,Han Li,Xing Liu,Bi Xue,Hongwei Li,Rui Jian,Daisy Shi He,Jing Qian,Matt Ma,Qunshu Zhang,Rui Li*

Main category: cs.IR

TL;DR: 本文提出了一种新的序列推荐模型ULTRA-HSTU，通过端到端的模型和系统协同设计，在输入序列、稀疏注意力机制及模型拓扑结构上进行了创新。该模型在保持高质量推荐的同时，实现了显著的扩展效率提升，包括训练速度提升了5倍以上，推理速度提高了21倍，并已在实际生产环境中部署，服务数十亿用户，促进了4%至8%的消费与参与度增长。


<details>
  <summary>Details</summary>
Motivation: 随着大规模推荐系统越来越多地依赖于从用户交互历史中学习的序列模型，以及大型语言模型展现出的良好扩展性规律，研究者们开始探索长序列建模和更深层次架构以应对推荐任务。然而，现有方法多依靠交叉注意力机制解决序列建模中的计算瓶颈问题，这可能限制了自注意力机制带来的表现力优势。为了解决这些问题并进一步提高推荐系统的性能与效率，提出了ULTRA-HSTU模型。

Method: ULTRA-HSTU采用了端到端的模型和系统协同设计方案，主要创新点在于：1) 输入序列的设计；2) 引入稀疏注意力机制；3) 优化模型拓扑结构。这些改进旨在克服传统序列推荐模型中存在的局限性，特别是针对计算效率低下的挑战。

Result: 全面基准测试表明，与传统模型相比，ULTRA-HSTU不仅在推荐质量上表现出色，而且在扩展效率方面也取得了显著进步——训练速度提高了超过5倍，推理速度加快了21倍。此外，在真实生产环境下，该模型还推动了4%到8%的消费量和用户参与度增长。

Conclusion: ULTRA-HSTU作为一种新型的序列推荐模型，通过一系列创新设计有效解决了当前序列推荐中存在的效率与性能之间的矛盾。它不仅极大地提高了处理速度，同时也保证了推荐结果的质量，在实际应用中展现了巨大潜力。

Abstract: Learning from user interaction history through sequential models has become a cornerstone of large-scale recommender systems. Recent advances in large language models have revealed promising scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms to address the quadratic computational bottleneck in sequential modeling, which can limit the representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential recommendation model developed through end-to-end model and system co-design. By innovating in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains -- over 5x faster training scaling and 21x faster inference scaling compared to conventional models -- while delivering superior recommendation quality. Our solution is fully deployed at scale, serving billions of users daily and driving significant 4% to 8% consumption and engagement improvements in real-world production environments.

</details>


### [95] [WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline](https://arxiv.org/abs/2602.16989)
*Chentong Hao,Minmao Wang*

Main category: cs.IR

TL;DR: 介绍了一个低成本的检索系统，用于WSDM Cup 2026多语言检索任务，通过结合基于大语言模型的查询扩展、BM25候选检索、使用jina-embeddings-v4进行密集排序以及利用Qwen3-Reranker-4B对前20名候选进行点式重排序等四个阶段，在官方评估中达到了nDCG@20为0.403和Judged@20为0.95的成绩。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个低成本且高效的解决方案来处理跨语言信息检索挑战，特别是从中文、波斯语和俄语文档中根据英文查询检索相关文档的任务。

Method: 采用四阶段流程：首先使用基于大型语言模型（LLM）的方法进行GRF风格的查询扩展；然后利用BM25算法执行候选文档检索；接着通过jina-embeddings-v4生成的长文本表示来进行密集型排序；最后使用Qwen3-Reranker-4B对排名前20位的结果实施点式重排序，并保持其余结果的密集顺序不变。

Result: 该系统在官方评测上取得了nDCG@20得分为0.403及Judged@20得分为0.95的良好表现。此外，通过广泛的消融实验分析了每个阶段的重要性以及不同技术在有限计算资源条件下的有效性。

Conclusion: 提出的方法证明了即使在预算有限的情况下，通过巧妙地结合多种现代NLP技术和策略也能构建出高效而准确的跨语言检索系统。

Abstract: We present a low-cost retrieval system for the WSDM Cup 2026 multilingual retrieval task, where English queries are used to retrieve relevant documents from a collection of approximately ten million news articles in Chinese, Persian, and Russian, and to output the top-1000 ranked results for each query. We follow a four-stage pipeline that combines LLM-based GRF-style query expansion with BM25 candidate retrieval, dense ranking using long-text representations from jina-embeddings-v4, and pointwise re-ranking of the top-20 candidates using Qwen3-Reranker-4B while preserving the dense order for the remaining results. On the official evaluation, the system achieves nDCG@20 of 0.403 and Judged@20 of 0.95. We further conduct extensive ablation experiments to quantify the contribution of each stage and to analyze the effectiveness of query expansion, dense ranking, and top-$k$ reranking under limited compute budgets.

</details>


### [96] [LiveGraph: Active-Structure Neural Re-ranking for Exercise Recommendation](https://arxiv.org/abs/2602.17036)
*Rong Fu,Zijian Zhang,Haiyun Wei,Jiekai Wu,Kun Liu,Xianda Li,Haoyu Zhao,Yang Li,Yongtai Liu,Ziming Wang,Rui Lu,Simon Fong*

Main category: cs.IR

TL;DR: 本文提出了一种名为LiveGraph的新颖神经重排序框架，旨在解决当前练习推荐系统在学生参与度长尾分布及适应个性化学习路径方面的不足。通过采用基于图的表示增强策略和动态重排序机制，LiveGraph不仅能够缩小活跃与非活跃学生之间的信息差距，还促进了内容多样性。实验结果显示，在预测准确性和练习多样性方面，LiveGraph优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 随着数字学习环境的不断扩大，对能够提供个性化教育内容的智能系统的需求日益增长。尽管现有的练习推荐框架已经取得了一定成就，但它们在处理学生参与度的长尾分布以及适应独特学习轨迹方面仍面临挑战。

Method: 研究者们提出了LiveGraph，这是一个新的主动结构神经重排序框架，它利用基于图的表示增强策略来弥合活跃与不活跃学生间的信息鸿沟，并结合了动态重排机制以促进内容多样性。该模型强调学习历史中的结构关系，从而有效平衡推荐精度与教学多样性。

Result: 通过对多个真实世界数据集进行综合实验证明，LiveGraph在预测准确性及练习多样性宽度上均超越了当前基准方法。

Conclusion: LiveGraph作为一种创新的练习推荐解决方案，成功地解决了传统方法中遇到的学生参与度不平衡问题，并且能更好地适应不同的学习路径，为实现更精准、更多样化的教育内容推荐提供了强有力的支持。

Abstract: The continuous expansion of digital learning environments has catalyzed the demand for intelligent systems capable of providing personalized educational content. While current exercise recommendation frameworks have made significant strides, they frequently encounter obstacles regarding the long-tailed distribution of student engagement and the failure to adapt to idiosyncratic learning trajectories. We present LiveGraph, a novel active-structure neural re-ranking framework designed to overcome these limitations. Our approach utilizes a graph-based representation enhancement strategy to bridge the information gap between active and inactive students while integrating a dynamic re-ranking mechanism to foster content diversity. By prioritizing the structural relationships within learning histories, the proposed model effectively balances recommendation precision with pedagogical variety. Comprehensive experimental evaluations conducted on multiple real-world datasets demonstrate that LiveGraph surpasses contemporary baselines in both predictive accuracy and the breadth of exercise diversity.

</details>


### [97] [On the Reliability of User-Centric Evaluation of Conversational Recommender Systems](https://arxiv.org/abs/2602.17264)
*Michael Müller,Amir Reza Mohammadi,Andreas Peintner,Beatriz Barroso Gstrein,Günther Specht,Eva Zangerle*

Main category: cs.IR

TL;DR: 本文对基于静态对话记录的用户中心型对话推荐系统评估的可靠性和结构进行了大规模实证研究，发现实用性及结果导向维度如准确性、有用性和满意度在聚合后达到了中等可靠性，而社会性基础构建如人性和默契度则明显不太可靠。此外，许多维度被简化为单一的整体质量信号，揭示了第三方评判中的强烈光环效应。


<details>
  <summary>Details</summary>
Motivation: 由于用户中心型评估已成为衡量对话推荐系统的关键范式，旨在捕捉诸如满意度、信任和默契等主观品质，但这种做法的可靠性很大程度上未被检验。因此，本研究旨在调查基于静态对话记录的用户中心型对话推荐系统评估的可靠性和结构。

Method: 通过收集124名众包工作者对200个ReDial对话按照18维CRS-Que框架进行的1,053次注释，并运用随机效应可靠性模型和相关性分析来量化各个维度的稳定性和它们之间的相互依赖关系。

Result: 结果显示，像准确性、有用性和满意度这样的实用性和结果导向维度在聚合后具有中等可靠性；而人性化和默契度这类以社会为基础的构念可靠性显著较低。而且很多维度最终都归结为一个全局的质量信号，显示出第三方评价中存在的强烈光环效应。

Conclusion: 这些发现质疑了单标注者和大语言模型基线评估协议的有效性，并强调了离线对话推荐系统评估中多评分者聚合和维度缩减的必要性。

Abstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisfaction achieve moderate reliability under aggregation, whereas socially grounded constructs such as humanness and rapport are substantially less reliable. Furthermore, many dimensions collapse into a single global quality signal, revealing a strong halo effect in third-party judgments. These findings challenge the validity of single-annotator and LLM-based evaluation protocols and motivate the need for multi-rater aggregation and dimension reduction in offline CRS evaluation.

</details>


### [98] [WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval](https://arxiv.org/abs/2602.17327)
*Michael Dinzinger,Laura Caspari,Ali Salman,Irvin Topi,Jelena Mitrović,Michael Granitzer*

Main category: cs.IR

TL;DR: WebFAQ 2.0, a significantly expanded and more diverse FAQ dataset covering 108 languages, introduces a novel data collection method and includes a hard negatives dataset for training dense retrievers. It supports two main fine-tuning strategies and is part of an ongoing effort with regular updates.


<details>
  <summary>Details</summary>
Motivation: The motivation behind WebFAQ 2.0 is to provide a more comprehensive, multilingual, and contextually rich FAQ-based resource for natural question-answer pairs, addressing the limitations of its predecessor by expanding language coverage, increasing the number of bilingual aligned QA pairs, and offering a hard negatives dataset for better training of dense retrievers in response to community feedback.

Method: WebFAQ 2.0 employs a new data collection strategy that directly crawls and extracts web content, leading to a more diverse and multilingual dataset. Additionally, it provides a hard negatives dataset mined using a two-stage retrieval pipeline, which is designed to improve the training of dense retrievers. The resource enables two primary fine-tuning strategies: Contrastive Learning with MultipleNegativesRanking loss and Knowledge Distillation with MarginMSE loss.

Result: The result is a significantly larger and more diverse FAQ-based resource, with over 198 million QA pairs across 108 languages, including over 14.3 million bilingual aligned QA pairs. The introduction of a hard negatives dataset, containing 1.25 million queries across 20 languages, enhances the utility of WebFAQ 2.0 for training and improving dense retrievers. This version also initiates a long-term effort for continuous expansion and refinement via the Open Web Index.

Conclusion: WebFAQ 2.0 represents a major advancement in FAQ-based datasets, offering extensive multilingual coverage, richer contextual information, and valuable resources for training and fine-tuning dense retrievers. Its ongoing development through the Open Web Index ensures it remains a dynamic and growing resource for the research community.

Abstract: We introduce WebFAQ 2.0, a new version of the WebFAQ dataset, containing 198 million FAQ-based natural question-answer pairs across 108 languages. Compared to the previous version, it significantly expands multilingual coverage and the number of bilingual aligned QA pairs to over 14.3M, making it the largest FAQ-based resource. Unlike the original release, WebFAQ 2.0 uses a novel data collection strategy that directly crawls and extracts relevant web content, resulting in a substantially more diverse and multilingual dataset with richer context through page titles and descriptions. In response to community feedback, we also release a hard negatives dataset for training dense retrievers, with 1.25M queries across 20 languages. These hard negatives were mined using a two-stage retrieval pipeline and include cross-encoder scores for 200 negatives per query. We further show how this resource enables two primary fine-tuning strategies for dense retrievers: Contrastive Learning with MultipleNegativesRanking loss, and Knowledge Distillation with MarginMSE loss. WebFAQ 2.0 is not a static resource but part of a long-term effort. Since late 2025, structured FAQs are being regularly released through the Open Web Index, enabling continuous expansion and refinement. We publish the datasets and training scripts to facilitate further research in multilingual and cross-lingual IR. The dataset itself and all related resources are publicly available on GitHub and HuggingFace.

</details>


### [99] [Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation](https://arxiv.org/abs/2602.17354)
*Daniele Malitesta,Emanuele Rossi,Claudio Pomo,Tommaso Di Noia,Fragkiskos D. Malliaros*

Main category: cs.IR

TL;DR: 本文针对多模态推荐系统中由于数据缺失导致的问题，提出了一种基于图结构的方法来填补缺失的多模态信息。通过将问题重新定义为项目-项目共购图上的特征插值问题，并提出了四种无需训练的方法来传播可用的多模态特征以填补缺失的部分。实验表明，这些解决方案不仅能够无缝地集成到现有的多模态推荐系统中，而且在不同缺失模态设置下比传统的机器学习填充方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统中存在因为噪音或丢失导致的数据不完整问题，通常做法是删除有缺失模态的条目然后仅在数据集的一个子样本上训练模型。然而，对于多模态推荐中缺失模态的问题，在文献中还没有得到充分的关注和精确的形式化描述。

Method: 文章首先对多模态推荐中的缺失模态问题进行了形式化描述。接着利用用户-项目图结构，将缺失多模态信息的问题转化为项目-项目共购图上的特征插值问题。基于此，作者提出了四种不需要训练的方法，通过在整个项目-项目图上传播可用的多模态特征来估计缺失特征。

Result: 广泛的实验证明了所提出的方案可以很容易地与任何现有的多模态推荐系统和基准测试框架整合，同时保持（甚至扩大）多模态推荐系统与传统推荐系统之间的性能差距。此外，研究还首次分析了项目-项目图上计算的特征同质性如何影响基于图的填补方法。

Conclusion: 这项工作为处理多模态推荐系统中缺失模态提供了一个新的视角，证明了基于图结构的方法在多种缺失模态设定下的有效性。

Abstract: Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisation as done with missing information in traditional machine learning. In this work, we first provide a problem formalisation for missing modalities in multimodal recommendation. Second, by leveraging the user-item graph structure, we re-cast the problem of missing multimodal information as a problem of graph features interpolation on the item-item co-purchase graph. On this basis, we propose four training-free approaches that propagate the available multimodal features throughout the item-item graph to impute the missing features. Extensive experiments on popular multimodal recommendation datasets demonstrate that our solutions can be seamlessly plugged into any existing multimodal RS and benchmarking framework while still preserving (or even widen) the performance gap between multimodal and traditional RSs. Moreover, we show that our graph-based techniques can perform better than traditional imputations in machine learning under different missing modalities settings. Finally, we analyse (for the first time in multimodal RSs) how feature homophily calculated on the item-item graph can influence our graph-based imputations.

</details>


### [100] [Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers](https://arxiv.org/abs/2602.17410)
*Bingqian Li,Bowen Zheng,Xiaolei Wang,Long Zhang,Jinpeng Wang,Sheng Chen,Wayne Xin Zhao,Ji-rong Wen*

Main category: cs.IR

TL;DR: 本文提出了一种基于大型语言模型的推荐系统的新偏好微调框架ILRec，通过从中间层提取自难负样本信号来改进偏好学习。实验表明该方法能有效提高基于LLM的推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于序列级、离线生成的负样本，在适应具有大量负项目空间的推荐任务时不够区分性和信息性。

Method: 提出了ILRec，一种新的偏好微调框架，利用来自中间层的自难负信号改善偏好学习。包括跨层偏好优化和跨层偏好蒸馏两个阶段，并引入轻量级协同过滤模型为负信号分配令牌级别奖励。

Result: 在三个数据集上的广泛实验表明，ILRec能够有效提升基于LLM的推荐系统的性能。

Conclusion: 通过动态地从中间层选取更细粒度的负监督并将其整合到训练中，ILRec提供了一种改进大型语言模型推荐效果的有效途径。

Abstract: Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommendation, leveraging self-hard negative signals extracted from intermediate layers to improve preference learning. Specifically, we identify self-hard negative tokens from intermediate layers as fine-grained negative supervision that dynamically reflects the model's preference learning process. To effectively integrate these signals into training, we design a two-stage framework comprising cross-layer preference optimization and cross-layer preference distillation, enabling the model to jointly discriminate informative negatives and enhance the quality of negative signals from intermediate layers. In addition, we introduce a lightweight collaborative filtering model to assign token-level rewards for negative signals, mitigating the risk of over-penalizing false negatives. Extensive experiments on three datasets demonstrate ILRec's effectiveness in enhancing the performance of LLM-based recommender systems.

</details>


### [101] [Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval](https://arxiv.org/abs/2602.17654)
*Jiaqi Xi,Raghav Saboo,Luming Chen,Martin Wang,Sudeep Das*

Main category: cs.IR

TL;DR: 提出了一种两阶段的"Mine and Refine"对比训练框架，用于增强多类别电子商务搜索检索中的语义文本嵌入。通过轻量级LLM微调实现可扩展且符合政策的一致性监督，并使用标签感知的监督对比目标训练多语言Siamese双塔检索器，再通过ANN挖掘难样本并重新标注，引入circle loss的多类扩展以明确区分不同相关级别之间的相似度界限，进一步完善和丰富了嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 大规模电子商务搜索需要能够适应长尾、噪声查询的嵌入技术，同时保持与产品和政策约束相兼容的可扩展监督。实际挑战在于相关性通常是分级的：用户接受替代品或补充品而不仅仅是精确匹配，生产系统从这些相关层面上清晰分离相似度得分中受益，以便于稳定混合和阈值设定。

Method: 1. 微调轻量级LLM处理人类注释下的三级相关指南，并通过参与驱动审核减少残余噪音。
2. 第一阶段采用标签意识的监督对比目标训练一个多语言Siamese双塔检索器。
3. 第二阶段利用近似最近邻（ANN）挖掘难样本，并用策略一致的LLM重新标注，引入一种circle loss的多类扩展方法来明确区分不同相关级别间的相似边界。
4. 通过附加拼写增强和合成查询生成进一步提高鲁棒性。

Result: 广泛的离线评估和生产A/B测试表明，该框架提高了检索相关性，并在参与度和商业影响方面取得了统计学上显著的提升。

Conclusion: 所提出的“Mine and Refine”两阶段对比训练框架有效提升了多类别电子商务搜索中的检索质量及用户体验，同时满足了可扩展性和政策一致性要求。

Abstract: We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [102] [CAFE: Channel-Autoregressive Factorized Encoding for Robust Biosignal Spatial Super-Resolution](https://arxiv.org/abs/2602.17011)
*Hongjun Liu,Leyu Zhou,Zijianghao Yang,Rujun Han,Shitong Duan,Kuanjian Tang,Chao Yao*

Main category: cs.MM

TL;DR: 提出了一种名为CAFE的即插即用生成方案，能够从低密度生物信号记录中重建全蒙太奇，通过逐步扩展恢复更远区域，减少伪影传播和错误的非局部相关性。在四种模态和六个数据集上测试，显示出比五种代表性基线更好的重建效果，并能跨三种骨干（MLP、Conv、Transformer）通用。


<details>
  <summary>Details</summary>
Motivation: 高密度生物信号记录对于神经解码和临床监测至关重要，但实际部署通常受限于硬件和操作条件，只能使用低密度布局。这导致了从低密度观察中进行空间超分辨率的需求，但由于稀疏且噪声较大的测量值下异质依赖关系的存在，常常会引起伪影传播和错误的非局部相关性问题。

Method: 提出了CAFE方案，该方案以几何对齐的方式分阶段重建整个蒙太奇。从低密度通道开始，先恢复邻近通道，然后逐步扩展到更远的区域，优先利用可靠的局部结构再引入非局部交互。训练过程中，通过对通道组进行分步监督，并沿组维度采用教师强迫与预定采样相结合的方法来减少暴露偏差，允许步骤间的并行计算。测试时，CAFE在各组之间执行自回归展开，同时保持即插即用特性，通过重复使用任何时间主干作为共享预测器。

Result: CAFE在4种模态和6个数据集上进行了评估，展示了跨越3种骨干（MLP、卷积、变换器）的一致更好重建性能，优于5种代表性基线方法。

Conclusion: CAFE提供了一种有效的解决方案，用于从低密度生物信号记录中高质量地重建全蒙太奇，其设计考虑到了实际应用中的局限性和挑战。

Abstract: High-density biosignal recordings are critical for neural decoding and clinical monitoring, yet real-world deployments often rely on low-density (LD) montages due to hardware and operational constraints. This motivates spatial super-resolution from LD observations, but heterogeneous dependencies under sparse and noisy measurements often lead to artifact propagation and false non-local correlations. To address this, we propose CAFE, a plug-and-play rollout generation scheme that reconstructs the full montage in geometry-aligned stages. Starting from the LD channels, CAFE first recovers nearby channels and then progressively expands to more distal regions, exploiting reliable local structure before introducing non-local interactions. During training, step-wise supervision is applied over channel groups and teacher forcing with epoch-level scheduled sampling along the group dimension is utilized to reduce exposure bias, enabling parallel computation across steps. At test time, CAFE performs an autoregressive rollout across groups, while remaining plug-and-play by reusing any temporal backbone as the shared predictor. Evaluated on $4$ modalities and $6$ datasets, CAFE demonstrates plug-and-play generality across $3$ backbones (MLP, Conv, Transformer) and achieves consistently better reconstruction than $5$ representative baselines.

</details>
