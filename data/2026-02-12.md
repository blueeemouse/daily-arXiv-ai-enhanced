<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Making Databases Faster with LLM Evolutionary Sampling](https://arxiv.org/abs/2602.10387)
*Mehmet Hamza Erol,Xiangpeng Hao,Federico Bianchi,Ciro Greco,Jacopo Tagliabue,James Zou*

Main category: cs.DB

TL;DR: 该论文提出了一种利用大型语言模型(LLM)对数据库查询进行物理计划优化的新方法。通过DBPlanBench工具，将物理执行计划暴露给LLM，并允许其提出局部修改建议。再结合进化搜索算法迭代优化这些修改建议。实验表明此方法在某些查询上可实现高达4.78倍的速度提升，并且小规模数据库上的优化可以有效地转移到大规模数据库中。


<details>
  <summary>Details</summary>
Motivation: 传统基于成本的查询优化器依赖预定义的启发式规则和统计模型来估计执行成本（如运行时间、内存和I/O）。改进这些启发式规则需要大量的工程努力，而且即使实现了新规则，也往往无法考虑到查询和模式中的语义相关性，这限制了生成更优物理执行计划的能力。

Method: 研究人员开发了一个名为DBPlanBench的框架，用于DataFusion引擎。该框架能够以紧凑序列化的形式展示物理执行计划，并允许大型语言模型提出具体的局部修改意见。随后，通过进化搜索算法对提出的修改方案进行迭代优化。

Result: 实验结果显示，在某些特定查询上，采用这种方法可以获得最高达4.78倍的速度提升。此外，研究还展示了从小型到大型数据库工作流程中，小型数据库上的优化策略可以有效迁移至更大规模的数据集上。

Conclusion: 本研究表明，利用大型语言模型理解和应用语义知识的能力，可以发现并实施非显而易见但有效的查询优化措施，比如最小化中间结果集基数的连接顺序调整等。这一方法不仅提高了查询性能，同时也为不同规模数据库间的优化转移提供了可能性。

Abstract: Traditional query optimization relies on cost-based optimizers that estimate execution cost (e.g., runtime, memory, and I/O) using predefined heuristics and statistical models. Improving these heuristics requires substantial engineering effort, and even when implemented, these heuristics often cannot take into account semantic correlations in queries and schemas that could enable better physical plans. Using our DBPlanBench harness for the DataFusion engine, we expose the physical plan through a compact serialized representation and let the LLM propose localized edits that can be applied and executed. We then apply an evolutionary search over these edits to refine candidates across iterations. Our key insight is that LLMs can leverage semantic knowledge to identify and apply non-obvious optimizations, such as join orderings that minimize intermediate cardinalities. We obtain up to 4.78$\times$ speedups on some queries and we demonstrate a small-to-large workflow in which optimizations found on small databases transfer effectively to larger databases.

</details>


### [2] [Benchmarking Large Language Models for Knowledge Graph Validation](https://arxiv.org/abs/2602.10748)
*Farzad Shami,Stefano Marchesin,Gianmaria Silvello*

Main category: cs.DB

TL;DR: 本文介绍了一个名为FactCheck的基准测试，用于评估大型语言模型在知识图谱事实验证中的表现。实验结果表明，尽管大型语言模型显示出了潜力，但它们还不足够稳定可靠以应用于实际的知识图谱验证场景中。


<details>
  <summary>Details</summary>
Motivation: 知识图谱对于许多应用至关重要，这些应用依赖于知识图谱的事实准确性。然而，事实验证既重要又具有挑战性。虽然专家手动验证是最理想的，但在大规模上是不切实际的。自动化方法显示出潜力，但对于现实世界中的知识图谱来说还不够成熟。大型语言模型提供了潜在的解决方案，但其适用性和有效性尚未得到充分探索。

Method: 提出了FactCheck基准测试来从三个关键方面评估大型语言模型对知识图谱事实验证的能力：（1）大型语言模型内部知识；（2）通过检索增强生成（RAG）获取外部证据；（3）采用多模型共识策略整合知识。研究者们在三个不同的真实世界知识图谱上评估了开源和商业的大型语言模型，并且FactCheck还包括一个专为知识图谱事实验证定制的包含超过2百万文档的RAG数据集。此外，还提供了一个交互式探索平台以便分析验证决策。

Result: 实验分析表明，尽管大型语言模型产生了有希望的结果，但它们仍不够稳定和可靠，无法在实际的知识图谱验证情境中使用。通过RAG方法集成外部证据的表现波动不定，在计算成本更高的情况下只带来了不一致的改进。同样地，基于多模型共识的策略并不总是优于单个模型，这强调了不存在一种适用于所有情况的解决方案。

Conclusion: 研究发现进一步突显了像FactCheck这样的基准测试对于系统地评估并推动这一困难而至关重要的任务进展的必要性。

Abstract: Knowledge Graphs (KGs) store structured factual knowledge by linking entities through relationships, crucial for many applications. These applications depend on the KG's factual accuracy, so verifying facts is essential, yet challenging. Expert manual verification is ideal but impractical on a large scale. Automated methods show promise but are not ready for real-world KGs. Large Language Models (LLMs) offer potential with their semantic understanding and knowledge access, yet their suitability and effectiveness for KG fact validation remain largely unexplored.
  In this paper, we introduce FactCheck, a benchmark designed to evaluate LLMs for KG fact validation across three key dimensions: (1) LLMs internal knowledge; (2) external evidence via Retrieval-Augmented Generation (RAG); and (3) aggregated knowledge employing a multi-model consensus strategy. We evaluated open-source and commercial LLMs on three diverse real-world KGs. FactCheck also includes a RAG dataset with 2+ million documents tailored for KG fact validation. Additionally, we offer an interactive exploration platform for analyzing verification decisions.
  The experimental analyses demonstrate that while LLMs yield promising results, they are still not sufficiently stable and reliable to be used in real-world KG validation scenarios. Integrating external evidence through RAG methods yields fluctuating performance, providing inconsistent improvements over more streamlined approaches -- at higher computational costs. Similarly, strategies based on multi-model consensus do not consistently outperform individual models, underscoring the lack of a one-fits-all solution. These findings further emphasize the need for a benchmark like FactCheck to systematically evaluate and drive progress on this difficult yet crucial task.

</details>


### [3] [GraphSeek: Next-Generation Graph Analytics with LLMs](https://arxiv.org/abs/2602.11052)
*Maciej Besta,Łukasz Jarmocik,Orest Hrycyna,Shachar Klaiman,Konrad Mączka,Robert Gerstenberger,Jürgen Müller,Piotr Nyczyk,Hubert Niewiadomski,Torsten Hoefler*

Main category: cs.DB

TL;DR: 本文提出了一种新的抽象方法，通过语义目录规划而非直接从自然语言生成图查询来改进大型属性图的多查询分析。基于此，作者开发了GraphSeek框架，它结合了LLM推理和数据库级执行，实现了更高效和有效的图数据分析。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）有望让非专家用户也能进行自然语言图分析，但它们在处理工业规模的属性图时存在效率低下、效果不佳的问题。这些问题源于数据集庞大、异构性强、结构复杂且动态变化的特点。

Method: 设计了一种新抽象，利用语义目录描述图模式与操作，将语义层面的LLM规划与执行层面的确定性查询分离。该方法提高了令牌效率及任务执行效果，即使使用小上下文LLM也是如此。

Result: GraphSeek框架相较于增强版LangChain成功率达到86%，展示了下一代经济实惠且易于访问的图分析工具潜力，能够统一LLM推理与大规模复杂属性图上的数据库级执行。

Conclusion: 通过引入基于语义目录的新抽象并实现GraphSeek框架，证明了将LLM与高级数据库执行相结合可以有效提升对大型复杂属性图进行自然语言分析的成功率与可访问性。

Abstract: Graphs are foundational across domains but remain hard to use without deep expertise. LLMs promise accessible natural language (NL) graph analytics, yet they fail to process industry-scale property graphs effectively and efficiently: such datasets are large, highly heterogeneous, structurally complex, and evolve dynamically. To address this, we devise a novel abstraction for complex multi-query analytics over such graphs. Its key idea is to replace brittle generation of graph queries directly from NL with planning over a Semantic Catalog that describes both the graph schema and the graph operations. Concretely, this induces a clean separation between a Semantic Plane for LLM planning and broader reasoning, and an Execution Plane for deterministic, database-grade query execution over the full dataset and tool implementations. This design yields substantial gains in both token efficiency and task effectiveness even with small-context LLMs. We use this abstraction as the basis of the first LLM-enhanced graph analytics framework called GraphSeek. GraphSeek achieves substantially higher success rates (e.g., 86% over enhanced LangChain) and points toward the next generation of affordable and accessible graph analytics that unify LLM reasoning with database-grade execution over large and complex property graphs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis](https://arxiv.org/abs/2602.10246)
*Mayur Akewar,Sandeep Madireddy,Dongsheng Luo,Janki Bhimani*

Main category: cs.DC

TL;DR: 本文介绍了一种名为KORAL的知识驱动推理框架，该框架结合了大型语言模型和结构化知识图谱来分析SSD的运行情况。它能从碎片化的遥测数据生成数据知识图谱，并与文献知识图谱集成，从而提供基于证据、可解释的分析结果。实际生产追踪评估显示KORAL可以提供专家级的诊断和建议，提高推理透明度，减少人工努力，并提供改善服务质量的实际见解。


<details>
  <summary>Details</summary>
Motivation: 固态硬盘(SSD)对于数据中心、消费者平台及关键任务系统至关重要，但其性能和可靠性的诊断非常困难，因为数据是分散且时间不连续的，现有方法需要大量数据集和专家输入才能提供有限的见解。此外，SSD的退化不仅由于工作负载变化和架构演变，还受到温度、湿度和振动等环境因素的影响。

Method: 提出了KORAL，一种结合了大型语言模型（LLMs）与结构化知识图谱（KGs）的知识驱动推理框架，用于生成关于SSD操作的见解。KORAL能够从碎片化的遥测数据中生成数据KG，并将其与已经组织好的来自文献、报告和跟踪记录的知识KG相结合，将非结构化来源转变为可查询的图形，同时把遥测信息转化为结构化知识。

Result: 通过使用真实的生产追踪进行评估表明，KORAL能够提供专家级别的诊断与建议，并附有基于事实的解释以增强推理透明度、指导操作员决策、减少手动劳动量并为提高服务品质提供可执行的洞察。

Conclusion: KORAL作为首个端到端地结合了LLMs与KGs用于全谱系SSD推理分析（包括描述性、预测性、规范性和假设情景分析）的系统，显著提高了对SSD状态的理解能力。研究团队还发布了专门针对SSD的知识图谱，以促进基于知识存储系统分析领域的可重复性研究。

Abstract: Solid State Drives (SSDs) are critical to datacenters, consumer platforms, and mission-critical systems. Yet diagnosing their performance and reliability is difficult because data are fragmented and time-disjoint, and existing methods demand large datasets and expert input while offering only limited insights. Degradation arises not only from shifting workloads and evolving architectures but also from environmental factors such as temperature, humidity, and vibration. We present KORAL, a knowledge driven reasoning framework that integrates Large Language Models (LLMs) with a structured Knowledge Graph (KG) to generate insights into SSD operations. Unlike traditional approaches that require extensive expert input and large datasets, KORAL generates a Data KG from fragmented telemetry and integrates a Literature KG that already organizes knowledge from literature, reports, and traces. This turns unstructured sources into a queryable graph and telemetry into structured knowledge, and both the Graphs guide the LLM to deliver evidence-based, explainable analysis aligned with the domain vocabulary and constraints. Evaluation using real production traces shows that the KORAL delivers expert-level diagnosis and recommendations, supported by grounded explanations that improve reasoning transparency, guide operator decisions, reduce manual effort, and provide actionable insights to improve service quality. To our knowledge, this is the first end-to-end system that combines LLMs and KGs for full-spectrum SSD reasoning including Descriptive, Predictive, Prescriptive, and What-if analysis. We release the generated SSD-specific KG to advance reproducible research in knowledge-based storage system analysis. GitHub Repository: https://github.com/Damrl-lab/KORAL

</details>


### [5] [Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A](https://arxiv.org/abs/2602.10262)
*Aaron Jarmusch,Connor Vitz,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: 本文通过微基准测试对AMD MI300A APU的FP8矩阵执行、异步计算引擎并发性以及结构化稀疏性进行了以执行为中心的特性描述，并提供了关于占用率感知调度、并发决策和稀疏性启用的实际指导。


<details>
  <summary>Details</summary>
Motivation: 随着现代高性能计算（HPC）及HPC-AI工作负载越来越依赖于先进的加速器特性，如FP8矩阵核心、异步计算引擎（ACE）、2:4结构化稀疏性等，但这些特性的执行特点及其系统级影响仍不够清楚。

Method: 作者们采用了针对性的微基准测试方法来量化MI300A上FP8矩阵执行、ACE并发性和结构化稀疏性的占用率阈值、公平性、并行执行下的吞吐量权衡以及上下文相关的稀疏性优势。此外，还通过代表性的案例研究——包括变换器风格、并发处理以及混合精度内核——展示了这些效应如何转化为应用层面的表现和可预测性。

Result: 研究结果为在MI300A类统一节点上的占用率意识调度、并发决策及启用稀疏性提供了实用指导。具体来说，它帮助理解了不同配置下性能的变化情况，比如在特定条件下利用稀疏性可以带来多少性能增益。

Conclusion: 通过对AMD MI300A APU关键加速器特性的深入分析，本研究不仅揭示了这些特性对于不同类型工作负载的影响，而且为优化该平台上的应用程序性能提出了具体的建议。

Abstract: The AMD MI300A APU integrates CDNA3 GPUs with high-bandwidth memory and advanced accelerator features: FP8 matrix cores, asynchronous compute engines (ACE), and 2:4 structured sparsity. These capabilities are increasingly relied upon by modern HPC and HPC-AI workloads, yet their execution characteristics and system-level implications remain insufficiently understood. In this paper, we present an execution-centric characterization of FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A using targeted microbenchmarks. We quantify occupancy thresholds, fairness, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. We evaluate representative case studies - transformer-style, concurrent, and mixed-precision kernels - to show how these effects translate into application-level performance and predictability. Our results provide practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes.

</details>


### [6] [Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores](https://arxiv.org/abs/2602.10378)
*Elliot L. Epstein,Rajat Vadiraj Dwaraknath,John Winnicki*

Main category: cs.DC

TL;DR: 通过重新排序SD-KDE计算以利用矩阵乘法结构，并使用Tensor Cores加速GPU实现，Flash-SD-KDE在单个GPU上对于大规模样本和查询任务实现了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: Score-debiased kernel density estimation (SD-KDE)虽然在渐近收敛率方面优于传统KDE，但其实现速度慢于经典方法，特别是由于使用了经验得分函数。为了解决这个问题并使SD-KDE在实践中更加高效，作者探索了如何通过硬件加速来提高其计算效率。

Method: 研究者们提出了一个名为Flash-SD-KDE的方法，通过对SD-KDE的计算进行重新排序，使其能够更好地利用矩阵乘法结构，并且能够在GPU上使用Tensor Cores进行加速。

Result: 实验结果表明，在处理32,000样本16维的问题时，该方法比强大的SD-KDE GPU基准快高达47倍，比scikit-learn中的KDE快3,300倍。在一个更大的包含1百万样本16维的任务中，当评估131,000次查询时，Flash-SD-KDE仅需2.3秒即可完成。

Conclusion: 通过采用新的计算顺序和利用现代GPU架构的特点，Flash-SD-KDE极大地提高了SD-KDE的速度，使得在以前不可行的大规模场景下应用得分偏差核密度估计成为可能。

Abstract: Score-debiased kernel density estimation (SD-KDE) achieves improved asymptotic convergence rates over classical KDE, but its use of an empirical score has made it significantly slower in practice. We show that by re-ordering the SD-KDE computation to expose matrix-multiplication structure, Tensor Cores can be used to accelerate the GPU implementation. On a 32k-sample 16-dimensional problem, our approach runs up to $47\times$ faster than a strong SD-KDE GPU baseline and $3{,}300\times$ faster than scikit-learn's KDE. On a larger 1M-sample 16-dimensional task evaluated on 131k queries, Flash-SD-KDE completes in $2.3$ s on a single GPU, making score-debiased density estimation practical at previously infeasible scales.

</details>


### [7] [Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems](https://arxiv.org/abs/2602.10486)
*Vijay K. Garg,Rohan Garg*

Main category: cs.DC

TL;DR: 本文提出了在并行和分布式环境中计算多个单调递增函数最小不动点的方法，并证明了三种收敛定理，适用于不同的同步放松程度。这些方法基于坐标覆盖而非联合合并或收缩假设，适用于诸如传递闭包、稳定婚姻匹配等问题。


<details>
  <summary>Details</summary>
Motivation: 经典Knaster-Tarski定理仅针对单个函数的顺序迭代问题，而现代计算系统需要能够处理并行执行、非原子更新以及陈旧读取的情况。因此，研究者们旨在开发一种新的方法来解决这些问题，同时保持在并行更新时也能精确地收敛到最小不动点。

Method: 通过定义三种不同级别的同步放松条件——交错语义下的公平调度、仅当值改变时才进行更新的并行执行方式、以及具有界限陈旧性和局部性的分布式执行——作者证明了在这几种条件下如何保证最小不动点的收敛性。与以往的工作相比，这种方法不依赖于联合操作或收缩假设，而是采用基于坐标的覆盖策略。

Result: 研究结果表明，在上述三种不同放松的同步条件下，所提出的方法能够确保即使是在使用覆盖而非联合操作的情况下，也能准确地达到最小不动点。这为并行及分布式算法的设计提供了新的理论基础。

Conclusion: 本研究首次为基于覆盖的并行更新提供了一个不需要联合操作或收缩假设就能精确收敛到最小不动点的理论保障。该成果对于设计并行和分布式的算法以解决如传递闭包、稳定婚姻匹配等实际问题具有重要意义。

Abstract: We present methods to compute least fixed points of multiple monotone inflationary functions in parallel and distributed settings. While the classic Knaster-Tarski theorem addresses a single function with sequential iteration, modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads. We prove three convergence theorems under progressively relaxed synchronization: (1) Interleaving semantics with fair scheduling, (2) Parallel execution with update-only-on-change semantics (processes write only on those coordinates whose values change), and (3) Distributed execution with bounded staleness (updates propagate within $T$ rounds) and $i$-locality (each process modifies only its own component).
  Our approach differs from prior work in fundamental ways: Cousot-Cousot's chaotic iteration uses join-based merges that preserve information. Instead, we use coordinate-wise overwriting. Bertsekas's asynchronous methods assume contractions. We use coordinate-wise overwriting with structural constraints (locality, bounded staleness) instead. Applications include parallel and distributed algorithms for the transitive closure, stable marriage, shortest paths, and fair division with subsidy problems. Our results provide the first exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.

</details>


### [8] [BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.10729)
*Youhe Jiang,Fangcheng Fu,Eiko Yoneki*

Main category: cs.DC

TL;DR: 本文提出了一种名为BOute的质量感知调度系统，该系统通过联合利用异构模型和GPU的能力来实现成本效益高的大型语言模型服务。采用多目标贝叶斯优化框架共同优化路由策略和模型部署，从而在保证响应质量的同时最大化服务系统的成本效率。实验结果表明，BOute在相同的成本预算和质量要求下，比最先进的LLM服务系统平均提高了59%至157%的性能，或在保持相同性能目标时降低了15%至61%的服务成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）部署的快速增长，开发成本高效的服务器系统变得至关重要。现有提升系统成本效率的方法主要从算法角度和系统角度出发，但为了进一步提高成本效率，需要对查询路由策略以及模型在不同GPU资源上的部署进行复杂管理。因此，作者提出了一个能同时考虑这两方面因素的解决方案，以达到更优的成本效率。

Method: BOute是一个质量感知的调度系统，它通过结合使用异构模型能力和GPU资源来提供成本效益高的LLM服务。该方法采用了多目标贝叶斯优化(MOBO)框架，旨在共同优化查询路由策略与模型部署方式，确保在满足延迟及质量要求的前提下，最大化整体系统性能。

Result: 评估结果显示，在同等成本预算和质量需求条件下，BOute相比当前最新的LLM服务系统表现出了显著优势，平均性能提升了59%到157%，或者在维持同样性能水平的情况下减少了15%-61%的服务开销。

Conclusion: BOute通过联合优化异构模型的选择与部署以及查询路由策略，成功地实现了高质量且具有高成本效益的大规模语言模型服务。这一成果为未来构建更加高效节能的语言模型服务平台提供了新的思路和技术支持。

Abstract: The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.

</details>


### [9] [Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots](https://arxiv.org/abs/2602.11125)
*Animesh Maiti,Abhinav Chakraborty,Bibhuti Das,Subhash Bhagat,Krishnendu Mukhopadhyaya*

Main category: cs.DC

TL;DR: 本文研究了一群移动机器人在有限线段和圆上最小化总移动距离的均匀覆盖问题，提出了在线段设置下达到最优解的分布式算法，并对圆形设置下问题可解的初始配置进行了刻画，同时提供了相应的最优解算法。


<details>
  <summary>Details</summary>
Motivation: 研究一群自主、匿名且相同的移动机器人如何在给定的有限线段或圆上通过协调运动实现均匀分布，以最小化所有机器人的总行进距离。

Method: 采用确定性分布式算法，在LCM（观察-计算-移动）模型下，对于线段设定，设计了能够使总移动成本最小化的算法；对于圆形设定，则首先定义了哪些初始配置会导致问题不可解，然后为其他所有情况提供了保证总行进距离最小的解决方案。

Result: 在线段设置中成功实现了最小总移动成本下的均匀覆盖；在圆形设置中，明确了导致问题不可解的所有初始配置，并为其余情况提供了最小化总行进距离的解决方案。

Conclusion: 本研究解决了在特定条件下移动机器人实现最小总移动距离均匀覆盖的问题，为未来进一步探索更复杂场景下的机器人协调策略奠定了基础。

Abstract: We study the \textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \textit{Look-Compute-Move} (LCM) model with \textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \textbf{line-segment setting}, the \textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Theory of Troubleshooting: The Developer's Cognitive Experience of Overcoming Confusion](https://arxiv.org/abs/2602.10540)
*Arty Starr,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 本文基于认知科学提出了一个故障排查理论，解释了软件开发者在故障排查过程中面临的挑战和项目风险。通过访谈27位专业开发人员并采用建构主义扎根理论方法构建了一个基于实证数据的理论。该理论为理解故障排查难度、疲劳以及可持续性风险提供了认知基础，并对研究和行业实践具有实际意义。


<details>
  <summary>Details</summary>
Motivation: 为了帮助软件开发者理解和应对故障排查过程中的挑战及由此产生的项目风险，同时阐明长时间进行故障排查可能导致的认知资源耗尽和认知疲劳问题。

Method: 采用建构主义扎根理论方法，通过对27名专业开发者的采访来收集关于他们故障排查经历的数据，并据此构建了一个基于实证数据的理论模型。

Result: 构建了一个基于认知科学的故障排查理论，该理论能够解释故障排查过程中遇到的问题及其对开发者认知负担的影响；同时揭示了故障排查活动如何消耗认知资源导致疲劳。

Conclusion: 提出的故障排查理论不仅加深了我们对软件开发过程中故障排查这一环节复杂性的理解，也为改善开发者体验提供了新的视角，特别是在减轻由故障排查引起的疲劳和支持更可持续的工作实践方面。

Abstract: This paper introduces a Theory of Troubleshooting that is rooted in cognitive science. This theory helps software developers explain the challenges they face and the project risks that emerge as troubleshooting becomes difficult. We define troubleshooting as the cognitive problem-solving process of identifying, understanding, and constructing a mental model of the cause of an unexpected system behavior, and consider the cognitive process of troubleshooting to be an integral part of the activity of debugging. Troubleshooting is a particularly intense and draining aspect of software work, placing sustained demands on attention, working memory, and mental modeling. By surfacing and naming the confusion experience inherent in troubleshooting in terms of neurological and attentional dynamics, our theory explains how prolonged troubleshooting can deplete cognitive resources and lead to cognitive fatigue. In the study presented in this paper, we interview 27 professional developers about their troubleshooting experiences, and follow a Constructivist Grounded Theory approach to construct a theory grounded in empirical data. Our theory contributes to research on Developer Experience by providing a cognitive foundation for understanding troubleshooting difficulty, fatigue, and sustainability risk--and offers practical implications for both research and industry.

</details>


### [11] [Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software](https://arxiv.org/abs/2602.10655)
*Muhammad Yousaf,Aitor Arrieta,Shaukat Ali,Paolo Arcaini,Shuai Wang*

Main category: cs.SE

TL;DR: 本研究对基于视觉-语言模型（VLMs）的自主水下机器人(AUR)感知模块进行了实证评估，以检测其在水下环境中识别垃圾的能力，并探讨了性能、不确定性和它们之间的关系，旨在帮助软件工程师为AUR软件选择合适的VLMs。


<details>
  <summary>Details</summary>
Motivation: 由于水下环境的独特挑战，包括低可见度和恶劣条件，这给开发AUR软件感知模块带来了困难。虽然深度学习被引入来支持AUR的操作，但该领域面临标记数据稀缺且噪声大的问题，可能影响到依赖于感知模块的AUR软件的可信度。鉴于此，以及工业伙伴对于保证与风险管理的需求，研究者们探索了VLMs作为潜在解决方案，特别是它们在处理未见过的对象及抵抗噪声干扰方面的潜力。

Method: 采用了一种实证评估方法，针对VLM为基础的感知模块在AUR软件中的表现进行了考察。通过计算这些模型检测水下垃圾的能力、不确定性及其相互间的关系来进行评估。

Result: 研究表明，基于VLM的感知模块能够提供一种有前景的方法来改善AUR软件在复杂水下环境下的性能；同时，也揭示了不同VLM之间在性能和不确定性上的差异，为软件工程师提供了选择适合特定应用场景的VLM时所需的信息。

Conclusion: VLMs展现出了作为解决AUR软件中感知挑战的有效工具之潜力，尤其是在提高对新物体的泛化能力和保持在噪音条件下稳定性方面。但是，仍需进一步研究以充分理解VLMs在水下环境中的表现极限及其适用性。

Abstract: Autonomous Underwater Robots (AURs) operate in challenging underwater environments, including low visibility and harsh water conditions. Such conditions present challenges for software engineers developing perception modules for the AUR software. To successfully carry out these tasks, deep learning has been incorporated into the AUR software to support its operations. However, the unique challenges of underwater environments pose difficulties for deep learning models, which often rely on labeled data that is scarce and noisy. This may undermine the trustworthiness of AUR software that relies on perception modules. Vision-Language Models (VLMs) offer promising solutions for AUR software as they generalize to unseen objects and remain robust in noisy conditions by inferring information from contextual cues. Despite this potential, their performance and uncertainty in underwater environments remain understudied from a software engineering perspective. Motivated by the needs of an industrial partner in assurance and risk management for maritime systems to assess the potential use of VLMs in this context, we present an empirical evaluation of VLM-based perception modules within the AUR software. We assess their ability to detect underwater trash by computing performance, uncertainty, and their relationship, to enable software engineers to select appropriate VLMs for their AUR software.

</details>


### [12] [VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection](https://arxiv.org/abs/2602.10787)
*Samal Mukhtar,Yinghua Yao,Zhu Sun,Mustafa Mustafa,Yew Soon Ong,Youcheng Sun*

Main category: cs.SE

TL;DR: 本文提出了一种基于知识图谱的软件漏洞推理和检测方法VulReaD，该方法不仅改进了二元分类的表现，还在CWE级别的多类分类上取得了显著进步。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在进行软件漏洞检测时，主要集中在二元评估上，并且生成的解释往往与CWE类别缺乏语义一致性。为了克服这些限制，作者提出了一个新的方法来改善这一情况。

Method: 通过利用安全知识图谱作为语义支撑，并使用强大的教师大型语言模型来生成符合CWE标准的对比推理监督信息，VulReaD能够在没有人工标注的情况下训练学生模型。此外，还采用Odds Ratio Preference Optimization（ORPO）对学生模型进行微调，旨在促进与分类体系一致的推理同时抑制不被支持的解释。

Result: 实验结果表明，在三个真实世界数据集上，相较于最先进的基线模型，VulReaD在二元F1分数上提高了8-10%，而在多类分类任务中，Macro-F1提高了30%，Micro-F1提高了18%。

Conclusion: 研究表明，大型语言模型在二进制检测方面优于深度学习基线，并且由知识图谱引导的推理方法能够提高CWE覆盖率和可解释性。

Abstract: Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.

</details>


### [13] [PELLI: Framework to effectively integrate LLMs for quality software generation](https://arxiv.org/abs/2602.10808)
*Rasmus Krebs,Somnath Mazumdar*

Main category: cs.SE

TL;DR: 本文提出了一种全面的代码质量评估框架PELLI，该框架基于迭代分析过程来支持高质量代码变更。研究通过选取五个流行的大型语言模型（LLMs）并根据三个主要非功能性需求（如可维护性、性能和可靠性）生成定量指标，对Python编码标准下的三个应用领域进行了综合评估。结果表明GPT-4T和Gemini在这些方面表现稍好，同时提示设计也会影响整体代码质量。


<details>
  <summary>Details</summary>
Motivation: 先前的研究在比较大型语言模型(LLM)时存在局限性，主要是只考虑了可靠性作为比较指标，并且选择的LLM种类有限。因此，有必要开发一个更加全面的评估框架，以便更好地理解这些模型在不同非功能性需求下的表现及其与人类开发者协作的能力。

Method: 提出了名为Programmatic Excellence via LLM Iteration (PELLI)的框架，它采用迭代分析方法来促进高质量代码修改。本研究选择了五个流行的大规模语言模型进行对比，并针对三个应用程序领域按照Python编程规范执行了详尽的评价工作，涵盖了维持性、效能及可靠性三大非功能性要求。

Result: 研究发现，在三个非功能性要求下，GPT-4T和Gemini的表现略胜一筹；此外还观察到提示的设计能够影响到最终产生的代码质量。不同应用场景下各个度量得分高低不一，即使在同一度量标准下使用不同提示也会产生差异。

Conclusion: 通过引入PELLI框架，实践者可以在遵循公认的质量标准的同时充分利用大型语言模型的能力。这项研究的结果对于推进真实世界中大型语言模型技术的应用至关重要，为利益相关者提供了清晰的理解，即哪些地方这些模型表现出色，哪些地方还需要进一步改进。

Abstract: Recent studies have revealed that when LLMs are appropriately prompted and configured, they demonstrate mixed results. Such results often meet or exceed the baseline performance. However, these comparisons have two primary issues. First, they mostly considered only reliability as a comparison metric and selected a few LLMs (such as Codex and ChatGPT) for comparision. This paper proposes a comprehensive code quality assessment framework called Programmatic Excellence via LLM Iteration (PELLI). PELLI is an iterative analysis-based process that upholds high-quality code changes. We extended the state-of-the-art by performing a comprehensive evaluation that generates quantitative metrics for analyzing three primary nonfunctional requirements (such as maintainability, performance, and reliability) while selecting five popular LLMs. For PELLI's applicability, we selected three application domains while following Python coding standards. Following this framework, practitioners can ensure harmonious integration between LLMs and human developers, ensuring that their potential is fully realized. PELLI can serve as a practical guide for developers aiming to leverage LLMs while adhering to recognized quality standards. This study's outcomes are crucial for advancing LLM technologies in real-world applications, providing stakeholders with a clear understanding of where these LLMs excel and where they require further refinement. Overall, based on three nonfunctional requirements, we have found that GPT-4T and Gemini performed slightly better. We also found that prompt design can influence the overall code quality. In addition, each application domain demonstrated high and low scores across various metrics, and even within the same metrics across different prompts.

</details>


### [14] [Deriving and Validating Requirements Engineering Principles for Large-Scale Agile Development: An Industrial Longitudinal Study](https://arxiv.org/abs/2602.10972)
*Hina Saeeda,Mijin Kim,Eric Knauss,Jesper Thyssen,Jesper Ørting,Jesper Lysemose Korsgaard,Niels Jørgen Strøm*

Main category: cs.SE

TL;DR: 本研究通过与Grundfos AB合作的五年纵向案例研究，探讨了大规模敏捷系统开发中的需求工程（RE）原则。研究通过25多个冲刺、约320次周同步会议和7个跨公司工作坊收集定性数据，并在2024年末通过五个深度焦点小组与高级领导进行回顾验证。研究旨在实证检验RE原则、探索其实践效益以及识别适用于大规模环境的一组可转移RE原则。最终，得出了六个关键RE原则，并通过与其他三家跨国公司的专家评估进一步验证了这些发现。


<details>
  <summary>Details</summary>
Motivation: 大型敏捷系统开发中缺乏统一的需求工程过程是一个主要挑战，尤其是在有效管理需求方面缺少高层次指导原则的情况下。为了解决这个问题，研究人员与瑞典软件中心合作，在Grundfos AB进行了为期五年的纵向案例研究。

Method: 研究方法包括定性数据收集，涵盖了超过25个冲刺、大约320次每周同步会议以及从2019年至2024年间举行的七次跨公司及公司内部研讨会。此外，在2024年末对Grundfos的高层领导进行了五个深入的焦点小组讨论，以回顾性地验证这些原则并评估它们的战略影响。使用主题分析法得出了六个关键的需求工程原则。

Result: 该研究得出了六个关键的需求工程原则：架构上下文、利益相关者驱动的验证与调整、随轻量文档演进的需求实践、委托需求管理、组织角色与职责以及对需求的共同理解。

Conclusion: 这项研究提供了一个可扩展且适应性强的基础，用于改善大规模敏捷组织中的需求实践。通过与Grundfos AB的合作以及Bosch、Ericsson和Volvo Cars等其他几家跨国公司的交叉验证，这些原则被证明是有效的。

Abstract: In large scale agile systems development, the lack of a unified requirements engineering (RE) process is a major challenge, exacerbated by the absence of high level guiding principles for effective requirements management. To address this challenge, we conducted a five year longitudinal case study with Grundfos AB, in collaboration with the Software Centre in Sweden. RE principles were first derived through qualitative data collection spanning more than 25 sprints, approximately 320 weekly synchronisation meetings, and seven cross-company, company-specific workshops between 2019 and 2024. These activities engaged practitioners from diverse roles, representing several hundred developers across domains. In late 2024, five in depth focus groups with senior leaders at Grundfos provided retrospective validation of the principles and assessed their strategic impact. We aim to (1) empirically examine RE principles in large scale agile system development, (2) explore their benefits in practice within the case company, and (3) identify a set of transferable RE principles for large scale contexts. Using thematic analysis, six key RE principles architectural context, stakeholder-driven validation and alignment, requirements practices in large-scale agile organisations. evolution with lightweight documentation, delegated requirements management, organisational roles and responsibilities, and a shared understanding of requirements are derived. The study was further validated through crosscompany expert evaluation with three additional multinational organisations (Bosch, Ericsson, and Volvo Cars), which are directly responsible for largescale requirements management. Together, these efforts provide a scalable and adaptable foundation for improving requirements practices in largescale agile organisations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [15] [JAG: Joint Attribute Graphs for Filtered Nearest Neighbor Search](https://arxiv.org/abs/2602.10258)
*Haike Xu,Guy Blelloch,Laxman Dhulipala,Lars Gottesbüren,Rajesh Jayaram,Jakub Łącki*

Main category: cs.IR

TL;DR: 提出了一种名为JAG（Joint Attribute Graphs）的图基算法，该算法通过引入属性和过滤距离，能够在整个选择性范围内提供稳健性能，并支持多种过滤类型。实验结果表明，JAG在吞吐量和召回率鲁棒性方面显著优于现有的最先进基准方法。


<details>
  <summary>Details</summary>
Motivation: 当前的过滤最近邻搜索算法对查询选择性和过滤器类型非常敏感，只能在特定情况下表现出色，难以适应新类型的过滤器或未知的选择性需求。

Method: 开发了JAG算法，它利用属性和过滤距离将二进制过滤约束转化为连续导航指导，并构建了一个同时优化向量相似度和属性接近度的邻近图，从而避免了导航死胡同问题。

Result: 实验跨越五个数据集和四种过滤类型（标签、范围、子集、布尔），证明了JAG在吞吐量和召回率稳定性方面明显优于现有基于图的过滤最近邻搜索方法。

Conclusion: JAG展现出了对于不同过滤类型及选择性条件下执行过滤最近邻搜索任务的强大能力与灵活性，为实际部署提供了更好的解决方案。

Abstract: Despite filtered nearest neighbor search being a fundamental task in modern vector search systems, the performance of existing algorithms is highly sensitive to query selectivity and filter type. In particular, existing solutions excel either at specific filter categories (e.g., label equality) or within narrow selectivity bands (e.g., pre-filtering for low selectivity) and are therefore a poor fit for practical deployments that demand generalization to new filter types and unknown query selectivities. In this paper, we propose JAG (Joint Attribute Graphs), a graph-based algorithm designed to deliver robust performance across the entire selectivity spectrum and support diverse filter types. Our key innovation is the introduction of attribute and filter distances, which transform binary filter constraints into continuous navigational guidance. By constructing a proximity graph that jointly optimizes for both vector similarity and attribute proximity, JAG prevents navigational dead-ends and allows JAG to consistently outperform prior graph-based filtered nearest neighbor search methods. Our experimental results across five datasets and four filter types (Label, Range, Subset, Boolean) demonstrate that JAG significantly outperforms existing state-of-the-art baselines in both throughput and recall robustness.

</details>


### [16] [Single-Turn LLM Reformulation Powered Multi-Stage Hybrid Re-Ranking for Tip-of-the-Tongue Known-Item Retrieval](https://arxiv.org/abs/2602.10321)
*Debayan Mukhopadhyay,Utshab Kumar Ghosh,Shubham Chatterjee*

Main category: cs.IR

TL;DR: 本研究提出了一种使用通用大语言模型（LLM）进行查询重构的方法，以解决Tip-of-the-Tongue (ToT)检索问题。通过将改写后的查询输入一个多阶段检索流程，包括稀疏检索、密集/后期交互重排序等步骤，显著提高了召回率和其他评估指标的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决从模糊描述中检索已知项目的问题，特别是当标准伪相关反馈方法因初始召回率低而失效时。

Method: 采用一个未针对特定领域或ToT任务进行微调的80亿参数LLM对查询进行重构，并通过多阶段检索流程（包括BM25稀疏检索、Contriever/E5-large-v2/ColBERTv2密集/后期交互重排序、monoT5交叉编码以及Qwen 2.5 72B列表级重排序）处理改写后的查询。

Result: 实验表明，原始查询的效果不佳，但通过轻量级预检索转换后，召回率提高了20.61%；后续重排序进一步提升了nDCG@10、MRR和MAP@10指标，分别增长了33.88%、29.92%和29.98%。

Conclusion: 该研究证明了即使不依赖于专门训练的语言模型，仅通过有效的提示策略也可以大幅提高ToT检索任务中的性能，从而有效激活下游排序器潜力。

Abstract: Retrieving known items from vague descriptions, Tip-of-the-Tongue (ToT) retrieval, remains a significant challenge. We propose using a single call to a generic 8B-parameter LLM for query reformulation, bridging the gap between ill-formed ToT queries and specific information needs. This method is particularly effective where standard Pseudo-Relevance Feedback fails due to poor initial recall. Crucially, our LLM is not fine-tuned for ToT or specific domains, demonstrating that gains stem from our prompting strategy rather than model specialization. Rewritten queries feed a multi-stage pipeline: sparse retrieval (BM25), dense/late-interaction reranking (Contriever, E5-large-v2, ColBERTv2), monoT5 cross-encoding, and list-wise reranking (Qwen 2.5 72B). Experiments on 2025 TREC-ToT datasets show that while raw queries yield poor performance, our lightweight pre-retrieval transformation improves Recall by 20.61%. Subsequent reranking improves nDCG@10 by 33.88%, MRR by 29.92%, and MAP@10 by 29.98%, offering a cost-effective intervention that unlocks the potential of downstream rankers. Code and data: https://github.com/debayan1405/TREC-TOT-2025

</details>


### [17] [GeoGR: A Generative Retrieval Framework for Spatio-Temporal Aware POI Recommendation](https://arxiv.org/abs/2602.10411)
*Fangye Wang,Haowen Lin,Yifang Yuan,Siyuan Wang,Xiaojiang Zhou,Song Yang,Pengjie Wang*

Main category: cs.IR

TL;DR: 本文提出了GeoGR，一种地理生成推荐框架，通过地理感知SID标记化管道和多阶段大语言模型训练策略来改进POI推荐。实验表明GeoGR在多个现实世界数据集上优于现有技术，并在AMAP平台上得到实际应用验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前基于SID的POI推荐方法在复杂、稀疏的真实环境中表现不佳，主要由于高质量SID建模不足及大语言模型与POI推荐任务之间对齐不良。为解决这些问题，提出了一种新的地理位置生成推荐框架GeoGR。

Method: GeoGR采用两阶段设计：(i)地理感知SID令牌化管道，通过地理约束共同访问的POI对、对比学习和迭代优化明确学习时空协作语义表示；(ii)多阶段LLM训练策略，通过基于模板的持续预训练(CPT)对非原生SID令牌进行对齐，并通过监督微调(SFT)实现自动回归POI生成。

Result: 广泛的实验证明了GeoGR在多种真实数据集上的优越性能。此外，在AMAP平台上的部署服务数百万用户并提升了多项在线指标，进一步确认了其在生产环境中的实用效果和可扩展性。

Conclusion: GeoGR作为一种针对导航类LBS设计的地理生成推荐框架，能够有效捕捉用户的上下文状态变化，实现了意图感知的POI推荐。它不仅在理论上优于现有基准方法，而且在实际应用中也表现出色。

Abstract: Next Point-of-Interest (POI) prediction is a fundamental task in location-based services, especially critical for large-scale navigation platforms like AMAP that serve billions of users across diverse lifestyle scenarios. While recent POI recommendation approaches based on SIDs have achieved promising, they struggle in complex, sparse real-world environments due to two key limitations: (1) inadequate modeling of high-quality SIDs that capture cross-category spatio-temporal collaborative relationships, and (2) poor alignment between large language models (LLMs) and the POI recommendation task. To this end, we propose GeoGR, a geographic generative recommendation framework tailored for navigation-based LBS like AMAP, which perceives users' contextual state changes and enables intent-aware POI recommendation. GeoGR features a two-stage design: (i) a geo-aware SID tokenization pipeline that explicitly learns spatio-temporal collaborative semantic representations via geographically constrained co-visited POI pairs, contrastive learning, and iterative refinement; and (ii) a multi-stage LLM training strategy that aligns non-native SID tokens through multiple template-based continued pre-training(CPT) and enables autoregressive POI generation via supervised fine-tuning(SFT). Extensive experiments on multiple real-world datasets demonstrate GeoGR's superiority over state-of-the-art baselines. Moreover, deployment on the AMAP platform, serving millions of users with multiple online metrics boosting, confirms its practical effectiveness and scalability in production.

</details>


### [18] [End-to-End Semantic ID Generation for Generative Advertisement Recommendation](https://arxiv.org/abs/2602.10445)
*Jie Jiang,Xinxun Zhang,Enming Zhang,Yuling Xiong,Jun Zhang,Jingwen Wang,Huan Yu,Yuxiang Wang,Hao Wang,Xiao Yan,Jiawei Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种名为UniSID的统一SID生成框架，用于生成式广告推荐。该方法通过端到端的方式从原始广告数据中联合优化嵌入和SID，引入多粒度对比学习策略来对齐不同SID级别的不同项目，并提出了基于摘要的广告重建机制以鼓励SID捕捉高级语义信息。实验表明，与最强基线相比，UniSID在下游广告场景中的命中率指标上最多提高了4.62%。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐（GR）方法主要通过残差量化（RQ）生成SID，但这种方法存在目标不一致性和由于两阶段压缩导致的语义退化问题，以及RQ结构内在的误差累积问题。为了解决这些问题，作者提出了一个新颖的SID生成框架。

Method: 提出了UniSID框架，它直接从原始广告数据中以端到端方式联合优化嵌入和SID；采用了多粒度对比学习策略来跨SID级别对齐不同的项目；设计了一个基于摘要的广告重建机制，促使SID能够捕捉到不在广告上下文中明确出现的高层次语义信息。

Result: 实验结果表明，在各种下游广告场景下，UniSID相比于当前最先进的SID生成方法，在命中率等关键指标上有显著提升，最高达到了4.62%的改进。

Conclusion: UniSID通过解决现有SID生成技术中存在的问题，提供了一种更有效的SID生成解决方案，对于提高生成式广告推荐系统的性能具有重要意义。

Abstract: Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline.

</details>


### [19] [Compute Only Once: UG-Separation for Efficient Large Recommendation Models](https://arxiv.org/abs/2602.10455)
*Hui Lu,Zheng Chai,Shipeng Bai,Hao Zhang,Zhifang Fan,Kunmin Bai,Yingwen Wu,Bingzheng Wei,Xiang Sun,Ziyan Gong,Tianyi Liu,Hua Chen,Deping Xie,Zhongkai Chen,Zhiliang Guo,Qiwei Chen,Yuchao Zheng*

Main category: cs.IR

TL;DR: 本文提出了一种名为User-Group Separation (UG-Sep)的新框架，该框架首次在密集交互模型中实现了用户端计算的复用。通过引入一种屏蔽机制，明确地将用户端和项目端的信息流在token混合层内解耦，使得一部分token能够在各层之间保持纯用户端表示。此外，还提出了信息补偿策略以自适应地重建被抑制的用户-项目互动，并结合了W8A16权重量化技术来减轻内存带宽瓶颈。在线上线下实验中，UG-Sep展示了最高可达20%的推理延迟减少，同时不损害用户体验或商业指标。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统规模的增长，越来越依赖大规模模型来捕捉复杂的特征交互和用户行为，但这导致了训练和推理成本过高。虽然长序列模型可以通过KV缓存重用用户端计算，但在密集特征交互架构中难以实现这样的重用，因为用户和组（候选项目）特征在多层间紧密交织。

Method: 提出了User-Group Separation (UG-Sep)框架，通过在token混合层引入一个屏蔽机制来明确分离用户端与项目端信息流，允许特定token跨层保留纯粹的用户端表示。为弥补因屏蔽可能引起的表达力损失，进一步提出了一种信息补偿策略来自适应地重构被压制的用户-项目互动。另外，针对UG-Sep大幅减少了用户端FLOPs并暴露了内存限制组件的情况，采用W8A16（8位权重，16位激活）仅权重量化方法来缓解内存带宽瓶颈。

Result: 通过字节跳动进行的大规模线上线下评估表明，UG-Sep能够降低高达20%的推理延迟，同时不对多种业务场景下的在线用户体验或商业指标造成负面影响，包括内容推荐和广告系统。

Conclusion: UG-Sep提供了一种有效的方法来解决大规模推荐系统中高推理成本的问题，通过启用用户端计算的重复利用以及优化模型效率，显著降低了推理延迟，同时保持了良好的性能表现。

Abstract: Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems.

</details>


### [20] [Boundary-Aware Multi-Behavior Dynamic Graph Transformer for Sequential Recommendation](https://arxiv.org/abs/2602.10493)
*Jingsong Su,Xuetao Ma,Mingming Li,Qiannan Zhu,Yu Guo*

Main category: cs.IR

TL;DR: 提出了一种边界感知的多行为动态图Transformer (MB-DGT) 模型，用于更全面和动态地表示用户偏好，并通过特定于用户的多行为损失函数优化模型，以提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在同时考虑图拓扑的动态性和用户偏好的序列模式方面存在不足，且未能充分捕捉多种用户行为界限。为了解决这些问题，提出了MB-DGT模型。

Method: 开发了一个基于transformer的动态图聚合器来建模用户偏好，该聚合器整合了变化中的图结构与用户行为序列；并通过一种特定于用户的多行为损失函数来优化模型，明确不同行为之间的兴趣边界。

Result: 跨三个数据集的综合实验表明，所提出的模型能够持续提供出色的推荐性能。

Conclusion: MB-DGT模型通过有效结合动态图结构与用户行为序列，在处理多行为界限问题上取得了显著进展，从而提高了个性化推荐系统的性能。

Abstract: In the landscape of contemporary recommender systems, user-item interactions are inherently dynamic and sequential, often characterized by various behaviors. Prior research has explored the modeling of user preferences through sequential interactions and the user-item interaction graph, utilizing advanced techniques such as graph neural networks and transformer-based architectures. However, these methods typically fall short in simultaneously accounting for the dynamic nature of graph topologies and the sequential pattern of interactions in user preference models. Moreover, they often fail to adequately capture the multiple user behavior boundaries during model optimization. To tackle these challenges, we introduce a boundary-aware Multi-Behavioral Dynamic Graph Transformer (MB-DGT) model that dynamically refines the graph structure to reflect the evolving patterns of user behaviors and interactions. Our model involves a transformer-based dynamic graph aggregator for user preference modeling, which assimilates the changing graph structure and the sequence of user behaviors. This integration yields a more comprehensive and dynamic representation of user preferences. For model optimization, we implement a user-specific multi-behavior loss function that delineates the interest boundaries among different behaviors, thereby enriching the personalized learning of user preferences. Comprehensive experiments across three datasets indicate that our model consistently delivers remarkable recommendation performance.

</details>


### [21] [Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking](https://arxiv.org/abs/2602.10577)
*Yiming Che,Mansi Mane,Keerthi Gopalakrishnan,Parisa Kaghazgaran,Murali Mohana Krishna Dandu,Archana Venkatachalapathy,Sinduja Subramaniam,Yokila Arora,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.IR

TL;DR: 提出了一种名为Campaign-2-PT-RAG的框架，该框架通过大型语言模型(LLM)解析广告活动内容并检索相关产品类型(PT)，生成用户—广告活动购买标签。此方法将模糊的归因问题转变为可处理的语义对齐任务，支持电商环境中广告活动排名优化等下游任务。实验表明，该方法在保持超过99%召回率的同时，精确度达到78-90%。


<details>
  <summary>Details</summary>
Motivation: 电子商务活动排名模型需要大规模的训练标签来指示哪些用户因为活动影响而购买。然而，由于活动使用的创意、主题语言并不能直接映射到产品购买上，在没有明确的产品层面归属的情况下，用于活动优化的监督学习受到了限制。

Method: 本文提出了一个名为Campaign-2-PT-RAG的标签生成框架，首先利用大型语言模型（LLMs）理解活动内容以捕捉隐含意图，然后通过对平台分类法进行语义搜索来检索候选产品类型(PTs)。接着，使用基于结构化LLM的分类器评估每个PT的相关性，从而产生特定于活动的产品覆盖集。用户购买符合这些PT的行为会为下游排名模型生成正面训练标签。

Result: 内部和合成数据集上的实验结果表明，与专家标注的活动-PT映射相比，所提出的LLM辅助方法能够以78-90%的精度生成高质量标签，同时保持超过99%的召回率。

Conclusion: 本研究介绍的方法通过将复杂的归属问题重新定义为一个更加易于管理的语义对齐任务，为生产环境中的电子商务活动排名优化等下游任务提供了可扩展且一致性的监督手段。

Abstract: E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present \textbf{Campaign-2-PT-RAG}, a scalable label generation framework that constructs user--campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign--PT mappings, show that our LLM-assisted approach generates high-quality labels with 78--90% precision while maintaining over 99% recall.

</details>


### [22] [S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage](https://arxiv.org/abs/2602.10606)
*Jie Jiang,Hongbo Tang,Wenjie Wu,Yangru Huang,Zhenmao Li,Qian Li,Changping Wang,Jun Zhang,Huan Yu*

Main category: cs.IR

TL;DR: 本论文提出了S-GRec，一种语义感知框架，通过分离在线轻量级生成器与离线基于大语言模型（LLM）的语义评判机制来解决在工业推荐系统中直接采用LLM所面临的成本和业务目标冲突问题。该框架引入了个性化语义评判(PSJ)两阶段过程以及非对称优势策略优化(A2PO)，以确保既符合商业回报又能从语义角度提供稳定奖励。实验结果表明，此方法不仅有效提升了点击率(CTR)及总商品价值(GMV)，还无需实时LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐模型虽然能够端到端地生成推荐项，但基于行为日志训练时往往只能提供关于用户意图的弱监督。尽管大型语言模型（LLMs）提供了丰富的语义先验知识可以补充这种监督，但在工业推荐场景下的直接应用受到了两个主要障碍：一是语义信号可能与平台业务目标相冲突；二是大规模使用LLMs进行推理的成本过高。

Method: 本文提出了一种名为S-GRec的语义感知框架，该框架将一个在线的轻量级生成器与一个离线的基于LLM的语义评判机制解耦开来，用于训练时的监督。S-GRec包含了一个两阶段的个性化语义评判过程（PSJ），该过程可以从成对反馈中学习用户条件聚合，并产生可解释的方面证据，从而生成稳定的语义奖励。为了防止语义监督偏离业务目标，还引入了非对称优势策略优化（A2PO），该方法基于业务回报（如eCPM）进行优化，并仅在语义优势与之相符时注入这些优势。

Result: 广泛的实验包括在公开基准测试和大规模生产系统上的验证显示，S-GRec不仅提高了点击率(CTR)并使GMV增加了1.19%，而且这些改进是在不需要实时LLM推理的情况下实现的，证明了其有效性和可扩展性。

Conclusion: S-GRec通过创新性地结合在线轻量级生成与离线深度语义评判，在不牺牲性能的前提下解决了将LLM应用于推荐系统中的成本与一致性挑战。这种方法为如何在保持业务目标的同时利用复杂的语义理解开辟了新的道路。

Abstract: Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\% lift in GMV in online A/B tests, without requiring real-time LLM inference.

</details>


### [23] [A Cognitive Distribution and Behavior-Consistent Framework for Black-Box Attacks on Recommender Systems](https://arxiv.org/abs/2602.10633)
*Hongyue Zhan,Mingming Li,Dongqin Liu,Hui Wang,Yaning Zhang,Xi Zhou,Honglei Lv,Jiao Dai,Jizhong Han*

Main category: cs.IR

TL;DR: 本文提出了一种双增强攻击框架，旨在解决序列推荐系统中的黑盒模型提取和对抗性操纵问题。通过引入认知分布驱动的提取机制以及行为感知的噪声项生成策略，该方法在攻击成功率和逃避检测率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着序列推荐系统在电子商务等领域的广泛应用，其黑盒接口引发了安全担忧：模型容易被提取并受到后续的对抗性操作。现有的基于硬标签或成对学习的黑盒提取攻击往往忽略了排名位置的重要性，并且纯粹基于梯度方法生成的对抗序列缺乏与真实用户行为的语义一致性，容易被发现。

Method: 提出了一个双增强攻击框架。首先，利用首因效应和位置偏差引入了一种认知分布驱动的提取机制，将离散排名映射为具有位置感知衰减的连续值分布；其次，设计了一种行为感知的噪声项生成策略，同时优化协作信号和梯度信号，确保了语义连贯性和统计隐蔽性。

Result: 在多个数据集上的广泛实验表明，所提方法在攻击成功率和逃避检测率方面显著优于现有方法。

Conclusion: 整合认知建模与行为一致性的方法对于构建安全推荐系统具有重要价值。

Abstract: With the growing deployment of sequential recommender systems in e-commerce and other fields, their black-box interfaces raise security concerns: models are vulnerable to extraction and subsequent adversarial manipulation. Existing black-box extraction attacks primarily rely on hard labels or pairwise learning, often ignoring the importance of ranking positions, which results in incomplete knowledge transfer. Moreover, adversarial sequences generated via pure gradient methods lack semantic consistency with real user behavior, making them easily detectable. To overcome these limitations, this paper proposes a dual-enhanced attack framework. First, drawing on primacy effects and position bias, we introduce a cognitive distribution-driven extraction mechanism that maps discrete rankings into continuous value distributions with position-aware decay, thereby advancing from order alignment to cognitive distribution alignment. Second, we design a behavior-aware noisy item generation strategy that jointly optimizes collaborative signals and gradient signals. This ensures both semantic coherence and statistical stealth while effectively promoting target item rankings. Extensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods in both attack success rate and evasion rate, validating the value of integrating cognitive modeling and behavioral consistency for secure recommender systems.

</details>


### [24] [Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval](https://arxiv.org/abs/2602.10833)
*William Xion,Wolfgang Nejdl*

Main category: cs.IR

TL;DR: 本研究通过控制实验评估了密集检索模型在不同训练阶段和数据源下对大型语言模型生成文本的偏好，发现这种偏好是训练引起的而非密集检索器的固有属性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在重新检验关于密集检索模型对大型语言模型（LLMs）生成文本存在广泛偏好的说法，并探索这一现象背后的原因是否与困惑度有关。

Method: 使用SciFact和自然问题（NQ320K）数据集的人工及LLM生成版本进行对比实验；比较了无监督检查点与基于领域内人工文本、领域内LLM生成文本以及MS MARCO微调后的模型表现。

Result: 1) 无监督检索者对于LLM生成文本没有统一的偏好，方向和程度取决于数据集。
2) 在所有测试设置中，使用MS MARCO进行监督微调始终会将排名偏向于LLM生成的文本。
3) 领域内微调导致特定于数据集且不一致的偏好变化。
4) 对LLM生成语料库进行微调会产生显著的亲LLM偏见。
此外，一个涉及将语言建模头重新附加到微调后密集检索编码器上的探针显示，与相关性接近偶然性的结果相一致，从而削弱了困惑度作为解释因素的力量。

Conclusion: 来源偏见是一个由训练引起的现象，而不是密集检索器的固有特性。

Abstract: Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference for text generated by large language models (LLMs). This bias is called "source bias", and it has been hypothesized that lower perplexity contributes to this effect. In this study, we revisit this claim by conducting a controlled evaluation to trace the emergence of such preferences across training stages and data sources. Using parallel human- and LLM-generated counterparts of the SciFact and Natural Questions (NQ320K) datasets, we compare unsupervised checkpoints with models fine-tuned using in-domain human text, in-domain LLM-generated text, and MS MARCO. Our results show the following: 1) Unsupervised retrievers do not exhibit a uniform pro-LLM preference. The direction and magnitude depend on the dataset. 2) Across the settings tested, supervised fine-tuning on MS MARCO consistently shifts the rankings toward LLM-generated text. 3) In-domain fine-tuning produces dataset-specific and inconsistent shifts in preference. 4) Fine-tuning on LLM-generated corpora induces a pronounced pro-LLM bias. Finally, a retriever-centric perplexity probe involving the reattachment of a language modeling head to the fine-tuned dense retriever encoder indicates agreement with relevance near chance, thereby weakening the explanatory power of perplexity. Our study demonstrates that source bias is a training-induced phenomenon rather than an inherent property of dense retrievers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke](https://arxiv.org/abs/2602.10119)
*Anjali K. Kapoor,Anton Alyakin,Jin Vivian Lee,Eunice Yang,Annelene M. Schulze,Krithik Vishwanath,Jinseok Lee,Yindalon Aphinyanaphongs,Howard Riina,Jennifer A. Frontera,Eric Karl Oermann*

Main category: cs.LG

TL;DR: 本研究探索了大型语言模型（LLMs）直接从常规入院记录中推断急性缺血性卒中后功能结果的能力。评估了几种编码器和生成型LLM在冻结和微调设置下对出院和90天mRS评分预测的表现。结果显示，微调后的Llama模型在90天mRS准确率上达到了33.9%，二分类准确率为76.3%；出院时的准确率分别为42.0%和75.0%。对于90天预测，Llama的表现与基于结构化数据的基线相当。这些发现支持开发无需手动数据提取即可融入临床工作流程的文本基础预后工具的发展。


<details>
  <summary>Details</summary>
Motivation: 准确预测急性缺血性卒中后的功能结果可以为临床决策和资源配置提供信息。以往关于改良Rankin量表(mRS)预测的工作主要依赖于结构化变量（如年龄、NIHSS评分）和传统机器学习方法。然而，大型语言模型(LLMs)能否直接从常规入院笔记中推断未来的mRS评分仍有待深入探索。

Method: 研究使用了一种大规模的真实世界卒中登记数据集来评估编码器（BERT, NYUTron）和生成式（Llama-3.1-8B, MedGemma-4B）LLMs，在冻结状态和微调状态下对出院及90天mRS评分进行预测的能力。采用时间分割法保留最近12个月的数据用于测试，并通过精确（7类）mRS准确性和二元功能性结果准确性（mRS 0-2 vs. 3-6）来评估性能。

Result: 微调后的Llama模型表现最佳，其90天mRS准确率达到33.9% [95% CI, 27.9-39.9%]，而二元准确性则为76.3% [95% CI, 70.7-81.9%]。对于出院情况，该模型实现了42.0% [95% CI, 39.0-45.0%] 的准确率以及75.0% [95% CI, 72.4-77.6%] 的二元准确性。就90天预测而言，Llama模型的表现与需要结构化变量抽象的模型相当。

Conclusion: 经过微调的大规模语言模型能够仅依靠入院记录预测卒中后的功能结果，并且其表现可与需要结构化变量抽取的模型相媲美。这表明发展无需人工数据提取就能无缝整合进临床工作流的文本基础预后工具有着巨大潜力。

Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.

</details>


### [26] [Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting](https://arxiv.org/abs/2602.10182)
*Benjamin R. Redhead,Thomas L. Lee,Peng Gu,Víctor Elvira,Amos Storkey*

Main category: cs.LG

TL;DR: 本文提出了一种新的概率预测评估框架，通过引入基于核的方法——签名最大均值差异(Sig-MMD)及一种新颖的删失Sig-MMD(CSig-MMD)，来解决现有方法在处理时间步或变量间依赖性以及尾部事件敏感度上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的概率预测评价体系缺乏共识度量标准，并且存在两个关键缺陷：经常假设时间步骤或变量之间的独立性，以及明显对尾部事件（即实际决策中最关键的发生）缺乏敏感性。为了解决这些问题并提高预测算法的鲁棒性，提出了新的评估指标。

Method: 提出了两种基于核的方法：签名最大均值差异（Sig-MMD）和删失签名最大均值差异（CSig-MMD）。这些方法利用了签名核的优势，能够捕捉复杂的跨变量和跨时间依赖关系，并且对于缺失数据保持稳健。特别是CSig-MMD通过引入审查方案，在确保适当性的同时增强了预测极端事件的能力。

Result: 新提出的度量标准能够更可靠地评估直接多步预测，有助于开发出更加鲁棒的概率算法。

Conclusion: 通过引入Sig-MMD和CSig-MMD这两种新的评价指标，可以更好地捕捉复杂的时间与变量间的依赖关系，尤其是提高了对尾部事件预测能力的关注，从而促进了更稳健的概率预测算法的发展。

Abstract: Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.

</details>


### [27] [Adaptive Optimization via Momentum on Variance-Normalized Gradients](https://arxiv.org/abs/2602.10204)
*Francisco Patitucci,Aryan Mokhtari*

Main category: cs.LG

TL;DR: 本文提出了一种名为MVN-Grad的优化器，它通过结合基于方差的归一化和在归一化后应用动量来提高稳定性和性能。实验表明，MVN-Grad在CIFAR-100图像分类和GPT风格的语言模型基准测试中与Adam、AdaBelief和LaProp相比具有更好的表现或至少持平，同时提供更平滑的训练过程和改进的一般性，且没有额外开销。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改善现有如Adam等优化算法中存在的问题，特别是解决过时动量与随机规范化因子之间的跨时间耦合问题，并减少单步条件更新方差，从而提高优化器的鲁棒性和收敛速度。

Method: 提出了MVN-Grad优化器，该方法首先根据梯度不确定性对每个坐标进行缩放，然后对得到的标准化梯度施加动量，以此解除了标准Adam类型更新中存在的旧动量与随机正规化项间的关联。此外，在低方差情况下，方差归一化还能避免与二阶矩缩放相关的符号型崩溃现象并可能加速收敛。

Result: 实验证明了MVN-Grad在面对异常值时具有均匀有界响应能力，能够处理单个梯度峰值；并且在CIFAR-100图像分类任务及GPT式语言建模任务上，MVN-Grad表现出色，不仅训练过程更加平稳，还提高了泛化能力。

Conclusion: 结论是MVN-Grad作为一种新颖的优化器设计，成功地将方差归一化与延迟动量相结合，从而在不同应用场景下展现出优于或至少可比于现有流行优化方法（如Adam）的表现。

Abstract: We introduce MVN-Grad (Momentum on Variance-Normalized Gradients), an Adam-style optimizer that improves stability and performance by combining two complementary ideas: variance-based normalization and momentum applied after normalization. MVN-Grad scales each coordinate by an exponential moving average of gradient uncertainty and applies momentum to the resulting normalized gradients, eliminating the cross-time coupling between stale momentum and a stochastic normalizer present in standard Adam-type updates. We prove that this decoupling yields strictly smaller one-step conditional update variance than momentum-then-normalize variance methods under standard noise assumptions, and that MVN-Grad is robust to outliers: it has a uniformly bounded response to single gradient spikes.
  In low-variance regimes, we further show variance normalization avoids sign-type collapse associated with second-moment scaling and can yield accelerated convergence. Across CIFAR-100 image classification and GPT-style language modeling benchmarks, MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp, delivering smoother training and improved generalization with no added overhead.

</details>


### [28] [Chamfer-Linkage for Hierarchical Agglomerative Clustering](https://arxiv.org/abs/2602.10444)
*Kishen N Gowda,Willem Fletcher,MohammadHossein Bateni,Laxman Dhulipala,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.LG

TL;DR: 本文提出了一种新的链接函数——Chamfer-linkage，用于衡量聚类之间的距离。该方法在理论和实验上都表现出比传统链接函数更好的性能，并且可以作为传统链接函数的实用替代品，以提高层次聚类的质量。


<details>
  <summary>Details</summary>
Motivation: 传统的层次聚集聚类方法（如单链接、平均链接和Ward方法）在实际应用中显示出对不同数据集的高度变异性，不能始终产生高质量的聚类结果。因此，需要一种新的链接函数来改善这一状况。

Method: 提出了Chamfer-linkage，这是一种基于机器学习和计算机视觉领域常用的Chamfer距离的新链接函数。通过使用Chamler距离度量点云间的距离，作者认为Chamfer-linkage满足了其他流行度量难以达到的概念表示属性。

Result: 理论上证明了采用Chamfer-linkage的HAC可以在O(n^2)时间内实现，与经典链接函数的效率相匹配。实验结果显示，在各种不同的数据集上，Chamfer-linkage持续地产生了比平均链接和Ward方法等经典链接更高品质的聚类结果。

Conclusion: Chamfer-linkage作为一种新型的链接函数，不仅在计算效率上达到了与传统链接函数相当的水平，而且在实践中也能够提供更加一致且高质量的聚类结果。这表明Chamfer-linkage可以作为一个实用的直接替换选项，为层次聚类提供了新的工具。

Abstract: Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.
  In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.

</details>


### [29] [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)
*Junhong Lin,Bing Zhang,Song Wang,Ziyan Liu,Dan Gutfreund,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种新的基准框架HybridRAG-Bench，用于评估基于混合知识（非结构化文本和结构化知识图谱）的检索密集型、多跳推理能力。该框架能够生成基于最新科学文献的知识密集型问答对，并支持灵活选择领域和时间范围，以适应模型与知识的发展。实验表明HybridRAG-Bench能有效区分真实检索与推理能力和参数记忆能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理需要最新信息和多跳推理的知识密集型问题上仍然存在困难。虽然通过结合非结构化文本和结构化知识图谱等外部知识来增强这些模型提供了一个有前景的替代方案，但可靠地评估它们的检索和推理能力变得至关重要。现有的许多基准测试数据集与语言模型预训练数据重叠较多，这使得很难区分真正的检索与推理能力还是仅仅是模型参数中的回忆。

Method: 引入了HybridRAG-Bench框架，它自动将来自arXiv最近科学文献的非结构化文本与结构化知识图谱表示相结合，并生成基于明确推理路径的知识密集型问答对。该框架允许根据领域和时间范围进行灵活的选择，从而支持考虑到污染意识和可定制化的评估方法。

Result: 跨三个领域（人工智能、治理与政策以及生物信息学）的实验表明，HybridRAG-Bench更倾向于奖励真实的检索与推理能力而非简单的参数召回，为评估混合知识增强型推理系统提供了原则性的测试平台。

Conclusion: HybridRAG-Bench作为一个新颖的评估框架，能够有效地评估利用混合形式外部知识增强的语言模型的真实检索与多步推理性能，而不是依赖于预训练期间学到的信息。

Abstract: Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.

</details>


### [30] [Rank-Accuracy Trade-off for LoRA: A Gradient-Flow Analysis](https://arxiv.org/abs/2602.10212)
*Michael Rushka,Diego Klabjan*

Main category: cs.LG

TL;DR: 本研究从动力系统角度出发，分析了LoRA在下游微调任务中不同秩更新的准确性，并推导出了LoRA参数更新的梯度流方程。通过这些方程，得出了LoRA秩与使用迹平方和Frobenius范数低秩近似损失函数时准确性的闭式关系。


<details>
  <summary>Details</summary>
Motivation: 尽管已有实证研究表明，即使是对秩-1更新的情况，LoRA也能在下游微调任务上达到与全参数方法相当的精度，但关于LoRA精度如何依赖于更新秩的理论基础仍相对未被充分探索。因此，这项工作的动机是从理论上填补这一空白。

Method: 采用动力系统视角对比分析了秩-r LoRA更新与全参数更新对于微调任务的准确性；对两种不同的损失函数（迹平方和Frobenius范数低秩近似）进行了全秩和低秩机制下的梯度流分析；严格推导了LoRA参数更新的梯度流方程形式，并证明了同时和顺序更新下该形式的一致性；利用所得的动力系统方程来获得LoRA秩与准确性之间的闭式关系。

Result: 成功地为两种损失函数（迹平方和Frobenius范数低秩近似）建立了LoRA秩与准确性之间的明确数学关系；证明了无论是在同时还是顺序更新场景下，LoRA参数更新的梯度流方程形式保持一致。

Conclusion: 本研究提供了一种新的理解LoRA性能的方式，特别是其准确性如何随着更新秩的变化而变化。通过建立LoRA秩与准确性之间具体的数学联系，为选择合适的LoRA配置提供了理论依据。

Abstract: Previous empirical studies have shown that LoRA achieves accuracy comparable to full-parameter methods on downstream fine-tuning tasks, even for rank-1 updates. By contrast, the theoretical underpinnings of the dependence of LoRA's accuracy on update rank remain relatively unexplored. In this work, we compare the accuracy of rank-r LoRA updates against full-parameter updates for fine-tuning tasks from a dynamical systems perspective. We perform gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy for two loss functions under LoRA. While gradient flow equations for LoRA are presented in prior work, we rigorously derive their form and show that they are identical for simultaneous and sequential LoRA parameter updates. We then use the resulting dynamical system equations to obtain closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions.

</details>


### [31] [ELROND: Exploring and decomposing intrinsic capabilities of diffusion models](https://arxiv.org/abs/2602.10216)
*Paweł Skierś,Tomasz Trzciński,Kamil Deja*

Main category: cs.LG

TL;DR: 本文提出了一种框架，直接在输入嵌入空间中解耦语义方向，通过收集固定提示词随机实现之间的差异反向传播得到的一组梯度，然后使用主成分分析或稀疏自动编码器分解为有意义的方向。该方法能够提供对单一概念的精确细粒度控制、有效缓解蒸馏模型中的模式崩溃问题，并基于发现的子空间维度建立了一个新的概念复杂性估计器。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督方法试图通过输出特征来分析由扩散模型生成的视觉输出变化，但忽略了底层的生成过程，用户无法直接控制图像中出现的具体语义变体。

Method: 本研究提出一种框架，在输入嵌入空间中直接解耦这些语义方向。通过反向传播一个固定提示词不同随机实现之间的差异来收集一组梯度，然后利用主成分分析（PCA）或稀疏自动编码器将这些梯度分解成有意义的方向。

Result: 该方法成功地分离出可解释且可控的方向，实现了对单一概念的精准细粒度控制；有效地缓解了蒸馏模型中因多样性丢失而造成的模式崩溃问题；并根据所发现子空间的维度提出了一个新的概念复杂性估计器。

Conclusion: 这项工作介绍了一个新框架，允许用户通过控制输入嵌入空间中的特定语义方向来影响扩散模型生成图像的结果，从而提高对生成内容的控制力和多样性。此外，还引入了一种评估概念复杂性的新方法。

Abstract: A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.

</details>


### [32] [Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance](https://arxiv.org/abs/2602.10217)
*Jacob L. Block,Mehryar Mohri,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 本文提出了一种名为T3-Unlearning的新方法，用于解决大型生成模型中的机器遗忘问题。通过先对基础分布进行温度调整以平滑高置信度峰值，再使用轻量级分类器倾斜调整后的分布来区分保留和遗忘样本，从而有效提高遗忘质量和生成实用性，并且仅需训练少量参数。


<details>
  <summary>Details</summary>
Motivation: 研究者们注意到，在处理集中分布的数据集时，现有的基于分类器指导的方法可能无法准确地完成机器遗忘任务。因此，他们旨在开发一种新的方法来改进这一过程，特别是针对那些具有尖锐、集中数据分布的遗忘集。

Method: 提出了Temper-Then-Tilt Unlearning (T3-Unlearning) 方法，该方法首先冻结基础模型，然后执行两步推理程序：一是通过对基础分布进行温度调整来减少高置信度峰值；二是利用一个轻量级分类器（该分类器被训练用来区分保留样本与遗忘样本）去倾斜调整过的分布。

Result: 实验结果表明，T3-Unlearning方法不仅在TOFU基准测试上优于现有基线方案，提高了遗忘质量及生成实用性，同时还能以极小的运行时间和较低的参数训练需求实现上述目标。

Conclusion: 通过引入T3-Unlearning方法，研究人员成功解决了当待遗忘数据呈现集中分布特征时，传统方法难以有效执行机器遗忘的问题。理论分析进一步证明了温度调整对于成功执行此类遗忘任务的重要性。

Abstract: We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.

</details>


### [33] [Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2602.10224)
*Shiting Huang,Zecheng Li,Yu Zeng,Qingnan Ren,Zhen Fang,Qisheng Su,Kou Shi,Lin Chen,Zehui Chen,Feng Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架——元经验学习（MEL），旨在通过将从过去错误中提取的可重用知识（即元经验）纳入大型语言模型（LLMs）的参数记忆中，来解决强化学习与可验证奖励（RLVR）在提升LLMs推理能力时遇到的元学习瓶颈问题。实验结果表明，MEL方法在不同模型规模上均实现了显著改进。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）虽然有效提升了大型语言模型（LLMs）的推理能力，但存在一个关键限制：缺乏类似于人类学习过程中的错误归因和经验内化机制，这阻碍了精细的信用分配以及可复用知识的形成。

Method: 提出了元经验学习（MEL）框架，该框架利用自我蒸馏的方法将从以往错误中获得的元经验整合进模型的参数记忆里。在此基础上，设计了一个额外组件，利用LLM自身的验证能力对正确与错误轨迹进行对比分析，识别出推理错误的具体分叉点，并将其总结为一般化的元经验。随后，通过最小化负对数似然的方式将这些元经验内部化到LLM的记忆中，从而生成一种连接正误推理路径的语言建模范式奖励信号，促进知识的有效再利用。

Result: 实验结果显示，在不同大小的模型上，MEL方法能够实现一致性的性能提升，具体表现为Pass@1指标提高了3.92%至4.73%。

Conclusion: 元经验学习（MEL）提供了一种有效增强LLMs推理能力的新途径，通过引入元经验的概念解决了现有RLVR方法中存在的局限性，促进了更高效的错误修正与知识重用。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.

</details>


### [34] [Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs](https://arxiv.org/abs/2602.10230)
*Joesph An,Phillip Keung,Jiaqi Wang,Orevaoghene Ahia,Noah A. Smith*

Main category: cs.LG

TL;DR: 提出了一种基于帧级内部工具使用的方法，通过训练音频语言模型利用自身内部的音频表示来直接执行时间定位任务。该方法引入了一个轻量级预测机制，并通过二元帧分类器和新颖的非均匀泊松过程损失函数进行训练，以解决音频理解中的时间定位问题。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在处理需要精确时间定位的任务时存在困难，如词对齐和说话人日记化。标准方法是将时间戳作为文本标记序列生成，这种方法计算成本高且容易产生幻觉，特别是在处理超出模型训练分布范围的音频长度时。

Method: 提出了一种称为帧级内部工具使用的新方法，该方法训练音频语言模型利用它们自己的内部音频表示来进行直接的时间定位。为此，研究者引入了一种轻量级预测机制，通过两个目标进行训练：一个是二元帧分类器，另一个是建模时间事件强度的新颖非均匀泊松过程（IHP）损失。

Result: 所提出的方法在词定位、说话人日记化以及事件定位任务上优于基于标记的基础方法。特别地，它实现了超过50倍的推理速度提升，并展示了强大的长度泛化能力，在标准基于标记模型完全失效的分布外音频持续时间内保持了高精度。

Conclusion: 本研究介绍的帧级内部工具使用方法不仅提高了音频语言模型处理时间定位任务的能力，还显著减少了所需的计算资源并增强了模型对于不同长度音频输入的适应性。

Abstract: Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.

</details>


### [35] [Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards](https://arxiv.org/abs/2602.10231)
*Kirill Pavlenko,Alexander Golubev,Simon Karasik,Boris Yangel*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，称为Blockwise Advantage Estimation，它为结构化生成中的每个目标分配独立的优势值，并仅应用于对应文本块中的令牌。这减少了对人工设计的标量奖励的依赖，并且可以自然地扩展到更多的目标上。


<details>
  <summary>Details</summary>
Motivation: Group Relative Policy Optimization (GRPO) 为完成中的所有令牌分配一个单一的标量优势，这在具有明确段落和目标的结构化生成中会导致不同段落间无关奖励信号的耦合，进而导致目标干扰和错误归因。

Method: 提出了Blockwise Advantage Estimation方法族，该方法给每个目标分配自己的优势，并且只将其应用于相应文本块内的令牌。此外，还引入了Outcome-Conditioned Baseline，通过根据前缀导出的中间结果来分层样本，以仅使用组内统计数据近似中间状态值。

Result: 在具有不确定性估计的数学任务上，该方法减轻了奖励干扰，与最先进的奖励设计方法相比具有竞争力，并保持了测试时从置信度加权集成获得的增益。

Conclusion: 更广泛地说，它提供了一种模块化的方法来优化结构化生成中的顺序目标，而无需额外的展开。

Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.

</details>


### [36] [Risk-Equalized Differentially Private Synthetic Data: Protecting Outliers by Controlling Record-Level Influence](https://arxiv.org/abs/2602.10232)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: 本文提出了一种风险均衡的差分隐私合成框架，该框架通过减少高风险记录对学习生成器的影响来优先保护这些记录。实验表明，这种方法能有效降低针对异常值记录的成员推理成功率。


<details>
  <summary>Details</summary>
Motivation: 在发布合成数据时，一些个体比其他人更难保护，特别是那些具有罕见疾病组合的患者或具有异常特征的交易更容易被识别。尽管差分隐私提供了最坏情况下的保证，但在中等隐私预算和辅助信息下，实证攻击（尤其是成员推理）往往更容易成功。因此，需要一种方法来更好地保护这些高风险记录。

Method: 提出了一个两阶段的风险均衡差分隐私合成机制：首先，使用少量隐私预算估计每条记录的“离群程度”；其次，差分隐私学习过程根据每条记录的风险评分反比地分配权重。对于高离群值的记录，通过故意减小其贡献度，可以为这些最需要保护的记录提供更严格的每实例隐私界限。

Result: 通过理论分析证明了整个流程的差分隐私保证，并推导出了合成阶段的每记录闭式界（评分阶段添加了一个统一的每记录项）。在模拟数据上的实验显示，风险加权显著降低了针对高离群值记录的成员推理成功率；消融研究表明是目标定位而非随机降权驱动了这种改进。在真实世界基准测试（乳腺癌、成人、德国信用）上，收益取决于数据集，突显了评分质量与合成管道之间的相互作用。

Conclusion: 风险均衡DP合成为保护高风险记录提供了一种有效的方法，通过减少它们对学习生成器的影响来实现更好的隐私保护，同时保持了数据的实用性。

Abstract: When synthetic data is released, some individuals are harder to protect than others. A patient with a rare disease combination or a transaction with unusual characteristics stands out from the crowd. Differential privacy provides worst-case guarantees, but empirical attacks -- particularly membership inference -- succeed far more often against such outliers, especially under moderate privacy budgets and with auxiliary information.
  This paper introduces risk-equalized DP synthesis, a framework that prioritizes protection for high-risk records by reducing their influence on the learned generator. The mechanism operates in two stages: first, a small privacy budget estimates each record's "outlierness"; second, a DP learning procedure weights each record inversely to its risk score. Under Gaussian mechanisms, a record's privacy loss is proportional to its influence on the output -- so deliberately shrinking outliers' contributions yields tighter per-instance privacy bounds for precisely those records that need them most.
  We prove end-to-end DP guarantees via composition and derive closed-form per-record bounds for the synthesis stage (the scoring stage adds a uniform per-record term). Experiments on simulated data with controlled outlier injection show that risk-weighting substantially reduces membership inference success against high-outlierness records; ablations confirm that targeting -- not random downweighting -- drives the improvement. On real-world benchmarks (Breast Cancer, Adult, German Credit), gains are dataset-dependent, highlighting the interplay between scorer quality and synthesis pipeline.

</details>


### [37] [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062)
*Jialin Liu,Zhaorui Zhang,Ray C. C. Cheung*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoToRec的新框架，通过稀疏正则化多模态分词来解决推荐系统中的冷启动问题。该方法使用了稀疏正则化的残差量化变分自动编码器生成可解释的离散token，同时引入自适应稀有度放大和层次多源图编码器以增强对冷启动项目的优先学习和支持信号融合。实验表明，MoToRec在整体性能和冷启动场景下均优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统虽然可以有效建模复杂的用户-项目交互，但数据稀疏性和物品冷启动问题严重影响了其性能，特别是对于没有或只有少量交互历史的新物品。尽管多模态内容提供了解决方案，但因数据中噪声及纠缠的存在，导致新物品的表示效果不佳。

Method: 研究者们提出了Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec)框架，基于稀疏正则化的残差量化变分自动编码器（RQ-VAE），生成由离散、可解释token组成的组合语义码。此外，MoToRec还包含了三个协同工作的组件：1）促进解耦表示的稀疏正则化RQ-VAE；2）鼓励冷启动物品优先学习的新颖自适应稀有度放大机制；3）用于与协作信号进行稳健信号融合的层次多源图编码器。

Result: 通过对三个大规模数据集进行广泛实验，结果证明了MoToRec不仅在整体表现上而且在冷启动场景下都超越了最先进的方法。

Conclusion: 研究表明，通过采用离散分词的方法能够有效地缓解长期存在的冷启动挑战，并且这种方式具有良好的扩展性。

Abstract: Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec's architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.

</details>


### [38] [Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation](https://arxiv.org/abs/2602.10249)
*Carlos Eduardo P. Silva,João Pedro M. Sena,Julio C. S. Reis,André G. Santos,Lucas N. Ferreira*

Main category: cs.LG

TL;DR: 本文提出了一种情境感知的推荐系统，该系统通过学生在课程中提交的源代码嵌入来建模学生的编程技能。这些嵌入可以预测学生在多个编程主题上的技能，并生成与未见的家庭作业问题所需技能相匹配的个人资料。为了生成推荐，我们计算学生个人资料和问题技能向量之间的余弦相似性，根据每个学生当前的能力对练习进行排名。我们使用来自我们大学入门编程课程的学生和练习的真实数据评估了我们的方法。结果表明，Jina嵌入在大多数技能上优于TF-IDF、CodeBERT-cpp和GraphCodeBERT。此外，通过对七个学期收集到的学生提交情况进行分析，我们还评估了该系统推荐与每周课程内容相符的练习的能力。我们的方法始终比基于正确性或解决方案时间的基线产生更合适的推荐，这表明预测的编程技能为问题推荐提供了更强的信号。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够准确预测学生编程技能并据此推荐适当练习的情境感知推荐系统，以提高学习效率和个人化教育体验。

Method: 1. 使用学生提交的源代码创建嵌入，用于预测他们在不同编程主题上的技能。
2. 通过计算学生技能档案与问题技能向量间的余弦相似度来生成个性化推荐。
3. 利用真实世界的数据集对比不同类型的嵌入（如Jina、TF-IDF等）的效果。
4. 分析系统推荐与实际课程进度匹配程度，以验证其有效性。

Result: 1. Jina嵌入在预测学生编程技能方面表现优于其他测试的方法。
2. 推荐系统能够根据学生当前能力提供更为合适的练习建议。
3. 与基于正确率或完成时间的传统推荐方式相比，基于预测技能的方法显示出更高的准确性。

Conclusion: 研究表明，利用源代码嵌入来估计学生的编程技能，并据此为他们推荐适合当前水平的学习材料是一种有效的方法。这种方法不仅提高了推荐的质量，也增强了个性化学习体验。

Abstract: In this paper, we propose a context-aware recommender system that models students' programming skills using embeddings of the source code they submit throughout a course. These embeddings predict students' skills across multiple programming topics, producing profiles that are matched to the skills required by unseen homework problems. To generate recommendations, we compute the cosine similarity between student profiles and problem skill vectors, ranking exercises according to their alignment with each student's current abilities. We evaluated our approach using real data from students and exercises in an introductory programming course at our university. First, we assessed the effectiveness of our source code embeddings for predicting skills, comparing them with token-based and graph-based alternatives. Results showed that Jina embeddings outperformed TF-IDF, CodeBERT-cpp, and GraphCodeBERT across most skills. Additionally, we evaluated the system's ability to recommend exercises aligned with weekly course content by analyzing student submissions collected over seven course offerings. Our approach consistently produced more suitable recommendations than baselines based on correctness or solution time, indicating that predicted programming skills provide a stronger signal for problem recommendation.

</details>


### [39] [Kernel-Based Learning of Chest X-ray Images for Predicting ICU Escalation among COVID-19 Patients](https://arxiv.org/abs/2602.10261)
*Qiyuan Shi,Jian Kang,Yi Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的多核学习方法GLIMARK，该方法扩展了传统的多核学习以适应属于指数族的结果变量，从而能够处理更广泛的数据类型。通过在COVID-19胸部X光数据集上的应用，证明了GLIMARK在预测ICU升级的二分类结果及提取具有临床意义特征方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统单核方法由于仅依赖于单一类型的核函数，在表示现实世界中异质或多面性质的数据时可能存在不足。而现有的多核学习方法主要针对连续性结果设计。因此，研究者提出了一个能处理指数族分布结果变量的新方法GLIMARK，以更好地适应不同类型的数据。

Method: 通过将多个简单的核组合成复合核，并结合来自不同来源的信息，开发出一种称为GLIMARK（广义线性模型与多加性回归核集成）的方法。这种方法能够适用于指数族分布的结果变量，从而扩展了多核学习的应用范围。

Result: 实证研究表明，GLIMARK能够有效地恢复或逼近真实的数据生成机制。在对COVID-19胸部X光片数据集进行分析时，GLIMARK不仅成功预测了需要转入ICU的二分类结果，还从中提取到了具有临床意义的特征。

Conclusion: 本研究提出的GLIMARK方法为多核学习提供了一个新的方向，使其能够处理更广泛的数据类型，特别是在医疗健康领域展现了其实际应用价值。

Abstract: Kernel methods have been extensively utilized in machine learning for classification and prediction tasks due to their ability to capture complex non-linear data patterns. However, single kernel approaches are inherently limited, as they rely on a single type of kernel function (e.g., Gaussian kernel), which may be insufficient to fully represent the heterogeneity or multifaceted nature of real-world data. Multiple kernel learning (MKL) addresses these limitations by constructing composite kernels from simpler ones and integrating information from heterogeneous sources. Despite these advances, traditional MKL methods are primarily designed for continuous outcomes. We extend MKL to accommodate the outcome variable belonging to the exponential family, representing a broader variety of data types, and refer to our proposed method as generalized linear models with integrated multiple additive regression with kernels (GLIMARK). Empirically, we demonstrate that GLIMARK can effectively recover or approximate the true data-generating mechanism. We have applied it to a COVID-19 chest X-ray dataset, predicting binary outcomes of ICU escalation and extracting clinically meaningful features, underscoring the practical utility of this approach in real-world scenarios.

</details>


### [40] [Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models](https://arxiv.org/abs/2602.10282)
*Kanta Yamaoka,Sumantrak Mukherjee,Thomas Gärtner,David Antony Selby,Stefan Konigorski,Eyke Hüllermeier,Viktor Bengs,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 研究引入了Linear-LLM-SCM框架，用于评估大型语言模型在给定有向无环图(DAG)时对线性高斯结构因果模型(SCM)参数化的性能。实验揭示了模型结果的随机性和对连续域中虚假边的敏感性等问题，并展示了当前大型语言模型作为定量因果参数化工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已经显示出识别定性因果关系的能力，但它们在估计连续领域内功能性关系所参数化的效应大小方面的能力尚未得到充分探索。为了填补这一空白，本研究旨在开发一个框架来专门评估LLMs在线性高斯结构因果模型中的定量因果推理能力。

Method: 研究人员提出了名为Linear-LLM-SCM的即插即用基准测试框架，该框架能够将DAG分解为局部父-子集，并促使LLM为每个节点生成回归样式的结构方程。这些方程随后被汇总并与可用的真实参数进行比较。

Result: 实验结果显示，在某些模型的结果中存在强烈的随机性，并且当连续领域内的DAG包含错误指定的边缘时，模型表现出脆弱性。此外，还观察到不同模型之间系数估计值的巨大变异性以及对结构和语义扰动的敏感性。

Conclusion: 研究指出了当前大型语言模型作为定量因果参数化器的一些限制。不过，通过开源这个基准测试框架，研究人员可以轻松地使用自己的DAGs和现成的LLMs来进行相关领域的评估。

Abstract: Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.

</details>


### [41] [R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting](https://arxiv.org/abs/2602.10312)
*Lipai Huang,Kai Yin,Chia-Fu Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: R2RAG-Flood框架结合了无需训练的检索增强生成方法与基于推理的知识库，用于风暴后财产损害的即时预测。通过构建包含结构化预测器、自然语言摘要和模型生成推理轨迹的知识库，在推理时利用相关推理轨迹来辅助大型语言模型进行适应性推理而非学习新的任务特定参数。在德克萨斯州哈里斯县的研究案例中，该框架展现出了接近监督基线模型的准确率，并且对于中等和高损害级别的分类准确度更高。此外，R2RAG-Flood变体显示出比监督基线模型及更大语言模型更高的成本效益效率，而且不需要针对任务的具体训练或微调。


<details>
  <summary>Details</summary>
Motivation: 为了提高风暴后财产损害即时预测（nowcasting）的能力，同时减少对大量数据依赖和避免耗时的任务特定训练过程。

Method: 开发了一个名为R2RAG-Flood的框架，它是一个强化推理、无需训练的检索增强生成框架。该框架基于现有监督表格预测器建立，创建了一个以推理为中心的知识库，其中每个样本都包含了结构化的预测因子、简洁的自然语言文本总结以及模型生成的推理路径。在推断过程中，R2RAG-Flood发出上下文增强提示，从附近的地理空间邻居和规范类别原型中检索并根据相关的推理路径进行调整。

Result: 在德克萨斯州哈里斯县的一个案例研究中，直接使用结构化预测因子训练的监督表格基线达到了0.714的整体准确率和针对中等到高损害等级的0.859准确率。而R2RAG-Flood跨七个大型语言模型骨干实现了0.613至0.668的整体准确率和0.757至0.896的损害级别准确率。此外，轻量级R2RAG-Flood变种展示出明显优于监督表格基线及更大语言模型的成本效益效率。

Conclusion: R2RAG-Flood能够在不经过特定任务训练的情况下，实现接近监督学习基线模型的预测性能，并且提供了每个预测背后的结构化理由。这种方法不仅提高了风暴后财产损害即时预测的准确性，还增强了成本效益效率。

Abstract: R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each sample includes structured predictors, a compact natural language text-mode summary, and a model-generated reasoning trajectory. During inference, R2RAG-Flood issues context-augmented prompts that retrieve and condition on relevant reasoning trajectories from nearby geospatial neighbors and canonical class prototypes, enabling the large language model backbone to emulate and adapt prior reasoning rather than learn new task-specific parameters. Predictions follow a two-stage procedure that first determines property damage occurrence and then refines severity within a three-level Property Damage Extent categorization, with a conditional downgrade step to correct over-predicted severity. In a case study of Harris County, Texas at the 12-digit Hydrologic Unit Code scale, the supervised tabular baseline trained directly on structured predictors achieves 0.714 overall accuracy and 0.859 damage class accuracy for medium and high damage classes. Across seven large language model backbones, R2RAG-Flood attains 0.613 to 0.668 overall accuracy and 0.757 to 0.896 damage class accuracy, approaching the supervised baseline while additionally producing a structured rationale for each prediction. Using a severity-per-cost efficiency metric derived from API pricing and GPU instance costs, lightweight R2RAG-Flood variants demonstrate substantially higher efficiency than both the supervised tabular baseline and larger language models, while requiring no task-specific training or fine-tuning.

</details>


### [42] [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)
*Jaeyeon Kim,Jonathan Geuter,David Alvarez-Melis,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: 提出了一种名为PUMA（Progressive UnMAsking）的方法，通过调整MDMs的前向遮罩过程来对齐训练和推理时的遮罩模式，从而加速训练并优化针对推理一致遮罩的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Masked Diffusion Models (MDMs)在生成建模中表现出色，但它们需要在一个指数级大小的遮罩模式集合上进行训练，这不仅计算成本高，还会导致训练-测试不匹配的问题。因此，研究者们旨在寻找一种方法来解决这些问题，同时保持MDM的优势。

Method: 研究者提出了PUMMA方法，它通过简单地修改MDMs中的前向遮罩过程，使得训练过程中使用的遮罩模式与推理过程中实际遇到的更加一致。这种调整有助于将优化集中在那些与推理相关的遮罩模式上，同时也加快了模型训练的速度。

Result: 实验结果显示，PUMA能够显著加速预训练过程，在1.25亿参数规模下大约提高了2.5倍的速度，并且可以与其他常用技术如自回归初始化等结合使用以取得额外的好处。

Conclusion: PUMA作为一种新颖而有效的解决方案，成功解决了MDMs中存在的训练复杂度高以及训练-测试不匹配问题，为未来的研究提供了有价值的方向。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a training complexity trade-off: MDMs train on an exponentially large set of masking patterns, which is not only computationally expensive, but also creates a train--test mismatch between the random masks used in training and the highly structured masks induced by inference-time unmasking. In this work, we propose Progressive UnMAsking (PUMA), a simple modification of the forward masking process that aligns training-time and inference-time masking patterns, thereby focusing optimization on inference-aligned masks and speeding up training. Empirically, PUMA speeds up pretraining at the 125M scale by $\approx 2.5\times$ and offers complementary advantages on top of common recipes like autoregressive initialization. We open-source our codebase at https://github.com/JaeyeonKim01/PUMA.

</details>


### [43] [Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution](https://arxiv.org/abs/2602.10357)
*Haixu Liao,Yating Zhou,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: 本文开发了一个理论框架来分析在不平衡数据下基于Transformer编码器的对比学习的训练动态，揭示了神经元权重演化的三个不同阶段，并提出剪枝可以恢复由不平衡导致的性能下降并增强特征分离。


<details>
  <summary>Details</summary>
Motivation: 对比学习已经成为一种强大的学习可泛化表示的框架，但其理论理解仍然有限，特别是在现实应用中普遍存在的不平衡数据分布情况下。这种不平衡会降低表示质量并引起模型行为偏差，但缺乏对其影响的严格描述。

Method: 通过开发一个理论框架来分析基于Transformer编码器的对比学习在处理不平衡数据时的训练动态。研究发现神经元权重在训练过程中经历三个不同的阶段，针对多数特征、少数特征和噪声有着不同的动态表现。

Result: 结果显示，少数特征减少了表征能力，增加了对更复杂架构的需求，并阻碍了真实特征与噪声之间的分离。受到这些神经元层面行为的启发，研究表明剪枝能够恢复因不平衡而降级的表现，并且加强特征分离。

Conclusion: 通过对不平衡数据下对比学习训练动态的深入分析，为理解和改进这一过程提供了新的见解。剪枝被证明是解决不平衡问题的有效方法之一，有助于提升模型性能和特征分离效果。

Abstract: Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.

</details>


### [44] [Simple LLM Baselines are Competitive for Model Diffing](https://arxiv.org/abs/2602.10371)
*Elias Kempf,Simon Schrodi,Bartosz Cywiński,Thomas Brox,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估指标，用于比较基于LLM的方法和基于SAE的方法在模型差异检测上的表现，结果表明改进后的基于LLM的基线方法与基于SAE的方法性能相当，但通常能揭示更抽象的行为差异。


<details>
  <summary>Details</summary>
Motivation: 标准的大规模语言模型（LLM）评估仅测试了设计者预设的能力或倾向，忽略了如不同版本模型间行为转变或新兴偏差等意外差异。模型差异分析试图通过自动发现系统性行为差异来解决这一局限性。然而，目前缺少对现有方法（包括基于LLM生成自然语言描述的方法以及识别可解释特征的稀疏自动编码器[SAE]方法）之间进行系统比较的研究，也缺乏既定的评估标准。

Method: 作者提出了针对关键需求（泛化能力、趣味性和抽象层次）的新评估指标，并使用这些指标来比较现有的基于LLM的方法与基于SAE的方法。

Result: 研究结果显示，经过改进的基于LLM的基线方法在性能上可以与基于SAE的方法相媲美，同时往往能够揭示更为抽象的行为差异。

Conclusion: 本研究填补了模型差异分析领域中不同方法对比及评价标准缺失的空白，为未来相关工作提供了有用的参考。

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>


### [45] [Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs](https://arxiv.org/abs/2602.10377)
*Luoyang Sun,Jiwen Jiang,Yifeng Ding,Fengfa Li,Yan Song,Haifeng Zhang,Jian Ying,Lei Ren,Kun Zhan,Wei Chen,Yan Xie,Cheng Deng*

Main category: cs.LG

TL;DR: 本文提出了一种硬件协同设计法则，该法则结合了模型准确性和推理性能，并通过在NVIDIA Jetson Orin上评估1,942个候选架构，训练选定的170个模型来拟合一个与训练损失相关的规模法则。此方法将架构选择时间从几个月缩短到几天，并且在目标硬件上达到与Qwen2.5-0.5B相同的延迟时，所设计的架构在WikiText-2上的困惑度降低了19.42%。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-行动模型（VLAs）作为物理AI的关键范例，在自动驾驶车辆、机器人和智能空间中日益普及。在这些资源受限的设备设置中，选择合适的大型语言模型（LLM）基础成为一个关键挑战：模型必须在准确性与严格的推理延迟和硬件效率限制之间取得平衡。因此，针对设备上LLM部署的硬件-软件协同设计成为了一个改变游戏规则的需求，每个硬件平台都需要定制化的架构解决方案。

Method: 研究者们提出了一个能够同时捕捉模型准确性和推理性能的硬件协同设计法则。具体来说，他们将训练损失建模为架构超参数的显式函数，并通过屋顶线建模来描述推理延迟。接着，他们在NVIDIA Jetson Orin平台上对1,942个候选架构进行了实证评估，挑选出170个模型进行各10B tokens的训练，以适应一个将架构与训练损失联系起来的比例法则。通过将这个比例法则与延迟建模相结合，建立了直接的准确性-延迟对应关系，并确定了硬件协同设计LLMs的帕累托前沿。此外，还把架构搜索公式化为精度与性能的联合优化问题，从而推导出符合工业硬件和应用预算的设计可行区域。

Result: 采用这种方法后，架构选择的过程被显著加速，从原来的数月减少到了几天。特别地，在目标硬件上保持与Qwen2.5-0.5B相同延迟的情况下，经过协同设计后的架构在WikiText-2数据集上的困惑度下降了19.42%。

Conclusion: 这是首次为设备端LLM部署提供了基于硬件协同设计缩放定律的原则性和操作性框架。研究团队计划公开发布代码及相关检查点，以便于更广泛的社区使用。

Abstract: Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.

</details>


### [46] [Deep learning outperforms traditional machine learning methods in predicting childhood malnutrition: evidence from survey data](https://arxiv.org/abs/2602.10381)
*Deepak Bastola,Yang Li*

Main category: cs.LG

TL;DR: 本研究首次全面评估了机器学习和深度学习方法在尼泊尔5岁以下儿童中识别营养不良的有效性。通过比较16种算法，发现TabNet表现最佳。主要预测因素包括母亲教育、家庭财富指数和儿童年龄等。这些结果展示了一个可扩展的基于调查的筛查框架，有助于指导针对性的营养干预措施，并支持尼泊尔实现可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 鉴于尼泊尔及其他资源匮乏地区儿童营养不良问题严重，而传统的病例发现方法既耗时又难以在偏远地区实施，本研究旨在探索利用机器学习和深度学习技术来更有效地识别营养不良儿童的可能性。

Method: 研究使用了2019年尼泊尔多指标集群调查（MICS）的数据，构建了一个综合营养不良指标，结合了发育迟缓、消瘦和体重不足的状态。然后，系统地比较了跨越深度学习、梯度提升以及传统机器学习家族的16种算法。模型性能通过十个指标进行评估，特别强调F1分数和召回率以考虑显著的类别不平衡及漏诊营养不良儿童的高成本。

Result: 结果显示，在所有测试模型中，TabNet表现出色，优于支持向量机和AdaBoost分类器。共识特征重要性分析揭示了母亲教育水平、家庭财富指数及儿童年龄是营养不良的主要预测因子，其次是地理特征、疫苗接种状态和餐食频率等因素。

Conclusion: 研究表明，基于调查数据的机器学习模型能够有效识别出营养不良风险较高的儿童群体，并为制定针对性营养干预措施提供依据。该方法不仅支持尼泊尔朝着可持续发展目标迈进，也为全球其他资源有限地区的类似挑战提供了可转移的方法论模板。

Abstract: Childhood malnutrition remains a major public health concern in Nepal and other low-resource settings, while conventional case-finding approaches are labor-intensive and frequently unavailable in remote areas. This study provides the first comprehensive assessment of machine learning and deep learning methodologies for identifying malnutrition among children under five years of age in Nepal. We systematically compared 16 algorithms spanning deep learning, gradient boosting, and traditional machine learning families, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was constructed by integrating stunting, wasting, and underweight status, and model performance was evaluated using ten metrics, with emphasis on F1-score and recall to account for substantial class imbalance and the high cost of failing to detect malnourished children. Among all models, TabNet demonstrated the best performance, likely attributable to its attention-based architecture, and outperformed both support vector machine and AdaBoost classifiers. A consensus feature importance analysis identified maternal education, household wealth index, and child age as the primary predictors of malnutrition, followed by geographic characteristics, vaccination status, and meal frequency. Collectively, these results demonstrate a scalable, survey-based screening framework for identifying children at elevated risk of malnutrition and for guiding targeted nutritional interventions. The proposed approach supports Nepal's progress toward the Sustainable Development Goals and offers a transferable methodological template for similar low-resource settings globally.

</details>


### [47] [Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series](https://arxiv.org/abs/2602.10385)
*Jia Li,Yu Hou,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间-Transformer架构LITT，它能够对序列事件进行临时对齐，并在虚拟的'相对时间线'上分配'相对时间戳'，从而实现对临床轨迹的个性化解释。通过实际纵向EHR数据验证了其可解释性和有效性，LITT在公共数据集上的表现优于基准和最先进的生存分析方法，为临床AI中的精准医疗提供了重要进步。


<details>
  <summary>Details</summary>
Motivation: 自动从大规模时间序列数据中发现个性化的序列事件对于临床研究中的精准医疗至关重要，但即使对于现代AI模型来说仍然具有挑战性。现有的transformer模型虽然能捕捉丰富的关联，但大多忽略了事件的时间和顺序，这可能会忽略潜在的因果推理。因此，需要一种方法来评估患者特定轨迹之间的'对齐程度'，并识别它们共享的模式，即将时间视为一个真正的'可计算'维度，允许模型为候选事件分配超出其观察到的实际时间之外的'相对时间戳'。

Method: 本文引入了一种新的Timing-Transformer架构LITT，该架构能够在虚拟的'相对时间线'上实现序列事件的临时对齐，从而实现以事件时间为焦点的关注和个人化的临床轨迹解读。

Result: LITT的有效性和可解释性通过来自3,276名乳腺癌患者的现实世界纵向EHR数据得到了验证，用于预测心脏毒性引起的心脏病发作时间。此外，在公共数据集上，LITT的表现超过了基准和最前沿的生存分析方法。

Conclusion: LITT代表了临床AI中精准医疗的重要进步，因为它不仅提高了预测性能，而且通过提供更个性化的序列事件理解，为医疗决策提供了有价值的见解。

Abstract: Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostly agnostic to event timing and ordering, thereby bypassing potential causal reasoning.
  Intuitively, we need a method capable of evaluating the "degree of alignment" among patient-specific trajectories and identifying their shared patterns, i.e., the significant events in a consistent sequence. This necessitates treating timing as a true \emph{computable} dimension, allowing models to assign ``relative timestamps'' to candidate events beyond their observed physical times.
  In this work, we introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual ``relative timeline'', thereby enabling \emph{event-timing-focused attention} and personalized interpretations of clinical trajectories. Its interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict the onset timing of cardiotoxicity-induced heart disease. Furthermore, LITT outperforms both the benchmark and state-of-the-art survival analysis methods on public datasets, positioning it as a significant step forward for precision medicine in clinical AI.

</details>


### [48] [Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models](https://arxiv.org/abs/2602.10386)
*Angelo Zangari,Peyman Baghershahi,Sourav Medya*

Main category: cs.LG

TL;DR: 该论文提出了一种将图结构转换为自然语言提示的新方法，通过使用Weisfeiler-Lehman相似性类变体并将其映射到类似人类理解的颜色标记上，从而改善了大型语言模型在处理图问题时的表现。实验结果表明这种方法在算法和预测性的图任务中都取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理无结构文本方面表现出色，但在处理图问题时面临挑战，因为后者需要对明确的结构、排列不变性和计算复杂的关系进行推理。本研究旨在探索如何克服这些障碍，使LLMs能够有效地应用于图问题。

Method: 提出了一种新的图到文本的翻译策略，它将图结构直接编码进自然语言提示中。这一过程涉及计算一种Weisfeiler-Lehman (WL) 相似性类别变体，并用更易于人类理解的颜色标记代替数字标签来表示这种结构信息。

Result: 实验结果显示，在多个合成数据集和真实世界数据集中，所提出的方法对于算法性和预测性的图任务均有显著性能提升。特别是当图任务要求对整个图结构进行推理时，新方法通过捕捉局部与全局依赖关系进一步提高了LLM的表现。

Conclusion: 通过采用更加直观且富含语义意义的编码方式而非难以理解的符号编码，可以让LLMs更好地理解和处理图相关的任务。这项研究表明，适当调整输入表示可以显著增强LLMs解决图问题的能力。

Abstract: Graph problems are fundamentally challenging for large language models (LLMs). While LLMs excel at processing unstructured text, graph tasks require reasoning over explicit structure, permutation invariance, and computationally complex relationships, creating a mismatch with the representations of text-based models. Our work investigates how LLMs can be effectively applied to graph problems despite these barriers. We introduce a human-interpretable structural encoding strategy for graph-to-text translation that injects graph structure directly into natural language prompts. Our method involves computing a variant of Weisfeiler-Lehman (WL) similarity classes and maps them to human-like color tokens rather than numeric labels. The key insight is that semantically meaningful and human-interpretable cues may be more effectively processed by LLMs than opaque symbolic encoding. Experimental results on multiple algorithmic and predictive graph tasks show the considerable improvements by our method on both synthetic and real-world datasets. By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure.

</details>


### [49] [Modular Multi-Task Learning for Chemical Reaction Prediction](https://arxiv.org/abs/2602.10404)
*Jiayun Pang,Ahmed M. Zaitoun,Xacobe Couso Cambeiro,Ivan Vulić*

Main category: cs.LG

TL;DR: 本文评估了低秩适应（LoRA）作为在有限复杂数据集上进行有机反应预测时，全微调的一种参数高效的替代方法。研究发现，LoRA不仅在准确性上与全微调相当，而且能更有效地缓解灾难性遗忘、保持多任务性能，并且在C-H功能化微调中显示出对特定反应模式的更有效适应。


<details>
  <summary>Details</summary>
Motivation: 针对化学和制药研发领域内将大型语言模型（LLMs）调整到较小、领域特定的反应数据集的关键挑战，探讨如何在学习新的反应知识的同时保持广泛的化学理解能力。

Method: 采用低秩适应（LoRA）技术，在USPTO反应类别及具有挑战性的C-H功能化反应基础上，对正向反应预测、逆合成分析以及试剂预测等任务进行了基准测试。

Result: LoRA在准确度方面与完全微调相媲美，同时能够更好地避免灾难性遗忘现象，并维持跨任务表现；两种微调方法均能在训练分布之外泛化，产生合理的溶剂预测结果；值得注意的是，在C-H功能化微调过程中观察到了LoRA与完全微调对于反应活性模式编码上的细微差异。

Conclusion: 随着大型语言模型规模的持续扩大，本研究表明模块化、参数高效微调策略对于其灵活应用于化学领域是实用的选择。

Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across related tasks. Here, we evaluate Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, we benchmark forward reaction prediction, retrosynthesis and reagent prediction. LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both fine-tuning approaches generalise beyond training distributions, producing plausible alternative solvent predictions. Notably, C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA. As LLMs continue to scale, our results highlight the practicality of modular, parameter-efficient fine-tuning strategies for their flexible deployment for chemistry applications.

</details>


### [50] [Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference](https://arxiv.org/abs/2602.10408)
*Andrei Kanavalau,Carmen Amo Alonso,Sanjay Lall*

Main category: cs.LG

TL;DR: 本文介绍了TaperNorm，一种用于替代RMSNorm/LayerNorm的新方法，它在训练初期与标准归一化器行为一致，随后逐渐过渡到一个学习到的样本无关线性/仿射映射。通过这种方式，TaperNorm能够在保持模型性能的同时消除逐token统计，并允许这些层在推理时合并到相邻的线性投影中，从而提高效率。


<details>
  <summary>Details</summary>
Motivation: 作者们重新审视了Transformer块内样本依赖型归一化的必要性，并探索了一种新的归一化技术TaperNorm，旨在理解输出归一化在稳定训练中的关键作用以及如何可能去除每层的归一化同时保持模型性能。

Method: 提出了TaperNorm，这是一种可以无缝替换RMSNorm/LayerNorm的技术，它最初模仿标准归一化器的行为，然后平滑地转换为基于学习的、样本独立的线性或仿射变换。此外，还引入了一个简单的固定目标辅助损失来提供另一种锚定方式，帮助移除最后的归一化层。

Result: 实验结果表明，TaperNorm可以在相同的设置下达到与使用归一化的基线模型相匹配的表现，同时消除了逐token统计并允许将内部缩放合并到邻近的线性投影中，在特定模式下最高可实现1.22倍的吞吐量提升。

Conclusion: 本研究表明，通过采用TaperNorm方法，有可能向无归一化Transformer迈出一步，同时也揭示了输出归一化在防止无界logit增长方面所扮演的独特角色。

Abstract: Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.

</details>


### [51] [LUCID: Attention with Preconditioned Representations](https://arxiv.org/abs/2602.10410)
*Sai Surya Duvvuri,Nirmal Patel,Nilesh Gupta,Inderjit S. Dhillon*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制——LUCID Attention，通过在注意力概率上应用预处理器来解决长序列场景下softmax函数导致的性能下降问题。这种方法允许模型在大量键中准确聚焦于重要键，并且不需要降低softmax温度从而避免了学习性问题。实验结果表明，与标准注意力相比，LUCID Attention在长上下文检索任务上的表现有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度增加，基于softmax的点积注意力机制由于其固有的局限性（将概率质量扩散到不相关的标记上）而导致在处理长序列时性能下降。此外，试图通过降低softmax温度来提高焦点清晰度的方法会因为梯度消失而阻碍学习。因此，需要一种新的方法来改善长序列场景下的注意力机制表现。

Method: 提出了LUCID Attention架构修改方案，该方案通过对注意力概率施加一个预处理器来工作。这个预处理器是从指数化的键-键相似性派生出来的，旨在最小化再生核希尔伯特空间内键之间的重叠，从而使查询能够精确地从大量键中识别出重要的键。这种基于预处理的方法还避免了对低温度的需求及其带来的学习性问题。

Result: 通过训练约10亿参数的语言模型并在最多128K个标记上进行评估，验证了所提方法的有效性。结果显示，在来自BABILong、RULER、SCROLLS和LongBench的长上下文检索任务上取得了显著的进步。例如，在BABILong任务上相比于标准注意力提高了高达18%，在RULER多针性能上则提升了14%。

Conclusion: LUCID Attention提供了一个有效的方法来增强长序列场景下的注意力机制，不仅提高了检索任务的表现，同时也保持了与标准注意力相同的计算复杂度。

Abstract: Softmax-based dot-product attention is a cornerstone of Transformer architectures, enabling remarkable capabilities such as in-context learning. However, as context lengths increase, a fundamental limitation of the softmax function emerges: it tends to diffuse probability mass to irrelevant tokens degrading performance in long-sequence scenarios. Furthermore, attempts to sharpen focus by lowering softmax temperature hinder learnability due to vanishing gradients. We introduce LUCID Attention, an architectural modification that applies a preconditioner to the attention probabilities. This preconditioner, derived from exponentiated key-key similarities, minimizes overlap between the keys in a Reproducing Kernel Hilbert Space, thus allowing the query to focus on important keys among large number of keys accurately with same computational complexity as standard attention. Additionally, LUCID's preconditioning-based approach to retrieval bypasses the need for low temperature and the learnability problems associated with it. We validate our approach by training ~1 billion parameter language models evaluated on up to 128K tokens. Our results demonstrate significant gains on long-context retrieval tasks, specifically retrieval tasks from BABILong, RULER, SCROLLS and LongBench. For instance, LUCID achieves up to 18% improvement in BABILong and 14% improvement in RULER multi-needle performance compared to standard attention.

</details>


### [52] [LightGTS-Cov: Covariate-Enhanced Time Series Forecasting](https://arxiv.org/abs/2602.10412)
*Yong Shang,Zhipeng Yao,Ning Jin,Xiangfei Qiu,Hui Zhang,Bin Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列基础模型LightGTS-Cov，该模型在保持轻量级、周期感知的骨干架构的同时，明确地整合了过去和已知未来的协变量，从而在电价预测和可再生能源预测等应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列基础模型通常忽略了外生协变量或者只是简单地将它们与目标序列连接起来，这限制了它们在富含协变量的应用中的有效性，例如电力价格预测和可再生能源预测。

Method: 通过扩展原有的LightGTS模型，引入了LightGTS-Cov，它基于约100万参数的LightGTS骨干，并额外添加了一个约0.1百万参数的多层感知机（MLP）插件来整合对齐的时间协变量，通过对解码过程输出进行残差修正的方式，以增强预测性能。

Result: 在电价和能源生成数据集上的协变量感知基准测试中，LightGTS-Cov不仅一贯优于原始的LightGTS，而且相较于其他协变量感知基线方法也展现出更好的性能。此外，在两个实际的能量案例应用——长期光伏功率预测及日前电价预测中，LightGTS-Cov实现了高的预测准确性和稳定的运行表现。

Conclusion: LightGTS-Cov模型在理论实验和实际工业场景下均展示了其在处理协变量丰富的时序预测任务中的优越性，证明了其实践价值。

Abstract: Time series foundation models are typically pre-trained on large, multi-source datasets; however, they often ignore exogenous covariates or incorporate them via simple concatenation with the target series, which limits their effectiveness in covariate-rich applications such as electricity price forecasting and renewable energy forecasting. We introduce LightGTS-Cov, a covariate-enhanced extension of LightGTS that preserves its lightweight, period-aware backbone while explicitly incorporating both past and future-known covariates. Built on a $\sim$1M-parameter LightGTS backbone, LightGTS-Cov adds only a $\sim$0.1M-parameter MLP plug-in that integrates time-aligned covariates into the target forecasts by residually refining the outputs of the decoding process. Across covariate-aware benchmarks on electricity price and energy generation datasets, LightGTS-Cov consistently outperforms LightGTS and achieves superior performance over other covariate-aware baselines under both settings, regardless of whether future-known covariates are provided. We further demonstrate its practical value in two real-world energy case applications: long-term photovoltaic power forecasting with future weather forecasts and day-ahead electricity price forecasting with weather and dispatch-plan covariates. Across both applications, LightGTS-Cov achieves strong forecasting accuracy and stable operational performance after deployment, validating its effectiveness in real-world industrial settings.

</details>


### [53] [AI-rithmetic](https://arxiv.org/abs/2602.10416)
*Alex Bie,Travis Dick,Alex Kulesza,Prabhakar Raghavan,Vinod Raman,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 尽管现代AI系统在高级数学任务中表现出色，但在基本算术方面表现不佳。研究发现，随着数字位数的增加，所有前沿模型在整数加法上的准确性显著下降。错误主要归因于操作数对齐问题或进位失败，这两种类型的错误分别解释了Claude Opus 4.1、GPT-5和Gemini 2.5 Pro大部分的错误。


<details>
  <summary>Details</summary>
Motivation: 研究人员注意到一个现象：尽管AI在复杂的数学任务上取得了成功，但在执行简单的算术运算如加法时却表现得很差。这种反差促使他们深入探讨背后的原因。

Method: 通过实验方法，研究者们系统地考察了多个先进的AI模型（包括Claude Opus 4.1, GPT-5, 和 Gemini 2.5 Pro）在进行不同位数整数相加时的表现，并对这些模型所产生的错误进行了分类与分析。

Result: 研究结果显示，随着需要处理的整数位数增多，所有测试中的前沿AI模型在整数加法任务上的准确度都会大幅降低。此外，大多数错误可以被归类为操作数未对齐或未能正确进位两大类。

Conclusion: 这项研究表明，即使是当前最先进的AI模型，在处理基础算术任务时也存在明显局限性。错误主要来源于操作数对齐不当以及进位机制失效两个方面。

Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failing on the simple task of adding two numbers. We present a systematic investigation of this phenomenon. We demonstrate empirically that all frontier models suffer significantly degraded accuracy for integer addition as the number of digits increases. Furthermore, we show that most errors made by these models are highly interpretable and can be attributed to either operand misalignment or a failure to correctly carry; these two error classes explain 87.9%, 62.9%, and 92.4% of Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro errors, respectively. Finally, we show that misalignment errors are frequently related to tokenization, and that carrying errors appear largely as independent random failures.

</details>


### [54] [Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning](https://arxiv.org/abs/2602.10420)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 本文探讨了流匹配框架在二值流形上的应用，指出了信号空间预测与基于速度的目标函数结合时存在的潜在结构不匹配问题，并提出通过重新对齐目标函数到信号空间来解决这一问题，从而实现稳健的训练过程。此外，还研究了针对二值数据的设计选择，为离散域上鲁棒的流匹配提供了理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: 作者注意到，在将流匹配框架应用于二值流形时，信号空间预测($x$-prediction)虽然有效，但当其与基于速度的目标函数($v$-loss)结合使用时会产生时间依赖性的奇异加权效应，这会放大梯度对于近似误差的敏感性。基于此观察，研究旨在找到一种方法以消除这种不良影响，提高模型训练的稳健性。

Method: 首先，正式定义了预测-损失对齐作为流匹配训练的一个必要条件；接着证明了将目标函数重新对齐至信号空间（即采用$x$-loss）能够消除奇异加权现象，使得梯度保持一致有界，允许在均匀时间步长采样下进行稳定训练而无需依赖启发式调度策略。最后，探索了适用于二值数据的具体设计方案，揭示了几何损失与概率目标之间的拓扑依赖性差异。

Result: 研究结果表明，通过对齐目标函数至信号空间可以显著改善模型训练稳定性，支持在没有复杂调度方案的情况下利用均匀时间步长进行高效学习。同时，对于处理二值数据而言，正确选择损失函数类型（如交叉熵或均方误差）至关重要，这取决于具体的应用场景及其背后的数学原理。

Conclusion: 本研究表明，为了在二值及相关的离散领域内实现鲁棒的流匹配，确保信号空间对齐是一个关键原则。该发现不仅为理解不同损失函数如何影响训练过程提供了新视角，也为实际应用中选择合适的方法论给出了指导建议。

Abstract: Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.

</details>


### [55] [Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation](https://arxiv.org/abs/2602.10430)
*Jie Jiang,Yusen Huo,Xiangxin Zhan,Changping Wang,Jun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种分布鲁棒策略优化（DRPO）方法，通过识别嵌入在噪声行为策略中的潜在高质量分布，解决了离线强化学习中因低质量数据导致的模型崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于策略的强化学习方法在处理离线历史日志时面临一个关键问题：低质量数据的主导会导致严重的模型崩溃。

Method: 提出了分布鲁棒策略优化（DRPO），将目标重新定义为乐观分布鲁棒优化问题，并证明了硬过滤是该优化目标的确切解，能够最优地恢复高质量行为同时严格排除引起差异的噪声。

Result: 广泛的实验表明，DRPO在混合质量推荐基准上达到了最先进的性能。

Conclusion: DRPO提供了一个有效的方法来解决离线强化学习中的模型崩溃问题，通过精确识别并利用高质量的行为模式，从而改善了推荐系统的性能。

Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.

</details>


### [56] [QTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMs](https://arxiv.org/abs/2602.10431)
*Kanghyun Noh,Jinheon Choi,Yulwha Kim*

Main category: cs.LG

TL;DR: 提出了一种名为QTALE的新框架，该框架可以将token-adaptive执行与量化无缝集成，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要大量的计算和内存资源，这给有效部署带来了挑战。尽管token-adaptive层执行和量化技术可以帮助解决这些问题，但直接结合这两种技术会导致额外的准确性下降。

Method: QTALE通过引入两个关键组件来克服传统方法的局限性：（1）一种训练策略，确保在微调过程中积极探索多样化的执行路径；（2）一个后训练机制，允许在推理时灵活调整执行比率，以在需要时重新引入冗余。

Result: 实验结果表明，QTALE能够实现token-adaptive层执行与量化的无缝集成，与仅使用量化模型相比，在CommonsenseQA基准测试上的准确率差距保持在0.5%以内。

Conclusion: 通过结合用于减少FLOPs的token-adaptive执行和用于节省内存的量化技术，QTALE为高效部署LLM提供了一个有效的解决方案。

Abstract: Large language models (LLMs) demand substantial computational and memory resources, posing challenges for efficient deployment. Two complementary approaches have emerged to address these issues: token-adaptive layer execution, which reduces floating-point operations (FLOPs) by selectively bypassing layers, and quantization, which lowers memory footprint by reducing weight precision. However, naively integrating these techniques leads to additional accuracy degradation due to reduced redundancy in token-adaptive models. We propose QTALE (Quantization-Robust Token-Adaptive Layer Execution for LLMs), a novel framework that enables seamless integration of token-adaptive execution with quantization while preserving accuracy. Conventional token-adaptive methods reduce redundancy in two ways: (1) by limiting the diversity of training paths explored during fine-tuning, and (2) by lowering the number of parameters actively involved in inference. To overcome these limitations, QTALE introduces two key components: (1) a training strategy that ensures diverse execution paths are actively explored during fine-tuning, and (2) a post-training mechanism that allows flexible adjustment of the execution ratio at inference to reintroduce redundancy when needed. Experimental results show that QTALE enables seamless integration of token-adaptive layer execution with quantization, showing no noticeable accuracy difference, with the gap to quantization-only models kept below 0.5% on CommonsenseQA benchmarks. By combining token-adaptive execution for FLOPs reduction and quantization for memory savings, QTALE provides an effective solution for efficient LLM deployment.

</details>


### [57] [A Unified Theory of Random Projection for Influence Functions](https://arxiv.org/abs/2602.10449)
*Pingbang Hu,Yuzheng Hu,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的理论，用于表征在何种情况下投影可以保证影响函数的保存，并为实际应用中选择合适的草图大小提供了原则性指导。


<details>
  <summary>Details</summary>
Motivation: 影响函数和相关数据归因分数的形式是$g^{\top}F^{-1}g^{\prime}$，其中$F\succeq 0$是一个曲率算子。在现代过参数化模型中，形成或反转$F\in\mathbb{R}^{d\times d}$是不切实际的，因此通过随机投影与草图$P \in \mathbb{R}^{m\times d}$来实现可扩展的影响计算变得很有吸引力。然而，通常用来证明这种做法合理的Johnson-Lindenstrauss (JL)引理并不能解决在逆运算下草图的表现问题，同时也没有现有理论解释草图如何与其他广泛使用的技术（如岭正则化和结构化曲率近似）相互作用。

Method: 研究者们开发了一个统一的理论框架，用于描述当满足什么条件时投影能够可靠地保留影响函数。对于$g,g^{\prime}\in\text{range}(F)$的情况，他们展示了以下几点：1) 无正则化的投影：只有当$P$在$\text{range}(F)$上是一对一映射时才能确保精确保留，这意味着$m\geq \text{rank}(F)$是必要的；2) 正则化后的投影：岭正则化从根本上改变了草绘障碍，其逼近保证由$F$在正则化尺度下的有效维度决定；3) 因子化影响：对于Kronecker因子化的曲率$F=A\otimes E$，即使这些草图表现出违反独立同分布假设的行间相关性，解耦草图$P=P_A\otimes P_E$的保证依然成立。此外，还分析了范围外测试梯度并量化了一个由于测试梯度在$\ker(F)$中有分量而产生的‘泄漏’项。

Result: 结果表明，在特定条件下，通过投影保持影响函数是可行的，并且提出了针对不同类型投影（包括未正则化、正则化及因子化）的具体理论保障。此外，还发现了在处理超出范围的测试梯度时出现的一个‘泄漏’现象，并对其进行了量化。

Conclusion: 这项工作提出的新理论不仅阐明了投影能够在什么条件下保证影响函数的保存，而且为实践中如何选取适当的草图尺寸提供了科学依据。

Abstract: Influence functions and related data attribution scores take the form of $g^{\top}F^{-1}g^{\prime}$, where $F\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\in\mathbb{R}^{d\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \in \mathbb{R}^{m\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.
  We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\prime}\in\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\text{range}(F)$, which necessitates $m\geq \text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a \emph{leakage} term that arises when test gradients have components in $\ker(F)$. This yields guarantees for influence queries on general test points.
  Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.

</details>


### [58] [Constructing Industrial-Scale Optimization Modeling Benchmark](https://arxiv.org/abs/2602.10450)
*Zhong Li,Hongliang Lu,Tao Wei,Wenyu Liu,Yuxuan Chen,Yuan Lan,Fan Zhang,Zaiwen Wen*

Main category: cs.LG

TL;DR: 本文介绍了一个新的基准测试集MIPLIB-NL，它通过从真实混合整数线性规划问题反向构建的方法生成，旨在解决自然语言要求到优化模型转换过程中的评估难题。实验表明，在这个新基准上，现有系统的表现显著下降，揭示了之前在小规模测试中未被发现的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已被用于将自然语言需求转化为正确的优化公式和可执行代码，但当前的评估主要基于小型或合成基准，未能反映实际工业问题的复杂度。缺乏与真实优化模型相关的自然语言规范和参考公式的基准是一个关键瓶颈。

Method: 作者提出了一种结构感知的逆向构建方法，从MIPLIB 2017中的真实混合整数线性程序出发，创建了名为MIPLIB-NL的新基准。该过程包括恢复紧凑且可重用的模型结构、根据统一格式反向生成自然语言规范以及通过专家审查和人机交互进行迭代语义验证。

Result: 研究者成功构建了包含223个一对一重构实例的MIPLIB-NL，这些实例保持了原始数学内容的同时提供了现实世界的自然语言到优化转换评价可能性。实验显示，对于在现有基准测试表现良好的系统来说，在MIPLIB-NL上的性能显著降低，暴露出了玩具级别测试无法揭示的问题模式。

Conclusion: MIPLIB-NL为评估自然语言到优化建模转换提供了一个更加贴近实际应用难度的平台，有助于未来研究更好地理解并解决这一领域面临的挑战。

Abstract: Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.

</details>


### [59] [Driving Reaction Trajectories via Latent Flow Matching](https://arxiv.org/abs/2602.10476)
*Yili Shen,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的化学反应预测模型LatentRxnFlow，该模型基于条件流匹配，从标准的反应物-产物对直接学习时间依赖性的潜在动力学，无需机制注释或策划的中间标签。它不仅在USPTO基准测试中达到了最新的性能水平，而且其连续公式还能暴露整个生成轨迹，便于进行更深层次的诊断分析和不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 尽管现有最先进的化学反应预测模型在标准基准上几乎达到饱和准确度，但它们通常将任务定义为从反应物到产物的一次性映射，缺乏对底层反应过程的理解。逐步生成的方法虽然提供了另一种选择，但往往需要特定于机制的监督、离散符号编辑以及计算成本高昂的推理。

Method: 本文提出了LatentRxnFlow，一种基于条件流匹配的新反应预测范式，该方法直接从标准反应物-产物对中学习时间依赖性的潜在动态，而不需要机制注解或人工整理的中间标签。

Result: LatentRxnFlow不仅在美国专利商标局（USPTO）基准测试中达到了最先进水平的表现，而且它的连续形式还揭示了完整的生成路径，使得难以通过离散或一次性模型实现的路径级诊断成为可能。此外，所学轨迹的几何特性提供了一个内在的知识不确定性信号，有助于优先处理可靠可预测的反应结果，并标记出模棱两可的情况以供进一步验证。

Conclusion: LatentRxnFlow结合了强大的预测准确性与改进的透明度、诊断能力和不确定性意识，推动了高通量发现工作流程中反应预测向更加可信的方向发展。

Abstract: Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

</details>


### [60] [Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation](https://arxiv.org/abs/2602.10489)
*Wei Chen,Xingyu Guo,Shuang Li,Zhao Zhang,Yan Zhong,Fuzhen Zhuang,Deqing wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的自适应分布对齐框架ADAlign，用于图域适应（GDA），该方法能够自动识别并联合对齐每个迁移任务中最相关的差异，无需手动指定对齐标准。通过引入神经谱差异（NSD）作为理论基础的距离度量，ADAlign能够在多种数据集和迁移任务上超越现有技术，并且在内存使用和训练速度方面也表现出更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的图域适应方法在处理复杂的、多方面的分布偏移时存在局限性，因为它们通常依赖于特定场景的启发式方法来选定图元素进行对齐，并需要预先设计好的图滤波器来提取相关特征。这些方法不够灵活，难以应对不同迁移场景下主导差异的变化。

Method: 提出了ADAlign，一个不需要手动指定对齐条件的自适应分布对齐框架，能够自动识别每次迁移中最相关的差异并同时对齐这些差异，从而捕捉属性、结构及其依赖关系之间的相互作用。此外，还引入了神经谱差异（NSD），这是一种理论上合理的参数距离，它利用频谱域中的神经特征函数编码所有阶次的特征-结构依赖关系，同时通过可学习的频率采样器根据最小最大范式强调每个任务最具信息性的频谱分量。

Result: 在10个数据集上的16个迁移任务中进行了广泛的实验，结果表明ADAlign不仅优于最先进基线的表现，而且在内存使用率和训练速度方面也取得了效率提升。

Conclusion: ADAlign作为一种新颖的自适应分布对齐方法，在处理图域适应问题时展现出更优的性能和灵活性，能够有效应对各种动态变化的分布偏移情况。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios. To address these limitations, we propose \textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts. To enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm. Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.

</details>


### [61] [Enhancing Ride-Hailing Forecasting at DiDi with Multi-View Geospatial Representation Learning from the Web](https://arxiv.org/abs/2602.10502)
*Xixuan Hao,Guicheng Li,Daiqiang Wu,Xusen Guo,Yumeng Zhu,Zhichao Zou,Peng Zhen,Yao Yao,Yuxuan Liang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架MVGR-Net，通过两阶段方法解决了由于地理空间异质性和对外部事件的高度敏感性而导致的网约车预测难题。


<details>
  <summary>Details</summary>
Motivation: 网约车服务的激增从根本上改变了城市出行模式，准确的网约车预测对于优化乘客体验和城市交通效率至关重要。然而，由于地理空间异质性和对外部事件的高度敏感性，网约车预测面临着重大挑战。

Method: 提出了MVGR-Net（多视图地理空间表示学习），这是一种新框架，通过两阶段方法解决上述挑战：在预训练阶段，通过整合兴趣点与时间移动模式来学习全面的地理空间表征，从而捕捉从语义属性和时间移动模式两个视角下的区域特征；在预测阶段，则利用这些表征，并通过一个赋能于提示的框架微调大型语言模型，同时纳入外部事件的影响。

Result: 使用滴滴的真实世界数据集进行的大规模实验表明，该方法达到了最先进的性能。

Conclusion: MVGR-Net有效地应对了网约车预测中的关键挑战，展示了其在提高预测准确性方面的潜力。

Abstract: The proliferation of ride-hailing services has fundamentally transformed urban mobility patterns, making accurate ride-hailing forecasting crucial for optimizing passenger experience and urban transportation efficiency. However, ride-hailing forecasting faces significant challenges due to geospatial heterogeneity and high susceptibility to external events. This paper proposes MVGR-Net(Multi-View Geospatial Representation Learning), a novel framework that addresses these challenges through a two-stage approach. In the pretraining stage, we learn comprehensive geospatial representations by integrating Points-of-Interest and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. The forecasting stage leverages these representations through a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events. Extensive experiments on DiDi's real-world datasets demonstrate the state-of-the-art performance.

</details>


### [62] [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)
*Williams Jonathan,Tureci Esin*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架RLTT，用于改进Looped语言模型（LoopLMs）的推理能力。与传统的仅在最终状态分配奖励的方法不同，RLTT在整个潜在推理路径上分配奖励，从而提高了模型在数学推理基准测试中的准确性，并且即使在非数学推理任务中也表现出了良好的迁移性能。


<details>
  <summary>Details</summary>
Motivation: 循环语言模型（LoopLMs）虽然在小参数预算下表现出色，但在尝试通过强化学习进一步提升其推理能力时遇到了瓶颈。标准的目标函数如群体相对策略优化（GRPO）只对最后的潜状态分配信用，这与模型内部计算机制存在根本性不匹配。为了解决这个问题，研究者提出了一个新的强化学习框架——奖励潜思维轨迹（RLTT）。

Method: 引入了RLTT（Reward Latent Thought Trajectories），这是一种新的强化学习方法，它能够在整个潜在推理过程中分配奖励，而不仅仅是在最后一个状态。这种方法提供了密集的、基于轨迹的信用分配方式，无需依赖外部验证器，并且几乎可以无缝替换GRPO。

Result: 实验结果表明，在相同的训练和推断条件下，相比于GRPO，使用RLTT能够显著提高Ouro-2.6B-Thinking模型在具有挑战性的数学推理基准测试上的准确率，包括MATH-500 (+14.4%)、AIME24 (+16.6%)以及BeyondAIME (+10.0%)。此外，尽管仅在数学问题上进行了训练，但RLTT同样有效地迁移到了非数学推理基准测试中。

Conclusion: 本研究表明，通过采用基于轨迹的信用分配方法，如RLTT，可以在保持低额外开销的同时大幅度改善LoopLMs在复杂推理任务上的表现。此方法不仅限于数学领域内的应用，还展示了良好的跨领域迁移潜力。

Abstract: Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.

</details>


### [63] [A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction](https://arxiv.org/abs/2602.10528)
*Seongwon Jin,Hanseul Choi,Sunggu Yang,Sungho Park,Jibum Kim*

Main category: cs.LG

TL;DR: A new ECoG-based benchmark dataset and a Swap-Adversarial Framework (SAF) are introduced for predicting Parkinson's disease, overcoming high inter-subject variability and achieving strong generalization across ECoG and EEG data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a reproducible benchmark for early Parkinson's disease (PD) prediction using ECoG, addressing the limitations of human studies due to ethical constraints and the lack of open benchmark datasets. Additionally, there is a need to mitigate high inter-subject variability and the HDLSS problem in ECoG data.

Method: A new dataset was introduced as a benchmark for PD prediction using long-term ECoG recordings from 6-OHDA-induced rat models. A Swap-Adversarial Framework (SAF) was developed, which includes robust preprocessing, Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation, and domain-adversarial training to reduce subject-specific bias.

Result: The SAF consistently outperformed all baselines in cross-subject, cross-session, and cross-dataset experiments, with the most significant improvements seen in highly variable environments. It also showed superior performance when applied to public EEG benchmarks, demonstrating its strong generalization ability.

Conclusion: The proposed Swap-Adversarial Framework (SAF) effectively addresses the high inter-subject variability and HDLSS problem in ECoG data, showing consistent outperformance across all experimental settings. The method also demonstrates strong generalization capability to EEG data, indicating its potential for broader applications in BCI datasets.

Abstract: Electroencephalography (ECoG) offers a promising alternative to conventional electrocorticography (EEG) for the early prediction of Parkinson's disease (PD), providing higher spatial resolution and a broader frequency range. However, reproducible comparisons has been limited by ethical constraints in human studies and the lack of open benchmark datasets. To address this gap, we introduce a new dataset, the first reproducible benchmark for PD prediction. It is constructed from long-term ECoG recordings of 6-hydroxydopamine (6-OHDA)-induced rat models and annotated with neural responses measured before and after electrical stimulation. In addition, we propose a Swap-Adversarial Framework (SAF) that mitigates high inter-subject variability and the high-dimensional low-sample-size (HDLSS) problem in ECoG data, while achieving robust domain generalization across ECoG and EEG-based Brain-Computer Interface (BCI) datasets. The framework integrates (1) robust preprocessing, (2) Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation, and (3) domain-adversarial training to suppress subject-specific bias. ISBCS randomly swaps channels between subjects to reduce inter-subject variability, and domain-adversarial training jointly encourages the model to learn task-relevant shared features. We validated the effectiveness of the proposed method through extensive experiments under cross-subject, cross-session, and cross-dataset settings. Our method consistently outperformed all baselines across all settings, showing the most significant improvements in highly variable environments. Furthermore, the proposed method achieved superior cross-dataset performance between public EEG benchmarks, demonstrating strong generalization capability not only within ECoG but to EEG data. The new dataset and source code will be made publicly available upon publication.

</details>


### [64] [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539)
*Guozheng Ma,Lu Li,Haoyu Wang,Zixuan Liu,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出了一种名为DAWN的方法，旨在解决残差强化学习中价值学习的独特挑战。通过基础策略转换作为价值锚点和批评者规范化来有效地区分价值差异，从而提高残差RL中的价值学习效率。


<details>
  <summary>Details</summary>
Motivation: 在残差强化学习（RL）中，虽然能够在线稳定地改进预训练策略，但其价值学习面临两大瓶颈：冷启动问题（即批评者缺乏对基础策略周围价值景观的理解）和结构规模不匹配（即残差贡献相对于基础动作显得微不足道）。这些挑战阻碍了高效的价值学习过程。

Method: 研究团队首先系统性地探讨了上述瓶颈背后的机制，并发现基于基础策略转移提供了一个重要的价值锚点用于隐式热身，同时批评者标准化能有效恢复表示敏感度以区分价值差异。基于此，提出了DAWN（Data-Anchored Warmup and Normalization），一种针对残差RL中价值学习优化的简洁方法。

Result: 实验表明，在多种基准测试、策略架构及观察模式下，DAWN方法显著提高了价值学习效率。

Conclusion: DAWN为改善残差强化学习环境下的价值学习提供了一种有效途径，通过简单的原理性解决方案解决了冷启动与结构规模不匹配的问题。

Abstract: Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.

</details>


### [65] [Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy](https://arxiv.org/abs/2602.10544)
*Wuyang Zhang,Zhen Luo,Chuqiao Gu,Jianming Ma,Yebo Cao,Wangming Yuan,Yinzhi Jin*

Main category: cs.LG

TL;DR: 本文提出了一种新的混合架构，用于提高自动EEG监测的临床测量精度。通过信号处理预先提取准确的临床值，并使用跨模态桥接技术将EEG数据转换为语言描述，同时保持事件级精度。在TUH和CHB-MIT数据集上的评估显示该方法减少了60%的误报率，检测速度提高了50%，并且实现了亚临床级别的测量精度。


<details>
  <summary>Details</summary>
Motivation: 当前自动化EEG监控系统存在因极度压缩导致的时间精度丢失问题，以及大语言模型缺乏对时间序列内在理解的问题，这可能导致系统产生临床不正确的测量值。

Method: 采用了一种分离测量提取与文本生成的混合架构，先通过信号处理计算出确切的临床数值，然后利用跨模态桥梁进行EEG到语言的翻译，并结合参数高效微调及冻结槽位周围的约束解码来实现。此外，还采用了多速率采样以维持长距离上下文的同时保持事件级别精度。

Result: 在TUH和CHB-MIT数据集上进行测试的结果表明，该系统能够减少60%的误报警情况，提高50%的检测速度，并达到低于临床标准的测量精确度。

Conclusion: 这是首个能够在自动生成的EEG报告中保证临床测量准确性达到专业医生水平的系统。

Abstract: Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.
  We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.

</details>


### [66] [$μ$pscaling small models: Principled warm starts and hyperparameter transfer](https://arxiv.org/abs/2602.10545)
*Yuxin Ma,Nan Chen,Mateo Díaz,Soufiane Hayou,Dmitriy Kunisky,Soledad Villar*

Main category: cs.LG

TL;DR: 本文提出了一种通用的模型放大方法，适用于各种架构和优化器，并扩展了μTransfer理论以实现超参数转移技术，实验证明该方法在实际数据集和架构上有效。


<details>
  <summary>Details</summary>
Motivation: 现有的模型放大方法对需要调整的超参数很敏感，直接针对目标放大模型大小进行调优成本过高。本文旨在解决这一问题，提出一种更高效的方法来放大模型宽度并调整超参数。

Method: 受μP和任意维度架构启发，引入了一个广泛适用的模型放大方法，并通过理论保证放大后的模型与其加宽版本等效；同时，将μTransfer理论扩展为超参数转移技术，用于使用所提方法放大的模型。

Result: 提出的模型放大方法及其配套的超参数转移技术在现实世界的数据集和架构上表现出有效性。

Conclusion: 本研究提供了一种新的模型放大途径，能够有效减少对于超参数的手动调节需求，同时保持甚至提升模型性能。

Abstract: Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergence. However, this method can be sensitive to hyperparameters that need to be tuned at the target upscaled model size, which is prohibitively costly to do directly. It remains unclear whether the most common workaround -- tuning on smaller models and extrapolating via hyperparameter scaling laws -- is still sound when using upscaling. We address this with principled approaches to upscaling with respect to model widths and efficiently tuning hyperparameters in this setting. First, motivated by $μ$P and any-dimensional architectures, we introduce a general upscaling method applicable to a broad range of architectures and optimizers, backed by theory guaranteeing that models are equivalent to their widened versions and allowing for rigorous analysis of infinite-width limits. Second, we extend the theory of $μ$Transfer to a hyperparameter transfer technique for models upscaled using our method and empirically demonstrate that this method is effective on realistic datasets and architectures.

</details>


### [67] [Online Min-Max Optimization: From Individual Regrets to Cumulative Saddle Points](https://arxiv.org/abs/2602.10565)
*Abhijeet Vyas,Brian Bullins*

Main category: cs.LG

TL;DR: 本文提出并研究了一种基于累积鞍点的在线min-max优化方法，适用于多种性能度量，并且超越了凸-凹设定。针对静态纳什均衡与个体遗憾的不兼容性，提出了一个受在线凸优化框架启发的静态对偶间隙。文章提供了达到静态对偶间隙和动态鞍点遗憾界限的算法，这些界限在强凸-强凹和min-max指数凹性条件下推导得出。此外，还确定了一类满足min-max EC条件的函数，捕捉到了经典投资组合选择问题的双人变体。最后，在双边Polyak-Łojasiewicz (PL)条件下，得出了与个体遗憾相容的动态遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 传统的静态纳什均衡在处理强凸-强凹函数时与个体遗憾不兼容，因此需要一种新的度量标准来评估在线min-max优化的表现。

Method: 通过引入一个新的静态对偶差距（SDual-Gap$_T$）和一个动态鞍点遗憾（DSP-Reg$_T$），利用经典的在线凸优化(OCO)问题的简化，提供能够达到这些新度量界限的算法。

Result: 得到了在强凸-强凹性和min-max指数凹性(min-max EC)条件下SDual-Gap$_T$和DSP-Reg$_T$的边界，并确定了一组满足min-max EC条件、能代表经典投资组合选择问题两玩家版本的函数类别。对于与个体遗憾相容的动态遗憾定义，也在双边Polyak-Łojasiewicz (PL)条件下给出了边界。

Conclusion: 本研究为在线min-max优化提供了一个新的视角，通过引入新的度量标准和相关算法，使得即便是在非凸-非凹设置下也能有效分析和优化。

Abstract: We propose and study an online version of min-max optimization based on cumulative saddle points under a variety of performance measures beyond convex-concave settings. After first observing the incompatibility of (static) Nash equilibrium (SNE-Reg$_T$) with individual regrets even for strongly convex-strongly concave functions, we propose an alternate \emph{static} duality gap (SDual-Gap$_T$) inspired by the online convex optimization (OCO) framework. We provide algorithms that, using a reduction to classic OCO problems, achieve bounds for SDual-Gap$_T$~and a novel \emph{dynamic} saddle point regret (DSP-Reg$_T$), which we suggest naturally represents a min-max version of the dynamic regret in OCO. We derive our bounds for SDual-Gap$_T$~and DSP-Reg$_T$~under strong convexity-strong concavity and a min-max notion of exponential concavity (min-max EC), and in addition we establish a class of functions satisfying min-max EC~that captures a two-player variant of the classic portfolio selection problem. Finally, for a dynamic notion of regret compatible with individual regrets, we derive bounds under a two-sided Polyak-Łojasiewicz (PL) condition.

</details>


### [68] [Gauss-Newton Unlearning for the LLM Era](https://arxiv.org/abs/2602.10568)
*Lev McKinney,Anvith Thudi,Juhan Bae,Tara Rezaei,Nicolas Papernot,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: 本文提出了一种名为K-FADE的新方法，用于改进大型语言模型（LLM）的遗忘机制。通过使用高斯-牛顿步骤和K-FAC近似海森矩阵，该方法能够在减少遗忘集输出的同时，最小化对保留集性能的影响，并且可以低成本地维护遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 在标准的大规模语言模型训练中，可能会产生一些不可接受的输出。虽然可以通过诸如LLM遗忘等方法来降低这些输出的概率，但这种方法可能会损害模型在其他分布上的表现。为了改善这种权衡，研究者们寻求一种既能有效删除特定数据又能保持模型在其他重要任务上性能的方法。

Method: 提出了一种新的遗忘方法K-FADE，它结合了高斯-牛顿步长调整与参数化的海森矩阵逼近技术如Kronecker-Factored Approximate Curvature (K-FAC)，以达到在删除特定数据集（称为遗忘集）时尽量减少对其他所需保持行为的数据集（称为保留集）影响的目的。

Result: 实验表明，K-FADE能够有效地抑制来自遗忘集的输出，并且在输出空间上接近于不包含遗忘集重新训练的结果。更重要的是，相比之前的方法，K-FADE对保留集上的输出改变更小。此外，当模型进行进一步训练时，K-FADE计算出的更新可以被重复应用，从而以较低成本维持遗忘效果。

Conclusion: K-FADE提供了一种概念简单、最先进的LLM遗忘方法，通过将整个保留集上的输出约束转化为权重约束，实现了在移除特定数据时最小化对模型行为的影响，同时支持后续训练过程中遗忘效果的持续维护。

Abstract: Standard large language model training can create models that produce outputs their trainer deems unacceptable in deployment. The probability of these outputs can be reduced using methods such as LLM unlearning. However, unlearning a set of data (called the forget set) can degrade model performance on other distributions where the trainer wants to retain the model's behavior. To improve this trade-off, we demonstrate that using the forget set to compute only a few uphill Gauss-Newton steps provides a conceptually simple, state-of-the-art unlearning approach for LLMs. While Gauss-Newton steps adapt Newton's method to non-linear models, it is non-trivial to efficiently and accurately compute such steps for LLMs. Hence, our approach crucially relies on parametric Hessian approximations such as Kronecker-Factored Approximate Curvature (K-FAC). We call this combined approach K-FADE (K-FAC for Distribution Erasure). Our evaluation on the WMDP and ToFU benchmarks demonstrates that K-FADE suppresses outputs from the forget set and approximates, in output space, the results of retraining without the forget set. Critically, our method does this while altering the outputs on the retain set less than previous methods. This is because K-FADE transforms a constraint on the model's outputs across the entire retain set into a constraint on the model's weights, allowing the algorithm to minimally change the model's behavior on the retain set at each step. Moreover, the unlearning updates computed by K-FADE can be reapplied later if the model undergoes further training, allowing unlearning to be cheaply maintained.

</details>


### [69] [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576)
*Boxiao Wang,Kai Li,Tianyi Liu,Chen Li,Junzhe Wang,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种新的框架PiT-PO，通过强化学习将大型语言模型转变为自适应生成器，用于符号回归。该框架结合了物理有效性和细粒度的令牌级惩罚机制，以生成科学上一致且结构简洁的方程。实验证明，PiT-PO在标准基准测试中达到了最先进的性能，并能够发现解决复杂流体动力学问题的新湍流模型。


<details>
  <summary>Details</summary>
Motivation: 当前利用大型语言模型进行符号回归的方法主要将这些模型视为静态生成器，未能根据搜索反馈更新模型内部表示，导致产生的数学表达式可能不符合物理规则或存在冗余。

Method: 开发了PiT-PO框架，采用强化学习技术让大型语言模型成为可以根据反馈调整的自适应生成器。该方法引入了一个双约束机制来确保生成的方程式既符合物理层级的有效性要求，也通过对单个token施加惩罚来避免结构上的冗余。

Result: 实验结果表明，PiT-PO不仅在多个标准基准测试中表现出色，还成功地发现了新颖的湍流模型来解决复杂的流体力学难题。此外，它还能使规模较小的模型超越那些封闭源的巨大模型，促进了高性能科学发现工具的普及。

Conclusion: PiT-PO提供了一种有效的途径，通过增强学习和特定设计的约束条件，使得大型语言模型能够生成更加准确、简洁且具有物理意义的数学方程，从而推动了符号回归领域的发展。

Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

</details>


### [70] [When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning](https://arxiv.org/abs/2602.10584)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于权重谱诊断的自适应裁剪策略，用于在保护隐私的训练过程中调整裁剪阈值。该方法通过分析模型参数中的特定权重矩阵，并利用谱分解估计出与训练稳定性相关的重尾谱指标。此指标经过时间平滑后，被送入一个有界反馈控制器中，后者以对数域内乘法方式更新裁剪阈值。由于仅使用了隐私保护训练过程中产生的参数，因此不会增加额外的隐私损失。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应裁剪方法通常依赖于每个样本梯度范数统计量，这不仅增加了计算开销，还对数据集和架构敏感。为了解决这些问题，同时保持良好的训练性能和隐私保护水平，提出了新的控制驱动裁剪策略。

Method: 新方法通过定期探测步骤来分析指定权重矩阵的谱分解，进而估计与训练稳定性有关的重尾谱指示器。该指示器随时间平滑处理，并输入到一个界限反馈控制器中，后者以对数形式按比例调整裁剪阈值。重要的是，整个过程只利用了隐私保护训练期间生成的参数信息，确保了不会进一步增加隐私泄露风险。

Result: 实验结果表明，所提出的基于权重谱特性的自适应裁剪方法能够在不牺牲过多准确率的情况下有效减少噪声的影响，同时避免了因过度裁剪导致的优化偏差问题。

Conclusion: 通过引入一种轻量级且仅依赖模型参数的谱诊断机制，本研究为差分隐私下随机优化提供了一个新颖而有效的自适应裁剪方案，有助于提高敏感数据上隐私保护训练的质量。

Abstract: Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.

</details>


### [71] [TRACE: Theoretical Risk Attribution under Covariate-shift Effects](https://arxiv.org/abs/2602.10588)
*Hosein Anjidani,S. Yahya S. R. Tehrani,Mohammad Mahdi Mojahedian,Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: 本文提出了TRACE框架，用于分析在协变量偏移下模型替换导致的风险变化。通过将风险变化分解为四个可操作因素，提供了一个强大的诊断工具来理解性能变化的原因，并且能够以高AUROC/AUPRC值支持安全有效的模型替换决策。


<details>
  <summary>Details</summary>
Motivation: 当使用偏移数据训练的模型替代原模型时，在源域上的表现可能变得不可预测。为了解决这个问题，研究了协变量偏移条件下两模型间风险变化$ΔR$。

Method: 引入了TRACE（Theoretical Risk Attribution under Covariate-shift Effects）框架，该框架将$|ΔR|$分解成一个可解释的上界。此分解将风险变化拆解为四个行动因素：两个泛化差距、一个模型变化惩罚项以及一个协变量偏移惩罚项。为了使TRACE成为完全可计算的诊断工具，对每一项进行了实例化。

Result: 在理想的线性回归设定中验证了TRACE框架，显示TRACE边界正确捕捉到了真实风险差异随偏移量大小的变化。在合成和视觉基准测试中，TRACE诊断保持与实际性能下降之间强烈的单调关系。特别地，推导出一个部署门控分数，它与$|ΔR|$强相关，并且对于门控决策具有很高的AUROC/AUPRC值。

Conclusion: TRACE框架不仅提供了对为什么性能会变化的理解，还支持安全且标签高效的模型替换。

Abstract: When a source-trained model $Q$ is replaced by a model $\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $ΔR := R_P(\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|ΔR|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|ΔR|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.

</details>


### [72] [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习算法RI-FedAvg，通过在本地目标中加入基于粗糙度指数(RI)的正则项来减轻客户端漂移问题，从而在非独立同分布(non-IID)情况下提高模型训练的收敛性和准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许跨分布式客户端协作进行模型训练的同时保护数据隐私，但在非独立同分布设置下由于客户端漂移问题而面临挑战，这会损害收敛性。

Method: 提出了RI-FedAvg算法，该算法通过向局部目标函数添加基于粗糙度指数(RI)的正则化项来适应性地惩罚更新，以此来量化高维损失函数的不平滑程度，并确保在异构环境下的稳健优化。

Result: 在MNIST、CIFAR-10和CIFAR-100上的大量实验表明，与包括FedAvg、FedProx、FedDyn、SCAFFOLD和DP-FedAvg在内的最先进基线相比，RI-FedAvg在非IID场景中实现了更高的准确性和更快的收敛速度。

Conclusion: 研究结果突出了RI-FedAvg增强实际异构环境中联邦学习鲁棒性和效率的潜力。

Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.

</details>


### [73] [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)
*Arnav Shah,Junzhe Li,Parsa Idehpour,Adibvafa Fallahpour,Brandon Wang,Sukjun Hwang,Bo Wang,Patrick D. Hsu,Hani Goodarzi,Albert Gu*

Main category: cs.LG

TL;DR: 提出了一种名为dnaHNet的新型无分词器自回归模型，该模型能够端到端地分割和建模基因组序列。通过采用可微动态切块机制，dnaHNet可以在保持预测准确性的同时压缩原始核苷酸为潜在token。预训练结果显示，dnaHNet在处理原核生物基因组时优于其他架构，并且在零样本任务中也表现出色，包括预测蛋白质变体适应性和基因必需性，同时还能自动发现层次化的生物结构。


<details>
  <summary>Details</summary>
Motivation: 当前的基因组基础模型在输入表示上面临一个基本权衡：标准固定词汇表分词器会破坏生物学上有意义的基序（如密码子和调控元件）的完整性，而基于核苷酸级别的模型虽然保持了生物学一致性但对长上下文而言计算成本过高。

Method: 开发了一个名为dnaHNet的无分词器自回归模型，它利用一种可微动态切块机制来将原始核苷酸压缩成潜在token，从而在压缩比与预测准确性之间找到平衡点。

Result: dnaHNet在处理原核生物基因组数据时超越了包括StripedHyena2在内的领先架构，在扩展性和效率方面表现更优；此外，相比Transformer，它实现了超过3倍的推理加速。对于零样本任务，dnaHNet不仅在预测蛋白质变体适应性和基因必需性方面展示了优越性能，还能够在没有监督的情况下自动发现层级式的生物结构。

Conclusion: 研究结果表明，dnaHNet作为一种可扩展且可解释性强的新一代基因组建模框架，为理解DNA语法提供了新的途径。

Abstract: Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.

</details>


### [74] [Pupillometry and Brain Dynamics for Cognitive Load in Working Memory](https://arxiv.org/abs/2602.10614)
*Nusaibah Farrukh,Malavika Pradeep,Akshay Sasi,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: 本研究通过比较EEG和瞳孔测量法在认知负荷分类中的应用，发现基于特征的方法（如Catch-22特征与经典机器学习模型）优于深度学习方法。特别是，瞳孔测量法单独使用时能够与EEG竞争，成为现实世界应用中可携带且实用的认知负荷指标。


<details>
  <summary>Details</summary>
Motivation: 准确评估认知负荷对于自适应学习、临床监测及脑机接口至关重要。尽管生理信号如瞳孔测量法和脑电图是公认的认知负荷生物标志物，但它们作为轻量级、可穿戴监控解决方案的相对效用和实际整合仍待进一步探索。此外，现有研究往往依赖于解释性有限且计算成本高昂的深度学习模型。

Method: 研究采用了基于特征与模型驱动相结合的方法来改进时间序列分析，并利用OpenNeuro 'Digit Span Task'数据集对来自EEG和瞳孔测量法的认知负荷进行分类。实验对比了使用Catch-22特征的经典机器学习模型与深度学习模型在二分类和多分类任务上的表现。

Result: 基于特征的方法（采用Catch-22特征及传统机器学习模型）在二分类和多分类任务上均优于深度学习方法。此外，研究结果表明，单凭瞳孔测量法就能与EEG相媲美，成为一个便携且实用的认知负荷替代指标。

Conclusion: 这项工作支持开发可穿戴且经济实惠的认知监控系统用于神经精神病学、教育和医疗保健领域。瞳孔动力学结合可解释模型和SHAP特征分析提供了具有生理意义的见解，挑战了只有EEG才能有效检测认知负荷的传统假设。

Abstract: Cognitive load, the mental effort required during working memory, is central to neuroscience, psychology, and human-computer interaction. Accurate assessment is vital for adaptive learning, clinical monitoring, and brain-computer interfaces. Physiological signals such as pupillometry and electroencephalography are established biomarkers of cognitive load, but their comparative utility and practical integration as lightweight, wearable monitoring solutions remain underexplored. EEG provides high temporal resolution of neural activity. Although non-invasive, it is technologically demanding and limited in wearability and cost due to its resource-intensive nature, whereas pupillometry is non-invasive, portable, and scalable. Existing studies often rely on deep learning models with limited interpretability and substantial computational expense. This study integrates feature-based and model-driven approaches to advance time-series analysis. Using the OpenNeuro 'Digit Span Task' dataset, this study investigates cognitive load classification from EEG and pupillometry. Feature-based approaches using Catch-22 features and classical machine learning models outperform deep learning in both binary and multiclass tasks. The findings demonstrate that pupillometry alone can compete with EEG, serving as a portable and practical proxy for real-world applications. These results challenge the assumption that EEG is necessary for load detection, showing that pupil dynamics combined with interpretable models and SHAP based feature analysis provide physiologically meaningful insights. This work supports the development of wearable, affordable cognitive monitoring systems for neuropsychiatry, education, and healthcare.

</details>


### [75] [Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631)
*Rustam Zhumagambetov,Niklas Giesa,Sebastian D. Boie,Stefan Haufe*

Main category: cs.LG

TL;DR: 研究了生成式时间序列模型在医疗数据合成中的隐私保护问题，通过一系列隐私攻击测试发现，当合成数据生成器使用足够大的训练数据集时，现有的隐私攻击对其无效。同时讨论了使用差分隐私机制可能不会带来隐私上的改进，反而会降低机器学习预测任务的实用性。


<details>
  <summary>Details</summary>
Motivation: 由于分享医疗数据用于机器学习模型训练存在泄露个人患者信息的风险，而生成的人工智能（genAI）模型虽然被视为一种可能遵守隐私法规的解决方案，但并不能保证隐私保护。此外，将已建立的隐私机制应用于生成的时间序列模型中具有挑战性。因此，对于生成的时间序列模型进行隐私审计变得不可或缺。

Method: 采用了多种已有的隐私攻击方法来审计基于MIMIC-IV公开数据集训练的状态下最先进的医院时间序列模型，并用eICU数据集对基于MIMIC-IV数据集训练的合成数据生成器发起了隐私攻击。

Result: 结果显示，针对生成的多变量临床时间序列，当合成数据生成器使用足够大的训练数据集时，现有隐私攻击是无效的。而且，为这些合成数据生成器使用现有的差分隐私机制并不会带来所期望的隐私改进，只会减少机器学习预测任务的实用性。

Conclusion: 研究强调了即使应用了具体的隐私机制，对生成式时间序列模型执行隐私审计的重要性。它还指出，当训练数据集足够大时，传统的隐私攻击方式可能不足以威胁到合成数据的安全性；同时，增加差分隐私可能会牺牲数据的实用性而不显著提高隐私保护水平。

Abstract: Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.

</details>


### [76] [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637)
*Weilong Chen,Bojun Zhao,Jan Eckwert,Julija Zavadlav*

Main category: cs.LG

TL;DR: 本文提出了一种粗粒度玻尔兹曼生成器（CG-BGs），它结合了可扩展的降阶建模与重要性采样的精确性，通过在粗粒度坐标空间中使用基于流模型生成的样本，并利用学习到的平均力势进行重加权来实现。这种方法能够有效地从快速收敛的数据中通过力匹配学习平均力势，并且结果表明CG-BGs能够在高度简化的表示中准确捕捉复杂的溶剂介导相互作用，为更大分子系统的无偏采样提供了可扩展途径。


<details>
  <summary>Details</summary>
Motivation: 传统玻尔兹曼生成器虽然能够很好地解决从玻尔兹曼分布中采样平衡分子构型这一长期挑战，但其实际应用中的可扩展性有限。而粗粒度替代模型虽能通过减少有效维度来模拟更大的系统，却常常缺乏必要的重加权过程以保证统计学上的正确性。

Method: 提出了Coarse-Grained Boltzmann Generators (CG-BGs)，这是一种将可扩展的降阶建模与重要性采样的精确性相结合的原则框架。CG-BGs工作在一个粗粒度坐标空间内，利用一个基于流模型生成样本，并通过学习得到的平均力势(PMF)对这些样本进行重加权。关键之处在于展示了这种PMF可以通过力匹配从迅速收敛的数据中高效学习获得。

Result: 结果显示，CG-BGs能够忠实地在高度简化的表示中捕捉由显式溶剂介导的复杂相互作用，为较大分子系统的无偏采样提供了一个可扩展的方法。

Conclusion: 该研究成功地开发出一种新型方法——粗粒度玻尔兹曼生成器(CG-BGs)，它不仅克服了传统玻尔兹曼生成器和粗粒度模型各自的局限性，还为大规模分子系统的研究开辟了新的道路。

Abstract: Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.

</details>


### [77] [Evaluation metrics for temporal preservation in synthetic longitudinal patient data](https://arxiv.org/abs/2602.10643)
*Katariina Perkonoja,Parisa Movahedi,Antti Airola,Kari Auranen,Joni Virta*

Main category: cs.LG

TL;DR: 本研究提出了一组评估合成纵向患者数据中时间保存的指标，强调了单个指标不足以全面捕捉时间保存情况，需要多维度评价来更全面地评估合成数据的质量。


<details>
  <summary>Details</summary>
Motivation: 为了更好地评估合成纵向患者数据在时间特征上的重现能力，以及理解哪些因素影响了这些特征的准确再现。

Method: 定义了一系列针对边际、协方差、个体水平和测量结构的时间特征度量，并通过这些度量分析了原始数据质量、测量频率及预处理策略等因素对时间保存的影响。

Result: 发现即使在边际层面有很强的相似性也可能掩盖协方差中的扭曲和个人轨迹中的中断；同时指出，没有单一指标能够充分捕捉时间保存情况，必须进行全面的特性评估才能得到可靠结论。

Conclusion: 提出的指标有助于清晰地了解时间结构是如何被保留或退化的，从而可以更可靠地评估和改进生成模型，支持创建更加真实的时间序列合成纵向患者数据。

Abstract: This study introduces a set of metrics for evaluating temporal preservation in synthetic longitudinal patient data, defined as artificially generated data that mimic real patients' repeated measurements over time. The proposed metrics assess how synthetic data reproduces key temporal characteristics, categorized into marginal, covariance, individual-level and measurement structures. We show that strong marginal-level resemblance may conceal distortions in covariance and disruptions in individual-level trajectories. Temporal preservation is influenced by factors such as original data quality, measurement frequency, and preprocessing strategies, including binning, variable encoding and precision. Variables with sparse or highly irregular measurement times provide limited information for learning temporal dependencies, resulting in reduced resemblance between the synthetic and original data. No single metric adequately captures temporal preservation; instead, a multidimensional evaluation across all characteristics provides a more comprehensive assessment of synthetic data quality. Overall, the proposed metrics clarify how and why temporal structures are preserved or degraded, enabling more reliable evaluation and improvement of generative models and supporting the creation of temporally realistic synthetic longitudinal patient data.

</details>


### [78] [Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments](https://arxiv.org/abs/2602.10670)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Daniel Ratner,Apurva Mehta*

Main category: cs.LG

TL;DR: 本文提出了一种基于领域知识的贝叶斯优化方法，通过坐标变换简化了高维、参数紧密耦合且目标景观高度不对称问题中的搜索问题。这种方法在12维6晶体分割延迟光学系统中表现出色，结合反向退火探索策略能够可靠地收敛到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法在处理高维问题时表现不佳，特别是在面对参数紧密关联和目标函数高度非对称的情况下。为解决这一难题，作者引入了物理洞察力来改进现有技术。

Method: 通过利用物理洞见进行坐标转换，将输入特征解耦，并使活动子空间与主要搜索轴对齐，从而简化了搜索问题。此外，还采用了逆退火探索策略以提高寻找全局最优解的能力。

Result: 实验表明，在一个复杂的12维6晶体分割-延迟光学系统上，该方法比标准贝叶斯优化、TurBO及多目标贝叶斯优化等常规方法更有效，能够快速准确地找到全局最优解。

Conclusion: 本研究表明，通过运用物理知识转化复杂高维优化问题为简单表示形式，可以实现快速稳健的自动化调优，这为未来复杂科学仪器的发展提供了新的解决方案。

Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach's efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem's active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.

</details>


### [79] [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693)
*Guobin Shen,Chenxiao Zhao,Xiang Cheng,Lei Huang,Xing Yu*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法VESPO，用于解决大型语言模型在强化学习训练中的稳定性问题。通过将方差减少纳入提案分布的变分公式中，VESPO能够直接对序列级重要性权重进行操作而无需长度归一化。实验表明，VESPO在过时率高达64倍和完全异步执行的情况下仍能保持稳定的训练，并且对于密集型和专家混合模型都表现出一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在针对大型语言模型（LLMs）的强化学习（RL）过程中，训练稳定性仍然是一个核心挑战。策略陈旧、异步训练以及训练与推理引擎之间的不匹配导致行为策略偏离当前策略，从而威胁到训练过程的稳定性。虽然重要性采样提供了一种原则性的纠正手段来应对这种分布偏移，但它存在高方差的问题；现有的解决方案如令牌级别裁剪和序列级别标准化缺乏统一的理论基础。

Method: 研究者们提出了变分序列级软策略优化（VESPO）。该方法通过在提案分布的变分公式中整合方差减少技术，推导出一种封闭形式的重塑内核，可以直接作用于序列级别的权重上，而不需要进行长度上的调整。

Result: 实验结果表明，在数学推理基准测试中，即使是在陈旧率达到64倍及完全异步运行条件下，VESPO也能维持稳定训练，并且无论是在密集型还是混合专家模型中都能带来一致的表现提升。

Conclusion: VESPO为处理由于策略陈旧等因素导致的大规模语言模型强化学习训练不稳定问题提供了一个有效的方法。它不仅解决了现有方法中方差高的问题，而且能够在多种模型架构下实现稳定的训练表现。

Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO

</details>


### [80] [Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes](https://arxiv.org/abs/2602.10708)
*Qiuran Zhao,Kai Ming Ting,Xinpeng Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于原型的图级别异常检测方法(ProtoGLAD)，该方法通过与最近的正常原型图对比来解释每个被检测出的异常。它使用点集核迭代地从数据集中发现多个正常的原型图及其相关联的簇，然后将远离所有已发现正常簇的图识别为异常。实验表明，ProtoGLAD在实现与最先进方法相当的异常检测性能的同时，提供了更易于人类理解的基于原型的解释。


<details>
  <summary>Details</summary>
Motivation: 当前深度图级别异常检测（GLAD）方法虽然表现良好，但其黑盒性质限制了它们在实际应用中的可靠性和部署。现有的一些尝试提供异常检测结果解释的方法要么不参考正常图，要么依赖于抽象潜在向量作为原型而非数据集中的具体图。

Method: 提出了Prototype-based Graph-Level Anomaly Detection (ProtoGLAD)框架，利用点集核技术从数据集中迭代地发现多个正常原型图及相应群集，并通过与最近的正常原型图明确对比来识别并解释异常。

Result: 通过在多个真实世界数据集上的广泛实验表明，ProtoGLAD不仅达到了与最先进的GLAD方法相媲美的异常检测性能，而且还提供了更好理解性的基于原型的解释。

Conclusion: ProtoGLAD作为一种可解释的无监督框架，在提高图级别异常检测准确性的同时，也增强了模型决策过程的透明度和可解释性，从而有利于其在现实世界中的应用。

Abstract: The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

</details>


### [81] [SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining](https://arxiv.org/abs/2602.10718)
*Yifan Zhang,Zunhai Su,Shuhao Hu,Rui Yang,Wei Wu,Yulei Qian,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出了一种名为SnapMLA的FP8 MLA解码框架，通过硬件感知的算法-内核协同优化技术解决了在DeepSeek多头潜注意力架构中集成FP8注意时遇到的挑战。实验表明，该方法在长上下文任务中实现了高达1.91倍的吞吐量提升，同时保持了性能不下降。


<details>
  <summary>Details</summary>
Motivation: 在将FP8注意力集成到DeepSeek多头潜注意力（MLA）架构的解码阶段时遇到了数值异质性、量化尺度错位等挑战。为了解决这些问题并提高长上下文处理效率，作者提出了SnapMLA框架。

Method: SnapMLA采用了以下几种硬件感知的算法-内核协同优化技术：(i) RoPE意识的每令牌KV量化，其中RoPE部分保持高精度，并且采用每令牌粒度来匹配自回归解码过程；(ii) 量化后的PV计算管道重建，解决了由MLA KV缓存共享结构引起的FP8 PV计算中的量化尺度错位问题；(iii) 端到端数据流优化，通过专门化的内核建立了高效的数据读写工作流程。

Result: 广泛的实验证明，在最先进的MLA大型语言模型上，SnapMLA能够实现高达1.91倍的吞吐量提升，在数学推理和代码生成基准测试等具有挑战性的长上下文任务中几乎没有性能下降的风险。

Conclusion: SnapMLA成功地克服了在MLA架构下应用FP8注意机制时面临的多个障碍，通过引入一系列创新措施显著提高了长上下文场景下的执行效率与稳定性。

Abstract: While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.

</details>


### [82] [Rising Multi-Armed Bandits with Known Horizons](https://arxiv.org/abs/2602.10727)
*Seockbean Song,Chenyu Gan,Youngsik Yoon,Siwei Wang,Wei Chen,Jungseul Ok*

Main category: cs.LG

TL;DR: 本文提出了一种新的CURE-UCB算法，该算法明确地将时间范围纳入考量，以解决在预期奖励随使用次数增加的场景下的多臂老虎机问题。通过严格的分析和广泛的实验验证了该方法的有效性，并证明了它在特定结构环境中的表现优于不考虑时间范围的策略。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决一类特殊的多臂老虎机问题——上升多臂老虎机（RMAB），其中每个选项的性能会随着重复使用而提高。与传统设置不同的是，在RMAB中最佳策略的选择高度依赖于可用预算T。因此，了解T对于最大化效用至关重要。然而，目前对这种具有时间意识设定的研究还较少。

Method: 为了解决上述挑战，作者们提出了名为CUmulative Reward Estimation UCB (CURE-UCB)的新方法。该方法特别设计用于直接整合时间范围信息到决策过程中去。通过对线性然后平稳等结构化环境进行分析，研究团队建立了新的后悔上界，并从理论上证明了CURE-UCB相比那些不考虑时间因素的方法具有更优的表现。

Result: 实验结果表明，在多种基准测试条件下，CURE-UCB相对于其他方法展现出了显著的优势。这些发现不仅强化了理论分析的有效性，也展示了新方法在实际应用中的潜力。

Conclusion: 综上所述，本研究成功地开发并验证了一种针对RMAB问题的新解决方案CURE-UCB。通过将时间范围作为关键因素纳入考虑，CURE-UCB能够在特定类型的环境中提供比现有技术更好的性能。

Abstract: The Rising Multi-Armed Bandit (RMAB) framework models environments where expected rewards of arms increase with plays, which models practical scenarios where performance of each option improves with the repeated usage, such as in robotics and hyperparameter tuning. For instance, in hyperparameter tuning, the validation accuracy of a model configuration (arm) typically increases with each training epoch. A defining characteristic of RMAB is em horizon-dependent optimality: unlike standard settings, the optimal strategy here shifts dramatically depending on the available budget $T$. This implies that knowledge of $T$ yields significantly greater utility in RMAB, empowering the learner to align its decision-making with this shifting optimality. However, the horizon-aware setting remains underexplored. To address this, we propose a novel CUmulative Reward Estimation UCB (CURE-UCB) that explicitly integrates the horizon. We provide a rigorous analysis establishing a new regret upper bound and prove that our method strictly outperforms horizon-agnostic strategies in structured environments like ``linear-then-flat'' instances. Extensive experiments demonstrate its significant superiority over baselines.

</details>


### [83] [Collaborative Threshold Watermarking](https://arxiv.org/abs/2602.10765)
*Tameem Bakr,Anish Ambreth,Nils Lukas*

Main category: cs.LG

TL;DR: 本文提出了一种$(t,K)$-门限水印技术，允许客户在联合训练模型时协作嵌入一个共享水印，只有至少$t$个客户的联盟才能重建水印密钥并验证可疑模型。该方法在大规模($K=128$)下保持了水印的可检测性，并且在遭受包括使用最多20%训练数据进行自适应微调等攻击时仍能保持高于检测阈值($z\ge 4$)的表现。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，由于每个参与者都投入了数据和计算资源，因此需要一种机制来证明联合训练模型的来源归属。传统的模型水印方法要么无法随着参与客户端数量增加而扩展，导致个人水印被稀释；要么赋予任何单个客户端验证甚至移除水印的能力。为了解决这些问题，提出了$(t, K)$-门限水印方案。

Method: 通过秘密共享水印密钥$\tau$的方式实现$(t, K)$-门限水印，确保少于$t$个客户的联盟无法重建密钥，同时支持不直接暴露$\tau$的情况下完成验证过程。该方案适用于白盒环境，并针对图像分类任务进行了实例化与评估。

Result: 所提出的水印方法能够在大规模（$K=128$）情况下维持水印的有效性，仅带来极小的准确度损失，并且即使面对利用高达训练数据20%的自适应微调等攻击手段，也能保持高于检测阈值($z \ge 4$)的良好表现。

Conclusion: 本研究展示了一种新的模型水印策略——$(t, K)$-门限水印，它不仅解决了传统水印方法存在的问题，还提供了更安全、更具扩展性的解决方案，适用于拥有大量参与者的联邦学习场景。

Abstract: In federated learning (FL), $K$ clients jointly train a model without sharing raw data. Because each participant invests data and compute, clients need mechanisms to later prove the provenance of a jointly trained model. Model watermarking embeds a hidden signal in the weights, but naive approaches either do not scale with many clients as per-client watermarks dilute as $K$ grows, or give any individual client the ability to verify and potentially remove the watermark. We introduce $(t,K)$-threshold watermarking: clients collaboratively embed a shared watermark during training, while only coalitions of at least $t$ clients can reconstruct the watermark key and verify a suspect model. We secret-share the watermark key $τ$ so that coalitions of fewer than $t$ clients cannot reconstruct it, and verification can be performed without revealing $τ$ in the clear. We instantiate our protocol in the white-box setting and evaluate on image classification. Our watermark remains detectable at scale ($K=128$) with minimal accuracy loss and stays above the detection threshold ($z\ge 4$) under attacks including adaptive fine-tuning using up to 20% of the training data.

</details>


### [84] [Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization](https://arxiv.org/abs/2602.10794)
*Benjy Friedmann,Nadav Dym*

Main category: cs.LG

TL;DR: 本文提出了CycFlow框架，通过确定性的点传输替代迭代的边去噪，加速了解决TSP问题的速度，同时保持了与现有最佳扩散基线相当的最优性差距。


<details>
  <summary>Details</summary>
Motivation: 最近神经组合优化（NCO）的进步主要依赖于将欧几里得旅行商问题（TSP）视为一个随机$N \times N$热图生成任务的扩散模型。这些方法通常采用迭代边缘去噪的方式，存在二次瓶颈限制。为了克服这一局限，作者提出了一种新的方法来处理这个问题。

Method: CycFlow学习了一个实例条件下的向量场，该向量场连续地将输入的2D坐标转移到规范的圆形排列中，在这个$2N$维表示中，通过角度排序恢复出最优路径。这种方法利用了数据依赖流匹配技术，从而避免了边缘评分的二次瓶颈，转而支持线性坐标动态。

Result: 相比最先进的扩散基线，这种范式转变使得解决问题的速度提高了多达三个数量级，同时维持了有竞争力的最优性差距。

Conclusion: CycFlow为解决TSP问题提供了一种有效的新途径，不仅大大加快了解决速度，而且在解的质量上也表现良好。

Abstract: Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement, where the optimal tour is recovered from this $2N$ dimensional representation via angular sorting. By leveraging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics. This paradigm shift accelerates solving speed by up to three orders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competitive optimality gaps.

</details>


### [85] [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819)
*Linxuan Xia,Xiaolong Yang,Yongyuan Chen,Enyue Zhao,Deng Cai,Yasheng Wang,Boxi Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Rephrasing Policy Optimization (RePO)的方法，旨在解决大型语言模型在特定领域数据上对齐时遇到的挑战。RePO通过让策略模型先理解离线知识再将其转换为符合自身风格和参数分布的轨迹来提高硬样本利用率，并保持在线训练动态稳定性。实验表明，RePO在多个基准测试中表现出色，超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLMs）在特定领域数据上的对齐问题，现有的监督微调（SFT）虽然能注入领域知识但会降低模型的一般性；而在线强化学习（RL）虽能保持一般性却难以有效吸收超出当前推理水平的难例。最近尝试利用离线强化学习来改进难例的使用效率，但由于强制向离线知识分布转移而导致训练极不稳定。因此，本研究旨在开发一种既能有效吸收离线知识又能保持在线RL稳定性的新方法。

Method: 提出了Rephrasing Policy Optimization (RePO) 方法。该方法首先促使策略模型去理解离线知识，然后将这些知识重新表述成与自身风格和参数分布相符的行为序列。接着，RePO动态地用这些高质量、经过重新表述的行为序列替换掉低回报的行为路径。这种方式不仅引导模型走向正确的推理路线，还严格维持了在线强化学习的训练动态特性。

Result: 实验结果表明，RePO显著提高了难例的利用效率，并且在几个标准评测任务上均超过了现有的基线方法，达到了最先进的性能水平。

Conclusion: RePO作为一种新的强化学习优化技术，成功解决了在保留在线训练稳定性的同时高效吸收离线知识的问题，为大型语言模型在特定领域的应用提供了更优解决方案。

Abstract: Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [86] [Adaptive Sampling for Private Worst-Case Group Optimization](https://arxiv.org/abs/2602.10820)
*Max Cairney-Leeming,Amartya Sanyal,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文提出了一种新的差分隐私最坏情况组优化算法ASC，通过自适应控制每个组的采样率和剪切阈值，确保所有组之间的一致隐私保证，同时提高了最坏情况组准确性，且不牺牲整体平均准确性。


<details>
  <summary>Details</summary>
Motivation: 针对训练模型时对小数据集或难以学习的数据群准确度低的问题，现有的加权目标优化方法在使用差分隐私学习时会导致不均匀的隐私保护，特别是对于少数群体。

Method: 提出了ASC（自适应采样和剪切的最坏情况组优化）算法，该算法能够自适应地控制各组的采样率和剪切阈值，使得更难学习的组可以被更频繁地采样，同时也保证了所有组间一致性的隐私保障。

Result: 与先前的工作相比，ASC算法实现了更低方差的梯度、更严格的隐私保证以及显著提高的最坏情况下组别准确度，而不会牺牲总体平均准确度。

Conclusion: ASC算法提供了一个有效解决差分隐私下最坏情况组优化问题的方法，在增强隐私保护的同时改善了模型对于困难数据组的表现。

Abstract: Models trained by minimizing the average loss often fail to be accurate on small or hard-to-learn groups of the data. Various methods address this issue by optimizing a weighted objective that focuses on the worst-performing groups. However, this approach becomes problematic when learning with differential privacy, as unequal data weighting can result in inhomogeneous privacy guarantees, in particular weaker privacy for minority groups. In this work, we introduce a new algorithm for differentially private worst-case group optimization called ASC (Adaptively Sampled and Clipped Worst-case Group Optimization). It adaptively controls both the sampling rate and the clipping threshold of each group. Thereby, it allows for harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups. Comparing ASC to prior work, we show that it results in lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy.

</details>


### [87] [SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios](https://arxiv.org/abs/2602.10840)
*Yanan Wang,Renxi Wang,Yongxin Wang,Xuezhi Liang,Fajri Koto,Timothy Baldwin,Xiaodan Liang,Haonan Li*

Main category: cs.LG

TL;DR: 本研究首次系统地探讨了大型语言模型（LLMs）在五个物理领域和52个物理概念上模拟物理场景的能力。通过自动收集并人工验证的数据集，对10种当前的LLMs进行了评估，并发现即使是表现最好的模型也仅达到了21.5%的通过率。为此，研究人员提出了一种结合视觉奖励的强化学习方法来训练文本模型，实验证明这种方法不仅提高了物理模拟的准确性，还显著增强了代码生成的整体性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已在数学竞赛、复杂编程和科学推理等任务中得到广泛研究，但它们准确表示并通过代码模拟物理场景的能力仍然未被充分探索。这项工作旨在填补这一空白，通过系统的训练与评估揭示LLMs在此领域的潜能与挑战。

Method: 构建了一个自动化流程来收集数据，并通过人工审核确保质量。最终形成了包含7,659个物理场景的数据集，其中334个人工验证的例子作为测试集。使用该数据集对10个当代大型语言模型进行了评估。此外，引入了一种利用视觉奖励的强化学习流水线，采用视觉-语言模型作为裁判来训练文本模型。

Result: 评估结果显示，即使是最强大的模型也只能达到21.5%的通过率，表明了任务难度之高。然而，通过使用所提出的方法进行训练，可以显著提高模型在物理模拟方面的表现以及整体代码生成功能。

Conclusion: SimuScene项目展示了大型语言模型在精确模拟物理现象方面存在的挑战，同时也指出了一条可能的改进路径——即通过结合视觉反馈的强化学习策略来增强模型能力。

Abstract: Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>


### [88] [Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval](https://arxiv.org/abs/2602.10847)
*Fanpu Cao,Lu Dai,Jindong Han,Hui Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种名为全局时间检索器（GTR）的轻量级插件模块，用于增强多变量时间序列预测中模型对长周期模式的学习能力。通过联合建模局部和全局依赖关系，GTR在不改变宿主模型结构的前提下提高了短期与长期预测的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多变量时间序列预测模型受限于有限的历史上下文，难以捕捉到跨越输入范围之外的全局周期性模式。简单地延长历史窗口会导致过拟合、计算成本高昂以及冗余信息处理等问题。

Method: 引入了全局时间检索器（GTR），一个轻量且即插即用的模块，它能够维护整个周期的自适应全局时间嵌入，并动态检索与输入序列相关的全球段落。通过2D卷积和残差融合共同建模局部和全局依赖性，从而连接短期观察与长期周期性。

Result: 在六个真实世界数据集上的广泛实验表明，GTR在短时和长时预测场景中均能持续提供最先进性能，同时参数和计算开销极小。

Conclusion: GTR被证明是一种有效且通用的方法，用于提高多变量时间序列预测任务中的全局周期性建模。

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles significantly longer than the input horizon - despite such patterns carrying strong predictive signals. Naive solutions, such as extending the historical window, lead to severe drawbacks, including overfitting, prohibitive computational costs, and redundant information processing. To address these challenges, we introduce the Global Temporal Retriever (GTR), a lightweight and plug-and-play module designed to extend any forecasting model's temporal awareness beyond the immediate historical context. GTR maintains an adaptive global temporal embedding of the entire cycle and dynamically retrieves and aligns relevant global segments with the input sequence. By jointly modeling local and global dependencies through a 2D convolution and residual fusion, GTR effectively bridges short-term observations with long-term periodicity without altering the host model architecture. Extensive experiments on six real-world datasets demonstrate that GTR consistently delivers state-of-the-art performance across both short-term and long-term forecasting scenarios, while incurring minimal parameter and computational overhead. These results highlight GTR as an efficient and general solution for enhancing global periodicity modeling in MTSF tasks. Code is available at this repository: https://github.com/macovaseas/GTR.

</details>


### [89] [Time Series Foundation Models for Energy Load Forecasting on Consumer Hardware: A Multi-Dimensional Zero-Shot Benchmark](https://arxiv.org/abs/2602.10848)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 本研究通过使用2020年至2024年的ERCOT每小时负载数据，对四种时间序列基础模型（Chronos-Bolt、Chronos-2、Moirai-2和TinyTimeMixer）与行业标准基线Prophet及两种统计参考（SARIMA和季节性朴素法）进行了多维度基准评估。实验在消费级硬件上进行，并从上下文长度敏感性、概率预测校准、分布变化下的鲁棒性以及操作决策支持的规范分析四个方面进行了评价。结果表明，顶级基础模型在长上下文长度下实现了接近0.31的MASE值，比季节性朴素法基线减少了47%。此外，预训练模型即使在上下文极短的情况下也能保持稳定准确性。


<details>
  <summary>Details</summary>
Motivation: 这项工作旨在探讨时间序列基础模型（TSFMs）是否能够将零样本预测能力应用于电力需求预测等关键任务中，这些任务对于准确性、校准度和鲁棒性的要求直接影响到电网运营。

Method: 采用了一种多维基准测试方法来评估几种时间序列基础模型（包括Chronos-Bolt, Chronos-2, Moirai-2, 和 TinyTimeMixer）的表现，同时与行业标准的Prophet模型及两个统计参考模型（SARIMA和Seasonal Naive）进行了对比。所有实验均在普通消费者级别的硬件上运行，没有使用GPU加速。评估涵盖了四个维度：(1) 从24到2048小时不同上下文长度的影响；(2) 概率预测的校准程度；(3) 在如COVID-19封锁期和冬季风暴Uri期间的数据分布变化情况下的鲁棒性；(4) 对于运营决策支持的规范分析能力。

Result: 最佳表现的基础模型在较长的上下文长度(C=2048小时, 日前预测)下达到了约0.31的MASE值，相较于季节性朴素法基线降低了47%。当拟合窗口短于其季节周期时，Prophet模型表现不佳(MASE>74在24小时上下文)，而TSFMs即使在最小上下文条件下也保持了稳定的准确性。关于校准方面，Chronos-2生成了良好的预测区间(名义水平为90%时实际覆盖率为95%)，而Moirai-2和Prophet则显示出过度自信的现象(大约70%的覆盖率)。

Conclusion: 研究表明，时间序列基础模型（TSFMs）在电力需求预测等关键应用场景中展现出优于传统方法的性能，特别是在长上下文长度下的准确性和鲁棒性方面。预训练模型能够识别出预先学习的时间模式，而不是从头开始估计，这使得它们即使在有限的历史数据下也能维持较高的预测精度。

Abstract: Time Series Foundation Models (TSFMs) have introduced zero-shot prediction capabilities that bypass the need for task-specific training. Whether these capabilities translate to mission-critical applications such as electricity demand forecasting--where accuracy, calibration, and robustness directly affect grid operations--remains an open question. We present a multi-dimensional benchmark evaluating four TSFMs (Chronos-Bolt, Chronos-2, Moirai-2, and TinyTimeMixer) alongside Prophet as an industry-standard baseline and two statistical references (SARIMA and Seasonal Naive), using ERCOT hourly load data from 2020 to 2024. All experiments run on consumer-grade hardware (AMD Ryzen 7, 16GB RAM, no GPU). The evaluation spans four axes: (1) context length sensitivity from 24 to 2048 hours, (2) probabilistic forecast calibration, (3) robustness under distribution shifts including COVID-19 lockdowns and Winter Storm Uri, and (4) prescriptive analytics for operational decision support. The top-performing foundation models achieve MASE values near 0.31 at long context lengths (C = 2048h, day-ahead horizon), a 47% reduction over the Seasonal Naive baseline. The inclusion of Prophet exposes a structural advantage of pre-trained models: Prophet fails when the fitting window is shorter than its seasonality period (MASE > 74 at 24-hour context), while TSFMs maintain stable accuracy even with minimal context because they recognise temporal patterns learned during pre-training rather than estimating them from scratch. Calibration varies substantially across models--Chronos-2 produces well-calibrated prediction intervals (95% empirical coverage at 90% nominal level) while both Moirai-2 and Prophet exhibit overconfidence (~70% coverage). We provide practical model selection guidelines and release the complete benchmark framework for reproducibility.

</details>


### [90] [The Sample Complexity of Uniform Approximation for Multi-Dimensional CDFs and Fixed-Price Mechanisms](https://arxiv.org/abs/2602.10868)
*Matteo Castiglioni,Anna Lunghi,Alberto Marchesi*

Main category: cs.LG

TL;DR: 本文研究了在仅有一比特反馈的情况下，学习n维累积分布函数(CDF)的样本复杂度，并得到了一个几乎与维度无关的样本复杂度上界。此外，还提供了学习小型市场中固定价格机制的紧致样本复杂度界限和新颖的遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 作者旨在探讨当观测值被限制为最小的一比特反馈时，学习n维累积分布函数（CDF）达到一定误差ε所需的样本数量。这扩展了多变量DKW不等式到'bandit feedback'场景下。

Method: 通过数学分析方法，作者证明了即使是在一比特反馈这种信息非常有限的情况下，也能以近似于维度不变的样本复杂度获得对n维CDF的均匀ε-逼近。

Result: 主要结果表明，在任意细网格上，用样本复杂度为1/ε^3 * log(1/ε)^O(n)可以获得对n维CDF的均匀ε-逼近，其中维度n只影响对数项。

Conclusion: 研究表明，即使在信息受限的情况下，也能够有效地学习高维CDF，并且这一发现直接应用于提供学习小市场中固定价格机制的样本复杂度边界及新的遗憾担保。

Abstract: We study the sample complexity of learning a uniform approximation of an $n$-dimensional cumulative distribution function (CDF) within an error $ε> 0$, when observations are restricted to a minimal one-bit feedback. This serves as a counterpart to the multivariate DKW inequality under ''full feedback'', extending it to the setting of ''bandit feedback''. Our main result shows a near-dimensional-invariance in the sample complexity: we get a uniform $ε$-approximation with a sample complexity $\frac{1}{ε^3}{\log\left(\frac 1 ε\right)^{\mathcal{O}(n)}}$ over a arbitrary fine grid, where the dimensionality $n$ only affects logarithmic terms. As direct corollaries, we provide tight sample complexity bounds and novel regret guarantees for learning fixed-price mechanisms in small markets, such as bilateral trade settings.

</details>


### [91] [FedPS: Federated data Preprocessing via aggregated Statistics](https://arxiv.org/abs/2602.10870)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedPS的联邦数据预处理框架，该框架基于聚合统计信息，并使用数据草图技术有效总结本地数据集，同时保持必要的统计信息。FedPS为实际联邦学习部署提供了灵活、通信高效且一致的数据预处理流程。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许多方在不共享原始数据的情况下协作训练机器学习模型。然而，在训练之前，必须对数据进行预处理以解决缺失值、格式不一致和特征尺度异质性等问题。尽管预处理阶段对于模型性能至关重要，但在FL研究中却很大程度上被忽视了。此外，隐私限制阻止了原始数据的集中化，而通信效率又给分布式预处理带来了进一步的挑战。

Method: 提出了FedPS，一个基于聚合统计数据的联邦数据预处理统一框架。FedPS利用数据概要技术有效地概括本地数据集，同时保留关键统计信息。基于这些摘要，设计了用于特征缩放、编码、离散化和缺失值插补的联邦算法，并将k-Means、k-最邻近以及贝叶斯线性回归等与预处理相关的模型扩展到水平和垂直FL场景。

Result: FedPS提供了一种灵活、通信高效并且一致的预处理管道，适用于现实世界中的联邦学习部署。通过这种方法，即使在存在隐私约束和需要考虑通信效率的情况下，也能实现有效的数据预处理。

Conclusion: FedPS解决了联邦学习场景下数据预处理所面临的独特挑战，包括隐私保护需求以及跨不同参与者之间的一致性问题。它为开发实用的联邦学习系统奠定了坚实的基础。

Abstract: Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.

</details>


### [92] [Resource-Efficient Model-Free Reinforcement Learning for Board Games](https://arxiv.org/abs/2602.10894)
*Kazuki Ota,Takayuki Osa,Motoki Omura,Tatsuya Harada*

Main category: cs.LG

TL;DR: 研究提出了一种针对棋盘游戏的无模型强化学习算法，旨在实现更高效的学习。通过在五种不同的棋盘游戏中进行实验，结果表明该方法比现有方法学习效率更高，并且消融研究表明了所使用的核心技术的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于搜索的强化学习方法如AlphaZero已经在人工智能领域取得了显著的成功，但它们巨大的计算需求被认为是阻碍其可重复性的因素。因此，研究者们提出了一个更加高效的、专为棋盘游戏设计的无模型强化学习算法。

Method: 研究中提出了一种新的无模型强化学习算法，并在五款不同的棋盘游戏（动物将棋、加德纳国际象棋、围棋、六角棋以及黑白棋）上进行了全面测试以验证其有效性。此外，还进行了广泛的消融研究来展示所采用核心技术的价值。

Result: 实验结果显示，在所有测试过的环境中，所提出的算法相较于现有的方法能够达到更高的学习效率。同时，消融研究也证明了本研究中应用的核心技术对于提升性能具有重要意义。

Conclusion: 这项工作展示了无模型强化学习在传统上由基于搜索的方法主导的领域中的潜力，提供了一个新的视角来看待如何提高这类问题解决策略的效率。

Abstract: Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.

</details>


### [93] [Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins](https://arxiv.org/abs/2602.10917)
*Qian Zuo,Zhiyong Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 提出了一种名为FlexDOME的新算法，能够在保证次线性强遗憾的同时，实现近常数级的强约束违反，并确保非渐近最终迭代收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的主对偶方法在实现次线性强奖励遗憾时，不可避免地会导致增长的强约束违反或仅限于平均迭代收敛，这是由于内在的波动性。为了解决这些限制，研究提出了新的方法。

Method: 引入了FlexDOME算法，该算法将时间变化的安全边际和正则化项纳入主对偶框架中。通过一种新颖的逐项渐近优势策略来进行理论分析，其中安全边际被严格安排以渐近主导优化和统计错误的功能衰减速率，从而将近乎恒定水平的累积违规行为控制住。此外，还通过政策-双重Lyapunov论证建立了非渐近最终迭代收敛保证。

Result: 实验结果支持了理论发现，表明FlexDOME算法能够有效达成设计目标。

Conclusion: FlexDOME是首个证明可以同时实现近常数级强约束违反、次线性强遗憾以及非渐近最终迭代收敛的算法。

Abstract: We study safe online reinforcement learning in Constrained Markov Decision Processes (CMDPs) under strong regret and violation metrics, which forbid error cancellation over time. Existing primal-dual methods that achieve sublinear strong reward regret inevitably incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations. To address these limitations, we propose the Flexible safety Domain Optimization via Margin-regularized Exploration (FlexDOME) algorithm, the first to provably achieve near-constant $\tilde{O}(1)$ strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence. FlexDOME incorporates time-varying safety margins and regularization terms into the primal-dual framework. Our theoretical analysis relies on a novel term-wise asymptotic dominance strategy, where the safety margin is rigorously scheduled to asymptotically majorize the functional decay rates of the optimization and statistical errors, thereby clamping cumulative violations to a near-constant level. Furthermore, we establish non-asymptotic last-iterate convergence guarantees via a policy-dual Lyapunov argument. Experiments corroborate our theoretical findings.

</details>


### [94] [Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink](https://arxiv.org/abs/2602.10956)
*Victoria Hankemeier,Malte Hankemeier*

Main category: cs.LG

TL;DR: 本文研究了时间注意力机制中是否存在由于过压缩导致的偏差，通过推导时间注意力层Jacobian期望值的敏感度界限来分析，并提出了正则化方法以缓解对角线注意力沉陷问题。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，在因果注意力或时间卷积中的过压缩会造成对首个token的偏见。基于此，文章旨在探讨这种偏见是否同样存在于时间注意力机制中，特别是探究非对角线注意力分数如何依赖序列长度以及时间注意力矩阵所遭受的对角线注意力沉陷现象。

Method: 通过对时间注意力层Jacobian期望值设定敏感度边界进行理论分析，揭示非对角线注意力评分与序列长度之间的关系；提出几种正则化方法用以减轻对角线注意力过度集中的问题。

Result: 研究表明，时间注意力机制确实存在一种随着序列长度增加而加剧的对角线注意力倾向（即对角线注意力沉陷），并且通过引入特定的正则化技术能够有效缓解这一现象。

Conclusion: 时间注意力机制容易受到序列长度影响而导致对角线注意力过度集中，但通过适当的设计和应用正则化手段可以改善模型性能。

Abstract: Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.

</details>


### [95] [Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers](https://arxiv.org/abs/2602.10959)
*Feilong Liu*

Main category: cs.LG

TL;DR: 本文通过将旋转位置嵌入（RoPE）重新解释为应用于复振荡器库的相位调制，利用经典信号处理理论对其进行分析。研究得出了关于RoPE基参数的下限，这对于在目标上下文长度内保持位置一致性是必要的，并且还推导了与浮点分辨率相关的上限。这些界限定义了一个精度和深度依赖的可行性区域，适用于长上下文变换器。案例研究表明，最先进的模型的成功、失败以及社区改造都与预测的边界紧密相关。


<details>
  <summary>Details</summary>
Motivation: 旋转位置嵌入（RoPE）广泛用于大型语言模型中，通过乘法旋转来编码令牌位置，但其在长上下文长度下的行为尚未得到充分表征。本研究旨在通过将其重新解释为一种相位调制方式，从而使用经典信号处理理论对RoPE进行分析，以更好地理解其在长序列中的表现。

Method: 通过对RoPE进行重新解释，作为应用于一组复数振荡器上的相位调制技术，使得能够借助经典信号处理理论来进行分析。基于这种形式化方法，研究者们推导出了维持给定上下文长度内位置一致性的RoPE基本参数的理论下界，包括一个类似于奈奎斯特极限的基本混叠界线以及一个限制低频位置模式相位漂移的直流分量稳定性界线。此外，该分析被进一步扩展到了深层变压器上，表明跨层重复旋转调制会累积角度错位，随着深度增加而提高基础要求。

Result: 研究结果包括了对于维持给定上下文长度内位置一致性的RoPE基本参数的理论下界，以及由有限浮点分辨率引起的上限。两者共同定义了一个取决于精度和深度的可行性区域——适合长上下文变换器的“适中地带”。通过综合案例研究验证了这一框架，展示了当前领先模型如LLaMA、Mistral及DeepSeek变体的表现、失效情况以及社区改进措施均与预测边界高度吻合。

Conclusion: 综上所述，本文提供了一种新的视角来理解和分析RoPE机制，特别是在长上下文场景下的行为。通过引入基于信号处理理论的方法，确定了确保RoPE有效性的参数范围，同时揭示了超出此范围时可能出现的问题。研究成果不仅加深了我们对现有模型局限性的理解，也为未来设计更高效的语言模型提供了有价值的指导。

Abstract: Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as phase modulation applied to a bank of complex oscillators, enabling analysis through classical signal processing theory.
  Under this formulation, we derive principled lower bounds on the RoPE base parameter that are necessary to preserve positional coherence over a target context length. These include a fundamental aliasing bound, analogous to a Nyquist limit, and a DC-component stability bound that constrains phase drift in low-frequency positional modes. We further extend this analysis to deep transformers, showing that repeated rotary modulation across layers compounds angular misalignment, tightening the base requirement as depth increases.
  Complementing these results, we derive a precision-dependent upper bound on the RoPE base arising from finite floating-point resolution. Beyond this limit, incremental phase updates become numerically indistinguishable, leading to positional erasure even in the absence of aliasing. Together, the lower and upper bounds define a precision- and depth-dependent feasibility region a Goldilocks zone for long-context transformers.
  We validate the framework through a comprehensive case study of state-of-the-art models, including LLaMA, Mistral, and DeepSeek variants, showing that observed successes, failures, and community retrofits align closely with the predicted bounds. Notably, models that violate the stability bound exhibit attention collapse and long-range degradation, while attempts to scale beyond one million tokens encounter a hard precision wall independent of architecture or training.

</details>


### [96] [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)
*Yupu Gu,Rongzhe Wei,Andy Zhu,Pan Li*

Main category: cs.LG

TL;DR: 本文提出了MoEEdit，这是一种专为稀疏Mixture-of-Experts（MoE）模型设计的知识编辑框架，通过每专家零空间投影重新参数化专家更新，从而保持路由输入不变并抑制路由偏移。实验表明，MoEEdit在保持高特异性和路由稳定性的同时，实现了最先进的效果和泛化能力，并且具有优秀的计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有的知识编辑方法主要针对密集架构设计，难以适用于日益普及的稀疏Mixture-of-Experts (MoE) 模型，后者是现代可扩展大型语言模型的基础。直接将密集模型编辑器应用于MoE不仅计算成本高昂，还容易导致路由分布变化，影响系统的稳定性和一致性。

Method: 研究者们提出了一种名为MoEEdit的新方法，它通过每专家零空间投影来重新参数化专家更新，以此保持路由器输入不变性，进而抑制路由变化。此外，采用块坐标下降(BCD)求解器有效解决了由此产生的块结构优化问题。

Result: 实验证明，MoEEdit在保持高特异性和路由稳定性的同时，达到了最先进水平的有效性和泛化能力，同时表现出色的计算与内存效率。

Conclusion: 该研究为稀疏大型语言模型中的可扩展、精确知识编辑奠定了坚实基础，并强调了路由稳定干预的重要性。

Abstract: Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.

</details>


### [97] [A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions](https://arxiv.org/abs/2602.10971)
*Sanghwa Kim,Junghyun Lee,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文提出了一种名为HCW-GLB-OMD的算法，用于解决异方差广义线性bandits（GLBs）在对抗性干扰下的问题。该算法结合了基于在线镜像下降(OMD)的估计器和基于Hessian矩阵的信心权重来实现抗干扰能力。在链接函数满足自协调假设的情况下，展示了其遗憾界，并且证明了该算法在不同实例中同时实现了实例级最小最大最优性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决在存在对抗性干扰情况下的异方差广义线性bandits问题，这包括了多种随机上下文bandit设置，如异方差线性bandits以及逻辑/泊松bandits等。

Method: 提出的方法名为HCW-GLB-OMD，它由两部分组成：一是基于在线镜像下降(OMD)的估计器；二是基于Hessian矩阵的信心权重机制，以增强对干扰的鲁棒性。此外，该方法每轮迭代的空间与时间复杂度仅为O(1)，表现出较高的计算效率。

Result: 在链接函数满足自协调性的条件下，作者为所提算法提供了一个遗憾上界，并给出了一个下界结果，从而统一了先前特定于问题的下界结论。

Conclusion: 通过引入HCW-GLB-OMD算法，在面对异方差性和对抗性干扰时，能够在各种异方差广义线性bandits实例中同时达到实例级最小最大最优性，仅在干扰项上有κ因子差距。

Abstract: We consider the problem of heteroskedastic generalized linear bandits (GLBs) with adversarial corruptions, which subsumes various stochastic contextual bandit settings, including heteroskedastic linear bandits and logistic/Poisson bandits. We propose HCW-GLB-OMD, which consists of two components: an online mirror descent (OMD)-based estimator and Hessian-based confidence weights to achieve corruption robustness. This is computationally efficient in that it only requires ${O}(1)$ space and time complexity per iteration. Under the self-concordance assumption on the link function, we show a regret bound of $\tilde{O}\left( d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d^2 g_{\max} κ+ d κC \right)$, where $\dotμ_{t,\star}$ is the slope of $μ$ around the optimal arm at time $t$, $g(τ_t)$'s are potentially exogenously time-varying dispersions (e.g., $g(τ_t) = σ_t^2$ for heteroskedastic linear bandits, $g(τ_t) = 1$ for Bernoulli and Poisson), $g_{\max} = \max_{t \in [T]} g(τ_t)$ is the maximum dispersion, and $C \geq 0$ is the total corruption budget of the adversary. We complement this with a lower bound of $\tildeΩ(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d C)$, unifying previous problem-specific lower bounds. Thus, our algorithm achieves, up to a $κ$-factor in the corruption term, instance-wise minimax optimality simultaneously across various instances of heteroskedastic GLBs with adversarial corruptions.

</details>


### [98] [ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression](https://arxiv.org/abs/2602.11008)
*Ammar Ali,Baher Mohammad,Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: ROCKET, a novel training-free model compression method, optimizes layer-wise compression through a multi-choice knapsack approach and employs a single-step sparse matrix factorization to minimize reconstruction error. It achieves state-of-the-art performance, retaining over 90% of the original model's accuracy at 30% compression, and even higher with light fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind ROCKET is to develop a more effective and efficient model compression technique that can significantly reduce the size of deep learning models while maintaining their performance. This is important for deploying large models in resource-constrained environments.

Method: ROCKET utilizes a two-pronged approach: (1) it treats the allocation of compression across layers as a multi-choice knapsack problem to minimize total reconstruction error under a global compression budget, and (2) it applies a single-step sparse matrix factorization, which sparsifies weight coefficients based on activation-weights sensitivity and updates the dictionary via least squares, avoiding iterative optimization or backpropagation.

Result: ROCKET outperforms other compression methods, achieving over 90% of the original model's performance at 30% compression. With additional light fine-tuning, such as compressing Qwen3-14B to an 8B-parameter model, ROCKET can nearly match the performance of the original smaller model after fine-tuning with just 30 million tokens.

Conclusion: ROCKET demonstrates superior performance in model compression, making it possible to deploy highly compressed models without significant loss in accuracy. The method is particularly advantageous when combined with a light fine-tuning phase, as it can restore most of the lost performance, making it suitable for a wide range of applications where model size is a critical constraint.

Abstract: We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.

</details>


### [99] [Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064)
*Firas Darwish,George Nicholson,Aiden Doherty,Hang Yuan*

Main category: cs.LG

TL;DR: 该研究探讨了在全身人体运动领域中，当真实数据稀缺时，使用合成数据进行预训练对基于可穿戴设备的人体活动识别（HAR）的影响。结果显示，结合真实数据或足够规模的合成数据预训练可以提高模型的泛化能力；然而，由于动作捕捉与可穿戴信号之间存在域不匹配问题，大规模动作捕捉预训练仅带来微小收益。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的数据稀缺情况下，合成数据为可扩展预训练提供了一条有吸引力的道路。但是，在实际部署环境中，基于合成数据预训练的模型往往难以可靠地迁移。这项工作旨在研究这一现象，并特别关注于基于可穿戴设备的人类活动识别（HAR），其中大规模的数据收集是不可行但至关重要的。

Method: 研究者们使用从动作捕捉衍生出的表示生成的合成数据来预训练动作时间序列模型，并评估这些模型在不同下游HAR任务上的迁移性能。

Result: 结果表明，当合成数据与真实数据混合使用或者在足够大的规模下使用时，能够改善模型的泛化能力。同时指出，由于与可穿戴信号之间的域不匹配问题，通过大规模动作捕捉进行预训练仅能获得边际效益。

Conclusion: 尽管存在一些局限性，如因域不匹配而导致的大规模动作捕捉预训练效益有限，但本研究表明，正确运用合成数据预训练方法对于提升HAR表示的可转移性仍然具有潜力和机会。

Abstract: Synthetic data offers a compelling path to scalable pretraining when real-world data is scarce, but models pretrained on synthetic data often fail to transfer reliably to deployment settings. We study this problem in full-body human motion, where large-scale data collection is infeasible but essential for wearable-based Human Activity Recognition (HAR), and where synthetic motion can be generated from motion-capture-derived representations. We pretrain motion time-series models using such synthetic data and evaluate their transfer across diverse downstream HAR tasks. Our results show that synthetic pretraining improves generalisation when mixed with real data or scaled sufficiently. We also demonstrate that large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals, clarifying key sim-to-real challenges and the limits and opportunities of synthetic motion data for transferable HAR representations.

</details>


### [100] [In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079)
*Frank Xiao,Santiago Aranguri*

Main category: cs.LG

TL;DR: 该论文提出了一种基于激活的数据归因方法，能够追踪预训练语言模型行为变化到负责的训练数据点。通过计算测试提示和偏好对的激活差向量并按余弦相似性排序来识别导致特定行为的数据点，并通过重新训练修改后的数据来因果验证这些归因。将此方法应用于OLMo 2的生产DPO训练时，发现了一个有害行为：当附加上无害格式指令时，模型会遵从危险请求。过滤排名靠前的数据点可以减少63%的这种行为，而改变它们的标签则可达到78%的效果。此方法在成本上比基于梯度的归因和大模型评判基线要便宜超过10倍。


<details>
  <summary>Details</summary>
Motivation: 研究者们想要开发一种方法来追踪预训练语言模型的行为变化至具体的训练数据点，以便更好地理解及控制模型出现的潜在有害行为。

Method: 采用了基于激活的数据归因方法，通过比较测试案例与偏好对之间的激活差异向量，并依据余弦相似度进行排序来定位关键训练数据点；同时通过对修改后数据集再训练以验证所找数据点对于特定模型行为的影响。

Result: 成功地识别出引起特定行为（如干扰触发顺从）的数据点，并且通过移除或调整这些数据点有效减少了模型的有害行为。相比其他基准方法，本方法在经济性和有效性方面都表现出色。

Conclusion: 提出的基于激活的数据归因技术为理解和改进大型语言模型的安全性提供了强有力的支持，特别是在处理意外出现而非故意注入的不良行为方面。

Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

</details>


### [101] [Token-Efficient Change Detection in LLM APIs](https://arxiv.org/abs/2602.11083)
*Timothée Chauvin,Clément Lalanne,Erwan Le Merrer,Jean-Michel Loubes,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出了一种名为Black-Box Border Input Tracking (B3IT)的方案，该方案仅依赖于模型输出令牌来检测大型语言模型中的远程变化，而无需访问模型权重或对数概率。通过利用边界输入，B3IT能够在严格黑盒环境下运作，并且相比现有方法成本降低了30倍。


<details>
  <summary>Details</summary>
Motivation: 当前远程变化检测方法要么部署成本过高，要么需要初始白盒访问模型权重或是灰盒访问对数概率，因此研究旨在实现低成本和严格的黑盒操作，仅观察输出令牌。

Method: 引入了称为边界输入（Border Inputs）的概念，这些是具有多个可能最高输出令牌的特定输入。基于统计学观点，最优的变化检测依赖于模型的雅可比矩阵和输出分布的费舍尔信息量。在低温条件下分析这些量表明边界输入能够启用强大的变化检测测试。基于这一见解提出了Black-Box Border Input Tracking (B3IT) 方案。

Result: 广泛的实验表明，对于非推理测试端点很容易找到边界输入，并且性能与最佳可用灰盒方法相当。B3IT相较于现有方法减少了30倍的成本，同时保持在严格的黑盒设置下运行。

Conclusion: B3IT方案提供了一种新的、成本效益高的方法来进行大规模LLM的远程变化检测，无需访问内部模型参数或结构。

Abstract: Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.
  Our approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token. From a statistical perspective, optimal change detection depends on the model's Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.
  Building on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by $30\times$ compared to existing methods, while operating in a strict black-box setting.

</details>


### [102] [GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084)
*Yuheng Luo,Shuyan Li,Zhong Cao*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架GRASP，它结合了基于Shapley值的归因与组$L_{21}$正则化，用于提取紧凑且非冗余的特征集。实验结果表明，GRASP在保持或超越现有方法预测准确度的同时，能够识别出更少、更不冗余且更稳定的特征。


<details>
  <summary>Details</summary>
Motivation: 当前医学预测领域中的特征选择方法（如LASSO）通常缺乏鲁棒性和可解释性。为解决这一问题，并提高模型的选择稳定性和特征解释能力，作者提出了新方法。

Method: 提出了名为GRASP的新框架，该框架首先通过SHAP从预训练树模型中提炼出组级别的重要性评分，然后通过组$L_{21}$正则化的逻辑回归强制执行结构稀疏性，从而产生稳定且可解释的特征选择。

Result: 与LASSO、SHAP及基于深度学习的方法相比，GRASP不仅能够持续提供相当甚至更好的预测准确性，而且还能识别出数量更少、冗余度更低且稳定性更高的特征。

Conclusion: GRASP作为一种创新性的特征选择方法，在医学预测应用中展示了其优越性，特别是在增强模型的解释力和稳定性方面表现突出。

Abstract: Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model via SHAP, then enforces structured sparsity through group $L_{21}$ regularized logistic regression, yielding stable and interpretable selections. Extensive comparisons with LASSO, SHAP, and deep learning based methods show that GRASP consistently delivers comparable or superior predictive accuracy, while identifying fewer, less redundant, and more stable features.

</details>


### [103] [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087)
*Jianxun Wang,Grant C. Forbes,Leonardo Villalobos-Arias,David L. Roberts*

Main category: cs.LG

TL;DR: 本文提出了一种基于$f$-散度的灵活函数公式，用于根据离线训练数据集自适应地调整算法学习目标上的约束条件。实验结果表明，该方法在处理具有挑战性的数据集时能够提高性能。


<details>
  <summary>Details</summary>
Motivation: 离线RL算法旨在改进生成收集数据的行为策略，同时限制所学策略处于数据集的支持范围内。然而，实际的离线数据集往往多样性不足或对环境探索有限，并且可能源自具有不同专业水平的多个行为策略。这种情况下，需要在RL目标和行为策略约束之间找到平衡点。

Method: 首先识别了$f$-散度与贝尔曼残差优化约束之间的联系，通过更一般的线性规划形式以及凸共轭来实现。接着提出了一个通用的灵活函数公式，用于将自适应约束纳入算法的学习目标中。

Result: MuJoCo、Fetch及AdroitHand环境中的实验结果显示了所提出的LP形式的正确性以及灵活$f$-散度在改善从困难数据集中学习表现方面的潜力。

Conclusion: 通过引入灵活的$f$-散度方法，可以更好地处理包含少量多样性和有限探索的实际离线数据集问题，从而提高离线强化学习算法的表现。

Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

</details>


### [104] [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)
*Cassandre Notton,Benjamin Stott,Philippe Schoeb,Anthony Walsh,Grégoire Leboucher,Vincent Espitalier,Vassilis Apostolou,Louis-Félix Vigneux,Alexia Salavrakos,Jean Senellart*

Main category: cs.LG

TL;DR: MerLin是一个开源框架，用于光子和混合量子机器学习的探索。它集成了线性光学电路的优化强模拟到PyTorch和scikit-learn工作流中，支持量子层的端到端可微训练，并围绕系统基准测试和可重复性设计。


<details>
  <summary>Details</summary>
Motivation: 为了在近期量子机器学习（QML）中识别出量子模型可能提供实际好处的地方，需要超越孤立的算法提案，转向跨模型、数据集和硬件约束的系统性和经验性探索。

Method: 开发了MerLin这一开源框架，它将线性光学电路的优化强模拟集成到标准PyTorch和scikit-learn工作流程中，实现了量子层的端到端可微训练。MerLin的设计注重系统性基准测试与再现性，且已实现硬件感知功能，允许在现有量子硬件上进行测试的同时探索超出当前能力的可能性。

Result: MerLin重现了18项最先进的光子和混合QML工作，涵盖了核方法、水库计算、卷积和循环架构、生成模型以及现代训练范式。这些重现作为可重用模块化实验发布，可以直接扩展和适应，建立了一个与现代人工智能广泛采用的经验基准方法一致的共享实验基线。

Conclusion: 通过将光子量子模型嵌入既定的机器学习生态系统，MerLin使从业者能够利用现有工具进行消融研究、跨模态比较和混合经典量子工作流程。MerLin被定位为一个面向未来的协同设计工具，连接算法、基准测试和硬件。

Abstract: Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.

</details>


### [105] [The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126)
*Stephanie Holly,Alexandru-Ciprian Zăvoianu,Siegfried Silber,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: 该研究探讨了生成方法在离线多目标优化（MOO）中的表现，尤其是在不同度量标准下的性能。结果表明，与进化算法相比，生成方法如扩散模型在其他度量如代际距离上表现较差。这归因于离线数据集与Pareto前沿之间的偏移现象，即离线前沿位移。为克服这一限制，提出了需要在目标空间中进行分布外采样的观点，并观察到生成方法倾向于保守地接近离线目标分布。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解生成方法在离线多目标优化（MOO）领域内的表现局限性，特别是它们在除了超体积之外的其他已建立的MOO度量标准下的行为。鉴于此，本研究旨在揭示为什么这些方法在特定情况下不如进化替代方案有效，并探索如何克服这种局限性。

Method: 通过分析和实验对比了生成方法与进化算法在处理离线多目标优化问题时的表现差异，特别关注了除超体积外的度量指标如代际距离。研究还探讨了通过引入目标空间中的分布外采样来解决由离线前沿位移引起的问题的可能性。

Result: 发现生成方法在面对离线前沿位移时，相较于进化算法表现出显著劣势，尤其是在代际距离等度量上。此外，实验证明，生成方法往往过于保守，不愿偏离给定的离线目标分布太远。

Conclusion: 离线多目标优化可以被视为一个受限于分布偏移的问题。为了改善生成方法在此类任务中的表现，未来的研究可能需要考虑如何有效地实现目标空间中的分布外采样。

Abstract: Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.

</details>


### [106] [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128)
*Reinhard Heckel,Mahdi Soltanolkotabi,Christos Thramboulidis*

Main category: cs.LG

TL;DR: 本文研究了在强化学习中，特别是从零开始的RL（如R1-Zero）中，对低成功率或甚至零成功率的提示给予更高权重的方法。这种方法能加速达到目标准确率的过程，特别是在初始成功概率较低的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的策略优化算法主要关注于处理那些具有中间成功概率的模糊提示，而降低了对于非常容易和非常难的提示的关注。本文试图通过赋予低成功率提示更高的权重来改进这一状况，尤其是在从头开始训练的强化学习场景下。

Method: 提出了一个不对称加权方案，该方案对低成功率或甚至零成功率的提示分配更高的权重。此外，还提供了理论支持，说明了哪些提示权重可以最小化在固定更新预算下将成功率从初始水平提高到目标准确率所需的时间。

Result: 发现不对称加权特别有利于从零开始的RL，在这种情况下，训练过程会经历广泛的准确性范围；而对于已经处于高准确性的后SFT RL来说效果则不那么明显。在低成功率区间内，最佳权重变得不对称，通过增加低成功率提示的权重加快了有效时间收敛速度。

Conclusion: 本研究表明，通过采用针对低成功率提示的不对称加权方法，可以在一定程度上加速模型达到目标准确率的过程，特别是在从零开始训练的RL情境中。

Abstract: Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

</details>


### [107] [From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers](https://arxiv.org/abs/2602.11130)
*Maximilian Plattner,Fabian Paischer,Johannes Brandstetter,Arturs Berzins*

Main category: cs.LG

TL;DR: 该论文发现3D扩散变换器在处理稀疏点云表面完成任务时，会因为输入点云上的微小扰动而产生输出断裂的现象（称为Meltdown）。通过机制可解释性技术定位到了问题所在，并提出了PowerRemap方法来稳定稀疏点云条件，在测试中达到了高达98.3%的稳定率。


<details>
  <summary>Details</summary>
Motivation: 研究者们注意到，尽管3D扩散变换器在此类任务上取得了最先进的成果，但它们对输入数据中的微小变化非常敏感，这可能导致生成的表面出现断裂。这种现象不仅影响了模型的可靠性，也限制了其在实际场景中的应用。

Method: 采用来自机制可解释性的激活修补技术，研究人员能够将Meltdown现象追踪到特定的早期去噪交叉注意力激活。进一步地，他们开发了一个名为PowerRemap的技术，它可以在测试阶段控制并显著提高模型面对稀疏点云时的表现稳定性。

Result: 研究结果显示，PowerRemap能够有效防止由于输入点云微小变化导致的表面断裂问题，对于不同架构、数据集及去噪策略都表现出良好的适应性和高效性，特别是在某些情况下实现了高达98.3%的问题解决率。

Conclusion: 这项工作展示了如何基于机制分析来理解和指导扩散模型的行为，揭示了电路级别的交叉注意机制与扩散动力学轨迹分叉之间的联系。此外，提出的方法为增强3D扩散模型在处理稀疏点云时的鲁棒性提供了一种新的途径。

Abstract: Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.

</details>


### [108] [Just on Time: Token-Level Early Stopping for Diffusion Language Models](https://arxiv.org/abs/2602.11133)
*Zahar Kohut,Severyn Shykula,Dmytro Khamula,Mykola Vysotskyi,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: 该论文提出了一种无需训练的、基于标记级别的提前停止方法，通过利用模型预测和局部上下文的轻量级信号来动态决定何时可以确定单个标记。这种方法在不牺牲生成质量的前提下，显著减少了所需的扩散步骤总数，在数学推理、一般问答和科学理解等多方面基准测试中实现了最先进的效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在生成文本时，通常需要经历多次迭代优化过程，这往往导致计算效率低下，因为许多标记在最终去噪步骤之前就已经稳定了。为了提高效率，同时保持生成文本的质量，提出了本方法。

Method: 提出的方法是一种不需要额外训练的、基于标记级别的提前停止策略，能够独立地识别每个位置上的收敛情况。它利用从模型预测及局部上下文中提取的轻量级信号来动态判断哪些单独的标记可以被固定下来，从而实现自适应的逐标记冻结，而无需针对特定任务进行微调。

Result: 实验表明，所提方法能够在多个基准测试上（包括数学推理、一般问答以及科学理解等领域）大幅减少总体所需的扩散步骤数量，并且在维持生成质量的同时获得了当前最优的效率改进。

Conclusion: 本文介绍了一种新的无训练需求、基于标记层面的提前终止策略，能有效识别并冻结已收敛的标记，从而大幅度提高了扩散语言模型的生成效率，同时保证了生成结果的质量。

Abstract: Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.

</details>


### [109] [Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137)
*Tessa Han,Sebastian Bordt,Hanlin Zhang,Sham Kakade*

Main category: cs.LG

TL;DR: 该研究从模型可塑性的角度探讨了大型语言模型的预训练过程，特别是权重衰减这一正则化参数的作用。实验表明，使用较大权重衰减值训练的模型在微调下游任务时表现出更高的性能提升。此外，权重衰减有助于形成线性可分表示、规范注意力矩阵，并减少对训练数据的过拟合。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型开发的主要模式是先预训练基础模型，然后通过进一步训练来提高性能和模型行为。然而，超参数优化和规模法则主要从基础模型验证损失的角度进行研究，忽略了下游适应性。本工作旨在从模型可塑性的视角出发，即基础模型通过微调成功适应下游任务的能力，来研究预训练过程。

Method: 通过系统性实验，重点考察了预训练过程中关键正则化参数——权重衰减的作用。

Result: 研究表明，以较大的权重衰减值训练的模型更具有可塑性，这意味着它们在针对下游任务进行微调后显示出更大的性能提升。这种现象可能导致一些反直觉的权衡，即预训练后表现较差的基础模型，在微调之后反而能够表现得更好。

Conclusion: 这项工作展示了除了交叉熵损失之外，采用更多评估指标进行超参数优化的重要性，并揭示了一个单独优化超参数在塑造模型行为方面所起的多方面作用。

Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.

</details>


### [110] [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)
*Jingang Qu,David Holzmüller,Gaël Varoquaux,Marine Le Morvan*

Main category: cs.LG

TL;DR: 本文介绍了TabICLv2，一种新的表格基础模型，在回归和分类任务中表现出色。它基于三个关键支柱：新颖的数据生成引擎、架构创新以及优化的预训练协议。在基准测试中，未经调优的TabICLv2超越了当前最先进的模型，并且在大规模数据集上具有良好的泛化能力和更快的速度。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过引入一种新的表格基础模型（TabICLv2），以提高对表格数据进行预测时的表现，特别是针对回归和分类问题。该研究希望通过一系列创新方法解决现有模型面临的挑战，如数据多样性不足、处理大规模数据集的能力有限等问题。

Method: TabICLv2的设计基于三大支柱：首先，开发了一种用于增加预训练多样性的新型合成数据生成引擎；其次，引入了多项架构上的改进，包括一个新的可扩展softmax注意力机制，以改善模型对更大规模数据集的泛化能力；最后，优化了预训练流程，特别地将AdamW优化器替换为Muon优化器。

Result: 实验结果表明，在TabArena和TALENT基准测试中，未经任何调整的TabICLv2已经超过了当前最先进模型RealTabPFN-2.5的表现。此外，TabICLv2能够在使用相对较少计算资源的情况下有效地泛化到百万级规模的数据集，并且比RealTabPFN-2.5快得多。

Conclusion: 研究表明，通过结合创新的数据生成技术、架构改进及优化的预训练策略，可以显著提升表格基础模型在各种任务上的性能。TabICLv2不仅在多个基准测试中取得了领先成绩，而且展示了对于大规模实际应用场景的强大适应性。

Abstract: Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.

</details>


### [111] [GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144)
*Ruichuan An,Sihan Yang,Ziyu Guo,Wei Dai,Zijun Shen,Haodong Li,Renrui Zhang,Xinyu Wei,Guopeng Li,Wenshan Wu,Wentao Zhang*

Main category: cs.LG

TL;DR: 本文提出了GENIUS评估套件，旨在评估生成流体智能（GFI），这是一种在模式诱导、即兴约束执行和适应情境知识方面的能力。通过系统地评估12个代表性模型，发现它们在这些任务上的表现存在显著不足，并提出了一种无需训练的注意力干预策略来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要评估的是结晶智力，即依赖于回忆积累的知识和学习到的模式，而忽略了生成流体智能（GFI）：即时归纳模式、基于约束进行推理以及适应新场景的能力。为了严格评估这种能力，作者引入了GENIUS。

Method: 定义GFI为三个基本要素的综合：隐式模式归纳、即兴约束执行与情境知识适应。使用这些标准对12个代表性统一多模态模型进行了系统性评估。此外，还提出了一个无需额外训练的注意机制干预策略以提高模型性能。

Result: 研究显示被评估的模型在GFI相关任务上表现不佳，但进一步诊断分析表明这主要是由于上下文理解能力有限而非内在生成能力不足所导致。提出的注意力干预方法有助于改善这种情况。

Conclusion: GENIUS为GFI设定了严格的评估标准，推动领域从单纯的知识利用向动态、通用推理转变。

Abstract: Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [112] [Rethinking Security of Diffusion-based Generative Steganography](https://arxiv.org/abs/2602.10219)
*Jihao Zhu,Zixuan Chen,Jiali Liu,Lingxiao Yang,Yi Zhou,Weiqi Luo,Xiaohua Xie*

Main category: cs.MM

TL;DR: 本文研究了基于扩散模型的生成图像隐写术（DM-GIS）的安全性关键因素，并提出了一种名为NS-DSer的新隐写分析框架，该框架能够在扩散模型噪声空间中检测到由DM-GIS生成的图像。实验结果证实了理论分析的有效性和NS-DSer在不同检测场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 作者旨在识别影响DM-GIS安全性的关键因素，并重新评估现有方法的安全性。他们注意到扩散模型的噪声空间作为主要嵌入域，任何干扰噪声分布的隐写操作都会破坏DM-GIS的安全性。

Method: 通过分析扩散模型噪声分布与DM-GIS安全性之间的关系，提出了基于噪声空间的扩散隐写分析器（NS-DSer）。使用NS-DSer来重新评估现有DM-GIS方法在不同难度检测情境下的安全性。

Result: 实验证明了关于DM-GIS安全性的理论分析是正确的，并且展示了NS-DSer在多种检测情景中的有效性。

Conclusion: 文章结论指出，通过保持扩散模型噪声分布不变可以提高DM-GIS的安全性；同时，NS-DSer提供了一种有效检测DM-GIS生成图像的方法。

Abstract: Generative image steganography is a technique that conceals secret messages within generated images, without relying on pre-existing cover images. Recently, a number of diffusion model-based generative image steganography (DM-GIS) methods have been introduced, which effectively combat traditional steganalysis techniques. In this paper, we identify the key factors that influence DM-GIS security and revisit the security of existing methods. Specifically, we first provide an overview of the general pipelines of current DM-GIS methods, finding that the noise space of diffusion models serves as the primary embedding domain. Further, we analyze the relationship between DM-GIS security and noise distribution of diffusion models, theoretically demonstrating that any steganographic operation that disrupts the noise distribution compromise DM-GIS security. Building on this insight, we propose a Noise Space-based Diffusion Steganalyzer (NS-DSer)-a simple yet effective steganalysis framework allowing for detecting DM-GIS generated images in the diffusion model noise space. We reevaluate the security of existing DM-GIS methods using NS-DSer across increasingly challenging detection scenarios. Experimental results validate our theoretical analysis of DM-GIS security and show the effectiveness of NS-DSer across diverse detection scenarios.

</details>
