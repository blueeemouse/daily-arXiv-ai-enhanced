<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 65]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [FeDecider: An LLM-Based Framework for Federated Cross-Domain Recommendation](https://arxiv.org/abs/2602.16034)
*Xinrui He,Ting-Wei Li,Tianxin Wei,Xuying Ning,Xinyu He,Wenxuan Bao,Hanghang Tong,Jingrui He*

Main category: cs.IR

TL;DR: 提出了一种基于大语言模型的联邦跨域推荐框架FeDecider，通过分解低秩更新和学习个性化权重来解决过拟合和跨域相似性度量的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦跨域推荐（Federated CDR）旨在在保护数据隐私的同时，跨异构领域协作学习个性化推荐模型。尽管基于大型语言模型（LLM）的推荐模型展现了强大的推理能力和广泛的知识利用能力，但在联邦CDR场景中采用这类模型引入了新的挑战，包括特定领域适配器导致的过拟合风险以及衡量跨域相似性的额外困难。

Method: 为了解决这些问题，本文提出了一个名为FeDecider的基于LLM的联邦跨域推荐框架。该框架通过分离每个客户端的低秩更新并仅共享方向组件来应对规模特异性噪声的问题；同时，为了让集成更加灵活有效，每个客户端还会学习到个性化的权重，以实现从其他领域获取更新的数据感知集成。

Result: 通过在不同数据集上的广泛实验验证了所提出的FeDecider的有效性。

Conclusion: FeDecider作为一种新颖的方法，在保持用户隐私的同时增强了跨异构领域的推荐性能，并解决了使用LLMs进行联邦跨域推荐时遇到的主要障碍。

Abstract: Federated cross-domain recommendation (Federated CDR) aims to collaboratively learn personalized recommendation models across heterogeneous domains while preserving data privacy. Recently, large language model (LLM)-based recommendation models have demonstrated impressive performance by leveraging LLMs' strong reasoning capabilities and broad knowledge. However, adopting LLM-based recommendation models in Federated CDR scenarios introduces new challenges. First, there exists a risk of overfitting with domain-specific local adapters. The magnitudes of locally optimized parameter updates often vary across domains, causing biased aggregation and overfitting toward domain-specific distributions. Second, unlike traditional recommendation models (e.g., collaborative filtering, bipartite graph-based methods) that learn explicit and comparable user/item representations, LLMs encode knowledge implicitly through autoregressive text generation training. This poses additional challenges for effectively measuring the cross-domain similarities under heterogeneity. To address these challenges, we propose an LLM-based framework for federated cross-domain recommendation, FeDecider. Specifically, FeDecider tackles the challenge of scale-specific noise by disentangling each client's low-rank updates and sharing only their directional components. To handle the need for flexible and effective integration, each client further learns personalized weights that achieve the data-aware integration of updates from other domains. Extensive experiments across diverse datasets validate the effectiveness of our proposed FeDecider.

</details>


### [2] [Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System](https://arxiv.org/abs/2602.16124)
*Jiang Zhang,Yubo Wang,Wei Chang,Lu Han,Xingying Cheng,Feng Zhang,Min Li,Songhao Jiang,Wei Zheng,Harry Tran,Zhen Wang,Lei Chen,Yueming Wang,Benyu Zhang,Xiangjun Fan,Bi Xue,Qifan Wang*

Main category: cs.IR

TL;DR: 本文提出了一种多面可学习索引（MFLI），它是一种可扩展的实时检索范式，能够在统一框架内学习多面项目嵌入和索引，并在服务时消除近似最近邻搜索。


<details>
  <summary>Details</summary>
Motivation: 近似最近邻(ANN)搜索在大规模推荐系统中广泛使用，但在检索质量和服务时计算成本方面存在局限性。文章旨在解决这些问题，特别是对于新创建项目的次优检索质量和工业规模下每个请求都必须运行ANN带来的大量计算成本问题。

Method: 通过残差量化构建多面层次码本并与嵌入共同训练，引入了高效的多面索引结构和支持实时更新机制。在服务时间直接使用学到的层次索引识别相关项，完全避免了ANN搜索。

Result: 在数十亿用户的真实数据上进行的广泛实验表明，与先前最先进的方法相比，MFLI在参与任务召回率上提高了最多11.8%，冷内容交付提高了最多57.29%，语义相关性提高了13.5%。此外，在线实验结果还显示了更高的参与度、更少的流行度偏差以及更高的服务效率。

Conclusion: MFLI提供了一种改进的解决方案，不仅提高了检索质量，还减少了服务时的计算成本，适用于大规模推荐系统中的实时检索场景。

Abstract: Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. However, ANN-based retrieval has two key limitations. First, item embeddings and their indices are typically learned in separate stages: indexing is often performed offline after embeddings are trained, which can yield suboptimal retrieval quality-especially for newly created items. Second, although ANN offers sublinear query time, it must still be run for every request, incurring substantial computation cost at industry scale. In this paper, we propose MultiFaceted Learnable Index (MFLI), a scalable, real-time retrieval paradigm that learns multifaceted item embeddings and indices within a unified framework and eliminates ANN search at serving time. Specifically, we construct a multifaceted hierarchical codebook via residual quantization of item embeddings and co-train the codebook with the embeddings. We further introduce an efficient multifaceted indexing structure and mechanisms that support real-time updates. At serving time, the learned hierarchical indices are used directly to identify relevant items, avoiding ANN search altogether. Extensive experiments on real-world data with billions of users show that MFLI improves recall on engagement tasks by up to 11.8\%, cold-content delivery by up to 57.29\%, and semantic relevance by 13.5\% compared with prior state-of-the-art methods. We also deploy MFLI in the system and report online experimental results demonstrating improved engagement, less popularity bias, and higher serving efficiency.

</details>


### [3] [MICE: Minimal Interaction Cross-Encoders for efficient Re-ranking](https://arxiv.org/abs/2602.16299)
*Mathias Vast,Victor Morand,Basile van Cooten,Laure Soulier,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 本文提出了一种名为MICE（最小交互跨编码器）的新架构，该架构通过减少不必要的交互作用来加速跨编码器的推理过程。MICE在保持跨编码器大部分域内有效性的基础上，显著降低了推理延迟，并且在域外数据集上表现出更优的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨编码器虽然在信息检索中提供了最先进的排名效果，但其较高的推理成本限制了它们作为第一阶段排序器的应用。现有工作主要从加速跨编码器推理或改进第一阶段检索效果两方面着手解决这一瓶颈问题。本文旨在结合这两种方法的优势，基于对跨编码器内部机制的深入理解开发新架构。

Method: 作者们通过对标准跨编码器进行细致分析，识别并移除了那些有害或非必要的交互作用，从而设计出一种新的类似于后期交互架构——MICE。

Result: 实验结果表明，与传统跨编码器相比，MICE能够将推理延迟降低四倍，同时保持了良好的域内有效性，并且在域外数据集上的表现优于ColBERT等后期交互模型。

Conclusion: MICE架构成功地结合了跨编码器和后期交互模型的优点，在保证检索质量的同时大幅提升了处理速度，为解决跨编码器高推理成本的问题提供了一个有效的解决方案。

Abstract: Cross-encoders deliver state-of-the-art ranking effectiveness in information retrieval, but have a high inference cost. This prevents them from being used as first-stage rankers, but also incurs a cost when re-ranking documents. Prior work has addressed this bottleneck from two largely separate directions: accelerating cross-encoder inference by sparsifying the attention process or improving first-stage retrieval effectiveness using more complex models, e.g. late-interaction ones. In this work, we propose to bridge these two approaches, based on an in-depth understanding of the internal mechanisms of cross-encoders. Starting from cross-encoders, we show that it is possible to derive a new late-interaction-like architecture by carefully removing detrimental or unnecessary interactions. We name this architecture MICE (Minimal Interaction Cross-Encoders). We extensively evaluate MICE across both in-domain (ID) and out-of-domain (OOD) datasets. MICE decreases fourfold the inference latency compared to standard cross-encoders, matching late-interaction models like ColBERT while retaining most of cross-encoder ID effectiveness and demonstrating superior generalization abilities in OOD.

</details>


### [4] [The Diversity Paradox revisited: Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2602.16315)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Dino Pedreschi,Luca Pappalardo*

Main category: cs.IR

TL;DR: 该研究提出了一种反馈循环模型，用于捕捉推荐系统中的隐式反馈、周期性再训练、概率性采用推荐以及异构推荐系统的特点。通过对在线零售和音乐流媒体数据的应用，发现增加推荐采纳可能会导致个人消费的多样化，但集体需求则以模型和领域依赖的方式重新分配，通常会放大流行度集中现象。静态评估中观察到的个体多样性增加在时间推移下并不成立；当采纳固定且时间推进时，所有模型下的个体多样性实际上都在持续减少。


<details>
  <summary>Details</summary>
Motivation: 现有对推荐系统反馈循环的系统效应理解不足，部分原因在于当前模拟研究中的假设不切实际。因此，有必要开发一个更符合实际情况的反馈循环模型，来更好地理解和设计推荐系统。

Method: 提出了一个包含隐式反馈、周期性再训练、推荐的概率性采用及不同类型的推荐系统的反馈循环模型，并将此框架应用于在线零售与音乐流媒体的真实数据上，分析了反馈循环的系统效应。

Result: 随着推荐采纳率的提高，个人层面的消费选择趋向多样化，但从整体上看，这种变化会导致某些产品或服务变得更为流行（即流行度集中）。此外，虽然静态评价显示个体多样性有所提升，但随时间发展，在所有测试模型中个体多样性实际上均呈现下降趋势。

Conclusion: 本研究表明需要超越静态评价方法，明确考虑反馈循环动态对于推荐系统设计的重要性。这提示我们，在构建推荐系统时，应更加关注长期影响而非仅仅基于短期效果做出决策。

Abstract: Recommender systems shape individual choices through feedback loops in which user behavior and algorithmic recommendations coevolve over time. The systemic effects of these loops remain poorly understood, in part due to unrealistic assumptions in existing simulation studies. We propose a feedback-loop model that captures implicit feedback, periodic retraining, probabilistic adoption of recommendations, and heterogeneous recommender systems. We apply the framework on online retail and music streaming data and analyze systemic effects of the feedback loop. We find that increasing recommender adoption may lead to a progressive diversification of individual consumption, while collective demand is redistributed in model- and domain-dependent ways, often amplifying popularity concentration. Temporal analyses further reveal that apparent increases in individual diversity observed in static evaluations are illusory: when adoption is fixed and time unfolds, individual diversity consistently decreases across all models. Our results highlight the need to move beyond static evaluations and explicitly account for feedback-loop dynamics when designing recommender systems.

</details>


### [5] [Variable-Length Semantic IDs for Recommender Systems](https://arxiv.org/abs/2602.16375)
*Kirill Khrylchenko*

Main category: cs.IR

TL;DR: 本文提出了一种用于推荐系统的可变长度语义标识符，通过使用Gumbel-Softmax重参数化的离散变分自动编码器来学习适应性长度的项目表示，从而解决了现有方法中项目描述长度固定的问题。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，生成模型面临着物品空间极大基数的问题，这使得训练生成模型变得困难，并且引入了自然语言和物品标识符之间的词汇差距。虽然现有的语义标识符（semantic IDs）可以有效地解决这一问题，但它们通常具有固定的长度，对所有物品分配相同的描述长度。这种做法不仅效率低下，而且与自然语言不一致，忽略了真实世界目录中频率结构的高度偏斜性，其中热门物品和稀有长尾物品的信息需求存在根本差异。

Method: 本文介绍了一种带有Gumbel-Softmax重参数化技术的离散变分自动编码器，该方法能够在概率框架下学习到适应性长度的物品表示，同时避免了基于REINFORCE训练方法的不稳定性和先前语义ID方法的固定长度限制。

Result: 文章未直接提供具体结果，但暗示了所提出的方法能够更有效地为推荐系统中的物品生成语义标识符，特别是对于那些具有不同信息需求的热门及长尾物品而言。

Conclusion: 通过结合推荐系统和新兴通信领域的思想，本研究提出的可变长度语义标识符为解决推荐系统中存在的问题提供了新的思路。

Abstract: Generative models are increasingly used in recommender systems, both for modeling user behavior as event sequences and for integrating large language models into recommendation pipelines. A key challenge in this setting is the extremely large cardinality of item spaces, which makes training generative models difficult and introduces a vocabulary gap between natural language and item identifiers. Semantic identifiers (semantic IDs), which represent items as sequences of low-cardinality tokens, have recently emerged as an effective solution to this problem.
  However, existing approaches generate semantic identifiers of fixed length, assigning the same description length to all items. This is inefficient, misaligned with natural language, and ignores the highly skewed frequency structure of real-world catalogs, where popular items and rare long-tail items exhibit fundamentally different information requirements. In parallel, the emergent communication literature studies how agents develop discrete communication protocols, often producing variable-length messages in which frequent concepts receive shorter descriptions. Despite the conceptual similarity, these ideas have not been systematically adopted in recommender systems.
  In this work, we bridge recommender systems and emergent communication by introducing variable-length semantic identifiers for recommendation. We propose a discrete variational autoencoder with Gumbel-Softmax reparameterization that learns item representations of adaptive length under a principled probabilistic framework, avoiding the instability of REINFORCE-based training and the fixed-length constraints of prior semantic ID methods.

</details>


### [6] [From Latent to Observable Position-Based Click Models in Carousel Interfaces](https://arxiv.org/abs/2602.16541)
*Santiago de Leon-Martinez,Robert Moro,Branislav Kveton,Maria Bielikova*

Main category: cs.IR

TL;DR: 本文研究了轮播界面中的位置点击模型，提出了一种基于观察到的检查信号的新模型OEPBM，并发现基于梯度的优化方法在点击预测方面表现更佳。但同时指出，仅依赖点击数据的模型可能无法准确反映用户的真实浏览行为。


<details>
  <summary>Details</summary>
Motivation: 现有的点击模型大多为单一排名列表界面设计，而现代推荐系统越来越多地使用复杂的界面如轮播图。这种界面支持更复杂的用户浏览行为，因此需要专门针对轮播图界面开发新的点击模型来更好地理解和预测用户行为。

Method: 提出了三种新的专为轮播图设计的位置点击模型，其中包括首个无潜在变量且结合了眼动追踪数据得出的观察检查信号的模型（OEPBM）。实现了一个通用框架支持多种优化技术，并通过实验比较了基于梯度的方法与经典方法（如期望最大化和最大似然估计）的效果。

Result: 基于梯度的优化方法在点击可能性预测上始终优于传统方法。在所有评估的模型中，OEPBM在点击预测方面表现最佳，并且其产生的检查模式最接近用户实际行为。然而，研究还表明良好的点击拟合并不意味着用户检查和浏览模式被真实地建模。

Conclusion: 虽然OEPBM在点击预测方面表现出色，但仅依靠点击数据的模型难以准确捕捉用户在复杂界面上的实际浏览模式。这揭示了单靠点击数据建模的局限性，并强调了在为基于轮播图的推荐系统设计点击模型时整合额外行为信号的重要性。

Abstract: Click models are a central component of learning and evaluation in recommender systems, yet most existing models are designed for single ranked-list interfaces. In contrast, modern recommender platforms increasingly use complex interfaces such as carousels, which consist of multiple swipeable lists that enable complex user browsing behaviors.
  In this paper, we study position-based click models in carousel interfaces and examine optimization methods, model structure, and alignment with user behavior. We propose three novel position-based models tailored to carousels, including the first position-based model without latent variables that incorporates observed examination signals derived from eye tracking data, called the Observed Examination Position-Based Model (OEPBM). We develop a general implementation of these carousel click models, supporting multiple optimization techniques and conduct experiments comparing gradient-based methods with classical approaches, namely expectation-maximization and maximum likelihood estimation.
  Our results show that gradient-based optimization consistently achieve better click likelihoods. Among the evaluated models, the OEPBM achieves the strongest performance in click prediction and produces examination patterns that most closely align to user behavior. However, we also demonstrate that strong click fit does not imply realistic modeling of user examination and browsing patterns. This reveals a fundamental limitation of click-only models in complex interfaces and the need for incorporating additional behavioral signals when designing click models for carousel-based recommender systems.

</details>


### [7] [Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models](https://arxiv.org/abs/2602.16587)
*Luankang Zhang,Yonghao Huang,Hang Lv,Mingjia Yin,Liangyue Li,Zulong Chen,Hao Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 研究发现将链式思维（CoT）整合到基于语义ID的推荐基础模型中会降低推荐性能，原因在于通用子空间中的文本惯性。为解决此问题，提出了一种无需训练的推理时间子空间对齐框架，通过压缩推理链并应用偏差减去对比解码来减少无根据的文本漂移，从而在不牺牲基于ID准确性的情况下利用推理。


<details>
  <summary>Details</summary>
Motivation: 研究者注意到将链式思维（CoT）整合至基于语义ID的推荐系统基础模型时，推荐效果反而下降了。这一现象背后的原因被归结于通用子空间内冗长的推理过程导致模型忽视了关键的语义ID信息。

Method: 提出了一个不需要额外训练的推理时期子空间对齐架构，该方法包括压缩推理链条以及采用去除偏差后的对比解码技术，以减轻非基于实际内容的文本漂移现象。

Result: 实验结果表明，所提出的方法能够有效地调整模型推断过程，使得基础模型能够在保持基于ID准确性的前提下更好地运用推理能力。

Conclusion: 通过引入推理时刻子空间对齐框架，可以在不影响基于语义ID准确性的同时增强推荐系统的推理能力。

Abstract: Integrating Chain-of-Thought (CoT) reasoning into Semantic ID-based recommendation foundation models (such as OpenOneRec) often paradoxically degrades recommendation performance. We identify the root cause as textual inertia from the General Subspace, where verbose reasoning dominates inference and causes the model to neglect critical Semantic ID. To address this, we propose a training-free Inference-Time Subspace Alignment framework. By compressing reasoning chains and applying bias-subtracted contrastive decoding, our approach mitigates ungrounded textual drift. Experiments show this effectively calibrates inference, allowing foundation models to leverage reasoning without sacrificing ID-grounded accuracy.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [8] [Emotion Collider: Dual Hyperbolic Mirror Manifolds for Sentiment Recovery via Anti Emotion Reflection](https://arxiv.org/abs/2602.16161)
*Rong Fu,Ziming Wang,Shuo Yin,Wenxin Zhang,Haiyun Wei,Kun Liu,Xianda Li,Zeli Su,Simon Fong*

Main category: cs.MM

TL;DR: 本文提出了一种名为Emotion Collider (EC-Net)的双曲超图框架，用于多模态情绪和情感建模。该框架通过Poincare球嵌入表示模态层次结构，并使用一种能够在节点与超边之间双向传递信息的超图机制执行融合。实验结果表明，EC-Net能够产生鲁棒且语义连贯的表现，并在标准多模态情绪基准测试中持续提高准确性，特别是在模态部分可用或受到噪声污染的情况下。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过开发一个能够处理自然交流中的情感表达的模型来改进人机交互的有效性。为了实现这一目标，提出了Emotion Collider (EC-Net)，它利用了显式的层级几何结构结合超图融合技术以增强对多模态情感的理解。

Method: EC-Net采用Poincare球嵌入来表示模态层次结构，并通过一种允许消息在节点和超边之间双向流动的超图机制进行信息融合。此外，还引入了基于双曲空间对比学习的方法，以解耦径向和角目标的方式加强类别区分度。

Result: 在多个标准多模态情绪数据集上的实验证明了EC-Net可以生成鲁棒性强且语义一致的表现形式，并且即使在模态不完全或存在噪声干扰时也能显著提高识别精度。

Conclusion: 研究表明，将明确的层级几何学与超图融合相结合对于建立稳健的多模态情感理解系统是有效的。

Abstract: Emotional expression underpins natural communication and effective human-computer interaction. We present Emotion Collider (EC-Net), a hyperbolic hypergraph framework for multimodal emotion and sentiment modeling. EC-Net represents modality hierarchies using Poincare-ball embeddings and performs fusion through a hypergraph mechanism that passes messages bidirectionally between nodes and hyperedges. To sharpen class separation, contrastive learning is formulated in hyperbolic space with decoupled radial and angular objectives. High-order semantic relations across time steps and modalities are preserved via adaptive hyperedge construction. Empirical results on standard multimodal emotion benchmarks show that EC-Net produces robust, semantically coherent representations and consistently improves accuracy, particularly when modalities are partially available or contaminated by noise. These findings indicate that explicit hierarchical geometry combined with hypergraph fusion is effective for resilient multimodal affect understanding.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs](https://arxiv.org/abs/2602.15995)
*Xiang Fu,Shiman Meng,Weiping Zhang,Luanzheng Guo,Kento Sato,Dong H. Ahn,Ignacio Laguna,Gregory L. Lee,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出了一种新的方法，使用分布式时钟（DC）和分布式纪元（DE）记录方案来减少OpenMP程序在记录和确定性重放过程中所需的线程同步量，从而提高效率。此外，该方法能够与MPI级别的重放工具结合，以支持非平凡的MPI+OpenMP应用程序的重放。


<details>
  <summary>Details</summary>
Motivation: 尽管OpenMP是最流行的共享内存编程框架之一，但其较高的非确定性执行水平给调试和测试带来了挑战。为了应对这一挑战，需要一种能够记录并确定性地重放程序执行的方法。然而，如何高效地重放OpenMP程序仍然是一个未解决的问题。

Method: 研究者提出了两种新技术：分布式时钟（Distributed Clock, DC）和分布式纪元（Distributed Epoch, DE）记录方案，旨在减少OpenMP记录和回放期间过度的线程同步问题。这些技术通过ReOMP实现，并且证明了可以轻松地与现有的可扩展MPI记录-回放工具ReMPI集成，仅带来少量与MPI规模无关的运行时开销。

Result: 对于代表性的高性能计算应用程序，采用DC和DE记录方案的方法比每次共享内存访问都进行同步的传统方法效率高出2到5倍。此外，该方法成功展示了它能与MPI级别的重放工具结合，有效地重放复杂的MPI+OpenMP应用程序。

Conclusion: 提出的基于DC和DE记录方案的新技术不仅显著提高了OpenMP程序记录与回放的效率，而且为MPI+OpenMP混合应用提供了有效的解决方案。

Abstract: After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.

</details>


### [10] [Scrutinizing Variables for Checkpoint Using Automatic Differentiation](https://arxiv.org/abs/2602.16010)
*Xin Huang,Weiping Zhang,Shiman Meng,Wubiao Xu,Xiang Fu,Luanzheng Guo,Kento Sato*

Main category: cs.DC

TL;DR: 本文提出了一种系统方法，利用自动微分来审查变量中的每个元素以进行检查点设置，从而识别关键和非关键元素，并从检查点中排除非关键元素。通过这种方法，可以在不影响应用程序输出的情况下节省高达20%的存储空间。


<details>
  <summary>Details</summary>
Motivation: 定期保存程序运行状态的检查点/重启机制会消耗大量系统资源。研究者注意到，在典型的高性能计算应用中，并不是所有数据都参与了计算；这种未使用的数据应该被排除在检查点之外，以提高存储/计算效率。

Method: 提出的方法利用了自动微分（AD）工具，对变量中的每一个单独元素进行检查点审查，以判断该元素是否对应用程序输出有影响。通过这种方法，可以识别出关键和非关键元素，并且能够从检查点中移除非关键元素。

Result: 研究者使用来自NAS并行基准套件的八个基准测试对其方法进行了实证验证。结果表明，该方法成功地可视化了变量内关键/非关键元素/区域及其对应用程序输出的影响（是或否）。对于NPB基准测试的评估显示，所提出的方法最多可以节省20%用于检查点的存储空间。

Conclusion: 通过采用自动微分技术识别并排除不重要的数据元素，本文提出的方法为改善高性能计算应用中的检查点/重启过程提供了新的途径，显著提高了存储效率。

Abstract: Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.

</details>


### [11] [LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum](https://arxiv.org/abs/2602.16100)
*Zijie Su,Muhammed Tawfiqul Islam,Mohammad Goudarzi,Adel N. Toosi*

Main category: cs.DC

TL;DR: 本文提出了一种动态流水线重新配置方法，以适应变化的工作负载，同时最小化服务停机时间和性能下降。实验结果表明，该迁移机制的服务停机时间少于50毫秒，并且在首次令牌时间和每输出令牌时间上增加的开销均低于10%。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速发展，在有限的GPU资源下高效地进行LLM推理已经成为一个关键挑战。尽管越来越多的研究探索了将无服务器计算范式应用于LLM服务以最大化资源利用率，但考虑到LLM推理工作负载的高度多样性和现代GPU集群的异构性，仍需要在线动态调整部署配置来更好地适应无服务器环境的弹性和动态特性。同时，由于LLM推理的状态性质和模型参数的巨大规模，在线重新配置变得尤为具有挑战性。

Method: 本文提出了一种动态流水线重新配置方法，允许系统根据变化的工作负载选择最优的流水线配置，从而实现在线调整的同时最小化服务中断和服务质量下降。

Result: 实验结果证明，在包括NVIDIA A100和L40s在内的异构GPU平台上，所提出的迁移机制能够将服务停机时间控制在50毫秒以内，同时对于首次令牌生成时间和每个输出令牌的时间开销增加不超过10%。

Conclusion: 通过引入一种新颖的动态流水线重新配置方案，本研究为解决大型语言模型在无服务器环境中面临的弹性扩展难题提供了有效途径。该方法不仅展示了良好的适应性和灵活性，还保证了高水平的服务质量和用户体验。

Abstract: With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).

</details>


### [12] [Near-optimal population protocols on bounded-degree trees](https://arxiv.org/abs/2602.16222)
*Joel Rybicki,Jakob Solnerzik,Robin Vacus*

Main category: cs.DC

TL;DR: 该论文研究了稀疏交互图中群体协议的空间-时间权衡问题。与完全图不同，有界度树上的群体协议在领导者选举和精确多数问题上没有显示出显著的渐近空间-时间权衡，并提出了基于两个新协议的解决方案，这些方案实现了接近最优的最坏情况预期稳定时间。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索除完全交互图外，其他图族是否也表现出类似的空间-时间复杂度权衡，尤其是针对稀疏图如有限度树，因为现有的下界技术难以应用于非常密集的图之外的情形。

Method: 通过开发两种新颖协议来实现：一种是为一般交互图设计的新快速自稳态2跳着色协议；另一种是在任何树上以最优时间构建根树的自稳态树定向算法。利用这些协议，可以在有向树上使用简单的恒定状态协议来快速解决领导者选举和精确多数问题。

Result: 研究表明，在有界度树上可以实现常数空间协议，并且对于领导者选举和精确多数问题，这些协议具有接近最优的最坏情况预期稳定时间。特别是“定向”湮灭动力学能够在有向树上以$O(n^2 \log n)$步解决精确多数问题。

Conclusion: 不同于完全图，有界度树上的群体协议在处理领导者选举和精确多数问题时并不展示出重要的渐近空间-时间权衡。所提出的协议能够在线性时间内加速解决问题，提供了一种新的方式来优化稀疏图中的群体协议性能。

Abstract: We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.
  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.
  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \log n)$ steps on directed trees.

</details>


### [13] [DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting](https://arxiv.org/abs/2602.16233)
*Prabhjot Singh,Adel N. Toosi,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了一种适用于量子电路切割的有意识估计器执行管道，通过将电路切割视为分阶段的分布式工作负载，并对每个估计器查询进行分区、子实验生成、并行执行和经典重建等阶段的操作。基于两个二分类任务（Iris和MNIST）的数据，研究了切割开销、扩展限制以及对于注入延迟的敏感性，并评估在匹配训练预算下精度和鲁棒性是否得以保持。结果显示，尽管切割引入了大量的端到端开销并且随着切割数量增加而增长，但测试准确性和鲁棒性在测量范围内得以保持，且某些切割设置下观察到了配置依赖性的改进。


<details>
  <summary>Details</summary>
Motivation: 以前的工作主要从子电路数量和采样复杂度的角度来描述切割开销，但对于迭代式、以估计器驱动的训练流程来说，其端到端影响尚未充分地从系统角度被衡量。因此，需要一种新的方法来更好地理解电路切割如何影响整个训练过程的表现。

Method: 作者们设计了一个“切割意识”估计器执行流程，它能够将电路切割当作一系列分布式的阶段性工作负载处理，并为每次估计请求划分出具体阶段如：分区、子实验创建、并行运行及经典重构等。通过对实际运行时轨迹记录以及在两个不同数据集上学习结果的分析，定量研究了切割带来的额外负担、可伸缩性界限及其对人为加入延迟因素的敏感度。

Result: 研究表明，尽管随着切割次数增加，整体结束到结束的额外负担显著增加，尤其是重建阶段占据了每项查询时间的主要部分，限制了通过增加并行程度所能获得的速度提升。然而，在所考察的情景中，测试准确性与抗扰动能力仍然得到了维持，甚至在特定切割设定下还有所提高。

Conclusion: 要实现学习任务中电路切割的实际规模应用，关键在于减少和重叠重建阶段所需时间，并采用考虑到由障碍物主导的关键路径的调度策略。

Abstract: Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.

</details>


### [14] [Load Balanced Parallel Node Generation for Meshless Numerical Methods](https://arxiv.org/abs/2602.16347)
*Jon Vehovar,Miha Rot,Matjaž Depolli,Gregor Kosec*

Main category: cs.DC

TL;DR: 本文提出了一种基于耦合空间索引和工作分配超树的并行执行方法，以改进n维Poisson圆盘采样算法。该方法减少了点插入碰撞检查所需的互斥锁获取次数，并探讨了该算法的行为及其与现有并行化尝试的性能比较。


<details>
  <summary>Details</summary>
Motivation: 为了提高无网格数值分析中节点生成的效率，特别是针对复杂几何形状以及支持可变节点密度的需求，研究者们寻求对现有的n维Poisson圆盘采样方法进行改进，使之能够更有效地利用并行计算资源。

Method: 通过引入耦合的空间索引与预先构建的工作分配超树来修改n维Poisson圆盘采样方法，其中工作分配超树根据节点密度函数预建而成，确保每个叶节点代表一个均衡的工作单元。线程分别推进不同的前端，并在需要时声明工作超树的叶子节点，同时避免声明被其他线程占用的相邻叶子节点。

Result: 所提出的算法有效减少了点插入过程中因碰撞检查而需获得互斥锁的数量，并且表现出了良好的并行执行性能。此外，还讨论了如何将开发出的算法适应于分布式系统的要求。

Conclusion: 这种改进后的n维Poisson圆盘采样方法为无网格数值分析提供了更加高效的支持，特别是在处理具有复杂几何特性和需要自适应节点密度的情况时。它不仅提高了并行计算环境下的执行效率，也为未来扩展到分布式系统打下了基础。

Abstract: Meshless methods are used to solve partial differential equations by approximating differential operators at a node as a weighted sum of values at its neighbours. One of the algorithms for generating nodes suitable for meshless numerical analysis is an n-dimensional Poisson disc sampling based method. It can handle complex geometries and supports variable node density, a crucial feature for adaptive analysis. We modify this method for parallel execution using coupled spatial indexing and work distribution hypertrees. The latter is prebuilt according to the node density function, ensuring that each leaf represents a balanced work unit. Threads advance separate fronts and claim work hypertree leaves as needed while avoiding leaves neighbouring those claimed by other threads. Node placement constraints and the partially prebuilt spatial hypertree are combined to eliminate the need to lock the tree while it is being modified. Thread collision handling is managed by the work hypertree at the leaf level, drastically reducing the number of required mutex acquisitions for point insertion collision checks. We explore the behaviour of the proposed algorithm and compare the performance with existing attempts at parallelisation and consider the requirements for adapting the developed algorithm to distributed systems.

</details>


### [15] [How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability](https://arxiv.org/abs/2602.16362)
*MHD Saria Allahham,Hossam S. Hassanein*

Main category: cs.DC

TL;DR: 本文提出了一种用于评估极端边缘计算（XEC）中计算可靠性的分析框架，定义了在给定服务质量(QoS)阈值下即时容量满足需求的概率。该框架提供了两种信息制度下的闭式可靠性表达式，并扩展到多设备部署场景，同时推导出最优工作负载分配规则和设备选择的解析界限。通过YOLO11m模型实现实时对象检测作为代表性分布式推理流工作负载来验证该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的工作负载越来越多地分布在消费者拥有的设备上以利用其接近用户的优势和普遍可用性，如何确保这些设备或设备集合能够维持流服务所需的处理速率成为了一个关键问题。由于消费者设备因竞争应用和不可预测的使用模式表现出波动的计算可用性，因此需要一种方法来量化设备保持所需处理速率的概率。

Method: 文章提出了一个针对极端边缘计算环境中的计算可靠性的分析框架，此框架基于两种信息体制：最小信息(MI)，仅需声明的操作边界；以及历史数据，通过最大似然估计从过去观察中精炼估计。该框架进一步扩展至支持系列、并行及分区工作负载配置的多设备部署情况。

Result: 研究得出了最佳工作负载分配规则和用于设备选择的分析界限，为编排者提供了评估部署可行性和配置分布式流系统的实用工具。实验结果表明，在不同的容量与需求配置下，分析预测与蒙特卡洛抽样及经验测量之间存在良好的一致性。

Conclusion: 提出的分析框架有效解决了极端边缘计算环境中面对的计算资源波动性挑战，为保证服务质量提供了可靠的手段。

Abstract: Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.

</details>


### [16] [FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving](https://arxiv.org/abs/2602.16603)
*Chia-chi Hsieh,Zan Zong,Xinyang Chen,Jianjiang Li,Jidong Zhai,Lijie Wen*

Main category: cs.DC

TL;DR: 本文提出了一种名为FlowPrefill的系统，旨在解决大型语言模型在处理并发请求时面临的头阻塞问题。通过操作级抢占和事件驱动调度两项创新，该系统能够同时优化响应时间和吞吐量，据实测数据表明，与现有最先进系统相比，最大吞吐量提高了5.6倍，并能满足多样化的服务等级目标。


<details>
  <summary>Details</summary>
Motivation: 随着对大型语言模型（LLMs）需求的增长，服务系统需要处理具有不同服务级别目标（SLOs）的许多并发请求。这加剧了计算密集型预填充阶段中的头阻塞（HoL），其中长时间运行的请求独占资源并延迟优先级更高的请求，导致首次令牌时间（TTFT）SLO违规广泛发生。尽管分块预填充可以实现中断性，但它引入了响应性和吞吐量之间的固有折衷：减少块大小可以改善响应延迟但会降低计算效率；增加块大小虽然能最大化吞吐量却加重了阻塞情况。因此，需要一种自适应抢占机制来解决这一矛盾。

Method: 为了解决上述挑战，本研究提出了FlowPrefill系统，该系统主要通过两个关键创新实现自适应预填充调度：1) 操作级抢占（Operator-Level Preemption），利用操作边界实现了细粒度执行中断，而不会带来固定小块划分所关联的效率损失；2) 事件驱动调度（Event-Driven Scheduling），仅在请求到达或完成事件触发时才作出调度决策，从而支持高效的抢占响应性同时最小化控制平面开销。

Result: 基于真实生产轨迹的评估显示，与当前最先进的系统相比，FlowPrefill可将最大良好吞吐量提高至多5.6倍，同时满足异构SLOs的需求。

Conclusion: FlowPrefill作为一种针对TTFT-良好吞吐量优化的服务系统，通过解耦抢占粒度与调度频率成功解决了响应性与吞吐量之间的冲突。其提出的操作级抢占和事件驱动调度策略不仅有效缓解了头阻塞问题，还显著提升了系统性能，确保了多样化服务级别目标的达成。

Abstract: The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.
  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [A Koopman-Bayesian Framework for High-Fidelity, Perceptually Optimized Haptic Surgical Simulation](https://arxiv.org/abs/2602.15834)
*Rohit Kaushik,Eva Kaushik*

Main category: cs.LG

TL;DR: 该论文提出了一种结合非线性动力学、感知心理物理学和高频触觉渲染的统一框架，以提高手术模拟的真实感。通过Koopman算子公式将手术设备与软组织之间的交互提升至增强状态空间，并采用基于韦伯-费希纳和史蒂文斯定律的贝叶斯校准模块来确保所渲染的力与人类感知极限一致。实验结果表明，该系统在延迟、力误差及感知辨别方面均优于传统方法，并讨论了其对医学教育尤其是手术训练的影响及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了提高手术模拟的真实性，本文旨在通过整合非线性动态建模、感知心理学原理以及高效触觉反馈技术开发一种新的手术仿真框架。

Method: 本研究首先利用Koopman算子理论将手术器械与软组织间的相互作用映射到一个扩增的状态空间中，从而实现对本质上为非线性的动力学过程进行线性预测与控制；其次，引入了一个基于韦伯-费希纳法则和史蒂文斯幂律的贝叶斯校正机制，使得生成的力反馈信号能够根据每位用户的感知阈值自适应调整；最后，在多种外科任务场景下测试了所提方案的有效性。

Result: 实验结果显示，对于包括触诊、切开及骨磨削在内的多种模拟外科操作，所提出的系统实现了平均4.3毫秒的渲染延迟、小于2.8%的力误差以及20%以上的感知辨别能力提升。此外，多变量统计分析（如多元方差分析和回归）进一步证实了相较于传统的弹簧-阻尼器模型和能量基础的渲染方法，新系统表现出了显著的优势。

Conclusion: 本文介绍的新框架不仅极大地提升了手术模拟中的触觉反馈质量，还为未来的虚拟现实医疗教育和神经反馈闭环触觉界面的研究奠定了基础。

Abstract: We introduce a unified framework that combines nonlinear dynamics, perceptual psychophysics and high frequency haptic rendering to enhance realism in surgical simulation. The interaction of the surgical device with soft tissue is elevated to an augmented state space with a Koopman operator formulation, allowing linear prediction and control of the dynamics that are nonlinear by nature. To make the rendered forces consistent with human perceptual limits, we put forward a Bayesian calibration module based on WeberFechner and Stevens scaling laws, which progressively shape force signals relative to each individual's discrimination thresholds. For various simulated surgical tasks such as palpation, incision, and bone milling, the proposed system attains an average rendering latency of 4.3 ms, a force error of less than 2.8% and a 20% improvement in perceptual discrimination. Multivariate statistical analyses (MANOVA and regression) reveal that the system's performance is significantly better than that of conventional spring-damper and energy, based rendering methods. We end by discussing the potential impact on surgical training and VR, based medical education, as well as sketching future work toward closed, loop neural feedback in haptic interfaces.

</details>


### [18] [ModalImmune: Immunity Driven Unlearning via Self Destructive Training](https://arxiv.org/abs/2602.16197)
*Rong Fu,Jia Yee Tan,Wenxin Zhang,Zijian Zhang,Ziming Wang,Zhaolu Kang,Muge Qi,Shuning Zhang,Simon Fong*

Main category: cs.LG

TL;DR: 本文提出了一种名为ModalImmune的训练框架，通过在训练过程中有意且可控地减少选定模态信息，使模型学习到对破坏性模态影响具有鲁棒性的联合表示。该框架结合了频谱自适应崩溃正则化器、信息增益导向控制器、曲率感知梯度掩蔽技术以及用于自动元参数调整的认证Neumann截断超梯度过程。实验结果表明，ModalImmune能够提高系统对于模态丢失和损坏的抵抗力，同时保持收敛稳定性和重建能力。


<details>
  <summary>Details</summary>
Motivation: 多模态系统在部署时容易受到部分或全部输入通道丢失的影响，这对现实世界中的可靠性构成了威胁。为了应对这一挑战，研究者们开发了ModalImmune，旨在增强模型对不同模态信息缺失情况下的鲁棒性。

Method: ModalImmune框架采用了四种关键技术：1) 频谱自适应崩溃正则化器，帮助模型学会处理特定模态信息缺失的情况；2) 信息增益导向控制器，允许对特定模态进行有针对性的操作；3) 曲率感知梯度掩蔽技术，用以稳定由故意引入的破坏性更新所导致的学习过程；4) 认证Neumann截断超梯度程序，实现了自动化的元参数调整。

Result: 通过对标准多模态基准数据集的评估显示，采用ModalImmune方法后，即使在面临模态移除或损坏的情况下，系统仍能表现出较好的性能，并且保持了良好的收敛稳定性和重建能力。

Conclusion: 研究表明，通过使用ModalImmune框架，在训练阶段就考虑到了可能发生的模态信息损失情况，从而有效提高了多模态系统的鲁棒性和实用性。

Abstract: Multimodal systems are vulnerable to partial or complete loss of input channels at deployment, which undermines reliability in real-world settings. This paper presents ModalImmune, a training framework that enforces modality immunity by intentionally and controllably collapsing selected modality information during training so the model learns joint representations that are robust to destructive modality influence. The framework combines a spectrum-adaptive collapse regularizer, an information-gain guided controller for targeted interventions, curvature-aware gradient masking to stabilize destructive updates, and a certified Neumann-truncated hyper-gradient procedure for automatic meta-parameter adaptation. Empirical evaluation on standard multimodal benchmarks demonstrates that ModalImmune improves resilience to modality removal and corruption while retaining convergence stability and reconstruction capacity.

</details>


### [19] [Memes-as-Replies: Can Models Select Humorous Manga Panel Responses?](https://arxiv.org/abs/2602.15842)
*Ryosuke Kohita,Seiichiro Yoshioka*

Main category: cs.LG

TL;DR: 本文介绍了Meme Reply Selection任务和一个名为MaMe-Re的基准，包含10万个由人类标注的日本漫画面板和社交媒体帖子对。研究发现大语言模型在理解复杂社交线索方面有所进展，但视觉信息的加入并未提高性能，并且这些模型难以区分语义相似选项中的微妙幽默差异。


<details>
  <summary>Details</summary>
Motivation: 尽管计算研究已经集中在分析模因的内在属性上，但是模因被动态地、基于上下文用来创造幽默这一点仍是一个未充分研究的领域。为了填补这一空白，作者们提出了Meme Reply Selection任务并创建了一个基准数据集。

Method: 研究者们构建了MaMe-Re（Manga Meme Reply Benchmark）基准，其中包含了10万个人类标注的数据对（总共50万条来自2,325个独特标注者的注释），包括开放许可的日本漫画片段和社交媒体帖子。

Result: 研究揭示了三个主要发现：(1) 大型语言模型展示了捕捉如夸张等复杂社交暗示的初步证据；(2) 视觉信息的添加并没有改善表现，这表明存在理解视觉内容与有效利用它进行上下文幽默之间的差距；(3) 尽管大型语言模型可以在受控环境中匹配人类判断，但在区分语义相似候选者之间细微的机智差异时显得力不从心。

Conclusion: 选择具有上下文幽默感的回答对于当前模型来说仍然是一个开放性的挑战。

Abstract: Memes are a popular element of modern web communication, used not only as static artifacts but also as interactive replies within conversations. While computational research has focused on analyzing the intrinsic properties of memes, the dynamic and contextual use of memes to create humor remains an understudied area of web science. To address this gap, we introduce the Meme Reply Selection task and present MaMe-Re (Manga Meme Reply Benchmark), a benchmark of 100,000 human-annotated pairs (500,000 total annotations from 2,325 unique annotators) consisting of openly licensed Japanese manga panels and social media posts. Our analysis reveals three key insights: (1) large language models (LLMs) show preliminary evidence of capturing complex social cues such as exaggeration, moving beyond surface-level semantic matching; (2) the inclusion of visual information does not improve performance, revealing a gap between understanding visual content and effectively using it for contextual humor; (3) while LLMs can match human judgments in controlled settings, they struggle to distinguish subtle differences in wit among semantically similar candidates. These findings suggest that selecting contextually humorous replies remains an open challenge for current models.

</details>


### [20] [Kalman-Inspired Runtime Stability and Recovery in Hybrid Reasoning Systems](https://arxiv.org/abs/2602.15855)
*Barak Or*

Main category: cs.LG

TL;DR: 本文从卡尔曼启发的角度研究了混合推理系统在运行时的稳定性问题，提出了一个监测内部创新信号、检测不稳定现象并触发恢复机制的框架，实验表明该方法能在任务失败前可靠地检测到不稳定性，并在可能的情况下使系统行为在有限时间内恢复到有界状态。


<details>
  <summary>Details</summary>
Motivation: 混合推理系统在工具增强决策循环中的应用越来越广泛，但在部分可观测性和持续证据不匹配情况下的运行时行为仍不够清楚。实践中，系统故障往往表现为内部推理动态逐渐偏离而非孤立的预测错误。

Method: 本文将推理建模为由内部创新信号驱动的随机推理过程，并引入认知漂移作为可测量的运行时现象。定义稳定性包括可检测性、有界发散和可恢复性。提出了一种运行时稳定性框架，用于监控创新统计数据、检测新出现的不稳定性并触发具有恢复意识的控制机制。

Result: 通过多步骤、工具增强的推理任务实验，展示了在任务失败之前能够可靠地检测到不稳定性，并且当恢复可行时，能够在有限时间内重新建立有界的内部行为。

Conclusion: 这些结果强调了运行时稳定性作为系统级要求对于在不确定性下实现可靠推理的重要性。

Abstract: Hybrid reasoning systems that combine learned components with model-based inference are increasingly deployed in tool-augmented decision loops, yet their runtime behavior under partial observability and sustained evidence mismatch remains poorly understood. In practice, failures often arise as gradual divergence of internal reasoning dynamics rather than as isolated prediction errors. This work studies runtime stability in hybrid reasoning systems from a Kalman-inspired perspective. We model reasoning as a stochastic inference process driven by an internal innovation signal and introduce cognitive drift as a measurable runtime phenomenon. Stability is defined in terms of detectability, bounded divergence, and recoverability rather than task-level correctness. We propose a runtime stability framework that monitors innovation statistics, detects emerging instability, and triggers recovery-aware control mechanisms. Experiments on multi-step, tool-augmented reasoning tasks demonstrate reliable instability detection prior to task failure and show that recovery, when feasible, re-establishes bounded internal behavior within finite time. These results emphasize runtime stability as a system-level requirement for reliable reasoning under uncertainty.

</details>


### [21] [Genetic Generalized Additive Models](https://arxiv.org/abs/2602.15877)
*Kaaustaaub Shankar,Kelly Cohen*

Main category: cs.LG

TL;DR: 本文提出了一种使用多目标遗传算法NSGA-II自动优化广义可加模型（GAMs）的方法，旨在同时最小化预测误差和复杂度惩罚。实验结果表明该方法在保持或超越基准模型准确性的同时，显著降低了模型复杂度，提高了模型的解释性。


<details>
  <summary>Details</summary>
Motivation: 广义可加模型（GAMs）虽然在预测准确性和可解释性之间取得了平衡，但手动配置其结构具有挑战性。因此，研究者们寻求一种能够自动优化GAMs结构的方法来解决这个问题。

Method: 研究采用多目标遗传算法NSGA-II来自动寻找最佳的GAMs配置，通过共同最小化预测误差（RMSE）与一个包含稀疏性、平滑度及不确定性的复杂度惩罚项实现。

Result: 实验基于加州住房数据集进行，结果显示NSGA-II发现的GAMs在准确性上优于基线LinearGAMs，或者在表现相当的情况下有着明显更低的复杂度。所得到的模型更加简单、平滑，并且表现出更窄的置信区间，从而增强了模型的可解释性。

Conclusion: 该框架提供了一种通用的方法用于自动化地优化既透明又高性能的模型，为提高GAMs等模型的实用性和可解释性开辟了新途径。

Abstract: Generalized Additive Models (GAMs) balance predictive accuracy and interpretability, but manually configuring their structure is challenging. We propose using the multi-objective genetic algorithm NSGA-II to automatically optimize GAMs, jointly minimizing prediction error (RMSE) and a Complexity Penalty that captures sparsity, smoothness, and uncertainty. Experiments on the California Housing dataset show that NSGA-II discovers GAMs that outperform baseline LinearGAMs in accuracy or match performance with substantially lower complexity. The resulting models are simpler, smoother, and exhibit narrower confidence intervals, enhancing interpretability. This framework provides a general approach for automated optimization of transparent, high-performing models. The code can be found at https://github.com/KaaustaaubShankar/GeneticAdditiveModels.

</details>


### [22] [IT-OSE: Exploring Optimal Sample Size for Industrial Data Augmentation](https://arxiv.org/abs/2602.15878)
*Mingchun Sun,Rongqiang Zhao,Zhennan Huang,Songyu Ding,Jie Liu*

Main category: cs.LG

TL;DR: 研究人员提出了一种基于信息理论的最优样本大小估计（IT-OSE）方法，用于工业数据增强，并引入了区间覆盖率和偏差（ICD）评分来直观评估估计出的OSS。实验结果表明，与经验估计相比，IT-OSE在分类任务中的准确率平均提高了4.38%，回归任务中的MAPE平均降低了18.80%。同时，相比于穷举搜索，IT-OSE在达到相同OSS的同时，平均减少了83.97%的计算成本和93.46%的数据成本。


<details>
  <summary>Details</summary>
Motivation: 在工业场景中，虽然数据增强是提高模型性能的有效手段，但其益处并非单向有利。目前缺乏对于数据增强时最佳样本大小(OSS)的理论研究或既定估计方法，也没有适当的指标来评价OSS的准确性或它与真实值之间的偏差。

Method: 提出了一个基于信息理论的最佳样本量估计（IT-OSE），为工业数据增强提供可靠的OSS估计。另外，还提出了区间覆盖与偏离（ICD）分数来直观地评价所估计的OSS。从理论上分析并制定了OSS与其主导因素之间的关系，从而增强了可解释性。

Result: 实验显示，与经验估计相比，IT-OSE在基线模型上的分类任务准确率平均提高了4.38%，回归任务上的平均绝对百分比误差(MAPE)降低了18.80%。下游模型性能提升更加稳定，ICD得分中的ICDdev也平均减少了49.30%。此外，与穷举搜索相比，在获得相同的OSS情况下，IT-OSE平均减少了83.97%的计算成本和93.46%的数据成本。实践性实验进一步证明了IT-OSE在代表性传感器基础工业场景中的普遍适用性。

Conclusion: 通过提出的信息论最优样本量估计(IT-OSE)方法，不仅能够有效提高工业数据增强情境下的模型性能，而且在减少计算及数据成本方面表现突出。此外，该方法具有良好的通用性和稳定性，适用于多种工业场景。

Abstract: In industrial scenarios, data augmentation is an effective approach to improve model performance. However, its benefits are not unidirectionally beneficial. There is no theoretical research or established estimation for the optimal sample size (OSS) in augmentation, nor is there an established metric to evaluate the accuracy of OSS or its deviation from the ground truth. To address these issues, we propose an information-theoretic optimal sample size estimation (IT-OSE) to provide reliable OSS estimation for industrial data augmentation. An interval coverage and deviation (ICD) score is proposed to evaluate the estimated OSS intuitively. The relationship between OSS and dominant factors is theoretically analyzed and formulated, thereby enhancing the interpretability. Experiments show that, compared to empirical estimation, the IT-OSE increases accuracy in classification tasks across baseline models by an average of 4.38%, and reduces MAPE in regression tasks across baseline models by an average of 18.80%. The improvements in downstream model performance are more stable. ICDdev in the ICD score is also reduced by an average of 49.30%. The determinism of OSS is enhanced. Compared to exhaustive search, the IT-OSE achieves the same OSS while reducing computational and data costs by an average of 83.97% and 93.46%. Furthermore, practicality experiments demonstrate that the IT-OSE exhibits generality across representative sensor-based industrial scenarios.

</details>


### [23] [BamaER: A Behavior-Aware Memory-Augmented Model for Exercise Recommendation](https://arxiv.org/abs/2602.15879)
*Qing Yang,Yuhao Jiang,Rui Wang,Jipeng Guo,Yejiang Wang,Xinghe Cheng,Zezheng Wu,Jiapu Wang,Jingwei Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的基于行为感知和记忆增强的练习推荐框架BamaER，通过三个核心模块来提高学习进度预测、知识掌握估计以及练习推荐的质量。实验表明，该方法在多个实际教育数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的运动推荐方法主要将学生的学习表示为运动序列，忽略了丰富的行为交互信息，导致对学习进度的估计存在偏差且不可靠。此外，固定长度的序列分割限制了早期学习经验的整合，阻碍了长期依赖关系的建模和知识掌握的准确估计。

Method: BamaER框架由三个核心模块组成：(i) 通过三向混合编码方案捕捉异构学生互动行为的学习进度预测模块；(ii) 维持动态记忆矩阵以联合建模历史和当前知识状态的记忆增强型知识追踪模块，用于稳健掌握估计；(iii) 将候选选择公式化为多样性意识优化问题的练习过滤模块，并通过河马优化算法解决以减少冗余并提高推荐覆盖率。

Result: 在五个真实世界的教育数据集上的实验显示，BamaER在一系列评估指标上始终优于最先进基线。

Conclusion: BamaER框架通过更全面地考虑学生的行为特征与历史学习经历，有效提高了练习推荐系统的性能。

Abstract: Exercise recommendation focuses on personalized exercise selection conditioned on students' learning history, personal interests, and other individualized characteristics. Despite notable progress, most existing methods represent student learning solely as exercise sequences, overlooking rich behavioral interaction information. This limited representation often leads to biased and unreliable estimates of learning progress. Moreover, fixed-length sequence segmentation limits the incorporation of early learning experiences, thereby hindering the modeling of long-term dependencies and the accurate estimation of knowledge mastery. To address these limitations, we propose BamaER, a Behavior-aware memory-augmented Exercise Recommendation framework that comprises three core modules: (i) the learning progress prediction module that captures heterogeneous student interaction behaviors via a tri-directional hybrid encoding scheme; (ii) the memory-augmented knowledge tracing module that maintains a dynamic memory matrix to jointly model historical and current knowledge states for robust mastery estimation; and (iii) the exercise filtering module that formulates candidate selection as a diversity-aware optimization problem, solved via the Hippopotamus Optimization Algorithm to reduce redundancy and improve recommendation coverage. Experiments on five real-world educational datasets show that BamaER consistently outperforms state-of-the-art baselines across a range of evaluation metrics.

</details>


### [24] [Adaptive Semi-Supervised Training of P300 ERP-BCI Speller System with Minimum Calibration Effort](https://arxiv.org/abs/2602.15955)
*Shumeng Chen,Jane E. Huggins,Tianwen Ma*

Main category: cs.LG

TL;DR: 该研究提出了一种基于P300 ERP的脑机接口拼写器的统一框架，通过使用自适应半监督EM-GMM算法减少了校准所需的数据量，从而提高了拼写效率。实验结果表明，在15名参与者中，有9名参与者的字符级准确率超过了0.7，并且在这些参与者中，有7名显示出新方法比基准方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统的P300 ERP脑机接口拼写器需要长时间的校准过程来构建二元分类器，这降低了整体效率。为了解决这个问题，本研究旨在开发一个只需要少量标记校准数据即可更新分类器的高效框架。

Method: 研究人员提出了一种新的统一框架，该框架利用了自适应半监督EM-GMM（期望最大化-高斯混合模型）算法，允许系统在仅有少量标记数据的情况下也能有效地更新其二元分类器。

Result: 实验结果显示，所提出的自适应方法对于大多数参与者来说优于传统基准方法，尤其是在提高字符级别预测准确性方面。此外，该方法也显示出了更高的信息传输率和BCI实用性。

Conclusion: 这项研究表明，所提出的半监督学习框架为实时脑机接口拼写系统提供了一个实用且高效的解决方案，特别是在标签数据有限的情况下能显著改善拼写效率。

Abstract: A P300 ERP-based Brain-Computer Interface (BCI) speller is an assistive communication tool. It searches for the P300 event-related potential (ERP) elicited by target stimuli, distinguishing it from the neural responses to non-target stimuli embedded in electroencephalogram (EEG) signals. Conventional methods require a lengthy calibration procedure to construct the binary classifier, which reduced overall efficiency. Thus, we proposed a unified framework with minimum calibration effort such that, given a small amount of labeled calibration data, we employed an adaptive semi-supervised EM-GMM algorithm to update the binary classifier. We evaluated our method based on character-level prediction accuracy, information transfer rate (ITR), and BCI utility. We applied calibration on training data and reported results on testing data. Our results indicate that, out of 15 participants, 9 participants exceed the minimum character-level accuracy of 0.7 using either on our adaptive method or the benchmark, and 7 out of these 9 participants showed that our adaptive method performed better than the benchmark. The proposed semi-supervised learning framework provides a practical and efficient alternative to improve the overall spelling efficiency in the real-time BCI speller system, particularly in contexts with limited labeled data.

</details>


### [25] [R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions](https://arxiv.org/abs/2602.15961)
*Zhi Sheng,Yuan Yuan,Guozhen Zhang,Yong Li*

Main category: cs.LG

TL;DR: 本文介绍了R$^2$Energy，一个用于可再生能源预测的大规模基准，它基于中国四个省份902个风能和太阳能电站的超过1070万条高保真小时记录。该基准提供了一个标准化、无泄漏的预测范式，并揭示了在极端天气条件下模型可靠性与其气象整合策略而非架构复杂性相关的关键'鲁棒性差距'。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源特别是风能和太阳能的迅速扩展，可靠的预测对于电力系统运行至关重要。尽管最近的深度学习模型已达到较高的平均准确度，但气候驱动的极端天气事件频发且强度增加对电网稳定性和运营安全构成了严重威胁。因此，开发能够抵御波动条件的稳健预测模型成为了一个重要挑战。

Method: 本文提出了R$^2$Energy，这是一个结合了数值天气预报（NWP）辅助的可再生能源预测大规模基准。它包含了来自中国四个省份902个风力和太阳能发电站的超过10.7百万条高保真小时记录。此外，还建立了一个标准化、无泄漏的预测模式，允许所有模型以相同的方式访问未来的NWP信号，从而实现跨不同先进预测架构之间的公平和可重复比较。

Result: 研究发现存在一个关键的'鲁棒性差距'，这通常被平均指标所掩盖。这个差距显示，在极端条件下，模型的可靠性更多地取决于其气象集成策略而不是结构的复杂性。

Conclusion: R$^2$Energy为评估和发展面向安全关键型电力系统应用的预测模型提供了原则性的基础。

Abstract: The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and intensity of climate-driven extreme weather events pose severe threats to grid stability and operational security. Consequently, developing robust forecasting models that can withstand volatile conditions has become a paramount challenge. In this paper, we present R$^2$Energy, a large-scale benchmark for NWP-assisted renewable energy forecasting. It comprises over 10.7 million high-fidelity hourly records from 902 wind and solar stations across four provinces in China, providing the diverse meteorological conditions necessary to capture the wide-ranging variability of renewable generation. We further establish a standardized, leakage-free forecasting paradigm that grants all models identical access to future Numerical Weather Prediction (NWP) signals, enabling fair and reproducible comparison across state-of-the-art representative forecasting architectures. Beyond aggregate accuracy, we incorporate regime-wise evaluation with expert-aligned extreme weather annotations, uncovering a critical ``robustness gap'' typically obscured by average metrics. This gap reveals a stark robustness-complexity trade-off: under extreme conditions, a model's reliability is driven by its meteorological integration strategy rather than its architectural complexity. R$^2$Energy provides a principled foundation for evaluating and developing forecasting models for safety-critical power system applications.

</details>


### [26] [B-DENSE: Branching For Dense Ensemble Network Learning](https://arxiv.org/abs/2602.15971)
*Cherish Puniani,Tushar Kumar,Arnav Bendre,Gaurav Kumar,Shree Singhi*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架B-DENSE，通过多分支轨迹对齐来解决扩散模型中由于稀疏监督导致的结构信息丢失和显著离散化误差问题，从而提高图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型受到非平衡热力学的启发，在生成建模方面取得了最先进的性能，但其迭代采样性质导致推理延迟高。虽然最近的蒸馏技术加速了采样过程，但这些技术忽略了中间轨迹步骤，这导致了结构信息的丢失并引入了大量的离散化误差。

Method: 作者提出了B-DENSE框架，该框架利用多分支轨迹对齐。学生架构被修改以输出K倍扩展通道，每个子集对应于代表教师轨迹中的特定离散中间步骤的一个分支。通过对这些分支进行训练，使其同时映射到教师目标时间步长的整个序列，强制执行密集的中间轨迹对齐。

Result: 实验结果表明，与基线蒸馏框架相比，从训练的最早阶段开始，学生模型就能够学习如何导航解空间，显示出更好的图像生成质量。

Conclusion: B-DENSE框架通过增强中间轨迹的一致性解决了现有蒸馏方法在加速扩散模型时遇到的问题，并且在图像生成任务上优于现有的蒸馏方法。

Abstract: Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.

</details>


### [27] [Fast Online Learning with Gaussian Prior-Driven Hierarchical Unimodal Thompson Sampling](https://arxiv.org/abs/2602.15972)
*Tianchi Zhao,He Liu,Hongyin Shi,Jinliang Li*

Main category: cs.LG

TL;DR: 本文研究了具有高斯奖励反馈的聚类臂的多臂老虎机问题，提出了基于汤普森采样的聚类臂算法（TSCG），并证明了该方法在利用两层结构时可以实现比普通TSG更低的遗憾界。此外，当奖励是单峰分布时，通过单峰汤普森采样聚类臂算法(UTSCG)可以获得更低的遗憾界。理论评估和数值实验均证实了所提算法的优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决实际应用中出现的一类多臂老虎机问题，其中臂的奖励反馈服从高斯分布并且被聚类，这在毫米波通信和风险资产组合管理等领域有广泛应用。

Method: 本文的方法基于汤普森采样算法，并针对具有两层分层结构的聚类臂情况提出了改进版本TSCG。另外，对于单峰奖励分布的情况，还提出了进一步优化的算法UTSCG。

Result: 研究表明，利用两层结构的TSCG算法相比于普通的TSG算法能够达到更低的遗憾界；而当奖励分布为单峰时，UTSCG算法能提供更优的表现。

Conclusion: 本研究提出的TSCG及UTSCG算法不仅在理论上得到了较低的上界遗憾估计，在实际数值实验中也验证了其相较于传统方法的有效性与优越性。

Abstract: We study a type of Multi-Armed Bandit (MAB) problems in which arms with a Gaussian reward feedback are clustered. Such an arm setting finds applications in many real-world problems, for example, mmWave communications and portfolio management with risky assets, as a result of the universality of the Gaussian distribution. Based on the Thompson Sampling algorithm with Gaussian prior (TSG) algorithm for the selection of the optimal arm, we propose our Thompson Sampling with Clustered arms under Gaussian prior (TSCG) specific to the 2-level hierarchical structure. We prove that by utilizing the 2-level structure, we can achieve a lower regret bound than we do with ordinary TSG. In addition, when the reward is Unimodal, we can reach an even lower bound on the regret by our Unimodal Thompson Sampling algorithm with Clustered Arms under Gaussian prior (UTSCG). Each of our proposed algorithms are accompanied by theoretical evaluation of the upper regret bound, and our numerical experiments confirm the advantage of our proposed algorithms.

</details>


### [28] [Verifier-Constrained Flow Expansion for Discovery Beyond the Data](https://arxiv.org/abs/2602.15984)
*Riccardo De Santi,Kimon Protopapas,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 本文提出了一种名为Flow Expander (FE)的方法，通过验证器约束的熵最大化来扩展预训练流模型的有效样本生成范围，同时保持样本的有效性。在理论分析和实验评估中，FE展示了其在增加分子构象多样性的同时保持有效性的能力。


<details>
  <summary>Details</summary>
Motivation: 流动和扩散模型通常只在有限的数据上进行预训练，这导致它们仅能从可行域的一小部分生成样本。为了解决这个问题，研究者希望利用验证器（如原子键检查器）来调整预训练的流模型，使其能够超出数据高可用区域生成有效的设计样本。

Method: 引入了强弱验证器的形式概念，并提出了全局和局部流扩展的算法框架。接着介绍了一种可扩展的镜像下降方案——Flow Expander (FE)，该方法通过对流过程噪声状态空间中的验证器约束熵最大化来解决上述问题。

Result: 提供了所提方法的详尽理论分析，并陈述了在理想化与一般假设下的收敛保证。最后，在可视化设置和分子设计任务上的实证评价表明，FE能够在保持有效性的同时扩大预训练流模型的样本生成范围并增加构象多样性。

Conclusion: Flow Expander 方法提供了一个有效的解决方案，用于扩展预训练流模型以生成更广泛的有效样本，特别是在科学发现应用中，它有助于探索已知数据分布之外的设计空间。

Abstract: Flow and diffusion models are typically pre-trained on limited available data (e.g., molecular samples), covering only a fraction of the valid design space (e.g., the full molecular space). As a consequence, they tend to generate samples from only a narrow portion of the feasible domain. This is a fundamental limitation for scientific discovery applications, where one typically aims to sample valid designs beyond the available data distribution. To this end, we address the challenge of leveraging access to a verifier (e.g., an atomic bonds checker), to adapt a pre-trained flow model so that its induced density expands beyond regions of high data availability, while preserving samples validity. We introduce formal notions of strong and weak verifiers and propose algorithmic frameworks for global and local flow expansion via probability-space optimization. Then, we present Flow Expander (FE), a scalable mirror descent scheme that provably tackles both problems by verifier-constrained entropy maximization over the flow process noised state space. Next, we provide a thorough theoretical analysis of the proposed method, and state convergence guarantees under both idealized and general assumptions. Ultimately, we empirically evaluate our method on both illustrative, yet visually interpretable settings, and on a molecular design task showcasing the ability of FE to expand a pre-trained flow model increasing conformer diversity while preserving validity.

</details>


### [29] [Geometry-Aware Uncertainty Quantification via Conformal Prediction on Manifolds](https://arxiv.org/abs/2602.16015)
*Marzieh Amiri Shahbazi,Ali Baheri*

Main category: cs.LG

TL;DR: 本文提出了一种自适应测地线共形预测方法，该方法通过使用测地线非一致性分数并结合一个交叉验证难度估计器来处理回归问题中响应位于黎曼流形上的情况。实验表明，这种方法能够显著减少条件覆盖率的变异性，并在最坏情况下使覆盖率更接近名义水平。


<details>
  <summary>Details</summary>
Motivation: 传统的共形预测方法假设输出空间为欧几里得空间，在响应变量位于黎曼流形上时会产生校准不良的预测区域。为了改善这一状况，特别是针对异方差噪声情形下的预测性能，提出了新的框架。

Method: 引入了基于测地线而非欧氏距离的非一致性评分，并通过一个经过交叉验证的难度评估器对其进行归一化处理。由此产生的预测区间是球面上的测地线帽，其面积独立于位置且大小可根据局部预测难度进行调整。

Result: 在合成球体实验和基于IGRF-14卫星数据的真实世界地磁场预测任务中，所提方法显著降低了条件覆盖率的变化性，并大幅提高了最坏情况下的覆盖率至接近名义水平。相比之下，基于坐标的基线方法由于图表失真而浪费了大量的覆盖面积。

Conclusion: 自适应测地线共形预测能够有效地提高当响应变量位于黎曼流形上时预测区间的校准度，特别是在存在强烈异方差噪声的情况下，表现优于传统方法。

Abstract: Conformal prediction provides distribution-free coverage guaranties for regression; yet existing methods assume Euclidean output spaces and produce prediction regions that are poorly calibrated when responses lie on Riemannian manifolds. We propose \emph{adaptive geodesic conformal prediction}, a framework that replaces Euclidean residuals with geodesic nonconformity scores and normalizes them by a cross-validated difficulty estimator to handle heteroscedastic noise. The resulting prediction regions, geodesic caps on the sphere, have position-independent area and adapt their size to local prediction difficulty, yielding substantially more uniform conditional coverage than non-adaptive alternatives. In a synthetic sphere experiment with strong heteroscedasticity and a real-world geomagnetic field forecasting task derived from IGRF-14 satellite data, the adaptive method markedly reduces conditional coverage variability and raises worst-case coverage much closer to the nominal level, while coordinate-based baselines waste a large fraction of coverage area due to chart distortion.

</details>


### [30] [AI-CARE: Carbon-Aware Reporting Evaluation Metric for AI Models](https://arxiv.org/abs/2602.16042)
*KC Santosh,Srikanth Baride,Rodrigue Rizk*

Main category: cs.LG

TL;DR: 本文提出了一种名为AI-CARE的评估工具，用于报告机器学习模型的能量消耗和碳排放，并引入了碳-性能折衷曲线来可视化性能与碳成本之间的帕累托前沿。该研究旨在推动透明、多目标的评估方法，使机器学习的发展与全球可持续发展目标保持一致。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习的快速发展，模型训练和推理过程中的环境成本成为社会关注的重点问题。当前基准测试主要集中在如准确率等标准性能指标上，而忽视了能量消耗和碳排放。这种单一目标的评估方式越来越不符合大规模部署的实际需求，特别是在能源受限的环境中。

Method: 提出了AI-CARE评估工具以报告ML模型的能量消耗及碳排放；引入碳-性能折衷曲线作为可视化不同模型在性能与碳成本之间平衡关系的可解释工具；通过理论分析和实证验证展示了碳意识基准测试如何改变模型间的相对排名并鼓励同时具备高精度和环境责任的设计。

Result: 研究表明，考虑碳排放因素后，某些模型的排名发生了变化；同时，使用AI-CARE能够帮助识别出既准确又环保的模型架构。

Conclusion: 本研究倡导采用更加透明且面向多重目标的评价体系，促使机器学习领域的进步更好地服务于全球可持续发展的长远目标。

Abstract: As machine learning (ML) continues its rapid expansion, the environmental cost of model training and inference has become a critical societal concern. Existing benchmarks overwhelmingly focus on standard performance metrics such as accuracy, BLEU, or mAP, while largely ignoring energy consumption and carbon emissions. This single-objective evaluation paradigm is increasingly misaligned with the practical requirements of large-scale deployment, particularly in energy-constrained environments such as mobile devices, developing regions, and climate-aware enterprises. In this paper, we propose AI-CARE, an evaluation tool for reporting energy consumption, and carbon emissions of ML models. In addition, we introduce the carbon-performance tradeoff curve, an interpretable tool that visualizes the Pareto frontier between performance and carbon cost. We demonstrate, through theoretical analysis and empirical validation on representative ML workloads, that carbon-aware benchmarking changes the relative ranking of models and encourages architectures that are simultaneously accurate and environmentally responsible. Our proposal aims to shift the research community toward transparent, multi-objective evaluation and align ML progress with global sustainability goals. The tool and documentation are available at https://github.com/USD-AI-ResearchLab/ai-care.

</details>


### [31] [MoE-Spec: Expert Budgeting for Efficient Speculative Decoding](https://arxiv.org/abs/2602.16052)
*Bradley McDanel,Steven Li,Sruthikesh Surineni,Harshit Khaitan*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoE-Spec的方法，通过在每个层设置固定的专家容量限制来解决混合专家（MoE）模型中推测解码带来的内存压力问题。实验表明该方法相比现有技术可以提高10-30%的吞吐量，并且可以通过调整预算进一步减少延迟。


<details>
  <summary>Details</summary>
Motivation: 对于混合专家（MoE）模型而言，现有的推测解码技术虽然能够加速大型语言模型的推理过程，但同时也引入了严重的瓶颈：大量的草稿树激活了很多独特的专家，显著增加了内存压力，并减少了相对于自回归解码的速度提升。

Method: 提出了MoE-Spec方法，在验证时无需额外训练即可实现专家预算管理。此方法通过在每一层强制执行固定的专家容量限制，仅加载对验证贡献最大的专家，并舍弃那些很少使用但驱动带宽开销的长尾专家，从而将推测深度与内存成本分离。

Result: 实验结果表明，跨多种模型规模和数据集，相比于最新的推测解码基线（如EAGLE-3），所提出的方法能够在保持相似质量的同时提供10-30%更高的吞吐量。此外，还可以通过更严格的预算灵活地以牺牲精度为代价进一步降低延迟。

Conclusion: MoE-Spec作为一种新颖的专家预算管理方案，有效地解决了MoE模型中推测解码导致的内存消耗过高问题，同时实现了性能上的显著改进。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by verifying multiple drafted tokens in parallel. However, for Mixture-of-Experts (MoE) models, this parallelism introduces a severe bottleneck: large draft trees activate many unique experts, significantly increasing memory pressure and diminishing speedups from speculative decoding relative to autoregressive decoding. Prior methods reduce speculation depth when MoE verification becomes expensive. We propose MoE-Spec, a training-free verification-time expert budgeting method that decouples speculation depth from memory cost by enforcing a fixed expert capacity limit at each layer, loading only the experts that contribute most to verification and dropping the long tail of rarely used experts that drive bandwidth overhead. Experiments across multiple model scales and datasets show that this method yields 10--30\% higher throughput than state-of-the-art speculative decoding baselines (EAGLE-3) at comparable quality, with flexibility to trade accuracy for further latency reductions through tighter budgets.

</details>


### [32] [Multi-Objective Alignment of Language Models for Personalized Psychotherapy](https://arxiv.org/abs/2602.16053)
*Mehrab Beikzadeh,Yasaman Asadollah Salmanpour,Ashima Suvarna,Sriram Sankararaman,Matteo Malgaroli,Majid Sarrafzadeh,Saadia Gabriel*

Main category: cs.LG

TL;DR: 研究开发了一种多目标直接偏好优化框架（MODPO），以平衡AI治疗中的患者偏好和临床安全。通过训练六个标准的奖励模型，MODPO在同理心与安全性之间取得了更好的平衡，并且在盲目的临床评估中得到一致偏好。


<details>
  <summary>Details</summary>
Motivation: 由于精神健康障碍影响了全球超过10亿人，但受到专业人员短缺和成本限制的影响，人们获取护理的机会有限。尽管人工智能系统显示出治疗潜力，但当前的方法未能很好地平衡患者的偏好与临床安全性。

Method: 研究团队首先对335名有心理健康经历的人进行了调查，收集了他们在治疗维度上的偏好排名。接着，基于这些数据开发了一个使用直接偏好优化的多目标校准框架，训练了包括同理心、安全性等在内的六个标准的奖励模型，并将多目标方法与单目标优化、监督微调和参数合并等方法进行了系统比较。

Result: 结果表明，多目标直接偏好优化(MODPO)在同理心(77.6%)和安全性(62.6%)方面相比单一目标优化（93.6%同理心, 47.8%安全性）实现了更优的平衡，并且在治疗标准上比通用沟通原则高出17.2%。此外，盲目的临床医生评估确认了MODPO被一致偏好，LLM-评估者一致性与临床医生间可靠性相当。

Conclusion: 这项研究表明，通过采用多目标直接偏好优化方法来调整AI治疗助手，可以在提高治疗效果的同时确保患者的安全性，从而为未来AI在心理健康领域的应用提供了新的方向。

Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.

</details>


### [33] [Extracting and Analyzing Rail Crossing Behavior Signatures from Videos using Tensor Methods](https://arxiv.org/abs/2602.16057)
*Dawon Ahn,Het Patel,Aemal Khattak,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文提出了一种多视图张量分解框架，用于分析铁路交叉口视频中驾驶员的行为模式。通过这种方法，发现位置比一天中的时间更能决定行为模式，并且接近阶段的行为提供了特别具有辨识度的特征。该框架为按行为相似性对位置进行分组以指导有针对性的安全干预措施奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 传统的铁路交叉口安全挑战分析方法仅单独考虑每个交叉口的情况，限制了识别不同地点间共享行为模式的能力。研究旨在开发一种新的方法来捕捉多个铁路交叉口之间在三个不同时间段（接近、等待和清除）内的行为相似性，从而提高安全性。

Method: 研究人员使用TimeSformer嵌入表示每个阶段，并构建了特定于阶段的相似矩阵。随后应用非负对称CP分解技术发现了具有独特时间特征的潜在行为成分。

Result: 研究结果显示，与一天中的时间相比，交叉口的位置似乎是行为模式更强的决定因素；并且接近阶段的行为提供了特别具有区分性的标志。可视化学习到的组件空间证实了基于位置的聚类现象，某些交叉口形成了明显的行为集群。

Conclusion: 所提出的自动化框架能够跨多个交叉口实现可扩展的模式发现，为根据行为相似性对位置进行分组以提供针对性的安全干预措施打下了基础。

Abstract: Railway crossings present complex safety challenges where driver behavior varies by location, time, and conditions. Traditional approaches analyze crossings individually, limiting the ability to identify shared behavioral patterns across locations. We propose a multi-view tensor decomposition framework that captures behavioral similarities across three temporal phases: Approach (warning activation to gate lowering), Waiting (gates down to train passage), and Clearance (train passage to gate raising). We analyze railway crossing videos from multiple locations using TimeSformer embeddings to represent each phase. By constructing phase-specific similarity matrices and applying non-negative symmetric CP decomposition, we discover latent behavioral components with distinct temporal signatures. Our tensor analysis reveals that crossing location appears to be a stronger determinant of behavior patterns than time of day, and that approach-phase behavior provides particularly discriminative signatures. Visualization of the learned component space confirms location-based clustering, with certain crossings forming distinct behavioral clusters. This automated framework enables scalable pattern discovery across multiple crossings, providing a foundation for grouping locations by behavioral similarity to inform targeted safety interventions.

</details>


### [34] [Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training](https://arxiv.org/abs/2602.16065)
*Kevin Wang,Hongqian Niu,Didong Li*

Main category: cs.LG

TL;DR: This paper explores the recursive training of generative AI models, particularly in scenarios where data becomes a mixture of human and AI-generated content. It presents a theoretical framework showing that even with data contamination, recursive training can still converge, marking the first positive result in this area without making specific assumptions about the data distribution. The study also considers the impact of sampling bias on the training process.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to address the challenges arising from the increasing interweaving of human and AI-generated content in web data, which complicates the separation of these two types. As generative AI models are trained on this mixed data, there is a concern over the potential for model collapse due to recursive training. This paper aims to fill the gap in understanding by providing a more general theoretical analysis that does not rely on overly simplified settings or assumptions about the data distribution.

Method: The authors adopt a general framework for their analysis, minimizing assumptions about the real data distribution and allowing the generative model to be a universal approximator. They theoretically examine the convergence of contaminated recursive training processes. Furthermore, they extend their analysis to account for sampling biases in data collection, supporting their findings through empirical studies.

Result: The key finding is that, under the proposed general framework, recursive training with data contamination still converges, with its rate being determined by the minimum between the baseline model's convergence rate and the proportion of real data utilized in each iteration. This represents a pioneering (positive) theoretical insight into recursive training without assuming specific data distributions. Empirical studies further validate these theoretical results.

Conclusion: The conclusion drawn from this work highlights the robustness of recursive training in the presence of data contamination, indicating that it can still achieve convergence under broad conditions. This finding is significant as it provides a foundational understanding for future developments in generative AI, especially concerning the integration of human and machine-generated data, and sets a new direction for exploring the implications of recursive training in more complex, real-world scenarios.

Abstract: Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.

</details>


### [35] [Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research](https://arxiv.org/abs/2602.16072)
*Chenda Duan,Yipeng Zhang,Sotaro Kanai,Yuanyi Ding,Atsuro Daida,Pengyue Yu,Tiancheng Zheng,Naoto Kuroda,Shaun A. Hussain,Eishi Asano,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.LG

TL;DR: 介绍了Omni-iEEG，一个大规模、术前iEEG资源库，包含302名患者和178小时高分辨率记录。该数据集不仅统一了临床元数据格式，还提供了超过36,000个病理事件的专业验证注释，旨在促进癫痫研究的可重复性、通用性和临床转化。


<details>
  <summary>Details</summary>
Motivation: 癫痫影响全球超过5000万人，其中三分之一的患者对药物治疗无效，手术成为控制癫痫发作的最佳选择。然而，精确定位致痫区依赖于颅内脑电图(iEEG)，而当前临床流程受限于耗时的手动审查过程。同时，现有的数据驱动方法通常基于单中心数据集开发，这些数据集在格式和元数据上不一致，缺乏标准化基准，并且很少公开病理事件标注，这给再现性、跨中心验证以及临床相关性带来了障碍。

Method: 通过广泛的努力来协调来自公开来源的不同iEEG格式、元数据及记录，研究团队构建了Omni-iEEG——一个大规模术前iEEG资源库，涵盖302名患者和178小时高分辨率记录。此数据集包括经过认证的临床元数据如发作起始区、切除范围与手术结果等信息；此外，它还提供了超过36,000个由专家验证过的病理事件注释。

Result: Omni-iEEG作为一个连接机器学习与癫痫研究之间的桥梁，定义了具有临床意义的任务及基于临床先验知识的一致评估指标，从而能够在临床相关的设置中系统地评估模型。此外，本文展示了端到端建模在长iEEG片段上的潜力，并强调了从非神经生理学领域预训练表示的可迁移性。

Conclusion: 综上所述，Omni-iEEG为可重复性、泛化能力和临床转化的癫痫研究奠定了基础。

Abstract: Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained by labor-intensive manual review. At the same time, existing data-driven approaches are typically developed on single-center datasets that are inconsistent in format and metadata, lack standardized benchmarks, and rarely release pathological event annotations, creating barriers to reproducibility, cross-center validation, and clinical relevance. With extensive efforts to reconcile heterogeneous iEEG formats, metadata, and recordings across publicly available sources, we present $\textbf{Omni-iEEG}$, a large-scale, pre-surgical iEEG resource comprising $\textbf{302 patients}$ and $\textbf{178 hours}$ of high-resolution recordings. The dataset includes harmonized clinical metadata such as seizure onset zones, resections, and surgical outcomes, all validated by board-certified epileptologists. In addition, Omni-iEEG provides over 36K expert-validated annotations of pathological events, enabling robust biomarker studies. Omni-iEEG serves as a bridge between machine learning and epilepsy research. It defines clinically meaningful tasks with unified evaluation metrics grounded in clinical priors, enabling systematic evaluation of models in clinically relevant settings. Beyond benchmarking, we demonstrate the potential of end-to-end modeling on long iEEG segments and highlight the transferability of representations pretrained on non-neurophysiological domains. Together, these contributions establish Omni-iEEG as a foundation for reproducible, generalizable, and clinically translatable epilepsy research. The project page with dataset and code links is available at omni-ieeg.github.io/omni-ieeg.

</details>


### [36] [Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff](https://arxiv.org/abs/2602.16092)
*Patrick Pynadath,Ruqi Zhang*

Main category: cs.LG

TL;DR: 本文探讨了任意顺序自回归模型（AO-ARMs）中两流注意力机制的作用，提出了一种名为解耦RoPE的方法来分离目标位置信息与内容。研究发现，两流注意力的成功不仅仅在于将位置与内容分开，还在于绕过了任意顺序生成固有的结构-语义权衡问题。


<details>
  <summary>Details</summary>
Motivation: 作者希望深入理解为什么在任意顺序自回归模型（AO-ARMs）中使用两流注意力机制可以提高性能，并探索是否可以通过其他方式达到类似效果而不必完全依赖于这种机制。

Method: 通过引入一种修改版的旋转位置嵌入——解耦RoPE，该方法能够向模型提供目标位置信息而不会泄露目标的内容细节。基于此，研究人员分析了解耦RoPE在不同序列长度下的表现以及它如何影响模型处理结构和语义信息的能力。

Result: 实验表明，在较短的序列上，解耦RoPE能够很好地工作，因为此时语义相关性和结构性接近性是一致的；然而，随着序列变长，两者之间的差异增加时，其性能开始下降。这证明了两流注意力机制成功的关键在于它能有效解决AO-ARMs内在的结构-语义权衡问题。

Conclusion: 研究表明，两流注意力机制在任意顺序生成任务中的优越表现，部分原因在于它能够同时优化对结构最近项的关注和对语义信息的捕捉，而不仅仅是简单地将位置信息与内容分离开来。

Abstract: Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.

</details>


### [37] [Axle Sensor Fusion for Online Continual Wheel Fault Detection in Wayside Railway Monitoring](https://arxiv.org/abs/2602.16101)
*Afonso Lourenço,Francisca Osório,Diogo Risca,Goreti Marreiros*

Main category: cs.LG

TL;DR: 本文提出了一种语义感知的、标签高效的持续学习框架，用于铁路故障诊断。通过变分自编码器对加速度计信号进行编码，并结合AI驱动的光纤布拉格光栅传感器峰值检测提取的语义元数据，增强了在未知操作条件下的异常检测能力。轻量级梯度提升监督分类器与基于回放的持续学习策略相结合，使得模型能够适应不断变化的操作环境而不发生灾难性遗忘。实验表明该模型能有效检测到由于扁平化和多边形化导致的小缺陷，并适应列车类型、速度、负载及轨道轮廓的变化。


<details>
  <summary>Details</summary>
Motivation: 铁路安全维护，尤其是在轮轨接触面，面临着磨损和故障的风险。虽然预测性维护框架越来越多地利用传感器生成的时间序列数据，但传统方法需要手动特征工程，而深度学习模型在线上环境中面对不断演变的操作模式时性能往往会下降。因此，开发一种既能减少人工干预又能适应环境变化的铁路故障诊断系统显得尤为重要。

Method: 研究者们提出了一种结合了变分自编码器（VAE）与语义元数据融合的新方法来解决上述问题。首先使用VAE对加速度计信号进行无监督编码，捕捉正常运行结构；接着，通过人工智能驱动的方法从抗电磁干扰的光纤布拉格光栅传感器中提取包括轴数、车轮索引以及基于应变的变形在内的语义信息，并将其与VAE产生的嵌入相融合；最后采用轻量级梯度增强监督分类器稳定异常评分，并借助基于重播的持续学习策略使模型能够适应领域变化而不丧失先前学到的知识。

Result: 实验结果证明，所提出的模型不仅能够识别出由扁平化和多边形化引起的小型缺陷，还能够在诸如列车种类、行驶速度、载重量以及轨道形态等操作条件发生变化的情况下保持良好的适应性和准确性。

Conclusion: 这项工作展示了一个新颖且有效的铁路故障诊断框架，它通过结合先进的机器学习技术与物理传感数据处理方式，实现了对复杂动态环境下铁路组件健康状态的有效监测。此外，该方案还展示了出色的泛化能力和对新情况的学习能力，为未来智能交通系统的构建提供了有价值的参考。

Abstract: Reliable and cost-effective maintenance is essential for railway safety, particularly at the wheel-rail interface, which is prone to wear and failure. Predictive maintenance frameworks increasingly leverage sensor-generated time-series data, yet traditional methods require manual feature engineering, and deep learning models often degrade in online settings with evolving operational patterns. This work presents a semantic-aware, label-efficient continual learning framework for railway fault diagnostics. Accelerometer signals are encoded via a Variational AutoEncoder into latent representations capturing the normal operational structure in a fully unsupervised manner. Importantly, semantic metadata, including axle counts, wheel indexes, and strain-based deformations, is extracted via AI-driven peak detection on fiber Bragg grating sensors (resistant to electromagnetic interference) and fused with the VAE embeddings, enhancing anomaly detection under unknown operational conditions. A lightweight gradient boosting supervised classifier stabilizes anomaly scoring with minimal labels, while a replay-based continual learning strategy enables adaptation to evolving domains without catastrophic forgetting. Experiments show the model detects minor imperfections due to flats and polygonization, while adapting to evolving operational conditions, such as changes in train type, speed, load, and track profiles, captured using a single accelerometer and strain gauge in wayside monitoring.

</details>


### [38] [On the Power of Source Screening for Learning Shared Feature Extractors](https://arxiv.org/abs/2602.16125)
*Leo,Wang,Connor Mclaughlin,Lili Su*

Main category: cs.LG

TL;DR: 本文探讨了在共享表示学习中如何选择性地利用数据源，提出了一种通过筛选出信息量大的子群体来达到统计最优子空间估计的方法，并通过理论分析和实证评估验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管已知低相关性或质量差的数据源可能会阻碍表示学习，但本研究进一步深入探讨了哪些数据源应该联合学习的问题，特别是针对那些通常被认为具有相似相关性和质量的‘好’数据源集合。

Method: 采用线性设置下共享低维子空间的方法，通过定义信息子群体的概念、开发算法及实用启发式方法来识别这样的子集，以实现即使丢弃大量数据也能达成最小最大优化的目标。

Result: 研究表明，对于广泛的问题实例，仅通过对精心挑选的部分数据源进行训练就足以实现最小最大优化。此发现通过合成数据集与真实世界数据集上的理论分析和实证评估得到了验证。

Conclusion: 源筛选在统计最优子空间估计中起着核心作用，而通过选择性地利用高质量的数据子集，可以有效提高学习效率并达到良好的性能。

Abstract: Learning with shared representation is widely recognized as an effective way to separate commonalities from heterogeneity across various heterogeneous sources. Most existing work includes all related data sources via simultaneously training a common feature extractor and source-specific heads. It is well understood that data sources with low relevance or poor quality may hinder representation learning. In this paper, we further dive into the question of which data sources should be learned jointly by focusing on the traditionally deemed ``good'' collection of sources, in which individual sources have similar relevance and qualities with respect to the true underlying common structure. Towards tractability, we focus on the linear setting where sources share a low-dimensional subspace. We find that source screening can play a central role in statistically optimal subspace estimation. We show that, for a broad class of problem instances, training on a carefully selected subset of sources suffices to achieve minimax optimality, even when a substantial portion of data is discarded. We formalize the notion of an informative subpopulation, develop algorithms and practical heuristics for identifying such subsets, and validate their effectiveness through both theoretical analysis and empirical evaluations on synthetic and real-world datasets.

</details>


### [39] [ASPEN: Spectral-Temporal Fusion for Cross-Subject Brain Decoding](https://arxiv.org/abs/2602.16147)
*Megan Lee,Seung Ha Hwang,Inhyeok Choi,Shreyas Darade,Mengchun Zhang,Kateryna Shapovalenko*

Main category: cs.LG

TL;DR: 研究发现，基于频谱的特征在跨被试脑机接口中比时间信号更稳定。受此启发，提出了ASPEN架构，该架构通过乘法融合结合了频谱和时间特征流。实验表明，ASPEN能够根据不同范式动态达到最佳的频谱-时间平衡，在多个基准数据集中实现了对未见被试的最佳准确率或竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 由于个体间神经信号的差异性，基于EEG的脑机接口在跨被试泛化方面仍面临挑战。研究旨在探索频谱表示是否比时间波形提供更稳定的跨被试转移特性。

Method: 通过分析三种EEG范式（SSVEP、P300和运动想象）中的相关性，引入了一种名为ASPEN的混合架构，该架构利用乘法融合方法整合频谱与时间特征流，要求跨模态一致性以便于特征传播。

Result: 实验跨越六个基准数据集显示，ASPEN能够根据不同的范式动态地实现最优的频谱-时间平衡。在六个数据集中的三个上，ASPEN达到了对于未见过的被试最高的准确度，并在其他数据集上表现出色。

Conclusion: 结果证明，通过乘法多模态融合可以有效地促进跨被试泛化，使得ASPEN在处理不同类型的EEG任务时展现出优越或极具竞争力的表现。

Abstract: Cross-subject generalization in EEG-based brain-computer interfaces (BCIs) remains challenging due to individual variability in neural signals. We investigate whether spectral representations offer more stable features for cross-subject transfer than temporal waveforms. Through correlation analyses across three EEG paradigms (SSVEP, P300, and Motor Imagery), we find that spectral features exhibit consistently higher cross-subject similarity than temporal signals. Motivated by this observation, we introduce ASPEN, a hybrid architecture that combines spectral and temporal feature streams via multiplicative fusion, requiring cross-modal agreement for features to propagate. Experiments across six benchmark datasets reveal that ASPEN is able to dynamically achieve the optimal spectral-temporal balance depending on the paradigm. ASPEN achieves the best unseen-subject accuracy on three of six datasets and competitive performance on others, demonstrating that multiplicative multimodal fusion enables effective cross-subject generalization.

</details>


### [40] [Differentially Private Non-convex Distributionally Robust Optimization](https://arxiv.org/abs/2602.16155)
*Difei Xu,Meng Ding,Zebin Ma,Huanyi Xie,Youming Tao,Aicha Slaitane,Di Wang*

Main category: cs.LG

TL;DR: 本文研究了在差分隐私(DP)保护下的分布鲁棒优化(DRO)，针对有限和DRO与ψ-散度及非凸损失函数的情况，提出了新的DP优化方法DP Double-Spider，并对KL散度特别改进为DP Recursive-Spider方法。实验表明，提出的方法优于现有的DP最小-最大优化方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的应用中经常遇到分布偏移、群体不平衡以及对抗性扰动等问题，在这些问题下传统的经验风险最小化(ERM)框架表现不佳。分布鲁棒优化(DRO)通过在一个分布不确定性集合上优化最坏情况下的预期损失来解决这个问题。同时，由于DRO中的训练数据涉及敏感信息，因此在差分隐私(DP)下防止这些信息泄露是至关重要的。相较于已经广泛研究的DP-ERM，具有不确定约束的最小-最大优化结构的DP-DRO受到了较少的关注。

Method: 首先，对于具有一般ψ-散度的DRO问题，作者将其重新表述为一个最小化问题，并开发了一种名为DP Double-Spider的新颖(ε, δ)-DP优化方法。此外，对于采用KL散度的DP-DRO，作者将问题转换成合成有限和优化问题，并为此开发了DP Recursive-Spider方法。

Result: 在温和假设条件下，DP Double-Spider方法达到了关于梯度范数的效用界$\mathcal{O}(\frac{1}{\sqrt{n}}+ (\frac{\sqrt{d \log (1/δ)}}{n \varepsilon})^{2/3})$；而对于特定于KL散度的DP-DRO，DP Recursive-Spider方法进一步改善了这一效用率至$\mathcal{O}((\frac{\sqrt{d \log(1/δ)}}{n\varepsilon})^{2/3} )$，这与已知的最佳非凸DP-ERM结果相匹配。实验结果显示，所提方法在DP最小-最大优化方面优于现有方法。

Conclusion: 本研究填补了DP-(有限和)-DRO领域的空白，特别是针对ψ-散度和非凸损失的情形提供了全面的研究。新提出的DP优化算法不仅在理论上达到了良好的性能保证，而且实验也验证了它们的有效性。

Abstract: Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.
  Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.
  Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.
  In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.
  To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $ψ$-divergence and non-convex loss.
  First, we study DRO with general $ψ$-divergence by reformulating it as a minimization problem, and develop a novel $(\varepsilon, δ)$-DP optimization method, called DP Double-Spider, tailored to this structure.
  Under mild assumptions, we show that it achieves a utility bound of $\mathcal{O}(\frac{1}{\sqrt{n}}+ (\frac{\sqrt{d \log (1/δ)}}{n \varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.
  We further improve the utility rate for specific divergences.
  In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\mathcal{O}((\frac{\sqrt{d \log(1/δ)}}{n\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.
  Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.

</details>


### [41] [Discrete Stochastic Localization for Non-autoregressive Generation](https://arxiv.org/abs/2602.16169)
*Yunshu Wu,Jiayi Cheng,Partha Thakuria,Rob Brekelmans,Evangelos E. Papalexakis,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出了一种名为DSL（Discrete Stochastic Localization）的方法，通过训练一个SNR不变的去噪器来提高MDLM/ReMDM采样的步骤效率。实验表明，DSL在少量步骤内就能显著提升文本生成的质量，相比现有方法减少了大约4倍的计算量，并且在高预算下达到了自回归模型的质量。


<details>
  <summary>Details</summary>
Motivation: 非自回归生成能够通过并行预测多个标记来减少解码延迟，但迭代精炼过程往往因自我生成草稿而导致错误累积和分布偏移。作者希望通过改进训练方法来提高这类模型的步骤效率，即用更少的迭代次数达到更好的生成效果。

Method: 提出了DSL方法，该方法训练了一个跨多种损坏程度的单一SNR不变去噪器，在同一扩散转换器中连接了中间草稿噪声与掩码样式的端点损坏。

Result: 在OpenWebText数据集上，DSL微调以较少的步骤预算获得了较大的MAUVE得分增长，使用约四分之一的去噪器评估次数超过了MDLM+ReMDM基线，并且在高预算时达到了自回归质量水平。分析显示，该方法提高了自我纠正能力和不确定性校准。

Conclusion: DSL方法成功地提高了MDLM/ReMDM采样的步骤效率，使得在较低的计算成本下也能获得高质量的文本生成结果。

Abstract: Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \(\sim\)4$\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.

</details>


### [42] [Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters](https://arxiv.org/abs/2602.16181)
*Diego Labate,Dipanwita Thakur,Giancarlo Fortino*

Main category: cs.LG

TL;DR: 提出了一种保护隐私的联邦学习框架，用于智能电网中的能源盗窃检测，该框架在保持隐私和效率的同时，实现了竞争性的准确率、精确度、召回率和AUC分数。


<details>
  <summary>Details</summary>
Motivation: 针对智能电网中能源盗窃对稳定性和效率构成的重大威胁，以及传统集中式机器学习方法在数据聚合时引发的隐私和数据安全问题，提出了一个既解决隐私问题又考虑了计算资源限制的解决方案。

Method: 采用轻量级多层感知器（MLP）模型，并通过在本地模型更新前注入高斯噪声来实现基础差分隐私（DP），从而确保正式的隐私保证而不影响学习性能。

Result: 在实际智能电表数据集上进行了评估，无论是IID还是非IID数据分布条件下，所提方法均展示了竞争力的准确性、精度、召回率及AUC得分，同时保持了良好的隐私性和效率。

Conclusion: 本研究提出的解决方案对于下一代智能电网基础设施中的安全能源盗窃检测来说是实用且可扩展的。

Abstract: Energy theft poses a significant threat to the stability and efficiency of smart grids, leading to substantial economic losses and operational challenges. Traditional centralized machine learning approaches for theft detection require aggregating user data, raising serious concerns about privacy and data security. These issues are further exacerbated in smart meter environments, where devices are often resource-constrained and lack the capacity to run heavy models. In this work, we propose a privacy-preserving federated learning framework for energy theft detection that addresses both privacy and computational constraints. Our approach leverages a lightweight multilayer perceptron (MLP) model, suitable for deployment on low-power smart meters, and integrates basic differential privacy (DP) by injecting Gaussian noise into local model updates before aggregation. This ensures formal privacy guarantees without compromising learning performance. We evaluate our framework on a real-world smart meter dataset under both IID and non-IID data distributions. Experimental results demonstrate that our method achieves competitive accuracy, precision, recall, and AUC scores while maintaining privacy and efficiency. This makes the proposed solution practical and scalable for secure energy theft detection in next-generation smart grid infrastructures.

</details>


### [43] [Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting](https://arxiv.org/abs/2602.16188)
*Filippos Bellos,NaveenJohn Premkumar,Yannis Avrithis,Nam H. Nguyen,Jason J. Corso*

Main category: cs.LG

TL;DR: 提出了一种新的时间序列处理方法——Temporal-Prior Conditioning (TPC)，该方法通过在多个层级上引入时间条件，改进了时间信息的处理方式，从而提高了长期预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的时间序列方法通常只在输入层浅层地引入位置或提示信息，这限制了模型随层数增加时对时间信息的有效利用和推理能力。

Method: 通过引入可学习的时间序列标记，并在选定的层中让这些标记与由冻结的语言模型编码得到的紧凑且易读的时间描述符交叉注意，然后通过自注意力机制反馈时间上下文信息，从而实现时间信息与时间序列信号的解耦。

Result: 实验表明，在仅训练交叉注意力模块的情况下，TPC方法能够在多种数据集上取得优于全微调及浅层条件策略的结果，特别是在长期预测方面表现突出。

Conclusion: Temporal-Prior Conditioning (TPC) 方法提供了一种有效提升时间序列分析尤其是长期预测任务性能的新途径，通过深层次整合时间信息而不大幅增加参数量。

Abstract: LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc

</details>


### [44] [Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16196)
*Emile Anand,Richard Hoffmann,Sarah Liaw,Adam Wierman*

Main category: cs.LG

TL;DR: 本文提出了一种名为GMFS的新框架，该框架能够处理大规模异质性多智能体强化学习中的协作问题。通过根据交互强度对κ个智能体进行子采样，实现了图权重平均场的近似，并以多项式(κ)的样本复杂度和O(1/√κ)的最优差距来学习策略。实验结果表明，GMFS在机器人协调方面接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，随着智能体数量增加，联合状态-动作空间呈指数增长，这给大规模智能体群体的协调带来了挑战。虽然均值场方法可以通过聚合智能体间的互动来减轻这一负担，但这些方法通常假设互动是同质化的；而基于图论的方法虽然可以捕捉到异质性，但计算成本随智能体数量增加而变得昂贵。因此，需要一种新的方法来有效处理具有异质性互动的大规模合作型多智能体系统。

Method: 提出了一个名为GMFS（Graphon Mean-Field Subsampling）的新框架，它通过根据交互强度对κ个智能体进行子采样，来近似图权重平均场。此方法允许从减少的样本集中学习策略，同时保持了与全集合理论上相近的学习效果。

Result: 理论分析显示，使用GMFS学习策略的样本复杂度为poly(κ)，且最优差距为O(1/√κ)。此外，通过在机器人协调场景下的数值模拟验证了所提方法的有效性，结果显示GMFS能够达到接近最优的表现。

Conclusion: GMFS提供了一个解决大规模异构多智能体系统协作问题的有效方案，能够在保持较低计算成本的同时，实现接近最优的性能。

Abstract: Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.

</details>


### [45] [Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform](https://arxiv.org/abs/2602.16198)
*Qijie Zhu,Zeqi Ye,Han Liu,Zhaoran Wang,Minshuo Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为DOIT（Doob导向的推理时间转换）的新方法，该方法无需额外训练且计算效率高，适用于一般非可微奖励。通过度量传输公式和Doob的h-变换来实现从预训练生成分布到高奖励目标分布的转变，并提供了收敛性保证。在D4RL离线强化学习基准测试中，该方法在保持采样效率的同时，持续优于最先进的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的适应方法通常将适应目标抽象为一个奖励函数，引导扩散模型生成高奖励样本，但这些方法可能因额外训练而产生高昂的计算开销，或者依赖于对奖励如可微性的严格假设。此外，尽管它们在实践中取得了成功，却很少有理论上的证明和保证。

Method: 提出了DOIT（Doob-Oriented Inference-time Transformation），一种无需训练且计算高效的适应方法，它能够应用于通用的、非可微分的奖励。该方法的核心框架是一种度量传输公式，旨在将预训练生成分布转换为目标高奖励分布。利用Doob的$h$-变换实现了这一传输过程，从而在不修改预训练模型的情况下对扩散采样过程进行动态修正，并支持基于模拟的有效计算。

Result: 理论层面，通过对动态Doob修正中的近似误差进行刻画，建立了到达目标高奖励分布的高概率收敛保障。实验方面，在D4RL离线强化学习基准测试中，所提方法不仅一致优于当前最先进基线方法的表现，同时还维持了良好的采样效率。

Conclusion: DOIT提供了一种新的、有效的方法来调整预训练扩散模型以适应各种应用需求，特别是在面对非可微奖励时展现出了显著优势。

Abstract: Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.

</details>


### [46] [Linked Data Classification using Neurochaos Learning](https://arxiv.org/abs/2602.16204)
*Pooja Honna,Ayush Patravali,Nithin Nagaraj,Nanjangud C. Narendra*

Main category: cs.LG

TL;DR: 本研究将神经混沌学习（NL）应用于链接数据，特别是知识图谱形式的数据。通过在知识图谱上实现节点聚合，并将聚合后的节点特征输入到最简单的NL架构ChaosNet中。实验结果表明，该方法在同质图上的效果优于异质图。


<details>
  <summary>Details</summary>
Motivation: 先前的工作已经证明了神经混沌学习（NL）在分类和回归任务中的优越性能。然而，NL尚未被应用于链接数据，如知识图谱。因此，本研究旨在探索NL在处理这种类型的数据时的潜力。

Method: 研究者们首先在知识图谱上实现了节点聚合技术，然后将聚合得到的节点特征输入到NL的一个简单模型ChaosNet中。通过对同质性和不同程度异质性的图数据集进行测试来评估方法的有效性。

Result: 结果显示，在处理同质图时，所提出的方法比处理高度异质图时表现得更好。

Conclusion: 这项研究表明，神经混沌学习可以有效地应用于某些类型的链接数据，尤其是在同质图中。对于未来工作，提出了进一步改进和扩展该方法的方向。

Abstract: Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements. In prior work, NL has been implemented and extensively tested on separable and time series data, and demonstrated its superior performance on both classification and regression tasks. In this paper, we investigate the next step in NL, viz., applying NL to linked data, in particular, data that is represented in the form of knowledge graphs. We integrate linked data into NL by implementing node aggregation on knowledge graphs, and then feeding the aggregated node features to the simplest NL architecture: ChaosNet. We demonstrate the results of our implementation on homophilic graph datasets as well as heterophilic graph datasets of verying heterophily. We show better efficacy of our approach on homophilic graphs than on heterophilic graphs. While doing so, we also present our analysis of the results, as well as suggestions for future work.

</details>


### [47] [Geometric Neural Operators via Lie Group-Constrained Latent Dynamics](https://arxiv.org/abs/2602.16209)
*Jiaquan Zhang,Fachrina Dewi Puspitasari,Songbo Zhang,Yibei Liu,Kuien Liu,Caiyan Qin,Fan Mo,Peng Wang,Yang Yang,Chaoning Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于低秩李代数参数化的流形约束方法（MCL），以解决现有神经算子在多层迭代和长期滚动预测中的不稳定性问题。通过在潜在表示上执行群作用更新，MCL作为即插即用模块增强了现有神经算子的几何归纳偏置，从而显著降低了相对预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有的神经算子在处理多层迭代和长期滚动预测时表现出不稳定性，这主要是因为它们在欧几里得潜在空间中的更新不受约束，违反了几何和守恒定律。为了解决这一挑战，作者提出了基于低秩李代数参数化的流形约束方法。

Method: 提出的Manifold Constraining based on Lie group (MCL) 方法，通过低秩李代数参数化来约束流形，并对潜在表示执行群作用更新。该方法设计为一个高效的即插即用模块，旨在向现有神经算子中引入几何归纳偏置。

Result: 通过对包括1-D Burgers方程和2-D Navier-Stokes方程在内的多种偏微分方程进行广泛的实验，在不同参数范围和步长下，MCL方法能够有效降低30-50%的相对预测误差，同时仅增加2.26%的参数量。

Conclusion: 研究表明，MCL方法提供了一个可扩展的解决方案，通过解决神经算子更新过程中缺乏的基本几何约束问题，提高了长期预测的准确性。

Abstract: Neural operators offer an effective framework for learning solutions of partial differential equations for many physical systems in a resolution-invariant and data-driven manner. Existing neural operators, however, often suffer from instability in multi-layer iteration and long-horizon rollout, which stems from the unconstrained Euclidean latent space updates that violate the geometric and conservation laws. To address this challenge, we propose to constrain manifolds with low-rank Lie algebra parameterization that performs group action updates on the latent representation. Our method, termed Manifold Constraining based on Lie group (MCL), acts as an efficient \emph{plug-and-play} module that enforces geometric inductive bias to existing neural operators. Extensive experiments on various partial differential equations, such as 1-D Burgers and 2-D Navier-Stokes, over a wide range of parameters and steps demonstrate that our method effectively lowers the relative prediction error by 30-50\% at the cost of 2.26\% of parameter increase. The results show that our approach provides a scalable solution for improving long-term prediction fidelity by addressing the principled geometric constraints absent in the neural operator updates.

</details>


### [48] [UCTECG-Net: Uncertainty-aware Convolution Transformer ECG Network for Arrhythmia Detection](https://arxiv.org/abs/2602.16216)
*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.LG

TL;DR: 本文提出了一种名为UCTECG-Net的不确定性感知混合架构，该架构结合了一维卷积和Transformer编码器来共同处理原始心电图信号及其频谱图。在MIT-BIH心律失常和PTB诊断数据集上进行评估时，UCTECG-Net在准确率、精确度、召回率和F1分数方面优于LSTM、CNN1D和Transformer基线模型，并且通过集成三种不确定性量化方法提供了更可靠的心电图决策支持基础。


<details>
  <summary>Details</summary>
Motivation: 深度学习改进了自动心电图分类，但由于对其预测可靠性的了解有限，阻碍了它在安全关键环境中的应用。

Method: 提出了UCTECG-Net，一种结合了一维卷积与Transformer编码器的不确定性感知混合架构，用于同时处理原始心电图信号及其频谱图。此外，还整合了三种不确定性量化方法（蒙特卡洛Dropout、深度集成和集成蒙特卡洛Dropout）到所有模型中，并使用不确定性意识混淆矩阵及衍生指标分析其行为。

Result: UCTECG-Net在MIT-BIH和PTB数据集上的表现超过了LSTM、CNN1D和Transformer基线模型，在准确率、精确度、召回率和F1分数方面均有显著提升。特别是在集成或EMCD的帮助下，UCTECG-Net提供的不确定性估计比其他竞争架构更加可靠且一致。

Conclusion: UCTECG-Net不仅在性能上超越了现有的心电图分类方法，而且通过集成不确定性量化技术为风险意识的心电图决策支持提供了更强的基础。

Abstract: Deep learning has improved automated electrocardiogram (ECG) classification, but limited insight into prediction reliability hinders its use in safety-critical settings. This paper proposes UCTECG-Net, an uncertainty-aware hybrid architecture that combines one-dimensional convolutions and Transformer encoders to process raw ECG signals and their spectrograms jointly. Evaluated on the MIT-BIH Arrhythmia and PTB Diagnostic datasets, UCTECG-Net outperforms LSTM, CNN1D, and Transformer baselines in terms of accuracy, precision, recall and F1 score, achieving up to 98.58% accuracy on MIT-BIH and 99.14% on PTB. To assess predictive reliability, we integrate three uncertainty quantification methods (Monte Carlo Dropout, Deep Ensembles, and Ensemble Monte Carlo Dropout) into all models and analyze their behavior using an uncertainty-aware confusion matrix and derived metrics. The results show that UCTECG-Net, particularly with Ensemble or EMCD, provides more reliable and better-aligned uncertainty estimates than competing architectures, offering a stronger basis for risk-aware ECG decision support.

</details>


### [49] [Multi-Class Boundary Extraction from Implicit Representations](https://arxiv.org/abs/2602.16217)
*Jash Vira,Andrew Myers,Simon Ratcliffe*

Main category: cs.LG

TL;DR: 本文提出了一种新的2D边界提取算法，用于处理多类情况下的隐式表示，该算法注重拓扑一致性且无孔洞，并通过地质建模数据验证了其适应性和处理复杂拓扑的能力。


<details>
  <summary>Details</summary>
Motivation: 目前还没有从保证拓扑正确性和没有孔洞的多类隐式表示中提取表面的方法。为了解决这个问题，作者们着手奠定了基础工作。

Method: 介绍了一种针对多类情况的2D边界提取算法，该算法强调拓扑一致性和水密性，同时允许对逼近设定最小细节限制。

Result: 通过地质建模数据评估了算法，展示了它在面对复杂拓扑时的适应性和能力。

Conclusion: 提出的新算法能够有效处理多类别的隐式表示，并且在保持拓扑一致性和避免孔洞方面表现出色。

Abstract: Surface extraction from implicit neural representations modelling a single class surface is a well-known task. However, there exist no surface extraction methods from an implicit representation of multiple classes that guarantee topological correctness and no holes. In this work, we lay the groundwork by introducing a 2D boundary extraction algorithm for the multi-class case focusing on topological consistency and water-tightness, which also allows for setting minimum detail restraint on the approximation. Finally, we evaluate our algorithm using geological modelling data, showcasing its adaptiveness and ability to honour complex topology.

</details>


### [50] [Bayesian Quadrature: Gaussian Processes for Integration](https://arxiv.org/abs/2602.16218)
*Maren Mahsereci,Toni Karvonen*

Main category: cs.LG

TL;DR: 本综述文章系统地介绍了贝叶斯求积方法的数学基础，提出了一个分类体系来归类不同的贝叶斯求积方法，并探讨了模型、推断和抽样三个轴上的不同选择对数值结果的影响。同时，还提供了一个几乎详尽的参考文献列表，涵盖了使用贝叶斯求积或等效方法的所有数学和工程领域。


<details>
  <summary>Details</summary>
Motivation: 尽管贝叶斯求积自20世纪80年代就已经流行起来，但至今没有一份系统而全面的处理该主题的文献。本文旨在填补这一空白，通过回顾贝叶斯求积的数学基础、提出分类法、收集理论保证以及进行控制性数值研究，为读者提供一个全面的理解框架。

Method: 文章采用了一种基于模型的方法来估计难以计算的积分或期望值。它从多个视角审视贝叶斯求积的数学根基；建立了一个包含建模、推理与抽样三大维度的系统分类学；并汇总了普遍适用的理论保障。此外，还进行了受控的数值研究以探索说明分类学各轴上不同选择的效果。

Result: 研究表明，在贝叶斯求积方法中对于模型、推理及抽样策略的不同选择会对最终结果产生显著影响。同时，也指出了实际应用中面临的挑战与限制。

Conclusion: 本文提供了关于贝叶斯求积方法的一个全面概述，包括其理论背景、实践指南及其在各个领域的应用情况。此外，还给出了一个最新的、几乎覆盖所有相关工作的参考书目。

Abstract: Bayesian quadrature is a probabilistic, model-based approach to numerical integration, the estimation of intractable integrals, or expectations. Although Bayesian quadrature was popularised already in the 1980s, no systematic and comprehensive treatment has been published. The purpose of this survey is to fill this gap. We review the mathematical foundations of Bayesian quadrature from different points of view; present a systematic taxonomy for classifying different Bayesian quadrature methods along the three axes of modelling, inference, and sampling; collect general theoretical guarantees; and provide a controlled numerical study that explores and illustrates the effect of different choices along the axes of the taxonomy. We also provide a realistic assessment of practical challenges and limitations to application of Bayesian quadrature methods and include an up-to-date and nearly exhaustive bibliography that covers not only machine learning and statistics literature but all areas of mathematics and engineering in which Bayesian quadrature or equivalent methods have seen use.

</details>


### [51] [Amortized Predictability-aware Training Framework for Time Series Forecasting and Classification](https://arxiv.org/abs/2602.16224)
*Xu Zhang,Peng Wang,Yichen Li,Wei Wang*

Main category: cs.LG

TL;DR: 提出了一种新的训练框架APTF，通过引入层级可预测性损失（HPL）和摊销模型来识别并惩罚低可预测性样本，从而提高时间序列分析任务的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据容易受到噪声的影响，并且训练样本可能包含偏离正常数据分布的低可预测性模式，导致训练不稳定或收敛到不良局部最小值。为了减轻低可预测性样本对时间序列分析任务（如时间序列预测TSF和时间序列分类TSC）的负面影响，提出了一个通用的摊销可预测性意识训练框架APTF。

Method: APTF框架包括两个关键设计：(i) 层级可预测性损失（HPL），能够动态地识别出低可预测性样本，并随着训练过程逐渐增加它们的损失惩罚；(ii) 一个摊销模型，用于减少由于模型偏差引起的可预测性估计误差，进一步增强HPL的有效性。

Result: 实验结果表明，APTF可以在时间序列预测(TSF)和时间序列分类(TSC)任务中有效提升模型表现。

Conclusion: APTF提供了一个有效的解决方案，通过特别处理低可预测性样本以改进深度学习模型在时间序列分析任务中的训练效果。

Abstract: Time series data are prone to noise in various domains, and training samples may contain low-predictability patterns that deviate from the normal data distribution, leading to training instability or convergence to poor local minima. Therefore, mitigating the adverse effects of low-predictability samples is crucial for time series analysis tasks such as time series forecasting (TSF) and time series classification (TSC). While many deep learning models have achieved promising performance, few consider how to identify and penalize low-predictability samples to improve model performance from the training perspective. To fill this gap, we propose a general Amortized Predictability-aware Training Framework (APTF) for both TSF and TSC. APTF introduces two key designs that enable the model to focus on high-predictability samples while still learning appropriately from low-predictability ones: (i) a Hierarchical Predictability-aware Loss (HPL) that dynamically identifies low-predictability samples and progressively expands their loss penalty as training evolves, and (ii) an amortization model that mitigates predictability estimation errors caused by model bias, further enhancing HPL's effectiveness. The code is available at https://github.com/Meteor-Stars/APTF.

</details>


### [52] [Factored Latent Action World Models](https://arxiv.org/abs/2602.16229)
*Zizhao Wang,Chang Shi,Jiaheng Hu,Kevin Rohling,Roberto Martín-Martín,Amy Zhang,Peter Stone*

Main category: cs.LG

TL;DR: 本文提出了一种新的分解动力学框架，称为Factored Latent Action Model (FLAM)，它能够将场景分解为独立的因素，每个因素推断自己的潜在动作并预测其下一步的因子值。实验表明，与整体模型相比，FLAM在复杂的多实体动态建模中提供了更高的准确性和更好的视频生成质量，并且有助于下游策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有的方法大多依赖于单一的逆向和正向动力学模型来学习控制整个场景的单一潜在动作，在多个实体同时作用的复杂环境中表现不佳。为了更准确地建模这种复杂的多实体动态，并提高无动作视频设置下的视频生成质量，提出了Factored Latent Action Model (FLAM)。

Method: 通过引入Factored Latent Action Model (FLAM)，一种能够将场景分解成独立因素的新框架，其中每个因素都能推断出自身的潜在行动并预测下一步的因子值。该方法旨在改善对于复杂多实体环境中的动态建模准确性及视频生成的质量。

Result: 基于模拟和真实世界多实体数据集的实验结果表明，FLAM在预测精度和表示质量方面优于先前的工作，并促进了后续策略的学习。

Conclusion: Factored Latent Action Model (FLAM) 提供了一种有效的途径来改进复杂多实体环境下视频生成的质量以及提升策略学习效果，显示出分解式潜在动作模型的优势。

Abstract: Learning latent actions from action-free video has emerged as a powerful paradigm for scaling up controllable world model learning. Latent actions provide a natural interface for users to iteratively generate and manipulate videos. However, most existing approaches rely on monolithic inverse and forward dynamics models that learn a single latent action to control the entire scene, and therefore struggle in complex environments where multiple entities act simultaneously. This paper introduces Factored Latent Action Model (FLAM), a factored dynamics framework that decomposes the scene into independent factors, each inferring its own latent action and predicting its own next-step factor value. This factorized structure enables more accurate modeling of complex multi-entity dynamics and improves video generation quality in action-free video settings compared to monolithic models. Based on experiments on both simulation and real-world multi-entity datasets, we find that FLAM outperforms prior work in prediction accuracy and representation quality, and facilitates downstream policy learning, demonstrating the benefits of factorized latent action models.

</details>


### [53] [Online Prediction of Stochastic Sequences with High Probability Regret Bounds](https://arxiv.org/abs/2602.16236)
*Matthias Frey,Jonathan H. Manton,Jingge Zhu*

Main category: cs.LG

TL;DR: 本研究重新审视了在已知有限时间范围T的情况下随机序列的通用预测经典问题，提出了高概率下成立的消失后悔界，并证明了在不增加额外假设情况下无法改进δ的指数。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索是否可以导出高概率下成立的消失后悔界，以补充现有文献中仅在期望意义下成立的界限。

Method: 通过理论分析方法，对随机过程特别是可数字母表上的过程进行分析，推导出新的高概率后悔界。

Result: 得出了一个收敛率形式为$\mathcal{O}(T^{-1/2} δ^{-1/2})$的新高概率界限，同时证明了在相同形式的界限内不可能改进$δ$的指数而无需做出更多假设。

Conclusion: 研究表明，对于随机序列的通用预测，在给定的时间范围内确实存在高概率下成立的消失后悔界，但δ的指数改进需要额外的假设条件。

Abstract: We revisit the classical problem of universal prediction of stochastic sequences with a finite time horizon $T$ known to the learner. The question we investigate is whether it is possible to derive vanishing regret bounds that hold with high probability, complementing existing bounds from the literature that hold in expectation. We propose such high-probability bounds which have a very similar form as the prior expectation bounds. For the case of universal prediction of a stochastic process over a countable alphabet, our bound states a convergence rate of $\mathcal{O}(T^{-1/2} δ^{-1/2})$ with probability as least $1-δ$ compared to prior known in-expectation bounds of the order $\mathcal{O}(T^{-1/2})$. We also propose an impossibility result which proves that it is not possible to improve the exponent of $δ$ in a bound of the same form without making additional assumptions.

</details>


### [54] [Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge](https://arxiv.org/abs/2602.16264)
*Zixian Wu,Xuebao Li,Yanfang Zheng,Rui Wang,Shunhuang Zhang,Jinfang Wei,Yongshang Lv,Liang Dong,Zamri Zainal Abidin,Noraisyah Mohamed Shah,Hongwei Ye,Pengchao Yan,Xuefeng Li,Xiaojia Ji,Xusheng Huang,Xiaotian Wang,Honglei Jin*

Main category: cs.LG

TL;DR: This paper introduces a new supervised classification framework with class-dependent rewards for predicting solar flares, demonstrating that the CDR-Transformer model outperforms traditional deep learning models and even NASA's system.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and reliability of solar flare prediction, specifically for $\geq$MM flares within a 24-hour window, by introducing a novel approach that incorporates class-dependent rewards into the predictive modeling process.

Method: A supervised classification framework with class-dependent rewards (CDR) was developed to predict $\geq$MM flares. Multiple datasets were constructed, and three deep learning models (CNN, CNN-BiLSTM, Transformer) alongside their CDR counterparts were applied. Performance was compared across different feature sets and reward engineering for CDR models was analyzed. SHAP method was used for model interpretability.

Result: (1) R_VALUE and AREA_ACR are the most effective among LOS feature combinations. (2) Combined LOS and vector magnetic field data enhance the performance of the Transformer model. (3) Knowledge-informed features lead to better performance than magnetograms. (4) While CNN and CNN-BiLSTM outperform their CDR versions on magnetogram data, the CDR-Transformer excels when knowledge-informed features are used. (5) CDR models' performance is robust to changes in reward settings. (6) TOTUSJH is more significant in CDR models, whereas R_VALUE is prioritized by the Transformer. (7) CDR-Transformer surpasses NASA/CCMC in predictive capabilities under the same conditions.

Conclusion: The CDR-Transformer model outperformed all other models, including those from NASA/CCMC, in predicting $\geq$MM flares within 24 hours. The analysis also highlighted the importance of specific features and the benefits of using combined LOS and vector magnetic field data.

Abstract: In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results. (2)Transformer achieves better performance with combined LOS and vector magnetic field data than with either alone. (3)Models using knowledge-informed features outperform those using magnetograms. (4)While CNN and CNN-BiLSTM outperform their CDR counterparts on magnetograms, CDR-Transformer is slightly superior to its deep learning counterpart when using knowledge-informed features. Among all models, CDR-Transformer achieves the best performance. (5)The predictive performance of the CDR models is not overly sensitive to the reward choices.(6)Through SHAP analysis, the CDR model tends to regard TOTUSJH as more important, while the Transformer tends to prioritize R_VALUE more.(7)Under identical prediction time and active region (AR) number, the CDR-Transformer shows superior predictive capabilities compared to NASA/CCMC.

</details>


### [55] [Regret and Sample Complexity of Online Q-Learning via Concentration of Stochastic Approximation with Time-Inhomogeneous Markov Chains](https://arxiv.org/abs/2602.16274)
*Rahul Singh,Siddharth Chandak,Eric Moulines,Vivek S. Borkar,Nicholas Bambos*

Main category: cs.LG

TL;DR: 本研究首次为无限期折扣马尔可夫决策过程中的经典在线Q学习提供了高概率遗憾界，无需依赖乐观性或奖励项。通过分析具有衰减温度的玻尔兹曼Q学习，发现其遗憾值与MDP的次优性差距密切相关。为解决这一局限性，研究引入了一种平滑$ε_n$-贪婪探索方案，结合了$ε_n$-贪婪和玻尔兹曼探索方法，并证明了接近$	ilde{O}(N^{9/10})$的差距鲁棒遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于克服现有方法对乐观性或奖励项依赖的问题，以及改善当次优性差距较小时在线Q学习表现不佳的情况。

Method: 首先采用具有衰减温度参数的玻尔兹曼Q学习进行分析；接着提出一种新的探索策略——平滑$ε_n$-贪婪探索方案，该方案综合了$ε_n$-贪婪与玻尔兹曼探索的优点；最后开发了一种针对迭代和时间相关转移动态的收缩马尔可夫随机逼近的高概率集中边界理论。

Result: 研究结果表明，对于足够大的次优性差距，玻尔兹曼Q学习能够实现亚线性的遗憾增长；而对于较小的差距，则可能退化至近似线性增长。新提出的平滑$ε_n$-贪婪探索方案在各种情况下均能保证近$	ilde{O}(N^{9/10})$的遗憾上界。此外，还得到了关于马尔可夫随机逼近的一个新的高概率集中不等式。

Conclusion: 这项工作不仅为在线Q学习提供了一个新的理论框架，而且还引入了一种有效处理不同次优性差距情况下的探索策略。

Abstract: We present the first high-probability regret bound for classical online Q-learning in infinite-horizon discounted Markov decision processes, without relying on optimism or bonus terms. We first analyze Boltzmann Q-learning with decaying temperature and show that its regret depends critically on the suboptimality gap of the MDP: for sufficiently large gaps, the regret is sublinear, while for small gaps it deteriorates and can approach linear growth. To address this limitation, we study a Smoothed $ε_n$-Greedy exploration scheme that combines $ε_n$-greedy and Boltzmann exploration, for which we prove a gap-robust regret bound of near-$\tilde{O}(N^{9/10})$. To analyze these algorithms, we develop a high-probability concentration bound for contractive Markovian stochastic approximation with iterate- and time-dependent transition dynamics. This bound may be of independent interest as the contraction factor in our bound is governed by the mixing time and is allowed to converge to one asymptotically.

</details>


### [56] [Fast KV Compaction via Attention Matching](https://arxiv.org/abs/2602.16284)
*Adam Zweiger,Xinghong Fu,Han Guo,Yoon Kim*

Main category: cs.LG

TL;DR: 本文提出了一种通过注意力匹配在潜在空间中快速压缩上下文的方法，该方法能够在保持性能的同时显著减少键值缓存的大小，并且开发了一系列方法，在压缩时间和质量之间取得了显著的平衡。


<details>
  <summary>Details</summary>
Motivation: 在长上下文场景下，语言模型的键值缓存大小成为扩展瓶颈。现有的通过摘要在标记空间内进行压缩的方法可能会导致大量的信息丢失，从而严重影响下游任务的表现。尽管Cartridges展示了在潜在空间训练紧凑键值缓存的可能性，但其端到端优化过程既慢又昂贵。因此，需要一种更有效率的方法来解决这个问题。

Method: 本文介绍了一种称为“注意力匹配”的方法，它能够构造紧凑的键和值以复制注意力输出，并在每个KV头级别上保留注意力质量。这种方法可以自然地分解成一些简单的子问题，其中一部分子问题有高效的闭式解。基于此框架，研究人员开发了一系列方法，旨在大幅度推进压缩时间与质量之间的帕累托前沿。

Result: 研究结果表明，所提出的方法可以在某些数据集上达到高达50倍的压缩率，同时几乎不损失质量，这标志着在压缩效率和效果方面取得了重要进展。

Conclusion: 本文提出的注意力匹配技术为长上下文的语言模型提供了一个高效且有效的上下文压缩方案，不仅极大地减少了键值缓存的存储需求，还保证了模型性能几乎不受影响。

Abstract: Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.

</details>


### [57] [The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks](https://arxiv.org/abs/2602.16340)
*Eitan Gronich,Gal Vardi*

Main category: cs.LG

TL;DR: 本文研究了基于动量的优化器在同质模型上的隐式偏置，扩展了最速下降法在同质模型中的已有结果，并证明了几种动量最速下降算法在学习率衰减计划下倾向于KKT点。


<details>
  <summary>Details</summary>
Motivation: 作者想要探索基于动量的优化器在同质模型中所表现出的隐式偏置，并理解这些优化器如何影响模型对不同范数边缘最大化问题的解的选择。

Method: 首先将最速下降法在同质模型中的已有结果拓展到了带可选学习率计划的标准化最速下降法；然后通过理论分析证明，在平滑同质模型上，几种动量最速下降算法（如Muon、MomentumGD和Signum）在衰减学习率计划下近似于最速下降轨迹，从而表明这些算法也偏向于对应的边缘最大化问题的KKT点。此外，还对Adam（不包含稳定性常数）、Muon-Signum和Muon-Adam进行了分析。

Result: 实验证据支持了理论分析，表明优化器的选择决定了最大化的边缘类型。

Conclusion: 这项工作扩展了之前关于同质模型中最速下降以及线性模型中基于动量的优化器的研究成果，揭示了动量优化方法对于特定边缘最大化问题解的影响。

Abstract: We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ($\ell_2$ norm), and Signum ($\ell_\infty$ norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the $\ell_\infty$ margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models.

</details>


### [58] [Explainability for Fault Detection System in Chemical Processes](https://arxiv.org/abs/2602.16341)
*Georgios Gravanis,Dimitrios Kyriakou,Spyros Voutetakis,Simira Papadopoulou,Konstantinos Diamantaras*

Main category: cs.LG

TL;DR: 本研究应用并比较了两种最新的可解释性人工智能（XAI）方法，即综合梯度法(IG)和SHapley加性解释(SHAP)，用于解释一个高度准确的长短期记忆(LSTM)分类器在田纳西伊士曼过程(TEP)中故障诊断决策。结果显示这两种方法都能帮助识别故障发生的子系统，并且SHAP方法在某些情况下似乎提供了更接近故障根源的信息。


<details>
  <summary>Details</summary>
Motivation: 为了提高对复杂化学过程故障诊断的理解，特别是通过使用先进的机器学习模型如LSTM时，需要能够清楚地解释这些模型做出特定决策的原因。

Method: 采用集成梯度法(IG)与SHapley加性解释(SHAP)两种XAI技术来分析LSTM分类器在田纳西伊士曼过程(TEP)中的故障诊断决定。

Result: 发现XAI方法确实有助于定位故障发生的子系统；对于大多数情况而言，两种方法都指出了相同的特征作为最重要的决策因素；但在某些情形下，SHAP方法表现得更为信息丰富，更接近于故障的根本原因。

Conclusion: 这项研究表明，XAI方法可以有效地辅助理解基于LSTM等复杂模型进行故障诊断背后的原因，而且由于这些XAI技术是模型无关的，因此该方法具有广泛的应用潜力。

Abstract: In this work, we apply and compare two state-of-the-art eXplainability Artificial Intelligence (XAI) methods, the Integrated Gradients (IG) and the SHapley Additive exPlanations (SHAP), that explain the fault diagnosis decisions of a highly accurate Long Short-Time Memory (LSTM) classifier. The classifier is trained to detect faults in a benchmark non-linear chemical process, the Tennessee Eastman Process (TEP). It is highlighted how XAI methods can help identify the subsystem of the process where the fault occurred. Using our knowledge of the process, we note that in most cases the same features are indicated as the most important for the decision, while insome cases the SHAP method seems to be more informative and closer to the root cause of the fault. Finally, since the used XAI methods are model-agnostic, the proposed approach is not limited to the specific process and can also be used in similar problems.

</details>


### [59] [Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks](https://arxiv.org/abs/2602.16357)
*Sarkis Ter Martirosyan,Xinyue Huang,David Qin,Anthony Yu,Stanislav Emelianov*

Main category: cs.LG

TL;DR: 本研究介绍了一种新的方法SPOI-AE，用于解决光谱光声成像中的光学反演和光谱分解问题，无需假设线性关系。实验表明，该方法在重建输入图像像素及估计生物参数方面优于传统算法，并通过模拟小鼠淋巴结的验证了其分解准确性。


<details>
  <summary>Details</summary>
Motivation: 由于光谱光声(sPA)成像中存在非线性和不适定性问题，使得准确估计色团相对浓度变得非常困难。为了解决这些问题并提高sPA成像中色团浓度估计的准确性，提出了SPOI-AE方法。

Method: 研究者开发了一个名为SPOI-AE（光谱光声光学反演自动编码器）的新模型，旨在不依赖线性假设的情况下处理sPA光学反演与光谱分解问题。SPOI-AE利用活体小鼠淋巴结的sPA图像进行训练和测试，这些图像没有已知的真实色团浓度数据作为参考。

Result: 结果表明，相比于传统算法，SPOI-AE能够更好地重建输入sPA图像像素，并提供更为合理的生物学参数估计值，包括光学参数、色团浓度以及组织氧饱和度百分比。此外，使用模拟的小鼠淋巴结来验证了SPOI-AE在光谱分解上的准确性。

Conclusion: SPOI-AE展示了在光谱光声成像领域内解决光学反演和光谱分解挑战的有效性，不仅提高了图像重建质量，还提供了可靠的生物信息估计，为理解生理过程提供了丰富的结构、功能及分子层面的信息。

Abstract: Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.

</details>


### [60] [Easy Data Unlearning Bench](https://arxiv.org/abs/2602.16400)
*Roy Rinberg,Pol Puigdemont,Martin Pawelczyk,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文提出了一种统一且可扩展的基准测试套件，简化了使用KLoM（边缘的KL散度）指标对遗忘算法的评估。通过标准化设置和度量标准，该框架促进了不同遗忘方法之间可重复、可扩展和公平的比较，并为加速研究和推广机器遗忘的最佳实践提供了实用基础。


<details>
  <summary>Details</summary>
Motivation: 评估机器遗忘方法在技术上具有挑战性，最近的基准测试需要复杂的设置和大量的工程开销。为了克服这些难题，作者们开发了一个新的基准测试工具包，旨在简化评价过程并促进研究发展。

Method: 作者们引入了一个统一且可扩展的基准测试套件，它利用KLoM（边际上的KL散度）度量来简化对遗忘算法的评估。此套件提供了预计算模型集成、oracle输出以及简化的基础设施以实现即插即用式的评估。

Result: 通过提供一个标准化的评估平台，该工作能够支持不同遗忘方法之间的可重复性、可扩展性和公平对比。此外，公开可用的代码与数据进一步增强了其实用价值。

Conclusion: 该研究旨在通过创建一个简化的、标准化的评估框架来加速机器遗忘领域的研究进展，并鼓励最佳实践的应用。

Abstract: Evaluating machine unlearning methods remains technically challenging, with recent benchmarks requiring complex setups and significant engineering overhead. We introduce a unified and extensible benchmarking suite that simplifies the evaluation of unlearning algorithms using the KLoM (KL divergence of Margins) metric. Our framework provides precomputed model ensembles, oracle outputs, and streamlined infrastructure for running evaluations out of the box. By standardizing setup and metrics, it enables reproducible, scalable, and fair comparison across unlearning methods. We aim for this benchmark to serve as a practical foundation for accelerating research and promoting best practices in machine unlearning. Our code and data are publicly available.

</details>


### [61] [Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent](https://arxiv.org/abs/2602.16436)
*Jean Dufraiche,Paul Mangold,Michaël Perrot,Marc Tommasi*

Main category: cs.LG

TL;DR: 本文提出了一种基于Weierstrass变换的方法来纠正局部差分隐私（LDP）下发布的数据在二分类任务中的偏差，并开发了一个名为IWP-SGD的新随机梯度下降算法，该算法可以以O(1/n)的速率收敛到真实的风险最小化。


<details>
  <summary>Details</summary>
Motivation: 一次性释放数据时采用非交互式本地差分隐私(LDP)虽然能够完全重用数据，但由此产生的噪声可能给后续分析带来偏差。本研究旨在解决这一问题，通过利用Weierstrass变换来描述这种偏差并提出相应的修正方法。

Method: 研究人员首先运用Weierstrass变换来表征在二分类情况下由LDP引入的数据偏差；接着证明了通过对该变换求逆可以获得一种偏误校正方法，用于计算对非线性函数无偏估计；最后基于此开发了一种新的随机梯度下降算法——Inverse Weierstrass Private SGD (IWP-SGD)，其设计目的在于使算法能够以\(\mathcal{O}(1/n)\)的速度收敛至真实的总体风险最小值点。

Result: 实验证明，所提出的IWP-SGD算法在合成及真实世界数据集上的二分类任务中表现良好，有效减少了因使用LDP保护机制而产生的数据偏差。

Conclusion: 这项工作展示了如何通过数学手段（如Weierstrass变换及其逆变换）来减轻甚至消除局部差分隐私技术带来的负面影响，为处理隐私保护与数据分析准确性之间的矛盾提供了一条新路径。

Abstract: Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.

</details>


### [62] [Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment](https://arxiv.org/abs/2602.16438)
*Eva Paraschou,Line Harder Clemmensen,Sneha Das*

Main category: cs.LG

TL;DR: 该研究探讨了针对性别的调整如何影响三个先进大语言模型（Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B）在九个敏感属性上的公平性。尽管总体上有所改善，但在模糊情境下，对于外貌、性取向和残疾状态等属性的偏见实际上加剧了，这表明需要采用多属性、情境感知的公平性评估框架来防止偏见溢出效应。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型公平性校准主要集中在减少单一敏感属性上的偏差，忽视了公平性本质上是一个多维度且特定于上下文的价值观。这种做法可能会导致系统在实现狭隘的公平指标的同时，在未被针对性处理的属性上加剧差异，即所谓的偏见溢出现象。

Method: 研究者们通过直接偏好优化技术以及BBQ基准测试来评估三种最先进的大语言模型（Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B）在明确与不明确的情境下的公平性表现。

Result: 研究发现存在明显的偏见溢出现象：虽然整体结果显示有所改进，但基于具体情境的分析揭示出在模糊情境中，特别是在外貌、性取向及残疾状况方面存在着显著恶化的情况。

Conclusion: 研究表明，提高一个属性上的公平性可能会无意间恶化其他属性上的差异，尤其是在不确定性条件下。因此，强调了采用能够考虑多种属性并具有情境意识的公平性评估框架的重要性。

Abstract: Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.

</details>


### [63] [GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation](https://arxiv.org/abs/2602.16449)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: 该论文提出了一种名为GICDM的方法，用于纠正真实和生成数据的邻域估计问题，解决了由于hubness现象导致的最近邻关系扭曲和基于距离度量的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型评估通常依赖于高维嵌入空间来计算样本间的距离，但这些空间中的数据表示受到hubness现象的影响，导致最近邻关系失真和基于距离的度量偏斜。为了解决这一问题，并提高与人类判断的一致性，研究者提出了新的方法。

Method: 研究人员基于经典的迭代上下文差异度量（ICDM）开发了生成式ICDM（GICDM），专门针对真实和生成的数据修正邻域估计。此外，还引入了一个多尺度扩展以改善实际表现。

Result: 通过在合成基准测试和真实基准上的广泛实验表明，GICDM能够解决由hubness引起的失败情况，恢复可靠的度量行为，并且提高了与人类判断的一致性。

Conclusion: GICDM作为一种有效的方法被提出，它能够纠正由于hubness现象导致的问题，从而改进了生成模型评价中基于距离度量的表现，并且更符合人类的直观判断。

Abstract: Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.

</details>


### [64] [Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC](https://arxiv.org/abs/2602.16456)
*Abdulla Jasem Almansoori,Maria Ivanova,Andrey Veprikov,Aleksandr Beznosikov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: 本研究提出了LoRSum，一种内存高效的子程序，用于通过将LoRA优化转换为近端子问题，并利用交替最小二乘更新有效地解决它，从而弥合使用低秩投影的全步骤训练（SVDLoRA）与LoRA微调之间的差距。此外，还提出了一种缩放变体，可以使用结构化度量如K-FAC和Shampoo，同时保持内存效率。实验表明该方法能够在保证LoRA风格参数效率的同时匹配或改进LoRA基线性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决使用低秩投影进行全步骤训练（SVDLoRA）与LoRA微调之间存在的差距，提高大模型微调时的内存效率及参数效率。

Method: 提出了一种名为LoRSum的新方法，该方法将LoRA优化视为一个近端子问题并通过交替最小二乘更新来高效解决；另外，为了处理带有预条件梯度下降的全步骤，研究人员开发了LoRSum的一种缩放版本，能够利用像K-FAC和Shampoo这样的结构化度量，同时仅存储这些度量的对角线以维持内存效率。

Result: 在合成任务、CIFAR-100数据集以及GLUE、SQuAD v2和WikiText-103上的语言模型微调实验中，所提方法能够匹配甚至优于LoRA基线，在适度增加计算开销的情况下避免了全矩阵SVD投影并保持了LoRA式的参数效率。

Conclusion: 新提出的LoRSum及其缩放版本提供了一种更有效的途径来进行大型模型的LoRA微调，既提高了内存利用率又保持了良好的模型性能。

Abstract: Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.

</details>


### [65] [HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.16468)
*Jung Min Choi,Vijaya Krishna Yalavarthi,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: 提出了一种名为Hierarchical Patching Mixer (HPMixer)的方法，用于长期多变量时间序列预测。该方法通过解耦但互补的方式建模周期性和残差动态，实验表明其在标准基准测试中达到了竞争性或最先进水平的性能。


<details>
  <summary>Details</summary>
Motivation: 在长期多变量时间序列预测中，有效捕捉周期模式和残差动态是至关重要的。为了在这个标准深度学习基准设定内解决这个问题，提出了一个新框架。

Method: 引入了Hierarchical Patching Mixer (HPimizer)，它以一种既分离又互补的方式处理周期性和残差。周期部分使用了一个可学习的循环模块加上非线性通道级MLP来提高表达能力；残差部分则通过可学习静态小波变换（LSWT）提取稳定、移位不变的频域表示。接着，采用通道混合编码器来建立明确的跨通道依赖关系，并利用两级非重叠层次修补机制捕捉粗细尺度上的残差变化。

Result: 广泛的实验显示，在标准多变量基准上，HPMixer相较于最近的基线方法能够达到具有竞争力或者是最先进的表现。

Conclusion: 通过结合解耦的周期性建模与结构化的多尺度残差学习，HPMixer为长期多变量时间序列预测提供了一个有效的框架。

Abstract: In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.

</details>


### [66] [Fast and Scalable Analytical Diffusion](https://arxiv.org/abs/2602.16498)
*Xinyi Shang,Peng Sun,Jingyu Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: The paper introduces GoldDiff, a method that tackles the scalability issue of analytical diffusion models by dynamically identifying a 'Golden Subset' for inference, significantly reducing computational requirements while maintaining or improving performance. This approach enables the first successful application of analytical diffusion to large-scale datasets like ImageNet-1K.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to address the scalability bottleneck of analytical diffusion models, which typically require a full-dataset scan at each timestep, making them computationally expensive as the dataset size increases. The authors aim to reduce this dependency without sacrificing model performance, thereby enabling the use of such models on larger datasets.

Method: GoldDiff utilizes the observation of Posterior Progressive Concentration, where the effective support for denoising score shrinks over time, to implement a dynamic, coarse-to-fine mechanism for selecting a 'Golden Subset' of data relevant for inference. This process decouples the inference complexity from the overall dataset size, allowing for more efficient computation.

Result: Empirical results show that GoldDiff can achieve up to a 71x speedup on the AFHQ dataset compared to full-scan methods, with equivalent or better performance. Notably, it also successfully scales analytical diffusion models to the ImageNet-1K dataset, demonstrating its potential for large-scale generative modeling applications.

Conclusion: The study concludes that by leveraging the principle of Posterior Progressive Concentration and implementing a dynamic subset selection strategy, GoldDiff provides an efficient, training-free solution for scaling analytical diffusion models to large datasets, opening new avenues for practical, high-performance generative modeling.

Abstract: Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\bf 71 \times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.

</details>


### [67] [Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects](https://arxiv.org/abs/2602.16503)
*Vasilis Gkolemis,Loukas Kavouras,Dimitrios Kyriakopoulos,Konstantinos Tsopelas,Dimitrios Rontogiannis,Giuseppe Casalicchio,Theodore Dalamagas,Christos Diou*

Main category: cs.LG

TL;DR: 提出了一种新的模型类——条件性加法局部模型（CALMs），它在保持GAMs的可解释性的同时，通过允许每个特征有多个单变量形状函数来提高准确性，这些函数在输入空间的不同区域活跃。实验表明CALMs在多样化的分类和回归任务上始终优于GAMs，并且与GA$^2$Ms相比具有相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 广义相加模型（GAMs）提供了解释性但当数据中存在交互作用时会欠拟合；GA$^2$Ms通过添加选定的成对交互来提高准确性，但牺牲了可解释性并限制了模型审计。

Method: 引入了条件性加法局部模型（CALMs），该模型允许每个特征拥有多个单变量形状函数，每个函数在输入空间的不同区域内活跃。这些区域是根据特征间交互定义的简单逻辑条件独立确定的。此外，还提出了一个基于提炼的训练流程，用于识别有限交互的同质区域并通过区域感知反向拟合方法来拟合可解释的形状函数。

Result: 在多样的分类和回归任务中的实验表明，CALMs不仅一致地超越了GAMs的表现，在预测准确性方面也达到了与GA$^2$Ms相媲美的水平。

Conclusion: 总的来说，CALMs为预测准确性和可解释性之间提供了吸引人的权衡。

Abstract: Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.

</details>


### [68] [Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?](https://arxiv.org/abs/2602.16507)
*Gaetan De Waele,Marek Wydmuch,Krzysztof Dembczyński,Wojciech Kotłowski,Willem Waegeman*

Main category: cs.LG

TL;DR: 本文研究了在使用深度学习方法分析液相色谱-串联质谱(LC-MS/MS)数据以识别化合物时，不同损失函数对模型性能的影响。研究表明，在优化指纹相似度和分子检索之间存在一个基本的权衡：提高指纹预测准确性通常会恶化检索结果，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 当前，使用深度学习方法来分析LC-MS/MS数据中的化合物变得越来越普遍。然而，对于训练这些预测模型时所使用的各种损失函数如何影响模型性能这一点上，仍缺乏足够的了解。

Method: 通过调查常用的损失函数，并推导出新的遗憾界（regret bounds），来描述当这些目标达到贝叶斯最优决策时必须出现差异的情况。该研究揭示了在指纹相似性和分子检索两个目标之间的根本性权衡，并且这种权衡取决于候选集的相似结构。

Result: 结果显示，优化指纹预测精度往往会导致检索结果变差，反之亦然。理论分析提供了关于损失函数选择以及指纹选择方面的指导。

Conclusion: 研究强调了在设计用于化合物识别的深度学习模型时，需要仔细考虑损失函数的选择，因为它们对最终模型在指纹相似性和分子检索任务上的表现有显著影响。

Abstract: One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.

</details>


### [69] [Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing](https://arxiv.org/abs/2602.16531)
*Daniel Boharon,Yehuda Dar*

Main category: cs.LG

TL;DR: 本研究探讨了在使用多个可能过度参数化的预训练模型进行线性回归任务的迁移学习。通过最小化目标数据集上的平方误差并惩罚所学模型与预训练模型之间的距离来定义目标学习任务。我们分析了利用更多预训练模型改善迁移学习的情况，并提出了一种简单的去偏方法，以减少过度参数化偏差，从而更好地利用这些预训练模型来学习目标预测器。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过利用多个预训练模型来改进线性回归任务中的迁移学习效果，特别是当这些模型是过度参数化的时候。

Method: 采用的方法包括将目标学习任务定义为一个优化问题，其中包含对目标数据集上平方误差的最小化以及对所学模型与预训练模型之间差异的一种惩罚。此外，还提出了一个基于乘法修正因子的简单去偏技术，旨在减轻由于预训练模型过度参数化所带来的偏差。

Result: 研究表明，在预训练模型为过度参数化的情况下，适当增加使用的预训练模型数量对于实现有益的迁移学习至关重要。然而，这种做法也可能受到预训练模型过度参数化偏差的影响。提出的去偏技术能够有效降低这种偏差。

Conclusion: 结论指出，虽然使用更多的过度参数化预训练模型可以增强迁移学习的效果，但同时也需要注意并处理由过度参数化引起的偏差问题。为此，引入了一个简易的去偏方法，它有助于更有效地利用预训练模型来进行目标预测器的学习。

Abstract: We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.
  We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.
  Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.

</details>


### [70] [Illustration of Barren Plateaus in Quantum Computing](https://arxiv.org/abs/2602.16558)
*Gerhard Stenzel,Tobias Rohe,Michael Kölle,Leo Sünkel,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 本文探讨了变分量子电路(VQCs)中参数共享的影响，尽管它能减少参数空间维度并可能缓解贫瘠高原现象，但同时也会通过误导性梯度改变优化景观。随着参数共享程度的增加，解景变得更加复杂，传统基于梯度的优化器表现逐渐恶化。研究引入了一种新的梯度欺骗检测算法和一个量化框架来测量量子电路中的优化难度，指出虽然参数共享可以大幅提高电路的表现力，但这会显著增加景观的欺骗性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于深入理解变分量子电路(VQCs)中参数共享对优化过程的具体影响，特别是如何通过误导性梯度改变优化景观，以及这种变化如何影响到现有优化方法的有效性。

Method: 采用系统实验分析法，通过对不同程度参数共享下解景复杂度、梯度幅度及欺骗性比率的变化进行研究；提出了一种新的梯度欺骗检测算法和一个用于衡量量子电路优化难易度的定量框架。

Result: 结果显示，随着参数共享程度的提升，解景变得更为复杂且具有更高的梯度幅度与欺骗性比率；传统基于梯度的优化方法（如Adam、SGD）在面对更高水平参数共享时其收敛性能显著下降，且非常依赖于超参数的选择。

Conclusion: 结论是，虽然参数共享能够以数量级地增强电路表达能力，但同时也带来了显著增加的优化景观欺骗性问题。这为实际应用中的量子电路设计提供了重要参考，并揭示了经典优化策略与由参数共享塑造的量子参数景观之间存在的根本不匹配。

Abstract: Variational Quantum Circuits (VQCs) have emerged as a promising paradigm for quantum machine learning in the NISQ era. While parameter sharing in VQCs can reduce the parameter space dimensionality and potentially mitigate the barren plateau phenomenon, it introduces a complex trade-off that has been largely overlooked. This paper investigates how parameter sharing, despite creating better global optima with fewer parameters, fundamentally alters the optimization landscape through deceptive gradients -- regions where gradient information exists but systematically misleads optimizers away from global optima. Through systematic experimental analysis, we demonstrate that increasing degrees of parameter sharing generate more complex solution landscapes with heightened gradient magnitudes and measurably higher deceptiveness ratios. Our findings reveal that traditional gradient-based optimizers (Adam, SGD) show progressively degraded convergence as parameter sharing increases, with performance heavily dependent on hyperparameter selection. We introduce a novel gradient deceptiveness detection algorithm and a quantitative framework for measuring optimization difficulty in quantum circuits, establishing that while parameter sharing can improve circuit expressivity by orders of magnitude, this comes at the cost of significantly increased landscape deceptiveness. These insights provide important considerations for quantum circuit design in practical applications, highlighting the fundamental mismatch between classical optimization strategies and quantum parameter landscapes shaped by parameter sharing.

</details>


### [71] [Steering diffusion models with quadratic rewards: a fine-grained analysis](https://arxiv.org/abs/2602.16570)
*Ankur Moitra,Andrej Risteski,Dhruv Rohatgi*

Main category: cs.LG

TL;DR: 本文研究了从奖励倾斜扩散模型中采样的任务，特别是对于二次奖励函数。作者证明了线性奖励倾斜总是可以有效采样，并且利用Hubbard-Stratonovich变换提出了一个针对低秩正定二次倾斜的有效算法。而对于负定倾斜，即使A的秩为1，问题也是难以处理的。


<details>
  <summary>Details</summary>
Motivation: 目前实践中使用的推理时间算法大多是启发式的，存在多种失败模式，我们对这些启发式方法何时能被有效改进了解甚少。为了填补这一空白，本文聚焦于给定奖励函数和预训练扩散oracle的情况下，从奖励倾斜扩散模型中采样的计算可行性分析。

Method: 通过对特定类型的二次奖励函数（即线性和低秩正/负定）进行细致分析，结合使用Hubbard-Stratonovich变换作为新的关键元素，开发出了一种高效算法来解决低秩正定情况下的采样问题。

Result: 展示了线性奖励倾斜总是能够被有效地采样；对于低秩正定二次倾斜，提出了一种基于Hubbard-Stratonovich变换的有效算法；而负定倾斜情况下，即使矩阵A的秩仅为1，该问题也被证明是难解的。

Conclusion: 本研究表明，在某些条件下（如线性和低秩正定二次奖励），可以从奖励倾斜扩散模型中高效地采样；然而，对于负定倾斜，即使在非常限制性的条件下（例如A的秩为1），问题也变得不可处理。

Abstract: Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.
  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\star}(x) \propto p(x) \exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\top A x + b^\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).

</details>


### [72] [AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS](https://arxiv.org/abs/2602.16579)
*Maria Luisa Taccari,Kenza Tazi,Oisín M. Morrison,Andreas Grafberger,Juan Colonese,Corentin Carton de Wiart,Christel Prudhomme,Cinzia Mazzetti,Matthew Chantry,Florian Pappenberger*

Main category: cs.LG

TL;DR: 本文介绍了一种基于LSTM的全球日径流预报模型AIFL，通过两阶段训练策略解决了从历史再分析到业务预报产品的性能差距问题，并在独立时间测试集上取得了高预测技能。


<details>
  <summary>Details</summary>
Motivation: 针对数据驱动模型从历史再分析过渡到业务预报产品时出现的性能差距问题，旨在开发一种可靠的全球日径流预报方法以提高洪水准备和水资源管理能力。

Method: 提出了AIFL模型，采用LSTM架构并实施了新颖的两阶段训练策略：首先使用ERA5-Land再分析资料进行预训练，接着利用IFS控制预报数据进行微调，整个过程在CARAVAN生态系统内完成。

Result: AIFL模型在全球18,588个流域上进行了测试，在2021-2024年的独立时间测试集中达到了中位修改后的Kling-Gupta效率系数(KGE')为0.66和Nash-Sutcliffe效率系数(NSE)为0.53的好成绩。此外，该模型在极端事件检测方面表现出色。

Conclusion: AIFL模型不仅能够有效缩小从再分析到预报转换过程中存在的性能差距，还展现出与当前最先进全球系统相媲美的准确性，同时保持了透明可重复性的强迫流程，为全球水文界提供了一个简化的、操作性强的基础方案。

Abstract: Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.

</details>


### [73] [Sequential Membership Inference Attacks](https://arxiv.org/abs/2602.16596)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: cs.LG

TL;DR: 研究开发了一种名为SeMI*的最优成员推理攻击方法，利用模型更新序列来识别在特定更新步骤中插入的目标。这种方法能够在有限样本条件下计算经验均值，并且与现有仅针对最终模型的攻击相比，能够避免成员推理信号被稀释。实验结果表明，SeMI*的实际变体提供了比基线更严格的隐私审计。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型在其生命周期中会经历多次更新，因此如何利用这些动态变化来创建更强的成员推理（MI）攻击和更严格的隐私审计成为了一个亟待解决的问题。尽管已有文献表明使用一系列模型更新可以增加MI攻击的能力，但对‘最优’MI攻击的严格分析还局限于拥有无限样本的静态模型。

Method: 提出了一种称为SeMI*的‘最优’MI攻击方法，该方法利用模型更新序列来识别在某个更新步骤中插入的目标。对于经验均值计算，研究人员推导了当访问有限数量样本时SeMI*的最佳效能，无论是否具有隐私保护措施。

Result: 研究发现，通过访问模型序列可以防止MI信号被稀释，这与现有的针对最终模型的攻击不同，在后者中随着训练数据量的增加MI信号会逐渐消失。此外，对手还可以利用SeMI*调整插入时间和诱饵以获得更严格的隐私审计。实验证明，SeMI*的实际变体在多种数据分布以及使用DP-SGD训练或微调的模型上都能提供比基线更为严格的隐私审计。

Conclusion: SeMI*作为一种利用模型更新序列来进行成员推理攻击的方法，不仅能够有效提高攻击能力，而且为执行更加准确有效的隐私审计提供了新的途径。

Abstract: Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.

</details>


### [74] [A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models](https://arxiv.org/abs/2602.16626)
*SungJun Cho,Chetan Gohil,Rukuang Huang,Oiwi Parker Jones,Mark W. Woolrich*

Main category: cs.LG

TL;DR: 本文系统地评估了用于处理脑磁图(MEG)数据的大规模神经影像模型中的样本级分词策略，比较了可学习和不可学习的分词器在信号重建保真度以及后续基础建模性能方面的影响。结果表明，无论是可学习还是不可学习的离散化方案都能达到较高的重建精度，并且在大多数评价标准上表现相当，意味着简单的固定样本级分词策略可以被应用于神经基础模型的开发。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理领域的成功激发了对大规模神经影像数据基础模型的兴趣，这些模型通常需要对连续的神经时间序列数据进行离散化（称为‘分词’）。但是，目前对于不同分词策略对神经数据影响的理解尚不充分。

Method: 研究者们对应用于脑磁图(MEG)数据的基于转换器的大规模神经影像模型进行了样本级分词策略的系统性评估。他们通过考察信号重建保真度及对后续基础建模性能（包括分词预测、生成数据的生物学合理性、个体特异性信息保存以及下游任务表现）的影响来对比可学习与非可学习分词器。对于可学习分词器，提出了一种基于自编码器的新方法。实验使用了三个公开可用的MEG数据集，涵盖了不同的采集地点、扫描仪和技术范式。

Result: 结果显示，不管是可学习还是非可学习的离散化方案都能够实现很高的重建准确性，并且在大部分评测指标上具有广泛可比的表现。这意味着简单的固定样本级别分词策略可以在神经基础模型的发展中被采用。

Conclusion: 这项研究表明，在开发用于处理MEG数据的大规模神经影像模型时，可以考虑使用简单有效的固定样本级分词策略，因为它们不仅能够提供良好的信号重建质量，而且还能保持较好的后续建模性能。

Abstract: Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.

</details>


### [75] [Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes](https://arxiv.org/abs/2602.16629)
*Ethan Blaser,Jiuqi Wang,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文证明了在不使用与状态访问次数相关的局部时钟的情况下，基于策略的n步差分TD算法几乎可以确定地收敛，并且给出了离策略n步差分TD算法同样能够不依赖于局部时钟而收敛的三个充分条件。


<details>
  <summary>Details</summary>
Motivation: 现有的差分时间差学习算法的收敛性保证需要一个与状态访问次数有关的学习率局部时钟，这在实际应用中并不常用，也不适用于表格以外的设置。本文旨在解决这一局限性，使差分TD算法的收敛分析更接近实际应用。

Method: 通过数学证明的方法，在标准递减学习率下，证明了基于策略的n步差分TD算法对于任意n值都能几乎确定地收敛。此外，还推导出使得离策略n步差分TD算法能够在没有局部时钟情况下收敛的三个充分条件。

Result: 研究结果表明，基于策略的n步差分TD算法可以在不依赖于局部时钟的情况下收敛；同时，只要满足特定条件，离策略版本也能实现类似效果。

Conclusion: 这项工作加强了差分TD算法的理论基础，并使其收敛性分析更加贴近现实世界的应用场景。

Abstract: The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.

</details>


### [76] [Optimizer choice matters for the emergence of Neural Collapse](https://arxiv.org/abs/2602.16642)
*Jim Zhao,Tin Sum Cheng,Wojciech Masarczyk,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 本文挑战了神经崩溃（NC）现象在优化方法中普遍存在的假设，通过引入新的诊断指标NC0证明了在AdamW等自适应优化器中解耦权重衰减下NC无法出现。研究还揭示了动量对使用SGD训练时NC的加速效应，并通过大量实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有的关于神经崩溃（NC）现象的研究大多忽略了优化器的作用，暗示着NC现象在不同的优化方法中是普遍存在的。本文旨在挑战这一假设，探究优化器选择对于NC现象出现的关键作用。

Method: 本文首先引入了一种新的诊断度量标准NC0，其收敛至零是发生NC的必要条件。利用NC0，作者们提供了理论证据表明，在类似AdamW这样的自适应优化器采用解耦合权重衰减的情况下，NC不会出现。此外，文章还证明了当使用SGD训练时，动量对于促进NC发生的额外影响。

Result: 研究表明，不同优化策略如SGD、带耦合权重衰减的SignGD（Adam的一种特殊情况）、以及带解耦合权重衰减的SignGD（AdamW的一种特殊情况），它们表现出明显不同的NC0动态特征。同时，实验证明了动量确实能够加速NC的发展过程。

Conclusion: 这项工作首次从理论上解释了基于优化器差异而引发的NC现象，并强调了权重衰减耦合方式在决定优化器隐式偏差方面的重要性。

Abstract: Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.

</details>


### [77] [Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment](https://arxiv.org/abs/2602.16643)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 本文提出了一种使用带有二次优化退火的因子分解机（FMQA）来解决RNA逆折叠问题的新方法。通过评估不同核苷酸到整数的分配方式和二进制-整数编码方法，研究发现独热编码和域壁编码比二进制和一元编码表现更好，并且在域壁编码中将鸟嘌呤和胞嘧啶分配给边界整数可以促进它们在茎区的富集，从而产生更稳定的二级结构。


<details>
  <summary>Details</summary>
Motivation: 现有的基于启发式或机器学习的方法解决RNA逆折叠问题时往往需要大量的序列评估，这限制了当实验验证成本高昂时这些方法的应用。因此，作者提出了一种新方法，旨在减少所需评估次数的同时保持解的质量。此外，对于如何将核苷酸转换为二进制变量以及这种转换对FMQA性能的影响尚未有深入研究，这也是本研究的一个重要动机。

Method: 采用带有二次优化退火的因子分解机（FMQA），这是一种能在有限次评估下获得高质量解决方案的离散黑箱优化方法。为了应用FMQA于RNA逆折叠问题，必须将核苷酸转换成二进制变量。为此，研究人员评估了所有24种可能的四核苷酸到有序整数(0-3)的分配方式，结合四种不同的二进制-整数编码方法进行测试。

Result: 研究结果表明，在归一化集合缺陷值方面，独热编码和域壁编码优于二进制和一元编码。特别是在域壁编码中，被分配给边界整数（0和3）的核苷酸出现频率更高；而在RNA逆折叠问题中，将鸟嘌呤和胞嘧啶分配给这些边界整数能够促进其在茎区的富集，从而形成比独热编码所得更为热力学稳定的二级结构。

Conclusion: 通过建立一种新的FMQA框架用于RNA逆折叠问题，并分析了不同核苷酸到整数的分配方式及二进制-整数编码方法对解决方案质量的影响，研究揭示了独热编码与域壁编码在特定条件下可有效提高RNA设计效率。特别是，合理选择核苷酸到整数的映射关系有助于增强RNA分子的稳定性。

Abstract: The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.

</details>


### [78] [Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition](https://arxiv.org/abs/2602.16684)
*Bo Pan,Peter Zhiping Zhang,Hao-Wei Pang,Alex Zhu,Xiang Yu,Liying Zhang,Liang Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种基于变量到变量的类比生成方法，并训练了一个基础模型来处理大规模匹配分子对转换（MMPTs），以在给定输入变量的情况下生成多样化的变量。通过开发提示机制，用户可以在生成过程中指定首选的转换模式。此外，还引入了MMPT-RAG框架，利用外部参考类比作为上下文指导，以指导生成并从特定项目系列中泛化。实验结果表明，该方法在通用化学语料库和专利特定数据集上提高了多样性、新颖性和可控性，并且在实际发现场景中能够恢复现实的类比结构。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法要么在整个分子层面操作，编辑控制能力有限，要么从受限的设置和小规模模型中学习MMP风格的编辑。为了克服这些限制，作者提出了一个新方法，旨在提高类比生成过程中的多样性、新颖性和可控性。

Method: 研究者提出了一种新的变量至变量的类比生成公式，并训练了一个能够在大型MMPT上运行的基础模型。为了增强实用性，他们开发了允许用户在生成时指定偏好的转换模式的提示机制。同时，研究者还介绍了一个名为MMPT-RAG的检索增强框架，该框架利用外部参考类比作为情境指导，帮助生成过程更加贴合特定项目需求。

Result: 实验结果显示，在通用化学语料库和专利特定数据集中，所提方法在提升产物多样性、新颖性以及可控性方面表现优异。此外，该方法也被证明能在实际探索场景下有效地恢复出接近真实的类比结构。

Conclusion: 通过提出一种新颖的变量到变量的类比生成方法及配套的MMPT-RAG框架，本研究为药物化学家提供了一种更高效、更具针对性的设计工具，有助于加速新药开发过程。

Abstract: Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.

</details>


### [79] [Protecting the Undeleted in Machine Unlearning](https://arxiv.org/abs/2602.16697)
*Aloni Cohen,Refael Kohen,Kobbi Nissim,Uri Stemmer*

Main category: cs.LG

TL;DR: 该论文揭示了机器遗忘（machine unlearning）过程中模仿“完美再训练”的方法存在隐私风险，提出了一种新的安全定义来保护未删除数据免受由于其他数据点的删除而造成的泄露。


<details>
  <summary>Details</summary>
Motivation: 研究者们发现，当前机器遗忘技术在试图模仿‘完美再训练’时，可能给剩余的数据点带来严重的隐私威胁。为了应对这一问题，并且让机器遗忘支持基本功能如准确求和等，提出了一个新的安全定义。

Method: 通过展示一种重建攻击，证明对于某些任务而言，仅需控制少量数据点并发出删除请求，就能几乎完全重建整个数据集。此外，还对现有的机器遗忘定义进行了调查分析，指出现有定义要么容易受到此类攻击，要么过于严格以至于无法支持基础功能。

Result: 提出了一个新安全定义，能够特别地保护未被删除的数据不因其它数据点的删除而发生泄漏。该定义允许实现包括公告板、求和以及统计学习在内的几个关键功能。

Conclusion: 本文介绍的新安全定义为机器遗忘提供了一个更加安全且实用的方向，有助于解决现有方法中存在的隐私泄露问题，同时保持了对重要功能的支持。

Abstract: Machine unlearning aims to remove specific data points from a trained model, often striving to emulate "perfect retraining", i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $ω(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.

</details>


### [80] [Causality is Key for Interpretability Claims to Generalise](https://arxiv.org/abs/2602.16698)
*Shruti Joshi,Aaron Mueller,David Klindt,Wieland Brendel,Patrik Reizinger,Dhanya Sridhar*

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型（LLMs）的可解释性研究中的常见问题，如结果不具普遍性、因果解释超出证据支持等。通过引入因果推断框架，特别是Pearl的因果层级理论，文章指出观察和干预可以为模型内部组件与行为间的关联提供不同程度的支持，但反事实声明仍难以验证。提出了一种基于因果表示学习的方法论来指导实践者选择合适的方法和技术，以确保研究发现具有更广泛的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的可解释性研究存在一些局限性，比如研究发现缺乏泛化能力以及对模型行为背后的因果关系做出过度解读等问题。为了克服这些问题，并提高研究结果的可靠性和泛化能力，作者认为有必要采用一种更加严谨的方法论——即因果推断方法。

Method: 该论文首先介绍了因果推断的基本概念及其在理解复杂系统中的作用；接着详细阐述了如何利用Pearl提出的因果层级理论来定义从模型激活到不变高层次结构的有效映射，并讨论了实现这种映射所需的数据或假设条件；最后提出了一个诊断框架，旨在帮助研究人员根据其研究目的选择适当的技术手段来进行分析。

Result: 通过将因果推理应用于大型语言模型的研究中，能够更好地理解模型内部机制与其表现之间的联系。此外，还展示了一个基于因果表示学习的具体实例，说明了这种方法如何帮助识别可以从激活中恢复哪些变量，并且明确了这样做需要满足的前提条件。

Conclusion: 采用因果推断的方法可以帮助改进大型语言模型的可解释性研究，使其结论更加稳健并具有更强的泛化能力。为此，建议研究人员遵循文中提出的指南，在设计实验时考虑因果层级的不同层面，从而促进研究发现的一致性和可靠性。

Abstract: Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.

</details>


### [81] [Knowledge-Embedded Latent Projection for Robust Representation Learning](https://arxiv.org/abs/2602.16709)
*Weijing Tang,Ming Yuan,Zongqi Xia,Tianxi Cai*

Main category: cs.LG

TL;DR: 提出了一种利用外部语义嵌入来规范表示学习的知识嵌入潜在投影模型，通过将列嵌入建模为语义嵌入的平滑函数，并结合基于核主成分分析的语义引导子空间构建和可扩展的投影梯度下降，以解决在不平衡状态下高维离散数据矩阵（如电子健康记录）的估计难题。


<details>
  <summary>Details</summary>
Motivation: 在处理像电子健康记录这样的高维离散数据矩阵时，当一个维度远大于另一个（比如特征空间非常大而样本量受限），传统的潜空间模型估计变得困难。受预训练临床概念嵌入等外部语义嵌入日益可用性的启发，作者旨在开发一种能够利用这些额外信息来改善表征学习的方法。

Method: 通过在再生核希尔伯特空间中的映射，将列嵌入视为语义嵌入的光滑函数进行建模。采用两步估计程序：首先使用基于核的主成分分析创建由语义指导的子空间；然后运用可扩展的投影梯度下降法进一步优化。此外，还提供了非凸优化过程的局部收敛保证。

Result: 理论分析上，给出了估计误差界限，明确了统计误差与核投影导致的近似误差之间的权衡。实践应用中，通过广泛的模拟研究及真实世界EHR案例验证了该方法的有效性。

Conclusion: 所提出的方法有效解决了在数据不平衡情况下高维离散数据矩阵的估计问题，特别是对于具有大量特征但样本数有限的应用场景而言，利用外部语义信息可以显著提高模型性能。

Abstract: Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [82] [DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows](https://arxiv.org/abs/2602.16585)
*Dimitri Yatsenko,Thinh T. Nguyen*

Main category: cs.DB

TL;DR: DataJoint 2.0 introduces a relational workflow model and four technical innovations to unify data structure, data, and computational transformations, supporting robust human-agent collaboration in scientific workflows.


<details>
  <summary>Details</summary>
Motivation: The need for operational rigor in human-agent collaboration is highlighted, with the observation that current approaches lack a unified system (SciOps) for managing scientific data pipelines, leading to fragmented provenance without transactional guarantees.

Method: DataJoint 2.0 employs a relational workflow model where tables represent steps, rows are artifacts, and foreign keys dictate execution order. It also integrates four key innovations: object-augmented schemas, semantic matching, an extensible type system, and distributed job coordination.

Result: By unifying all aspects of data and computation into a single, queryable, and machine-readable system, DataJoint 2.0 provides a robust foundation for SciOps, enabling safe and efficient participation of agents in scientific workflows.

Conclusion: DataJoint 2.0 addresses the gap in current scientific data management practices by offering a comprehensive solution for maintaining data integrity and facilitating effective human-agent collaboration within scientific projects.

Abstract: Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [83] [ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization](https://arxiv.org/abs/2602.15983)
*Junbo Jacob Lian,Yujun Sun,Huiling Chen,Chaoyu Zhang,Chung-Piaw Teo*

Main category: cs.SE

TL;DR: 本文介绍了一种名为ReLoop的方法，通过结构化生成和行为验证两种互补的方式解决大型语言模型在将自然语言转换为优化代码时出现的静默错误问题。该方法提高了代码的正确性和执行率，并且在不同模型和基准测试中表现出了持续的改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能够将自然语言转化为优化代码，但存在一个严重的问题：即使代码可以成功执行并返回求解器可行解，也可能包含语义上不正确的公式，这会导致组合问题上的正确性差距高达90个百分点。

Method: 提出了ReLoop方法，它从两个互补的角度处理静默失败问题。一方面，通过结构化生成方式模仿专家建模实践，把代码生成分解成理解、形式化、合成及验证四个阶段，强化变量类型推理和自我验证来防止源头上的公式错误；另一方面，采用行为验证来检测那些经过生成后仍存在的错误，通过对基于求解器参数扰动下公式的响应是否正确来进行检验，而不需要依赖于真实值。此外，还利用IIS增强诊断进行执行恢复。

Result: ReLoop显著提升了代码的正确性和可执行性，在最强模型上，正确性从22.6%提升到了31.1%，执行成功率则从72.1%提高到了100%。这些改进在跨越三种范式（基础、SFT、RL）和三个基准测试的五个模型上都得到了一致体现。

Conclusion: ReLoop通过结合结构化生成与行为验证这两种机制有效地解决了由大型语言模型产生的优化代码中的静默错误问题，特别是在复杂组合问题以及局部公式缺陷问题上表现出色。

Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.

</details>


### [84] [Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?](https://arxiv.org/abs/2602.16091)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: 该研究旨在通过将因果意识的分裂标准纳入符号模型中，来探讨这是否能提高其在软件工程任务中的稳定性与鲁棒性，并且比较了人类专家判断与自动化模型之间的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于相关性的模型和因果发现算法在软件工程任务中表现出显著的不稳定性，这种不稳定性源于相关性方法混淆了关联与因果关系以及因果发现算法因结构学习的NP难性质而依赖于启发式近似导致的问题。研究目的是探索引入因果意识能否改善这一状况而不牺牲预测或优化性能。

Method: 研究使用来自多目标优化任务库（MOOT）中的120多个多目标优化任务，采用预注册的自助集成协议测量方差与获胜分数分配来评估稳定性。对比了人类因果评估、基于相关性的决策树（EZR）以及利用条件熵分裂标准和混杂因素过滤的因果意识树之间的稳定性及性能差异，分析手段包括统计方法如方差、基尼不纯度、KS检验、Cliff's delta等。

Result: 研究结果提供了关于因果意识分裂标准对提高符号模型稳定性与鲁棒性的证据，同时揭示了相对于传统相关性模型，这种方法在预测或优化性能上的影响。此外，还比较了人类专家判断与自动模型之间稳定性的差异。

Conclusion: 引入因果意识的分裂标准到符号模型中可以有效提升其在实际软件工程项目中的稳定性和鲁障性，尽管可能会影响到一定程度的预测或优化表现。

Abstract: Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)

</details>


### [85] [Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs](https://arxiv.org/abs/2602.16106)
*Shahriar Rumi Dipto,Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 研究提出了一种基于算法的代码翻译管道，通过引入一种语言中立的中间规范来改善大型语言模型在不同编程语言间自动转换程序时的准确性与可靠性。实验结果表明，这种方法显著提高了从Python到Java或反之的代码翻译准确率，并减少了各种错误的发生。


<details>
  <summary>Details</summary>
Motivation: 直接一次性代码翻译往往不能很好地保持程序意图，导致控制流、类型处理和I/O行为等方面的错误。为了解决这个问题，研究人员提出了一个基于算法的流程，旨在通过引入语言中立的中间规格来捕捉这些细节，从而提高翻译的质量。

Method: 本研究设计了一个自动化配对实验，利用五种广泛使用的大型语言模型，在Avatar和CodeNet数据集上进行Python与Java之间的相互翻译。对于每种组合（模型、数据集、方法及方向），都编译并执行了翻译后的程序，并运行了提供的测试用例。记录了编译结果、运行时行为、超时情况以及测试结果。

Result: 基于算法的方法将微平均准确度从67.7%提升到了78.5%，即增长了10.8%。它完全消除了词法和标记错误，减少了72.7%的不完整结构问题，以及61.1%的结构和声明问题。此外，该方法还将运行时依赖性和入口点失败降低了78.4%。

Conclusion: 基于算法的管道能够实现更可靠且保留原意的代码翻译，这为构建强大的多语言编程助手奠定了基础。

Abstract: Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.

</details>


### [86] [Software-heavy Asset Administration Shells: Classification and Use Cases](https://arxiv.org/abs/2602.16499)
*Carsten Ellwein,David Dietrich,Jessica Roth,Rozana Cvitkovic,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文旨在填补将软件服务直接集成到资产管理壳(AAS)中的系统性软件架构分析的研究空白，基于软件质量标准和典型的制造用例区分不同的架构，并为学术界和实践者提供一个解读指南。


<details>
  <summary>Details</summary>
Motivation: 随着软件在制造业中变得越来越重要，特别是在数字化制造和人工智能的使用方面，对建模软件的需求以及将服务直接集成到AAS中的需求也在增加。虽然现有文献中有一些针对软件密集型AAS的单独解决方案，但缺乏对能够直接将软件服务整合进AAS的软件架构进行系统性分析。

Method: 该论文采用了基于软件质量标准及典型制造用例来区分不同软件架构的方法，以期为软件密集型AAS提供一种解释指导。

Result: 研究结果有助于识别适合于软件密集型AAS的不同软件架构，并根据特定的质量标准和应用场景对其进行评估。

Conclusion: 本工作为软件密集型AAS提出了一个解释指南，旨在支持学术界与实践者更好地理解和应用相关技术。

Abstract: The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.

</details>


### [87] [SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation](https://arxiv.org/abs/2602.16671)
*Jaid Monwar Chowdhury,Chi-An Fu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出了一种名为SPARC的神经符号框架，旨在解决C语言自动化单元测试生成中遇到的语义鸿沟问题。通过四个阶段的操作，SPARC能够有效提高测试覆盖率和代码质量，并在实际应用中表现出优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 针对C语言中由于指针算术和手动内存管理带来的严格语法限制，使得从高层次程序意图到代码生成之间存在较大语义差距的问题，现有基于大型语言模型（LLMs）的方法往往难以直接合成高质量的测试用例。这导致了诸如无法编译、虚幻函数签名以及低分支覆盖率等问题。

Method: 提出了一个名为SPARC的神经符号框架，该框架通过四个步骤来解决上述挑战：(1) 控制流图(CFG)分析；(2) 通过操作映射将LLM推理与验证过的实用辅助工具相结合；(3) 针对路径的目标测试生成；(4) 利用编译器和运行时反馈进行迭代自修正验证循环。

Result: 实验结果表明，在59个真实世界及算法主题上，相比仅使用提示生成的基础线，SPARC在线覆盖率提高了31.36%，分支覆盖率提升了26.01%，变异得分增加了20.78%。此外，SPARC还保持了94.3%的测试案例经过迭代修复后仍然有效，并且生成的代码具有更高的可读性和可维护性评价。

Conclusion: 通过将LLM推理与程序结构相匹配，SPARC为工业级遗留C代码库的测试提供了一个可扩展的解决方案。

Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.

</details>
