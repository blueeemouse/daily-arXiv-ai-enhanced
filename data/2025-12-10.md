<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.IR](#cs.IR) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Formally and Empirically Verified Methodologies for Scalable Hierarchical Full-Stack Systems](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: 本文介绍了两种形式上和经验上都经过验证的软件工程方法：初级广度优先开发（PBFD）和初级深度优先开发（PDFD），它们通过图论建模来保证软件结构和行为的正确性。此外，提出了一种新的三级封装（TLE）位掩码编码方案以高效管理大规模层次数据。PBFD在企业部署中表现出色，相比其他方法具有更快的开发速度、查询性能和更少的存储需求。


<details>
  <summary>Details</summary>
Motivation: 为了提高全栈软件工程的可扩展性和工业级应用，同时确保软件架构和行为上的正确性，研究者们希望通过结合形式化方法与实际应用来实现这一目标。

Method: 文章提出了初级广度优先开发(PBFD) 和初级深度优先开发(PDFD) 方法，并采用图论建模技术将软件开发过程表示为分层有向图及统一状态机。引入了基于位掩码的三级封装(TLE) 编码方案用于处理大规模层级数据。

Result: PBFD方法在八年企业部署过程中未发生任何重大故障，其开发速度比Salesforce OmniScript快约20倍，查询性能提高7-8倍，相较于传统关系型模型存储需求减少了11.7倍。这些成果通过长期观察研究、准实验运行时比较以及控制组模式级别实验得到证实。

Conclusion: 本研究表明，通过采用PBFD或PDFD等创新方法，可以显著提升软件工程项目在开发效率、执行性能以及资源利用方面的表现。开放源代码MVP实现进一步验证了该方法的有效性。

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary Depth-First Development (PDFD)-formally and empirically verified methodologies for scalable, industrial-grade full-stack software engineering. Both approaches enforce structural and behavioral correctness through graph-theoretic modeling, bridging formal methods and real-world practice. PBFD and PDFD model software development as layered directed graphs with unified state machines, verified using Communicating Sequential Processes (CSP) and Linear Temporal Logic (LTL). This guarantees bounded-refinement termination, deadlock freedom, and structural completeness. To manage hierarchical data at scale, we present the Three-Level Encapsulation (TLE)-a novel bitmask-based encoding scheme. TLE operations are verified via CSP failures-divergences refinement, ensuring constant-time updates and compact storage that underpin PBFD's robust performance. PBFD demonstrates exceptional industrial viability through eight years of enterprise deployment with zero critical failures, achieving approximately 20x faster develop-ment than Salesforce OmniScript, 7-8x faster query performance, and 11.7x storage reduction compared to conventional relational models. These results are established through longitudinal observational studies, quasi-experimental runtime comparisons, and controlled schema-level experiments. Open-source Minimum Viable Product implementations validate key behavioral properties, including bounded refinement and constant-time bitmask operations, un-der reproducible conditions. All implementations, formal specifications, and non-proprietary datasets are publicly available.

</details>


### [2] [Gamification with Purpose: What Learners Prefer to Motivate Their Learning](https://arxiv.org/abs/2512.08551)
*Kai Marquardt,Mona Schulz,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 本研究通过系统文献回顾确定了十种广泛讨论的游戏设计元素，并使用最佳-最差缩放（BWS）调查法来收集125名参与者对这些元素的偏好排名。研究发现，学习者更倾向于那些直接支持学习过程的游戏化元素，如进度条、概念图、即时反馈和成就。建议游戏化策略应优先考虑能够可视化学习进度并提供可操作性反馈的工具，而不是单纯依赖外部激励。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探索学习者对于教育情境中游戏设计元素的偏好，以指导目的导向的游戏化策略的发展。它强调了一种以学习者为中心的方法，将游戏化设计与教学目标相一致，同时降低风险，比如内在动机的侵蚀。

Method: 进行了系统的文献综述以识别出十个被广泛讨论的游戏设计元素。为每个元素开发了视觉原型，并通过一个包含125名参与者的最佳-最差缩放（BWS）调查来获取偏好排名。此外还收集了定性的反馈以揭示动机驱动因素。

Result: 学习者始终偏好那些能够直接支持学习过程的游戏设计元素——特别是进度条、概念地图、即时反馈以及成就。定性分析揭露了六个重复出现的动机主题，包括可见的进步、内容相关性和建设性的反馈。

Conclusion: 研究结果表明，当游戏化元素与教育内容有意义地结合在一起并且支持内在动机时，学习者会更加重视这些游戏化元素。为了实现目标一致的游戏化，应该优先考虑可以可视化学习进展并提供实用反馈的工具，而不仅仅是依靠外在奖励。

Abstract: This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.

</details>


### [3] [An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face](https://arxiv.org/abs/2512.07983)
*Nan Jia,Anita Raja,Raffi Khatchadourian*

Main category: cs.SE

TL;DR: 本文提出了一种经验框架，用于通过挖掘HuggingFace上的模型演化数据来评估学习型软件系统(LESS)中的语义保持。研究通过分析大量模型的提交历史、模型卡片和性能指标，揭示了基于提交信息分析的常见重构模式，并检测了跨提交的评估指标中的语义漂移。主要贡献包括：一个大规模的机器学习模型演化数据集、一个实用的语义保持评估管道以及展示实际语义漂移的经验案例研究。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习（ML）成为高自治系统的一个组成部分，确保学习型软件系统（LESS）的可信度变得至关重要。然而，ML的非确定性和运行时定义的语义复杂化了传统的软件重构。文章旨在定义并评估LESS中的语义保持，即智能组件的优化不改变系统的整体功能行为。

Method: 通过从HuggingFace获取模型演变数据，包括提交历史、《模型卡片》和性能指标，构建了一个经验框架来评估语义保持。在三个领域进行了案例研究以建立基准线，追踪不同版本之间的性能变化。通过对提交信息的分析，识别出常见的重构模式和语义漂移现象。

Result: 分析表明，可以通过跨提交的评估指标检测到语义漂移，并且基于提交消息分析揭示了一些常见的重构模式。虽然API限制使得无法估计全规模阈值，但该管道为定义社区接受的语义保持边界提供了基础。

Conclusion: 这项工作推进了更可维护和可信的机器学习系统的基础建设，包括提供了一个大规模的数据集、一个实用的语义保持评估管道及其实证案例研究。

Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.

</details>


### [4] [A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering](https://arxiv.org/abs/2512.07990)
*Thanh Nguyen,Chaima Boufaied,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文回顾了灰色文献，探讨了AI环境下的公平性要求，包括它们在不同应用领域的定义、软件开发生命周期中的管理方式以及违反这些要求的原因和后果。研究强调需要一致的框架和实践来将公平性融入AI软件中。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（特别是机器学习）在各种软件中的广泛应用，当前的关注点主要集中在AI模型的有效性上，而对公平性的关注相对较少。文章旨在通过审查现有灰色文献，探讨AI系统中公平性要求的定义、管理及其违背所带来的影响。

Method: 通过灰色文献调查的方法，分析了AI系统中公平性要求的不同定义、在整个软件开发生命周期中的管理实践，以及违反这些要求的原因与后果。

Result: 研究表明，AI系统的公平性要求通常强调跨不同人口和社会属性的非歧视和平等对待；公平性要求管理实践在软件开发生命周期各阶段有所不同；违反公平性要求的原因多样，包括数据表示偏见、算法设计偏见等，并且可能导致广泛的社会和个人层面的负面影响。

Conclusion: 为了确保AI支持决策的信任度和合法性，有必要开发一致的框架和实践以同等重视AI软件中的公平性和有效性。

Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.

</details>


### [5] [What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions](https://arxiv.org/abs/2512.08032)
*Arghavan Sanei,Chaima Amiri,Atefeh Shokrizadeh,Jinghui Cheng*

Main category: cs.SE

TL;DR: 研究通过分析五个开源软件项目中的论点话语和质量，揭示了开源软件（OSS）可用性讨论主要由论点驱动，但质量参差不齐。问题评论比原始帖子显示出更低的论点质量，表明在OSS社区中关于可用性的集体智慧不足。此外，论点的话语和质量对参与者后续行为有着不同的影响。该研究为提高OSS的可用性和支持分布式协作社区提供了见解。


<details>
  <summary>Details</summary>
Motivation: 尽管开源软件（OSS）的可用性很重要，但在往往被技术与功能复杂性所掩盖。争论在OSS可用性讨论中对于不同利益相关者表达意见和说服他人至关重要。然而，这些讨论中论点话语的特点尚不清楚，导致难以向讨论参与者提供有效的支持。

Method: 本研究通过对五个OSS项目的论点话语和质量进行全面分析来解决这一问题。

Result: 结果表明，可用性讨论主要是由论点驱动的，不过它们的质量存在差异；问题评论中的论点质量低于原帖发布的论点质量，这暗示了OSS社群中有关可用性的集体智慧有所欠缺。另外，论点话语及其质量对参与者的后续行为产生多种影响。

Conclusion: 总的来说，这项研究为帮助OSS利益相关者构建更有效的论据，并最终改善OSS的可用性提供了见解。这些见解也可以为其他分布式协作社区的研究提供信息。

Abstract: The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.

</details>


### [6] [Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs](https://arxiv.org/abs/2512.08213)
*Md Nazmul Haque,Elizabeth Lin,Lawrence Arkoh,Biruk Tadesse,Bowen Xu*

Main category: cs.SE

TL;DR: 本研究首次系统地探讨了量化对大型语言模型生成的Go语言包中的包幻觉率和漏洞风险的影响。结果表明，量化显著增加了包幻觉率，尤其是4位量化模型表现最为严重。同时，即使在正确生成的包中，随着精度降低安全漏洞出现率也会上升，揭示了部署量化后的大型语言模型用于代码生成和依赖推荐时可能带来的可靠性和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被用于生成软件工件，包括库和包的推荐，但最近的证据显示这些模型常常会幻想出不存在的包名或生成含有已知安全漏洞的依赖项，给开发者及下游软件供应链带来了重大风险。同时，尽管量化技术已被广泛采用以减少推理成本并使大型语言模型能够在资源受限环境中部署，但对于量化如何影响LLM生成软件依赖项的正确性和安全性却知之甚少。

Method: 研究人员选择了五种不同规模的Qwen模型，在全精度、8位以及4位量化条件下，针对SO、MBPP和paraphrase三个数据集进行了评估。

Result: 实验结果显示，量化显著提高了包幻觉率（PHR），其中4位模型退化最为严重；而且即便是在那些正确生成的包里，随着精度下降其包含的安全漏洞存在率（VPR）也有所上升；此外，分析还发现大多数虚构出来的包看起来像真实的基于URL的Go模块路径，比如最常见的错误格式或不存在于GitHub和golang.org上的仓库地址。

Conclusion: 总体而言，该研究提供了关于部署量化后大型语言模型进行代码生成和依赖推荐时所面临可靠性与安全性挑战的重要见解。

Abstract: Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.
  In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.

</details>


### [7] [Migrating QAOA from Qiskit 1.x to 2.x: An experience report](https://arxiv.org/abs/2512.08245)
*Julien Cardinal,Imen Benzarti,Ghizlane El boussaidi,Christophe Pere*

Main category: cs.SE

TL;DR: 本研究通过将量子近似优化算法（QAOA）从Qiskit 1.x迁移到Qiskit 2.x，揭示了采样预算（即每迭代电路执行次数）对结果准确性的影响。尽管电路、优化器和哈密顿量相同，新版本的结果却大相径庭，原因是Qiskit 2.x默认的10,000次采样仅覆盖了状态空间的23%。增加采样到250,000次恢复了准确度。


<details>
  <summary>Details</summary>
Motivation: 迁移量子算法至新框架时可能会引入细微的行为变化，这些变化会影响算法的准确性和可重复性。本研究旨在探索这种迁移过程中遇到的问题，并找出导致结果差异的根本原因。

Method: 研究人员将QAOA从使用Qiskit 1.x的Qiskit Algorithms转换为基于Qiskit 2.x自定义实现。他们保持了相同的电路设计、优化器及哈密顿量设定，但注意到在新的实现中得到了截然不同的结果。随后，通过对问题进行系统分析，特别是针对每次迭代中的电路执行次数（采样预算）进行了深入研究。

Result: 研究发现，Qiskit 1.x库隐式地采用了无限次的采样预算，这使得能够获得密集的概率分布；而Qiskit 2.x的默认设置仅为10,000次采样，只能捕捉到大约23%的状态空间。通过将采样次数提高到250,000次，研究者们成功恢复到了与原始库相当的结果精度。

Conclusion: 这项研究表明，在量子-经典交互层面上隐藏参数如采样预算可以显著影响混合型量子算法的表现。为了确保量子软件迁移过程中的结果可再现性，开发者和框架设计者应重视并适当调整这些参数。

Abstract: Migrating quantum algorithms across evolving frameworks introduces subtle behavioral changes that affect accuracy and reproducibility. This paper reports our experience converting the Quantum Approximate Optimization Algorithm (QAOA) from Qiskit Algorithms with Qiskit 1.x (v1 primitives) to a custom implementation using Qiskit 2.x (v2 primitives). Despite identical circuits, optimizers, and Hamiltonians, the new version produced drastically different results. A systematic analysis revealed the root cause: the sampling budget -- the number of circuit executions (shots) per iteration. The library's implicit use of unlimited shots yielded dense probability distributions, whereas the v2 default of 10 000 shots captured only 23% of the state space. Increasing shots to 250 000 restored library-level accuracy. This study highlights how hidden parameters at the quantum--classical interaction level can dominate hybrid algorithm performance and provides actionable recommendations for developers and framework designers to ensure reproducible results in quantum software migration.

</details>


### [8] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: 本文提出了一种名为Token Sugar的概念，通过用可逆的简写替换频繁且冗长的代码模式来减少源代码中的token数量。该方法从代码语料库中挖掘高频、token密集的模式，并为每个模式分配一个独特的简写，然后将这些简写整合到大型语言模型（LLM）的预训练过程中。实验结果表明，这种方法不仅能在生成过程中显著减少token使用量（最多可达11.2%），而且保持了与基线模型相近的Pass@1得分。


<details>
  <summary>Details</summary>
Motivation: 由于编程语言固有的冗余性（如不必要的格式元素和冗长的样板代码），导致大型语言模型在处理代码生成和理解任务时面临高昂的计算成本问题。尽管已有工作尝试通过简化编程语言语法来减少token使用，但它们主要集中在语法层面的转换，未能充分利用语义层面的机会进一步减少token。

Method: 提出了Token Sugar概念，旨在通过可逆的简写形式替代常见且冗长的代码模式，从而降低源代码中的token数量。研究者设计了一个系统解决方案，包括从代码语料库中挖掘高频次、token密集型模式，给每个模式分配唯一的简写，并通过代码转换将其融入到大型语言模型的预训练中。

Result: 通过应用Token Sugar，研究人员获得了799对（代码模式，简写），能够减少高达15.1%的源代码token数量，并且与现有的侧重于语法的方法互补。进一步地，三个广泛使用的大型语言模型在经过Token Sugar增强的数据上进行了训练。实验结果显示，这些模型在生成过程中实现了显著的token节省（最高达到11.2%的减少），同时保持了与未经处理代码训练的基线模型几乎相同的Pass@1得分。

Conclusion: Token Sugar提供了一种有效途径以减少大型语言模型在执行代码相关任务时所需的token数量，从而降低了计算成本而不牺牲性能。此外，它还展示了与现有专注于语法优化的方法相结合的良好潜力。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [9] [FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection](https://arxiv.org/abs/2512.08277)
*Yihan Liao,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Jialong Li*

Main category: cs.SE

TL;DR: FedLAD是一个统一的平台，旨在联邦学习（FL）约束下训练和评估基于日志的异常检测（LAD）模型。它支持多种LAD模型、基准数据集和聚合策略的即插即用集成，并提供运行时支持以实现验证日志记录、参数调优和自适应策略控制。


<details>
  <summary>Details</summary>
Motivation: 现有的基于日志的异常检测方法通常假设集中式训练，这在实际应用中由于隐私限制和系统日志的分散特性而难以实现。虽然联邦学习为这一问题提供了潜在解决方案，但缺乏专门针对联邦设置下的LAD需求设计的测试平台。

Method: 提出了FedLAD平台，该平台能够支持不同LAD模型、基准数据集以及聚合策略之间的灵活集成，并且具备自我监控、自我配置与自我适应功能，以便于可重复性和可扩展性的实验研究。

Result: FedLAD成功地填补了联邦学习框架与LAD需求之间的空白，为未来的研究奠定了坚实的基础。

Conclusion: 通过提供一个专为基于日志的异常检测定制的联邦学习测试平台，FedLAD促进了该领域内更加高效和隐私友好的研究与发展。

Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.

</details>


### [10] [Measuring Agile Agreement: Development and Validation of the Manifesto and Principle Scales](https://arxiv.org/abs/2512.08461)
*Nicolas Matton,Anthony Simonofski,Marie-Ange Remiche,Benoît Vanderose*

Main category: cs.SE

TL;DR: 本文设计并验证了两种新的量表——敏捷宣言同意度量表（MAS）和原则同意度量表（PAS），以区分个人对敏捷宣言的价值观和具体实践的认同程度。通过系统性的项目创建、选择、调查设计和验证过程，研究证明了两个量表都具有良好的内部一致性和结构效度，并且虽然两者之间存在中等相关性，但它们捕捉的是敏捷认同的不同维度。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能有效地区分个体对于敏捷宣言中的抽象价值观与源自12原则的具体日常实践之间的认同差异，导致'敏捷认同'测量领域定义模糊且充满挑战。

Method: 本研究通过设计并验证两种不同的工具：全新的敏捷宣言同意度量表（MAS）以及经过系统调整和改进的原则同意度量表（PAS），来填补这一方法论空白。详细介绍了项目的创建与选择、问卷设计及验证过程。

Result: 结果显示，两个量表均显示出重要的内部一致性和构建有效性。通过比例几率逻辑回归、Bland-Altman图和组内相关系数(ICC)进行的收敛性和发散性分析表明，尽管两个量表之间有中等程度的相关性，但它们并非可互换的，而是捕捉到了敏捷认同的不同方面。

Conclusion: 这项工作的主要贡献在于提供了一对公开可用的工具，这些工具在比利时IT专业人士这一特定群体中得到了验证。这些量表代表了促进更细致地衡量敏捷认同的重要第一步，有助于区分不同层次的认知下的敏捷认同，并支持更加精细的人-敏捷匹配解释。

Abstract: While the importance of human factors in agile software development is widely acknowledged, the measurement of an individual's "agile agreement" remains an ill-defined and challenging area. A key limitation in existing research is the failure to distinguish between agreement with the abstract, high-level values of the Agile Manifesto and agreement with the concrete, day-to-day practices derived from the 12 Principles. This paper addresses this methodological gap by presenting the design and validation of two distinct instruments: the novel Manifesto Agreement Scale (MAS), and the Principle Agreement Scale (PAS), which is a systematic adaptation and refinement of a prior instrument.
  We detail the systematic process of item creation and selection, survey design, and validation. The results demonstrate that both scales possess important internal consistency and construct validity. A convergence and divergence analysis, including Proportional Odds Logistic Regression, a Bland-Altman plot, and an Intraclass Correlation Coefficient (ICC), reveals that while the two scales are moderately correlated, they are not interchangeable and capture distinct dimensions of agile agreement. The primary contribution of this work is a pair of publicly available instruments, validated within a specific demographic of Belgian IT professionals. These scales represent a critical initial step toward facilitating a more nuanced measurement of agile agreement, distinguishing agile agreement across various levels of perception and aiding in a more refined interpretation of person-agile fit.

</details>


### [11] [Measuring Computer Science Enthusiasm: A Questionnaire-Based Analysis of Age and Gender Effects on Students' Interest](https://arxiv.org/abs/2512.08472)
*Kai Marquardt,Robert Hanak,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 该研究通过区分年龄和性别对多样化的青少年样本的影响，提供了关于学生对计算机科学教育兴趣的新见解。研究发现，年龄比性别在塑造兴趣发展方面更为关键，并揭示了重要的发展转折点。尽管较年长的学生最初的态度较低，但他们在干预后表现出最大的积极变化，表明即使在较大年龄，精心设计的短期活动也能有效重新激活兴趣。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过解开年龄和性别对学生在计算机科学（CS）教育中的兴趣的不同影响，为理解这种兴趣提供新的视角。基于人-物兴趣理论（POI），研究者将热情视为一种短期、激活的兴趣表达形式，结合正面情感、相关性感知及再参与意愿。经历这样的热情可以暂时改变对CS的态度并加强未来参与意向，使其成为评估简短推广活动价值的重要视角。

Method: 研究开发了一种基于理论的问卷，用于前后评估CS干预措施的热情潜力。利用超过400名参加在线CS课程的学生的数据，研究人员考察了不同年龄段和性别间热情表现的模式。

Result: 研究结果挑战了早期接触是通往持续对CS感兴趣的主要途径这一普遍观点。相反地，研究发现青春期早期尤其是女孩中存在显著的热情下降趋势，同时各年龄段间兴趣轨迹存在很大差异。重要的是，分析显示，在塑造兴趣发展中年龄是一个比性别更加决定性的因素，并揭示了一些关键的发展转折点。值得注意的是，尽管开始时态度基线较低，但较年长的学生在干预后表现出最大幅度的积极性变化。

Conclusion: 总体而言，这项研究表明需要一个动态且考虑年龄敏感性的CS教育框架，其中教学策略应与发育轨迹相一致。

Abstract: This study offers new insights into students' interest in computer science (CS) education by disentangling the distinct effects of age and gender across a diverse adolescent sample. Grounded in the person-object theory of interest (POI), we conceptualize enthusiasm as a short-term, activating expression of interest that combines positive affect, perceived relevance, and intention to re-engage. Experiencing such enthusiasm can temporarily shift CS attitudes and strengthen future engagement intentions, making it a valuable lens for evaluating brief outreach activities. To capture these dynamics, we developed a theoretically grounded questionnaire for pre-post assessment of the enthusiasm potential of CS interventions. Using data from more than 400 students participating in online CS courses, we examined age- and gender-related patterns in enthusiasm. The findings challenge the prevailing belief that early exposure is the primary pathway to sustained interest in CS. Instead, we identify a marked decline in enthusiasm during early adolescence, particularly among girls, alongside substantial variability in interest trajectories across age groups. Crucially, our analyses reveal that age is a more decisive factor than gender in shaping interest development and uncover key developmental breakpoints. Despite starting with lower baseline attitudes, older students showed the largest positive changes following the intervention, suggesting that well-designed short activities can effectively re-activate interest even at later ages. Overall, the study highlights the need for a dynamic, age-sensitive framework for CS education in which instructional strategies are aligned with developmental trajectories.

</details>


### [12] [Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain](https://arxiv.org/abs/2512.08657)
*Renato Cordeiro Ferreira,Aditya Dhinavahi,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本经验报告介绍了在构建海洋守护者（一个用于海事领域异常检测的ML-启用系统）过程中应用的软件架构重用技术，特别是如何利用端口和适配器模式从单一代码库支持多个微服务的构建。


<details>
  <summary>Details</summary>
Motivation: 旨在通过分享Ocean Guard项目中遇到的挑战和吸取的经验教训，鼓励软件工程师、机器学习工程师以及数据科学家们采用六边形架构模式来开发他们的ML-启用系统。

Method: 采用了端口和适配器设计模式以促进代码复用，并支持基于同一代码库开发多个微服务。

Result: 成功地运用了六边形架构下的端口与适配器模式，实现了针对海事领域异常检测系统的高效且灵活的服务构建方式。

Conclusion: 该报告展示了如何有效地将六边形架构应用于ML-启用系统的设计当中，强调了此方法对于提高软件组件可重用性和灵活性的价值。

Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.

</details>


### [13] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: 本文介绍了一种基于大语言模型的方法RESTifAI，用于生成可复用且适用于CI/CD的REST API测试用例，同时处理了正向和负向场景以验证功能正确性和鲁棒性。该工具在多个方面与最新LLM工具表现相当，并解决了复用性、oracle复杂度及集成相关的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的API测试工具主要关注于内部服务器错误检测，而忽视了系统地构建有效的测试场景（即正常路径）以及派生出负面案例来验证预期功能（2xx响应）和对无效输入或业务规则违反的鲁棒性（4xx响应）。为了解决这些问题并提高测试用例的复用性和降低oracle复杂度，作者提出了RESTifAI。

Method: RESTifAI是一种由大语言模型驱动的方法，它能够系统化地创建符合‘快乐路径’策略的REST API测试，并且也能够生成一些负面案例来确保API不仅在正常情况下工作良好，而且能够妥善处理异常情况。

Result: 实验结果表明，RESTifAI的表现与最新的LLM工具AutoRestTest和LogiAgent相当，同时它还克服了关于测试用例复用性、oracle问题简化以及更顺畅集成等方面的限制。

Conclusion: 通过引入RESTifAI，作者提供了一种创新的方式来生成高质量的REST API测试用例，这些测试用例既考虑到了功能性的验证也考虑到了对抗非法输入或违反业务规则情况下的鲁棒性验证。此外，RESTifAI还强调了测试用例的复用价值及其易于集成到持续集成/持续部署流程中的特点。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [14] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 本文研究了代码生成模型的多校准方法，以提高其对代码正确性预测的信心分数准确性。通过在三个函数合成基准上测试四种多校准方法，发现相较于未经校准的模型和基础校准方法，多校准能够显著提升技能评分，并且公开了用于进一步研究的数据集。


<details>
  <summary>Details</summary>
Motivation: 随着基于AI的代码生成变得越来越普遍，研究者们正在探索如何校准代码语言模型，以确保它们的信心分数能准确反映代码正确的实际可能性。为此，作者引入了多校准的概念来捕捉编码问题中的额外因素，如复杂度、代码长度或所用编程语言等。

Method: 研究采用了最新的代码语言模型（Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill），并在在这三个函数合成基准上评估了四种不同的多校准方法的效果。

Result: 实验结果表明，与未经过校准的令牌似然值相比，使用多校准技术可以将技能评分提高1.03；相对于基线校准方法，则提高了0.37。此外，还探讨了上述因素在校准过程中影响的重要性。

Conclusion: 多校准方法能够有效改善代码语言模型对于代码正确性的预测准确性，为未来该领域的研究提供了新的方向。同时，公开发布的数据集也有助于促进后续相关工作的开展。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [15] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文分析了软件开发过程中知识问答任务的重要性及现有基准的局限性，提出并构建了一个新的多语言基准SimpleDevQA，旨在更好地评估大模型在开发知识问答方面的能力。实验结果表明代码模型在该任务上表现优于通用模型，且通过检索增强生成策略可以进一步提高准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决软件开发过程中的知识问答需求，并发现现有基准存在范围有限和非真实用户查询的问题，作者设计了一个将真实对话转换为问答对的流程，从而创建了一个新的基准数据集SimpleDevQA。

Method: 采用三阶段管道处理真实世界对话，将其转化为简单的开发知识问答对；基于此方法构建了包含2,740个问题答案对、涵盖三种语言（英语、中文、俄语）的SimpleDevQA基准。

Result: 实验显示代码专用的大规模语言模型比通用型同规模模型表现更好；利用检索增强生成策略可平均提升11.3%的准确率；同时观察到大规模语言模型在开发知识问答中表现出系统性的过度自信现象，其回答准确度与其声明的信心水平呈正相关关系；通常情况下，在代码生成方面性能更强的大规模语言模型也显示出在开发知识问答方面的更佳表现。

Conclusion: 研究强调了软件开发过程中知识问答任务的重要性，并通过建立新基准SimpleDevQA来解决现有评估标准的不足。此外，还揭示了特定领域模型的优势以及使用检索增强技术改善模型性能的可能性。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


### [16] [Exploring the Garden of Forking Paths in Empirical Software Engineering Research: A Multiverse Analysis](https://arxiv.org/abs/2512.08910)
*Nathan Cassee,Robert Feldt*

Main category: cs.SE

TL;DR: 研究通过多宇宙分析方法重新审视了一篇已发表的软件工程实证研究论文，揭示了方法选择对结果的重大影响，并建议研究人员补充标准报告以进行鲁棒性检查或明确证明每个分析决策。


<details>
  <summary>Details</summary>
Motivation: 在实证软件工程研究中，研究人员在处理数据、使用操作化方法和拟合统计模型方面有很大的自由度，这种自由虽然常被视为优势，但也对稳健性和可重复性构成威胁。为了更好地理解这一风险，本研究选取了一篇已发表的软件工程实证研究论文进行了所谓的多宇宙分析。

Method: 选取了一篇关于挖掘软件仓库的研究论文作为案例，确定了九个关键的分析决策点，每个都有至少一个同样合理的替代方案，并系统地在原始数据集上重新运行了所有3,072种可能的分析流程。

Result: 仅有6个（<0.2%）的分析流程再现了已发表的结果；绝大多数产生了定性不同的，甚至有时是相反的发现。

Conclusion: 这项针对应用于软件工程数据分析的方法案例研究表明，方法论的选择可以比通常认为的对结果产生更深远的影响。因此，提倡软件工程研究人员除了标准报告外，还应补充跨合理分析变体的鲁棒性检查，或者至少明确论证每个分析决定。此外，展示了多宇宙分析作为一种实用工具如何帮助提高科学工作的可靠性与可重复性。

Abstract: In empirical software engineering (SE) research, researchers have considerable freedom to decide how to process data, what operationalizations to use, and which statistical model to fit. Gelman and Loken refer to this freedom as leading to a "garden of forking paths". Although this freedom is often seen as an advantage, it also poses a threat to robustness and replicability: variations in analytical decisions, even when justifiable, can lead to divergent conclusions.
  To better understand this risk, we conducted a so-called multiverse analysis on a published empirical SE paper. The paper we picked is a Mining Software Repositories study, as MSR studies commonly use non-trivial statistical models to analyze post-hoc, observational data. In the study, we identified nine pivotal analytical decisions-each with at least one equally defensible alternative and systematically reran all the 3,072 resulting analysis pipelines on the original dataset. Interestingly, only 6 of these universes (<0.2%) reproduced the published results; the overwhelming majority produced qualitatively different, and sometimes even opposite, findings.
  This case study of a data analytical method commonly applied to empirical software engineering data reveals how methodological choices can exert a more profound influence on outcomes than is often acknowledged. We therefore advocate that SE researchers complement standard reporting with robustness checks across plausible analysis variants or, at least, explicitly justify each analytical decision. We propose a structured classification model to help classify and improve justification for methodological choices. Secondly, we show how the multiverse analysis is a practical tool in the methodological arsenal of SE researchers, one that can help produce more reliable, reproducible science.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [17] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出了一种新的性能评估工具链，结合了扩展的性能模型，用于基于消息的通信。该方法分析了MPI应用程序的数据访问模式，以预测使用CXL.mem技术进行数据交换可能带来的性能提升。


<details>
  <summary>Details</summary>
Motivation: 异构内存技术在解决HPC系统中的内存墙问题上变得越来越重要。虽然大多数技术部署在单节点设置中，但CXL.mem能够同时连接到多个节点，从而实现共享内存池。这种技术为高效的节点间通信提供了新的可能性。

Method: 作者开发了一个新的性能评估工具链和一个扩展的性能模型来预测如果使用CXL.mem而非传统MPI消息进行数据传输可能带来的性能改进。此方法通过分析MPI应用的节点内访问以及跨节点MPI流量来理解内存性能的影响，并结合这些数据在一个扩展的性能模型中，以每条MPI调用粒度工作，帮助识别并优化代码中可以利用CXL.mem加速的部分。为了构建这个工具链，他们还扩展了内存跟踪采样工具Mitos来提取数据访问行为。

Result: 提出的模型在两个示例应用上进行了验证——一个2D热传导迷你应用程序和HPCG基准测试，并展示了它们如何支持通过集成CXL.mem来进行有针对性的优化。

Conclusion: 研究介绍了一种新颖的方法来评估CXL.mem对于改善特定MPI调用性能的潜力，为将来利用CXL.mem技术进行高效数据交换提供了有价值的见解。

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [18] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS (CFS)是首个在POSIX兼容框架内整合多凭证功能的文件系统，利用DataCapsule作为存储提供者，并基于边缘计算领域的全局数据平面构建。尽管其读写性能较为普通，但其高度的功能正确性使其成为现实软件开发场景中的可行选择。


<details>
  <summary>Details</summary>
Motivation: 创建一个能够提供多凭证访问API并在POSIX兼容框架内运行的文件系统，以满足边缘计算领域的需求。

Method: 通过设计和实现由DataCapsule服务器、运行于可信执行环境中的中间件以及客户端组件构成的三部分架构来达成目标。

Result: CFS虽然在读写性能上表现一般，但它具有很高的功能正确性，这使得它适合用于实际的软件开发情境中。

Conclusion: CFS成功地实现了为边缘计算环境下的用户提供一个多凭证访问接口的目标，且未来的工作将侧重于进一步提高其实用性。

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [19] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: 本文介绍了一个名为Chopper的分析框架，它能够收集、对齐并可视化多粒度的GPU内核跟踪和硬件性能计数器。通过使用Chopper对Llama 3 8B模型在AMD InstinctTM MI300X GPU上的训练过程进行全面分析，揭示了之前未被充分探索的瓶颈与行为，并识别出频率开销是理论与实际性能差距的最大贡献者。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要集中在单个GPU微基准测试或内核级性能上，而针对多GPU环境下的大规模语言模型（LLM）训练过程中通信、计算、内存行为以及功耗管理之间复杂交互的研究相对不足。为了解决这个问题，作者们开发了Chopper这一分析工具，旨在提供一个多维度视角来理解这些交互如何影响整体效率。

Method: Chopper是一个专门设计用于收集、对齐和展示不同级别（从单独的内核到操作、层、阶段、迭代乃至整个GPU）下GPU内核追踪信息及硬件性能指标的框架。利用该工具，研究团队对采用完全分片数据并行(FSDP)技术，在由八个AMD InstinctTM MI300X组成的节点上进行训练的Llama 3 8B模型进行了详尽的端到端特征描绘。

Result: 研究表明，通过应用Chopper可以发现一些之前较少关注的问题点，比如确定性内存访问模式能够支持更高效稳定的GPU与内存运行频率。此外，还指出动态电压频率调整(DVFS)带来的频率开销是导致理论与实际性能差异的主要因素之一，其影响甚至超过了矩阵乘法累积(MFMA)利用率损失、通讯/计算重叠以及内核启动延迟等因素。

Conclusion: Chopper为AMD InstinctTM MI300X GPU上执行的大规模语言模型训练提供了首个全面且多层次的特性描述。它不仅有助于更好地理解当前系统中的低效之处，也为优化训练框架、改进功耗管理策略以及指导未来GPU架构和系统设计提供了有价值的见解。

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [20] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文介绍了一种名为HERMES的新框架，该框架旨在通过智能编排资源、分布式市场中的数据和服务变现以及语义互操作性来改变计算连续体中的连接性和数据利用。


<details>
  <summary>Details</summary>
Motivation: 当前解决方案由于在可扩展性、互操作性和信任方面的固有限制，难以满足从云到边缘的超分布式应用需求。

Method: 提出了HERMES框架，它创造了一个开放、无缝和安全的环境，能够智能地编排从云服务器到微小边缘设备的各种资源，并通过分布式市场实现数据和服务的货币化，同时借助语义互操作性分享知识。

Result: HERMES为新一代更加高效、可靠且自主的分布式应用程序奠定了基础。

Conclusion: HERMES通过结合关键方面如智能资源管理、分布式市场的数据与服务货币化及语义互操作性，解决了现有方案在支持跨云计算至边缘端的超分布式应用时遇到的问题。

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [21] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: This paper introduces high-performance methods for emulating single- and double-precision complex matrix multiplication on INT8 hardware, achieving significant speedups over native cuBLAS routines while also offering flexibility in accuracy-speed trade-offs.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the trend towards low-precision computing architectures, which offer higher throughput compared to high-precision units. The goal is to develop efficient techniques for emulating high-precision matrix multiplication using these low-precision hardware, specifically for complex numbers, following the successful application of similar techniques for real numbers.

Method: The researchers build upon the Ozaki-II scheme, a general framework for emulating matrix multiplication, to develop their methods for single- and double-precision complex matrix multiplication on INT8 matrix engines. These methods are designed to be both high-performance and power-efficient.

Result: The proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, when tested on an NVIDIA B200 GPU. Additionally, they provide options for trading off between speed and accuracy, allowing for either faster execution with lower precision or slightly slower but more accurate results than standard routines.

Conclusion: The study concludes that the developed methods for emulating complex matrix multiplications on INT8 hardware not only significantly outperform traditional high-precision implementations in terms of speed but also offer a flexible balance between computational efficiency and result accuracy, making them suitable as a default algorithm across various applications.

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [22] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: 本文提出了一种新的方法——差异能量调试，通过该方法开发了Magneton这一能量分析工具，能够比较类似机器学习系统之间的能耗，并自动识别导致过度能耗的代码区域和配置选择。


<details>
  <summary>Details</summary>
Motivation: 现有的优化工作主要集中在硬件能效上，而软件设计不佳导致的能源浪费往往被忽视。这种低效率在广泛使用的机器学习框架和应用中出现，但开发者通常缺乏检测和诊断这些问题的可见性和工具。

Method: 提出了差异能量调试方法，基于不同ML系统即使实现相似功能时也可能存在巨大能耗差异这一观察结果。基于此洞察，设计并实现了Magneton，一个能够在操作员级别比较相似ML系统之间能耗的能量分析器，自动定位造成过度能耗的代码区域和配置选项。

Result: 将Magneton应用于9个流行的ML系统（包括LLM推理、通用ML框架和图像生成），成功检测并诊断出16个已知的软件能耗效率低下案例，并进一步发现了8个先前未知的情况，其中7个已被开发者确认。

Conclusion: 通过引入差异能量调试及其实现工具Magneton，研究人员为解决软件层面的机器学习模型能耗问题提供了一条新途径，有助于提高整体系统的能效。

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [23] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文研究了轻量级线程（也称为异步调用或协程）访问的互斥锁实现，指出传统操作系统线程设计的互斥锁在新环境中可能引发死锁问题，并且不同的轻量级线程库需要特定的锁实现。文章提出了对TTAS和MCS锁的修改版本以适应轻量级线程，并建议使用队列锁来平衡不同设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 针对轻量级线程访问的数据结构的研究不足，特别是互斥锁的设计需要考虑手动上下文切换的特点。此外，不同编程语言中的轻量级线程库有着不同的实现和接口，这要求开发与之相适应的不同锁机制。

Method: 通过对现有的TTAS和MCS锁进行修改，使其适用于轻量级线程环境。研究中比较了两种上下文切换机制：yielding（让出）和sleeping（休眠），并分析它们对于锁性能的影响。

Result: 实验结果显示，修改后的TTAS和MCS锁能够在轻量级线程环境下有效工作，但其性能会根据具体设置而有很大差异。为了解决这个问题，作者推荐了一种结合了多个MCS队列与一个共享TTAS的队列锁方案。

Conclusion: 为了确保轻量级线程能够高效地使用互斥锁，需要特别设计适合这种环境的锁机制。文中提出的队列锁方案可以作为一种通用解决方案，在多种轻量级线程库中提供较好的性能。

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [24] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文介绍了一种基于模型测试的方法，用于为采用actor模型编写的分布式系统生成详尽的测试套件，从而确保实现与形式化规范的一致性。


<details>
  <summary>Details</summary>
Motivation: 设计和实现正确的分布式系统面临挑战，尽管这些系统通常会附带经过模型检查技术验证的形式化规范，但实现与其形式化规范之间仍存在差距。

Method: 提出了一种能够将系统模型解释为有限状态机的方法，并针对使用actor模型编写的分布式系统高效地生成覆盖所有可能状态和转换的详尽测试套件。该方法不需要对代码进行任何修改或干扰分布式系统的执行环境。

Result: 通过一个基于Viewstamped Replication复制算法的真实世界系统实现作为例子进行了验证。

Conclusion: 通过应用所提出的方法，可以有效地缩小分布式系统实现与其形式化规范之间的差距，而无需对现有系统进行改动。

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [25] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 本研究通过模拟分析了云工作负载在空间和时间上的转移对减少碳、水和土地使用足迹的潜力。结果显示，空间转移可以显著降低这些足迹，而时间转移也有助于减少但效果较小。结合两种策略能实现最大的总体减排效果，并且这种转移对于预测误差和季节变化具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨通过调整云服务的工作负载分布（包括地理位置和时间安排）来降低其对环境的影响，特别是减少碳排放、水资源消耗以及土地利用。

Method: 采用来自多个云服务商的真实数据进行仿真研究，涵盖了不同应用场景如大数据分析与函数即服务(FaaS)等。

Result: 空间转移显示出极大的潜力，可使碳、水及土地使用的足迹减少20%至85%，具体取决于场景设置和优化目标；时间转移同样有助于减少足迹，但程度较低；当同时应用这两种策略时，可以获得最佳的整体减排效果。

Conclusion: 通过合理规划云计算资源的空间布局和使用时机，能够有效减轻其对自然环境造成的负担，为构建更加绿色可持续的信息技术基础设施提供了新的思路。

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [26] [Analyzing Deviations from Monotonic Trends through Database Repair](https://arxiv.org/abs/2512.08526)
*Shunit Agmon,Jonathan Gal,Amir Gilad,Ester Livshits,Or Mutay,Brit Youngmann,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 本文提出了一种名为聚合顺序依赖（AODs）的概念，用于量化数据集偏离预期单调趋势的程度，并探讨了通过删除最少数量的元组来修复AOD问题的方法。


<details>
  <summary>Details</summary>
Motivation: 数据集中经常出现违反预期单调趋势的情况，比如更高的教育水平与平均工资成正比等。本文旨在量化数据集偏离这种趋势的程度，并提供一种解决方案。

Method: 定义了聚合顺序依赖(AODs)，作为之前研究的顺序依赖性的以聚合为中心的扩展。将AOD修复问题定义为从表中找到要删除的最小元组集合，以便满足给定的AOD。分析了该问题的计算复杂性并提出了一个通用算法模板来解决它。

Result: 实验研究表明，所提出的算法在实际应用中是高效的，并且提供了关于启发式方法性能的见解。案例研究还展示了如何使用该框架发现和解释意外的AOD违规行为。

Conclusion: 通过引入AODs概念以及开发相关算法和优化技术，本文为理解和修正数据集中违反单调趋势的问题提供了有效途径。

Abstract: Datasets often exhibit violations of expected monotonic trends - for example, higher education level correlating with higher average salary, newer homes being more expensive, or diabetes prevalence increasing with age. We address the problem of quantifying how far a dataset deviates from such trends. To this end, we introduce Aggregate Order Dependencies (AODs), an aggregation-centric extension of the previously studied order dependencies. An AOD specifies that the aggregated value of a target attribute (e.g., mean salary) should monotonically increase or decrease with the grouping attribute (e.g., education level).
  We formulate the AOD repair problem as finding the smallest set of tuples to delete from a table so that the given AOD is satisfied. We analyze the computational complexity of this problem and propose a general algorithmic template for solving it. We instantiate the template for common aggregation functions, introduce optimization techniques that substantially improve the runtime of the template instances, and develop efficient heuristic alternatives. Our experimental study, carried out on both real-world and synthetic datasets, demonstrates the practical efficiency of the algorithms and provides insight into the performance of the heuristics. We also present case studies that uncover and explain unexpected AOD violations using our framework.

</details>


### [27] [Causal Explanations for Disparate Trends: Where and Why?](https://arxiv.org/abs/2512.08679)
*Tal Blau,Brit Youngmann,Anna Fariha,Yuval Moskovitch*

Main category: cs.DB

TL;DR: 本文介绍了一个名为ExDis的框架，用于发现两组兴趣之间差异的因果解释。该框架不仅能识别差异最显著的数据区域（子群体），还能关联在每个已识别数据区域内对差异有因果贡献的具体因素。通过实验证明，ExDis能够生成有意义的因果解释，优于先前的方法，并且能够有效地扩展以处理大规模、高维度的数据集。


<details>
  <summary>Details</summary>
Motivation: 在数据分析过程中，观察到的数据集中两组兴趣之间的某些差异常常令人困惑。为了更好地理解这些观察到的差异，需要能够指出差异最为显著的数据区域及其原因的解释，即缓解或加剧差异的因素。特别是对于大型和高维数据集来说，这是一项复杂而繁琐的任务，因此需要一个自动系统来发现观察到的差异的原因（数据区域和因素）。重要的是，对于差异的解释不仅要可解读，而且还要可操作性，使用户能够做出明智的、基于数据的决策。这就要求解释超越表面的相关性，而是捕捉因果关系。

Method: 提出了ExDis框架，用于发现两组兴趣间差异的因果解释。该框架设计了相关优化问题并分析其复杂性，开发了一种有效的算法来解决这个问题。ExDis可以识别差异最显著（或逆转）的数据区域（子群体），并确定在每个已识别数据区域内对差异产生因果影响的具体因素。

Result: 通过三个真实世界数据集上的广泛实验表明，ExDis能够生成有意义的因果解释，表现优于先前方法，并能有效扩展以应对大型、高维度数据集。

Conclusion: ExDis框架为理解数据中两组间的差异提供了强有力的支持，它不仅能够准确地定位差异最大的数据区域，还能够揭示造成这种差异背后的因果因素，从而帮助用户根据数据驱动做出更明智的决策。

Abstract: During data analysis, we are often perplexed by certain disparities observed between two groups of interest within a dataset. To better understand an observed disparity, we need explanations that can pinpoint the data regions where the disparity is most pronounced, along with its causes, i.e., factors that alleviate or exacerbate the disparity. This task is complex and tedious, particularly for large and high-dimensional datasets, demanding an automatic system for discovering explanations (data regions and causes) of an observed disparity. It is critical that explanations for disparities are not only interpretable but also actionable-enabling users to make informed, data-driven decisions. This requires explanations to go beyond surface-level correlations and instead capture causal relationships. We introduce ExDis, a framework for discovering causal Explanations for Disparities between two groups of interest. ExDis identifies data regions (subpopulations) where disparities are most pronounced (or reversed), and associates specific factors that causally contribute to the disparity within each identified data region. We formally define the ExDis framework and the associated optimization problem, analyze its complexity, and develop an efficient algorithm to solve the problem. Through extensive experiments over three real-world datasets, we demonstrate that ExDis generates meaningful causal explanations, outperforms prior methods, and scales effectively to handle large, high-dimensional datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver, a new framework for adaptive parallel reasoning in large language models, matches the accuracy of leading sequential reasoning models while reducing inference latency, through innovations like a two-stage parallel trajectory generator, trie-based training-inference co-design, and a reinforcement learning framework that balances accuracy with parallelization. It demonstrates up to 1.53x speedup in token latency across six mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency of inference in Large Language Models (LLMs) by addressing the high latency caused by sequential decoding, especially on complex tasks, without sacrificing accuracy or requiring customized inference engines.

Method: ThreadWeaver introduces a three-pronged approach: a two-stage parallel trajectory generator for creating high-quality CoT data, a trie-based method compatible with off-the-shelf autoregressive inference engines, and a reinforcement learning framework that teaches the model to balance accuracy with parallelization.

Result: Across six challenging mathematical reasoning benchmarks, ThreadWeaver, when trained on top of Qwen3-8B, achieves an average accuracy of 71.9% and up to 79.9% on AIME24, while providing an average speedup of 1.53x in token latency compared to sequential reasoning models.

Conclusion: ThreadWeaver successfully establishes a new Pareto frontier between accuracy and efficiency for LLMs, offering a solution that is both accurate and efficient, without the need for custom inference engines, making it a practical choice for deployment.

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [29] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: 本文提出了RaX-Crash模型，用于纽约市交通事故伤害严重程度的预测。通过集成多个数据表、构建统一特征模式，并训练紧凑型树集合模型（如随机森林和XGBoost），与小型语言模型相比，在测试集上表现更优。研究表明，可解释的小型模型集合对于城市规模的伤害分析来说是强有力的基础，而结合表格预测器与小型语言模型生成的叙述可以改善沟通而不牺牲可扩展性。


<details>
  <summary>Details</summary>
Motivation: 面对纽约市每年报告的大量机动车碰撞事故及其带来的健康负担，需要开发一种高效且可解释的方法来预测事故伤害严重程度。

Method: 开发了RaX-Crash系统，该系统整合了三个包含数千万条记录的数据表，创建了一个分区存储中的统一特征架构，并基于此训练了紧凑型的树状集成模型（随机森林和XGBoost）。此外，还与本地部署的小型语言模型进行了对比实验。

Result: 在时间上保留的测试集上，XGBoost和随机森林分别达到了0.7828和0.7794的准确率，明显优于小型语言模型的表现（分别为0.594和0.496）。类别不平衡分析表明，简单的类别加权可以在适度降低整体准确性的前提下提高致命事件召回率；SHAP归因则强调了人类脆弱因素、时机及地点作为预测严重程度的主要驱动因素。

Conclusion: 研究结果表明，可解释的小型模型集合在城市规模的伤害分析中仍然保持为强基线方法；同时，结合表格预测器与小型语言模型产生的叙述能够增强交流效果而不损害系统的可扩展性。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [30] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: 本文提出了一种基于对数域注意力预测的算法-架构协同设计（LAPA），通过创新性计算方案和策略，提高了Transformer模型在不同阶段的稀疏加速效率，并在实验中显示出比现有技术更高的能效。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏Transformer方法大多基于单阶段设计，当应用于多个阶段时，其稀疏性预测机制会导致显著的功耗开销。随着输入序列的变化，Transformer模型中的计算瓶颈表现出跨阶段的动态行为，这要求有一种跨阶段的稀疏加速策略。

Method: 提出了一个名为LAPA（Log-domain Attention Prediction Algorithm-architecture co-design）的方法，包括：1. 不对称领先一计算（ALOC）方案来消除昂贵的乘法操作；2. 混合精度多轮移位累加（MRSA）机制减少累积开销；3. 数据特征依赖过滤（DDF）策略与MRSA过程协同工作；4. 设计了专门的加速器将理论增强转化为实际硬件改进。

Result: 实验结果表明，LAPA相比最先进作品Spatten、Sanger和FACT分别实现了3.52倍、3.24倍及2.79倍的能量效率提升。

Conclusion: 通过引入LAPA这一创新性解决方案，针对Transformer模型存在的跨阶段计算瓶颈问题提供了一条有效路径，极大地提升了处理自然语言处理和计算机视觉任务时的能效表现。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [31] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: 本文提出了一种基于图的深度学习模型CLDD，用于疾病检测。通过利用疾病间的关联和患者间的相似性，该模型能够减少对具体医疗测试的依赖，并在大规模疾病筛查中表现出色。实验表明，CLDD相比基准方法在召回率和精确度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 准确的疾病检测对于有效的医疗治疗和患者护理至关重要，但这一过程通常伴随着大量的医学检查和高昂的成本。这使得为每位患者进行全面的医学测试以诊断或预测数百乃至数千种疾病变得不切实际。因此，需要一种新的方法来提高疾病检测效率同时降低成本。

Method: 提出了协作学习疾病检测（Collaborative Learning for Disease Detection, CLDD）模型，这是一种新颖的基于图的深度学习方法，它将疾病检测视为一个协作学习任务，通过自适应地利用疾病之间的联系以及患者之间的相似性来进行。CLDD整合了电子健康记录中的患者-疾病交互作用和人口统计特征，能够在几乎不依赖相应的医学测试的情况下为每位患者检测出数百甚至数千种疾病。

Result: 在处理过的MIMIC-IV数据集上的广泛实验显示，CLDD在多个指标上始终优于代表性基线方法，实现了6.33%的召回率改善和7.63%的精确度提升。此外，针对个别患者的案例研究表明，CLDD能够在顶级预测中成功恢复被掩盖的疾病，展示了其在疾病预测方面的可解释性和可靠性。

Conclusion: CLDD通过减少诊断成本并提高医疗服务的可及性，在大规模疾病筛查和社会健康安全保障方面展现出巨大潜力。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [32] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: 本文提出了SA^2GFM框架，通过结构感知语义增强提高图基础模型的鲁棒性和领域适应性。该框架利用基于熵的编码树转化为结构感知文本提示以增强特征，并通过信息瓶颈机制提炼出鲁棒可迁移表示。为解决跨域适应中的负面迁移问题，引入了专家自适应路由机制。实验表明，SA^2GFM在节点和图分类任务上优于9种最先进基线方法，在抵抗随机噪声和对抗扰动方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的图基础模型（GFMs）虽然在各种任务中取得了显著进展，但在应对领域噪声、结构扰动以及对抗攻击方面的鲁棒性仍待探索。此外，对层次结构语义建模不足限制了模型的泛化能力。

Method: 提出SA^2GFM框架，主要包含以下几点：1. 通过将基于熵的编码树转换成结构感知文本提示来增强输入特征；2. 利用自我监督的信息瓶颈机制处理增强后的输入，通过结构引导压缩提取鲁棒且可转移的表示；3. 采用专家自适应路由机制处理跨域适应时可能出现的负迁移现象；4. 设计了一个微调模块，通过联合内部和社区间结构学习优化层次结构。

Result: 广泛的实验证明，对于节点和图分类任务，SA^2GFM不仅在效果上超越了9个最新的基准方法，而且在面对随机噪音及对抗性干扰时也展现出了更高的鲁棒性。

Conclusion: 本研究开发了一种新的鲁棒图基础模型框架——SA^2GFM，它通过结构感知语义增强提高了模型的领域适应能力和鲁棒性，为未来的研究提供了新思路。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [33] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: 提出了一种轻量级的频率感知交互型曼巴模型(FAIM)用于时间序列分类(TSC)，通过自适应滤波块和交互型曼巴块解决了现有深度学习架构在TSC任务中的局限性，如高计算成本、对噪声敏感及过拟合等问题。实验表明FAIM在多个基准测试中优于当前最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习架构虽然能够捕捉时间依赖关系，但在时间序列分类任务上存在高计算成本、对噪声扰动敏感以及在小规模数据集上容易过拟合的问题。为了解决这些问题，并提高模型在复杂时间模式理解和高噪声场景下的鲁棒性，提出了FAIM模型。

Method: FAIM由两个主要组件构成：自适应滤波块（AFB）与交互型曼巴块（IMB）。AFB利用傅里叶变换从时序数据中提取频域特征，并结合可学习的自适应阈值动态抑制噪声；同时，通过全局和局部语义自适应过滤之间的元素级耦合，深入建模不同频率成分间的协同作用。IMB则旨在促进多粒度信息的有效互动，在提取精细判别特征的同时保持全面的全局上下文信息获取。此外，还引入了自我监督预训练机制来增强模型对复杂时间模式的理解能力及其跨领域的鲁拨性。

Result: 广泛的实验证明，相比现有最先进的方法，FAIM不仅在准确性上表现出色，而且具有更高的效率。它在处理不同类型的时间序列数据集时均能展现出卓越的表现力和稳定性。

Conclusion: FAIM作为一种新颖的时间序列分类方法，成功地克服了传统深度学习模型面临的挑战，包括高运算负担、易受噪音影响及过拟合风险等。通过引入独特的自适应滤波技术和交互式信息处理机制，FAIM能够在保证高效性的同时提供准确可靠的分类结果。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [34] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: 提出了一种新的半监督异常检测框架SetAD，该框架将异常检测任务视为集合级别的任务，通过基于注意力机制的集合编码器和上下文校准的异常评分机制来学习复杂的数据组内交互，从而提高异常检测的准确性。实验表明，随着集合大小的增加，SetAD的表现持续优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督异常检测方法通常只关注单个点或简单对之间的关系，忽略了异常值是由其偏离集体群体定义的这一事实，也未能充分利用可以从集合组合中产生的丰富监督信号。因此，这些模型难以利用对于学习区分性表示至关重要的高阶数据交互。

Method: SetAD采用了一个基于注意力机制的集合编码器，并通过分级学习目标进行训练，以量化整个集合内的异常程度。此外，为了增强鲁棒性和评分校准，提出了一个上下文校准的异常评分机制，该机制通过汇总一个点在多个多样化上下文集合中的标准化偏差来评估其异常得分。

Result: 在10个真实世界数据集上的广泛实验表明，SetAD显著优于最先进模型，并且随着集合规模的增加，模型性能持续改善，为基于集合的方法提供了强有力的实证支持。

Conclusion: SetAD通过引入集合级别的异常检测方法以及上下文校准的异常评分机制，在半监督异常检测上取得了显著进展，证明了这种方法的有效性和优越性。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [35] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 本文提出了一种使用无监督机器学习框架来监测环境条约遵守情况的方法，通过分析大量复杂的海关数据集，结合了K-Means聚类、异常检测和启发式标记等技术，并成功识别出1,351个价格异常值和1,288个高优先级的货物用于海关审查。


<details>
  <summary>Details</summary>
Motivation: 需要新的方法来监控像《蒙特利尔议定书》这样的环境条约，通过对大量的复杂海关数据进行审查。

Method: 论文中引入了一个框架，该框架利用无监督机器学习系统地检测可疑贸易模式，并突出显示需要审查的活动。研究方法应用于10万条贸易记录，结合了多种机器学习技术：无监督聚类（K-Means）基于货物价值和重量发现自然贸易原型；异常检测（孤立森林法和四分位数范围）识别罕见的“巨量交易”以及商业上不常见的每公斤价格值的货物；并辅以启发式标记来查找诸如模糊的货物描述之类的策略。这些层次被综合成一个优先级分数。

Result: 模型成功识别了1,351个价格异常值和1,288个高优先级的货物供海关审查。重要发现是高优先级商品显示出与一般商品不同的且更有价值的价值-重量比。这通过可解释的人工智能（SHAP）得到验证，确认了模糊描述和高价值是最显著的风险预测因素。模型的敏感性也得到了验证，因为它检测到了2021年初“巨量交易”的巨大峰值，这直接与美国AIM法案的实际监管影响相关联。

Conclusion: 这项工作展示了一个可重复的无监督学习流程，能够将原始贸易数据转化为优先排序的、可用的情报，为监管团体提供支持。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [36] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 本研究将大规模瑞典登记数据转化为文本生命轨迹，以解决数据分析中的两个长期难题：分类变量的高基数和随时间变化的编码方案不一致。通过将690万个体（2001-2013年）的数据转换为语义丰富的文本，并预测个人在后续几年（2013-2017年）的居住流动性。比较了多种NLP架构（包括LSTM、DistilBERT、BERT和Qwen），发现序列模型和基于transformer的模型比基线模型更有效地捕捉到了时间与语义结构。结果表明，文本化登记数据保留了关于个人路径的有意义信息，并支持复杂的可扩展建模。


<details>
  <summary>Details</summary>
Motivation: 旨在解决数据分析中面临的两大挑战：分类变量的高基数问题以及随时间推移出现的编码方案不一致性。同时，希望通过结合语义丰富的登记资料与现代语言模型来推动社会科学领域内纵向分析的进步。

Method: 利用全面的人口登记册，将2001至2013年间690万人的注册数据转变为包含人口统计学信息及每年居住地、工作、教育、收入和家庭状况变动情况的语义丰富文本。然后运用多种自然语言处理架构（如LSTM、DistilBERT、BERT和Qwen）进行对比实验，评估这些序列对支持长期预测的有效性。

Result: 结果显示，相较于基准模型，序列型和基于Transformer的模型能够更加有效地捕捉到时间上和语义上的结构特征。文本化的注册数据不仅保留了关于个人发展路径的重要信息，还支持了复杂且可扩展的建模需求。

Conclusion: 结合富含语义的注册数据与现代语言模型可以显著促进社会科学领域的纵向分析方法的发展。此外，由于很少有国家能维持如此覆盖广泛且精确度高的纵向微观数据集，因此该数据集为开发和评估新的序列建模方法提供了严格的测试平台。

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [37] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种基于专家混合模型（MoE）的噪声估计器，用于生理时间序列信号重建。通过设计自适应感受野的MoE模块（RFAMoE）和融合MoE模块，该方法能够在单一推理步骤中生成并融合多个噪声信号，从而提高性能同时减少了计算成本与延迟。实验结果显示，所提框架在不同任务及数据集上均优于当前最先进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在时间序列信号重建方面展现出了巨大潜力，但在医学时间序列领域尚未得到充分探索。由于生理时间序列信号具有多变量、高时间变异性、极高噪声以及易受干扰等特点，使得基于深度学习的方法在诸如插补等任务上仍然面临挑战。此外，虽然多次推断后平均重构信号能够有效减少误差，但这带来了显著的计算负担与延迟问题。

Method: 1. 提出了一种基于得分的扩散框架下的Mixture of Experts (MoE) 噪声估计器。
2. 设计了Receptive Field Adaptive MoE (RFAMoE) 模块，使每个通道能在扩散过程中自适应选择所需的感受野。
3. 引入Fusion MoE模块，利用MoE的本质并行生成K个噪声信号，并通过路由机制将它们融合，在单次推理步骤中完成信号重建。

Result: 广泛的实验结果表明，所提出的框架在不同的任务和数据集上始终优于基于扩散模型的最先进工作。

Conclusion: 通过引入RFAMoE和Fusion MoE模块，本文所提出的基于MoE的噪声估计器不仅提高了生理时间序列信号重建任务中的性能，还成功解决了多轮推理带来的额外计算开销和时延问题。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [38] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器学习模型，称为Softly Symbolified Kolmogorov-Arnold Networks (S2KAN)，该模型在训练过程中直接集成了符号原语。通过这种方式，S2KAN能够在保持或超越现有方法准确性的同时，发现更易于解释的形式，并且能够自适应地简化表示形式。


<details>
  <summary>Details</summary>
Motivation: 虽然Kolmogorov-Arnold Networks（KANs）为可解释的机器学习提供了一个有前景的方向，但实践中训练出的激活函数往往缺乏符号保真度，导致分解结果没有实际意义。因此，作者旨在开发一种既能保持高精度又能生成有意义的、易于理解的模型结构的方法。

Method: 提出了Softly Symbolified Kolmogorov-Arnold Networks (S2KAN) 方法，该方法允许每个激活从一个包含符号和密集术语的字典中选择，同时通过可学习的门机制来稀疏化表示。此过程是可微分的，支持端到端优化，并受到最小描述长度目标的指导。

Result: 实验表明，在符号基准测试、动力系统预测以及真实世界预测任务上，S2KAN相比其他方法具有竞争力甚至更优的表现，同时使用了明显较小的模型规模。此外，即使没有正则化压力，也观察到了自我稀疏化的现象。

Conclusion: S2KAN成功地将符号表达与机器学习相结合，在确保模型准确性和效率的同时增强了模型的可解释性。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [39] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 提出了一种新的损失函数SpecMatch-CL，通过最小化归一化Laplacian之间的差异来对齐视图特定的图-of-图，并在理论上证明了该方法的有效性。实验结果表明，在无监督学习和半监督学习中，该方法在多个基准测试上达到了最新的技术水平，并且在迁移学习任务上也取得了持续的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法如InfoNCE能够优化跨视图图嵌入的一致性，但缺乏控制由这些嵌入构建的视图特定图-of-图的全局结构机制。

Method: 引入了一种名为SpecMatch-CL的新损失函数，它通过最小化不同视角下图-of-图的归一化Laplacian之间的差异来实现对齐。此外，理论分析表明，在某些假设条件下，归一化Laplacian之间的差异为理想完美对齐对比损失与当前损失之差提供了上界，同时也适用于Uniformly损失。

Result: 实验部分展示了SpecMatch-CL在八个TU基准测试（无监督和低标签率下的半监督学习场景）中创造了新纪录，并且在PPI-306K和ZINC 2M数据集上的迁移学习任务中表现出一致的性能提升。

Conclusion: SpecMatch-CL提供了一种有效的方法来改善图表示学习中的全局结构一致性问题，其不仅在理论上得到了支持，而且通过广泛的实验证明了其实用价值。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [40] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文从几何角度重新审视了非负矩阵分解(NMF)，提出了Cone Collapse算法，该算法能够找到数据生成的最小锥，并基于此开发了锥感知的正交NMF模型(CC-NMF)。实验结果表明CC-NMF在聚类纯度方面优于或等同于多个强大的NMF基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的NMF及ONMF变体通常是从优化的角度推导出来的，并没有明确地利用NMF诱导出的锥形几何结构：数据点位于一个凸锥内，其极射线编码了基本方向或‘主题’。为了更好地利用这种几何特性，作者提出了一种新的方法来改进NMF的应用效果。

Method: 提出了一种名为Cone Collapse的新算法，它从整个非负象限开始，逐步缩小范围直至达到由数据生成的最小锥。基于此，通过将单正交NMF应用于恢复的极射线上，进一步发展了一个锥感知的正交NMF模型（即CC-NMF）。

Result: 在16个基准基因表达、文本和图像数据集上进行测试时，CC-NMF在聚类纯度方面始终与强NMF基线——包括乘法更新、ANLS、投影NMF、ONMF以及稀疏NMF——相匹配甚至超越。

Conclusion: 显式地恢复数据锥可以产生理论基础扎实且经验上表现优秀的基于NMF的聚类方法。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [41] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本研究发现对比学习中存在一个关键的学习率阈值，超过该阈值后会导致维度坍缩问题。基于此，提出了一种新的半监督损失函数CLOP来促进类别嵌入之间形成正交线性子空间，从而防止维度坍缩。实验表明，CLOP在图像分类和物体检测任务上提高了性能，并且对不同的学习率和批量大小表现出更高的稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对对比学习中存在的维度坍缩问题，特别是当使用标准对比损失时，在半监督和自监督设置下，随着学习率的变化，模型的嵌入可能会收敛到一个更低维度的空间内。这种现象限制了模型学习有效特征表示的能力。

Method: 首先定义了一个关键的学习率阈值点，超过这个阈值使用传统的对比损失训练会导致维度坍缩。接着介绍了CLOP（Contrastive Learning with Orthogonal Projections），这是一种旨在通过鼓励不同类别的嵌入形成正交线性子空间来避免维度坍缩的新方法。

Result: 通过在真实数据集和合成数据集上的广泛测试显示，CLOP不仅能够提高图像分类与目标检测任务中的表现，而且相比于传统方法，它对于变化的学习率及批次大小展现出更强的鲁棒性。

Conclusion: 研究证明了CLOP作为一种有效的解决方案，可以克服对比学习中遇到的维度坍缩难题，为提升相关视觉任务的表现提供了新思路。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [42] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2通过联合算法-系统重新设计，解决了原始GSPN实现中的GPU内核重复启动、全局GPU内存数据传输过多以及为每个通道保持独立传播权重导致的冗余计算问题，实现了在图像分类和文本到图像合成任务上以显著更低的计算成本达到与transformer相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的GSPN虽然通过线扫描传播方案接近线性地降低了高分辨率图像及长视频应用的成本，但仍存在由于反复启动GPU内核造成的沉重开销、从全局GPU内存进行过多的数据转移以及为每个通道维护单独传播权重导致的冗余计算等问题。

Method: GSPN-2引入了联合算法-系统重新设计，将数千次微小启动合并为一个单一2D内核，明确地将一个warp固定到每个通道切片，并利用共享内存暂存前一列的激活。模型方面，引入了一种紧凑的通道传播策略来替换每通道矩阵，减少了参数量，并自然地与变压器注意力中使用的亲和图对齐。

Result: 实验表明，GSPN-2在图像分类和文本到图像合成任务上能够以显著降低的计算成本达到与transformer相当的准确性。

Conclusion: GSPN-2通过其独特的结构化矩阵变换与GPU优化实现相结合的方式，在视觉应用中建模全局空间上下文方面设定了新的效率前沿。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [43] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: 本文介绍了一种新的符号回归算法Brush，该算法结合了决策树式的分割算法与非线性常数优化，旨在将基于规则的逻辑无缝集成到符号回归和分类模型中。Brush在SRBench上达到了帕累托最优性能，并被应用于重新构建两个广泛使用的临床评分系统，相比其他方法，它能够产生更简单且具有可比或更优预测性能的模型。


<details>
  <summary>Details</summary>
Motivation: 传统的符号回归算法（SR）仅限于连续函数形式及其参数搜索空间，这使得它难以模拟包含规则的医疗决策过程。然而，由于SR能够推导出数据驱动且可解释的模型，因此它有潜力开发出数据驱动的临床风险评分。为此，研究者们提出了Brush算法，旨在解决这一问题。

Method: 提出了一种名为Brush的新算法，该算法通过结合类似决策树的分割算法和非线性常数优化技术，实现了基于规则逻辑与符号回归及分类模型的无缝整合。

Result: Brush算法在SRBench基准测试中表现出了帕累托最优性能；当应用于重构两个广为接受的临床评分体系时，相较于决策树、随机森林及其他SR方法，Brush不仅能够保持相当甚至更佳的预测准确率，同时还能生成更加简洁易懂的模型。

Conclusion: Brush算法为医疗领域提供了有效工具，用以开发高度准确且易于理解的数据驱动型临床评分系统。相比现有的决策树、随机森林等方法，Brush能够在保持甚至提高预测性能的同时简化模型结构。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [44] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: 本文提出了一种名为CIP-Net的无示例自解释原型模型，用于持续学习。它避免了存储过去的样本，并保持了简单的架构，同时提供了有用的解释和强大的性能。实验表明，与先前的方法相比，CIP-Net在任务增量和类增量设置下都取得了最先进的表现，同时大幅降低了内存相关的开销。


<details>
  <summary>Details</summary>
Motivation: 持续学习要求模型随时间学习新任务而不忘记已学知识。然而，在这种情况下，灾难性遗忘是一个关键挑战，即学习新信息会导致模型在先前任务上的性能下降。虽然可解释的人工智能（特别是自解释模型）被提议为更好地理解和减少遗忘的一种有希望的方式，但现有的大多数可解释方法使用事后解释或需要为每个新任务增加额外的内存，导致扩展性有限。

Method: 提出了CIP-Net，这是一种专为持续学习设计的无示例自解释原型模型。该模型不需要存储过去的数据样例，并且维持了一个简单的架构，但仍能提供有用的解释以及优秀的性能。

Result: CIP-Net在任务增量和类增量两种场景下都达到了比之前无示例和自解释方法更好的性能，而且它的内存相关开销显著更低。

Conclusion: CIP-Net作为一种实际且可解释的持续学习解决方案，展示了其在不牺牲性能的同时降低内存需求的能力，使其成为持续学习领域的一个有效工具。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [45] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY, a new medical world model, improves upon existing methods by integrating temporal and clinical contexts for more accurate and personalized disease evolution predictions, thus enhancing treatment planning. It outperforms recent models on the MU-Glioma-Post dataset.


<details>
  <summary>Details</summary>
Motivation: Current AI predictors in oncology are limited to static predictions and do not adequately account for dynamic disease evolution or patient-specific contexts. There is a need for a model that can generate more physiologically faithful and individualized treatment plans based on causal, physiological transitions over time.

Method: CLARITY, a medical world model, was developed to forecast disease evolution within a structured latent space. It incorporates both temporal (time intervals) and clinical (patient-specific data) contexts to model the progression of diseases under different treatments as interpretable trajectories. Additionally, it establishes a prediction-to-decision framework to convert these forecasts into actionable treatment recommendations.

Result: On the MU-Glioma-Post dataset, CLARITY outperformed the recent MeWM model by 12% and also surpassed other medical-specific large language models, demonstrating its effectiveness in treatment planning and personalized disease forecasting.

Conclusion: CLARITY represents a significant advancement in medical world models, offering more accurate, personalized, and causally informed predictions of disease evolution. Its ability to integrate specific patient data and provide actionable treatment recommendations marks a step forward in the application of AI to oncology treatment planning.

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [46] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: 本文提出了一种名为LUNA的线性注意力机制，它通过学习内核特征映射而非使用固定的随机特征映射来保持线性计算成本的同时，达到甚至超过二次注意力机制的精度。实验证明，LUNA在长序列任务中表现出色，并且能够有效替换微调后的BERT和ViT模型中的softmax层，恢复大部分原始性能。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制面临的主要瓶颈是其二次计算复杂度，限制了它在长序列任务中的应用。而现有的线性注意力机制虽然降低了计算成本，但通常依赖于固定随机特征映射或手工设计函数，这导致了准确性与效率之间的权衡。为了解决这个问题，提出了LUNA。

Method: LUNA是一种基于学习到的内核特征映射的线性注意力机制，能够根据特定数据和任务自适应地调整特征基，从而克服了静态特征方法表达能力有限的问题。该方法保证了正定核的存在，并允许流式处理形式，使得序列长度上的时间和内存开销呈线性增长。

Result: 实验结果表明，在长范围竞技场（LRA）测试中，当参数数量、训练步骤和近似FLOPs相同时，LUNA在高效变换器中达到了最先进的平均准确率。此外，将预训练好的BERT和ViT-B/16模型中的softmax替换成LUNA并进行简短微调后，可以恢复大部分原有的表现，显著优于其他固定线性化方法。

Conclusion: LUNA提供了一个解决注意力机制中效率与准确性之间权衡问题的新途径，不仅保持了线性时间复杂度，还提高了模型在不同任务下的表现。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [47] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出了一种名为Deep Kernel Aalen-Johansen (DKAJ)估计器的可解释深度竞争风险模型，该模型在四个标准的竞争风险数据集上表现与最先进方法相当，并且能够提供可视化帮助理解模型。


<details>
  <summary>Details</summary>
Motivation: 为了提高对复杂数据（如患者数据）中累积发病率函数（CIFs）预测的理解性和准确性，研究者们提出了一个新的模型来更好地捕捉数据中的模式并让模型结果更易于解读。

Method: 通过将每个数据点表示为一组集群的加权组合，并使用自动学习到的核函数来衡量任意两个数据点之间的相似性，从而构建了DKAJ估计器。如果一个数据点仅属于一个集群，则其预测的CIFs对应于经典Aalen-Johansen估计器对该集群内数据点的限制。

Result: DKAJ估计器在四个标准的竞争风险数据集上的表现与当前最先进的基线方法相媲美，并且能够提供有助于模型解释的可视化输出。

Conclusion: DKAJ估计器不仅保持了高预测性能，还增强了模型的可解释性，使其成为处理竞争风险问题时的一个强有力的选择。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [48] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种因果引导的多模态域泛化(MMDG)框架，结合了对抗解缠与统一表示学习，以解决危机分类中的跨域泛化问题。实验表明该方法在未见过的灾难场景中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的危机分类方法主要依赖深度学习来融合文本和视觉线索，在不同类型的危机间泛化能力较差。这主要是因为它们无法区分虚假和因果特征，并且难以在共享空间内对齐异构模态表示，限制了单模态域泛化技术的应用。

Method: 提出了一种新的因果引导的多模态域泛化（MMDG）框架，通过对抗性解缠鼓励模型专注于领域不变的因果特征，同时利用统一表示学习将来自不同模态的特征对齐到一个共享潜在空间内，从而使得单模态域泛化策略能够无缝扩展到多模态学习中。

Result: 实验结果证明了所提方法在面对未见过的灾难情况时，相比现有方法能取得更好的性能。

Conclusion: 通过引入因果引导的多模态域泛化框架，可以有效提升社交媒体中危机分类任务在未知灾害类型上的泛化能力。

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [49] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 本文提出了一种方法，通过请求大型语言模型（LLMs）在给出初始回答后提供一种自我报告式的“忏悔”，以诚实地表达其不足之处。这种忏悔的奖励仅基于其诚实性，并不正面或负面地影响主要答案的奖励。研究展示了这种方法在识别模型错误行为方面的有效性，并且指出忏悔机制可以提高模型在出错时的诚实度，从而支持监控、拒绝采样以及向用户展示问题等干预措施。


<details>
  <summary>Details</summary>
Motivation: 由于强化学习过程中奖励塑造的挑战可能导致训练过程无意中激励模型撒谎或歪曲其行为，文章旨在探索如何让大型语言模型更诚实地报告它们的行为和信念上的缺陷。

Method: 提出了一种名为“忏悔”的自报告机制，该机制要求模型在其原始响应之后提供一个全面说明其遵守政策和指令情况的输出。对忏悔内容的评价仅以其诚实性为依据，在训练过程中不会影响到主回答所获得的奖励。

Result: 实验结果表明，当GPT-5-Thinking模型在“主”回答中撒谎或遗漏缺点时，它往往会诚实地承认这些行为，并且随着训练的进行，这种诚实度有所提高。

Conclusion: 通过引入‘忏悔’机制，可以在一定程度上鼓励模型更加诚实地报告自身存在的问题，这对于改善模型行为、提高透明度及增强用户信任具有重要意义。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [50] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: 本文研究了基于模型的强化学习，特别是基于模型的价值扩展，是否可以为解决离线RL中复杂、长期的任务提供可扩展的方法。通过引入动作块模型和拒绝采样方法来减少累积误差并防止模型利用分布外的动作，从而提出了一种名为MAC的新方法。实验表明，MAC在具有挑战性的长期任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 探索基于模型的强化学习能否有效解决离线RL中的复杂且时间跨度长的任务问题，并针对现有方法存在的偏差与累积误差问题提出改进方案。

Method: 提出了一个动作块（action-chunk）模型用于从一系列动作预测未来状态，以此减少累积误差；同时采用来自表达性行为动作块策略的拒绝采样方法，避免因采取分布外动作而对模型造成利用。该方法被称为基于模型的带有动作块的强化学习（MAC）。

Result: 在使用大规模数据集（高达1亿次转换）进行的高度挑战性任务测试中，MAC相较于其他离线基于模型的RL算法表现最佳，特别是在处理困难的长期任务时。

Conclusion: 基于模型的价值扩展结合动作块模型及拒绝采样技术形成的MAC方法，在应对复杂的长期任务方面显示出巨大潜力，为离线RL领域提供了新的解决方案。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [51] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 本文提出使用Youden的J统计量和平衡准确率作为选择最佳评估者（分类器）来比较大型语言模型的指标，这些指标在理论上更适合处理类别不平衡问题，并通过分析和实证研究证明了它们能够提供更稳健的分类器选择。


<details>
  <summary>Details</summary>
Motivation: 对于大型语言模型的严格评估依赖于比较不同模型之间良好或不良行为的发生率，而这些发生率估计值是由一个分类器产生的。常用的评估指标如准确率、精确度和F1分数对类别不平衡敏感且可能受到正类选择的影响，从而偏向于扭曲实际发生率的评判者。因此，需要找到一种更加可靠的方法来选择合适的评判者以确保评估的可信度。

Method: 论文引入了Youden的J统计量作为理论依据，用来确定最佳评判者的选择标准，并指出平衡准确率是J统计量的一个等价线性变换形式。通过理论分析与实例模拟相结合的方式，展示了基于平衡准确率挑选评判者可以带来更好的分类效果。

Result: 研究表明，利用平衡准确率作为评判者选择的标准，确实能够实现更优且更稳定的分类器选取结果。

Conclusion: 采用Youden的J统计量及与其等效的平衡准确率作为评判标准，有助于提高大型语言模型间对比评估时所选评判者的质量，进而提升整个评估过程的可靠性。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [52] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出了一种名为类子空间正交化（CSO）的新方法，通过抑制固有特征同时优化给定类别的检测统计量，以提高后门检测的敏感度。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练后门检测方法在某些非目标类别容易与所有其他类别区分时可能会失败，或者当后门是微妙且其特征相对于内在类别区分特征较弱时也会失效。

Method: 通过制定一个约束优化问题，利用给定类别的少量干净样本，同时在优化检测统计量时对类别的内在特征进行正交化处理。

Result: 对于非目标类别，这种抑制会显著降低可达到的统计量；而对于目标类别，来自后门触发器的（显著）贡献仍然存在。该方法被命名为类子空间正交化（CSO），并针对混合标签和自适应攻击进行了评估。

Conclusion: CSO方法为解决现有后门检测技术中的局限性提供了一个更敏感的解决方案，特别是在处理微妙后门或特定条件下自然表现出极端检测统计数据的非目标类别时。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [53] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 本文提出了一个名为Biothreat Benchmark Generation (BBG) Framework的新框架，旨在帮助模型开发者和评估者可靠地测量和评估现有及未来AI模型的生物安全风险。该框架首先针对细菌生物威胁进行开发，并基于生物威胁类别、元素和任务的分层结构来构建任务查询架构，即Bacterial Biothreat Schema。


<details>
  <summary>Details</summary>
Motivation: 为了量化并减轻前沿人工智能（尤其是大型语言模型）可能带来的生物恐怖主义或生物武器获取的风险，需要开发能够评估特定模型生物安全风险的基准。

Method: 开发了Biothreat Benchmark Generation (BBG) Framework的第一个组成部分——Bacterial Biothreat Schema，它基于生物威胁类别、元素和任务的分层结构来生成与任务对齐的查询。

Result: 创建了一个新的框架用于衡量由AI模型引发的细菌生物威胁风险，该框架考虑到了不同行动者的能力水平以及操作和技术上的风险因素。

Conclusion: BBG框架及其组成部分Bacterial Biothreat Schema提供了一种强大且可重复使用的结构，用以评估由大型语言模型引起的细菌生物风险，涵盖了生物对手的技术和操作要求，并考虑到了广泛的生物对手能力范围。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [54] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: 介绍了PolyLingua，一个轻量级的基于Transformer的语言识别模型，适用于领域内语言检测和细粒度语言分类。它在两个挑战性数据集上表现出色，同时使用了比Sonnet 3.5少10倍的参数，非常适合计算和延迟受限的环境。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别工具在处理如音乐请求等关键情况时表现不佳，开源工具速度快但准确性较低，而大型语言模型虽然有效但成本高昂。为了提高多语言系统中的语言识别准确性，并适应低延迟或资源有限的场景，提出了PolyLingua。

Method: PolyLingua是一个基于Transformer架构的轻量级模型，采用两层对比学习框架，结合实例级分离与类级别对齐及自适应边缘技术，旨在即使对于紧密相关的语言也能产生紧凑且良好区分的嵌入表示。

Result: 在Amazon Massive（多语言数字助理话语）和Song数据集（含有频繁代码切换的音乐请求）上的评估显示，PolyLingua分别达到了99.25%和98.15%的F1分数，超过了Sonnet 3.5的表现，同时使用的参数量仅为后者的十分之一。

Conclusion: 通过引入PolyLingua，该研究为多语言系统提供了一个高效准确的语言识别解决方案，尤其适合于那些需要快速响应并且资源有限的应用场景。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [55] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架TreeGRPO，通过将去噪过程重塑为搜索树来大幅提高训练效率。该方法在相同训练样本下实现了更高的性能、细粒度的奖励分配以及摊销计算，使得基于RL的视觉生成模型对齐更加高效和可扩展。实验显示TreeGRPO比GRPO基线快2.4倍，并且在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 强化学习对于调整生成模型以符合人类偏好至关重要，但其高昂的计算成本阻碍了广泛应用。因此，研究者们旨在开发一种更高效的强化学习框架，以减少训练时间和资源消耗，同时保持或提高模型性能。

Method: 研究提出了TreeGRPO，这是一种新的强化学习框架，它将去噪过程重新构想为一棵搜索树。从共享初始噪声样本开始，TreeGRPO能够策略性地分支出多条候选轨迹，并有效复用它们共有的前缀部分。这种方法带来了三个主要优势：高样本效率、细粒度的信用分配以及通过多子分支实现的摊销计算。

Result: 广泛的实验证明，与GRPO基线相比，TreeGRPO在扩散模型和流式模型上实现了2.4倍更快的训练速度，并且在效率-回报权衡空间内建立了更优的帕累托前沿。此外，TreeGRPO在多个基准测试及奖励模型中均表现出色。

Conclusion: TreeGRPO提供了一个可扩展且有效的途径，用于基于强化学习的视觉生成模型对齐，通过引入树结构的方法显著提升了训练效率而不牺牲性能。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [56] [LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks](https://arxiv.org/abs/2512.08160)
*Nanda K. Unnikrishnan,Keshab K. Parhi*

Main category: cs.LG

TL;DR: LayerPipe2 provides a formal derivation for the LayerPipe approach, offering insights into gradient delay requirements and proposing a pipeline-aware moving average to reduce memory costs, thus enabling efficient pipelined training of neural networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a principled understanding of the gradient delay needed at each layer for effective pipelining in neural network training, addressing an unexplored aspect from their prior work, LayerPipe, and to tackle the storage issue that arises with pipelining.

Method: The method involves formally deriving the LayerPipe technique using variable delayed gradient adaptation and retiming, identifying legal points for inserting delays, and developing a pipeline-aware moving average to manage the storage of historical weights without compromising accuracy.

Result: The results show that inner layers require fewer delays, while outer layers need longer delays. The paper also presents a solution to the storage bottleneck through a pipeline-aware moving average, which reduces memory cost without sacrificing accuracy, allowing for scalable pipelined training.

Conclusion: LayerPipe2 offers a framework that clarifies how to design LayerPipe architectures, forecasts delay requirements, and eases the storage challenge, supporting scalable pipelined training with managed trade-offs between communication and computation.

Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

</details>


### [57] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: 本文提出了MobileFineTuner，一个统一的开源框架，能够在普通手机上直接进行大型语言模型（LLM）的端到端微调。它通过参数分片、梯度累积和能耗感知计算调度等系统级优化解决了手机内存和能源限制的问题，并在真实手机上成功对GPT-2, Gemma 3, 和Qwen 2.5进行了微调。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共数据对于大型语言模型逐渐耗尽，在设备上进行微调提供了一个利用私人用户数据同时保护隐私的机会。但目前的方法主要基于模拟或依赖于物联网设备和个人电脑，很少探索普通手机的应用。此外，缺乏一个能够实现在手机上实际进行LLM微调的开源框架。

Method: 开发了MobileFineTuner，这是一个支持全参数微调（Full-FT）和参数高效微调（PEFT）的统一开源框架。为解决手机固有的内存与能量限制问题，引入了包括参数分片、梯度累积以及能耗感知计算调度在内的系统级别优化措施。

Result: 通过在真实手机上对GPT-2, Gemma 3, 和Qwen 2.5进行微调展示了MobileFineTuner的实用性。广泛的实验和消融研究表明所提出的优化方法有效，并确立了MobileFineTuner作为未来关于设备上LLM训练研究可行基础的地位。

Conclusion: MobileFineTuner作为一个创新解决方案填补了市场上空白，使得在资源受限环境如移动电话中也能有效地执行大型语言模型的微调工作。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [58] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 本研究探讨了去耦权重衰减（decoupled weight decay）与学习率γ的关系，提出并验证了去耦权重衰减应正比于γ^2的观点，而非传统的正比于γ。通过分析更新步骤对权重范数的影响，并基于更新最终独立于权重的假设，该研究还引入了总更新贡献(TUC)的概念来表征Scion优化器下小批量的有效学习率，并证明了这种方法可以更好地控制训练动态和提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 先前的研究质疑了去耦权重衰减应该与学习率γ成正比的传统设定，提出了在稳态下基于正交性论据应与γ^2成正比的新观点。然而，作者们发现移除更新的垂直分量对权重范数影响甚微。因此，这项工作的动机是基于一个简化的假设——即在稳态时更新变得与权重无关——来推导出去耦权重衰减应当与γ^2成比例，并探索这种设置如何帮助稳定权重和梯度范数以及改进模型表现。

Method: 研究者首先从理论上探讨了不同形式下去耦权重衰减对于权重范数稳定性的影响，特别是当更新过程被认为是在稳态条件下与权重本身独立时。接着，他们定义了一个名为“总更新贡献”(TUC)的新指标，用来更准确地描述在Scion优化器作用下一个minibatch带来的影响。此外，通过实验验证了提出的理论框架，展示了采用与γ^2成比例的去耦权重衰减能够实现更加稳定的训练过程及更好的模型效果。

Result: 研究结果表明，将去耦权重衰减设为与γ^2成比例确实有助于维持权重和梯度范数的稳定性。同时，使用基于动量的有效学习率作为TUC的度量标准被证明是有效的，且其最优值可以在不同场景间转移。这使得研究人员能够更好地理解和控制训练动态，从而达到改善模型性能的目的。

Conclusion: 本文挑战了关于去耦权重衰减与学习率之间关系的传统看法，提供了新的见解，即去耦权重衰减应当与γ^2成正比才能有效稳定权重和梯度范数。通过引入TUC概念和相关实验验证，进一步支持了这一结论，并展示了这种方法在提高模型性能方面的潜力。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [59] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: 本研究提出了一种基于原型的特征转换方法SPROCKET，用于时间序列分类。实验结果表明，该方法在UCR和UEA时间序列分类档案中的大部分上表现良好，并且与现有的卷积算法相比，其性能相当。此外，当与MultiROCKET和HYDRA结合形成新的集成模型MR-HY-SP时，其平均准确率超过了之前最佳的卷积集成模型HYDRA-MR。


<details>
  <summary>Details</summary>
Motivation: 传统的时序分类算法主要依赖于特征工程策略，其中ROCKET通过随机核特征实现了强大性能。然而，探索基于原型的新特征工程策略以进一步提高时序分类的准确性和鲁棒性是必要的。

Method: 研究者们提出了SPROCKET（Selected Prototype Random Convolutional Kernel Transform），这是一种基于原型的时间序列特征转换方法。此外，还创建了一个新的集成模型MR-HY-SP，它结合了MultiROCKET、HYDRA以及SPROCKET的优势。

Result: 在大多数UCR和UEA时间序列分类档案中，SPROCKET的表现与现有卷积算法相当；更重要的是，MR-HY-SP集成模型的平均准确度排名超过了先前最好的卷积集成模型HYDRA-MR。

Conclusion: 实验结果显示，基于原型的特征转换能够增强时间序列分类中的准确性和鲁棒性。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [60] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 该论文提出了一种统一的几何-随机多模态深度学习框架，用于整合EEG、ECG、呼吸、SpO2、EMG和fMRI信号来建模癫痫猝死(SUDEP)和中风的脆弱性。通过黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流动建模以及跨模态注意力机制等方法，提高了预测准确性，并从多种生物标志物中提取出可解释的信息。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种数学原理基础的方法，以提高SUDEP和急性缺血性中风这两种威胁生命的状况的早期检测、风险分层及可解释的多模态建模能力。这些疾病涉及大脑皮层、脑干和自主神经系统之间的复杂相互作用。

Method: 本研究采用了一种结合了黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流动建模及跨模态注意力机制的统一几何-随机多模态深度学习框架。此外，还利用分数流行病扩散模型在结构脑图上模拟中风传播过程。

Result: 实验结果表明，在MULTI-CLARID数据集上的测试显示，所提出的框架不仅提高了预测准确性，而且还能够从流形曲率、分数记忆指数、注意力熵和扩散中心性等方面推导出具有解释性的生物标志物。

Conclusion: 本文提出的框架为神经-自主系统紊乱疾病的早期发现、风险评估以及多模态信息的可解释建模提供了坚实的数学理论基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [61] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF, or Jacobian-Aligned Random Forests, is a method that enhances the performance of axis-aligned decision trees on datasets with complex decision boundaries by using a single global linear preconditioner. This approach maintains the simplicity and efficiency of axis-aligned trees while improving their capability to handle oblique boundaries and feature interactions, often matching or outperforming oblique forests in terms of accuracy and training time.


<details>
  <summary>Details</summary>
Motivation: The motivation behind JARF is to address the limitations of axis-aligned decision trees, which perform poorly on datasets with rotated or interaction-dependent decision boundaries. While oblique forests can handle such cases, they come with higher computational costs and complexity. JARF aims to provide a simpler, more efficient alternative that retains the benefits of axis-aligned trees while improving their performance on complex datasets.

Method: JARF begins by fitting an axis-aligned forest to estimate class probabilities or regression outputs. It then computes finite-difference gradients of these predictions concerning each feature, aggregates them into an expected Jacobian outer product, and uses this as a global linear preconditioner for all inputs. This process applies a single global rotation to the feature space, after which the transformed data is fed back into a standard axis-aligned forest. The method is designed to be compatible with any model providing gradients, but the focus here is on random forests and gradient-boosted trees.

Result: On various tabular classification and regression benchmarks, JARF consistently improves the performance of axis-aligned forests, often achieving results comparable to or better than those of oblique forests. Additionally, it does so with improved training times, demonstrating its effectiveness in enhancing the capabilities of traditional axis-aligned decision trees without significantly increasing computational cost or complexity.

Conclusion: JARF offers a promising solution for improving the performance of axis-aligned decision trees on datasets with complex decision boundaries, combining the simplicity and efficiency of these trees with the ability to effectively capture oblique boundaries and feature interactions. Experimental evidence and theoretical analysis support JARF's potential to deliver high accuracy similar to oblique forests, while maintaining the robustness and ease of use associated with axis-aligned trees.

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [62] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习优化方法，通过约束Hessian矩阵的最大特征值来改善模型的泛化性能，并引入了名为'MAN'的新正则化技术，该技术在客户端模型上最小化每层激活的范数，从而减少整体Hessian矩阵的最大特征值，确保收敛到平坦的极小值。


<details>
  <summary>Details</summary>
Motivation: 联邦学习可能导致全局模型收敛到一个‘尖锐极小值’，这会负面影响FL训练模型的泛化能力。因此，本研究旨在通过引入‘平坦度’约束的联邦学习优化问题来提高联邦设置下训练模型的泛化性能。

Method: 提出了一个新的计算效率高的正则化技术，称为'MAN'，它在客户端模型上的每一层最小化激活的范数。理论上表明，减小激活范数可以降低客户损失的逐层Hessian的最大特征值，进而减少整个Hessian的最大特征值，保证收敛到平坦的极小值。

Result: 将提出的平坦度约束优化应用于现有的联邦学习技术后，取得了显著改进，建立了新的最先进水平。

Conclusion: 通过引入平坦度约束以及MAN正则化技术，能够有效提升联邦学习中训练模型的泛化能力，为联邦学习框架下的模型优化提供了新思路。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [63] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 本文提出了一种新的抽样算法，该算法考虑了标签间的依赖关系，并使用多变量伯努利分布来估计参数和计算每个标签组合的权重，从而在保持标签频率顺序的同时，减少了最常见和最少见类别之间的频率差异，并提高了少数类别的代表性。


<details>
  <summary>Details</summary>
Motivation: 在处理包含多个非互斥标签的数据集时，如果标签出现频率差异很大，那么获取足够的稀有标签样本以进行相关推断变得具有挑战性。此外，需要确保样本与总体频率以已知方式偏离。

Method: 研究者们采用多变量伯努利分布作为多标签问题的基本分布，并提出一种新采样算法。此算法利用观察到的标签频率来估计多变量伯努利分布参数，并为每种标签组合计算权重，同时考虑到了标签间的依赖关系。

Result: 当应用于Web of Science中带有64个生物医学主题类别的研究文章样本时，该方法能够保持类别频率顺序、减少最常见与最少见类别间的频率差异，并考虑到类别间依赖关系，从而生成了一个更加均衡的子样本，增强了少数类别的表现力。

Conclusion: 通过考虑标签依赖关系并采用加权抽样策略，所提出的算法能够更有效地处理含有多标签的数据集，在保证样本代表性的前提下改善了少数标签的表现。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [64] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: This paper presents the creation of a Bacterial Biothreat Benchmark (B3) dataset, part of a larger Biothreat Benchmark Generation (BBG) framework, designed to assess and help mitigate the biosecurity risks associated with advanced AI models, particularly LLMs.


<details>
  <summary>Details</summary>
Motivation: Due to concerns about the potential misuse of AI, especially large language models (LLMs), in facilitating bioterrorism or access to biological weapons, there is a need to develop model benchmarks to assess and mitigate the biosecurity risks posed by these models. The aim is to provide a tool for both model developers and policymakers to better understand and manage such risks.

Method: The development process for the B3 dataset involved three approaches: web-based prompt generation, red teaming, and mining existing benchmark corpora. Over 7,000 potential benchmarks were created, which were then de-duplicated, assessed for uplift diagnosticity, and subjected to general quality control measures, resulting in the final set of 1,010 benchmarks.

Result: A Bacterial Biothreat Benchmark (B3) dataset was produced, consisting of 1,010 benchmarks that have been refined to ensure they are diagnostic, directly related to biosecurity, and compatible with a broader biosecurity assessment framework. These benchmarks will be used to evaluate the biosecurity risk of AI models.

Conclusion: The paper successfully generated a Bacterial Biothreat Benchmark (B3) dataset, containing 1,010 final benchmarks that are diagnostic, relevant to biosecurity threats, and aligned with a larger biosecurity architecture for nuanced analysis. This is the second component of the Biothreat Benchmark Generation (BBG) framework.

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [65] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 本文介绍了细菌生物威胁基准（B3）数据集的试点实施，作为评估大型语言模型（LLMs）所带来的生物安全风险的一种可行且细致的方法。通过让一个样本前沿AI模型运行这些基准，并对模型响应进行人工评估和多维度的风险分析，研究发现B3数据集能够有效识别风险的主要来源，并为优先缓解领域提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着快速发展的前沿人工智能模型，特别是大型语言模型（LLMs），有可能促进生物恐怖主义或获取生物武器的情况日益引起政策制定者、学术界及公众的关注。为了量化并减轻这种风险，需要开发能够评估特定模型生物安全风险的模型基准。

Method: 本文是关于Biothreat Benchmark Generation (BBG)框架系列三篇论文中的第三篇，重点在于B3数据集的实际应用测试。该试点包括将基准应用于一个样本前沿AI模型中，然后由人类专家评估模型的回答，并从多个角度进行了实际风险分析。

Result: 试点项目表明，B3数据集提供了一种快速评估大型语言模型带来的生物安全风险的有效方法，能够识别出主要风险源，并为优先考虑的缓解措施指明方向。

Conclusion: B3数据集被证明是一种有效的工具，可以用来评估大型语言模型在生物安全方面的潜在风险，同时也为如何优先处理这些风险提供了有价值的见解。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [66] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，该框架结合了基于Transformer的架构和多模态输入（包括fMRI数据和DICOM元数据），以提高模型准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习和深度学习方法在处理fMRI数据方面取得了一些进展，但未能充分利用DICOM元数据提供的上下文丰富性。为了克服这一限制并捕捉复杂的时空模式及上下文关系，提出了本研究的新框架。

Method: 通过整合基于Transformer的架构与包含fMRI数据和DICOM元数据在内的多模态输入，并利用注意力机制来捕捉这些复杂的关系。

Result: 该框架展示了在临床诊断、认知神经科学以及个性化医疗领域应用的潜力。尽管存在元数据变异性高和计算需求大等局限性，但论文也讨论了未来优化可扩展性和泛化能力的方向。

Conclusion: 新提出的融合fMRI数据与DICOM元数据的框架通过采用注意机制增强了对复杂空间-时间模式的理解，为神经科学研究及临床应用开辟了新的可能性。

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [67] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的全局预算分配攻击策略，该策略基于样本的TD误差敏感性来分配扰动幅度，从而在离线强化学习中以最小的扰动实现高达80%的性能下降，并且能够避开最新的统计和频谱防御检测。


<details>
  <summary>Details</summary>
Motivation: 现有的针对离线强化学习的数据投毒攻击方法通常依赖于对所有样本无差别地进行局部均匀扰动，这种方法效率低下且缺乏隐蔽性。为解决这一问题，提出了一个新的全局预算分配攻击策略。

Method: 通过利用理论洞察力，即样本对价值函数收敛的影响与其TD误差成正比，将攻击建模为一个全局资源分配问题。推导出一种闭式解，其中扰动幅度按照TD误差敏感性比例分配，并受全局L2约束。

Result: D4RL基准测试中的实验结果表明，所提方法显著优于基线策略，实现了高达80%的性能降级，同时施加了极小的扰动，这些扰动能够逃避最先进的统计和频谱防御检测。

Conclusion: 新提出的全局预算分配攻击策略证明了在提高数据投毒攻击的有效性和隐蔽性方面的潜力，对于理解并增强离线强化学习系统的鲁棒性具有重要意义。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [68] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文改进了联邦近端梯度(FedProxGrad)方法的渐近收敛性分析，提出了DS FedProxGrad框架，在满足一定条件下证明了算法能够达到渐近平稳，并且收敛速率不依赖于方差引起的噪声底限。


<details>
  <summary>Details</summary>
Motivation: 先前的工作引入了Federated Proximal Gradient (FedProxGrad)来解决组公平联邦学习中的非凸复合优化问题，但其收敛分析仅限于由方差引起的噪声主导的邻域内。本文旨在提供一个改进的渐近收敛性分析，以克服这一限制。

Method: 提出了一种称为DS FedProxGrad（Decay Step Size FedProxGrad）的广义分析框架，该框架允许局部近似解存在一定的误差并明确加入了公平性正则化。采用Robbins-Monro步长调度策略以及对方差引起的局部不精确度施加温和的衰减条件。

Result: 在给定的假设条件下，证明了$\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$成立，这意味着所提出的算法能够达到渐近平稳状态，并且其收敛性能不再受到方差引起噪声底限的影响。

Conclusion: 通过引入DS FedProxGrad框架及适当的假设条件，成功地提升了FedProxGrad方法的理论性能，使其能够在更广泛的场景下实现更好的收敛表现。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [69] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于动态时间规整（DTW）-迁移学习（TL）框架的方法，用于增材制造零件的性能验证。通过将低成本聚合物的应力-应变行为知识迁移到金属上，该方法展示了在不同材料和增材制造技术间有效迁移学习的能力，从而提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在增材制造领域中，确保零件能够一致地生产并在关键应用中可靠使用非常重要。为了达到这一目标，需要准确预测增材制造零件复杂的应力-应变行为。本研究旨在开发一种新方法来提高这种预测能力，特别是通过利用已知的、成本较低的聚合物数据来帮助理解和预测更昂贵或难以测试的金属材料的行为。

Method: 研究者们构建了一个结合了动态时间规整(DTW)与迁移学习(TL)的技术框架。首先利用DTW从几种不同的聚合物材料中挑选出与目标金属材料最相关的作为源域。接着，采用长短期记忆(LSTM)模型，将所选聚合物的数据集作为训练基础，并将其知识迁移到三个目标金属材料上。

Result: 实验结果表明，提出的DTW-TL框架能够有效地识别聚合物与金属之间的最佳匹配关系，进而选择一个最适合的聚合物数据集作为源域。当以三种金属为目标域时，DTW-TL模型实现了最低的平均绝对百分比误差12.41%以及最高的决定系数0.96，优于没有使用迁移学习的普通LSTM模型及直接用四个聚合物数据集预训练的迁移学习模型。

Conclusion: 研究表明，通过将低成本聚合物的应力-应变行为知识迁移到金属上，可以显著提升增材制造零件资格认证过程中对于金属材料性能预测的准确性。这为增材制造行业内跨材料类型的知识迁移提供了新的思路。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [70] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Bias-Guided Prompt Search (BGPS)的框架，该框架能自动生成旨在最大化图像中偏见出现的提示。通过使用大型语言模型(LLM)生成属性中立的提示，并结合属性分类器引导LLM朝向放大感兴趣图像属性的方向解码，BGPS能够揭示之前未被记录的微妙偏见。实验结果表明，这种方法不仅改善了公平性指标，还提供了一个新的评估工具来检测和缓解文本到图像(TTI)模型中的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在训练或评估过程中经常依赖于精心策划的提示数据集，这既增加了策展成本，也有可能忽视那些触发偏见生成但不太明显的提示。即使经过去偏处理的模型也可能存在这种情况。为了克服这个问题并揭示更多潜在的社会偏见，作者们开发了BGPS框架。

Method: BGPS框架由两部分组成：一是指示大型语言模型（LLM）产生属性中立的提示；二是利用作用于TTI内部表示的属性分类器，这些分类器将LLM的解码过程导向那些能够放大目标图像属性的空间区域。

Result: 通过在Stable Diffusion 1.5和最先进的去偏模型上进行广泛的实验，研究人员发现了一系列细微且以前未曾记录过的偏见，这些偏见严重损害了公平性度量。重要的是，所发现的提示是可解释的，普通用户也可以输入它们，相较于一个重要的硬提示优化对手，在困惑度指标上有定量上的改进。

Conclusion: 这项工作展示了BGPS作为一种新工具的有效性，它不仅能帮助扩大偏见搜索空间，还能作为评估偏见缓解策略的新手段。此外，它还揭示了现有TTI模型中存在的隐秘偏见问题，为未来的研究提供了方向。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [71] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 本研究引入了神经常微分方程（NODEs）作为学习蛋白质组和代谢组之间复杂相互作用的动态框架，并应用于工程大肠杆菌菌株的时间序列数据。相比传统机器学习方法，该模型在捕捉系统动态方面表现更优，对柠檬烯和异戊醇路径数据集的均方根误差分别提高了94.38%和97.65%，同时推理速度提高了1000倍。


<details>
  <summary>Details</summary>
Motivation: 随着高通量多组学数据变得越来越丰富，将这些数据转化为可操作的预测模型仍然是一个瓶颈。为了克服这一挑战并推动人类健康寿命和生物工程的进步，需要开发能够从观测数据中直接推断潜在交互作用的高容量、数据驱动的仿真系统。

Method: 本研究采用神经常微分方程（Neural Ordinary Differential Equations, NODEs）来模拟代谢途径的连续动力学特性。通过利用来自经过工程改造的大肠杆菌菌株的时间序列数据，研究者们构建了一个能够学习蛋白组与代谢组间复杂互动关系的动态框架。

Result: 实验结果表明，所提出的NODE架构在捕捉系统动态方面优于传统的机器学习管道，在柠檬烯和异戊醇路径数据集上相较于基线实现了超过90%的均方根误差改进。此外，NODE模型还展示了比现有方法快1000倍的推理时间。

Conclusion: 本研究表明，NODEs作为一种新颖且高效的工具，不仅能够以更高的准确性模拟生物系统的动态行为，而且显著提升了计算效率，为未来的代谢工程及生物学发现提供了强有力的支持。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [72] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于提升分数预测的双变量贝塔分布的反事实估计器，这种方法可以用来估计潜在结果的联合分布，而不需要额外的因果假设。这种技术在电信客户流失问题上展示了有效性，并提供了标准机器学习或单独使用提升模型无法获得的见解。


<details>
  <summary>Details</summary>
Motivation: 提升建模评估干预措施的因果效应，即治疗与对照下潜在结果之间的差异，而反事实识别旨在恢复这些潜在结果的联合分布。虽然联合反事实分布提供了比提升更为丰富的信息，但其估计难度更大。然而，这两种方法是协同的：可以利用提升模型来进行反事实估计。

Method: 作者提出了一种新的反事实估计方法，该方法通过拟合一个双变量贝塔分布到预测的提升分数上，从而产生关于反事实结果的后验分布。此方法除了提升建模所需的因果假设外，不需任何额外的因果假设。

Result: 仿真研究表明了该方法的有效性，特别是在解决电信行业的客户流失问题时，它揭示了仅凭标准机器学习或单纯依靠提升模型所不能提供的洞察。

Conclusion: 这项研究展示了一种将提升建模和反事实推理相结合的新途径，为理解干预效果提供了一个有力工具，尤其适用于那些需要从更深层次理解因果关系的应用场景。

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [73] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: 本文介绍了一种新的框架WAAPO，用于生成针对天气预报模型的对抗扰动，这些扰动既能在操纵预测方面有效，又具有隐蔽性以避免被发现。实验表明，即使是小的初始条件扰动也可能导致预测天气模式的重大偏差，强调了在操作性预报系统中需要强有力的保护措施来防止对抗性利用。


<details>
  <summary>Details</summary>
Motivation: 随着对AI模型进行天气预报依赖度的增加，评估这些模型对于对抗性扰动的脆弱性变得至关重要。

Method: 提出了天气自适应对抗扰动优化（WAAPO）框架，该框架通过结合通道稀疏性、空间局部化和平滑性的约束条件来生成针对性的对抗扰动，确保扰动保持物理上的现实性和不易察觉性。

Result: 使用ERA5数据集和FourCastNet展示了WAAPO能够产生与预定义目标紧密匹配的对抗轨迹，即使是在受限条件下也是如此。实验证明了AI驱动预报模型中的关键脆弱点：初始条件的小幅扰动可能导致预测天气模式的重大偏离。

Conclusion: 研究结果强调了在操作性预报系统中实施强有力保护措施以抵御对抗性利用的重要性。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [74] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文提出了一种TD($\lambda$)的改进形式STD($\lambda$)，它基于相对状态值训练函数逼近器以解决二元决策问题，证明了在两状态系统中该方法能够单调地改善策略，并通过几个示例展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管TD($\lambda$)结合函数逼近已被证实能有效解决一些复杂的强化学习问题，但就策略而言，关键在于状态间的相对排序误差而非状态值误差本身。文章指出，在简单双态或三态系统乃至西洋双陆棋游戏中，TD($\lambda$)从最优策略出发也可能收敛到次优策略。

Method: 提出了名为STD($\lambda$)的TD($\lambda$)改进版本，其中函数近似器根据二元决策问题中的相对状态值进行训练。

Result: 理论分析包括了对于两状态系统中STD($\lambda$)单调策略改进的证明，并与Bertsekas提出的差异训练方法进行了对比。此外，还成功地将STD($\lambda$)应用于两状态系统和一个著名的acrobot问题变体上。

Conclusion: 研究表明，相对于传统的TD($\lambda$)，STD($\lambda$)能够在保持策略改进的同时，更好地处理因使用函数逼近而导致的状态间相对价值错误问题。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [75] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 本研究提出了一种文本到IMU运动合成框架，通过采用基于加速度的二阶损失（L_acc）微调预训练扩散模型来生成逼真的IMU数据。实验结果表明，这种方法在高动态活动中的改进尤为显著，并且使用改进后的合成IMU数据进行人类活动识别（HAR）分类时性能提升了8.7%。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一种能够根据文本输入生成符合IMU特性的、逼真的人体动作数据的方法。通过引入特别针对IMU加速度信号设计的损失函数，旨在提高现有扩散模型生成的动作数据的真实性与一致性。

Method: 方法包括：1) 定义了一个基于加速度的二阶损失函数 L_acc；2) 将 L_acc 整合进现有扩散模型的训练目标中，并对模型进行了微调以获得IMU特定的动作先验；3) 评估了所提方法在不同动态活动下的表现及与真实IMU记录之间的差异；4) 利用生成的数据进行了人类活动识别任务的测试。

Result: 结果显示，当将 L_acc 加入到早期扩散模型的目标函数中并继续训练后，L_acc 相对于原始模型减少了12.7%，尤其在高动态活动中改进更为明显。此外，在低维嵌入空间中，由改进模型产生的合成IMU数据分布更接近于真实IMU记录。基于此方法生成的数据训练的HAR分类器比使用早期扩散模型和另一个表现最佳的对比模型分别提高了8.7%和7.6%的性能。

Conclusion: 结论是，加入加速度意识的扩散细化提供了一种有效的方法来使动作生成与IMU合成保持一致，这突出了灵活的深度学习管道如何为传感器特定的任务定制通用文本到动作先验。

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [76] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型（LLMs）在生成高质量表格合成数据时可能引发的隐私泄露问题。通过引入一种名为LevAtt的无盒成员推理攻击方法，研究人员发现无论是微调小型模型还是提示大型模型，都存在复现训练数据中数字模式的趋势，导致隐私风险。为解决这一问题，文章提出了两种防御方法，包括一种新的采样策略，能够在保证合成数据保真度和实用性的同时有效抵御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在生成高质量表格合成数据方面展现出优异表现，人们开始探索如何将这些模型应用于实际场景。但与此同时，也出现了关于这类应用可能带来的隐私泄露担忧。本文旨在探讨并分析使用LLM生成合成数据过程中存在的隐私泄漏风险，并提出相应的缓解措施。

Method: 研究者们首先介绍了两种主要的将LLM用于表格数据生成的方法：直接对较小模型进行微调以及给较大的模型提供上下文示例。接着，他们设计了一种名为LevAtt的新颖无盒成员推理攻击方法，专门针对生成数据中的数字序列进行攻击测试。此外，还提出了两种防御机制来减少这种隐私泄露的风险，特别是强调了一种创新性的采样策略。

Result: 实验结果表明，许多流行的基于LLM的数据生成方案确实存在严重的隐私泄露问题，在某些情况下甚至可以作为完美的成员分类器。而所提出的防御措施能够显著降低隐私泄露风险，同时保持合成数据的质量与实用价值。

Conclusion: 本研究表明，基于LLM的合成数据生成技术面临着独特的隐私泄露挑战。通过采用文中提出的防御策略，尤其是改进后的采样方法，可以在很大程度上减轻这种威胁而不牺牲数据的有效性。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [77] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: 提出了DAO-GP，一种新的、完全自适应的、无超参数的在线高斯过程模型，能够检测并适应概念漂移，具有动态调整模型行为的能力。实验表明，与现有最先进模型相比，DAO-GP在不同条件下均表现出优越或竞争性的性能，为在线非线性回归提供了一个抗漂移解决方案。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据集通常表现出由数据分布演变引起的时间动态特性。忽略这种现象（即概念漂移）会显著降低模型的预测准确性。此外，在线模型中的超参数通常是固定的，用户无法根据变化的数据分布进行动态调整。虽然高斯过程(GP)模型提供了强大的非参数回归能力，并且能够量化不确定性，但传统的在线GP方法存在几个关键限制，如缺乏对漂移的意识、依赖于固定超参数等。

Method: 提出了一种名为DAO-GP的新颖方法，这是一种完全自适应的、无超参数的、衰减的和稀疏的非线性回归模型。它具备内置的概念漂移检测和适应机制，可以根据漂移严重程度动态调整模型行为。

Result: 广泛的实证评估证实了DAO-GP在静止条件、不同类型漂移（突变、渐进、逐渐）以及不同数据特征下的鲁棒性。分析显示了其动态适应能力、内存管理效率及诱导点随时间演变的情况。与最先进的参数化和非参数化模型相比，DAO-GP始终达到更优或相当的性能。

Conclusion: DAO-GP作为一种针对在线环境设计的漂移感知高斯过程模型，通过引入动态调整机制克服了传统在线GP方法的局限性。它不仅提高了模型面对各种类型数据漂移时的适应性和鲁棒性，还保持了高效的资源利用率。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [78] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 本文提出了一种基于拓扑的损失函数的无监督学习方法，用于自动选择最优带宽，并与经典技术进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 核密度估计是机器学习、贝叶斯推断、随机动力学和信号处理中多种算法的关键组成部分，但其需要调整一个关键超参数：核带宽。带宽的选择至关重要，因为它通过过度或不足平滑拓扑特征来控制偏差-方差权衡。

Method: 提出了一个基于拓扑数据分析的方法，利用数学手段量化如连通分量、环路、空洞等拓扑特性，即使在高维情况下也能如此。该方法使用一种基于拓扑的损失函数来进行自动化且无需监督的最佳带宽选择。

Result: 新方法在不同维度上展示了其潜力，并与传统技术进行了比较，显示了其优势。

Conclusion: 基于拓扑的损失函数为自动化选择最佳带宽提供了一个新的视角，这可能对提升涉及核密度估计的任务性能有所帮助。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [79] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: The Open Polymer Challenge (OPC) has released a benchmark dataset for polymer informatics, featuring 10K polymers and their properties, to advance machine learning in the discovery of sustainable materials. The challenge focused on multi-task property prediction, with participants using various ML techniques to develop models under realistic constraints. Lessons learned from the competition will guide future data preparation and simulation practices.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the lack of large, high-quality, and openly accessible datasets for polymer research, which hinders the application of machine learning in discovering sustainable polymer materials. By initiating the Open Polymer Challenge (OPC), the authors aim to fill this gap and provide a benchmark that can accelerate the development of such materials.

Method: The method employed in this work involves creating and releasing a comprehensive dataset as part of the OPC, which includes 10,000 polymers and five key properties. The challenge invites researchers to predict these properties using machine learning approaches, while dealing with real-world issues like small data sizes, label imbalance, and varying simulation sources. Participants are encouraged to use advanced techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and ensemble strategies.

Result: As a result of the challenge, participants have developed predictive models for polymer properties, and important insights were gained regarding data preparation, distribution shifts, and the consistency of simulations across different groups. These findings contribute to establishing best practices for handling large-scale polymer datasets in the future.

Conclusion: In conclusion, the Open Polymer Challenge has successfully established a new benchmark for polymer informatics, providing a valuable resource for the development of machine learning models in material science. The outcomes, including the created models, analyses, and shared data, lay the groundwork for advancing molecular AI within the field of polymer science and support the quest for more sustainable and energy-efficient materials.

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [80] [MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction](https://arxiv.org/abs/2512.07846)
*Guoyao Li,Ran He,Shusen Jing,Kayhan Behdin,Yubo Wang,Sundara Raman Ramachandran,Chanh Nguyen,Jian Sheng,Xiaojing Ma,Chuanrui Zhu,Sriram Vasudevan,Muchen Wu,Sayan Ghosh,Lin Su,Qingquan Song,Xiaoqing Wang,Zhipeng Wang,Qing Lan,Yanning Chen,Jingwei Wu,Luke Simon,Wenjing Zhang,Qi Guo,Fedor Borisyuk*

Main category: cs.IR

TL;DR: MixLM框架通过减少输入上下文长度，同时保持跨编码器排名器的语义强度，显著提高了基于大型语言模型的推荐和搜索系统的吞吐量。它使用文本和嵌入令牌的混合来表示输入，将项目描述编码为几个嵌入令牌，并存储在近线缓存中。部署到LinkedIn的实际搜索应用后，在相同的延迟预算下，MixLM相比强基线吞吐量提高了10倍，且相关性指标保持不变。此外，还带来了每日活跃用户(DAU)0.47%的增长。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在现代推荐和搜索系统中表现出色，但面临工业级延迟和吞吐量要求下的高计算开销问题。特别是跨编码器排序系统通常因为需要处理长上下文预填充工作负载而增加计算负担。为了提高系统吞吐量并维持语义表达力，提出了MixLM框架。

Method: MixLM通过采用文本与嵌入令牌相结合的方式来呈现输入内容，减少了输入上下文长度。具体来说，该方法将目录中的所有条目编码成少量嵌入令牌，并存储于近线缓存中，从而在线推理时有效缩短了从数千个文本令牌至少数个嵌入令牌的项目长度。

Result: 实验结果表明，在相同的延迟预算下，MixLM相比强大基线提升了10倍的吞吐量，同时保持了相关性度量。MixLM带来的效率提升使得LLM驱动的搜索能够全面部署，导致在线A/B测试中日活跃用户数(DAU)显著增加了0.47%。

Conclusion: MixLM作为一种新颖的基于LLM的排名框架，通过优化输入表示形式成功解决了现有跨编码器系统面临的高计算成本问题，不仅极大提高了系统吞吐量，而且对实际业务产生了积极影响。

Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.

</details>


### [81] [A Comparative Study of Retrieval Methods in Azure AI Search](https://arxiv.org/abs/2512.08078)
*Qiang Mao,Han Qin,Robert Neary,Charles Wang,Fusheng Wei,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本研究评估了微软Azure的检索增强生成(RAG)框架内的检索策略，以识别电子发现中早期案件评估(ECA)的有效方法。比较了Azure AI搜索的关键词、语义、向量、混合和混合-语义检索方法的性能，并展示了每种方法AI生成响应的准确性、相关性和一致性。


<details>
  <summary>Details</summary>
Motivation: 律师们越来越有兴趣超越关键词和语义搜索，以提高在文档审阅任务中查找关键信息的效率。大型语言模型现在被视为律师可以用来自然语言提问工具，以便在文档审阅期间从数据中获得准确简洁的答案。

Method: 通过比较Azure AI Search提供的关键词、语义、向量、混合以及混合-语义检索方法的表现来评估检索策略的效果。

Result: 研究结果揭示了不同检索方法下AI生成回答的准确性、相关性及一致性表现。

Conclusion: 法律从业者可以利用这项研究的结果来改进未来RAG配置的选择。

Abstract: Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future.

</details>


### [82] [Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery](https://arxiv.org/abs/2512.08079)
*Qiang Mao,Fusheng Wei,Robert Neary,Charles Wang,Han Qin,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文探讨了通过整合图像聚类、图像字幕生成和大型语言模型来自动生成集群描述的方法。研究结果表明，使用每集群20张图片的战略性采样方法在保证描述质量的同时显著降低了计算成本；基于大型语言模型的方法在表现上优于传统的TF-IDF基线方法，并且标准提示方式比链式思维提示更适合此任务。


<details>
  <summary>Details</summary>
Motivation: 随着数字图像创建与保存的快速增长，在法律发现、数字存档及内容管理过程中面临着巨大挑战。企业和法律团队需要在严格的时间压力下从大量的图像集合中组织、分析并提取有价值的信息，使得手动审查变得不切实际且成本高昂。因此，对于能够高效地组织和描述大规模图像数据集的自动化方法的需求日益增加。

Method: 采用K-means算法将图像分成20个视觉上一致的集群，并利用Azure AI Vision API生成基础字幕。接着评估了集群描述过程中的三个关键方面：1）图像采样策略，比较随机、基于中心点、分层、混合以及基于密度的采样方法与使用所有集群图像之间的差异；2）提示技术，对比标准提示与链式思维提示的效果；3）描述生成方法，比较基于大型语言模型的方法与传统TF-IDF及模板化方法的表现。

Result: 实验结果显示，每集群选取20张图片的战略性采样方案在保持良好描述质量的同时大幅度减少了计算开销，而只有分层采样的效果略有下降。基于大型语言模型的方法在性能上始终优于TF-IDF基准方法，并且针对这项任务而言，标准提示方式优于链式思维提示。

Conclusion: 本研究为部署可扩展且准确的集群描述系统提供了实用指导，支持法律发现及其他需要自动组织大量图像集合领域的高容量工作流程。

Abstract: The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections.

</details>


### [83] [Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters](https://arxiv.org/abs/2512.08083)
*Keith Huffman,Jianping Zhang,Nathaniel Huber-Fliflet,Fusheng Wei,Peter Gronvall*

Main category: cs.IR

TL;DR: 本文研究了大语言模型在律师-客户特权文档检测中的应用，探讨了随机性控制参数对分类性能的影响，并提出了一种利用随机性提高准确性的方法。实验结果显示该方法显著提升了准确性，并有助于增强企业对LLM输出的信心。


<details>
  <summary>Details</summary>
Motivation: 随着组织越来越多地依赖大语言模型来辅助合规工作流程，减少输出变异性对于建立内部和监管机构对基于LLM的制裁筛选决策的信任至关重要。本文旨在探索如何通过调整随机性控制参数以及开发新方法来优化LLM在识别法律特权文件方面的表现。

Method: 通过对使用不同随机性设置的大语言模型进行实证研究，评估它们在识别律师-客户特权文档时的有效性；分析随机性控制参数对分类结果的具体影响；并提出一种利用随机性以增强准确性的新方法论。

Result: 实验表明，大语言模型能够有效识别特权文档；随机性控制参数对分类性能的影响较小；所提出的利用随机性改进准确性的方法显著提高了模型的表现。

Conclusion: 本研究表明，通过适当的方法利用随机性可以显著提高大语言模型在律师-客户特权文档检测任务中的准确性，同时也有助于提升企业对该技术应用于制裁合规过程的信心。

Abstract: In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions.

</details>


### [84] [Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring](https://arxiv.org/abs/2512.08398)
*Jiin Park,Hyuna Jeon,Yoonseo Lee,Jisu Hong,Misuk Kim*

Main category: cs.IR

TL;DR: 本文提出了一种基于本体的知识图谱构建方法，该方法通过组织文档的层次语义结构、分解句子和表格为原子命题，并通过基于大语言模型的三元组提取将其整合到本体-知识图谱中。实验结果表明，与现有方法相比，该方法在所有类型的问答上都取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 工业标准包含了大量技术信息和复杂规则，这些内容以高度结构化的格式呈现，包括表格、适用范围、约束条件、例外情况以及数值计算等，这使得知识图谱（KG）的构建变得特别具有挑战性。传统的方法难以准确地反映领域特定的语义。

Method: 研究者们提出了一种新方法，它首先将文档组织成一个层次化的语义结构，然后将句子和表格分解为源自条件和数值规则的原子命题，最后通过基于大语言模型（LLM）的三元组提取技术将这些原子命题集成到本体-知识图谱中。

Result: 为了验证其有效性，从工业标准中构建了规则、表格及多跳QA数据集，以及有毒条款检测数据集，并实现了一个本体感知的KG-RAG框架进行对比评估。实验结果显示，在所有类型的问答上，相较于现有的KG-RAG方法，所提方法均实现了显著的性能改进。

Conclusion: 研究表明，即使对于那些包含交织条件、约束和范围的工业文档来说，可靠且可扩展的知识表示也是可行的，这有助于未来领域特定RAG的发展和智能文档管理。

Abstract: Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.

</details>


### [85] [VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation](https://arxiv.org/abs/2512.08702)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Zitong Wan,Hewei Wang,Weijie Liu,Yijie Li,Edith C. H. Ngai*

Main category: cs.IR

TL;DR: 本文提出了一种名为VI-MMRec的框架，该框架通过基于用户交互项的模态特定特征相似性构建虚拟用户-项目交互来丰富稀疏的用户-项目交互。VI-MMRec引入了两种不同的策略（Overlay和Synergistic）来处理模态特定偏好及互补偏好，并设计了一个统计信息驱动的权重分配机制以自适应地为虚拟交互分配权重。作为一个即插即用的框架，VI-MMRec能够与现有模型无缝集成，提高性能而无需修改其核心架构。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推荐模型尽管表现良好，但仍然受到数据稀疏问题的影响，这主要是因为用户通常只与一小部分可用项目进行互动，导致现有模型将未观察到的项目随意视为负面样本。

Method: 提出了VI-MMRec框架，该框架利用用户已交互项目的模态特定特征相似性生成虚拟用户-项目交互，以此来扩充稀疏的用户-项目交互数据。VI-MMRec包括两种策略：(1) Overlay，独立聚合模态特定相似性以保留模态特定用户偏好；(2) Synergistic，综合融合跨模态相似性以捕捉互补用户偏好。此外，还设计了一个根据数据集特定模态相关性自适应分配权重给虚拟用户-项目交互的机制。

Result: 在六个真实世界数据集上使用七种最先进多模态推荐模型进行了全面实验，验证了VI-MMRec的有效性。

Conclusion: VI-MMRec作为一款模型无关且无训练成本的框架，能有效解决用户-项目交互稀疏的问题，同时保持或提升原有模型的性能。它具有灵活性高、易于实现等优点，非常适合实际部署。

Abstract: Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.

</details>
