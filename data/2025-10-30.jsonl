{"id": "2510.25220", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25220", "abs": "https://arxiv.org/abs/2510.25220", "authors": ["Zhijie Lin", "Zhuofeng Li", "Chenglei Dai", "Wentian Bao", "Shuai Lin", "Enyun Yu", "Haoxiang Zhang", "Liang Zhao"], "title": "GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction", "comment": "Accepted by CIKM 2025", "summary": "In a multi-stage recommendation system, reranking plays a crucial role in\nmodeling intra-list correlations among items. A key challenge lies in exploring\noptimal sequences within the combinatorial space of permutations. Recent\nresearch follows a two-stage (generator-evaluator) paradigm, where a generator\nproduces multiple feasible sequences, and an evaluator selects the best one. In\npractice, the generator is typically implemented as an autoregressive model.\nHowever, these two-stage methods face two main challenges. First, the\nseparation of the generator and evaluator hinders end-to-end training. Second,\nautoregressive generators suffer from inference efficiency. In this work, we\npropose a Unified Generative Efficient Reranking Framework (GReF) to address\nthe two primary challenges. Specifically, we introduce Gen-Reranker, an\nautoregressive generator featuring a bidirectional encoder and a dynamic\nautoregressive decoder to generate causal reranking sequences. Subsequently, we\npre-train Gen-Reranker on the item exposure order for high-quality parameter\ninitialization. To eliminate the need for the evaluator while integrating\nsequence-level evaluation during training for end-to-end optimization, we\npropose post-training the model through Rerank-DPO. Moreover, for efficient\nautoregressive inference, we introduce ordered multi-token prediction (OMTP),\nwhich trains Gen-Reranker to simultaneously generate multiple future items\nwhile preserving their order, ensuring practical deployment in real-time\nrecommender systems. Extensive offline experiments demonstrate that GReF\noutperforms state-of-the-art reranking methods while achieving latency that is\nnearly comparable to non-autoregressive models. Additionally, GReF has also\nbeen deployed in a real-world video app Kuaishou with over 300 million daily\nactive users, significantly improving online recommendation quality."}
{"id": "2510.25259", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25259", "abs": "https://arxiv.org/abs/2510.25259", "authors": ["Yehjin Shin", "Jeongwhan Choi", "Seojin Kim", "Noseong Park"], "title": "TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation", "comment": "The 39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Recently, convolutional filters have been increasingly adopted in sequential\nrecommendation for their ability to capture local sequential patterns. However,\nmost of these models complement convolutional filters with self-attention. This\nis because convolutional filters alone, generally fixed filters, struggle to\ncapture global interactions necessary for accurate recommendation. We propose\nTime-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a\nmodel inspired by graph signal processing, where time-variant graph filters\ncapture position-dependent temporal variations in user sequences. By replacing\nboth fixed kernels and self-attention with time-variant filters, TV-Rec\nachieves higher expressive power and better captures complex interaction\npatterns in user behavior. This design not only eliminates the need for\nself-attention but also reduces computation while accelerating inference.\nExtensive experiments on six public benchmarks show that TV-Rec outperforms\nstate-of-the-art baselines by an average of 7.49%."}
{"id": "2510.25285", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25285", "abs": "https://arxiv.org/abs/2510.25285", "authors": ["Qiushi Pan", "Hao Wang", "Guoyuan An", "Luankang Zhang", "Wei Guo", "Yong Liu"], "title": "Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts", "comment": null, "summary": "In recommendation systems, how to effectively scale up recommendation models\nhas been an essential research topic. While significant progress has been made\nin developing advanced and scalable architectures for sequential\nrecommendation(SR) models, there are still challenges due to items'\nmulti-faceted characteristics and dynamic item relevance in the user context.\nTo address these issues, we propose Fuxi-MME, a framework that integrates a\nmulti-embedding strategy with a Mixture-of-Experts (MoE) architecture.\nSpecifically, to efficiently capture diverse item characteristics in a\ndecoupled manner, we decompose the conventional single embedding matrix into\nseveral lower-dimensional embedding matrices. Additionally, by substituting\nrelevant parameters in the Fuxi Block with an MoE layer, our model achieves\nadaptive and specialized transformation of the enriched representations.\nEmpirical results on public datasets show that our proposed framework\noutperforms several competitive baselines."}
{"id": "2510.25402", "categories": ["cs.IR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.25402", "abs": "https://arxiv.org/abs/2510.25402", "authors": ["Yuqian Chai", "Chaochao Wang", "Weilei Wang"], "title": "Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework", "comment": null, "summary": "Despite the surge in patent applications and emergence of AI drafting tools,\nsystematic evaluation of patent content quality has received limited research\nattention. To address this gap, We propose to evaluate patents using regulatory\ncompliance, technical coherence, and figure-reference consistency detection\nmodules, and then generate improvement suggestions via an integration module.\nThe framework is validated on a comprehensive dataset comprising 80\nhuman-authored and 80 AI-generated patents from two patent drafting tools.\nExperimental results show balanced accuracies of 99.74\\%, 82.12\\%, and 91.2\\%\nrespectively across the three detection modules when validated against expert\nannotations. Additional analysis was conducted to examine defect distributions\nacross patent sections, technical domains, and authoring sources. Section-based\nanalysis indicates that figure-text consistency and technical detail precision\nrequire particular attention. Mechanical Engineering and Construction show more\nclaim-specification inconsistencies due to complex technical documentation\nrequirements. AI-generated patents show a significant gap compared to\nhuman-authored ones. While human-authored patents primarily contain\nsurface-level errors like typos, AI-generated patents exhibit more structural\ndefects in figure-text alignment and cross-references."}
{"id": "2510.24769", "categories": ["cs.MM", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24769", "abs": "https://arxiv.org/abs/2510.24769", "authors": ["Mojtaba Mozhganfar", "Pooya Jamshidi", "Seyyed Ali Aghamiri", "Mohsen Ghasemi", "Mahdi Dolati", "Farzad Tashtarian", "Ahmad Khonsari", "Christian Timmerer"], "title": "YTLive: A Dataset of Real-World YouTube Live Streaming Sessions", "comment": null, "summary": "Live streaming plays a major role in today's digital platforms, supporting\nentertainment, education, social media, etc. However, research in this field is\nlimited by the lack of large, publicly available datasets that capture\nreal-time viewer behavior at scale. To address this gap, we introduce YTLive, a\npublic dataset focused on YouTube Live. Collected through the YouTube\nResearcher Program over May and June 2024, YTLive includes more than 507000\nrecords from 12156 live streams, tracking concurrent viewer counts at\nfive-minute intervals along with precise broadcast durations. We describe the\ndataset design and collection process and present an initial analysis of\ntemporal viewing patterns. Results show that viewer counts are higher and more\nstable on weekends, especially during afternoon hours. Shorter streams attract\nlarger and more consistent audiences, while longer streams tend to grow slowly\nand exhibit greater variability. These insights have direct implications for\nadaptive streaming, resource allocation, and Quality of Experience (QoE)\nmodeling. YTLive offers a timely, open resource to support reproducible\nresearch and system-level innovation in live streaming. The dataset is publicly\navailable at github."}
{"id": "2510.25428", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25428", "abs": "https://arxiv.org/abs/2510.25428", "authors": ["Thang-Long Nguyen-Ho", "Minh-Khoi Pham", "Hoang-Bao Le"], "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report", "comment": "Alibaba International E-commerce Product Search Competition @ CIKM\n  2025", "summary": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search."}
{"id": "2510.25225", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.25225", "abs": "https://arxiv.org/abs/2510.25225", "authors": ["Shota Nakada", "Kazuhiro Saito", "Yuchi Ishikawa", "Hokuto Munakata", "Tatsuya Komatsu", "Masayoshi Kondo"], "title": "Hallucination Localization in Video Captioning", "comment": "under review", "summary": "We propose a novel task, hallucination localization in video captioning,\nwhich aims to identify hallucinations in video captions at the span level (i.e.\nindividual words or phrases). This allows for a more detailed analysis of\nhallucinations compared to existing sentence-level hallucination detection\ntask. To establish a benchmark for hallucination localization, we construct\nHLVC-Dataset, a carefully curated dataset created by manually annotating 1,167\nvideo-caption pairs from VideoLLM-generated captions. We further implement a\nVideoLLM-based baseline method and conduct quantitative and qualitative\nevaluations to benchmark current performance on hallucination localization."}
{"id": "2510.25488", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25488", "abs": "https://arxiv.org/abs/2510.25488", "authors": ["Yiteng Tu", "Weihang Su", "Yujia Zhou", "Yiqun Liu", "Fen Lin", "Qin Liu", "Qingyao Ai"], "title": "Generalized Pseudo-Relevance Feedback", "comment": null, "summary": "Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting."}
{"id": "2510.25600", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.25600", "abs": "https://arxiv.org/abs/2510.25600", "authors": ["Zhonghua Jiang", "Kunxi Li", "Yiyun Zhou", "Sihao Liu", "Zhaode Wang", "Chengfei lv", "Shengyu Zhang"], "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models", "comment": null, "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."}
{"id": "2510.25622", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25622", "abs": "https://arxiv.org/abs/2510.25622", "authors": ["Yi Xu", "Moyu Zhang", "Chaofan Fan", "Jinxin Hu", "Xiaochen Li", "Yu Zhang", "Xiaoyi Zeng", "Jing Zhang"], "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation", "comment": null, "summary": "Industrial recommender systems rely on unique Item Identifiers (ItemIDs).\nHowever, this method struggles with scalability and generalization in large,\ndynamic datasets that have sparse long-tail data.Content-based Semantic IDs\n(SIDs) address this by sharing knowledge through content quantization. However,\nby ignoring dynamic behavioral properties, purely content-based SIDs have\nlimited expressive power. Existing methods attempt to incorporate behavioral\ninformation but overlook a critical distinction: unlike relatively uniform\ncontent features, user-item interactions are highly skewed and diverse,\ncreating a vast information gap in quality and quantity between popular and\nlong-tail items. This oversight leads to two critical limitations: (1) Noise\nCorruption: Indiscriminate behavior-content alignment allows collaborative\nnoise from long-tail items to corrupt their content representations, leading to\nthe loss of critical multimodal information. (2)Signal Obscurity: The\nequal-weighting scheme for SIDs fails to reflect the varying importance of\ndifferent behavioral signals, making it difficult for downstream tasks to\ndistinguish important SIDs from uninformative ones. To tackle these issues, we\npropose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,\nDenoise, and Amplify multimodal information from content and behavior\nmodalities for semantic IDs learning. The semantic IDs generated by this\nframework named ADA-SID. It introduces two innovations: an adaptive\nbehavior-content alignment that is aware of information richness to shield\nrepresentations from noise, and a dynamic behavioral router to amplify critical\nsignals by applying different weights to SIDs. Extensive experiments on public\nand large-scale industrial datasets demonstrate ADA-SID's significant\nsuperiority in both generative and discriminative recommendation tasks."}
{"id": "2510.25718", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.25718", "abs": "https://arxiv.org/abs/2510.25718", "authors": ["Jamie Mahowald", "Benjamin Charles Germain Lee"], "title": "Retrieval-Augmented Search for Large-Scale Map Collections with ColPali", "comment": "5 pages, 5 figures", "summary": "Multimodal approaches have shown great promise for searching and navigating\ndigital collections held by libraries, archives, and museums. In this paper, we\nintroduce map-RAS: a retrieval-augmented search system for historic maps. In\naddition to introducing our framework, we detail our publicly-hosted demo for\nsearching 101,233 map images held by the Library of Congress. With our system,\nusers can multimodally query the map collection via ColPali, summarize search\nresults using Llama 3.2, and upload their own collections to perform\ninter-collection search. We articulate potential use cases for archivists,\ncurators, and end-users, as well as future work with our system in both machine\nlearning and the digital humanities. Our demo can be viewed at:\nhttp://www.mapras.com."}
