{"id": "2511.19514", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.19514", "abs": "https://arxiv.org/abs/2511.19514", "authors": ["Yang Wu", "Qian Li", "Yuling Xiong", "Hongbo Tang", "Xun Liu", "Jun Zhang", "Huan Yu", "Jie Jiang", "Hailong Shi"], "title": "SCoTER: Structured Chain-of-Thought Transfer for Enhanced Recommendation", "comment": "12 pages,4 figures", "summary": "Harnessing the reasoning power of Large Language Models (LLMs) for recommender systems is hindered by two fundamental challenges. First, current approaches lack a mechanism for automated, data-driven discovery of effective reasoning patterns, relying instead on brittle manual templates or unstable zero-shot prompting. Second, they employ structure-collapsing integration: direct prompting incurs prohibitive online inference costs, while feature extraction collapses reasoning chains into single vectors, discarding stepwise logic. To address these challenges, we propose SCoTER (Structured Chain-of-Thought Transfer for Enhanced Recommendation), a unified framework that treats pattern discovery and structure-aware transfer as a jointly optimized problem. Specifically, SCoTER operationalizes this through two synergistic components: a GVM pipeline for automated pattern discovery and a structure-preserving integration architecture that transfers stepwise logic to efficient models. Formally, we provide information-theoretic justification proving that structure-preserving transfer achieves tighter performance bounds than structure-agnostic alternatives. Empirically, experiments on four benchmarks demonstrate improvements of 3.75\\%-11.59\\% over a strong TIGER backbone. Moreover, in production deployment on the Tencent Advertising Platform, SCoTER achieved a 2.14\\% lift in Gross Merchandise Value (GMV) while eliminating online LLM inference costs. Overall, SCoTER establishes a principled and production-validated blueprint for transferring structured LLM reasoning to large-scale recommender systems.", "AI": {"tldr": "SCoTER\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u63a8\u7406\u6a21\u5f0f\u53d1\u73b0\u548c\u7ed3\u6784\u4fdd\u6301\u96c6\u6210\uff0c\u5c06LLM\u7684\u63a8\u7406\u80fd\u529b\u9ad8\u6548\u8fc1\u79fb\u5230\u63a8\u8350\u7cfb\u7edf\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u63d0\u53473.75%-11.59%\uff0c\u5e76\u5728\u817e\u8baf\u5e7f\u544a\u5e73\u53f0\u5b9e\u73b02.14%\u7684GMV\u589e\u957f\u4e14\u6d88\u9664\u5728\u7ebfLLM\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u63a8\u8350\u7cfb\u7edf\u5229\u7528LLM\u63a8\u7406\u80fd\u529b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u81ea\u52a8\u5316\u63a8\u7406\u6a21\u5f0f\u53d1\u73b0\u673a\u5236\uff08\u4f9d\u8d56\u8106\u5f31\u7684\u624b\u52a8\u6a21\u677f\u6216\u4e0d\u7a33\u5b9a\u7684\u96f6\u6837\u672c\u63d0\u793a\uff09\uff0c\u4ee5\u53ca\u7ed3\u6784\u7834\u574f\u6027\u96c6\u6210\u65b9\u6cd5\uff08\u76f4\u63a5\u63d0\u793a\u5bfc\u81f4\u8fc7\u9ad8\u5728\u7ebf\u6210\u672c\uff0c\u7279\u5f81\u63d0\u53d6\u4e22\u5f03\u9010\u6b65\u63a8\u7406\u903b\u8f91\uff09\u3002", "method": "\u63d0\u51faSCoTER\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a1) GVM\u7ba1\u9053\u7528\u4e8e\u81ea\u52a8\u5316\u63a8\u7406\u6a21\u5f0f\u53d1\u73b0\uff1b2) \u7ed3\u6784\u4fdd\u6301\u96c6\u6210\u67b6\u6784\uff0c\u5c06\u9010\u6b65\u63a8\u7406\u903b\u8f91\u8fc1\u79fb\u5230\u9ad8\u6548\u6a21\u578b\u3002\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u8bba\u8bc1\u660e\u7ed3\u6784\u4fdd\u6301\u8fc1\u79fb\u6bd4\u5ffd\u7565\u7ed3\u6784\u7684\u65b9\u6cd5\u6709\u66f4\u7d27\u7684\u6027\u80fd\u754c\u9650\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u76f8\u6bd4TIGER\u57fa\u7ebf\u63d0\u53473.75%-11.59%\uff1b\u5728\u817e\u8baf\u5e7f\u544a\u5e73\u53f0\u90e8\u7f72\u4e2d\uff0cGMV\u63d0\u53472.14%\u4e14\u5b8c\u5168\u6d88\u9664\u5728\u7ebfLLM\u63a8\u7406\u6210\u672c\u3002", "conclusion": "SCoTER\u4e3a\u5c06\u7ed3\u6784\u5316LLM\u63a8\u7406\u8fc1\u79fb\u5230\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u4e14\u7ecf\u8fc7\u751f\u4ea7\u9a8c\u8bc1\u7684\u84dd\u56fe\uff0c\u89e3\u51b3\u4e86\u63a8\u7406\u6a21\u5f0f\u53d1\u73b0\u548c\u7ed3\u6784\u4fdd\u6301\u96c6\u6210\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2511.19931", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19931", "abs": "https://arxiv.org/abs/2511.19931", "authors": ["Ziwei Liu", "Qidong Liu", "Wanyu Wang", "Yejing Wang", "Tong Xu", "Wei Huang", "Chong Chen", "Peng Chuan", "Xiangyu Zhao"], "title": "LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training", "comment": null, "summary": "Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {https://anonymous.4open.science/r/LLM-EDT-583F}.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM-EDT\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff0c\u901a\u8fc7\u53ef\u8f6c\u79fb\u9879\u76ee\u589e\u5f3a\u5668\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8f6c\u6362\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u548c\u8de8\u57df\u504f\u597d\u8f6c\u6362\u56f0\u96be\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u65e0\u5173\u566a\u58f0\u548c\u7c97\u7565\u753b\u50cf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53ef\u8f6c\u79fb\u9879\u76ee\u589e\u5f3a\u5668\u81ea\u9002\u5e94\u751f\u6210\u8de8\u57df\u884c\u4e3a\uff1b\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7ebf\u7a0b\u548c\u5171\u4eab\u80cc\u666f\uff1b\u8bbe\u8ba1\u9886\u57df\u611f\u77e5\u753b\u50cf\u6a21\u5757\u751f\u6210\u5168\u9762\u7528\u6237\u753b\u50cf\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LLM-EDT\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "LLM-EDT\u901a\u8fc7\u667a\u80fd\u6570\u636e\u589e\u5f3a\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u63a8\u8350\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.19979", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.19979", "abs": "https://arxiv.org/abs/2511.19979", "authors": ["Kaike Zhang", "Jiakai Tang", "Du Su", "Shuchang Liu", "Julian McAuley", "Lina Yao", "Qi Cao", "Yue Feng", "Fei Sun"], "title": "The 2nd Workshop on Human-Centered Recommender Systems", "comment": null, "summary": "Recommender systems shape how people discover information, form opinions, and connect with society. Yet, as their influence grows, traditional metrics, e.g., accuracy, clicks, and engagement, no longer capture what truly matters to humans. The workshop on Human-Centered Recommender Systems (HCRS) calls for a paradigm shift from optimizing engagement toward designing systems that truly understand, involve, and benefit people. It brings together researchers in recommender systems, human-computer interaction, AI safety, and social computing to explore how human values, e.g., trust, safety, fairness, transparency, and well-being, can be integrated into recommendation processes. Centered around three thematic axes-Human Understanding, Human Involvement, and Human Impact-HCRS features keynotes, panels, and papers covering topics from LLM-based interactive recommenders to societal welfare optimization. By fostering interdisciplinary collaboration, HCRS aims to shape the next decade of responsible and human-aligned recommendation research.", "AI": {"tldr": "Recommender systems need shift from engagement metrics to human-centered values (trust, safety, fairness) through interdisciplinary collaboration.", "motivation": "Traditional metrics (accuracy, clicks) fail to capture human values as recommender systems' societal influence grows.", "method": "Workshop with keynotes, panels, and papers exploring human values integration via three axes: Human Understanding, Involvement, and Impact.", "result": "Fosters collaboration across recommender systems, HCI, AI safety, and social computing to align systems with human welfare.", "conclusion": "HCRS aims to drive a paradigm shift toward responsible, human-aligned recommendation research for the next decade."}}
{"id": "2511.19877", "categories": ["cs.MM", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.19877", "abs": "https://arxiv.org/abs/2511.19877", "authors": ["Xiangyu Zhao", "Yaling Shen", "Yiwen Jiang", "Zimu Wang", "Jiahe Liu", "Maxmartwell H Cheng", "Guilherme C Oliveira", "Robert Desimone", "Dominic Dwyer", "Zongyuan Ge"], "title": "It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models", "comment": null, "summary": "Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001LLM\u6846\u67b6\uff0c\u7528\u4e8e\u6291\u90c1\u75c7\u68c0\u6d4b\uff0c\u901a\u8fc7\u65f6\u95f4\u6233\u7ea7\u522b\u7684\u89c6\u542c\u7279\u5f81\u5bf9\u9f50\uff0c\u5728DAIC-WoZ\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u5168\u7403\u6700\u666e\u904d\u7684\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u4e4b\u4e00\u3002\u4f20\u7edfLLM\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\uff0c\u65e0\u6cd5\u5904\u7406\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u4e2d\u7684\u975e\u8bed\u8a00\u7ebf\u7d22\u3002\u867d\u7136\u591a\u6a21\u6001LLM\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u5f88\u5c11\u6709\u9488\u5bf9\u5fc3\u7406\u5b66\u5e94\u7528\u5b9a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001LLM\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4ee5\u5177\u5907\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u65f6\u95f4\u6233\u7ea7\u522b\u5bf9\u9f50\u89c6\u542c\u7279\u5f81\u3002\u8fd9\u79cd\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6539\u5584\u4e86\u8de8\u6a21\u6001\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u3002", "result": "\u5728DAIC-WoZ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u548c\u5148\u524d\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u6269\u5c55\u4ee5\u7eb3\u5165\u989d\u5916\u7684\u751f\u7406\u4fe1\u53f7\uff0c\u4e3a\u8d85\u8d8a\u5fc3\u7406\u5065\u5eb7\u7684\u66f4\u5e7f\u6cdb\u4e34\u5e8a\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.20167", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.20167", "abs": "https://arxiv.org/abs/2511.20167", "authors": ["Yadong Liu", "Shangfei Wang"], "title": "FINE: Factorized multimodal sentiment analysis via mutual INformation Estimation", "comment": "15 pages, 9 figures, conference", "summary": "Multimodal sentiment analysis remains a challenging task due to the inherent heterogeneity across modalities. Such heterogeneity often manifests as asynchronous signals, imbalanced information between modalities, and interference from task-irrelevant noise, hindering the learning of robust and accurate sentiment representations. To address these issues, we propose a factorized multimodal fusion framework that first disentangles each modality into shared and unique representations, and then suppresses task-irrelevant noise within both to retain only sentiment-critical representations. This fine-grained decomposition improves representation quality by reducing redundancy, prompting cross-modal complementarity, and isolating task-relevant sentiment cues. Rather than manipulating the feature space directly, we adopt a mutual information-based optimization strategy to guide the factorization process in a more stable and principled manner. To further support feature extraction and long-term temporal modeling, we introduce two auxiliary modules: a Mixture of Q-Formers, placed before factorization, which precedes the factorization and uses learnable queries to extract fine-grained affective features from multiple modalities, and a Dynamic Contrastive Queue, placed after factorization, which stores latest high-level representations for contrastive learning, enabling the model to capture long-range discriminative patterns and improve class-level separability. Extensive experiments on multiple public datasets demonstrate that our method consistently outperforms existing approaches, validating the effectiveness and robustness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u89e3\u5f0f\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u5206\u89e3\u548c\u4fe1\u606f\u4f18\u5316\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9762\u4e34\u6a21\u6001\u5f02\u6784\u6027\u6311\u6218\uff0c\u5305\u62ec\u5f02\u6b65\u4fe1\u53f7\u3001\u6a21\u6001\u95f4\u4fe1\u606f\u4e0d\u5e73\u8861\u548c\u4efb\u52a1\u65e0\u5173\u566a\u58f0\u5e72\u6270\uff0c\u5f71\u54cd\u60c5\u611f\u8868\u793a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "1. \u5c06\u5404\u6a21\u6001\u5206\u89e3\u4e3a\u5171\u4eab\u548c\u72ec\u7279\u8868\u793a\uff1b2. \u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u4f18\u5316\u7b56\u7565\u6291\u5236\u4efb\u52a1\u65e0\u5173\u566a\u58f0\uff1b3. \u5f15\u5165Q-Former\u6df7\u5408\u6a21\u5757\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\uff1b4. \u4f7f\u7528\u52a8\u6001\u5bf9\u6bd4\u961f\u5217\u589e\u5f3a\u957f\u65f6\u5e8f\u5efa\u6a21\u548c\u7c7b\u522b\u533a\u5206\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5206\u89e3\u5f0f\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8868\u793a\u5206\u89e3\u548c\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u6a21\u6001\u5f02\u6784\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.20122", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20122", "abs": "https://arxiv.org/abs/2511.20122", "authors": ["Ximing Chen", "Pui Ieng Lei", "Yijun Sheng", "Yanyan Liu", "Zhiguo Gong"], "title": "Towards A Tri-View Diffusion Framework for Recommendation", "comment": "13 pages, 11 figures, accepted by KDD2026 (First Cycle)", "summary": "Diffusion models (DMs) have recently gained significant interest for their exceptional potential in recommendation tasks. This stems primarily from their prominent capability in distilling, modeling, and generating comprehensive user preferences. However, previous work fails to examine DMs in recommendation tasks through a rigorous lens. In this paper, we first experimentally investigate the completeness of recommender models from a thermodynamic view. We reveal that existing DM-based recommender models operate by maximizing the energy, while classic recommender models operate by reducing the entropy. Based on this finding, we propose a minimalistic diffusion framework that incorporates both factors via the maximization of Helmholtz free energy. Meanwhile, to foster the optimization, our reverse process is armed with a well-designed denoiser to maintain the inherent anisotropy, which measures the user-item cross-correlation in the context of bipartite graphs. Finally, we adopt an Acceptance-Rejection Gumbel Sampling Process (AR-GSP) to prioritize the far-outnumbered unobserved interactions for model robustness. AR-GSP integrates an acceptance-rejection sampling to ensure high-quality hard negative samples for general recommendation tasks, and a timestep-dependent Gumbel Softmax to handle an adaptive sampling strategy for diffusion models. Theoretical analyses and extensive experiments demonstrate that our proposed framework has distinct superiority over baselines in terms of accuracy and efficiency.", "AI": {"tldr": "\u4e00\u7bc7\u5173\u4e8e\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e94\u7528\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u70ed\u529b\u5b66\u89c6\u89d2\u7684\u6700\u5c0f\u5316\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4ea5\u59c6\u970d\u5179\u81ea\u7531\u80fd\u91cf\u548c\u72ec\u7279\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u6765\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u7f3a\u4e4f\u4e25\u8c28\u7684\u7406\u8bba\u5206\u6790\uff0c\u7279\u522b\u662f\u4ece\u70ed\u529b\u5b66\u89d2\u5ea6\u5bf9\u63a8\u8350\u6a21\u578b\u5b8c\u6574\u6027\u7684\u7814\u7a76\u4e0d\u8db3\u3002\u4f5c\u8005\u53d1\u73b0\u4f20\u7edf\u63a8\u8350\u6a21\u578b\u901a\u8fc7\u964d\u4f4e\u71b5\u5de5\u4f5c\uff0c\u800c\u6269\u6563\u6a21\u578b\u901a\u8fc7\u6700\u5927\u5316\u80fd\u91cf\u5de5\u4f5c\uff0c\u8fd9\u4e2a\u53d1\u73b0\u63ed\u793a\u4e86\u7ed3\u5408\u4e24\u8005\u7684\u53ef\u80fd\u6027\u3002", "method": "1. \u4ece\u70ed\u529b\u5b66\u89d2\u5ea6\u5b9e\u9a8c\u7814\u7a76\u63a8\u8350\u6a21\u578b\u7684\u5b8c\u6574\u6027\n2. \u63d0\u51fa\u6700\u5927\u5316\u4ea5\u59c6\u970d\u5179\u81ea\u7531\u80fd\u91cf\u7684\u6700\u5c0f\u5316\u6269\u6563\u6846\u67b6\n3. \u5728\u53cd\u5411\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53bb\u566a\u5668\u4fdd\u6301\u5404\u5411\u5f02\u6027\n4. \u91c7\u7528\u63a5\u53d7-\u62d2\u7eddGumbel\u91c7\u6837\u8fc7\u7a0b(AR-GSP)\u4f18\u5148\u5904\u7406\u672a\u89c2\u5bdf\u7684\u4ea4\u4e92", "result": "\u7406\u8bba\u5206\u6790\u548c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002AR-GSP\u80fd\u591f\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u786c\u8d1f\u6837\u672c\uff0c\u5e76\u5904\u7406\u6269\u6563\u6a21\u578b\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u70ed\u529b\u5b66\u89c6\u89d2\u4e3a\u6269\u6563\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7684\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
