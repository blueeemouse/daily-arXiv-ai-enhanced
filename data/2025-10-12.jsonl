{"id": "2510.07326", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.07326", "abs": "https://arxiv.org/abs/2510.07326", "authors": ["Han Hu", "Dongheng Lin", "Qiming Huang", "Yuqi Hou", "Hyung Jin Chang", "Jianbo Jiao"], "title": "Audio-Visual Separation with Hierarchical Fusion and Representation Alignment", "comment": null, "summary": "Self-supervised audio-visual source separation leverages natural correlations\nbetween audio and vision modalities to separate mixed audio signals. In this\nwork, we first systematically analyse the performance of existing multimodal\nfusion methods for audio-visual separation task, demonstrating that the\nperformance of different fusion strategies is closely linked to the\ncharacteristics of the sound: middle fusion is better suited for handling\nshort, transient sounds, while late fusion is more effective for capturing\nsustained and harmonically rich sounds. We thus propose a hierarchical fusion\nstrategy that effectively integrates both fusion stages. In addition, training\ncan be made easier by incorporating high-quality external audio\nrepresentations, rather than relying solely on the audio branch to learn them\nindependently. To explore this, we propose a representation alignment approach\nthat aligns the latent features of the audio encoder with embeddings extracted\nfrom pre-trained audio models. Extensive experiments on MUSIC, MUSIC-21 and\nVGGSound datasets demonstrate that our approach achieves state-of-the-art\nresults, surpassing existing methods under the self-supervised setting. We\nfurther analyse the impact of representation alignment on audio features,\nshowing that it reduces modality gap between the audio and visual modalities."}
{"id": "2510.07355", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.07355", "abs": "https://arxiv.org/abs/2510.07355", "authors": ["Krish Patel", "Dingkun Zhou", "Ajay Kankipati", "Akshaj Gupta", "Zeyi Austin Li", "Mohul Shukla", "Vibhor Narang", "Sara Kofman", "Zongli Ye", "Grace Wang", "Xiaoyu Shi", "Tingle Li", "Guan-Ting Lin", "Kan Jen Cheng", "Huang-Cheng Chou", "Jiachen Lian", "Gopala Anumanchipalli"], "title": "AV-EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Omni-modal LLMS with Audio-visual Cues", "comment": null, "summary": "Emotions conveyed through voice and face shape engagement and context in\nhuman-AI interaction. Despite rapid progress in omni-modal large language\nmodels (LLMs), the holistic evaluation of emotional reasoning with audiovisual\ncues remains limited. To address this gap, we introduce AV-EMO-Reasoning, a\nbenchmark designed to systematically assess emotional coherence in LLMs. The\nframework leverages a curated, single- and multi-turn synthetic audiovisual\ncorpus with a real-world set and is assessed under continuous, categorical, and\nperceptual metrics. Experiments with leading LLMs show that visual cues\nreliably improve emotional coherence over audio-only baselines. Moreover, LLMs\ncan leverage audio-visual cues to generate more emotion-aware speech. Models\nexhibit complementary strengths across metric families, indicating that\nautomatic scores capture facets distinct from perceptual judgments. By\nreleasing a systematic evaluation benchmark, AV-EMO-Reasoning offers a\nreproducible standard for evaluating emotion-aware dialogue and advances toward\nmore natural, adaptive human-AI interaction."}
