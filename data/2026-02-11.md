<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2602.09323)
*Jie Kong,Wei Wang,Jiehan Zhou,Chen Yu*

Main category: cs.DC

TL;DR: 提出了一种名为LLM-CoOpt的算法-硬件协同设计框架，通过优化键值缓存、引入分组查询注意力机制和采用分页注意力技术，有效提升了大规模语言模型推理过程中的吞吐量和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLMs）在推理过程中遇到的内存带宽瓶颈、计算冗余以及长序列处理效率低下等问题。

Method: 1. 优化键值缓存（Opt-KV），通过改进KV缓存写入与读取路径，并使用FP8量化减少内存占用。
2. 引入分组查询注意力机制（Opt-GQA），将多头自注意力重组为共享键值投影的分组查询注意力，以降低整体计算复杂度。
3. 采用分页注意力技术（Opt-Pa）处理长序列，通过两步策略首先将长序列分割成可管理的小块，然后应用懒加载内存映射与计算来减轻内存压力。

Result: 实验表明，在LLaMa-13BGPTQ模型上，LLM-CoOpt能够将推理吞吐量提高最多13.43%，延迟降低最多16.79%，同时保持了模型准确性。

Conclusion: LLM-CoOpt提供了一个实用且高性能的大规模语言模型实际推理优化方案。

Abstract: Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.

</details>


### [2] [The Coordination Criterion](https://arxiv.org/abs/2602.09435)
*Joseph M. Hellerstein*

Main category: cs.DC

TL;DR: 本研究提出了一种协调标准，用以判断在分布式规范中何时内在地需要协调。该标准基于Lamport历史和可观察结果，不依赖于特定的编程语言、对象实现或协议结构，并为CAP风格的不可能性、CALM风格的无协调自由度等多种经典结果提供了一个统一的解释框架。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确定分布式规范中什么时候本质上需要协调，而不是由特定协议或实现策略所强制要求的。

Method: 通过异步消息传递模型，展示了一个规范只有在其关于历史扩展在某种适当的可观察结果顺序下是单调的时候，才能拥有无需协调的实现。这个协调标准直接建立在Lamport历史——即在发生之前部分排序执行——以及规范定义的可观察结果之上。

Result: 得出了一个明确区分能够无协调实现的规范与不可避免需要协调的规范之间的界限。此外，该标准还为一系列经典结果（如CAP风格的不可能性、CALM风格的无协调自由度等）提供了统一的解释。

Conclusion: 这项工作确立了判断分布式系统是否需要内在协调的标准，它不仅适用于多种场景，而且为理解不同一致性现象提供了通用的语言。

Abstract: When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.
  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.

</details>


### [3] [High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors](https://arxiv.org/abs/2602.09604)
*Ruimin Shi,Gabin Schieffer,Pei-Hung Lin,Maya Gokhale,Andreas Herten,Ivy Peng*

Main category: cs.DC

TL;DR: 本研究通过量子态矢量模拟这一重要工作负载，探讨了在支持灵活向量长度的ARM SVE和RISC-V RVV架构上实现高性能可移植性的可能性。提出了一种与向量长度无关（VLA）的设计及优化技术，并在Google的Qsim中实现了该设计。实验结果表明，在Fujitsu A64FX、NVIDIA Grace以及AWS Graviton3三种处理器上，相比传统方法，分别获得了最高达4.5倍、2.5倍和1.5倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 随着ARM SVE和RISC-V RVV等支持灵活向量长度的新一代向量架构逐渐成为高端处理器的重要组成部分，探索如何在此类架构上实现针对特定工作负载（如量子计算中的状态矢量模拟）的高效且可移植的性能变得尤为重要。

Method: 研究者们提出了一种与向量长度无关（VLA）的设计方案，结合了几项关键优化技术：根据向量长度自适应调整内存布局、加载缓冲、精细循环控制以及基于门融合的算术强度适配。此外，还定义了新的度量标准和PMU事件来量化向量化活动，从而为未来VLA设计提供通用见解。

Result: 在对多达36个量子比特的五种量子电路进行评估后发现，所提出的单源VLA实现在Fujitsu A64FX处理器上达到了最高4.5倍的速度提升，在NVIDIA Grace处理器上达到2.5倍，在AWS Graviton3处理器上则为1.5倍。

Conclusion: 研究表明，通过对量子态矢量模拟这类工作负载采用VLA设计并应用相关优化技术，可以在不同处理器平台上显著提高性能。这不仅验证了VLA设计的有效性，也为未来类似场景下的高性能计算提供了宝贵的经验。

Abstract: ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.

</details>


### [4] [Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems](https://arxiv.org/abs/2602.09721)
*Guowei Liu,Hongming Li,Yaning Guo,Yongxi Lyu,Mo Zhou,Yi Liu,Zhaogeng Li,Yanpeng Wang*

Main category: cs.DC

TL;DR: 本文系统地分析了Attention-FFN Disaggregation (AFD)架构在大规模MoE模型部署中的性能边界，揭示了标准集群上的性能死区，并指出对于特定硬件和模型组合而言，AFD是一个有前景的方法。


<details>
  <summary>Details</summary>
Motivation: 大型MoE模型的部署面临内存容量和带宽方面的挑战，而Attention-FFN Disaggregation (AFD)作为一种潜在解决方案被提出以解耦计算与内存资源，但其相对于传统的专家并行(EP)方法的性能界限尚未得到充分探索。

Method: 通过扩展roofline模型至通信层面，关联互连带宽、算术强度及硬件FLOPS利用率（HFU）来进行系统性分析。

Result: 发现标准集群上存在一个‘死区’：增加FFN实例数量无法提高HFU，因为计算工作负载受到扩展带宽限制；此外，相较于EP连续批处理调整，AFD的离散节点级扩展会导致更高的不平衡惩罚。然而，在具备丰富互连带宽的超级Pod类硬件以及粗粒度专家较少稀疏性的模型中，AFD的优势更为明显。

Conclusion: AFD对于某些特定的硬件-模型组合来说是一个很有潜力的方法，但它并不是一种通用解决方案。

Abstract: Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Enhanced Graph Transformer with Serialized Graph Tokens](https://arxiv.org/abs/2602.09065)
*Ruixiang Wang,Yuyang Hong,Shiming Xiang,Chunhong Pan*

Main category: cs.LG

TL;DR: 本文提出了一种新的序列化令牌范式，以更有效地封装全局信号，从而改进了图级别表示的生成方法。实验结果表明该方法在多个图级别基准测试中达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的方法在生成图级别表示时遇到了信息瓶颈问题，主要因为普遍采用的单一令牌范式未能充分利用自注意力机制在编码令牌序列方面的内在优势，反而退化成节点信号的加权和。

Method: 设计了一种新的序列化令牌范式来更有效地封装全局信号。具体来说，提出了一种图序列化方法，将节点信号聚合为序列化的图令牌，并自动包含位置编码。然后，应用堆叠的自注意力层对这个令牌序列进行编码并捕捉其内部依赖关系。

Result: 通过建模多个图令牌之间的复杂交互，本方法能够产生更具表达力的图表示。实验结果显示，在几个图级别的基准测试中，该方法达到了最先进的结果。消融研究验证了所提出模块的有效性。

Conclusion: 提出的序列化令牌范式提供了一种有效利用自注意力机制处理图级任务的新方法，显著提高了图表示的学习能力。

Abstract: Transformers have demonstrated success in graph learning, particularly for node-level tasks. However, existing methods encounter an information bottleneck when generating graph-level representations. The prevalent single token paradigm fails to fully leverage the inherent strength of self-attention in encoding token sequences, and degenerates into a weighted sum of node signals. To address this issue, we design a novel serialized token paradigm to encapsulate global signals more effectively. Specifically, a graph serialization method is proposed to aggregate node signals into serialized graph tokens, with positional encoding being automatically involved. Then, stacked self-attention layers are applied to encode this token sequence and capture its internal dependencies. Our method can yield more expressive graph representations by modeling complex interactions among multiple graph tokens. Experimental results show that our method achieves state-of-the-art results on several graph-level benchmarks. Ablation studies verify the effectiveness of the proposed modules.

</details>


### [6] [Spectral Disentanglement and Enhancement: A Dual-domain Contrastive Framework for Representation Learning](https://arxiv.org/abs/2602.09066)
*Jinjin Guo,Yexin Li,Zhichao Huang,Jun Fang,Zhiyuan Liu,Chao Liu,Pengzhang Liu,Qixia Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架——频谱解缠与增强（SDE），通过自适应地将特征维度划分为强信号、弱信号和噪声，并采用基于课程的频谱增强策略来放大信息成分，同时引入双域对比损失以优化特征空间和频谱空间的一致性。实验表明，SDE能够持续提高表示的鲁棒性和泛化能力，在大规模多模态基准测试中超越了当前最先进方法的表现。


<details>
  <summary>Details</summary>
Motivation: 大规模多模态对比学习在学习丰富且可迁移的表征方面取得了显著的成功，但其本质上受限于对特征维度的统一处理以及忽略了所学特征内在的频谱结构。实证证据显示高维嵌入倾向于塌缩为窄锥，使得任务相关语义集中在一小部分子空间内，而大部分维度则被噪声和虚假关联占据。这种频谱失衡和纠缠削弱了模型的泛化能力。

Method: 提出了一个名为Spectral Disentanglement and Enhancement (SDE)的新框架，该框架利用奇异值分解自适应地将特征维度分割成捕捉任务关键语义的强信号、反映辅助关联的弱信号以及代表无关扰动的噪声。接着应用基于课程的频谱增强策略，有选择地放大信息成分，并保证训练稳定性。此外，还引入了一个双域对比损失函数，它能够在特征空间和频谱空间中联合优化一致性。

Result: 广泛的实验结果表明，SDE在大型多模态基准上一致地提高了表示的鲁棒性和泛化性能，优于现有最先进的方法。

Conclusion: SDE提供了一个有效的解决方案用于多模态表征学习，可以无缝集成到现有的对比学习流程中，通过频谱正则化促进更丰富、更稳健的表征生成。

Abstract: Large-scale multimodal contrastive learning has recently achieved impressive success in learning rich and transferable representations, yet it remains fundamentally limited by the uniform treatment of feature dimensions and the neglect of the intrinsic spectral structure of the learned features. Empirical evidence indicates that high-dimensional embeddings tend to collapse into narrow cones, concentrating task-relevant semantics in a small subspace, while the majority of dimensions remain occupied by noise and spurious correlations. Such spectral imbalance and entanglement undermine model generalization. We propose Spectral Disentanglement and Enhancement (SDE), a novel framework that bridges the gap between the geometry of the embedded spaces and their spectral properties. Our approach leverages singular value decomposition to adaptively partition feature dimensions into strong signals that capture task-critical semantics, weak signals that reflect ancillary correlations, and noise representing irrelevant perturbations. A curriculum-based spectral enhancement strategy is then applied, selectively amplifying informative components with theoretical guarantees on training stability. Building upon the enhanced features, we further introduce a dual-domain contrastive loss that jointly optimizes alignment in both the feature and spectral spaces, effectively integrating spectral regularization into the training process and encouraging richer, more robust representations. Extensive experiments on large-scale multimodal benchmarks demonstrate that SDE consistently improves representation robustness and generalization, outperforming state-of-the-art methods. SDE integrates seamlessly with existing contrastive pipelines, offering an effective solution for multimodal representation learning.

</details>


### [7] [Learning to Remember, Learn, and Forget in Attention-Based Models](https://arxiv.org/abs/2602.09075)
*Djohan Bonnet,Jamie Lohoff,Jan Finkbeiner,Elidona Skhikerujah,Emre Neftci*

Main category: cs.LG

TL;DR: 本文提出了一种名为Palimpsest的自注意力模型，通过将在线关联记忆视为持续学习问题来解决稳定性-可塑性困境。它使用贝叶斯元塑性机制，显著扩展了记忆容量，并在多个基准测试中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前的变换器中的上下文学习（ICL）作为在线联想记忆，支持其在复杂序列处理任务上的高性能。然而，在门控线性注意力模型中，这种记忆具有固定的容量并且容易受到干扰，特别是在长序列的情况下。为了解决这个问题，提出了Palimpsest模型。

Method: Palimpsest模型采用贝叶斯元塑性方法，其中每个注意力状态的可塑性与由先验分布支撑的重要性状态相关联，该先验分布捕获了累积的知识。此外，展示了各种门控线性注意力模型作为特定架构选择和后验近似出现，并且Mamba2是Palimpsest的一个特例，其中遗忘占主导地位。

Result: 实验表明，Palimpsest在多查询关联回忆（MQAR）基准测试以及常识推理任务上始终优于基线。同时，这种方法能够把任何非元塑性模型转换成元塑性模型，大大扩展了其记忆容量。

Conclusion: Palimpsest提供了一种有效的方法来增强基于变换器的模型的记忆能力，同时保持对新信息的学习灵活性。这不仅提高了它们在长序列任务上的表现，还为理解这些模型如何处理连续学习提供了新的视角。

Abstract: In-Context Learning (ICL) in transformers acts as an online associative memory and is believed to underpin their high performance on complex sequence processing tasks. However, in gated linear attention models, this memory has a fixed capacity and is prone to interference, especially for long sequences. We propose Palimpsa, a self-attention model that views ICL as a continual learning problem that must address a stability-plasticity dilemma. Palimpsa uses Bayesian metaplasticity, where the plasticity of each attention state is tied to an importance state grounded by a prior distribution that captures accumulated knowledge. We demonstrate that various gated linear attention models emerge as specific architecture choices and posterior approximations, and that Mamba2 is a special case of Palimpsa where forgetting dominates. This theoretical link enables the transformation of any non-metaplastic model into a metaplastic one, significantly expanding its memory capacity. Our experiments show that Palimpsa consistently outperforms baselines on the Multi-Query Associative Recall (MQAR) benchmark and on Commonsense Reasoning tasks.

</details>


### [8] [Patient foundation model for risk stratification in low-risk overweight patients](https://arxiv.org/abs/2602.09079)
*Zachary N. Flamholz,Dillon Tracy,Ripple Khera,Jordan Wolinsky,Nicholas Lee,Nathaniel Tann,Xiao Yin Zhu,Harry Phillips,Jeffrey Sherman*

Main category: cs.LG

TL;DR: PatientTPP, a neural temporal point process model, enhances risk stratification for overweight or obese patients by learning from large-scale clinical data, outperforming traditional measures like BMI in predicting future healthcare costs and obesity-related outcomes.


<details>
  <summary>Details</summary>
Motivation: 准确的风险分层对于超重或肥胖患者的预防护理指导和高成本治疗（如GLP-1受体激动剂）的分配至关重要。

Method: 研究者开发了PatientTPP，一种神经时间点过程(TPP)模型，该模型通过超过50万条真实世界的临床轨迹来学习患者表征，这些轨迹包括诊断、实验室检查和药物信息。此外，该方法还扩展了现有的TPP建模方式，以纳入静态和数值特征，并结合临床知识进行事件编码。

Result: PatientTPP不仅支持下游预测任务，比如在低风险个体中分类与肥胖相关的结局，即便这些事件在训练过程中没有被明确建模也能处理。在健康经济学评估方面，PatientTPP比身体质量指数(BMI)更能有效地对患者未来心血管相关医疗费用进行分层，识别出更高风险的患者。

Conclusion: 通过同时建模临床事件的类型和时间，PatientTPP为患者风险建模提供了一个可解释且通用的基础框架，在直接应用于与肥胖相关的护理及成本目标定位上表现出色。

Abstract: Accurate risk stratification in patients with overweight or obesity is critical for guiding preventive care and allocating high-cost therapies such as GLP-1 receptor agonists. We present PatientTPP, a neural temporal point process (TPP) model trained on over 500,000 real-world clinical trajectories to learn patient representations from sequences of diagnoses, labs, and medications. We extend existing TPP modeling approaches to include static and numeric features and incorporate clinical knowledge for event encoding. PatientTPP representations support downstream prediction tasks, including classification of obesity-associated outcomes in low-risk individuals, even for events not explicitly modeled during training. In health economic evaluation, PatientTPP outperformed body mass index in stratifying patients by future cardiovascular-related healthcare costs, identifying higher-risk patients more efficiently. By modeling both the type and timing of clinical events, PatientTPP offers an interpretable, general-purpose foundation for patient risk modeling with direct applications to obesity-related care and cost targeting.

</details>


### [9] [Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide](https://arxiv.org/abs/2602.09109)
*Hossam Amer,Rezaul Karim,Ali Pourranjbar,Weiwei Zhang,Walid Ahmed,Boxing Chen*

Main category: cs.LG

TL;DR: 本文全面回顾了集体操作和分布式并行策略，通过数学公式加深理论理解，并探讨了自动搜索最优混合并行化策略的最新进展。同时提供了主流架构类别的案例研究，揭示了实证见解以指导研究人员和平行策略选择者。最后，指出了当前大规模语言模型训练范式的开放挑战和局限性，并概述了下一代大规模模型开发的有前景方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速增长，已经开发出了一系列方法来跨硬件设备分配计算和内存，以便高效地进行训练和推理。然而，现有的调查仅提供了这些技术的描述性概览，缺乏对它们的优点和权衡的系统分析，以及如何利用这种洞察力为设计最佳分布式系统提供原则性方法。此外，对于混合并行化设计的讨论，特别是关于在模型部署不同阶段（包括训练和推理）中计算与通信重叠的关注较少。

Method: 本文采用综合性文献综述的方法，结合数学公式来深入理解集体操作和分布式并行策略。进一步探讨了混合并行化设计方案，强调在模型训练和推理过程中不同阶段的计算-通信重叠问题。还讨论了使用成本模型自动搜索最优混合并行化策略的最新进展。通过对主流架构类别中的案例研究，展示了实证结果。

Result: 文章揭示了现有大型语言模型训练范式所面临的开放挑战和局限性，并基于此提出了未来大规模模型开发的一些有前景的方向。

Conclusion: 该论文不仅提供了对集体操作和分布式并行策略的深度理解，而且通过实例研究给出了实用建议，同时也指出了当前领域内存在的挑战及未来研究方向。

Abstract: With the rapid growth of large language models (LLMs), a wide range of methods have been developed to distribute computation and memory across hardware devices for efficient training and inference. While existing surveys provide descriptive overviews of these techniques, systematic analysis of their benefits and trade offs and how such insights can inform principled methodology for designing optimal distributed systems remain limited. This paper offers a comprehensive review of collective operations and distributed parallel strategies, complemented by mathematical formulations to deepen theoretical understanding. We further examine hybrid parallelization designs, emphasizing communication computation overlap across different stages of model deployment, including both training and inference. Recent advances in automated search for optimal hybrid parallelization strategies using cost models are also discussed. Moreover, we present case studies with mainstream architecture categories to reveal empirical insights to guide researchers and practitioners in parallelism strategy selection. Finally, we highlight open challenges and limitations of current LLM training paradigms and outline promising directions for the next generation of large scale model development.

</details>


### [10] [Looping Back to Move Forward: Recursive Transformers for Efficient and Flexible Large Multimodal Models](https://arxiv.org/abs/2602.09080)
*Ruihan Xu,Yuting Gao,Lan Wang,Jianing Li,Weihao Chen,Qingpei Guo,Ming Yang,Shiliang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为RecursiveVLM的递归Transformer架构，专为大型多模态模型设计。通过引入递归连接器和单调递归损失，使得在不增加模型大小的情况下，能够通过循环利用模型参数来提取更强的多模态表示。实验结果表明，该方法相比标准Transformer提高了3%，比简单的递归基线提高了7%。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在视觉-语言任务上取得了显著成就，但它们庞大的参数量在训练和推理过程中往往没有得到充分利用。本研究旨在通过循环利用模型参数进行递归细化，以抽取更强的多模态表示，同时保持模型规模不变。

Method: 提出了RecursiveVLM，一种专门针对LMMs定制的递归Transformer架构。其核心创新包括：(i) 递归连接器，它通过融合中间层隐藏状态并应用特定于模态的投影来对齐跨递归步骤的功能；(ii) 单调递归损失，确保每一步都受到监督，并保证随着递归深度的增加性能单调提升。

Result: 实验结果显示，在不同条件下，与标准Transformer相比平均有+3%的增益，而相较于简单的递归基线则达到了+7%的改进。

Conclusion: 研究表明，通过策略性地使用循环机制，可以在不扩大模型尺寸的前提下有效提高多模态模型的表现力，从而实现高效且适应部署环境的LMMs。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in vision-language tasks, yet their vast parameter counts are often underutilized during both training and inference. In this work, we embrace the idea of looping back to move forward: reusing model parameters through recursive refinement to extract stronger multimodal representations without increasing model size. We propose RecursiveVLM, a recursive Transformer architecture tailored for LMMs. Two key innovations enable effective looping: (i) a Recursive Connector that aligns features across recursion steps by fusing intermediate-layer hidden states and applying modality-specific projections, respecting the distinct statistical structures of vision and language tokens; (ii) a Monotonic Recursion Loss that supervises every step and guarantees performance improves monotonically with recursion depth. This design transforms recursion into an on-demand refinement mechanism: delivering strong results with few loops on resource-constrained devices and progressively improving outputs when more computation resources are available. Experiments show consistent gains of +3% over standard Transformers and +7% over vanilla recursive baselines, demonstrating that strategic looping is a powerful path toward efficient, deployment-adaptive LMMs.

</details>


### [11] [Rashomon Sets and Model Multiplicity in Federated Learning](https://arxiv.org/abs/2602.09520)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato*

Main category: cs.LG

TL;DR: 本文首次在联邦学习（FL）中对罗生门集进行了形式化定义，区分了全局、t-一致性和个体罗生门集三种视角，并展示了如何在保护隐私的前提下估计标准多样性指标。此外，还提出了一种考虑多样性的FL流程，并通过实证研究证明了所提出的三种联邦罗生门集定义能够帮助客户端部署更符合本地数据特性和公平性要求的模型。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习（FL）场景下，现有的罗生门集定义及多样性度量假设不适用，而直接选择单一最优模型可能会导致预测行为同质化、偏见放大或损害公平性保证。因此需要一种新的方法来理解和处理FL中的模型多样性问题。

Method: 1. 为FL环境下的罗生门集提供正式定义，包括基于所有客户端聚合统计信息的全局罗生门集、代表一定比例客户端局部罗生门集交集的t-一致性罗生门集以及针对每个客户端本地分布的个体罗生门集。
2. 展示如何在遵守FL隐私约束的情况下估算标准多样性指标。
3. 提出一个考虑多样性的FL流程并进行实证研究。

Result: 研究表明，所提出的三种联邦罗生门集定义均能提供有价值的信息，使客户能够部署更加符合其本地数据特性、公平考量和实际需求的模型。

Conclusion: 通过引入适用于联邦学习的罗生门集概念及其相关度量方法，可以更好地理解跨多个参与者的模型多样性，从而促进更加透明、公平且鲁棒的模型开发。

Abstract: The Rashomon set captures the collection of models that achieve near-identical empirical performance yet may differ substantially in their decision boundaries. Understanding the differences among these models, i.e., their multiplicity, is recognized as a crucial step toward model transparency, fairness, and robustness, as it reveals decision boundaries instabilities that standard metrics obscure. However, the existing definitions of Rashomon set and multiplicity metrics assume centralized learning and do not extend naturally to decentralized, multi-party settings like Federated Learning (FL). In FL, multiple clients collaboratively train models under a central server's coordination without sharing raw data, which preserves privacy but introduces challenges from heterogeneous client data distribution and communication constraints. In this setting, the choice of a single best model may homogenize predictive behavior across diverse clients, amplify biases, or undermine fairness guarantees. In this work, we provide the first formalization of Rashomon sets in FL.First, we adapt the Rashomon set definition to FL, distinguishing among three perspectives: (I) a global Rashomon set defined over aggregated statistics across all clients, (II) a t-agreement Rashomon set representing the intersection of local Rashomon sets across a fraction t of clients, and (III) individual Rashomon sets specific to each client's local distribution.Second, we show how standard multiplicity metrics can be estimated under FL's privacy constraints. Finally, we introduce a multiplicity-aware FL pipeline and conduct an empirical study on standard FL benchmark datasets. Our results demonstrate that all three proposed federated Rashomon set definitions offer valuable insights, enabling clients to deploy models that better align with their local data, fairness considerations, and practical requirements.

</details>


### [12] [From Adam to Adam-Like Lagrangians: Second-Order Nonlocal Dynamics](https://arxiv.org/abs/2602.09101)
*Carlos Heredia*

Main category: cs.LG

TL;DR: 本文提出了Adam算法的一种加速连续时间形式，通过将其建模为二阶积分-微分动力系统，并引入了受Adam启发的非局部拉格朗日公式，提供了数值模拟以展示所提动力学与离散Adam的一致性。


<details>
  <summary>Details</summary>
Motivation: 为了改进Adam优化算法，研究者们希望通过构建一个加速的连续时间版本来提高其性能。

Method: 将Adam算法建模为一个二阶积分-微分动态系统，并通过α-细化极限关联到现有的第一个非局部Adam流；同时提供基于Lyapunov的稳定性及收敛性分析，并引入了一个受Adam启发的非局部拉格朗日公式。

Result: 结果表明，所提出的动力学模型与离散Adam在Rosenbrock类型示例上表现一致。

Conclusion: 该研究成功地为Adam算法开发了一种加速的连续时间表述，并通过理论分析和数值实验验证了其有效性。

Abstract: In this paper, we derive an accelerated continuous-time formulation of Adam by modeling it as a second-order integro-differential dynamical system. We relate this inertial nonlocal model to an existing first-order nonlocal Adam flow through an $α$-refinement limit, and we provide Lyapunov-based stability and convergence analyses. We also introduce an Adam-inspired nonlocal Lagrangian formulation, offering a variational viewpoint. Numerical simulations on Rosenbrock-type examples show agreement between the proposed dynamics and discrete Adam.

</details>


### [13] [Benchmarking the Energy Savings with Speculative Decoding Strategies](https://arxiv.org/abs/2602.09113)
*Rohit Dutta,Paramita Koley,Soham Poddar,Janardan Misra,Sanjay Podder,Naveen Balani,Saptarshi Ghosh,Niloy Ganguly*

Main category: cs.LG

TL;DR: 本论文对推测解码策略的能量需求进行了全面的调查，探讨了模型大小与家族、推测解码策略以及数据集特性等因素如何影响能量优化。


<details>
  <summary>Details</summary>
Motivation: 由于目前对于大型语言模型（LLM）推测解码方法在减少延迟和推理成本方面的研究中，缺乏对其能源需求的关注，本文旨在填补这一空白。

Method: 通过文献综述的方式，系统地分析了不同因素下推测解码策略的能量消耗情况。

Result: 发现模型规模、类型、采用的具体推测解码技术及处理的数据集特点都会显著影响到最终的能量效率表现。

Conclusion: 为提高LLM应用中的能效比提供了有价值的见解，并指出了未来研究方向。

Abstract: Speculative decoding has emerged as an effective method to reduce latency and inference cost of LLM inferences. However, there has been inadequate attention towards the energy requirements of these models. To address this gap, this paper presents a comprehensive survey of energy requirements of speculative decoding strategies, with detailed analysis on how various factors -- model size and family, speculative decoding strategies, and dataset characteristics -- influence the energy optimizations.

</details>


### [14] [SpinCastML an Open Decision-Making Application for Inverse Design of Electrospinning Manufacturing: A Machine Learning, Optimal Sampling and Inverse Monte Carlo Approach](https://arxiv.org/abs/2602.09120)
*Elisa Roldan,Tasneem Sabir*

Main category: cs.LG

TL;DR: SpinCastML是一个开源的、分布感知的、化学信息机器学习和逆蒙特卡洛软件，用于电纺丝的逆向设计。它能够预测纤维直径的整个分布，并支持回顾性分析和前瞻性逆向设计，生成物理和化学上可行的聚合物溶剂参数组合，从而减少实验浪费、加速发现并使先进的建模技术更加普及。


<details>
  <summary>Details</summary>
Motivation: 尽管电纺丝技术在生产特定应用架构的微纳米级纤维方面非常强大，但溶液或操作条件的微小变化就可能导致纤维直径分布偏离高斯分布。现有的框架无法实现针对期望纤维结果的逆向设计，也无法集成聚合物溶剂化学约束或预测完整分布。

Method: SpinCastML基于一个精心整理的数据集构建而成，该数据集包含来自16种聚合物的1,778个数据集中68,480根纤维的直径。它整合了三种结构化采样方法、一套11种高性能学习器以及化学意识约束来预测不仅平均直径还有整个分布。采用Cubist模型与聚合物平衡Sobol D最优采样相结合提供了最高的全局性能（R2 > 0.92）。

Result: 逆蒙特卡洛准确地捕捉到了纤维分布，实现了R2 > 0.90且预测与实验成功率之间的误差小于1%。IMC引擎支持回顾性分析和前瞻性逆向设计，能为用户定义的目标生成具有量化成功概率的物理和化学上可行的聚合物-溶剂参数组合。

Conclusion: SpinCastML将电纺丝从试错过程转变为可重复的、数据驱动的设计流程。作为一个开源可执行文件，它允许实验室分析自己的数据集，并共同创建一个不断扩大的社区软件。SpinCastML减少了实验浪费，加速了发现，并使高级建模变得更加普及，为生物医学、过滤和能源应用中的可持续纳米纤维制造设立了分布感知逆向设计的新标准。

Abstract: Electrospinning is a powerful technique for producing micro to nanoscale fibers with application specific architectures. Small variations in solution or operating conditions can shift the jet regime, generating non Gaussian fiber diameter distributions. Despite substantial progress, no existing framework enables inverse design toward desired fiber outcomes while integrating polymer solvent chemical constraints or predicting full distributions. SpinCastML is an open source, distribution aware, chemically informed machine learning and Inverse Monte Carlo (IMC) software for inverse electrospinning design. Built on a rigorously curated dataset of 68,480 fiber diameters from 1,778 datasets across 16 polymers, SpinCastML integrates three structured sampling methods, a suite of 11 high-performance learners, and chemistry aware constraints to predict not only mean diameter but the entire distribution. Cubist model with a polymer balanced Sobol D optimal sampling provides the highest global performance (R2 > 0.92). IMC accurately captures the fiber distributions, achieving R2 > 0.90 and <1% error between predicted and experimental success rates. The IMC engine supports both retrospective analysis and forward-looking inverse design, generating physically and chemically feasible polymer solvent parameter combinations with quantified success probabilities for user-defined targets. SpinCastML reframes electrospinning from trial and error to a reproducible, data driven design process. As an open source executable, it enables laboratories to analyze their own datasets and co create an expanding community software. SpinCastML reduces experimental waste, accelerates discovery, and democratizes access to advanced modeling, establishing distribution aware inverse design as a new standard for sustainable nanofiber manufacturing across biomedical, filtration, and energy applications.

</details>


### [15] [Epistemic Throughput: Fundamental Limits of Attention-Constrained Inference](https://arxiv.org/abs/2602.09127)
*Lei You*

Main category: cs.LG

TL;DR: 本文探讨了在注意力受限的情况下，如何通过低成本筛选和高成本验证过程最大化信息产出。提出了一种名为“JaKoB”的缩放定律，该定律表明即使在信息记录稀少的情况下，扩大低成本筛选也能非线性地放大稀缺的验证资源的效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统能够以低成本提供大量候选选项，但只有少量可以被仔细检查，这导致了下游决策者需要在有限注意力下从众多公开记录中形成可靠的后验概率。本文旨在解决这种注意力约束下的推理问题，提高决策效率。

Method: 本文通过建立注意力约束推理(ACI)模型来形式化这一问题，在此模型中，低成本筛选阶段处理K个记录，而高成本验证阶段最多只能跟进其中的B个。研究了在贝叶斯对数损失下，每个窗口内可实现的最大后验不确定性减少量（称为认知吞吐量）。

Result: 主要发现是提出了一个‘JaKoB’缩放定律，指出认知吞吐量有一个随验证能力和普遍性线性增长的基础项，以及一个与J、K、B平方根成比例的信息杠杆项，其中J代表筛选质量。此外，在弱筛选限制下该缩放关系紧密，并且当处于稀疏验证状态时（B远小于K），显著的杠杆效应需要重尾分布；对于轻尾分数，放大效果仅为对数级。

Conclusion: 扩大低成本筛选可以在即使信息记录稀少的情况下，非线性地增强稀缺验证资源的有效性。这意味着通过优化筛选流程，可以更有效地利用有限的验证能力来提高整体决策质量。

Abstract: Recent generative and tool-using AI systems can surface a large volume of candidates at low marginal cost, yet only a small fraction can be checked carefully. This creates a decoder-side bottleneck: downstream decision-makers must form reliable posteriors from many public records under scarce attention. We formalize this regime via Attention-Constrained Inference (ACI), in which a cheap screening stage processes $K$ records and an expensive verification stage can follow up on at most $B$ of them. Under Bayes log-loss, we study the maximum achievable reduction in posterior uncertainty per window, which we call \emph{epistemic throughput}. Our main result is a ``JaKoB'' scaling law showing that epistemic throughput has a baseline term that grows linearly with verification and prevalence, and an additional \emph{information-leverage} term that scales as $\sqrt{JKB}$, where $J$ summarizes screening quality. Thus, expanding cheap screening can nonlinearly amplify scarce verification, even when informative records are rare. We further show that this scaling is tight in a weak-screening limit, and that in the sparse-verification regime ($B \ll K$), substantial leverage requires heavy-tailed score distributions; for light-tailed scores the amplification is only logarithmic.

</details>


### [16] [Beyond the Unit Hypersphere: Embedding Magnitude in Contrastive Learning](https://arxiv.org/abs/2602.09229)
*Xincan Feng,Taro Watanabe*

Main category: cs.LG

TL;DR: 本文系统地研究了在对比学习中嵌入向量的大小所携带的信息，以及如何利用这些信息。研究发现，在文本检索任务中输出（文档）的大小与相关性高度相关；输入和输出大小在模型训练中扮演着不对称的角色；并且，大小学习对于非对称任务是有益的但对于对称任务则有害。


<details>
  <summary>Details</summary>
Motivation: 余弦相似度在对比学习中被广泛使用，但其隐含假设是嵌入向量的大小视为噪音。先前的工作偶尔发现点积与余弦相似度相当，但未回答大小携带什么信息、何时有帮助以及如何利用它。

Method: 通过一个$2 \times 2$的消融研究，独立控制文本和视觉模型的输入侧和输出侧标准化。

Result: 研究揭示了三个关键见解：1. 在文本检索中，输出（文档）大小与相关性之间存在强烈关联；2. 输入和输出大小在作用上是不对称的，输出大小直接影响相似度分数而输入大小调节训练动态；3. 大小学习有利于非对称任务（如文本检索），但损害对称任务（如STS，图文对齐）。

Conclusion: 这些发现建立了一个任务对称性原则：选择余弦还是点积取决于任务是否具有不同的输入角色，这允许通过简单移除不必要的约束来实现成本免费的改进。

Abstract: Cosine similarity is prevalent in contrastive learning, yet it makes an implicit assumption: embedding magnitude is noise. Prior work occasionally found dot product and cosine similarity comparable, but left unanswered WHAT information magnitude carries, WHEN it helps, and HOW to leverage it. We conduct a systematic study through a $2 \times 2$ ablation that independently controls input-side and output-side normalization across text and vision models. Our findings reveal three key insights. First, in text retrieval, output (document) magnitude strongly correlates with relevance (Cohen's $d$ up to 1.80), yielding the largest gains on reasoning-intensive tasks. Second, input and output magnitudes serve asymmetric roles: output magnitude directly scales similarity scores while input magnitude modulates training dynamics. Third, magnitude learning benefits asymmetric tasks (text retrieval, RAG) but harms symmetric tasks (STS, text-image alignment). These findings establish a task symmetry principle: the choice between cosine and dot product depends on whether the task has distinct input roles, enabling cost-free improvements by simply removing an unnecessary constraint.

</details>


### [17] [UniComp: A Unified Evaluation of Large Language Model Compression via Pruning, Quantization and Distillation](https://arxiv.org/abs/2602.09130)
*Jonathan von Rad,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: 本文提出了一种统一的评估框架UniComp，用于比较大型语言模型压缩技术如剪枝、量化和知识蒸馏的效果。通过在超过40个数据集上对六种压缩技术进行广泛评估，发现压缩技术对于知识密集型任务保持较好性能，但对于推理、多语言及指令跟随能力有显著退化；量化提供了最佳的整体性能与效率折衷方案，而蒸馏则以高计算成本为代价提供了强大的运行时加速增益；针对特定任务的校准可将剪枝模型的推理能力提高至多50%。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的压缩方法评价主要集中在知识为中心的基准测试上，缺乏全面的方法覆盖度。因此，需要一个更加综合的评估框架来对比不同的压缩技术，包括剪枝、量化和知识蒸馏等，并从性能、可靠性和效率三个维度进行全面考量。

Method: 引入了名为UniComp的统一评估框架，该框架旨在比较剪枝、量化以及知识蒸馏这三种压缩技术。它基于多种能力导向和安全导向的基准测试来评估被压缩模型的表现、可靠性和效率，并且还考虑到了硬件感知的效率分析。

Result: 研究显示，压缩技术在处理知识密集型任务时能够较好地保持原有性能，但会大幅削弱模型的推理、多语言支持及执行指令的能力；量化技术能够在保持性能的同时提供最好的效率优势，而知识蒸馏虽然可以显著加快运行速度，但伴随着较高的计算开销；此外，通过对特定任务进行校准，可以大幅度提升经过剪枝处理后的模型推理能力。

Conclusion: UniComp框架揭示了不同压缩技术应用于现代大型语言模型时存在的差异性影响，强调了量化技术作为平衡性能与效率的有效手段，同时也指出通过适当调整可能弥补某些压缩方法导致的能力下降问题。

Abstract: Model compression is increasingly essential for deploying large language models (LLMs), yet existing evaluations are limited in method coverage and focus primarily on knowledge-centric benchmarks. Thus, we introduce UniComp, a unified evaluation framework for comparing pruning, quantization, and knowledge distillation. UniComp evaluates compressed models along three dimensions: performance, reliability, and efficiency, using a diverse set of capability- and safety-oriented benchmarks together with a hardware-aware efficiency analysis. Through extensive evaluation of six compression techniques on modern LLMs across more than 40 datasets, we find that (i) compression exhibits a consistent knowledge bias, where knowledge-intensive tasks are relatively preserved while reasoning, multilingual, and instruction-following capabilities degrade substantially; (ii) quantization provides the best overall trade-off between retained performance and efficiency, whereas distillation yields strong runtime acceleration gains at high computational cost; and (iii) task-specific calibration can significantly improve the reasoning ability of pruned models by up to 50%.

</details>


### [18] [What do Geometric Hallucination Detection Metrics Actually Measure?](https://arxiv.org/abs/2602.09158)
*Eric Yeats,John Buckheit,Sarah Scullen,Brendan Kennedy,Loc Truong,Davis Brown,Bill Kay,Cliff Joslyn,Tegan Emerson,Michael J. Henry,John Emanuello,Henry Kvinge*

Main category: cs.LG

TL;DR: 研究了大语言模型内部状态的几何信号与不同类型的幻觉之间的关系，发现不同的几何统计量捕捉不同类型幻觉。此外，提出了一种简单的归一化方法来减少领域转换对几何统计量的影响，在多领域设置中AUROC提高了34个百分点。


<details>
  <summary>Details</summary>
Motivation: 幻觉是将生成模型部署到高后果应用中的一个障碍，尤其是在没有现成的外部真实信息来验证模型输出的情况下。因此，研究旨在探索语言模型内部状态中的几何信号是否能够预测幻觉，并且只需要有限的外部知识。

Method: 通过生成一个合成数据集来改变与幻觉相关的输出特性，包括正确性、置信度、相关性、连贯性和完整性。然后分析了几何统计量如何捕捉这些特定性质，并展示了现有几何检测方法对任务领域变化的敏感性。基于此，引入了一种简单的归一化方法以减轻领域转换对几何统计量的影响。

Result: 不同几何统计量确实可以捕捉不同类型幻觉的不同属性。另外，许多现有的几何检测方法对于任务领域的变动非常敏感（例如数学问题与历史问题）。所提出的归一化方法在跨领域场景下使AUROC提高了34个百分点。

Conclusion: 几何信号可用于识别生成模型中的幻觉类型，但其性能受任务领域影响较大。采用适当的归一化技术可显著提高这类方法在不同领域间的一致性和准确性。

Abstract: Hallucination remains a barrier to deploying generative models in high-consequence applications. This is especially true in cases where external ground truth is not readily available to validate model outputs. This situation has motivated the study of geometric signals in the internal state of an LLM that are predictive of hallucination and require limited external knowledge. Given that there are a range of factors that can lead model output to be called a hallucination (e.g., irrelevance vs incoherence), in this paper we ask what specific properties of a hallucination these geometric statistics actually capture. To assess this, we generate a synthetic dataset which varies distinct properties of output associated with hallucination. This includes output correctness, confidence, relevance, coherence, and completeness. We find that different geometric statistics capture different types of hallucinations. Along the way we show that many existing geometric detection methods have substantial sensitivity to shifts in task domain (e.g., math questions vs. history questions). Motivated by this, we introduce a simple normalization method to mitigate the effect of domain shift on geometric statistics, leading to AUROC gains of +34 points in multi-domain settings.

</details>


### [19] [Boltzmann Reinforcement Learning for Noise resilience in Analog Ising Machines](https://arxiv.org/abs/2602.09162)
*Aditya Choudhary,Saaketh Desai,Prasad Iyer*

Main category: cs.LG

TL;DR: 提出了BRAIN，一种利用变分强化学习来逼近玻尔兹曼分布的学习框架，用于解决模拟伊辛机在组合优化问题中的测量噪声问题。BRAIN能够抵抗高斯噪声，并且在实际3%的高斯测量噪声下保持98%的基础态保真度，同时比MCMC方法快192倍。


<details>
  <summary>Details</summary>
Motivation: 传统的优化和采样算法在模拟伊辛机平台上的性能受限于固有的测量噪声。为了解决这个问题并提高这些系统在组合优化问题中的效率和准确性，研究者们开发了BRAIN框架。

Method: BRAIN采用变分强化学习方法来近似玻尔兹曼分布。与逐个状态取样不同，它通过聚合多个噪声测量结果的信息来抵抗模拟伊辛机特有的高斯噪声。

Result: 实验表明，在存在3%高斯测量噪声的情况下，BRAIN能够维持高达98%的基础态保真度，而MCMC方法只能达到51%。此外，BRAIN的速度最高可达MCMC方法的192倍。对于多达65,536个自旋的情况，BRAIN展示出了$\mathcal{O}(N^{1.55})$的可扩展性，并且在高达40%的严重测量不确定性下依然保持鲁棒性。

Conclusion: BRAIN不仅提高了模拟伊辛机在基础态优化方面的性能，而且还能准确捕捉热力学相变和亚稳态，为使用模拟计算架构处理复杂优化问题提供了一种可扩展且抗噪的方法。

Abstract: Analog Ising machines (AIMs) have emerged as a promising paradigm for combinatorial optimization, utilizing physical dynamics to solve Ising problems with high energy efficiency. However, the performance of traditional optimization and sampling algorithms on these platforms is often limited by inherent measurement noise. We introduce BRAIN (Boltzmann Reinforcement for Analog Ising Networks), a distribution learning framework that utilizes variational reinforcement learning to approximate the Boltzmann distribution. By shifting from state-by-state sampling to aggregating information across multiple noisy measurements, BRAIN is resilient to Gaussian noise characteristic of AIMs. We evaluate BRAIN across diverse combinatorial topologies, including the Curie-Weiss and 2D nearest-neighbor Ising systems. We find that under realistic 3\% Gaussian measurement noise, BRAIN maintains 98\% ground state fidelity, whereas Markov Chain Monte Carlo (MCMC) methods degrade to 51\% fidelity. Furthermore, BRAIN reaches the MCMC-equivalent solution up to 192x faster under these conditions. BRAIN exhibits $\mathcal{O}(N^{1.55})$ scaling up to 65,536 spins and maintains robustness against severe measurement uncertainty up to 40\%. Beyond ground state optimization, BRAIN accurately captures thermodynamic phase transitions and metastable states, providing a scalable and noise-resilient method for utilizing analog computing architectures in complex optimizations.

</details>


### [20] [Faster Rates For Federated Variational Inequalities](https://arxiv.org/abs/2602.09164)
*Guanghui Wang,Satyen Kale*

Main category: cs.LG

TL;DR: 本文研究了联邦优化方法解决随机变分不等式问题，通过改进的Local Extra SGD算法和提出的新LIPPAX算法，实现了更优的收敛率，并在多种设定下减少了客户端漂移。


<details>
  <summary>Details</summary>
Motivation: 当前联邦优化方法在处理随机变分不等式（VIs）时，其收敛速度与最先进的联邦凸优化边界之间存在显著差距。为缩小这一差距并提高效率而进行了本研究。

Method: 首先对通用平滑单调变分不等式采用经典Local Extra SGD算法进行细化分析以获得更紧致的保证；接着针对Local Extra SGD存在的导致过多客户端漂移的问题，提出了新的Local Inexact Proximal Point Algorithm with Extra Step (LIPPAX)算法来缓解此问题，并在多个条件下实现改进的保障。

Result: 新提出的LIPPAX算法不仅能够有效减少客户端漂移现象，而且在诸如Hessian有界、算子有界以及低方差设置等多种情况下均能提供优于现有方法的收敛保证。此外，还将结果扩展到了联邦复合变分不等式上，进一步确立了改进后的收敛性保障。

Conclusion: 通过引入LIPPAX算法，本文为解决随机变分不等式问题提供了更有效的联邦学习方案，在不同应用场景中均表现出色，为未来相关领域的研究奠定了基础。

Abstract: In this paper, we study federated optimization for solving stochastic variational inequalities (VIs), a problem that has attracted growing attention in recent years. Despite substantial progress, a significant gap remains between existing convergence rates and the state-of-the-art bounds known for federated convex optimization. In this work, we address this limitation by establishing a series of improved convergence rates. First, we show that, for general smooth and monotone variational inequalities, the classical Local Extra SGD algorithm admits tighter guarantees under a refined analysis. Next, we identify an inherent limitation of Local Extra SGD, which can lead to excessive client drift. Motivated by this observation, we propose a new algorithm, the Local Inexact Proximal Point Algorithm with Extra Step (LIPPAX), and show that it mitigates client drift and achieves improved guarantees in several regimes, including bounded Hessian, bounded operator, and low-variance settings. Finally, we extend our results to federated composite variational inequalities and establish improved convergence guarantees.

</details>


### [21] [Train Less, Infer Faster: Efficient Model Finetuning and Compression via Structured Sparsity](https://arxiv.org/abs/2602.09169)
*Jonathan Svirsky,Yehonathan Refael,Ofir Lindenbaum*

Main category: cs.LG

TL;DR: 本文提出了一种通过稀疏化进行有效微调的方法，使用训练随机门控方案，在不需要调整权重的情况下，减少推理时间并移除20-40%的模型参数，同时保持准确性。实验结果显示该方法在效率和性能上优于最近的微调基线，并提供了关于随机门控过程收敛性的理论保证。


<details>
  <summary>Details</summary>
Motivation: 完全微调数十亿参数的基础语言模型通常由于高计算成本、内存需求以及过拟合的风险而不切实际。虽然像低秩适配器这样的方法通过向冻结的语言模型添加小的可训练模块来解决这些问题，但它们也增加了内存使用量且不减少推理延迟。

Method: 研究揭示了一个有趣的现象：通过稀疏化特定模型行与列可以在无需权重调整的情况下实现高效的任务适应。为此，作者提出了一种利用训练随机门控进行稀疏化以实现有效微调的方法，这种方法需要最少的可训练参数，能够减少推理时间，并且可以在不明显损失准确度的前提下移除20-40%的模型参数。

Result: 实证结果表明，所提出的方法在效率和性能方面超过了最近的微调基准。此外，还为这种随机门控过程的收敛性提供了理论保证，并展示了相比LoRA，本方法拥有更简单且条件更好的优化景观。

Conclusion: 研究结果强调了稀疏性作为语言模型中任务特定适应机制的一个吸引人的选择。

Abstract: Fully finetuning foundation language models (LMs) with billions of parameters is often impractical due to high computational costs, memory requirements, and the risk of overfitting. Although methods like low-rank adapters help address these challenges by adding small trainable modules to the frozen LM, they also increase memory usage and do not reduce inference latency. We uncover an intriguing phenomenon: sparsifying specific model rows and columns enables efficient task adaptation without requiring weight tuning. We propose a scheme for effective finetuning via sparsification using training stochastic gates, which requires minimal trainable parameters, reduces inference time, and removes 20--40\% of model parameters without significant accuracy loss. Empirical results show it outperforms recent finetuning baselines in efficiency and performance. Additionally, we provide theoretical guarantees for the convergence of this stochastic gating process, and show that our method admits a simpler and better-conditioned optimization landscape compared to LoRA. Our results highlight sparsity as a compelling mechanism for task-specific adaptation in LMs.

</details>


### [22] [Weighted Wasserstein Barycenter of Gaussian Processes for exotic Bayesian Optimization tasks](https://arxiv.org/abs/2602.09181)
*Antonio Candelieri,Francesco Archetti*

Main category: cs.LG

TL;DR: 本研究通过利用高斯分布与高斯过程后验之间的类比，提出了一种加权Wasserstein Barycenter of Gaussian Processes (W2BGP)方法，该方法可以统一处理多种特殊的贝叶斯优化任务，如协作/联邦贝叶斯优化、（同步）批量贝叶斯优化和多保真度贝叶斯优化。此外，研究还展示了如何在所提出的框架下重新解释常见的贝叶斯优化获取函数，并提供了一种比现有机器学习文献中的方法更有效的计算Wasserstein中心的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一种通用框架，能够统一并简化对于不同类型的特殊贝叶斯优化任务的处理，同时保持框架本身的不变性，仅需调整权重策略即可适应不同的任务需求。

Method: 通过引入加权Wasserstein Barycenter的概念到高斯过程中，研究人员创建了一个灵活的模型，它允许根据特定任务的需求简单地修改权重来执行不同类型的任务。此方法还支持对现有贝叶斯优化获取函数的新解释，从而促进了更高效的计算解决方案。

Result: 实证分析表明，只需为W2BGP选择合适的权重方案，就可以成功应用于协作/联邦贝叶斯优化、批量贝叶斯优化及多保真度贝叶斯优化等多种任务中。此外，研究还发现这种方法在计算效率上优于当前机器学习领域内的其他方法。

Conclusion: 本研究表明，通过使用加权Wasserstein Barycenter of Gaussian Processes可以在一个共同框架内解决多种特殊的贝叶斯优化问题，且能有效提升计算效率。未来的研究方向包括进一步探索该方法在更多场景下的应用潜力。

Abstract: Exploiting the analogy between Gaussian Distributions and Gaussian Processes' posterior, we present how the weighted Wasserstein Barycenter of Gaussian Processes (W2BGP) can be used to unify, under a common framework, different exotic Bayesian Optimization (BO) tasks. Specifically, collaborative/federated BO, (synchronous) batch BO, and multi-fidelity BO are considered in this paper. Our empirical analysis proves that each one of these tasks requires just an appropriate weighting schema for the W2BGP, while the entire framework remains untouched. Moreover, we demonstrate that the most well-known BO acquisition functions can be easily re-interpreted under the proposed framework and also enable a more computationally efficient way to deal with the computation of the Wasserstein Barycenter, compared with state-of-the-art methods from the Machine Learning literature. Finally, research perspectives branching from the proposed approach are presented.

</details>


### [23] [ML-DCN: Masked Low-Rank Deep Crossing Network Towards Scalable Ads Click-through Rate Prediction at Pinterest](https://arxiv.org/abs/2602.09194)
*Jiacheng Li,Yixiong Meng,Yi wu,Yun Zhao,Sharare Zehtabian,Jiayin Jin,Degao Peng,Jinfeng Zhuang,Qifei Shen,Kungang Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为ML-DCN的交互模块，该模块通过在低秩交叉层中集成实例条件掩码来选择和放大显著的交互方向，同时保持高效的计算。实验表明，与现有方法相比，ML-DCN在匹配FLOPs下实现了更高的AUC，并且随着计算量增加显示出更优的AUC-FLOPs权衡。在线A/B测试进一步验证了其对关键广告指标（如点击率和点击质量）的统计学显著改进，已在生产系统中部署，服务成本保持中立。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐系统依赖于特征交互模块来建模用户-项目之间的复杂关系。然而，在大规模广告排序中，尽管增加模型容量有助于提高预测性能和业务成果，但实际服务预算对延迟和FLOPs提出了严格限制。因此，需要找到一种既能有效扩展又能保持高效计算的交互模块设计。

Method: 研究团队提出了ML-DCN，这是一种结合了DCNv2和MaskNet优点的新架构。它通过引入实例条件掩码到低秩交叉层中，允许每个样本选择并加强重要的交互路径，同时保证了计算效率。

Result: 内部Pinterest广告数据集上的实验证明了ML-DCN相比其他方案在相同FLOPs条件下达到了更好的AUC值，并且随著计算资源的增长表现出更加有利的AUC-FLOPs平衡。此外，线上A/B测试显示了对于关键广告指标（例如CTR和点击质量度量）有统计意义上显著的提升。

Conclusion: ML-DCN作为一种新的特征交互模块，在保持高效计算的同时成功解决了现有方法随计算资源增加而收益递减的问题。这不仅提高了离线评估的表现，也通过在线实验验证了其商业价值。

Abstract: Deep learning recommendation systems rely on feature interaction modules to model complex user-item relationships across sparse categorical and dense features. In large-scale ad ranking, increasing model capacity is a promising path to improving both predictive performance and business outcomes, yet production serving budgets impose strict constraints on latency and FLOPs. This creates a central tension: we want interaction modules that both scale effectively with additional compute and remain compute-efficient at serving time. In this work, we study how to scale feature interaction modules under a fixed serving budget. We find that naively scaling DCNv2 and MaskNet, despite their widespread adoption in industry, yields rapidly diminishing offline gains in the Pinterest ads ranking system. To overcome aforementioned limitations, we propose ML-DCN, an interaction module that integrates an instance-conditioned mask into a low-rank crossing layer, enabling per-example selection and amplification of salient interaction directions while maintaining efficient computation. This novel architecture combines the strengths of DCNv2 and MaskNet, scales efficiently with increased compute, and achieves state-of-the-art performance. Experiments on a large internal Pinterest ads dataset show that ML-DCN achieves higher AUC than DCNv2, MaskNet, and recent scaling-oriented alternatives at matched FLOPs, and it scales more favorably overall as compute increases, exhibiting a stronger AUC-FLOPs trade-off. Finally, online A/B tests demonstrate statistically significant improvements in key ads metrics (including CTR and click-quality measures) and ML-DCN has been deployed in the production system with neutral serving cost.

</details>


### [24] [Fair Feature Importance Scores via Feature Occlusion and Permutation](https://arxiv.org/abs/2602.09196)
*Camille Little,Madeline Navarro,Santiago Segarra,Genevera Allen*

Main category: cs.LG

TL;DR: 本文提出两种模型无关的方法来衡量特征对公平性的影响，一种是通过置换特征值前后比较模型的公平性，另一种是评估有无特定特征时训练出的模型的公平性。这些方法简单、可扩展且易于解释，为负责任的机器学习开发提供了新的工具。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型对社会的影响日益增加，其不透明性给信任和责任带来了挑战，特别是在公平性方面。虽然已经建立了用于准确性的特征重要性度量标准，但评估特征对公平性贡献的方法仍较少被探索。

Method: 提出了两种模型无关的方法来测量公平特征的重要性：1. 通过在置换特征值前后比较模型的公平性；2. 评估有无给定特征情况下训练得到的模型之间的公平性差异。后一种基于遮挡的方法利用了迷你块学习简化计算过程。

Result: 实证结果表明，所提出的度量指标对于多个预测任务既简单又有效。两种方法都提供了简单、可扩展且易于理解的解决方案，以量化特征对公平性的影响。

Conclusion: 该研究为理解和提高机器学习模型中的公平性提供了一种新颖的方法论，有助于促进更负责任地开发机器学习应用。

Abstract: As machine learning models increasingly impact society, their opaque nature poses challenges to trust and accountability, particularly in fairness contexts. Understanding how individual features influence model outcomes is crucial for building interpretable and equitable models. While feature importance metrics for accuracy are well-established, methods for assessing feature contributions to fairness remain underexplored. We propose two model-agnostic approaches to measure fair feature importance. First, we propose to compare model fairness before and after permuting feature values. This simple intervention-based approach decouples a feature and model predictions to measure its contribution to training. Second, we evaluate the fairness of models trained with and without a given feature. This occlusion-based score enjoys dramatic computational simplification via minipatch learning. Our empirical results reflect the simplicity and effectiveness of our proposed metrics for multiple predictive tasks. Both methods offer simple, scalable, and interpretable solutions to quantify the influence of features on fairness, providing new tools for responsible machine learning development.

</details>


### [25] [CausalGDP: Causality-Guided Diffusion Policies for Reinforcement Learning](https://arxiv.org/abs/2602.09207)
*Xiaofeng Xiao,Xiao Hu,Yang Ye,Xubo Yue*

Main category: cs.LG

TL;DR: 本文提出了Causality-guided Diffusion Policy (CausalGDP)，一种将因果推理整合进基于扩散的强化学习中的统一框架，通过考虑动作、状态和奖励之间的因果关系而非仅仅是统计关联来优化策略。实验结果表明，在复杂高维度控制任务中，CausalGDP相比现有方法表现出色或更优。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的策略主要依赖于统计关联性，并未明确考虑到状态、动作与回报间的因果关系，这限制了识别真正导致高收益的动作组件的能力。因此，需要一种能够利用因果关系改进决策过程的方法。

Method: 开发了一种名为Causality-guided Diffusion Policy (CausalGDP)的新框架，该框架首先从离线数据中学习基础扩散策略及初始因果动态模型，捕捉状态、动作和奖励间的因果依赖；在实时互动过程中，持续更新并融合因果信息作为指导信号，引导扩散过程朝向那些对将来状态和奖励有因果影响的动作发展。

Result: 实验结果显示，CausalGDP在复杂的高维控制任务上相对于最新的基于扩散和其他离线RL方法而言，能够一致地实现竞争性的或更优越的表现。

Conclusion: 通过将因果推理引入到基于扩散的强化学习中，CausalGDP能够专注于那些真正驱动性能提升的动作组成部分进行策略优化，从而在复杂环境中提供更有效的解决方案。

Abstract: Reinforcement learning (RL) has achieved remarkable success in a wide range of sequential decision-making problems. Recent diffusion-based policies further improve RL by modeling complex, high-dimensional action distributions. However, existing diffusion policies primarily rely on statistical associations and fail to explicitly account for causal relationships among states, actions, and rewards, limiting their ability to identify which action components truly cause high returns. In this paper, we propose Causality-guided Diffusion Policy (CausalGDP), a unified framework that integrates causal reasoning into diffusion-based RL. CausalGDP first learns a base diffusion policy and an initial causal dynamical model from offline data, capturing causal dependencies among states, actions, and rewards. During real-time interaction, the causal information is continuously updated and incorporated as a guidance signal to steer the diffusion process toward actions that causally influence future states and rewards. By explicitly considering causality beyond association, CausalGDP focuses policy optimization on action components that genuinely drive performance improvements. Experimental results demonstrate that CausalGDP consistently achieves competitive or superior performance over state-of-the-art diffusion-based and offline RL methods, especially in complex, high-dimensional control tasks.

</details>


### [26] [A Lightweight Multi-View Approach to Short-Term Load Forecasting](https://arxiv.org/abs/2602.09220)
*Julien Guité-Vinet,Alexandre Blondin Massé,Éric Beaudry*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级多视角方法用于短期负荷预测，该方法利用单值嵌入和缩放时间范围输入来高效捕捉时间相关特征，并引入了嵌入dropout机制以防止过度依赖特定特征并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然基于transformer和大参数模型的方法在时间序列预测上取得了最先进的结果，但它们的复杂性可能导致过拟合和不稳定的预测，特别是在旧数据点变得不那么相关的情况下。因此，需要一种更轻量、更能有效处理时间相关特征且不易过拟合的方法来进行短期负荷预测。

Method: 本研究提出的方法包括使用单值嵌入以及缩放的时间范围输入来捕捉与时间相关的特征，并通过引入一个嵌入dropout机制来避免对特定特征的过度依赖，从而增强模型的可解释性和鲁棒性。

Result: 实验表明，该方法在多个数据集上表现出了竞争力强的结果，包括在噪声或稀疏数据场景下的稳健性能，同时使用的参数显著减少。此外，它还能够提供关于单个特征对预测贡献的见解。

Conclusion: 提出的方法证明了即使是在参数数量大大减少的情况下，也能实现高精度的短期负荷预测，并且具有良好的泛化能力和对于不同质量数据的适应性。

Abstract: Time series forecasting is a critical task across domains such as energy, finance, and meteorology, where accurate predictions enable informed decision-making. While transformer-based and large-parameter models have recently achieved state-of-the-art results, their complexity can lead to overfitting and unstable forecasts, especially when older data points become less relevant. In this paper, we propose a lightweight multi-view approach to short-term load forecasting that leverages single-value embeddings and a scaled time-range input to capture temporally relevant features efficiently. We introduce an embedding dropout mechanism to prevent over-reliance on specific features and enhance interpretability. Our method achieves competitive performance with significantly fewer parameters, demonstrating robustness across multiple datasets, including scenarios with noisy or sparse data, and provides insights into the contributions of individual features to the forecast.

</details>


### [27] [RAPID: Risk of Attribute Prediction-Induced Disclosure in Synthetic Microdata](https://arxiv.org/abs/2602.09235)
*Matthias Templ,Oscar Thees,Roman Müller*

Main category: cs.LG

TL;DR: 本文提出了一种新的披露风险度量方法RAPID，用于量化在实际攻击模型下合成数据中敏感属性的推理脆弱性。它适用于连续和分类敏感属性，并提供了一个实用且基于攻击者现实情况的风险上限，以补充现有的效用诊断和披露控制框架。


<details>
  <summary>Details</summary>
Motivation: 随着统计数据分析越来越依赖于完全合成的微观数据，传统的身份披露度量对于评估泄露风险变得不够充分。需要一种新的方法来直接衡量对手从发布的合成数据中推断出敏感信息的能力。

Method: 开发了名为RAPID（Risk of Attribute Prediction–Induced Disclosure）的风险度量工具，它允许对手仅使用发布的合成数据训练预测模型并将其应用于真实个体的准标识符上。对于连续敏感属性，RAPID报告预测值落在特定相对误差容限内的记录比例；对于分类属性，则提出了一个基线归一化的置信分数，用来衡量对手对真实类别的确定程度超出类别普遍性期望的程度。

Result: 通过模拟和真实数据展示了阈值校准、不确定性量化以及合成数据生成器的比较评估。结果表明，RAPID能够为属性推理披露风险提供一个实用且基于现实攻击者的上限，同时与现有效用诊断和披露控制框架相辅相成。

Conclusion: RAPID作为一种新颖的风险度量工具，为评估合成数据集中的敏感属性推理提供了可解释性强、有界且鲁棒的方法，有助于提高数据匿名化处理的安全性和有效性。

Abstract: Statistical data anonymization increasingly relies on fully synthetic microdata, for which classical identity disclosure measures are less informative than an adversary's ability to infer sensitive attributes from released data. We introduce RAPID (Risk of Attribute Prediction--Induced Disclosure), a disclosure risk measure that directly quantifies inferential vulnerability under a realistic attack model. An adversary trains a predictive model solely on the released synthetic data and applies it to real individuals' quasi-identifiers. For continuous sensitive attributes, RAPID reports the proportion of records whose predicted values fall within a specified relative error tolerance. For categorical attributes, we propose a baseline-normalized confidence score that measures how much more confident the attacker is about the true class than would be expected from class prevalence alone, and we summarize risk as the fraction of records exceeding a policy-defined threshold. This construction yields an interpretable, bounded risk metric that is robust to class imbalance, independent of any specific synthesizer, and applicable with arbitrary learning algorithms. We illustrate threshold calibration, uncertainty quantification, and comparative evaluation of synthetic data generators using simulations and real data. Our results show that RAPID provides a practical, attacker-realistic upper bound on attribute-inference disclosure risk that complements existing utility diagnostics and disclosure control frameworks.

</details>


### [28] [Feature salience -- not task-informativeness -- drives machine learning model explanations](https://arxiv.org/abs/2602.09238)
*Benedict Clark,Marta Oliveira,Rick Wilming,Stefan Haufe*

Main category: cs.LG

TL;DR: 该研究通过在二元图像分类任务中引入不同类型的水印，探讨了可解释AI（XAI）方法标记特征重要性的依据。结果表明，XAI方法更倾向于根据测试时图像结构的显著性而非模型学习到的统计关联来分配重要性。这提示我们需要重新评估过往依赖于特征显著性和信息性偶然一致性的XAI应用，并仔细审查基于特征归因方法的工作流程。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于验证XAI工具在标注输入特征的重要性时是否主要基于这些特征对于目标变量的信息价值，还是受到其他因素如统计抑制、测试时的新颖性或高特征显著性的影响。

Method: 研究人员训练了深度学习模型处理三种不同设置下的二元图像分类问题，包括无水印、作为类别相关干扰的水印以及类别无关噪声形式存在的水印。随后，使用五种流行的属性方法分析了这些模型的行为。

Result: 实验结果显示，在所有条件下，被标记为重要的区域都显著地集中在水印上（RIW），而水印与类别的相关性对RIW影响甚微。此外，XAI方法的表现类似于模型无关的边缘检测滤波器，并且当图像中的亮区由较小而非较大的特征值编码时，它们赋予水印的重要性大大降低。

Conclusion: 结论指出，XAI方法确定特征重要性更多地依赖于测试时刻图像结构的显著性，而不是机器学习模型所学到的统计关联。因此，需要对过去展示XAI成功应用的研究进行重新评估，并且对于那些将特征归因方法作为构建块的工作流程也应更加审慎。

Abstract: Explainable AI (XAI) promises to provide insight into machine learning models' decision processes, where one goal is to identify failures such as shortcut learning. This promise relies on the field's assumption that input features marked as important by an XAI must contain information about the target variable. However, it is unclear whether informativeness is indeed the main driver of importance attribution in practice, or if other data properties such as statistical suppression, novelty at test-time, or high feature salience substantially contribute. To clarify this, we trained deep learning models on three variants of a binary image classification task, in which translucent watermarks are either absent, act as class-dependent confounds, or represent class-independent noise. Results for five popular attribution methods show substantially elevated relative importance in watermarked areas (RIW) for all models regardless of the training setting ($R^2 \geq .45$). By contrast, whether the presence of watermarks is class-dependent or not only has a marginal effect on RIW ($R^2 \leq .03$), despite a clear impact impact on model performance and generalisation ability. XAI methods show similar behaviour to model-agnostic edge detection filters and attribute substantially less importance to watermarks when bright image intensities are encoded by smaller instead of larger feature values. These results indicate that importance attribution is most strongly driven by the salience of image structures at test time rather than statistical associations learned by machine learning models. Previous studies demonstrating successful XAI application should be reevaluated with respect to a possibly spurious concurrency of feature salience and informativeness, and workflows using feature attribution methods as building blocks should be scrutinised.

</details>


### [29] [The effect of whitening on explanation performance](https://arxiv.org/abs/2602.09278)
*Benedict Clark,Stoyan Karastoyanov,Rick Wilming,Stefan Haufe*

Main category: cs.LG

TL;DR: 本研究探讨了数据白化处理是否能够减少特征归因方法中非信息变量（如抑制变量）的错误重要性分配问题。通过XAI-TRIS基准测试和一个最小线性二维分类问题，对16种流行的特征归因方法与5种不同的白化变换组合进行了评估。结果表明，虽然特定的白化技术可以提高解释性能，但这种改善程度在不同XAI方法和模型架构之间差异很大。


<details>
  <summary>Details</summary>
Motivation: 可解释的人工智能(XAI)旨在提供对机器学习模型的透明洞察，但许多特征归因方法的可靠性仍然是一个关键挑战。先前的研究表明，这些方法经常错误地将显著的重要性赋予非信息变量，比如抑制变量，导致基本误解。鉴于统计抑制是由特征依赖性引起的，本研究调查了一种常见的用于去相关的预处理技术——数据白化能否减轻此类错误。

Method: 研究使用了XAI-TRIS基准测试，该测试提供了合成的真实数据和定量的解释正确性度量，对结合了5种不同白化变换的16种流行特征归因方法进行了实证评估。此外，还分析了一个最小线性二维分类问题，以理论评估白化是否可以从贝叶斯最优模型中去除抑制特征的影响。

Result: 结果显示，尽管某些特定的白化技术能够提升解释表现，但在不同XAI方法及模型架构间，其改进程度存在较大差异。

Conclusion: 研究发现强调了数据非线性、预处理质量和属性保真度之间的复杂关系，并突出了预处理技术在增强模型可解释性方面所起的关键作用。

Abstract: Explainable Artificial Intelligence (XAI) aims to provide transparent insights into machine learning models, yet the reliability of many feature attribution methods remains a critical challenge. Prior research (Haufe et al., 2014; Wilming et al., 2022, 2023) has demonstrated that these methods often erroneously assign significant importance to non-informative variables, such as suppressor variables, leading to fundamental misinterpretations. Since statistical suppression is induced by feature dependencies, this study investigates whether data whitening, a common preprocessing technique for decorrelation, can mitigate such errors. Using the established XAI-TRIS benchmark (Clark et al., 2024b), which offers synthetic ground-truth data and quantitative measures of explanation correctness, we empirically evaluate 16 popular feature attribution methods applied in combination with 5 distinct whitening transforms. Additionally, we analyze a minimal linear two-dimensional classification problem (Wilming et al., 2023) to theoretically assess whether whitening can remove the impact of suppressor features from Bayes-optimal models. Our results indicate that, while specific whitening techniques can improve explanation performance, the degree of improvement varies substantially across XAI methods and model architectures. These findings highlight the complex relationship between data non-linearities, preprocessing quality, and attribution fidelity, underscoring the vital role of pre-processing techniques in enhancing model interpretability.

</details>


### [30] [Positive-Unlabelled Active Learning to Curate a Dataset for Orca Resident Interpretation](https://arxiv.org/abs/2602.09295)
*Bret Nestor,Bohan Yao,Jasmine Moore,Jasper Kanes*

Main category: cs.LG

TL;DR: 本研究整理了迄今为止最大规模的南方定居虎鲸及其栖息环境中其他海洋哺乳动物的声音数据集，使用弱监督、正未标记、主动学习策略开发出的基于transformer的探测器在准确性、能效和速度上超越现有技术。该数据集适用于无监督机器翻译、栖息地使用调查及保护工作。


<details>
  <summary>Details</summary>
Motivation: 为了更好地了解和保护濒危物种南方定居虎鲸及其生态环境，研究人员对超过30年的公开水下声音记录进行了系统性搜索与分析。

Method: 采用了一种弱监督、正未标记、主动学习的方法来识别所有海洋哺乳动物的声音实例，并开发了基于transformer的探测器。

Result: 生成的数据集中包含919小时的南方定居虎鲸数据以及其他多种海洋哺乳动物的声音资料；新开发的探测器在多个数据集上的表现优于现有最佳模型，在DCLDE-2026数据集上实现了特定物种分类准确率达到42.1%（训练11类，测试4类）和生态型分类准确率为43.0%（训练4类，测试5类）。

Conclusion: 这项研究为南方定居虎鲸等海洋哺乳动物提供了大量有价值的声音数据资源，有助于未来的研究和保护工作。

Abstract: This work presents the largest curation of Southern Resident Killer Whale (SRKW) acoustic data to date, also containing other marine mammals in their environment. We systematically search all available public archival hydrophone data within the SRKW habitat (over 30 years of audio data). The search consists of a weakly-supervised, positive-unlabelled, active learning strategy to identify all instances of marine mammals. The resulting transformer-based detectors outperform state-of-the-art detectors on the DEEPAL, DCLDE-2026, and two newly introduced expert-annotated datasets in terms of accuracy, energy efficiency, and speed. The detection model has a specificity of 0-28.8% at 95% sensitivity. Our multiclass species classifier obtains a top-1 accuracy of 42.1% (11 train classes, 4 test classes) and our ecotype classifier obtains a top-1 accuracy of 43.0% (4 train classes, 5 test classes) on the DCLDE-2026 dataset.
  We yield 919 hours of SRKW data, 230 hours of Bigg's orca data, 1374 hours of orca data from unlabelled ecotypes, 1501 hours of humpback data, 88 hours of sea lion data, 246 hours of pacific white-sided dolphin data, and over 784 hours of unspecified marine mammal data. This SRKW dataset is larger than DCLDE-2026, Ocean Networks Canada, and OrcaSound combined. The curated species labels are available under CC-BY 4.0 license, and the corresponding audio data are available under the licenses of the original owners. The comprehensive nature of this dataset makes it suitable for unsupervised machine translation, habitat usage surveys, and conservation endeavours for this critically endangered ecotype.

</details>


### [31] [The Laplacian Mechanism Improves Transformers by Reshaping Token Geometry](https://arxiv.org/abs/2602.09297)
*Yuchong Zhang,Vardan Papyan*

Main category: cs.LG

TL;DR: 本文提出了一种将注意力机制修改为拉普拉斯机制的方法，使得模型能够更直接地控制标记的方差。通过在视觉和语言基准测试中验证了该方法的有效性，并使用多种工具分析了拉普拉斯机制如何影响标记表示的几何形状，结果显示该机制促进了同类标记的聚合以及类均值的神经崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在通过改进现有的Transformer架构来更好地控制token表示的方差，从而达到理想的token几何布局。

Method: 提出将传统的注意力机制转变为一种拉普拉斯机制，以增强模型对token方差的直接控制能力；利用主成分分析、余弦相似度量、方差分析及神经崩溃指标等工具探讨拉普拉斯机制对token表示几何结构的影响。

Result: 实验表明，在计算机视觉和自然语言处理任务上引入拉普拉斯机制可以带来一致性的性能提升；此外，还观察到这种机制促使token嵌入向最大可分离性方向变化，即同类别的token趋向于聚集在一起，并且类别平均值表现出神经崩溃的现象。

Conclusion: 拉普拉斯机制不仅有助于提高Transformer模型在不同任务上的表现，而且还能优化token表示的空间分布，使之更加有利于分类任务。

Abstract: Transformers leverage attention, the residual connection, and layer normalization to control the variance of token representations. We propose to modify attention into a Laplacian mechanism that gives the model more direct control over token variance. We conjecture that this helps transformers achieve the ideal token geometry. To investigate our conjecture, we first show that incorporating the Laplacian mechanism into transformers induces consistent improvements across benchmarks in computer vision and language. Next, we study how the Laplacian mechanism impacts the geometry of token representations using various tools: 1) principal component analysis, 2) cosine similarity metric, 3) analysis of variance, and 4) Neural Collapse metrics. Our investigation shows that the Laplacian mechanism reshapes token embeddings toward a geometry of maximal separability: tokens collapse according to their classes, and the class means exhibit Neural Collapse.

</details>


### [32] [Risk-sensitive reinforcement learning using expectiles, shortfall risk and optimized certainty equivalent risk](https://arxiv.org/abs/2602.09300)
*Sumedh Gupte,Shrey Rakeshkumar Patel,Soumen Pachal,Prashanth L. A.,Sanjay P. Bhat*

Main category: cs.LG

TL;DR: 本文提出了针对三种风险度量（即期望值、基于效用的不足风险和优化确定性等价风险）的风险敏感强化学习算法，并推导了策略梯度定理，提出了风险敏感策略梯度估计器，建立了估计器的均方误差界限，并在标准假设下证明了风险敏感目标的平滑性，从而为提出的整体算法提供了收敛速度边界。此外，通过数值实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 为了处理强化学习中的风险敏感问题，作者提出了针对不同风险度量的新算法，这些算法能够更准确地评估决策过程中的风险，并且提供了一种有效的方法来优化面对风险时的决策过程。

Method: 对于每一种风险度量，在有限时间马尔可夫决策过程中，首先推导出策略梯度定理；其次，提出相应风险度量下的风险敏感策略梯度估计器，并建立估计器的均方误差界限；最后，在策略梯度类型算法的标准假设下，证明了风险敏感目标函数的平滑性，并给出了所提算法的稳定收敛率边界。

Result: 研究得出了风险敏感策略梯度估计器的均方误差随轨迹数量增加而减小的结论，同时证明了在适当条件下，所提出的算法可以平稳收敛。此外，通过流行的RL基准测试进行了数值实验，以验证理论发现。

Conclusion: 该研究表明，通过引入特定的风险度量到强化学习中，可以有效地对决策过程中的风险进行建模，并通过提出的风险敏感强化学习算法实现了良好的性能。

Abstract: We propose risk-sensitive reinforcement learning algorithms catering to three families of risk measures, namely expectiles, utility-based shortfall risk and optimized certainty equivalent risk. For each risk measure, in the context of a finite horizon Markov decision process, we first derive a policy gradient theorem. Second, we propose estimators of the risk-sensitive policy gradient for each of the aforementioned risk measures, and establish $\mathcal{O}\left(1/m\right)$ mean-squared error bounds for our estimators, where $m$ is the number of trajectories. Further, under standard assumptions for policy gradient-type algorithms, we establish smoothness of the risk-sensitive objective, in turn leading to stationary convergence rate bounds for the overall risk-sensitive policy gradient algorithm that we propose. Finally, we conduct numerical experiments to validate the theoretical findings on popular RL benchmarks.

</details>


### [33] [Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation](https://arxiv.org/abs/2602.09305)
*Pei-Chi Pan,Yingbin Liang,Sen Lin*

Main category: cs.LG

TL;DR: 本文提出了一种名为推理对齐强化学习（RARL）的新框架，系统化了多步推理的多种奖励范式，并分析了奖励设计与核心语言模型挑战之间的关系，旨在构建更加稳健、可验证和可信的推理模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）展示了变革性的潜力，但其推理仍然不稳定且不可靠。基于强化学习（RL）的微调是改进的关键机制，但其有效性从根本上受奖励设计的影响。然而，奖励建模与核心LLM挑战（如评估偏差、幻觉、分布偏移及高效学习）之间的关系尚未得到充分理解。

Method: 本文提出了推理对齐强化学习（RARL）框架，该框架统一了多样化的奖励范式用于多步推理。在这一框架内，作者提供了一个奖励机制分类法，分析了作为普遍失败模式的奖励黑客行为，并探讨了奖励信号如何解决从推理时间扩展到减少幻觉等一系列挑战。此外，还批判性地评估了现有基准测试，指出了数据污染和奖励错位等脆弱性，并为更稳健的评估指明了方向。

Result: 通过整合分散的研究线索并阐明奖励设计与基本推理能力之间的相互作用，这项工作为构建稳健、可验证和值得信赖的推理模型提供了基础路线图。

Conclusion: 本文强调了奖励建模在调整推理一致性中的核心作用，而不仅仅是实现细节。RARL框架提供了一种系统化的方法来处理与奖励设计相关的各种挑战，为未来研究如何通过精心设计的奖励机制提高LLM的可靠性铺平了道路。

Abstract: Large Language Models (LLMs) demonstrate transformative potential, yet their reasoning remains inconsistent and unreliable. Reinforcement learning (RL)-based fine-tuning is a key mechanism for improvement, but its effectiveness is fundamentally governed by reward design. Despite its importance, the relationship between reward modeling and core LLM challenges--such as evaluation bias, hallucination, distribution shift, and efficient learning--remains poorly understood. This work argues that reward modeling is not merely an implementation detail but a central architect of reasoning alignment, shaping what models learn, how they generalize, and whether their outputs can be trusted. We introduce Reasoning-Aligned Reinforcement Learning (RARL), a unifying framework that systematizes diverse reward paradigms for multi-step reasoning. Within this framework, we present a taxonomy of reward mechanisms, analyze reward hacking as a pervasive failure mode, and examine how reward signals unify challenges ranging from inference-time scaling to hallucination mitigation. We further critically evaluate existing benchmarks, highlighting vulnerabilities such as data contamination and reward misalignment, and outline directions for more robust evaluation. By integrating fragmented research threads and clarifying the interplay between reward design and fundamental reasoning capabilities, this work provides a foundational roadmap for building reasoning models that are robust, verifiable, and trustworthy.

</details>


### [34] [Priority-Aware Shapley Value](https://arxiv.org/abs/2602.09326)
*Kiljae Lee,Ziqi Liu,Weijing Tang,Yuan Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的方法，即优先级感知的Shapley值（PASV），用于处理数据估值和特征归因中贡献者之间存在依赖关系或需根据信任、风险等因素调整贡献的情况。该方法同时考虑了硬性优先约束和软性的贡献者特定优先权重，并通过实验展示了其在结构忠实度上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的Shapley值方法假设贡献者是可互换的，但在实际场景中，比如数据重用/增强或因果特征排序时，贡献者可能不是独立的，或者其贡献需要基于如信任度或风险等额外因素进行调整。

Method: 本文介绍了一种名为优先级感知Shapley值(Priority-Aware Shapley Value, PASV)的新方法，该方法能够结合硬性优先级约束与软性的、针对每个贡献者的优先权权重。此外，还开发了一个高效的邻近交换Metropolis-Hastings采样器来实现可扩展的蒙特卡洛估计，并研究了极端优先权权重诱导下的限制机制。

Result: 在MNIST/CIFAR10的数据估值以及Census Income的特征归因实验中，PASV展示了更符合结构特性的分配结果，并通过提出的'优先级扫描'方法提供了一种实用的敏感性分析手段。

Conclusion: PASV为处理具有依赖性或需要基于额外因素调整贡献值的情况提供了一种有效的方法，它不仅适用于一般优先级结构，还能恢复仅基于优先级或仅基于权重的Shapley变体作为特殊情况，并且通过自然公理得到了唯一表征。

Abstract: Shapley values are widely used for model-agnostic data valuation and feature attribution, yet they implicitly assume contributors are interchangeable. This can be problematic when contributors are dependent (e.g., reused/augmented data or causal feature orderings) or when contributions should be adjusted by factors such as trust or risk. We propose Priority-Aware Shapley Value (PASV), which incorporates both hard precedence constraints and soft, contributor-specific priority weights. PASV is applicable to general precedence structures, recovers precedence-only and weight-only Shapley variants as special cases, and is uniquely characterized by natural axioms. We develop an efficient adjacent-swap Metropolis-Hastings sampler for scalable Monte Carlo estimation and analyze limiting regimes induced by extreme priority weights. Experiments on data valuation (MNIST/CIFAR10) and feature attribution (Census Income) demonstrate more structure-faithful allocations and a practical sensitivity analysis via our proposed "priority sweeping".

</details>


### [35] [MacrOData: New Benchmarks of Thousands of Datasets for Tabular Outlier Detection](https://arxiv.org/abs/2602.09329)
*Xueying Ding,Simon Klüttermann,Haomin Wen,Yilong Chen,Leman Akoglu*

Main category: cs.LG

TL;DR: 本文介绍了一个名为MacrOData的大规模基准测试套件，用于表格数据的异常检测。它包含三个部分：OddBench、OvrBench和SynBench，共2,446个数据集，旨在提供全面且统计上稳健的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测（OD）基准测试规模较小，限制了多样性和统计能力。为了公平准确地追踪科学进展，并让从业者能够做出明智的方法选择，需要一个大规模且多样化的OD基准测试套件。

Method: 创建了MacrOData，一个包含790个真实世界语义异常数据集、856个具有真实世界统计离群值的数据集以及800个合成生成数据集的大规模OD基准测试套件。所有数据集均配有标准化训练/测试分割、公开/私有基准划分等，并通过多种OD方法进行广泛实验。

Result: MacrOData由于其规模和多样性，能够对表格OD方法进行全面且统计上稳健的评估。实验结果提供了详细的实证发现、实用指南及对未来研究有用的参考性能。

Conclusion: MacrOData为表格数据异常检测提供了一个新的大型基准测试标准，有助于提高该领域研究的深度与广度。

Abstract: Quality benchmarks are essential for fairly and accurately tracking scientific progress and enabling practitioners to make informed methodological choices. Outlier detection (OD) on tabular data underpins numerous real-world applications, yet existing OD benchmarks remain limited. The prominent OD benchmark AdBench is the de facto standard in the literature, yet comprises only 57 datasets. In addition to other shortcomings discussed in this work, its small scale severely restricts diversity and statistical power. We introduce MacrOData, a large-scale benchmark suite for tabular OD comprising three carefully curated components: OddBench, with 790 datasets containing real-world semantic anomalies; OvrBench, with 856 datasets featuring real-world statistical outliers; and SynBench, with 800 synthetically generated datasets spanning diverse data priors and outlier archetypes. Owing to its scale and diversity, MacrOData enables comprehensive and statistically robust evaluation of tabular OD methods. Our benchmarks further satisfy several key desiderata: We provide standardized train/test splits for all datasets, public/private benchmark partitions with held-out test labels for the latter reserved toward an online leaderboard, and annotate our datasets with semantic metadata. We conduct extensive experiments across all benchmarks, evaluating a broad range of OD methods comprising classical, deep, and foundation models, over diverse hyperparameter configurations. We report detailed empirical findings, practical guidelines, as well as individual performances as references for future research. All benchmarks containing 2,446 datasets combined are open-sourced, along with a publicly accessible leaderboard hosted at https://huggingface.co/MacrOData-CMU.

</details>


### [36] [Sparse Layer Sharpness-Aware Minimization for Efficient Fine-Tuning](https://arxiv.org/abs/2602.09395)
*Yifei Cheng,Xianglin Yang,Guoxia Wang,Chao Huang,Fei Ma,Dianhai Yu,Xiaochun Cao,Li Shen*

Main category: cs.LG

TL;DR: 提出了一种名为SL-SAM的方法，通过引入稀疏技术选择模型的部分层进行参数扰动和更新步骤，从而在保持与现有方法相当性能的同时显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为了解决Sharpness-aware Minimization (SAM)方法因额外的参数扰动步骤导致计算成本翻倍的问题，提出了SL-SAM方法以减少实际应用中的计算负担。

Method: SL-SAM将动态选择参与梯度上升（扰动）和下降（更新）步骤的层次视作一个多臂老虎机问题处理，并基于梯度范数在每次迭代开始时选取部分层次参与反向传播过程中的参数扰动及更新，以此降低计算复杂性。

Result: 实验表明，在多个任务中对模型进行微调时，SL-SAM不仅达到了与最先进基线相比拟的表现（包括在大型语言模型微调上获得了第一名），而且相较于传统的SAM方法大大减少了反向传播过程中激活参数的比例（视觉模型、中型及大型语言模型分别只激活了47%、22%和21%的参数）。

Conclusion: SL-SAM提供了一个有效的解决方案来克服SAM方法存在的计算效率瓶颈，同时保证了良好的泛化能力和性能表现。

Abstract: Sharpness-aware minimization (SAM) seeks the minima with a flat loss landscape to improve the generalization performance in machine learning tasks, including fine-tuning. However, its extra parameter perturbation step doubles the computation cost, which becomes the bottleneck of SAM in the practical implementation. In this work, we propose an approach SL-SAM to break this bottleneck by introducing the sparse technique to layers. Our key innovation is to frame the dynamic selection of layers for both the gradient ascent (perturbation) and descent (update) steps as a multi-armed bandit problem. At the beginning of each iteration, SL-SAM samples a part of the layers of the model according to the gradient norm to participate in the backpropagation of the following parameter perturbation and update steps, thereby reducing the computation complexity. We then provide the analysis to guarantee the convergence of SL-SAM. In the experiments of fine-tuning models in several tasks, SL-SAM achieves the performances comparable to the state-of-the-art baselines, including a \#1 rank on LLM fine-tuning. Meanwhile, SL-SAM significantly reduces the ratio of active parameters in backpropagation compared to vanilla SAM (SL-SAM activates 47\%, 22\% and 21\% parameters on the vision, moderate and large language model respectively while vanilla SAM always activates 100\%), verifying the efficiency of our proposed algorithm.

</details>


### [37] [Learning with Multiple Correct Answers -- A Trichotomy of Regret Bounds under Different Feedback Models](https://arxiv.org/abs/2602.09402)
*Alireza F. Pour,Farnam Mansouri,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文研究了一个在线学习问题，其中每个实例都有多个正确答案。文章探讨了三种反馈模型下的最优错误界限和遗憾界，并得出了批量设置下的样本复杂度边界。


<details>
  <summary>Details</summary>
Motivation: 受到语言生成任务的启发，在这些任务中一个提示可能有多种可接受的答案但并非所有答案都是合适的。因此，研究者们探索了具有多个有效标签的学习问题。

Method: 研究者定义了三个不同的反馈模型来分析这个问题，并在可实现的情况下使用适当的组合维度来描述最优错误界限。此外，还在不可知情况下建立了跨三种模型的遗憾界三元组。

Result: 结果表明，每种反馈模型下都可以确定最优错误界限，并且在不可知情况下可以建立相应的遗憾界。此外，还推导出依赖于相应组合维度的批量设置样本复杂度界限。

Conclusion: 通过研究不同反馈模型下的多答案在线学习问题，该论文提供了关于如何衡量此类问题难度及学习效率的新见解。

Abstract: We study an online learning problem with multiple correct answers, where each instance admits a set of valid labels, and in each round the learner must output a valid label for the queried example. This setting is motivated by language generation tasks, in which a prompt may admit many acceptable completions, but not every completion is acceptable. We study this problem under three feedback models. For each model, we characterize the optimal mistake bound in the realizable setting using an appropriate combinatorial dimension. We then establish a trichotomy of regret bounds across the three models in the agnostic setting. Our results also imply sample complexity bounds for the batch setup that depend on the respective combinatorial dimensions.

</details>


### [38] [Diffusion-Guided Pretraining for Brain Graph Foundation Models](https://arxiv.org/abs/2602.09437)
*Xinxu Wei,Rong Zhou,Lifang He,Yu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的预训练框架，旨在解决现有的对比和掩码自动编码方法在处理脑图时遇到的问题。该框架通过保持大脑图谱的语义意义同时维持有效的预训练多样性，并且能够进行拓扑感知的图级别读出和节点级别的全局重建来改进对连通性数据的学习表示。


<details>
  <summary>Details</summary>
Motivation: 当前针对脑信号的基础模型中，基于图的预训练已经成为一种有前景的学习可迁移表示的方法。但是，现有的对比学习和掩码自动编码器方法通常依赖于简单的随机删除或遮蔽来进行增强，这对脑图和超图来说并不合适，因为这会破坏具有语义意义的连接模式。此外，常用的图级读出和重构方案未能捕捉到全局结构信息，限制了所学表示的鲁棒性。

Method: 作者提出了一种统一的基于扩散的预训练框架，该框架设计了扩散过程来指导结构感知的删除和遮蔽策略，以保持脑图的语义同时保证有效的预训练多样性。另外，扩散过程还允许图嵌入和被遮蔽节点从全球相关的区域聚集信息，从而实现拓扑感知的图级别读出及节点级别的全局重构。

Result: 通过对多个神经影像数据集的广泛实验，涉及超过25,000名受试者和60,000次扫描，包括各种精神障碍和大脑图谱，证明了该方法能够持续提升表现。

Conclusion: 提出的基于扩散的预训练框架有效地解决了现有技术在处理脑图时面临的挑战，通过维护语义意义并增强了表示学习的鲁棒性，在多个神经成像应用上取得了性能上的改进。

Abstract: With the growing interest in foundation models for brain signals, graph-based pretraining has emerged as a promising paradigm for learning transferable representations from connectome data. However, existing contrastive and masked autoencoder methods typically rely on naive random dropping or masking for augmentation, which is ill-suited for brain graphs and hypergraphs as it disrupts semantically meaningful connectivity patterns. Moreover, commonly used graph-level readout and reconstruction schemes fail to capture global structural information, limiting the robustness of learned representations. In this work, we propose a unified diffusion-based pretraining framework that addresses both limitations. First, diffusion is designed to guide structure-aware dropping and masking strategies, preserving brain graph semantics while maintaining effective pretraining diversity. Second, diffusion enables topology-aware graph-level readout and node-level global reconstruction by allowing graph embeddings and masked nodes to aggregate information from globally related regions. Extensive experiments across multiple neuroimaging datasets with over 25,000 subjects and 60,000 scans involving various mental disorders and brain atlases demonstrate consistent performance improvements.

</details>


### [39] [Taming the Monster Every Context: Complexity Measure and Unified Framework for Offline-Oracle Efficient Contextual Bandits](https://arxiv.org/abs/2602.09456)
*Hao Qin,Chicheng Zhang*

Main category: cs.LG

TL;DR: 提出了一个名为OE2D的算法框架，该框架将带有通用奖励函数近似的上下文强盗学习问题简化为离线回归问题。此框架允许对于具有大动作空间的上下文强盗在T轮中通过O(log(T))次调用离线回归oracle来实现接近最优的遗憾，并且当T已知时仅需O(loglog(T))次调用。


<details>
  <summary>Details</summary>
Motivation: 为了解决具有大动作空间的上下文强盗学习问题，同时保证低遗憾和良好的探索与利用之间的权衡。

Method: 设计了OE2D算法框架，该框架通过选择一种称为“exploitative F-design”的动作分布来同时确保低遗憾和良好覆盖。引入了一个新的复杂度量——决策-离线估计系数（DOEC），并证明其在每上下文有界Eluder维度和平滑遗憾设置下是有界的。还建立了DOEC与决策估计系数（DEC）之间的关系。

Result: OE2D能够在T轮中通过O(log(T))次调用离线回归oracle达到接近最优的遗憾，并且如果T已知则只需O(loglog(T))次调用。此外，新提出的DOEC度量有助于理解算法的有效性，并首次连接了离线和在线oracle高效上下文强盗算法的设计原则。

Conclusion: OE2D框架提供了一种有效的方法来解决带有一般奖励函数近似的大动作空间上下文强盗问题，通过减少到离线回归问题的方式，并通过新的复杂度量DOEC促进了对算法性能的理解。

Abstract: We propose an algorithmic framework, Offline Estimation to Decisions (OE2D), that reduces contextual bandit learning with general reward function approximation to offline regression. The framework allows near-optimal regret for contextual bandits with large action spaces with $O(log(T))$ calls to an offline regression oracle over $T$ rounds, and makes $O(loglog(T))$ calls when $T$ is known. The design of OE2D algorithm generalizes Falcon~\citep{simchi2022bypassing} and its linear reward version~\citep[][Section 4]{xu2020upper} in that it chooses an action distribution that we term ``exploitative F-design'' that simultaneously guarantees low regret and good coverage that trades off exploration and exploitation. Central to our regret analysis is a new complexity measure, the Decision-Offline Estimation Coefficient (DOEC), which we show is bounded in bounded Eluder dimension per-context and smoothed regret settings. We also establish a relationship between DOEC and Decision Estimation Coefficient (DEC)~\citep{foster2021statistical}, bridging the design principles of offline- and online-oracle efficient contextual bandit algorithms for the first time.

</details>


### [40] [Online Learning in MDPs with Partially Adversarial Transitions and Losses](https://arxiv.org/abs/2602.09474)
*Ofir Schlisselberg,Tal Lancewicki,Yishay Mansour*

Main category: cs.LG

TL;DR: 本研究探讨了在大部分步骤中转移函数为随机但在每集的固定子集Λ步可能表现出对抗行为的MDP中的强化学习问题。引入了条件占用度量，并设计了两种算法来处理这种情况，一种适用于任意对抗性步骤，另一种假设对抗性步骤是连续的。此外，还提供了无需知道哪些步骤是对抗性的K^(2/3)遗憾减少方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决那些在大多数情况下稳定但存在少数易受攻击点的环境下的强化学习问题。

Method: 通过引入条件占用度量，并基于此设计了两种新算法。第一种算法针对任意对抗步骤，而第二种则假设对抗步骤连续出现以优化对状态数S的依赖程度。

Result: 对于任意对抗步骤的情况，实现了遗憾上界\(\tilde{O}(H S^Λ\sqrt{K S A^{Λ+1}})\)；若对抗步骤连续，则进一步改善了对\(S\)的依赖至\(\tilde{O}(H\sqrt{K S^{3} A^{Λ+1}})\)。此外，提出了一种不需要事先了解哪些步骤属于\(Λ\)个对抗步骤的方法，其遗憾减少率为\(K^{2/3}\)。

Conclusion: 这项工作不仅提出了新的算法来处理部分对抗性MDP问题，而且还为完全对抗性设置下的MDP特征化了遗憾界限，包括全信息反馈和bandit反馈情况下的上下界分析。

Abstract: We study reinforcement learning in MDPs whose transition function is stochastic at most steps but may behave adversarially at a fixed subset of $Λ$ steps per episode. This model captures environments that are stable except at a few vulnerable points. We introduce \emph{conditioned occupancy measures}, which remain stable across episodes even with adversarial transitions, and use them to design two algorithms. The first handles arbitrary adversarial steps and achieves regret $\tilde{O}(H S^Λ\sqrt{K S A^{Λ+1}})$, where $K$ is the number of episodes, $S$ is the number of state, $A$ is the number of actions and $H$ is the episode's horizon. The second, assuming the adversarial steps are consecutive, improves the dependence on $S$ to $\tilde{O}(H\sqrt{K S^{3} A^{Λ+1}})$. We further give a $K^{2/3}$-regret reduction that removes the need to know which steps are the $Λ$ adversarial steps. We also characterize the regret of adversarial MDPs in the \emph{fully adversarial} setting ($Λ=H-1$) both for full-information and bandit feedback, and provide almost matching upper and lower bounds (slightly strengthen existing lower bounds, and clarify how different feedback structures affect the hardness of learning).

</details>


### [41] [Computationally Efficient Replicable Learning of Parities](https://arxiv.org/abs/2602.09499)
*Moshe Noivirt,Jessica Sorrell,Eliad Tsfadia*

Main category: cs.LG

TL;DR: 研究了可复制性学习与差分隐私学习及统计查询模型之间的计算关系，并首次提出了一个针对任意分布上奇偶性的高效可复制算法，表明了在一般分布上高效的可复制学习比SQ学习更强大，且其能力接近于差分私密学习。


<details>
  <summary>Details</summary>
Motivation: 探索可复制性学习与其他稳定性概念（如差分隐私和统计查询模型）之间的计算关联，特别是为了填补在有效（即多项式时间）可复制学习算法方面相对于差分隐私学习的空白。

Method: 开发了一种新的、高效的、可复制的算法，该算法能够从给定的一组向量中输出一个子空间来覆盖大多数向量，从而实现对任意分布上的奇偶性进行实际学习。

Result: 提出了一种对于任意分布上奇偶性的首个高效可复制算法，这一任务在统计查询模型中是困难的，但在差分隐私下可以完成。这为证明高效可复制学习在通用分布上超越了SQ学习提供了初步证据。

Conclusion: 结果表明，高效的可复制学习在处理一般分布问题时，其能力范围超过了SQ学习，而更接近于差分隐私学习的能力边界。

Abstract: We study the computational relationship between replicability (Impagliazzo et al. [STOC `22], Ghazi et al. [NeurIPS `21]) and other stability notions. Specifically, we focus on replicable PAC learning and its connections to differential privacy (Dwork et al. [TCC 2006]) and to the statistical query (SQ) model (Kearns [JACM `98]). Statistically, it was known that differentially private learning and replicable learning are equivalent and strictly more powerful than SQ-learning. Yet, computationally, all previously known efficient (i.e., polynomial-time) replicable learning algorithms were confined to SQ-learnable tasks or restricted distributions, in contrast to differentially private learning.
  Our main contribution is the first computationally efficient replicable algorithm for realizable learning of parities over arbitrary distributions, a task that is known to be hard in the SQ-model, but possible under differential privacy. This result provides the first evidence that efficient replicable learning over general distributions strictly extends efficient SQ-learning, and is closer in power to efficient differentially private learning, despite computational separations between replicability and privacy. Our main building block is a new, efficient, and replicable algorithm that, given a set of vectors, outputs a subspace of their linear span that covers most of them.

</details>


### [42] [Mitigating the Likelihood Paradox in Flow-based OOD Detection via Entropy Manipulation](https://arxiv.org/abs/2602.09581)
*Donghwan Kim,Hyunsoo Yoon*

Main category: cs.LG

TL;DR: 本文提出了一种通过基于语义相似性控制输入熵来缓解深度生成模型对分布外数据分配过高似然度问题的方法，并在标准基准测试中显示出一致的AUROC改进。


<details>
  <summary>Details</summary>
Motivation: 为了解决能够有效地计算输入似然性的深度生成模型（如规范化流）往往会给分布外(OOD)输入分配异常高似然性的问题。

Method: 作者提出了一个基于语义相似性调整输入熵的方法，对于与内存库中分布内样本不太相似的输入施加更强的扰动，并且该方法不需要额外训练密度模型。

Result: 理论分析表明，熵控制可以增加分布内和OOD样本之间的预期对数似然差距，有利于分布内样本；实验结果在标准基准上显示了相对于基线的一致AUROC改进。

Conclusion: 所提出的方法能够在不进行额外训练的情况下有效提高对分布外检测器性能。

Abstract: Deep generative models that can tractably compute input likelihoods, including normalizing flows, often assign unexpectedly high likelihoods to out-of-distribution (OOD) inputs. We mitigate this likelihood paradox by manipulating input entropy based on semantic similarity, applying stronger perturbations to inputs that are less similar to an in-distribution memory bank. We provide a theoretical analysis showing that entropy control increases the expected log-likelihood gap between in-distribution and OOD samples in favor of the in-distribution, and we explain why the procedure works without any additional training of the density model. We then evaluate our method against likelihood-based OOD detectors on standard benchmarks and find consistent AUROC improvements over baselines, supporting our explanation.

</details>


### [43] [Why the Counterintuitive Phenomenon of Likelihood Rarely Appears in Tabular Anomaly Detection with Deep Generative Models?](https://arxiv.org/abs/2602.09593)
*Donghwan Kim,Junghun Phee,Hyunsoo Yoon*

Main category: cs.LG

TL;DR: 研究发现，与图像领域不同，在表格数据中使用归一化流进行基于似然性的异常检测时，模型较少给异常数据分配更高的似然性。实验表明，仅基于似然性的检测方法在表格领域是实用且可靠的。


<details>
  <summary>Details</summary>
Motivation: 作者旨在探讨深度生成模型（如归一化流）在表格数据上进行基于似然性的异常检测的有效性，特别是考虑到在图像领域内这些模型有时会为异常数据分配更高似然值的现象是否同样存在于表格数据中。

Method: 首先提出了一种跨领域的公式来一致地检测和评估这种反直觉现象；然后通过ADBench中的47个表格数据集及10个CV/NLP嵌入数据集对13个基线模型进行了广泛的测试；最后从理论和实证两个角度深入分析了数据维度以及特征相关性差异的作用。

Result: 结果显示，在一般表格数据中，定义下的反常现象（即给异常数据更高似然值）相对罕见。此外，研究表明数据的维度和特征间的关联性对这一现象有重要影响。

Conclusion: 研究结论指出，对于表格数据而言，使用归一化流仅基于似然性的异常检测方法是一种实用且可靠的选择。

Abstract: Deep generative models with tractable and analytically computable likelihoods, exemplified by normalizing flows, offer an effective basis for anomaly detection through likelihood-based scoring. We demonstrate that, unlike in the image domain where deep generative models frequently assign higher likelihoods to anomalous data, such counterintuitive behavior occurs far less often in tabular settings. We first introduce a domain-agnostic formulation that enables consistent detection and evaluation of the counterintuitive phenomenon, addressing the absence of precise definition. Through extensive experiments on 47 tabular datasets and 10 CV/NLP embedding datasets in ADBench, benchmarked against 13 baseline models, we demonstrate that the phenomenon, as defined, is consistently rare in general tabular data. We further investigate this phenomenon from both theoretical and empirical perspectives, focusing on the roles of data dimensionality and difference in feature correlation. Our results suggest that likelihood-only detection with normalizing flows offers a practical and reliable approach for anomaly detection in tabular domains.

</details>


### [44] [LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection](https://arxiv.org/abs/2602.09634)
*Naveen Gill,Ajvad Haneef K,Madhu Kumar S D*

Main category: cs.LG

TL;DR: 本研究探讨了大型语言模型（LLMs）在零样本设置下，仅使用特征名称和任务描述来指导特征选择的可能性。通过在EMBOD数据集上评估多个LLM与传统特征选择方法相比的性能，结果显示LLM引导的零样本特征选择不仅在准确性等指标上具有竞争力，还提供了更好的可解释性、稳定性和对标签数据依赖性的减少。


<details>
  <summary>Details</summary>
Motivation: 传统的特征选择方法主要依赖于统计启发式或基于模型的重要性评分，往往忽略了特征的语义背景。受到最近关于LLM驱动特征选择进展的启发，研究者们探索了大型语言模型是否可以在只有特征名和任务描述的情况下，在零样本设置中有效指导特征选择。

Method: 研究选择了包括GPT-5.0, GPT-4.0, Gemini-2.5在内的多个大型语言模型，并将它们应用于由EMBER和BODMAS基准数据集融合而成的EMBOD数据集上。这些模型被用来与已有的特征选择方法进行对比，评价指标涵盖了准确率、精确度、召回率、F1分数、AUC值、MCC以及运行时间等多个方面。

Result: 结果表明，基于LLM的零样本特征选择方法不仅在各种分类器上的表现可以与传统方法相媲美，而且在可解释性、稳定性以及降低对标注数据依赖度方面展现出额外的优势。

Conclusion: 零样本LLM驱动的特征选择作为一种有效的替代策略，在实现高效且可解释的恶意软件检测方面显示出了巨大潜力，为安全关键应用中的知识导向型特征选择铺平了道路。

Abstract: Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features. Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches. We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchmark datasets), comparing them against established FS methods across several classifiers, including Random Forest, Extra Trees, MLP, and KNN. Performance is assessed using accuracy, precision, recall, F1, AUC, MCC, and runtime. Our results demonstrate that LLM-guided zero-shot feature selection achieves competitive performance with traditional FS methods while offering additional advantages in interpretability, stability, and reduced dependence on labeled data. These findings position zero-shot LLM-based FS as a promising alternative strategy for effective and interpretable malware detection, paving the way for knowledge-guided feature selection in security-critical applications

</details>


### [45] [Blind denoising diffusion models and the blessings of dimensionality](https://arxiv.org/abs/2602.09639)
*Zahra Kadkhodaie,Aram-Alexandre Pooladian,Sinho Chewi,Eero Simoncelli*

Main category: cs.LG

TL;DR: 该论文分析了基于盲去噪器的生成扩散模型（BDDMs）在理论和实践上的表现，证明了即使不提供噪声幅度信息，BDDMs也能自动追踪特定隐含噪声计划，并在多项式步数内准确采样。实验结果表明，无调度的BDDMs相比非盲模型能产生更高质量的样本，这归功于它们纠正了实际残余噪声与非盲扩散模型中假设噪声之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索当去噪器在训练或采样过程中都不被给予噪声幅度时，生成扩散模型的表现如何。特别是，在数据分布具有低内在维度的前提下，了解这样的模型是否能够有效学习并生成符合目标的数据分布。

Method: 通过理论分析和实证研究相结合的方法，作者们首先从理论上证明了盲去噪扩散模型能够在没有明确噪声水平指示的情况下自动调整至一个有效的反向过程路径；然后利用合成数据集及图像数据验证了这一理论发现的有效性。

Result: 结果显示，盲去噪扩散模型不仅能在多项式时间内完成对数据分布的有效采样，而且在实际应用中（例如图像生成任务上），其生成样本的质量优于传统使用固定噪声计划的非盲模型。

Conclusion: 结论指出，盲去噪扩散模型提供了一种新颖且有效的生成建模方法，特别是在处理具有低内在维度的数据时表现出色。此外，它还揭示了正确估计真实残差噪声对于提高生成质量的重要性。

Abstract: We analyze, theoretically and empirically, the performance of generative diffusion models based on \emph{blind denoisers}, in which the denoiser is not given the noise amplitude in either the training or sampling processes. Assuming that the data distribution has low intrinsic dimensionality, we prove that blind denoising diffusion models (BDDMs), despite not having access to the noise amplitude, \emph{automatically} track a particular \emph{implicit} noise schedule along the reverse process. Our analysis shows that BDDMs can accurately sample from the data distribution in polynomially many steps as a function of the intrinsic dimension. Empirical results corroborate these mathematical findings on both synthetic and image data, demonstrating that the noise variance is accurately estimated from the noisy image. Remarkably, we observe that schedule-free BDDMs produce samples of higher quality compared to their non-blind counterparts. We provide evidence that this performance gain arises because BDDMs correct the mismatch between the true residual noise (of the image) and the noise assumed by the schedule used in non-blind diffusion models.

</details>


### [46] [Resilient Class-Incremental Learning: on the Interplay of Drifting, Unlabelled and Imbalanced Data Streams](https://arxiv.org/abs/2602.09681)
*Jin Li,Kleanthis Malialis,Marios Polycarpou*

Main category: cs.LG

TL;DR: 本文提出了一种名为SCIL（流式类增量学习）的框架，旨在解决在概念漂移、类别不平衡、标签稀缺和新类别出现等挑战下的数据表示稳定性问题。通过集成自编码器与多层感知机，并采用双损失策略、纠正伪标签、队列管理类别及过采样处理不平衡等方法，SCIL在真实世界和合成数据集上的表现优于现有基准和其他最先进方法。


<details>
  <summary>Details</summary>
Motivation: 面对由概念漂移、类别不平衡、标签稀缺以及新类别出现所带来的挑战时，传统的数据流处理方式难以保证数据表示的稳定性，且容易导致模型偏向于过时的数据分布，从而降低检测系统的鲁棒性和可靠性。

Method: SCIL框架结合了自编码器(AE)与多层感知机用于多分类预测任务；采用了分类与重构双重损失策略来支持预测及新类别检测；利用修正后的伪标签进行在线训练；通过队列机制管理类别信息；并采取过采样技术解决类别不平衡问题。

Result: 实验结果表明，在包含类别不平衡、增量类别以及概念漂移特征的真实世界和合成数据集上，SCIL相比强大的基线方法以及其他当前最优技术展现出了更好的性能。

Conclusion: SCIL为应对大规模流式数据中的动态变化提供了一个有效解决方案，其通过综合运用多种技术手段成功提升了模型对于不断变化环境的适应能力。

Abstract: In today's connected world, the generation of massive streaming data across diverse domains has become commonplace. In the presence of concept drift, class imbalance, label scarcity, and new class emergence, they jointly degrade representation stability, bias learning toward outdated distributions, and reduce the resilience and reliability of detection in dynamic environments. This paper proposes SCIL (Streaming Class-Incremental Learning) to address these challenges. The SCIL framework integrates an autoencoder (AE) with a multi-layer perceptron for multi-class prediction, uses a dual-loss strategy (classification and reconstruction) for prediction and new class detection, employs corrected pseudo-labels for online training, manages classes with queues, and applies oversampling to handle imbalance. The rationale behind the method's structure is elucidated through ablation studies and a comprehensive experimental evaluation is performed using both real-world and synthetic datasets that feature class imbalance, incremental classes, and concept drifts. Our results demonstrate that SCIL outperforms strong baselines and state-of-the-art methods. Based on our commitment to Open Science, we make our code and datasets available to the community.

</details>


### [47] [Model soups need only one ingredient](https://arxiv.org/abs/2602.09689)
*Alireza Abdollahpoorrostam,Nikolaos Dimitriadis,Adam Hazimeh,Pascal Frossard*

Main category: cs.LG

TL;DR: 本文提出了一种名为MonoSoup的新方法，它通过仅使用单个检查点实现了强大的ID-OOD平衡。该方法基于奇异值分解(SVD)对每个层的更新进行处理，并根据熵的有效秩自动调整这些组件，考虑了模型的频谱和几何结构。实验表明，这种方法在保持多检查点方法优点的同时，避免了其计算开销，适用于图像识别和语言模型等领域。


<details>
  <summary>Details</summary>
Motivation: 现有针对大型预训练模型微调以适应目标分布的方法虽然提高了同分布（ID）准确性，但往往牺牲了异分布（OOD）鲁棒性。尽管权重空间集成方法如Model Soups可以通过平均多个检查点来缓解这一问题，但它们需要大量的训练和存储资源。因此，作者们旨在开发一种简单、无需数据、无超参数且后处理的方法，能够在减少计算成本的前提下达到良好的ID-OOD平衡。

Method: MonoSoup通过对每个层更新执行奇异值分解(SVD)，将其分解为捕获任务特定适应性的高能量方向以及引入噪声但仍可能编码有助于鲁棒性的残留信号的低能量方向。接着，利用基于熵的有效秩来自动生成层级系数，以重新加权这些组成部分，同时考虑到模型的频谱与几何结构特性。

Result: 实验结果表明，在CLIP模型于ImageNet上微调并在自然分布偏移下评估时，以及Qwen语言模型在数学推理及多项选择基准测试中，MonoSoup作为一种即插即用方案，能够有效地作为多检查点方法的替代品，保留大部分优势的同时大幅降低了计算需求。

Conclusion: MonoSoup提供了一种简洁有效的解决方案，解决了在提高同分布性能的同时保持良好异分布鲁棒性的挑战，且无需额外的数据或复杂的超参数调节过程。

Abstract: Fine-tuning large pre-trained models on a target distribution often improves in-distribution (ID) accuracy, but at the cost of out-of-distribution (OOD) robustness as representations specialize to the fine-tuning data. Weight-space ensembling methods, such as Model Soups, mitigate this effect by averaging multiple checkpoints, but they are computationally prohibitive, requiring the training and storage of dozens of fine-tuned models. In this paper, we introduce MonoSoup, a simple, data-free, hyperparameter-free, post-hoc method that achieves a strong ID-OOD balance using only a single checkpoint. Our method applies Singular Value Decomposition (SVD) to each layer's update and decomposes it into high-energy directions that capture task-specific adaptation and low-energy directions that introduce noise but may still encode residual signals useful for robustness. MonoSoup then uses entropy-based effective rank to automatically re-weigh these components with layer-wise coefficients that account for the spectral and geometric structure of the model. Experiments on CLIP models fine-tuned on ImageNet and evaluated under natural distribution shifts, as well as on Qwen language models tested on mathematical reasoning and multiple-choice benchmarks, show that this plug-and-play approach is a practical and effective alternative to multi-checkpoint methods, retaining much of their benefits without their computational overhead.

</details>


### [48] [Contextual and Seasonal LSTMs for Time Series Anomaly Detection](https://arxiv.org/abs/2602.09690)
*Lingpei Zhang,Qingming Li,Yong Yang,Jiahao Chen,Rui Zeng,Chenyang Lyu,Shouling Ji*

Main category: cs.LG

TL;DR: 本文提出了一种名为CS-LSTMs的新颖预测框架，用于解决单变量时间序列中微妙异常检测的问题。通过结合上下文依赖性和季节性模式，并整合时域和频域表示，该方法在公共基准数据集上表现出优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重构和基于预测的方法难以捕捉某些细微异常，特别是小点异常和缓慢上升的异常。为了应对这些挑战，作者提出了新的方法来提高对这些微妙异常的检测能力。

Method: 提出的方法是Contextual and Seasonal LSTMs (CS-LSTMs)，它建立在噪声分解策略之上，并同时利用了上下文依赖关系和季节性模式，以增强对细微异常的检测。此外，该方法还结合了时域与频域的表现形式，从而更准确地建模周期趋势并定位异常。

Result: 通过对公共基准数据集进行广泛评估显示，CS-LSTMs持续优于最先进方法的表现，展示了其在鲁棒时间序列异常检测中的有效性和实用价值。

Conclusion: CS-LSTMs为单变量时间序列中的微妙异常提供了一种有效的检测手段，其通过综合考虑上下文信息及季节特征，提升了对于小点异常和缓慢增长型异常的识别精度，在实际应用中具有显著优势。

Abstract: Univariate time series (UTS), where each timestamp records a single variable, serve as crucial indicators in web systems and cloud servers. Anomaly detection in UTS plays an essential role in both data mining and system reliability management. However, existing reconstruction-based and prediction-based methods struggle to capture certain subtle anomalies, particularly small point anomalies and slowly rising anomalies. To address these challenges, we propose a novel prediction-based framework named Contextual and Seasonal LSTMs (CS-LSTMs). CS-LSTMs are built upon a noise decomposition strategy and jointly leverage contextual dependencies and seasonal patterns, thereby strengthening the detection of subtle anomalies. By integrating both time-domain and frequency-domain representations, CS-LSTMs achieve more accurate modeling of periodic trends and anomaly localization. Extensive evaluations on public benchmark datasets demonstrate that CS-LSTMs consistently outperform state-of-the-art methods, highlighting their effectiveness and practical value in robust time series anomaly detection.

</details>


### [49] [Physics-informed diffusion models in spectral space](https://arxiv.org/abs/2602.09708)
*Davide Gallon,Philippe von Wurstemberger,Patrick Cheridito,Arnulf Jentzen*

Main category: cs.LG

TL;DR: 本文提出了一种将生成式潜在扩散模型与物理信息机器学习相结合的方法，用于基于部分观测值生成参数偏微分方程（PDEs）的解，包括正向和逆向PDE问题。通过在具有控制规则性的函数对应的高斯噪声所在的潜空间中学习PDE参数和解的联合分布，该方法实现了相对于基于网格的扩散模型显著的维度减少，并确保了在函数空间内诱导的过程保持在PDE算子良好定义的函数类中。借助于扩散后验采样，在推理过程中施加物理信息约束和测量条件，每个扩散步骤应用基于Adam的更新。实验表明，该方法在泊松、亥姆霍兹以及不可压缩纳维-斯托克斯方程上的准确性和计算效率方面优于现有的基于稀疏观测的PDE求解器。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于部分观测值生成参数偏微分方程(PDEs)的解这一挑战，特别是针对正向及逆向PDE问题，同时提高解决方案的准确性和计算效率。

Method: 通过结合生成式潜在扩散模型与物理信息机器学习，在一个潜空间中学习PDE参数和解的联合分布，其中高斯噪声对应着具有一定规则性的函数。利用这种谱形式化处理，相较于传统的基于网格的扩散模型，能够实现维度上的大幅度缩减，并且保证在函数空间内所诱导的过程维持在一个使得PDE算子明确界定的函数类别之内。此外，基于扩散后验采样的技术，在推断期间根据物理信息约束和测量条件执行调整，具体来说就是在每一个扩散步骤里采用基于Adam优化算法的更新策略。

Result: 研究结果表明，本方法在解决泊松、亥姆霍兹以及不可压缩纳维-斯托克斯方程时，不仅提高了准确性，还增强了计算效率，尤其是在处理稀疏观测数据的情况下表现尤为突出。

Conclusion: 结合生成式潜在扩散模型与物理信息机器学习提供了一种新的途径来高效地解决基于部分观测值的参数偏微分方程问题，展示了在不同类型的PDE上超越现有技术水平的潜力。

Abstract: We propose a methodology that combines generative latent diffusion models with physics-informed machine learning to generate solutions of parametric partial differential equations (PDEs) conditioned on partial observations, which includes, in particular, forward and inverse PDE problems. We learn the joint distribution of PDE parameters and solutions via a diffusion process in a latent space of scaled spectral representations, where Gaussian noise corresponds to functions with controlled regularity. This spectral formulation enables significant dimensionality reduction compared to grid-based diffusion models and ensures that the induced process in function space remains within a class of functions for which the PDE operators are well defined. Building on diffusion posterior sampling, we enforce physics-informed constraints and measurement conditions during inference, applying Adam-based updates at each diffusion step. We evaluate the proposed approach on Poisson, Helmholtz, and incompressible Navier--Stokes equations, demonstrating improved accuracy and computational efficiency compared with existing diffusion-based PDE solvers, which are state of the art for sparse observations. Code is available at https://github.com/deeplearningmethods/PISD.

</details>


### [50] [ExO-PPO: an Extended Off-policy Proximal Policy Optimization Algorithm](https://arxiv.org/abs/2602.09726)
*Hanyong Wang,Menglong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的PPO变体，称为ExO-PPO，它结合了on-policy的稳定性和off-policy的数据利用效率。通过扩展off-policy改进和调整剪切机制，并使用过去M个策略生成的轨迹进行off-policy训练，实验表明ExO-PPO在多种任务上表现出更好的性能平衡。


<details>
  <summary>Details</summary>
Motivation: 由于策略梯度构建和训练动态的特点，深度强化学习模型的调优仍然是一个挑战。虽然PPO算法确保了可靠且稳定的策略改进，但这种模式可能会牺牲样本效率。与此同时，off-policy方法虽然能更充分地利用数据，却增加了估计方差和偏差。因此，本文旨在结合两种方法的优点，提出一种新的解决方案。

Method: 1. 从广义策略改进下界的期望形式中推导出扩大的off-policy改进。
2. 使用分段指数函数扩展裁剪机制，以适应合适的替代目标函数。
3. 将过去M个策略产生的轨迹组织到重放缓冲区中用于off-policy训练。该方法被称为Extended Off-policy Proximal Policy Optimization (ExO-PPO)。

Result: 与PPO以及其他一些最先进的变体相比，ExO-PPO在不同任务上的实证实验中展示了改善的表现，同时保持了样本效率和稳定性之间的良好平衡。

Conclusion: 提出的ExO-PPO方法成功地将on-policy迭代的稳定性保证与更加高效的off-policy数据利用结合起来，从而在多样化的任务测试中实现了性能的提升。

Abstract: Deep reinforcement learning has been able to solve various tasks successfully, however, due to the construction of policy gradient and training dynamics, tuning deep reinforcement learning models remains challenging. As one of the most successful deep reinforcement-learning algorithm, the Proximal Policy Optimization algorithm (PPO) clips the policy gradient within a conservative on-policy updates, which ensures reliable and stable policy improvement. However, this training pattern may sacrifice sample efficiency. On the other hand, off-policy methods make more adequate use of data through sample reuse, though at the cost of increased the estimation variance and bias. To leverage the advantages of both, in this paper, we propose a new PPO variant based on the stability guarantee from conservative on-policy iteration with a more efficient off-policy data utilization. Specifically, we first derive an extended off-policy improvement from an expectation form of generalized policy improvement lower bound. Then, we extend the clipping mechanism with segmented exponential functions for a suitable surrogate objective function. Third, the trajectories generated by the past $M$ policies are organized in the replay buffer for off-policy training. We refer to this method as Extended Off-policy Proximal Policy Optimization (ExO-PPO). Compared with PPO and some other state-of-the-art variants, we demonstrate an improved performance of ExO-PPO with balanced sample efficiency and stability on varied tasks in the empirical experiments.

</details>


### [51] [Explainability in Generative Medical Diffusion Models: A Faithfulness-Based Analysis on MRI Synthesis](https://arxiv.org/abs/2602.09781)
*Surjo Dey,Pallabi Saikia*

Main category: cs.LG

TL;DR: 本研究探讨了生成扩散模型在医学成像（特别是MRI合成）中的可解释性问题，通过基于原型的可解释性方法如ProtoPNet、Enhanced ProtoPNet和ProtoPool来分析生成图像与训练特征之间的联系。实验表明，Enhanced ProtoPNet提供了最高的忠实度（得分为0.1534），从而使得生成过程更加透明可信，有助于提高生成AI在医疗保健领域应用的安全性和可解读性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成逼真的医学图像方面表现出色，但其内部决策过程依然不透明。为了解决这一问题并提高模型在医疗保健领域的信任度和安全性，需要开发一种能够更好地理解这些模型如何生成图像的方法。

Method: 提出了一种基于忠实度的可解释性框架，利用包括ProtoPNet (PPNet)、Enhanced ProtoPNet (EPPNet) 和 ProtoPool在内的基于原型的可解释性方法来探索生成图像与训练数据特征间的关系，并通过去噪轨迹分析理解图像形成背后的逻辑。

Result: 实验分析显示，在所测试的方法中，Enhanced ProtoPNet实现了最高的忠实度评分(0.1534)，这表明它能提供更可靠且具有洞察力的关于生成过程的解释。

Conclusion: 通过基于忠实度的解释方法可以使扩散模型变得更加透明和值得信赖，这对于促进生成式人工智能技术在医疗健康领域的安全及可解释应用至关重要。

Abstract: This study investigates the explainability of generative diffusion models in the context of medical imaging, focusing on Magnetic resonance imaging (MRI) synthesis. Although diffusion models have shown strong performance in generating realistic medical images, their internal decision making process remains largely opaque. We present a faithfulness-based explainability framework that analyzes how prototype-based explainability methods like ProtoPNet (PPNet), Enhanced ProtoPNet (EPPNet), and ProtoPool can link the relationship between generated and training features. Our study focuses on understanding the reasoning behind image formation through denoising trajectory of diffusion model and subsequently prototype explainability with faithfulness analysis. Experimental analysis shows that EPPNet achieves the highest faithfulness (with score 0.1534), offering more reliable insights, and explainability into the generative process. The results highlight that diffusion models can be made more transparent and trustworthy through faithfulness-based explanations, contributing to safer and more interpretable applications of generative AI in healthcare.

</details>


### [52] [Flexible Entropy Control in RLVR with Gradient-Preserving Perspective](https://arxiv.org/abs/2602.09782)
*Kun Chen,Peng Shi,Fanfan Liu,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于动态裁剪阈值的调节机制来精确管理强化学习中的熵，通过设计和评估包括先增后减、减增减及振荡衰减在内的动态熵控制策略，有效缓解了因持续训练导致的策略熵崩溃问题，并在多个基准测试中取得了优异的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理强化学习与可验证奖励（RLVR）时，面对连续训练容易导致策略熵崩溃的问题，表现为熵快速下降，造成过早自信、输出多样性降低以及梯度范数消失从而阻碍学习进程。尽管Gradient-Preserving Clipping是影响这些动态的主要因素之一，但当前缺乏有效的框架将裁剪机制与精确熵控联系起来。

Method: 首先从理论上和实验上验证了特定重要性采样比区域对熵增长和减少的贡献；然后引入了一种利用动态裁剪阈值的新调节机制来精确控制熵；最后设计并评估了几种不同的动态熵控制策略，如先增后减、减增减和振荡衰减等。

Result: 实验结果显示，所提出的策略能够有效减轻熵崩溃现象，并且在多个基准测试中表现出色。

Conclusion: 通过重新塑造从Gradient-Preserving Clipping角度出发的熵控制方法，本研究为解决强化学习中由于连续训练引发的熵崩溃问题提供了新途径，证明了动态熵控制策略的有效性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.

</details>


### [53] [Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path](https://arxiv.org/abs/2602.09784)
*Andres Saurez,Neha Sengar,Dongsoo Har*

Main category: cs.LG

TL;DR: 该论文提出了一种电路指纹假设，表明变压器中的电路发现和激活导向遵循相同的几何原则。通过此方法，无需梯度或因果干预即可实现与基于梯度的方法相当的电路发现性能，并且能够以更高的准确率控制情绪分类，同时保持事实准确性。


<details>
  <summary>Details</summary>
Motivation: 研究者们注意到，在变压器中电路发现和激活导向虽然作为独立的研究方向发展，但两者实际上作用于相同的表示空间。他们提出了一个假设：答案标记在单独处理时编码了会产生它们的方向。这个假设旨在探索电路发现和激活导向是否是同一底层结构的不同视图。

Method: 论文中介绍了一种基于几何对齐的方法来验证电路指纹假设，这种方法不需要使用梯度或进行因果干预就能恢复与基于梯度的方法相类似的结构。研究者们还测试了这种新方法在标准基准（IOI、SVA、MCQA）上的表现以及在四个模型家族中的应用情况。

Result: 实验结果表明，所提出的方法在电路发现任务上达到了与基于梯度的方法相似的表现水平，并且利用相同的方向可以实现受控转向——在情感分类任务上达到了69.8%的准确率，而仅使用指令提示则为53.1%，同时保持了较高的事实准确性。

Conclusion: 研究表明，变压器电路本质上是几何结构，解释性和可控性代表了同一对象的不同方面。这一读写二元性不仅促进了方法的发展，也揭示了变压器内部运作机制的新理解。

Abstract: Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based methods through geometric alignment alone. We validate this on standard benchmarks (IOI, SVA, MCQA) across four model families, achieving circuit discovery performance comparable to gradient-based methods. The same directions that identify circuit components also enable controlled steering -- achieving 69.8\% emotion classification accuracy versus 53.1\% for instruction prompting while preserving factual accuracy. Beyond method development, this read-write duality reveals that transformer circuits are fundamentally geometric structures: interpretability and controllability are two facets of the same object.

</details>


### [54] [When Less is More: The LLM Scaling Paradox in Context Compression](https://arxiv.org/abs/2602.09789)
*Ruishan Guo,Yibing Liu,Guoxin Ma,Yan Wang,Yueyang Zhang,Long Xia,Kecheng Chen,Zhiyuan Sun,Daiting Shi*

Main category: cs.LG

TL;DR: 研究发现，在压缩-解码设置中增加模型大小可能会导致重建上下文的忠实度降低，称为'规模-保真度悖论'。这主要是由于知识覆盖和语义漂移两个因素造成的。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索为何在压缩-降维设置下，增大模型尺寸反而会导致生成内容对于原始信息忠实度下降的现象，即所谓的“规模-保真度悖论”。

Method: 通过从0.6B到90B参数量的不同规模模型进行广泛的实验观察，并分析了导致这一现象的主要原因：1）知识覆盖：大型模型更倾向于用自身先验信念替换源事实；2）语义漂移：大型模型更容易对内容进行改写或重组而非逐字再现。

Result: 结果表明，问题不在于参数数量本身，而是随着模型扩展而来的过度语义容量和放大的生成不确定性。具体来说，上下文嵌入的秩增加促进了先验知识侵入，而对令牌预测分布较高的熵则鼓励了重写行为。

Conclusion: 本研究表明，仅仅增加模型大小并不能保证在开放性生成任务中忠实地保留信息，指出了现有缩放法则的一个局限性。

Abstract: Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases. Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\to$ ``Bob hit Alice''. By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting. Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation.

</details>


### [55] [A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer](https://arxiv.org/abs/2602.09810)
*Azka Nasir,Fatima Dossa,Muhammad Ahmed Atif,Mohammad Ahmed Atif*

Main category: cs.LG

TL;DR: 本研究通过控制实验，探讨了Double Deep Q-Networks (DDQN)和Dueling DQN在跨环境迁移学习中的表现差异。结果表明，在相同的条件下，DDQN能够避免负面迁移，并保持与基线性能相当的学习动态；而Dueling DQN则表现出负面迁移特征，如奖励减少和优化过程不稳定。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同架构（特别是DDQN与Dueling DQN）如何影响深度强化学习中跨环境的迁移学习效果，尤其是在显著领域转移的情况下迁移学习可能失败的问题。

Method: 采用CartPole作为源任务、LunarLander为目标任务来评估固定层级表示迁移协议的效果。所有实验均在相同的超参数和训练条件下进行，同时使用从零开始训练的基线智能体以对比迁移效果。

Result: 实验证明，DDQN在所考察设置下始终避免了负面迁移，并且在目标环境中保持了与基线性能相似的学习动态；相反地，Dueling DQN在同一条件下持续表现出负面迁移现象，表现为奖励降低以及优化行为不稳定。

Conclusion: 研究结果指出，在给定的迁移协议下，架构诱导偏差与基于价值的深度强化学习中跨环境迁移的鲁棒性密切相关。

Abstract: Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments. Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects. Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior. Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.

</details>


### [56] [CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization](https://arxiv.org/abs/2602.09851)
*Beicheng Xu,Keyao Ding,Wei Liu,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: 本文提出了CoFEH，一种结合了基于大型语言模型（LLMs）的特征工程（FE）和贝叶斯超参数优化（HPO）的协作框架，旨在实现稳健的端到端自动机器学习。通过利用思维树（ToT）驱动的FE优化器、贝叶斯优化模块以及动态优化器选择器，CoFEH能够探索灵活的FE管道，并在联合优化下展现出优于传统及基于LLM方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的自动机器学习方法将特征工程视为黑盒搜索，缺乏领域意识且受限于预定义的搜索空间。虽然大型语言模型提供了通过语义推理生成无界操作符的可能性，但现有技术无法构建自由形式的特征工程流程，也未能与机器学习模型的超参数优化联合进行，导致难以捕捉到特征工程与超参数优化之间的强相互作用。

Method: 提出了一种名为CoFEH的合作框架，该框架结合了基于大型语言模型的特征工程和贝叶斯超参数优化来实现端到端的自动机器学习。它包括一个由思维树支持的特征工程优化器、用于解决超参数优化问题的贝叶斯优化模块，以及一个可以根据需要自适应安排特征工程和超参数优化步骤的动态优化器选择器。此外，还引入了一种互条件机制，允许大型语言模型与贝叶斯优化之间共享上下文，从而做出更加知情的决策。

Result: 实验结果表明，CoFEH不仅超越了传统的和基于大型语言模型的特征工程基线，在联合优化条件下也实现了更优的整体性能。

Conclusion: 通过整合基于大型语言模型的特征工程与贝叶斯超参数优化，CoFEH提供了一个新的解决方案，以克服当前自动机器学习中特征工程阶段所面临的问题，展示了其在提高端到端自动机器学习效率方面的潜力。

Abstract: Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy "FE-then-HPO" workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.

</details>


### [57] [Statistical benchmarking of transformer models in low signal-to-noise time-series forecasting](https://arxiv.org/abs/2602.09869)
*Cyril Garcia,Guillaume Remy*

Main category: cs.LG

TL;DR: 本文研究了在仅有几年每日观察数据的低数据环境中，变压器架构在多变量时间序列预测中的表现。通过合成具有已知时间和横截面依赖结构的过程，并使用不同信噪比进行自举实验，直接评估与最佳真实预测因子的样本外相关性。结果显示，在各种设置下，交替进行时间和横截面自我注意的双向注意变压器优于标准基线（Lasso、提升方法和全连接多层感知器）。此外，还介绍了一种应用于训练期间注意力矩阵的动态稀疏化过程，证明在噪声环境中非常有效。对学习到的注意力模式的分析揭示了可解释的结构，并暗示了与经典回归中稀疏诱导正则化的联系，为这些模型为什么能在噪声下有效泛化提供了见解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索变压器架构是否能在低数据环境下的多变量时间序列预测任务中表现出色，特别是在处理仅包含几年每日观测值的数据集时。同时，也旨在考察在不同信噪比条件下，通过引入动态稀疏化过程改进变压器性能的可能性。

Method: 采用了合成生成的过程来建立实验基础，这些过程拥有已知的时间和横截面依赖结构及不同的信噪比。通过自举实验方法，直接利用与最优真实预测器的样本外相关性来进行评估。特别地，本研究提出了一个针对注意力矩阵的动态稀疏化过程，并在训练过程中应用该过程以提高模型在噪声环境下的表现。

Result: 研究发现，当面对包括低信噪比在内的多种情况时，采用交替时间和横截面自我注意机制的双向注意变压器相比Lasso、提升方法以及全连接多层感知器等标准基准模型展现出更优的表现。进一步地，所提出的动态稀疏化过程被证实对于改善噪声环境下的预测效果尤为显著。

Conclusion: 结论指出，通过结合时间和横截面自我注意机制，双向注意变压器能够在低数据量场景下提供更好的多变量时间序列预测性能；而动态稀疏化过程的应用则有助于增强模型在高噪声条件下的鲁棒性和准确性。

Abstract: We study the performance of transformer architectures for multivariate time-series forecasting in low-data regimes consisting of only a few years of daily observations. Using synthetically generated processes with known temporal and cross-sectional dependency structures and varying signal-to-noise ratios, we conduct bootstrapped experiments that enable direct evaluation via out-of-sample correlations with the optimal ground-truth predictor. We show that two-way attention transformers, which alternate between temporal and cross-sectional self-attention, can outperform standard baselines-Lasso, boosting methods, and fully connected multilayer perceptrons-across a wide range of settings, including low signal-to-noise regimes. We further introduce a dynamic sparsification procedure for attention matrices applied during training, and demonstrate that it becomes significantly effective in noisy environments, where the correlation between the target variable and the optimal predictor is on the order of a few percent. Analysis of the learned attention patterns reveals interpretable structure and suggests connections to sparsity-inducing regularization in classical regression, providing insight into why these models generalize effectively under noise.

</details>


### [58] [Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education](https://arxiv.org/abs/2602.09904)
*Anna Bodonhelyi,Mengdi Wang,Efe Bozkir,Babette Bühler,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: 本文提出了一种利用跨设备联邦学习的框架，通过面部表情和注视特征来检测远程学习中的认知脱离情况，旨在解决在线课程中缺乏直接教师支持导致的学生注意力分散问题。该方法不仅保护了用户的数据隐私，还增强了模型性能，并通过五个数据集上的广泛实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于COVID-19大流行，在线课程虽然扩大了教育的可及性，但缺乏直接的教学支持挑战了学习者自我调节注意力和参与度的能力。走神和脱离可能对学习结果产生负面影响，因此，通过基于视频的指标自动检测这些问题对于实时学习者支持来说是一个有希望的方法。然而，基于机器学习的方法通常需要共享敏感数据，这引发了隐私问题。

Method: 研究提出了一个利用跨设备联邦学习处理远程学习过程中行为与认知脱离不同表现形式（特别是行为脱离、走神和无聊）的框架。该方法使用面部表情和注视特征来拟合基于视频的认知脱离检测模型。此外，通过整合相关特征解决了眼镜带来的挑战，从而提高了整体模型性能。

Result: 为了验证方法的有效性，研究在五个数据集上进行了广泛的实验，并对比了多种联邦学习算法。结果显示，该方法在促进学习者参与度方面展现出了巨大潜力，同时为隐私保护型教育技术提供了新的解决方案。

Conclusion: 采用联邦学习的方法不仅能够保护用户的数据隐私，而且提供了一个具有实时学习者支持潜力的新颖解决方案。研究表明，这种方法在提高远程学习环境中学习者的参与度方面是有效的。

Abstract: Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support. However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load. We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features. By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms. Our results show great promise for privacy-preserving educational technologies promoting learner engagement.

</details>


### [59] [Causal Identification in Multi-Task Demand Learning with Confounding](https://arxiv.org/abs/2602.09969)
*Varun Gupta,Vijay Kamble*

Main category: cs.LG

TL;DR: 本文提出了一种新的估计框架——决策条件遮罩结果元学习（DCMOML），该方法能够在价格与潜在任务结构存在任意依赖的情况下实现因果识别。通过精心设计元学习者的信息集来利用跨任务异质性，同时考虑内生决策历史。研究证明，在每个任务的价格适应性受到轻微限制的情况下，该方法能够识别给定设计信息集下任务特定因果参数的条件均值。


<details>
  <summary>Details</summary>
Motivation: 在零售定价等多任务需求学习问题中，企业试图在一个大的决策情境集合中估计异质性的线性价格响应函数。然而，由于历史价格选择可能与未观察到的任务级需求决定因素随意相关，导致了内生性问题。这使得常用的方法如联合回归和元学习无法识别因果价格效应。

Method: 提出了名为决策条件遮罩结果元学习（DCMOML）的新框架，该框架通过巧妙地设计元学习器的信息集以利用跨任务差异性，并且考虑到内生性决策历史的影响。

Result: 研究表明，在对每个任务中的价格适应性做出轻微约束的前提下，该方法能够根据设计的信息集识别出任务特定因果参数的条件平均值。

Conclusion: 该研究为具有内生价格和小样本量的大规模需求估计提供了保证，为运营环境中部署基于因果关系的数据驱动定价模型奠定了理论基础。

Abstract: We study a canonical multi-task demand learning problem motivated by retail pricing, in which a firm seeks to estimate heterogeneous linear price-response functions across a large collection of decision contexts. Each context is characterized by rich observable covariates yet typically exhibits only limited historical price variation, motivating the use of multi-task learning to borrow strength across tasks. A central challenge in this setting is endogeneity: historical prices are chosen by managers or algorithms and may be arbitrarily correlated with unobserved, task-level demand determinants. Under such confounding by latent fundamentals, commonly used approaches, such as pooled regression and meta-learning, fail to identify causal price effects.
  We propose a new estimation framework that achieves causal identification despite arbitrary dependence between prices and latent task structure. Our approach, Decision-Conditioned Masked-Outcome Meta-Learning (DCMOML), involves carefully designing the information set of a meta-learner to leverage cross-task heterogeneity while accounting for endogenous decision histories. Under a mild restriction on price adaptivity in each task, we establish that this method identifies the conditional mean of the task-specific causal parameters given the designed information set. Our results provide guarantees for large-scale demand estimation with endogenous prices and small per-task samples, offering a principled foundation for deploying causal, data-driven pricing models in operational environments.

</details>


### [60] [Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings](https://arxiv.org/abs/2602.09985)
*Alexander Fertig,Karthikeyan Chandra Sekaran,Lakshman Balasubramanian,Michael Botsch*

Main category: cs.LG

TL;DR: 提出了一种基于自监督嵌入方法的在线监控框架，用于检测物体状态表示中的异常。该框架通过JEPA构建自监督预测任务，无需异常标签即可训练，并生成丰富的对象嵌入，作为已建立的异常检测方法的输入，以识别物体状态表示中的异常。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆的推出，需要采取措施确保其安全运行。特别是对于已经投入运行的系统，经常采用监控框架来持续在线地在后台监督系统的状态并记录异常情况。关键挑战在于如何在没有异常标签的情况下创建一个异常检测框架，因为对于未知的异常通常是没有标签的。

Method: 本工作应用了自监督嵌入方法将对象数据转换为潜在表示空间。为此，构建了一个基于JEPA（Joint Embedding Predictive Architecture）的自监督预测任务，允许在没有异常标签的情况下进行训练，并创造丰富的对象嵌入。这些表达性强的JEPA嵌入被用作既定异常检测方法的输入，以便识别物体状态表示中的异常。

Result: 实验是在公开可用的真实世界nuScenes数据集上进行的，展示了该框架的能力。

Conclusion: 所提出的在线监控框架特别适用于真实环境中的应用，在这种环境中，操作过程中可能会出现新的或未知的异常，而这些异常是没有标签可供使用的。

Abstract: As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations. Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly labels and the creation of rich object embeddings. The resulting expressive JEPA embeddings serve as input for established anomaly detection methods, in order to identify anomalies within object state representations. This framework is particularly useful for applications in real-world environments, where new or unknown anomalies may occur during operation for which there are no labels available. Experiments performed on the publicly available, real-world nuScenes dataset illustrate the framework's capabilities.

</details>


### [61] [Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions](https://arxiv.org/abs/2602.09987)
*J Rosser,Robert Kirk,Edward Grefenstette,Jakob Foerster,Laura Ruis*

Main category: cs.LG

TL;DR: 研究者开发了Infusion框架，该框架利用影响函数的近似方法来对训练文档进行微小改动，从而诱导模型行为的变化。实验表明，在CIFAR-10数据集上仅修改0.2%的数据就能有效改变模型行为，并且这种效果可以在不同架构之间传递。此外，初步的语言实验也探索了这种方法在增加目标行为概率方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过精心设计训练数据来诱导模型的行为变化，而非仅仅分析模型行为与训练数据之间的关系。

Method: 提出了名为Infusion的框架，它使用可扩展的影响函数近似技术来计算训练文档中的微小扰动，这些扰动旨在通过参数偏移引起模型行为的目标变化。

Result: 在CIFAR-10数据集上的实验显示，通过对仅0.2%（即45,000个样本中的100个）的训练文档进行细微编辑，可以有效地竞争过插入少量显式行为示例的基线方法。此外，发现Infusion能够在不同的架构间（如ResNet和CNN）转移其效果。语言领域的初步实验则表明，该方法在放大模型已学习到的行为方面最为有效。

Conclusion: 研究表明，通过对训练数据进行小而微妙的编辑，可以系统地塑造模型的行为，这强调了训练数据可解释性对于对抗者和防御者都非常重要。

Abstract: Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents that induce targeted changes in model behavior through parameter shifts. We evaluate Infusion on data poisoning tasks across vision and language domains. On CIFAR-10, we show that making subtle edits via Infusion to just 0.2% (100/45,000) of the training documents can be competitive with the baseline of inserting a small number of explicit behavior examples. We also find that Infusion transfers across architectures (ResNet $\leftrightarrow$ CNN), suggesting a single poisoned corpus can affect multiple independently trained models. In preliminary language experiments, we characterize when our approach increases the probability of target behaviors and when it fails, finding it most effective at amplifying behaviors the model has already learned. Taken together, these results show that small, subtle edits to training data can systematically shape model behavior, underscoring the importance of training data interpretability for adversaries and defenders alike. We provide the code here: https://github.com/jrosseruk/infusion.

</details>


### [62] [Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning](https://arxiv.org/abs/2602.10006)
*Shijie Zhang,Xiang Guo,Rujun Guo,Shaoyu Liu,Xiaozhao Wang,Guanjun Jiang,Kevin Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的Answer-First, Reason Later (AFRL)范式，旨在构建一个既能满足在线系统毫秒级响应需求，又能保留大型语言模型解释性推理痕迹的搜索相关性模型。通过采用'监督微调(SFT)+强化学习(RL)'管道，并引入模式平衡优化策略以解决模式崩溃问题，该方法在实验中展示了最先进性能，同时实现了知识的有效蒸馏，使小型模型也能达到专家级逻辑。


<details>
  <summary>Details</summary>
Motivation: 为了在满足在线系统对毫秒级响应时间要求的同时，保持大型语言模型（LLMs）所提供的可解释性推理路径，提出了一个新的解决方案。

Method: 1. 提出Answer-First, Reason Later (AFRL)新范式，要求模型首先输出确定的相关性评分，随后提供结构化的逻辑解释。
2. 采用“监督微调(SFT) + 强化学习(RL)”流程来实现AFRL。
3. 针对直接应用现有RL训练可能导致的‘模式崩溃’问题，从信息论角度分析了原因，并提出了结合SFT辅助损失的Mode-Balanced Optimization策略。
4. 构建自动化指令进化系统和多阶段课程设计，确保数据质量达到专家水平。

Result: 实验结果表明，所提出的32B教师模型达到了最先进的性能；此外，AFRL架构还支持高效的知识蒸馏过程，能够将专家级别的逻辑成功转移至0.6B的小型模型中，从而在推理深度与部署延迟之间找到了平衡点。

Conclusion: 本文介绍了一种新颖的方法——Answer-First, Reason Later (AFRL)，它结合了监督微调和强化学习技术，解决了传统搜索相关性模型中存在的延迟高、缺乏解释性等问题。通过引入模式平衡优化策略，进一步克服了强化学习过程中容易出现的模式崩溃现象。最终，不仅提升了模型的整体表现，还使得小规模模型也能拥有强大的推理能力。

Abstract: Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a "Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)" pipeline to achieve AFRL. However, directly applying existing RL training often leads to \textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to "reward hacking." On the other hand, SFT minimizes the \textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.

</details>


### [63] [A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula](https://arxiv.org/abs/2602.10014)
*Chenruo Liu,Yijun Dong,Yiqiu Shen,Qi Lei*

Main category: cs.LG

TL;DR: 本文通过建模每轮自我改进为在奖励过滤分布上的最大似然微调，提供了有限样本设置下迭代自我改进过程的理论基础。分析揭示了更好的模型在每次迭代中接受更多数据的反馈循环，并探讨了任务难度、样本预算等因素对模型初始化的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管自我改进在实证上取得了成功，但其生成性和迭代性程序在实际有限样本设置中的理论基础仍然有限。为了填补这一空白，研究旨在建立一个理论框架来理解迭代自我改进的过程。

Method: 采用的方法是将每一轮自我改进视为对奖励过滤分布的最大似然微调，并在此基础上推导出预期奖励的有限样本保证。此外，还从任务中心视角出发，考虑了具有多个难度级别的推理任务，并证明了模型初始化、任务难度及样本预算之间可量化的条件，在这些条件下易到难的学习课程比固定混合任务训练更能获得更好的保障。

Result: 研究结果表明存在一个明确的反馈循环，即更优的模型在每次迭代过程中能够接受更多的数据，这支持了持续的自我改进同时也解释了这种改进最终达到饱和的原因。对于不同难度的任务，当满足特定条件下，采取由易至难的学习顺序可以比直接训练所有任务混合起来的情况取得更好效果。

Conclusion: 这项工作为迭代自我改进提供了一定程度上的理论支撑，特别是关于如何根据任务特性调整学习策略以实现最佳性能方面给出了新的见解。

Abstract: Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.

</details>


### [64] [ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning](https://arxiv.org/abs/2602.10019)
*Qingnan Ren,Shiting Huang,Zhen Fang,Zehui Chen,Lin Chen,Lijun Li,Feng Zhao*

Main category: cs.LG

TL;DR: ADORA, a new framework for policy optimization in reinforcement learning, improves the efficiency of policy updates by dynamically adjusting the advantage function's weighting. This is achieved through adaptive categorization of training data into advantageous and disadvantageous samples, based on their evolving utility during online rollouts. ADORA enhances long-term reasoning in complex tasks without the need for sensitive hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper stems from the inefficiency of static advantage estimation in reinforcement learning, which leads to suboptimal policy updates, slower convergence rates, and increased learning instability. The authors aim to address these issues by proposing a method that can adaptively adjust the advantage function's weighting according to the dynamic utility of training samples over time.

Method: ADORA (Advantage Dynamics via Online Rollout Adaptation) is introduced as a solution. It dynamically adjusts the advantage function's weighting by adaptively classifying training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This approach allows for more efficient credit assignment and does not require significant changes to existing policy optimization algorithms, making it easy to integrate and use.

Result: Evaluations across different model families and data scales show that ADORA significantly enhances long-term reasoning capabilities in both geometric and mathematical tasks. It consistently achieves notable performance improvements compared to traditional methods, all while not requiring sensitive adjustments to hyperparameters.

Conclusion: ADORA presents a robust and efficient way to optimize policies in reinforcement learning, particularly for tasks involving long-term reasoning. By dynamically adapting the advantage function's weighting, it enables more effective learning from experiences, leading to faster convergence and better stability, without the need for meticulous hyperparameter tuning.

Abstract: Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \textbf{ADORA} (\textbf{A}dvantage \textbf{D}ynamics via \textbf{O}nline \textbf{R}ollout \textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function's weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.

</details>


### [65] [Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems](https://arxiv.org/abs/2602.10037)
*Tetsuro Abe,Masashi Yamashita,Shu Tanaka*

Main category: cs.LG

TL;DR: 该研究通过结合二进制自动编码器(bAE)和基于量子退火的因子分解机(FMQA)，在黑盒组合优化中提高了搜索效率。使用小规模旅行商问题作为测试平台，研究表明bAE能准确重构可行路径，并且与手动设计的编码相比，在相似压缩率下更好地对齐了路径距离与潜在汉明距离，产生了更平滑的邻域以及较少的局部最优解。这些几何特性解释了为什么bAE+FMQA能够更快地提高近似比，同时在整个优化过程中保持可行性。


<details>
  <summary>Details</summary>
Motivation: 在黑盒组合优化中，目标评估通常成本高昂，因此需要在有限预算内找到高质量解决方案。虽然因子分解机与量子退火（FMQA）能够构建二次替代模型并利用Ising机进行优化，但其要求决策变量为二进制形式，对于非二进制结构如整数排列来说，选择合适的二进制编码对搜索效率有着重大影响。不当的编码可能无法反映原始邻域结构，导致小范围变动不对应于原解空间中有意义的变化，并且约束性问题可能会产生许多不可行候选方案浪费评估机会。

Method: 研究采用了一种结合二进制自动编码器（bAE）的方法来学习紧凑的二进制潜在代码，从而改善了FMQA的表现。以小型旅行商问题为例，展示了bAE如何精确重建可行路径，并且相对于人工设计的编码方式，在相近压缩水平下更加有效地将路径间的实际距离与潜伏空间中的汉明距离相匹配，进而形成了更为流畅连续的邻域关系，并减少了局部最优解的数量。

Result: 实验结果表明，bAE能够准确重构可行解，并且与相同压缩程度的手动设计编码相比，bAE可以更好地使旅游路径的距离与潜在汉明距离保持一致，形成更平滑的邻居关系，并产生更少的局部最优值。这些发现揭示了几何属性是如何帮助bAE+FMQA方法在加速逼近比的同时保持整个优化过程中的解的可行性。

Conclusion: 这项工作阐明了bAE+FMQA策略背后的性能提升机制，即通过增强解空间的几何一致性来提高搜索效率。研究还为开发适用于黑盒优化问题的潜在表示提供了指导原则。

Abstract: In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutations, the choice of binary encoding strongly affects search efficiency. If the encoding fails to reflect the original neighborhood structure, small Hamming moves may not correspond to meaningful modifications in the original solution space, and constrained problems can yield many infeasible candidates that waste evaluations. Recent work combines FMQA with a binary autoencoder (bAE) that learns a compact binary latent code from feasible solutions, yet the mechanism behind its performance gains is unclear. Using a small traveling salesman problem as an interpretable testbed, we show that the bAE reconstructs feasible tours accurately and, compared with manually designed encodings at similar compression, better aligns tour distances with latent Hamming distances, yields smoother neighborhoods under small bit flips, and produces fewer local optima. These geometric properties explain why bAE+FMQA improves the approximation ratio faster while maintaining feasibility throughout optimization, and they provide guidance for designing latent representations for black-box optimization.

</details>


### [66] [Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning](https://arxiv.org/abs/2602.10044)
*Akshay Mete,Shahid Aamir Sheikh,Tzu-Hsiang Lin,Dileep Kalathil,P. R. Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种新的乐观世界模型（OWMs），通过将乐观主义直接融入模型学习，从而在稀疏奖励环境中改善了深度强化学习中的探索效率。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励环境下，高效的探索仍然是强化学习中的一大挑战。本文旨在通过引入一种新颖的方法来解决这一问题，该方法能够提高样本效率和累积回报。

Method: 研究者们提出了乐观世界模型（OWMs），这是一种基于原则且可扩展的框架，它将自适应控制中的经典奖励偏置最大似然估计（RBMLE）带入了深度强化学习领域。与上置信界(UCB)风格的探索方法不同的是，OWMs通过增加一个乐观的动力学损失项直接促进更倾向于高回报结果的想象转换过程。这种方法完全基于梯度下降进行优化，不需要不确定性估计也不涉及约束优化。

Result: OWMs被集成到两种最先进的世界模型架构中，形成了Optimistic DreamerV3和Optimistic STORM两个实例。实验结果显示，相较于基线版本，这两个实例在样本效率和累积回报方面均显示出显著提升。

Conclusion: 通过引入乐观世界模型（OWMs），本研究为处理稀疏奖励环境下的高效探索提供了一个新途径，并展示了其相对于现有方法的有效性。

Abstract: Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.

</details>


### [67] [Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization](https://arxiv.org/abs/2602.10048)
*Xinchen Han,Hossam Afifi,Michel Marot,Xilu Wang,Lu Yin*

Main category: cs.LG

TL;DR: 本文提出了一种名为FGO的强化学习算法，该算法通过细分响应并基于长度和熵分配适当的权重来有效压缩链式思维（CoT），解决了GRPO的数据利用效率低和熵崩溃的问题。实验结果表明，FGO在不降低性能的情况下实现了有效的CoT压缩，并克服了GRPO的关键限制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）经常生成冗长的链式思维（CoT）推理，这增加了计算成本和延迟，但没有相应的性能提升。此外，现有的组相对策略优化（GRPO）方法存在数据利用效率低下和熵崩溃的问题。

Method: 提出了细粒度组策略优化（FGO），这是一种强化学习算法，通过对响应进行细分并根据长度和熵分配合适的权重来实现有效的CoT压缩。作为GRPO的一种改进变体，FGO旨在解决GRPO中的两个主要局限性：数据利用不足和熵崩溃问题。

Result: FGO在多个推理LLM和基准测试中进行了评估，包括MATH500、AIME24、AMC23和Minerva等。实验结果显示，FGO能够在不牺牲性能的前提下实现高效的CoT压缩，同时成功解决了GRPO的核心问题。

Conclusion: 通过引入FGO算法，研究者们为提高LLMs的推理效率提供了一个新的解决方案，它不仅能够减少不必要的计算开销，而且还能保持甚至可能改善模型的表现。

Abstract: Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \textbf{F}ine-grained \textbf{G}roup policy \textbf{O}ptimization (\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.

</details>


### [68] [Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability](https://arxiv.org/abs/2602.10067)
*Aaditya Vikram Prasad,Connor Watts,Jack Merullo,Dhruvil Gala,Owen Lewis,Thomas McGrath,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 本文提出了一种新的范式，即利用特征作为奖励函数的强化学习管道（RLFR），用于减少语言模型在开放任务中的幻觉现象。该方法基于一种新的探测框架识别出潜在的幻觉声明，并教导模型在其对事实性不确定时干预并纠正其生成内容。结果显示，与原始模型相比，使用此方法后的模型产生幻觉的可能性降低了58%，同时保持了标准基准测试的表现。


<details>
  <summary>Details</summary>
Motivation: 研究者们发现大规模数据集训练的语言模型能够学到编码抽象概念如事实性和意图等特征。这些特征通常被用来进行测试时间监控或指导。本文探索了另一种可能性：将这些特征用作开放任务中可扩展监督的一种形式，特别是针对幻觉减少这一目标行为。

Method: 设计了一个名为RLFR（从特征奖励中学习的强化学习）的管道，该方法利用特征作为奖励函数。通过一个新颖的探测框架来识别候选的幻觉陈述，进而教育模型在对其完成内容的事实性感到不确定时进行干预和修正。整个过程以Gemma-3-12B-IT为实验对象进行了操作化实施。

Result: 结果表明，与原模型相比，采用RLFR方法后的新策略减少了58%的幻觉发生率，同时在标准基准上的表现得以维持。

Conclusion: 本论文介绍了一种新范式，即通过基于特征的语言来提供监督，这为学习开放任务引入了新的视角。

Abstract: Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.

</details>


### [69] [Step-resolved data attribution for looped transformers](https://arxiv.org/abs/2602.10097)
*Georgios Kaissis,David Mildenberger,Juan Felipe Gomez,Martin J. Menten,Eleni Triantafillou*

Main category: cs.LG

TL;DR: 本文提出了一种名为Step-Decomposed Influence (SDI)的新方法，该方法能够分解TracIn影响估计器，以揭示训练样本在循环计算过程中何时起作用。通过张量速写实现，SDI无需具体化每个样本的梯度，从而实现在transformer规模上的应用。实验表明SDI具有良好的扩展性、与全梯度基线匹配且误差低，并支持多种数据归因和可解释性任务。


<details>
  <summary>Details</summary>
Motivation: 现有的训练数据影响估计器如TracIn仅提供一个汇总所有循环迭代过程的单一标量分数，这掩盖了训练样本在循环计算中何时重要的信息。为了解决这一问题，研究者们提出了Step-Decomposed Influence (SDI)，旨在通过展开循环计算图来将影响力归因于特定的循环迭代步骤，从而更细致地理解每个训练样本如何影响模型内部的推理过程。

Method: Step-Decomposed Influence (SDI)方法通过对循环计算图进行展开并把TracIn的影响分解成长度为τ的影响轨迹，进而将影响力归因至具体的循环迭代步骤。为了使SDI在transformer规模上实际可行，研究人员还开发了一种基于TensorSketch的实现方案，该方案不需要具体化每个样本的梯度。

Result: 实验结果证明，SDI不仅在扩展性方面表现优异，而且与使用完整梯度作为基准的方法相比，其误差也很小。此外，SDI还能够在一系列的数据归属和可解释性任务中发挥作用，提供关于潜在推理过程每一步洞见的支持。

Conclusion: 通过引入Step-Decomposed Influence (SDI)，研究成功地解决了现有训练数据影响估计方法不能清晰展示训练样本对循环计算过程各个阶段影响的问题。SDI不仅具备良好的实用性和准确性，还开启了对于transformer模型中潜藏推理机制更深层次的理解之路。

Abstract: We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$τ$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.

</details>


### [70] [Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders](https://arxiv.org/abs/2602.10099)
*Amandeep Kumar,Vishal M. Patel*

Main category: cs.LG

TL;DR: 该论文提出了一种新的方法RJF（Riemannian Flow Matching with Jacobi Regularization），通过约束生成过程在流形测地线上并修正由曲率引起的误差传播，解决了标准扩散变换器无法直接收敛于表示编码的问题。这种方法使得标准的DiT-B架构能够在不增加宽度的情况下有效收敛，达到3.37的FID分数。


<details>
  <summary>Details</summary>
Motivation: 作者发现，当利用表示编码器进行生成建模时，传统的扩散变换器无法直接在这些表示上有效地收敛。之前的研究认为这是由于容量瓶颈问题，并建议通过扩大扩散变换器的宽度来解决，但这会带来计算成本的显著增加。本研究则指出这个问题的根本原因在于几何干扰。

Method: 提出了Riemannian Flow Matching with Jacobi Regularization (RJF) 方法，该方法通过将生成过程限制在流形测地线之上，并对由曲率导致的误差传播进行了校正，从而允许标准扩散变压器架构无需扩大宽度即可成功收敛。

Result: 实验表明，采用RJF方法后，标准的DiT-B架构(131M参数)能够有效收敛，达到了3.37的FID评分，而之前的方法在此条件下无法实现收敛。

Conclusion: 通过引入RJF方法，解决了传统扩散变换器在处理高维特征空间中遇到的几何干扰问题，提供了一种更高效、不需要额外增加模型复杂度就能提高生成质量的新途径。

Abstract: Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF

</details>


### [71] [Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy](https://arxiv.org/abs/2602.10100)
*Júlio Oliveira,Rodrigo Ferreira,André Riker,Glaucio H. S. Carvalho,Eirini Eleni Tsilopoulou*

Main category: cs.LG

TL;DR: 本文提出了一种结合了增强数据隐私和可解释性的机器学习模型FEXT-DP，该模型基于决策树并应用差分隐私来保护数据隐私。尽管差分隐私会对系统的可解释性造成负面影响，但性能评估表明FEXT-DP在训练速度、均方误差以及可解释性方面有所提升。


<details>
  <summary>Details</summary>
Motivation: 为了同时提高现代机器学习系统中的数据隐私性和可解释性，本文旨在开发一种能够结合这两方面的机器学习模型。

Method: 提出了一种名为Federated EXplainable Trees with Differential Privacy (FEXT-DP)的联邦学习解决方案，该方案基于决策树，并通过应用差分隐私技术来为基于树的模型提供额外的数据隐私保护层。

Result: 虽然引入差分隐私对系统的可解释性有一定影响，但性能评估显示FEXT-DP在加快训练过程（即减少轮数）、降低均方误差及保持一定程度的可解释性方面表现良好。

Conclusion: 研究表明，FEXT-DP能够在保障增强数据隐私的同时维持较好的可解释性，从而为设计更安全且透明的机器学习系统提供了新的思路。

Abstract: Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [72] [RuleFlow : Generating Reusable Program Optimizations with LLMs](https://arxiv.org/abs/2602.09051)
*Avaljot Singh,Dushyant Bharadwaj,Stefanos Baziotis,Kaushik Varadharajan,Charith Mendis*

Main category: cs.SE

TL;DR: 提出了一种名为RuleFlow的三阶段混合方法来优化Pandas程序，该方法在发现和部署之间架起桥梁，通过将特定程序的优化转换为通用重写规则，并将其集成到编译器中自动应用，从而减少了对大型语言模型（LLMs）的重复依赖。实验显示，与之前的最先进方法相比，该方法在PandasBench基准测试上实现了高达4.3倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的系统和基于编译器的方法虽然可靠，但要么过于繁重要么仅支持有限数量的优化；而使用大型语言模型进行逐程序优化虽能合成非平凡优化，却不可靠、成本高昂且产出率低。因此，需要一种新的方法来解决Pandas程序优化中的这些挑战。

Method: 引入了一种称为RuleFlow的三阶段混合方法：首先，发现针对具体程序的优化；其次，将这些优化转化为通用的重写规则；最后，将这些规则整合进一个编译器中，以便自动应用于所有适用情况，避免了对大型语言模型的重复依赖。

Result: 在PandasBench基准测试中，RuleFlow相比Dias（以前基于编译器的最佳技术）实现了最高达4.3倍的速度提升，相比Modin（以前基于系统的最佳技术）则达到了1914.9倍的速度提升。

Conclusion: RuleFlow作为一种创新的Pandas优化框架，在提高性能方面展示了其作为当前最新技术水平的地位，为Pandas程序提供了一种更高效可靠的优化解决方案。

Abstract: Optimizing Pandas programs is a challenging problem. Existing systems and compiler-based approaches offer reliability but are either heavyweight or support only a limited set of optimizations. Conversely, using LLMs in a per-program optimization methodology can synthesize nontrivial optimizations, but is unreliable, expensive, and offers a low yield. In this work, we introduce a hybrid approach that works in a 3-stage manner that decouples discovery from deployment and connects them via a novel bridge. First, it discovers per-program optimizations (discovery). Second, they are converted into generalised rewrite rules (bridge). Finally, these rules are incorporated into a compiler that can automatically apply them wherever applicable, eliminating repeated reliance on LLMs (deployment). We demonstrate that RuleFlow is the new state-of-the-art (SOTA) Pandas optimization framework on PandasBench, a challenging Pandas benchmark consisting of Python notebooks. Across these notebooks, we achieve a speedup of up to 4.3x over Dias, the previous compiler-based SOTA, and 1914.9x over Modin, the previous systems-based SOTA.
  Our code is available at https://github.com/ADAPT-uiuc/RuleFlow.

</details>


### [73] [Predicting Open Source Software Sustainability with Deep Temporal Neural Hierarchical Architectures and Explainable AI](https://arxiv.org/abs/2602.09064)
*S M Rakib Ul Karim,Wenyi Lu,Enock Kasaadha,Sean Goggins*

Main category: cs.SE

TL;DR: 本文提出了一种层次预测框架，用于根据开源软件（OSS）项目的贡献活动、社区参与和维护动态来识别其生命周期阶段。该方法结合了工程化的表格指标与24个月的时间活动序列，并通过多阶段分类流程实现了超过94%的总体准确率，在理解OSS项目可持续性方面提供了深入见解。


<details>
  <summary>Details</summary>
Motivation: 现有的关于开源软件（OSS）项目的研究大多依赖于静态或聚合度量标准，如项目年龄或累积活动，这限制了对OSS随着时间推移如何维持活力的理解。为了更好地评估项目组织及健康状况，需要一种能够捕捉到OSS发展过程中不同生命阶段的方法。

Method: 作者们开发了一个层次预测框架，该框架将OSS项目归类为基于已建立的社会技术分类的不同生命周期阶段。此框架不仅整合了贡献活动、社区参与度以及维护动态等多个维度，还结合了人工设计的表格指标与长达24个月的时间活动序列数据，并使用了一个多阶段分类流程来进行生命周期阶段的区分。此外，研究中还引入了可解释的人工智能技术以增强模型预测结果的透明度。

Result: 在大量OSS仓库上进行测试后，所提出的这种方法在生命周期阶段分类上的整体准确率超过了94%。通过归因分析发现，贡献活动及相关社区特征是影响模型预测的主要信号，强调了集体参与动力学的重要性。

Conclusion: 本研究展示了一种有效的方法来理解和预测开源软件项目的生命周期阶段，这对于想要大规模评估项目组织和健康的利益相关者来说非常重要。它不仅提高了我们对于OSS项目如何随时间演变的认识，而且也突出了贡献活动和社区参与度作为关键因素的作用。

Abstract: Open Source Software (OSS) projects follow diverse lifecycle trajectories shaped by evolving patterns of contribution, coordination, and community engagement. Understanding these trajectories is essential for stakeholders seeking to assess project organization and health at scale. However, prior work has largely relied on static or aggregated metrics, such as project age or cumulative activity, providing limited insight into how OSS sustainability unfolds over time. In this paper, we propose a hierarchical predictive framework that models OSS projects as belonging to distinct lifecycle stages grounded in established socio-technical categorizations of OSS development. Rather than treating sustainability solely as project longevity, these lifecycle stages operationalize sustainability as a multidimensional construct integrating contribution activity, community participation, and maintenance dynamics. The framework combines engineered tabular indicators with 24-month temporal activity sequences and employs a multi-stage classification pipeline to distinguish lifecycle stages associated with different coordination and participation regimes. To support transparency, we incorporate explainable AI techniques to examine the relative contribution of feature categories to model predictions. Evaluated on a large corpus of OSS repositories, the proposed approach achieves over 94\% overall accuracy in lifecycle stage classification. Attribution analyses consistently identify contribution activity and community-related features as dominant signals, highlighting the central role of collective participation dynamics.

</details>


### [74] [DRAGON: Robust Classification for Very Large Collections of Software Repositories](https://arxiv.org/abs/2602.09071)
*Stefano Balla,Stefano Zacchiroli,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: DRAGON, a new repository classifier, improves the accuracy of topic classification for code repositories, even in the absence of README files, by utilizing file and directory names. It outperforms existing methods on large and diverse software collections and is accompanied by the release of a large open dataset for further research.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of current approaches in automatically classifying source code repositories, which often depend heavily on README files and other metadata that are not always available. The goal is to create a more robust and widely applicable solution for navigating or searching through extensive software collections, especially in real-world scenarios where documentation may be sparse or inconsistent.

Method: The method introduced in this paper is DRAGON, a repository classifier designed specifically for handling very large and diverse software collections. Unlike previous methods, DRAGON operates primarily using lightweight signals that are commonly found in version control systems, such as file and directory names, and only uses README files when they are available. This approach aims to improve the applicability and performance of repository classification in real-world settings, where complete documentation cannot be guaranteed.

Result: DRAGON achieves an improvement in F1@5 score from 54.8% to 60.8%, surpassing the state-of-the-art performance in repository classification. Notably, its effectiveness remains high even without README files, with only a 6% drop in performance compared to when READMEs are present. Additionally, many of the misclassifications made by DRAGON are still semantically close to the correct topics, making its predictions particularly useful for guiding search and discovery in software collections. The creation of DRAGON also led to the release of the largest open dataset for repository classification, featuring 825,000 repositories with ground-truth topics, sourced from the Software Heritage archive.

Conclusion: In conclusion, DRAGON represents a significant advancement in the automatic classification of source code repositories, offering improved accuracy and robustness in the face of missing metadata. Its ability to perform well even without README files, combined with the provision of a large open dataset, sets a strong foundation for future research in understanding and managing large-scale, language-agnostic software repositories.

Abstract: The ability to automatically classify source code repositories with ''topics'' that reflect their content and purpose is very useful, especially when navigating or searching through large software collections. However, existing approaches often rely heavily on README files and other metadata, which are frequently missing, limiting their applicability in real-world large-scale settings. We present DRAGON, a repository classifier designed for very large and diverse software collections. It operates entirely on lightweight signals commonly stored in version control systems: file and directory names, and optionally the README when available. In repository classification at scale, DRAGON improves F1@5 from 54.8% to 60.8%, surpassing the state of the art. DRAGON remains effective even when README files are absent, with performance degrading by only 6% w.r.t. when they are present. This robustness makes it practical for real-world settings where documentation is sparse or inconsistent. Furthermore, many of the remaining classification errors are near misses, where predicted labels are semantically close to the correct topics. This property increases the practical value of the predictions in real-world software collections, where suggesting a few related topics can still guide search and discovery. As a byproduct of developing DRAGON, we also release the largest open dataset to date for repository classification, consisting of 825 thousand repositories with associated ground-truth topics, sourced from the Software Heritage archive, providing a foundation for future large-scale and language-agnostic research on software repository understanding.

</details>


### [75] [Towards an OSF-based Registered Report Template for Software Engineering Controlled Experiments](https://arxiv.org/abs/2602.09292)
*Ana B. M. Bett,Thais S. Nepomuceno,Edson OliveiraJr,Maria Teresa Baldassarre,Valdemar V. Graciano Neto,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文探讨了软件工程控制实验中注册报告（RR）的使用，以提高实验的严谨性、可重复性和透明度。通过分析OSF上选定的RR模板类型，发现虽然其中一种类型与许多文档指南中的建议相一致，但没有一个完全覆盖所有指南。因此，建立针对软件工程的RR特定指南被认为是必要的。


<details>
  <summary>Details</summary>
Motivation: 经验软件工程（ESE）社区多年来一直在努力改进实验过程，但仍存在描述控制实验时缺乏严谨性的问题，这阻碍了研究的可重复性和透明度。引入注册报告（RR）机制旨在解决这些问题，通过在执行前对研究假设、方法和/或分析进行登记，并经过同行评审可能提前接受，有助于减轻如p-hacking、发表偏倚及不适当的后验分析等不良实践的影响。

Method: 研究者们基于开放科学框架(OSF)选取了几种类型的RR模板，并根据控制实验的文档指南对其进行了分析。

Result: 尽管有一种RR类型符合了许多指南中的建议，但是没有任何一种RR类型能够全面覆盖所有的指南建议。此外，还指出了OSF RR模板定制方面的局限性。

Conclusion: 即使在ESE领域取得了进展，但在规划和记录实验方面仍然缺乏足够的严谨性，这对可重复性构成了威胁。建议采用基于OSF的RRs，但同时也指出目前可用的任何RR类型都不能完全满足指南要求，强调了为软件工程制定专门RR指南的重要性。

Abstract: Context: The empirical software engineering (ESE) community has contributed to improving experimentation over the years. However, there is still a lack of rigor in describing controlled experiments, hindering reproducibility and transparency. Registered Reports (RR) have been discussed in the ESE community to address these issues. A RR registers a study's hypotheses, methods, and/or analyses before execution, involving peer review and potential acceptance before data collection. This helps mitigate problematic practices such as p-hacking, publication bias, and inappropriate post hoc analysis. Objective: This paper presents initial results toward establishing an RR template for Software Engineering controlled experiments using the Open Science Framework (OSF). Method: We analyzed templates of selected OSF RR types in light of documentation guidelines for controlled experiments. Results: The observed lack of rigor motivated our investigation of OSF-based RR types. Our analysis showed that, although one of the RR types aligned with many of the documentation suggestions contained in the guidelines, none of them covered the guidelines comprehensively. The study also highlights limitations in OSF RR template customization. Conclusion: Despite progress in ESE, planning and documenting experiments still lack rigor, compromising reproducibility. Adopting OSF-based RRs is proposed. However, no currently available RR type fully satisfies the guidelines. Establishing RR-specific guidelines for SE is deemed essential.

</details>


### [76] [Cross-Project Flakiness: A Case Study of the OpenStack Ecosystem](https://arxiv.org/abs/2602.09311)
*Tao Xiao,Dong Wang,Shane McIntosh,Hideaki Hata,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文对OpenStack生态系统中的测试波动性进行了实证研究，特别是跨项目波动性和不一致波动性。分析了649个OpenStack项目后，确定了1,535个跨项目波动测试和1,105个不一致波动测试，并发现跨项目波动影响了55%的OpenStack项目，显著增加了审查时间和计算成本。通过定性分析，观察到CI中的竞争条件、构建配置不一致以及依赖关系不匹配是导致不一致波动性的主要原因。


<details>
  <summary>Details</summary>
Motivation: 自动化回归测试是现代软件开发的基础，但一些测试存在波动性问题，即它们的结果非确定性地变化。这种波动性会侵蚀开发者对测试结果的信任，浪费计算资源，并削弱持续集成(CI)的可靠性。尽管先前的研究已经探讨了个别项目内的测试波动性，但它在整个生态系统范围内的更广泛影响仍大部分未被探索。

Method: 通过对649个OpenStack项目的分析，识别出1,535个跨项目波动测试（这些测试影响多个项目）及1,105个不一致波动测试（在某些项目中表现出波动但在其他项目中保持稳定）。采用了定量与定性相结合的方法来分析造成测试波动性的原因。

Result: 研究发现，跨项目波动影响了55%的OpenStack项目，并且显著增加了代码审查时间以及计算开销。令人惊讶的是，70%的单元测试表现出跨项目波动性，这挑战了单元测试天然隔离于模块间问题如集成或系统级测试的观点。此外，还发现了引起不一致波动性的主要原因是CI中的竞态条件、构建配置不一致以及依赖项错配等问题。

Conclusion: 研究表明，在复杂的生态系统内需要更好的协调、标准化CI配置以及改进的测试隔离策略以应对测试波动性带来的挑战。

Abstract: Automated regression testing is a cornerstone of modern software development, often contributing directly to code review and Continuous Integration (CI). Yet some tests suffer from flakiness, where their outcomes vary non-deterministically. Flakiness erodes developer trust in test results, wastes computational resources, and undermines CI reliability. While prior research has examined test flakiness within individual projects, its broader ecosystem-wide impact remains largely unexplored. In this paper, we present an empirical study of test flakiness in the OpenStack ecosystem, which focuses on (1) cross-project flakiness, where flaky tests impact multiple projects, and (2) inconsistent flakiness, where a test exhibits flakiness in some projects but remains stable in others. By analyzing 649 OpenStack projects, we identify 1,535 cross-project flaky tests and 1,105 inconsistently flaky tests. We find that cross-project flakiness affects 55% of OpenStack projects and significantly increases both review time and computational costs. Surprisingly, 70% of unit tests exhibit cross-project flakiness, challenging the assumption that unit tests are inherently insulated from issues that span modules like integration and system-level tests. Through qualitative analysis, we observe that race conditions in CI, inconsistent build configurations, and dependency mismatches are the primary causes of inconsistent flakiness. These findings underline the need for better coordination across complex ecosystems, standardized CI configurations, and improved test isolation strategies.

</details>


### [77] [Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository](https://arxiv.org/abs/2602.09467)
*Sota Nakashima,Masanari Kondo,Mahmoud Alfadel,Aly Ahmad,Toshihiro Nakae,Hidenori Matsuzaki*

Main category: cs.SE

TL;DR: 本文首次尝试建立被拒绝的贡献与相关源代码之间的可追溯性链接，使用LLM驱动的流水线实现对官方Go仓库提案的链接生成，准确率达到了0.836，平均精确度为0.643。此外，还通过失败分析明确了链接被拒提案所面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 在开源软件项目中，被接受的贡献之间的可追溯性链接已被广泛研究，但被讨论后拒绝的贡献往往被忽视。这些被拒贡献背后的讨论包含了关于软件决策制定的宝贵设计理由和隐含知识。本研究旨在填补这一空白，通过建立被拒绝贡献与其对应源代码之间的联系，揭示判断实施与否的标准。

Method: 提出了一种初步的链接方法，并利用来自官方Go仓库的提案作为数据集进行实证分析。为了将被拒绝的提案与源代码关联起来，设计了一个基于大型语言模型（LLM）的处理流程。

Result: 结果表明，该流水线能够以0.836的准确率为每个被拒绝提案选择正确的粒度，并且在此粒度上生成正确链接的平均精度达到0.643。对于未能成功生成链接的情况进行了失败原因分析，发现主要问题在于讨论内容冗余且缺乏具体信息。

Conclusion: 这项工作展示了自动建立被拒绝贡献与源代码之间可追溯性链接的可能性，并指出了当前方法面临的主要挑战。

Abstract: Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).

</details>


### [78] [JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)](https://arxiv.org/abs/2602.09930)
*Nishil Amin,Zhiwei Fei,Xiang Li,Justyna Petke,He Ye*

Main category: cs.SE

TL;DR: 本文构建了一个基准，用于评估大型语言模型在Java 8到Java 11代码迁移任务中的表现。通过一个精炼的数据集评估了Mistral Codestral模型，结果显示该模型能够处理简单的API替换但在复杂迁移上存在困难。


<details>
  <summary>Details</summary>
Motivation: 为了解决代码迁移过程中可能遇到的挑战，特别是从Java 8升级至Java 11时，研究者们希望建立一个可靠的基准来测试大型语言模型的能力。

Method: 首先收集了来自开源仓库的功能对数据集，并基于此创建了一个涵盖八类已弃用API的新数据集。使用CodeBLEU和基于关键词的度量标准对该数据集上的Mistral Codestral模型进行了评估。

Result: Mistral Codestral模型在处理直接的一对一API替换方面取得了一定成功（11.11%的情况下实现了完全相同的迁移），但对于像CORBA或JAX-WS这样的复杂迁移则表现不佳。

Conclusion: 尽管Mistral Codestral模型可以部分减轻开发人员的工作负担，特别是在自动化重复性迁移任务方面，但它还不能完全替代人类完成JMigBench基准范围内的工作。

Abstract: We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.

</details>


### [79] [QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs](https://arxiv.org/abs/2602.09942)
*Junjie Luo,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了一种新的量子软件栈测试方法QEMI，通过生成包含死代码的随机量子程序并运用改编自经典编译器测试的EMI技术来移除这些死代码以创建变体，以此比较变体行为来检测潜在bug。该方法在Qiskit、Q#和Cirq上应用时成功识别了11个崩溃错误和1个行为不一致问题。


<details>
  <summary>Details</summary>
Motivation: 随着量子算法和硬件的发展，确保量子软件栈（QSS）的正确性变得越来越重要。然而，由于oracle问题的存在，即缺乏可靠的基础真值来预期程序的行为，测试QSS仍然充满挑战。现有的元态测试方法通常依赖于等效电路转换、后端修改或参数调整来解决这个问题。

Method: 受到Equivalence Modulo Inputs (EMI)的启发，提出了Quantum EMI (QEMI)，一种新的针对QSS的测试方法。主要贡献包括：(1) 一个基于量子控制流结构产生含有死代码的随机量子程序生成器；(2) 将EMI技术从经典编译器测试中适应过来，通过移除死代码的方式生成变体。通过对比这些变体的行为，可以检测到QSS实现中的潜在错误。

Result: 将QEMI应用于Qiskit, Q#, 和 Cirq，并成功地识别出了11个崩溃错误和1个行为不一致性问题。

Conclusion: QEMI扩展了可用于量子软件堆栈的有限测试技术集，超越了结构转换并将保留语义的方法纳入量子程序分析之中。

Abstract: As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.

</details>


### [80] [Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents](https://arxiv.org/abs/2602.09944)
*Xiang Li,Zhiwei Fei,Ying Ma,Jerry Zhang,Sarro Federica,He Ye*

Main category: cs.SE

TL;DR: 该论文探讨了在代码迁移过程中自动化环境交互的重要性，提出了一种将自动化环境设置与代码迁移工作流紧密结合的新框架范式，并讨论了面临的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有的研究大多集中在代码迁移任务本身，如重构、API适配及依赖项更新上，而忽视了伴随其进行的环境迁移自动化问题。代码与其运行环境紧密相关，仅依靠静态环境分析会导致对目标环境理解不足、反馈周期延长以及项目效率降低等问题。

Method: 首先概述了当前自动化环境构建的状态，然后提出了一种新的框架范式，该范式将自动化环境搭建与代码迁移流程紧密结合。

Result: 研究发现，若缺乏自动化环境交互，则代码迁移的自动化只能算完成了一半；强调了软件演进需要一个综合视角来整合代码和环境迁移。

Conclusion: 成功的软件演化不仅需要关注代码本身的迁移，还需要重视配套环境的自动化迁移。本文提出的新框架为实现这一目标提供了一条可行路径，并指出了领域内未来的研究方向。

Abstract: Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [81] [Query-Mixed Interest Extraction and Heterogeneous Interaction: A Scalable CTR Model for Industrial Recommender Systems](https://arxiv.org/abs/2602.09387)
*Fangye Wang,Guowei Yang,Xiaojiang Zhou,Song Yang,Pengjie Wang*

Main category: cs.IR

TL;DR: HeMix, a scalable ranking model, unifies adaptive sequence tokenization and heterogeneous interaction structure to better handle sparse multi-field inputs and long user behavior sequences, outperforming strong baselines and delivering significant online gains when deployed on the AMAP platform.


<details>
  <summary>Details</summary>
Motivation: The main motivation behind HeMix is to address the challenges faced by modern recommender systems in industrial settings, such as dealing with sparse multi-field inputs and ultra-long user behavior sequences. Existing methods often struggle to construct both context-aware and context-independent user intent from these sequences and suffer from inefficient, homogeneous interaction mechanisms that limit their prediction performance.

Method: HeMix introduces a Query-Mixed Interest Extraction module for modeling both types of user interests using dynamic and fixed queries over global and real-time behavior sequences. It also replaces self-attention with the HeteroMixer block, which facilitates efficient, multi-granularity cross-feature interactions through multi-head token fusion, heterogeneous interaction, and group-aligned reconstruction pipelines.

Result: Experiments on industrial-scale datasets demonstrate that HeMix scales effectively and consistently outperforms strong baselines. Its deployment on the AMAP platform has resulted in notable online improvements, including a 0.61% increase in GMV, a 2.32% rise in PV_CTR, and a 0.81% boost in UV_CVR.

Conclusion: By unifying adaptive sequence tokenization and heterogeneous interaction structure, HeMix provides an effective solution for improving recommendation accuracy in the face of sparse data and long user behavior sequences, showcasing its practical value through successful real-world application.

Abstract: Learning effective feature interactions is central to modern recommender systems, yet remains challenging in industrial settings due to sparse multi-field inputs and ultra-long user behavior sequences. While recent scaling efforts have improved model capacity, they often fail to construct both context-aware and context-independent user intent from the long-term and real-time behavior sequence. Meanwhile, recent work also suffers from inefficient and homogeneous interaction mechanisms, leading to suboptimal prediction performance. To address these limitations, we propose HeMix, a scalable ranking model that unifies adaptive sequence tokenization and heterogeneous interaction structure. Specifically, HeMix introduces a Query-Mixed Interest Extraction module that jointly models context-aware and context-independent user interests via dynamic and fixed queries over global and real-time behavior sequences. For interaction, we replace self-attention with the HeteroMixer block, enabling efficient, multi-granularity cross-feature interactions that adopt the multi-head token fusion, heterogeneous interaction and group-aligned reconstruction pipelines. HeMix demonstrates favorable scaling behavior, driven by the HeteroMixer block, where increasing model scale via parameter expansion leads to steady improvements in recommendation accuracy. Experiments on industrial-scale datasets show that HeMix scales effectively and consistently outperforms strong baselines. Most importantly, HeMix has been deployed on the AMAP platform, delivering significant online gains: +0.61% GMV, +2.32% PV_CTR, and +0.81% UV_CVR.

</details>


### [82] [SARM: LLM-Augmented Semantic Anchor for End-to-End Live-Streaming Ranking](https://arxiv.org/abs/2602.09401)
*Ruochen Yang,Yueyang Liu,Zijie Zhuang,Changxin Lao,Yuhui Zhang,Jiangxia Cao,Jia Xu,Xiang Chen,Haoke Xiao,Xiangyu Wu,Xiaoyou Zhou,Xiao Lv,Shuang Yang,Tingwen Liu,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出了一种名为SARM的端到端排名架构，它直接将自然语言语义锚点整合进排名优化中，以解决大规模直播推荐中的非静态内容语义建模问题。通过学习文本标记与排名特征联合优化，该模型能够适应内容描述以符合排名目标，并且已经全面部署服务于超过4亿日活跃用户。


<details>
  <summary>Details</summary>
Motivation: 在工业应用中，对于大规模直播推荐，现有的两种常见方法存在根本性限制：离散语义抽象通过对聚类牺牲了描述精度；而密集多模态嵌入独立提取并且与排名优化弱对齐，这限制了细粒度的内容感知排名。为了克服这些限制，研究者们提出了新的解决方案。

Method: 研究者们提出了一种名为SARM（Semantic Anchor based Ranking Model）的新方法，这是一种端到端的排名架构，它直接把自然语言语义锚点整合进了排名优化过程之中。每个语义锚点都被表示为可学习的文字令牌，与排名特征一起进行优化，这样可以让模型根据排名目的调整内容描述。此外，还采用了轻量级双令牌门控设计来捕捉特定领域的直播语义，以及一种不对称部署策略保证在线训练和服务的低延迟。

Result: 广泛的离线评估和大规模A/B测试表明，相较于生产基线，SARM展现出了持续性的改进。

Conclusion: SARM已被全面部署并每天为超过4亿用户提供服务，证明了其在提高大规模直播推荐系统性能方面的有效性。

Abstract: Large-scale live-streaming recommendation requires precise modeling of non-stationary content semantics under strict real-time serving constraints. In industrial deployment, two common approaches exhibit fundamental limitations: discrete semantic abstractions sacrifice descriptive precision through clustering, while dense multimodal embeddings are extracted independently and remain weakly aligned with ranking optimization, limiting fine-grained content-aware ranking. To address these limitations, we propose \textbf{SARM}, an end-to-end ranking architecture that integrates natural-language semantic anchors directly into ranking optimization, enabling fine-grained author representations conditioned on multimodal content. Each semantic anchor is represented as learnable text tokens jointly optimized with ranking features, allowing the model to adapt content descriptions to ranking objectives. A lightweight dual-token gated design captures domain-specific live-streaming semantics, while an asymmetric deployment strategy preserves low-latency online training and serving. Extensive offline evaluation and large-scale A/B tests show consistent improvements over production baselines. SARM is fully deployed and serves over 400 million users daily.

</details>


### [83] [Personalized Parameter-Efficient Fine-Tuning of Foundation Models for Multimodal Recommendation](https://arxiv.org/abs/2602.09445)
*Sunwoo Kim,Hyunjin Hwang,Kijung Shin*

Main category: cs.IR

TL;DR: 本文提出了一种名为PerPEFT的个性化参数高效微调策略，用于多模态推荐系统。通过根据用户兴趣分组并为每个组分配一个独特的PEFT模块，PerPEFT能够捕捉对各组购买决策最具有预测性的细粒度项目特征。实验表明，该方法相较于最强基线提高了最多15.3%（NDCG@20），并且在不同的PEFT变体上保持了一致的表现提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型虽然可以通过参数高效微调(PEFT)来适应推荐任务，但生成的项目嵌入并不考虑用户的兴趣差异，这意味着不同兴趣的用户关注项目的方面并未被充分考虑到。

Method: 提出了PerPEFT，一种针对多模态推荐的个性化PEFT策略。该方法首先依据用户兴趣进行分群，并为每个群体配备一个专门的PEFT模块以捕捉最能反映该群体购买偏好的项目细节。此外，还引入了特定训练技术以增强这种基于用户群体的调节效果。值得注意的是，PerPEFT不受限于特定PEFT方法，可以与适用于多模态基础模型的各种PEFT方案结合使用。

Result: 广泛的实验证明了PerPEFT的有效性：它比最强基线提高了高达15.3% (NDCG@20)，并且在多种PEFT变体中均展现出一致的优势。即使加入了个性化元素，整个PEFT过程仍然保持轻量级，仅增加了基础模型参数数量的1.3%。

Conclusion: 通过引入PerPEFT这一新颖的方法论，研究成功地解决了现有基于多模态元数据的推荐系统中未能充分考虑用户个人兴趣的问题，从而显著提升了推荐性能。

Abstract: In recent years, substantial research has integrated multimodal item metadata into recommender systems, often by using pre-trained multimodal foundation models to encode such data. Since these models are not originally trained for recommendation tasks, recent works efficiently adapt them via parameter-efficient fine-tuning (PEFT). However, even with PEFT, item embeddings from multimodal foundation models remain user-blind: item embeddings are not conditioned on user interests, despite the fact that users with diverse interests attend to different item aspects. To address this limitation, we propose PerPEFT, a personalized PEFT strategy for multimodal recommendation. Specifically, PerPEFT groups users by interest and assigns a distinct PEFT module to each group, enabling each module to capture the fine-grained item aspects most predictive of that group`s purchase decisions. We further introduce a specialized training technique that strengthens this user-group conditioning. Notably, PerPEFT is PEFT-agnostic and can be paired with any PEFT method applicable to multimodal foundation models. Through extensive experiments, we show that (1) PerPEFT outperforms the strongest baseline by up to 15.3% (NDCG@20) and (2) delivers consistent gains across diverse PEFT variants. It is noteworthy that, even with personalization, PEFT remains lightweight, adding only 1.3% of the parameter count of the foundation model. We provide our code and datasets at https://github.com/kswoo97/PerPEFT.

</details>


### [84] [The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training](https://arxiv.org/abs/2602.09448)
*Xincan Feng,Noriki Nishida,Yusuke Sakai,Yuji Matsumoto*

Main category: cs.IR

TL;DR: 该论文解决了合成数据生成中查询多样性对密集检索影响的矛盾结果问题，通过设计Q-D指标量化了多样性的影响，并提出了复杂度-多样性原则（CDP），据此原则开发了一种零样本多查询合成方法，在多跳任务上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 先前的研究对于在密集检索的合成数据生成中查询多样性的影响报告了相互矛盾的结果。本文旨在解决这种冲突，并提供一个可以衡量的方法来评估多样性的影响。

Method: 作者设计了Q-D指标来量化多样性的影响，并通过对4种基准类型（31个数据集）进行实验，特别是深入分析了多跳检索数据，发现查询复杂性与多样性的好处之间存在强相关性。基于此，他们提出了复杂度-多样性原则(CDP)，并依据这一原则提出了一种针对多跳任务的零样本多查询合成方法。

Result: 研究结果显示，查询多样性特别有利于多跳检索。此外，根据提出的复杂度-多样性原则(CDP)，当内容词数量大于10时建议使用多样性；而当内容词少于7时则应避免使用。新提出的零样本多查询合成方法在多跳任务上实现了最先进的性能。

Conclusion: 本研究表明，通过引入复杂度-多样性原则(CDP)能够有效指导何时以及如何利用查询多样性以优化密集检索任务中的合成数据生成过程。

Abstract: Prior work reports conflicting results on query diversity in synthetic data generation for dense retrieval. We identify this conflict and design Q-D metrics to quantify diversity's impact, making the problem measurable. Through experiments on 4 benchmark types (31 datasets), we find query diversity especially benefits multi-hop retrieval. Deep analysis on multi-hop data reveals that diversity benefit correlates strongly with query complexity ($r$$\geq$0.95, $p$$<$0.05 in 12/14 conditions), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides actionable thresholds (CW$>$10: use diversity; CW$<$7: avoid it). Guided by CDP, we propose zero-shot multi-query synthesis for multi-hop tasks, achieving state-of-the-art performance.

</details>


### [85] [With Argus Eyes: Assessing Retrieval Gaps via Uncertainty Scoring to Detect and Remedy Retrieval Blind Spots](https://arxiv.org/abs/2602.09616)
*Zeinab Sadat Taghavi,Ali Modarressi,Hinrich Schutze,Andreas Marfurt*

Main category: cs.IR

TL;DR: 研究揭示了神经检索器在RAG系统中的盲点问题，即无法检索到与查询相关但与查询嵌入相似度低的实体。通过分析训练导致的偏差以及引入新的评分方法RPS，研究人员能够预测这些盲点，并提出了ARGUS管道来增强高风险实体的可检索性。实验表明，ARGUS能显著提高不同检索器的表现，特别是在挑战性子集上。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决可靠检索增强生成（RAG）系统中神经检索器存在的盲点问题，这些问题导致了对某些相关但低相似度实体的检索失败，从而影响了系统的整体性能和可靠性。

Method: 1. 定义并识别出由于训练偏差造成的盲点实体。
2. 使用从Wikidata关系及Wikipedia首段构建的大规模数据集，结合提出的检索概率得分(RPS)评估盲点风险。
3. 开发ARGUS管道，通过对知识库中的文档进行针对性扩充来改善高风险(低RPS)实体的检索能力。

Result: ARGUS在多个基准测试（BRIGHT, IMPLIRET, RAR-B）上实现了对所有评估检索器的一致改进，平均nDCG@5增加3.4点，nDCG@10增加4.5点，在更具挑战性的子集中收益更大。这证明了提前修复盲点对于构建稳健且值得信赖的RAG系统的重要性。

Conclusion: 研究发现，通过预先识别并修正由训练引起的检索器盲点，可以显著提升RAG系统的性能与可信度。ARGUS作为一种有效的方法论被提出，它通过目标文档增强策略提高了难以检索实体的可见性，为未来开发更加强大可靠的RAG应用提供了重要参考。

Abstract: Reliable retrieval-augmented generation (RAG) systems depend fundamentally on the retriever's ability to find relevant information. We show that neural retrievers used in RAG systems have blind spots, which we define as the failure to retrieve entities that are relevant to the query, but have low similarity to the query embedding. We investigate the training-induced biases that cause such blind spot entities to be mapped to inaccessible parts of the embedding space, resulting in low retrievability. Using a large-scale dataset constructed from Wikidata relations and first paragraphs of Wikipedia, and our proposed Retrieval Probability Score (RPS), we show that blind spot risk in standard retrievers (e.g., CONTRIEVER, REASONIR) can be predicted pre-index from entity embedding geometry, avoiding expensive retrieval evaluations. To address these blind spots, we introduce ARGUS, a pipeline that enables the retrievability of high-risk (low-RPS) entities through targeted document augmentation from a knowledge base (KB), first paragraphs of Wikipedia, in our case. Extensive experiments on BRIGHT, IMPLIRET, and RAR-B show that ARGUS achieves consistent improvements across all evaluated retrievers (averaging +3.4 nDCG@5 and +4.5 nDCG@10 absolute points), with substantially larger gains in challenging subsets. These results establish that preemptively remedying blind spots is critical for building robust and trustworthy RAG systems (Code and Data).

</details>


### [86] [Efficient Learning of Sparse Representations from Interactions](https://arxiv.org/abs/2602.09935)
*Vojtěch Vančura,Martin Spišák,Rodrigo Alves,Ladislav Peška*

Main category: cs.IR

TL;DR: 本文提出了一种训练策略，通过学习高维稀疏嵌入层来替代传统的密集型嵌入层，以平衡效率、表示表达性和可解释性。在修改了生产级别的协同过滤自动编码器ELSA后，实现了高达10倍的嵌入大小减少而没有推荐准确性的损失，甚至可以达到100倍减少仅损失2.5%准确性。此外，活跃的嵌入维度揭示了一个可解释的倒排索引结构，能够直接与模型的潜在空间对齐，从而允许在候选检索模型中集成段级推荐功能（例如2D主页布局）。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统初期检索阶段，实践者面临嵌入表达性和服务组件的可扩展性及延迟之间的固有折衷问题，需要同时紧凑且富有表现力的表示方法。为解决这一挑战，研究提出了新的训练策略。

Method: 提出了一种用于学习高维稀疏嵌入层的训练策略，替换了传统密集型嵌入层，并通过对生产级别协同过滤自动编码器ELSA进行修改来展示该方法的有效性。

Result: 实现了高达10倍的嵌入大小减少而没有推荐准确性的损失，甚至可以达到100倍减少仅损失2.5%准确性。此外，还发现活跃的嵌入维度形成了一个可以直接与模型潜在空间对齐的可解释倒排索引结构。

Conclusion: 新提出的训练策略能够在保持或仅有轻微损失推荐准确性的同时大幅减小嵌入大小，提供了一种有效的方法来提高推荐系统的效率和可解释性。

Abstract: Behavioral patterns captured in embeddings learned from interaction data are pivotal across various stages of production recommender systems. However, in the initial retrieval stage, practitioners face an inherent tradeoff between embedding expressiveness and the scalability and latency of serving components, resulting in the need for representations that are both compact and expressive. To address this challenge, we propose a training strategy for learning high-dimensional sparse embedding layers in place of conventional dense ones, balancing efficiency, representational expressiveness, and interpretability. To demonstrate our approach, we modified the production-grade collaborative filtering autoencoder ELSA, achieving up to 10x reduction in embedding size with no loss of recommendation accuracy, and up to 100x reduction with only a 2.5% loss. Moreover, the active embedding dimensions reveal an interpretable inverted-index structure that segments items in a way directly aligned with the model's latent space, thereby enabling integration of segment-level recommendation functionality (e.g., 2D homepage layouts) within the candidate retrieval model itself. Source codes, additional results, as well as a live demo are available at https://github.com/zombak79/compressed_elsa

</details>


### [87] [Kunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design](https://arxiv.org/abs/2602.10016)
*Bojian Hou,Xiaolong Liu,Xiaoyi Liu,Jiaqi Xu,Yasmine Badr,Mengyue Hang,Sudhanshu Chanpuriya,Junqing Zhou,Yuhang Yang,Han Xu,Qiuling Suo,Laming Chen,Yuxi Hu,Jiasheng Zhang,Huaqing Xiong,Yuzhen Huang,Chao Chen,Yue Dong,Yi Yang,Shuo Chang,Xiaorui Gan,Wenlin Chen,Santanu Kolay,Darren Liu,Jade Nie,Chunzhi Yang,Jiyan Yang,Huayu Li*

Main category: cs.IR

TL;DR: 本文介绍了Kunlun，一种可扩展架构，通过一系列优化提高了推荐系统的模型效率和资源分配，从而显著提升了生产环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 针对大规模推荐系统中模型性能与计算资源投资之间关系的可预测性问题，尤其是处理用户历史和上下文特征时存在的挑战，提出了改进方案。

Method: 提出了Kunlun架构，包括低级优化如广义点积注意力（GDPA）、分层种子池化（HSP）以及滑动窗口注意力机制；高级创新包括计算跳过（CompSkip）和个人事件级别个性化。

Result: 这些改进措施使得在NVIDIA B200 GPU上的MFU从17%提升到了37%，相比现有最先进方法将扩展效率提高了一倍，并已在Meta Ads的主要模型中部署使用。

Conclusion: 通过引入Kunlun架构及其相关优化技术，成功解决了推荐系统中因模块效率低下及资源分配不佳导致的可扩展性差的问题，为设计和资源配置提供了新的指导方向。

Abstract: Deriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify poor scaling efficiency as the main barrier to predictable power-law scaling, stemming from inefficient modules with low Model FLOPs Utilization (MFU) and suboptimal resource allocation. We introduce Kunlun, a scalable architecture that systematically improves model efficiency and resource allocation. Our low-level optimizations include Generalized Dot-Product Attention (GDPA), Hierarchical Seed Pooling (HSP), and Sliding Window Attention. Our high-level innovations feature Computation Skip (CompSkip) and Event-level Personalization. These advances increase MFU from 17% to 37% on NVIDIA B200 GPUs and double scaling efficiency over state-of-the-art methods. Kunlun is now deployed in major Meta Ads models, delivering significant production impact.

</details>


### [88] [Overview of the TREC 2025 RAGTIME Track](https://arxiv.org/abs/2602.10024)
*Dawn Lawrie,Sean MacAvaney,James Mayfield,Luca Soldaini,Eugene Yang,Andrew Yates*

Main category: cs.IR

TL;DR: RAGTIME track at TREC focuses on studying report generation from multilingual source documents, with tasks including Multilingual Report Generation, English Report Generation, and MLIR. 125 runs were submitted by 13 teams for evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the RAGTIME track is to advance the field of multilingual information processing, specifically in the area of generating reports from sources in multiple languages, which is a critical need in today's globalized world.

Method: The method involves creating a document collection in four languages (Arabic, Chinese, English, and Russian) and setting up three different tasks for participants to generate reports or retrieve information. The performance of the submissions is then evaluated based on predefined criteria.

Result: A total of 125 runs were submitted across three tasks by 13 teams, providing a comprehensive set of results that can be used to assess the current state of the art in multilingual report generation and information retrieval.

Conclusion: The RAGTIME track at TREC has successfully provided a platform for evaluating and advancing techniques in multilingual report generation and information retrieval, contributing to the development of more effective cross-lingual information processing systems.

Abstract: The principal goal of the RAG TREC Instrument for Multilingual Evaluation (RAGTIME) track at TREC is to study report generation from multilingual source documents. The track has created a document collection containing Arabic, Chinese, English, and Russian news stories. RAGTIME includes three task types: Multilingual Report Generation, English Report Generation, and Multilingual Information Retrieval (MLIR). A total of 125 runs were submitted by 13 participating teams (and as baselines by the track coordinators) for three tasks. This overview describes these three tasks and presents the available results.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [89] [Scaling GraphLLM with Bilevel-Optimized Sparse Querying](https://arxiv.org/abs/2602.09038)
*Yangzhe Peng,Haiquan Qiu,Quanming Yao,Kun He*

Main category: cs.DB

TL;DR: 本文提出了一种名为BOSQ的框架，该框架通过选择性地利用LLM生成的解释特征来增强文本属性图(TAGs)上的节点级任务性能，并设计了一种自适应稀疏查询策略以减少计算开销。实验结果表明，与现有的GraphLLM方法相比，BOSQ在保持相同或更优性能的同时实现了数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLMs)在通过提供解释特征来改进文本属性图(TAGs)上的节点级任务方面展现出巨大潜力，但其实用性受到高计算成本和经济成本的严重限制。为了降低这些成本，同时保持甚至提高性能，作者提出了BOSQ框架。

Method: BOSQ采用双层优化稀疏查询方法，通过自适应地决定何时调用LLM来避免冗余或低收益查询，从而显著减少了计算负担。这种方法旨在选择性地利用由LLM生成的解释特征来增强TAGs上节点级任务的表现。

Result: 在六个真实世界的TAG数据集上进行的广泛实验显示，BOSQ相对于现有GraphLLM方法而言，在处理速度上有数量级的提升，同时在两类节点级任务中保持了相当或更好的性能表现。

Conclusion: BOSQ为有效利用LLM于文本属性图(TAGs)中的节点级任务提供了一个新的解决方案，通过智能选择性查询策略大幅降低了计算成本，同时保持了高水平的任务执行效率。

Abstract: LLMs have recently shown strong potential in enhancing node-level tasks on text-attributed graphs (TAGs) by providing explanation features. However, their practical use is severely limited by the high computational and monetary cost of repeated LLM queries. To illustrate, naively generating explanations for all nodes on a medium-sized benchmark like Photo (48k nodes) using a representative method (e.g., TAPE) would consume days of processing time. In this paper, we propose Bilevel-Optimized Sparse Querying (BOSQ), a general framework that selectively leverages LLM-derived explanation features to enhance performance on node-level tasks on TAGs. We design an adaptive sparse querying strategy that selectively decides when to invoke LLMs, avoiding redundant or low-gain queries and significantly reducing computation overhead. Extensive experiments on six real-world TAG datasets involving two types of node-level tasks demonstrate that BOSQ achieves orders of magnitude speedups over existing GraphLLM methods while consistently delivering on-par or superior performance.

</details>


### [90] [Efficient Distance Pruning for Process Suffix Comparison in Prescriptive Process Monitoring](https://arxiv.org/abs/2602.09039)
*Sarra Madad*

Main category: cs.DB

TL;DR: 本文提出了一种基于三角不等式的高效检索方法，通过距离优化的中心点来定义边界，从而剪枝冗余比较，以加速后缀比较并支持可扩展的规范系统。


<details>
  <summary>Details</summary>
Motivation: 描述性过程监控旨在通过分析正在进行的案例的可能延续来推荐改善过程结果的动作。一个主要障碍是大规模后缀比较的沉重计算成本，随着日志大小的增长而迅速增加。

Method: 提出了一种利用三角不等式的高效检索方法：到一组优化中心点的距离定义了能够剪枝掉冗余比较的边界。

Result: 这种方法显著减少了运行时间，并且可以完全并行化。重要的是，这种剪枝是精确的：检索到的后缀与详尽比较得到的结果完全相同，因此保持了准确性。

Conclusion: 结果显示，基于度量的剪枝可以加速后缀比较，并支持可扩展的规范系统。

Abstract: Prescriptive process monitoring seeks to recommend actions that improve process outcomes by analyzing possible continuations of ongoing cases. A key obstacle is the heavy computational cost of large-scale suffix comparisons, which grows rapidly with log size. We propose an efficient retrieval method exploiting the triangle inequality: distances to a set of optimized pivots define bounds that prune redundant comparisons. This substantially reduces runtime and is fully parallelizable. Crucially, pruning is exact: the retrieved suffixes are identical to those from exhaustive comparison, thereby preserving accuracy. These results show that metric-based pruning can accelerate suffix comparison and support scalable prescriptive systems.

</details>


### [91] [Predictive Query Language: A Domain-Specific Language for Predictive Modeling on Relational Databases](https://arxiv.org/abs/2602.09572)
*Vid Kocijan,Jinu Sunil,Jan Eric Lenssen,Viman Deb,Xinwei Xe,Federco Reyes Gomez,Matthias Fey,Jure Leskovec*

Main category: cs.DB

TL;DR: 介绍了一种新的声明式语言PQL，它受到SQL启发，旨在简化从关系数据库中提取训练示例的过程，以支持各种机器学习任务。PQL已经在多种场景下成功应用，包括金融欺诈检测、物品推荐和工作负载预测等。


<details>
  <summary>Details</summary>
Motivation: 在对关系数据进行预测建模时，需要手动从数据库中提取所需的训练样本，这个过程既耗时又容易出错。为了简化这一流程并提高效率，提出了Predictive Query Language (PQL)。

Method: 开发了Predictive Query Language (PQL)，一种受SQL启发的声明性语言，用于在关系数据库上定义预测任务。通过单个声明式查询指定预测任务，PQL能够为回归、分类、时间序列预测及推荐系统等多种机器学习任务自动生成训练标签。

Result: PQL已被成功集成到一个预测性AI平台并在多个使用案例中得到应用，包括但不限于金融欺诈检测、商品推荐以及工作负载预测等领域。此外，还展示了两种实现方式：一种适用于小规模低延迟场景；另一种则能处理大规模数据库。

Conclusion: PQL作为一种新型的声明式语言，显著减少了准备机器学习模型所需训练数据的手动工作量，并且已经被证明可以在多种实际应用场景下有效工作。

Abstract: The purpose of predictive modeling on relational data is to predict future or missing values in a relational database, for example, future purchases of a user, risk of readmission of the patient, or the likelihood that a financial transaction is fraudulent. Typically powered by machine learning methods, predictive models are used in recommendations, financial fraud detection, supply chain optimization, and other systems, providing billions of predictions every day. However, training a machine learning model requires manual work to extract the required training examples - prediction entities and target labels - from the database, which is slow, laborious, and prone to mistakes. Here, we present the Predictive Query Language (PQL), a SQL-inspired declarative language for defining predictive tasks on relational databases. PQL allows specifying a predictive task in a single declarative query, enabling the automatic computation training labels for a large variety of machine learning tasks, such as regression, classification, time-series forecasting, and recommender systems. PQL is already successfully integrated and used in a collection of use cases as part of a predictive AI platform. The versatility of the language can be demonstrated through its many ongoing use cases, including financial fraud, item recommendations, and workload prediction. We demonstrate its versatile design through two implementations; one for small-scale, low-latency use and one that can handle large-scale databases.

</details>


### [92] [Optimal Bounds-Only Pruning for Spatial AkNN Joins](https://arxiv.org/abs/2602.10027)
*Dominik Winecki*

Main category: cs.DB

TL;DR: 提出了一种仅基于边界修剪测试的方法，用于分区空间数据集上的精确欧几里得AkNN连接。通过构建边界并提前将连接评估定位到几个分区，从而加速处理过程而不必加载所有数据来建立空间索引。


<details>
  <summary>Details</summary>
Motivation: 为了加速在不维护索引的大表上进行的AkNN连接查询，利用已有的行组统计数据（如边界信息）来减少不必要的分区加载和计算。

Method: 设计了一个三边界邻近性测试方法，用来判断一个分区内所有点是否都比另一个可能被遮挡的分区内存在更近的邻居。这种方法能够更好地捕捉方向性语义，从而有效跳过不需要的分区。

Result: 提出的算法既是最优的也是高效的，能够在AkNN连接的最早阶段就识别出可以跳过的分区。

Conclusion: 新提出的三边界测试方法为使用分区统计信息加速AkNN连接提供了有效手段，展示了其在提高查询效率方面的潜力。

Abstract: We propose a bounds-only pruning test for exact Euclidean AkNN joins on partitioned spatial datasets. Data warehouses commonly partition large tables and store row group statistics for them to accelerate searches and joins, rather than maintaining indexes. AkNN joins can benefit from such statistics by constructing bounds and localizing join evaluations to a few partitions before loading them to build spatial indexes. Existing pruning methods are overly conservative for bounds-only spatial data because they do not fully capture its directional semantics, thereby missing opportunities to skip unneeded partitions at the earliest stages of a join. We propose a three-bound proximity test to determine whether all points within a partition have a closer neighbor in one partition than in another, potentially occluded partition. We show that our algorithm is both optimal and efficient.

</details>
