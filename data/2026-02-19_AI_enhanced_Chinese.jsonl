{"id": "2602.16585", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16585", "abs": "https://arxiv.org/abs/2602.16585", "authors": ["Dimitri Yatsenko", "Thinh T. Nguyen"], "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows", "comment": "20 pages, 2 figures, 1 table", "summary": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.", "AI": {"tldr": "DataJoint 2.0 introduces a relational workflow model and four technical innovations to unify data structure, data, and computational transformations, supporting robust human-agent collaboration in scientific workflows.", "motivation": "The need for operational rigor in human-agent collaboration is highlighted, with the observation that current approaches lack a unified system (SciOps) for managing scientific data pipelines, leading to fragmented provenance without transactional guarantees.", "method": "DataJoint 2.0 employs a relational workflow model where tables represent steps, rows are artifacts, and foreign keys dictate execution order. It also integrates four key innovations: object-augmented schemas, semantic matching, an extensible type system, and distributed job coordination.", "result": "By unifying all aspects of data and computation into a single, queryable, and machine-readable system, DataJoint 2.0 provides a robust foundation for SciOps, enabling safe and efficient participation of agents in scientific workflows.", "conclusion": "DataJoint 2.0 addresses the gap in current scientific data management practices by offering a comprehensive solution for maintaining data integrity and facilitating effective human-agent collaboration within scientific projects."}}
{"id": "2602.16161", "categories": ["cs.MM", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16161", "abs": "https://arxiv.org/abs/2602.16161", "authors": ["Rong Fu", "Ziming Wang", "Shuo Yin", "Wenxin Zhang", "Haiyun Wei", "Kun Liu", "Xianda Li", "Zeli Su", "Simon Fong"], "title": "Emotion Collider: Dual Hyperbolic Mirror Manifolds for Sentiment Recovery via Anti Emotion Reflection", "comment": "25 pages, 14 figures", "summary": "Emotional expression underpins natural communication and effective human-computer interaction. We present Emotion Collider (EC-Net), a hyperbolic hypergraph framework for multimodal emotion and sentiment modeling. EC-Net represents modality hierarchies using Poincare-ball embeddings and performs fusion through a hypergraph mechanism that passes messages bidirectionally between nodes and hyperedges. To sharpen class separation, contrastive learning is formulated in hyperbolic space with decoupled radial and angular objectives. High-order semantic relations across time steps and modalities are preserved via adaptive hyperedge construction. Empirical results on standard multimodal emotion benchmarks show that EC-Net produces robust, semantically coherent representations and consistently improves accuracy, particularly when modalities are partially available or contaminated by noise. These findings indicate that explicit hierarchical geometry combined with hypergraph fusion is effective for resilient multimodal affect understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEmotion Collider (EC-Net)\u7684\u53cc\u66f2\u8d85\u56fe\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u7eea\u548c\u60c5\u611f\u5efa\u6a21\u3002\u8be5\u6846\u67b6\u901a\u8fc7Poincare\u7403\u5d4c\u5165\u8868\u793a\u6a21\u6001\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u4e00\u79cd\u80fd\u591f\u5728\u8282\u70b9\u4e0e\u8d85\u8fb9\u4e4b\u95f4\u53cc\u5411\u4f20\u9012\u4fe1\u606f\u7684\u8d85\u56fe\u673a\u5236\u6267\u884c\u878d\u5408\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEC-Net\u80fd\u591f\u4ea7\u751f\u9c81\u68d2\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u8868\u73b0\uff0c\u5e76\u5728\u6807\u51c6\u591a\u6a21\u6001\u60c5\u7eea\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u6a21\u6001\u90e8\u5206\u53ef\u7528\u6216\u53d7\u5230\u566a\u58f0\u6c61\u67d3\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u81ea\u7136\u4ea4\u6d41\u4e2d\u7684\u60c5\u611f\u8868\u8fbe\u7684\u6a21\u578b\u6765\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u63d0\u51fa\u4e86Emotion Collider (EC-Net)\uff0c\u5b83\u5229\u7528\u4e86\u663e\u5f0f\u7684\u5c42\u7ea7\u51e0\u4f55\u7ed3\u6784\u7ed3\u5408\u8d85\u56fe\u878d\u5408\u6280\u672f\u4ee5\u589e\u5f3a\u5bf9\u591a\u6a21\u6001\u60c5\u611f\u7684\u7406\u89e3\u3002", "method": "EC-Net\u91c7\u7528Poincare\u7403\u5d4c\u5165\u6765\u8868\u793a\u6a21\u6001\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u5141\u8bb8\u6d88\u606f\u5728\u8282\u70b9\u548c\u8d85\u8fb9\u4e4b\u95f4\u53cc\u5411\u6d41\u52a8\u7684\u8d85\u56fe\u673a\u5236\u8fdb\u884c\u4fe1\u606f\u878d\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u53cc\u66f2\u7a7a\u95f4\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u8026\u5f84\u5411\u548c\u89d2\u76ee\u6807\u7684\u65b9\u5f0f\u52a0\u5f3a\u7c7b\u522b\u533a\u5206\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u591a\u6a21\u6001\u60c5\u7eea\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86EC-Net\u53ef\u4ee5\u751f\u6210\u9c81\u68d2\u6027\u5f3a\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u8868\u73b0\u5f62\u5f0f\uff0c\u5e76\u4e14\u5373\u4f7f\u5728\u6a21\u6001\u4e0d\u5b8c\u5168\u6216\u5b58\u5728\u566a\u58f0\u5e72\u6270\u65f6\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u660e\u786e\u7684\u5c42\u7ea7\u51e0\u4f55\u5b66\u4e0e\u8d85\u56fe\u878d\u5408\u76f8\u7ed3\u5408\u5bf9\u4e8e\u5efa\u7acb\u7a33\u5065\u7684\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u7cfb\u7edf\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2602.16034", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.16034", "abs": "https://arxiv.org/abs/2602.16034", "authors": ["Xinrui He", "Ting-Wei Li", "Tianxin Wei", "Xuying Ning", "Xinyu He", "Wenxuan Bao", "Hanghang Tong", "Jingrui He"], "title": "FeDecider: An LLM-Based Framework for Federated Cross-Domain Recommendation", "comment": "Accepted to The Web Conference (WWW) 2026", "summary": "Federated cross-domain recommendation (Federated CDR) aims to collaboratively learn personalized recommendation models across heterogeneous domains while preserving data privacy. Recently, large language model (LLM)-based recommendation models have demonstrated impressive performance by leveraging LLMs' strong reasoning capabilities and broad knowledge. However, adopting LLM-based recommendation models in Federated CDR scenarios introduces new challenges. First, there exists a risk of overfitting with domain-specific local adapters. The magnitudes of locally optimized parameter updates often vary across domains, causing biased aggregation and overfitting toward domain-specific distributions. Second, unlike traditional recommendation models (e.g., collaborative filtering, bipartite graph-based methods) that learn explicit and comparable user/item representations, LLMs encode knowledge implicitly through autoregressive text generation training. This poses additional challenges for effectively measuring the cross-domain similarities under heterogeneity. To address these challenges, we propose an LLM-based framework for federated cross-domain recommendation, FeDecider. Specifically, FeDecider tackles the challenge of scale-specific noise by disentangling each client's low-rank updates and sharing only their directional components. To handle the need for flexible and effective integration, each client further learns personalized weights that achieve the data-aware integration of updates from other domains. Extensive experiments across diverse datasets validate the effectiveness of our proposed FeDecider.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8054\u90a6\u8de8\u57df\u63a8\u8350\u6846\u67b6FeDecider\uff0c\u901a\u8fc7\u5206\u89e3\u4f4e\u79e9\u66f4\u65b0\u548c\u5b66\u4e60\u4e2a\u6027\u5316\u6743\u91cd\u6765\u89e3\u51b3\u8fc7\u62df\u5408\u548c\u8de8\u57df\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u6311\u6218\u3002", "motivation": "\u8054\u90a6\u8de8\u57df\u63a8\u8350\uff08Federated CDR\uff09\u65e8\u5728\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u8de8\u5f02\u6784\u9886\u57df\u534f\u4f5c\u5b66\u4e60\u4e2a\u6027\u5316\u63a8\u8350\u6a21\u578b\u3002\u5c3d\u7ba1\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u8350\u6a21\u578b\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u548c\u5e7f\u6cdb\u7684\u77e5\u8bc6\u5229\u7528\u80fd\u529b\uff0c\u4f46\u5728\u8054\u90a6CDR\u573a\u666f\u4e2d\u91c7\u7528\u8fd9\u7c7b\u6a21\u578b\u5f15\u5165\u4e86\u65b0\u7684\u6311\u6218\uff0c\u5305\u62ec\u7279\u5b9a\u9886\u57df\u9002\u914d\u5668\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u98ce\u9669\u4ee5\u53ca\u8861\u91cf\u8de8\u57df\u76f8\u4f3c\u6027\u7684\u989d\u5916\u56f0\u96be\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFeDecider\u7684\u57fa\u4e8eLLM\u7684\u8054\u90a6\u8de8\u57df\u63a8\u8350\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u4f4e\u79e9\u66f4\u65b0\u5e76\u4ec5\u5171\u4eab\u65b9\u5411\u7ec4\u4ef6\u6765\u5e94\u5bf9\u89c4\u6a21\u7279\u5f02\u6027\u566a\u58f0\u7684\u95ee\u9898\uff1b\u540c\u65f6\uff0c\u4e3a\u4e86\u8ba9\u96c6\u6210\u66f4\u52a0\u7075\u6d3b\u6709\u6548\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u8fd8\u4f1a\u5b66\u4e60\u5230\u4e2a\u6027\u5316\u7684\u6743\u91cd\uff0c\u4ee5\u5b9e\u73b0\u4ece\u5176\u4ed6\u9886\u57df\u83b7\u53d6\u66f4\u65b0\u7684\u6570\u636e\u611f\u77e5\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684FeDecider\u7684\u6709\u6548\u6027\u3002", "conclusion": "FeDecider\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u8de8\u5f02\u6784\u9886\u57df\u7684\u63a8\u8350\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u8054\u90a6\u8de8\u57df\u63a8\u8350\u65f6\u9047\u5230\u7684\u4e3b\u8981\u969c\u788d\u3002"}}
{"id": "2602.15995", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15995", "abs": "https://arxiv.org/abs/2602.15995", "authors": ["Xiang Fu", "Shiman Meng", "Weiping Zhang", "Luanzheng Guo", "Kento Sato", "Dong H. Ahn", "Ignacio Laguna", "Gregory L. Lee", "Martin Schulz"], "title": "Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs", "comment": "IEEE Cluster 2024", "summary": "After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \\toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u65f6\u949f\uff08DC\uff09\u548c\u5206\u5e03\u5f0f\u7eaa\u5143\uff08DE\uff09\u8bb0\u5f55\u65b9\u6848\u6765\u51cf\u5c11OpenMP\u7a0b\u5e8f\u5728\u8bb0\u5f55\u548c\u786e\u5b9a\u6027\u91cd\u653e\u8fc7\u7a0b\u4e2d\u6240\u9700\u7684\u7ebf\u7a0b\u540c\u6b65\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4e0eMPI\u7ea7\u522b\u7684\u91cd\u653e\u5de5\u5177\u7ed3\u5408\uff0c\u4ee5\u652f\u6301\u975e\u5e73\u51e1\u7684MPI+OpenMP\u5e94\u7528\u7a0b\u5e8f\u7684\u91cd\u653e\u3002", "motivation": "\u5c3d\u7ba1OpenMP\u662f\u6700\u6d41\u884c\u7684\u5171\u4eab\u5185\u5b58\u7f16\u7a0b\u6846\u67b6\u4e4b\u4e00\uff0c\u4f46\u5176\u8f83\u9ad8\u7684\u975e\u786e\u5b9a\u6027\u6267\u884c\u6c34\u5e73\u7ed9\u8c03\u8bd5\u548c\u6d4b\u8bd5\u5e26\u6765\u4e86\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bb0\u5f55\u5e76\u786e\u5b9a\u6027\u5730\u91cd\u653e\u7a0b\u5e8f\u6267\u884c\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5982\u4f55\u9ad8\u6548\u5730\u91cd\u653eOpenMP\u7a0b\u5e8f\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6280\u672f\uff1a\u5206\u5e03\u5f0f\u65f6\u949f\uff08Distributed Clock, DC\uff09\u548c\u5206\u5e03\u5f0f\u7eaa\u5143\uff08Distributed Epoch, DE\uff09\u8bb0\u5f55\u65b9\u6848\uff0c\u65e8\u5728\u51cf\u5c11OpenMP\u8bb0\u5f55\u548c\u56de\u653e\u671f\u95f4\u8fc7\u5ea6\u7684\u7ebf\u7a0b\u540c\u6b65\u95ee\u9898\u3002\u8fd9\u4e9b\u6280\u672f\u901a\u8fc7ReOMP\u5b9e\u73b0\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u53ef\u4ee5\u8f7b\u677e\u5730\u4e0e\u73b0\u6709\u7684\u53ef\u6269\u5c55MPI\u8bb0\u5f55-\u56de\u653e\u5de5\u5177ReMPI\u96c6\u6210\uff0c\u4ec5\u5e26\u6765\u5c11\u91cf\u4e0eMPI\u89c4\u6a21\u65e0\u5173\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "result": "\u5bf9\u4e8e\u4ee3\u8868\u6027\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u7a0b\u5e8f\uff0c\u91c7\u7528DC\u548cDE\u8bb0\u5f55\u65b9\u6848\u7684\u65b9\u6cd5\u6bd4\u6bcf\u6b21\u5171\u4eab\u5185\u5b58\u8bbf\u95ee\u90fd\u8fdb\u884c\u540c\u6b65\u7684\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u9ad8\u51fa2\u52305\u500d\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5c55\u793a\u4e86\u5b83\u80fd\u4e0eMPI\u7ea7\u522b\u7684\u91cd\u653e\u5de5\u5177\u7ed3\u5408\uff0c\u6709\u6548\u5730\u91cd\u653e\u590d\u6742\u7684MPI+OpenMP\u5e94\u7528\u7a0b\u5e8f\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eDC\u548cDE\u8bb0\u5f55\u65b9\u6848\u7684\u65b0\u6280\u672f\u4e0d\u4ec5\u663e\u8457\u63d0\u9ad8\u4e86OpenMP\u7a0b\u5e8f\u8bb0\u5f55\u4e0e\u56de\u653e\u7684\u6548\u7387\uff0c\u800c\u4e14\u4e3aMPI+OpenMP\u6df7\u5408\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15834", "categories": ["cs.LG", "cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15834", "abs": "https://arxiv.org/abs/2602.15834", "authors": ["Rohit Kaushik", "Eva Kaushik"], "title": "A Koopman-Bayesian Framework for High-Fidelity, Perceptually Optimized Haptic Surgical Simulation", "comment": "11 pages, 6 figures", "summary": "We introduce a unified framework that combines nonlinear dynamics, perceptual psychophysics and high frequency haptic rendering to enhance realism in surgical simulation. The interaction of the surgical device with soft tissue is elevated to an augmented state space with a Koopman operator formulation, allowing linear prediction and control of the dynamics that are nonlinear by nature. To make the rendered forces consistent with human perceptual limits, we put forward a Bayesian calibration module based on WeberFechner and Stevens scaling laws, which progressively shape force signals relative to each individual's discrimination thresholds. For various simulated surgical tasks such as palpation, incision, and bone milling, the proposed system attains an average rendering latency of 4.3 ms, a force error of less than 2.8% and a 20% improvement in perceptual discrimination. Multivariate statistical analyses (MANOVA and regression) reveal that the system's performance is significantly better than that of conventional spring-damper and energy, based rendering methods. We end by discussing the potential impact on surgical training and VR, based medical education, as well as sketching future work toward closed, loop neural feedback in haptic interfaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3001\u611f\u77e5\u5fc3\u7406\u7269\u7406\u5b66\u548c\u9ad8\u9891\u89e6\u89c9\u6e32\u67d3\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u624b\u672f\u6a21\u62df\u7684\u771f\u5b9e\u611f\u3002\u901a\u8fc7Koopman\u7b97\u5b50\u516c\u5f0f\u5c06\u624b\u672f\u8bbe\u5907\u4e0e\u8f6f\u7ec4\u7ec7\u4e4b\u95f4\u7684\u4ea4\u4e92\u63d0\u5347\u81f3\u589e\u5f3a\u72b6\u6001\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u97e6\u4f2f-\u8d39\u5e0c\u7eb3\u548c\u53f2\u8482\u6587\u65af\u5b9a\u5f8b\u7684\u8d1d\u53f6\u65af\u6821\u51c6\u6a21\u5757\u6765\u786e\u4fdd\u6240\u6e32\u67d3\u7684\u529b\u4e0e\u4eba\u7c7b\u611f\u77e5\u6781\u9650\u4e00\u81f4\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u5ef6\u8fdf\u3001\u529b\u8bef\u5dee\u53ca\u611f\u77e5\u8fa8\u522b\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9\u533b\u5b66\u6559\u80b2\u5c24\u5176\u662f\u624b\u672f\u8bad\u7ec3\u7684\u5f71\u54cd\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u624b\u672f\u6a21\u62df\u7684\u771f\u5b9e\u6027\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408\u975e\u7ebf\u6027\u52a8\u6001\u5efa\u6a21\u3001\u611f\u77e5\u5fc3\u7406\u5b66\u539f\u7406\u4ee5\u53ca\u9ad8\u6548\u89e6\u89c9\u53cd\u9988\u6280\u672f\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u624b\u672f\u4eff\u771f\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5c06\u624b\u672f\u5668\u68b0\u4e0e\u8f6f\u7ec4\u7ec7\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u6620\u5c04\u5230\u4e00\u4e2a\u6269\u589e\u7684\u72b6\u6001\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u672c\u8d28\u4e0a\u4e3a\u975e\u7ebf\u6027\u7684\u52a8\u529b\u5b66\u8fc7\u7a0b\u8fdb\u884c\u7ebf\u6027\u9884\u6d4b\u4e0e\u63a7\u5236\uff1b\u5176\u6b21\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u97e6\u4f2f-\u8d39\u5e0c\u7eb3\u6cd5\u5219\u548c\u53f2\u8482\u6587\u65af\u5e42\u5f8b\u7684\u8d1d\u53f6\u65af\u6821\u6b63\u673a\u5236\uff0c\u4f7f\u5f97\u751f\u6210\u7684\u529b\u53cd\u9988\u4fe1\u53f7\u80fd\u591f\u6839\u636e\u6bcf\u4f4d\u7528\u6237\u7684\u611f\u77e5\u9608\u503c\u81ea\u9002\u5e94\u8c03\u6574\uff1b\u6700\u540e\uff0c\u5728\u591a\u79cd\u5916\u79d1\u4efb\u52a1\u573a\u666f\u4e0b\u6d4b\u8bd5\u4e86\u6240\u63d0\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u5305\u62ec\u89e6\u8bca\u3001\u5207\u5f00\u53ca\u9aa8\u78e8\u524a\u5728\u5185\u7684\u591a\u79cd\u6a21\u62df\u5916\u79d1\u64cd\u4f5c\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5e73\u57474.3\u6beb\u79d2\u7684\u6e32\u67d3\u5ef6\u8fdf\u3001\u5c0f\u4e8e2.8%\u7684\u529b\u8bef\u5dee\u4ee5\u53ca20%\u4ee5\u4e0a\u7684\u611f\u77e5\u8fa8\u522b\u80fd\u529b\u63d0\u5347\u3002\u6b64\u5916\uff0c\u591a\u53d8\u91cf\u7edf\u8ba1\u5206\u6790\uff08\u5982\u591a\u5143\u65b9\u5dee\u5206\u6790\u548c\u56de\u5f52\uff09\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u5f39\u7c27-\u963b\u5c3c\u5668\u6a21\u578b\u548c\u80fd\u91cf\u57fa\u7840\u7684\u6e32\u67d3\u65b9\u6cd5\uff0c\u65b0\u7cfb\u7edf\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u7684\u65b0\u6846\u67b6\u4e0d\u4ec5\u6781\u5927\u5730\u63d0\u5347\u4e86\u624b\u672f\u6a21\u62df\u4e2d\u7684\u89e6\u89c9\u53cd\u9988\u8d28\u91cf\uff0c\u8fd8\u4e3a\u672a\u6765\u7684\u865a\u62df\u73b0\u5b9e\u533b\u7597\u6559\u80b2\u548c\u795e\u7ecf\u53cd\u9988\u95ed\u73af\u89e6\u89c9\u754c\u9762\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16197", "categories": ["cs.LG", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.16197", "abs": "https://arxiv.org/abs/2602.16197", "authors": ["Rong Fu", "Jia Yee Tan", "Wenxin Zhang", "Zijian Zhang", "Ziming Wang", "Zhaolu Kang", "Muge Qi", "Shuning Zhang", "Simon Fong"], "title": "ModalImmune: Immunity Driven Unlearning via Self Destructive Training", "comment": "23 pages, 8 figures", "summary": "Multimodal systems are vulnerable to partial or complete loss of input channels at deployment, which undermines reliability in real-world settings. This paper presents ModalImmune, a training framework that enforces modality immunity by intentionally and controllably collapsing selected modality information during training so the model learns joint representations that are robust to destructive modality influence. The framework combines a spectrum-adaptive collapse regularizer, an information-gain guided controller for targeted interventions, curvature-aware gradient masking to stabilize destructive updates, and a certified Neumann-truncated hyper-gradient procedure for automatic meta-parameter adaptation. Empirical evaluation on standard multimodal benchmarks demonstrates that ModalImmune improves resilience to modality removal and corruption while retaining convergence stability and reconstruction capacity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aModalImmune\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6709\u610f\u4e14\u53ef\u63a7\u5730\u51cf\u5c11\u9009\u5b9a\u6a21\u6001\u4fe1\u606f\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5bf9\u7834\u574f\u6027\u6a21\u6001\u5f71\u54cd\u5177\u6709\u9c81\u68d2\u6027\u7684\u8054\u5408\u8868\u793a\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9891\u8c31\u81ea\u9002\u5e94\u5d29\u6e83\u6b63\u5219\u5316\u5668\u3001\u4fe1\u606f\u589e\u76ca\u5bfc\u5411\u63a7\u5236\u5668\u3001\u66f2\u7387\u611f\u77e5\u68af\u5ea6\u63a9\u853d\u6280\u672f\u4ee5\u53ca\u7528\u4e8e\u81ea\u52a8\u5143\u53c2\u6570\u8c03\u6574\u7684\u8ba4\u8bc1Neumann\u622a\u65ad\u8d85\u68af\u5ea6\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cModalImmune\u80fd\u591f\u63d0\u9ad8\u7cfb\u7edf\u5bf9\u4e8e\u6a21\u6001\u4e22\u5931\u548c\u635f\u574f\u7684\u62b5\u6297\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6536\u655b\u7a33\u5b9a\u6027\u548c\u91cd\u5efa\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u90e8\u7f72\u65f6\u5bb9\u6613\u53d7\u5230\u90e8\u5206\u6216\u5168\u90e8\u8f93\u5165\u901a\u9053\u4e22\u5931\u7684\u5f71\u54cd\uff0c\u8fd9\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u9760\u6027\u6784\u6210\u4e86\u5a01\u80c1\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86ModalImmune\uff0c\u65e8\u5728\u589e\u5f3a\u6a21\u578b\u5bf9\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "ModalImmune\u6846\u67b6\u91c7\u7528\u4e86\u56db\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u9891\u8c31\u81ea\u9002\u5e94\u5d29\u6e83\u6b63\u5219\u5316\u5668\uff0c\u5e2e\u52a9\u6a21\u578b\u5b66\u4f1a\u5904\u7406\u7279\u5b9a\u6a21\u6001\u4fe1\u606f\u7f3a\u5931\u7684\u60c5\u51b5\uff1b2) \u4fe1\u606f\u589e\u76ca\u5bfc\u5411\u63a7\u5236\u5668\uff0c\u5141\u8bb8\u5bf9\u7279\u5b9a\u6a21\u6001\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u64cd\u4f5c\uff1b3) \u66f2\u7387\u611f\u77e5\u68af\u5ea6\u63a9\u853d\u6280\u672f\uff0c\u7528\u4ee5\u7a33\u5b9a\u7531\u6545\u610f\u5f15\u5165\u7684\u7834\u574f\u6027\u66f4\u65b0\u6240\u5bfc\u81f4\u7684\u5b66\u4e60\u8fc7\u7a0b\uff1b4) \u8ba4\u8bc1Neumann\u622a\u65ad\u8d85\u68af\u5ea6\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u7684\u5143\u53c2\u6570\u8c03\u6574\u3002", "result": "\u901a\u8fc7\u5bf9\u6807\u51c6\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u91c7\u7528ModalImmune\u65b9\u6cd5\u540e\uff0c\u5373\u4f7f\u5728\u9762\u4e34\u6a21\u6001\u79fb\u9664\u6216\u635f\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u4ecd\u80fd\u8868\u73b0\u51fa\u8f83\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6536\u655b\u7a33\u5b9a\u6027\u548c\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528ModalImmune\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5c31\u8003\u8651\u5230\u4e86\u53ef\u80fd\u53d1\u751f\u7684\u6a21\u6001\u4fe1\u606f\u635f\u5931\u60c5\u51b5\uff0c\u4ece\u800c\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.16124", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16124", "abs": "https://arxiv.org/abs/2602.16124", "authors": ["Jiang Zhang", "Yubo Wang", "Wei Chang", "Lu Han", "Xingying Cheng", "Feng Zhang", "Min Li", "Songhao Jiang", "Wei Zheng", "Harry Tran", "Zhen Wang", "Lei Chen", "Yueming Wang", "Benyu Zhang", "Xiangjun Fan", "Bi Xue", "Qifan Wang"], "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System", "comment": null, "summary": "Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. However, ANN-based retrieval has two key limitations. First, item embeddings and their indices are typically learned in separate stages: indexing is often performed offline after embeddings are trained, which can yield suboptimal retrieval quality-especially for newly created items. Second, although ANN offers sublinear query time, it must still be run for every request, incurring substantial computation cost at industry scale. In this paper, we propose MultiFaceted Learnable Index (MFLI), a scalable, real-time retrieval paradigm that learns multifaceted item embeddings and indices within a unified framework and eliminates ANN search at serving time. Specifically, we construct a multifaceted hierarchical codebook via residual quantization of item embeddings and co-train the codebook with the embeddings. We further introduce an efficient multifaceted indexing structure and mechanisms that support real-time updates. At serving time, the learned hierarchical indices are used directly to identify relevant items, avoiding ANN search altogether. Extensive experiments on real-world data with billions of users show that MFLI improves recall on engagement tasks by up to 11.8\\%, cold-content delivery by up to 57.29\\%, and semantic relevance by 13.5\\% compared with prior state-of-the-art methods. We also deploy MFLI in the system and report online experimental results demonstrating improved engagement, less popularity bias, and higher serving efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9762\u53ef\u5b66\u4e60\u7d22\u5f15\uff08MFLI\uff09\uff0c\u5b83\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u68c0\u7d22\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5b66\u4e60\u591a\u9762\u9879\u76ee\u5d4c\u5165\u548c\u7d22\u5f15\uff0c\u5e76\u5728\u670d\u52a1\u65f6\u6d88\u9664\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u3002", "motivation": "\u8fd1\u4f3c\u6700\u8fd1\u90bb(ANN)\u641c\u7d22\u5728\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5728\u68c0\u7d22\u8d28\u91cf\u548c\u670d\u52a1\u65f6\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u65b0\u521b\u5efa\u9879\u76ee\u7684\u6b21\u4f18\u68c0\u7d22\u8d28\u91cf\u548c\u5de5\u4e1a\u89c4\u6a21\u4e0b\u6bcf\u4e2a\u8bf7\u6c42\u90fd\u5fc5\u987b\u8fd0\u884cANN\u5e26\u6765\u7684\u5927\u91cf\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6b8b\u5dee\u91cf\u5316\u6784\u5efa\u591a\u9762\u5c42\u6b21\u7801\u672c\u5e76\u4e0e\u5d4c\u5165\u5171\u540c\u8bad\u7ec3\uff0c\u5f15\u5165\u4e86\u9ad8\u6548\u7684\u591a\u9762\u7d22\u5f15\u7ed3\u6784\u548c\u652f\u6301\u5b9e\u65f6\u66f4\u65b0\u673a\u5236\u3002\u5728\u670d\u52a1\u65f6\u95f4\u76f4\u63a5\u4f7f\u7528\u5b66\u5230\u7684\u5c42\u6b21\u7d22\u5f15\u8bc6\u522b\u76f8\u5173\u9879\uff0c\u5b8c\u5168\u907f\u514d\u4e86ANN\u641c\u7d22\u3002", "result": "\u5728\u6570\u5341\u4ebf\u7528\u6237\u7684\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cMFLI\u5728\u53c2\u4e0e\u4efb\u52a1\u53ec\u56de\u7387\u4e0a\u63d0\u9ad8\u4e86\u6700\u591a11.8%\uff0c\u51b7\u5185\u5bb9\u4ea4\u4ed8\u63d0\u9ad8\u4e86\u6700\u591a57.29%\uff0c\u8bed\u4e49\u76f8\u5173\u6027\u63d0\u9ad8\u4e8613.5%\u3002\u6b64\u5916\uff0c\u5728\u7ebf\u5b9e\u9a8c\u7ed3\u679c\u8fd8\u663e\u793a\u4e86\u66f4\u9ad8\u7684\u53c2\u4e0e\u5ea6\u3001\u66f4\u5c11\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u4ee5\u53ca\u66f4\u9ad8\u7684\u670d\u52a1\u6548\u7387\u3002", "conclusion": "MFLI\u63d0\u4f9b\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u7d22\u8d28\u91cf\uff0c\u8fd8\u51cf\u5c11\u4e86\u670d\u52a1\u65f6\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u68c0\u7d22\u573a\u666f\u3002"}}
{"id": "2602.15983", "categories": ["cs.SE", "cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.15983", "abs": "https://arxiv.org/abs/2602.15983", "authors": ["Junbo Jacob Lian", "Yujun Sun", "Huiling Chen", "Chaoyu Zhang", "Chung-Piaw Teo"], "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization", "comment": "Code and benchmark: \\url{https://github.com/junbolian/ReLoop}", "summary": "Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aReLoop\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u548c\u884c\u4e3a\u9a8c\u8bc1\u4e24\u79cd\u4e92\u8865\u7684\u65b9\u5f0f\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4f18\u5316\u4ee3\u7801\u65f6\u51fa\u73b0\u7684\u9759\u9ed8\u9519\u8bef\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u6267\u884c\u7387\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u4f18\u5316\u4ee3\u7801\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u4e25\u91cd\u7684\u95ee\u9898\uff1a\u5373\u4f7f\u4ee3\u7801\u53ef\u4ee5\u6210\u529f\u6267\u884c\u5e76\u8fd4\u56de\u6c42\u89e3\u5668\u53ef\u884c\u89e3\uff0c\u4e5f\u53ef\u80fd\u5305\u542b\u8bed\u4e49\u4e0a\u4e0d\u6b63\u786e\u7684\u516c\u5f0f\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u7ec4\u5408\u95ee\u9898\u4e0a\u7684\u6b63\u786e\u6027\u5dee\u8ddd\u9ad8\u8fbe90\u4e2a\u767e\u5206\u70b9\u3002", "method": "\u63d0\u51fa\u4e86ReLoop\u65b9\u6cd5\uff0c\u5b83\u4ece\u4e24\u4e2a\u4e92\u8865\u7684\u89d2\u5ea6\u5904\u7406\u9759\u9ed8\u5931\u8d25\u95ee\u9898\u3002\u4e00\u65b9\u9762\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u65b9\u5f0f\u6a21\u4eff\u4e13\u5bb6\u5efa\u6a21\u5b9e\u8df5\uff0c\u628a\u4ee3\u7801\u751f\u6210\u5206\u89e3\u6210\u7406\u89e3\u3001\u5f62\u5f0f\u5316\u3001\u5408\u6210\u53ca\u9a8c\u8bc1\u56db\u4e2a\u9636\u6bb5\uff0c\u5f3a\u5316\u53d8\u91cf\u7c7b\u578b\u63a8\u7406\u548c\u81ea\u6211\u9a8c\u8bc1\u6765\u9632\u6b62\u6e90\u5934\u4e0a\u7684\u516c\u5f0f\u9519\u8bef\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u91c7\u7528\u884c\u4e3a\u9a8c\u8bc1\u6765\u68c0\u6d4b\u90a3\u4e9b\u7ecf\u8fc7\u751f\u6210\u540e\u4ecd\u5b58\u5728\u7684\u9519\u8bef\uff0c\u901a\u8fc7\u5bf9\u57fa\u4e8e\u6c42\u89e3\u5668\u53c2\u6570\u6270\u52a8\u4e0b\u516c\u5f0f\u7684\u54cd\u5e94\u662f\u5426\u6b63\u786e\u6765\u8fdb\u884c\u68c0\u9a8c\uff0c\u800c\u4e0d\u9700\u8981\u4f9d\u8d56\u4e8e\u771f\u5b9e\u503c\u3002\u6b64\u5916\uff0c\u8fd8\u5229\u7528IIS\u589e\u5f3a\u8bca\u65ad\u8fdb\u884c\u6267\u884c\u6062\u590d\u3002", "result": "ReLoop\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u53ef\u6267\u884c\u6027\uff0c\u5728\u6700\u5f3a\u6a21\u578b\u4e0a\uff0c\u6b63\u786e\u6027\u4ece22.6%\u63d0\u5347\u5230\u4e8631.1%\uff0c\u6267\u884c\u6210\u529f\u7387\u5219\u4ece72.1%\u63d0\u9ad8\u5230\u4e86100%\u3002\u8fd9\u4e9b\u6539\u8fdb\u5728\u8de8\u8d8a\u4e09\u79cd\u8303\u5f0f\uff08\u57fa\u7840\u3001SFT\u3001RL\uff09\u548c\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e94\u4e2a\u6a21\u578b\u4e0a\u90fd\u5f97\u5230\u4e86\u4e00\u81f4\u4f53\u73b0\u3002", "conclusion": "ReLoop\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u751f\u6210\u4e0e\u884c\u4e3a\u9a8c\u8bc1\u8fd9\u4e24\u79cd\u673a\u5236\u6709\u6548\u5730\u89e3\u51b3\u4e86\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7684\u4f18\u5316\u4ee3\u7801\u4e2d\u7684\u9759\u9ed8\u9519\u8bef\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7ec4\u5408\u95ee\u9898\u4ee5\u53ca\u5c40\u90e8\u516c\u5f0f\u7f3a\u9677\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.16010", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16010", "abs": "https://arxiv.org/abs/2602.16010", "authors": ["Xin Huang", "Weiping Zhang", "Shiman Meng", "Wubiao Xu", "Xiang Fu", "Luanzheng Guo", "Kento Sato"], "title": "Scrutinizing Variables for Checkpoint Using Automatic Differentiation", "comment": "The Second Workshop on Enabling Predictive Science with Optimization and Uncertainty Quantification in HPC (EPSOUQ-HPC) in conjunction with SC24", "summary": "Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u52a8\u5fae\u5206\u6765\u5ba1\u67e5\u53d8\u91cf\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u4ee5\u8fdb\u884c\u68c0\u67e5\u70b9\u8bbe\u7f6e\uff0c\u4ece\u800c\u8bc6\u522b\u5173\u952e\u548c\u975e\u5173\u952e\u5143\u7d20\uff0c\u5e76\u4ece\u68c0\u67e5\u70b9\u4e2d\u6392\u9664\u975e\u5173\u952e\u5143\u7d20\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u5e94\u7528\u7a0b\u5e8f\u8f93\u51fa\u7684\u60c5\u51b5\u4e0b\u8282\u7701\u9ad8\u8fbe20%\u7684\u5b58\u50a8\u7a7a\u95f4\u3002", "motivation": "\u5b9a\u671f\u4fdd\u5b58\u7a0b\u5e8f\u8fd0\u884c\u72b6\u6001\u7684\u68c0\u67e5\u70b9/\u91cd\u542f\u673a\u5236\u4f1a\u6d88\u8017\u5927\u91cf\u7cfb\u7edf\u8d44\u6e90\u3002\u7814\u7a76\u8005\u6ce8\u610f\u5230\uff0c\u5728\u5178\u578b\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u4e2d\uff0c\u5e76\u4e0d\u662f\u6240\u6709\u6570\u636e\u90fd\u53c2\u4e0e\u4e86\u8ba1\u7b97\uff1b\u8fd9\u79cd\u672a\u4f7f\u7528\u7684\u6570\u636e\u5e94\u8be5\u88ab\u6392\u9664\u5728\u68c0\u67e5\u70b9\u4e4b\u5916\uff0c\u4ee5\u63d0\u9ad8\u5b58\u50a8/\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u81ea\u52a8\u5fae\u5206\uff08AD\uff09\u5de5\u5177\uff0c\u5bf9\u53d8\u91cf\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5355\u72ec\u5143\u7d20\u8fdb\u884c\u68c0\u67e5\u70b9\u5ba1\u67e5\uff0c\u4ee5\u5224\u65ad\u8be5\u5143\u7d20\u662f\u5426\u5bf9\u5e94\u7528\u7a0b\u5e8f\u8f93\u51fa\u6709\u5f71\u54cd\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u8bc6\u522b\u51fa\u5173\u952e\u548c\u975e\u5173\u952e\u5143\u7d20\uff0c\u5e76\u4e14\u80fd\u591f\u4ece\u68c0\u67e5\u70b9\u4e2d\u79fb\u9664\u975e\u5173\u952e\u5143\u7d20\u3002", "result": "\u7814\u7a76\u8005\u4f7f\u7528\u6765\u81eaNAS\u5e76\u884c\u57fa\u51c6\u5957\u4ef6\u7684\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u5176\u65b9\u6cd5\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u53ef\u89c6\u5316\u4e86\u53d8\u91cf\u5185\u5173\u952e/\u975e\u5173\u952e\u5143\u7d20/\u533a\u57df\u53ca\u5176\u5bf9\u5e94\u7528\u7a0b\u5e8f\u8f93\u51fa\u7684\u5f71\u54cd\uff08\u662f\u6216\u5426\uff09\u3002\u5bf9\u4e8eNPB\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6700\u591a\u53ef\u4ee5\u8282\u770120%\u7528\u4e8e\u68c0\u67e5\u70b9\u7684\u5b58\u50a8\u7a7a\u95f4\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u81ea\u52a8\u5fae\u5206\u6280\u672f\u8bc6\u522b\u5e76\u6392\u9664\u4e0d\u91cd\u8981\u7684\u6570\u636e\u5143\u7d20\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u6539\u5584\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u4e2d\u7684\u68c0\u67e5\u70b9/\u91cd\u542f\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b58\u50a8\u6548\u7387\u3002"}}
{"id": "2602.15842", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15842", "abs": "https://arxiv.org/abs/2602.15842", "authors": ["Ryosuke Kohita", "Seiichiro Yoshioka"], "title": "Memes-as-Replies: Can Models Select Humorous Manga Panel Responses?", "comment": null, "summary": "Memes are a popular element of modern web communication, used not only as static artifacts but also as interactive replies within conversations. While computational research has focused on analyzing the intrinsic properties of memes, the dynamic and contextual use of memes to create humor remains an understudied area of web science. To address this gap, we introduce the Meme Reply Selection task and present MaMe-Re (Manga Meme Reply Benchmark), a benchmark of 100,000 human-annotated pairs (500,000 total annotations from 2,325 unique annotators) consisting of openly licensed Japanese manga panels and social media posts. Our analysis reveals three key insights: (1) large language models (LLMs) show preliminary evidence of capturing complex social cues such as exaggeration, moving beyond surface-level semantic matching; (2) the inclusion of visual information does not improve performance, revealing a gap between understanding visual content and effectively using it for contextual humor; (3) while LLMs can match human judgments in controlled settings, they struggle to distinguish subtle differences in wit among semantically similar candidates. These findings suggest that selecting contextually humorous replies remains an open challenge for current models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Meme Reply Selection\u4efb\u52a1\u548c\u4e00\u4e2a\u540d\u4e3aMaMe-Re\u7684\u57fa\u51c6\uff0c\u5305\u542b10\u4e07\u4e2a\u7531\u4eba\u7c7b\u6807\u6ce8\u7684\u65e5\u672c\u6f2b\u753b\u9762\u677f\u548c\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u5bf9\u3002\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u793e\u4ea4\u7ebf\u7d22\u65b9\u9762\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u89c6\u89c9\u4fe1\u606f\u7684\u52a0\u5165\u5e76\u672a\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6a21\u578b\u96be\u4ee5\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u9009\u9879\u4e2d\u7684\u5fae\u5999\u5e7d\u9ed8\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u8ba1\u7b97\u7814\u7a76\u5df2\u7ecf\u96c6\u4e2d\u5728\u5206\u6790\u6a21\u56e0\u7684\u5185\u5728\u5c5e\u6027\u4e0a\uff0c\u4f46\u662f\u6a21\u56e0\u88ab\u52a8\u6001\u5730\u3001\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7528\u6765\u521b\u9020\u5e7d\u9ed8\u8fd9\u4e00\u70b9\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u7814\u7a76\u7684\u9886\u57df\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u4eec\u63d0\u51fa\u4e86Meme Reply Selection\u4efb\u52a1\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u8005\u4eec\u6784\u5efa\u4e86MaMe-Re\uff08Manga Meme Reply Benchmark\uff09\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b\u4e8610\u4e07\u4e2a\u4eba\u7c7b\u6807\u6ce8\u7684\u6570\u636e\u5bf9\uff08\u603b\u517150\u4e07\u6761\u6765\u81ea2,325\u4e2a\u72ec\u7279\u6807\u6ce8\u8005\u7684\u6ce8\u91ca\uff09\uff0c\u5305\u62ec\u5f00\u653e\u8bb8\u53ef\u7684\u65e5\u672c\u6f2b\u753b\u7247\u6bb5\u548c\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e09\u4e2a\u4e3b\u8981\u53d1\u73b0\uff1a(1) \u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c55\u793a\u4e86\u6355\u6349\u5982\u5938\u5f20\u7b49\u590d\u6742\u793e\u4ea4\u6697\u793a\u7684\u521d\u6b65\u8bc1\u636e\uff1b(2) \u89c6\u89c9\u4fe1\u606f\u7684\u6dfb\u52a0\u5e76\u6ca1\u6709\u6539\u5584\u8868\u73b0\uff0c\u8fd9\u8868\u660e\u5b58\u5728\u7406\u89e3\u89c6\u89c9\u5185\u5bb9\u4e0e\u6709\u6548\u5229\u7528\u5b83\u8fdb\u884c\u4e0a\u4e0b\u6587\u5e7d\u9ed8\u4e4b\u95f4\u7684\u5dee\u8ddd\uff1b(3) \u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5339\u914d\u4eba\u7c7b\u5224\u65ad\uff0c\u4f46\u5728\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u5019\u9009\u8005\u4e4b\u95f4\u7ec6\u5fae\u7684\u673a\u667a\u5dee\u5f02\u65f6\u663e\u5f97\u529b\u4e0d\u4ece\u5fc3\u3002", "conclusion": "\u9009\u62e9\u5177\u6709\u4e0a\u4e0b\u6587\u5e7d\u9ed8\u611f\u7684\u56de\u7b54\u5bf9\u4e8e\u5f53\u524d\u6a21\u578b\u6765\u8bf4\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u7684\u6311\u6218\u3002"}}
{"id": "2602.16100", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16100", "abs": "https://arxiv.org/abs/2602.16100", "authors": ["Zijie Su", "Muhammed Tawfiqul Islam", "Mohammad Goudarzi", "Adel N. Toosi"], "title": "LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u65b0\u914d\u7f6e\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u670d\u52a1\u505c\u673a\u65f6\u95f4\u548c\u6027\u80fd\u4e0b\u964d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8fc1\u79fb\u673a\u5236\u7684\u670d\u52a1\u505c\u673a\u65f6\u95f4\u5c11\u4e8e50\u6beb\u79d2\uff0c\u5e76\u4e14\u5728\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\u548c\u6bcf\u8f93\u51fa\u4ee4\u724c\u65f6\u95f4\u4e0a\u589e\u52a0\u7684\u5f00\u9500\u5747\u4f4e\u4e8e10%\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5728\u6709\u9650\u7684GPU\u8d44\u6e90\u4e0b\u9ad8\u6548\u5730\u8fdb\u884cLLM\u63a8\u7406\u5df2\u7ecf\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u5c3d\u7ba1\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u5c06\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u8303\u5f0f\u5e94\u7528\u4e8eLLM\u670d\u52a1\u4ee5\u6700\u5927\u5316\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46\u8003\u8651\u5230LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9ad8\u5ea6\u591a\u6837\u6027\u548c\u73b0\u4ee3GPU\u96c6\u7fa4\u7684\u5f02\u6784\u6027\uff0c\u4ecd\u9700\u8981\u5728\u7ebf\u52a8\u6001\u8c03\u6574\u90e8\u7f72\u914d\u7f6e\u6765\u66f4\u597d\u5730\u9002\u5e94\u65e0\u670d\u52a1\u5668\u73af\u5883\u7684\u5f39\u6027\u548c\u52a8\u6001\u7279\u6027\u3002\u540c\u65f6\uff0c\u7531\u4e8eLLM\u63a8\u7406\u7684\u72b6\u6001\u6027\u8d28\u548c\u6a21\u578b\u53c2\u6570\u7684\u5de8\u5927\u89c4\u6a21\uff0c\u5728\u7ebf\u91cd\u65b0\u914d\u7f6e\u53d8\u5f97\u5c24\u4e3a\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u65b0\u914d\u7f6e\u65b9\u6cd5\uff0c\u5141\u8bb8\u7cfb\u7edf\u6839\u636e\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u6700\u4f18\u7684\u6d41\u6c34\u7ebf\u914d\u7f6e\uff0c\u4ece\u800c\u5b9e\u73b0\u5728\u7ebf\u8c03\u6574\u7684\u540c\u65f6\u6700\u5c0f\u5316\u670d\u52a1\u4e2d\u65ad\u548c\u670d\u52a1\u8d28\u91cf\u4e0b\u964d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u5728\u5305\u62ecNVIDIA A100\u548cL40s\u5728\u5185\u7684\u5f02\u6784GPU\u5e73\u53f0\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u8fc1\u79fb\u673a\u5236\u80fd\u591f\u5c06\u670d\u52a1\u505c\u673a\u65f6\u95f4\u63a7\u5236\u572850\u6beb\u79d2\u4ee5\u5185\uff0c\u540c\u65f6\u5bf9\u4e8e\u9996\u6b21\u4ee4\u724c\u751f\u6210\u65f6\u95f4\u548c\u6bcf\u4e2a\u8f93\u51fa\u4ee4\u724c\u7684\u65f6\u95f4\u5f00\u9500\u589e\u52a0\u4e0d\u8d85\u8fc710%\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u65b0\u914d\u7f6e\u65b9\u6848\uff0c\u672c\u7814\u7a76\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u9762\u4e34\u7684\u5f39\u6027\u6269\u5c55\u96be\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5c55\u793a\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\uff0c\u8fd8\u4fdd\u8bc1\u4e86\u9ad8\u6c34\u5e73\u7684\u670d\u52a1\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2602.15855", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15855", "abs": "https://arxiv.org/abs/2602.15855", "authors": ["Barak Or"], "title": "Kalman-Inspired Runtime Stability and Recovery in Hybrid Reasoning Systems", "comment": "Under review", "summary": "Hybrid reasoning systems that combine learned components with model-based inference are increasingly deployed in tool-augmented decision loops, yet their runtime behavior under partial observability and sustained evidence mismatch remains poorly understood. In practice, failures often arise as gradual divergence of internal reasoning dynamics rather than as isolated prediction errors. This work studies runtime stability in hybrid reasoning systems from a Kalman-inspired perspective. We model reasoning as a stochastic inference process driven by an internal innovation signal and introduce cognitive drift as a measurable runtime phenomenon. Stability is defined in terms of detectability, bounded divergence, and recoverability rather than task-level correctness. We propose a runtime stability framework that monitors innovation statistics, detects emerging instability, and triggers recovery-aware control mechanisms. Experiments on multi-step, tool-augmented reasoning tasks demonstrate reliable instability detection prior to task failure and show that recovery, when feasible, re-establishes bounded internal behavior within finite time. These results emphasize runtime stability as a system-level requirement for reliable reasoning under uncertainty.", "AI": {"tldr": "\u672c\u6587\u4ece\u5361\u5c14\u66fc\u542f\u53d1\u7684\u89d2\u5ea6\u7814\u7a76\u4e86\u6df7\u5408\u63a8\u7406\u7cfb\u7edf\u5728\u8fd0\u884c\u65f6\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u76d1\u6d4b\u5185\u90e8\u521b\u65b0\u4fe1\u53f7\u3001\u68c0\u6d4b\u4e0d\u7a33\u5b9a\u73b0\u8c61\u5e76\u89e6\u53d1\u6062\u590d\u673a\u5236\u7684\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5728\u4efb\u52a1\u5931\u8d25\u524d\u53ef\u9760\u5730\u68c0\u6d4b\u5230\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u4f7f\u7cfb\u7edf\u884c\u4e3a\u5728\u6709\u9650\u65f6\u95f4\u5185\u6062\u590d\u5230\u6709\u754c\u72b6\u6001\u3002", "motivation": "\u6df7\u5408\u63a8\u7406\u7cfb\u7edf\u5728\u5de5\u5177\u589e\u5f3a\u51b3\u7b56\u5faa\u73af\u4e2d\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u4f46\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u6301\u7eed\u8bc1\u636e\u4e0d\u5339\u914d\u60c5\u51b5\u4e0b\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\u4ecd\u4e0d\u591f\u6e05\u695a\u3002\u5b9e\u8df5\u4e2d\uff0c\u7cfb\u7edf\u6545\u969c\u5f80\u5f80\u8868\u73b0\u4e3a\u5185\u90e8\u63a8\u7406\u52a8\u6001\u9010\u6e10\u504f\u79bb\u800c\u975e\u5b64\u7acb\u7684\u9884\u6d4b\u9519\u8bef\u3002", "method": "\u672c\u6587\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u7531\u5185\u90e8\u521b\u65b0\u4fe1\u53f7\u9a71\u52a8\u7684\u968f\u673a\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u8ba4\u77e5\u6f02\u79fb\u4f5c\u4e3a\u53ef\u6d4b\u91cf\u7684\u8fd0\u884c\u65f6\u73b0\u8c61\u3002\u5b9a\u4e49\u7a33\u5b9a\u6027\u5305\u62ec\u53ef\u68c0\u6d4b\u6027\u3001\u6709\u754c\u53d1\u6563\u548c\u53ef\u6062\u590d\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u884c\u65f6\u7a33\u5b9a\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u63a7\u521b\u65b0\u7edf\u8ba1\u6570\u636e\u3001\u68c0\u6d4b\u65b0\u51fa\u73b0\u7684\u4e0d\u7a33\u5b9a\u6027\u5e76\u89e6\u53d1\u5177\u6709\u6062\u590d\u610f\u8bc6\u7684\u63a7\u5236\u673a\u5236\u3002", "result": "\u901a\u8fc7\u591a\u6b65\u9aa4\u3001\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u4efb\u52a1\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u5728\u4efb\u52a1\u5931\u8d25\u4e4b\u524d\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u5230\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u4e14\u5f53\u6062\u590d\u53ef\u884c\u65f6\uff0c\u80fd\u591f\u5728\u6709\u9650\u65f6\u95f4\u5185\u91cd\u65b0\u5efa\u7acb\u6709\u754c\u7684\u5185\u90e8\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u8fd0\u884c\u65f6\u7a33\u5b9a\u6027\u4f5c\u4e3a\u7cfb\u7edf\u7ea7\u8981\u6c42\u5bf9\u4e8e\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5b9e\u73b0\u53ef\u9760\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.16299", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.16299", "abs": "https://arxiv.org/abs/2602.16299", "authors": ["Mathias Vast", "Victor Morand", "Basile van Cooten", "Laure Soulier", "Josiane Mothe", "Benjamin Piwowarski"], "title": "MICE: Minimal Interaction Cross-Encoders for efficient Re-ranking", "comment": "9 pages, 5 figures", "summary": "Cross-encoders deliver state-of-the-art ranking effectiveness in information retrieval, but have a high inference cost. This prevents them from being used as first-stage rankers, but also incurs a cost when re-ranking documents. Prior work has addressed this bottleneck from two largely separate directions: accelerating cross-encoder inference by sparsifying the attention process or improving first-stage retrieval effectiveness using more complex models, e.g. late-interaction ones. In this work, we propose to bridge these two approaches, based on an in-depth understanding of the internal mechanisms of cross-encoders. Starting from cross-encoders, we show that it is possible to derive a new late-interaction-like architecture by carefully removing detrimental or unnecessary interactions. We name this architecture MICE (Minimal Interaction Cross-Encoders). We extensively evaluate MICE across both in-domain (ID) and out-of-domain (OOD) datasets. MICE decreases fourfold the inference latency compared to standard cross-encoders, matching late-interaction models like ColBERT while retaining most of cross-encoder ID effectiveness and demonstrating superior generalization abilities in OOD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMICE\uff08\u6700\u5c0f\u4ea4\u4e92\u8de8\u7f16\u7801\u5668\uff09\u7684\u65b0\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u4ea4\u4e92\u4f5c\u7528\u6765\u52a0\u901f\u8de8\u7f16\u7801\u5668\u7684\u63a8\u7406\u8fc7\u7a0b\u3002MICE\u5728\u4fdd\u6301\u8de8\u7f16\u7801\u5668\u5927\u90e8\u5206\u57df\u5185\u6709\u6548\u6027\u7684\u57fa\u7840\u4e0a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u4e14\u5728\u57df\u5916\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8de8\u7f16\u7801\u5668\u867d\u7136\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u63d0\u4f9b\u4e86\u6700\u5148\u8fdb\u7684\u6392\u540d\u6548\u679c\uff0c\u4f46\u5176\u8f83\u9ad8\u7684\u63a8\u7406\u6210\u672c\u9650\u5236\u4e86\u5b83\u4eec\u4f5c\u4e3a\u7b2c\u4e00\u9636\u6bb5\u6392\u5e8f\u5668\u7684\u5e94\u7528\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u4ece\u52a0\u901f\u8de8\u7f16\u7801\u5668\u63a8\u7406\u6216\u6539\u8fdb\u7b2c\u4e00\u9636\u6bb5\u68c0\u7d22\u6548\u679c\u4e24\u65b9\u9762\u7740\u624b\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u57fa\u4e8e\u5bf9\u8de8\u7f16\u7801\u5668\u5185\u90e8\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u5f00\u53d1\u65b0\u67b6\u6784\u3002", "method": "\u4f5c\u8005\u4eec\u901a\u8fc7\u5bf9\u6807\u51c6\u8de8\u7f16\u7801\u5668\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\uff0c\u8bc6\u522b\u5e76\u79fb\u9664\u4e86\u90a3\u4e9b\u6709\u5bb3\u6216\u975e\u5fc5\u8981\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u8bbe\u8ba1\u51fa\u4e00\u79cd\u65b0\u7684\u7c7b\u4f3c\u4e8e\u540e\u671f\u4ea4\u4e92\u67b6\u6784\u2014\u2014MICE\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u8de8\u7f16\u7801\u5668\u76f8\u6bd4\uff0cMICE\u80fd\u591f\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u56db\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u57df\u5185\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728\u57df\u5916\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8eColBERT\u7b49\u540e\u671f\u4ea4\u4e92\u6a21\u578b\u3002", "conclusion": "MICE\u67b6\u6784\u6210\u529f\u5730\u7ed3\u5408\u4e86\u8de8\u7f16\u7801\u5668\u548c\u540e\u671f\u4ea4\u4e92\u6a21\u578b\u7684\u4f18\u70b9\uff0c\u5728\u4fdd\u8bc1\u68c0\u7d22\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\uff0c\u4e3a\u89e3\u51b3\u8de8\u7f16\u7801\u5668\u9ad8\u63a8\u7406\u6210\u672c\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.16091", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16091", "abs": "https://arxiv.org/abs/2602.16091", "authors": ["Amirali Rayegan", "Tim Menzies"], "title": "Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?", "comment": "Submitted to MSR'26 in Registered Report track", "summary": "Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5c06\u56e0\u679c\u610f\u8bc6\u7684\u5206\u88c2\u6807\u51c6\u7eb3\u5165\u7b26\u53f7\u6a21\u578b\u4e2d\uff0c\u6765\u63a2\u8ba8\u8fd9\u662f\u5426\u80fd\u63d0\u9ad8\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u4e0e\u81ea\u52a8\u5316\u6a21\u578b\u4e4b\u95f4\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u6a21\u578b\u548c\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u76f8\u5173\u6027\u65b9\u6cd5\u6df7\u6dc6\u4e86\u5173\u8054\u4e0e\u56e0\u679c\u5173\u7cfb\u4ee5\u53ca\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u56e0\u7ed3\u6784\u5b66\u4e60\u7684NP\u96be\u6027\u8d28\u800c\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u8fd1\u4f3c\u5bfc\u81f4\u7684\u95ee\u9898\u3002\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\u5f15\u5165\u56e0\u679c\u610f\u8bc6\u80fd\u5426\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u800c\u4e0d\u727a\u7272\u9884\u6d4b\u6216\u4f18\u5316\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u6765\u81ea\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\u5e93\uff08MOOT\uff09\u4e2d\u7684120\u591a\u4e2a\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\uff0c\u91c7\u7528\u9884\u6ce8\u518c\u7684\u81ea\u52a9\u96c6\u6210\u534f\u8bae\u6d4b\u91cf\u65b9\u5dee\u4e0e\u83b7\u80dc\u5206\u6570\u5206\u914d\u6765\u8bc4\u4f30\u7a33\u5b9a\u6027\u3002\u5bf9\u6bd4\u4e86\u4eba\u7c7b\u56e0\u679c\u8bc4\u4f30\u3001\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u51b3\u7b56\u6811\uff08EZR\uff09\u4ee5\u53ca\u5229\u7528\u6761\u4ef6\u71b5\u5206\u88c2\u6807\u51c6\u548c\u6df7\u6742\u56e0\u7d20\u8fc7\u6ee4\u7684\u56e0\u679c\u610f\u8bc6\u6811\u4e4b\u95f4\u7684\u7a33\u5b9a\u6027\u53ca\u6027\u80fd\u5dee\u5f02\uff0c\u5206\u6790\u624b\u6bb5\u5305\u62ec\u7edf\u8ba1\u65b9\u6cd5\u5982\u65b9\u5dee\u3001\u57fa\u5c3c\u4e0d\u7eaf\u5ea6\u3001KS\u68c0\u9a8c\u3001Cliff's delta\u7b49\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u56e0\u679c\u610f\u8bc6\u5206\u88c2\u6807\u51c6\u5bf9\u63d0\u9ad8\u7b26\u53f7\u6a21\u578b\u7a33\u5b9a\u6027\u4e0e\u9c81\u68d2\u6027\u7684\u8bc1\u636e\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u76f8\u5bf9\u4e8e\u4f20\u7edf\u76f8\u5173\u6027\u6a21\u578b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u9884\u6d4b\u6216\u4f18\u5316\u6027\u80fd\u4e0a\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u4e0e\u81ea\u52a8\u6a21\u578b\u4e4b\u95f4\u7a33\u5b9a\u6027\u7684\u5dee\u5f02\u3002", "conclusion": "\u5f15\u5165\u56e0\u679c\u610f\u8bc6\u7684\u5206\u88c2\u6807\u51c6\u5230\u7b26\u53f7\u6a21\u578b\u4e2d\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5176\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u9879\u76ee\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u969c\u6027\uff0c\u5c3d\u7ba1\u53ef\u80fd\u4f1a\u5f71\u54cd\u5230\u4e00\u5b9a\u7a0b\u5ea6\u7684\u9884\u6d4b\u6216\u4f18\u5316\u8868\u73b0\u3002"}}
{"id": "2602.16222", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.16222", "abs": "https://arxiv.org/abs/2602.16222", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Robin Vacus"], "title": "Near-optimal population protocols on bounded-degree trees", "comment": "37 pages, 7 figures", "summary": "We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.\n  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.\n  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \\log n)$ steps on directed trees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7a00\u758f\u4ea4\u4e92\u56fe\u4e2d\u7fa4\u4f53\u534f\u8bae\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\u95ee\u9898\u3002\u4e0e\u5b8c\u5168\u56fe\u4e0d\u540c\uff0c\u6709\u754c\u5ea6\u6811\u4e0a\u7684\u7fa4\u4f53\u534f\u8bae\u5728\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u4e0a\u6ca1\u6709\u663e\u793a\u51fa\u663e\u8457\u7684\u6e10\u8fd1\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e24\u4e2a\u65b0\u534f\u8bae\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e9b\u65b9\u6848\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6700\u574f\u60c5\u51b5\u9884\u671f\u7a33\u5b9a\u65f6\u95f4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u9664\u5b8c\u5168\u4ea4\u4e92\u56fe\u5916\uff0c\u5176\u4ed6\u56fe\u65cf\u662f\u5426\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u7684\u7a7a\u95f4-\u65f6\u95f4\u590d\u6742\u5ea6\u6743\u8861\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7a00\u758f\u56fe\u5982\u6709\u9650\u5ea6\u6811\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u4e0b\u754c\u6280\u672f\u96be\u4ee5\u5e94\u7528\u4e8e\u975e\u5e38\u5bc6\u96c6\u7684\u56fe\u4e4b\u5916\u7684\u60c5\u5f62\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e24\u79cd\u65b0\u9896\u534f\u8bae\u6765\u5b9e\u73b0\uff1a\u4e00\u79cd\u662f\u4e3a\u4e00\u822c\u4ea4\u4e92\u56fe\u8bbe\u8ba1\u7684\u65b0\u5feb\u901f\u81ea\u7a33\u60012\u8df3\u7740\u8272\u534f\u8bae\uff1b\u53e6\u4e00\u79cd\u662f\u5728\u4efb\u4f55\u6811\u4e0a\u4ee5\u6700\u4f18\u65f6\u95f4\u6784\u5efa\u6839\u6811\u7684\u81ea\u7a33\u6001\u6811\u5b9a\u5411\u7b97\u6cd5\u3002\u5229\u7528\u8fd9\u4e9b\u534f\u8bae\uff0c\u53ef\u4ee5\u5728\u6709\u5411\u6811\u4e0a\u4f7f\u7528\u7b80\u5355\u7684\u6052\u5b9a\u72b6\u6001\u534f\u8bae\u6765\u5feb\u901f\u89e3\u51b3\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u6709\u754c\u5ea6\u6811\u4e0a\u53ef\u4ee5\u5b9e\u73b0\u5e38\u6570\u7a7a\u95f4\u534f\u8bae\uff0c\u5e76\u4e14\u5bf9\u4e8e\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\uff0c\u8fd9\u4e9b\u534f\u8bae\u5177\u6709\u63a5\u8fd1\u6700\u4f18\u7684\u6700\u574f\u60c5\u51b5\u9884\u671f\u7a33\u5b9a\u65f6\u95f4\u3002\u7279\u522b\u662f\u201c\u5b9a\u5411\u201d\u6e6e\u706d\u52a8\u529b\u5b66\u80fd\u591f\u5728\u6709\u5411\u6811\u4e0a\u4ee5$O(n^2 \\log n)$\u6b65\u89e3\u51b3\u7cbe\u786e\u591a\u6570\u95ee\u9898\u3002", "conclusion": "\u4e0d\u540c\u4e8e\u5b8c\u5168\u56fe\uff0c\u6709\u754c\u5ea6\u6811\u4e0a\u7684\u7fa4\u4f53\u534f\u8bae\u5728\u5904\u7406\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u65f6\u5e76\u4e0d\u5c55\u793a\u51fa\u91cd\u8981\u7684\u6e10\u8fd1\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\u3002\u6240\u63d0\u51fa\u7684\u534f\u8bae\u80fd\u591f\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u52a0\u901f\u89e3\u51b3\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u4f18\u5316\u7a00\u758f\u56fe\u4e2d\u7684\u7fa4\u4f53\u534f\u8bae\u6027\u80fd\u3002"}}
{"id": "2602.15877", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.15877", "abs": "https://arxiv.org/abs/2602.15877", "authors": ["Kaaustaaub Shankar", "Kelly Cohen"], "title": "Genetic Generalized Additive Models", "comment": "Accepted to NAFIPS 2026", "summary": "Generalized Additive Models (GAMs) balance predictive accuracy and interpretability, but manually configuring their structure is challenging. We propose using the multi-objective genetic algorithm NSGA-II to automatically optimize GAMs, jointly minimizing prediction error (RMSE) and a Complexity Penalty that captures sparsity, smoothness, and uncertainty. Experiments on the California Housing dataset show that NSGA-II discovers GAMs that outperform baseline LinearGAMs in accuracy or match performance with substantially lower complexity. The resulting models are simpler, smoother, and exhibit narrower confidence intervals, enhancing interpretability. This framework provides a general approach for automated optimization of transparent, high-performing models. The code can be found at https://github.com/KaaustaaubShankar/GeneticAdditiveModels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5NSGA-II\u81ea\u52a8\u4f18\u5316\u5e7f\u4e49\u53ef\u52a0\u6a21\u578b\uff08GAMs\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u540c\u65f6\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\u548c\u590d\u6742\u5ea6\u60e9\u7f5a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u57fa\u51c6\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002", "motivation": "\u5e7f\u4e49\u53ef\u52a0\u6a21\u578b\uff08GAMs\uff09\u867d\u7136\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4f46\u624b\u52a8\u914d\u7f6e\u5176\u7ed3\u6784\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u4eec\u5bfb\u6c42\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u4f18\u5316GAMs\u7ed3\u6784\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5NSGA-II\u6765\u81ea\u52a8\u5bfb\u627e\u6700\u4f73\u7684GAMs\u914d\u7f6e\uff0c\u901a\u8fc7\u5171\u540c\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\uff08RMSE\uff09\u4e0e\u4e00\u4e2a\u5305\u542b\u7a00\u758f\u6027\u3001\u5e73\u6ed1\u5ea6\u53ca\u4e0d\u786e\u5b9a\u6027\u7684\u590d\u6742\u5ea6\u60e9\u7f5a\u9879\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u57fa\u4e8e\u52a0\u5dde\u4f4f\u623f\u6570\u636e\u96c6\u8fdb\u884c\uff0c\u7ed3\u679c\u663e\u793aNSGA-II\u53d1\u73b0\u7684GAMs\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebfLinearGAMs\uff0c\u6216\u8005\u5728\u8868\u73b0\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\u6709\u7740\u660e\u663e\u66f4\u4f4e\u7684\u590d\u6742\u5ea6\u3002\u6240\u5f97\u5230\u7684\u6a21\u578b\u66f4\u52a0\u7b80\u5355\u3001\u5e73\u6ed1\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u66f4\u7a84\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u7528\u4e8e\u81ea\u52a8\u5316\u5730\u4f18\u5316\u65e2\u900f\u660e\u53c8\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff0c\u4e3a\u63d0\u9ad8GAMs\u7b49\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.16315", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16315", "abs": "https://arxiv.org/abs/2602.16315", "authors": ["Gabriele Barlacchi", "Margherita Lalli", "Emanuele Ferragina", "Fosca Giannotti", "Dino Pedreschi", "Luca Pappalardo"], "title": "The Diversity Paradox revisited: Systemic Effects of Feedback Loops in Recommender Systems", "comment": null, "summary": "Recommender systems shape individual choices through feedback loops in which user behavior and algorithmic recommendations coevolve over time. The systemic effects of these loops remain poorly understood, in part due to unrealistic assumptions in existing simulation studies. We propose a feedback-loop model that captures implicit feedback, periodic retraining, probabilistic adoption of recommendations, and heterogeneous recommender systems. We apply the framework on online retail and music streaming data and analyze systemic effects of the feedback loop. We find that increasing recommender adoption may lead to a progressive diversification of individual consumption, while collective demand is redistributed in model- and domain-dependent ways, often amplifying popularity concentration. Temporal analyses further reveal that apparent increases in individual diversity observed in static evaluations are illusory: when adoption is fixed and time unfolds, individual diversity consistently decreases across all models. Our results highlight the need to move beyond static evaluations and explicitly account for feedback-loop dynamics when designing recommender systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u9988\u5faa\u73af\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u9690\u5f0f\u53cd\u9988\u3001\u5468\u671f\u6027\u518d\u8bad\u7ec3\u3001\u6982\u7387\u6027\u91c7\u7528\u63a8\u8350\u4ee5\u53ca\u5f02\u6784\u63a8\u8350\u7cfb\u7edf\u7684\u7279\u70b9\u3002\u901a\u8fc7\u5bf9\u5728\u7ebf\u96f6\u552e\u548c\u97f3\u4e50\u6d41\u5a92\u4f53\u6570\u636e\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u589e\u52a0\u63a8\u8350\u91c7\u7eb3\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e2a\u4eba\u6d88\u8d39\u7684\u591a\u6837\u5316\uff0c\u4f46\u96c6\u4f53\u9700\u6c42\u5219\u4ee5\u6a21\u578b\u548c\u9886\u57df\u4f9d\u8d56\u7684\u65b9\u5f0f\u91cd\u65b0\u5206\u914d\uff0c\u901a\u5e38\u4f1a\u653e\u5927\u6d41\u884c\u5ea6\u96c6\u4e2d\u73b0\u8c61\u3002\u9759\u6001\u8bc4\u4f30\u4e2d\u89c2\u5bdf\u5230\u7684\u4e2a\u4f53\u591a\u6837\u6027\u589e\u52a0\u5728\u65f6\u95f4\u63a8\u79fb\u4e0b\u5e76\u4e0d\u6210\u7acb\uff1b\u5f53\u91c7\u7eb3\u56fa\u5b9a\u4e14\u65f6\u95f4\u63a8\u8fdb\u65f6\uff0c\u6240\u6709\u6a21\u578b\u4e0b\u7684\u4e2a\u4f53\u591a\u6837\u6027\u5b9e\u9645\u4e0a\u90fd\u5728\u6301\u7eed\u51cf\u5c11\u3002", "motivation": "\u73b0\u6709\u5bf9\u63a8\u8350\u7cfb\u7edf\u53cd\u9988\u5faa\u73af\u7684\u7cfb\u7edf\u6548\u5e94\u7406\u89e3\u4e0d\u8db3\uff0c\u90e8\u5206\u539f\u56e0\u5728\u4e8e\u5f53\u524d\u6a21\u62df\u7814\u7a76\u4e2d\u7684\u5047\u8bbe\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u4e2a\u66f4\u7b26\u5408\u5b9e\u9645\u60c5\u51b5\u7684\u53cd\u9988\u5faa\u73af\u6a21\u578b\uff0c\u6765\u66f4\u597d\u5730\u7406\u89e3\u548c\u8bbe\u8ba1\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u9690\u5f0f\u53cd\u9988\u3001\u5468\u671f\u6027\u518d\u8bad\u7ec3\u3001\u63a8\u8350\u7684\u6982\u7387\u6027\u91c7\u7528\u53ca\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u7684\u53cd\u9988\u5faa\u73af\u6a21\u578b\uff0c\u5e76\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u5728\u7ebf\u96f6\u552e\u4e0e\u97f3\u4e50\u6d41\u5a92\u4f53\u7684\u771f\u5b9e\u6570\u636e\u4e0a\uff0c\u5206\u6790\u4e86\u53cd\u9988\u5faa\u73af\u7684\u7cfb\u7edf\u6548\u5e94\u3002", "result": "\u968f\u7740\u63a8\u8350\u91c7\u7eb3\u7387\u7684\u63d0\u9ad8\uff0c\u4e2a\u4eba\u5c42\u9762\u7684\u6d88\u8d39\u9009\u62e9\u8d8b\u5411\u591a\u6837\u5316\uff0c\u4f46\u4ece\u6574\u4f53\u4e0a\u770b\uff0c\u8fd9\u79cd\u53d8\u5316\u4f1a\u5bfc\u81f4\u67d0\u4e9b\u4ea7\u54c1\u6216\u670d\u52a1\u53d8\u5f97\u66f4\u4e3a\u6d41\u884c\uff08\u5373\u6d41\u884c\u5ea6\u96c6\u4e2d\uff09\u3002\u6b64\u5916\uff0c\u867d\u7136\u9759\u6001\u8bc4\u4ef7\u663e\u793a\u4e2a\u4f53\u591a\u6837\u6027\u6709\u6240\u63d0\u5347\uff0c\u4f46\u968f\u65f6\u95f4\u53d1\u5c55\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\u4e2a\u4f53\u591a\u6837\u6027\u5b9e\u9645\u4e0a\u5747\u5448\u73b0\u4e0b\u964d\u8d8b\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\u9700\u8981\u8d85\u8d8a\u9759\u6001\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u660e\u786e\u8003\u8651\u53cd\u9988\u5faa\u73af\u52a8\u6001\u5bf9\u4e8e\u63a8\u8350\u7cfb\u7edf\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002\u8fd9\u63d0\u793a\u6211\u4eec\uff0c\u5728\u6784\u5efa\u63a8\u8350\u7cfb\u7edf\u65f6\uff0c\u5e94\u66f4\u52a0\u5173\u6ce8\u957f\u671f\u5f71\u54cd\u800c\u975e\u4ec5\u4ec5\u57fa\u4e8e\u77ed\u671f\u6548\u679c\u505a\u51fa\u51b3\u7b56\u3002"}}
{"id": "2602.16106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16106", "abs": "https://arxiv.org/abs/2602.16106", "authors": ["Shahriar Rumi Dipto", "Saikat Mondal", "Chanchal K. Roy"], "title": "Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs", "comment": "Accepted at 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026)", "summary": "Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u6cd5\u7684\u4ee3\u7801\u7ffb\u8bd1\u7ba1\u9053\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u8bed\u8a00\u4e2d\u7acb\u7684\u4e2d\u95f4\u89c4\u8303\u6765\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u95f4\u81ea\u52a8\u8f6c\u6362\u7a0b\u5e8f\u65f6\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ecePython\u5230Java\u6216\u53cd\u4e4b\u7684\u4ee3\u7801\u7ffb\u8bd1\u51c6\u786e\u7387\uff0c\u5e76\u51cf\u5c11\u4e86\u5404\u79cd\u9519\u8bef\u7684\u53d1\u751f\u3002", "motivation": "\u76f4\u63a5\u4e00\u6b21\u6027\u4ee3\u7801\u7ffb\u8bd1\u5f80\u5f80\u4e0d\u80fd\u5f88\u597d\u5730\u4fdd\u6301\u7a0b\u5e8f\u610f\u56fe\uff0c\u5bfc\u81f4\u63a7\u5236\u6d41\u3001\u7c7b\u578b\u5904\u7406\u548cI/O\u884c\u4e3a\u7b49\u65b9\u9762\u7684\u9519\u8bef\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b97\u6cd5\u7684\u6d41\u7a0b\uff0c\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u4e2d\u7acb\u7684\u4e2d\u95f4\u89c4\u683c\u6765\u6355\u6349\u8fd9\u4e9b\u7ec6\u8282\uff0c\u4ece\u800c\u63d0\u9ad8\u7ffb\u8bd1\u7684\u8d28\u91cf\u3002", "method": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u914d\u5bf9\u5b9e\u9a8c\uff0c\u5229\u7528\u4e94\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728Avatar\u548cCodeNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884cPython\u4e0eJava\u4e4b\u95f4\u7684\u76f8\u4e92\u7ffb\u8bd1\u3002\u5bf9\u4e8e\u6bcf\u79cd\u7ec4\u5408\uff08\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u53ca\u65b9\u5411\uff09\uff0c\u90fd\u7f16\u8bd1\u5e76\u6267\u884c\u4e86\u7ffb\u8bd1\u540e\u7684\u7a0b\u5e8f\uff0c\u5e76\u8fd0\u884c\u4e86\u63d0\u4f9b\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002\u8bb0\u5f55\u4e86\u7f16\u8bd1\u7ed3\u679c\u3001\u8fd0\u884c\u65f6\u884c\u4e3a\u3001\u8d85\u65f6\u60c5\u51b5\u4ee5\u53ca\u6d4b\u8bd5\u7ed3\u679c\u3002", "result": "\u57fa\u4e8e\u7b97\u6cd5\u7684\u65b9\u6cd5\u5c06\u5fae\u5e73\u5747\u51c6\u786e\u5ea6\u4ece67.7%\u63d0\u5347\u5230\u4e8678.5%\uff0c\u5373\u589e\u957f\u4e8610.8%\u3002\u5b83\u5b8c\u5168\u6d88\u9664\u4e86\u8bcd\u6cd5\u548c\u6807\u8bb0\u9519\u8bef\uff0c\u51cf\u5c11\u4e8672.7%\u7684\u4e0d\u5b8c\u6574\u7ed3\u6784\u95ee\u9898\uff0c\u4ee5\u53ca61.1%\u7684\u7ed3\u6784\u548c\u58f0\u660e\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5c06\u8fd0\u884c\u65f6\u4f9d\u8d56\u6027\u548c\u5165\u53e3\u70b9\u5931\u8d25\u964d\u4f4e\u4e8678.4%\u3002", "conclusion": "\u57fa\u4e8e\u7b97\u6cd5\u7684\u7ba1\u9053\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u4e14\u4fdd\u7559\u539f\u610f\u7684\u4ee3\u7801\u7ffb\u8bd1\uff0c\u8fd9\u4e3a\u6784\u5efa\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u7f16\u7a0b\u52a9\u624b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16233", "categories": ["cs.DC", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.16233", "abs": "https://arxiv.org/abs/2602.16233", "authors": ["Prabhjot Singh", "Adel N. Toosi", "Rajkumar Buyya"], "title": "DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting", "comment": null, "summary": "Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u91cf\u5b50\u7535\u8def\u5207\u5272\u7684\u6709\u610f\u8bc6\u4f30\u8ba1\u5668\u6267\u884c\u7ba1\u9053\uff0c\u901a\u8fc7\u5c06\u7535\u8def\u5207\u5272\u89c6\u4e3a\u5206\u9636\u6bb5\u7684\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u4f30\u8ba1\u5668\u67e5\u8be2\u8fdb\u884c\u5206\u533a\u3001\u5b50\u5b9e\u9a8c\u751f\u6210\u3001\u5e76\u884c\u6267\u884c\u548c\u7ecf\u5178\u91cd\u5efa\u7b49\u9636\u6bb5\u7684\u64cd\u4f5c\u3002\u57fa\u4e8e\u4e24\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff08Iris\u548cMNIST\uff09\u7684\u6570\u636e\uff0c\u7814\u7a76\u4e86\u5207\u5272\u5f00\u9500\u3001\u6269\u5c55\u9650\u5236\u4ee5\u53ca\u5bf9\u4e8e\u6ce8\u5165\u5ef6\u8fdf\u7684\u654f\u611f\u6027\uff0c\u5e76\u8bc4\u4f30\u5728\u5339\u914d\u8bad\u7ec3\u9884\u7b97\u4e0b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u662f\u5426\u5f97\u4ee5\u4fdd\u6301\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u5207\u5272\u5f15\u5165\u4e86\u5927\u91cf\u7684\u7aef\u5230\u7aef\u5f00\u9500\u5e76\u4e14\u968f\u7740\u5207\u5272\u6570\u91cf\u589e\u52a0\u800c\u589e\u957f\uff0c\u4f46\u6d4b\u8bd5\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5728\u6d4b\u91cf\u8303\u56f4\u5185\u5f97\u4ee5\u4fdd\u6301\uff0c\u4e14\u67d0\u4e9b\u5207\u5272\u8bbe\u7f6e\u4e0b\u89c2\u5bdf\u5230\u4e86\u914d\u7f6e\u4f9d\u8d56\u6027\u7684\u6539\u8fdb\u3002", "motivation": "\u4ee5\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u4ece\u5b50\u7535\u8def\u6570\u91cf\u548c\u91c7\u6837\u590d\u6742\u5ea6\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0\u5207\u5272\u5f00\u9500\uff0c\u4f46\u5bf9\u4e8e\u8fed\u4ee3\u5f0f\u3001\u4ee5\u4f30\u8ba1\u5668\u9a71\u52a8\u7684\u8bad\u7ec3\u6d41\u7a0b\u6765\u8bf4\uff0c\u5176\u7aef\u5230\u7aef\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u5730\u4ece\u7cfb\u7edf\u89d2\u5ea6\u88ab\u8861\u91cf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u7406\u89e3\u7535\u8def\u5207\u5272\u5982\u4f55\u5f71\u54cd\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u201c\u5207\u5272\u610f\u8bc6\u201d\u4f30\u8ba1\u5668\u6267\u884c\u6d41\u7a0b\uff0c\u5b83\u80fd\u591f\u5c06\u7535\u8def\u5207\u5272\u5f53\u4f5c\u4e00\u7cfb\u5217\u5206\u5e03\u5f0f\u7684\u9636\u6bb5\u6027\u5de5\u4f5c\u8d1f\u8f7d\u5904\u7406\uff0c\u5e76\u4e3a\u6bcf\u6b21\u4f30\u8ba1\u8bf7\u6c42\u5212\u5206\u51fa\u5177\u4f53\u9636\u6bb5\u5982\uff1a\u5206\u533a\u3001\u5b50\u5b9e\u9a8c\u521b\u5efa\u3001\u5e76\u884c\u8fd0\u884c\u53ca\u7ecf\u5178\u91cd\u6784\u7b49\u3002\u901a\u8fc7\u5bf9\u5b9e\u9645\u8fd0\u884c\u65f6\u8f68\u8ff9\u8bb0\u5f55\u4ee5\u53ca\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5b66\u4e60\u7ed3\u679c\u7684\u5206\u6790\uff0c\u5b9a\u91cf\u7814\u7a76\u4e86\u5207\u5272\u5e26\u6765\u7684\u989d\u5916\u8d1f\u62c5\u3001\u53ef\u4f38\u7f29\u6027\u754c\u9650\u53ca\u5176\u5bf9\u4eba\u4e3a\u52a0\u5165\u5ef6\u8fdf\u56e0\u7d20\u7684\u654f\u611f\u5ea6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u968f\u7740\u5207\u5272\u6b21\u6570\u589e\u52a0\uff0c\u6574\u4f53\u7ed3\u675f\u5230\u7ed3\u675f\u7684\u989d\u5916\u8d1f\u62c5\u663e\u8457\u589e\u52a0\uff0c\u5c24\u5176\u662f\u91cd\u5efa\u9636\u6bb5\u5360\u636e\u4e86\u6bcf\u9879\u67e5\u8be2\u65f6\u95f4\u7684\u4e3b\u8981\u90e8\u5206\uff0c\u9650\u5236\u4e86\u901a\u8fc7\u589e\u52a0\u5e76\u884c\u7a0b\u5ea6\u6240\u80fd\u83b7\u5f97\u7684\u901f\u5ea6\u63d0\u5347\u3002\u7136\u800c\uff0c\u5728\u6240\u8003\u5bdf\u7684\u60c5\u666f\u4e2d\uff0c\u6d4b\u8bd5\u51c6\u786e\u6027\u4e0e\u6297\u6270\u52a8\u80fd\u529b\u4ecd\u7136\u5f97\u5230\u4e86\u7ef4\u6301\uff0c\u751a\u81f3\u5728\u7279\u5b9a\u5207\u5272\u8bbe\u5b9a\u4e0b\u8fd8\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u8981\u5b9e\u73b0\u5b66\u4e60\u4efb\u52a1\u4e2d\u7535\u8def\u5207\u5272\u7684\u5b9e\u9645\u89c4\u6a21\u5e94\u7528\uff0c\u5173\u952e\u5728\u4e8e\u51cf\u5c11\u548c\u91cd\u53e0\u91cd\u5efa\u9636\u6bb5\u6240\u9700\u65f6\u95f4\uff0c\u5e76\u91c7\u7528\u8003\u8651\u5230\u7531\u969c\u788d\u7269\u4e3b\u5bfc\u7684\u5173\u952e\u8def\u5f84\u7684\u8c03\u5ea6\u7b56\u7565\u3002"}}
{"id": "2602.15878", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15878", "abs": "https://arxiv.org/abs/2602.15878", "authors": ["Mingchun Sun", "Rongqiang Zhao", "Zhennan Huang", "Songyu Ding", "Jie Liu"], "title": "IT-OSE: Exploring Optimal Sample Size for Industrial Data Augmentation", "comment": null, "summary": "In industrial scenarios, data augmentation is an effective approach to improve model performance. However, its benefits are not unidirectionally beneficial. There is no theoretical research or established estimation for the optimal sample size (OSS) in augmentation, nor is there an established metric to evaluate the accuracy of OSS or its deviation from the ground truth. To address these issues, we propose an information-theoretic optimal sample size estimation (IT-OSE) to provide reliable OSS estimation for industrial data augmentation. An interval coverage and deviation (ICD) score is proposed to evaluate the estimated OSS intuitively. The relationship between OSS and dominant factors is theoretically analyzed and formulated, thereby enhancing the interpretability. Experiments show that, compared to empirical estimation, the IT-OSE increases accuracy in classification tasks across baseline models by an average of 4.38%, and reduces MAPE in regression tasks across baseline models by an average of 18.80%. The improvements in downstream model performance are more stable. ICDdev in the ICD score is also reduced by an average of 49.30%. The determinism of OSS is enhanced. Compared to exhaustive search, the IT-OSE achieves the same OSS while reducing computational and data costs by an average of 83.97% and 93.46%. Furthermore, practicality experiments demonstrate that the IT-OSE exhibits generality across representative sensor-based industrial scenarios.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u7684\u6700\u4f18\u6837\u672c\u5927\u5c0f\u4f30\u8ba1\uff08IT-OSE\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5de5\u4e1a\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u5f15\u5165\u4e86\u533a\u95f4\u8986\u76d6\u7387\u548c\u504f\u5dee\uff08ICD\uff09\u8bc4\u5206\u6765\u76f4\u89c2\u8bc4\u4f30\u4f30\u8ba1\u51fa\u7684OSS\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7ecf\u9a8c\u4f30\u8ba1\u76f8\u6bd4\uff0cIT-OSE\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8\u4e864.38%\uff0c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684MAPE\u5e73\u5747\u964d\u4f4e\u4e8618.80%\u3002\u540c\u65f6\uff0c\u76f8\u6bd4\u4e8e\u7a77\u4e3e\u641c\u7d22\uff0cIT-OSE\u5728\u8fbe\u5230\u76f8\u540cOSS\u7684\u540c\u65f6\uff0c\u5e73\u5747\u51cf\u5c11\u4e8683.97%\u7684\u8ba1\u7b97\u6210\u672c\u548c93.46%\u7684\u6570\u636e\u6210\u672c\u3002", "motivation": "\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\uff0c\u867d\u7136\u6570\u636e\u589e\u5f3a\u662f\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u624b\u6bb5\uff0c\u4f46\u5176\u76ca\u5904\u5e76\u975e\u5355\u5411\u6709\u5229\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u4e8e\u6570\u636e\u589e\u5f3a\u65f6\u6700\u4f73\u6837\u672c\u5927\u5c0f(OSS)\u7684\u7406\u8bba\u7814\u7a76\u6216\u65e2\u5b9a\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4e5f\u6ca1\u6709\u9002\u5f53\u7684\u6307\u6807\u6765\u8bc4\u4ef7OSS\u7684\u51c6\u786e\u6027\u6216\u5b83\u4e0e\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u7684\u6700\u4f73\u6837\u672c\u91cf\u4f30\u8ba1\uff08IT-OSE\uff09\uff0c\u4e3a\u5de5\u4e1a\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u53ef\u9760\u7684OSS\u4f30\u8ba1\u3002\u53e6\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u533a\u95f4\u8986\u76d6\u4e0e\u504f\u79bb\uff08ICD\uff09\u5206\u6570\u6765\u76f4\u89c2\u5730\u8bc4\u4ef7\u6240\u4f30\u8ba1\u7684OSS\u3002\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u5e76\u5236\u5b9a\u4e86OSS\u4e0e\u5176\u4e3b\u5bfc\u56e0\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u7ecf\u9a8c\u4f30\u8ba1\u76f8\u6bd4\uff0cIT-OSE\u5728\u57fa\u7ebf\u6a21\u578b\u4e0a\u7684\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8\u4e864.38%\uff0c\u56de\u5f52\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee(MAPE)\u964d\u4f4e\u4e8618.80%\u3002\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u63d0\u5347\u66f4\u52a0\u7a33\u5b9a\uff0cICD\u5f97\u5206\u4e2d\u7684ICDdev\u4e5f\u5e73\u5747\u51cf\u5c11\u4e8649.30%\u3002\u6b64\u5916\uff0c\u4e0e\u7a77\u4e3e\u641c\u7d22\u76f8\u6bd4\uff0c\u5728\u83b7\u5f97\u76f8\u540c\u7684OSS\u60c5\u51b5\u4e0b\uff0cIT-OSE\u5e73\u5747\u51cf\u5c11\u4e8683.97%\u7684\u8ba1\u7b97\u6210\u672c\u548c93.46%\u7684\u6570\u636e\u6210\u672c\u3002\u5b9e\u8df5\u6027\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86IT-OSE\u5728\u4ee3\u8868\u6027\u4f20\u611f\u5668\u57fa\u7840\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u666e\u904d\u9002\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u4fe1\u606f\u8bba\u6700\u4f18\u6837\u672c\u91cf\u4f30\u8ba1(IT-OSE)\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5de5\u4e1a\u6570\u636e\u589e\u5f3a\u60c5\u5883\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff0c\u800c\u4e14\u5728\u51cf\u5c11\u8ba1\u7b97\u53ca\u6570\u636e\u6210\u672c\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2602.16375", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16375", "abs": "https://arxiv.org/abs/2602.16375", "authors": ["Kirill Khrylchenko"], "title": "Variable-Length Semantic IDs for Recommender Systems", "comment": null, "summary": "Generative models are increasingly used in recommender systems, both for modeling user behavior as event sequences and for integrating large language models into recommendation pipelines. A key challenge in this setting is the extremely large cardinality of item spaces, which makes training generative models difficult and introduces a vocabulary gap between natural language and item identifiers. Semantic identifiers (semantic IDs), which represent items as sequences of low-cardinality tokens, have recently emerged as an effective solution to this problem.\n  However, existing approaches generate semantic identifiers of fixed length, assigning the same description length to all items. This is inefficient, misaligned with natural language, and ignores the highly skewed frequency structure of real-world catalogs, where popular items and rare long-tail items exhibit fundamentally different information requirements. In parallel, the emergent communication literature studies how agents develop discrete communication protocols, often producing variable-length messages in which frequent concepts receive shorter descriptions. Despite the conceptual similarity, these ideas have not been systematically adopted in recommender systems.\n  In this work, we bridge recommender systems and emergent communication by introducing variable-length semantic identifiers for recommendation. We propose a discrete variational autoencoder with Gumbel-Softmax reparameterization that learns item representations of adaptive length under a principled probabilistic framework, avoiding the instability of REINFORCE-based training and the fixed-length constraints of prior semantic ID methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u53d8\u957f\u5ea6\u8bed\u4e49\u6807\u8bc6\u7b26\uff0c\u901a\u8fc7\u4f7f\u7528Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u7684\u79bb\u6563\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u6765\u5b66\u4e60\u9002\u5e94\u6027\u957f\u5ea6\u7684\u9879\u76ee\u8868\u793a\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u9879\u76ee\u63cf\u8ff0\u957f\u5ea6\u56fa\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u751f\u6210\u6a21\u578b\u9762\u4e34\u7740\u7269\u54c1\u7a7a\u95f4\u6781\u5927\u57fa\u6570\u7684\u95ee\u9898\uff0c\u8fd9\u4f7f\u5f97\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u53d8\u5f97\u56f0\u96be\uff0c\u5e76\u4e14\u5f15\u5165\u4e86\u81ea\u7136\u8bed\u8a00\u548c\u7269\u54c1\u6807\u8bc6\u7b26\u4e4b\u95f4\u7684\u8bcd\u6c47\u5dee\u8ddd\u3002\u867d\u7136\u73b0\u6709\u7684\u8bed\u4e49\u6807\u8bc6\u7b26\uff08semantic IDs\uff09\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5177\u6709\u56fa\u5b9a\u7684\u957f\u5ea6\uff0c\u5bf9\u6240\u6709\u7269\u54c1\u5206\u914d\u76f8\u540c\u7684\u63cf\u8ff0\u957f\u5ea6\u3002\u8fd9\u79cd\u505a\u6cd5\u4e0d\u4ec5\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u4e14\u4e0e\u81ea\u7136\u8bed\u8a00\u4e0d\u4e00\u81f4\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u76ee\u5f55\u4e2d\u9891\u7387\u7ed3\u6784\u7684\u9ad8\u5ea6\u504f\u659c\u6027\uff0c\u5176\u4e2d\u70ed\u95e8\u7269\u54c1\u548c\u7a00\u6709\u957f\u5c3e\u7269\u54c1\u7684\u4fe1\u606f\u9700\u6c42\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5e26\u6709Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u6280\u672f\u7684\u79bb\u6563\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6982\u7387\u6846\u67b6\u4e0b\u5b66\u4e60\u5230\u9002\u5e94\u6027\u957f\u5ea6\u7684\u7269\u54c1\u8868\u793a\uff0c\u540c\u65f6\u907f\u514d\u4e86\u57fa\u4e8eREINFORCE\u8bad\u7ec3\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u5148\u524d\u8bed\u4e49ID\u65b9\u6cd5\u7684\u56fa\u5b9a\u957f\u5ea6\u9650\u5236\u3002", "result": "\u6587\u7ae0\u672a\u76f4\u63a5\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u6697\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7269\u54c1\u751f\u6210\u8bed\u4e49\u6807\u8bc6\u7b26\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u5177\u6709\u4e0d\u540c\u4fe1\u606f\u9700\u6c42\u7684\u70ed\u95e8\u53ca\u957f\u5c3e\u7269\u54c1\u800c\u8a00\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u63a8\u8350\u7cfb\u7edf\u548c\u65b0\u5174\u901a\u4fe1\u9886\u57df\u7684\u601d\u60f3\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u53ef\u53d8\u957f\u5ea6\u8bed\u4e49\u6807\u8bc6\u7b26\u4e3a\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.16499", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16499", "abs": "https://arxiv.org/abs/2602.16499", "authors": ["Carsten Ellwein", "David Dietrich", "Jessica Roth", "Rozana Cvitkovic", "Andreas Wortmann"], "title": "Software-heavy Asset Administration Shells: Classification and Use Cases", "comment": null, "summary": "The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u586b\u8865\u5c06\u8f6f\u4ef6\u670d\u52a1\u76f4\u63a5\u96c6\u6210\u5230\u8d44\u4ea7\u7ba1\u7406\u58f3(AAS)\u4e2d\u7684\u7cfb\u7edf\u6027\u8f6f\u4ef6\u67b6\u6784\u5206\u6790\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u57fa\u4e8e\u8f6f\u4ef6\u8d28\u91cf\u6807\u51c6\u548c\u5178\u578b\u7684\u5236\u9020\u7528\u4f8b\u533a\u5206\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u5e76\u4e3a\u5b66\u672f\u754c\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e00\u4e2a\u89e3\u8bfb\u6307\u5357\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5728\u5236\u9020\u4e1a\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u6570\u5b57\u5316\u5236\u9020\u548c\u4eba\u5de5\u667a\u80fd\u7684\u4f7f\u7528\u65b9\u9762\uff0c\u5bf9\u5efa\u6a21\u8f6f\u4ef6\u7684\u9700\u6c42\u4ee5\u53ca\u5c06\u670d\u52a1\u76f4\u63a5\u96c6\u6210\u5230AAS\u4e2d\u7684\u9700\u6c42\u4e5f\u5728\u589e\u52a0\u3002\u867d\u7136\u73b0\u6709\u6587\u732e\u4e2d\u6709\u4e00\u4e9b\u9488\u5bf9\u8f6f\u4ef6\u5bc6\u96c6\u578bAAS\u7684\u5355\u72ec\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u80fd\u591f\u76f4\u63a5\u5c06\u8f6f\u4ef6\u670d\u52a1\u6574\u5408\u8fdbAAS\u7684\u8f6f\u4ef6\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u8be5\u8bba\u6587\u91c7\u7528\u4e86\u57fa\u4e8e\u8f6f\u4ef6\u8d28\u91cf\u6807\u51c6\u53ca\u5178\u578b\u5236\u9020\u7528\u4f8b\u6765\u533a\u5206\u4e0d\u540c\u8f6f\u4ef6\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u4ee5\u671f\u4e3a\u8f6f\u4ef6\u5bc6\u96c6\u578bAAS\u63d0\u4f9b\u4e00\u79cd\u89e3\u91ca\u6307\u5bfc\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u8bc6\u522b\u9002\u5408\u4e8e\u8f6f\u4ef6\u5bc6\u96c6\u578bAAS\u7684\u4e0d\u540c\u8f6f\u4ef6\u67b6\u6784\uff0c\u5e76\u6839\u636e\u7279\u5b9a\u7684\u8d28\u91cf\u6807\u51c6\u548c\u5e94\u7528\u573a\u666f\u5bf9\u5176\u8fdb\u884c\u8bc4\u4f30\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u8f6f\u4ef6\u5bc6\u96c6\u578bAAS\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u91ca\u6307\u5357\uff0c\u65e8\u5728\u652f\u6301\u5b66\u672f\u754c\u4e0e\u5b9e\u8df5\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u7528\u76f8\u5173\u6280\u672f\u3002"}}
{"id": "2602.15879", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.15879", "abs": "https://arxiv.org/abs/2602.15879", "authors": ["Qing Yang", "Yuhao Jiang", "Rui Wang", "Jipeng Guo", "Yejiang Wang", "Xinghe Cheng", "Zezheng Wu", "Jiapu Wang", "Jingwei Zhang"], "title": "BamaER: A Behavior-Aware Memory-Augmented Model for Exercise Recommendation", "comment": null, "summary": "Exercise recommendation focuses on personalized exercise selection conditioned on students' learning history, personal interests, and other individualized characteristics. Despite notable progress, most existing methods represent student learning solely as exercise sequences, overlooking rich behavioral interaction information. This limited representation often leads to biased and unreliable estimates of learning progress. Moreover, fixed-length sequence segmentation limits the incorporation of early learning experiences, thereby hindering the modeling of long-term dependencies and the accurate estimation of knowledge mastery. To address these limitations, we propose BamaER, a Behavior-aware memory-augmented Exercise Recommendation framework that comprises three core modules: (i) the learning progress prediction module that captures heterogeneous student interaction behaviors via a tri-directional hybrid encoding scheme; (ii) the memory-augmented knowledge tracing module that maintains a dynamic memory matrix to jointly model historical and current knowledge states for robust mastery estimation; and (iii) the exercise filtering module that formulates candidate selection as a diversity-aware optimization problem, solved via the Hippopotamus Optimization Algorithm to reduce redundancy and improve recommendation coverage. Experiments on five real-world educational datasets show that BamaER consistently outperforms state-of-the-art baselines across a range of evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u884c\u4e3a\u611f\u77e5\u548c\u8bb0\u5fc6\u589e\u5f3a\u7684\u7ec3\u4e60\u63a8\u8350\u6846\u67b6BamaER\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u6765\u63d0\u9ad8\u5b66\u4e60\u8fdb\u5ea6\u9884\u6d4b\u3001\u77e5\u8bc6\u638c\u63e1\u4f30\u8ba1\u4ee5\u53ca\u7ec3\u4e60\u63a8\u8350\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5b9e\u9645\u6559\u80b2\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u8fd0\u52a8\u63a8\u8350\u65b9\u6cd5\u4e3b\u8981\u5c06\u5b66\u751f\u7684\u5b66\u4e60\u8868\u793a\u4e3a\u8fd0\u52a8\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u4e30\u5bcc\u7684\u884c\u4e3a\u4ea4\u4e92\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u5b66\u4e60\u8fdb\u5ea6\u7684\u4f30\u8ba1\u5b58\u5728\u504f\u5dee\u4e14\u4e0d\u53ef\u9760\u3002\u6b64\u5916\uff0c\u56fa\u5b9a\u957f\u5ea6\u7684\u5e8f\u5217\u5206\u5272\u9650\u5236\u4e86\u65e9\u671f\u5b66\u4e60\u7ecf\u9a8c\u7684\u6574\u5408\uff0c\u963b\u788d\u4e86\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u7684\u5efa\u6a21\u548c\u77e5\u8bc6\u638c\u63e1\u7684\u51c6\u786e\u4f30\u8ba1\u3002", "method": "BamaER\u6846\u67b6\u7531\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u7ec4\u6210\uff1a(i) \u901a\u8fc7\u4e09\u5411\u6df7\u5408\u7f16\u7801\u65b9\u6848\u6355\u6349\u5f02\u6784\u5b66\u751f\u4e92\u52a8\u884c\u4e3a\u7684\u5b66\u4e60\u8fdb\u5ea6\u9884\u6d4b\u6a21\u5757\uff1b(ii) \u7ef4\u6301\u52a8\u6001\u8bb0\u5fc6\u77e9\u9635\u4ee5\u8054\u5408\u5efa\u6a21\u5386\u53f2\u548c\u5f53\u524d\u77e5\u8bc6\u72b6\u6001\u7684\u8bb0\u5fc6\u589e\u5f3a\u578b\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u5757\uff0c\u7528\u4e8e\u7a33\u5065\u638c\u63e1\u4f30\u8ba1\uff1b(iii) \u5c06\u5019\u9009\u9009\u62e9\u516c\u5f0f\u5316\u4e3a\u591a\u6837\u6027\u610f\u8bc6\u4f18\u5316\u95ee\u9898\u7684\u7ec3\u4e60\u8fc7\u6ee4\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u6cb3\u9a6c\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u4ee5\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u63a8\u8350\u8986\u76d6\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6559\u80b2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cBamaER\u5728\u4e00\u7cfb\u5217\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u3002", "conclusion": "BamaER\u6846\u67b6\u901a\u8fc7\u66f4\u5168\u9762\u5730\u8003\u8651\u5b66\u751f\u7684\u884c\u4e3a\u7279\u5f81\u4e0e\u5386\u53f2\u5b66\u4e60\u7ecf\u5386\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u7ec3\u4e60\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2602.16541", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16541", "abs": "https://arxiv.org/abs/2602.16541", "authors": ["Santiago de Leon-Martinez", "Robert Moro", "Branislav Kveton", "Maria Bielikova"], "title": "From Latent to Observable Position-Based Click Models in Carousel Interfaces", "comment": null, "summary": "Click models are a central component of learning and evaluation in recommender systems, yet most existing models are designed for single ranked-list interfaces. In contrast, modern recommender platforms increasingly use complex interfaces such as carousels, which consist of multiple swipeable lists that enable complex user browsing behaviors.\n  In this paper, we study position-based click models in carousel interfaces and examine optimization methods, model structure, and alignment with user behavior. We propose three novel position-based models tailored to carousels, including the first position-based model without latent variables that incorporates observed examination signals derived from eye tracking data, called the Observed Examination Position-Based Model (OEPBM). We develop a general implementation of these carousel click models, supporting multiple optimization techniques and conduct experiments comparing gradient-based methods with classical approaches, namely expectation-maximization and maximum likelihood estimation.\n  Our results show that gradient-based optimization consistently achieve better click likelihoods. Among the evaluated models, the OEPBM achieves the strongest performance in click prediction and produces examination patterns that most closely align to user behavior. However, we also demonstrate that strong click fit does not imply realistic modeling of user examination and browsing patterns. This reveals a fundamental limitation of click-only models in complex interfaces and the need for incorporating additional behavioral signals when designing click models for carousel-based recommender systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8f6e\u64ad\u754c\u9762\u4e2d\u7684\u4f4d\u7f6e\u70b9\u51fb\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u68c0\u67e5\u4fe1\u53f7\u7684\u65b0\u6a21\u578bOEPBM\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u70b9\u51fb\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002\u4f46\u540c\u65f6\u6307\u51fa\uff0c\u4ec5\u4f9d\u8d56\u70b9\u51fb\u6570\u636e\u7684\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u7528\u6237\u7684\u771f\u5b9e\u6d4f\u89c8\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7684\u70b9\u51fb\u6a21\u578b\u5927\u591a\u4e3a\u5355\u4e00\u6392\u540d\u5217\u8868\u754c\u9762\u8bbe\u8ba1\uff0c\u800c\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u590d\u6742\u7684\u754c\u9762\u5982\u8f6e\u64ad\u56fe\u3002\u8fd9\u79cd\u754c\u9762\u652f\u6301\u66f4\u590d\u6742\u7684\u7528\u6237\u6d4f\u89c8\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8f6e\u64ad\u56fe\u754c\u9762\u5f00\u53d1\u65b0\u7684\u70b9\u51fb\u6a21\u578b\u6765\u66f4\u597d\u5730\u7406\u89e3\u548c\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u4e13\u4e3a\u8f6e\u64ad\u56fe\u8bbe\u8ba1\u7684\u4f4d\u7f6e\u70b9\u51fb\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u62ec\u9996\u4e2a\u65e0\u6f5c\u5728\u53d8\u91cf\u4e14\u7ed3\u5408\u4e86\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u5f97\u51fa\u7684\u89c2\u5bdf\u68c0\u67e5\u4fe1\u53f7\u7684\u6a21\u578b\uff08OEPBM\uff09\u3002\u5b9e\u73b0\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u652f\u6301\u591a\u79cd\u4f18\u5316\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u4e0e\u7ecf\u5178\u65b9\u6cd5\uff08\u5982\u671f\u671b\u6700\u5927\u5316\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff09\u7684\u6548\u679c\u3002", "result": "\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u70b9\u51fb\u53ef\u80fd\u6027\u9884\u6d4b\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u5728\u6240\u6709\u8bc4\u4f30\u7684\u6a21\u578b\u4e2d\uff0cOEPBM\u5728\u70b9\u51fb\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u5176\u4ea7\u751f\u7684\u68c0\u67e5\u6a21\u5f0f\u6700\u63a5\u8fd1\u7528\u6237\u5b9e\u9645\u884c\u4e3a\u3002\u7136\u800c\uff0c\u7814\u7a76\u8fd8\u8868\u660e\u826f\u597d\u7684\u70b9\u51fb\u62df\u5408\u5e76\u4e0d\u610f\u5473\u7740\u7528\u6237\u68c0\u67e5\u548c\u6d4f\u89c8\u6a21\u5f0f\u88ab\u771f\u5b9e\u5730\u5efa\u6a21\u3002", "conclusion": "\u867d\u7136OEPBM\u5728\u70b9\u51fb\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ec5\u4f9d\u9760\u70b9\u51fb\u6570\u636e\u7684\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u6355\u6349\u7528\u6237\u5728\u590d\u6742\u754c\u9762\u4e0a\u7684\u5b9e\u9645\u6d4f\u89c8\u6a21\u5f0f\u3002\u8fd9\u63ed\u793a\u4e86\u5355\u9760\u70b9\u51fb\u6570\u636e\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u4e3a\u57fa\u4e8e\u8f6e\u64ad\u56fe\u7684\u63a8\u8350\u7cfb\u7edf\u8bbe\u8ba1\u70b9\u51fb\u6a21\u578b\u65f6\u6574\u5408\u989d\u5916\u884c\u4e3a\u4fe1\u53f7\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16671", "abs": "https://arxiv.org/abs/2602.16671", "authors": ["Jaid Monwar Chowdhury", "Chi-An Fu", "Reyhaneh Jabbarvand"], "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation", "comment": "9 pages, 6 figures, 4 tables", "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPARC\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3C\u8bed\u8a00\u81ea\u52a8\u5316\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u9047\u5230\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002\u901a\u8fc7\u56db\u4e2a\u9636\u6bb5\u7684\u64cd\u4f5c\uff0cSPARC\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u9488\u5bf9C\u8bed\u8a00\u4e2d\u7531\u4e8e\u6307\u9488\u7b97\u672f\u548c\u624b\u52a8\u5185\u5b58\u7ba1\u7406\u5e26\u6765\u7684\u4e25\u683c\u8bed\u6cd5\u9650\u5236\uff0c\u4f7f\u5f97\u4ece\u9ad8\u5c42\u6b21\u7a0b\u5e8f\u610f\u56fe\u5230\u4ee3\u7801\u751f\u6210\u4e4b\u95f4\u5b58\u5728\u8f83\u5927\u8bed\u4e49\u5dee\u8ddd\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u76f4\u63a5\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002\u8fd9\u5bfc\u81f4\u4e86\u8bf8\u5982\u65e0\u6cd5\u7f16\u8bd1\u3001\u865a\u5e7b\u51fd\u6570\u7b7e\u540d\u4ee5\u53ca\u4f4e\u5206\u652f\u8986\u76d6\u7387\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSPARC\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u6b65\u9aa4\u6765\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff1a(1) \u63a7\u5236\u6d41\u56fe(CFG)\u5206\u6790\uff1b(2) \u901a\u8fc7\u64cd\u4f5c\u6620\u5c04\u5c06LLM\u63a8\u7406\u4e0e\u9a8c\u8bc1\u8fc7\u7684\u5b9e\u7528\u8f85\u52a9\u5de5\u5177\u76f8\u7ed3\u5408\uff1b(3) \u9488\u5bf9\u8def\u5f84\u7684\u76ee\u6807\u6d4b\u8bd5\u751f\u6210\uff1b(4) \u5229\u7528\u7f16\u8bd1\u5668\u548c\u8fd0\u884c\u65f6\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u81ea\u4fee\u6b63\u9a8c\u8bc1\u5faa\u73af\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u572859\u4e2a\u771f\u5b9e\u4e16\u754c\u53ca\u7b97\u6cd5\u4e3b\u9898\u4e0a\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u63d0\u793a\u751f\u6210\u7684\u57fa\u7840\u7ebf\uff0cSPARC\u5728\u7ebf\u8986\u76d6\u7387\u63d0\u9ad8\u4e8631.36%\uff0c\u5206\u652f\u8986\u76d6\u7387\u63d0\u5347\u4e8626.01%\uff0c\u53d8\u5f02\u5f97\u5206\u589e\u52a0\u4e8620.78%\u3002\u6b64\u5916\uff0cSPARC\u8fd8\u4fdd\u6301\u4e8694.3%\u7684\u6d4b\u8bd5\u6848\u4f8b\u7ecf\u8fc7\u8fed\u4ee3\u4fee\u590d\u540e\u4ecd\u7136\u6709\u6548\uff0c\u5e76\u4e14\u751f\u6210\u7684\u4ee3\u7801\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u8bc4\u4ef7\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u63a8\u7406\u4e0e\u7a0b\u5e8f\u7ed3\u6784\u76f8\u5339\u914d\uff0cSPARC\u4e3a\u5de5\u4e1a\u7ea7\u9057\u7559C\u4ee3\u7801\u5e93\u7684\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.16347", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16347", "abs": "https://arxiv.org/abs/2602.16347", "authors": ["Jon Vehovar", "Miha Rot", "Matja\u017e Depolli", "Gregor Kosec"], "title": "Load Balanced Parallel Node Generation for Meshless Numerical Methods", "comment": null, "summary": "Meshless methods are used to solve partial differential equations by approximating differential operators at a node as a weighted sum of values at its neighbours. One of the algorithms for generating nodes suitable for meshless numerical analysis is an n-dimensional Poisson disc sampling based method. It can handle complex geometries and supports variable node density, a crucial feature for adaptive analysis. We modify this method for parallel execution using coupled spatial indexing and work distribution hypertrees. The latter is prebuilt according to the node density function, ensuring that each leaf represents a balanced work unit. Threads advance separate fronts and claim work hypertree leaves as needed while avoiding leaves neighbouring those claimed by other threads. Node placement constraints and the partially prebuilt spatial hypertree are combined to eliminate the need to lock the tree while it is being modified. Thread collision handling is managed by the work hypertree at the leaf level, drastically reducing the number of required mutex acquisitions for point insertion collision checks. We explore the behaviour of the proposed algorithm and compare the performance with existing attempts at parallelisation and consider the requirements for adapting the developed algorithm to distributed systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8026\u5408\u7a7a\u95f4\u7d22\u5f15\u548c\u5de5\u4f5c\u5206\u914d\u8d85\u6811\u7684\u5e76\u884c\u6267\u884c\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdbn\u7ef4Poisson\u5706\u76d8\u91c7\u6837\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u70b9\u63d2\u5165\u78b0\u649e\u68c0\u67e5\u6240\u9700\u7684\u4e92\u65a5\u9501\u83b7\u53d6\u6b21\u6570\uff0c\u5e76\u63a2\u8ba8\u4e86\u8be5\u7b97\u6cd5\u7684\u884c\u4e3a\u53ca\u5176\u4e0e\u73b0\u6709\u5e76\u884c\u5316\u5c1d\u8bd5\u7684\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u65e0\u7f51\u683c\u6570\u503c\u5206\u6790\u4e2d\u8282\u70b9\u751f\u6210\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u9488\u5bf9\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4ee5\u53ca\u652f\u6301\u53ef\u53d8\u8282\u70b9\u5bc6\u5ea6\u7684\u9700\u6c42\uff0c\u7814\u7a76\u8005\u4eec\u5bfb\u6c42\u5bf9\u73b0\u6709\u7684n\u7ef4Poisson\u5706\u76d8\u91c7\u6837\u65b9\u6cd5\u8fdb\u884c\u6539\u8fdb\uff0c\u4f7f\u4e4b\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u8026\u5408\u7684\u7a7a\u95f4\u7d22\u5f15\u4e0e\u9884\u5148\u6784\u5efa\u7684\u5de5\u4f5c\u5206\u914d\u8d85\u6811\u6765\u4fee\u6539n\u7ef4Poisson\u5706\u76d8\u91c7\u6837\u65b9\u6cd5\uff0c\u5176\u4e2d\u5de5\u4f5c\u5206\u914d\u8d85\u6811\u6839\u636e\u8282\u70b9\u5bc6\u5ea6\u51fd\u6570\u9884\u5efa\u800c\u6210\uff0c\u786e\u4fdd\u6bcf\u4e2a\u53f6\u8282\u70b9\u4ee3\u8868\u4e00\u4e2a\u5747\u8861\u7684\u5de5\u4f5c\u5355\u5143\u3002\u7ebf\u7a0b\u5206\u522b\u63a8\u8fdb\u4e0d\u540c\u7684\u524d\u7aef\uff0c\u5e76\u5728\u9700\u8981\u65f6\u58f0\u660e\u5de5\u4f5c\u8d85\u6811\u7684\u53f6\u5b50\u8282\u70b9\uff0c\u540c\u65f6\u907f\u514d\u58f0\u660e\u88ab\u5176\u4ed6\u7ebf\u7a0b\u5360\u7528\u7684\u76f8\u90bb\u53f6\u5b50\u8282\u70b9\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u70b9\u63d2\u5165\u8fc7\u7a0b\u4e2d\u56e0\u78b0\u649e\u68c0\u67e5\u800c\u9700\u83b7\u5f97\u4e92\u65a5\u9501\u7684\u6570\u91cf\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u5e76\u884c\u6267\u884c\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u5982\u4f55\u5c06\u5f00\u53d1\u51fa\u7684\u7b97\u6cd5\u9002\u5e94\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8981\u6c42\u3002", "conclusion": "\u8fd9\u79cd\u6539\u8fdb\u540e\u7684n\u7ef4Poisson\u5706\u76d8\u91c7\u6837\u65b9\u6cd5\u4e3a\u65e0\u7f51\u683c\u6570\u503c\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u52a0\u9ad8\u6548\u7684\u652f\u6301\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u51e0\u4f55\u7279\u6027\u548c\u9700\u8981\u81ea\u9002\u5e94\u8282\u70b9\u5bc6\u5ea6\u7684\u60c5\u51b5\u65f6\u3002\u5b83\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5e76\u884c\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u6267\u884c\u6548\u7387\uff0c\u4e5f\u4e3a\u672a\u6765\u6269\u5c55\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\u6253\u4e0b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16587", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.16587", "abs": "https://arxiv.org/abs/2602.16587", "authors": ["Luankang Zhang", "Yonghao Huang", "Hang Lv", "Mingjia Yin", "Liangyue Li", "Zulong Chen", "Hao Wang", "Enhong Chen"], "title": "Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models", "comment": null, "summary": "Integrating Chain-of-Thought (CoT) reasoning into Semantic ID-based recommendation foundation models (such as OpenOneRec) often paradoxically degrades recommendation performance. We identify the root cause as textual inertia from the General Subspace, where verbose reasoning dominates inference and causes the model to neglect critical Semantic ID. To address this, we propose a training-free Inference-Time Subspace Alignment framework. By compressing reasoning chains and applying bias-subtracted contrastive decoding, our approach mitigates ungrounded textual drift. Experiments show this effectively calibrates inference, allowing foundation models to leverage reasoning without sacrificing ID-grounded accuracy.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c06\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6574\u5408\u5230\u57fa\u4e8e\u8bed\u4e49ID\u7684\u63a8\u8350\u57fa\u7840\u6a21\u578b\u4e2d\u4f1a\u964d\u4f4e\u63a8\u8350\u6027\u80fd\uff0c\u539f\u56e0\u5728\u4e8e\u901a\u7528\u5b50\u7a7a\u95f4\u4e2d\u7684\u6587\u672c\u60ef\u6027\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u95f4\u5b50\u7a7a\u95f4\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u63a8\u7406\u94fe\u5e76\u5e94\u7528\u504f\u5dee\u51cf\u53bb\u5bf9\u6bd4\u89e3\u7801\u6765\u51cf\u5c11\u65e0\u6839\u636e\u7684\u6587\u672c\u6f02\u79fb\uff0c\u4ece\u800c\u5728\u4e0d\u727a\u7272\u57fa\u4e8eID\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u5229\u7528\u63a8\u7406\u3002", "motivation": "\u7814\u7a76\u8005\u6ce8\u610f\u5230\u5c06\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6574\u5408\u81f3\u57fa\u4e8e\u8bed\u4e49ID\u7684\u63a8\u8350\u7cfb\u7edf\u57fa\u7840\u6a21\u578b\u65f6\uff0c\u63a8\u8350\u6548\u679c\u53cd\u800c\u4e0b\u964d\u4e86\u3002\u8fd9\u4e00\u73b0\u8c61\u80cc\u540e\u7684\u539f\u56e0\u88ab\u5f52\u7ed3\u4e8e\u901a\u7528\u5b50\u7a7a\u95f4\u5185\u5197\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u6a21\u578b\u5ffd\u89c6\u4e86\u5173\u952e\u7684\u8bed\u4e49ID\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u9700\u8981\u989d\u5916\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u671f\u5b50\u7a7a\u95f4\u5bf9\u9f50\u67b6\u6784\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u538b\u7f29\u63a8\u7406\u94fe\u6761\u4ee5\u53ca\u91c7\u7528\u53bb\u9664\u504f\u5dee\u540e\u7684\u5bf9\u6bd4\u89e3\u7801\u6280\u672f\uff0c\u4ee5\u51cf\u8f7b\u975e\u57fa\u4e8e\u5b9e\u9645\u5185\u5bb9\u7684\u6587\u672c\u6f02\u79fb\u73b0\u8c61\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8c03\u6574\u6a21\u578b\u63a8\u65ad\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u57fa\u7840\u6a21\u578b\u80fd\u591f\u5728\u4fdd\u6301\u57fa\u4e8eID\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u66f4\u597d\u5730\u8fd0\u7528\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u63a8\u7406\u65f6\u523b\u5b50\u7a7a\u95f4\u5bf9\u9f50\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u57fa\u4e8e\u8bed\u4e49ID\u51c6\u786e\u6027\u7684\u540c\u65f6\u589e\u5f3a\u63a8\u8350\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.16362", "categories": ["cs.DC", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16362", "abs": "https://arxiv.org/abs/2602.16362", "authors": ["MHD Saria Allahham", "Hossam S. Hassanein"], "title": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability", "comment": null, "summary": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\uff08XEC\uff09\u4e2d\u8ba1\u7b97\u53ef\u9760\u6027\u7684\u5206\u6790\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u5728\u7ed9\u5b9a\u670d\u52a1\u8d28\u91cf(QoS)\u9608\u503c\u4e0b\u5373\u65f6\u5bb9\u91cf\u6ee1\u8db3\u9700\u6c42\u7684\u6982\u7387\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e24\u79cd\u4fe1\u606f\u5236\u5ea6\u4e0b\u7684\u95ed\u5f0f\u53ef\u9760\u6027\u8868\u8fbe\u5f0f\uff0c\u5e76\u6269\u5c55\u5230\u591a\u8bbe\u5907\u90e8\u7f72\u573a\u666f\uff0c\u540c\u65f6\u63a8\u5bfc\u51fa\u6700\u4f18\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u89c4\u5219\u548c\u8bbe\u5907\u9009\u62e9\u7684\u89e3\u6790\u754c\u9650\u3002\u901a\u8fc7YOLO11m\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u5bf9\u8c61\u68c0\u6d4b\u4f5c\u4e3a\u4ee3\u8868\u6027\u5206\u5e03\u5f0f\u63a8\u7406\u6d41\u5de5\u4f5c\u8d1f\u8f7d\u6765\u9a8c\u8bc1\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740AI\u9a71\u52a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8d8a\u6765\u8d8a\u591a\u5730\u5206\u5e03\u5728\u6d88\u8d39\u8005\u62e5\u6709\u7684\u8bbe\u5907\u4e0a\u4ee5\u5229\u7528\u5176\u63a5\u8fd1\u7528\u6237\u7684\u4f18\u52bf\u548c\u666e\u904d\u53ef\u7528\u6027\uff0c\u5982\u4f55\u786e\u4fdd\u8fd9\u4e9b\u8bbe\u5907\u6216\u8bbe\u5907\u96c6\u5408\u80fd\u591f\u7ef4\u6301\u6d41\u670d\u52a1\u6240\u9700\u7684\u5904\u7406\u901f\u7387\u6210\u4e3a\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u7531\u4e8e\u6d88\u8d39\u8005\u8bbe\u5907\u56e0\u7ade\u4e89\u5e94\u7528\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u4f7f\u7528\u6a21\u5f0f\u8868\u73b0\u51fa\u6ce2\u52a8\u7684\u8ba1\u7b97\u53ef\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u91cf\u5316\u8bbe\u5907\u4fdd\u6301\u6240\u9700\u5904\u7406\u901f\u7387\u7684\u6982\u7387\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u8ba1\u7b97\u53ef\u9760\u6027\u7684\u5206\u6790\u6846\u67b6\uff0c\u6b64\u6846\u67b6\u57fa\u4e8e\u4e24\u79cd\u4fe1\u606f\u4f53\u5236\uff1a\u6700\u5c0f\u4fe1\u606f(MI)\uff0c\u4ec5\u9700\u58f0\u660e\u7684\u64cd\u4f5c\u8fb9\u754c\uff1b\u4ee5\u53ca\u5386\u53f2\u6570\u636e\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4ece\u8fc7\u53bb\u89c2\u5bdf\u4e2d\u7cbe\u70bc\u4f30\u8ba1\u3002\u8be5\u6846\u67b6\u8fdb\u4e00\u6b65\u6269\u5c55\u81f3\u652f\u6301\u7cfb\u5217\u3001\u5e76\u884c\u53ca\u5206\u533a\u5de5\u4f5c\u8d1f\u8f7d\u914d\u7f6e\u7684\u591a\u8bbe\u5907\u90e8\u7f72\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e86\u6700\u4f73\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u89c4\u5219\u548c\u7528\u4e8e\u8bbe\u5907\u9009\u62e9\u7684\u5206\u6790\u754c\u9650\uff0c\u4e3a\u7f16\u6392\u8005\u63d0\u4f9b\u4e86\u8bc4\u4f30\u90e8\u7f72\u53ef\u884c\u6027\u548c\u914d\u7f6e\u5206\u5e03\u5f0f\u6d41\u7cfb\u7edf\u7684\u5b9e\u7528\u5de5\u5177\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u7684\u5bb9\u91cf\u4e0e\u9700\u6c42\u914d\u7f6e\u4e0b\uff0c\u5206\u6790\u9884\u6d4b\u4e0e\u8499\u7279\u5361\u6d1b\u62bd\u6837\u53ca\u7ecf\u9a8c\u6d4b\u91cf\u4e4b\u95f4\u5b58\u5728\u826f\u597d\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u9762\u5bf9\u7684\u8ba1\u7b97\u8d44\u6e90\u6ce2\u52a8\u6027\u6311\u6218\uff0c\u4e3a\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u624b\u6bb5\u3002"}}
{"id": "2602.15955", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15955", "abs": "https://arxiv.org/abs/2602.15955", "authors": ["Shumeng Chen", "Jane E. Huggins", "Tianwen Ma"], "title": "Adaptive Semi-Supervised Training of P300 ERP-BCI Speller System with Minimum Calibration Effort", "comment": "8 pages, 8 figures", "summary": "A P300 ERP-based Brain-Computer Interface (BCI) speller is an assistive communication tool. It searches for the P300 event-related potential (ERP) elicited by target stimuli, distinguishing it from the neural responses to non-target stimuli embedded in electroencephalogram (EEG) signals. Conventional methods require a lengthy calibration procedure to construct the binary classifier, which reduced overall efficiency. Thus, we proposed a unified framework with minimum calibration effort such that, given a small amount of labeled calibration data, we employed an adaptive semi-supervised EM-GMM algorithm to update the binary classifier. We evaluated our method based on character-level prediction accuracy, information transfer rate (ITR), and BCI utility. We applied calibration on training data and reported results on testing data. Our results indicate that, out of 15 participants, 9 participants exceed the minimum character-level accuracy of 0.7 using either on our adaptive method or the benchmark, and 7 out of these 9 participants showed that our adaptive method performed better than the benchmark. The proposed semi-supervised learning framework provides a practical and efficient alternative to improve the overall spelling efficiency in the real-time BCI speller system, particularly in contexts with limited labeled data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eP300 ERP\u7684\u8111\u673a\u63a5\u53e3\u62fc\u5199\u5668\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u81ea\u9002\u5e94\u534a\u76d1\u7763EM-GMM\u7b97\u6cd5\u51cf\u5c11\u4e86\u6821\u51c6\u6240\u9700\u7684\u6570\u636e\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u62fc\u5199\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u572815\u540d\u53c2\u4e0e\u8005\u4e2d\uff0c\u67099\u540d\u53c2\u4e0e\u8005\u7684\u5b57\u7b26\u7ea7\u51c6\u786e\u7387\u8d85\u8fc7\u4e860.7\uff0c\u5e76\u4e14\u5728\u8fd9\u4e9b\u53c2\u4e0e\u8005\u4e2d\uff0c\u67097\u540d\u663e\u793a\u51fa\u65b0\u65b9\u6cd5\u6bd4\u57fa\u51c6\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4f20\u7edf\u7684P300 ERP\u8111\u673a\u63a5\u53e3\u62fc\u5199\u5668\u9700\u8981\u957f\u65f6\u95f4\u7684\u6821\u51c6\u8fc7\u7a0b\u6765\u6784\u5efa\u4e8c\u5143\u5206\u7c7b\u5668\uff0c\u8fd9\u964d\u4f4e\u4e86\u6574\u4f53\u6548\u7387\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u53ea\u9700\u8981\u5c11\u91cf\u6807\u8bb0\u6821\u51c6\u6570\u636e\u5373\u53ef\u66f4\u65b0\u5206\u7c7b\u5668\u7684\u9ad8\u6548\u6846\u67b6\u3002", "method": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e86\u81ea\u9002\u5e94\u534a\u76d1\u7763EM-GMM\uff08\u671f\u671b\u6700\u5927\u5316-\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff09\u7b97\u6cd5\uff0c\u5141\u8bb8\u7cfb\u7edf\u5728\u4ec5\u6709\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5730\u66f4\u65b0\u5176\u4e8c\u5143\u5206\u7c7b\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u4e0e\u8005\u6765\u8bf4\u4f18\u4e8e\u4f20\u7edf\u57fa\u51c6\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u63d0\u9ad8\u5b57\u7b26\u7ea7\u522b\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4e5f\u663e\u793a\u51fa\u4e86\u66f4\u9ad8\u7684\u4fe1\u606f\u4f20\u8f93\u7387\u548cBCI\u5b9e\u7528\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e3a\u5b9e\u65f6\u8111\u673a\u63a5\u53e3\u62fc\u5199\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6807\u7b7e\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u80fd\u663e\u8457\u6539\u5584\u62fc\u5199\u6548\u7387\u3002"}}
{"id": "2602.16603", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16603", "abs": "https://arxiv.org/abs/2602.16603", "authors": ["Chia-chi Hsieh", "Zan Zong", "Xinyang Chen", "Jianjiang Li", "Jidong Zhai", "Lijie Wen"], "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving", "comment": "13 pages", "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlowPrefill\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5e76\u53d1\u8bf7\u6c42\u65f6\u9762\u4e34\u7684\u5934\u963b\u585e\u95ee\u9898\u3002\u901a\u8fc7\u64cd\u4f5c\u7ea7\u62a2\u5360\u548c\u4e8b\u4ef6\u9a71\u52a8\u8c03\u5ea6\u4e24\u9879\u521b\u65b0\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u540c\u65f6\u4f18\u5316\u54cd\u5e94\u65f6\u95f4\u548c\u541e\u5410\u91cf\uff0c\u636e\u5b9e\u6d4b\u6570\u636e\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\u76f8\u6bd4\uff0c\u6700\u5927\u541e\u5410\u91cf\u63d0\u9ad8\u4e865.6\u500d\uff0c\u5e76\u80fd\u6ee1\u8db3\u591a\u6837\u5316\u7684\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\u3002", "motivation": "\u968f\u7740\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u6c42\u7684\u589e\u957f\uff0c\u670d\u52a1\u7cfb\u7edf\u9700\u8981\u5904\u7406\u5177\u6709\u4e0d\u540c\u670d\u52a1\u7ea7\u522b\u76ee\u6807\uff08SLOs\uff09\u7684\u8bb8\u591a\u5e76\u53d1\u8bf7\u6c42\u3002\u8fd9\u52a0\u5267\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578b\u9884\u586b\u5145\u9636\u6bb5\u4e2d\u7684\u5934\u963b\u585e\uff08HoL\uff09\uff0c\u5176\u4e2d\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u8bf7\u6c42\u72ec\u5360\u8d44\u6e90\u5e76\u5ef6\u8fdf\u4f18\u5148\u7ea7\u66f4\u9ad8\u7684\u8bf7\u6c42\uff0c\u5bfc\u81f4\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09SLO\u8fdd\u89c4\u5e7f\u6cdb\u53d1\u751f\u3002\u5c3d\u7ba1\u5206\u5757\u9884\u586b\u5145\u53ef\u4ee5\u5b9e\u73b0\u4e2d\u65ad\u6027\uff0c\u4f46\u5b83\u5f15\u5165\u4e86\u54cd\u5e94\u6027\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u56fa\u6709\u6298\u8877\uff1a\u51cf\u5c11\u5757\u5927\u5c0f\u53ef\u4ee5\u6539\u5584\u54cd\u5e94\u5ef6\u8fdf\u4f46\u4f1a\u964d\u4f4e\u8ba1\u7b97\u6548\u7387\uff1b\u589e\u52a0\u5757\u5927\u5c0f\u867d\u7136\u80fd\u6700\u5927\u5316\u541e\u5410\u91cf\u5374\u52a0\u91cd\u4e86\u963b\u585e\u60c5\u51b5\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u62a2\u5360\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86FlowPrefill\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4e3b\u8981\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u521b\u65b0\u5b9e\u73b0\u81ea\u9002\u5e94\u9884\u586b\u5145\u8c03\u5ea6\uff1a1) \u64cd\u4f5c\u7ea7\u62a2\u5360\uff08Operator-Level Preemption\uff09\uff0c\u5229\u7528\u64cd\u4f5c\u8fb9\u754c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u6267\u884c\u4e2d\u65ad\uff0c\u800c\u4e0d\u4f1a\u5e26\u6765\u56fa\u5b9a\u5c0f\u5757\u5212\u5206\u6240\u5173\u8054\u7684\u6548\u7387\u635f\u5931\uff1b2) \u4e8b\u4ef6\u9a71\u52a8\u8c03\u5ea6\uff08Event-Driven Scheduling\uff09\uff0c\u4ec5\u5728\u8bf7\u6c42\u5230\u8fbe\u6216\u5b8c\u6210\u4e8b\u4ef6\u89e6\u53d1\u65f6\u624d\u4f5c\u51fa\u8c03\u5ea6\u51b3\u7b56\uff0c\u4ece\u800c\u652f\u6301\u9ad8\u6548\u7684\u62a2\u5360\u54cd\u5e94\u6027\u540c\u65f6\u6700\u5c0f\u5316\u63a7\u5236\u5e73\u9762\u5f00\u9500\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u751f\u4ea7\u8f68\u8ff9\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u76f8\u6bd4\uff0cFlowPrefill\u53ef\u5c06\u6700\u5927\u826f\u597d\u541e\u5410\u91cf\u63d0\u9ad8\u81f3\u591a5.6\u500d\uff0c\u540c\u65f6\u6ee1\u8db3\u5f02\u6784SLOs\u7684\u9700\u6c42\u3002", "conclusion": "FlowPrefill\u4f5c\u4e3a\u4e00\u79cd\u9488\u5bf9TTFT-\u826f\u597d\u541e\u5410\u91cf\u4f18\u5316\u7684\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u62a2\u5360\u7c92\u5ea6\u4e0e\u8c03\u5ea6\u9891\u7387\u6210\u529f\u89e3\u51b3\u4e86\u54cd\u5e94\u6027\u4e0e\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u5176\u63d0\u51fa\u7684\u64cd\u4f5c\u7ea7\u62a2\u5360\u548c\u4e8b\u4ef6\u9a71\u52a8\u8c03\u5ea6\u7b56\u7565\u4e0d\u4ec5\u6709\u6548\u7f13\u89e3\u4e86\u5934\u963b\u585e\u95ee\u9898\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u786e\u4fdd\u4e86\u591a\u6837\u5316\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u7684\u8fbe\u6210\u3002"}}
{"id": "2602.15961", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15961", "abs": "https://arxiv.org/abs/2602.15961", "authors": ["Zhi Sheng", "Yuan Yuan", "Guozhen Zhang", "Yong Li"], "title": "R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions", "comment": null, "summary": "The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and intensity of climate-driven extreme weather events pose severe threats to grid stability and operational security. Consequently, developing robust forecasting models that can withstand volatile conditions has become a paramount challenge. In this paper, we present R$^2$Energy, a large-scale benchmark for NWP-assisted renewable energy forecasting. It comprises over 10.7 million high-fidelity hourly records from 902 wind and solar stations across four provinces in China, providing the diverse meteorological conditions necessary to capture the wide-ranging variability of renewable generation. We further establish a standardized, leakage-free forecasting paradigm that grants all models identical access to future Numerical Weather Prediction (NWP) signals, enabling fair and reproducible comparison across state-of-the-art representative forecasting architectures. Beyond aggregate accuracy, we incorporate regime-wise evaluation with expert-aligned extreme weather annotations, uncovering a critical ``robustness gap'' typically obscured by average metrics. This gap reveals a stark robustness-complexity trade-off: under extreme conditions, a model's reliability is driven by its meteorological integration strategy rather than its architectural complexity. R$^2$Energy provides a principled foundation for evaluating and developing forecasting models for safety-critical power system applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86R$^2$Energy\uff0c\u4e00\u4e2a\u7528\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u9884\u6d4b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u5b83\u57fa\u4e8e\u4e2d\u56fd\u56db\u4e2a\u7701\u4efd902\u4e2a\u98ce\u80fd\u548c\u592a\u9633\u80fd\u7535\u7ad9\u7684\u8d85\u8fc71070\u4e07\u6761\u9ad8\u4fdd\u771f\u5c0f\u65f6\u8bb0\u5f55\u3002\u8be5\u57fa\u51c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u65e0\u6cc4\u6f0f\u7684\u9884\u6d4b\u8303\u5f0f\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u6a21\u578b\u53ef\u9760\u6027\u4e0e\u5176\u6c14\u8c61\u6574\u5408\u7b56\u7565\u800c\u975e\u67b6\u6784\u590d\u6742\u6027\u76f8\u5173\u7684\u5173\u952e'\u9c81\u68d2\u6027\u5dee\u8ddd'\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u7279\u522b\u662f\u98ce\u80fd\u548c\u592a\u9633\u80fd\u7684\u8fc5\u901f\u6269\u5c55\uff0c\u53ef\u9760\u7684\u9884\u6d4b\u5bf9\u4e8e\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5df2\u8fbe\u5230\u8f83\u9ad8\u7684\u5e73\u5747\u51c6\u786e\u5ea6\uff0c\u4f46\u6c14\u5019\u9a71\u52a8\u7684\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u9891\u53d1\u4e14\u5f3a\u5ea6\u589e\u52a0\u5bf9\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u8fd0\u8425\u5b89\u5168\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u80fd\u591f\u62b5\u5fa1\u6ce2\u52a8\u6761\u4ef6\u7684\u7a33\u5065\u9884\u6d4b\u6a21\u578b\u6210\u4e3a\u4e86\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86R$^2$Energy\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u6570\u503c\u5929\u6c14\u9884\u62a5\uff08NWP\uff09\u8f85\u52a9\u7684\u53ef\u518d\u751f\u80fd\u6e90\u9884\u6d4b\u5927\u89c4\u6a21\u57fa\u51c6\u3002\u5b83\u5305\u542b\u4e86\u6765\u81ea\u4e2d\u56fd\u56db\u4e2a\u7701\u4efd902\u4e2a\u98ce\u529b\u548c\u592a\u9633\u80fd\u53d1\u7535\u7ad9\u7684\u8d85\u8fc710.7\u767e\u4e07\u6761\u9ad8\u4fdd\u771f\u5c0f\u65f6\u8bb0\u5f55\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u65e0\u6cc4\u6f0f\u7684\u9884\u6d4b\u6a21\u5f0f\uff0c\u5141\u8bb8\u6240\u6709\u6a21\u578b\u4ee5\u76f8\u540c\u7684\u65b9\u5f0f\u8bbf\u95ee\u672a\u6765\u7684NWP\u4fe1\u53f7\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u4e0d\u540c\u5148\u8fdb\u9884\u6d4b\u67b6\u6784\u4e4b\u95f4\u7684\u516c\u5e73\u548c\u53ef\u91cd\u590d\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684'\u9c81\u68d2\u6027\u5dee\u8ddd'\uff0c\u8fd9\u901a\u5e38\u88ab\u5e73\u5747\u6307\u6807\u6240\u63a9\u76d6\u3002\u8fd9\u4e2a\u5dee\u8ddd\u663e\u793a\uff0c\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u7684\u53ef\u9760\u6027\u66f4\u591a\u5730\u53d6\u51b3\u4e8e\u5176\u6c14\u8c61\u96c6\u6210\u7b56\u7565\u800c\u4e0d\u662f\u7ed3\u6784\u7684\u590d\u6742\u6027\u3002", "conclusion": "R$^2$Energy\u4e3a\u8bc4\u4f30\u548c\u53d1\u5c55\u9762\u5411\u5b89\u5168\u5173\u952e\u578b\u7535\u529b\u7cfb\u7edf\u5e94\u7528\u7684\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u57fa\u7840\u3002"}}
{"id": "2602.15971", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.15971", "abs": "https://arxiv.org/abs/2602.15971", "authors": ["Cherish Puniani", "Tushar Kumar", "Arnav Bendre", "Gaurav Kumar", "Shree Singhi"], "title": "B-DENSE: Branching For Dense Ensemble Network Learning", "comment": "11 pages, 5 figures, 4 algorithms and 2 tables. Submitted to iclr 2026 delta workshop and still under review", "summary": "Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6B-DENSE\uff0c\u901a\u8fc7\u591a\u5206\u652f\u8f68\u8ff9\u5bf9\u9f50\u6765\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7531\u4e8e\u7a00\u758f\u76d1\u7763\u5bfc\u81f4\u7684\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u548c\u663e\u8457\u79bb\u6563\u5316\u8bef\u5dee\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u53d7\u5230\u975e\u5e73\u8861\u70ed\u529b\u5b66\u7684\u542f\u53d1\uff0c\u5728\u751f\u6210\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5176\u8fed\u4ee3\u91c7\u6837\u6027\u8d28\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3002\u867d\u7136\u6700\u8fd1\u7684\u84b8\u998f\u6280\u672f\u52a0\u901f\u4e86\u91c7\u6837\u8fc7\u7a0b\uff0c\u4f46\u8fd9\u4e9b\u6280\u672f\u5ffd\u7565\u4e86\u4e2d\u95f4\u8f68\u8ff9\u6b65\u9aa4\uff0c\u8fd9\u5bfc\u81f4\u4e86\u7ed3\u6784\u4fe1\u606f\u7684\u4e22\u5931\u5e76\u5f15\u5165\u4e86\u5927\u91cf\u7684\u79bb\u6563\u5316\u8bef\u5dee\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86B-DENSE\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u591a\u5206\u652f\u8f68\u8ff9\u5bf9\u9f50\u3002\u5b66\u751f\u67b6\u6784\u88ab\u4fee\u6539\u4ee5\u8f93\u51faK\u500d\u6269\u5c55\u901a\u9053\uff0c\u6bcf\u4e2a\u5b50\u96c6\u5bf9\u5e94\u4e8e\u4ee3\u8868\u6559\u5e08\u8f68\u8ff9\u4e2d\u7684\u7279\u5b9a\u79bb\u6563\u4e2d\u95f4\u6b65\u9aa4\u7684\u4e00\u4e2a\u5206\u652f\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u5206\u652f\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u5176\u540c\u65f6\u6620\u5c04\u5230\u6559\u5e08\u76ee\u6807\u65f6\u95f4\u6b65\u957f\u7684\u6574\u4e2a\u5e8f\u5217\uff0c\u5f3a\u5236\u6267\u884c\u5bc6\u96c6\u7684\u4e2d\u95f4\u8f68\u8ff9\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u84b8\u998f\u6846\u67b6\u76f8\u6bd4\uff0c\u4ece\u8bad\u7ec3\u7684\u6700\u65e9\u9636\u6bb5\u5f00\u59cb\uff0c\u5b66\u751f\u6a21\u578b\u5c31\u80fd\u591f\u5b66\u4e60\u5982\u4f55\u5bfc\u822a\u89e3\u7a7a\u95f4\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "B-DENSE\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u4e2d\u95f4\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\u89e3\u51b3\u4e86\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u5728\u52a0\u901f\u6269\u6563\u6a21\u578b\u65f6\u9047\u5230\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\u3002"}}
{"id": "2602.15972", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15972", "abs": "https://arxiv.org/abs/2602.15972", "authors": ["Tianchi Zhao", "He Liu", "Hongyin Shi", "Jinliang Li"], "title": "Fast Online Learning with Gaussian Prior-Driven Hierarchical Unimodal Thompson Sampling", "comment": null, "summary": "We study a type of Multi-Armed Bandit (MAB) problems in which arms with a Gaussian reward feedback are clustered. Such an arm setting finds applications in many real-world problems, for example, mmWave communications and portfolio management with risky assets, as a result of the universality of the Gaussian distribution. Based on the Thompson Sampling algorithm with Gaussian prior (TSG) algorithm for the selection of the optimal arm, we propose our Thompson Sampling with Clustered arms under Gaussian prior (TSCG) specific to the 2-level hierarchical structure. We prove that by utilizing the 2-level structure, we can achieve a lower regret bound than we do with ordinary TSG. In addition, when the reward is Unimodal, we can reach an even lower bound on the regret by our Unimodal Thompson Sampling algorithm with Clustered Arms under Gaussian prior (UTSCG). Each of our proposed algorithms are accompanied by theoretical evaluation of the upper regret bound, and our numerical experiments confirm the advantage of our proposed algorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u9ad8\u65af\u5956\u52b1\u53cd\u9988\u7684\u805a\u7c7b\u81c2\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6c64\u666e\u68ee\u91c7\u6837\u7684\u805a\u7c7b\u81c2\u7b97\u6cd5\uff08TSCG\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5229\u7528\u4e24\u5c42\u7ed3\u6784\u65f6\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u666e\u901aTSG\u66f4\u4f4e\u7684\u9057\u61be\u754c\u3002\u6b64\u5916\uff0c\u5f53\u5956\u52b1\u662f\u5355\u5cf0\u5206\u5e03\u65f6\uff0c\u901a\u8fc7\u5355\u5cf0\u6c64\u666e\u68ee\u91c7\u6837\u805a\u7c7b\u81c2\u7b97\u6cd5(UTSCG)\u53ef\u4ee5\u83b7\u5f97\u66f4\u4f4e\u7684\u9057\u61be\u754c\u3002\u7406\u8bba\u8bc4\u4f30\u548c\u6570\u503c\u5b9e\u9a8c\u5747\u8bc1\u5b9e\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u51fa\u73b0\u7684\u4e00\u7c7b\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5176\u4e2d\u81c2\u7684\u5956\u52b1\u53cd\u9988\u670d\u4ece\u9ad8\u65af\u5206\u5e03\u5e76\u4e14\u88ab\u805a\u7c7b\uff0c\u8fd9\u5728\u6beb\u7c73\u6ce2\u901a\u4fe1\u548c\u98ce\u9669\u8d44\u4ea7\u7ec4\u5408\u7ba1\u7406\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u672c\u6587\u7684\u65b9\u6cd5\u57fa\u4e8e\u6c64\u666e\u68ee\u91c7\u6837\u7b97\u6cd5\uff0c\u5e76\u9488\u5bf9\u5177\u6709\u4e24\u5c42\u5206\u5c42\u7ed3\u6784\u7684\u805a\u7c7b\u81c2\u60c5\u51b5\u63d0\u51fa\u4e86\u6539\u8fdb\u7248\u672cTSCG\u3002\u53e6\u5916\uff0c\u5bf9\u4e8e\u5355\u5cf0\u5956\u52b1\u5206\u5e03\u7684\u60c5\u51b5\uff0c\u8fd8\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u7b97\u6cd5UTSCG\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u4e24\u5c42\u7ed3\u6784\u7684TSCG\u7b97\u6cd5\u76f8\u6bd4\u4e8e\u666e\u901a\u7684TSG\u7b97\u6cd5\u80fd\u591f\u8fbe\u5230\u66f4\u4f4e\u7684\u9057\u61be\u754c\uff1b\u800c\u5f53\u5956\u52b1\u5206\u5e03\u4e3a\u5355\u5cf0\u65f6\uff0cUTSCG\u7b97\u6cd5\u80fd\u63d0\u4f9b\u66f4\u4f18\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684TSCG\u53caUTSCG\u7b97\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u5f97\u5230\u4e86\u8f83\u4f4e\u7684\u4e0a\u754c\u9057\u61be\u4f30\u8ba1\uff0c\u5728\u5b9e\u9645\u6570\u503c\u5b9e\u9a8c\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u5176\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.15984", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15984", "abs": "https://arxiv.org/abs/2602.15984", "authors": ["Riccardo De Santi", "Kimon Protopapas", "Ya-Ping Hsieh", "Andreas Krause"], "title": "Verifier-Constrained Flow Expansion for Discovery Beyond the Data", "comment": "ICLR 2026", "summary": "Flow and diffusion models are typically pre-trained on limited available data (e.g., molecular samples), covering only a fraction of the valid design space (e.g., the full molecular space). As a consequence, they tend to generate samples from only a narrow portion of the feasible domain. This is a fundamental limitation for scientific discovery applications, where one typically aims to sample valid designs beyond the available data distribution. To this end, we address the challenge of leveraging access to a verifier (e.g., an atomic bonds checker), to adapt a pre-trained flow model so that its induced density expands beyond regions of high data availability, while preserving samples validity. We introduce formal notions of strong and weak verifiers and propose algorithmic frameworks for global and local flow expansion via probability-space optimization. Then, we present Flow Expander (FE), a scalable mirror descent scheme that provably tackles both problems by verifier-constrained entropy maximization over the flow process noised state space. Next, we provide a thorough theoretical analysis of the proposed method, and state convergence guarantees under both idealized and general assumptions. Ultimately, we empirically evaluate our method on both illustrative, yet visually interpretable settings, and on a molecular design task showcasing the ability of FE to expand a pre-trained flow model increasing conformer diversity while preserving validity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlow Expander (FE)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u7ea6\u675f\u7684\u71b5\u6700\u5927\u5316\u6765\u6269\u5c55\u9884\u8bad\u7ec3\u6d41\u6a21\u578b\u7684\u6709\u6548\u6837\u672c\u751f\u6210\u8303\u56f4\uff0c\u540c\u65f6\u4fdd\u6301\u6837\u672c\u7684\u6709\u6548\u6027\u3002\u5728\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0cFE\u5c55\u793a\u4e86\u5176\u5728\u589e\u52a0\u5206\u5b50\u6784\u8c61\u591a\u6837\u6027\u7684\u540c\u65f6\u4fdd\u6301\u6709\u6548\u6027\u7684\u80fd\u529b\u3002", "motivation": "\u6d41\u52a8\u548c\u6269\u6563\u6a21\u578b\u901a\u5e38\u53ea\u5728\u6709\u9650\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u5bfc\u81f4\u5b83\u4eec\u4ec5\u80fd\u4ece\u53ef\u884c\u57df\u7684\u4e00\u5c0f\u90e8\u5206\u751f\u6210\u6837\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5229\u7528\u9a8c\u8bc1\u5668\uff08\u5982\u539f\u5b50\u952e\u68c0\u67e5\u5668\uff09\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u7684\u6d41\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u8d85\u51fa\u6570\u636e\u9ad8\u53ef\u7528\u533a\u57df\u751f\u6210\u6709\u6548\u7684\u8bbe\u8ba1\u6837\u672c\u3002", "method": "\u5f15\u5165\u4e86\u5f3a\u5f31\u9a8c\u8bc1\u5668\u7684\u5f62\u5f0f\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e86\u5168\u5c40\u548c\u5c40\u90e8\u6d41\u6269\u5c55\u7684\u7b97\u6cd5\u6846\u67b6\u3002\u63a5\u7740\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u955c\u50cf\u4e0b\u964d\u65b9\u6848\u2014\u2014Flow Expander (FE)\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6d41\u8fc7\u7a0b\u566a\u58f0\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u9a8c\u8bc1\u5668\u7ea6\u675f\u71b5\u6700\u5927\u5316\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002", "result": "\u63d0\u4f9b\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u8be6\u5c3d\u7406\u8bba\u5206\u6790\uff0c\u5e76\u9648\u8ff0\u4e86\u5728\u7406\u60f3\u5316\u4e0e\u4e00\u822c\u5047\u8bbe\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\u3002\u6700\u540e\uff0c\u5728\u53ef\u89c6\u5316\u8bbe\u7f6e\u548c\u5206\u5b50\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4ef7\u8868\u660e\uff0cFE\u80fd\u591f\u5728\u4fdd\u6301\u6709\u6548\u6027\u7684\u540c\u65f6\u6269\u5927\u9884\u8bad\u7ec3\u6d41\u6a21\u578b\u7684\u6837\u672c\u751f\u6210\u8303\u56f4\u5e76\u589e\u52a0\u6784\u8c61\u591a\u6837\u6027\u3002", "conclusion": "Flow Expander \u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6269\u5c55\u9884\u8bad\u7ec3\u6d41\u6a21\u578b\u4ee5\u751f\u6210\u66f4\u5e7f\u6cdb\u7684\u6709\u6548\u6837\u672c\uff0c\u7279\u522b\u662f\u5728\u79d1\u5b66\u53d1\u73b0\u5e94\u7528\u4e2d\uff0c\u5b83\u6709\u52a9\u4e8e\u63a2\u7d22\u5df2\u77e5\u6570\u636e\u5206\u5e03\u4e4b\u5916\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002"}}
{"id": "2602.16015", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16015", "abs": "https://arxiv.org/abs/2602.16015", "authors": ["Marzieh Amiri Shahbazi", "Ali Baheri"], "title": "Geometry-Aware Uncertainty Quantification via Conformal Prediction on Manifolds", "comment": null, "summary": "Conformal prediction provides distribution-free coverage guaranties for regression; yet existing methods assume Euclidean output spaces and produce prediction regions that are poorly calibrated when responses lie on Riemannian manifolds. We propose \\emph{adaptive geodesic conformal prediction}, a framework that replaces Euclidean residuals with geodesic nonconformity scores and normalizes them by a cross-validated difficulty estimator to handle heteroscedastic noise. The resulting prediction regions, geodesic caps on the sphere, have position-independent area and adapt their size to local prediction difficulty, yielding substantially more uniform conditional coverage than non-adaptive alternatives. In a synthetic sphere experiment with strong heteroscedasticity and a real-world geomagnetic field forecasting task derived from IGRF-14 satellite data, the adaptive method markedly reduces conditional coverage variability and raises worst-case coverage much closer to the nominal level, while coordinate-based baselines waste a large fraction of coverage area due to chart distortion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6d4b\u5730\u7ebf\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u6d4b\u5730\u7ebf\u975e\u4e00\u81f4\u6027\u5206\u6570\u5e76\u7ed3\u5408\u4e00\u4e2a\u4ea4\u53c9\u9a8c\u8bc1\u96be\u5ea6\u4f30\u8ba1\u5668\u6765\u5904\u7406\u56de\u5f52\u95ee\u9898\u4e2d\u54cd\u5e94\u4f4d\u4e8e\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u60c5\u51b5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6761\u4ef6\u8986\u76d6\u7387\u7684\u53d8\u5f02\u6027\uff0c\u5e76\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u4f7f\u8986\u76d6\u7387\u66f4\u63a5\u8fd1\u540d\u4e49\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5047\u8bbe\u8f93\u51fa\u7a7a\u95f4\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u5728\u54cd\u5e94\u53d8\u91cf\u4f4d\u4e8e\u9ece\u66fc\u6d41\u5f62\u4e0a\u65f6\u4f1a\u4ea7\u751f\u6821\u51c6\u4e0d\u826f\u7684\u9884\u6d4b\u533a\u57df\u3002\u4e3a\u4e86\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5f02\u65b9\u5dee\u566a\u58f0\u60c5\u5f62\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u6d4b\u5730\u7ebf\u800c\u975e\u6b27\u6c0f\u8ddd\u79bb\u7684\u975e\u4e00\u81f4\u6027\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u7ecf\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u7684\u96be\u5ea6\u8bc4\u4f30\u5668\u5bf9\u5176\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\u3002\u7531\u6b64\u4ea7\u751f\u7684\u9884\u6d4b\u533a\u95f4\u662f\u7403\u9762\u4e0a\u7684\u6d4b\u5730\u7ebf\u5e3d\uff0c\u5176\u9762\u79ef\u72ec\u7acb\u4e8e\u4f4d\u7f6e\u4e14\u5927\u5c0f\u53ef\u6839\u636e\u5c40\u90e8\u9884\u6d4b\u96be\u5ea6\u8fdb\u884c\u8c03\u6574\u3002", "result": "\u5728\u5408\u6210\u7403\u4f53\u5b9e\u9a8c\u548c\u57fa\u4e8eIGRF-14\u536b\u661f\u6570\u636e\u7684\u771f\u5b9e\u4e16\u754c\u5730\u78c1\u573a\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6761\u4ef6\u8986\u76d6\u7387\u7684\u53d8\u5316\u6027\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u4e86\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8986\u76d6\u7387\u81f3\u63a5\u8fd1\u540d\u4e49\u6c34\u5e73\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u4e8e\u5750\u6807\u7684\u57fa\u7ebf\u65b9\u6cd5\u7531\u4e8e\u56fe\u8868\u5931\u771f\u800c\u6d6a\u8d39\u4e86\u5927\u91cf\u7684\u8986\u76d6\u9762\u79ef\u3002", "conclusion": "\u81ea\u9002\u5e94\u6d4b\u5730\u7ebf\u5171\u5f62\u9884\u6d4b\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u5f53\u54cd\u5e94\u53d8\u91cf\u4f4d\u4e8e\u9ece\u66fc\u6d41\u5f62\u4e0a\u65f6\u9884\u6d4b\u533a\u95f4\u7684\u6821\u51c6\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u5f3a\u70c8\u5f02\u65b9\u5dee\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2602.16042", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16042", "abs": "https://arxiv.org/abs/2602.16042", "authors": ["KC Santosh", "Srikanth Baride", "Rodrigue Rizk"], "title": "AI-CARE: Carbon-Aware Reporting Evaluation Metric for AI Models", "comment": "7 pages, 3 figures", "summary": "As machine learning (ML) continues its rapid expansion, the environmental cost of model training and inference has become a critical societal concern. Existing benchmarks overwhelmingly focus on standard performance metrics such as accuracy, BLEU, or mAP, while largely ignoring energy consumption and carbon emissions. This single-objective evaluation paradigm is increasingly misaligned with the practical requirements of large-scale deployment, particularly in energy-constrained environments such as mobile devices, developing regions, and climate-aware enterprises. In this paper, we propose AI-CARE, an evaluation tool for reporting energy consumption, and carbon emissions of ML models. In addition, we introduce the carbon-performance tradeoff curve, an interpretable tool that visualizes the Pareto frontier between performance and carbon cost. We demonstrate, through theoretical analysis and empirical validation on representative ML workloads, that carbon-aware benchmarking changes the relative ranking of models and encourages architectures that are simultaneously accurate and environmentally responsible. Our proposal aims to shift the research community toward transparent, multi-objective evaluation and align ML progress with global sustainability goals. The tool and documentation are available at https://github.com/USD-AI-ResearchLab/ai-care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAI-CARE\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u7528\u4e8e\u62a5\u544a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u80fd\u91cf\u6d88\u8017\u548c\u78b3\u6392\u653e\uff0c\u5e76\u5f15\u5165\u4e86\u78b3-\u6027\u80fd\u6298\u8877\u66f2\u7ebf\u6765\u53ef\u89c6\u5316\u6027\u80fd\u4e0e\u78b3\u6210\u672c\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u8be5\u7814\u7a76\u65e8\u5728\u63a8\u52a8\u900f\u660e\u3001\u591a\u76ee\u6807\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\u4e0e\u5168\u7403\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u4fdd\u6301\u4e00\u81f4\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u73af\u5883\u6210\u672c\u6210\u4e3a\u793e\u4f1a\u5173\u6ce8\u7684\u91cd\u70b9\u95ee\u9898\u3002\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u51c6\u786e\u7387\u7b49\u6807\u51c6\u6027\u80fd\u6307\u6807\u4e0a\uff0c\u800c\u5ffd\u89c6\u4e86\u80fd\u91cf\u6d88\u8017\u548c\u78b3\u6392\u653e\u3002\u8fd9\u79cd\u5355\u4e00\u76ee\u6807\u7684\u8bc4\u4f30\u65b9\u5f0f\u8d8a\u6765\u8d8a\u4e0d\u7b26\u5408\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u80fd\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86AI-CARE\u8bc4\u4f30\u5de5\u5177\u4ee5\u62a5\u544aML\u6a21\u578b\u7684\u80fd\u91cf\u6d88\u8017\u53ca\u78b3\u6392\u653e\uff1b\u5f15\u5165\u78b3-\u6027\u80fd\u6298\u8877\u66f2\u7ebf\u4f5c\u4e3a\u53ef\u89c6\u5316\u4e0d\u540c\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u78b3\u6210\u672c\u4e4b\u95f4\u5e73\u8861\u5173\u7cfb\u7684\u53ef\u89e3\u91ca\u5de5\u5177\uff1b\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u5c55\u793a\u4e86\u78b3\u610f\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u5982\u4f55\u6539\u53d8\u6a21\u578b\u95f4\u7684\u76f8\u5bf9\u6392\u540d\u5e76\u9f13\u52b1\u540c\u65f6\u5177\u5907\u9ad8\u7cbe\u5ea6\u548c\u73af\u5883\u8d23\u4efb\u7684\u8bbe\u8ba1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8003\u8651\u78b3\u6392\u653e\u56e0\u7d20\u540e\uff0c\u67d0\u4e9b\u6a21\u578b\u7684\u6392\u540d\u53d1\u751f\u4e86\u53d8\u5316\uff1b\u540c\u65f6\uff0c\u4f7f\u7528AI-CARE\u80fd\u591f\u5e2e\u52a9\u8bc6\u522b\u51fa\u65e2\u51c6\u786e\u53c8\u73af\u4fdd\u7684\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "\u672c\u7814\u7a76\u5021\u5bfc\u91c7\u7528\u66f4\u52a0\u900f\u660e\u4e14\u9762\u5411\u591a\u91cd\u76ee\u6807\u7684\u8bc4\u4ef7\u4f53\u7cfb\uff0c\u4fc3\u4f7f\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u8fdb\u6b65\u66f4\u597d\u5730\u670d\u52a1\u4e8e\u5168\u7403\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u957f\u8fdc\u76ee\u6807\u3002"}}
{"id": "2602.16052", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16052", "abs": "https://arxiv.org/abs/2602.16052", "authors": ["Bradley McDanel", "Steven Li", "Sruthikesh Surineni", "Harshit Khaitan"], "title": "MoE-Spec: Expert Budgeting for Efficient Speculative Decoding", "comment": "12 pages, 10 figures", "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by verifying multiple drafted tokens in parallel. However, for Mixture-of-Experts (MoE) models, this parallelism introduces a severe bottleneck: large draft trees activate many unique experts, significantly increasing memory pressure and diminishing speedups from speculative decoding relative to autoregressive decoding. Prior methods reduce speculation depth when MoE verification becomes expensive. We propose MoE-Spec, a training-free verification-time expert budgeting method that decouples speculation depth from memory cost by enforcing a fixed expert capacity limit at each layer, loading only the experts that contribute most to verification and dropping the long tail of rarely used experts that drive bandwidth overhead. Experiments across multiple model scales and datasets show that this method yields 10--30\\% higher throughput than state-of-the-art speculative decoding baselines (EAGLE-3) at comparable quality, with flexibility to trade accuracy for further latency reductions through tighter budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoE-Spec\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u5c42\u8bbe\u7f6e\u56fa\u5b9a\u7684\u4e13\u5bb6\u5bb9\u91cf\u9650\u5236\u6765\u89e3\u51b3\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u4e2d\u63a8\u6d4b\u89e3\u7801\u5e26\u6765\u7684\u5185\u5b58\u538b\u529b\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u53ef\u4ee5\u63d0\u9ad810-30%\u7684\u541e\u5410\u91cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u9884\u7b97\u8fdb\u4e00\u6b65\u51cf\u5c11\u5ef6\u8fdf\u3002", "motivation": "\u5bf9\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u800c\u8a00\uff0c\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u867d\u7136\u80fd\u591f\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u540c\u65f6\u4e5f\u5f15\u5165\u4e86\u4e25\u91cd\u7684\u74f6\u9888\uff1a\u5927\u91cf\u7684\u8349\u7a3f\u6811\u6fc0\u6d3b\u4e86\u5f88\u591a\u72ec\u7279\u7684\u4e13\u5bb6\uff0c\u663e\u8457\u589e\u52a0\u4e86\u5185\u5b58\u538b\u529b\uff0c\u5e76\u51cf\u5c11\u4e86\u76f8\u5bf9\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u7684\u901f\u5ea6\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86MoE-Spec\u65b9\u6cd5\uff0c\u5728\u9a8c\u8bc1\u65f6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u4e13\u5bb6\u9884\u7b97\u7ba1\u7406\u3002\u6b64\u65b9\u6cd5\u901a\u8fc7\u5728\u6bcf\u4e00\u5c42\u5f3a\u5236\u6267\u884c\u56fa\u5b9a\u7684\u4e13\u5bb6\u5bb9\u91cf\u9650\u5236\uff0c\u4ec5\u52a0\u8f7d\u5bf9\u9a8c\u8bc1\u8d21\u732e\u6700\u5927\u7684\u4e13\u5bb6\uff0c\u5e76\u820d\u5f03\u90a3\u4e9b\u5f88\u5c11\u4f7f\u7528\u4f46\u9a71\u52a8\u5e26\u5bbd\u5f00\u9500\u7684\u957f\u5c3e\u4e13\u5bb6\uff0c\u4ece\u800c\u5c06\u63a8\u6d4b\u6df1\u5ea6\u4e0e\u5185\u5b58\u6210\u672c\u5206\u79bb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8de8\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u96c6\uff0c\u76f8\u6bd4\u4e8e\u6700\u65b0\u7684\u63a8\u6d4b\u89e3\u7801\u57fa\u7ebf\uff08\u5982EAGLE-3\uff09\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u76f8\u4f3c\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b10-30%\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u66f4\u4e25\u683c\u7684\u9884\u7b97\u7075\u6d3b\u5730\u4ee5\u727a\u7272\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\u8fdb\u4e00\u6b65\u964d\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "MoE-Spec\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u4e13\u5bb6\u9884\u7b97\u7ba1\u7406\u65b9\u6848\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86MoE\u6a21\u578b\u4e2d\u63a8\u6d4b\u89e3\u7801\u5bfc\u81f4\u7684\u5185\u5b58\u6d88\u8017\u8fc7\u9ad8\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2602.16053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16053", "abs": "https://arxiv.org/abs/2602.16053", "authors": ["Mehrab Beikzadeh", "Yasaman Asadollah Salmanpour", "Ashima Suvarna", "Sriram Sankararaman", "Matteo Malgaroli", "Majid Sarrafzadeh", "Saadia Gabriel"], "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy", "comment": null, "summary": "Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6846\u67b6\uff08MODPO\uff09\uff0c\u4ee5\u5e73\u8861AI\u6cbb\u7597\u4e2d\u7684\u60a3\u8005\u504f\u597d\u548c\u4e34\u5e8a\u5b89\u5168\u3002\u901a\u8fc7\u8bad\u7ec3\u516d\u4e2a\u6807\u51c6\u7684\u5956\u52b1\u6a21\u578b\uff0cMODPO\u5728\u540c\u7406\u5fc3\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5e76\u4e14\u5728\u76f2\u76ee\u7684\u4e34\u5e8a\u8bc4\u4f30\u4e2d\u5f97\u5230\u4e00\u81f4\u504f\u597d\u3002", "motivation": "\u7531\u4e8e\u7cbe\u795e\u5065\u5eb7\u969c\u788d\u5f71\u54cd\u4e86\u5168\u7403\u8d85\u8fc710\u4ebf\u4eba\uff0c\u4f46\u53d7\u5230\u4e13\u4e1a\u4eba\u5458\u77ed\u7f3a\u548c\u6210\u672c\u9650\u5236\u7684\u5f71\u54cd\uff0c\u4eba\u4eec\u83b7\u53d6\u62a4\u7406\u7684\u673a\u4f1a\u6709\u9650\u3002\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u663e\u793a\u51fa\u6cbb\u7597\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u7684\u65b9\u6cd5\u672a\u80fd\u5f88\u597d\u5730\u5e73\u8861\u60a3\u8005\u7684\u504f\u597d\u4e0e\u4e34\u5e8a\u5b89\u5168\u6027\u3002", "method": "\u7814\u7a76\u56e2\u961f\u9996\u5148\u5bf9335\u540d\u6709\u5fc3\u7406\u5065\u5eb7\u7ecf\u5386\u7684\u4eba\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u6536\u96c6\u4e86\u4ed6\u4eec\u5728\u6cbb\u7597\u7ef4\u5ea6\u4e0a\u7684\u504f\u597d\u6392\u540d\u3002\u63a5\u7740\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u591a\u76ee\u6807\u6821\u51c6\u6846\u67b6\uff0c\u8bad\u7ec3\u4e86\u5305\u62ec\u540c\u7406\u5fc3\u3001\u5b89\u5168\u6027\u7b49\u5728\u5185\u7684\u516d\u4e2a\u6807\u51c6\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5c06\u591a\u76ee\u6807\u65b9\u6cd5\u4e0e\u5355\u76ee\u6807\u4f18\u5316\u3001\u76d1\u7763\u5fae\u8c03\u548c\u53c2\u6570\u5408\u5e76\u7b49\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u591a\u76ee\u6807\u76f4\u63a5\u504f\u597d\u4f18\u5316(MODPO)\u5728\u540c\u7406\u5fc3(77.6%)\u548c\u5b89\u5168\u6027(62.6%)\u65b9\u9762\u76f8\u6bd4\u5355\u4e00\u76ee\u6807\u4f18\u5316\uff0893.6%\u540c\u7406\u5fc3, 47.8%\u5b89\u5168\u6027\uff09\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5e73\u8861\uff0c\u5e76\u4e14\u5728\u6cbb\u7597\u6807\u51c6\u4e0a\u6bd4\u901a\u7528\u6c9f\u901a\u539f\u5219\u9ad8\u51fa17.2%\u3002\u6b64\u5916\uff0c\u76f2\u76ee\u7684\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u786e\u8ba4\u4e86MODPO\u88ab\u4e00\u81f4\u504f\u597d\uff0cLLM-\u8bc4\u4f30\u8005\u4e00\u81f4\u6027\u4e0e\u4e34\u5e8a\u533b\u751f\u95f4\u53ef\u9760\u6027\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u91c7\u7528\u591a\u76ee\u6807\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u6765\u8c03\u6574AI\u6cbb\u7597\u52a9\u624b\uff0c\u53ef\u4ee5\u5728\u63d0\u9ad8\u6cbb\u7597\u6548\u679c\u7684\u540c\u65f6\u786e\u4fdd\u60a3\u8005\u7684\u5b89\u5168\u6027\uff0c\u4ece\u800c\u4e3a\u672a\u6765AI\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.16057", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.16057", "abs": "https://arxiv.org/abs/2602.16057", "authors": ["Dawon Ahn", "Het Patel", "Aemal Khattak", "Jia Chen", "Evangelos E. Papalexakis"], "title": "Extracting and Analyzing Rail Crossing Behavior Signatures from Videos using Tensor Methods", "comment": "6 pages, 10 figures. Accepted at InnovaRail 2026", "summary": "Railway crossings present complex safety challenges where driver behavior varies by location, time, and conditions. Traditional approaches analyze crossings individually, limiting the ability to identify shared behavioral patterns across locations. We propose a multi-view tensor decomposition framework that captures behavioral similarities across three temporal phases: Approach (warning activation to gate lowering), Waiting (gates down to train passage), and Clearance (train passage to gate raising). We analyze railway crossing videos from multiple locations using TimeSformer embeddings to represent each phase. By constructing phase-specific similarity matrices and applying non-negative symmetric CP decomposition, we discover latent behavioral components with distinct temporal signatures. Our tensor analysis reveals that crossing location appears to be a stronger determinant of behavior patterns than time of day, and that approach-phase behavior provides particularly discriminative signatures. Visualization of the learned component space confirms location-based clustering, with certain crossings forming distinct behavioral clusters. This automated framework enables scalable pattern discovery across multiple crossings, providing a foundation for grouping locations by behavioral similarity to inform targeted safety interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u5f20\u91cf\u5206\u89e3\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u94c1\u8def\u4ea4\u53c9\u53e3\u89c6\u9891\u4e2d\u9a7e\u9a76\u5458\u7684\u884c\u4e3a\u6a21\u5f0f\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f4d\u7f6e\u6bd4\u4e00\u5929\u4e2d\u7684\u65f6\u95f4\u66f4\u80fd\u51b3\u5b9a\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u4e14\u63a5\u8fd1\u9636\u6bb5\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u7279\u522b\u5177\u6709\u8fa8\u8bc6\u5ea6\u7684\u7279\u5f81\u3002\u8be5\u6846\u67b6\u4e3a\u6309\u884c\u4e3a\u76f8\u4f3c\u6027\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u5206\u7ec4\u4ee5\u6307\u5bfc\u6709\u9488\u5bf9\u6027\u7684\u5b89\u5168\u5e72\u9884\u63aa\u65bd\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u7684\u94c1\u8def\u4ea4\u53c9\u53e3\u5b89\u5168\u6311\u6218\u5206\u6790\u65b9\u6cd5\u4ec5\u5355\u72ec\u8003\u8651\u6bcf\u4e2a\u4ea4\u53c9\u53e3\u7684\u60c5\u51b5\uff0c\u9650\u5236\u4e86\u8bc6\u522b\u4e0d\u540c\u5730\u70b9\u95f4\u5171\u4eab\u884c\u4e3a\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6355\u6349\u591a\u4e2a\u94c1\u8def\u4ea4\u53c9\u53e3\u4e4b\u95f4\u5728\u4e09\u4e2a\u4e0d\u540c\u65f6\u95f4\u6bb5\uff08\u63a5\u8fd1\u3001\u7b49\u5f85\u548c\u6e05\u9664\uff09\u5185\u7684\u884c\u4e3a\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528TimeSformer\u5d4c\u5165\u8868\u793a\u6bcf\u4e2a\u9636\u6bb5\uff0c\u5e76\u6784\u5efa\u4e86\u7279\u5b9a\u4e8e\u9636\u6bb5\u7684\u76f8\u4f3c\u77e9\u9635\u3002\u968f\u540e\u5e94\u7528\u975e\u8d1f\u5bf9\u79f0CP\u5206\u89e3\u6280\u672f\u53d1\u73b0\u4e86\u5177\u6709\u72ec\u7279\u65f6\u95f4\u7279\u5f81\u7684\u6f5c\u5728\u884c\u4e3a\u6210\u5206\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4e00\u5929\u4e2d\u7684\u65f6\u95f4\u76f8\u6bd4\uff0c\u4ea4\u53c9\u53e3\u7684\u4f4d\u7f6e\u4f3c\u4e4e\u662f\u884c\u4e3a\u6a21\u5f0f\u66f4\u5f3a\u7684\u51b3\u5b9a\u56e0\u7d20\uff1b\u5e76\u4e14\u63a5\u8fd1\u9636\u6bb5\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u7279\u522b\u5177\u6709\u533a\u5206\u6027\u7684\u6807\u5fd7\u3002\u53ef\u89c6\u5316\u5b66\u4e60\u5230\u7684\u7ec4\u4ef6\u7a7a\u95f4\u8bc1\u5b9e\u4e86\u57fa\u4e8e\u4f4d\u7f6e\u7684\u805a\u7c7b\u73b0\u8c61\uff0c\u67d0\u4e9b\u4ea4\u53c9\u53e3\u5f62\u6210\u4e86\u660e\u663e\u7684\u884c\u4e3a\u96c6\u7fa4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u6846\u67b6\u80fd\u591f\u8de8\u591a\u4e2a\u4ea4\u53c9\u53e3\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6a21\u5f0f\u53d1\u73b0\uff0c\u4e3a\u6839\u636e\u884c\u4e3a\u76f8\u4f3c\u6027\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u5206\u7ec4\u4ee5\u63d0\u4f9b\u9488\u5bf9\u6027\u7684\u5b89\u5168\u5e72\u9884\u63aa\u65bd\u6253\u4e0b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16065", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16065", "abs": "https://arxiv.org/abs/2602.16065", "authors": ["Kevin Wang", "Hongqian Niu", "Didong Li"], "title": "Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training", "comment": null, "summary": "Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.", "AI": {"tldr": "This paper explores the recursive training of generative AI models, particularly in scenarios where data becomes a mixture of human and AI-generated content. It presents a theoretical framework showing that even with data contamination, recursive training can still converge, marking the first positive result in this area without making specific assumptions about the data distribution. The study also considers the impact of sampling bias on the training process.", "motivation": "The motivation behind this research is to address the challenges arising from the increasing interweaving of human and AI-generated content in web data, which complicates the separation of these two types. As generative AI models are trained on this mixed data, there is a concern over the potential for model collapse due to recursive training. This paper aims to fill the gap in understanding by providing a more general theoretical analysis that does not rely on overly simplified settings or assumptions about the data distribution.", "method": "The authors adopt a general framework for their analysis, minimizing assumptions about the real data distribution and allowing the generative model to be a universal approximator. They theoretically examine the convergence of contaminated recursive training processes. Furthermore, they extend their analysis to account for sampling biases in data collection, supporting their findings through empirical studies.", "result": "The key finding is that, under the proposed general framework, recursive training with data contamination still converges, with its rate being determined by the minimum between the baseline model's convergence rate and the proportion of real data utilized in each iteration. This represents a pioneering (positive) theoretical insight into recursive training without assuming specific data distributions. Empirical studies further validate these theoretical results.", "conclusion": "The conclusion drawn from this work highlights the robustness of recursive training in the presence of data contamination, indicating that it can still achieve convergence under broad conditions. This finding is significant as it provides a foundational understanding for future developments in generative AI, especially concerning the integration of human and machine-generated data, and sets a new direction for exploring the implications of recursive training in more complex, real-world scenarios."}}
{"id": "2602.16072", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.16072", "abs": "https://arxiv.org/abs/2602.16072", "authors": ["Chenda Duan", "Yipeng Zhang", "Sotaro Kanai", "Yuanyi Ding", "Atsuro Daida", "Pengyue Yu", "Tiancheng Zheng", "Naoto Kuroda", "Shaun A. Hussain", "Eishi Asano", "Hiroki Nariai", "Vwani Roychowdhury"], "title": "Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research", "comment": "Published as a conference paper at ICLR 2026", "summary": "Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained by labor-intensive manual review. At the same time, existing data-driven approaches are typically developed on single-center datasets that are inconsistent in format and metadata, lack standardized benchmarks, and rarely release pathological event annotations, creating barriers to reproducibility, cross-center validation, and clinical relevance. With extensive efforts to reconcile heterogeneous iEEG formats, metadata, and recordings across publicly available sources, we present $\\textbf{Omni-iEEG}$, a large-scale, pre-surgical iEEG resource comprising $\\textbf{302 patients}$ and $\\textbf{178 hours}$ of high-resolution recordings. The dataset includes harmonized clinical metadata such as seizure onset zones, resections, and surgical outcomes, all validated by board-certified epileptologists. In addition, Omni-iEEG provides over 36K expert-validated annotations of pathological events, enabling robust biomarker studies. Omni-iEEG serves as a bridge between machine learning and epilepsy research. It defines clinically meaningful tasks with unified evaluation metrics grounded in clinical priors, enabling systematic evaluation of models in clinically relevant settings. Beyond benchmarking, we demonstrate the potential of end-to-end modeling on long iEEG segments and highlight the transferability of representations pretrained on non-neurophysiological domains. Together, these contributions establish Omni-iEEG as a foundation for reproducible, generalizable, and clinically translatable epilepsy research. The project page with dataset and code links is available at omni-ieeg.github.io/omni-ieeg.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Omni-iEEG\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u672f\u524diEEG\u8d44\u6e90\u5e93\uff0c\u5305\u542b302\u540d\u60a3\u8005\u548c178\u5c0f\u65f6\u9ad8\u5206\u8fa8\u7387\u8bb0\u5f55\u3002\u8be5\u6570\u636e\u96c6\u4e0d\u4ec5\u7edf\u4e00\u4e86\u4e34\u5e8a\u5143\u6570\u636e\u683c\u5f0f\uff0c\u8fd8\u63d0\u4f9b\u4e86\u8d85\u8fc736,000\u4e2a\u75c5\u7406\u4e8b\u4ef6\u7684\u4e13\u4e1a\u9a8c\u8bc1\u6ce8\u91ca\uff0c\u65e8\u5728\u4fc3\u8fdb\u766b\u75eb\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u3001\u901a\u7528\u6027\u548c\u4e34\u5e8a\u8f6c\u5316\u3002", "motivation": "\u766b\u75eb\u5f71\u54cd\u5168\u7403\u8d85\u8fc75000\u4e07\u4eba\uff0c\u5176\u4e2d\u4e09\u5206\u4e4b\u4e00\u7684\u60a3\u8005\u5bf9\u836f\u7269\u6cbb\u7597\u65e0\u6548\uff0c\u624b\u672f\u6210\u4e3a\u63a7\u5236\u766b\u75eb\u53d1\u4f5c\u7684\u6700\u4f73\u9009\u62e9\u3002\u7136\u800c\uff0c\u7cbe\u786e\u5b9a\u4f4d\u81f4\u75eb\u533a\u4f9d\u8d56\u4e8e\u9885\u5185\u8111\u7535\u56fe(iEEG)\uff0c\u800c\u5f53\u524d\u4e34\u5e8a\u6d41\u7a0b\u53d7\u9650\u4e8e\u8017\u65f6\u7684\u624b\u52a8\u5ba1\u67e5\u8fc7\u7a0b\u3002\u540c\u65f6\uff0c\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5355\u4e2d\u5fc3\u6570\u636e\u96c6\u5f00\u53d1\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5728\u683c\u5f0f\u548c\u5143\u6570\u636e\u4e0a\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5e76\u4e14\u5f88\u5c11\u516c\u5f00\u75c5\u7406\u4e8b\u4ef6\u6807\u6ce8\uff0c\u8fd9\u7ed9\u518d\u73b0\u6027\u3001\u8de8\u4e2d\u5fc3\u9a8c\u8bc1\u4ee5\u53ca\u4e34\u5e8a\u76f8\u5173\u6027\u5e26\u6765\u4e86\u969c\u788d\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u52aa\u529b\u6765\u534f\u8c03\u6765\u81ea\u516c\u5f00\u6765\u6e90\u7684\u4e0d\u540ciEEG\u683c\u5f0f\u3001\u5143\u6570\u636e\u53ca\u8bb0\u5f55\uff0c\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86Omni-iEEG\u2014\u2014\u4e00\u4e2a\u5927\u89c4\u6a21\u672f\u524diEEG\u8d44\u6e90\u5e93\uff0c\u6db5\u76d6302\u540d\u60a3\u8005\u548c178\u5c0f\u65f6\u9ad8\u5206\u8fa8\u7387\u8bb0\u5f55\u3002\u6b64\u6570\u636e\u96c6\u5305\u62ec\u7ecf\u8fc7\u8ba4\u8bc1\u7684\u4e34\u5e8a\u5143\u6570\u636e\u5982\u53d1\u4f5c\u8d77\u59cb\u533a\u3001\u5207\u9664\u8303\u56f4\u4e0e\u624b\u672f\u7ed3\u679c\u7b49\u4fe1\u606f\uff1b\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u4f9b\u4e86\u8d85\u8fc736,000\u4e2a\u7531\u4e13\u5bb6\u9a8c\u8bc1\u8fc7\u7684\u75c5\u7406\u4e8b\u4ef6\u6ce8\u91ca\u3002", "result": "Omni-iEEG\u4f5c\u4e3a\u4e00\u4e2a\u8fde\u63a5\u673a\u5668\u5b66\u4e60\u4e0e\u766b\u75eb\u7814\u7a76\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u5b9a\u4e49\u4e86\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u4efb\u52a1\u53ca\u57fa\u4e8e\u4e34\u5e8a\u5148\u9a8c\u77e5\u8bc6\u7684\u4e00\u81f4\u8bc4\u4f30\u6307\u6807\uff0c\u4ece\u800c\u80fd\u591f\u5728\u4e34\u5e8a\u76f8\u5173\u7684\u8bbe\u7f6e\u4e2d\u7cfb\u7edf\u5730\u8bc4\u4f30\u6a21\u578b\u3002\u6b64\u5916\uff0c\u672c\u6587\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u5efa\u6a21\u5728\u957fiEEG\u7247\u6bb5\u4e0a\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u4ece\u975e\u795e\u7ecf\u751f\u7406\u5b66\u9886\u57df\u9884\u8bad\u7ec3\u8868\u793a\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u7efc\u4e0a\u6240\u8ff0\uff0cOmni-iEEG\u4e3a\u53ef\u91cd\u590d\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u8f6c\u5316\u7684\u766b\u75eb\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16092", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16092", "abs": "https://arxiv.org/abs/2602.16092", "authors": ["Patrick Pynadath", "Ruqi Zhang"], "title": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff", "comment": null, "summary": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4efb\u610f\u987a\u5e8f\u81ea\u56de\u5f52\u6a21\u578b\uff08AO-ARMs\uff09\u4e2d\u4e24\u6d41\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u89e3\u8026RoPE\u7684\u65b9\u6cd5\u6765\u5206\u79bb\u76ee\u6807\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u5185\u5bb9\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e24\u6d41\u6ce8\u610f\u529b\u7684\u6210\u529f\u4e0d\u4ec5\u4ec5\u5728\u4e8e\u5c06\u4f4d\u7f6e\u4e0e\u5185\u5bb9\u5206\u5f00\uff0c\u8fd8\u5728\u4e8e\u7ed5\u8fc7\u4e86\u4efb\u610f\u987a\u5e8f\u751f\u6210\u56fa\u6709\u7684\u7ed3\u6784-\u8bed\u4e49\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u4f5c\u8005\u5e0c\u671b\u6df1\u5165\u7406\u89e3\u4e3a\u4ec0\u4e48\u5728\u4efb\u610f\u987a\u5e8f\u81ea\u56de\u5f52\u6a21\u578b\uff08AO-ARMs\uff09\u4e2d\u4f7f\u7528\u4e24\u6d41\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5176\u4ed6\u65b9\u5f0f\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u800c\u4e0d\u5fc5\u5b8c\u5168\u4f9d\u8d56\u4e8e\u8fd9\u79cd\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u4fee\u6539\u7248\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u2014\u2014\u89e3\u8026RoPE\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5411\u6a21\u578b\u63d0\u4f9b\u76ee\u6807\u4f4d\u7f6e\u4fe1\u606f\u800c\u4e0d\u4f1a\u6cc4\u9732\u76ee\u6807\u7684\u5185\u5bb9\u7ec6\u8282\u3002\u57fa\u4e8e\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u5206\u6790\u4e86\u89e3\u8026RoPE\u5728\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u4e0b\u7684\u8868\u73b0\u4ee5\u53ca\u5b83\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5904\u7406\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8f83\u77ed\u7684\u5e8f\u5217\u4e0a\uff0c\u89e3\u8026RoPE\u80fd\u591f\u5f88\u597d\u5730\u5de5\u4f5c\uff0c\u56e0\u4e3a\u6b64\u65f6\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7ed3\u6784\u6027\u63a5\u8fd1\u6027\u662f\u4e00\u81f4\u7684\uff1b\u7136\u800c\uff0c\u968f\u7740\u5e8f\u5217\u53d8\u957f\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u589e\u52a0\u65f6\uff0c\u5176\u6027\u80fd\u5f00\u59cb\u4e0b\u964d\u3002\u8fd9\u8bc1\u660e\u4e86\u4e24\u6d41\u6ce8\u610f\u529b\u673a\u5236\u6210\u529f\u7684\u5173\u952e\u5728\u4e8e\u5b83\u80fd\u6709\u6548\u89e3\u51b3AO-ARMs\u5185\u5728\u7684\u7ed3\u6784-\u8bed\u4e49\u6743\u8861\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e24\u6d41\u6ce8\u610f\u529b\u673a\u5236\u5728\u4efb\u610f\u987a\u5e8f\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u8868\u73b0\uff0c\u90e8\u5206\u539f\u56e0\u5728\u4e8e\u5b83\u80fd\u591f\u540c\u65f6\u4f18\u5316\u5bf9\u7ed3\u6784\u6700\u8fd1\u9879\u7684\u5173\u6ce8\u548c\u5bf9\u8bed\u4e49\u4fe1\u606f\u7684\u6355\u6349\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7b80\u5355\u5730\u5c06\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u5185\u5bb9\u5206\u79bb\u5f00\u6765\u3002"}}
{"id": "2602.16101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16101", "abs": "https://arxiv.org/abs/2602.16101", "authors": ["Afonso Louren\u00e7o", "Francisca Os\u00f3rio", "Diogo Risca", "Goreti Marreiros"], "title": "Axle Sensor Fusion for Online Continual Wheel Fault Detection in Wayside Railway Monitoring", "comment": null, "summary": "Reliable and cost-effective maintenance is essential for railway safety, particularly at the wheel-rail interface, which is prone to wear and failure. Predictive maintenance frameworks increasingly leverage sensor-generated time-series data, yet traditional methods require manual feature engineering, and deep learning models often degrade in online settings with evolving operational patterns. This work presents a semantic-aware, label-efficient continual learning framework for railway fault diagnostics. Accelerometer signals are encoded via a Variational AutoEncoder into latent representations capturing the normal operational structure in a fully unsupervised manner. Importantly, semantic metadata, including axle counts, wheel indexes, and strain-based deformations, is extracted via AI-driven peak detection on fiber Bragg grating sensors (resistant to electromagnetic interference) and fused with the VAE embeddings, enhancing anomaly detection under unknown operational conditions. A lightweight gradient boosting supervised classifier stabilizes anomaly scoring with minimal labels, while a replay-based continual learning strategy enables adaptation to evolving domains without catastrophic forgetting. Experiments show the model detects minor imperfections due to flats and polygonization, while adapting to evolving operational conditions, such as changes in train type, speed, load, and track profiles, captured using a single accelerometer and strain gauge in wayside monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u3001\u6807\u7b7e\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u94c1\u8def\u6545\u969c\u8bca\u65ad\u3002\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5bf9\u52a0\u901f\u5ea6\u8ba1\u4fe1\u53f7\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u7ed3\u5408AI\u9a71\u52a8\u7684\u5149\u7ea4\u5e03\u62c9\u683c\u5149\u6805\u4f20\u611f\u5668\u5cf0\u503c\u68c0\u6d4b\u63d0\u53d6\u7684\u8bed\u4e49\u5143\u6570\u636e\uff0c\u589e\u5f3a\u4e86\u5728\u672a\u77e5\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002\u8f7b\u91cf\u7ea7\u68af\u5ea6\u63d0\u5347\u76d1\u7763\u5206\u7c7b\u5668\u4e0e\u57fa\u4e8e\u56de\u653e\u7684\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u64cd\u4f5c\u73af\u5883\u800c\u4e0d\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u80fd\u6709\u6548\u68c0\u6d4b\u5230\u7531\u4e8e\u6241\u5e73\u5316\u548c\u591a\u8fb9\u5f62\u5316\u5bfc\u81f4\u7684\u5c0f\u7f3a\u9677\uff0c\u5e76\u9002\u5e94\u5217\u8f66\u7c7b\u578b\u3001\u901f\u5ea6\u3001\u8d1f\u8f7d\u53ca\u8f68\u9053\u8f6e\u5ed3\u7684\u53d8\u5316\u3002", "motivation": "\u94c1\u8def\u5b89\u5168\u7ef4\u62a4\uff0c\u5c24\u5176\u662f\u5728\u8f6e\u8f68\u63a5\u89e6\u9762\uff0c\u9762\u4e34\u7740\u78e8\u635f\u548c\u6545\u969c\u7684\u98ce\u9669\u3002\u867d\u7136\u9884\u6d4b\u6027\u7ef4\u62a4\u6846\u67b6\u8d8a\u6765\u8d8a\u591a\u5730\u5229\u7528\u4f20\u611f\u5668\u751f\u6210\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7ebf\u4e0a\u73af\u5883\u4e2d\u9762\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u64cd\u4f5c\u6a21\u5f0f\u65f6\u6027\u80fd\u5f80\u5f80\u4f1a\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u53c8\u80fd\u9002\u5e94\u73af\u5883\u53d8\u5316\u7684\u94c1\u8def\u6545\u969c\u8bca\u65ad\u7cfb\u7edf\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4e0e\u8bed\u4e49\u5143\u6570\u636e\u878d\u5408\u7684\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u9996\u5148\u4f7f\u7528VAE\u5bf9\u52a0\u901f\u5ea6\u8ba1\u4fe1\u53f7\u8fdb\u884c\u65e0\u76d1\u7763\u7f16\u7801\uff0c\u6355\u6349\u6b63\u5e38\u8fd0\u884c\u7ed3\u6784\uff1b\u63a5\u7740\uff0c\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u65b9\u6cd5\u4ece\u6297\u7535\u78c1\u5e72\u6270\u7684\u5149\u7ea4\u5e03\u62c9\u683c\u5149\u6805\u4f20\u611f\u5668\u4e2d\u63d0\u53d6\u5305\u62ec\u8f74\u6570\u3001\u8f66\u8f6e\u7d22\u5f15\u4ee5\u53ca\u57fa\u4e8e\u5e94\u53d8\u7684\u53d8\u5f62\u5728\u5185\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u4e0eVAE\u4ea7\u751f\u7684\u5d4c\u5165\u76f8\u878d\u5408\uff1b\u6700\u540e\u91c7\u7528\u8f7b\u91cf\u7ea7\u68af\u5ea6\u589e\u5f3a\u76d1\u7763\u5206\u7c7b\u5668\u7a33\u5b9a\u5f02\u5e38\u8bc4\u5206\uff0c\u5e76\u501f\u52a9\u57fa\u4e8e\u91cd\u64ad\u7684\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u9886\u57df\u53d8\u5316\u800c\u4e0d\u4e27\u5931\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u8bc6\u522b\u51fa\u7531\u6241\u5e73\u5316\u548c\u591a\u8fb9\u5f62\u5316\u5f15\u8d77\u7684\u5c0f\u578b\u7f3a\u9677\uff0c\u8fd8\u80fd\u591f\u5728\u8bf8\u5982\u5217\u8f66\u79cd\u7c7b\u3001\u884c\u9a76\u901f\u5ea6\u3001\u8f7d\u91cd\u91cf\u4ee5\u53ca\u8f68\u9053\u5f62\u6001\u7b49\u64cd\u4f5c\u6761\u4ef6\u53d1\u751f\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u4e2a\u65b0\u9896\u4e14\u6709\u6548\u7684\u94c1\u8def\u6545\u969c\u8bca\u65ad\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u4e0e\u7269\u7406\u4f20\u611f\u6570\u636e\u5904\u7406\u65b9\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\u94c1\u8def\u7ec4\u4ef6\u5065\u5eb7\u72b6\u6001\u7684\u6709\u6548\u76d1\u6d4b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6848\u8fd8\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u65b0\u60c5\u51b5\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2602.16125", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16125", "abs": "https://arxiv.org/abs/2602.16125", "authors": ["Leo", "Wang", "Connor Mclaughlin", "Lili Su"], "title": "On the Power of Source Screening for Learning Shared Feature Extractors", "comment": null, "summary": "Learning with shared representation is widely recognized as an effective way to separate commonalities from heterogeneity across various heterogeneous sources. Most existing work includes all related data sources via simultaneously training a common feature extractor and source-specific heads. It is well understood that data sources with low relevance or poor quality may hinder representation learning. In this paper, we further dive into the question of which data sources should be learned jointly by focusing on the traditionally deemed ``good'' collection of sources, in which individual sources have similar relevance and qualities with respect to the true underlying common structure. Towards tractability, we focus on the linear setting where sources share a low-dimensional subspace. We find that source screening can play a central role in statistically optimal subspace estimation. We show that, for a broad class of problem instances, training on a carefully selected subset of sources suffices to achieve minimax optimality, even when a substantial portion of data is discarded. We formalize the notion of an informative subpopulation, develop algorithms and practical heuristics for identifying such subsets, and validate their effectiveness through both theoretical analysis and empirical evaluations on synthetic and real-world datasets.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5171\u4eab\u8868\u793a\u5b66\u4e60\u4e2d\u5982\u4f55\u9009\u62e9\u6027\u5730\u5229\u7528\u6570\u636e\u6e90\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7b5b\u9009\u51fa\u4fe1\u606f\u91cf\u5927\u7684\u5b50\u7fa4\u4f53\u6765\u8fbe\u5230\u7edf\u8ba1\u6700\u4f18\u5b50\u7a7a\u95f4\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u77e5\u4f4e\u76f8\u5173\u6027\u6216\u8d28\u91cf\u5dee\u7684\u6570\u636e\u6e90\u53ef\u80fd\u4f1a\u963b\u788d\u8868\u793a\u5b66\u4e60\uff0c\u4f46\u672c\u7814\u7a76\u8fdb\u4e00\u6b65\u6df1\u5165\u63a2\u8ba8\u4e86\u54ea\u4e9b\u6570\u636e\u6e90\u5e94\u8be5\u8054\u5408\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u90a3\u4e9b\u901a\u5e38\u88ab\u8ba4\u4e3a\u5177\u6709\u76f8\u4f3c\u76f8\u5173\u6027\u548c\u8d28\u91cf\u7684\u2018\u597d\u2019\u6570\u636e\u6e90\u96c6\u5408\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u8bbe\u7f6e\u4e0b\u5171\u4eab\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u4fe1\u606f\u5b50\u7fa4\u4f53\u7684\u6982\u5ff5\u3001\u5f00\u53d1\u7b97\u6cd5\u53ca\u5b9e\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u8bc6\u522b\u8fd9\u6837\u7684\u5b50\u96c6\uff0c\u4ee5\u5b9e\u73b0\u5373\u4f7f\u4e22\u5f03\u5927\u91cf\u6570\u636e\u4e5f\u80fd\u8fbe\u6210\u6700\u5c0f\u6700\u5927\u4f18\u5316\u7684\u76ee\u6807\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u5e7f\u6cdb\u7684\u95ee\u9898\u5b9e\u4f8b\uff0c\u4ec5\u901a\u8fc7\u5bf9\u7cbe\u5fc3\u6311\u9009\u7684\u90e8\u5206\u6570\u636e\u6e90\u8fdb\u884c\u8bad\u7ec3\u5c31\u8db3\u4ee5\u5b9e\u73b0\u6700\u5c0f\u6700\u5927\u4f18\u5316\u3002\u6b64\u53d1\u73b0\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u6e90\u7b5b\u9009\u5728\u7edf\u8ba1\u6700\u4f18\u5b50\u7a7a\u95f4\u4f30\u8ba1\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u800c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5229\u7528\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u5b50\u96c6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u5e76\u8fbe\u5230\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.16147", "categories": ["cs.LG", "cs.AI", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16147", "abs": "https://arxiv.org/abs/2602.16147", "authors": ["Megan Lee", "Seung Ha Hwang", "Inhyeok Choi", "Shreyas Darade", "Mengchun Zhang", "Kateryna Shapovalenko"], "title": "ASPEN: Spectral-Temporal Fusion for Cross-Subject Brain Decoding", "comment": null, "summary": "Cross-subject generalization in EEG-based brain-computer interfaces (BCIs) remains challenging due to individual variability in neural signals. We investigate whether spectral representations offer more stable features for cross-subject transfer than temporal waveforms. Through correlation analyses across three EEG paradigms (SSVEP, P300, and Motor Imagery), we find that spectral features exhibit consistently higher cross-subject similarity than temporal signals. Motivated by this observation, we introduce ASPEN, a hybrid architecture that combines spectral and temporal feature streams via multiplicative fusion, requiring cross-modal agreement for features to propagate. Experiments across six benchmark datasets reveal that ASPEN is able to dynamically achieve the optimal spectral-temporal balance depending on the paradigm. ASPEN achieves the best unseen-subject accuracy on three of six datasets and competitive performance on others, demonstrating that multiplicative multimodal fusion enables effective cross-subject generalization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u9891\u8c31\u7684\u7279\u5f81\u5728\u8de8\u88ab\u8bd5\u8111\u673a\u63a5\u53e3\u4e2d\u6bd4\u65f6\u95f4\u4fe1\u53f7\u66f4\u7a33\u5b9a\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u63d0\u51fa\u4e86ASPEN\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u4e58\u6cd5\u878d\u5408\u7ed3\u5408\u4e86\u9891\u8c31\u548c\u65f6\u95f4\u7279\u5f81\u6d41\u3002\u5b9e\u9a8c\u8868\u660e\uff0cASPEN\u80fd\u591f\u6839\u636e\u4e0d\u540c\u8303\u5f0f\u52a8\u6001\u8fbe\u5230\u6700\u4f73\u7684\u9891\u8c31-\u65f6\u95f4\u5e73\u8861\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u88ab\u8bd5\u7684\u6700\u4f73\u51c6\u786e\u7387\u6216\u7ade\u4e89\u6027\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u4e2a\u4f53\u95f4\u795e\u7ecf\u4fe1\u53f7\u7684\u5dee\u5f02\u6027\uff0c\u57fa\u4e8eEEG\u7684\u8111\u673a\u63a5\u53e3\u5728\u8de8\u88ab\u8bd5\u6cdb\u5316\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u9891\u8c31\u8868\u793a\u662f\u5426\u6bd4\u65f6\u95f4\u6ce2\u5f62\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u8de8\u88ab\u8bd5\u8f6c\u79fb\u7279\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e09\u79cdEEG\u8303\u5f0f\uff08SSVEP\u3001P300\u548c\u8fd0\u52a8\u60f3\u8c61\uff09\u4e2d\u7684\u76f8\u5173\u6027\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aASPEN\u7684\u6df7\u5408\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u4e58\u6cd5\u878d\u5408\u65b9\u6cd5\u6574\u5408\u9891\u8c31\u4e0e\u65f6\u95f4\u7279\u5f81\u6d41\uff0c\u8981\u6c42\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u4ee5\u4fbf\u4e8e\u7279\u5f81\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8de8\u8d8a\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u663e\u793a\uff0cASPEN\u80fd\u591f\u6839\u636e\u4e0d\u540c\u7684\u8303\u5f0f\u52a8\u6001\u5730\u5b9e\u73b0\u6700\u4f18\u7684\u9891\u8c31-\u65f6\u95f4\u5e73\u8861\u3002\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u4e09\u4e2a\u4e0a\uff0cASPEN\u8fbe\u5230\u4e86\u5bf9\u4e8e\u672a\u89c1\u8fc7\u7684\u88ab\u8bd5\u6700\u9ad8\u7684\u51c6\u786e\u5ea6\uff0c\u5e76\u5728\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\uff0c\u901a\u8fc7\u4e58\u6cd5\u591a\u6a21\u6001\u878d\u5408\u53ef\u4ee5\u6709\u6548\u5730\u4fc3\u8fdb\u8de8\u88ab\u8bd5\u6cdb\u5316\uff0c\u4f7f\u5f97ASPEN\u5728\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684EEG\u4efb\u52a1\u65f6\u5c55\u73b0\u51fa\u4f18\u8d8a\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002"}}
{"id": "2602.16155", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16155", "abs": "https://arxiv.org/abs/2602.16155", "authors": ["Difei Xu", "Meng Ding", "Zebin Ma", "Huanyi Xie", "Youming Tao", "Aicha Slaitane", "Di Wang"], "title": "Differentially Private Non-convex Distributionally Robust Optimization", "comment": null, "summary": "Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.\n  Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.\n  Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.\n  In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.\n  To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $\u03c8$-divergence and non-convex loss.\n  First, we study DRO with general $\u03c8$-divergence by reformulating it as a minimization problem, and develop a novel $(\\varepsilon, \u03b4)$-DP optimization method, called DP Double-Spider, tailored to this structure.\n  Under mild assumptions, we show that it achieves a utility bound of $\\mathcal{O}(\\frac{1}{\\sqrt{n}}+ (\\frac{\\sqrt{d \\log (1/\u03b4)}}{n \\varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.\n  We further improve the utility rate for specific divergences.\n  In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\\mathcal{O}((\\frac{\\sqrt{d \\log(1/\u03b4)}}{n\\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.\n  Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5dee\u5206\u9690\u79c1(DP)\u4fdd\u62a4\u4e0b\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316(DRO)\uff0c\u9488\u5bf9\u6709\u9650\u548cDRO\u4e0e\u03c8-\u6563\u5ea6\u53ca\u975e\u51f8\u635f\u5931\u51fd\u6570\u7684\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684DP\u4f18\u5316\u65b9\u6cd5DP Double-Spider\uff0c\u5e76\u5bf9KL\u6563\u5ea6\u7279\u522b\u6539\u8fdb\u4e3aDP Recursive-Spider\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684DP\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u7ecf\u5e38\u9047\u5230\u5206\u5e03\u504f\u79fb\u3001\u7fa4\u4f53\u4e0d\u5e73\u8861\u4ee5\u53ca\u5bf9\u6297\u6027\u6270\u52a8\u7b49\u95ee\u9898\uff0c\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0b\u4f20\u7edf\u7684\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316(ERM)\u6846\u67b6\u8868\u73b0\u4e0d\u4f73\u3002\u5206\u5e03\u9c81\u68d2\u4f18\u5316(DRO)\u901a\u8fc7\u5728\u4e00\u4e2a\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u4e0a\u4f18\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9884\u671f\u635f\u5931\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u540c\u65f6\uff0c\u7531\u4e8eDRO\u4e2d\u7684\u8bad\u7ec3\u6570\u636e\u6d89\u53ca\u654f\u611f\u4fe1\u606f\uff0c\u56e0\u6b64\u5728\u5dee\u5206\u9690\u79c1(DP)\u4e0b\u9632\u6b62\u8fd9\u4e9b\u4fe1\u606f\u6cc4\u9732\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002\u76f8\u8f83\u4e8e\u5df2\u7ecf\u5e7f\u6cdb\u7814\u7a76\u7684DP-ERM\uff0c\u5177\u6709\u4e0d\u786e\u5b9a\u7ea6\u675f\u7684\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u7ed3\u6784\u7684DP-DRO\u53d7\u5230\u4e86\u8f83\u5c11\u7684\u5173\u6ce8\u3002", "method": "\u9996\u5148\uff0c\u5bf9\u4e8e\u5177\u6709\u4e00\u822c\u03c8-\u6563\u5ea6\u7684DRO\u95ee\u9898\uff0c\u4f5c\u8005\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u4e2a\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aDP Double-Spider\u7684\u65b0\u9896(\u03b5, \u03b4)-DP\u4f18\u5316\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u91c7\u7528KL\u6563\u5ea6\u7684DP-DRO\uff0c\u4f5c\u8005\u5c06\u95ee\u9898\u8f6c\u6362\u6210\u5408\u6210\u6709\u9650\u548c\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e3a\u6b64\u5f00\u53d1\u4e86DP Recursive-Spider\u65b9\u6cd5\u3002", "result": "\u5728\u6e29\u548c\u5047\u8bbe\u6761\u4ef6\u4e0b\uff0cDP Double-Spider\u65b9\u6cd5\u8fbe\u5230\u4e86\u5173\u4e8e\u68af\u5ea6\u8303\u6570\u7684\u6548\u7528\u754c$\\mathcal{O}(\\frac{1}{\\sqrt{n}}+ (\\frac{\\sqrt{d \\log (1/\u03b4)}}{n \\varepsilon})^{2/3})$\uff1b\u800c\u5bf9\u4e8e\u7279\u5b9a\u4e8eKL\u6563\u5ea6\u7684DP-DRO\uff0cDP Recursive-Spider\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6539\u5584\u4e86\u8fd9\u4e00\u6548\u7528\u7387\u81f3$\\mathcal{O}((\\frac{\\sqrt{d \\log(1/\u03b4)}}{n\\varepsilon})^{2/3} )$\uff0c\u8fd9\u4e0e\u5df2\u77e5\u7684\u6700\u4f73\u975e\u51f8DP-ERM\u7ed3\u679c\u76f8\u5339\u914d\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728DP\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86DP-(\u6709\u9650\u548c)-DRO\u9886\u57df\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u9488\u5bf9\u03c8-\u6563\u5ea6\u548c\u975e\u51f8\u635f\u5931\u7684\u60c5\u5f62\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7814\u7a76\u3002\u65b0\u63d0\u51fa\u7684DP\u4f18\u5316\u7b97\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u8fbe\u5230\u4e86\u826f\u597d\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u800c\u4e14\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u5b83\u4eec\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.16169", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16169", "abs": "https://arxiv.org/abs/2602.16169", "authors": ["Yunshu Wu", "Jiayi Cheng", "Partha Thakuria", "Rob Brekelmans", "Evangelos E. Papalexakis", "Greg Ver Steeg"], "title": "Discrete Stochastic Localization for Non-autoregressive Generation", "comment": null, "summary": "Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \\emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \\textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \\textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \\(\\sim\\)4$\\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSL\uff08Discrete Stochastic Localization\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2aSNR\u4e0d\u53d8\u7684\u53bb\u566a\u5668\u6765\u63d0\u9ad8MDLM/ReMDM\u91c7\u6837\u7684\u6b65\u9aa4\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDSL\u5728\u5c11\u91cf\u6b65\u9aa4\u5185\u5c31\u80fd\u663e\u8457\u63d0\u5347\u6587\u672c\u751f\u6210\u7684\u8d28\u91cf\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c11\u4e86\u5927\u7ea64\u500d\u7684\u8ba1\u7b97\u91cf\uff0c\u5e76\u4e14\u5728\u9ad8\u9884\u7b97\u4e0b\u8fbe\u5230\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8d28\u91cf\u3002", "motivation": "\u975e\u81ea\u56de\u5f52\u751f\u6210\u80fd\u591f\u901a\u8fc7\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u6807\u8bb0\u6765\u51cf\u5c11\u89e3\u7801\u5ef6\u8fdf\uff0c\u4f46\u8fed\u4ee3\u7cbe\u70bc\u8fc7\u7a0b\u5f80\u5f80\u56e0\u81ea\u6211\u751f\u6210\u8349\u7a3f\u800c\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u5206\u5e03\u504f\u79fb\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u6765\u63d0\u9ad8\u8fd9\u7c7b\u6a21\u578b\u7684\u6b65\u9aa4\u6548\u7387\uff0c\u5373\u7528\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\u8fbe\u5230\u66f4\u597d\u7684\u751f\u6210\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86DSL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8de8\u591a\u79cd\u635f\u574f\u7a0b\u5ea6\u7684\u5355\u4e00SNR\u4e0d\u53d8\u53bb\u566a\u5668\uff0c\u5728\u540c\u4e00\u6269\u6563\u8f6c\u6362\u5668\u4e2d\u8fde\u63a5\u4e86\u4e2d\u95f4\u8349\u7a3f\u566a\u58f0\u4e0e\u63a9\u7801\u6837\u5f0f\u7684\u7aef\u70b9\u635f\u574f\u3002", "result": "\u5728OpenWebText\u6570\u636e\u96c6\u4e0a\uff0cDSL\u5fae\u8c03\u4ee5\u8f83\u5c11\u7684\u6b65\u9aa4\u9884\u7b97\u83b7\u5f97\u4e86\u8f83\u5927\u7684MAUVE\u5f97\u5206\u589e\u957f\uff0c\u4f7f\u7528\u7ea6\u56db\u5206\u4e4b\u4e00\u7684\u53bb\u566a\u5668\u8bc4\u4f30\u6b21\u6570\u8d85\u8fc7\u4e86MDLM+ReMDM\u57fa\u7ebf\uff0c\u5e76\u4e14\u5728\u9ad8\u9884\u7b97\u65f6\u8fbe\u5230\u4e86\u81ea\u56de\u5f52\u8d28\u91cf\u6c34\u5e73\u3002\u5206\u6790\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3002", "conclusion": "DSL\u65b9\u6cd5\u6210\u529f\u5730\u63d0\u9ad8\u4e86MDLM/ReMDM\u91c7\u6837\u7684\u6b65\u9aa4\u6548\u7387\uff0c\u4f7f\u5f97\u5728\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u4e0b\u4e5f\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u751f\u6210\u7ed3\u679c\u3002"}}
{"id": "2602.16181", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16181", "abs": "https://arxiv.org/abs/2602.16181", "authors": ["Diego Labate", "Dipanwita Thakur", "Giancarlo Fortino"], "title": "Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters", "comment": null, "summary": "Energy theft poses a significant threat to the stability and efficiency of smart grids, leading to substantial economic losses and operational challenges. Traditional centralized machine learning approaches for theft detection require aggregating user data, raising serious concerns about privacy and data security. These issues are further exacerbated in smart meter environments, where devices are often resource-constrained and lack the capacity to run heavy models. In this work, we propose a privacy-preserving federated learning framework for energy theft detection that addresses both privacy and computational constraints. Our approach leverages a lightweight multilayer perceptron (MLP) model, suitable for deployment on low-power smart meters, and integrates basic differential privacy (DP) by injecting Gaussian noise into local model updates before aggregation. This ensures formal privacy guarantees without compromising learning performance. We evaluate our framework on a real-world smart meter dataset under both IID and non-IID data distributions. Experimental results demonstrate that our method achieves competitive accuracy, precision, recall, and AUC scores while maintaining privacy and efficiency. This makes the proposed solution practical and scalable for secure energy theft detection in next-generation smart grid infrastructures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u62a4\u9690\u79c1\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u667a\u80fd\u7535\u7f51\u4e2d\u7684\u80fd\u6e90\u76d7\u7a83\u68c0\u6d4b\uff0c\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u9690\u79c1\u548c\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cAUC\u5206\u6570\u3002", "motivation": "\u9488\u5bf9\u667a\u80fd\u7535\u7f51\u4e2d\u80fd\u6e90\u76d7\u7a83\u5bf9\u7a33\u5b9a\u6027\u548c\u6548\u7387\u6784\u6210\u7684\u91cd\u5927\u5a01\u80c1\uff0c\u4ee5\u53ca\u4f20\u7edf\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u805a\u5408\u65f6\u5f15\u53d1\u7684\u9690\u79c1\u548c\u6570\u636e\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e2\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u53c8\u8003\u8651\u4e86\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5728\u672c\u5730\u6a21\u578b\u66f4\u65b0\u524d\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u6765\u5b9e\u73b0\u57fa\u7840\u5dee\u5206\u9690\u79c1\uff08DP\uff09\uff0c\u4ece\u800c\u786e\u4fdd\u6b63\u5f0f\u7684\u9690\u79c1\u4fdd\u8bc1\u800c\u4e0d\u5f71\u54cd\u5b66\u4e60\u6027\u80fd\u3002", "result": "\u5728\u5b9e\u9645\u667a\u80fd\u7535\u8868\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u65e0\u8bba\u662fIID\u8fd8\u662f\u975eIID\u6570\u636e\u5206\u5e03\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u5747\u5c55\u793a\u4e86\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u53caAUC\u5f97\u5206\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u9690\u79c1\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5bf9\u4e8e\u4e0b\u4e00\u4ee3\u667a\u80fd\u7535\u7f51\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u5b89\u5168\u80fd\u6e90\u76d7\u7a83\u68c0\u6d4b\u6765\u8bf4\u662f\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u3002"}}
{"id": "2602.16188", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16188", "abs": "https://arxiv.org/abs/2602.16188", "authors": ["Filippos Bellos", "NaveenJohn Premkumar", "Yannis Avrithis", "Nam H. Nguyen", "Jason J. Corso"], "title": "Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting", "comment": "Accepted to ICASSP 2026", "summary": "LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u65b9\u6cd5\u2014\u2014Temporal-Prior Conditioning (TPC)\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u591a\u4e2a\u5c42\u7ea7\u4e0a\u5f15\u5165\u65f6\u95f4\u6761\u4ef6\uff0c\u6539\u8fdb\u4e86\u65f6\u95f4\u4fe1\u606f\u7684\u5904\u7406\u65b9\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u957f\u671f\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u901a\u5e38\u53ea\u5728\u8f93\u5165\u5c42\u6d45\u5c42\u5730\u5f15\u5165\u4f4d\u7f6e\u6216\u63d0\u793a\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u968f\u5c42\u6570\u589e\u52a0\u65f6\u5bf9\u65f6\u95f4\u4fe1\u606f\u7684\u6709\u6548\u5229\u7528\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u6807\u8bb0\uff0c\u5e76\u5728\u9009\u5b9a\u7684\u5c42\u4e2d\u8ba9\u8fd9\u4e9b\u6807\u8bb0\u4e0e\u7531\u51bb\u7ed3\u7684\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5f97\u5230\u7684\u7d27\u51d1\u4e14\u6613\u8bfb\u7684\u65f6\u95f4\u63cf\u8ff0\u7b26\u4ea4\u53c9\u6ce8\u610f\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u53cd\u9988\u65f6\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u65f6\u95f4\u4fe1\u606f\u4e0e\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u7684\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4ec5\u8bad\u7ec3\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\uff0cTPC\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u4e8e\u5168\u5fae\u8c03\u53ca\u6d45\u5c42\u6761\u4ef6\u7b56\u7565\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Temporal-Prior Conditioning (TPC) \u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5c24\u5176\u662f\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\u7684\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u6df1\u5c42\u6b21\u6574\u5408\u65f6\u95f4\u4fe1\u606f\u800c\u4e0d\u5927\u5e45\u589e\u52a0\u53c2\u6570\u91cf\u3002"}}
{"id": "2602.16196", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16196", "abs": "https://arxiv.org/abs/2602.16196", "authors": ["Emile Anand", "Richard Hoffmann", "Sarah Liaw", "Adam Wierman"], "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning", "comment": "43 pages, 5 figures, 1 table", "summary": "Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\\texttt{GMFS}$, a $\\textbf{G}$raphon $\\textbf{M}$ean-$\\textbf{F}$ield $\\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $\u03ba$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\\mathrm{poly}(\u03ba)$ and optimality gap $O(1/\\sqrt\u03ba)$. We verify our theory with numerical simulations in robotic coordination, showing that $\\texttt{GMFS}$ achieves near-optimal performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGMFS\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u5f02\u8d28\u6027\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u534f\u4f5c\u95ee\u9898\u3002\u901a\u8fc7\u6839\u636e\u4ea4\u4e92\u5f3a\u5ea6\u5bf9\u03ba\u4e2a\u667a\u80fd\u4f53\u8fdb\u884c\u5b50\u91c7\u6837\uff0c\u5b9e\u73b0\u4e86\u56fe\u6743\u91cd\u5e73\u5747\u573a\u7684\u8fd1\u4f3c\uff0c\u5e76\u4ee5\u591a\u9879\u5f0f(\u03ba)\u7684\u6837\u672c\u590d\u6742\u5ea6\u548cO(1/\u221a\u03ba)\u7684\u6700\u4f18\u5dee\u8ddd\u6765\u5b66\u4e60\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGMFS\u5728\u673a\u5668\u4eba\u534f\u8c03\u65b9\u9762\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u968f\u7740\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\uff0c\u8054\u5408\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u5448\u6307\u6570\u589e\u957f\uff0c\u8fd9\u7ed9\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u7fa4\u4f53\u7684\u534f\u8c03\u5e26\u6765\u4e86\u6311\u6218\u3002\u867d\u7136\u5747\u503c\u573a\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u805a\u5408\u667a\u80fd\u4f53\u95f4\u7684\u4e92\u52a8\u6765\u51cf\u8f7b\u8fd9\u4e00\u8d1f\u62c5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4e92\u52a8\u662f\u540c\u8d28\u5316\u7684\uff1b\u800c\u57fa\u4e8e\u56fe\u8bba\u7684\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u6355\u6349\u5230\u5f02\u8d28\u6027\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u968f\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u800c\u53d8\u5f97\u6602\u8d35\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6709\u6548\u5904\u7406\u5177\u6709\u5f02\u8d28\u6027\u4e92\u52a8\u7684\u5927\u89c4\u6a21\u5408\u4f5c\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGMFS\uff08Graphon Mean-Field Subsampling\uff09\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6839\u636e\u4ea4\u4e92\u5f3a\u5ea6\u5bf9\u03ba\u4e2a\u667a\u80fd\u4f53\u8fdb\u884c\u5b50\u91c7\u6837\uff0c\u6765\u8fd1\u4f3c\u56fe\u6743\u91cd\u5e73\u5747\u573a\u3002\u6b64\u65b9\u6cd5\u5141\u8bb8\u4ece\u51cf\u5c11\u7684\u6837\u672c\u96c6\u4e2d\u5b66\u4e60\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5168\u96c6\u5408\u7406\u8bba\u4e0a\u76f8\u8fd1\u7684\u5b66\u4e60\u6548\u679c\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\uff0c\u4f7f\u7528GMFS\u5b66\u4e60\u7b56\u7565\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e3apoly(\u03ba)\uff0c\u4e14\u6700\u4f18\u5dee\u8ddd\u4e3aO(1/\u221a\u03ba)\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u673a\u5668\u4eba\u534f\u8c03\u573a\u666f\u4e0b\u7684\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793aGMFS\u80fd\u591f\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u8868\u73b0\u3002", "conclusion": "GMFS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u89e3\u51b3\u5927\u89c4\u6a21\u5f02\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u4f5c\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2602.16198", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16198", "abs": "https://arxiv.org/abs/2602.16198", "authors": ["Qijie Zhu", "Zeqi Ye", "Han Liu", "Zhaoran Wang", "Minshuo Chen"], "title": "Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform", "comment": "36 pages, 3 figures", "summary": "Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDOIT\uff08Doob\u5bfc\u5411\u7684\u63a8\u7406\u65f6\u95f4\u8f6c\u6362\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u4e00\u822c\u975e\u53ef\u5fae\u5956\u52b1\u3002\u901a\u8fc7\u5ea6\u91cf\u4f20\u8f93\u516c\u5f0f\u548cDoob\u7684h-\u53d8\u6362\u6765\u5b9e\u73b0\u4ece\u9884\u8bad\u7ec3\u751f\u6210\u5206\u5e03\u5230\u9ad8\u5956\u52b1\u76ee\u6807\u5206\u5e03\u7684\u8f6c\u53d8\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u4fdd\u8bc1\u3002\u5728D4RL\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u91c7\u6837\u6548\u7387\u7684\u540c\u65f6\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u9002\u5e94\u65b9\u6cd5\u901a\u5e38\u5c06\u9002\u5e94\u76ee\u6807\u62bd\u8c61\u4e3a\u4e00\u4e2a\u5956\u52b1\u51fd\u6570\uff0c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u5956\u52b1\u6837\u672c\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u56e0\u989d\u5916\u8bad\u7ec3\u800c\u4ea7\u751f\u9ad8\u6602\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u6216\u8005\u4f9d\u8d56\u4e8e\u5bf9\u5956\u52b1\u5982\u53ef\u5fae\u6027\u7684\u4e25\u683c\u5047\u8bbe\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u5374\u5f88\u5c11\u6709\u7406\u8bba\u4e0a\u7684\u8bc1\u660e\u548c\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86DOIT\uff08Doob-Oriented Inference-time Transformation\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u5e94\u7528\u4e8e\u901a\u7528\u7684\u3001\u975e\u53ef\u5fae\u5206\u7684\u5956\u52b1\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u6846\u67b6\u662f\u4e00\u79cd\u5ea6\u91cf\u4f20\u8f93\u516c\u5f0f\uff0c\u65e8\u5728\u5c06\u9884\u8bad\u7ec3\u751f\u6210\u5206\u5e03\u8f6c\u6362\u4e3a\u76ee\u6807\u9ad8\u5956\u52b1\u5206\u5e03\u3002\u5229\u7528Doob\u7684$h$-\u53d8\u6362\u5b9e\u73b0\u4e86\u8fd9\u4e00\u4f20\u8f93\u8fc7\u7a0b\uff0c\u4ece\u800c\u5728\u4e0d\u4fee\u6539\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5bf9\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u8fdb\u884c\u52a8\u6001\u4fee\u6b63\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u6a21\u62df\u7684\u6709\u6548\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u5c42\u9762\uff0c\u901a\u8fc7\u5bf9\u52a8\u6001Doob\u4fee\u6b63\u4e2d\u7684\u8fd1\u4f3c\u8bef\u5dee\u8fdb\u884c\u523b\u753b\uff0c\u5efa\u7acb\u4e86\u5230\u8fbe\u76ee\u6807\u9ad8\u5956\u52b1\u5206\u5e03\u7684\u9ad8\u6982\u7387\u6536\u655b\u4fdd\u969c\u3002\u5b9e\u9a8c\u65b9\u9762\uff0c\u5728D4RL\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u4e00\u81f4\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u540c\u65f6\u8fd8\u7ef4\u6301\u4e86\u826f\u597d\u7684\u91c7\u6837\u6548\u7387\u3002", "conclusion": "DOIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4ee5\u9002\u5e94\u5404\u79cd\u5e94\u7528\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u975e\u53ef\u5fae\u5956\u52b1\u65f6\u5c55\u73b0\u51fa\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.16204", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16204", "abs": "https://arxiv.org/abs/2602.16204", "authors": ["Pooja Honna", "Ayush Patravali", "Nithin Nagaraj", "Nanjangud C. Narendra"], "title": "Linked Data Classification using Neurochaos Learning", "comment": null, "summary": "Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements. In prior work, NL has been implemented and extensively tested on separable and time series data, and demonstrated its superior performance on both classification and regression tasks. In this paper, we investigate the next step in NL, viz., applying NL to linked data, in particular, data that is represented in the form of knowledge graphs. We integrate linked data into NL by implementing node aggregation on knowledge graphs, and then feeding the aggregated node features to the simplest NL architecture: ChaosNet. We demonstrate the results of our implementation on homophilic graph datasets as well as heterophilic graph datasets of verying heterophily. We show better efficacy of our approach on homophilic graphs than on heterophilic graphs. While doing so, we also present our analysis of the results, as well as suggestions for future work.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\uff08NL\uff09\u5e94\u7528\u4e8e\u94fe\u63a5\u6570\u636e\uff0c\u7279\u522b\u662f\u77e5\u8bc6\u56fe\u8c31\u5f62\u5f0f\u7684\u6570\u636e\u3002\u901a\u8fc7\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u5b9e\u73b0\u8282\u70b9\u805a\u5408\uff0c\u5e76\u5c06\u805a\u5408\u540e\u7684\u8282\u70b9\u7279\u5f81\u8f93\u5165\u5230\u6700\u7b80\u5355\u7684NL\u67b6\u6784ChaosNet\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u540c\u8d28\u56fe\u4e0a\u7684\u6548\u679c\u4f18\u4e8e\u5f02\u8d28\u56fe\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u8bc1\u660e\u4e86\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\uff08NL\uff09\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u7136\u800c\uff0cNL\u5c1a\u672a\u88ab\u5e94\u7528\u4e8e\u94fe\u63a5\u6570\u636e\uff0c\u5982\u77e5\u8bc6\u56fe\u8c31\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22NL\u5728\u5904\u7406\u8fd9\u79cd\u7c7b\u578b\u7684\u6570\u636e\u65f6\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u8005\u4eec\u9996\u5148\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u5b9e\u73b0\u4e86\u8282\u70b9\u805a\u5408\u6280\u672f\uff0c\u7136\u540e\u5c06\u805a\u5408\u5f97\u5230\u7684\u8282\u70b9\u7279\u5f81\u8f93\u5165\u5230NL\u7684\u4e00\u4e2a\u7b80\u5355\u6a21\u578bChaosNet\u4e2d\u3002\u901a\u8fc7\u5bf9\u540c\u8d28\u6027\u548c\u4e0d\u540c\u7a0b\u5ea6\u5f02\u8d28\u6027\u7684\u56fe\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5\u6765\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5904\u7406\u540c\u8d28\u56fe\u65f6\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u5904\u7406\u9ad8\u5ea6\u5f02\u8d28\u56fe\u65f6\u8868\u73b0\u5f97\u66f4\u597d\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u67d0\u4e9b\u7c7b\u578b\u7684\u94fe\u63a5\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u540c\u8d28\u56fe\u4e2d\u3002\u5bf9\u4e8e\u672a\u6765\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u548c\u6269\u5c55\u8be5\u65b9\u6cd5\u7684\u65b9\u5411\u3002"}}
{"id": "2602.16209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16209", "abs": "https://arxiv.org/abs/2602.16209", "authors": ["Jiaquan Zhang", "Fachrina Dewi Puspitasari", "Songbo Zhang", "Yibei Liu", "Kuien Liu", "Caiyan Qin", "Fan Mo", "Peng Wang", "Yang Yang", "Chaoning Zhang"], "title": "Geometric Neural Operators via Lie Group-Constrained Latent Dynamics", "comment": null, "summary": "Neural operators offer an effective framework for learning solutions of partial differential equations for many physical systems in a resolution-invariant and data-driven manner. Existing neural operators, however, often suffer from instability in multi-layer iteration and long-horizon rollout, which stems from the unconstrained Euclidean latent space updates that violate the geometric and conservation laws. To address this challenge, we propose to constrain manifolds with low-rank Lie algebra parameterization that performs group action updates on the latent representation. Our method, termed Manifold Constraining based on Lie group (MCL), acts as an efficient \\emph{plug-and-play} module that enforces geometric inductive bias to existing neural operators. Extensive experiments on various partial differential equations, such as 1-D Burgers and 2-D Navier-Stokes, over a wide range of parameters and steps demonstrate that our method effectively lowers the relative prediction error by 30-50\\% at the cost of 2.26\\% of parameter increase. The results show that our approach provides a scalable solution for improving long-term prediction fidelity by addressing the principled geometric constraints absent in the neural operator updates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u674e\u4ee3\u6570\u53c2\u6570\u5316\u7684\u6d41\u5f62\u7ea6\u675f\u65b9\u6cd5\uff08MCL\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u5728\u591a\u5c42\u8fed\u4ee3\u548c\u957f\u671f\u6eda\u52a8\u9884\u6d4b\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5728\u6f5c\u5728\u8868\u793a\u4e0a\u6267\u884c\u7fa4\u4f5c\u7528\u66f4\u65b0\uff0cMCL\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u589e\u5f3a\u4e86\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u4e86\u76f8\u5bf9\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u7b97\u5b50\u5728\u5904\u7406\u591a\u5c42\u8fed\u4ee3\u548c\u957f\u671f\u6eda\u52a8\u9884\u6d4b\u65f6\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u5b83\u4eec\u5728\u6b27\u51e0\u91cc\u5f97\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u66f4\u65b0\u4e0d\u53d7\u7ea6\u675f\uff0c\u8fdd\u53cd\u4e86\u51e0\u4f55\u548c\u5b88\u6052\u5b9a\u5f8b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f4e\u79e9\u674e\u4ee3\u6570\u53c2\u6570\u5316\u7684\u6d41\u5f62\u7ea6\u675f\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7684Manifold Constraining based on Lie group (MCL) \u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u674e\u4ee3\u6570\u53c2\u6570\u5316\u6765\u7ea6\u675f\u6d41\u5f62\uff0c\u5e76\u5bf9\u6f5c\u5728\u8868\u793a\u6267\u884c\u7fa4\u4f5c\u7528\u66f4\u65b0\u3002\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u65e8\u5728\u5411\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u4e2d\u5f15\u5165\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u901a\u8fc7\u5bf9\u5305\u62ec1-D Burgers\u65b9\u7a0b\u548c2-D Navier-Stokes\u65b9\u7a0b\u5728\u5185\u7684\u591a\u79cd\u504f\u5fae\u5206\u65b9\u7a0b\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5728\u4e0d\u540c\u53c2\u6570\u8303\u56f4\u548c\u6b65\u957f\u4e0b\uff0cMCL\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4e30-50%\u7684\u76f8\u5bf9\u9884\u6d4b\u8bef\u5dee\uff0c\u540c\u65f6\u4ec5\u589e\u52a02.26%\u7684\u53c2\u6570\u91cf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cMCL\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u89e3\u51b3\u795e\u7ecf\u7b97\u5b50\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u7684\u57fa\u672c\u51e0\u4f55\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.16216", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16216", "abs": "https://arxiv.org/abs/2602.16216", "authors": ["Hamzeh Asgharnezhad", "Pegah Tabarisaadi", "Abbas Khosravi", "Roohallah Alizadehsani", "U. Rajendra Acharya"], "title": "UCTECG-Net: Uncertainty-aware Convolution Transformer ECG Network for Arrhythmia Detection", "comment": null, "summary": "Deep learning has improved automated electrocardiogram (ECG) classification, but limited insight into prediction reliability hinders its use in safety-critical settings. This paper proposes UCTECG-Net, an uncertainty-aware hybrid architecture that combines one-dimensional convolutions and Transformer encoders to process raw ECG signals and their spectrograms jointly. Evaluated on the MIT-BIH Arrhythmia and PTB Diagnostic datasets, UCTECG-Net outperforms LSTM, CNN1D, and Transformer baselines in terms of accuracy, precision, recall and F1 score, achieving up to 98.58% accuracy on MIT-BIH and 99.14% on PTB. To assess predictive reliability, we integrate three uncertainty quantification methods (Monte Carlo Dropout, Deep Ensembles, and Ensemble Monte Carlo Dropout) into all models and analyze their behavior using an uncertainty-aware confusion matrix and derived metrics. The results show that UCTECG-Net, particularly with Ensemble or EMCD, provides more reliable and better-aligned uncertainty estimates than competing architectures, offering a stronger basis for risk-aware ECG decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUCTECG-Net\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6df7\u5408\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u4e00\u7ef4\u5377\u79ef\u548cTransformer\u7f16\u7801\u5668\u6765\u5171\u540c\u5904\u7406\u539f\u59cb\u5fc3\u7535\u56fe\u4fe1\u53f7\u53ca\u5176\u9891\u8c31\u56fe\u3002\u5728MIT-BIH\u5fc3\u5f8b\u5931\u5e38\u548cPTB\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0cUCTECG-Net\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u4f18\u4e8eLSTM\u3001CNN1D\u548cTransformer\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u901a\u8fc7\u96c6\u6210\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5fc3\u7535\u56fe\u51b3\u7b56\u652f\u6301\u57fa\u7840\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6539\u8fdb\u4e86\u81ea\u52a8\u5fc3\u7535\u56fe\u5206\u7c7b\uff0c\u4f46\u7531\u4e8e\u5bf9\u5176\u9884\u6d4b\u53ef\u9760\u6027\u7684\u4e86\u89e3\u6709\u9650\uff0c\u963b\u788d\u4e86\u5b83\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86UCTECG-Net\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86\u4e00\u7ef4\u5377\u79ef\u4e0eTransformer\u7f16\u7801\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u540c\u65f6\u5904\u7406\u539f\u59cb\u5fc3\u7535\u56fe\u4fe1\u53f7\u53ca\u5176\u9891\u8c31\u56fe\u3002\u6b64\u5916\uff0c\u8fd8\u6574\u5408\u4e86\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff08\u8499\u7279\u5361\u6d1bDropout\u3001\u6df1\u5ea6\u96c6\u6210\u548c\u96c6\u6210\u8499\u7279\u5361\u6d1bDropout\uff09\u5230\u6240\u6709\u6a21\u578b\u4e2d\uff0c\u5e76\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u6df7\u6dc6\u77e9\u9635\u53ca\u884d\u751f\u6307\u6807\u5206\u6790\u5176\u884c\u4e3a\u3002", "result": "UCTECG-Net\u5728MIT-BIH\u548cPTB\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8d85\u8fc7\u4e86LSTM\u3001CNN1D\u548cTransformer\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u7279\u522b\u662f\u5728\u96c6\u6210\u6216EMCD\u7684\u5e2e\u52a9\u4e0b\uff0cUCTECG-Net\u63d0\u4f9b\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6bd4\u5176\u4ed6\u7ade\u4e89\u67b6\u6784\u66f4\u52a0\u53ef\u9760\u4e14\u4e00\u81f4\u3002", "conclusion": "UCTECG-Net\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5fc3\u7535\u56fe\u5206\u7c7b\u65b9\u6cd5\uff0c\u800c\u4e14\u901a\u8fc7\u96c6\u6210\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u4e3a\u98ce\u9669\u610f\u8bc6\u7684\u5fc3\u7535\u56fe\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7840\u3002"}}
{"id": "2602.16217", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16217", "abs": "https://arxiv.org/abs/2602.16217", "authors": ["Jash Vira", "Andrew Myers", "Simon Ratcliffe"], "title": "Multi-Class Boundary Extraction from Implicit Representations", "comment": null, "summary": "Surface extraction from implicit neural representations modelling a single class surface is a well-known task. However, there exist no surface extraction methods from an implicit representation of multiple classes that guarantee topological correctness and no holes. In this work, we lay the groundwork by introducing a 2D boundary extraction algorithm for the multi-class case focusing on topological consistency and water-tightness, which also allows for setting minimum detail restraint on the approximation. Finally, we evaluate our algorithm using geological modelling data, showcasing its adaptiveness and ability to honour complex topology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76842D\u8fb9\u754c\u63d0\u53d6\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u591a\u7c7b\u60c5\u51b5\u4e0b\u7684\u9690\u5f0f\u8868\u793a\uff0c\u8be5\u7b97\u6cd5\u6ce8\u91cd\u62d3\u6251\u4e00\u81f4\u6027\u4e14\u65e0\u5b54\u6d1e\uff0c\u5e76\u901a\u8fc7\u5730\u8d28\u5efa\u6a21\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u9002\u5e94\u6027\u548c\u5904\u7406\u590d\u6742\u62d3\u6251\u7684\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u8fd8\u6ca1\u6709\u4ece\u4fdd\u8bc1\u62d3\u6251\u6b63\u786e\u6027\u548c\u6ca1\u6709\u5b54\u6d1e\u7684\u591a\u7c7b\u9690\u5f0f\u8868\u793a\u4e2d\u63d0\u53d6\u8868\u9762\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u4eec\u7740\u624b\u5960\u5b9a\u4e86\u57fa\u7840\u5de5\u4f5c\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u7c7b\u60c5\u51b5\u76842D\u8fb9\u754c\u63d0\u53d6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5f3a\u8c03\u62d3\u6251\u4e00\u81f4\u6027\u548c\u6c34\u5bc6\u6027\uff0c\u540c\u65f6\u5141\u8bb8\u5bf9\u903c\u8fd1\u8bbe\u5b9a\u6700\u5c0f\u7ec6\u8282\u9650\u5236\u3002", "result": "\u901a\u8fc7\u5730\u8d28\u5efa\u6a21\u6570\u636e\u8bc4\u4f30\u4e86\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u5b83\u5728\u9762\u5bf9\u590d\u6742\u62d3\u6251\u65f6\u7684\u9002\u5e94\u6027\u548c\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u7c7b\u522b\u7684\u9690\u5f0f\u8868\u793a\uff0c\u5e76\u4e14\u5728\u4fdd\u6301\u62d3\u6251\u4e00\u81f4\u6027\u548c\u907f\u514d\u5b54\u6d1e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.16218", "categories": ["cs.LG", "math.NA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16218", "abs": "https://arxiv.org/abs/2602.16218", "authors": ["Maren Mahsereci", "Toni Karvonen"], "title": "Bayesian Quadrature: Gaussian Processes for Integration", "comment": null, "summary": "Bayesian quadrature is a probabilistic, model-based approach to numerical integration, the estimation of intractable integrals, or expectations. Although Bayesian quadrature was popularised already in the 1980s, no systematic and comprehensive treatment has been published. The purpose of this survey is to fill this gap. We review the mathematical foundations of Bayesian quadrature from different points of view; present a systematic taxonomy for classifying different Bayesian quadrature methods along the three axes of modelling, inference, and sampling; collect general theoretical guarantees; and provide a controlled numerical study that explores and illustrates the effect of different choices along the axes of the taxonomy. We also provide a realistic assessment of practical challenges and limitations to application of Bayesian quadrature methods and include an up-to-date and nearly exhaustive bibliography that covers not only machine learning and statistics literature but all areas of mathematics and engineering in which Bayesian quadrature or equivalent methods have seen use.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u6587\u7ae0\u7cfb\u7edf\u5730\u4ecb\u7ecd\u4e86\u8d1d\u53f6\u65af\u6c42\u79ef\u65b9\u6cd5\u7684\u6570\u5b66\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u4f53\u7cfb\u6765\u5f52\u7c7b\u4e0d\u540c\u7684\u8d1d\u53f6\u65af\u6c42\u79ef\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u3001\u63a8\u65ad\u548c\u62bd\u6837\u4e09\u4e2a\u8f74\u4e0a\u7684\u4e0d\u540c\u9009\u62e9\u5bf9\u6570\u503c\u7ed3\u679c\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51e0\u4e4e\u8be6\u5c3d\u7684\u53c2\u8003\u6587\u732e\u5217\u8868\uff0c\u6db5\u76d6\u4e86\u4f7f\u7528\u8d1d\u53f6\u65af\u6c42\u79ef\u6216\u7b49\u6548\u65b9\u6cd5\u7684\u6240\u6709\u6570\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u3002", "motivation": "\u5c3d\u7ba1\u8d1d\u53f6\u65af\u6c42\u79ef\u81ea20\u4e16\u7eaa80\u5e74\u4ee3\u5c31\u5df2\u7ecf\u6d41\u884c\u8d77\u6765\uff0c\u4f46\u81f3\u4eca\u6ca1\u6709\u4e00\u4efd\u7cfb\u7edf\u800c\u5168\u9762\u7684\u5904\u7406\u8be5\u4e3b\u9898\u7684\u6587\u732e\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u56de\u987e\u8d1d\u53f6\u65af\u6c42\u79ef\u7684\u6570\u5b66\u57fa\u7840\u3001\u63d0\u51fa\u5206\u7c7b\u6cd5\u3001\u6536\u96c6\u7406\u8bba\u4fdd\u8bc1\u4ee5\u53ca\u8fdb\u884c\u63a7\u5236\u6027\u6570\u503c\u7814\u7a76\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u7406\u89e3\u6846\u67b6\u3002", "method": "\u6587\u7ae0\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u96be\u4ee5\u8ba1\u7b97\u7684\u79ef\u5206\u6216\u671f\u671b\u503c\u3002\u5b83\u4ece\u591a\u4e2a\u89c6\u89d2\u5ba1\u89c6\u8d1d\u53f6\u65af\u6c42\u79ef\u7684\u6570\u5b66\u6839\u57fa\uff1b\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u5efa\u6a21\u3001\u63a8\u7406\u4e0e\u62bd\u6837\u4e09\u5927\u7ef4\u5ea6\u7684\u7cfb\u7edf\u5206\u7c7b\u5b66\uff1b\u5e76\u6c47\u603b\u4e86\u666e\u904d\u9002\u7528\u7684\u7406\u8bba\u4fdd\u969c\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u53d7\u63a7\u7684\u6570\u503c\u7814\u7a76\u4ee5\u63a2\u7d22\u8bf4\u660e\u5206\u7c7b\u5b66\u5404\u8f74\u4e0a\u4e0d\u540c\u9009\u62e9\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u8d1d\u53f6\u65af\u6c42\u79ef\u65b9\u6cd5\u4e2d\u5bf9\u4e8e\u6a21\u578b\u3001\u63a8\u7406\u53ca\u62bd\u6837\u7b56\u7565\u7684\u4e0d\u540c\u9009\u62e9\u4f1a\u5bf9\u6700\u7ec8\u7ed3\u679c\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u4e5f\u6307\u51fa\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u6311\u6218\u4e0e\u9650\u5236\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u8d1d\u53f6\u65af\u6c42\u79ef\u65b9\u6cd5\u7684\u4e00\u4e2a\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u5176\u7406\u8bba\u80cc\u666f\u3001\u5b9e\u8df5\u6307\u5357\u53ca\u5176\u5728\u5404\u4e2a\u9886\u57df\u7684\u5e94\u7528\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u8fd8\u7ed9\u51fa\u4e86\u4e00\u4e2a\u6700\u65b0\u7684\u3001\u51e0\u4e4e\u8986\u76d6\u6240\u6709\u76f8\u5173\u5de5\u4f5c\u7684\u53c2\u8003\u4e66\u76ee\u3002"}}
{"id": "2602.16224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16224", "abs": "https://arxiv.org/abs/2602.16224", "authors": ["Xu Zhang", "Peng Wang", "Yichen Li", "Wei Wang"], "title": "Amortized Predictability-aware Training Framework for Time Series Forecasting and Classification", "comment": "This work is accepted by the proceedings of the ACM Web Conference 2026 (WWW 2026). The code is available at the link https://github.com/Meteor-Stars/APTF", "summary": "Time series data are prone to noise in various domains, and training samples may contain low-predictability patterns that deviate from the normal data distribution, leading to training instability or convergence to poor local minima. Therefore, mitigating the adverse effects of low-predictability samples is crucial for time series analysis tasks such as time series forecasting (TSF) and time series classification (TSC). While many deep learning models have achieved promising performance, few consider how to identify and penalize low-predictability samples to improve model performance from the training perspective. To fill this gap, we propose a general Amortized Predictability-aware Training Framework (APTF) for both TSF and TSC. APTF introduces two key designs that enable the model to focus on high-predictability samples while still learning appropriately from low-predictability ones: (i) a Hierarchical Predictability-aware Loss (HPL) that dynamically identifies low-predictability samples and progressively expands their loss penalty as training evolves, and (ii) an amortization model that mitigates predictability estimation errors caused by model bias, further enhancing HPL's effectiveness. The code is available at https://github.com/Meteor-Stars/APTF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6APTF\uff0c\u901a\u8fc7\u5f15\u5165\u5c42\u7ea7\u53ef\u9884\u6d4b\u6027\u635f\u5931\uff08HPL\uff09\u548c\u644a\u9500\u6a21\u578b\u6765\u8bc6\u522b\u5e76\u60e9\u7f5a\u4f4e\u53ef\u9884\u6d4b\u6027\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u8bad\u7ec3\u6837\u672c\u53ef\u80fd\u5305\u542b\u504f\u79bb\u6b63\u5e38\u6570\u636e\u5206\u5e03\u7684\u4f4e\u53ef\u9884\u6d4b\u6027\u6a21\u5f0f\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u6536\u655b\u5230\u4e0d\u826f\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u4e3a\u4e86\u51cf\u8f7b\u4f4e\u53ef\u9884\u6d4b\u6027\u6837\u672c\u5bf9\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\uff08\u5982\u65f6\u95f4\u5e8f\u5217\u9884\u6d4bTSF\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u7c7bTSC\uff09\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u644a\u9500\u53ef\u9884\u6d4b\u6027\u610f\u8bc6\u8bad\u7ec3\u6846\u67b6APTF\u3002", "method": "APTF\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a(i) \u5c42\u7ea7\u53ef\u9884\u6d4b\u6027\u635f\u5931\uff08HPL\uff09\uff0c\u80fd\u591f\u52a8\u6001\u5730\u8bc6\u522b\u51fa\u4f4e\u53ef\u9884\u6d4b\u6027\u6837\u672c\uff0c\u5e76\u968f\u7740\u8bad\u7ec3\u8fc7\u7a0b\u9010\u6e10\u589e\u52a0\u5b83\u4eec\u7684\u635f\u5931\u60e9\u7f5a\uff1b(ii) \u4e00\u4e2a\u644a\u9500\u6a21\u578b\uff0c\u7528\u4e8e\u51cf\u5c11\u7531\u4e8e\u6a21\u578b\u504f\u5dee\u5f15\u8d77\u7684\u53ef\u9884\u6d4b\u6027\u4f30\u8ba1\u8bef\u5dee\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3aHPL\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPTF\u53ef\u4ee5\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b(TSF)\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b(TSC)\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "APTF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7279\u522b\u5904\u7406\u4f4e\u53ef\u9884\u6d4b\u6027\u6837\u672c\u4ee5\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2602.16229", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16229", "abs": "https://arxiv.org/abs/2602.16229", "authors": ["Zizhao Wang", "Chang Shi", "Jiaheng Hu", "Kevin Rohling", "Roberto Mart\u00edn-Mart\u00edn", "Amy Zhang", "Peter Stone"], "title": "Factored Latent Action World Models", "comment": null, "summary": "Learning latent actions from action-free video has emerged as a powerful paradigm for scaling up controllable world model learning. Latent actions provide a natural interface for users to iteratively generate and manipulate videos. However, most existing approaches rely on monolithic inverse and forward dynamics models that learn a single latent action to control the entire scene, and therefore struggle in complex environments where multiple entities act simultaneously. This paper introduces Factored Latent Action Model (FLAM), a factored dynamics framework that decomposes the scene into independent factors, each inferring its own latent action and predicting its own next-step factor value. This factorized structure enables more accurate modeling of complex multi-entity dynamics and improves video generation quality in action-free video settings compared to monolithic models. Based on experiments on both simulation and real-world multi-entity datasets, we find that FLAM outperforms prior work in prediction accuracy and representation quality, and facilitates downstream policy learning, demonstrating the benefits of factorized latent action models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u89e3\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u79f0\u4e3aFactored Latent Action Model (FLAM)\uff0c\u5b83\u80fd\u591f\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u72ec\u7acb\u7684\u56e0\u7d20\uff0c\u6bcf\u4e2a\u56e0\u7d20\u63a8\u65ad\u81ea\u5df1\u7684\u6f5c\u5728\u52a8\u4f5c\u5e76\u9884\u6d4b\u5176\u4e0b\u4e00\u6b65\u7684\u56e0\u5b50\u503c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6574\u4f53\u6a21\u578b\u76f8\u6bd4\uff0cFLAM\u5728\u590d\u6742\u7684\u591a\u5b9e\u4f53\u52a8\u6001\u5efa\u6a21\u4e2d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u597d\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u4e14\u6709\u52a9\u4e8e\u4e0b\u6e38\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u5355\u4e00\u7684\u9006\u5411\u548c\u6b63\u5411\u52a8\u529b\u5b66\u6a21\u578b\u6765\u5b66\u4e60\u63a7\u5236\u6574\u4e2a\u573a\u666f\u7684\u5355\u4e00\u6f5c\u5728\u52a8\u4f5c\uff0c\u5728\u591a\u4e2a\u5b9e\u4f53\u540c\u65f6\u4f5c\u7528\u7684\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u8fd9\u79cd\u590d\u6742\u7684\u591a\u5b9e\u4f53\u52a8\u6001\uff0c\u5e76\u63d0\u9ad8\u65e0\u52a8\u4f5c\u89c6\u9891\u8bbe\u7f6e\u4e0b\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u63d0\u51fa\u4e86Factored Latent Action Model (FLAM)\u3002", "method": "\u901a\u8fc7\u5f15\u5165Factored Latent Action Model (FLAM)\uff0c\u4e00\u79cd\u80fd\u591f\u5c06\u573a\u666f\u5206\u89e3\u6210\u72ec\u7acb\u56e0\u7d20\u7684\u65b0\u6846\u67b6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u56e0\u7d20\u90fd\u80fd\u63a8\u65ad\u51fa\u81ea\u8eab\u7684\u6f5c\u5728\u884c\u52a8\u5e76\u9884\u6d4b\u4e0b\u4e00\u6b65\u7684\u56e0\u5b50\u503c\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u6539\u5584\u5bf9\u4e8e\u590d\u6742\u591a\u5b9e\u4f53\u73af\u5883\u4e2d\u7684\u52a8\u6001\u5efa\u6a21\u51c6\u786e\u6027\u53ca\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u3002", "result": "\u57fa\u4e8e\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u591a\u5b9e\u4f53\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFLAM\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u8868\u793a\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\uff0c\u5e76\u4fc3\u8fdb\u4e86\u540e\u7eed\u7b56\u7565\u7684\u5b66\u4e60\u3002", "conclusion": "Factored Latent Action Model (FLAM) \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9014\u5f84\u6765\u6539\u8fdb\u590d\u6742\u591a\u5b9e\u4f53\u73af\u5883\u4e0b\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u4ee5\u53ca\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u6548\u679c\uff0c\u663e\u793a\u51fa\u5206\u89e3\u5f0f\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.16236", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.16236", "abs": "https://arxiv.org/abs/2602.16236", "authors": ["Matthias Frey", "Jonathan H. Manton", "Jingge Zhu"], "title": "Online Prediction of Stochastic Sequences with High Probability Regret Bounds", "comment": "Accepted for publication at The Fourteenth International Conference on Learning Representations (ICLR 2026)", "summary": "We revisit the classical problem of universal prediction of stochastic sequences with a finite time horizon $T$ known to the learner. The question we investigate is whether it is possible to derive vanishing regret bounds that hold with high probability, complementing existing bounds from the literature that hold in expectation. We propose such high-probability bounds which have a very similar form as the prior expectation bounds. For the case of universal prediction of a stochastic process over a countable alphabet, our bound states a convergence rate of $\\mathcal{O}(T^{-1/2} \u03b4^{-1/2})$ with probability as least $1-\u03b4$ compared to prior known in-expectation bounds of the order $\\mathcal{O}(T^{-1/2})$. We also propose an impossibility result which proves that it is not possible to improve the exponent of $\u03b4$ in a bound of the same form without making additional assumptions.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86\u5728\u5df2\u77e5\u6709\u9650\u65f6\u95f4\u8303\u56f4T\u7684\u60c5\u51b5\u4e0b\u968f\u673a\u5e8f\u5217\u7684\u901a\u7528\u9884\u6d4b\u7ecf\u5178\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9ad8\u6982\u7387\u4e0b\u6210\u7acb\u7684\u6d88\u5931\u540e\u6094\u754c\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u4e0d\u589e\u52a0\u989d\u5916\u5047\u8bbe\u60c5\u51b5\u4e0b\u65e0\u6cd5\u6539\u8fdb\u03b4\u7684\u6307\u6570\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5bfc\u51fa\u9ad8\u6982\u7387\u4e0b\u6210\u7acb\u7684\u6d88\u5931\u540e\u6094\u754c\uff0c\u4ee5\u8865\u5145\u73b0\u6709\u6587\u732e\u4e2d\u4ec5\u5728\u671f\u671b\u610f\u4e49\u4e0b\u6210\u7acb\u7684\u754c\u9650\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9\u968f\u673a\u8fc7\u7a0b\u7279\u522b\u662f\u53ef\u6570\u5b57\u6bcd\u8868\u4e0a\u7684\u8fc7\u7a0b\u8fdb\u884c\u5206\u6790\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684\u9ad8\u6982\u7387\u540e\u6094\u754c\u3002", "result": "\u5f97\u51fa\u4e86\u4e00\u4e2a\u6536\u655b\u7387\u5f62\u5f0f\u4e3a$\\mathcal{O}(T^{-1/2} \u03b4^{-1/2})$\u7684\u65b0\u9ad8\u6982\u7387\u754c\u9650\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5728\u76f8\u540c\u5f62\u5f0f\u7684\u754c\u9650\u5185\u4e0d\u53ef\u80fd\u6539\u8fdb$\u03b4$\u7684\u6307\u6570\u800c\u65e0\u9700\u505a\u51fa\u66f4\u591a\u5047\u8bbe\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u968f\u673a\u5e8f\u5217\u7684\u901a\u7528\u9884\u6d4b\uff0c\u5728\u7ed9\u5b9a\u7684\u65f6\u95f4\u8303\u56f4\u5185\u786e\u5b9e\u5b58\u5728\u9ad8\u6982\u7387\u4e0b\u6210\u7acb\u7684\u6d88\u5931\u540e\u6094\u754c\uff0c\u4f46\u03b4\u7684\u6307\u6570\u6539\u8fdb\u9700\u8981\u989d\u5916\u7684\u5047\u8bbe\u6761\u4ef6\u3002"}}
{"id": "2602.16264", "categories": ["cs.LG", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2602.16264", "abs": "https://arxiv.org/abs/2602.16264", "authors": ["Zixian Wu", "Xuebao Li", "Yanfang Zheng", "Rui Wang", "Shunhuang Zhang", "Jinfang Wei", "Yongshang Lv", "Liang Dong", "Zamri Zainal Abidin", "Noraisyah Mohamed Shah", "Hongwei Ye", "Pengchao Yan", "Xuefeng Li", "Xiaojia Ji", "Xusheng Huang", "Xiaotian Wang", "Honglei Jin"], "title": "Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge", "comment": "24 pages,12 figures", "summary": "In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results. (2)Transformer achieves better performance with combined LOS and vector magnetic field data than with either alone. (3)Models using knowledge-informed features outperform those using magnetograms. (4)While CNN and CNN-BiLSTM outperform their CDR counterparts on magnetograms, CDR-Transformer is slightly superior to its deep learning counterpart when using knowledge-informed features. Among all models, CDR-Transformer achieves the best performance. (5)The predictive performance of the CDR models is not overly sensitive to the reward choices.(6)Through SHAP analysis, the CDR model tends to regard TOTUSJH as more important, while the Transformer tends to prioritize R_VALUE more.(7)Under identical prediction time and active region (AR) number, the CDR-Transformer shows superior predictive capabilities compared to NASA/CCMC.", "AI": {"tldr": "This paper introduces a new supervised classification framework with class-dependent rewards for predicting solar flares, demonstrating that the CDR-Transformer model outperforms traditional deep learning models and even NASA's system.", "motivation": "To improve the accuracy and reliability of solar flare prediction, specifically for $\\geq$MM flares within a 24-hour window, by introducing a novel approach that incorporates class-dependent rewards into the predictive modeling process.", "method": "A supervised classification framework with class-dependent rewards (CDR) was developed to predict $\\geq$MM flares. Multiple datasets were constructed, and three deep learning models (CNN, CNN-BiLSTM, Transformer) alongside their CDR counterparts were applied. Performance was compared across different feature sets and reward engineering for CDR models was analyzed. SHAP method was used for model interpretability.", "result": "(1) R_VALUE and AREA_ACR are the most effective among LOS feature combinations. (2) Combined LOS and vector magnetic field data enhance the performance of the Transformer model. (3) Knowledge-informed features lead to better performance than magnetograms. (4) While CNN and CNN-BiLSTM outperform their CDR versions on magnetogram data, the CDR-Transformer excels when knowledge-informed features are used. (5) CDR models' performance is robust to changes in reward settings. (6) TOTUSJH is more significant in CDR models, whereas R_VALUE is prioritized by the Transformer. (7) CDR-Transformer surpasses NASA/CCMC in predictive capabilities under the same conditions.", "conclusion": "The CDR-Transformer model outperformed all other models, including those from NASA/CCMC, in predicting $\\geq$MM flares within 24 hours. The analysis also highlighted the importance of specific features and the benefits of using combined LOS and vector magnetic field data."}}
{"id": "2602.16274", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16274", "abs": "https://arxiv.org/abs/2602.16274", "authors": ["Rahul Singh", "Siddharth Chandak", "Eric Moulines", "Vivek S. Borkar", "Nicholas Bambos"], "title": "Regret and Sample Complexity of Online Q-Learning via Concentration of Stochastic Approximation with Time-Inhomogeneous Markov Chains", "comment": null, "summary": "We present the first high-probability regret bound for classical online Q-learning in infinite-horizon discounted Markov decision processes, without relying on optimism or bonus terms. We first analyze Boltzmann Q-learning with decaying temperature and show that its regret depends critically on the suboptimality gap of the MDP: for sufficiently large gaps, the regret is sublinear, while for small gaps it deteriorates and can approach linear growth. To address this limitation, we study a Smoothed $\u03b5_n$-Greedy exploration scheme that combines $\u03b5_n$-greedy and Boltzmann exploration, for which we prove a gap-robust regret bound of near-$\\tilde{O}(N^{9/10})$. To analyze these algorithms, we develop a high-probability concentration bound for contractive Markovian stochastic approximation with iterate- and time-dependent transition dynamics. This bound may be of independent interest as the contraction factor in our bound is governed by the mixing time and is allowed to converge to one asymptotically.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u4e3a\u65e0\u9650\u671f\u6298\u6263\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u7ecf\u5178\u5728\u7ebfQ\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6982\u7387\u9057\u61be\u754c\uff0c\u65e0\u9700\u4f9d\u8d56\u4e50\u89c2\u6027\u6216\u5956\u52b1\u9879\u3002\u901a\u8fc7\u5206\u6790\u5177\u6709\u8870\u51cf\u6e29\u5ea6\u7684\u73bb\u5c14\u5179\u66fcQ\u5b66\u4e60\uff0c\u53d1\u73b0\u5176\u9057\u61be\u503c\u4e0eMDP\u7684\u6b21\u4f18\u6027\u5dee\u8ddd\u5bc6\u5207\u76f8\u5173\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u5e73\u6ed1$\u03b5_n$-\u8d2a\u5a6a\u63a2\u7d22\u65b9\u6848\uff0c\u7ed3\u5408\u4e86$\u03b5_n$-\u8d2a\u5a6a\u548c\u73bb\u5c14\u5179\u66fc\u63a2\u7d22\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u63a5\u8fd1$\tilde{O}(N^{9/10})$\u7684\u5dee\u8ddd\u9c81\u68d2\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e50\u89c2\u6027\u6216\u5956\u52b1\u9879\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u6539\u5584\u5f53\u6b21\u4f18\u6027\u5dee\u8ddd\u8f83\u5c0f\u65f6\u5728\u7ebfQ\u5b66\u4e60\u8868\u73b0\u4e0d\u4f73\u7684\u60c5\u51b5\u3002", "method": "\u9996\u5148\u91c7\u7528\u5177\u6709\u8870\u51cf\u6e29\u5ea6\u53c2\u6570\u7684\u73bb\u5c14\u5179\u66fcQ\u5b66\u4e60\u8fdb\u884c\u5206\u6790\uff1b\u63a5\u7740\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u63a2\u7d22\u7b56\u7565\u2014\u2014\u5e73\u6ed1$\u03b5_n$-\u8d2a\u5a6a\u63a2\u7d22\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u7efc\u5408\u4e86$\u03b5_n$-\u8d2a\u5a6a\u4e0e\u73bb\u5c14\u5179\u66fc\u63a2\u7d22\u7684\u4f18\u70b9\uff1b\u6700\u540e\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9\u8fed\u4ee3\u548c\u65f6\u95f4\u76f8\u5173\u8f6c\u79fb\u52a8\u6001\u7684\u6536\u7f29\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u903c\u8fd1\u7684\u9ad8\u6982\u7387\u96c6\u4e2d\u8fb9\u754c\u7406\u8bba\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u8db3\u591f\u5927\u7684\u6b21\u4f18\u6027\u5dee\u8ddd\uff0c\u73bb\u5c14\u5179\u66fcQ\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u4e9a\u7ebf\u6027\u7684\u9057\u61be\u589e\u957f\uff1b\u800c\u5bf9\u4e8e\u8f83\u5c0f\u7684\u5dee\u8ddd\uff0c\u5219\u53ef\u80fd\u9000\u5316\u81f3\u8fd1\u4f3c\u7ebf\u6027\u589e\u957f\u3002\u65b0\u63d0\u51fa\u7684\u5e73\u6ed1$\u03b5_n$-\u8d2a\u5a6a\u63a2\u7d22\u65b9\u6848\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\u5747\u80fd\u4fdd\u8bc1\u8fd1$\tilde{O}(N^{9/10})$\u7684\u9057\u61be\u4e0a\u754c\u3002\u6b64\u5916\uff0c\u8fd8\u5f97\u5230\u4e86\u5173\u4e8e\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u903c\u8fd1\u7684\u4e00\u4e2a\u65b0\u7684\u9ad8\u6982\u7387\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u5728\u7ebfQ\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u800c\u4e14\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u6548\u5904\u7406\u4e0d\u540c\u6b21\u4f18\u6027\u5dee\u8ddd\u60c5\u51b5\u4e0b\u7684\u63a2\u7d22\u7b56\u7565\u3002"}}
{"id": "2602.16284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16284", "abs": "https://arxiv.org/abs/2602.16284", "authors": ["Adam Zweiger", "Xinghong Fu", "Han Guo", "Yoon Kim"], "title": "Fast KV Compaction via Attention Matching", "comment": null, "summary": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6ce8\u610f\u529b\u5339\u914d\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5feb\u901f\u538b\u7f29\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u952e\u503c\u7f13\u5b58\u7684\u5927\u5c0f\uff0c\u5e76\u4e14\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u65b9\u6cd5\uff0c\u5728\u538b\u7f29\u65f6\u95f4\u548c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u7684\u5e73\u8861\u3002", "motivation": "\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\uff0c\u8bed\u8a00\u6a21\u578b\u7684\u952e\u503c\u7f13\u5b58\u5927\u5c0f\u6210\u4e3a\u6269\u5c55\u74f6\u9888\u3002\u73b0\u6709\u7684\u901a\u8fc7\u6458\u8981\u5728\u6807\u8bb0\u7a7a\u95f4\u5185\u8fdb\u884c\u538b\u7f29\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5927\u91cf\u7684\u4fe1\u606f\u4e22\u5931\uff0c\u4ece\u800c\u4e25\u91cd\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u3002\u5c3d\u7ba1Cartridges\u5c55\u793a\u4e86\u5728\u6f5c\u5728\u7a7a\u95f4\u8bad\u7ec3\u7d27\u51d1\u952e\u503c\u7f13\u5b58\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5176\u7aef\u5230\u7aef\u4f18\u5316\u8fc7\u7a0b\u65e2\u6162\u53c8\u6602\u8d35\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7387\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u6ce8\u610f\u529b\u5339\u914d\u201d\u7684\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u6784\u9020\u7d27\u51d1\u7684\u952e\u548c\u503c\u4ee5\u590d\u5236\u6ce8\u610f\u529b\u8f93\u51fa\uff0c\u5e76\u5728\u6bcf\u4e2aKV\u5934\u7ea7\u522b\u4e0a\u4fdd\u7559\u6ce8\u610f\u529b\u8d28\u91cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u81ea\u7136\u5730\u5206\u89e3\u6210\u4e00\u4e9b\u7b80\u5355\u7684\u5b50\u95ee\u9898\uff0c\u5176\u4e2d\u4e00\u90e8\u5206\u5b50\u95ee\u9898\u6709\u9ad8\u6548\u7684\u95ed\u5f0f\u89e3\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u65b9\u6cd5\uff0c\u65e8\u5728\u5927\u5e45\u5ea6\u63a8\u8fdb\u538b\u7f29\u65f6\u95f4\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u9ad8\u8fbe50\u500d\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u51e0\u4e4e\u4e0d\u635f\u5931\u8d28\u91cf\uff0c\u8fd9\u6807\u5fd7\u7740\u5728\u538b\u7f29\u6548\u7387\u548c\u6548\u679c\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u5339\u914d\u6280\u672f\u4e3a\u957f\u4e0a\u4e0b\u6587\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6848\uff0c\u4e0d\u4ec5\u6781\u5927\u5730\u51cf\u5c11\u4e86\u952e\u503c\u7f13\u5b58\u7684\u5b58\u50a8\u9700\u6c42\uff0c\u8fd8\u4fdd\u8bc1\u4e86\u6a21\u578b\u6027\u80fd\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\u3002"}}
{"id": "2602.16340", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16340", "abs": "https://arxiv.org/abs/2602.16340", "authors": ["Eitan Gronich", "Gal Vardi"], "title": "The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks", "comment": "11 pages, 1 figure (with appendix: 48 pages, 2 figures), under review for ICML 2026", "summary": "We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ($\\ell_2$ norm), and Signum ($\\ell_\\infty$ norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the $\\ell_\\infty$ margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u5668\u5728\u540c\u8d28\u6a21\u578b\u4e0a\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u6269\u5c55\u4e86\u6700\u901f\u4e0b\u964d\u6cd5\u5728\u540c\u8d28\u6a21\u578b\u4e2d\u7684\u5df2\u6709\u7ed3\u679c\uff0c\u5e76\u8bc1\u660e\u4e86\u51e0\u79cd\u52a8\u91cf\u6700\u901f\u4e0b\u964d\u7b97\u6cd5\u5728\u5b66\u4e60\u7387\u8870\u51cf\u8ba1\u5212\u4e0b\u503e\u5411\u4e8eKKT\u70b9\u3002", "motivation": "\u4f5c\u8005\u60f3\u8981\u63a2\u7d22\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u5668\u5728\u540c\u8d28\u6a21\u578b\u4e2d\u6240\u8868\u73b0\u51fa\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u5e76\u7406\u89e3\u8fd9\u4e9b\u4f18\u5316\u5668\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5bf9\u4e0d\u540c\u8303\u6570\u8fb9\u7f18\u6700\u5927\u5316\u95ee\u9898\u7684\u89e3\u7684\u9009\u62e9\u3002", "method": "\u9996\u5148\u5c06\u6700\u901f\u4e0b\u964d\u6cd5\u5728\u540c\u8d28\u6a21\u578b\u4e2d\u7684\u5df2\u6709\u7ed3\u679c\u62d3\u5c55\u5230\u4e86\u5e26\u53ef\u9009\u5b66\u4e60\u7387\u8ba1\u5212\u7684\u6807\u51c6\u5316\u6700\u901f\u4e0b\u964d\u6cd5\uff1b\u7136\u540e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\uff0c\u5728\u5e73\u6ed1\u540c\u8d28\u6a21\u578b\u4e0a\uff0c\u51e0\u79cd\u52a8\u91cf\u6700\u901f\u4e0b\u964d\u7b97\u6cd5\uff08\u5982Muon\u3001MomentumGD\u548cSignum\uff09\u5728\u8870\u51cf\u5b66\u4e60\u7387\u8ba1\u5212\u4e0b\u8fd1\u4f3c\u4e8e\u6700\u901f\u4e0b\u964d\u8f68\u8ff9\uff0c\u4ece\u800c\u8868\u660e\u8fd9\u4e9b\u7b97\u6cd5\u4e5f\u504f\u5411\u4e8e\u5bf9\u5e94\u7684\u8fb9\u7f18\u6700\u5927\u5316\u95ee\u9898\u7684KKT\u70b9\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9Adam\uff08\u4e0d\u5305\u542b\u7a33\u5b9a\u6027\u5e38\u6570\uff09\u3001Muon-Signum\u548cMuon-Adam\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8bc1\u636e\u652f\u6301\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8868\u660e\u4f18\u5316\u5668\u7684\u9009\u62e9\u51b3\u5b9a\u4e86\u6700\u5927\u5316\u7684\u8fb9\u7f18\u7c7b\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6269\u5c55\u4e86\u4e4b\u524d\u5173\u4e8e\u540c\u8d28\u6a21\u578b\u4e2d\u6700\u901f\u4e0b\u964d\u4ee5\u53ca\u7ebf\u6027\u6a21\u578b\u4e2d\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u5668\u7684\u7814\u7a76\u6210\u679c\uff0c\u63ed\u793a\u4e86\u52a8\u91cf\u4f18\u5316\u65b9\u6cd5\u5bf9\u4e8e\u7279\u5b9a\u8fb9\u7f18\u6700\u5927\u5316\u95ee\u9898\u89e3\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.16341", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16341", "abs": "https://arxiv.org/abs/2602.16341", "authors": ["Georgios Gravanis", "Dimitrios Kyriakou", "Spyros Voutetakis", "Simira Papadopoulou", "Konstantinos Diamantaras"], "title": "Explainability for Fault Detection System in Chemical Processes", "comment": null, "summary": "In this work, we apply and compare two state-of-the-art eXplainability Artificial Intelligence (XAI) methods, the Integrated Gradients (IG) and the SHapley Additive exPlanations (SHAP), that explain the fault diagnosis decisions of a highly accurate Long Short-Time Memory (LSTM) classifier. The classifier is trained to detect faults in a benchmark non-linear chemical process, the Tennessee Eastman Process (TEP). It is highlighted how XAI methods can help identify the subsystem of the process where the fault occurred. Using our knowledge of the process, we note that in most cases the same features are indicated as the most important for the decision, while insome cases the SHAP method seems to be more informative and closer to the root cause of the fault. Finally, since the used XAI methods are model-agnostic, the proposed approach is not limited to the specific process and can also be used in similar problems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5e94\u7528\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u6700\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\uff0c\u5373\u7efc\u5408\u68af\u5ea6\u6cd5(IG)\u548cSHapley\u52a0\u6027\u89e3\u91ca(SHAP)\uff0c\u7528\u4e8e\u89e3\u91ca\u4e00\u4e2a\u9ad8\u5ea6\u51c6\u786e\u7684\u957f\u77ed\u671f\u8bb0\u5fc6(LSTM)\u5206\u7c7b\u5668\u5728\u7530\u7eb3\u897f\u4f0a\u58eb\u66fc\u8fc7\u7a0b(TEP)\u4e2d\u6545\u969c\u8bca\u65ad\u51b3\u7b56\u3002\u7ed3\u679c\u663e\u793a\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u5e2e\u52a9\u8bc6\u522b\u6545\u969c\u53d1\u751f\u7684\u5b50\u7cfb\u7edf\uff0c\u5e76\u4e14SHAP\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f3c\u4e4e\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u6545\u969c\u6839\u6e90\u7684\u4fe1\u606f\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5bf9\u590d\u6742\u5316\u5b66\u8fc7\u7a0b\u6545\u969c\u8bca\u65ad\u7684\u7406\u89e3\uff0c\u7279\u522b\u662f\u901a\u8fc7\u4f7f\u7528\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5982LSTM\u65f6\uff0c\u9700\u8981\u80fd\u591f\u6e05\u695a\u5730\u89e3\u91ca\u8fd9\u4e9b\u6a21\u578b\u505a\u51fa\u7279\u5b9a\u51b3\u7b56\u7684\u539f\u56e0\u3002", "method": "\u91c7\u7528\u96c6\u6210\u68af\u5ea6\u6cd5(IG)\u4e0eSHapley\u52a0\u6027\u89e3\u91ca(SHAP)\u4e24\u79cdXAI\u6280\u672f\u6765\u5206\u6790LSTM\u5206\u7c7b\u5668\u5728\u7530\u7eb3\u897f\u4f0a\u58eb\u66fc\u8fc7\u7a0b(TEP)\u4e2d\u7684\u6545\u969c\u8bca\u65ad\u51b3\u5b9a\u3002", "result": "\u53d1\u73b0XAI\u65b9\u6cd5\u786e\u5b9e\u6709\u52a9\u4e8e\u5b9a\u4f4d\u6545\u969c\u53d1\u751f\u7684\u5b50\u7cfb\u7edf\uff1b\u5bf9\u4e8e\u5927\u591a\u6570\u60c5\u51b5\u800c\u8a00\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u6307\u51fa\u4e86\u76f8\u540c\u7684\u7279\u5f81\u4f5c\u4e3a\u6700\u91cd\u8981\u7684\u51b3\u7b56\u56e0\u7d20\uff1b\u4f46\u5728\u67d0\u4e9b\u60c5\u5f62\u4e0b\uff0cSHAP\u65b9\u6cd5\u8868\u73b0\u5f97\u66f4\u4e3a\u4fe1\u606f\u4e30\u5bcc\uff0c\u66f4\u63a5\u8fd1\u4e8e\u6545\u969c\u7684\u6839\u672c\u539f\u56e0\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0cXAI\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u8f85\u52a9\u7406\u89e3\u57fa\u4e8eLSTM\u7b49\u590d\u6742\u6a21\u578b\u8fdb\u884c\u6545\u969c\u8bca\u65ad\u80cc\u540e\u7684\u539f\u56e0\uff0c\u800c\u4e14\u7531\u4e8e\u8fd9\u4e9bXAI\u6280\u672f\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u56e0\u6b64\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.16357", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.16357", "abs": "https://arxiv.org/abs/2602.16357", "authors": ["Sarkis Ter Martirosyan", "Xinyue Huang", "David Qin", "Anthony Yu", "Stanislav Emelianov"], "title": "Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks", "comment": null, "summary": "Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \\textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5SPOI-AE\uff0c\u7528\u4e8e\u89e3\u51b3\u5149\u8c31\u5149\u58f0\u6210\u50cf\u4e2d\u7684\u5149\u5b66\u53cd\u6f14\u548c\u5149\u8c31\u5206\u89e3\u95ee\u9898\uff0c\u65e0\u9700\u5047\u8bbe\u7ebf\u6027\u5173\u7cfb\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8f93\u5165\u56fe\u50cf\u50cf\u7d20\u53ca\u4f30\u8ba1\u751f\u7269\u53c2\u6570\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5c0f\u9f20\u6dcb\u5df4\u7ed3\u7684\u9a8c\u8bc1\u4e86\u5176\u5206\u89e3\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u5149\u8c31\u5149\u58f0(sPA)\u6210\u50cf\u4e2d\u5b58\u5728\u975e\u7ebf\u6027\u548c\u4e0d\u9002\u5b9a\u6027\u95ee\u9898\uff0c\u4f7f\u5f97\u51c6\u786e\u4f30\u8ba1\u8272\u56e2\u76f8\u5bf9\u6d53\u5ea6\u53d8\u5f97\u975e\u5e38\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8sPA\u6210\u50cf\u4e2d\u8272\u56e2\u6d53\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86SPOI-AE\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSPOI-AE\uff08\u5149\u8c31\u5149\u58f0\u5149\u5b66\u53cd\u6f14\u81ea\u52a8\u7f16\u7801\u5668\uff09\u7684\u65b0\u6a21\u578b\uff0c\u65e8\u5728\u4e0d\u4f9d\u8d56\u7ebf\u6027\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u5904\u7406sPA\u5149\u5b66\u53cd\u6f14\u4e0e\u5149\u8c31\u5206\u89e3\u95ee\u9898\u3002SPOI-AE\u5229\u7528\u6d3b\u4f53\u5c0f\u9f20\u6dcb\u5df4\u7ed3\u7684sPA\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u56fe\u50cf\u6ca1\u6709\u5df2\u77e5\u7684\u771f\u5b9e\u8272\u56e2\u6d53\u5ea6\u6570\u636e\u4f5c\u4e3a\u53c2\u8003\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff0cSPOI-AE\u80fd\u591f\u66f4\u597d\u5730\u91cd\u5efa\u8f93\u5165sPA\u56fe\u50cf\u50cf\u7d20\uff0c\u5e76\u63d0\u4f9b\u66f4\u4e3a\u5408\u7406\u7684\u751f\u7269\u5b66\u53c2\u6570\u4f30\u8ba1\u503c\uff0c\u5305\u62ec\u5149\u5b66\u53c2\u6570\u3001\u8272\u56e2\u6d53\u5ea6\u4ee5\u53ca\u7ec4\u7ec7\u6c27\u9971\u548c\u5ea6\u767e\u5206\u6bd4\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u6a21\u62df\u7684\u5c0f\u9f20\u6dcb\u5df4\u7ed3\u6765\u9a8c\u8bc1\u4e86SPOI-AE\u5728\u5149\u8c31\u5206\u89e3\u4e0a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SPOI-AE\u5c55\u793a\u4e86\u5728\u5149\u8c31\u5149\u58f0\u6210\u50cf\u9886\u57df\u5185\u89e3\u51b3\u5149\u5b66\u53cd\u6f14\u548c\u5149\u8c31\u5206\u89e3\u6311\u6218\u7684\u6709\u6548\u6027\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u751f\u7269\u4fe1\u606f\u4f30\u8ba1\uff0c\u4e3a\u7406\u89e3\u751f\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u7ed3\u6784\u3001\u529f\u80fd\u53ca\u5206\u5b50\u5c42\u9762\u7684\u4fe1\u606f\u3002"}}
{"id": "2602.16400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16400", "abs": "https://arxiv.org/abs/2602.16400", "authors": ["Roy Rinberg", "Pol Puigdemont", "Martin Pawelczyk", "Volkan Cevher"], "title": "Easy Data Unlearning Bench", "comment": "ICML 2025 Workshop on Machine Unlearning for Generative AI", "summary": "Evaluating machine unlearning methods remains technically challenging, with recent benchmarks requiring complex setups and significant engineering overhead. We introduce a unified and extensible benchmarking suite that simplifies the evaluation of unlearning algorithms using the KLoM (KL divergence of Margins) metric. Our framework provides precomputed model ensembles, oracle outputs, and streamlined infrastructure for running evaluations out of the box. By standardizing setup and metrics, it enables reproducible, scalable, and fair comparison across unlearning methods. We aim for this benchmark to serve as a practical foundation for accelerating research and promoting best practices in machine unlearning. Our code and data are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7b80\u5316\u4e86\u4f7f\u7528KLoM\uff08\u8fb9\u7f18\u7684KL\u6563\u5ea6\uff09\u6307\u6807\u5bf9\u9057\u5fd8\u7b97\u6cd5\u7684\u8bc4\u4f30\u3002\u901a\u8fc7\u6807\u51c6\u5316\u8bbe\u7f6e\u548c\u5ea6\u91cf\u6807\u51c6\uff0c\u8be5\u6846\u67b6\u4fc3\u8fdb\u4e86\u4e0d\u540c\u9057\u5fd8\u65b9\u6cd5\u4e4b\u95f4\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u548c\u516c\u5e73\u7684\u6bd4\u8f83\uff0c\u5e76\u4e3a\u52a0\u901f\u7814\u7a76\u548c\u63a8\u5e7f\u673a\u5668\u9057\u5fd8\u7684\u6700\u4f73\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "motivation": "\u8bc4\u4f30\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5728\u6280\u672f\u4e0a\u5177\u6709\u6311\u6218\u6027\uff0c\u6700\u8fd1\u7684\u57fa\u51c6\u6d4b\u8bd5\u9700\u8981\u590d\u6742\u7684\u8bbe\u7f6e\u548c\u5927\u91cf\u7684\u5de5\u7a0b\u5f00\u9500\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u96be\u9898\uff0c\u4f5c\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u5305\uff0c\u65e8\u5728\u7b80\u5316\u8bc4\u4ef7\u8fc7\u7a0b\u5e76\u4fc3\u8fdb\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5b83\u5229\u7528KLoM\uff08\u8fb9\u9645\u4e0a\u7684KL\u6563\u5ea6\uff09\u5ea6\u91cf\u6765\u7b80\u5316\u5bf9\u9057\u5fd8\u7b97\u6cd5\u7684\u8bc4\u4f30\u3002\u6b64\u5957\u4ef6\u63d0\u4f9b\u4e86\u9884\u8ba1\u7b97\u6a21\u578b\u96c6\u6210\u3001oracle\u8f93\u51fa\u4ee5\u53ca\u7b80\u5316\u7684\u57fa\u7840\u8bbe\u65bd\u4ee5\u5b9e\u73b0\u5373\u63d2\u5373\u7528\u5f0f\u7684\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u8be5\u5de5\u4f5c\u80fd\u591f\u652f\u6301\u4e0d\u540c\u9057\u5fd8\u65b9\u6cd5\u4e4b\u95f4\u7684\u53ef\u91cd\u590d\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u516c\u5e73\u5bf9\u6bd4\u3002\u6b64\u5916\uff0c\u516c\u5f00\u53ef\u7528\u7684\u4ee3\u7801\u4e0e\u6570\u636e\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u7b80\u5316\u7684\u3001\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u52a0\u901f\u673a\u5668\u9057\u5fd8\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u9f13\u52b1\u6700\u4f73\u5b9e\u8df5\u7684\u5e94\u7528\u3002"}}
{"id": "2602.16436", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16436", "abs": "https://arxiv.org/abs/2602.16436", "authors": ["Jean Dufraiche", "Paul Mangold", "Micha\u00ebl Perrot", "Marc Tommasi"], "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent", "comment": "30 pages, 8 figures", "summary": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWeierstrass\u53d8\u6362\u7684\u65b9\u6cd5\u6765\u7ea0\u6b63\u5c40\u90e8\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u4e0b\u53d1\u5e03\u7684\u6570\u636e\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u504f\u5dee\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aIWP-SGD\u7684\u65b0\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u4ee5O(1/n)\u7684\u901f\u7387\u6536\u655b\u5230\u771f\u5b9e\u7684\u98ce\u9669\u6700\u5c0f\u5316\u3002", "motivation": "\u4e00\u6b21\u6027\u91ca\u653e\u6570\u636e\u65f6\u91c7\u7528\u975e\u4ea4\u4e92\u5f0f\u672c\u5730\u5dee\u5206\u9690\u79c1(LDP)\u867d\u7136\u80fd\u591f\u5b8c\u5168\u91cd\u7528\u6570\u636e\uff0c\u4f46\u7531\u6b64\u4ea7\u751f\u7684\u566a\u58f0\u53ef\u80fd\u7ed9\u540e\u7eed\u5206\u6790\u5e26\u6765\u504f\u5dee\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528Weierstrass\u53d8\u6362\u6765\u63cf\u8ff0\u8fd9\u79cd\u504f\u5dee\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u4fee\u6b63\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u9996\u5148\u8fd0\u7528Weierstrass\u53d8\u6362\u6765\u8868\u5f81\u5728\u4e8c\u5206\u7c7b\u60c5\u51b5\u4e0b\u7531LDP\u5f15\u5165\u7684\u6570\u636e\u504f\u5dee\uff1b\u63a5\u7740\u8bc1\u660e\u4e86\u901a\u8fc7\u5bf9\u8be5\u53d8\u6362\u6c42\u9006\u53ef\u4ee5\u83b7\u5f97\u4e00\u79cd\u504f\u8bef\u6821\u6b63\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u5bf9\u975e\u7ebf\u6027\u51fd\u6570\u65e0\u504f\u4f30\u8ba1\uff1b\u6700\u540e\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u2014\u2014Inverse Weierstrass Private SGD (IWP-SGD)\uff0c\u5176\u8bbe\u8ba1\u76ee\u7684\u5728\u4e8e\u4f7f\u7b97\u6cd5\u80fd\u591f\u4ee5\\(\\mathcal{O}(1/n)\\)\u7684\u901f\u5ea6\u6536\u655b\u81f3\u771f\u5b9e\u7684\u603b\u4f53\u98ce\u9669\u6700\u5c0f\u503c\u70b9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684IWP-SGD\u7b97\u6cd5\u5728\u5408\u6210\u53ca\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u56e0\u4f7f\u7528LDP\u4fdd\u62a4\u673a\u5236\u800c\u4ea7\u751f\u7684\u6570\u636e\u504f\u5dee\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6570\u5b66\u624b\u6bb5\uff08\u5982Weierstrass\u53d8\u6362\u53ca\u5176\u9006\u53d8\u6362\uff09\u6765\u51cf\u8f7b\u751a\u81f3\u6d88\u9664\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u6280\u672f\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4e3a\u5904\u7406\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u5206\u6790\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.16438", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16438", "abs": "https://arxiv.org/abs/2602.16438", "authors": ["Eva Paraschou", "Line Harder Clemmensen", "Sneha Das"], "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment", "comment": "Submitted to the BiAlign CHI Workshop 2026", "summary": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u9488\u5bf9\u6027\u522b\u7684\u8c03\u6574\u5982\u4f55\u5f71\u54cd\u4e09\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff08Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B\uff09\u5728\u4e5d\u4e2a\u654f\u611f\u5c5e\u6027\u4e0a\u7684\u516c\u5e73\u6027\u3002\u5c3d\u7ba1\u603b\u4f53\u4e0a\u6709\u6240\u6539\u5584\uff0c\u4f46\u5728\u6a21\u7cca\u60c5\u5883\u4e0b\uff0c\u5bf9\u4e8e\u5916\u8c8c\u3001\u6027\u53d6\u5411\u548c\u6b8b\u75be\u72b6\u6001\u7b49\u5c5e\u6027\u7684\u504f\u89c1\u5b9e\u9645\u4e0a\u52a0\u5267\u4e86\uff0c\u8fd9\u8868\u660e\u9700\u8981\u91c7\u7528\u591a\u5c5e\u6027\u3001\u60c5\u5883\u611f\u77e5\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u6846\u67b6\u6765\u9632\u6b62\u504f\u89c1\u6ea2\u51fa\u6548\u5e94\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u516c\u5e73\u6027\u6821\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u51cf\u5c11\u5355\u4e00\u654f\u611f\u5c5e\u6027\u4e0a\u7684\u504f\u5dee\uff0c\u5ffd\u89c6\u4e86\u516c\u5e73\u6027\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u591a\u7ef4\u5ea6\u4e14\u7279\u5b9a\u4e8e\u4e0a\u4e0b\u6587\u7684\u4ef7\u503c\u89c2\u3002\u8fd9\u79cd\u505a\u6cd5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u5728\u5b9e\u73b0\u72ed\u9698\u7684\u516c\u5e73\u6307\u6807\u7684\u540c\u65f6\uff0c\u5728\u672a\u88ab\u9488\u5bf9\u6027\u5904\u7406\u7684\u5c5e\u6027\u4e0a\u52a0\u5267\u5dee\u5f02\uff0c\u5373\u6240\u8c13\u7684\u504f\u89c1\u6ea2\u51fa\u73b0\u8c61\u3002", "method": "\u7814\u7a76\u8005\u4eec\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6280\u672f\u4ee5\u53caBBQ\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B\uff09\u5728\u660e\u786e\u4e0e\u4e0d\u660e\u786e\u7684\u60c5\u5883\u4e0b\u7684\u516c\u5e73\u6027\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u660e\u663e\u7684\u504f\u89c1\u6ea2\u51fa\u73b0\u8c61\uff1a\u867d\u7136\u6574\u4f53\u7ed3\u679c\u663e\u793a\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u57fa\u4e8e\u5177\u4f53\u60c5\u5883\u7684\u5206\u6790\u63ed\u793a\u51fa\u5728\u6a21\u7cca\u60c5\u5883\u4e2d\uff0c\u7279\u522b\u662f\u5728\u5916\u8c8c\u3001\u6027\u53d6\u5411\u53ca\u6b8b\u75be\u72b6\u51b5\u65b9\u9762\u5b58\u5728\u7740\u663e\u8457\u6076\u5316\u7684\u60c5\u51b5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u63d0\u9ad8\u4e00\u4e2a\u5c5e\u6027\u4e0a\u7684\u516c\u5e73\u6027\u53ef\u80fd\u4f1a\u65e0\u610f\u95f4\u6076\u5316\u5176\u4ed6\u5c5e\u6027\u4e0a\u7684\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u3002\u56e0\u6b64\uff0c\u5f3a\u8c03\u4e86\u91c7\u7528\u80fd\u591f\u8003\u8651\u591a\u79cd\u5c5e\u6027\u5e76\u5177\u6709\u60c5\u5883\u610f\u8bc6\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u6846\u67b6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.16449", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16449", "abs": "https://arxiv.org/abs/2602.16449", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation", "comment": null, "summary": "Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGICDM\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ea0\u6b63\u771f\u5b9e\u548c\u751f\u6210\u6570\u636e\u7684\u90bb\u57df\u4f30\u8ba1\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u7531\u4e8ehubness\u73b0\u8c61\u5bfc\u81f4\u7684\u6700\u8fd1\u90bb\u5173\u7cfb\u626d\u66f2\u548c\u57fa\u4e8e\u8ddd\u79bb\u5ea6\u91cf\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u4e8e\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u6765\u8ba1\u7b97\u6837\u672c\u95f4\u7684\u8ddd\u79bb\uff0c\u4f46\u8fd9\u4e9b\u7a7a\u95f4\u4e2d\u7684\u6570\u636e\u8868\u793a\u53d7\u5230hubness\u73b0\u8c61\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6700\u8fd1\u90bb\u5173\u7cfb\u5931\u771f\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u5ea6\u91cf\u504f\u659c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u57fa\u4e8e\u7ecf\u5178\u7684\u8fed\u4ee3\u4e0a\u4e0b\u6587\u5dee\u5f02\u5ea6\u91cf\uff08ICDM\uff09\u5f00\u53d1\u4e86\u751f\u6210\u5f0fICDM\uff08GICDM\uff09\uff0c\u4e13\u95e8\u9488\u5bf9\u771f\u5b9e\u548c\u751f\u6210\u7684\u6570\u636e\u4fee\u6b63\u90bb\u57df\u4f30\u8ba1\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u5c3a\u5ea6\u6269\u5c55\u4ee5\u6539\u5584\u5b9e\u9645\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGICDM\u80fd\u591f\u89e3\u51b3\u7531hubness\u5f15\u8d77\u7684\u5931\u8d25\u60c5\u51b5\uff0c\u6062\u590d\u53ef\u9760\u7684\u5ea6\u91cf\u884c\u4e3a\uff0c\u5e76\u4e14\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "GICDM\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u5b83\u80fd\u591f\u7ea0\u6b63\u7531\u4e8ehubness\u73b0\u8c61\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u4ece\u800c\u6539\u8fdb\u4e86\u751f\u6210\u6a21\u578b\u8bc4\u4ef7\u4e2d\u57fa\u4e8e\u8ddd\u79bb\u5ea6\u91cf\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u66f4\u7b26\u5408\u4eba\u7c7b\u7684\u76f4\u89c2\u5224\u65ad\u3002"}}
{"id": "2602.16456", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16456", "abs": "https://arxiv.org/abs/2602.16456", "authors": ["Abdulla Jasem Almansoori", "Maria Ivanova", "Andrey Veprikov", "Aleksandr Beznosikov", "Samuel Horv\u00e1th", "Martin Tak\u00e1\u010d"], "title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC", "comment": "20 pages, 5 figures, 4 tables", "summary": "Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86LoRSum\uff0c\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u5b50\u7a0b\u5e8f\uff0c\u7528\u4e8e\u901a\u8fc7\u5c06LoRA\u4f18\u5316\u8f6c\u6362\u4e3a\u8fd1\u7aef\u5b50\u95ee\u9898\uff0c\u5e76\u5229\u7528\u4ea4\u66ff\u6700\u5c0f\u4e8c\u4e58\u66f4\u65b0\u6709\u6548\u5730\u89e3\u51b3\u5b83\uff0c\u4ece\u800c\u5f25\u5408\u4f7f\u7528\u4f4e\u79e9\u6295\u5f71\u7684\u5168\u6b65\u9aa4\u8bad\u7ec3\uff08SVDLoRA\uff09\u4e0eLoRA\u5fae\u8c03\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7f29\u653e\u53d8\u4f53\uff0c\u53ef\u4ee5\u4f7f\u7528\u7ed3\u6784\u5316\u5ea6\u91cf\u5982K-FAC\u548cShampoo\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1LoRA\u98ce\u683c\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5339\u914d\u6216\u6539\u8fdbLoRA\u57fa\u7ebf\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f7f\u7528\u4f4e\u79e9\u6295\u5f71\u8fdb\u884c\u5168\u6b65\u9aa4\u8bad\u7ec3\uff08SVDLoRA\uff09\u4e0eLoRA\u5fae\u8c03\u4e4b\u95f4\u5b58\u5728\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8\u5927\u6a21\u578b\u5fae\u8c03\u65f6\u7684\u5185\u5b58\u6548\u7387\u53ca\u53c2\u6570\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoRSum\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06LoRA\u4f18\u5316\u89c6\u4e3a\u4e00\u4e2a\u8fd1\u7aef\u5b50\u95ee\u9898\u5e76\u901a\u8fc7\u4ea4\u66ff\u6700\u5c0f\u4e8c\u4e58\u66f4\u65b0\u6765\u9ad8\u6548\u89e3\u51b3\uff1b\u53e6\u5916\uff0c\u4e3a\u4e86\u5904\u7406\u5e26\u6709\u9884\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u7684\u5168\u6b65\u9aa4\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86LoRSum\u7684\u4e00\u79cd\u7f29\u653e\u7248\u672c\uff0c\u80fd\u591f\u5229\u7528\u50cfK-FAC\u548cShampoo\u8fd9\u6837\u7684\u7ed3\u6784\u5316\u5ea6\u91cf\uff0c\u540c\u65f6\u4ec5\u5b58\u50a8\u8fd9\u4e9b\u5ea6\u91cf\u7684\u5bf9\u89d2\u7ebf\u4ee5\u7ef4\u6301\u5185\u5b58\u6548\u7387\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u3001CIFAR-100\u6570\u636e\u96c6\u4ee5\u53caGLUE\u3001SQuAD v2\u548cWikiText-103\u4e0a\u7684\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u5339\u914d\u751a\u81f3\u4f18\u4e8eLoRA\u57fa\u7ebf\uff0c\u5728\u9002\u5ea6\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u907f\u514d\u4e86\u5168\u77e9\u9635SVD\u6295\u5f71\u5e76\u4fdd\u6301\u4e86LoRA\u5f0f\u7684\u53c2\u6570\u6548\u7387\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684LoRSum\u53ca\u5176\u7f29\u653e\u7248\u672c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u9014\u5f84\u6765\u8fdb\u884c\u5927\u578b\u6a21\u578b\u7684LoRA\u5fae\u8c03\uff0c\u65e2\u63d0\u9ad8\u4e86\u5185\u5b58\u5229\u7528\u7387\u53c8\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.16468", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16468", "abs": "https://arxiv.org/abs/2602.16468", "authors": ["Jung Min Choi", "Vijaya Krishna Yalavarthi", "Lars Schmidt-Thieme"], "title": "HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting", "comment": "18 pages, 5 figures, 5 tables, PAKDD 2026", "summary": "In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHierarchical Patching Mixer (HPMixer)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u957f\u671f\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u4f46\u4e92\u8865\u7684\u65b9\u5f0f\u5efa\u6a21\u5468\u671f\u6027\u548c\u6b8b\u5dee\u52a8\u6001\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u957f\u671f\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u6709\u6548\u6355\u6349\u5468\u671f\u6a21\u5f0f\u548c\u6b8b\u5dee\u52a8\u6001\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002\u4e3a\u4e86\u5728\u8fd9\u4e2a\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u8bbe\u5b9a\u5185\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86Hierarchical Patching Mixer (HPimizer)\uff0c\u5b83\u4ee5\u4e00\u79cd\u65e2\u5206\u79bb\u53c8\u4e92\u8865\u7684\u65b9\u5f0f\u5904\u7406\u5468\u671f\u6027\u548c\u6b8b\u5dee\u3002\u5468\u671f\u90e8\u5206\u4f7f\u7528\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5faa\u73af\u6a21\u5757\u52a0\u4e0a\u975e\u7ebf\u6027\u901a\u9053\u7ea7MLP\u6765\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff1b\u6b8b\u5dee\u90e8\u5206\u5219\u901a\u8fc7\u53ef\u5b66\u4e60\u9759\u6001\u5c0f\u6ce2\u53d8\u6362\uff08LSWT\uff09\u63d0\u53d6\u7a33\u5b9a\u3001\u79fb\u4f4d\u4e0d\u53d8\u7684\u9891\u57df\u8868\u793a\u3002\u63a5\u7740\uff0c\u91c7\u7528\u901a\u9053\u6df7\u5408\u7f16\u7801\u5668\u6765\u5efa\u7acb\u660e\u786e\u7684\u8de8\u901a\u9053\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u4e24\u7ea7\u975e\u91cd\u53e0\u5c42\u6b21\u4fee\u8865\u673a\u5236\u6355\u6349\u7c97\u7ec6\u5c3a\u5ea6\u4e0a\u7684\u6b8b\u5dee\u53d8\u5316\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6807\u51c6\u591a\u53d8\u91cf\u57fa\u51c6\u4e0a\uff0cHPMixer\u76f8\u8f83\u4e8e\u6700\u8fd1\u7684\u57fa\u7ebf\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u5177\u6709\u7ade\u4e89\u529b\u6216\u8005\u662f\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u89e3\u8026\u7684\u5468\u671f\u6027\u5efa\u6a21\u4e0e\u7ed3\u6784\u5316\u7684\u591a\u5c3a\u5ea6\u6b8b\u5dee\u5b66\u4e60\uff0cHPMixer\u4e3a\u957f\u671f\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2602.16498", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16498", "abs": "https://arxiv.org/abs/2602.16498", "authors": ["Xinyi Shang", "Peng Sun", "Jingyu Lin", "Zhiqiang Shen"], "title": "Fast and Scalable Analytical Diffusion", "comment": null, "summary": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.", "AI": {"tldr": "The paper introduces GoldDiff, a method that tackles the scalability issue of analytical diffusion models by dynamically identifying a 'Golden Subset' for inference, significantly reducing computational requirements while maintaining or improving performance. This approach enables the first successful application of analytical diffusion to large-scale datasets like ImageNet-1K.", "motivation": "The primary motivation is to address the scalability bottleneck of analytical diffusion models, which typically require a full-dataset scan at each timestep, making them computationally expensive as the dataset size increases. The authors aim to reduce this dependency without sacrificing model performance, thereby enabling the use of such models on larger datasets.", "method": "GoldDiff utilizes the observation of Posterior Progressive Concentration, where the effective support for denoising score shrinks over time, to implement a dynamic, coarse-to-fine mechanism for selecting a 'Golden Subset' of data relevant for inference. This process decouples the inference complexity from the overall dataset size, allowing for more efficient computation.", "result": "Empirical results show that GoldDiff can achieve up to a 71x speedup on the AFHQ dataset compared to full-scan methods, with equivalent or better performance. Notably, it also successfully scales analytical diffusion models to the ImageNet-1K dataset, demonstrating its potential for large-scale generative modeling applications.", "conclusion": "The study concludes that by leveraging the principle of Posterior Progressive Concentration and implementing a dynamic subset selection strategy, GoldDiff provides an efficient, training-free solution for scaling analytical diffusion models to large datasets, opening new avenues for practical, high-performance generative modeling."}}
{"id": "2602.16503", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16503", "abs": "https://arxiv.org/abs/2602.16503", "authors": ["Vasilis Gkolemis", "Loukas Kavouras", "Dimitrios Kyriakopoulos", "Konstantinos Tsopelas", "Dimitrios Rontogiannis", "Giuseppe Casalicchio", "Theodore Dalamagas", "Christos Diou"], "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects", "comment": null, "summary": "Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7c7b\u2014\u2014\u6761\u4ef6\u6027\u52a0\u6cd5\u5c40\u90e8\u6a21\u578b\uff08CALMs\uff09\uff0c\u5b83\u5728\u4fdd\u6301GAMs\u7684\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5141\u8bb8\u6bcf\u4e2a\u7279\u5f81\u6709\u591a\u4e2a\u5355\u53d8\u91cf\u5f62\u72b6\u51fd\u6570\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u8fd9\u4e9b\u51fd\u6570\u5728\u8f93\u5165\u7a7a\u95f4\u7684\u4e0d\u540c\u533a\u57df\u6d3b\u8dc3\u3002\u5b9e\u9a8c\u8868\u660eCALMs\u5728\u591a\u6837\u5316\u7684\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e0a\u59cb\u7ec8\u4f18\u4e8eGAMs\uff0c\u5e76\u4e14\u4e0eGA$^2$Ms\u76f8\u6bd4\u5177\u6709\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5e7f\u4e49\u76f8\u52a0\u6a21\u578b\uff08GAMs\uff09\u63d0\u4f9b\u4e86\u89e3\u91ca\u6027\u4f46\u5f53\u6570\u636e\u4e2d\u5b58\u5728\u4ea4\u4e92\u4f5c\u7528\u65f6\u4f1a\u6b20\u62df\u5408\uff1bGA$^2$Ms\u901a\u8fc7\u6dfb\u52a0\u9009\u5b9a\u7684\u6210\u5bf9\u4ea4\u4e92\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u727a\u7272\u4e86\u53ef\u89e3\u91ca\u6027\u5e76\u9650\u5236\u4e86\u6a21\u578b\u5ba1\u8ba1\u3002", "method": "\u5f15\u5165\u4e86\u6761\u4ef6\u6027\u52a0\u6cd5\u5c40\u90e8\u6a21\u578b\uff08CALMs\uff09\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u6bcf\u4e2a\u7279\u5f81\u62e5\u6709\u591a\u4e2a\u5355\u53d8\u91cf\u5f62\u72b6\u51fd\u6570\uff0c\u6bcf\u4e2a\u51fd\u6570\u5728\u8f93\u5165\u7a7a\u95f4\u7684\u4e0d\u540c\u533a\u57df\u5185\u6d3b\u8dc3\u3002\u8fd9\u4e9b\u533a\u57df\u662f\u6839\u636e\u7279\u5f81\u95f4\u4ea4\u4e92\u5b9a\u4e49\u7684\u7b80\u5355\u903b\u8f91\u6761\u4ef6\u72ec\u7acb\u786e\u5b9a\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u63d0\u70bc\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bc6\u522b\u6709\u9650\u4ea4\u4e92\u7684\u540c\u8d28\u533a\u57df\u5e76\u901a\u8fc7\u533a\u57df\u611f\u77e5\u53cd\u5411\u62df\u5408\u65b9\u6cd5\u6765\u62df\u5408\u53ef\u89e3\u91ca\u7684\u5f62\u72b6\u51fd\u6570\u3002", "result": "\u5728\u591a\u6837\u7684\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCALMs\u4e0d\u4ec5\u4e00\u81f4\u5730\u8d85\u8d8a\u4e86GAMs\u7684\u8868\u73b0\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4e5f\u8fbe\u5230\u4e86\u4e0eGA$^2$Ms\u76f8\u5ab2\u7f8e\u7684\u6c34\u5e73\u3002", "conclusion": "\u603b\u7684\u6765\u8bf4\uff0cCALMs\u4e3a\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5438\u5f15\u4eba\u7684\u6743\u8861\u3002"}}
{"id": "2602.16507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16507", "abs": "https://arxiv.org/abs/2602.16507", "authors": ["Gaetan De Waele", "Marek Wydmuch", "Krzysztof Dembczy\u0144ski", "Wojciech Kot\u0142owski", "Willem Waegeman"], "title": "Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?", "comment": null, "summary": "One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u6db2\u76f8\u8272\u8c31-\u4e32\u8054\u8d28\u8c31(LC-MS/MS)\u6570\u636e\u4ee5\u8bc6\u522b\u5316\u5408\u7269\u65f6\uff0c\u4e0d\u540c\u635f\u5931\u51fd\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u4f18\u5316\u6307\u7eb9\u76f8\u4f3c\u5ea6\u548c\u5206\u5b50\u68c0\u7d22\u4e4b\u95f4\u5b58\u5728\u4e00\u4e2a\u57fa\u672c\u7684\u6743\u8861\uff1a\u63d0\u9ad8\u6307\u7eb9\u9884\u6d4b\u51c6\u786e\u6027\u901a\u5e38\u4f1a\u6076\u5316\u68c0\u7d22\u7ed3\u679c\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002", "motivation": "\u5f53\u524d\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u5206\u6790LC-MS/MS\u6570\u636e\u4e2d\u7684\u5316\u5408\u7269\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8bad\u7ec3\u8fd9\u4e9b\u9884\u6d4b\u6a21\u578b\u65f6\u6240\u4f7f\u7528\u7684\u5404\u79cd\u635f\u5931\u51fd\u6570\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u8fd9\u4e00\u70b9\u4e0a\uff0c\u4ecd\u7f3a\u4e4f\u8db3\u591f\u7684\u4e86\u89e3\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63a8\u5bfc\u51fa\u65b0\u7684\u9057\u61be\u754c\uff08regret bounds\uff09\uff0c\u6765\u63cf\u8ff0\u5f53\u8fd9\u4e9b\u76ee\u6807\u8fbe\u5230\u8d1d\u53f6\u65af\u6700\u4f18\u51b3\u7b56\u65f6\u5fc5\u987b\u51fa\u73b0\u5dee\u5f02\u7684\u60c5\u51b5\u3002\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u6307\u7eb9\u76f8\u4f3c\u6027\u548c\u5206\u5b50\u68c0\u7d22\u4e24\u4e2a\u76ee\u6807\u4e4b\u95f4\u7684\u6839\u672c\u6027\u6743\u8861\uff0c\u5e76\u4e14\u8fd9\u79cd\u6743\u8861\u53d6\u51b3\u4e8e\u5019\u9009\u96c6\u7684\u76f8\u4f3c\u7ed3\u6784\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4f18\u5316\u6307\u7eb9\u9884\u6d4b\u7cbe\u5ea6\u5f80\u5f80\u4f1a\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u53d8\u5dee\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u5173\u4e8e\u635f\u5931\u51fd\u6570\u9009\u62e9\u4ee5\u53ca\u6307\u7eb9\u9009\u62e9\u65b9\u9762\u7684\u6307\u5bfc\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u7528\u4e8e\u5316\u5408\u7269\u8bc6\u522b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\uff0c\u56e0\u4e3a\u5b83\u4eec\u5bf9\u6700\u7ec8\u6a21\u578b\u5728\u6307\u7eb9\u76f8\u4f3c\u6027\u548c\u5206\u5b50\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2602.16531", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16531", "abs": "https://arxiv.org/abs/2602.16531", "authors": ["Daniel Boharon", "Yehuda Dar"], "title": "Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing", "comment": null, "summary": "We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.\n  We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.\n  Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4f7f\u7528\u591a\u4e2a\u53ef\u80fd\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u65b9\u8bef\u5dee\u5e76\u60e9\u7f5a\u6240\u5b66\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u5b9a\u4e49\u76ee\u6807\u5b66\u4e60\u4efb\u52a1\u3002\u6211\u4eec\u5206\u6790\u4e86\u5229\u7528\u66f4\u591a\u9884\u8bad\u7ec3\u6a21\u578b\u6539\u5584\u8fc1\u79fb\u5b66\u4e60\u7684\u60c5\u51b5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u8fc7\u5ea6\u53c2\u6570\u5316\u504f\u5dee\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5229\u7528\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u5b66\u4e60\u76ee\u6807\u9884\u6d4b\u5668\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u6539\u8fdb\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\uff0c\u7279\u522b\u662f\u5f53\u8fd9\u4e9b\u6a21\u578b\u662f\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u65f6\u5019\u3002", "method": "\u91c7\u7528\u7684\u65b9\u6cd5\u5305\u62ec\u5c06\u76ee\u6807\u5b66\u4e60\u4efb\u52a1\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u5305\u542b\u5bf9\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u5e73\u65b9\u8bef\u5dee\u7684\u6700\u5c0f\u5316\u4ee5\u53ca\u5bf9\u6240\u5b66\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u5dee\u5f02\u7684\u4e00\u79cd\u60e9\u7f5a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e58\u6cd5\u4fee\u6b63\u56e0\u5b50\u7684\u7b80\u5355\u53bb\u504f\u6280\u672f\uff0c\u65e8\u5728\u51cf\u8f7b\u7531\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u8fc7\u5ea6\u53c2\u6570\u5316\u6240\u5e26\u6765\u7684\u504f\u5dee\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e3a\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u9002\u5f53\u589e\u52a0\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6570\u91cf\u5bf9\u4e8e\u5b9e\u73b0\u6709\u76ca\u7684\u8fc1\u79fb\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u505a\u6cd5\u4e5f\u53ef\u80fd\u53d7\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u8fc7\u5ea6\u53c2\u6570\u5316\u504f\u5dee\u7684\u5f71\u54cd\u3002\u63d0\u51fa\u7684\u53bb\u504f\u6280\u672f\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8fd9\u79cd\u504f\u5dee\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u867d\u7136\u4f7f\u7528\u66f4\u591a\u7684\u8fc7\u5ea6\u53c2\u6570\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u589e\u5f3a\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u679c\uff0c\u4f46\u540c\u65f6\u4e5f\u9700\u8981\u6ce8\u610f\u5e76\u5904\u7406\u7531\u8fc7\u5ea6\u53c2\u6570\u5316\u5f15\u8d77\u7684\u504f\u5dee\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u7b80\u6613\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u5b83\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u5730\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u8fdb\u884c\u76ee\u6807\u9884\u6d4b\u5668\u7684\u5b66\u4e60\u3002"}}
{"id": "2602.16558", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.16558", "abs": "https://arxiv.org/abs/2602.16558", "authors": ["Gerhard Stenzel", "Tobias Rohe", "Michael K\u00f6lle", "Leo S\u00fcnkel", "Jonas Stein", "Claudia Linnhoff-Popien"], "title": "Illustration of Barren Plateaus in Quantum Computing", "comment": "Extended version of a short paper to be published at ICAART-QAIO 2026", "summary": "Variational Quantum Circuits (VQCs) have emerged as a promising paradigm for quantum machine learning in the NISQ era. While parameter sharing in VQCs can reduce the parameter space dimensionality and potentially mitigate the barren plateau phenomenon, it introduces a complex trade-off that has been largely overlooked. This paper investigates how parameter sharing, despite creating better global optima with fewer parameters, fundamentally alters the optimization landscape through deceptive gradients -- regions where gradient information exists but systematically misleads optimizers away from global optima. Through systematic experimental analysis, we demonstrate that increasing degrees of parameter sharing generate more complex solution landscapes with heightened gradient magnitudes and measurably higher deceptiveness ratios. Our findings reveal that traditional gradient-based optimizers (Adam, SGD) show progressively degraded convergence as parameter sharing increases, with performance heavily dependent on hyperparameter selection. We introduce a novel gradient deceptiveness detection algorithm and a quantitative framework for measuring optimization difficulty in quantum circuits, establishing that while parameter sharing can improve circuit expressivity by orders of magnitude, this comes at the cost of significantly increased landscape deceptiveness. These insights provide important considerations for quantum circuit design in practical applications, highlighting the fundamental mismatch between classical optimization strategies and quantum parameter landscapes shaped by parameter sharing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53d8\u5206\u91cf\u5b50\u7535\u8def(VQCs)\u4e2d\u53c2\u6570\u5171\u4eab\u7684\u5f71\u54cd\uff0c\u5c3d\u7ba1\u5b83\u80fd\u51cf\u5c11\u53c2\u6570\u7a7a\u95f4\u7ef4\u5ea6\u5e76\u53ef\u80fd\u7f13\u89e3\u8d2b\u7620\u9ad8\u539f\u73b0\u8c61\uff0c\u4f46\u540c\u65f6\u4e5f\u4f1a\u901a\u8fc7\u8bef\u5bfc\u6027\u68af\u5ea6\u6539\u53d8\u4f18\u5316\u666f\u89c2\u3002\u968f\u7740\u53c2\u6570\u5171\u4eab\u7a0b\u5ea6\u7684\u589e\u52a0\uff0c\u89e3\u666f\u53d8\u5f97\u66f4\u52a0\u590d\u6742\uff0c\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5668\u8868\u73b0\u9010\u6e10\u6076\u5316\u3002\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u68af\u5ea6\u6b3a\u9a97\u68c0\u6d4b\u7b97\u6cd5\u548c\u4e00\u4e2a\u91cf\u5316\u6846\u67b6\u6765\u6d4b\u91cf\u91cf\u5b50\u7535\u8def\u4e2d\u7684\u4f18\u5316\u96be\u5ea6\uff0c\u6307\u51fa\u867d\u7136\u53c2\u6570\u5171\u4eab\u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8\u7535\u8def\u7684\u8868\u73b0\u529b\uff0c\u4f46\u8fd9\u4f1a\u663e\u8457\u589e\u52a0\u666f\u89c2\u7684\u6b3a\u9a97\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6df1\u5165\u7406\u89e3\u53d8\u5206\u91cf\u5b50\u7535\u8def(VQCs)\u4e2d\u53c2\u6570\u5171\u4eab\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u8bef\u5bfc\u6027\u68af\u5ea6\u6539\u53d8\u4f18\u5316\u666f\u89c2\uff0c\u4ee5\u53ca\u8fd9\u79cd\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u5230\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5b9e\u9a8c\u5206\u6790\u6cd5\uff0c\u901a\u8fc7\u5bf9\u4e0d\u540c\u7a0b\u5ea6\u53c2\u6570\u5171\u4eab\u4e0b\u89e3\u666f\u590d\u6742\u5ea6\u3001\u68af\u5ea6\u5e45\u5ea6\u53ca\u6b3a\u9a97\u6027\u6bd4\u7387\u7684\u53d8\u5316\u8fdb\u884c\u7814\u7a76\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68af\u5ea6\u6b3a\u9a97\u68c0\u6d4b\u7b97\u6cd5\u548c\u4e00\u4e2a\u7528\u4e8e\u8861\u91cf\u91cf\u5b50\u7535\u8def\u4f18\u5316\u96be\u6613\u5ea6\u7684\u5b9a\u91cf\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u968f\u7740\u53c2\u6570\u5171\u4eab\u7a0b\u5ea6\u7684\u63d0\u5347\uff0c\u89e3\u666f\u53d8\u5f97\u66f4\u4e3a\u590d\u6742\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u68af\u5ea6\u5e45\u5ea6\u4e0e\u6b3a\u9a97\u6027\u6bd4\u7387\uff1b\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\uff08\u5982Adam\u3001SGD\uff09\u5728\u9762\u5bf9\u66f4\u9ad8\u6c34\u5e73\u53c2\u6570\u5171\u4eab\u65f6\u5176\u6536\u655b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u975e\u5e38\u4f9d\u8d56\u4e8e\u8d85\u53c2\u6570\u7684\u9009\u62e9\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u867d\u7136\u53c2\u6570\u5171\u4eab\u80fd\u591f\u4ee5\u6570\u91cf\u7ea7\u5730\u589e\u5f3a\u7535\u8def\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u663e\u8457\u589e\u52a0\u7684\u4f18\u5316\u666f\u89c2\u6b3a\u9a97\u6027\u95ee\u9898\u3002\u8fd9\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5e76\u63ed\u793a\u4e86\u7ecf\u5178\u4f18\u5316\u7b56\u7565\u4e0e\u7531\u53c2\u6570\u5171\u4eab\u5851\u9020\u7684\u91cf\u5b50\u53c2\u6570\u666f\u89c2\u4e4b\u95f4\u5b58\u5728\u7684\u6839\u672c\u4e0d\u5339\u914d\u3002"}}
{"id": "2602.16570", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.16570", "abs": "https://arxiv.org/abs/2602.16570", "authors": ["Ankur Moitra", "Andrej Risteski", "Dhruv Rohatgi"], "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis", "comment": null, "summary": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.\n  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\\star}(x) \\propto p(x) \\exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\\top A x + b^\\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u5956\u52b1\u503e\u659c\u6269\u6563\u6a21\u578b\u4e2d\u91c7\u6837\u7684\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e8c\u6b21\u5956\u52b1\u51fd\u6570\u3002\u4f5c\u8005\u8bc1\u660e\u4e86\u7ebf\u6027\u5956\u52b1\u503e\u659c\u603b\u662f\u53ef\u4ee5\u6709\u6548\u91c7\u6837\uff0c\u5e76\u4e14\u5229\u7528Hubbard-Stratonovich\u53d8\u6362\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4f4e\u79e9\u6b63\u5b9a\u4e8c\u6b21\u503e\u659c\u7684\u6709\u6548\u7b97\u6cd5\u3002\u800c\u5bf9\u4e8e\u8d1f\u5b9a\u503e\u659c\uff0c\u5373\u4f7fA\u7684\u79e9\u4e3a1\uff0c\u95ee\u9898\u4e5f\u662f\u96be\u4ee5\u5904\u7406\u7684\u3002", "motivation": "\u76ee\u524d\u5b9e\u8df5\u4e2d\u4f7f\u7528\u7684\u63a8\u7406\u65f6\u95f4\u7b97\u6cd5\u5927\u591a\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u5b58\u5728\u591a\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e9b\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f55\u65f6\u80fd\u88ab\u6709\u6548\u6539\u8fdb\u4e86\u89e3\u751a\u5c11\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u805a\u7126\u4e8e\u7ed9\u5b9a\u5956\u52b1\u51fd\u6570\u548c\u9884\u8bad\u7ec3\u6269\u6563oracle\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u5956\u52b1\u503e\u659c\u6269\u6563\u6a21\u578b\u4e2d\u91c7\u6837\u7684\u8ba1\u7b97\u53ef\u884c\u6027\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u4e8c\u6b21\u5956\u52b1\u51fd\u6570\uff08\u5373\u7ebf\u6027\u548c\u4f4e\u79e9\u6b63/\u8d1f\u5b9a\uff09\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\uff0c\u7ed3\u5408\u4f7f\u7528Hubbard-Stratonovich\u53d8\u6362\u4f5c\u4e3a\u65b0\u7684\u5173\u952e\u5143\u7d20\uff0c\u5f00\u53d1\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u6765\u89e3\u51b3\u4f4e\u79e9\u6b63\u5b9a\u60c5\u51b5\u4e0b\u7684\u91c7\u6837\u95ee\u9898\u3002", "result": "\u5c55\u793a\u4e86\u7ebf\u6027\u5956\u52b1\u503e\u659c\u603b\u662f\u80fd\u591f\u88ab\u6709\u6548\u5730\u91c7\u6837\uff1b\u5bf9\u4e8e\u4f4e\u79e9\u6b63\u5b9a\u4e8c\u6b21\u503e\u659c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHubbard-Stratonovich\u53d8\u6362\u7684\u6709\u6548\u7b97\u6cd5\uff1b\u800c\u8d1f\u5b9a\u503e\u659c\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u77e9\u9635A\u7684\u79e9\u4ec5\u4e3a1\uff0c\u8be5\u95ee\u9898\u4e5f\u88ab\u8bc1\u660e\u662f\u96be\u89e3\u7684\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff08\u5982\u7ebf\u6027\u548c\u4f4e\u79e9\u6b63\u5b9a\u4e8c\u6b21\u5956\u52b1\uff09\uff0c\u53ef\u4ee5\u4ece\u5956\u52b1\u503e\u659c\u6269\u6563\u6a21\u578b\u4e2d\u9ad8\u6548\u5730\u91c7\u6837\uff1b\u7136\u800c\uff0c\u5bf9\u4e8e\u8d1f\u5b9a\u503e\u659c\uff0c\u5373\u4f7f\u5728\u975e\u5e38\u9650\u5236\u6027\u7684\u6761\u4ef6\u4e0b\uff08\u4f8b\u5982A\u7684\u79e9\u4e3a1\uff09\uff0c\u95ee\u9898\u4e5f\u53d8\u5f97\u4e0d\u53ef\u5904\u7406\u3002"}}
{"id": "2602.16579", "categories": ["cs.LG", "cs.AI", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.16579", "abs": "https://arxiv.org/abs/2602.16579", "authors": ["Maria Luisa Taccari", "Kenza Tazi", "Ois\u00edn M. Morrison", "Andreas Grafberger", "Juan Colonese", "Corentin Carton de Wiart", "Christel Prudhomme", "Cinzia Mazzetti", "Matthew Chantry", "Florian Pappenberger"], "title": "AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS", "comment": null, "summary": "Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u7684\u5168\u7403\u65e5\u5f84\u6d41\u9884\u62a5\u6a21\u578bAIFL\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u4e86\u4ece\u5386\u53f2\u518d\u5206\u6790\u5230\u4e1a\u52a1\u9884\u62a5\u4ea7\u54c1\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\uff0c\u5e76\u5728\u72ec\u7acb\u65f6\u95f4\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u9884\u6d4b\u6280\u80fd\u3002", "motivation": "\u9488\u5bf9\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4ece\u5386\u53f2\u518d\u5206\u6790\u8fc7\u6e21\u5230\u4e1a\u52a1\u9884\u62a5\u4ea7\u54c1\u65f6\u51fa\u73b0\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53ef\u9760\u7684\u5168\u7403\u65e5\u5f84\u6d41\u9884\u62a5\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6d2a\u6c34\u51c6\u5907\u548c\u6c34\u8d44\u6e90\u7ba1\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86AIFL\u6a21\u578b\uff0c\u91c7\u7528LSTM\u67b6\u6784\u5e76\u5b9e\u65bd\u4e86\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528ERA5-Land\u518d\u5206\u6790\u8d44\u6599\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u63a5\u7740\u5229\u7528IFS\u63a7\u5236\u9884\u62a5\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u6574\u4e2a\u8fc7\u7a0b\u5728CARAVAN\u751f\u6001\u7cfb\u7edf\u5185\u5b8c\u6210\u3002", "result": "AIFL\u6a21\u578b\u5728\u5168\u740318,588\u4e2a\u6d41\u57df\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u57282021-2024\u5e74\u7684\u72ec\u7acb\u65f6\u95f4\u6d4b\u8bd5\u96c6\u4e2d\u8fbe\u5230\u4e86\u4e2d\u4f4d\u4fee\u6539\u540e\u7684Kling-Gupta\u6548\u7387\u7cfb\u6570(KGE')\u4e3a0.66\u548cNash-Sutcliffe\u6548\u7387\u7cfb\u6570(NSE)\u4e3a0.53\u7684\u597d\u6210\u7ee9\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u5728\u6781\u7aef\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "AIFL\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u7f29\u5c0f\u4ece\u518d\u5206\u6790\u5230\u9884\u62a5\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u8fd8\u5c55\u73b0\u51fa\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u5168\u7403\u7cfb\u7edf\u76f8\u5ab2\u7f8e\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u900f\u660e\u53ef\u91cd\u590d\u6027\u7684\u5f3a\u8feb\u6d41\u7a0b\uff0c\u4e3a\u5168\u7403\u6c34\u6587\u754c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u3001\u64cd\u4f5c\u6027\u5f3a\u7684\u57fa\u7840\u65b9\u6848\u3002"}}
{"id": "2602.16596", "categories": ["cs.LG", "cs.CR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16596", "abs": "https://arxiv.org/abs/2602.16596", "authors": ["Thomas Michel", "Debabrota Basu", "Emilie Kaufmann"], "title": "Sequential Membership Inference Attacks", "comment": "27 pages, 10 figures", "summary": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aSeMI*\u7684\u6700\u4f18\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u66f4\u65b0\u5e8f\u5217\u6765\u8bc6\u522b\u5728\u7279\u5b9a\u66f4\u65b0\u6b65\u9aa4\u4e2d\u63d2\u5165\u7684\u76ee\u6807\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u6709\u9650\u6837\u672c\u6761\u4ef6\u4e0b\u8ba1\u7b97\u7ecf\u9a8c\u5747\u503c\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u4ec5\u9488\u5bf9\u6700\u7ec8\u6a21\u578b\u7684\u653b\u51fb\u76f8\u6bd4\uff0c\u80fd\u591f\u907f\u514d\u6210\u5458\u63a8\u7406\u4fe1\u53f7\u88ab\u7a00\u91ca\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSeMI*\u7684\u5b9e\u9645\u53d8\u4f53\u63d0\u4f9b\u4e86\u6bd4\u57fa\u7ebf\u66f4\u4e25\u683c\u7684\u9690\u79c1\u5ba1\u8ba1\u3002", "motivation": "\u73b0\u4ee3AI\u6a21\u578b\u5728\u5176\u751f\u547d\u5468\u671f\u4e2d\u4f1a\u7ecf\u5386\u591a\u6b21\u66f4\u65b0\uff0c\u56e0\u6b64\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\u6765\u521b\u5efa\u66f4\u5f3a\u7684\u6210\u5458\u63a8\u7406\uff08MI\uff09\u653b\u51fb\u548c\u66f4\u4e25\u683c\u7684\u9690\u79c1\u5ba1\u8ba1\u6210\u4e3a\u4e86\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u6709\u6587\u732e\u8868\u660e\u4f7f\u7528\u4e00\u7cfb\u5217\u6a21\u578b\u66f4\u65b0\u53ef\u4ee5\u589e\u52a0MI\u653b\u51fb\u7684\u80fd\u529b\uff0c\u4f46\u5bf9\u2018\u6700\u4f18\u2019MI\u653b\u51fb\u7684\u4e25\u683c\u5206\u6790\u8fd8\u5c40\u9650\u4e8e\u62e5\u6709\u65e0\u9650\u6837\u672c\u7684\u9759\u6001\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aSeMI*\u7684\u2018\u6700\u4f18\u2019MI\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6a21\u578b\u66f4\u65b0\u5e8f\u5217\u6765\u8bc6\u522b\u5728\u67d0\u4e2a\u66f4\u65b0\u6b65\u9aa4\u4e2d\u63d2\u5165\u7684\u76ee\u6807\u3002\u5bf9\u4e8e\u7ecf\u9a8c\u5747\u503c\u8ba1\u7b97\uff0c\u7814\u7a76\u4eba\u5458\u63a8\u5bfc\u4e86\u5f53\u8bbf\u95ee\u6709\u9650\u6570\u91cf\u6837\u672c\u65f6SeMI*\u7684\u6700\u4f73\u6548\u80fd\uff0c\u65e0\u8bba\u662f\u5426\u5177\u6709\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u8bbf\u95ee\u6a21\u578b\u5e8f\u5217\u53ef\u4ee5\u9632\u6b62MI\u4fe1\u53f7\u88ab\u7a00\u91ca\uff0c\u8fd9\u4e0e\u73b0\u6709\u7684\u9488\u5bf9\u6700\u7ec8\u6a21\u578b\u7684\u653b\u51fb\u4e0d\u540c\uff0c\u5728\u540e\u8005\u4e2d\u968f\u7740\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u589e\u52a0MI\u4fe1\u53f7\u4f1a\u9010\u6e10\u6d88\u5931\u3002\u6b64\u5916\uff0c\u5bf9\u624b\u8fd8\u53ef\u4ee5\u5229\u7528SeMI*\u8c03\u6574\u63d2\u5165\u65f6\u95f4\u548c\u8bf1\u9975\u4ee5\u83b7\u5f97\u66f4\u4e25\u683c\u7684\u9690\u79c1\u5ba1\u8ba1\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cSeMI*\u7684\u5b9e\u9645\u53d8\u4f53\u5728\u591a\u79cd\u6570\u636e\u5206\u5e03\u4ee5\u53ca\u4f7f\u7528DP-SGD\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u6a21\u578b\u4e0a\u90fd\u80fd\u63d0\u4f9b\u6bd4\u57fa\u7ebf\u66f4\u4e3a\u4e25\u683c\u7684\u9690\u79c1\u5ba1\u8ba1\u3002", "conclusion": "SeMI*\u4f5c\u4e3a\u4e00\u79cd\u5229\u7528\u6a21\u578b\u66f4\u65b0\u5e8f\u5217\u6765\u8fdb\u884c\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u653b\u51fb\u80fd\u529b\uff0c\u800c\u4e14\u4e3a\u6267\u884c\u66f4\u52a0\u51c6\u786e\u6709\u6548\u7684\u9690\u79c1\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2602.16626", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.16626", "abs": "https://arxiv.org/abs/2602.16626", "authors": ["SungJun Cho", "Chetan Gohil", "Rukuang Huang", "Oiwi Parker Jones", "Mark W. Woolrich"], "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models", "comment": "15 pages, 10 figures, 1 table", "summary": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u7528\u4e8e\u5904\u7406\u8111\u78c1\u56fe(MEG)\u6570\u636e\u7684\u5927\u89c4\u6a21\u795e\u7ecf\u5f71\u50cf\u6a21\u578b\u4e2d\u7684\u6837\u672c\u7ea7\u5206\u8bcd\u7b56\u7565\uff0c\u6bd4\u8f83\u4e86\u53ef\u5b66\u4e60\u548c\u4e0d\u53ef\u5b66\u4e60\u7684\u5206\u8bcd\u5668\u5728\u4fe1\u53f7\u91cd\u5efa\u4fdd\u771f\u5ea6\u4ee5\u53ca\u540e\u7eed\u57fa\u7840\u5efa\u6a21\u6027\u80fd\u65b9\u9762\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u53ef\u5b66\u4e60\u8fd8\u662f\u4e0d\u53ef\u5b66\u4e60\u7684\u79bb\u6563\u5316\u65b9\u6848\u90fd\u80fd\u8fbe\u5230\u8f83\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728\u5927\u591a\u6570\u8bc4\u4ef7\u6807\u51c6\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u610f\u5473\u7740\u7b80\u5355\u7684\u56fa\u5b9a\u6837\u672c\u7ea7\u5206\u8bcd\u7b56\u7565\u53ef\u4ee5\u88ab\u5e94\u7528\u4e8e\u795e\u7ecf\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5bf9\u5927\u89c4\u6a21\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u57fa\u7840\u6a21\u578b\u7684\u5174\u8da3\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u9700\u8981\u5bf9\u8fde\u7eed\u7684\u795e\u7ecf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u79bb\u6563\u5316\uff08\u79f0\u4e3a\u2018\u5206\u8bcd\u2019\uff09\u3002\u4f46\u662f\uff0c\u76ee\u524d\u5bf9\u4e8e\u4e0d\u540c\u5206\u8bcd\u7b56\u7565\u5bf9\u795e\u7ecf\u6570\u636e\u5f71\u54cd\u7684\u7406\u89e3\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u7814\u7a76\u8005\u4eec\u5bf9\u5e94\u7528\u4e8e\u8111\u78c1\u56fe(MEG)\u6570\u636e\u7684\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u5927\u89c4\u6a21\u795e\u7ecf\u5f71\u50cf\u6a21\u578b\u8fdb\u884c\u4e86\u6837\u672c\u7ea7\u5206\u8bcd\u7b56\u7565\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u4ed6\u4eec\u901a\u8fc7\u8003\u5bdf\u4fe1\u53f7\u91cd\u5efa\u4fdd\u771f\u5ea6\u53ca\u5bf9\u540e\u7eed\u57fa\u7840\u5efa\u6a21\u6027\u80fd\uff08\u5305\u62ec\u5206\u8bcd\u9884\u6d4b\u3001\u751f\u6210\u6570\u636e\u7684\u751f\u7269\u5b66\u5408\u7406\u6027\u3001\u4e2a\u4f53\u7279\u5f02\u6027\u4fe1\u606f\u4fdd\u5b58\u4ee5\u53ca\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff09\u7684\u5f71\u54cd\u6765\u5bf9\u6bd4\u53ef\u5b66\u4e60\u4e0e\u975e\u53ef\u5b66\u4e60\u5206\u8bcd\u5668\u3002\u5bf9\u4e8e\u53ef\u5b66\u4e60\u5206\u8bcd\u5668\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u65b0\u65b9\u6cd5\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e09\u4e2a\u516c\u5f00\u53ef\u7528\u7684MEG\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u7684\u91c7\u96c6\u5730\u70b9\u3001\u626b\u63cf\u4eea\u548c\u6280\u672f\u8303\u5f0f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u7ba1\u662f\u53ef\u5b66\u4e60\u8fd8\u662f\u975e\u53ef\u5b66\u4e60\u7684\u79bb\u6563\u5316\u65b9\u6848\u90fd\u80fd\u591f\u5b9e\u73b0\u5f88\u9ad8\u7684\u91cd\u5efa\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u5927\u90e8\u5206\u8bc4\u6d4b\u6307\u6807\u4e0a\u5177\u6709\u5e7f\u6cdb\u53ef\u6bd4\u7684\u8868\u73b0\u3002\u8fd9\u610f\u5473\u7740\u7b80\u5355\u7684\u56fa\u5b9a\u6837\u672c\u7ea7\u522b\u5206\u8bcd\u7b56\u7565\u53ef\u4ee5\u5728\u795e\u7ecf\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u4e2d\u88ab\u91c7\u7528\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u5728\u5f00\u53d1\u7528\u4e8e\u5904\u7406MEG\u6570\u636e\u7684\u5927\u89c4\u6a21\u795e\u7ecf\u5f71\u50cf\u6a21\u578b\u65f6\uff0c\u53ef\u4ee5\u8003\u8651\u4f7f\u7528\u7b80\u5355\u6709\u6548\u7684\u56fa\u5b9a\u6837\u672c\u7ea7\u5206\u8bcd\u7b56\u7565\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u4ec5\u80fd\u591f\u63d0\u4f9b\u826f\u597d\u7684\u4fe1\u53f7\u91cd\u5efa\u8d28\u91cf\uff0c\u800c\u4e14\u8fd8\u80fd\u4fdd\u6301\u8f83\u597d\u7684\u540e\u7eed\u5efa\u6a21\u6027\u80fd\u3002"}}
{"id": "2602.16629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16629", "abs": "https://arxiv.org/abs/2602.16629", "authors": ["Ethan Blaser", "Jiuqi Wang", "Shangtong Zhang"], "title": "Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes", "comment": null, "summary": "The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u4e0d\u4f7f\u7528\u4e0e\u72b6\u6001\u8bbf\u95ee\u6b21\u6570\u76f8\u5173\u7684\u5c40\u90e8\u65f6\u949f\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u7b56\u7565\u7684n\u6b65\u5dee\u5206TD\u7b97\u6cd5\u51e0\u4e4e\u53ef\u4ee5\u786e\u5b9a\u5730\u6536\u655b\uff0c\u5e76\u4e14\u7ed9\u51fa\u4e86\u79bb\u7b56\u7565n\u6b65\u5dee\u5206TD\u7b97\u6cd5\u540c\u6837\u80fd\u591f\u4e0d\u4f9d\u8d56\u4e8e\u5c40\u90e8\u65f6\u949f\u800c\u6536\u655b\u7684\u4e09\u4e2a\u5145\u5206\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u7684\u5dee\u5206\u65f6\u95f4\u5dee\u5b66\u4e60\u7b97\u6cd5\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u9700\u8981\u4e00\u4e2a\u4e0e\u72b6\u6001\u8bbf\u95ee\u6b21\u6570\u6709\u5173\u7684\u5b66\u4e60\u7387\u5c40\u90e8\u65f6\u949f\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e76\u4e0d\u5e38\u7528\uff0c\u4e5f\u4e0d\u9002\u7528\u4e8e\u8868\u683c\u4ee5\u5916\u7684\u8bbe\u7f6e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u4f7f\u5dee\u5206TD\u7b97\u6cd5\u7684\u6536\u655b\u5206\u6790\u66f4\u63a5\u8fd1\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u5728\u6807\u51c6\u9012\u51cf\u5b66\u4e60\u7387\u4e0b\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u7b56\u7565\u7684n\u6b65\u5dee\u5206TD\u7b97\u6cd5\u5bf9\u4e8e\u4efb\u610fn\u503c\u90fd\u80fd\u51e0\u4e4e\u786e\u5b9a\u5730\u6536\u655b\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u5bfc\u51fa\u4f7f\u5f97\u79bb\u7b56\u7565n\u6b65\u5dee\u5206TD\u7b97\u6cd5\u80fd\u591f\u5728\u6ca1\u6709\u5c40\u90e8\u65f6\u949f\u60c5\u51b5\u4e0b\u6536\u655b\u7684\u4e09\u4e2a\u5145\u5206\u6761\u4ef6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u7b56\u7565\u7684n\u6b65\u5dee\u5206TD\u7b97\u6cd5\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u4e8e\u5c40\u90e8\u65f6\u949f\u7684\u60c5\u51b5\u4e0b\u6536\u655b\uff1b\u540c\u65f6\uff0c\u53ea\u8981\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\uff0c\u79bb\u7b56\u7565\u7248\u672c\u4e5f\u80fd\u5b9e\u73b0\u7c7b\u4f3c\u6548\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u52a0\u5f3a\u4e86\u5dee\u5206TD\u7b97\u6cd5\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4f7f\u5176\u6536\u655b\u6027\u5206\u6790\u66f4\u52a0\u8d34\u8fd1\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.16642", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16642", "abs": "https://arxiv.org/abs/2602.16642", "authors": ["Jim Zhao", "Tin Sum Cheng", "Wojciech Masarczyk", "Aurelien Lucchi"], "title": "Optimizer choice matters for the emergence of Neural Collapse", "comment": "Published as a conference paper at ICLR 2026", "summary": "Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u795e\u7ecf\u5d29\u6e83\uff08NC\uff09\u73b0\u8c61\u5728\u4f18\u5316\u65b9\u6cd5\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5047\u8bbe\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bca\u65ad\u6307\u6807NC0\u8bc1\u660e\u4e86\u5728AdamW\u7b49\u81ea\u9002\u5e94\u4f18\u5316\u5668\u4e2d\u89e3\u8026\u6743\u91cd\u8870\u51cf\u4e0bNC\u65e0\u6cd5\u51fa\u73b0\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u52a8\u91cf\u5bf9\u4f7f\u7528SGD\u8bad\u7ec3\u65f6NC\u7684\u52a0\u901f\u6548\u5e94\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5173\u4e8e\u795e\u7ecf\u5d29\u6e83\uff08NC\uff09\u73b0\u8c61\u7684\u7814\u7a76\u5927\u591a\u5ffd\u7565\u4e86\u4f18\u5316\u5668\u7684\u4f5c\u7528\uff0c\u6697\u793a\u7740NC\u73b0\u8c61\u5728\u4e0d\u540c\u7684\u4f18\u5316\u65b9\u6cd5\u4e2d\u662f\u666e\u904d\u5b58\u5728\u7684\u3002\u672c\u6587\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u7a76\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u4e8eNC\u73b0\u8c61\u51fa\u73b0\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u672c\u6587\u9996\u5148\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8bca\u65ad\u5ea6\u91cf\u6807\u51c6NC0\uff0c\u5176\u6536\u655b\u81f3\u96f6\u662f\u53d1\u751fNC\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u5229\u7528NC0\uff0c\u4f5c\u8005\u4eec\u63d0\u4f9b\u4e86\u7406\u8bba\u8bc1\u636e\u8868\u660e\uff0c\u5728\u7c7b\u4f3cAdamW\u8fd9\u6837\u7684\u81ea\u9002\u5e94\u4f18\u5316\u5668\u91c7\u7528\u89e3\u8026\u5408\u6743\u91cd\u8870\u51cf\u7684\u60c5\u51b5\u4e0b\uff0cNC\u4e0d\u4f1a\u51fa\u73b0\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u8bc1\u660e\u4e86\u5f53\u4f7f\u7528SGD\u8bad\u7ec3\u65f6\uff0c\u52a8\u91cf\u5bf9\u4e8e\u4fc3\u8fdbNC\u53d1\u751f\u7684\u989d\u5916\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u4f18\u5316\u7b56\u7565\u5982SGD\u3001\u5e26\u8026\u5408\u6743\u91cd\u8870\u51cf\u7684SignGD\uff08Adam\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff09\u3001\u4ee5\u53ca\u5e26\u89e3\u8026\u5408\u6743\u91cd\u8870\u51cf\u7684SignGD\uff08AdamW\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff09\uff0c\u5b83\u4eec\u8868\u73b0\u51fa\u660e\u663e\u4e0d\u540c\u7684NC0\u52a8\u6001\u7279\u5f81\u3002\u540c\u65f6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u52a8\u91cf\u786e\u5b9e\u80fd\u591f\u52a0\u901fNC\u7684\u53d1\u5c55\u8fc7\u7a0b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4e86\u57fa\u4e8e\u4f18\u5316\u5668\u5dee\u5f02\u800c\u5f15\u53d1\u7684NC\u73b0\u8c61\uff0c\u5e76\u5f3a\u8c03\u4e86\u6743\u91cd\u8870\u51cf\u8026\u5408\u65b9\u5f0f\u5728\u51b3\u5b9a\u4f18\u5316\u5668\u9690\u5f0f\u504f\u5dee\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.16643", "categories": ["cs.LG", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.16643", "abs": "https://arxiv.org/abs/2602.16643", "authors": ["Shuta Kikuchi", "Shu Tanaka"], "title": "Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment", "comment": "17 pages, 10 figures", "summary": "The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5e26\u6709\u4e8c\u6b21\u4f18\u5316\u9000\u706b\u7684\u56e0\u5b50\u5206\u89e3\u673a\uff08FMQA\uff09\u6765\u89e3\u51b3RNA\u9006\u6298\u53e0\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\u3002\u901a\u8fc7\u8bc4\u4f30\u4e0d\u540c\u6838\u82f7\u9178\u5230\u6574\u6570\u7684\u5206\u914d\u65b9\u5f0f\u548c\u4e8c\u8fdb\u5236-\u6574\u6570\u7f16\u7801\u65b9\u6cd5\uff0c\u7814\u7a76\u53d1\u73b0\u72ec\u70ed\u7f16\u7801\u548c\u57df\u58c1\u7f16\u7801\u6bd4\u4e8c\u8fdb\u5236\u548c\u4e00\u5143\u7f16\u7801\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u5728\u57df\u58c1\u7f16\u7801\u4e2d\u5c06\u9e1f\u560c\u5464\u548c\u80de\u5627\u5576\u5206\u914d\u7ed9\u8fb9\u754c\u6574\u6570\u53ef\u4ee5\u4fc3\u8fdb\u5b83\u4eec\u5728\u830e\u533a\u7684\u5bcc\u96c6\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u7a33\u5b9a\u7684\u4e8c\u7ea7\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u542f\u53d1\u5f0f\u6216\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u89e3\u51b3RNA\u9006\u6298\u53e0\u95ee\u9898\u65f6\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u5e8f\u5217\u8bc4\u4f30\uff0c\u8fd9\u9650\u5236\u4e86\u5f53\u5b9e\u9a8c\u9a8c\u8bc1\u6210\u672c\u9ad8\u6602\u65f6\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u6240\u9700\u8bc4\u4f30\u6b21\u6570\u7684\u540c\u65f6\u4fdd\u6301\u89e3\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u5982\u4f55\u5c06\u6838\u82f7\u9178\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u53d8\u91cf\u4ee5\u53ca\u8fd9\u79cd\u8f6c\u6362\u5bf9FMQA\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u6709\u6df1\u5165\u7814\u7a76\uff0c\u8fd9\u4e5f\u662f\u672c\u7814\u7a76\u7684\u4e00\u4e2a\u91cd\u8981\u52a8\u673a\u3002", "method": "\u91c7\u7528\u5e26\u6709\u4e8c\u6b21\u4f18\u5316\u9000\u706b\u7684\u56e0\u5b50\u5206\u89e3\u673a\uff08FMQA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u80fd\u5728\u6709\u9650\u6b21\u8bc4\u4f30\u4e0b\u83b7\u5f97\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u7684\u79bb\u6563\u9ed1\u7bb1\u4f18\u5316\u65b9\u6cd5\u3002\u4e3a\u4e86\u5e94\u7528FMQA\u4e8eRNA\u9006\u6298\u53e0\u95ee\u9898\uff0c\u5fc5\u987b\u5c06\u6838\u82f7\u9178\u8f6c\u6362\u6210\u4e8c\u8fdb\u5236\u53d8\u91cf\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u6240\u670924\u79cd\u53ef\u80fd\u7684\u56db\u6838\u82f7\u9178\u5230\u6709\u5e8f\u6574\u6570(0-3)\u7684\u5206\u914d\u65b9\u5f0f\uff0c\u7ed3\u5408\u56db\u79cd\u4e0d\u540c\u7684\u4e8c\u8fdb\u5236-\u6574\u6570\u7f16\u7801\u65b9\u6cd5\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5f52\u4e00\u5316\u96c6\u5408\u7f3a\u9677\u503c\u65b9\u9762\uff0c\u72ec\u70ed\u7f16\u7801\u548c\u57df\u58c1\u7f16\u7801\u4f18\u4e8e\u4e8c\u8fdb\u5236\u548c\u4e00\u5143\u7f16\u7801\u3002\u7279\u522b\u662f\u5728\u57df\u58c1\u7f16\u7801\u4e2d\uff0c\u88ab\u5206\u914d\u7ed9\u8fb9\u754c\u6574\u6570\uff080\u548c3\uff09\u7684\u6838\u82f7\u9178\u51fa\u73b0\u9891\u7387\u66f4\u9ad8\uff1b\u800c\u5728RNA\u9006\u6298\u53e0\u95ee\u9898\u4e2d\uff0c\u5c06\u9e1f\u560c\u5464\u548c\u80de\u5627\u5576\u5206\u914d\u7ed9\u8fd9\u4e9b\u8fb9\u754c\u6574\u6570\u80fd\u591f\u4fc3\u8fdb\u5176\u5728\u830e\u533a\u7684\u5bcc\u96c6\uff0c\u4ece\u800c\u5f62\u6210\u6bd4\u72ec\u70ed\u7f16\u7801\u6240\u5f97\u66f4\u4e3a\u70ed\u529b\u5b66\u7a33\u5b9a\u7684\u4e8c\u7ea7\u7ed3\u6784\u3002", "conclusion": "\u901a\u8fc7\u5efa\u7acb\u4e00\u79cd\u65b0\u7684FMQA\u6846\u67b6\u7528\u4e8eRNA\u9006\u6298\u53e0\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6838\u82f7\u9178\u5230\u6574\u6570\u7684\u5206\u914d\u65b9\u5f0f\u53ca\u4e8c\u8fdb\u5236-\u6574\u6570\u7f16\u7801\u65b9\u6cd5\u5bf9\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u72ec\u70ed\u7f16\u7801\u4e0e\u57df\u58c1\u7f16\u7801\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u6709\u6548\u63d0\u9ad8RNA\u8bbe\u8ba1\u6548\u7387\u3002\u7279\u522b\u662f\uff0c\u5408\u7406\u9009\u62e9\u6838\u82f7\u9178\u5230\u6574\u6570\u7684\u6620\u5c04\u5173\u7cfb\u6709\u52a9\u4e8e\u589e\u5f3aRNA\u5206\u5b50\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.16684", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16684", "abs": "https://arxiv.org/abs/2602.16684", "authors": ["Bo Pan", "Peter Zhiping Zhang", "Hao-Wei Pang", "Alex Zhu", "Xiang Yu", "Liying Zhang", "Liang Zhao"], "title": "Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition", "comment": null, "summary": "Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u91cf\u5230\u53d8\u91cf\u7684\u7c7b\u6bd4\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\u6765\u5904\u7406\u5927\u89c4\u6a21\u5339\u914d\u5206\u5b50\u5bf9\u8f6c\u6362\uff08MMPTs\uff09\uff0c\u4ee5\u5728\u7ed9\u5b9a\u8f93\u5165\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u591a\u6837\u5316\u7684\u53d8\u91cf\u3002\u901a\u8fc7\u5f00\u53d1\u63d0\u793a\u673a\u5236\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6307\u5b9a\u9996\u9009\u7684\u8f6c\u6362\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86MMPT-RAG\u6846\u67b6\uff0c\u5229\u7528\u5916\u90e8\u53c2\u8003\u7c7b\u6bd4\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6307\u5bfc\uff0c\u4ee5\u6307\u5bfc\u751f\u6210\u5e76\u4ece\u7279\u5b9a\u9879\u76ee\u7cfb\u5217\u4e2d\u6cdb\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901a\u7528\u5316\u5b66\u8bed\u6599\u5e93\u548c\u4e13\u5229\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u53d1\u73b0\u573a\u666f\u4e2d\u80fd\u591f\u6062\u590d\u73b0\u5b9e\u7684\u7c7b\u6bd4\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u5728\u6574\u4e2a\u5206\u5b50\u5c42\u9762\u64cd\u4f5c\uff0c\u7f16\u8f91\u63a7\u5236\u80fd\u529b\u6709\u9650\uff0c\u8981\u4e48\u4ece\u53d7\u9650\u7684\u8bbe\u7f6e\u548c\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u5b66\u4e60MMP\u98ce\u683c\u7684\u7f16\u8f91\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u7c7b\u6bd4\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u53ef\u63a7\u6027\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u91cf\u81f3\u53d8\u91cf\u7684\u7c7b\u6bd4\u751f\u6210\u516c\u5f0f\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u80fd\u591f\u5728\u5927\u578bMMPT\u4e0a\u8fd0\u884c\u7684\u57fa\u7840\u6a21\u578b\u3002\u4e3a\u4e86\u589e\u5f3a\u5b9e\u7528\u6027\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u5141\u8bb8\u7528\u6237\u5728\u751f\u6210\u65f6\u6307\u5b9a\u504f\u597d\u7684\u8f6c\u6362\u6a21\u5f0f\u7684\u63d0\u793a\u673a\u5236\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8005\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMMPT-RAG\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5916\u90e8\u53c2\u8003\u7c7b\u6bd4\u4f5c\u4e3a\u60c5\u5883\u6307\u5bfc\uff0c\u5e2e\u52a9\u751f\u6210\u8fc7\u7a0b\u66f4\u52a0\u8d34\u5408\u7279\u5b9a\u9879\u76ee\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u901a\u7528\u5316\u5b66\u8bed\u6599\u5e93\u548c\u4e13\u5229\u7279\u5b9a\u6570\u636e\u96c6\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u63d0\u5347\u4ea7\u7269\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u4ee5\u53ca\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4e5f\u88ab\u8bc1\u660e\u80fd\u5728\u5b9e\u9645\u63a2\u7d22\u573a\u666f\u4e0b\u6709\u6548\u5730\u6062\u590d\u51fa\u63a5\u8fd1\u771f\u5b9e\u7684\u7c7b\u6bd4\u7ed3\u6784\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u53d8\u91cf\u5230\u53d8\u91cf\u7684\u7c7b\u6bd4\u751f\u6210\u65b9\u6cd5\u53ca\u914d\u5957\u7684MMPT-RAG\u6846\u67b6\uff0c\u672c\u7814\u7a76\u4e3a\u836f\u7269\u5316\u5b66\u5bb6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u9488\u5bf9\u6027\u7684\u8bbe\u8ba1\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u65b0\u836f\u5f00\u53d1\u8fc7\u7a0b\u3002"}}
{"id": "2602.16697", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.16697", "abs": "https://arxiv.org/abs/2602.16697", "authors": ["Aloni Cohen", "Refael Kohen", "Kobbi Nissim", "Uri Stemmer"], "title": "Protecting the Undeleted in Machine Unlearning", "comment": null, "summary": "Machine unlearning aims to remove specific data points from a trained model, often striving to emulate \"perfect retraining\", i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $\u03c9(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u673a\u5668\u9057\u5fd8\uff08machine unlearning\uff09\u8fc7\u7a0b\u4e2d\u6a21\u4eff\u201c\u5b8c\u7f8e\u518d\u8bad\u7ec3\u201d\u7684\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u5b9a\u4e49\u6765\u4fdd\u62a4\u672a\u5220\u9664\u6570\u636e\u514d\u53d7\u7531\u4e8e\u5176\u4ed6\u6570\u636e\u70b9\u7684\u5220\u9664\u800c\u9020\u6210\u7684\u6cc4\u9732\u3002", "motivation": "\u7814\u7a76\u8005\u4eec\u53d1\u73b0\uff0c\u5f53\u524d\u673a\u5668\u9057\u5fd8\u6280\u672f\u5728\u8bd5\u56fe\u6a21\u4eff\u2018\u5b8c\u7f8e\u518d\u8bad\u7ec3\u2019\u65f6\uff0c\u53ef\u80fd\u7ed9\u5269\u4f59\u7684\u6570\u636e\u70b9\u5e26\u6765\u4e25\u91cd\u7684\u9690\u79c1\u5a01\u80c1\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u4e14\u8ba9\u673a\u5668\u9057\u5fd8\u652f\u6301\u57fa\u672c\u529f\u80fd\u5982\u51c6\u786e\u6c42\u548c\u7b49\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u5b9a\u4e49\u3002", "method": "\u901a\u8fc7\u5c55\u793a\u4e00\u79cd\u91cd\u5efa\u653b\u51fb\uff0c\u8bc1\u660e\u5bf9\u4e8e\u67d0\u4e9b\u4efb\u52a1\u800c\u8a00\uff0c\u4ec5\u9700\u63a7\u5236\u5c11\u91cf\u6570\u636e\u70b9\u5e76\u53d1\u51fa\u5220\u9664\u8bf7\u6c42\uff0c\u5c31\u80fd\u51e0\u4e4e\u5b8c\u5168\u91cd\u5efa\u6574\u4e2a\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u5b9a\u4e49\u8fdb\u884c\u4e86\u8c03\u67e5\u5206\u6790\uff0c\u6307\u51fa\u73b0\u6709\u5b9a\u4e49\u8981\u4e48\u5bb9\u6613\u53d7\u5230\u6b64\u7c7b\u653b\u51fb\uff0c\u8981\u4e48\u8fc7\u4e8e\u4e25\u683c\u4ee5\u81f3\u4e8e\u65e0\u6cd5\u652f\u6301\u57fa\u7840\u529f\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u5b89\u5168\u5b9a\u4e49\uff0c\u80fd\u591f\u7279\u522b\u5730\u4fdd\u62a4\u672a\u88ab\u5220\u9664\u7684\u6570\u636e\u4e0d\u56e0\u5176\u5b83\u6570\u636e\u70b9\u7684\u5220\u9664\u800c\u53d1\u751f\u6cc4\u6f0f\u3002\u8be5\u5b9a\u4e49\u5141\u8bb8\u5b9e\u73b0\u5305\u62ec\u516c\u544a\u677f\u3001\u6c42\u548c\u4ee5\u53ca\u7edf\u8ba1\u5b66\u4e60\u5728\u5185\u7684\u51e0\u4e2a\u5173\u952e\u529f\u80fd\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u7684\u65b0\u5b89\u5168\u5b9a\u4e49\u4e3a\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u52a0\u5b89\u5168\u4e14\u5b9e\u7528\u7684\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u91cd\u8981\u529f\u80fd\u7684\u652f\u6301\u3002"}}
{"id": "2602.16698", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16698", "abs": "https://arxiv.org/abs/2602.16698", "authors": ["Shruti Joshi", "Aaron Mueller", "David Klindt", "Wieland Brendel", "Patrik Reizinger", "Dhanya Sridhar"], "title": "Causality is Key for Interpretability Claims to Generalise", "comment": null, "summary": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u5982\u7ed3\u679c\u4e0d\u5177\u666e\u904d\u6027\u3001\u56e0\u679c\u89e3\u91ca\u8d85\u51fa\u8bc1\u636e\u652f\u6301\u7b49\u3002\u901a\u8fc7\u5f15\u5165\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u7279\u522b\u662fPearl\u7684\u56e0\u679c\u5c42\u7ea7\u7406\u8bba\uff0c\u6587\u7ae0\u6307\u51fa\u89c2\u5bdf\u548c\u5e72\u9884\u53ef\u4ee5\u4e3a\u6a21\u578b\u5185\u90e8\u7ec4\u4ef6\u4e0e\u884c\u4e3a\u95f4\u7684\u5173\u8054\u63d0\u4f9b\u4e0d\u540c\u7a0b\u5ea6\u7684\u652f\u6301\uff0c\u4f46\u53cd\u4e8b\u5b9e\u58f0\u660e\u4ecd\u96be\u4ee5\u9a8c\u8bc1\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\u8bba\u6765\u6307\u5bfc\u5b9e\u8df5\u8005\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u548c\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u7814\u7a76\u53d1\u73b0\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u6bd4\u5982\u7814\u7a76\u53d1\u73b0\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u5bf9\u6a21\u578b\u884c\u4e3a\u80cc\u540e\u7684\u56e0\u679c\u5173\u7cfb\u505a\u51fa\u8fc7\u5ea6\u89e3\u8bfb\u7b49\u95ee\u9898\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u7814\u7a76\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u8005\u8ba4\u4e3a\u6709\u5fc5\u8981\u91c7\u7528\u4e00\u79cd\u66f4\u52a0\u4e25\u8c28\u7684\u65b9\u6cd5\u8bba\u2014\u2014\u5373\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u9996\u5148\u4ecb\u7ecd\u4e86\u56e0\u679c\u63a8\u65ad\u7684\u57fa\u672c\u6982\u5ff5\u53ca\u5176\u5728\u7406\u89e3\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\uff1b\u63a5\u7740\u8be6\u7ec6\u9610\u8ff0\u4e86\u5982\u4f55\u5229\u7528Pearl\u63d0\u51fa\u7684\u56e0\u679c\u5c42\u7ea7\u7406\u8bba\u6765\u5b9a\u4e49\u4ece\u6a21\u578b\u6fc0\u6d3b\u5230\u4e0d\u53d8\u9ad8\u5c42\u6b21\u7ed3\u6784\u7684\u6709\u6548\u6620\u5c04\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u73b0\u8fd9\u79cd\u6620\u5c04\u6240\u9700\u7684\u6570\u636e\u6216\u5047\u8bbe\u6761\u4ef6\uff1b\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u6839\u636e\u5176\u7814\u7a76\u76ee\u7684\u9009\u62e9\u9002\u5f53\u7684\u6280\u672f\u624b\u6bb5\u6765\u8fdb\u884c\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5c06\u56e0\u679c\u63a8\u7406\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u4e2d\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u6a21\u578b\u5185\u90e8\u673a\u5236\u4e0e\u5176\u8868\u73b0\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u5177\u4f53\u5b9e\u4f8b\uff0c\u8bf4\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5982\u4f55\u5e2e\u52a9\u8bc6\u522b\u53ef\u4ee5\u4ece\u6fc0\u6d3b\u4e2d\u6062\u590d\u54ea\u4e9b\u53d8\u91cf\uff0c\u5e76\u4e14\u660e\u786e\u4e86\u8fd9\u6837\u505a\u9700\u8981\u6ee1\u8db3\u7684\u524d\u63d0\u6761\u4ef6\u3002", "conclusion": "\u91c7\u7528\u56e0\u679c\u63a8\u65ad\u7684\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u4f7f\u5176\u7ed3\u8bba\u66f4\u52a0\u7a33\u5065\u5e76\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u9075\u5faa\u6587\u4e2d\u63d0\u51fa\u7684\u6307\u5357\uff0c\u5728\u8bbe\u8ba1\u5b9e\u9a8c\u65f6\u8003\u8651\u56e0\u679c\u5c42\u7ea7\u7684\u4e0d\u540c\u5c42\u9762\uff0c\u4ece\u800c\u4fc3\u8fdb\u7814\u7a76\u53d1\u73b0\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.16709", "categories": ["cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.16709", "abs": "https://arxiv.org/abs/2602.16709", "authors": ["Weijing Tang", "Ming Yuan", "Zongqi Xia", "Tianxi Cai"], "title": "Knowledge-Embedded Latent Projection for Robust Representation Learning", "comment": null, "summary": "Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5916\u90e8\u8bed\u4e49\u5d4c\u5165\u6765\u89c4\u8303\u8868\u793a\u5b66\u4e60\u7684\u77e5\u8bc6\u5d4c\u5165\u6f5c\u5728\u6295\u5f71\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5217\u5d4c\u5165\u5efa\u6a21\u4e3a\u8bed\u4e49\u5d4c\u5165\u7684\u5e73\u6ed1\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6838\u4e3b\u6210\u5206\u5206\u6790\u7684\u8bed\u4e49\u5f15\u5bfc\u5b50\u7a7a\u95f4\u6784\u5efa\u548c\u53ef\u6269\u5c55\u7684\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff0c\u4ee5\u89e3\u51b3\u5728\u4e0d\u5e73\u8861\u72b6\u6001\u4e0b\u9ad8\u7ef4\u79bb\u6563\u6570\u636e\u77e9\u9635\uff08\u5982\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff09\u7684\u4f30\u8ba1\u96be\u9898\u3002", "motivation": "\u5728\u5904\u7406\u50cf\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8fd9\u6837\u7684\u9ad8\u7ef4\u79bb\u6563\u6570\u636e\u77e9\u9635\u65f6\uff0c\u5f53\u4e00\u4e2a\u7ef4\u5ea6\u8fdc\u5927\u4e8e\u53e6\u4e00\u4e2a\uff08\u6bd4\u5982\u7279\u5f81\u7a7a\u95f4\u975e\u5e38\u5927\u800c\u6837\u672c\u91cf\u53d7\u9650\uff09\uff0c\u4f20\u7edf\u7684\u6f5c\u7a7a\u95f4\u6a21\u578b\u4f30\u8ba1\u53d8\u5f97\u56f0\u96be\u3002\u53d7\u9884\u8bad\u7ec3\u4e34\u5e8a\u6982\u5ff5\u5d4c\u5165\u7b49\u5916\u90e8\u8bed\u4e49\u5d4c\u5165\u65e5\u76ca\u53ef\u7528\u6027\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5229\u7528\u8fd9\u4e9b\u989d\u5916\u4fe1\u606f\u6765\u6539\u5584\u8868\u5f81\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u6620\u5c04\uff0c\u5c06\u5217\u5d4c\u5165\u89c6\u4e3a\u8bed\u4e49\u5d4c\u5165\u7684\u5149\u6ed1\u51fd\u6570\u8fdb\u884c\u5efa\u6a21\u3002\u91c7\u7528\u4e24\u6b65\u4f30\u8ba1\u7a0b\u5e8f\uff1a\u9996\u5148\u4f7f\u7528\u57fa\u4e8e\u6838\u7684\u4e3b\u6210\u5206\u5206\u6790\u521b\u5efa\u7531\u8bed\u4e49\u6307\u5bfc\u7684\u5b50\u7a7a\u95f4\uff1b\u7136\u540e\u8fd0\u7528\u53ef\u6269\u5c55\u7684\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u975e\u51f8\u4f18\u5316\u8fc7\u7a0b\u7684\u5c40\u90e8\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u4e0a\uff0c\u7ed9\u51fa\u4e86\u4f30\u8ba1\u8bef\u5dee\u754c\u9650\uff0c\u660e\u786e\u4e86\u7edf\u8ba1\u8bef\u5dee\u4e0e\u6838\u6295\u5f71\u5bfc\u81f4\u7684\u8fd1\u4f3c\u8bef\u5dee\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5b9e\u8df5\u5e94\u7528\u4e2d\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u7814\u7a76\u53ca\u771f\u5b9e\u4e16\u754cEHR\u6848\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5728\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u9ad8\u7ef4\u79bb\u6563\u6570\u636e\u77e9\u9635\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u5927\u91cf\u7279\u5f81\u4f46\u6837\u672c\u6570\u6709\u9650\u7684\u5e94\u7528\u573a\u666f\u800c\u8a00\uff0c\u5229\u7528\u5916\u90e8\u8bed\u4e49\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002"}}
