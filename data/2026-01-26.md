<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 37]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple](https://arxiv.org/abs/2601.16294)
*Evangelos Georganas,Alexander Heinecke,Pradeep Dubey*

Main category: cs.DC

TL;DR: 本研究利用广义空间填充曲线（如广义希尔伯特曲线）来分区矩阵乘法计算空间，实现了对平台和矩阵形状不敏感的矩阵乘法方案，并通过结合通信避免算法复制输入张量以最小化关键路径上的通信/数据移动，从而在多种CPU平台上超越了现有供应商库的表现，对于一系列GEMM形状最高可达到2倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 针对现代CPU平台上矩阵引擎所面临的数据移动挑战以及当前优化方法需要大量手动调优的问题，提出了一种基于空间填充曲线的新方法来提高矩阵乘法性能并减少调优工作。

Method: 采用广义的空间填充曲线技术将多维坐标转换为一维坐标保持高维空间中相邻点在一维顺序中的接近性；利用该技术分区矩阵乘法计算空间；扩展基于SFC的工作分区以实现通信避免算法，复制输入张量并在关键路径上最小化通信/数据移动。

Result: 开发出一种与平台无关、形状无关的矩阵乘法方案，具有高度的数据局部性；结合通信避免算法后能够产生紧凑代码（约30行），并且在多种CPU平台上针对不同GEMM形状实现了比供应商库高达2倍几何平均速度提升的结果。

Conclusion: 通过应用广义空间填充曲线及通信避免算法于矩阵乘法中，可以有效提高数据局部性和减少跨内存层次的数据移动，进而显著提升各种CPU平台上的GEMM执行效率。

Abstract: General Matrix Multiplication (GEMM) is the cornerstone of Deep Learning and HPC workloads; accordingly, academia and industry have heavily optimized this kernel. Modern platforms with matrix multiplication accelerators exhibit high FLOP/Byte machine balance, which makes implementing optimal matrix multiplication challenging. On modern CPU platforms with matrix engines, state-of-the-art vendor libraries tune input tensor layouts, parallelization schemes, and cache blocking to minimize data movement across the memory hierarchy and maximize throughput. However, the best settings for these parameters depend strongly on the target platform (number of cores, memory hierarchy, cache sizes) and on the shapes of the matrices, making exhaustive tuning infeasible; in practice this leads to performance "glass jaws". In this work we revisit space filling curves (SFC) to alleviate the problem of this cumbersome tuning. SFC convert multi-dimensional coordinates (e.g. 2D) into a single dimension (1D), keeping nearby points in the high-dimensional space close in the 1D order. We partition the Matrix Multiplication computation space using recent advancements in generalized SFC (Generalized Hilbert Curves), and we obtain platform-oblivious and shape-oblivious matrix-multiplication schemes that exhibit inherently high degree of data locality. Furthermore, we extend the SFC-based work partitioning to implement Communication-Avoiding (CA) algorithms that replicate the input tensors and provably minimize communication/data-movement on the critical path. The integration of CA-algorithms is seamless and yields compact code (~30 LOC), yet it achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x(geometric-mean speedup) for a range of GEMM shapes.

</details>


### [2] [Consensus In Asynchrony](https://arxiv.org/abs/2601.16460)
*Ivan Klianev*

Main category: cs.DC

TL;DR: 本研究展示了基于事件的同步机制在异步环境中解决确定性容错共识问题的有效性。提出了一种能够保证安全性、活跃性和容忍单点故障的算法。此外，还探讨了FLP不可能性结果背后的隐含假设，并通过实验表明其中一个假设缺乏实证支持。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索和证明基于事件的同步方法是否足以在存在故障的情况下于异步系统中达成共识，同时对著名的FLP不可能性定理进行深入分析以揭示其背后依赖的前提条件。

Method: 设计并实现了一个新的算法来达成向量一致性的共识，该算法能够在确保安全性和活跃性的同时容忍一次崩溃。通过对FLP不可能性结果进行分析，明确了两种类型的协议（数据独立型与数据依赖型）以及FLP定理正确性所依据的三个隐含假设。

Result: 开发出的算法被证实可以成功地在异步条件下达成共识，同时满足安全性、活跃性要求及对单次故障的容忍度。研究还发现，FLP不可能性对于数据依赖型协议而言取决于其中两个假设；而对于所有类型协议来说，则完全依赖于第三个假设。实验结果显示，这最后一个假设实际上没有得到证据支持。

Conclusion: 基于事件的同步确实为解决异步环境下的确定性容错共识问题提供了一条可行路径。本工作不仅贡献了一个实用算法，也促进了对FLP不可能性理论更深层次的理解。

Abstract: We demonstrate sufficiency of events-based synchronisation for solving deterministic fault-tolerant consensus in asynchrony. Main result is an algorithm that terminates with valid vector agreement, hence operates with safety, liveness, and tolerance to one crash. Reconciling with the FLP impossibility result, we identified: i) existence of two types of agreements: data-independent and data-dependent; and ii) dependence of FLP theorem correctness on three implicit assumptions. Consensus impossibility with data-dependent agreement is contingent on two of them. The theorem-stated impossibility with every agreement type hinges entirely on the third. We provide experimental results showing that the third assumption has no evidence in support.

</details>


### [3] [W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs](https://arxiv.org/abs/2601.16536)
*Yuanhong He,Peiyu Niu,Jun Chen,Chenchen Zhang,Chao Yang*

Main category: cs.DC

TL;DR: 本文介绍了针对华为Ascend 910 NPU设计的首个实用W4A16矩阵乘法内核，通过利用向量核心进行INT4到FP16的即时反量化、立方体核心进行高吞吐量GEMM以及Split-K并行化来减少内存延迟。实验表明，在K远大于N的情况下，该方法比数据并行方法性能更优，速度提升范围从1.01倍至1.74倍。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）规模的增长，权重量化（W4A16：4位权重，16位激活值）对于在最小精度损失下减少内存占用变得至关重要。然而，由于华为Ascend 910神经处理单元(NPU)有限的原生混合精度支持及解耦计算架构的特点，其高效部署面临挑战。

Method: 研究者们提出了专为Ascend 910 NPU定制的第一个实用W4A16矩阵乘法内核。该设计方案利用了向量核心实现INT4到FP16的即时反量化过程，使用立方体核心完成高吞吐量的通用矩阵乘法(GEMM)，并通过采用Split-K并行化策略减轻了内存延迟的影响。

Result: 不同矩阵形状和批量大小下的性能评估显示，当K远大于N时，本方法相比数据并行方法表现出更好的性能，能够实现从1.01倍到1.74倍的速度提升。此外，分析还指出主要瓶颈并非反量化计算本身，而是额外的全局内存传输导致的，使得W4A16相对于PyTorch中原生FP16x FP16矩阵乘法的最大提速仅为1.48倍。

Conclusion: 长期来看，这项工作不仅为在各种领域特定加速器上高效部署量化后的大型语言模型奠定了坚实基础，而且提供了宝贵的见解。

Abstract: As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture. To enable quantization on such architecture, we present the first practical W4A16 matrix multiplication kernel tailored for the Ascend 910 NPU. Our design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Performance evaluations across diverse matrix shapes and batch sizes show our method outperforms data-parallel approaches when K >> N, a typical scenario in LLM decoding. Specially, our method can achieve a speedup ranging from 1.01x to 1.74x. In addition, our profile reveals the primary bottleneck is not dequantization compution itself, but extra global memory transfer for the weight, making W4A16 only reaching a maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch. In the long run, our method lays a solid foundation and provides insightful views for the efficient deployment of quantized large language models on various domain-specific accelerators.

</details>


### [4] [GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms](https://arxiv.org/abs/2601.16637)
*Jun Doi,Tomonori Shirakawa,Yukio Kawashima,Seiji Yunoki,Hiroshi Horii*

Main category: cs.DC

TL;DR: 本文提出了一种基于Thrust库的GPU加速实现方法，用于样本量子对角化(SQD)中的选基对角化(SBD)。通过重组关键组件，并利用现代GPU架构，该方法相较于CPU执行实现了高达约40倍的速度提升，显著减少了SQD迭代的总运行时间。


<details>
  <summary>Details</summary>
Motivation: 在样本量子对角化(SQD)过程中，选基对角化(SBD)是形成主要经典工作负载的核心部分。为了提高SBD的计算效率，作者们旨在利用GPU架构的优势来加速这一过程。

Method: 研究者使用Thrust库开发了SBD的一种GPU加速实现方案。他们围绕细粒度的数据并行原语和扁平化的GPU友好数据布局重新构建了包括配置处理、激发生成以及矩阵-向量操作在内的关键组件。

Result: 实验结果显示，基于Thrust的SBD相比于CPU执行能够达到大约40倍的速度提升，并且大幅度降低了SQD迭代所需的总运行时长。

Conclusion: 结果表明，GPU原生的并行原语为加速基于SQD的量子-经典工作流提供了一个简单、可移植且高性能的基础。

Abstract: Selected Basis Diagonalization (SBD) plays a central role in Sample-based Quantum Diagonalization (SQD), where iterative diagonalization of the Hamiltonian in selected configuration subspaces forms the dominant classical workload. We present a GPU-accelerated implementation of SBD using the Thrust library. By restructuring key components -- including configuration processing, excitation generation, and matrix-vector operations -- around fine-grained data-parallel primitives and flattened GPU-friendly data layouts, the proposed approach efficiently exploits modern GPU architectures. In our experiments, the Thrust-based SBD achieves up to $\sim$40$\times$ speedup over CPU execution and substantially reduces the total runtime of SQD iterations. These results demonstrate that GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows.

</details>


### [5] [DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers](https://arxiv.org/abs/2601.16956)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: DataStates-LLM, a new checkpointing architecture, optimizes the process for large language models by using State Providers to decouple state abstraction from data movement, leading to up to 4x higher checkpointing throughput and up to 2.2x reduction in end-to-end training time on 70B parameter models across 256 A100-40GB GPUs.


<details>
  <summary>Details</summary>
Motivation: The need to train Large Language Models (LLMs) with trillions of parameters across thousands of GPUs using complex parallelism strategies has created a demand for efficient checkpointing solutions that can handle the '3D heterogeneity' of distributed model states without causing significant runtime overheads.

Method: The paper introduces DataStates-LLM, which utilizes State Providers to separate state abstraction from data movement. It leverages the immutability of model parameters during forward and backward passes to create non-blocking asynchronous snapshots, and it efficiently combines fragmented, heterogeneous shards while overlapping metadata serialization with bulk tensor I/O.

Result: Evaluation on models with up to 70 billion parameters running on 256 A100-40GB GPUs showed that DataStates-LLM could achieve up to 4 times higher checkpointing throughput and reduce the overall training time by up to 2.2 times when compared to existing state-of-the-art solutions.

Conclusion: DataStates-LLM effectively addresses the challenges of checkpointing for extremely large-scale LLMs, significantly improving both the throughput of checkpointing and reducing the total training time, thereby mitigating the bottlenecks caused by serialization and data heterogeneity.

Abstract: The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Identifying Concurrency Bug Reports via Linguistic Patterns](https://arxiv.org/abs/2601.16338)
*Shuai Shao,Lu Xiao,Tingting Yu*

Main category: cs.SE

TL;DR: 本文提出了一种基于语言模式的框架，用于自动识别并发错误报告。通过从730份手动标记的并发错误报告中提取58个不同的语言模式，并采用四种互补的方法（匹配、学习、基于提示和微调）进行评估。研究结果表明，在预训练语言模型中加入语言模式丰富的输入并进行微调可以达到最佳性能，GitHub上的精确度达到91%，Jira上达到93%。


<details>
  <summary>Details</summary>
Motivation: 随着多核架构越来越普遍，同时系统变得至关重要但更易于遇到诸如数据竞争和死锁等复杂问题。虽然现代的问题跟踪系统简化了这些问题的报告过程，但是标记与并发相关的错误报告仍然是一项劳动密集且容易出错的任务。因此，需要一种自动化的方法来提高这一过程的效率和准确性。

Method: 研究者们从730份人工标注的并发bug报告中提炼出了覆盖四个层次（词汇级、短语级、句子级及报告级）的58种不同语言模式。为了测试这些模式的有效性，他们采用了四种补充方法——匹配、学习、基于提示以及微调，后者包括了传统机器学习技术、大型语言模型（LLMs）、以及预训练的语言模型（PLMs）。

Result: 综合评估结果显示，利用富含语言模式的信息对预训练语言模型进行微调能够取得最佳效果，具体表现为在GitHub平台上的准确率达到91%，而在Jira平台上则达到了93%，并且对于截止日期后的数据依旧保持了很高的准确率（91%）。

Conclusion: 这项工作的贡献在于：(1) 对并发错误的语言模式进行了全面分类；(2) 提出了一种新的微调策略，将领域特定的语言知识整合到预训练语言模型中；(3) 提供了一个经过精心整理和标注的数据集以支持可重复的研究。这些进步共同为提高并发错误分类的自动化程度、精度和可解释性奠定了基础。

Abstract: With the growing ubiquity of multi-core architectures, concurrent systems have become essential but increasingly prone to complex issues such as data races and deadlocks. While modern issue-tracking systems facilitate the reporting of such problems, labeling concurrency-related bug reports remains a labor-intensive and error-prone task. This paper presents a linguistic-pattern-based framework for automatically identifying concurrency bug reports. We derive 58 distinct linguistic patterns from 730 manually labeled concurrency bug reports, organized across four levels: word-level (keywords), phrase-level (n-grams), sentence-level (semantic), and bug report-level (contextual). To assess their effectiveness, we evaluate four complementary approaches-matching, learning, prompt-based, and fine-tuning-spanning traditional machine learning, large language models (LLMs), and pre-trained language models (PLMs). Our comprehensive evaluation on 12 large-scale open-source projects (10,920 issue reports from GitHub and Jira) demonstrates that fine-tuning PLMs with linguistic-pattern-enriched inputs achieves the best performance, reaching a precision of 91% on GitHub and 93% on Jira, and maintaining strong precision on post cut-off data (91%). The contributions of this work include: (1) a comprehensive taxonomy of linguistic patterns for concurrency bugs, (2) a novel fine-tuning strategy that integrates domain-specific linguistic knowledge into PLMs, and (3) a curated, labeled dataset to support reproducible research. Together, these advances provide a foundation for improving the automation, precision, and interpretability of concurrency bug classification.

</details>


### [7] [SE Research is a Complex Ecosystem: Isolated Fixes Keep Failing -- and Systems Thinking Shows Why](https://arxiv.org/abs/2601.16363)
*Mary Shaw,Mary Lou Maher,Keith Webster*

Main category: cs.SE

TL;DR: The paper discusses the challenges faced by the software engineering research community, including overburdened review processes, metric-driven incentives, and publication practices. It proposes a holistic system-level approach to address these issues, using concepts from complex systems, ecosystems, and theory of change to identify leverage points for reform.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the constellation of challenges within the software engineering (SE) research ecosystem, which are often treated in isolation, through a more integrated and systemic perspective that considers the deeper structural dynamics and societal role of research.

Method: The authors employ a framework based on ideas from complex systems, ecosystems, and theory of change to reframe the SE's challenges, aiming to uncover non-linear feedback loops that perpetuate current dysfunctions and to pinpoint leverage points for initiating effective reforms.

Result: By adopting a holistic view, the analysis uncovers interconnected issues within the SE research community and suggests that meaningful progress can be achieved by implementing coordinated sets of solutions across the entire SE ecosystem, rather than isolated fixes.

Conclusion: To make significant advancements in addressing the challenges of the SE research community, it is essential to adopt a comprehensive and systemic approach that targets multiple aspects of the research ecosystem simultaneously, fostering a healthier and more sustainable environment.

Abstract: The software engineering research community is productive, yet it faces a constellation of challenges: swamped review processes, metric-driven incentives, distorted publication practices, and increasing pressures from AI, scale, and outright scams. These issues are often treated in isolation, yet they arise from deep structural dynamics within the research ecosystem itself and distract us from the larger role of research in society. Meaningful progress requires a holistic system-level view. We sketch such a framework drawing on ideas from complex systems, ecosystems, and theory of change. Reframing SE's challenges through this lens reveals non-linear feedback loops that sustain current dysfunctions, and it helps to identify leverage points for reform. These are less a matter of isolated fixes and more a matter of exploring coordinated sets of fixes that operate across the SE ecosystem

</details>


### [8] [RubberDuckBench: A Benchmark for AI Coding Assistants](https://arxiv.org/abs/2601.16456)
*Ferida Mohammad,Fatma Ayad,Petros Maniatis,Satish Chandra,Elizabeth Dinella*

Main category: cs.SE

TL;DR: 本文介绍了一个新的基准测试集RubberDuckBench，用于评估AI编程助手对代码相关问题的回答能力。通过对比20种不同语言模型的表现，发现即使是顶尖模型也难以给出一致且正确的答案，并且模型成本与性能之间没有直接关联。


<details>
  <summary>Details</summary>
Motivation: 程序员们正在转向使用AI编码助手来解答关于他们代码的问题。为了合理地评估这些系统并理解其性能，需要建立一个基准。

Method: 研究者从Github拉取请求评论中提取了实际情境中的问题，创建了一个多语言的代码相关问题基准——RubberDuckBench，并制定了详细的评分标准。接着，他们用这个基准评估了20个不同的大型语言模型（包括专有和开源）回答这些问题的能力。

Result: 结果显示，即使是目前最先进的模型也无法在基准上始终如一地提供正确答案。Grok 4、Claude Opus 4 和 GPT-5 的表现最好，但并没有显著优于其他九个最佳模型。大多数模型仅靠部分得分，最好的模型也只能完全正确回答最多两个问题。此外，平均而言模型在58.3%的回答中有编造事实的现象。成本分析表明，费用（API定价或参数数量）与性能之间没有关联。

Conclusion: 本研究表明当前AI编码助手在准确性和一致性方面存在不足，而RubberDuckBench旨在成为未来研究可靠和正确AI编码助手的目标。

Abstract: Programmers are turning to AI coding assistants to answer questions about their code. Benchmarks are needed to soundly evaluate these systems and understand their performance. To enable such a study, we curate a benchmark of real-world contextualized questions derived from Github pull request comments. Out of this work, we present RubberDuckBench: a multilingual benchmark of questions about code, along with detailed rubrics for evaluating answers. We evaluate a diverse set of 20 LLMs (proprietary & open-source) on answering these questions. We find that even state of the art models fail to give consistent, correct responses across the benchmark. Grok 4 (69.29%), Claude Opus 4 (68.5%), and GPT-5 (67.8%) perform best overall, but do not exhibit pairwise significant superiority over the next 9 best performing models. Most models obtain points through partial credit, with the best performing models only answering at most 2 questions completely correctly across all trials. Furthermore, models often hallucinate with lies in 58.3\% of responses on average. Cost analysis reveals no correlation between expense (API pricing or parameter count) and performance. We intend this benchmark to be a target for future research in trustworthy and correct AI coding assistants.

</details>


### [9] [Bridging Expert Reasoning and LLM Detection: A Knowledge-Driven Framework for Malicious Packages](https://arxiv.org/abs/2601.16458)
*Wenbo Guo,Shiwen Song,Jiaxun Guo,Zhengzi Xu,Chengwei Liu,Haoran Ou,Mengmeng Ge,Yang Liu*

Main category: cs.SE

TL;DR: IntelGuard, a system that uses retrieval-augmented generation and expert reasoning for detecting malicious packages in open-source ecosystems, has been shown to achieve high accuracy with a low false positive rate, and has successfully identified previously undetected malicious packages on PyPI.org.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to address the increasing threat of supply chain attacks within open-source ecosystems such as NPM and PyPI, where current detection methods are either based on easily bypassed handcrafted rules or data-driven features that do not adapt to the changing tactics of attackers.

Method: IntelGuard utilizes a retrieval-augmented generation (RAG) framework that incorporates a structured knowledge base built from a large number of threat intelligence reports. It analyzes new packages by finding semantically similar malicious code examples and employs language model-guided reasoning to determine if the package's behavior matches its stated functionality.

Result: IntelGuard demonstrated 99% accuracy and a 0.50% false positive rate when tested on 4,027 real-world packages. It also maintained 96.5% accuracy on obfuscated code. Upon deployment, it uncovered 54 previously unreported malicious packages on PyPI.org.

Conclusion: The IntelGuard system effectively enhances the detection of malicious packages through the integration of expert analytical reasoning and RAG, providing a robust and interpretable solution against evolving supply chain attacks.

Abstract: Open-source ecosystems such as NPM and PyPI are increasingly targeted by supply chain attacks, yet existing detection methods either depend on fragile handcrafted rules or data-driven features that fail to capture evolving attack semantics. We present IntelGuard, a retrieval-augmented generation (RAG) based framework that integrates expert analytical reasoning into automated malicious package detection. IntelGuard constructs a structured knowledge base from over 8,000 threat intelligence reports, linking malicious code snippets with behavioral descriptions and expert reasoning. When analyzing new packages, it retrieves semantically similar malicious examples and applies LLM-guided reasoning to assess whether code behaviors align with intended functionality. Experiments on 4,027 real-world packages show that IntelGuard achieves 99% accuracy and a 0.50% false positive rate, while maintaining 96.5% accuracy on obfuscated code. Deployed on PyPI.org, it discovered 54 previously unreported malicious packages, demonstrating interpretable and robust detection guided by expert knowledge.

</details>


### [10] [Revisiting the Role of Natural Language Code Comments in Code Translation](https://arxiv.org/abs/2601.16661)
*Monika Gupta,Ajay Meena,Anamitra Roy Choudhury,Vijay Arya,Srikanta Bedathur*

Main category: cs.SE

TL;DR: 本研究通过大规模实证分析，评估了代码注释对编程语言之间代码翻译准确性的影响，结果表明描述代码整体目的的注释显著提高了翻译质量。基于此，提出了COMMENTRA方法，能够极大提升基于大型语言模型的代码翻译性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，自动化的编程语言间代码转换进入了新时代。然而，现有的代码翻译基准中很少包含有助于提高翻译质量的自然语言代码注释。因此，这项研究旨在全面探索代码注释对不同编程语言之间翻译质量的具体影响。

Method: 研究采用了超过80,000次翻译实验，涉及来自两个不同基准测试的1100多个代码样本，并涵盖C、C++、Go、Java和Python五种语言之间的成对翻译。通过对有无注释情况下的翻译进行对比分析来评估注释的作用。

Result: 研究发现，尤其是那些描述整个代码目的而非逐行功能的注释，能显著提高翻译准确性。此外，提出了一种名为COMMENTRA的新代码翻译方法，展示了其在基于LLM的代码翻译方面具有将性能翻倍的潜力。

Conclusion: 该研究表明，利用描述性的代码注释可以显著改善跨编程语言的代码翻译质量。所提出的COMMENTRA方法为如何有效利用这些注释以增强基于大型语言模型的代码翻译提供了一个新的视角。

Abstract: The advent of large language models (LLMs) has ushered in a new era in automated code translation across programming languages. Since most code-specific LLMs are pretrained on well-commented code from large repositories like GitHub, it is reasonable to hypothesize that natural language code comments could aid in improving translation quality. Despite their potential relevance, comments are largely absent from existing code translation benchmarks, rendering their impact on translation quality inadequately characterised. In this paper, we present a large-scale empirical study evaluating the impact of comments on translation performance. Our analysis involves more than $80,000$ translations, with and without comments, of $1100+$ code samples from two distinct benchmarks covering pairwise translations between five different programming languages: C, C++, Go, Java, and Python. Our results provide strong evidence that code comments, particularly those that describe the overall purpose of the code rather than line-by-line functionality, significantly enhance translation accuracy. Based on these findings, we propose COMMENTRA, a code translation approach, and demonstrate that it can potentially double the performance of LLM-based code translation. To the best of our knowledge, our study is the first in terms of its comprehensiveness, scale, and language coverage on how to improve code translation accuracy using code comments.

</details>


### [11] [The Green Side of the Lua](https://arxiv.org/abs/2601.16670)
*André Brandão,Diogo Matos,Miguel Guimarães,Simão Cunha,João Saraiva*

Main category: cs.SE

TL;DR: 本文通过对25个官方Lua解释器版本和即时编译器的运行性能和能源效率进行实证研究，发现所有LuaJIT编译器在执行时间和能耗方面都显著优于标准Lua解释器，并且接近C语言的效率。


<details>
  <summary>Details</summary>
Motivation: 为了响应联合国2030年可持续发展议程中关于减少全球碳足迹的目标，研究者们着眼于通过提高编程语言如Lua的能源效率来降低软件能耗。鉴于解释型语言通常比编译型语言更耗能，本研究旨在探索即时（JIT）编译技术对改善Lua语言性能及能效的作用。

Method: 研究采用了涵盖25个不同版本Lua官方解释器以及即时编译器的大规模基准测试套件，通过测量执行时间和能耗来分析Lua的发展历程、JIT编译的影响以及其他语言间的比较。

Result: 结果表明，所有LuaJIT编译器在执行时间和能耗上均大幅优于标准Lua解释器；最高效的LuaJIT相比最佳Lua解释器大约减少了七倍的能耗并快了七倍。此外，与C语言相比，LuaJIT虽然能耗高出约六倍且运行速度慢八倍左右，但依然展示了即时编译对于提升解释型语言性能和能效的巨大潜力。

Conclusion: 研究表明，采用即时编译技术可以极大地提高像Lua这样的解释型语言的性能和能源效率，使其更加接近于编译型语言如C的表现，从而为实现更加绿色节能的软件开发提供了新的方向。

Abstract: The United Nations' 2030 Agenda for Sustainable Development highlights the importance of energy-efficient software to reduce the global carbon footprint. Programming languages and execution models strongly influence software energy consumption, with interpreted languages generally being less efficient than compiled ones. Lua illustrates this trade-off: despite its popularity, it is less energy-efficient than greener and faster languages such as C.
  This paper presents an empirical study of Lua's runtime performance and energy efficiency across 25 official interpreter versions and just-in-time (JIT) compilers. Using a comprehensive benchmark suite, we measure execution time and energy consumption to analyze Lua's evolution, the impact of JIT compilation, and comparisons with other languages. Results show that all LuaJIT compilers significantly outperform standard Lua interpreters. The most efficient LuaJIT consumes about seven times less energy and runs seven times faster than the best Lua interpreter. Moreover, LuaJIT approaches C's efficiency, using roughly six times more energy and running about eight times slower, demonstrating the substantial benefits of JIT compilation for improving both performance and energy efficiency in interpreted languages.

</details>


### [12] [Supporting Stakeholder Requirements Expression with LLM Revisions: An Empirical Evaluation](https://arxiv.org/abs/2601.16699)
*Michael Mircea,Emre Gevrek,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLMs）在需求表达中的辅助作用，发现参与者对LLM修订后的需求陈述评价更高，认为其在意图一致性、可读性、逻辑性和明确性上都有显著提升。此外，这种以利益相关者为中心的方法有助于提高需求的完整性、清晰度和一致性，并促进AI在需求工程中的负责任使用。


<details>
  <summary>Details</summary>
Motivation: 利益相关者由于领域知识有限或认知限制，在准确表达需求时遇到障碍，导致表述与实际意图之间的不一致，使得需求获取和验证变得复杂。传统的需求获取技术耗时且存在迭代过程中扭曲原始意图的风险。研究旨在探索大型语言模型是否能有效支持需求表达，并对领域知识有限的利益相关者提供帮助。

Method: 通过一项包括26名参与者的研究，总共生成了130个需求陈述。每位参与者首先独立表达需求，随后评估根据其上下文定制的LLM生成的修订版本。

Result: 参与者普遍认为LLM修订后的陈述在所有维度上——意图一致性、可读性、逻辑性和明确性——都优于他们最初的表述。定性反馈进一步表明，LLM修订常常揭示出参与者认为重要的隐含细节，并帮助他们更好地理解自己的需求。

Conclusion: 利用LLM作为表达助手的需求中心方法，在需求获取和验证中展现出改善需求感知完整度、清晰度以及一致性方面的潜力。这种方法通过将利益相关者保留在验证循环中，促进了AI在需求工程领域的负责任及可信使用。

Abstract: Stakeholders often struggle to accurately express their requirements due to articulation barriers arising from limited domain knowledge or from cognitive constraints. This can cause misalignment between expressed and intended requirements, complicating elicitation and validation. Traditional elicitation techniques, such as interviews and follow-up sessions, are time-consuming and risk distorting stakeholders' original intent across iterations. Large Language Models (LLMs) can infer user intentions from context, suggesting potential for assisting stakeholders in expressing their needs. This raises the questions of (i) how effectively LLMs can support requirement expression and (ii) whether such support benefits stakeholders with limited domain expertise. We conducted a study with 26 participants who produced 130 requirement statements. Each participant first expressed requirements unaided, then evaluated LLM-generated revisions tailored to their context. Participants rated LLM revisions significantly higher than their original statements across all dimensions-alignment with intent, readability, reasoning, and unambiguity. Qualitative feedback further showed that LLM revisions often surfaced tacit details stakeholders considered important and helped them better understand their own requirements. We present and evaluate a stakeholder-centered approach that leverages LLMs as articulation aids in requirements elicitation and validation. Our results show that LLM-assisted reformulation improves perceived completeness, clarity, and alignment of requirements. By keeping stakeholders in the validation loop, this approach promotes responsible and trustworthy use of AI in Requirements Engineering.

</details>


### [13] [Adoption of Generative Artificial Intelligence in the German Software Engineering Industry: An Empirical Study](https://arxiv.org/abs/2601.16700)
*Ludwig Felder,Tobias Eisenreich,Mahsa Fischer,Stefan Wagner,Chunyang Chen*

Main category: cs.SE

TL;DR: 本研究通过18次探索性访谈和对109名参与者的调查，首次系统地探讨了德国软件工程师在严格的监管要求下对生成式人工智能（GenAI）工具的采用情况。研究表明，开发者的经验水平影响着他们对GenAI工具好处的看法，而生产力提升并不均匀分布于所有开发者之中。此外，组织规模也会影响工具的选择及使用强度。项目背景意识不足被认为是最大的障碍。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式人工智能（GenAI）工具在软件开发者中迅速普及，特别是在有着严格监管要求如德国这样的环境中，但对于这些工具有效使用的背后因素，包括交互深度、组织约束以及经验考量等，并未得到充分研究。因此，有必要填补这一空白，特别是针对德国背景下GenAI工具采纳动态进行系统性的实证研究。

Method: 采用了混合方法研究设计，首先进行了18次与实践者的探索性访谈以获取深入见解，随后实施了一项涉及109名参与者的开发者调查来量化分析工具采纳模式、提示策略以及影响效率的组织因素。

Result: 研究发现表明，开发人员的经验水平对其从GenAI工具中获得的好处感知有调节作用；生产力增益并非均匀分布；组织规模影响工具选择及使用强度；对于项目上下文理解有限是主要障碍之一。

Conclusion: 基于研究结果，为希望推进AI辅助软件开发的开发者、组织及工具供应商总结出了一系列可操作的建议。

Abstract: Generative artificial intelligence (GenAI) tools have seen rapid adoption among software developers. While adoption rates in the industry are rising, the underlying factors influencing the effective use of these tools, including the depth of interaction, organizational constraints, and experience-related considerations, have not been thoroughly investigated. This issue is particularly relevant in environments with stringent regulatory requirements, such as Germany, where practitioners must address the GDPR and the EU AI Act while balancing productivity gains with intellectual property considerations. Despite the significant impact of GenAI on software engineering, to the best of our knowledge, no empirical study has systematically examined the adoption dynamics of GenAI tools within the German context. To address this gap, we present a comprehensive mixed-methods study on GenAI adoption among German software engineers. Specifically, we conducted 18 exploratory interviews with practitioners, followed by a developer survey with 109 participants. We analyze patterns of tool adoption, prompting strategies, and organizational factors that influence effectiveness. Our results indicate that experience level moderates the perceived benefits of GenAI tools, and productivity gains are not evenly distributed among developers. Further, organizational size affects both tool selection and the intensity of tool use. Limited awareness of the project context is identified as the most significant barrier. We summarize a set of actionable implications for developers, organizations, and tool vendors seeking to advance artificial intelligence (AI) assisted software development.

</details>


### [14] [Developer Perspectives on REST API Usability: A Study of REST API Guidelines](https://arxiv.org/abs/2601.16705)
*Sven Peldszus,Jan Rutenkolk,Marcel Heide,Jan Sollmann,Benjamin Klatt,Frank Köhne,Thorsten Berger*

Main category: cs.SE

TL;DR: 本文通过对16位来自工业界的REST API专家的访谈研究，探讨了API可用性的概念、指南的有效性因素、采用和设计指南的挑战以及最佳实践。研究发现遵循惯例是影响REST API可用性的最重要因素之一，并提出自动检查工具可以作为嵌入合规性强制执行流程的同时提供教育性解释的方法。


<details>
  <summary>Details</summary>
Motivation: 随着服务导向（即软件即服务）的发展，API已经成为核心业务资产，其设计质量直接影响到服务的成功与否。尽管已有很多关于如何设计易用API的指南被提出，但在实践中开发者仍然面临困难。因此，需要增强我们对于采用、使用及创建REST API指南的实际理解。

Method: 通过与16位工业界REST API专家进行访谈的方式来进行研究。

Result: 确定了影响REST API可用性的八个因素，其中遵守约定被认为是最为重要的；同时发现虽然指南确实能够有效提高API的可用性，但开发者对严格指南存在显著抵触情绪；指南的规模及其是否符合组织需求是两个需要考虑的重要因素；建议REST指南应该随着组织的成长而发展，并且所有利益相关者都应参与到其开发和维护过程中；自动化审查不仅能够将合规性强制执行嵌入到流程中，还能通过教育性解释来证明指南规则的合理性。

Conclusion: 为了改善REST API的设计情况，必须加强我们对采纳、使用以及创建REST API指南的经验理解。本研究表明，虽然开发者对严格的指导方针有抵触情绪，但遵循既定惯例等方法仍可有效提升API的可用性。此外，指南应当随着组织的发展而进化，并且所有利益相关方都应该参与到其制定和维护的过程中去。

Abstract: REST is today's most widely used architectural style for providing web-based services. In the age of service-orientation (a.k.a. Software as a Service (SaaS)) APIs have become core business assets and can easily expose hundreds of operations. While well-designed APIs contribute to the commercial success of a service, poorly designed APIs can threaten entire organizations. Recognizing their relevance and value, many guidelines have been proposed for designing usable APIs, similar to design patterns and coding standards. For example, Zalando and Microsoft provide popular REST API guidelines. However, they are often considered as too large and inapplicable, so many companies create and maintain their own guidelines, which is a challenge in itself. In practice, however, developers still struggle to design effective REST APIs. To improve the situation, we need to improve our empirical understanding of adopting, using, and creating REST API guidelines.
  We present an interview study with 16 REST API experts from industry. We determine the notion of API usability, guideline effectiveness factors, challenges of adopting and designing guidelines, and best practices. We identified eight factors influencing REST API usability, among which the adherence to conventions is the most important one. While guidelines can in fact be an effective means to improve API usability, there is significant resistance from developers against strict guidelines. Guideline size and how it fits with organizational needs are two important factors to consider. REST guidelines also have to grow with the organization, while all stakeholders need to be involved in their development and maintenance. Automated linting provides an opportunity to not only embed compliance enforcement into processes, but also to justify guideline rules with educational explanations.

</details>


### [15] [Variability-Aware Detection and Repair of Compilation Errors Using Foundation Models in Configurable Systems](https://arxiv.org/abs/2601.16755)
*Rohit Gheyi,Lucas Albuquerque,Márcio Ribeiro,Eduardo Almeida,Danyllo Albuquerque,Mirko Perkusich*

Main category: cs.SE

TL;DR: 本研究通过使用基础模型（如GPT-OSS-20B和GEMINI 3 PRO）来检测并修复可配置C系统中由特性变异性引起的编译错误，与现有工具TYPECHEF相比，在小规模系统上显著提高了检测覆盖率，并且在实际项目分析中也展示了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 软件系统常依赖条件编译支持可选特性和多部署场景，但由此产生的编译错误往往只在特定功能组合下出现，难以被传统编译器或现有变体感知工具有效检测。

Method: 研究采用了两种基础模型GPT-OSS-20B和GEMINI 3 PRO进行评估，并与先进的变体感知解析器TYPECHEF比较；实验设计包括了5,000个小规模可配置系统、14个真实GitHub提交以及额外的变异测试场景。

Result: 结果显示，在小规模系统上GPT-OSS-20B达到了0.97的精确率、0.90的召回率及0.94的准确度，明显优于TYPECHEF的表现；对于编译错误修复，GPT-OSS-20B能够处理超过70%的情况。此外，在分析真实提交时，CHATGPT-5.2几乎识别出了所有注入的故障。

Conclusion: 当前最先进的基础模型为检测和修复由特性变异性导致的编译错误提供了实用且低投入的方法，可以作为传统变体感知分析的有效补充。

Abstract: Modern software systems often rely on conditional compilation to support optional features and multiple deployment scenarios. In configurable systems, compilation errors may arise only under specific combinations of features, remaining hidden during development and testing. Such variability-induced errors are difficult to detect in practice, as traditional compilers analyze only a single configuration at a time, while existing variability-aware tools typically require complex setup and incur high analysis costs. In this article, we present an empirical study on the use of foundation models to detect and fix compilation errors caused by feature variability in configurable C systems. We evaluate GPT-OSS-20B and GEMINI 3 PRO, and compare them with TYPECHEF, a state-of-the-art variability-aware parser. Our evaluation considers two complementary settings: 5,000 small configurable systems designed to systematically exercise variability-induced compilation behavior, comprising both systems with and without compilation errors, and 14 real-world GitHub commits, as well as an additional set of mutation testing scenarios (42). Our results show that foundation models can effectively identify variability-induced compilation errors. On small configurable systems, GPT-OSS-20B achieved a precision of 0.97, recall of 0.90, and accuracy of 0.94, substantially increasing detection coverage compared to TYPECHEF, and exhibiting performance comparable to GEMINI 3. For compilation error repair, GPT-OSS-20B produced compilable fixes in over 70% of the cases. In the analysis of real commits, CHATGPT-5.2 detected all injected faults except for two cases and identified a potential real compilation bug in a Linux commit with more than 1,000 modified lines. Our findings indicate that current state-of-the-art foundation models provide a practical and low-effort complement to traditional variability-aware analyses.

</details>


### [16] [Assessing the Feasibility of Selective Instrumentation for Runtime Code Coverage in Large C++ Game Engines](https://arxiv.org/abs/2601.16881)
*Ian Gauk,Doriane Olewicki,Joshua Romoff,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 本文提出了一种针对大型C++游戏引擎的选择性工具化方法，该方法可以减少工具化的范围，同时保留与开发者提交相关的覆盖率数据。通过这种方法，在几乎不增加编译开销的情况下提供了即时的覆盖率反馈，并且在性能评估中保持了良好的帧率，避免了全工具化时可能出现的测试不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 在AAA游戏中，虽然代码覆盖率是测试的重要指南，但工具化带来的开销与严格的性能要求相冲突，还可能破坏自动化测试的稳定性。为解决这一问题，作者提出了一个专门针对大型C++游戏引擎设计的选择性工具化方案。

Method: 研究者开发了一个框架，它能够集成到工业级的游戏测试流程中，让开发者能够直接获取其变更所触发测试的即刻覆盖率反馈。此方法通过限制工具化的范围来减少开销，同时仍能收集到对开发者有用的覆盖率信息。

Result: 实验结果表明，即便是在最差情况下，新方法也能保持超过非工具化基线50%以上的帧率；并且在整个两个生产测试套件中，没有引发任何自动化测试失败的情况。此外，仅当处理超过2,000次提交时，构建时间才会翻倍。

Conclusion: 研究表明，对于大规模C++游戏引擎来说，实现提交级别或构建级别的代码覆盖率是可以以最小开销完成的，同时不会影响测试的稳定性。

Abstract: Code coverage is a valuable guide for testing, but in AAA games the overhead of instrumentation conflicts with strict performance requirements and can destabilize automated tests. We propose and assess a selective instrumentation approach tailored to large game engines written in \texttt{C++}, which reduces the scope of instrumentation while preserving relevant coverage data to developer commits. Our framework integrates into an industrial game testing pipeline, enabling developers to receive immediate coverage feedback on tests run against their changes. The compilation overhead of our approach is minimal, allowing instrumentation of over 2,000 commits before doubling build time. In performance evaluations, even the worst-case scenario maintains frame rates above 50\% of the non-instrumented baseline. Across two production test suites maintained by our industry partner, our framework caused no automated test failures, avoiding the instability observed under full instrumentation. Our work shows that commit-level or build-level coverage of large \texttt{C++} game engines can be achieved with minimal overhead and without compromising test stability.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [iPDB -- Optimizing SQL Queries with ML and LLM Predicates](https://arxiv.org/abs/2601.16432)
*Udesh Kumarasinghe,Tyler Liu,Chunwei Liu,Walid G. Aref*

Main category: cs.DB

TL;DR: 本文介绍了iPDB，一个支持在数据库内进行机器学习和大型语言模型推理的系统，通过扩展SQL语法实现更高效的语义查询处理。


<details>
  <summary>Details</summary>
Motivation: 当前应用开发环境中，利用学习模型执行复杂任务的需求日益增长，但传统的SQL查询语言及关系数据库系统对这些基于学习模型的工作负载要么不兼容，要么效率低下，导致需要复杂的工程处理和多次数据迁移操作。

Method: 提出了iPDB系统，该系统通过扩展SQL语法来支持数据库内的机器学习(ML)和大型语言模型(LLM)推理。在iPDB中，LLMs和ML调用可以作为语义投影、执行语义选择和连接的谓词或用于分组子句中的语义分组，并引入了新颖的关系预测运算符与语义查询优化技术。

Result: iPDB允许用户编写并高效执行语义SQL查询，在性能上超越了现有技术。

Conclusion: iPDB提供了一种新的方法，能够更好地将机器学习模型集成到传统的关系数据库管理系统中，从而简化了数据处理流程，提高了对于涉及学习模型工作的效率。

Abstract: Structured Query Language (SQL) has remained the standard query language for databases. SQL is highly optimized for processing structured data laid out in relations. Meanwhile, in the present application development landscape, it is highly desirable to utilize the power of learned models to perform complex tasks. Large language models (LLMs) have been shown to understand and extract information from unstructured textual data. However, SQL as a query language and accompanying relational database systems are either incompatible or inefficient for workloads that require leveraging learned models. This results in complex engineering and multiple data migration operations that move data between the data sources and the model inference platform. In this paper, we present iPDB, a relational system that supports in-database machine learning (ML) and large language model (LLM) inferencing using extended SQL syntax. In iPDB, LLMs and ML calls can function as semantic projects, as predicates to perform semantic selects and semantic joins, or for semantic grouping in group-by clauses. iPDB has a novel relational predict operator and semantic query optimizations that enable users to write and efficiently execute semantic SQL queries, outperforming the state-of-the-art.

</details>


### [18] [A Scalable Transaction Management Framework for Consistent Document-Oriented NoSQL Databases](https://arxiv.org/abs/2601.16490)
*Adam A. E. Alflahi,Mohammed A. Y. Mohammed,Abdallah Alsammani*

Main category: cs.DB

TL;DR: 研究提出了一种针对文档型NoSQL数据库（以MongoDB为参考平台）的四阶段事务管理框架，结合了生命周期管理、操作分类、预执行冲突检测以及基于超时机制的自适应锁定策略。实验结果表明，该方法降低了事务中止率和延迟变化，提高了吞吐量，并且在分布式环境下表现出良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: NoSQL数据库因其可扩展性和模式灵活性而被现代应用广泛使用，但它们通常依赖于最终一致性模型，这限制了可靠的事务处理能力。为了改善这一情况并提高数据完整性，同时保持系统的可扩展性，提出了本研究中的事务管理框架。

Method: 该研究设计了一个四阶段的事务管理框架，包括事务生命周期管理、操作分类、预执行冲突检测以及一个带有基于超时死锁预防机制的自适应锁定策略。采用Yahoo Cloud Serving Benchmark (YCSB)工作负载A、B和F进行了评估，测试了从1到100个客户端的不同并发水平下的表现。此外，还进行了敏感性分析来确定最佳参数设置。

Result: 实验结果显示，在高并发情况下，事务中止率由8.3%降至4.7%，消除了观察到的死锁现象，延迟变化减少了34.2%。吞吐量提高了6.3%至18.4%，特别是在读-修改-写工作负载下更为明显。分布式的实验进一步验证了方案的有效性，与基准系统相比，吞吐量提高了15.2%，中止率降低了53%。

Conclusion: 通过精心设计的一致性机制，可以在不损害NoSQL系统可扩展性的前提下显著提高其数据完整性。提出的框架在保证一致性和性能开销之间达到了良好的平衡。

Abstract: NoSQL databases are widely used in modern applications due to their scalability and schema flexibility, yet they often rely on eventual consistency models that limit reliable transaction processing. This study proposes a four-stage transaction management framework for document-oriented NoSQL databases, with MongoDB as the reference platform. The framework combines transaction lifecycle management, operation classification, pre-execution conflict detection, and an adaptive locking strategy with timeout-based deadlock prevention. Formal correctness analysis shows that the proposed approach guarantees conflict serializability under defined conditions. An experimental evaluation using the Yahoo Cloud Serving Benchmark (YCSB) workloads A, B, and F, with concurrency levels ranging from 1 to 100 clients, demonstrates a reduction in transaction abort rates from 8.3% to 4.7%, the elimination of observed deadlocks, and a 34.2% decrease in latency variance. Throughput improvements ranging from 6.3% to 18.4% are observed under high concurrency, particularly for read-modify-write workloads. Distributed experiments on clusters of up to 9 nodes confirm scalability, achieving 15.2% higher throughput and 53% lower abort rates than baseline systems. Comparisons with MongoDB's native transactions, CockroachDB, and TiDB indicate that the proposed framework strikes a good balance between consistency guarantees and performance overhead. Sensitivity analysis identifies optimal parameter settings, including a lock timeout of 100 ms, an initial backoff of 10 ms, and a maximum backoff of 500 ms. These results show that carefully designed consistency mechanisms can significantly improve data integrity in NoSQL systems without undermining scalability.

</details>


### [19] [A Categorical Approach to Semantic Interoperability across Building Lifecycle](https://arxiv.org/abs/2601.16663)
*Zoltan Nagy,Ryan Wisnesky,Kevin Carlson,Eswaran Subrahmanian,Gioele Zardini*

Main category: cs.DB

TL;DR: 该论文提出了一种基于范畴论的方法来解决建筑生命周期中异构数据的集成问题，通过将建筑本体形式化为一阶理论，并使用范畴查询语言（CQL）实现不同本体间的结构保持转换。实验结果表明，这种方法能够以线性复杂度整合多个本体，支持自动双向迁移和跨本体查询，为构建可靠的应用生态系统提供了数学基础。


<details>
  <summary>Details</summary>
Motivation: 建筑物在其生命周期内会产生大量异构数据，但如何有效整合这些数据仍是一个未解决的关键问题。现有的方法要么依赖于点对点映射导致复杂度随模式数量呈二次增长，要么采用难以管理的通用本体。本文旨在通过引入数学基础——即范畴论——来解决这一挑战，从而以更高效的方式实现数据集成。

Method: 研究者们首先将建筑本体正式定义为一阶理论，然后利用范畴查询语言(CQL)开发了两个概念验证实现：1）从IFC设计数据生成BRICK模型；2）实现IFC、BRICK与RealEstateCore之间的三方集成，其中仅需定义两组明确映射即可自动生成第三组。整个过程遵循构造正确性原则，将属性集视为顶级架构实体处理，并提供自动化双向迁移功能。

Result: 研究表明，基于范畴论的数据集成方法能够以线性规格复杂度($O(n)$)整合$n$个本体，不仅支持自动双向迁移还使得跨本体查询成为可能。通过实际案例证明了该方法在处理建筑领域异构数据时的有效性和可行性。

Conclusion: 本文介绍了一种新的基于范畴论的方法来解决建筑领域内异构数据集成的问题，相比传统解决方案，它具有更低的复杂度和更好的可扩展性。此外，该方法还为未来开发针对建筑领域的应用生态系统奠定了坚实的数学基础。

Abstract: Buildings generate heterogeneous data across their lifecycle, yet integrating these data remains a critical unsolved challenge. Despite three decades of standardization efforts, over 40 metadata schemas now span the building lifecycle, with fragmentation accelerating rather than resolving. Current approaches rely on point-to-point mappings that scale quadratically with the number of schemas, or universal ontologies that become unwieldy monoliths. The fundamental gap is the absence of mathematical foundations for structure-preserving transformations across heterogeneous building data. Here we show that category theory provides these foundations, enabling systematic data integration with $O(n)$ specification complexity for $n$ ontologies. We formalize building ontologies as first-order theories and demonstrate two proof-of-concept implementations in Categorical Query Language (CQL): 1) generating BRICK models from IFC design data at commissioning, and 2) three-way integration of IFC, BRICK, and RealEstateCore where only two explicit mappings yield the third automatically through categorical composition. Our correct-by-construction approach treats property sets as first-class schema entities and provides automated bidirectional migrations, and enables cross-ontology queries. These results establish feasibility of categorical methods for building data integration and suggest a path toward an app ecosystem for buildings, where mathematical foundations enable reliable component integration analogous to smartphone platforms.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [20] [LLM-based Semantic Search for Conversational Queries in E-commerce](https://arxiv.org/abs/2601.16492)
*Emad Siddiqui,Venkatesh Terikuti,Xuan Lu*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的语义搜索框架，通过结合领域特定嵌入与结构化过滤器，从对话式查询中有效捕捉用户意图。该方法生成合成数据以微调两个模型：一个将语义相似产品在表示空间中紧密排列的嵌入模型，另一个是将自然语言查询转换为结构化约束的生成模型。实验表明，相比基线方法，本框架在真实数据集上实现了更高的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 随着会话式用户查询变得越来越普遍，传统的电商搜索系统面临挑战，因为它们通常是针对关键词查询进行优化的。为了应对这一变化，并解决标记数据有限的问题，作者开发了一套新的解决方案。

Method: 提出了一个基于大型语言模型（LLM）的语义搜索框架，该框架利用领域特异性嵌入与结构化筛选相结合的方式处理会话式查询。此外，还采用了LLMs来生成合成数据，用于指导两个模型的微调过程：一个是确保语义相似商品在表征空间内位置相近的嵌入模型；另一个则是能够将自然语言查询转化为结构化约束条件的生成模型。

Result: 通过整合基于相似性的检索与基于约束的过滤技术，该框架在各种场景下均表现出色，在真实世界的数据集上相对于基准方法而言，其精确度和召回率都有显著提升。

Conclusion: 这项研究展示了一种新颖且有效的电子商务平台搜索解决方案，它不仅能够更好地理解复杂的用户请求，还能在实际应用中提供更加准确的结果。

Abstract: Conversational user queries are increasingly challenging traditional e-commerce platforms, whose search systems are typically optimized for keyword-based queries. We present an LLM-based semantic search framework that effectively captures user intent from conversational queries by combining domain-specific embeddings with structured filters. To address the challenge of limited labeled data, we generate synthetic data using LLMs to guide the fine-tuning of two models: an embedding model that positions semantically similar products close together in the representation space, and a generative model for converting natural language queries into structured constraints. By combining similarity-based retrieval with constraint-based filtering, our framework achieves strong precision and recall across various settings compared to baseline approaches on a real-world dataset.

</details>


### [21] [LLM-powered Real-time Patent Citation Recommendation for Financial Technologies](https://arxiv.org/abs/2601.16775)
*Tianang Deng,Yu Deng,Tianchen Gao,Yonghong Hu,Rui Pan*

Main category: cs.IR

TL;DR: 本研究提出了一种针对大量且快速变化的金融专利文集设计的实时专利引用推荐框架。该框架使用大型语言模型嵌入来表示专利摘要的语义内容，通过高效的近似最近邻搜索构建可管理的候选集，并按语义相似性对候选人进行排名以产生前k个引用建议。此外，通过基于层次可导航小世界（HNSW）图的增量索引策略直接解决了专利系统的动态特性问题。实验表明，与基于重建的索引相比，增量更新方法在提高召回率的同时显著降低了计算成本，并且该方法也一致优于传统的基于文本的基线和其他最近邻检索方法。


<details>
  <summary>Details</summary>
Motivation: 随着金融创新的快速发展，专利活动急剧增加，使得及时全面地发现现有技术变得更加困难。特别是在金融科技领域，创新迅速发展、专利集合不断增长，而引用推荐系统需要随着新申请的到来而更新。现有的专利检索和引用推荐方法通常依赖于静态索引或周期性再训练，这限制了它们在这种动态环境下的有效性。

Method: 本研究构建了一个三阶段推荐流程：首先利用大型语言模型（LLM）嵌入来表示专利摘要的语义内容；其次，应用高效的近似最近邻搜索来构造一个可管理的候选集；最后，根据语义相似性对候选者进行排名，从而生成顶级k个引用推荐。此外，通过采用基于层次可导航小世界（HNSW）图的增量索引策略，新发布的专利可以无需重建整个索引来添加。

Result: 实验显示，与基于重建索引的方法相比，所提出的增量更新方法不仅提高了召回率，还大大减少了计算成本。此外，该方法在性能上也始终优于传统的基于文本的基准和其他最近邻检索方法。

Conclusion: 这项工作介绍了一种专为快速发展的金融专利领域设计的实时专利引用推荐系统。它不仅提高了推荐准确性，而且通过有效的增量索引策略解决了专利系统中的动态挑战。

Abstract: Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval and citation recommendation methods typically rely on static indexes or periodic retraining, which limits their ability to operate effectively in such dynamic settings. In this study, we propose a real-time patent citation recommendation framework designed for large and fast-changing financial patent corpora. Using a dataset of 428,843 financial patents granted by the China National Intellectual Property Administration (CNIPA) between 2000 and 2024, we build a three-stage recommendation pipeline. The pipeline uses large language model (LLM) embeddings to represent the semantic content of patent abstracts, applies efficient approximate nearest-neighbor search to construct a manageable candidate set, and ranks candidates by semantic similarity to produce top-k citation recommendations. In addition to improving recommendation accuracy, the proposed framework directly addresses the dynamic nature of patent systems. By using an incremental indexing strategy based on hierarchical navigable small-world (HNSW) graphs, newly issued patents can be added without rebuilding the entire index. A rolling day-by-day update experiment shows that incremental updating improves recall while substantially reducing computational cost compared with rebuild-based indexing. The proposed method also consistently outperforms traditional text-based baselines and alternative nearest-neighbor retrieval approaches.

</details>


### [22] [Explaining Group Recommendations via Counterfactuals](https://arxiv.org/abs/2601.16882)
*Maria Stratigi,Nikos Bikakis*

Main category: cs.IR

TL;DR: 本文提出了一种针对群体的反事实解释框架，旨在提高群体推荐系统的透明度。通过引入适用于群体的效用和公平性度量，并设计了如基于帕累托的过滤和生长-修剪策略等启发式算法来发现有效的解释。实验表明，不同方法在成本、公平性和简洁性之间存在权衡，其中帕累托过滤启发式在稀疏设置下表现出显著效率提升。


<details>
  <summary>Details</summary>
Motivation: 群组推荐系统虽然帮助用户做出集体选择，但往往缺乏透明度，导致群组成员不清楚为什么推荐特定项目。现有的解释方法主要关注个体，对涉及多种偏好的群组支持有限。

Method: 提出了一个群组反事实解释框架，定义了该概念，并为群组定制了效用与公平性度量标准。此外，还开发了几种启发式算法，包括基于帕累托的过滤技术和生长-修剪策略，以促进高效地寻找解释。

Result: 实验部分使用了MovieLens和Amazon数据集，结果显示低成本方法生成更大且不太公平的解释；而其他方法则能够以更高的代价产生更加简明且平衡的结果。特别地，在稀疏环境中，基于帕累托过滤的方法显示出极大的效率改进。

Conclusion: 提出的群组反事实解释框架及其相关算法能够在一定程度上解决现有群组推荐系统中透明度不足的问题，同时在不同场景下提供了关于成本、公平性以及解释简洁性的权衡方案。

Abstract: Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past interactions would change a group recommendation. We formalize this concept, introduce utility and fairness measures tailored to groups, and design heuristic algorithms, such as Pareto-based filtering and grow-and-prune strategies, for efficient explanation discovery. Experiments on MovieLens and Amazon datasets show clear trade-offs: low-cost methods produce larger, less fair explanations, while other approaches yield concise and balanced results at higher cost. Furthermore, the Pareto-filtering heuristic demonstrates significant efficiency improvements in sparse settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Ordering-based Causal Discovery via Generalized Score Matching](https://arxiv.org/abs/2601.16249)
*Vy Vo,He Zhao,Trung Le,Edwin V. Bonilla,Dinh Phung*

Main category: cs.LG

TL;DR: 本文扩展了原本用于连续数据的得分匹配框架，引入了一种基于离散得分函数的新叶节点判别准则，通过模拟和真实世界实验表明该方法能够从观察到的离散数据中准确推断出真实的因果顺序，并且识别出的顺序可以显著提高现有因果发现基线方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 从纯观察数据中学习DAG结构一直是跨科学领域的一个长期挑战。

Method: 本文拓展了得分匹配框架用于因果发现，特别是针对离散数据，并提出了一个新的基于离散得分函数的叶节点判别标准。

Result: 实验证明，所提出的方法能够从观测到的离散数据中准确地推断出真正的因果顺序，并且这种顺序识别能够显著提升现有因果发现基准在几乎所有设置下的准确性。

Conclusion: 新的叶节点判别准则和得分匹配框架的扩展为从离散数据中学习DAG结构提供了有效途径，有助于更准确地进行因果关系发现。

Abstract: Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.

</details>


### [24] [Student Mental Health Screening via Fitbit Data Collected During the COVID-19 Pandemic](https://arxiv.org/abs/2601.16324)
*Rebecca Lopez,Avantika Shrestha,ML Tlachac,Kevin Hickey,Xingtong Guo,Shichao Liu,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 研究收集了疫情期间学生使用Fitbit设备的数据，评估了机器学习模型通过不同生理指标（如心率和睡眠）筛查抑郁、焦虑和压力的潜力。结果表明这些生理指标在筛查心理健康状况方面具有较高准确性，突出了可穿戴设备在持续监测心理健康方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 大学生面临许多压力源，导致高水平的焦虑和抑郁。尽管可穿戴技术能够提供不引人注意的传感器数据以用于早期检测精神疾病，但目前的研究在心理测量工具种类、生理模式及时间序列参数方面存在局限性。

Method: 研究者们从所在机构的学生中收集了疫情期间的Student Mental and Environmental Health (StudentMEH) Fitbit数据集，并对利用不同Fitbit模式进行抑郁、焦虑和压力筛查的预测性机器学习模型进行了全面评估。

Result: 发现心率与睡眠等生理模式在筛查精神疾病方面展现出潜力，对于焦虑的F1分数高达0.79，压力筛查时前者达到0.77，而后者在抑郁筛查上达到了0.78。

Conclusion: 这项研究表明，可穿戴设备支持连续心理健康监控有着巨大潜力，并强调了识别最佳数据聚合水平及针对不同心理健康问题筛选适当模式的重要性。

Abstract: College students experience many stressors, resulting in high levels of anxiety and depression. Wearable technology provides unobtrusive sensor data that can be used for the early detection of mental illness. However, current research is limited concerning the variety of psychological instruments administered, physiological modalities, and time series parameters. In this research, we collect the Student Mental and Environmental Health (StudentMEH) Fitbit dataset from students at our institution during the pandemic. We provide a comprehensive assessment of the ability of predictive machine learning models to screen for depression, anxiety, and stress using different Fitbit modalities. Our findings indicate potential in physiological modalities such as heart rate and sleep to screen for mental illness with the F1 scores as high as 0.79 for anxiety, the former modality reaching 0.77 for stress screening, and the latter modality achieving 0.78 for depression. This research highlights the potential of wearable devices to support continuous mental health monitoring, the importance of identifying best data aggregation levels and appropriate modalities for screening for different mental ailments.

</details>


### [25] [Efficient Gaussian process learning via subspace projections](https://arxiv.org/abs/2601.16332)
*Felipe Tobar,Elsa Cazelles*

Main category: cs.LG

TL;DR: 提出了一种新的基于数据低维线性投影的高斯过程训练目标，称为投影似然(PL)，它在准确性和计算效率上优于精确GP训练和稀疏GP的变分自由能方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高高斯过程(GP)在中等规模数据集上的训练效率和准确性，同时减少计算复杂度。

Method: 引入了投影似然(PL)作为新的训练目标，该目标是通过数据的低维线性投影构建的。还提供了一个关于PL信息损失的闭式表达，并展示了如何通过单位球面上的随机投影来减少这种损失。

Result: 实验结果表明，在不同优化器、核函数以及中等大小的数据集上，PL方法在准确性和计算效率方面均优于传统的精确GP训练和使用变分自由能方法的稀疏GP。

Conclusion: 提出的投影似然(PL)方法为解决高斯过程中存在的计算效率问题提供了一条有效途径，特别是在处理具有中等规模的数据集时表现出色。

Abstract: We propose a novel training objective for GPs constructed using lower-dimensional linear projections of the data, referred to as \emph{projected likelihood} (PL). We provide a closed-form expression for the information loss related to the PL and empirically show that it can be reduced with random projections on the unit sphere. We show the superiority of the PL, in terms of accuracy and computational efficiency, over the exact GP training and the variational free energy approach to sparse GPs over different optimisers, kernels and datasets of moderately large sizes.

</details>


### [26] [A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning](https://arxiv.org/abs/2601.16399)
*Sihan Zeng,Sujay Bhatt,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 提出了一种单循环、一阶的actor-critic算法，通过基于惩罚的重构优化双层目标。在下层RL目标中引入了衰减熵正则化，允许渐近无偏估计上层超梯度而无需精确解决未正则化的RL问题。该方法在特定类型的Polyak-Lojasiewicz条件下被证明具有有限时间和有限样本收敛性，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究结构化的双层优化问题，其中上层目标是一个平滑函数，下层问题是马尔可夫决策过程（MDP）中的策略优化。现有方法往往需要二阶信息、对下层强正则化或通过嵌套循环程序低效使用样本。

Method: 提出了一种单循环、一阶的actor-critic算法，它通过基于惩罚的方法来优化双层目标。此外，在下层RL目标中加入了衰减熵正则化以实现渐进无偏的上层超梯度估计。

Result: 所提算法对于原始未经正则化的双层优化问题，在一种特殊的Polyak-Lojasiewicz条件下能够保证有限时间和样本复杂度下的收敛性。实验结果表明该方法在GridWorld目标定位问题以及通过人类反馈进行强化学习生成快乐推文的任务中表现良好。

Conclusion: 提出的方法为解决结构化双层优化问题提供了一种有效途径，避免了对二阶信息的需求和复杂的嵌套循环操作，同时实现了良好的实际应用效果。

Abstract: We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).

</details>


### [27] [Towards a Theoretical Understanding to the Generalization of RLHF](https://arxiv.org/abs/2601.16403)
*Zhaochun Li,Mingyang Yi,Yue Wang,Shisheng Cui,Yong Liu*

Main category: cs.LG

TL;DR: 本文通过算法稳定性框架建立了线性奖励模型下大型语言模型的RLHF（从人类反馈中学习强化学习）的泛化理论，证明了在满足关键特征覆盖条件的情况下，策略模型的经验最优具有$\mathcal{O}(n^{-\frac{1}{2}})$阶的泛化界。此外，这些结果可以推广到基于梯度的学习算法所得到的参数上。


<details>
  <summary>Details</summary>
Motivation: 尽管从人类反馈中学习强化学习(RLHF)及其变体在使大型语言模型与人类意图对齐方面表现出色，但在高维设定下的理论泛化性质仍有待探索。为了填补这一空白，研究者们旨在构建一个更符合实际操作的端到端学习框架内的泛化理论。

Method: 采用了算法稳定性的框架来分析线性奖励模型条件下大型语言模型的RLHF过程。不同于以往依赖于奖励模型最大似然估计一致性的方法，本研究假设了一个重要的‘特征覆盖’条件，并在此基础上进行了分析。

Result: 研究表明，在满足特定的‘特征覆盖’条件下，策略模型的经验最优解可以获得$\mathcal{O}(n^{-\frac{1}{2}})$阶的泛化界限。此外，还指出该结论同样适用于通过梯度上升（GA）和随机梯度上升（SGA）等基于梯度的学习算法获得的参数。

Conclusion: 这项工作为理解RLHF后大型语言模型观察到的泛化能力提供了新的理论依据。

Abstract: Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.

</details>


### [28] [Reasoning-Enhanced Rare-Event Prediction with Balanced Outcome Correction](https://arxiv.org/abs/2601.16406)
*Vitaly Bulgakov,Alexander Turchin*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段框架LPCORP，旨在解决罕见事件预测中的极端类别不平衡问题。通过结合增强推理预测和基于置信度的结果修正，该方法在不使用重采样策略的情况下将高度不平衡的数据集转化为平衡状态，并在真实世界数据集中展示了显著的性能改进，特别是在精确度方面。


<details>
  <summary>Details</summary>
Motivation: 罕见事件预测在医疗保健、金融、可靠性工程、客户服务、航空安全等领域至关重要，这些领域的正向结果虽然少见但可能带来灾难性后果。然而，由于极端的类别不平衡，传统模型往往偏向于多数类别的预测，从而限制了召回率、校准以及操作实用性。

Method: 提出了LPCORP（低流行度纠正预测）框架，该框架分为两个阶段：首先利用推理模型从叙述性输入中生成丰富的预测；然后，一个轻量级逻辑回归分类器评估并选择性地修正这些输出，以减轻流行度驱动的偏见。整个过程中没有采用任何重采样策略。

Result: 在来自医疗和消费者服务领域的实际数据集上的测试表明，LPCORP能够将高度不平衡的情况转变为良好平衡的状态，同时保持原始样本数量不变。特别地，在处理低流行度数据时已知较弱的精度指标上表现出了显著提升。此外，成本减少分析显示，在某些情况下，与未采取预防措施相比，应用低成本基于预测的预防干预措施可减少超过50%的成本。

Conclusion: LPCORP提供了一个有效的方法来改善罕见事件预测中的极端类别不平衡问题，不仅提高了预测准确性还降低了相关成本。

Abstract: Rare-event prediction is critical in domains such as healthcare, finance, reliability engineering, customer support, aviation safety, where positive outcomes are infrequent yet potentially catastrophic. Extreme class imbalance biases conventional models toward majority-class predictions, limiting recall, calibration, and operational usefulness. We propose LPCORP (Low-Prevalence CORrector for Prediction)*, a two-stage framework that combines reasoningenhanced prediction with confidence-based outcome correction. A reasoning model first produces enriched predictions from narrative inputs, after which a lightweight logistic-regression classifier evaluates and selectively corrects these outputs to mitigate prevalence-driven bias. We evaluate LPCORP on real-world datasets from medical and consumer service domains. The results show that this method transforms a highly imbalanced setting into a well-balanced one while preserving the original number of samples and without applying any resampling strategies. Test-set evaluation demonstrates substantially improved performance, particularly in precision, which is a known weakness in low-prevalence data. We further provide a costreduction analysis comparing the expenses associated with rare-event damage control without preventive measures to those incurred when low-cost, prediction-based preventive interventions are applied that showed more than 50% reduction in some cases. * Patent pending: U.S. Provisional 63/933,518, filed 8 December 2025.

</details>


### [29] [A Refinement of Vapnik--Chervonenkis' Theorem](https://arxiv.org/abs/2601.16411)
*A. Iosevich,A. Vagharshakyan,E. Wyman*

Main category: cs.LG

TL;DR: 该论文重新审视了Vapnik-Chervonenkis定理的概率部分，通过使用带有显式Berry-Esseen误差控制的正态逼近而非Hoeffding不等式，为VC估计提供了一个中偏差锐化，当ε√n很大时，在主要指数项中增加了一个阶数为(ε√n)^-1的因子。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过改进经典论证中的概率成分来提高Vapnik-Chervonenkis定理的一致收敛速度估计的精度。

Method: 采用的方法是用具有明确Berry-Esseen误差控制的正态近似替换原有论证最后一步使用的Hoeffding不等式。

Result: 研究结果表明，这种新方法可以对通常的VC估计进行适度偏差的修正，并且在ε√n较大时，主要指数项中会多出一个(ε√n)^-1量级的因素。

Conclusion: 结论是，通过这种方法得到的VC估计比传统方法更精确，特别是在数据量大且偏差适中的情况下。

Abstract: Vapnik--Chervonenkis' theorem is a seminal result in machine learning. It establishes sufficient conditions for empirical probabilities to converge to theoretical probabilities, uniformly over families of events. It also provides an estimate for the rate of such uniform convergence.
  We revisit the probabilistic component of the classical argument. Instead of applying Hoeffding's inequality at the final step, we use a normal approximation with explicit Berry--Esseen error control. This yields a moderate-deviation sharpening of the usual VC estimate, with an additional factor of order $(\varepsilon\sqrt{n})^{-1}$ in the leading exponential term when $\varepsilon\sqrt{n}$ is large.

</details>


### [30] [PyHealth 2.0: A Comprehensive Open-Source Toolkit for Accessible and Reproducible Clinical Deep Learning](https://arxiv.org/abs/2601.16414)
*John Wu,Yongda Fan,Zhenbang Wu,Paul Landes,Eric Schrock,Sayeed Sajjad Razin,Arjun Chatterjee,Naveen Baskaran,Joshua Steier,Andrea Fitzpatrick,Bilal Arif,Rian Atri,Jathurshan Pradeepkumar,Siddhartha Laghuvarapu,Junyi Gao,Adam R. Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: PyHealth 2.0 is an enhanced clinical deep learning toolkit that simplifies AI research in healthcare by addressing key challenges such as reproducibility, computational costs, and the need for domain expertise. It supports a wide range of datasets, tasks, models, and interpretability methods, and is designed to be accessible and efficient, with a supportive open-source community.


<details>
  <summary>Details</summary>
Motivation: The motivation behind PyHealth 2.0 is to overcome persistent barriers in clinical AI research, including difficulty in replicating baselines, high computational costs, and the requirement for specialized domain knowledge. The aim is to make healthcare AI more accessible, reproducible, and efficient for researchers and practitioners with varying levels of expertise and resources.

Method: PyHealth 2.0 provides a comprehensive and unified framework that includes over 15 datasets, 20 clinical tasks, 25+ models, and various interpretability methods alongside uncertainty quantification. This toolkit is designed to support different types of clinical data, from signals and imaging to electronic health records, and it also facilitates the translation between multiple medical coding standards. Additionally, PyHealth 2.0 optimizes resource usage, allowing for faster processing and lower memory consumption, thus making it suitable for use on a wide range of hardware, from personal laptops to production systems. An active open-source community further enhances the toolkit's value by providing extensive documentation, fostering collaborations, and offering multi-language support through RHealth.

Result: With PyHealth 2.0, researchers can now perform predictive modeling using as few as 7 lines of code, significantly lowering the barrier to entry for conducting AI research in healthcare. The toolkit demonstrates up to 39 times faster processing and 20 times lower memory usage compared to previous solutions, enabling its effective use across diverse computational environments. Moreover, the involvement of an active open-source community with over 400 members ensures continuous improvement, accessibility, and support for users at all levels of expertise, thereby promoting the advancement of healthcare AI.

Conclusion: In conclusion, PyHealth 2.0 represents a significant step forward in making clinical AI research more accessible, reproducible, and efficient. By unifying a broad array of datasets, tasks, and models into a single, user-friendly framework, and through the support of a vibrant open-source community, PyHealth 2.0 stands out as a foundational tool for advancing healthcare AI, available for installation via pip install pyhealth.

Abstract: Difficulty replicating baselines, high computational costs, and required domain expertise create persistent barriers to clinical AI research. To address these challenges, we introduce PyHealth 2.0, an enhanced clinical deep learning toolkit that enables predictive modeling in as few as 7 lines of code. PyHealth 2.0 offers three key contributions: (1) a comprehensive toolkit addressing reproducibility and compatibility challenges by unifying 15+ datasets, 20+ clinical tasks, 25+ models, 5+ interpretability methods, and uncertainty quantification including conformal prediction within a single framework that supports diverse clinical data modalities - signals, imaging, and electronic health records - with translation of 5+ medical coding standards; (2) accessibility-focused design accommodating multimodal data and diverse computational resources with up to 39x faster processing and 20x lower memory usage, enabling work from 16GB laptops to production systems; and (3) an active open-source community of 400+ members lowering domain expertise barriers through extensive documentation, reproducible research contributions, and collaborations with academic health systems and industry partners, including multi-language support via RHealth. PyHealth 2.0 establishes an open-source foundation and community advancing accessible, reproducible healthcare AI. Available at pip install pyhealth.

</details>


### [31] [Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance](https://arxiv.org/abs/2601.16425)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 本文通过一个经典源反演问题系统地比较了KL散度和Wasserstein距离两种效用函数标准在贝叶斯实验设计中的表现，发现KL散度在没有模型差异时收敛更快，而Wasserstein指标在存在不可忽略的模型差异时提供更稳健的结果。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨在贝叶斯实验设计（BED）中选择不同的效用函数对信息获取的影响，并特别关注KL散度与Wasserstein距离作为效用函数标准时各自的表现及其适用场景。

Method: 首先使用了一个简单示例来展示Wasserstein距离存在的一个问题，即固定形状后验分布的主要质量相对于其支撑集的位置会影响Wasserstein距离值，且可能产生与信息增益无关的虚假奖励。接着，通过BED文献中的经典源反演问题对比分析了KL散度和Wasserstein距离这两种标准。

Result: 研究结果显示，在不存在模型差异的情况下，KL散度通常会导致更快的收敛速度；然而，当模型差异显著时，Wasserstein指标能够为连续性BED提供更加鲁棒的结果。

Conclusion: 该研究明确了KL散度与Wasserstein度量作为效用函数时各自的权衡点，并为实际应用中选择合适的评价标准提供了指导。

Abstract: Designing experiments that systematically gather data from complex physical systems is central to accelerating scientific discovery. While Bayesian experimental design (BED) provides a principled, information-based framework that integrates experimental planning with probabilistic inference, the selection of utility functions in BED is a long-standing and active topic, where different criteria emphasize different notions of information. Although Kullback--Leibler (KL) divergence has been one of the most common choices, recent studies have proposed Wasserstein distance as an alternative. In this work, we first employ a toy example to illustrate an issue of Wasserstein distance - the value of Wasserstein distance of a fixed-shape posterior depends on the relative position of its main mass within the support and can exhibit false rewards unrelated to information gain, especially with a non-informative prior (e.g., uniform distribution). We then further provide a systematic comparison between these two criteria through a classical source inversion problem in the BED literature, revealing that the KL divergence tends to lead to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results if model discrepancy is non-negligible. These findings clarify the trade-offs between KL divergence and Wasserstein metrics for the utility function and provide guidelines for selecting suitable criteria in practical BED applications.

</details>


### [32] [On the Expressive Power of Floating-Point Transformers](https://arxiv.org/abs/2601.16450)
*Sejun Park,Yeachan Park,Geonho Hwang*

Main category: cs.LG

TL;DR: 本研究探讨了浮点变换器在使用浮点参数和操作时的表示能力，揭示了它们可以表示非置换等变函数，并且在序列长度有限制时能够表示所有置换等变函数，但长序列则不能。此外，还发现了浮点变换器中的最小等变结构，并指出所有非平凡的加法位置编码都会损害其表示能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明，在实数参数与精确运算条件下，变换器对于所有紧凑域上的置换等变连续函数具有近似能力；但实际计算机实现受限于有限数值集及带有舍入误差的机器运算。因此，本研究旨在探索采用浮点参数和浮点运算的变换器所能表达的功能范围。

Method: 通过理论分析方法，研究者首先证明了即使不使用位置编码，浮点变换器也能够表示一类非置换等变函数。接着，进一步证实了当序列长度受到限制时，浮点变换器能够代表所有的置换等变函数；但在处理较长序列时，则丧失了这种能力。此外，还确定了浮点变换器内部存在的最基本等变结构，并考察了不同类型位置编码对模型表示能力的影响。

Result: 研究发现，浮点变换器不仅可以表示非置换等变函数，而且在特定条件下（即序列长度有限）能够覆盖所有置换等变函数的表示。然而，一旦序列变得过长，这种全面性便不再成立。另一个重要发现是，除了最简单的形式外，任何形式的位置编码实际上都可能削弱浮点变换器的表示力。

Conclusion: 这项工作揭示了浮点变换器在不同条件下的独特表示特性，强调了考虑计算精度限制的重要性。结果表明，虽然浮点变换器具备一定的灵活性来表示多种类型函数，但对于特别长的序列或使用复杂位置编码的情况，其性能可能会受到显著影响。

Abstract: The study on the expressive power of transformers shows that transformers are permutation equivariant, and they can approximate all permutation-equivariant continuous functions on a compact domain. However, these results are derived under real parameters and exact operations, while real implementations on computers can only use a finite set of numbers and inexact machine operations with round-off errors. In this work, we investigate the representability of floating-point transformers that use floating-point parameters and floating-point operations. Unlike existing results under exact operations, we first show that floating-point transformers can represent a class of non-permutation-equivariant functions even without positional encoding. Furthermore, we prove that floating-point transformers can represent all permutation-equivariant functions when the sequence length is bounded, but they cannot when the sequence length is large. We also found the minimal equivariance structure in floating-point transformers, and show that all non-trivial additive positional encoding can harm the representability of floating-point transformers.

</details>


### [33] [On the Effects of Adversarial Perturbations on Distribution Robustness](https://arxiv.org/abs/2601.16464)
*Yipei Wang,Zhaoying Pan,Xiaoqian Wang*

Main category: cs.LG

TL;DR: 本文分析了对抗鲁棒性和分布鲁棒性之间的权衡，发现对具有中等偏差的数据施加$\ell_\infty$扰动可以提高分布鲁棒性，并且当简单性偏差导致依赖核心特征时，在高度偏斜的数据上这种提升仍然存在。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗鲁棒性和分布鲁棒性都旨在确保模型的可靠性能，但之前的研究揭示了这两者之间存在权衡：对抗训练可能会增加对虚假特征的依赖，这可能损害分布鲁棒性，尤其是在某些代表性不足的子群体中的表现。因此，研究旨在通过研究在扰动数据上训练的模型来提供每步对抗训练的可处理替代方案，同时探索这一权衡现象背后的更细致机制。

Method: 本研究提出了对抗性和分布鲁棒性的理论分析方法，包括但不限于使用$\ell_\infty$扰动作为研究手段之一。

Result: 研究发现了当数据具有中等偏差时，对其应用$\ell_\infty$扰动实际上能够增加分布鲁棒性；此外，在数据高度偏斜且简单性偏差促使模型依赖于核心特征（即表现出更大的特征可分性）的情况下，这种分布鲁棒性的增益仍然保持。

Conclusion: 除了进一步确认对抗鲁棒性和分布鲁棒性之间的权衡关系外，该工作还强调了特征可分性在其中所扮演的角色，指出忽视这一点可能导致关于鲁棒性的误导性结论。

Abstract: Adversarial robustness refers to a model's ability to resist perturbation of inputs, while distribution robustness evaluates the performance of the model under data shifts. Although both aim to ensure reliable performance, prior work has revealed a tradeoff in distribution and adversarial robustness. Specifically, adversarial training might increase reliance on spurious features, which can harm distribution robustness, especially the performance on some underrepresented subgroups. We present a theoretical analysis of adversarial and distribution robustness that provides a tractable surrogate for per-step adversarial training by studying models trained on perturbed data. In addition to the tradeoff, our work further identified a nuanced phenomenon that $\ell_\infty$ perturbations on data with moderate bias can yield an increase in distribution robustness. Moreover, the gain in distribution robustness remains on highly skewed data when simplicity bias induces reliance on the core feature, characterized as greater feature separability. Our theoretical analysis extends the understanding of the tradeoff by highlighting the interplay of the tradeoff and the feature separability. Despite the tradeoff that persists in many cases, overlooking the role of feature separability may lead to misleading conclusions about robustness.

</details>


### [34] [A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study](https://arxiv.org/abs/2601.16467)
*Maxwell Reynolds,Chaitanya Srinivasan,Vijay Cherupally,Michael Leone,Ke Yu,Li Sun,Tigmanshu Chaudhary,Andreas Pfenning,Kayhan Batmanghelich*

Main category: cs.LG

TL;DR: 本文介绍了一种新的自监督学习框架R-NCE，用于从结构MRI中发现更有效的阿尔茨海默病生物标志物。R-NCE不仅在疾病分类、转化预测等方面优于传统特征和其他SSL方法，并且通过脑年龄差距（BAG）测量和全基因组关联研究展示了其生物学相关性。


<details>
  <summary>Details</summary>
Motivation: 当前利用结构MRI进行阿尔茨海默病检测主要依赖于手工设计的特征如皮层厚度或体积，而现有的自监督学习(SSL)方法在此类任务中的表现未能超越基于FreeSurfer的方法。本研究旨在探索一种新的SSL框架来从相同的MRI数据中挖掘出更为强大的生物标志物。

Method: 提出了一种名为残差噪声对比估计(R-NCE)的新自监督学习框架，该方法结合了辅助的FreeSurfer特征同时最大化额外的增强不变信息。

Result: 实验表明，R-NCE在包括AD转换预测在内的多个基准测试中超过了传统特征及其他现有SSL方法的表现。此外，通过计算脑年龄差距(BAG)并执行全基因组关联研究，R-NCE-BAG显示出高遗传率以及与MAPT和IRAG1基因之间的关联，这些发现指向了神经退行性和脑血管过程的重要性。

Conclusion: R-NCE作为一种新颖的自监督学习策略，在改善阿尔茨海默病早期检测与监测方面展现出巨大潜力。它不仅提高了诊断准确性，而且揭示了与疾病相关的生物学机制。

Abstract: Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.

</details>


### [35] [Robust Categorical Data Clustering Guided by Multi-Granular Competitive Learning](https://arxiv.org/abs/2601.16491)
*Shenghong Cai,Yiqun Zhang,Xiaopeng Luo,Yiu-Ming Cheung,Hong Jia,Peng Liu*

Main category: cs.LG

TL;DR: 提出了一种多粒度竞争惩罚学习(MGCPL)算法和基于MGCPL编码的聚类聚合策略(CAME)，用于处理类别数据中的嵌套粒度聚类效应。该方法能够自动探索多层次聚类的嵌套分布，对不同领域的类别数据集具有很高的鲁棒性，并且由于其线性时间复杂度，在大规模数据集上具有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 类别特征在大数据分析任务中非常常见，但它们通常只包含有限数量的质量可能值，导致类别数据隐含的离散距离空间中普遍存在嵌套粒度聚类效应。这种效应使得数据对象经常重叠形成小而紧凑的聚类，相似的小聚类又会形成更大的聚类。然而，由于类别数据值是定性的，无法像欧氏距离那样明确定义距离空间，这对类别数据的聚类分析带来了巨大挑战。

Method: 设计了多粒度竞争惩罚学习（MGCPL）算法，允许潜在聚类相互调优并分阶段收敛到不同数量的自然紧凑聚类。此外，提出了基于MGCPL编码（CAME）的聚类聚合策略，首先根据学习到的多粒度分布对数据对象进行编码，然后对编码结果执行最终聚类。

Result: 所提出的MGCPL引导的类别数据聚类（MCDC）方法能够自动探索多粒度聚类的嵌套分布，对于来自各个领域的类别数据集表现出极高的鲁棒性。由于其线性时间复杂度，MCDC可以很好地扩展到大规模数据集，并有望通过预分区数据集或计算节点来促进分布式计算。

Conclusion: 实验表明，与现有最先进方法相比，MCDC方法在各种实际公开数据集上展示了优越性。

Abstract: Data set composed of categorical features is very common in big data analysis tasks. Since categorical features are usually with a limited number of qualitative possible values, the nested granular cluster effect is prevalent in the implicit discrete distance space of categorical data. That is, data objects frequently overlap in space or subspace to form small compact clusters, and similar small clusters often form larger clusters. However, the distance space cannot be well-defined like the Euclidean distance due to the qualitative categorical data values, which brings great challenges to the cluster analysis of categorical data. In view of this, we design a Multi-Granular Competitive Penalization Learning (MGCPL) algorithm to allow potential clusters to interactively tune themselves and converge in stages with different numbers of naturally compact clusters. To leverage MGCPL, we also propose a Cluster Aggregation strategy based on MGCPL Encoding (CAME) to first encode the data objects according to the learned multi-granular distributions, and then perform final clustering on the embeddings. It turns out that the proposed MGCPL-guided Categorical Data Clustering (MCDC) approach is competent in automatically exploring the nested distribution of multi-granular clusters and highly robust to categorical data sets from various domains. Benefiting from its linear time complexity, MCDC is scalable to large-scale data sets and promising in pre-partitioning data sets or compute nodes for boosting distributed computing. Extensive experiments with statistical evidence demonstrate its superiority compared to state-of-the-art counterparts on various real public data sets.

</details>


### [36] [BoostFGL: Boosting Fairness in Federated Graph Learning](https://arxiv.org/abs/2601.16496)
*Zekai Chen,Kairui Yang,Xunkai Li,Henan Sun,Zhihan Zhang,Jia Li,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为BoostFGL的框架，旨在解决联邦图学习（FGL）中由于标签偏斜、拓扑混淆和聚合稀释导致的不公平问题。通过客户端节点增强、客户端拓扑增强以及服务器端模型增强三种机制，BoostFGL能够显著提高公平性同时保持良好的整体性能。实验结果表明，在9个数据集上，BoostFGL相比其他FGL方法在Overall-F1指标上提高了8.43%。


<details>
  <summary>Details</summary>
Motivation: 当前的联邦图学习（FGL）方法虽然能够实现较高的整体准确率，但这种平均表现可能掩盖了对不利节点群体性能严重下降的问题。从公平性的角度来看，这种差异主要来源于三个相互关联的因素：偏向多数模式的标签偏斜、消息传播中的拓扑混淆以及来自困难客户端更新信息的聚合稀释。为了解决这些问题并提高FGL的公平性，作者提出了BoostFGL框架。

Method: BoostFGL是一个专为提升公平性设计的联邦图学习框架，它包含了三个协同工作的机制：
- 客户端节点增强：调整本地训练信号以突出那些系统性服务不足的节点。
- 客户端拓扑增强：重新分配传播重点至可靠但未充分利用的结构，并减弱误导性邻居的影响。
- 服务器端模型增强：执行难度与可靠性感知聚合，保留困难客户端提供的有用更新同时稳定全局模型。

Result: 通过对9个不同数据集进行广泛的实验评估，结果显示BoostFGL能够在保持竞争力的整体性能的同时，显著提高公平性。特别是在Overall-F1得分上，相较于强大的FGL基准方法，BoostFGL实现了8.43%的提升。

Conclusion: 本研究揭示了现有FGL方法中存在的公平性问题，并通过引入BoostFGL框架提供了一个有效的解决方案。该框架通过特定的设计来减轻因标签偏斜、拓扑混淆及聚合稀释所造成的负面影响，从而不仅增强了模型对于弱势群体的表现，还进一步提升了整体性能。

Abstract: Federated graph learning (FGL) enables collaborative training of graph neural networks (GNNs) across decentralized subgraphs without exposing raw data. While existing FGL methods often achieve high overall accuracy, we show that this average performance can conceal severe degradation on disadvantaged node groups. From a fairness perspective, these disparities arise systematically from three coupled sources: label skew toward majority patterns, topology confounding in message propagation, and aggregation dilution of updates from hard clients. To address this, we propose \textbf{BoostFGL}, a boosting-style framework for fairness-aware FGL. BoostFGL introduces three coordinated mechanisms: \ding{182} \emph{Client-side node boosting}, which reshapes local training signals to emphasize systematically under-served nodes; \ding{183} \emph{Client-side topology boosting}, which reallocates propagation emphasis toward reliable yet underused structures and attenuates misleading neighborhoods; and \ding{184} \emph{Server-side model boosting}, which performs difficulty- and reliability-aware aggregation to preserve informative updates from hard clients while stabilizing the global model. Extensive experiments on 9 datasets show that BoostFGL delivers substantial fairness gains, improving Overall-F1 by 8.43\%, while preserving competitive overall performance against strong FGL baselines.

</details>


### [37] [kNN-Graph: An adaptive graph model for $k$-nearest neighbors](https://arxiv.org/abs/2601.16509)
*Jiaye Li,Gang Chen,Hang Xu,Shichao Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合HNSW图和预计算投票机制的自适应图模型，以解决k-最近邻算法在大规模应用中的推理速度与准确性之间的权衡问题。该模型通过将邻居选择和加权的计算负担转移到训练阶段，实现了快速推理而不牺牲分类精度，并在六个不同数据集上进行了基准测试，证明了其实时性能和准确性。


<details>
  <summary>Details</summary>
Motivation: k-最近邻(kNN)算法作为人工智能中非参数分类的基础，在大规模应用部署中面临推理速度与准确性的计算权衡问题。现有近似最近邻解决方案虽然加快了检索速度，但往往降低了分类精度且难以适应性选择最佳邻居数量（k）。

Method: 研究者们提出了一种新的自适应图模型，该模型通过整合层次可导航小世界(HNSW)图与一种预计算投票机制来彻底转移邻居选择与加权过程中的计算负担至训练阶段。这种拓扑结构允许高层图实现快速导航，而低层则编码精确、节点特定的决策边界以及自适应邻居数目。

Result: 通过对八个最先进基线方法及六个多样化数据集进行基准测试，证实了所提架构能够在不牺牲分类准确度的情况下显著加速推理过程，达到实时性能表现。

Conclusion: 本研究为kNN长期存在的推理瓶颈提供了一个可扩展、稳健的解决方案，建立了基于图的非参数学习新范式。

Abstract: The k-nearest neighbors (kNN) algorithm is a cornerstone of non-parametric classification in artificial intelligence, yet its deployment in large-scale applications is persistently constrained by the computational trade-off between inference speed and accuracy. Existing approximate nearest neighbor solutions accelerate retrieval but often degrade classification precision and lack adaptability in selecting the optimal neighborhood size (k). Here, we present an adaptive graph model that decouples inference latency from computational complexity. By integrating a Hierarchical Navigable Small World (HNSW) graph with a pre-computed voting mechanism, our framework completely transfers the computational burden of neighbor selection and weighting to the training phase. Within this topological structure, higher graph layers enable rapid navigation, while lower layers encode precise, node-specific decision boundaries with adaptive neighbor counts. Benchmarking against eight state-of-the-art baselines across six diverse datasets, we demonstrate that this architecture significantly accelerates inference speeds, achieving real-time performance, without compromising classification accuracy. These findings offer a scalable, robust solution to the long-standing inference bottleneck of kNN, establishing a new structural paradigm for graph-based nonparametric learning.

</details>


### [38] [Finite-Time Analysis of Gradient Descent for Shallow Transformers](https://arxiv.org/abs/2601.16514)
*Enes Arda,Semih Cayci,Atilla Eryilmaz*

Main category: cs.LG

TL;DR: 本研究分析了浅层Transformer在核机制下的训练情况，揭示了模型宽度与样本量对数成正比关系以及优化误差不依赖于序列长度的特点。


<details>
  <summary>Details</summary>
Motivation: 理解为什么Transformer表现如此出色是一个挑战，特别是在其非凸优化景观方面。

Method: 采用具有m个独立头的浅层Transformer，并通过投影梯度下降法在核机制下进行训练。

Result: 发现所需模型宽度仅以样本大小n的对数形式增长；优化误差与序列长度T无关。这与循环架构形成鲜明对比，后者优化误差可能随T指数级增长。

Conclusion: 这项工作提供了关于Transformer性能优越性的新见解，特别是它们如何在长序列上保持稳定的优化性能，尽管需要更多的内存来保存整个上下文。

Abstract: Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer's memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.

</details>


### [39] [DANCE: Dynamic, Available, Neighbor-gated Condensation for Federated Text-Attributed Graphs](https://arxiv.org/abs/2601.16519)
*Zekai Chen,Haodong Lu,Xunkai Li,Henan Sun,Jia Li,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的文本属性图联邦学习（TAG-FGL）范式DANCE，通过引入图凝聚技术解决了现有TAG-FGL方法中的高开销、次优性能和解释性差的问题。DANCE不仅提高了模型的准确性，还减少了所需令牌数量，并增强了模型预测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，在联邦图学习中利用文本属性变得越来越重要。然而，当前的方法在处理长文本时面临高昂的成本、次优的表现以及由LLM引起的黑盒问题。为了解决这些问题，研究提出了DANCE框架。

Method: DANCE通过实施轮次级别的模型内循环凝聚更新来解决次优性能问题，同时保存可本地检查的证据包以增强解释性。该方法旨在减少计算负担的同时提高模型对特定客户端的适应性和透明度。

Result: 实验表明，在8个TAG数据集上，与基线相比，DANCE能够在压缩比为8%的情况下将准确率提高2.33%，同时减少33.42%的令牌使用量。

Conclusion: DANCE成功地解决了TAG-FGL领域内的关键挑战，包括成本控制、性能优化及解释性的改善，展示了其作为未来研究方向的潜力。

Abstract: Federated graph learning (FGL) enables collaborative training on graph data across multiple clients. With the rise of large language models (LLMs), textual attributes in FGL graphs are gaining attention. Text-attributed graph federated learning (TAG-FGL) improves FGL by explicitly leveraging LLMs to process and integrate these textual features. However, current TAG-FGL methods face three main challenges: \textbf{(1) Overhead.} LLMs for processing long texts incur high token and computation costs. To make TAG-FGL practical, we introduce graph condensation (GC) to reduce computation load, but this choice also brings new issues. \textbf{(2) Suboptimal.} To reduce LLM overhead, we introduce GC into TAG-FGL by compressing multi-hop texts/neighborhoods into a condensed core with fixed LLM surrogates. However, this one-shot condensation is often not client-adaptive, leading to suboptimal performance. \textbf{(3) Interpretability.} LLM-based condensation further introduces a black-box bottleneck: summaries lack faithful attribution and clear grounding to specific source spans, making local inspection and auditing difficult. To address the above issues, we propose \textbf{DANCE}, a new TAG-FGL paradigm with GC. To improve \textbf{suboptimal} performance, DANCE performs round-wise, model-in-the-loop condensation refresh using the latest global model. To enhance \textbf{interpretability}, DANCE preserves provenance by storing locally inspectable evidence packs that trace predictions to selected neighbors and source text spans. Across 8 TAG datasets, DANCE improves accuracy by \textbf{2.33\%} at an \textbf{8\%} condensation ratio, with \textbf{33.42\%} fewer tokens than baselines.

</details>


### [40] [Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs](https://arxiv.org/abs/2601.16527)
*Xianya Fang,Feiyang Ren,Xiang Chen,Yu Tian,Zhen Bi,Haiyang Yu,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SARE的新方法，通过将去学习过程视为目标最小-最大优化问题，并利用Targeted-SAM机制来明确地平滑围绕幻觉概念的损失景观，从而有效抑制多模态大语言模型中的物体幻觉现象。实验表明，SARE不仅在删除效果上优于基线方法，而且能够保持对再学习和参数更新的持续幻觉抑制，验证了几何稳定化的有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然强大，但容易产生物体幻觉，这描述了不存在的实体并损害了可靠性。尽管最近的去学习方法试图缓解这个问题，但研究者们发现了一个关键缺陷：结构脆弱性。标准擦除方法只能实现表面的压制，使得模型陷入尖锐的局部极小值，在轻量级再学习后幻觉会灾难性地重新出现。

Method: 提出了SARE框架，将去学习视作一个有针对性的最小-最大优化问题，并采用Targeted-SAM机制来直接平滑与幻觉概念相关的损失景观区域。通过在模拟最坏情况下的参数扰动下抑制幻觉，该框架确保了即使在权重发生变化时也能稳健地移除幻觉。

Result: 广泛的实验证明，SARE在删除效率方面显著优于基准方法，同时还能保持一般的生成质量。更重要的是，面对再学习和参数更新时，它能维持持续的幻觉抑制作用，证实了几何稳定化策略的有效性。

Conclusion: SARE提供了一种有效的解决方案来解决多模态大语言模型中的物体幻觉问题，通过几何稳定化实现了长期且稳定的幻觉抑制，为提高此类模型的可靠性和实用性开辟了新的途径。

Abstract: Multimodal LLMs are powerful but prone to object hallucinations, which describe non-existent entities and harm reliability. While recent unlearning methods attempt to mitigate this, we identify a critical flaw: structural fragility. We empirically demonstrate that standard erasure achieves only superficial suppression, trapping the model in sharp minima where hallucinations catastrophically resurge after lightweight relearning. To ensure geometric stability, we propose SARE, which casts unlearning as a targeted min-max optimization problem and uses a Targeted-SAM mechanism to explicitly flatten the loss landscape around hallucinated concepts. By suppressing hallucinations under simulated worst-case parameter perturbations, our framework ensures robust removal stable against weight shifts. Extensive experiments demonstrate that SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Crucially, it maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.

</details>


### [41] [A Collision-Free Hot-Tier Extension for Engram-Style Conditional Memory: A Controlled Study of Training Dynamics](https://arxiv.org/abs/2601.16531)
*Tao Lin*

Main category: cs.LG

TL;DR: 研究发现，无碰撞设计并不总是提高验证损失。通过分层评估，揭示了训练过程中'热到冷优势翻转'的现象，并且无碰撞配置比有碰撞的基准更早发生翻转，表明碰撞起到了隐式正则化的作用。此外，门控机制的学习偏好在翻转后仍然保持，导致对高损失位置分配更高的权重。结论是改善查找精度并不能保证更好的训练结果，关键限制可能在于门控信用分配而非索引准确性，碰撞带来的噪声可能提供了有益的正则化效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨高频键碰撞是否为Engram风格条件记忆的主要瓶颈。

Method: 引入Engram-Nine作为无碰撞热层扩展，利用最小完美哈希函数(MPHF)映射最频繁的n-grams，同时保留原始多头哈希查找作为冷层。通过路由分层评估来分解每个令牌损失为热/冷贡献。

Result: 无碰撞设计并未始终如一地改善验证损失；观察到了'热至冷优势反转'现象；无碰撞配置比容易发生碰撞的基础模型更早经历这一反转；存在门控不匹配问题。

Conclusion: 仅提高查找精度不足以确保更好的训练成果，主要限制可能是门控信用分配的问题而非索引准确性，由碰撞引起的噪声可能提供了不应被简单消除的有益正则化。

Abstract: We investigate whether high-frequency key collisions are a primary bottleneck in Engram-style conditional memory. To isolate the effect of collisions, we introduce Engram-Nine, a collision-free hot-tier extension that maps the most frequent n-grams through a Minimal Perfect Hash Function (MPHF) while retaining the original multi-head hashed lookup as a cold tier. Under a strictly iso-parameter setup, the collision-free design does not consistently improve validation loss.
  Through route-stratified evaluation (decomposing per-token loss into hot/cold contributions), we uncover a consistent "hot-to-cold advantage flip" during training: hot (high-frequency) positions initially have lower loss, but cold positions eventually surpass them. Crucially, collision-free configurations flip earlier than collision-prone baselines, suggesting that collisions act as implicit regularization. We also identify a gating mismatch: the gate learns to favor hot positions early in training, but this preference persists even after the flip, assigning higher weights to positions with higher loss.
  Our findings suggest that improving lookup precision alone does not guarantee better training outcomes. The dominant limitation may lie in gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that should not be naively eliminated.

</details>


### [42] [Understanding and Improving UMAP with Geometric and Topological Priors: The JORC-UMAP Algorithm](https://arxiv.org/abs/2601.16552)
*Xiaobin Li,Run Zhang*

Main category: cs.LG

TL;DR: 本研究提出了一种名为JORC-UMAP的新方法，通过引入Ollivier-Ricci曲率作为几何先验，并结合Jaccard相似性作为拓扑先验，来改善UMAP在处理高维数据时的局限性。实验表明，JORC-UMAP能够更有效地减少结构撕裂和崩溃现象，同时保持计算效率，从而提供更准确的数据可视化。


<details>
  <summary>Details</summary>
Motivation: UMAP等非线性降维技术虽然广泛用于高维数据可视化，但其基于局部欧几里得距离的假设往往无法捕捉到内在流形几何结构，导致了拓扑撕裂和结构坍塌的问题。此外，UMAP对k近邻图非常敏感，这进一步加剧了上述问题。

Method: 研究人员提出了JORC-UMAP方法，该方法通过引入Ollivier-Ricci曲率强化几何瓶颈处的边并减少冗余链接，同时利用Jaccard相似度确保邻域一致性以克服噪声敏感性问题。

Result: 实验结果证明，与标准UMAP及其他降维方法相比，JORC-UMAP在合成及真实世界数据集上更能有效减少结构撕裂和坍塌现象，且SVM准确性和三元组保留得分更高，同时保持了良好的计算效率。

Conclusion: 这项工作为UMAP提供了一个几何感知增强方案，有助于实现更加忠实于原始数据结构的可视化效果。

Abstract: Nonlinear dimensionality reduction techniques, particularly UMAP, are widely used for visualizing high-dimensional data. However, UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, leading to topological tearing and structural collapse. We identify UMAP's sensitivity to the k-nearest neighbor graph as a key cause. To address this, we introduce Ollivier-Ricci curvature as a geometric prior, reinforcing edges at geometric bottlenecks and reducing redundant links. Since curvature estimation is noise-sensitive, we also incorporate a topological prior using Jaccard similarity to ensure neighborhood consistency. The resulting method, JORC-UMAP, better distinguishes true manifold structure from spurious connections. Experiments on synthetic and real-world datasets show that JORC-UMAP reduces tearing and collapse more effectively than standard UMAP and other DR methods, as measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency. This work offers a geometry-aware enhancement to UMAP for more faithful data visualization.

</details>


### [43] [Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach](https://arxiv.org/abs/2601.16568)
*Abdurahman Maarouf,Alket Bakiaj,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出了一种基于大型语言模型的上下文学习框架kNN-ICL，用于初创企业成功预测。该方法不需要模型训练，并且仅利用少量标记的企业作为示例。实验表明，与传统的监督机器学习基线和纯上下文学习相比，kNN-ICL方法在预测准确性上表现更优。此外，研究还发现即使只有50个示例也能达到较高的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 风险投资（VC）对于早期创业公司的投资如果成功的话可以带来高回报，但因数据稀缺问题使得传统依赖大规模标记数据集进行训练的机器学习方法难以有效应用于早期创业公司成功的预测。为了解决这个问题，文章提出了一个新框架。

Method: 提出了kNN-ICL，一种基于k近邻算法的上下文学习框架，通过选取与待评估项目最相似的历史案例作为示范样本来进行预测，而无需经过模型训练过程。

Result: 使用Crunchbase的真实数据测试后显示，kNN-ICL方法比监督式机器学习基线以及普通的上下文学习方法具有更高的预测精度。而且，当有至少50个示例时就能获得很好的平衡准确度。

Conclusion: 研究表明，在数据稀缺环境下，上下文学习可以成为VC公司决策的有效工具。

Abstract: Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.

</details>


### [44] [Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting](https://arxiv.org/abs/2601.16632)
*Haonan Yang,Jianchao Tang,Zhuo Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为DPAD的框架，该框架通过动态解耦和利用时间序列中的复杂模式来改进时间序列预测。它包括一个动态双原型库（DDP）和一种双路径上下文感知路由机制（DPC），并通过解缠引导损失（DGLoss）确保每个原型库专注于其指定角色。实验表明，DPAD能够一致地提高最先进模型在多种实际基准测试中的预测性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列预测方法虽然通过修改架构或引入新的增强策略提高了预测性能，但往往无法动态解耦并利用时间序列中复杂的、交织在一起的时间模式，导致学习到的是静态、平均化的表示形式，缺乏上下文感知能力。

Method: 提出了Dual-Prototype Adaptive Disentanglement (DPAD) 框架，其中包括：1. 动态双原型库（DDP），由捕捉主要趋势或季节性模式的公共模式库以及动态记忆关键但不频繁事件的稀有模式库组成；2. 双路径上下文感知路由(DPC)机制，用于从DDP选择性检索特定上下文模式表示以增强输出；3. 解缠引导损失（DGLoss），确保每个原型库在其指定角色上专精的同时保持全面覆盖。

Result: 广泛的实验证明了DPAD可以一致地提高最先进的模型在多样化的真实世界基准测试中的预测性能和可靠性。

Conclusion: DPAD作为一种模型无关的辅助方法，赋予了预测模型模式解耦和上下文感知适应的能力，从而有效提升了时间序列预测的质量。

Abstract: Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representations that lack context-aware capabilities. To address this, we propose the Dual-Prototype Adaptive Disentanglement framework (DPAD), a model-agnostic auxiliary method that equips forecasting models with the ability of pattern disentanglement and context-aware adaptation. Specifically, we construct a Dynamic Dual-Prototype bank (DDP), comprising a common pattern bank with strong temporal priors to capture prevailing trend or seasonal patterns, and a rare pattern bank dynamically memorizing critical yet infrequent events, and then an Dual-Path Context-aware routing (DPC) mechanism is proposed to enhance outputs with selectively retrieved context-specific pattern representations from the DDP. Additionally, we introduce a Disentanglement-Guided Loss (DGLoss) to ensure that each prototype bank specializes in its designated role while maintaining comprehensive coverage. Comprehensive experiments demonstrate that DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

</details>


### [45] [Provably Robust Bayesian Counterfactual Explanations under Model Changes](https://arxiv.org/abs/2601.16659)
*Jamie Duell,Xiuyi Fan*

Main category: cs.LG

TL;DR: 本文提出了一种生成概率安全的反事实解释（PSCE）的方法，该方法确保了高预测置信度和低预测方差，并在模型变化下提供了正式的概率保证。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，由于机器学习模型经常更新，现有的反事实解释很快会变得无效或不可靠。为了应对这一问题，作者们旨在开发一种能够在模型变动情况下依旧保持可靠性的反事实解释方法。

Method: 基于贝叶斯原理，提出了概率安全的反事实解释(PSCE)，通过δ-安全性确保高预测信心，ε-稳健性确保低预测方差。不确定性感知约束被整合进优化框架内。

Result: 通过多种数据集验证了所提方法的有效性，并与最先进贝叶斯CE方法进行了对比，结果表明PSCE不仅生成的反事实解释更合理、更具区分度，而且在模型变更时具有可证明的鲁棒性。

Conclusion: 本研究成功地引入了一种新方法——概率安全的反事实解释（PSCE），它能够为机器学习预测提供即使在模型更新的情况下也依然有效的解释。

Abstract: Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that are $δ$-safe, to ensure high predictive confidence, and $ε$-robust to ensure low predictive variance. Based on Bayesian principles, PSCE provides formal probabilistic guarantees for CEs under model changes which are adhered to in what we refer to as the $\langle δ, ε\rangle$-set. Uncertainty-aware constraints are integrated into our optimization framework and we validate our method empirically across diverse datasets. We compare our approach against state-of-the-art Bayesian CE methods, where PSCE produces counterfactual explanations that are not only more plausible and discriminative, but also provably robust under model change.

</details>


### [46] [Dynamic Expert-Guided Model Averaging for Causal Discovery](https://arxiv.org/abs/2601.16715)
*Adrick Tench,Thomas Demeester*

Main category: cs.LG

TL;DR: 本文提出了一种灵活的模型平均方法，该方法利用动态请求的专家知识来集成多种因果发现算法。实验表明，即使在面对不完美专家（如大型语言模型）和噪声数据的情况下，这种方法也是有效的。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健领域理解因果关系至关重要。然而，实践者面临众多算法选择而无明确的最佳选项，并且实际应用中常常遇到违反常见因果发现算法假设的问题，需要依赖专家知识。

Method: 提出一种结合动态请求专家知识的模型平均方法，以集成各种因果发现算法。

Result: 实验显示，对于不完美的专家（例如大型语言模型）以及干净和有噪声的数据，所提方法均有效。还分析了不同程度的专家准确性对结果的影响，并评估了大型语言模型在临床因果发现中的能力。

Conclusion: 通过结合动态获取的专家意见与多样化的因果发现技术，可以提高因果推断的准确性和鲁棒性，为医疗领域的从业者提供了有价值的见解。

Abstract: Understanding causal relationships is critical for healthcare. Accurate causal models provide a means to enhance the interpretability of predictive models, and furthermore a basis for counterfactual and interventional reasoning and the estimation of treatment effects. However, would-be practitioners of causal discovery face a dizzying array of algorithms without a clear best choice. This abundance of competitive algorithms makes ensembling a natural choice for practical applications. At the same time, real-world use cases frequently face challenges that violate the assumptions of common causal discovery algorithms, forcing heavy reliance on expert knowledge. Inspired by recent work on dynamically requested expert knowledge and LLMs as experts, we present a flexible model averaging method leveraging dynamically requested expert knowledge to ensemble a diverse array of causal discovery algorithms. Experiments demonstrate the efficacy of our method with imperfect experts such as LLMs on both clean and noisy data. We also analyze the impact of different degrees of expert correctness and assess the capabilities of LLMs for clinical causal discovery, providing valuable insights for practitioners.

</details>


### [47] [Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing](https://arxiv.org/abs/2601.16812)
*Francesca Lanzillotta,Chiara Albisani,Davide Pucci,Daniele Baracchi,Alessandro Piva,Matteo Lapucci*

Main category: cs.LG

TL;DR: 本文提出了一种序列惩罚方法来处理学习任务中的严格约束，该方法在深度学习场景下具有合理的收敛保证，并通过图像处理任务的实验证明了其实际可行性。


<details>
  <summary>Details</summary>
Motivation: 在许多学习任务中，对单个数据样本处理的某些要求应该被正式化为底层优化问题中的严格约束，而不是通过任意惩罚手段。

Method: 提出了一种序列惩罚方法，该方法允许恰当地处理这些约束条件。

Result: 所提出的算法在深度学习场景下的合理假设下显示出了收敛性保证。此外，在图像处理任务上的实验结果表明这种方法实际上是可以使用的。

Conclusion: 序列惩罚方法对于处理学习任务中的严格约束是有效的，并且在实践中（如图像处理任务）显示出良好的应用前景。

Abstract: In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.

</details>


### [48] [Calibrated Probabilistic Interpolation for GEDI Biomass](https://arxiv.org/abs/2601.16834)
*Robin Young,Srinivasan Keshav*

Main category: cs.LG

TL;DR: 本研究针对NASA的GEDI任务中稀疏激光雷达观测数据在不同地形上的插值问题，提出了一种新的方法——注意力神经过程（ANPs），该方法能够根据局部观测集和地理空间基础模型嵌入条件性地做出预测。相比传统的机器学习方法如随机森林和XGBoost，ANPs能够在复杂地形上扩展不确定性估计，在同质区域收缩，从而实现近乎理想的不确定性校准。此外，通过少量样本适应展示了其跨区域转移时的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法在处理GEDI任务中的生物量映射时，未能充分考虑地形异质性对预测难度的影响，导致预测区间不够准确。这些问题源于将集成方差与偶然不确定性混淆，并忽略了局部空间背景。因此，需要一种新的方法来解决这个问题，特别是在地球观测的大规模应用中。

Method: 提出了注意力神经过程(ANPs)，这是一种概率元学习框架，它明确地基于局部观测集和地理空间基础模型嵌入进行预测。与固定的集成方法不同，ANPs学习了一个灵活的空间协方差函数，允许不确定性估计随着环境复杂性的增加而扩大或减少。

Result: 通过对包括热带亚马逊森林到北方和高山生态系统在内的五个不同生物群落的研究验证了该方法的有效性。结果表明，ANPs不仅达到了竞争性的准确性，而且保持了接近理想的不确定性校准。此外，通过少量样本适应实验显示，使用极少的本地数据即可恢复跨区域转移时大部分性能差距。

Conclusion: 本研究表明，ANPs提供了一种可扩展且理论严谨的方法，用于大陆尺度的地球观测任务中代替传统的集成方差方法。这种方法不仅提高了预测精度，还改善了不确定性估计的质量，为未来的地球科学研究提供了有力支持。

Abstract: Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context.
  To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation.

</details>


### [49] [The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics](https://arxiv.org/abs/2601.16849)
*Henri Nikoleit,Ankit Anand,Anurag Murty Naredla,Heiko Röglin*

Main category: cs.LG

TL;DR: 本文展示了人类与大语言模型合作在解决理论计算机科学中的开放问题方面的力量，通过改进FunSearch算法的输出，为组合优化问题提供了新的下界估计，并强调了人类专家在将大语言模型生成的模式转化为严谨数学构造过程中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 旨在展示人类与大型语言模型（LLM）的合作如何有效地解决理论计算机科学领域内的开放性难题，特别是针对那些长久以来未见显著进展的组合优化问题。

Method: 研究者们基于FunSearch算法的结果进行迭代和优化，专注于生成对抗实例来测试标准启发式方法的表现极限。通过对算法输出的人工精炼，他们在层次k-中位数聚类、背包问题以及Lovász汽油问题的推广等多个经典问题上取得了突破。

Result: 成功地提高了几个重要组合优化问题的标准启发式解法的下界估计，包括一些十年来几乎没有进展的问题。这表明通过结合人类专业知识与LLM提供的初始模式可以克服长期存在的障碍。

Conclusion: 本研究表明，虽然LLM能够提供重要的初始思路，但要将其转化为具有数学严格性和洞察力的解决方案仍然需要人类的专业知识。这突出了LLM作为数学和计算机科学研究中强有力协作工具的价值。

Abstract: We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.
  Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.

</details>


### [50] [FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization](https://arxiv.org/abs/2601.16897)
*Antesh Upadhyay,Sang Bin Moon,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: FedSGM is a novel framework for federated constrained optimization, tackling issues like functional constraints, communication bottlenecks, local updates, and partial client participation. It uses a switching gradient method with bi-directional error feedback to manage these challenges, providing convergence guarantees and experimental validation on specific tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind FedSGM is to address the significant challenges in federated learning (FL) such as functional constraints, limited communication, the need for local updates, and the reality of partial client participation. The aim is to create a unified and theoretically robust solution that can handle these aspects without requiring complex dual-variable tuning or inner solvers, thus making FL more practical and efficient.

Method: FedSGM employs a switching gradient method to provide projection-free, primal-only updates. It integrates bi-directional error feedback to deal with the communication limits and the biases from data compression, while also understanding the interaction between compression noise and multiple-step local updates. A soft switching version of FedSGM is introduced to further stabilize the updates when near the feasibility boundary.

Result: The results include theoretical convergence guarantees, demonstrating that the averaged iterate reaches the Θ(1/\sqrt{T}) rate, along with high-probability bounds that separate optimization progress from sampling noise. Experimental validation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks supports the theoretical findings.

Conclusion: FedSGM establishes a solid foundation for constrained federated learning by unifying the handling of functional constraints, compression, multiple local updates, and partial client participation. It offers a practical and efficient approach to federated learning, supported by both theoretical analysis and empirical evidence.

Abstract: We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\boldsymbol{\mathcal{O}}(1/\sqrt{T})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks.

</details>


### [51] [Embedding -based Crop Type Classification in the Groundnut Basin of Senegal](https://arxiv.org/abs/2601.16900)
*Madeline C. Lisaius,Srinivasan Keshav,Andrew Blake,Clement Atzberger*

Main category: cs.LG

TL;DR: 本研究通过建立一个包含性能、合理性、可转移性和可访问性的四部分标准来评估基于地理空间基础模型嵌入的方法（TESSERA和AlphaEarth）在塞内加尔花生盆地地区作物类型制图上的应用。结果表明，基于TESSERA的方法最符合选择标准，在一个时间迁移示例中比次优方法准确度高28%，证明了TESSERA嵌入对于塞内加尔的作物类型分类和制图任务是有效的方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有大多数卫星遥感方法不适合小农地区的条件，研究旨在寻找一种适用于这些区域的作物类型地图绘制方法，以支持粮食安全、当地生计及气候变化缓解工作。

Method: 研究者提出了一个由性能、合理性、可转移性和可访问性组成的四部分评判标准，并使用该标准评估了两种基于地理空间基础模型嵌入的方法——TESSERA与AlphaEarth——对比当前基准方法在塞内加尔花生盆地的表现。

Result: 基于TESSERA的方法在满足所提出的选择标准方面表现最佳，特别是在一个特定的时间迁移测试案例中，其准确性比第二好的方法高出28%。

Conclusion: TESSERA嵌入法被证明是针对塞内加尔作物类型分类与制图的有效手段，为小农户地区提供了一种改进的食物安全保障技术。

Abstract: Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.

</details>


### [52] [GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints](https://arxiv.org/abs/2601.16905)
*Andy Zhu,Rongzhe Wei,Yupu Gu,Pan Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为GRIP的框架，用于解决大型语言模型中Mixture-of-Experts (MoE)架构下的机器遗忘问题。通过实施特定的几何约束，GRIP能够在不影响模型效用的情况下实现知识的有效删除，从而解决了传统方法导致的表面遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘方法无法很好地应用于Mixture-of-Experts (MoE)架构，并且这些方法往往通过操纵路由器来避免真正地删除知识，这导致了模型效用的损失和表面上的知识遗忘。

Method: 提出了一个算法无关的框架——Geometric Routing Invariance Preservation (GRIP)，该框架通过对路由器梯度更新施加几何约束（具体来说是将其投影到专家特有的零空间），使得在保持已保留知识的路由稳定性的同时，路由器参数仍能在零空间内进行调整，以满足遗忘目标的要求。

Result: 大规模实验表明，在所有测试的遗忘方法下，GRIP能够消除专家选择偏移（达到超过95%的路由稳定性），同时保持其效用。

Conclusion: 通过防止现有算法利用MoE模型的路由器漏洞，GRIP成功地将现有的针对密集架构设计的遗忘研究适配到了MoE上，为提高AI安全性提供了一个新的解决方案。

Abstract: Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.

</details>


### [53] [The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning](https://arxiv.org/abs/2601.16906)
*Calarina Muslimani,Yunshu Du,Kenta Kawamoto,Kaushik Subramanian,Peter Stone,Peter Wurman*

Main category: cs.LG

TL;DR: 本研究通过引入轨迹对齐系数（TAC）来辅助强化学习从业者更准确地设置奖励函数权重，并提出了Soft-TAC，一种可微分的TAC近似形式，用于直接从人类偏好数据中训练奖励模型。实验表明，使用TAC可以提高奖励函数的表现力并减少认知负担，而基于Soft-TAC训练的模型在Gran Turismo 7赛车模拟器上表现出更加多样化的行为特征。


<details>
  <summary>Details</summary>
Motivation: 设计合适的奖励函数对于强化学习的成功至关重要，但这一过程耗时且容易出错。为了帮助从业者更好地设定奖励函数权重以及减轻手动设计奖励函数的工作量，研究人员探索了如何利用轨迹对齐系数（TAC）来改进这一过程，并进一步提出了一种新的方法——Soft-TAC，旨在直接优化奖励模型以最大化TAC值。

Method: 1. 利用TAC评估奖励函数与领域专家偏好的匹配程度。
2. 进行了一项有人参与的研究，让强化学习从业者调整Lunar Lander游戏中的奖励权重，并观察提供TAC反馈是否有助于改善结果。
3. 提出Soft-TAC，这是一种TAC的可微分近似形式，可用于作为损失函数从人类偏好数据中训练奖励模型。
4. 在Gran Turismo 7赛车模拟环境中验证了Soft-TAC的有效性。

Result: 1. 使用TAC指导奖励调整使得参与者能够创建出性能更好的奖励函数，并报告较低的认知工作负荷。
2. 即便有了TAC的帮助，手动调整奖励权重仍然是一项劳动密集型任务。
3. 通过Soft-TAC训练的奖励模型能够捕捉到特定于偏好的目标，在Gran Turismo 7中生成了定性上更为不同的行为模式。

Conclusion: 研究表明，TAC不仅能够作为指导奖励函数调整的有效工具，还能作为一种学习复杂领域内奖励模型的目标函数发挥作用。基于Soft-TAC的方法为从人类偏好数据中自动学习奖励提供了新途径。

Abstract: The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.

</details>


### [54] [Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces](https://arxiv.org/abs/2601.16907)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 本研究通过使用基于人类相似性判断训练的保序回归，构建了一个单调变换来校准预训练嵌入空间中的原始余弦相似度值，从而在保持排名相关性和局部稳定性的同时恢复其绝对值的可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练嵌入空间中的原始余弦相似度与人类判断之间显示出强烈的等级相关性，但各向异性导致了绝对值的系统性偏差：无论实际语义关联如何，分数都集中在狭窄的高度相似带中，限制了它作为定量测量的可解释性。

Method: 采用基于人类相似性判断训练的保序回归，构造一个能够实现几乎完美校准同时保持等级相关性和局部稳定性的单调变换。

Result: 该方法达到了接近完美的校准效果，并且在七种扰动类型下保持了98%的稳定性。此外，所有基于顺序的构造（如角度排序、最近邻、阈值图和基于分位数的决策）在这种变换下都是不变的。

Conclusion: 本贡献不在于替换余弦相似度，而是通过单调校准恢复其绝对值的可解释性，同时不改变其排名属性。

Abstract: While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, contrastive fine tuning), but such transformations alter geometric structure and require recomputing all embeddings.
  Using isotonic regression trained on human similarity judgments, we construct a monotonic transformation that achieves near-perfect calibration while preserving rank correlation and local stability(98% across seven perturbation types). Our contribution is not to replace cosine similarity, but to restore interpretability of its absolute values through monotone calibration, without altering its ranking properties.
  We characterize isotonic calibration as an order-preserving reparameterization and prove that all order-based constructions (angular ordering, nearest neighbors, threshold graphs and quantile-based decisions) are invariant under this transformation.

</details>


### [55] [Group-realizable multi-group learning by minimizing empirical risk](https://arxiv.org/abs/2601.16922)
*Navid Ardeshir,Samuel Deng,Daniel Hsu,Jingwen Liu*

Main category: cs.LG

TL;DR: 本文展示了在组可实现设定下，多组学习的样本复杂度相比于不可知设定有所改进。即使组的家族是无限的，只要其VC维度有限即可。然而，这种方法的实现被证明在计算上是难以处理的，并提出了一种基于非正确学习的替代方法。


<details>
  <summary>Details</summary>
Motivation: 探索在组可实现设定下，与不可知设定相比，是否可以降低多组学习的样本复杂度。

Method: 通过在组可实现概念类上进行经验风险最小化来改善样本复杂度。同时指出，直接实现该方法存在计算上的难题，并提出了基于非正确学习的解决方案。

Result: 证实了即使对于具有无限VC维度的概念类，在组可实现设定下的样本复杂度也优于不可知设定。此外，直接的经验风险最小化方法在计算上可能是棘手的。

Conclusion: 研究强调了在满足一定条件下（如有限VC维度），多组学习能够以更少的样本来达到良好的学习效果。同时也指出了未来研究中需要解决的计算效率问题。

Abstract: The sample complexity of multi-group learning is shown to improve in the group-realizable setting over the agnostic setting, even when the family of groups is infinite so long as it has finite VC dimension. The improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension. Implementing this approach is also shown to be computationally intractable, and an alternative approach is suggested based on improper learning.

</details>


### [56] [Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles](https://arxiv.org/abs/2601.16936)
*Anton Zamyatin,Patrick Indri,Sagar Malhotra,Thomas Gärtner*

Main category: cs.LG

TL;DR: BatchEnsemble, a method designed to offer ensemble-like epistemic uncertainty at lower costs, is found to underperform compared to Deep Ensembles and behaves more like a single model in terms of accuracy, calibration, and out-of-distribution detection. A study on MNIST further suggests that BatchEnsemble's members are nearly identical, indicating limited capacity for diverse predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to evaluate the effectiveness of BatchEnsemble, a technique proposed to efficiently obtain uncertainty estimates with lower computational and memory costs compared to traditional Deep Ensembles, especially in environments where resources and latency are critical factors.

Method: The research compares BatchEnsemble against Deep Ensembles and a single model baseline using various metrics such as accuracy, calibration, and out-of-distribution (OOD) detection on datasets including CIFAR10, CIFAR10C, and SVHN. Additionally, a controlled study on the MNIST dataset is conducted to investigate the diversity among BatchEnsemble's members in both function and parameter space.

Result: Results show that BatchEnsemble underperforms relative to Deep Ensembles and closely resembles the performance of a single model in terms of accuracy, calibration, and OOD detection. The MNIST study reveals that the members within BatchEnsemble are almost identical, suggesting it has a limited ability to generate distinct predictive modes, which is a characteristic expected from a true ensemble.

Conclusion: The conclusion drawn from the analysis is that, despite its aim to provide efficient uncertainty estimation, BatchEnsemble fails to deliver the benefits of a true ensemble and instead exhibits behavior more akin to that of a single model. This finding raises questions about the adequacy of BatchEnsemble as a replacement for Deep Ensembles in applications requiring robust uncertainty quantification.

Abstract: In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble.

</details>


### [57] [3D Molecule Generation from Rigid Motifs via SE(3) Flows](https://arxiv.org/abs/2601.16955)
*Roman Poletukhin,Marcel Kollovieh,Eike Eberhard,Stephan Günnemann*

Main category: cs.LG

TL;DR: 该论文提出了一种基于刚性基元的3D分子生成方法，通过SE(3)-等变生成模型从这些基元中生成新的3D分子结构。相比现有技术，此方法在分子稳定性、生成步骤效率以及分子表示压缩方面表现更优或相当。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有的三维分子结构生成通常基于单个原子层面进行，而分子图生成技术则更多地考虑片段作为其结构单元，本研究旨在将这种片段化的思想扩展到三维空间，以处理一般分子为一组刚体基序的问题。

Method: 研究者们利用了基于框架的蛋白质结构生成的进步，并将其应用于更广泛的分子上，采用SE(3)等变生成建模技术来从刚性基元出发进行全新的三维分子生成。

Result: 评估结果显示，在基准测试中，该方法与最先进技术相比具有可比性甚至更优的表现，在GEOM-Drugs上的原子稳定性超过了现有技术，同时生成步骤减少了2倍至10倍，并且相对于传统的基于原子的方法而言，分子表示被压缩了3.5倍。

Conclusion: 通过使用基于刚性基元和SE(3)-等变生成模型的新方法，能够有效地提高三维分子结构生成过程中的效率及质量。

Abstract: Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.

</details>


### [58] [Auto-Regressive Masked Diffusion Models](https://arxiv.org/abs/2601.16971)
*Mahdi Karami,Ali Ghodsi*

Main category: cs.LG

TL;DR: 本文提出了一种新的自回归掩码扩散(ARMD)模型，该模型结合了自回归模型的训练效率和基于扩散模型的并行生成能力。通过将掩码扩散过程重新定义为块因果模型，并设计了一个严格因果、排列等变的架构，使得所有条件概率可以在单个并行前向传递中计算完成。此外，还引入了一种新颖的步进并行生成策略来加速推理。实验结果显示，ARMD在标准语言建模基准上达到了最先进的性能，同时需要更少的训练步骤。


<details>
  <summary>Details</summary>
Motivation: 目前掩码扩散模型（MDMs）虽然在语言建模方面显示出了潜力，但与自回归模型相比存在性能差距，并且需要更多的训练迭代次数。为了缩小这一差距，研究者希望开发一种能够结合自回归模型高效训练特点以及基于扩散模型并行生成优势的新架构。

Method: 研究团队提出了Auto-Regressive Masked Diffusion (ARMD) 模型，这是一种旨在通过统一自回归模型的训练效率与基于扩散模型的并行生成能力来缩小两者间差距的架构。主要创新点包括：1) 将掩码扩散过程重新框架化为一个块状因果模型；2) 设计了一个严格因果、排列不变性的架构，能够在单一并行前向传递过程中计算出多个去噪步骤中的所有条件概率；3) 提出了一种新的步进式并行生成策略，可在保持全局一致性的同时加速推理过程。

Result: 实验证明，ARMD不仅在标准语言建模基准测试中达到了最先进水平，超越了现有的扩散基线模型，而且所需的训练步骤显著减少。此外，它还在并行文本生成方面树立了新标杆，有效弥合了并行解码与顺序解码之间的性能差距。

Conclusion: 通过整合自回归模型的高效训练特性和基于扩散模型的强大并行生成能力，ARMD模型成功地提高了语言建模任务的表现，并减少了所需训练时间。这标志着我们在构建更快速、更高效的自然语言处理系统方面迈出了重要一步。

Abstract: Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.

</details>


### [59] [A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs](https://arxiv.org/abs/2601.16979)
*Dayal Singh Kalra,Jean-Christophe Gagnon-Audet,Andrey Gromov,Ishita Mediratta,Kelvin Niu,Alexander H Miller,Michael Shvartsman*

Main category: cs.LG

TL;DR: Researchers introduced a computationally efficient measure, 'critical sharpness' ($λ_c$), to analyze the curvature evolution of the loss landscape in neural networks, especially for Large Language Models (LLMs). This new metric, requiring fewer resources than traditional Hessian sharpness, successfully captures key phenomena such as progressive sharpening and Edge of Stability. The study also presents 'relative critical sharpness' ($λ_c^{1\to 2}$) to assess curvatures across different optimization stages, aiding in the transition from pre-training to fine-tuning and guiding data mixing strategies. These measures offer practical tools for understanding and improving large-scale training dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to develop a more efficient method for measuring the curvature of the loss landscape in neural networks, particularly for Large Language Models (LLMs), where traditional methods like Hessian sharpness are computationally expensive and impractical. By introducing a novel measure, 'critical sharpness', the researchers aim to provide a tool that can be used at scale to better understand and optimize the training dynamics of these models, including transitions between pre-training and fine-tuning phases.

Method: The method involves the introduction and analysis of 'critical sharpness' ($λ_c$) as a computationally efficient alternative to Hessian sharpness. This measure requires significantly fewer computational resources, needing less than 10 forward passes given the update direction $Δ\mathbfθ$. The researchers also introduce 'relative critical sharpness' ($λ_c^{1\to 2}$) to evaluate the curvature changes when transitioning from one optimization task to another, such as moving from pre-training to fine-tuning. These measures were applied to OLMo-2 models up to 7 billion parameters, both during pre-training and mid-training, to demonstrate their effectiveness in capturing well-documented sharpness phenomena.

Result: The results show that 'critical sharpness' effectively captures known phenomena related to Hessian sharpness, such as progressive sharpening and the Edge of Stability, even in very large models with up to 7 billion parameters. The 'relative critical sharpness' provides insights into how the curvature of the loss landscape changes during the transition from pre-training to fine-tuning, which can inform data mixing strategies and improve the overall training process. These findings validate the proposed measures as useful tools for diagnosing and optimizing the training of large-scale neural networks.

Conclusion: In conclusion, the introduction of 'critical sharpness' and 'relative critical sharpness' offers a practical and scalable solution for analyzing the curvature evolution of the loss landscape in neural networks, especially for LLMs. These measures not only reduce the computational burden compared to traditional Hessian sharpness but also enable the first-ever demonstration of sharpness phenomena at a significant scale. The ability to efficiently diagnose curvature dynamics and guide data composition choices makes these measures valuable for advancing the understanding and optimization of large-scale model training.

Abstract: Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\textit{critical sharpness}$ ($λ_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $Δ\mathbfθ$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\textit{relative critical sharpness}$ ($λ_c^{1\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.

</details>
