<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.LG](#cs.LG) [Total: 47]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Legal Retrieval for Public Defenders](https://arxiv.org/abs/2601.14348)
*Dominik Stammbach,Kylie Zhang,Patty Liu,Nimra Nadeem,Lucia Zheng,Peter Henderson*

Main category: cs.IR

TL;DR: 本文介绍了与新泽西公共辩护办公室合作开发的NJ BriefBank工具，该工具通过提供相关上诉摘要来简化法律研究和写作。研究发现，现有的法律检索基准不适用于公共辩护搜索，但添加领域知识可以提高检索质量。此外，还提供了一个现实的辩护人搜索查询分类法，并发布了一个手动注释的公共辩护检索数据集，以促进进一步的研究。


<details>
  <summary>Details</summary>
Motivation: 面对繁重的工作量和有限资源，公共辩护从业者面临着特别苛刻的条件。然而，目前几乎没有证据表明AI如何能够有意义地支持辩护人的日常工作。

Method: 与新泽西公共辩护办公室合作开发了名为NJ BriefBank的检索工具，该工具旨在通过提供相关的上诉摘要来简化法律研究和写作过程。研究中指出，尽管现有的法律检索标准在公共辩护搜索上表现不佳，但通过引入领域知识如法律推理、特定领域的数据以及精心策划的人工示例，能够显著改善检索效果。

Result: 研究表明，通过引入包括法律推理在内的领域知识、特定领域的数据及精选的人工示例，可以有效提升检索质量。同时，为便于未来研究，项目团队还提出了一套针对实际防御者搜索请求的分类体系，并公开了一份经过人工标注的公共辩护检索数据集。

Conclusion: 本工作为构建面向公共辩护领域的实用且可靠的检索AI工具提供了起点，并促进了更贴近实际情况的法律检索基准的发展。

Abstract: AI tools are increasingly suggested as solutions to assist public agencies with heavy workloads. In public defense, where a constitutional right to counsel meets the complexities of law, overwhelming caseloads and constrained resources, practitioners face especially taxing conditions. Yet, there is little evidence of how AI could meaningfully support defenders' day-to-day work. In partnership with the New Jersey Office of the Public Defender, we develop the NJ BriefBank, a retrieval tool which surfaces relevant appellate briefs to streamline legal research and writing. We show that existing legal retrieval benchmarks fail to transfer to public defense search, however adding domain knowledge improves retrieval quality. This includes query expansion with legal reasoning, domain-specific data and curated synthetic examples. To facilitate further research, we provide a taxonomy of realistic defender search queries and release a manually annotated public defense retrieval dataset. Together, our work offers starting points towards building practical, reliable retrieval AI tools for public defense, and towards more realistic legal retrieval benchmarks.

</details>


### [2] [Trust Me on This: A User Study of Trustworthiness for RAG Responses](https://arxiv.org/abs/2601.14460)
*Weronika Łajewska,Krisztian Balog*

Main category: cs.IR

TL;DR: 研究了不同类型的解释如何影响用户对检索增强生成系统响应的信任。结果表明，虽然解释显著地引导用户选择更高质量的响应，但用户的判断还受到响应清晰度、可操作性和个人先前知识的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在信息访问系统中集成生成式AI时，不同类型的解释（如来源归属、事实依据和信息覆盖）如何影响用户对合成答案的信任。

Method: 通过一个控制的两阶段用户研究，参与者从一对响应中选择更值得信赖的一个，其中一个客观上比另一个质量更高，并且有无三种类型解释之一：(1) 来源归属，(2) 事实依据，(3) 信息覆盖。

Result: 解释能够显著指导用户选择更高质量的回答；然而，除了客观质量外，用户对于回答的信任还受到回复清晰度、实用性以及个人先验知识的影响。

Conclusion: 为提高用户对生成式AI系统的信任感，仅提供高质量的信息是不够的，还需要考虑增加回答的透明度、明确性和与用户背景知识的相关性。

Abstract: The integration of generative AI into information access systems often presents users with synthesized answers that lack transparency. This study investigates how different types of explanations can influence user trust in responses from retrieval-augmented generation systems. We conducted a controlled, two-stage user study where participants chose the more trustworthy response from a pair-one objectively higher quality than the other-both with and without one of three explanation types: (1) source attribution, (2) factual grounding, and (3) information coverage. Our results show that while explanations significantly guide users toward selecting higher quality responses, trust is not dictated by objective quality alone: Users' judgments are also heavily influenced by response clarity, actionability, and their own prior knowledge.

</details>


### [3] [Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.14546)
*Fangzheng Tian,Debasis Ganguly,Craig Macdonald*

Main category: cs.IR

TL;DR: 本文定义了检索增强生成（RAG）中的两个预测任务：检索性能预测（RPP）和生成性能预测（GPP），旨在估计检索文档的效用和最终答案的质量。通过结合多个特征类别的预测因子，包括检索器中心信号、阅读者中心特征以及与查询无关的文档质量和可读性特征，线性回归模型在自然问题（NQ）数据集上的实验表明多类别预测因子组合可以最准确地估计RAG的表现。


<details>
  <summary>Details</summary>
Motivation: 提高检索增强生成（RAG）中大型语言模型产生的答案质量面临的主要挑战是预测检索到的文档实用性及最终答案的正确性和相关性。本文旨在探索如何通过不同类型的特征来更准确地进行这两方面的预测。

Method: 定义了两个预测任务——检索性能预测（RPP）和生成性能预测（GPP）。提出假设认为检索文档的主题相关性与其效用存在关联，并且适应查询性能预测方法可用于RPP和GPP。此外，还考虑了诸如基于输入查询对检索上下文的大规模语言模型困惑度等以读者为中心的特性，以及反映文档质量和可读性的特征。使用上述各类预测因素训练线性回归模型。

Result: 实验结果表明，在自然问题数据集上，结合来自多个特征类别的预测因子能够产生对于RAG表现最为准确的估计。

Conclusion: 通过整合多种类型的预测因素，包括检索器中心信号、阅读者中心特征以及文档质量和可读性指标，能够有效提升对检索增强生成过程中检索文档效用及最终答案质量预测的准确性。

Abstract: The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents -- quantified as the performance gain from using context over generation without context -- and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved documents correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM's perplexity of the retrieved context conditioned on the input query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic document quality and readability can also provide useful signals to the predictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Questions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance.

</details>


### [4] [When Text-as-Vision Meets Semantic IDs in Generative Recommendation: An Empirical Study](https://arxiv.org/abs/2601.14697)
*Shutong Qiao,Wei Yuan,Tong Chen,Xiangyu Zhao,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 该论文重新审视了语义ID学习中的表示设计，通过将文本视为视觉信号，并使用基于OCR的文本表示方法来处理项目描述。实验表明，在单模态和多模态设置下，OCR文本表现优于或至少等同于标准文本嵌入用于语义ID学习的效果，并且在极端空间分辨率压缩下仍保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐模型中使用的预训练文本编码器主要针对良好形式的自然语言进行了优化，但在实际推荐数据中，项目描述往往是符号化的、以属性为中心的，包含数字、单位和缩写。这些文本编码器会把这些信息分割成碎片化的标记，削弱了语义连贯性和属性间的关系。此外，在转向多模态生成推荐时，依赖标准文本编码器还引入了文本与图像嵌入几何结构不匹配的问题，影响了跨模态融合的有效性和稳定性。

Method: 研究者们通过将文本视作一种视觉信号来重新考虑语义ID学习中的表示设计。他们进行了一项系统的实证研究，探索基于OCR（光学字符识别）的文本表示方法。具体来说，是将项目描述转换为图像，并利用基于视觉的OCR模型对其进行编码。

Result: 实验结果表明，在四个数据集和两个生成骨架上，无论是在单模态还是多模态环境下，基于OCR的文本表示法对于语义ID学习而言，总是能够匹配甚至超越传统的文本嵌入方法。此外，基于OCR的语义ID即使在极端的空间分辨率压缩条件下也表现出强大的鲁棒性和效率。

Conclusion: 采用基于OCR的方法处理文本作为视觉信号可以有效提升语义ID学习的表现，特别是在处理非传统文本格式以及促进更有效的跨模态融合方面。这种方法不仅改善了推荐系统的性能，而且在实际部署中展示了更高的鲁棒性和效率。

Abstract: Semantic ID learning is a key interface in Generative Recommendation (GR) models, mapping items to discrete identifiers grounded in side information, most commonly via a pretrained text encoder. However, these text encoders are primarily optimized for well-formed natural language. In real-world recommendation data, item descriptions are often symbolic and attribute-centric, containing numerals, units, and abbreviations. These text encoders can break these signals into fragmented tokens, weakening semantic coherence and distorting relationships among attributes. Worse still, when moving to multimodal GR, relying on standard text encoders introduces an additional obstacle: text and image embeddings often exhibit mismatched geometric structures, making cross-modal fusion less effective and less stable.
  In this paper, we revisit representation design for Semantic ID learning by treating text as a visual signal. We conduct a systematic empirical study of OCR-based text representations, obtained by rendering item descriptions into images and encoding them with vision-based OCR models. Experiments across four datasets and two generative backbones show that OCR-text consistently matches or surpasses standard text embeddings for Semantic ID learning in both unimodal and multimodal settings. Furthermore, we find that OCR-based Semantic IDs remain robust under extreme spatial-resolution compression, indicating strong robustness and efficiency in practical deployments.

</details>


### [5] [Unified Multimodal and Multilingual Retrieval via Multi-Task Learning with NLU Integration](https://arxiv.org/abs/2601.14714)
*Xinyuan Zhang,Lina Zhang,Lisung Chen,Guangyao Liu,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.IR

TL;DR: 提出了一种多任务学习框架，该框架统一了图像、长短文本和意图丰富的查询的特征表示，并在单一框架内联合优化了多语言图像检索、文本检索和自然语言理解（NLU）任务。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）采用了文本编码器，但在纯文本检索任务中，其表现始终不如专门的文本模型。此外，增加额外的文本编码器会提高存储需求、推断开销，并加剧检索效率低下问题，特别是在多语言环境中。

Method: 通过引入一种多任务学习框架来解决上述局限性，该框架旨在跨图像、长/短文本及富含意图的查询统一特征表达。这是首次尝试在一个框架内同时最优化多语种图像检索、文本检索以及自然语言理解任务的工作。方法中提到使用一个共享文本编码器来整合图像与文本检索过程，且此编码器被NLU特性增强以改进意图理解和检索准确性。

Result: 虽然摘要没有提供具体的实验结果或性能指标，但可以推测该方法旨在提升多模态检索系统的整体性能，特别是在处理多语言内容时的效率和准确度。

Conclusion: 这项研究提出了一个新的多任务学习框架，它能够改善多语言环境下图像和文本检索的表现，同时也提高了对于用户查询意图的理解能力。

Abstract: Multimodal retrieval systems typically employ Vision Language Models (VLMs) that encode images and text independently into vectors within a shared embedding space. Despite incorporating text encoders, VLMs consistently underperform specialized text models on text-only retrieval tasks. Moreover, introducing additional text encoders increases storage, inference overhead, and exacerbates retrieval inefficiencies, especially in multilingual settings. To address these limitations, we propose a multi-task learning framework that unifies the feature representation across images, long and short texts, and intent-rich queries. To our knowledge, this is the first work to jointly optimize multilingual image retrieval, text retrieval, and natural language understanding (NLU) tasks within a single framework. Our approach integrates image and text retrieval with a shared text encoder that is enhanced by NLU features for intent understanding and retrieval accuracy.

</details>


### [6] [From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems](https://arxiv.org/abs/2601.15122)
*Parviz Ahmadov,Masoud Mansoury*

Main category: cs.IR

TL;DR: 提出了一种名为PopSteer的后处理方法，利用稀疏自动编码器来解释和缓解推荐系统中的流行度偏差问题。通过实验表明，该方法能够有效提高公平性，同时对准确性的影响很小，并且提供了可解释性的见解以及对公平性-准确性权衡的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 流行度偏差是推荐系统中普遍存在的挑战，少数热门项目吸引了大部分注意力，而大多数不那么热门的项目则被忽视。这种不平衡会降低推荐质量并导致不公平的项目曝光。尽管现有的缓解方法在某种程度上解决了这个问题，但它们往往缺乏操作透明度。

Method: 本文介绍了一种称为PopSteer的方法，它使用稀疏自动编码器（SAE）来模仿训练模型的行为，同时提供神经元级别的可解释性。通过引入具有强烈偏好于热门或非热门项目的合成用户，研究者根据这些神经元的激活模式识别出编码了流行度信号的神经元。随后，通过对最偏置的神经元调整其激活值来引导推荐结果。

Result: 在三个公开数据集上的实验表明，PopSteer显著提高了推荐系统的公平性，而且几乎不影响其准确性。此外，这种方法还为理解和调节公平性和准确性之间的平衡提供了有价值的见解。

Conclusion: PopSteer是一种有效的后处理手段，用于减轻推荐系统中的流行度偏差问题。它不仅有助于改善推荐结果的公平性，同时也保持了较高的准确率，并为研究人员提供了有关如何更精细地管理公平性与准确性之间关系的洞察。

Abstract: Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mitigate popularity bias in recommendation models. The SAE is trained to replicate a trained model's behavior while enabling neuron-level interpretability. By introducing synthetic users with strong preferences for either popular or unpopular items, we identify neurons encoding popularity signals through their activation patterns. We then steer recommendations by adjusting the activations of the most biased neurons. Experiments on three public datasets with a sequential recommendation model demonstrate that PopSteer significantly enhances fairness with minimal impact on accuracy, while providing interpretable insights and fine-grained control over the fairness-accuracy trade-off.

</details>


### [7] [Beyond the Geometric Curse: High-Dimensional N-Gram Hashing for Dense Retrieval](https://arxiv.org/abs/2601.15205)
*Sangeet Sharma*

Main category: cs.IR

TL;DR: NUMEN, 一种基于确定性字符哈希的高维向量投影方法，无需训练即可在LIMIT基准测试中以32,768维达到93.90%的Recall@100，超越了传统BM25模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决即使是强大的7B参数嵌入模型在处理简单检索任务时也比不上几十年前的BM25模型的问题，理论认为这是由于维度瓶颈造成的。

Method: 提出了一种名为NUMEN的新方法，该方法通过去除学习过程，使用确定性的字符哈希直接将语言投射到高维向量上，从而打破了维度瓶颈。

Result: 在LIMIT基准测试中，NUMEN在32,768维度下达到了93.90%的Recall@100，成为首个正式超过稀疏BM25基线（93.6%）的密集检索模型。

Conclusion: 研究表明，在密集检索领域真正的问题并不在于架构设计，而是嵌入层本身；解决方案不一定是更聪明的训练方式，而可能是为数据提供更多的表达空间。

Abstract: Why do even the most powerful 7B-parameter embedding models struggle with simple retrieval tasks that the decades old BM25 handles with ease? Recent theory suggests that this happens because of a dimensionality bottleneck. This occurs when we force infinite linguistic nuances into small, fixed-length learned vectors. We developed NUMEN to break this bottleneck by removing the learning process entirely. Instead of training heavy layers to map text to a constrained space, NUMEN uses deterministic character hashing to project language directly onto high-dimensional vectors. This approach requires no training, supports an unlimited vocabulary, and allows the geometric capacity scale as needed. On the LIMIT benchmark, NUMEN achieves 93.90 % Recall@100 at 32,768 dimensions. This makes it the first dense retrieval model to officially surpass the sparse BM25 baseline 93.6 %. Our findings show that the real problem in dense retrieval isn't the architecture, but the embedding layer itself. The solution isn't necessarily smarter training, but simply providing more room to breathe.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [8] [Structured Image-based Coding for Efficient Gaussian Splatting Compression](https://arxiv.org/abs/2601.14510)
*Pedro Martin,Antonio Rodrigues,Joao Ascenso,Maria Paula Queluz*

Main category: cs.MM

TL;DR: 本文提出了一种新的高斯点渲染（Gaussian Splatting, GS）模型压缩方法GSICO，通过将GS参数映射成结构化图像并使用传统图像编码器进行压缩，实现了平均20.2倍的压缩率，同时保持了良好的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的GS模型需要存储数百万个参数，导致文件体积庞大，限制了其在实际多媒体系统中的应用。为了解决这个问题，本文提出了GSICO来有效压缩预训练的GS模型，同时保持感知保真度。

Method: GSICO的核心贡献在于一个映射过程，该过程能够将GS参数组织成结构化的图像，并通过一种新算法增强空间连贯性。随后，这些GS参数图像被输入到常规图像编码器中进行编码。

Result: 实验评估表明，GSICO在Tanks and Temples、Deep Blending以及Mip-NeRF360数据集上实现了平均20.2倍的压缩比，且几乎不损失视觉质量，依据PSNR、SSIM和LPIPS指标衡量。与当前最先进的GS压缩方法相比，所提出的编解码器始终提供更优的率失真(RD)权衡表现。

Conclusion: GSICO是一种有效的GS模型压缩方案，它显著减少了存储需求而不牺牲视觉保真度，为GS技术的实际应用铺平了道路。

Abstract: Gaussian Splatting (GS) has recently emerged as a state-of-the-art representation for radiance fields, combining real-time rendering with high visual fidelity. However, GS models require storing millions of parameters, leading to large file sizes that impair their use in practical multimedia systems. To address this limitation, this paper introduces GS Image-based Compression (GSICO), a novel GS codec that efficiently compresses pre-trained GS models while preserving perceptual fidelity. The core contribution lies in a mapping procedure that arranges GS parameters into structured images, guided by a novel algorithm that enhances spatial coherence. These GS parameter images are then encoded using a conventional image codec. Experimental evaluations on Tanks and Temples, Deep Blending, and Mip-NeRF360 datasets show that GSICO achieves average compression factors of 20.2x with minimal loss in visual quality, as measured by PSNR, SSIM, and LPIPS. Compared with state-of-the-art GS compression methods, the proposed codec consistently yields superior rate-distortion (RD) trade-offs.

</details>


### [9] [HCVR Scene Generation: High Compatibility Virtual Reality Environment Generation for Extended Redirected Walking](https://arxiv.org/abs/2601.14679)
*Yiran Zhang,Xingpeng Sun,Aniket Bera*

Main category: cs.MM

TL;DR: 本文提出了一种新的框架HCVR，用于生成与物理空间高度兼容的虚拟现实环境，通过优化物体的选择、缩放和放置来提高基于对齐的重定向行走控制器的效果。实验表明，使用HCVR生成的场景比基于大语言模型生成的场景在减少物理碰撞方面有显著优势，并且用户对于布局设计的反馈也更好。


<details>
  <summary>Details</summary>
Motivation: 自然行走可以增强虚拟环境中的沉浸感，但物理空间限制和障碍物阻碍了探索，特别是在大型虚拟场景中。虽然重定向行走（RDW）技术可以通过微妙地操控虚拟摄像机来引导用户避开实际碰撞，但在物理和虚拟环境之间存在显著几何差异时，其效果会大大降低。现有场景生成方法主要关注对象关系或布局美学，往往忽视了有效RDW所需的重要物理兼容性。为解决这一问题，提出了HCVR框架。

Method: HCVR首先采用ENI++，这是一种新型边界敏感度量标准，通过比较旋转敏感可视多边形来评估物理与虚拟空间之间的不兼容性。接着，在ENI++兼容性地图及用户提示指导下，HCVR利用大型语言模型进行上下文感知的3D资产检索和初始布局生成。然后，该框架通过策略性调整物体选择、缩放及放置以最大化覆盖虚拟不兼容区域，从而有效地将用户导向可行的RDW路径。

Result: 用户研究显示，相较于仅基于大语言模型生成并结合RDW的方法，使用HCVR生成的场景能够使物理碰撞减少22.78倍，同时ENI++得分降低了35.89%。此外，在用户对布局设计的反馈上，HCVR获得了高出12.5%的好评。

Conclusion: HCVR提供了一种有效手段来生成支持重定向行走技术的虚拟环境，显著减少了物理碰撞的发生率，提高了用户的沉浸体验。

Abstract: Natural walking enhances immersion in virtual environments (VEs), but physical space limitations and obstacles hinder exploration, especially in large virtual scenes. Redirected Walking (RDW) techniques mitigate this by subtly manipulating the virtual camera to guide users away from physical collisions within pre-defined VEs. However, RDW efficacy diminishes significantly when substantial geometric divergence exists between the physical and virtual environments, leading to unavoidable collisions. Existing scene generation methods primarily focus on object relationships or layout aesthetics, often neglecting the crucial aspect of physical compatibility required for effective RDW. To address this, we introduce HCVR (High Compatibility Virtual Reality Environment Generation), a novel framework that generates virtual scenes inherently optimized for alignment-based RDW controllers. HCVR first employs ENI++, a novel, boundary-sensitive metric to evaluate the incompatibility between physical and virtual spaces by comparing rotation-sensitive visibility polygons. Guided by the ENI++ compatibility map and user prompts, HCVR utilizes a Large Language Model (LLM) for context-aware 3D asset retrieval and initial layout generation. The framework then strategically adjusts object selection, scaling, and placement to maximize coverage of virtually incompatible regions, effectively guiding users towards RDW-feasible paths. User studies evaluating physical collisions and layout quality demonstrate HCVR's effectiveness with HCVR-generated scenes, resulting in 22.78 times fewer physical collisions and received 35.89\% less on ENI++ score compared to LLM-based generation with RDW, while also receiving 12.5\% higher scores on user feedback to layout design.

</details>


### [10] [Interpreting Multimodal Communication at Scale in Short-Form Video: Visual, Audio, and Textual Mental Health Discourse on TikTok](https://arxiv.org/abs/2601.15278)
*Mingyue Zha,Ho-Chun Herbert Chang*

Main category: cs.MM

TL;DR: 本研究提出了一种结合自动多模态特征提取和基于Shapley值的可解释性方法，用于分析文本、视觉和音频如何共同影响短视频平台上的用户参与度。通过对162,965个TikTok视频及814,825张关于社交焦虑障碍(SAD)图片的应用，发现面部表情比文本情绪更能预测观看量，信息内容比情感支持更能吸引注意力，跨模式协同效应具有阈值依赖性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在分析短视频平台的内容时往往孤立地看待文本、视觉和音频这些模态，缺乏能够综合理解它们联合贡献的可扩展框架。这项工作旨在通过引入一种新的分析管道来填补这一空白，以更好地理解不同模态是如何共同作用于用户参与度的。

Method: 研究采用了一种将自动化多模态特征提取与基于Shapley值的解释性技术相结合的方法论框架。该框架被应用于大量关于社交焦虑障碍的TikTok视频和图像数据集上，以探索哪些因素最能促进用户互动。

Result: 研究结果显示，面部表情对于预测视频观看次数比单纯的文字情绪更加有效；提供信息性质的内容相比给予情感支持更能引起用户的注意；同时，跨模态间存在一定的协同效应，但这种效应表现出对某些条件下的依赖性。

Conclusion: 通过多模态分析揭示了单模态方法无法观察到的交互模式，不仅为理解算法媒介环境中的心理健康沟通提供了新视角，也为未来进行可解释性的多模态研究奠定了可复制的方法基础。

Abstract: Short-form video platforms integrate text, visuals, and audio into complex communicative acts, yet existing research analyzes these modalities in isolation, lacking scalable frameworks to interpret their joint contributions. This study introduces a pipeline combining automated multimodal feature extraction with Shapley value-based interpretability to analyze how text, visuals, and audio jointly influence engagement. Applying this framework to 162,965 TikTok videos and 814,825 images about social anxiety disorder (SAD), we find that facial expressions outperform textual sentiment in predicting viewership, informational content drives more attention than emotional support, and cross-modal synergies exhibit threshold-dependent effects. These findings demonstrate how multimodal analysis reveals interaction patterns invisible to single-modality approaches. Methodologically, we contribute a reproducible framework for interpretable multimodal research applicable across domains; substantively, we advance understanding of mental health communication in algorithmically mediated environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [JAXMg: A multi-GPU linear solver in JAX](https://arxiv.org/abs/2601.14466)
*Roeland Wiersema*

Main category: cs.DC

TL;DR: JAXMg通过将NVIDIA的cuSOLVERMg与JAX接口，提供了多GPU密集线性代数支持，允许在单个GPU内存限制以上的矩阵上执行Cholesky分解和对称特征分解，并保持了与JAX转换组合的能力，从而使得端到端科学工作流中可以嵌入可扩展的线性代数运算。


<details>
  <summary>Details</summary>
Motivation: 解决大规模密集线性系统和特征值问题是科学计算许多领域的核心需求，但这些操作在现代编程框架内难以扩展到单个GPU之外。尽管存在高度优化的多GPU求解器库，但它们通常很难集成到可组合的即时编译Python工作流中。

Method: JAXMg通过XLA外部函数接口使JAX能够与NVIDIA的cuSOLVERMg进行接口，为超过单个GPU内存限制的矩阵提供基于Cholesky的线性求解和对称特征分解。该设计允许可扩展的线性代数直接嵌入到JAX程序中，保持与JAX转换的组合能力，并且能够在端到端的科学工作流程中启用多GPU执行。

Result: JAXMg成功地将分布式GPU求解器作为JIT兼容的JAX原语暴露出来，使得用户可以在JAX环境中使用多GPU资源处理大规模线性代数问题，同时保持了与JAX生态系统的无缝集成。

Conclusion: JAXMg为需要处理超出单个GPU内存容量的大规模密集线性代数问题提供了有效的解决方案，并且能够很好地融入现有的JAX工作流，从而提高了科学计算中的效率和灵活性。

Abstract: Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.

</details>


### [12] [Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI](https://arxiv.org/abs/2601.14608)
*Torben R. Lahnor,Mia Reitz,Jonas Posner,Patrick Diehl*

Main category: cs.DC

TL;DR: 本文通过Task Bench框架对比了Itoyori、ItoyoriFBC两种新型集群AMT运行时与MPI和HPX在性能及编程效率上的差异。结果显示，尽管MPI在通信量少的任务中效率最高，但Itoyori在通信密集型配置下效率最高且编程效率领先。


<details>
  <summary>Details</summary>
Motivation: 由于异步多任务（AMT）运行时提供了比消息传递接口（MPI）更高效的选择，但不同的AMT系统间难以进行公平比较。为此，文章旨在通过将Itoyori和ItoyoriFBC集成到Task Bench框架来全面评估这些系统相对于MPI和HPX的性能和程序员生产力。

Method: 采用Task Bench参数化框架对Itoyori和ItoyoriFBC这两种基于PGAS模型的AMT系统进行了整合，并与MPI及HPX在多种配置下进行了对比测试。性能评估包括计算密集型内核、弱扩展性以及不平衡和通信密集型模式等方面；而编程效率则通过代码行数(LOC)和库构造数量(NLC)来衡量。

Result: 结果表明，对于规则且通信需求低的工作负载，MPI表现出最高的效率，但需要详细的底层编码。HPX在不同节点数量下的负载不平衡情况下保持稳定的效率，但在生产率指标上排名最后。Itoyori在通信密集型配置中达到了最高的效率，同时在编程效率方面也处于领先地位。ItoyoriFBC虽然效率略低于Itoyori，但其基于未来的同步机制为表达不规则工作负载提供了潜力。

Conclusion: 研究揭示了不同类型并行编程系统之间的权衡：MPI适用于常规轻量级通信任务，但编程复杂度高；HPX能在负载不平衡时保持稳定性能，但编程效率较低；相比之下，Itoyori及其变种ItoyoriFBC不仅在通信密集场景下表现优异，而且显著提高了编程效率。

Abstract: Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.
  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).
  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.

</details>


### [13] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://arxiv.org/abs/2601.14612)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 本文提出了一种新的随机调度算法ROSS，用于混合云环境中具有硬性截止日期的任务调度。该算法在合理截止日期下实现了理论上最优的竞争比√K，相比现有方法显著提升了成本节省效果，通过Azure和AWS的实际数据评估显示其在不同现货市场条件下最高可节省30%的成本。


<details>
  <summary>Details</summary>
Motivation: 针对混合云环境下任务的截止日期感知在线调度挑战，其中任务可能运行在成本低廉但不可靠的竞价实例或更昂贵的按需实例上，并且需要满足严格的截止时间要求。

Method: 首先确定了现有（主要是）确定性策略的基本限制，证明了最坏情况下的竞争比为Ω(K)，这里K是指按需实例与竞价实例之间的成本比率。接着提出了一种新颖的随机调度算法——ROSS，该算法在合理的截止日期条件下能够达到理论最优的竞争比√K。

Result: 通过对来自Azure和AWS的真实追踪数据进行广泛评估表明，ROSS算法能够有效地平衡成本优化与截止日期保证，在多样化的竞价市场条件下始终优于现有最先进方法，最多可实现高达30%的成本节约。

Conclusion: 研究证明了ROSS算法在处理混合云环境下的截止日期敏感型工作负载时的有效性，不仅改善了成本效益还提高了对截止日期遵守的能力。

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\%$ in cost savings, across diverse spot market conditions.

</details>


### [14] [Specifying and Verifying RDMA Synchronisation (Extended Version)](https://arxiv.org/abs/2601.14642)
*Guillaume Ambal,Max Stupple,Brijesh Dongol,Azalea Raad*

Main category: cs.DC

TL;DR: 本文提出了RDMA\u00bTSO_RMW，这是首个针对TSO CPU上远程读-修改-写指令的语义模型，并基于此开发了可组合的同步抽象库和三种适用于不同场景的远程锁。此外还定义了一个强RDMA模型RDMA\u00bSC_RMW，与共享内存架构中的顺序一致性类似。这些库与现有的高性能库LOCO兼容，确保了组合性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有的RDMA\u00bTSO语义未能正式化远程同步机制，导致像锁这样的常见抽象实现无法得到验证。为了填补这一空白，研究者们提出了新的语义来支持远程读-修改-写（RMW）指令。

Method: 通过引入RDMA\u00bTSO_RMW语义来描述TSO CPU上的远程RMW行为；接着基于该语义构建了包括RDMA\u00bWAIT_RMW在内的同步抽象库；最后根据不同的应用场景指定了三种类型的远程锁，并且提出了一种更强大的RDMA模型RDMA\u00bSC_RMW。

Result: 成功地为TSO CPU环境下的RDMA通信提供了第一个支持远程RMW操作的形式化语义；设计并实现了几个关键的同步原语及其正确性的形式化证明；提出的解决方案与现有的高效能库LOCO相兼容。

Conclusion: 本研究不仅弥补了现有RDMA\u00bTSO语义在处理远程同步方面的不足，而且为开发既高效又正确的分布式系统提供了一套完整的工具集。

Abstract: Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\text{RDMA}^\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\text{RDMA}^{\text{TSO}}_{\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$ library. Underpinned by $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\text{RDMA}^{\text{SC}}_{\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.

</details>


### [15] [Application-level observability for adaptive Edge to Cloud continuum systems](https://arxiv.org/abs/2601.14923)
*Kaddour Sidi,Daniel Balouek,Baptiste Jonglez*

Main category: cs.DC

TL;DR: 本文介绍了一个应用级的可观测性框架，该框架结合了OpenTelemetry、Prometheus、K3s和Chaos Mesh等工具，旨在实现边缘到云系统中的实时监控与自适应控制。通过视频处理用例展示了如何利用应用程序级别的指标自动调整以保持目标帧率、延迟和检测准确性，即使在工作负载变化和故障注入的情况下也能维持性能。初步结果表明该方法增强了系统的可扩展性、容错能力和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现代边缘到云（E2C）系统需要细粒度的可观测性来确保在异构和动态环境下的自适应行为及性能目标的一致性。

Method: 提出了一个集成了开发者驱动的工具化和SLO意识反馈机制的应用级别可观测性框架，通过整合OpenTelemetry、Prometheus、K3s以及Chaos Mesh等技术，实现了跨连续体的实时监控与自适应控制。

Result: 使用视频处理作为案例研究，证明了所提出的方法能够根据应用程序级指标指导自动调整，从而在变化的工作负载条件下以及面对故意引入的错误时仍能维持目标帧率、延迟和检测精度。实验结果显示出改进后的系统具有更好的可扩展性、容错能力及响应速度。

Conclusion: 这项工作为构建适应性强且符合服务等级目标(SLO)的E2C应用提供了一种实用的基础。

Abstract: Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [Trajectory-Driven Multi-Product Influence Maximization in Billboard Advertising](https://arxiv.org/abs/2601.14737)
*Dildar Ali,Suman Banerjee,Rajibul Islam*

Main category: cs.DB

TL;DR: 本文研究了如何选择有限数量的广告位以最大化多个产品广告影响力的问题，提出了两种变体并设计了相应的近似算法。实验表明所提方法的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 针对户外广告牌广告中如何选取有限个数的位置来最大化品牌影响力的问题，特别是当一个商业机构希望推广多个产品且每个产品都有特定的影响力需求时，需要找到一种有效的方法来选择这些位置。

Method: 对于第一种问题变体，将其建模为一个多子模覆盖问题，并基于连续贪婪框架和随机化舍入设计了一个双准则近似算法。对于第二种不相交槽位变体，提出了一种基于采样的近似方法以及一个高效的原始-对偶贪婪算法。

Result: 通过使用真实世界中的轨迹和广告牌数据集进行广泛实验，结果证明了所提出的解决方案方法在有效性和效率上的优越性。

Conclusion: 本研究为解决多产品户外广告牌广告选址问题提供了新的视角和方法，通过设计近似算法有效地解决了两个变体问题，并通过实验证明了其实际应用价值。

Abstract: Billboard Advertising has emerged as an effective out-of-home advertising technique, where the goal is to select a limited number of slots and play advertisement content there, with the hope that it will be observed by many people and, effectively, a significant number of them will be influenced towards the brand. Given a trajectory and a billboard database and a positive integer $k$, how can we select $k$ highly influential slots to maximize influence? In this paper, we study a variant of this problem where a commercial house wants to make a promotion of multiple products, and there is an influence demand for each product. We have studied two variants of the problem. In the first variant, our goal is to select $k$ slots such that the respective influence demand of each product is satisfied. In the other variant of the problem, we are given with $\ell$ integers $k_1,k_2, \ldots, k_{\ell}$, the goal here is to search for $\ell$ many set of slots $S_1, S_2, \ldots, S_{\ell}$ such that for all $i \in [\ell]$, $|S_{i}| \leq k_i$ and for all $i \neq j$, $S_i \cap S_j=\emptyset$ and the influence demand of each of the products gets satisfied. We model the first variant of the problem as a multi-submodular cover problem and the second variant as its generalization. To solve the common-slot variant, we formulate the problem as a multi-submodular cover problem and design a bi-criteria approximation algorithm based on the continuous greedy framework and randomized rounding. For the disjoint-slot variant, we proposed a sampling-based approximation approach along with an efficient primal-dual greedy algorithm that enforces disjointness naturally. Extensive experiments with real-world trajectory and billboard datasets highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Unpacking Security Scanners for GitHub Actions Workflows](https://arxiv.org/abs/2601.14455)
*Madjda Fares,Yogya Gamage,Benoit Baudry*

Main category: cs.SE

TL;DR: 本文首次系统地比较了9种GitHub Actions工作流安全扫描器，从它们的覆盖范围、检测能力和易用性等方面进行了评估。通过建立一个包含10种安全弱点的分类法，并对596个工作流进行测试后发现，这些扫描器在识别安全弱点方面存在显著差异。基于此，文章为开发者提供了强化GitHub Actions工作流的具体建议。


<details>
  <summary>Details</summary>
Motivation: 随着GitHub Actions平台的普及，它成为了软件供应链攻击的目标之一。面对这些利用权限过度、版本模糊或缺乏制品完整性检查来破坏工作流的攻击，出现了多种安全扫描器以帮助开发者加强其工作流的安全性。然而，对于这些工具的有效性和使用场景，尚缺少系统性的对比研究。

Method: 首先定义了一个包括10种可能出现在GitHub Actions工作流中的安全弱点的分类体系；接着选取了9个市面上常见的安全扫描器，并用这一体系作为基准来评估每个扫描器的能力；最后，在一个精心挑选的包含596个工作流的数据集上运行这些扫描器，以比较它们在不同方面的表现。

Result: 研究表明，GitHub Actions工作流安全扫描器领域内既有覆盖面广的工具也有非常专注特定问题的工具。更重要的是，不同的扫描器对于相同的安全弱点有着不同的解读方式，导致报告出来的弱点类型和数量存在很大差异。

Conclusion: 通过对9种GitHub Actions工作流安全扫描器的首次系统性比较，揭示了这些工具之间存在着明显的差异性。基于实证分析结果，为开发者提供了具体可行的建议，帮助他们更好地选择适合自己的安全扫描工具并据此加固其GitHub Actions工作流。

Abstract: GitHub Actions is a widely used platform that allows developers to automate the build and deployment of their projects through configurable workflows. As the platform's popularity continues to grow, it has become a target of choice for recent software supply chain attacks. These attacks exploit excessive permissions, ambiguous versions, or the absence of artifact integrity checks to compromise workflows. In response to these attacks, several security scanners have emerged to help developers harden their workflows.
  In this paper, we perform the first systematic comparison of 9 GitHub Actions workflow security scanners. We compare them in terms of scope (which security weaknesses they target), detection capabilities (how many weaknesses they detect), and usability (how long they take to scan a workflow). To compare scanners on a common ground, we first establish a taxonomy of 10 security weaknesses that can occur in GitHub Actions workflows. Then, we run the scanners against a curated set of 596 workflows.
  Our study reveals that the landscape of GitHub Actions workflow security scanners is diverse, with both broad-scope tools and very focused ones. More importantly, we show that scanners interpret security weaknesses differently, leading to significant differences in the type and number of reported weaknesses. Based on this empirical evidence, we make actionable recommendations for developers to harden their GitHub Actions workflows.

</details>


### [18] [AQUA: an Agile Process to Develop Quantum Annealing Applications](https://arxiv.org/abs/2601.14501)
*Lodovica Marchesi,Amal Nasharti,Michele Marchesi*

Main category: cs.SE

TL;DR: 本文介绍了一种名为AQUA的敏捷生命周期，专为QUBO/QA开发定制，通过四个阶段解决当前量子退火技术在实际应用中面临的挑战，并通过实际案例验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 由于量子硬件的发展，QUBO问题的研究越来越受到关注。然而，该领域的实际应用受到数学复杂性、硬件限制以及缺乏稳健软件工程过程的阻碍。为了克服这些障碍，本文提出了AQUA（Agile QUantum Annealing）框架。

Method: 采用设计科学研究（DSR）方法论，AQUA将Scrum方法调整以适应QUBO/QA开发的需求，并将工作划分为四个阶段：正式建模的初步评估、原型驱动的算法选择、敏捷实现及持续维护的部署，每个阶段都由里程碑控制。

Result: AQUA在实际信用评分案例中得到验证，展示了其实现的可能性，并提供了一个明确且系统化的QA工程框架。

Conclusion: 本研究的关键贡献包括：一个专门针对QUBO/QA的软件流程，使用DSR方法进行创建和设计，以及在一个简单但真实的案例研究中的实证验证。

Abstract: Quadratic unconstrained binary optimization (QUBO) is a field of operations research that is attracting growing interest due to the recent availability of quantum hardware targeted at solving QUBO problems. However, practical adoption is hindered by mathematical intricacy, hardware constraints, and a lack of sound software engineering processes for QUBO development. This work presents AQUA (Agile QUantum Annealing), an agile lifecycle for QUBO/QA development created through an industry-academia partnership between NetService S.p.A and the University of Cagliari. Using the Design Science Research (DSR) approach, AQUA customizes Scrum to the needs of QUBO/QA development, structuring work into four stages: initial assessment with formal modeling, prototype-driven algorithm selection, agile implementation, and deployment with ongoing maintenance, each gated by milestones. Validated on a real credit-scoring case, AQUA shows feasibility and offers an explicit, systematic QA engineering framework. Key contributions of our work are: a dedicated QUBO/QA software process, its creation and design using DSR approach, and its empirical validation on a simple yet real case study.

</details>


### [19] [HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation](https://arxiv.org/abs/2601.14598)
*Yonatan Gizachew Achamyeleh,Harsh Thomare,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: 本文提出了一种名为HELIOS的框架，该框架将基于大型语言模型（LLMs）的反编译视为结构化推理任务。通过将二进制文件的控制流和函数调用总结成一个层次化的文本表示，并结合通用LLM以及可选的编译器反馈机制，HELIOS显著提高了生成代码的可编译性和功能正确性，尤其在处理优化后的二进制文件时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLMs）的二进制反编译方法通常将代码当作纯文本处理，忽略了程序控制流图的重要性，导致输出结果往往在语法上脆弱且逻辑不一致，特别是在处理经过优化的二进制文件时。为了解决这些问题，提出了HELIOS框架。

Method: HELIOS框架采用的方法是将二进制文件的控制流程及函数调用转化为一种层级化的文本形式，这种形式清晰地表达了基本块、其后继者以及诸如循环和条件判断等高级模式。然后，此表示与原始反编译器输出一起提供给一个通用型的大规模语言模型。此外，还引入了一个编译器循环机制，在生成的代码无法构建时返回错误信息，以进一步提高代码质量。

Result: 实验结果显示，在x86_64架构下的HumanEval-Decompile测试中，对于Gemini 2.0，HELIOS能够将平均对象文件的可编译性从45.0%提升到85.2%，而对于GPT-4.1 Mini则从71.4%增加到了89.6%。当加入编译器反馈后，可编译性超过了94%，并且功能正确性比仅使用文本提示提高了最多5.6个百分点。同时，HELIOS在保持语法正确性一贯高水平的同时，减少了不同硬件目标之间功能正确性的差异。

Conclusion: HELIOS框架证明了通过将基于LLM的反编译过程重新定义为一项结构化推理任务，可以显著改善生成代码的质量，尤其是在跨多种硬件平台时保持高水准的语义忠实度方面。这使得HELIOS成为安全分析场景下逆向工程工作流中的实用组件。

Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.
  On HumanEval-Decompile for \texttt{x86\_64}, \textsc{HELIOS} raises average object file compilability from 45.0\% to 85.2\% for Gemini~2.0 and from 71.4\% to 89.6\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.

</details>


### [20] [ARFT-Transformer: Modeling Metric Dependencies for Cross-Project Aging-Related Bug Prediction](https://arxiv.org/abs/2601.14731)
*Shuning Ge,Fangyun Qin,Xiaohui Wan,Yang Liu,Qian Dai,Zheng Zheng*

Main category: cs.SE

TL;DR: 本文提出了一种基于Transformer的跨项目老化相关错误（ARB）预测框架ARFT-Transformer，通过引入度量级多头注意力机制来捕捉度量间的交互作用，并结合Focal Loss函数有效处理类别不平衡问题。实验表明，在单源和多源情况下，该方法在平衡度量上比现有方法提高了最多29.54%和19.92%。


<details>
  <summary>Details</summary>
Motivation: 长时间运行的软件系统经常遭受由老化相关错误(ARB)引起的问题。为了在开发阶段早期减轻ARB的风险，研究者提出了ARB预测。然而，由于收集ARB数据困难，导致了跨项目ARB预测的需求增加，这带来了领域适应性问题及严重的类别不平衡挑战。现有的跨项目ARB预测方法通常独立处理输入指标，忽略了指标间的相互依赖关系，可能导致信息重叠和对指标重要性的误判。此外，这些方法常使用交叉熵作为损失函数，不能区分样本分类的难易程度。

Method: 为了解决这些问题，文章提出了ARFT-Transformer，这是一种基于Transformer的跨项目ARB预测框架。它采用了度量级别的多头注意力机制来捕捉度量之间的交互，并且整合了Focal Loss函数以更好地解决类别不平衡问题。

Result: 通过对三个大规模开源项目的实验证明，无论是单一来源还是多来源的情况下，ARFT-Transformer相比其他最先进跨项目ARB预测方法在平衡度量上表现更优，分别达到了最高29.54%和19.92%的提升。

Conclusion: ARFT-Transformer通过引入新的注意力机制以及改进损失函数的方式有效地解决了跨项目ARB预测中的领域适应性和类别不平衡两大难题，从而提高了预测性能。

Abstract: Software systems that run for long periods often suffer from software aging, which is typically caused by Aging-Related Bugs (ARBs). To mitigate the risk of ARBs early in the development phase, ARB prediction has been introduced into software aging research. However, due to the difficulty of collecting ARBs, within-project ARB prediction faces the challenge of data scarcity, leading to the proposal of cross-project ARB prediction. This task faces two major challenges: 1) domain adaptation issue caused by distribution difference between source and target projects; and 2) severe class imbalance between ARB-prone and ARB-free samples. Although various methods have been proposed for cross-project ARB prediction, existing approaches treat the input metrics independently and often neglect the rich inter-metric dependencies, which can lead to overlapping information and misjudgment of metric importance, potentially affecting the model's performance. Moreover, they typically use cross-entropy as the loss function during training, which cannot distinguish the difficulty of sample classification. To overcome these limitations, we propose ARFT-Transformer, a transformer-based cross-project ARB prediction framework that introduces a metric-level multi-head attention mechanism to capture metric interactions and incorporates Focal Loss function to effectively handle class imbalance. Experiments conducted on three large-scale open-source projects demonstrate that ARFT-Transformer on average outperforms state-of-the-art cross-project ARB prediction methods in both single-source and multi-source cases, achieving up to a 29.54% and 19.92% improvement in Balance metric.

</details>


### [21] [ARISE - Adaptive Refinement and Iterative Scenario Engineering](https://arxiv.org/abs/2601.14743)
*Konstantin Poddubnyy,Igor Vozniak,Nils Lipp,Ivan Burmistrov,Davit Hovhannisyan,Christian Mueller,Philipp Slusallek*

Main category: cs.SE

TL;DR: 本文提出了一种名为ARISE的多阶段工具，该工具通过迭代式的大规模语言模型引导的优化过程，将自然语言提示转换为可执行的Scenic脚本。此方法在生成语义准确且可执行的交通场景方面表现出更高的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的从文本到模拟的流程通常依赖于静态片段检索、有限的语法、单次解码或缺乏强大的可执行性检查，并且严重依赖于约束性的大规模语言模型提示和最少的后处理。这些限制使得创建高质量多样化的合成交通场景变得困难。

Method: 开发了ARISE（自适应细化与迭代场景工程），这是一种多阶段工具，能够通过迭代式的大规模语言模型引导的细化过程，把自然语言提示转化为可执行的Scenic脚本。每次生成之后，ARISE都会在仿真软件中测试脚本的可执行性，并将结构化的诊断信息反馈给大规模语言模型，直到满足句法和功能性要求为止。

Result: 经过广泛的评估，ARISE在生成语义准确及可执行的交通场景上超越了基线方法，展现出更高的可靠性和鲁棒性。

Conclusion: ARISE提供了一个有效的方法来克服现有文本转模拟流程中的局限性，对于提高碰撞避免轨迹规划器所需训练数据的质量和多样性特别有用。

Abstract: The effectiveness of collision-free trajectory planners depends on the quality and diversity of training data, especially for rare scenarios. A widely used approach to improve dataset diversity involves generating realistic synthetic traffic scenarios. However, producing such scenarios remains difficult due to the precision required when scripting them manually or generating them in a single pass. Natural language offers a flexible way to describe scenarios, but existing text-to-simulation pipelines often rely on static snippet retrieval, limited grammar, single-pass decoding, or lack robust executability checks. Moreover, they depend heavily on constrained LLM prompting with minimal post-processing. To address these limitations, we introduce ARISE - Adaptive Refinement and Iterative Scenario Engineering, a multi-stage tool that converts natural language prompts into executable Scenic scripts through iterative LLM-guided refinement. After each generation, ARISE tests script executability in simulation software, feeding structured diagnostics back to the LLM until both syntactic and functional requirements are met. This process significantly reduces the need for manual intervention. Through extensive evaluation, ARISE outperforms the baseline in generating semantically accurate and executable traffic scenarios with greater reliability and robustness.

</details>


### [22] [FastFI: Enhancing API Call-Site Robustness in Microservice-Based Systems with Fault Injection](https://arxiv.org/abs/2601.14800)
*Yuzhen Tan,Jian Wang,Shuaiyu Xie,Bing Li,Yunqing Yong,Neng Zhang,Shaolin Tan*

Main category: cs.SE

TL;DR: 提出了一种名为FastFI的框架，它通过DFS求解器和动态故障注入来发现所有有效的组合故障，并利用故障注入结果识别需要加固的关键API调用点，以增强基于微服务系统的健壮性。实验表明，FastFI相比现有技术平均减少了76.12%的端到端故障注入时间，同时保持了可接受的资源开销，并且能够准确地识别高影响API并为调用点加固提供可行指导。


<details>
  <summary>Details</summary>
Motivation: 随着微服务架构复杂性的增加，传统的随机故障注入方法变得低效；而现有的基于谱系的方法虽然有所改进，但仍然受限于通用SAT求解器在处理特定结构的CNF公式时效率低下，以及缺乏故障注入后的具体指导。

Method: FastFI框架采用了基于深度优先搜索（DFS）的求解器配合动态故障注入策略，旨在发现所有的有效组合故障。此外，该框架还利用故障注入的结果来定位那些对系统稳定性至关重要的API接口位置，从而建议开发者进行针对性的加固处理。

Result: FastFI在四个代表性的微服务基准测试中表现出色，相较于最先进基线方案，其端到端故障注入时间平均减少了76.12%，并且没有显著增加额外的资源消耗。更重要的是，FastFI能够精准地找出具有重大影响的API，并就如何加强这些API的调用点提供了切实可行的意见。

Conclusion: FastFI作为一种新的故障注入导向框架，不仅提高了故障注入过程的效率，而且通过对关键API调用点的识别与加固建议增强了微服务系统的健壮性。

Abstract: Fault injection is a key technique for assessing software reliability, enabling proactive detection of system defects before they manifest in production. However, the increasing complexity of microservice architectures leads to exponential growth in the fault-injection space, rendering traditional random injection inefficient. Recent lineage-driven approaches mitigate this problem through heuristic pruning, but they face two limitations. First, combinatorial-fault discovery remains bottlenecked by general-purpose SAT solvers, which fail to exploit the monotone and low-overlap structure of derived CNF formulas and typically rely on a static upper bound on fault size. Second, existing techniques provide limited post-injection guidance beyond reporting detected faults. To address these challenges, we propose FastFI, a fault-injection-guided framework to enhance the robustness of API call sites in microservice-based systems. FastFI features a DFS-based solver with dynamic fault injection to discover all valid combinatorial faults, and it leverages fault-injection results to identify critical APIs whose call sites should be hardened for robustness. Experiments on four representative microservice benchmarks show that FastFI reduces end-to-end fault-injection time by an average of 76.12\% compared to state-of-the-art baselines while maintaining acceptable resource overhead. Moreover, FastFI accurately identifies high-impact APIs and provides actionable guidance for call-site hardening.

</details>


### [23] [Understanding Usefulness in Developer Explanations on Stack Overflow](https://arxiv.org/abs/2601.14865)
*Martin Obaidi,Kushtrim Qengaj,Hannah Deters,Jakob Droste,Marc Herrmann,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: The study analyzed 3,323 questions and 59,398 answers from Stack Overflow to determine the factors that make explanations in software engineering (SE) and requirements communication effective. Key findings include the positive impact of structural and contextual elements like explanation length, code inclusion, timing, and author reputation on perceived usefulness, with sentiment polarity having little influence. The research offers insights into crafting more effective explanations for developers and RE practitioners, potentially enhancing communication in both open and organizational settings.


<details>
  <summary>Details</summary>
Motivation: Understanding what makes explanations in SE and requirements communication effective is crucial for clarifying ambiguities, justifying design choices, and building a shared understanding among stakeholders. Despite the importance of online Q&A forums like Stack Overflow as sources of these explanations, there's limited knowledge about the specific characteristics that contribute to their usefulness. This study aims to fill this gap by investigating the relative influence of structural, contextual, and linguistic features on the perceived usefulness of explanations.

Method: The researchers conducted an analysis of 3,323 questions and 59,398 answers sourced from Stack Overflow. They utilized text analysis and statistical modeling techniques to explore how various attributes of explanations, such as content richness, timing, and sentiment, relate to their perceived usefulness, measured through normalized upvotes. The study also provided methodological transparency by making its data and replication materials openly available.

Result: The results showed that structural and contextual factors, including the length of the explanation, whether it includes code, the timing of the response, and the reputation of the author, have small to moderate positive effects on the perceived usefulness of explanations. In contrast, the sentiment polarity of the explanations had a negligible effect, indicating that clarity and substance are more important than tone in technical communications.

Conclusion: This empirical study provides evidence-based guidance for developers and RE practitioners on how to create clearer and more effective explanations. It highlights the significance of structural and contextual elements over sentiment in enhancing the quality of explanations. By applying these findings, professionals can improve their communication, reduce ambiguities, and better articulate rationales in day-to-day requirements discussions, thereby supporting fairer and more effective communication in diverse contexts.

Abstract: Explanations are essential in software engineering (SE) and requirements communication, helping stakeholders clarify ambiguities, justify design choices, and build shared understanding. Online Q&A forums such as Stack Overflow provide large-scale settings where such explanations are produced and evaluated, offering valuable insights into what makes them effective. While prior work has explored answer acceptance and voting behavior, little is known about which specific features make explanations genuinely useful. The relative influence of structural, contextual, and linguistic factors, such as content richness, timing, and sentiment, remains unclear. We analyzed 3,323 questions and 59,398 answers from Stack Overflow, combining text analysis and statistical modeling to examine how explanation attributes relate to perceived usefulness (normalized upvotes). Structural and contextual factors, especially explanation length, code inclusion, timing, and author reputation, show small to moderate positive effects. Sentiment polarity has negligible influence, suggesting that clarity and substance outweigh tone in technical communication. This study provides an empirical account of what drives perceived usefulness in developer explanations. It contributes methodological transparency through open data and replication materials, and conceptual insight by relating observed communication patterns to principles of requirements communication. The findings offer evidence-based implications for how developers and RE practitioners can craft clearer and more effective explanations, potentially supporting fairer communication in both open and organizational contexts. From an RE perspective, these determinants can be interpreted as practical signals for ambiguity reduction and rationale articulation in day-to-day requirements communication.

</details>


### [24] [LLM-Based Repair of C++ Implicit Data Loss Compiler Warnings: An Industrial Case Study](https://arxiv.org/abs/2601.14936)
*Chansong You,Hyun Deok Choi,Jingun Hong*

Main category: cs.SE

TL;DR: 本文提出了一种使用大型语言模型（LLMs）自动修复大型C++项目中隐式数据丢失警告的方法，通过Language Server Protocol (LSP)收集上下文、Tree-sitter提取相关代码，并利用LLMs进行决策和生成修复。实验表明，该方法在代码审查过程中由人工开发者接受的修复率为92.73%，相比基线修复策略减少了39.09%因范围检查和异常处理而引入额外指令的警告修复更改数量，虽然比人类开发者创建的最佳解决方案落后13.56%。


<details>
  <summary>Details</summary>
Motivation: 针对大型C++项目中出现的隐式数据丢失警告问题，旨在开发一种能够减少手动修复工作量同时保持代码质量和性能的方法。

Method: 采用大型语言模型（LLMs），结合Language Server Protocol (LSP)获取代码上下文以及Tree-sitter解析技术来抽取关键代码片段，然后基于这些信息让LLM做出判断并生成修复建议。

Result: 在实际C++项目测试中，本方法生成的修复方案被开发者以92.73%的比例接受；与基础修复策略相比，减少了39.09%因添加范围检查及异常处理带来的额外指令增加情况下的警告修复变更次数；表现略逊于人类专家直接提供的最佳解约13.56%。

Conclusion: 基于LLM的方法能够在真实场景下有效减轻处理编译器警告所需的人力投入，同时维持甚至提升代码的质量与性能水平，显示出良好前景用于增强现有软件开发流程中的代码维护实践。

Abstract: This paper presents a method to automatically fix implicit data loss warnings in large C++ projects using Large Language Models (LLMs). Our approach uses the Language Server Protocol (LSP) to gather context, Tree-sitter to extract relevant code, and LLMs to make decisions and generate fixes. The method evaluates the necessity of range checks concerning performance implications and generates appropriate fixes. We tested this method in a large C++ project, resulting in a 92.73% acceptance rate of the fixes by human developers during the code review. Our LLM-generated fixes reduced the number of warning fix changes that introduced additional instructions due to range checks and exception handling by 39.09% compared to a baseline fix strategy. This result was 13.56% behind the optimal solutions created by human developers. These findings demonstrate that our LLM-based approach can reduce the manual effort to address compiler warnings while maintaining code quality and performance in a real-world scenario. Our automated approach shows promise for integration into existing development workflows, potentially improving code maintenance practices in complex C++ software projects.

</details>


### [25] [DeLog: An Efficient Log Compression Framework with Pattern Signature Synthesis](https://arxiv.org/abs/2601.15084)
*Siyu Yu,Yifan Wu,Junjielong Xu,Ying Fu,Ning Wang,Maoyin Liu,Pancheng Jiang,Xiang Zhang,Tong Jia,Pinjia He,Ying Li*

Main category: cs.SE

TL;DR: 研究发现提高解析准确率并不一定带来更好的压缩比，基于此提出了DeLog，一种通过模式签名合成机制实现高效模式分组的新日志压缩器，在公共和生产数据集上达到了最先进的压缩比和速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于解析器的日志压缩方法在处理复杂生产日志时表现不佳，且其核心日志解析组件的准确性下降，这促使作者探索解析准确性和压缩比率之间的关系。

Method: 通过首次实证研究量化了解析准确度与压缩比之间的关系，并基于发现设计了一种新的日志压缩工具DeLog，它采用模式签名合成机制来达到有效的基于模式的分组。

Result: 研究表明更高的解析准确度并不能保证更好的压缩比；而通过有效模式分组与编码则可以实现更好的压缩效果。DeLog在16个公开及10个生产数据集中展现出了顶级的压缩比例和处理速度。

Conclusion: 本研究揭示了日志解析准确度与压缩比之间并非正相关的关系，并提出了一种新颖的日志压缩方案DeLog，该方案通过优化模式分组显著提升了压缩性能。

Abstract: Parser-based log compression, which separates static tem- plates from dynamic variables, is a promising approach to exploit the unique structure of log data. However, its perfor- mance on complex production logs is often unsatisfactory. This performance gap coincides with a known degradation in the accuracy of its core log parsing component on such data, motivating our investigation into a foundational yet unverified question: does higher parsing accuracy necessarily lead to better compression ratio?
  To answer this, we conduct the first empirical study quanti- fying this relationship and find that a higher parsing accuracy does not guarantee a better compression ratio. Instead, our findings reveal that compression ratio is dictated by achiev- ing effective pattern-based grouping and encoding, i.e., the partitioning of tokens into low entropy, highly compressible groups.
  Guided by this insight, we design DeLog, a novel log com- pressor that implements a Pattern Signature Synthesis mecha- nism to achieve efficient pattern-based grouping. On 16 public and 10 production datasets, DeLog achieves state-of-the-art compression ratio and speed.

</details>


### [26] [Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks](https://arxiv.org/abs/2601.15094)
*Md Zahidul Haque,Saima Afrin,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 本研究探讨了QLoRA方法在多任务微调中的有效性，特别是针对代码生成、翻译和总结三大任务。结果表明，多任务QLoRA微调能够有效利用迁移学习，在保持功能性的同时达到与单任务QLoRA或全量多任务微调相媲美甚至更优的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管QLoRA优化的大规模代码模型（LCMs）已经在多种任务中显示出强大的性能，但对于同一个模型通过QLoRA为多个代码相关任务进行微调时是否仍然有效尚不清楚。此外，关于多任务微调与QLoRA优化之间的交互作用以及迁移学习如何影响生成工件的正确性和质量方面仍缺乏深入研究。

Method: 选取了代码生成、翻译和总结作为代表性任务，采用多任务QLoRA微调方法，并通过基于执行和相似性的度量标准评估功能正确性，同时进行全面的代码质量分析。

Result: 研究发现，多任务QLoRA微调能够有效地利用迁移学习，相对于单任务QLoRA和全量多任务微调，在不同大小的模型上均表现出竞争性或更优的性能；较大模型在正确性和质量之间表现出更加一致的平衡，而较小模型虽然能较好地保持功能，但在更可能遇到与质量相关的问题。

Conclusion: 多任务QLoRA微调是一种有效的方法，它能够在减少资源需求的同时，通过迁移学习改善代码相关任务上的表现。不过，对于小型模型而言，在追求高质量输出时需谨慎考虑其局限性。

Abstract: Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.

</details>


### [27] [Why Authors and Maintainers Link (or Don't Link) Their PyPI Libraries to Code Repositories and Donation Platforms](https://arxiv.org/abs/2601.15139)
*Alexandros Tsakpinis,Nicolas Raube,Alexander Pretschner*

Main category: cs.SE

TL;DR: 本研究通过向50,000名PyPI作者和维护者发送调查问卷，分析了超过1,400份回复，使用大型语言模型（LLM）主题建模来揭示与链接代码仓库和捐赠平台相关的动机和障碍。研究发现，虽然链接仓库URL有助于促进合作、增加透明度和支持问题追踪，但一些维护者由于疏忽、懒惰或认为其对项目无关紧要而忽略了这一点。对于捐赠平台链接，尽管它们可以帮助开源工作获得资金支持，但由于怀疑态度、技术摩擦和组织限制等因素的影响，其使用受到了阻碍。此外，过时的链接、缺乏意识以及不明确的指导等跨领域挑战也影响着这两种类型的元数据。


<details>
  <summary>Details</summary>
Motivation: Python包索引(PyPI)上库的元数据（包括源代码仓库链接和捐赠平台链接）在支持开源库的透明性、信任度及可持续性方面起着关键作用。然而，许多包缺少这些元数据，并且关于背后原因知之甚少。

Method: 该论文呈现了一项大规模实证研究，结合两次针对50,000名PyPI作者和维护者的定向调查。研究者们采用基于大语言模型的主题建模方法分析了超过1,400份回复，以揭示与链接仓库及捐赠平台有关的关键动机和障碍。

Result: 研究显示，仓库URL通常被链接起来以促进协作、提高透明度并启用问题跟踪；不过，部分维护者因忽视、懒散或认为对其项目无直接关联而未提供。至于捐赠平台链接，则是为了支持开源工作或接收财务贡献，但受到质疑、技术阻力及组织约束的影响。另外，陈旧链接、认知不足及指南不明等问题也普遍存在于这两类元数据中。

Conclusion: 这项研究为理解PyPI上的元数据实践提供了实证见解，并就如何改进提出建议。同时，它还展示了利用主题建模方法处理短文本调查回应的有效性。

Abstract: Metadata of libraries on the Python Package Index (PyPI)-including links to source code repositories and donation platforms-plays a critical role in supporting the transparency, trust, and sustainability of open-source libraries. Yet, many packages lack such metadata, and little is known about the underlying reasons. This paper presents a large-scale empirical study combining two targeted surveys sent to 50,000 PyPI authors and maintainers. We analyze more than 1,400 responses using large language model (LLM)-based topic modeling to uncover key motivations and barriers related to linking repositories and donation platforms. While repository URLs are often linked to foster collaboration, increase transparency, and enable issue tracking, some maintainers omit them due to oversight, laziness, or the perceived irrelevance to their project. Donation platform links are reported to support open source work or receive financial contributions, but are hindered by skepticism, technical friction, and organizational constraints. Cross-cutting challenges-such as outdated links, lack of awareness, and unclear guidance-affect both types of metadata. We further assess the robustness of our topic modeling pipeline across 30 runs (84% lexical and 89% semantic similarity) and validate topic quality with 23 expert raters (Randolph's kappa = 0.55). The study contributes empirical insights into PyPI's metadata practices and provides recommendations for improving them, while also demonstrating the effectiveness of our topic modeling approach for analyzing short-text survey responses.

</details>


### [28] [SAGA: Detecting Security Vulnerabilities Using Static Aspect Analysis](https://arxiv.org/abs/2601.15154)
*Yoann Marquer,Domenico Bianculli,Lionel C. Briand*

Main category: cs.SE

TL;DR: 提出了SAGA方法，一种用于检测Python源代码中多种安全漏洞的方法。通过使用源代码解析器和领域特定语言定义静态方面，SAGA能够有效地识别包括完整性、机密性等在内的安全相关属性的问题。实验结果显示，SAGA在敏感性和特异性上均表现出色，并且比其他四种常见的安全分析工具更快。


<details>
  <summary>Details</summary>
Motivation: 现有的针对Python的分析工具只能检测少数类型的漏洞，而随着Python项目中出现的安全漏洞类型越来越多，需要有一种能检测多种漏洞的新方法。

Method: SAGA方法结合了源代码解析技术来提取控制流与数据流信息，并将其表示为符号控制流图；同时，开发了一种领域特定语言用来定义源代码的静态特性及其在图形遍历过程中的演变。基于此，研究人员创建了一个关于完整性、机密性及其他安全属性的静态方面库。

Result: SAGA在包含108个漏洞的数据集上进行了评估，达到了100%的敏感度和99.15%的特异度，仅有一次误报。此外，其运行速度远超比较对象，完成整个分析过程所需时间不到31秒，比基准工具快2.5到512.1倍不等。

Conclusion: SAGA提供了一种高效准确地检测Python源码中多样化安全漏洞的方式，其性能优于现有的一些主流工具，在速度上也有显著优势。

Abstract: Python is one of the most popular programming languages; as such, projects written in Python involve an increasing number of diverse security vulnerabilities. However, existing state-of-the-art analysis tools for Python only support a few vulnerability types. Hence, there is a need to detect a large variety of vulnerabilities in Python projects.
  In this paper, we propose the SAGA approach to detect and locate vulnerabilities in Python source code in a versatile way. SAGA includes a source code parser able to extract control- and data-flow information and to represent it as a symbolic control-flow graph, as well as a domain-specific language defining static aspects of the source code and their evolution during graph traversals. We have leveraged this language to define a library of static aspects for integrity, confidentiality, and other security-related properties.
  We have evaluated SAGA on a dataset of 108 vulnerabilities, obtaining 100% sensitivity and 99.15% specificity, with only one false positive, while outperforming four common security analysis tools. This analysis was performed in less than 31 seconds, i.e., between 2.5 and 512.1 times faster than the baseline tools.

</details>


### [29] [Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback](https://arxiv.org/abs/2601.15188)
*Stephan Wallraven,Tim Köhne,Hartmut Westenberger,Andreas Moser*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLMs）在生成ABAP代码方面的表现，通过180个任务的基准测试发现，更强大的LLMs在经过几次迭代后可以达到约75%的成功率，并且能够很好地利用编译器反馈进行改进。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI已经在许多编程语言中成功应用，但对于ABAP代码生成的系统性分析却很少。本研究旨在探索不同类型LLMs在生成语法正确且功能完整的ABAP代码方面的能力、它们如何有效地使用编译器反馈来迭代改善以及哪些类型的任务特别具有挑战性。

Method: 创建了一个包含180个任务的基准，这些任务由改编的人类评估任务和实际SAP场景组成。通过这个基准测试不同LLMs的表现。

Result: 研究表明，性能更强的LLMs在几轮迭代后可以达到大约75%的成功率，并且从编译器反馈中获益良多；相比之下，较小的模型表现要弱得多。

Conclusion: 研究强调了强大LLMs在ABAP开发过程中的高潜力，特别是在迭代错误修正方面。

Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [GCG Attack On A Diffusion LLM](https://arxiv.org/abs/2601.14266)
*Ruben Neyroud,Sam Corley*

Main category: cs.LG

TL;DR: 本研究探索了Greedy Coordinate Gradient (GCG)风格的对抗性提示攻击在基于扩散的语言模型LLaDA上的有效性，通过评估多种攻击变体提供了关于扩散语言模型鲁棒性和攻击面的初步见解。


<details>
  <summary>Details</summary>
Motivation: 尽管贪婪坐标梯度（GCG）攻击对自回归模型有效，但对于扩散语言模型的有效性仍不清楚。本研究旨在填补这一空白，特别是针对开源扩散语言模型LLaDA，探索不同类型的GCG风格对抗性提示攻击的效果，并希望为这类模型的对抗性分析开发出新的优化与评估策略。

Method: 采用来自AdvBench数据集中的有害提示作为测试样本，实施了包括前缀扰动和基于后缀的对抗生成在内的多种GCG式攻击变体来评估LLaDA模型。

Result: 研究表明，对于像LLaDA这样的扩散语言模型来说，GCG风格的对抗性提示攻击确实构成了一定威胁，揭示了这些模型潜在的安全脆弱点。

Conclusion: 该工作为理解扩散语言模型面对对抗性攻击时的鲁棒性提供了初步但重要的视角，强调了开发新方法以增强此类模型安全性的必要性。

Abstract: While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.

</details>


### [31] [Divide and Refine: Enhancing Multimodal Representation and Explainability for Emotion Recognition in Conversation](https://arxiv.org/abs/2601.14274)
*Anh-Tuan Mai,Cam-Van Thi Nguyen,Duc-Trong Le*

Main category: cs.LG

TL;DR: 本文提出了一种名为DnR（Divide and Refine）的两阶段框架，用于多模态对话中的情绪识别。该方法通过显式地分解每个模ality的独特性、成对冗余性和协同作用，并使用定制目标来增强这些组件的信息量，同时保持它们各自的角色。实验结果表明，这种方法在多个MERC后端上表现出一致的改进。


<details>
  <summary>Details</summary>
Motivation: 多模态情绪识别在对话中需要能够有效整合来自多种模态信号的表示方法，但现有方法往往忽视了数据准备过程中保留独特性、冗余性和协同贡献的重要性。直接对原始输入或融合嵌入进行增强可能会模糊模态独特性和跨模态信号之间的界限。

Method: 提出了一个两阶段框架DnR，第一阶段Divide将每个模态分解为独特性、成对冗余性和协同作用；第二阶段Refine则通过定制的目标函数提升这些组成部分的信息量，同时维持它们各自不同的角色。

Result: 在IEMOCAP和MELD数据集上的广泛实验表明，提出的DnR框架可以与多种多模态处理流程兼容，并且相对于多种MERC基础架构均展示出一致性的性能提升。

Conclusion: 通过明确划分、精炼以及重新组合多模态表征，作为一种有原则的方法，在推进情绪识别方面被证明是有效的。

Abstract: Multimodal emotion recognition in conversation (MERC) requires representations that effectively integrate signals from multiple modalities. These signals include modality-specific cues, information shared across modalities, and interactions that emerge only when modalities are combined. In information-theoretic terms, these correspond to \emph{unique}, \emph{redundant}, and \emph{synergistic} contributions. An ideal representation should leverage all three, yet achieving such balance remains challenging. Recent advances in contrastive learning and augmentation-based methods have made progress, but they often overlook the role of data preparation in preserving these components. In particular, applying augmentations directly to raw inputs or fused embeddings can blur the boundaries between modality-unique and cross-modal signals. To address this challenge, we propose a two-phase framework \emph{\textbf{D}ivide and \textbf{R}efine} (\textbf{DnR}). In the \textbf{Divide} phase, each modality is explicitly decomposed into uniqueness, pairwise redundancy, and synergy. In the \textbf{Refine} phase, tailored objectives enhance the informativeness of these components while maintaining their distinct roles. The refined representations are plug-and-play compatible with diverse multimodal pipelines. Extensive experiments on IEMOCAP and MELD demonstrate consistent improvements across multiple MERC backbones. These results highlight the effectiveness of explicitly dividing, refining, and recombining multimodal representations as a principled strategy for advancing emotion recognition. Our implementation is available at https://github.com/mattam301/DnR-WACV2026

</details>


### [32] [Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct](https://arxiv.org/abs/2601.14277)
*Uygar Kurt*

Main category: cs.LG

TL;DR: 本文对llama.cpp在Llama-3.1-8B-Instruct模型上的量化方案进行了统一的实证研究，涵盖了从3到8位K-量化及传统格式。通过评估不同量化方案在标准推理、知识、指令跟随和真实性基准测试中的下游任务性能，以及困惑度和CPU吞吐量等指标，为选择合适的量化方案提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 由于现有的量化格式经常被不一致地评估，使得用户难以在这些方案中做出选择。此外，量化技术能够减少大语言模型部署时的内存使用并提高运行效率，尤其对于本地运行模型的用户而言尤为重要。因此，需要一个全面的研究来帮助用户根据自己的需求和资源预算做出明智的选择。

Method: 本研究采用了一种统一的方法，对Llama-3.1-8B-Instruct（FP16, GGUF）这一现代模型上实施的llama.cpp量化进行了实证分析。研究覆盖了3至8位K-量化及旧有格式，并通过一系列标准推理、知识掌握、遵循指示及诚实性评测来衡量下游任务的表现。同时考察了困惑度、CPU处理能力（预填充/解码）、模型大小、压缩率与量化所需时间等多方面指标。

Result: 研究结果揭示了不同量化方案对于Llama-3.1-8B-Instruct模型性能的影响，包括但不限于推理能力、知识保留程度、指令执行准确性和诚实性等方面的变化。此外，还提供了关于每个方案下的模型大小缩减比例、压缩效率、计算速度提升幅度以及完成量化过程所需时间的数据。

Conclusion: 该工作为选择适合特定应用场景和个人资源限制条件下的llama.cpp量化方案提供了一份详尽的指导手册，有助于读者基于具体需求做出更加合理的决策。

Abstract: Quantization is a practical technique for making large language models easier to deploy by reducing the precision used to store and operate on model weights. This can lower memory use and improve runtime feasibility on constrained hardware, which is especially relevant for users running models locally. Quantization in llama.cpp enables large language models to run on commodity hardware, but available formats are often evaluated inconsistently, making it hard to choose among schemes. We present a unified empirical study of the llama.cpp quantization on a single modern model, Llama-3.1-8B-Instruct (FP16, GGUF), covering 3-8 bit K-quant and legacy formats. We evaluate downstream task performance across standard reasoning, knowledge, instruction-following, and truthfulness benchmarks, and also measure perplexity and CPU throughput (prefill/decoding) alongside model size, compression, and quantization time. Ultimately, this work is a practical guide for choosing a llama.cpp quantization scheme, helping readers make informed, context-aware decisions for their intended use and resource budget.

</details>


### [33] [Beyond Denial-of-Service: The Puppeteer's Attack for Fine-Grained Control in Ranking-Based Federated Learning](https://arxiv.org/abs/2601.14687)
*Zhihao Chen,Zirui Gong,Jianting Ning,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 本文介绍了一种针对联邦排名学习（FRL）的新型精细控制攻击——边缘控制攻击（ECA），该攻击能够精确地降低竞争对手的准确率至任意目标水平，同时保持正常的收敛轨迹以避免被检测。实验结果表明ECA在多种基准数据集和拜占庭鲁棒聚合规则下的表现优于基线方法，平均误差仅为0.224%。


<details>
  <summary>Details</summary>
Motivation: 虽然联邦排名学习（FRL）通过采用基于排名的离散更新机制减少了模型中毒攻击的风险，但其仍可能受到新型局部模型中毒攻击的影响。本文旨在探索并提出一种针对FRL框架的有效攻击手段，即边缘控制攻击（ECA），以展示当前FRL系统中存在的安全漏洞。

Method: 提出了边缘控制攻击（ECA），这是一种专门针对排名型联邦学习框架设计的精细控制攻击方法。ECA分为两个阶段执行：首先识别并操纵上升沿和下降沿来使全局模型与目标模型对齐；然后扩大选择边界差距以稳定全局模型于特定精度。

Result: 通过对七个基准数据集及九种拜占庭鲁棒聚合规则进行广泛测试后发现，ECA可以实现精准的精度操控，平均误差仅为0.224%，相比基础方法最多提高了17倍的表现。

Conclusion: 尽管联邦排名学习（FRL）在减少通信成本和抵御连续空间内优化恶意更新方面具有优势，但它仍然容易受到如ECA这样的新型精细控制攻击。研究强调了开发更强防御措施对抗高级中毒攻击的重要性。

Abstract: Federated Rank Learning (FRL) is a promising Federated Learning (FL) paradigm designed to be resilient against model poisoning attacks due to its discrete, ranking-based update mechanism. Unlike traditional FL methods that rely on model updates, FRL leverages discrete rankings as a communication parameter between clients and the server. This approach significantly reduces communication costs and limits an adversary's ability to scale or optimize malicious updates in the continuous space, thereby enhancing its robustness. This makes FRL particularly appealing for applications where system security and data privacy are crucial, such as web-based auction and bidding platforms. While FRL substantially reduces the attack surface, we demonstrate that it remains vulnerable to a new class of local model poisoning attack, i.e., fine-grained control attacks. We introduce the Edge Control Attack (ECA), the first fine-grained control attack tailored to ranking-based FL frameworks. Unlike conventional denial-of-service (DoS) attacks that cause conspicuous disruptions, ECA enables an adversary to precisely degrade a competitor's accuracy to any target level while maintaining a normal-looking convergence trajectory, thereby avoiding detection. ECA operates in two stages: (i) identifying and manipulating Ascending and Descending Edges to align the global model with the target model, and (ii) widening the selection boundary gap to stabilize the global model at the target accuracy. Extensive experiments across seven benchmark datasets and nine Byzantine-robust aggregation rules (AGRs) show that ECA achieves fine-grained accuracy control with an average error of only 0.224%, outperforming the baseline by up to 17x. Our findings highlight the need for stronger defenses against advanced poisoning attacks. Our code is available at: https://github.com/Chenzh0205/ECA

</details>


### [34] [On the Limits of Learned Importance Scoring for KV Cache Compression](https://arxiv.org/abs/2601.14279)
*Brady Steele*

Main category: cs.LG

TL;DR: 研究了通过投机性重要性预测(SIP)学习的KV缓存压缩，但SIP并未超越简单基线方法。位置基础启发式方法和预填充注意力提供了与复杂学习评分器相当的信息，表明KV表示中用于重要性预测的边际信息有限。


<details>
  <summary>Details</summary>
Motivation: 探索使用投机性重要性预测（SIP）来改善KV缓存压缩的效果，试图通过学习方法提高模型性能。

Method: 采用了一种名为投机性重要性预测(SIP)的方法，它是一个1.7M参数的非查询感知评分器，仅从KV表示预测令牌的重要性。实验设计包括多视界前瞻和跨注意力机制，并在5个种子、4个保留级别以及3个任务上进行了测试。

Result: 发现基于位置的启发式方法（例如保持前4个+最后N个令牌）能够匹配甚至超过学习方法的表现；预填充注意力提供的信号等同于复杂的学到的评分器所提供的；KV表示之外的位置和预填充注意力对于重要性预测来说似乎含有有限的边际信息。

Conclusion: 尽管尝试了较为复杂的架构，SIP方法未能超越简单的基线方法，包括随机选择。这表明，未来查询与生成轨迹之间的循环依赖可能是造成这一难题的原因之一。

Abstract: We investigate learned KV cache compression through Speculative Importance Prediction (SIP), a 1.7M parameter non-query-aware scorer that predicts token importance from KV representations alone. Despite architectural sophistication (multi-horizon lookahead, cross-attention), SIP does not outperform simple baselines, including random selection, across 5 seeds, 4 retention levels, and 3 tasks. Key findings: (1) position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches; (2) prefill attention provides equivalent signal to complex learned scorers; (3) marginal information in KV representations beyond position and prefill attention appears limited for importance prediction. We hypothesize that circular dependence between future queries and generation trajectories contributes to this difficulty.

</details>


### [35] [RadixMLP - Intra-batch Deduplication for Causal Transformers](https://arxiv.org/abs/2601.15013)
*Michael Feil,Julius Lipp*

Main category: cs.LG

TL;DR: RadixMLP is a technique that eliminates redundant computations for shared prefixes in batch inference workloads of causal transformer models, achieving 1.44-1.59× speedups on realistic reranking tasks and up to 5× on synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the inefficiency in standard inference engines which redundantly recompute identical MLP activations for shared prefixes across different sequences in batch inference, leading to unnecessary computational costs.

Method: RadixMLP utilizes the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to dynamically map batches to a prefix trie. This approach gathers shared segments into a compressed form for computation and scatters results back only at attention boundaries, thereby eliminating redundancy without requiring stateful operations or additional passes.

Result: In end-to-end serving benchmarks using MS MARCO v1.1 with Qwen3 models (ranging from 0.6B to 8B parameters), RadixMLP was able to achieve 1.44-1.59× speedups for realistic reranking workloads. Furthermore, it showed potential for even greater efficiency, reaching up to 5× speedups in synthetic benchmarks designed with longer shared prefixes.

Conclusion: The paper concludes by highlighting the effectiveness of RadixMLP in significantly improving the efficiency of batch inference for causal transformer models, particularly in scenarios where input sequences share common prefixes. The method's stateless operation and single-pass execution make it a practical solution for enhancing performance in real-world applications.

Abstract: Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every copy of the shared prefix. We introduce RadixMLP, a technique that exploits the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to eliminate this redundancy. RadixMLP dynamically maps batches to a prefix trie, gathering shared segments into a compressed representation for position-wise computation and scattering results back only at attention boundaries. RadixMLP is stateless and operates within a single forward pass. In end-to-end serving benchmarks on MS~MARCO v1.1 with Qwen3 models (0.6B to 8B parameters), RadixMLP achieves 1.44-1.59$\times$ speedups in realistic reranking workloads, with up to $5\times$ speedups on synthetic benchmarks with longer shared prefixes. Our code is available at https://github.com/michaelfeil/radix-mlp.

</details>


### [36] [Beyond Affinity: A Benchmark of 1D, 2D, and 3D Methods Reveals Critical Trade-offs in Structure-Based Drug Design](https://arxiv.org/abs/2601.14283)
*Kangyu Zheng,Kai Zhang,Jiale Tan,Xuehan Chen,Yingzhou Lu,Zaixi Zhang,Lichao Sun,Marinka Zitnik,Tianfan Fu,Zhiding Liang*

Main category: cs.LG

TL;DR: 本文建立了一个基准来评估基于不同算法基础的15种模型在药物设计中的表现，通过分析生成分子的药理性质及其与特定目标蛋白的对接亲和力和姿态。研究发现3D结构基础模型在结合亲和力方面表现出色但化学有效性及姿态质量不稳定；1D模型在标准分子指标上表现可靠但难以达到最优结合亲和力；2D模型则在保持高化学有效性的同时取得中等结合分数。


<details>
  <summary>Details</summary>
Motivation: 当前基于结构的药物设计领域主要由搜索算法、深度生成模型和强化学习三种算法主导，而跨算法比较仍然较少。为了填补这一空白，本研究旨在对比不同算法类型下模型的表现情况。

Method: 通过对15种不同算法基础的模型进行评估，考察它们生成分子的药理特性以及与指定靶标蛋白之间的对接亲和力和姿态。特别强调了可以将对接函数视为黑盒预言机的一维/二维配体中心药物设计方法应用于SBDD中。

Result: 3D结构导向模型在结合亲和力方面表现出色，但在化学有效性和姿态质量存在不一致；1D模型在标准分子度量上表现稳定，却很少能达到最佳结合亲和力；2D模型提供了平衡性能，在维持高化学有效性的同时实现了中等程度的结合得分。

Conclusion: 研究揭示了各类别模型的独特优势，并为未来SBDD模型的设计提出了建议。同时指出了每种类别的关键改进领域，鼓励研究人员结合不同类型方法的优势并克服其局限性。

Abstract: Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of fifteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities and poses with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. Our evaluation reveals distinct patterns across model categories. 3D structure-based models excel in binding affinities but show inconsistencies in chemical validity and pose quality. 1D models demonstrate reliable performance in standard molecular metrics but rarely achieve optimal binding affinities. 2D models offer balanced performance, maintaining high chemical validity while achieving moderate binding scores. Through detailed analysis across multiple protein targets, we identify key improvement areas for each model category, providing insights for researchers to combine strengths of different approaches while addressing their limitations. All the code that are used for benchmarking is available in https://github.com/zkysfls/2025-sbdd-benchmark

</details>


### [37] [A Comparison of Polynomial-Based Tree Clustering Methods](https://arxiv.org/abs/2601.14285)
*Pengyu Liu,Mariel Vázquez,Nataša Jonoska*

Main category: cs.LG

TL;DR: 本文探讨了树多项式在生物数据，特别是RNA二级结构分析中的应用，并比较了基于树区分多项式的不同距离方法在树聚类中的性能。发现基于条目级归一化距离的方法在比较中具有最高的聚类准确性。此外，还实现了两种基本的自动编码器模型用于利用多项式进行树聚类。


<details>
  <summary>Details</summary>
Motivation: 随着测序技术和人工智能的发展产生了大量的可以用树结构表示的生物数据，需要新的方法来进行树结构数据分析。树多项式提供了一种计算效率高、可解释性强且全面的方式来将树结构编码为矩阵，兼容大多数数据分析工具。

Method: 使用基于树多项式的堪培拉距离等机器学习方法来分析系统发生和核酸结构。比较了基于树区分多项式的不同距离方法在树聚类中的表现，并实现了两种基础的自动编码器模型以利用多项式进行树聚类。

Result: 结果表明，基于条目级归一化距离的距离方法在所比较的方法中具有最高的聚类准确性。

Conclusion: 树多项式及其相关的距离方法提供了有效处理树结构生物数据的手段，尤其是对于RNA二级结构的研究。通过比较不同距离方法在树聚类上的表现，证实了特定距离度量的优势。

Abstract: Tree structures appear in many fields of the life sciences, including phylogenetics, developmental biology and nucleic acid structures. Trees can be used to represent RNA secondary structures, which directly relate to the function of non-coding RNAs. Recent developments in sequencing technology and artificial intelligence have yielded numerous biological data that can be represented with tree structures. This requires novel methods for tree structure data analytics. Tree polynomials provide a computationally efficient, interpretable and comprehensive way to encode tree structures as matrices, which are compatible with most data analytics tools. Machine learning methods based on the Canberra distance between tree polynomials have been introduced to analyze phylogenies and nucleic acid structures. In this paper, we compare the performance of different distances in tree clustering methods based on a tree distinguishing polynomial. We also implement two basic autoencoder models for clustering trees using the polynomial. We find that the distance based methods with entry-level normalized distances have the highest clustering accuracy among the compared methods.

</details>


### [38] [Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents](https://arxiv.org/abs/2601.14287)
*Xiucheng Xu,Bingbing Xu,Xueyun Tian,Zihe Huang,Rongxin Chen,Yunfan Li,Huawei Shen*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架CoM（记忆链），该框架通过轻量级构建与复杂利用相结合来改进大型语言模型的外部记忆系统。实验表明，CoM在提高准确性的同时显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的外部记忆系统通常采用两阶段过程：首先进行昂贵的记忆结构化处理，然后是简单的检索增强生成。但这种方法存在两个主要问题：复杂的构造成本高且性能提升有限；简单的内容拼接无法有效连接检索召回与推理准确性。

Method: 提出了名为CoM（Chain-of-Memory）的新框架，它提倡轻量级构建与高级利用相结合的方式。CoM通过动态演化将检索到的信息片段组织成连贯的推理路径，并使用自适应截断去除无关噪声。

Result: 在LongMemEval和LoCoMo基准测试中，CoM相比强大基线实现了7.5%至10.4%的准确率提升，同时大幅减少了计算负担，其消耗的token数量和延迟分别约为复杂记忆架构的2.7%和6.0%。

Conclusion: CoM提供了一种更高效的方法来改善大型语言模型的持久知识维护及长期决策能力，通过简化构建过程并优化信息利用方式，在保持甚至提高模型性能的同时大大降低了资源消耗。

Abstract: External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.

</details>


### [39] [Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity](https://arxiv.org/abs/2601.14300)
*Jun Liu,Leo Yu Zhang,Fengpeng Li,Isao Echizen,Jiantao Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的攻击框架，结合了零查询频率域初始化和模式驱动优化策略，旨在硬标签黑盒环境下有效恢复损失梯度符号。理论分析与实验结果表明，该方法在攻击成功率和查询效率上均优于现有最先进方法，尤其在低查询次数下表现突出，并且能够成功绕过最新的状态防御Blacklight。


<details>
  <summary>Details</summary>
Motivation: 在仅能观察到top-1预测标签的硬标签黑盒环境中，理解模型行为具有实际重要性但存在根本限制。核心挑战在于能否从这种离散响应中恢复有意义的梯度信息。

Method: 开发了一个统一的理论视角来解释现有基于符号翻转的硬标签攻击实际上是在隐式逼近真实损失梯度的符号；提出了一个新的攻击框架，包括零查询频率域初始化及Pattern-Driven Optimization (PDO)策略。

Result: 理论保证了初始化相比随机基线能达到更高的预期余弦相似度至真实的梯度符号，而提出的PDO过程比现有的结构化搜索方法实现了显著更低的查询复杂度。实验证明，在CIFAR-10、ImageNet等数据集上，本方法在攻击成功率和查询效率方面均优于当前最先进的硬标签攻击技术。

Conclusion: 新提出的攻击框架不仅在图像分类任务中表现出色，而且还能很好地泛化到受污染的数据、生物医学数据集以及密集预测任务中，并成功规避了名为Blacklight的状态防御机制。

Abstract: Hard-label black-box settings, where only top-1 predicted labels are observable, pose a fundamentally constrained yet practically important feedback model for understanding model behavior. A central challenge in this regime is whether meaningful gradient information can be recovered from such discrete responses. In this work, we develop a unified theoretical perspective showing that a wide range of existing sign-flipping hard-label attacks can be interpreted as implicitly approximating the sign of the true loss gradient. This observation reframes hard-label attacks from heuristic search procedures into instances of gradient sign recovery under extremely limited feedback. Motivated by this first-principles understanding, we propose a new attack framework that combines a zero-query frequency-domain initialization with a Pattern-Driven Optimization (PDO) strategy. We establish theoretical guarantees demonstrating that, under mild assumptions, our initialization achieves higher expected cosine similarity to the true gradient sign compared to random baselines, while the proposed PDO procedure attains substantially lower query complexity than existing structured search approaches. We empirically validate our framework through extensive experiments on CIFAR-10, ImageNet, and ObjectNet, covering standard and adversarially trained models, commercial APIs, and CLIP-based models. The results show that our method consistently surpasses SOTA hard-label attacks in both attack success rate and query efficiency, particularly in low-query regimes. Beyond image classification, our approach generalizes effectively to corrupted data, biomedical datasets, and dense prediction tasks. Notably, it also successfully circumvents Blacklight, a SOTA stateful defense, resulting in a $0\%$ detection rate. Our code will be released publicly soon at https://github.com/csjunjun/DPAttack.git.

</details>


### [40] [Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2601.14327)
*YuanLab. ai,Shawn Wu,Jiangang Luo,Tong Yu,Darcy Chen,Sean Wang,Xudong Zhao,Louie Li,Claire Wang,Hunter He,Carol Wang,Allen Wang*

Main category: cs.LG

TL;DR: 本文提出了一种针对MoE大型语言模型预训练阶段的层自适应专家剪枝算法（LAEP），通过选择性地剪枝未充分利用的专家并根据令牌分布统计重新组织计算设备上的专家，从而提高训练效率和减少模型大小。实验表明，该方法在从头开始预训练1010B基础模型时，能够实现48.3%的训练效率提升和33.3%的参数减少，同时保持了跨多个领域的优秀性能。


<details>
  <summary>Details</summary>
Motivation: 尽管混合专家（MoE）大型语言模型（LLMs）能够以较少的活跃参数提供更优的准确性，但其预训练由于专家利用率低及训练效率有限而成为一个重要的计算瓶颈。

Method: 本工作介绍了一种名为Layer-Adaptive Expert Pruning (LAEP) 的算法，专为MoE LLMs的预训练阶段设计。与主要在后训练阶段发挥作用的先前专家剪枝方法不同，所提出的算法通过选择性地剪枝未被充分利用的专家，并依据令牌分布统计数据跨计算设备重新组织专家来增强训练效率。

Result: 全面的实验证明LAEP能够有效减小模型尺寸，并显著提高预训练效率。特别地，在从零开始预训练1010B Base模型时，LAEP实现了训练效率上48.3%的增长以及参数量33.3%的削减，同时仍然保证了多领域内的出色表现。

Conclusion: Layer-Adaptive Expert Pruning (LAEP) 算法不仅提高了MoE LLMs的预训练效率，还减少了模型规模，同时保持了出色的性能水平。

Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.

</details>


### [41] [Hierarchical Contextual Uplift Bandits for Catalog Personalization](https://arxiv.org/abs/2601.14333)
*Anupam Agrawal,Rajesh Mohanty,Shamik Bhattacharjee,Abhimanyu Mittal*

Main category: cs.LG

TL;DR: 本文提出了一种分层上下文提升Bandit框架，用于解决动态环境中的个性化推荐问题，特别是在幻想体育中。该方法通过动态调整上下文粒度，并结合提升建模原则，有效提升了推荐质量与用户满意度，在大规模A/B测试和实际部署后观察到了显著的收入增长。


<details>
  <summary>Details</summary>
Motivation: 现有的情境Bandit算法在处理幻想体育等快速变化环境下的个性化推荐时面临挑战，特别是当用户行为迅速改变或外部因素导致奖励分布发生剧烈变动时，这些算法需要频繁重新训练以保持其有效性。

Method: 提出了一个分层次的情境提升Bandit框架，能够根据情境相似性动态地从广泛的系统级洞察转换到详细的用户特定情境，促进了有效的策略转移并缓解了冷启动问题。此外，还融入了提升建模原则来增强模型性能。

Result: 在Dream11幻想体育平台上的大规模A/B测试结果显示，所提方法相较于当前生产系统而言，不仅显著提高了推荐的质量，实现了0.4%的收入增长，同时也改善了用户满意度指标。进一步地，在2025年5月将该系统作为默认目录个性化系统上线后，又额外观察到了0.5%的收入增长。

Conclusion: 通过引入分层上下文提升Bandit框架，研究成功解决了动态环境中个性化推荐面临的难题，尤其适用于幻想体育领域。实验结果表明，新方法不仅能提高推荐系统的性能，还能带来可观的经济效益。

Abstract: Contextual Bandit (CB) algorithms are widely adopted for personalized recommendations but often struggle in dynamic environments typical of fantasy sports, where rapid changes in user behavior and dramatic shifts in reward distributions due to external influences necessitate frequent retraining. To address these challenges, we propose a Hierarchical Contextual Uplift Bandit framework. Our framework dynamically adjusts contextual granularity from broad, system-wide insights to detailed, user-specific contexts, using contextual similarity to facilitate effective policy transfer and mitigate cold-start issues. Additionally, we integrate uplift modeling principles into our approach. Results from large-scale A/B testing on the Dream11 fantasy sports platform show that our method significantly enhances recommendation quality, achieving a 0.4% revenue improvement while also improving user satisfaction metrics compared to the current production system. We subsequently deployed this system to production as the default catalog personalization system in May 2025 and observed a further 0.5% revenue improvement.

</details>


### [42] [VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models](https://arxiv.org/abs/2601.14354)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 本文提出了变分JEPA（VJEPA）和贝叶斯JEPA（BJEPA），通过概率方法预测未来潜在状态的分布，从而在不依赖像素重构的情况下支持最优控制，并能够过滤掉高方差的干扰因素。


<details>
  <summary>Details</summary>
Motivation: 现有的JEPA框架依赖于确定性回归目标，这掩盖了概率语义并限制了其在随机控制中的应用。

Method: 通过引入VJEPA来学习未来潜在状态的概率分布，并通过变分目标进行优化；进一步提出BJEPA，将预测信念分解为学习的动力学专家与模块化先验专家，以实现零样本任务迁移和满足约束条件。

Result: 实验表明，VJEPA和BJEPA能够在噪声环境中成功过滤掉导致表示崩溃的高方差干扰因素，并且不需要依赖于观察似然性的自回归模型。

Conclusion: VJEPA提供了一个基础框架，用于在高维、嘈杂环境下的可扩展、稳健、意识到不确定性的规划，同时保持对观测值的无似然性。

Abstract: Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Variational JEPA (VJEPA)}, a \textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.

</details>


### [43] [Adaptive KDE for Real-Time Thresholding: Prioritized Queues for Financial Crime Investigation](https://arxiv.org/abs/2601.14473)
*Danny Butvinik,Nana Boateng,Achi Hackmon*

Main category: cs.LG

TL;DR: 本研究提出了一种将风险评分流转换为一个或多个审查队列的方法，通过在线自适应核密度估计和尾部质量曲线来满足容量限制，并在带宽上检测到的持续密度谷处进行切割。该方法无需标签、支持多队列路由并实时运行。


<details>
  <summary>Details</summary>
Motivation: 为了在明确的摄入量约束下，将连续的风险评分转换成一个或多个审核队列，而不需要使用前K名或者手动调整阈值。

Method: 采用在线自适应核密度拟合评分流，将其转化为尾质量曲线以满足容量需求，并且根据跨带宽检测到的持久性密度谷来进行'切割'。整个过程不依赖于标签，支持多队列路由，并能利用滑动窗口或指数衰减实现实时操作。

Result: 在合成的、漂移的、多模态的数据流上，该方法实现了具有竞争力的容量遵守度，同时减少了阈值抖动。每次事件更新的时间复杂度为O(G)，并且每个活动占用常数内存。

Conclusion: 所提出的方法能够在满足容量要求的同时，有效减少阈值抖动，适用于处理动态变化的风险评分数据流。

Abstract: We study the problem of converting a stream of risk scores into one or more review queues under explicit intake constraints[cite: 6]. Instead of top-$K$ or manually tuned cutoffs, we fit an online adaptive kernel density to the score stream, transform the density into a tail-mass curve to meet capacity, and ``snap'' the resulting cut to a persistent density valley detected across bandwidths[cite: 7]. The procedure is label-free, supports multi-queue routing, and operates in real time with sliding windows or exponential forgetting[cite: 8]. On synthetic, drifting, multimodal streams, the method achieves competitive capacity adherence while reducing threshold jitter[cite: 9]. Updates cost $O(G)$ per event with constant memory per activity

</details>


### [44] [GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling](https://arxiv.org/abs/2601.14476)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.LG

TL;DR: 研究发现设备变异性不仅能降低还能提高基于概率位的算法性能，特别是通过利用时间变异性。文章介绍了一个GPU加速、开源的基于概率位的模拟退火框架，该框架能够模拟关键的设备变异性因素，并在MAX-CUT基准测试中实现了相较于CPU实现两倍数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 探讨了使用新兴设备如磁隧道结(MTJs)实现概率位时引入的设备变异性对计算性能的影响，并揭示了设备变异性对于算法性能具有意想不到的正面影响。

Method: 开发了一个基于GPU加速和开源的模拟退火框架，该框架建模了包括时间、强度和偏移在内的关键设备变异性因素，以反映真实设备行为。使用CUDA进行仿真。

Result: 在处理规模从800到20,000节点的问题时，相对于CPU实现，本方法在MAX-CUT基准测试上实现了两个数量级的速度提升。

Conclusion: 通过提供一个可扩展且易于访问的工具，此框架旨在推进概率计算领域的研究，支持不同领域中的优化应用。

Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -timing, intensity, and offset- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.

</details>


### [45] [Stabilizing autoregressive forecasts in chaotic systems via multi-rate latent recurrence](https://arxiv.org/abs/2601.14487)
*Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: 本文提出了一种名为MSR-HINE的分层隐式预测器，通过结合多尺度潜在先验与不同时间尺度上的多速率循环模块，有效提高了混沌动力系统长期自回归预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于快速误差放大和分布偏移问题，混沌动力系统中的长时域自回归预测仍然具有挑战性。小的一步误差会在物理上不一致的展开中累积，并导致大规模统计特性的崩溃。

Method: 研究者们提出了MSR-HINE，这是一种将多尺度潜在先验与在不同时间尺度上运行的多速率循环模块相结合的分层隐式预测模型。该架构能够维持慢流形上的长期上下文同时保持快速度变异性，从而减轻了混沌展开过程中错误累积的问题。

Result: 在两个经典基准测试中，相较于U-Net自回归基线模型，MSR-HINE表现出了显著的优势：对于Kuramoto-Sivashinsky方程，在H=400时减少了62.8%的端点RMSE，并且提高了+0.983的端点ACC值；对于Lorenz-96系统，在H=100时减少了27.0%的RMSE，并提升了+0.402的端点ACC值。

Conclusion: 实验结果表明，MSR-HINE能够有效地减少长时域预测中的误差累积，并改善了混沌系统的可预测性。

Abstract: Long-horizon autoregressive forecasting of chaotic dynamical systems remains challenging due to rapid error amplification and distribution shift: small one-step inaccuracies compound into physically inconsistent rollouts and collapse of large-scale statistics. We introduce MSR-HINE, a hierarchical implicit forecaster that augments multiscale latent priors with multi-rate recurrent modules operating at distinct temporal scales. At each step, coarse-to-fine recurrent states generate latent priors, an implicit one-step predictor refines the state with multiscale latent injections, and a gated fusion with posterior latents enforces scale-consistent updates; a lightweight hidden-state correction further aligns recurrent memories with fused latents. The resulting architecture maintains long-term context on slow manifolds while preserving fast-scale variability, mitigating error accumulation in chaotic rollouts. Across two canonical benchmarks, MSR-HINE yields substantial gains over a U-Net autoregressive baseline: on Kuramoto-Sivashinsky it reduces end-horizon RMSE by 62.8% at H=400 and improves end-horizon ACC by +0.983 (from -0.155 to 0.828), extending the ACC >= 0.5 predictability horizon from 241 to 400 steps; on Lorenz-96 it reduces RMSE by 27.0% at H=100 and improves end horizon ACC by +0.402 (from 0.144 to 0.545), extending the ACC >= 0.5 horizon from 58 to 100 steps.

</details>


### [46] [On the Runway Cascade of Transformers for Language Modeling](https://arxiv.org/abs/2601.14522)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制重连方法，称为跑道感知重连，旨在解决因果变压器中由于直接路径注意和间接路径（即跑道）之间的信息传播方式不一致导致的问题。实验表明，该方法在通用语言建模以及信息检索和外推能力方面优于标准变压器。


<details>
  <summary>Details</summary>
Motivation: 文章动机在于解决近期研究中观察到的因果变压器的一些失效模式，这些问题可能因直接路径注意力与通过中间令牌形成的间接路径这两种信息传播方式之间存在错位而加剧。

Method: 提出了跑道感知重连作为解决方案，它通过将跑道上下文更明确地纳入每个令牌的直接路径注意力中来工作。这种方法基于每个令牌的跑道景观总览重新连接注意力模式，使得能够意识到累积的表现影响，并允许更平衡的信息传播。

Result: 实证结果表明，采用重连后的变压器在一般语言建模方面取得了稳定改进，并且相较于标准变压器，在信息检索和外推能力上表现出明显更强的能力。

Conclusion: 通过引入跑道感知重连机制，可以在不增加额外参数的情况下改善变压器模型的性能，特别是在处理需要精确信息传播的任务时。

Abstract: In decoder-only (causal) transformers, the computation graph created by causal masking routes information through both direct-path attention and indirect paths formed by intermediate tokens. We denote these indirect paths between token pairs as their runways. We argue that certain failure modes of causal transformers as observed by a growing body of recent works are likely exacerbated by a misalignment between these two information propagation modes. We formalize runway cascade as a phenomenon whereby this misalignment results in redundancies and irrelevant information cascading to token representations despite adequately learned attention patterns. As a solution, we propose runway-aware rewiring as a more explicit way of incorporating runway context directly into each token's direct-path attention. This mechanism re-wires the attention pattern for each token based on a summary of its runway landscape, enabling awareness of accumulating representational influences and allowing for more balanced information propagation. Our proposed methodology introduces no additional parameters and can seamlessly be integrated into standard attention mechanism. Empirically, our rewired transformer results in steady improvements in general language modeling as well as noticeably stronger information retrieval and extrapolation abilities compared to standard transformers.

</details>


### [47] [Search over Self-Edit Strategies for LLM Adaptation](https://arxiv.org/abs/2601.14532)
*Alistair Cheong,Haolin Cong,Tyler Yang,Dustin Miao*

Main category: cs.LG

TL;DR: 本研究探讨了大型语言模型（LLM）是否能够利用任务反馈来决定如何更新其权重，以实现自我改进。通过使用SEAL框架，并允许模型生成自己的自我编辑模板，研究测试了两种变体在SQuAD数据集上的表现。结果显示，带存档的变体优于不带存档版本且接近但未超过人类设计的最佳基准。此外，研究指出简单的存档机制虽然可以提供短期稳健性，但也可能导致同质化加速，暗示为了持续超越精心优化的人类策略可能需要明确的新颖性压力。


<details>
  <summary>Details</summary>
Motivation: 当前许多基于LLM的开放式搜索系统固定了提出改进建议的基础模型，这可能限制了长期进步。尽管有研究尝试在测试时更新提议模型，但更新策略通常仍由人工指定。因此，本研究旨在探索LLM能否根据任务反馈自行决定权重更新方式，以促进自我改进。

Method: 本研究聚焦于单轮自我改进的情况，并将更新操作限定为自监督的下一个词预测（NTP），让模型有权选择训练数据和关键NTP超参数。基于SEAL框架进行实验，放松了固定的人员模板约束，允许模型生成自己的自我编辑模板，从而对其训练数据和超参数拥有更多控制权。研究中比较了两种变体的表现：一种是不依赖历史模板存档的变体；另一种则是在轻量级历史模板存档基础上生成模板。

Result: 在SEAL的单段知识整合设置下，采用Qwen3-8B模型对SQuAD数据集进行了测试。结果表明，没有存档支持的变体与较弱的'Implications'基线性能相当；而使用存档的变体不仅超过了'Implications'基线，还接近但未能超越最强的人类设计'Rewrite'基线。进一步分析发现，简单的存档虽能带来一定的短期稳定性，却也可能加快模型探索过程中的同质化进程。

Conclusion: 研究表明，通过给予模型更多控制权以选择训练数据和调整超参数，可以提高其在特定任务上的表现。然而，为了持续超越精心设计的人类策略，可能还需要引入更加积极的新颖性激励机制。

Abstract: Many LLM-based open-ended search systems freeze the foundation model that proposes improvements to existing solutions, which may bottleneck long-run progress. Recent work has explored updating the proposal model at test time [arXiv:2511.23473], but the update strategy is still typically hand-specified. Therefore, this study investigated whether an LLM can use task feedback to decide how it should update its weights. For tractability, we focused on the simpler case where there is only one round of self-improvement, and restricted the update operator to self-supervised next token prediction (NTP), leaving the model freedom in choosing its training data and key NTP hyperparameters. Using the Self-Adapting Language Models (SEAL) [arXiv:2506.10943] framework as a testbed, we relaxed its fixed human template constraint and allowed the model to generate its own self-edit templates, thereby giving it more control over its training data and hyperparameters. Two variants were studied, differing in whether template generation was conditioned on a lightweight archive of past templates. In SEAL's Single-Passage Knowledge Incorporation setting with Qwen3-8B on SQuAD [arXiv:1606.05250], the no-archive variant performed comparably to the weaker "Implications" baseline, while the archive variant outperformed "Implications" and approached the strongest human-designed "Rewrite" baseline without surpassing it. Further analysis of collapse in the model's exploration revealed that a naive archive can confer some short-term robustness but can also accelerate homogenization, suggesting that explicit novelty pressure may be required to consistently advance beyond carefully optimized human strategies. Our code is available at https://github.com/cheongalc/search-self-edit-strategies .

</details>


### [48] [QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design](https://arxiv.org/abs/2601.14549)
*Nilesh Prasad Pandey,Jangseon Park,Onat Gungor,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: 本文提出了一种无需重新训练的量化方法QMC，结合了异常值感知量化和异构内存架构设计，旨在解决小型语言模型在边缘设备部署时面临的内存、延迟和能耗限制问题。通过将常规权重存储于高密度的阻变存储器（ReRAM）中，并将关键异常值保存在高精度的磁阻存储器（MRAM）上，QMC能够有效减少噪声引起的性能下降。实验显示，与当前最先进的量化方法相比，QMC不仅在压缩率上有所提升，在实际部署场景下也显著降低了内存使用量、外部数据传输次数、能耗及延迟。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLMs）在边缘平台上的部署对于实现实时且注重隐私保护的生成式人工智能至关重要，但受限于内存大小、延迟以及能源消耗等条件。现有的量化技术虽然可以减小模型尺寸并降低成本，但在新兴非易失性存储器中会受到设备噪音的影响；而传统的内存层次结构也无法高效地满足需求。因此，需要开发一种针对大型语言模型推理优化的混合内存组织方案。

Method: 提出了名为Outlier-aware Quantization with Memory Co-design (QMC)的新方法，该方法结合了无需重新训练的量化技术和新颖的异构内存架构设计。具体来说，它能够识别SLMs中的常规权重和异常值权重，将前者存放在紧凑型多级阻变存储器(ReRAM)中，后者则以高精度形式保存在片上磁阻存储器(MRAM)里，以此来缓解因噪声导致的质量下降问题。

Result: 在语言建模与推理基准测试中，QMC的表现优于或可媲美采用先进算法和混合数据格式的最先进量化方法，并且在仅考虑算法效果或是模拟真实部署条件下都能实现更高的压缩比。特别地，在最新的边缘AI平台上对比其他领先量化方法时，QMC相较于FP16格式，在减少内存占用(6.3x-7.3x)、降低外部数据传输频率(7.6x)、节省能量消耗(11.7x)及缩短延迟时间(12.5x)方面均有显著优势。

Conclusion: 研究证明了QMC作为一种可扩展且适用于实际部署环境的协同设计方案的有效性，为提高边缘设备上小型语言模型推理效率提供了一个新的途径。

Abstract: Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional memory hierarchies further limit efficiency. SRAM provides fast access but has low density, DRAM must simultaneously accommodate static weights and dynamic KV caches, which creates bandwidth contention, and Flash, although dense, is primarily used for initialization and remains inactive during inference. These limitations highlight the need for hybrid memory organizations tailored to LLM inference. We propose Outlier-aware Quantization with Memory Co-design (QMC), a retraining-free quantization with a novel heterogeneous memory architecture. QMC identifies inlier and outlier weights in SLMs, storing inlier weights in compact multi-level Resistive-RAM (ReRAM) while preserving critical outliers in high-precision on-chip Magnetoresistive-RAM (MRAM), mitigating noise-induced degradation. On language modeling and reasoning benchmarks, QMC outperforms and matches state-of-the-art quantization methods using advanced algorithms and hybrid data formats, while achieving greater compression under both algorithm-only evaluation and realistic deployment settings. Specifically, compared against SoTA quantization methods on the latest edge AI platform, QMC reduces memory usage by 6.3x-7.3x, external data transfers by 7.6x, energy by 11.7x, and latency by 12.5x when compared to FP16, establishing QMC as a scalable, deployment-ready co-design for efficient on-device inference.

</details>


### [49] [Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation](https://arxiv.org/abs/2601.14590)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 本研究评估了大型语言模型（包括GPT-4、BioMistral-7B和LLaMA-3.1-8B）在生成反事实解释(CFEs)上的表现，特别是在AI-READI临床数据集上。结果表明，微调后的LLMs，特别是LLaMA-3.1-8B，在产生高质量干预措施、特征多样性和增强效果方面表现出色。这些CFEs不仅提高了分类器性能，还提供了比基于优化方法如DiCE、CFNOW和NICE更灵活且模型无关的方法，为数字健康中的可解释性干预设计与高效模型训练带来了希望。


<details>
  <summary>Details</summary>
Motivation: 反事实解释(CFEs)通过识别改变机器学习模型预测所需的最小、可操作变化来提供以人为中心的可解释性。它们可以用于异常预防干预以及作为增强数据以训练鲁棒模型。因此，探索使用大型语言模型(LLMs)生成CFEs的能力及其在改善模型训练和提高模型鲁棒性方面的潜力成为研究动机。

Method: 本研究采用了多种大型语言模型（包括GPT-4零样本和少样本配置，以及两个开源模型BioMistral-7B和LLaMA-3.1-8B），并在预训练和微调两种设置下进行了综合评估。利用多模态AI-READI临床数据集，从干预质量、特征多样性及增强有效性三个维度对CFs进行评估。此外，还将LLM生成的CFEs与基于优化的方法如DiCE、CFNOW和NICE进行了比较。

Result: 微调后的LLMs，尤其是LLaMA-3.1-8B，能够生成具有高度可信度（高达99%）、强有效性和现实行为可调整特征变化的CFEs。当在控制标签稀缺的情况下用于数据增强时，LLM生成的CFEs显著恢复了分类器性能，在三种稀缺情况下平均F1得分恢复20%。相较于传统优化方法，LLMs提供了一种更为灵活且模型无关的方法，能够生成更具临床行动力和语义连贯性的反事实解释。

Conclusion: 这项工作展示了LLM驱动的反事实解释在设计可解释性干预措施以及在传感器为基础的数字健康领域中实现高效模型训练方面的潜力。特别地，SenseCF通过对LLM进行微调来生成有效的、代表性的反事实解释，并补充不平衡数据集中的少数类，从而改进模型训练并提升模型的鲁棒性和预测性能。

Abstract: Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.
  Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance

</details>


### [50] [Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective](https://arxiv.org/abs/2601.14599)
*Xiao Hu,Hong Xie,Tao Tan,Defu Lian,Jianyu Han*

Main category: cs.LG

TL;DR: 本文通过提出一个自下而上的实验流程来研究强化微调LLMs过程中各优化选择的作用及瓶颈问题，实验结果提供了新的理解并为领域发展提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 在针对LLM的强化微调优化中存在着许多启发式方法，但不一致的说法使得该领域难以捉摸。因此，本文旨在澄清两个基本问题：每个优化选项的作用是什么？哪些是瓶颈？

Method: 文章提出了一种自底向上的实验流程，底层是一个极简配置，包括单一训练数据、每轮一次展开以及直接作为学习信号的奖励（无需优势函数设计）。这个极简配置与具有极大离散动作空间的多臂老虎机学习相联系，并提供理论支持实验发现。随后逐层扩展极简配置，考察每一设计选择的角色。

Result: 通过对三个LLM和两个推理数据集进行实验，不仅揭示了对设计选择的新理解，也得出了塑造该领域的关键洞见。

Conclusion: 通过系统地分析不同优化选项的影响，本研究为理解和改进LLM的强化微调过程提供了宝贵的见解。

Abstract: A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.

</details>


### [51] [Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum](https://arxiv.org/abs/2601.14603)
*Jingru Li,Yibo Fan,Huan Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的优化方法Muon及其两个变体Muon-NSR和Muon-VS，通过正交动量更新加速大型语言模型的预训练过程。实验表明，这些方法比现有的AdamW和Muon基准更快地达到更低的验证损失。


<details>
  <summary>Details</summary>
Motivation: 鉴于Adam可以被视为一种方差自适应符号更新算法的新视角，研究者们开发了两种基于Muon的改进版本，旨在通过引入方差自适应归一化来进一步提高大型语言模型预训练的效率。

Method: 提出了两种新的优化方法：Muon-NSR和Muon-VS。前者采用噪声信号比调制，后者则进行基于方差的比例缩放而不增加额外的超参数。

Result: 在GPT-2和LLaMA预训练上的实验显示，所提出的方法能够加速收敛，并且相比经过良好调整的AdamW和Muon基线而言，始终可以获得更低的验证损失。例如，在LLaMA-1.2B模型上，与最近基准测试中表现良好的Muon相比，Muon-NSR和Muon-VS所需迭代次数减少了1.36倍以达到目标验证损失。

Conclusion: 新提出的优化器变体Muon-NSR和Muon-VS在加速大型语言模型预训练方面表现出色，不仅提高了收敛速度，还降低了最终验证损失。

Abstract: Large Language Models (LLMs) achieve competitive performance across diverse natural language processing (NLP) tasks, yet pretraining is computationally demanding, making optimizer efficiency an important practical consideration. Muon accelerates LLM pretraining via orthogonal momentum updates that serve as a matrix analogue of the element-wise sign operator. Motivated by the recent perspective that Adam is a variance-adaptive sign update algorithm, we propose two variants of Muon, Muon-NSR and Muon-VS, which apply variance-adaptive normalization to momentum before orthogonalization. Muon-NSR applies noise-to-signal ratio (NSR) modulation, while Muon-VS performs variance-based scaling without introducing additional hyperparameters. Experiments on GPT-2 and LLaMA pretraining demonstrate that our proposed methods accelerate convergence and consistently achieve lower validation loss than both competitive, well-tuned AdamW and Muon baselines. For example, on the LLaMA-1.2B model, Muon-NSR and Muon-VS reduce the iterations required to reach the target validation loss by $1.36\times$ relative to the well-tuned Muon following the recent benchmark.

</details>


### [52] [Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport](https://arxiv.org/abs/2601.14653)
*Yuyu Liu,Jiannan Yang,Ziyang Yu,Weishen Pan,Fei Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: 本研究提出了一种基于最优传输的插补算法CROT，旨在解决单细胞测序数据集中大块缺失数据的问题。该方法不仅提高了插补准确性，还显著减少了运行时间，适用于大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序数据中的缺失值给生物学意义的提取带来了重大挑战，而现有的插补方法难以有效处理大面积的数据缺失情况。

Method: 研究者们开发了CROT算法，利用最优传输理论来填补表格格式下的块状缺失数据，能够更好地捕捉高缺失率情况下的底层数据结构。

Result: CROT在保证较高插补精度的同时大幅度降低了计算耗时，证明了其在处理大规模、异质性及高维数据集时的有效性和高效性。

Conclusion: 这项工作为具有结构性缺失的数据提供了强大的插补解决方案，解决了生物和临床数据分析中的关键问题。

Abstract: Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we present CROT, an optimal transport-based imputation algorithm designed to handle patch-based missing data in tabular formats. Our approach effectively captures the underlying data structure in the presence of significant missingness. Notably, it achieves superior imputation accuracy while significantly reducing runtime, demonstrating its scalability and efficiency for large-scale datasets. This work introduces a robust solution for imputation in heterogeneous, high-dimensional datasets with structured data absence, addressing critical challenges in both biological and clinical data analysis. Our code is available at Anomalous Github.

</details>


### [53] [CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation](https://arxiv.org/abs/2601.14695)
*Yutong Chen,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 提出了一种名为CoScale-RL的新策略，通过收集多个解决方案和利用模型合并技术Re-distillation来提高大型推理模型的数据和计算效率，从而在没有大量SFT数据集的情况下提升模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 训练大型推理模型（LRM）通常不稳定且难以预测，尤其是在处理难题或基于弱基础模型时。尽管如此，当前的后训练扩展策略仍然可以改善这些情况。

Method: 提出了CoScale-RL策略，该策略首先通过为每个问题收集多个解决方案而非简单地扩大数据集规模来解决问题的可解性；接着，通过增加rollout计算来稳定强化学习过程；最后，采用一种称为Re-distillation的模型合并技术，在扩展过程中保持甚至提高计算效率。

Result: 所提方法显著提高了数据与计算效率，在四个基准测试上平均准确率提高了3.76倍，并能够在不依赖广泛SFT数据集的前提下增强LRM的能力边界。

Conclusion: CoScale-RL为提高LRM的推理能力提供了一个新的扩展方向。

Abstract: Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.

</details>


### [54] [Case-Guided Sequential Assay Planning in Drug Discovery](https://arxiv.org/abs/2601.14710)
*Tianchi Chen,Jan Bima,Sean L. Wu,Otto Ritter,Bingjia Yang,Xiang Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于模型的强化学习框架——隐式贝叶斯马尔可夫决策过程（IBMDP），用于解决药物发现中在缺乏明确环境模拟器或转换数据情况下的实验序列优化问题。通过构建历史结果指导的隐式模型，结合贝叶斯信念更新和集成MCTS规划技术，该方法能够在保证决策信心的同时大幅度减少资源消耗，并且在合成环境中与最优策略保持了更高的对齐度。


<details>
  <summary>Details</summary>
Motivation: 在药物发现领域，如何有效地安排实验顺序是一个高风险的计划问题，尤其是在面对严重不确定性以及资源限制的情况下。标准强化学习方法由于缺少明确的环境模拟器或状态-动作-新状态数据而难以直接应用。本研究旨在开发一种无需依赖此类数据的新方法来改善实验设计流程。

Method: 提出了名为隐式贝叶斯马尔可夫决策过程（IBMDP）的新框架，它能够仅依靠静态的历史结果数据库来工作。该框架首先利用相似的历史案例构建一个非参数化的信念分布以表示转移动态；然后随着证据累积执行贝叶斯信念更新；最后采用集成蒙特卡洛树搜索（MCTS）规划生成既注重信息增益又考虑资源效率的稳定策略。

Result: 通过对真实世界中的中枢神经系统药物发现任务进行测试表明，相比于现有启发式方法，IBMDP最多可降低92%的资源消耗同时保持良好的决策自信水平。此外，在具有可计算最优策略的人造环境中评估时，IBMDP相比使用相同相似性基础模型但采取确定性价值迭代方法而言，显示出与最优策略更佳的一致性。

Conclusion: IBMDP为那些数据丰富但缺乏合适模拟器的问题域提供了一个实用且有效的连续实验设计方案。

Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.

</details>


### [55] [PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning](https://arxiv.org/abs/2601.14716)
*Yao Lu,Dengdong Fan,Jianzheng Nie,Fan Xu,Jie Chen,Bin Zhou,Yonghong Tian*

Main category: cs.LG

TL;DR: 介绍了PCL-Reasoner-V1.5，一个基于Qwen2.5-32B并通过监督微调和强化学习改进的320亿参数大语言模型，特别提出了离线RL方法以提高训练稳定性和效率。该模型在AIME 2024和2025上取得了领先的成绩，证明了离线RL对于推进LLM中的推理能力的有效性。


<details>
  <summary>Details</summary>
Motivation: 开发更先进的数学推理大语言模型，并通过引入一种新的离线强化学习方法来改善现有模型的训练过程稳定性与效率问题。

Method: 基于Qwen2.5-32B进行构建，并采用了监督微调(SFT)及后续的强化学习(RL)策略；特别是提出了一种离线RL方法，相比传统的在线RL技术如GRPO提供了更好的训练稳定性和效率。

Result: PCL-Reasoner-V1.5在AIME 2024和2025测试中分别达到了90.9%和85.6%的平均准确率，成为基于Qwen2.5-32B后训练模型中的佼佼者。

Conclusion: 本研究表明，采用离线强化学习是一种稳定且高效的方法，能够有效促进大型语言模型中的推理能力发展。

Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.

</details>


### [56] [RefProtoFL: Communication-Efficient Federated Learning via External-Referenced Prototype Alignment](https://arxiv.org/abs/2601.14746)
*Hongyue Wu,Hangyu Li,Guodong Fan,Haoran Zhu,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: 本文提出了一种名为RefProtoFL的通信高效联邦学习框架，该框架通过外部参考原型对齐（ERPA）和自适应概率更新丢弃（APUD）来解决异构客户端间表示不一致的问题，并减少上行成本。实验表明，RefProtoFL在标准基准测试中比现有的基于原型的联邦学习方法获得了更高的分类精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习能够在边缘环境中实现无需共享原始数据的合作模型训练，但受限于有限的通信带宽和异构的客户端数据分布。尽管基于原型的联邦学习通过交换类别特征原型而非完整模型参数减轻了这一问题，但在严重通信限制下现有方法仍存在泛化能力不足的问题。

Method: 提出的RefProtoFL框架结合了用于表示一致性维护的外部参考原型对齐（ERPA）技术以及为提高通信效率而设计的自适应概率更新丢弃（APUD）机制。具体来说，将模型分解成私有骨干部分与轻量级共享适配器，仅限于适配器参数之间的联邦通信。为了进一步降低上行开销，APUD采用基于重要性感知的Top-K稀疏化策略，只传输最重要的适配器更新到服务器端进行聚合。此外，ERPA利用少量服务器持有的公共数据集构建外部参考原型作为共享语义锚点，对于被公共数据覆盖的类别直接对齐本地表征至由公共数据诱导出的原型；而对于未覆盖的类别，则依赖加权平均得到的全局参考原型来进行对齐。

Result: 广泛的实验验证显示，相比最先进的基于原型的联邦学习基线，RefProtoFL能够达到更高的分类准确度。

Conclusion: RefProtoFL提供了一个有效的方法来改善联邦学习环境下的通信效率与模型性能，特别是针对具有挑战性的通信限制情况。

Abstract: Federated learning (FL) enables collaborative model training without sharing raw data in edge environments, but is constrained by limited communication bandwidth and heterogeneous client data distributions. Prototype-based FL mitigates this issue by exchanging class-wise feature prototypes instead of full model parameters; however, existing methods still suffer from suboptimal generalization under severe communication constraints. In this paper, we propose RefProtoFL, a communication-efficient FL framework that integrates External-Referenced Prototype Alignment (ERPA) for representation consistency with Adaptive Probabilistic Update Dropping (APUD) for communication efficiency. Specifically, we decompose the model into a private backbone and a lightweight shared adapter, and restrict federated communication to the adapter parameters only. To further reduce uplink cost, APUD performs magnitude-aware Top-K sparsification, transmitting only the most significant adapter updates for server-side aggregation. To address representation inconsistency across heterogeneous clients, ERPA leverages a small server-held public dataset to construct external reference prototypes that serve as shared semantic anchors. For classes covered by public data, clients directly align local representations to public-induced prototypes, whereas for uncovered classes, alignment relies on server-aggregated global reference prototypes via weighted averaging. Extensive experiments on standard benchmarks demonstrate that RefProtoFL attains higher classification accuracy than state-of-the-art prototype-based FL baselines.

</details>


### [57] [Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models](https://arxiv.org/abs/2601.14758)
*Injin Kong,Hyoungjoon Lee,Yohan Jo*

Main category: cs.LG

TL;DR: 本文通过对比分析自回归模型(ARMs)和其后训练的掩码扩散模型(MDMs)，揭示了从ARMs到MDMs转换过程中内部算法机制的系统性转变。这种变化依赖于任务的结构性质，对于主要涉及局部因果依赖的任务，MDMs保留了大部分自回归结构；而对于全局规划任务，MDMs则表现出显著的早期层处理增强现象。此外，研究还发现了从ARMs中的局部专业化向MDMs中分布式整合的语义转变。结论是，扩散后训练不仅仅是参数调整，而是从根本上重组了内部计算以支持非顺序性的全局规划。


<details>
  <summary>Details</summary>
Motivation: 当前，将预训练的自回归模型（ARMs）后训练为掩码扩散模型（MDMs）已成为一种成本效益高的策略，用以克服顺序生成带来的局限性。然而，这种模式转变所引发的内部算法转换尚未被探索清楚，这使得人们不清楚后训练得到的MDMs是否真正获得了双向推理能力，还是仅仅重新包装了自回归启发式方法。本研究旨在解答这一问题。

Method: 采用比较电路分析法，对自回归模型（ARMs）及其对应的掩码扩散模型（MDMs）进行了深入探讨。

Result: 研究发现了一种与任务结构特性相关的“机制转变”。在主要依靠局部因果关系的任务上，MDMs大体保持了原有的自回归电路；而在面对需要全局规划的任务时，则观察到了明显的路径放弃现象，并且伴随着早期层处理活动增加的特点。此外，还观察到了从ARMs中尖锐、局部的专业化向MDMs中更加分散化的整合方式转变的趋势。

Conclusion: 扩散后训练不仅调整了模型参数，更重要的是它从根本上重新组织了内部计算流程，使得模型能够更好地支持非顺序性的全局规划任务。

Abstract: Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative circuit analysis of ARMs and their MDM counterparts. Our analysis reveals a systematic "mechanism shift" dependent on the structural nature of the task. Structurally, we observe a distinct divergence: while MDMs largely retain autoregressive circuitry for tasks dominated by local causal dependencies, they abandon initialized pathways for global planning tasks, exhibiting distinct rewiring characterized by increased early-layer processing. Semantically, we identify a transition from sharp, localized specialization in ARMs to distributed integration in MDMs. Through these findings, we conclude that diffusion post-training does not merely adapt model parameters but fundamentally reorganizes internal computation to support non-sequential global planning.

</details>


### [58] [Anytime Optimal Decision Tree Learning with Continuous Features](https://arxiv.org/abs/2601.14765)
*Harold Kiossou,Pierre Schaus,Siegfried Nijssen*

Main category: cs.LG

TL;DR: 提出了一种新的方法，通过有限差异搜索来改进在连续特征上学习最优决策树的过程，该方法能够在任何中断点提供高质量的决策树，并且在任意时间性能方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的用于连续特征的最优决策树学习算法虽然能够找到最优解，但计算时间随深度迅速增加，导致其实用性受到限制；此外，如果提前终止，则可能得到非常不平衡和次优的决策树。

Method: 采用有限差异搜索的方法，使得计算工作更均匀地分布在整个树结构中，从而保证即使在早期中断时也能获得较高质量的决策树。

Result: 实验结果表明，新提出的方法在任意时间性能方面优于现有方法。

Conclusion: 本文介绍了一种基于有限差异搜索的新方法，它提高了在处理连续特征时学习最优决策树的效率与质量，特别是在面对需要随时中断的情况时表现更好。

Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.

</details>


### [59] [Statistical Learning Theory for Distributional Classification](https://arxiv.org/abs/2601.14818)
*Christian Fiedler*

Main category: cs.LG

TL;DR: 本文研究了在两阶段抽样设置中基于分布输入的监督学习问题，特别是使用支持向量机(SVM)进行分类的情况。通过引入核均值嵌入(KMEs)，将概率分布嵌入到希尔伯特空间，并在此基础上应用标准核方法。文章提供了新的oracle不等式，一致性结果和学习率分析。对于使用hinge损失和高斯核的SVM，还提出了一个新的噪声假设以建立学习率。此外，为高斯核开发的新特征空间具有独立的研究价值。


<details>
  <summary>Details</summary>
Motivation: 在实际应用场景如基于学习的医学筛查或因果学习中，输入数据是以概率分布形式存在的，但在学习阶段无法直接获取这些分布，只能得到它们的样本。这种情况下，利用基于核的学习方法处理这类问题非常合适。

Method: 作者采用的方法是首先利用核均值嵌入（KMEs）技术将分布或其样本映射到希尔伯特空间，然后在该嵌入空间上应用支持向量机（SVM）等标准核方法。特别地，在使用带有hinge损失函数和高斯核的支持向量机时，提出了一种新的噪声假设条件。

Result: 研究成果包括一个新的oracle不等式、一致性和学习速率的结果。对于采用hinge损失与高斯核的支持向量机，根据新提出的噪声假设得到了学习速率。同时，为高斯核定义了一个新颖的特征空间，这对其他研究也有潜在价值。

Conclusion: 本文深入探讨了当输入为概率分布且仅能访问其样本时如何有效地执行监督学习任务，特别是在使用支持向量机进行分类的情况下。通过理论分析证明了所提方法的有效性，并为未来相关研究提供了有用的工具和技术。

Abstract: In supervised learning with distributional inputs in the two-stage sampling setup, relevant to applications like learning-based medical screening or causal learning, the inputs (which are probability distributions) are not accessible in the learning phase, but only samples thereof. This problem is particularly amenable to kernel-based learning methods, where the distributions or samples are first embedded into a Hilbert space, often using kernel mean embeddings (KMEs), and then a standard kernel method like Support Vector Machines (SVMs) is applied, using a kernel defined on the embedding Hilbert space. In this work, we contribute to the theoretical analysis of this latter approach, with a particular focus on classification with distributional inputs using SVMs. We establish a new oracle inequality and derive consistency and learning rate results. Furthermore, for SVMs using the hinge loss and Gaussian kernels, we formulate a novel variant of an established noise assumption from the binary classification literature, under which we can establish learning rates. Finally, some of our technical tools like a new feature space for Gaussian kernels on Hilbert spaces are of independent interest.

</details>


### [60] [From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps](https://arxiv.org/abs/2601.14848)
*Mohamed Abouras,Catherine M. Elias*

Main category: cs.LG

TL;DR: 本文研究了高速公路出入口区域（AoI）与直线路段之间的差异，并使用ExiD无人机数据集训练多层LSTM架构模型来预测这些区域内的车辆行为。结果显示，在最多4秒的预测范围内，AoI的预测准确率从约76%开始，而普通高速公路场景下的预测准确率可达94%。


<details>
  <summary>Details</summary>
Motivation: 由于高速公路出入口区域引入了更多不确定性，从而影响道路安全，因此研究这一区域的车辆行为变得尤为重要。通过提高对这些区域内车辆行为的预测能力，可以减少不确定因素的影响并提升行车安全。

Method: 采用多层LSTM架构模型，利用ExiD无人机收集的数据集进行训练。实验中测试了不同预测时长以及不同模型的工作流程。

Result: 在最多达4秒的预测范围内，针对AoI的预测准确率从大约76%起步；而在一般高速公路情境下，最远预测范围内的准确率达到了94%。

Conclusion: 本研究表明，通过应用多层LSTM架构于特定兴趣区域（如高速公路上下口），能够有效改善短期车辆行为预测的表现，特别是在提高安全性方面展现出了巨大潜力。

Abstract: On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.

</details>


### [61] [Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference](https://arxiv.org/abs/2601.14855)
*Baojun Che,Yifan Chen,Daniel Zhengyu Huang,Xinying Mao,Weijie Wang*

Main category: cs.LG

TL;DR: 本文提出了一种结合仿射不变预处理、指数积分器和自适应时间步长的稳定高效框架，用于具有高斯混合族的黑盒变分推理，证明了该方法在无噪声情况下的指数收敛性和蒙特卡洛估计下的几乎必然收敛性，并通过数值实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准数值优化方法在使用高斯混合族进行黑盒变分推理时存在不稳定性与低效率的问题。

Method: 提出一个整合了仿射不变预处理（通过自然梯度公式）、保持协方差矩阵正定性的指数积分器以及确保稳定性的自适应时间步长的新框架。

Result: 对于高斯后验，证明了在无噪声情况下指数收敛及在蒙特卡洛估计下几乎确定收敛；并通过多模态分布、Neal的多尺度漏斗和基于偏微分方程的Darcy流贝叶斯逆问题等数值实验展示了所提方法的有效性。

Conclusion: 该方法为使用高斯混合族进行黑盒变分推理提供了一个更稳定且高效的解决方案。

Abstract: Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We develop a stable and efficient framework that combines three key components: (1) affine-invariant preconditioning via natural gradient formulations, (2) an exponential integrator that unconditionally preserves the positive definiteness of covariance matrices, and (3) adaptive time stepping to ensure stability and to accommodate distinct warm-up and convergence phases. The proposed approach has natural connections to manifold optimization and mirror descent. For Gaussian posteriors, we prove exponential convergence in the noise-free setting and almost-sure convergence under Monte Carlo estimation, rigorously justifying the necessity of adaptive time stepping. Numerical experiments on multimodal distributions, Neal's multiscale funnel, and a PDE-based Bayesian inverse problem for Darcy flow demonstrate the effectiveness of the proposed method.

</details>


### [62] [Strategic Doctrine Language Models (sdLM): A Learning-System Framework for Doctrinal Consistency and Geopolitical Forecasting](https://arxiv.org/abs/2601.14862)
*Olaf Yunus Laitinen Imanov,Taner Yilmaz,Derya Umut Kulali*

Main category: cs.LG

TL;DR: 介绍了一种名为Strategic Doctrine Language Models (sdLM)的学习系统框架，该框架通过整合多文档注意力、时间编码以及教义一致性层来改进长期预测和计划的合理性，同时减少严重的教义违背。在多项基准测试中，sdLM的表现优于强大的通用大语言模型，并且在长期判断上与人类专家具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种能够处理多文档策略推理的语言模型，同时保持教义的一致性并校准不确定性，以提高长期预测准确性及计划的可行性。

Method: 提出了一种结合了多文档注意力机制、时间编码以及教义一致性层的新方法sdLM。

Result: sdLM在战略质量与校准方面优于强大的通用大语言模型基线，并且在长时间范围内的判断上接近甚至达到人类专家水平。

Conclusion: sdLM为多文档战略推理提供了一个有效的解决方案，在保持教义一致性的同时提高了长期预测能力。

Abstract: We introduce Strategic Doctrine Language Models (sdLM), a learning-system framework for multi-document strategic reasoning with doctrinal consistency constraints and calibrated uncertainty. The approach combines multi-document attention, temporal encoding, and a doctrine-consistency layer to improve long-horizon forecasting and plan plausibility while reducing severe doctrinal violations. We evaluate sdLM using (i) expert-panel scoring of strategic scenarios (N=47), (ii) doctrine consistency on 336 doctrine publications (12,847 statements), and (iii) geopolitical forecasting on 127 historical counterfactuals (1945-2020) across 12-60 month horizons. Across these benchmarks, sdLM achieves higher strategic quality and better calibration than strong general-purpose LLM baselines, and remains competitive with human experts on long-horizon judgments. We further report ablations, scaling trends, and deployment-oriented performance/latency characteristics to clarify which components drive improvements and how they translate to operational settings.

</details>


### [63] [What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study](https://arxiv.org/abs/2601.14888)
*Keyu Lv,Manyi Zhang,Xiaobo Xia,Jingchen Ni,Shannan Yan,Xianzhi Yu,Lu Hou,Chun Yuan,Haoli Bai*

Main category: cs.LG

TL;DR: 本研究系统地探讨了推理模型的量化感知训练(QAT)，发现知识蒸馏是稳健的训练目标，后训练量化(PTQ)为QAT提供了良好的初始化，强化学习在给定合适冷启动的情况下对量化模型依然有效，并且使PTQ校准域与QAT训练域保持一致可以加速收敛并提高最终准确性。基于这些发现，提出了一个优化的工作流程（Reasoning-QAT），其表现优于现有的PTQ方法。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型擅长处理复杂任务，如编程和数学，但它们的推理过程往往较慢且效率低下。后训练量化虽可提高推理效率，但在低比特设置下通常会导致准确率大幅下降。

Method: 进行了关于推理模型量化感知训练(QAT)的系统实证研究，探索了知识蒸馏、PTQ初始化、强化学习对于量化模型的有效性以及PTQ校准域与QAT训练域一致性的影响。

Result: 发现知识蒸馏是一种稳健的目标；PTQ为QAT提供了强初始化，提高了准确性同时降低了训练成本；给定合适的冷启动条件下，强化学习仍适用于量化模型并带来额外收益；PTQ校准域与QAT训练域的一致性有助于加快收敛速度并通常改善最终准确性。

Conclusion: 提出了一种名为Reasoning-QAT的优化工作流程，该流程在多个大型语言模型骨架及推理数据集上持续优于最先进的PTQ方法。例如，在Qwen3-0.6B模型上，它在MATH-500数据集上的表现比GPTQ高出44.53%，并在2比特模式下稳定恢复性能。

Abstract: Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.

</details>


### [64] [Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models](https://arxiv.org/abs/2601.14917)
*Giorgia Rigamonti,Mirko Paolo Barbato,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的个性化血糖预测方法，通过利用患者特定数据来提高预测准确性和响应性。研究结果表明，个性化的模型能显著改善不良事件的预测，从而实现更精确和及时的干预。此外，文章还探讨了实现有效个性化所需的最少数据量，这对于数据收集困难的实际应用尤为重要。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病的有效管理需要持续的血糖监测和精确的胰岛素调整以防止高血糖和低血糖。随着可穿戴式血糖监测仪和移动健康应用程序的日益普及，准确的血糖预测对于增强自动胰岛素输送和决策支持系统至关重要。因此，开发一种能够考虑个体差异、提高预测精度的个性化血糖预测方法变得十分必要。

Method: 本研究采用了一种基于深度学习的方法来进行个性化的血糖预测，并利用患者特定的数据来优化预测模型。为了评估不同策略对患者特异性动态建模的能力，比较了留一法交叉验证与微调策略的效果。同时，通过实验对比多模态患者特定方法与传统仅CGM（连续血糖监测）方法的表现，并进行了消融研究来探索训练集大小如何影响模型性能。

Result: 结果显示，个性化的模型在预测不利事件方面明显优于通用模型，使得在现实场景中可以进行更加精准及时的干预措施。此外，研究还确定了实现有效个性化所需的数据最小量，这为实际应用中的数据收集提供了重要参考。

Conclusion: 本文的研究成果强调了适应性强、个性化的血糖预测模型在下一代糖尿病管理中的潜力，特别是在可穿戴设备和移动健康平台上的应用，这有助于提升面向消费者的糖尿病护理解决方案。

Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.

</details>


### [65] [Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning](https://arxiv.org/abs/2601.14942)
*Hang Zhao,Hongru Li,Dongfang Xu,Shenghui Song,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出了一种三阶段通信感知分布式学习框架，旨在提高多模态边缘推理的训练和推理效率，同时保持无线信道上的鲁棒性。该框架通过本地自监督学习、分布式微调与集中证据融合以及不确定性引导的反馈机制来减少通信开销、校准每种模态的不确定性，并优化了分布式设置中的通信-准确性权衡。实验表明，所提框架在RGB-深度室内场景分类任务上以更少的训练通信轮次达到更高的准确率，并且对模态退化或信道变化具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于语义通信能够传达与任务相关的意义，因此成为分布式边缘智能的关键推动者。然而，在无线链路上实现通信高效的训练和鲁棒推理仍然具有挑战性。对于多模态边缘推理（MMEI），这一挑战因以下两个因素而加剧：1）由于系统的多模态性质，在带宽受限的无线链路上进行分布式学习时会产生巨大的通信开销；2）在变化的信道条件和噪声多模态输入下系统鲁棒性的局限。

Method: 本文提出了一个三阶段通信感知分布式学习框架，以提升无线信道上训练和推理的效率及鲁棒性。第一阶段，设备执行局部多模态自监督学习，以获取共享编码器和特定模态编码器，无需设备-服务器间的数据交换，从而减少了通信成本。第二阶段，通过集中式证据融合进行分布式微调，校准每种模态下的不确定性，并可靠地聚合由噪声或信道衰减导致失真的特征。第三阶段，利用不确定性引导的反馈机制选择性地请求额外特征用于不确定样本，优化了分布式环境下的通信-准确性平衡。

Result: 实验结果表明，所提出的框架在RGB-深度室内场景分类任务中，以远少于传统方法的训练通信轮数达到了更高的准确度，并且对模态质量下降或信道状况变化表现出更好的鲁棒性，优于现有的自监督学习和全监督基线方法。

Conclusion: 综上所述，本研究提出的三阶段通信感知分布式学习框架有效提升了多模态边缘推理应用中的性能，特别是在通信效率和鲁棒性方面展现出了显著优势。

Abstract: Semantic communication is emerging as a key enabler for distributed edge intelligence due to its capability to convey task-relevant meaning. However, achieving communication-efficient training and robust inference over wireless links remains challenging. This challenge is further exacerbated for multi-modal edge inference (MMEI) by two factors: 1) prohibitive communication overhead for distributed learning over bandwidth-limited wireless links, due to the \emph{multi-modal} nature of the system; and 2) limited robustness under varying channels and noisy multi-modal inputs. In this paper, we propose a three-stage communication-aware distributed learning framework to improve training and inference efficiency while maintaining robustness over wireless channels. In Stage~I, devices perform local multi-modal self-supervised learning to obtain shared and modality-specific encoders without device--server exchange, thereby reducing the communication cost. In Stage~II, distributed fine-tuning with centralized evidential fusion calibrates per-modality uncertainty and reliably aggregates features distorted by noise or channel fading. In Stage~III, an uncertainty-guided feedback mechanism selectively requests additional features for uncertain samples, optimizing the communication--accuracy tradeoff in the distributed setting. Experiments on RGB--depth indoor scene classification show that the proposed framework attains higher accuracy with far fewer training communication rounds and remains robust to modality degradation or channel variation, outperforming existing self-supervised and fully supervised baselines.

</details>


### [66] [Multimodal Rumor Detection Enhanced by External Evidence and Forgery Features](https://arxiv.org/abs/2601.14954)
*Han Li,Hua Sun*

Main category: cs.LG

TL;DR: 提出了一种结合外部证据和伪造特征的多模态谣言检测模型，通过改进特征提取、对齐及融合策略，在微博和Twitter数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中图文混合的信息传播容易被谣言利用，特别是深层语义不匹配的谣言难以通过单纯的内容分析来识别。现有的多模态谣言检测方法在跨模态建模方面有所进步，但在特征提取、噪声对齐、灵活融合策略以及使用外部事实验证复杂谣言方面存在不足。

Method: 本研究提出了一个增强型多模态谣言检测模型，该模型整合了外部证据与伪造特征。具体来说，采用了ResNet34作为视觉编码器、BERT作为文本编码器，并通过傅里叶变换提取频率域痕迹和压缩伪影来构成伪造特征模块。此外，利用BLIP生成图像描述以连接图像与文本间的语义空间；双对比学习模块则用于计算文本-图像对和文本-描述对之间的对比损失，以此提高检测语义不一致性的能力。最后，引入了一个门控自适应特征缩放融合机制来动态调整多模态融合并减少冗余。

Result: 实验结果表明，在微博和Twitter的数据集上，所提出的模型在宏观准确率、召回率以及F1分数等指标上均超过了主流基线方法。

Conclusion: 本研究开发的新模型能够更有效地处理复杂的多模态谣言检测问题，特别是在应对深层语义不匹配谣言时展现出优越性能，为提升社交媒体平台上的信息真实性提供了有力支持。

Abstract: Social media increasingly disseminates information through mixed image text posts, but rumors often exploit subtle inconsistencies and forged content, making detection based solely on post content difficult. Deep semantic mismatch rumors, which superficially align images and texts, pose particular challenges and threaten online public opinion. Existing multimodal rumor detection methods improve cross modal modeling but suffer from limited feature extraction, noisy alignment, and inflexible fusion strategies, while ignoring external factual evidence necessary for verifying complex rumors. To address these limitations, we propose a multimodal rumor detection model enhanced with external evidence and forgery features. The model uses a ResNet34 visual encoder, a BERT text encoder, and a forgery feature module extracting frequency-domain traces and compression artifacts via Fourier transformation. BLIP-generated image descriptions bridge image and text semantic spaces. A dual contrastive learning module computes contrastive losses between text image and text description pairs, improving detection of semantic inconsistencies. A gated adaptive feature-scaling fusion mechanism dynamically adjusts multimodal fusion and reduces redundancy. Experiments on Weibo and Twitter datasets demonstrate that our model outperforms mainstream baselines in macro accuracy, recall, and F1 score.

</details>


### [67] [InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement](https://arxiv.org/abs/2601.14968)
*Mingyue Cheng,Xiaoyu Tao,Huajian Zhang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 提出了一种新的时间序列分类框架InstructTime，该框架将分类任务重新定义为一个多模态生成任务，并引入了时间序列离散化模块、对齐投影层和生成式自监督预训练策略来增强跨模态表示对齐。基于此，还提出了InstructTime++，进一步结合隐式特征建模以提高模型性能。实验表明InstructTime++在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列分类方法大多采用判别范式直接将输入序列映射到独热编码的类别标签，这限制了上下文特征的融入及类间语义关系的捕捉。

Method: 开发了一个名为InstructTime的新框架，它通过处理连续数值序列、上下文文本特征以及任务指令作为多模态输入，并利用调优后的语言模型生成文本形式的类别标签。为了克服模态间的差异，InstructTime设计了一个时间序列离散化组件用来转化连续序列为离散的时间标记，同时配合有对齐投射层与生成式的自监督预训练策略用以促进跨模态表征的一致性。此外，还发展了InstructTime++版本，额外整合了隐含特征建模技术，使用专门工具包从原始时间序列和上下文输入中挖掘有用的隐含模式（如统计特征提取、基于视觉-语言的图像描述）并将其转化为文本描述以便于融合。

Result: 广泛的实验证明，在多个基准数据集上，InstructTime++表现出色。

Conclusion: InstructTime及其扩展版本InstructTime++为时间序列分类提供了一种创新的方法，不仅能够有效结合上下文信息与类间语义关联，而且通过引入隐式特征建模进一步提升了模型性能。

Abstract: Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.

</details>


### [68] [Lineup Regularized Adjusted Plus-Minus (L-RAPM): Basketball Lineup Ratings with Informed Priors](https://arxiv.org/abs/2601.15000)
*Christos Petridis,Konstantinos Pelechrinis*

Main category: cs.LG

TL;DR: 提出了一种基于回归的方法L-RAPM，用于评估篮球阵容的表现，该方法考虑了对阵情况以及球员信息，实验表明此方法比现有基线方法具有更好的预测能力，尤其是在阵容样本量较小时。


<details>
  <summary>Details</summary>
Motivation: 在体育分析中，识别出表现良好的球员组合（即阵容）是一项重要任务。然而，由于比赛中频繁的换人导致数据非常稀疏，这给统计和预测带来了挑战。例如，一个NBA球队在一个赛季中会使用超过600个不同的阵容，这意味着每个阵容平均只会在25-30次进攻机会中出现。因此，收集到的关于这些阵容的数据往往噪音很大，并且预测价值低。

Method: 本文介绍了一种新的基于回归的方法，称为L-RAPM，它不仅控制了每个阵容面对的对手情况，还利用了组成阵容的球员的信息来提高预测准确性。

Result: 实验结果显示，与当前使用的基线方法相比，L-RAPM提供了更强的预测力，而且当阵容样本量变得更小的时候，这种改进更加明显。

Conclusion: 通过引入L-RAPM模型，研究解决了因频繁换人而导致的数据稀疏性问题，从而提高了对篮球阵容表现预测的准确度。

Abstract: Identifying combinations of players (that is, lineups) in basketball - and other sports - that perform well when they play together is one of the most important tasks in sports analytics. One of the main challenges associated with this task is the frequent substitutions that occur during a game, which results in highly sparse data. In particular, a National Basketball Association (NBA) team will use more than 600 lineups during a season, which translates to an average lineup having seen the court in approximately 25-30 possessions. Inevitably, any statistics that one collects for these lineups are going to be noisy, with low predictive value. Yet, there is no existing work (in the public at least) that addresses this problem. In this work, we propose a regression-based approach that controls for the opposition faced by each lineup, while it also utilizes information about the players making up the lineups. Our experiments show that L-RAPM provides improved predictive power than the currently used baseline, and this improvement increases as the sample size for the lineups gets smaller.

</details>


### [69] [Mixture-of-Experts Models in Vision: Routing, Optimization, and Generalization](https://arxiv.org/abs/2601.15021)
*Adam Rokah,Daniel Veress,Caleb Caulk,Sourav Sharan*

Main category: cs.LG

TL;DR: 该研究探讨了Mixture-of-Experts (MoE)架构在图像分类设置中的表现，比较了密集型、SoftMoE和SparseMoE分类器头在CIFAR10数据集上的性能。尽管所有模型的泛化性能相当，但发现SoftMoE在基于Hessian的锐度指标上表现出更高的锐度。此外，实际实现中条件路由并未在现代硬件上带来推理速度的提升。


<details>
  <summary>Details</summary>
Motivation: 这项工作的动机是研究Mixture-of-Experts（MoE）架构在图像分类场景下的行为，特别是预测性能、专家利用率和泛化能力。通常，MoE被作为扩大大型语言模型的一种机制来考虑，但本研究转向了另一个应用领域。

Method: 研究者们使用了CIFAR10数据集，并且在类似模型容量下对比了密集型、SoftMoE以及SparseMoE分类器的表现。为了分析泛化能力，计算了收敛时基于Hessian矩阵的最大特征值与Hessian矩阵迹等锐度指标。同时，还进行了损失曲面扰动分析以揭示不同模型间非局部行为的质量差异。

Result: 结果表明，两种MoE变体相较于密集型基线稍微提高了验证准确率，且通过正则化保持了专家使用的平衡。虽然所有模型都达到了相似的泛化性能，但根据所选的度量标准，SoftMoE显示出更高的锐度。另外，初步实现在这种规模上没有显示出条件路由带来的推理效率提高。

Conclusion: 研究结论指出，在给定条件下，MoE架构可以在图像分类任务中提供轻微的准确性改进，同时维持良好的专家利用情况。然而，对于稀疏MoE模型而言，理论上预期的推理效率增益并未在实践中观察到。

Abstract: Mixture-of-Experts (MoE) architectures enable conditional computation by routing inputs to multiple expert subnetworks and are often motivated as a mechanism for scaling large language models. In this project, we instead study MoE behavior in an image classification setting, focusing on predictive performance, expert utilization, and generalization. We compare dense, SoftMoE, and SparseMoE classifier heads on the CIFAR10 dataset under comparable model capacity. Both MoE variants achieve slightly higher validation accuracy than the dense baseline while maintaining balanced expert utilization through regularization, avoiding expert collapse. To analyze generalization, we compute Hessian-based sharpness metrics at convergence, including the largest eigenvalue and trace of the loss Hessian, evaluated on both training and test data. We find that SoftMoE exhibits higher sharpness by these metrics, while Dense and SparseMoE lie in a similar curvature regime, despite all models achieving comparable generalization performance. Complementary loss surface perturbation analyses reveal qualitative differences in non-local behavior under finite parameter perturbations between dense and MoE models, which help contextualize curvature-based measurements without directly explaining validation accuracy. We further evaluate empirical inference efficiency and show that naively implemented conditional routing does not yield inference speedups on modern hardware at this scale, highlighting the gap between theoretical and realized efficiency in sparse MoE models.

</details>


### [70] [Field-Space Autoencoder for Scalable Climate Emulators](https://arxiv.org/abs/2601.15102)
*Johannes Meuer,Maximilian Witte,Étiénne Plésiat,Thomas Ludwig,Christopher Kadow*

Main category: cs.LG

TL;DR: 本文介绍了一种基于球面压缩模型的可扩展气候模拟框架——Field-Space Autoencoder，能够有效处理公里尺度地球系统模型产生的PB级输出数据，同时避免了将球面数据强制转换到欧几里得网格上导致的几何失真。通过训练生成扩散模型，该方法可以从大量低分辨率数据中学习内部变异性，并从稀疏高分辨率数据中学习精细物理细节。


<details>
  <summary>Details</summary>
Motivation: 公里尺度的地球系统模型对于捕捉局部气候变化至关重要，但它们计算成本高昂且产生PB级规模的输出数据，这限制了其在如概率风险评估等应用中的实用性。因此，需要一种既能保持物理结构又可以有效减少数据量的方法来解决这个问题。

Method: 提出了Field-Space Autoencoder，这是一种基于球面压缩模型的气候模拟框架。该模型采用Field-Space Attention技术直接处理原始气候模型输出，避免了几何失真问题。此外，还利用生成扩散模型对压缩后的场进行进一步处理，使得模型能够同时从丰富的低分辨率数据中学到内在变异性和从稀缺的高分辨率数据中理解细微物理现象。

Result: 与传统的卷积基线相比，这种方法在保持物理结构方面表现更好。它不仅能作为下游生成性模拟的良好基础，还能执行零样本超分辨率任务，将低分辨率大集合和少量高分辨率数据映射到一个共同表示空间内。

Conclusion: 本研究通过提出的新方法有效地解决了当前地球系统模型面临的挑战，即如何高效地处理大规模数据集并准确地模拟气候变化。这项工作为连接大量的低分辨率集成统计信息与稀缺的高分辨率物理细节之间搭建了桥梁。

Abstract: Kilometer-scale Earth system models are essential for capturing local climate change. However, these models are computationally expensive and produce petabyte-scale outputs, which limits their utility for applications such as probabilistic risk assessment. Here, we present the Field-Space Autoencoder, a scalable climate emulation framework based on a spherical compression model that overcomes these challenges. By utilizing Field-Space Attention, the model efficiently operates on native climate model output and therefore avoids geometric distortions caused by forcing spherical data onto Euclidean grids. This approach preserves physical structures significantly better than convolutional baselines. By producing a structured compressed field, it serves as a good baseline for downstream generative emulation. In addition, the model can perform zero-shot super-resolution that maps low-resolution large ensembles and scarce high-resolution data into a shared representation. We train a generative diffusion model on these compressed fields. The model can simultaneously learn internal variability from abundant low-resolution data and fine-scale physics from sparse high-resolution data. Our work bridges the gap between the high volume of low-resolution ensemble statistics and the scarcity of high-resolution physical detail.

</details>


### [71] [Auditing Language Model Unlearning via Information Decomposition](https://arxiv.org/abs/2601.15111)
*Anmol Goel,Alan Ritter,Iryna Gurevych*

Main category: cs.LG

TL;DR: 本文揭示了当前语言模型中机器遗忘方法的一个关键局限性：即使遗忘算法看似成功，被遗忘数据的信息仍然可以从内部表示中线性解码。通过引入基于部分信息分解（PID）的可解释、信息理论框架来审计遗忘过程，并据此提出了一种基于表示的风险评分机制，以指导在推理时对敏感输入的回避，从而提供一种实用机制来减轻隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示现有机器遗忘方法未能彻底清除被遗忘数据信息的问题，即便表面上看这些算法似乎有效。

Method: 作者引入了基于部分信息分解（PID）的信息论框架来系统地评估这一差异。通过比较遗忘前后的模型表示，将与遗忘数据之间的互信息分解为不同的组成部分，从而形式化定义了已遗忘和残留知识的概念。

Result: 分析表明，跨模型共享的冗余信息构成了持续存在的残留知识，并且与已知对抗性重建攻击的易感性相关联。基于这些发现，研究者提出了一个基于表示的风险评分，可以在推理时引导对敏感输入的选择性回避。

Conclusion: 该工作提出了一种原则性的、基于表示层面的审计方法用于机器遗忘，不仅提供了理论上的见解，还为更安全的语言模型部署提供了实用工具。

Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.

</details>


### [72] [Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.15124)
*Haonan Yuan,Qingyun Sun,Jiacheng Tao,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为RAG-GFM的模型，该模型通过引入检索增强生成技术来解决图基础模型（GFMs）中的内存瓶颈问题。它利用双模态统一检索模块和双重视角对齐目标来外部化图知识并保持异构信息，同时通过上下文增强来提高下游任务适应性。实验结果表明，RAG-GFM在跨域节点分类和图分类任务上优于13种最先进的基线方法，表现出更高的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的图基础模型（GFMs）试图将知识编码到模型参数中，这限制了语义容量、导致严重的有损压缩冲突，并且以阻碍高效适应的方式将图表示与知识纠缠在一起，从而影响了可扩展性和可解释性。为了解决这些问题，作者提出了RAG-GFM模型，旨在通过从参数中卸载知识并补充参数化学习来克服上述局限。

Method: RAG-GFM采用了检索增强生成的方法，设计了一个包含前缀结构文本的语义存储库和基于中心性的模式结构存储库的双模态统一检索模块。为了保留异构信息，开发了对比两种模态的双重视角对齐目标。此外，通过上下文增强技术，使用检索到的文本和模式作为上下文证据来丰富支持实例，以促进下游任务的有效适应。

Result: 通过对五个基准图数据集进行广泛的实验评估，RAG-GFM在跨域节点分类和图分类两项任务上均持续优于13种最先进基线方法，显示出其卓越的有效性和效率。

Conclusion: 研究证明，通过引入检索增强生成技术，RAG-GFM能够成功地解决现有图基础模型中存在的内存瓶颈问题，不仅提高了模型处理异构信息的能力，还增强了其对于不同任务的适应性。这些发现强调了RAG-GFM作为一种新颖架构，在提升图学习领域性能方面的潜力。

Abstract: Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.

</details>


### [73] [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)
*Tianshi Xu,Yuteng Chen,Meng Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为CLEANER的方法，通过利用模型内在的自我修正能力，在数据收集过程中直接消除含有错误的上下文，从而解决了参数受限模型在使用工具时遇到的探索阶段频繁执行失败的问题。实验结果显示，该方法在多个基准测试中取得了显著的性能提升，并且仅用三分之一的训练步骤就达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在处理参数受限的大规模语言模型（如4B-7B参数量）时，会因为频繁的执行失败而产生大量噪声轨迹，这不仅阻碍了策略优化，还导致了关键的信用分配问题。现有解决办法要么容易被奖励机制所滥用，要么计算成本过高。

Method: 本文提出了CLEANER方法，特别是其中的相似度感知自适应回滚（SAAR）机制，它能够自动构建干净、净化过的轨迹，通过回顾性地将失败替换为成功的自我纠正来实现这一点。基于语义相似性，SAAR能从浅层执行修复到深层推理替代之间调整替换粒度。

Result: 在AIME24/25, GPQA, 和LiveCodeBench上的实验证明，与基线相比，平均准确率分别提高了6%，3%，和5%。值得注意的是，CLEANER仅用三分之一的训练步数就达到了最先进的表现。

Conclusion: 通过CLEANER方法，可以在不增加额外计算成本的情况下有效提高强化学习过程中对于复杂问题解决的能力，特别是对于那些受到参数限制但仍需高效解决问题的大规模语言模型而言。

Abstract: Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

</details>


### [74] [Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data](https://arxiv.org/abs/2601.15158)
*Yuval Ran-Milo,Yotam Alexander,Shahar Mendel,Nadav Cohen*

Main category: cs.LG

TL;DR: 研究了通过强化学习训练的Transformer如何在仅基于最终答案正确性的监督下自发地发展出链式思维的能力，证明了梯度流可以促使模型收敛到一个结构化、可解释的算法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于结果的监督下的强化学习训练的Transformer能够自发地生成中间推理步骤（链式思维），但这种系统性推理能力是如何在稀疏奖励驱动下通过梯度下降被发现的机制仍不清楚。

Method: 分析了单层Transformer在合成图遍历任务上的梯度流动动力学，该任务没有链式思维就无法解决，但允许简单的迭代解法，并且证明了即使只基于最终答案的正确性进行训练，梯度流也能使模型收敛到一种按顶点逐步遍历图的结构化、可解释的算法。

Result: 确定了“简单示例”对于这种出现的关键作用；当训练分布对这些更简单的实例给予足够重视时，模型学会了一种可推广的遍历策略，可以外推到更长的链条上；如果这种重视消失，则基于梯度的学习变得不可行。

Conclusion: 通过合成数据实验和真实世界语言模型在数学推理任务上的实验，验证了理论发现适用于实际设置。

Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of "simple examples": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.

</details>


### [75] [ZENITH: Automated Gradient Norm Informed Stochastic Optimization](https://arxiv.org/abs/2601.15212)
*Dhrubo Saha*

Main category: cs.LG

TL;DR: 本文提出了一种名为ZENITH的优化器，它能够根据梯度范数的时间演变自动调整学习率。实验表明，ZENITH在图像分类、物体检测、关键点检测以及实例分割任务上都优于基线方法，并且与正则化技术兼容，可以进一步提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应优化器虽然能自动安排学习率，但存在计算和内存开销大、与正则化不兼容及学习率选择次优等问题。因此，需要一种新的方法来解决这些问题。

Method: 提出了ZENITH（Zero-overhead Evolution using Norm-Informed Training History）优化器，该优化器利用训练过程中梯度范数的变化来自动调节学习率。

Result: 跨6种CNN架构和6个基准测试的图像分类实验证明，ZENITH相比基线方法能在更短的实际时间里达到更高的测试准确率。此外，在MS COCO数据集上使用R-CNN系列模型进行对象检测、关键点检测和实例分割时也取得了更好的mAP表现。同时，ZENITH与正则化技术的兼容性还促进了模型泛化性能的提升。

Conclusion: ZENITH优化器提供了一种有效的方法来自适应地调整深度计算机视觉模型的学习率，不仅减少了对人工干预的需求，而且在多种视觉任务中展示了优越的表现。

Abstract: Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.

</details>


### [76] [Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism](https://arxiv.org/abs/2601.15249)
*Garrett G. Wen,Buxin Su,Natalie Collina,Zhun Deng,Weijie Su*

Main category: cs.LG

TL;DR: 本文提出了一种作者辅助机制，用于促进最佳论文奖的评选。通过Isotonic机制获取作者对自己提交作品的质量评估排名，并据此调整原始评审分数以优化估计提交作品的真实质量。研究证明了当作者的效用是调整后分数的凸加性函数时，激励作者真实报告，并且在作者只有一个提名名额的情况下，即使效用函数仅是非递减和加性的，真实性也成立。此外，该机制还被扩展到处理作者重叠的常见情况。模拟结果表明，此机制显著提高了所选获奖论文的质量。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能会议（如NeurIPS和ICML）接收的投稿数量激增，保持同行评审过程的质量和一致性变得越来越具有挑战性。特别是对于最佳论文奖的选择，这已经成为近年来讨论的一个热点问题。因此，需要一种新的方法来帮助提高这一过程的有效性和公平性。

Method: 本研究引入了一种基于Isotonic机制的方法，让作者对自己的投稿进行自我评估并提供一个排名，然后利用这个排名去校正最初的评审得分，以便更准确地估计每篇稿件的实际质量。此外，研究者们还探讨了在这种机制下，如何确保作者能够诚实地报告自己的评估，并将该机制推广到了多个作者合作的情形中。

Result: 研究表明，在给定条件下，作者有动机如实报告其对自身工作的评价；并且，通过对公开数据集上的分析验证了关于最佳论文奖项选择过程中效用函数的凸性假设。进一步地，针对单个提名的情况，即便放宽条件至效用函数只需满足非递减及可加性，也能保证诚实报告策略最优。最后，仿真结果显示新提出的机制确实有助于挑选出更高质量的获奖论文。

Conclusion: 提出的新机制为最佳论文奖的评选提供了更加高效、公正的方法，同时解决了以往工作中对作者行为假设过于严格的问题。它不仅提高了最终选定获奖论文的整体质量，而且在实际应用中展现出良好的适应性。

Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.

</details>
