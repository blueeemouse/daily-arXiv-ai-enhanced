<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 27]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning](https://arxiv.org/abs/2512.21409)
*Giacomo Turri,Grégoire Pacreau,Giacomo Meanti,Timothée Devergne,Daniel Ordonez,Erfan Mirzaei,Bruno Belucci,Karim Lounici,Vladimir Kostic,Massimiliano Pontil,Pietro Novelli*

Main category: cs.LG

TL;DR: kooplearn是一个机器学习库，实现了动力学算子及其谱分解的线性、核和深度学习估计器，支持离散时间和连续时间系统的建模与分析。


<details>
  <summary>Details</summary>
Motivation: 为了通过谱方法分析动力系统、推导数据驱动的降阶模型以及预测未来状态和可观察量，同时促进现有机器学习和数据科学工作流中的集成，并支持实验、可重复性和算法公平比较。

Method: kooplearn使用了线性、核方法和深度学习技术来估计离散时间演化算子（如Koopman/Transfer算子）及连续时间无穷小生成器。该库遵循scikit-learn API设计，便于用户上手使用。

Result: 开发出了一个名为kooplearn的软件包，它不仅能够帮助研究者们更好地理解和预测复杂动力系统的行为模式，而且还提供了一系列基准数据集以供测试和验证新的学习算法。

Conclusion: kooplearn为那些希望利用现代机器学习技术来研究和应用动力系统的人士提供了一个强大而灵活的工具。

Abstract: kooplearn is a machine-learning library that implements linear, kernel, and deep-learning estimators of dynamical operators and their spectral decompositions. kooplearn can model both discrete-time evolution operators (Koopman/Transfer) and continuous-time infinitesimal generators. By learning these operators, users can analyze dynamical systems via spectral methods, derive data-driven reduced-order models, and forecast future states and observables. kooplearn's interface is compliant with the scikit-learn API, facilitating its integration into existing machine learning and data science workflows. Additionally, kooplearn includes curated benchmark datasets to support experimentation, reproducibility, and the fair comparison of learning algorithms. The software is available at https://github.com/Machine-Learning-Dynamical-Systems/kooplearn.

</details>


### [2] [dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.21446)
*Shirui Chen,Jiantao Jiao,Lillian J. Ratliff,Banghua Zhu*

Main category: cs.LG

TL;DR: 提出了一种名为dUltra的强化学习框架，该框架通过联合优化基础扩散语言模型和解码顺序规划器来提高并行解码效率。在数学推理和代码生成任务中，dUltra相较于现有方法，在准确性和效率之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散语言模型（MDLMs）尽管具有并行生成令牌的潜力，但在实际应用中每次前向传递仅能解码少量令牌，导致其采样速度与自回归加投机解码方案相当，优势有限。同时，基于蒸馏的方法虽然尝试加速MDLMs，但容易偏离策略且受限于基模型样本质量。

Method: 开发了一个称为dUltra的新框架，它基于组相对策略优化（GRPO），采用一种在线强化学习方法来学习有效的并行解码策略。dUltra引入了一个解码计划头，用于预测每个令牌独立伯努利分布下的解码概率，并通过结合可验证奖励、蒸馏奖励以及解码步骤数来共同优化基础扩散语言模型及解码顺序规划器。

Result: 实验表明，在数学推理和代码生成等任务上，dUltra相比最先进的启发式方法和蒸馏基线，在保持或提高准确性的同时显著提高了解码效率。

Conclusion: dUltra为实现扩散模型相对于自回归模型的‘扩散优越性’提供了一条新途径，展示了在不同应用场景下提升并行解码性能的巨大潜力。

Abstract: Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.

</details>


### [3] [MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding](https://arxiv.org/abs/2512.21506)
*Aiwei Zhang,Arvind Pillai,Andrew Campbell,Nicholas C. Jacobson*

Main category: cs.LG

TL;DR: 本文介绍了一个名为MotionTeller的生成框架，该框架能够将原始生理信号（如加速度计数据）转换为自然语言摘要。通过结合预训练的行为编码器和轻量级投影模块，MotionTeller能够在冻结的解码器型大型语言模型中自动生成日常行为总结。基于真实世界NHANES记录构建的新数据集上进行训练后，MotionTeller在语义保真度和词汇准确性方面表现出色，相比基于提示的基础模型有明显提升。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴传感技术变得越来越普遍，一个关键挑战是如何从原始生理信号（比如加速计收集的分钟级移动数据）生成自然语言摘要。

Method: 研究者们提出了MotionTeller，一种生成式框架，它原生地整合了分钟级可穿戴活动数据与大规模语言模型（LLM）。这个框架利用了预先训练的行为编码器加上一个轻量级投射模块，将行为嵌入映射到冻结的仅解码器型LLM的token空间内，从而实现自由文本、自回归式的每日行为总结生成。他们还构建了一个由现实世界NHANES记录得出的新数据集，并使用交叉熵损失函数对模型进行了训练，监督仅限于语言tokens上。

Result: MotionTeller达到了高语义忠实度（BERTScore-F1 = 0.924）和词汇精确度（ROUGE-1 = 0.722），比基于提示的基础线方法提高了7%的ROUGE-1分数。平均训练损失在第15个epoch时收敛到了0.38，表明优化过程稳定。定性分析确认了MotionTeller能够捕捉昼夜结构及行为转变，而PCA图则显示了训练后嵌入空间中聚类对齐情况有所改善。

Conclusion: 这些结果共同表明，MotionTeller作为一个可扩展且易于解释的系统，在将可穿戴传感器数据转化为流畅的人类中心描述方面具有潜力，为行为监测、临床审查和个人化健康干预开辟了新的路径。

Abstract: As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral summaries. We construct a novel dataset of 54383 (actigraphy, text) pairs derived from real-world NHANES recordings, and train the model using cross-entropy loss with supervision only on the language tokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7 percent in ROUGE-1. The average training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative analysis confirms that MotionTeller captures circadian structure and behavioral transitions, while PCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these results position MotionTeller as a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, introducing new pathways for behavioral monitoring, clinical review, and personalized health interventions.

</details>


### [4] [Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering](https://arxiv.org/abs/2512.21510)
*Wenyuan Yang,Jie Xu,Hongqing He,Jiangzhang Gan,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于缺失模式树的不完全多视图聚类框架TreeEIC，通过定义缺失模式树来对数据进行分组，并在每个决策集中执行多视图聚类。此外，还引入了一个多视图决策集成模块和一个从集成到个体的知识蒸馏模块，以提高模型性能和鲁棒性。实验结果表明TreeEIC在高度不一致的缺失模式下表现出了最先进的性能和优越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的不完全多视图聚类（IMVC）方法未能充分利用可用但不完整的多视图对，导致模型性能受限。为了解决这一问题，提出了新的方法来更有效地利用这些数据对。

Method: 定义了缺失模式树模型将数据按照不同的缺失模式分成多个决策集，并在每个集合内执行多视图聚类；提出了一个多视图决策集成模块来整合所有决策集中的聚类结果，并通过不确定性权重抑制不可靠的聚类决定；最后，通过一个从集成到个体的知识蒸馏模块将集成知识传递给视图特定的聚类模型，优化跨视图一致性与群间区分损失，从而促进集成与个体模块之间的相互提升。

Result: 在多个基准数据集上的广泛实验显示，提出的TreeEIC方法达到了最新的IMVC性能，并且在高度不一致的缺失模式下表现出优越的鲁棒性。

Conclusion: TreeEIC通过有效利用不完整但可用的多视图对，解决了现有IMVC方法中存在的配对利用不足问题，显著提高了聚类性能和鲁棒性。

Abstract: Real-world multi-view data usually exhibits highly inconsistent missing patterns which challenges the effectiveness of incomplete multi-view clustering (IMVC). Although existing IMVC methods have made progress from both imputation-based and imputation-free routes, they have overlooked the pair under-utilization issue, i.e., inconsistent missing patterns make the incomplete but available multi-view pairs unable to be fully utilized, thereby limiting the model performance. To address this, we propose a novel missing-pattern tree based IMVC framework entitled TreeEIC. Specifically, to achieve full exploitation of available multi-view pairs, TreeEIC first defines the missing-pattern tree model to group data into multiple decision sets according to different missing patterns, and then performs multi-view clustering within each set. Furthermore, a multi-view decision ensemble module is proposed to aggregate clustering results from all decision sets, which infers uncertainty-based weights to suppress unreliable clustering decisions and produce robust decisions. Finally, an ensemble-to-individual knowledge distillation module transfers the ensemble knowledge to view-specific clustering models, which enables ensemble and individual modules to promote each other by optimizing cross-view consistency and inter-cluster discrimination losses. Extensive experiments on multiple benchmark datasets demonstrate that our TreeEIC achieves state-of-the-art IMVC performance and exhibits superior robustness under highly inconsistent missing patterns.

</details>


### [5] [Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training](https://arxiv.org/abs/2512.21515)
*Lei Liu,Hao Zhu,Yue Shen,Zhixuan Chu,Jian Wang,Jinjie Gu,Kui Ren*

Main category: cs.LG

TL;DR: 本文提出了一种基于困惑度的数据缩放法则，用于连续预训练中高效选择高质量的训练子集，从而提高特定领域应用的表现。


<details>
  <summary>Details</summary>
Motivation: 在为特定领域应用调整基础模型时，仅仅增加数据量进行连续预训练（CPT）带来的边际收益会迅速减少，导致数据利用效率低下和训练不经济。为了克服这一挑战，研究者们开发了一个新的方法来更有效地选择训练样本。

Method: 研究团队提出了一种困惑度感知的数据缩放法则，通过已预训练模型对领域特定数据产生的困惑度作为知识差距的一个估计指标，进而量化候选训练样本的信息困惑度格局。接着，通过对不同困惑度区间内的这种缩放法则进行拟合，实现了高价值数据子集的自适应选取，优先考虑能够最大化知识吸收同时最小化冗余和噪声的内容。

Result: 广泛的实验证明了该方法能够持续地识别出接近最优的训练子集，并且在医疗以及通用领域的基准测试上均取得了超越现有技术的表现。

Conclusion: 所提出的方法通过优化数据选择策略，在保证甚至提升模型性能的同时，也提高了训练过程中的数据利用效率。

Abstract: Continual Pre-training (CPT) serves as a fundamental approach for adapting foundation models to domain-specific applications. Scaling laws for pre-training define a power-law relationship between dataset size and the test loss of an LLM. However, the marginal gains from simply increasing data for CPT diminish rapidly, yielding suboptimal data utilization and inefficient training. To address this challenge, we propose a novel perplexity-aware data scaling law to establish a predictive relationship between the perplexity landscape of domain-specific data and the test loss. Our approach leverages the perplexity derived from the pre-trained model on domain data as a proxy for estimating the knowledge gap, effectively quantifying the informational perplexity landscape of candidate training samples. By fitting this scaling law across diverse perplexity regimes, we enable adaptive selection of high-utility data subsets, prioritizing content that maximizes knowledge absorption while minimizing redundancy and noise. Extensive experiments demonstrate that our method consistently identifies near-optimal training subsets and achieves superior performance on both medical and general-domain benchmarks.

</details>


### [6] [Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data](https://arxiv.org/abs/2512.21516)
*Hongqing He,Jie Xu,Wenyuan Yang,Yonghua Zhu,Guoqiu Wen,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种基于对比学习的多视图聚类框架，旨在解决不完整和噪声数据中的稀疏配对和错误配对问题。通过构建全局图引导的对比学习来克服稀疏配对挑战，并利用局部图加权对比学习减轻错误配对的影响，从而有效提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于对比学习（CL）的多视图聚类方法在面对真实世界中常见的数据不完整性或噪声时遇到重大挑战，包括稀疏配对样本不足导致无法充分探索互补信息，以及错误配对误导模型优化方向的问题。

Method: 1. 提出一个全局-局部图指导下的统一对比学习框架。
2. 为了解决稀疏配对问题，设计了由所有视角样本构成的全局视图亲和图，形成新的样本对以充分利用互补信息。
3. 针对错误配对问题，引入局部图加权对比学习机制，该机制能够根据局部邻居生成成对权重，自适应地增强或减弱对比学习效果。

Result: 实验结果表明，在处理不完整和含噪多视图数据集方面，所提方法相较于现有技术表现出更优的聚类性能。

Conclusion: 该研究提供了一种有效的方法来提高在存在缺失值和噪声情况下多视图数据上的聚类有效性，通过综合考虑全局和局部结构信息，增强了对比学习策略的鲁棒性和准确性。

Abstract: Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.

</details>


### [7] [First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions](https://arxiv.org/abs/2512.21521)
*Egor Shulgin,Grigory Malinovsky,Sarit Khirirat,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文提出了Fed-α-NormEC，这是一个在支持实际联邦学习特性（如多本地更新和部分客户端参与）的同时，提供可证明收敛性和差分隐私保证的首个框架。


<details>
  <summary>Details</summary>
Motivation: 当前的私有方法依赖于不切实际的假设，限制了它们的实际应用；而那些放松这些假设的工作通常忽略了联邦学习中的实际特征。

Method: 通过集成本地更新、分离服务器和客户端步长以及关键的部分客户端参与，开发了一种新的差分隐私联邦学习框架Fed-α-NormEC。

Result: 理论保证得到了私人深度学习任务实验的支持。

Conclusion: Fed-α-NormEC是第一个能够在标准假设下同时支持多本地更新和部分客户端参与等实用特性的差分隐私联邦学习框架，为现实世界部署提供了可能性。

Abstract: Federated Learning (FL) enables collaborative training on decentralized data. Differential privacy (DP) is crucial for FL, but current private methods often rely on unrealistic assumptions (e.g., bounded gradients or heterogeneity), hindering practical application. Existing works that relax these assumptions typically neglect practical FL features, including multiple local updates and partial client participation. We introduce Fed-$α$-NormEC, the first differentially private FL framework providing provable convergence and DP guarantees under standard assumptions while fully supporting these practical features. Fed-$α$-NormE integrates local updates (full and incremental gradient steps), separate server and client stepsizes, and, crucially, partial client participation, which is essential for real-world deployment and vital for privacy amplification. Our theoretical guarantees are corroborated by experiments on private deep learning tasks.

</details>


### [8] [Generative Actor Critic](https://arxiv.org/abs/2512.21527)
*Aoyang Qin,Deqian Kong,Wei Wang,Ying Nian Wu,Song-Chun Zhu,Sirui Xie*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架——生成式演员批评（GAC），它通过将策略评估视为学习轨迹和收益的联合分布的生成模型，以及将策略改进视为对该模型进行多样的推理来解耦顺序决策过程。基于一种具有连续潜在计划向量的潜在变量模型实现GAC，并开发了针对利用和探索的新推理策略。实验表明，GAC在离线性能上表现出色，在没有逐步骤奖励的情况下也显著提高了从离线到在线的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习算法通常专注于估计或最大化预期回报，但在使用在线经验改善预训练模型时面临挑战。

Method: 提出了生成式演员批评（GAC）这一新框架，该框架通过将“策略评估”重新定义为学习轨迹与收益的联合分布的生成模型p(τ, y)，并把“策略改进”看作是在这个学习到的模型上执行多种推断来解耦顺序决策制定。此外，还基于一个特征为连续潜伏计划向量的潜变量模型介绍了GAC的具体实例化，并为此开发了新颖的推断策略，既包括通过优化潜伏计划以最大化预期收益来进行‘利用’，也包括根据动态调整的目标收益条件采样潜伏计划以促进‘探索’。

Result: 在Gym-MuJoCo和Maze2D基准测试中的实验显示，即使在缺少逐步骤奖励的情况下，GAC相比现有最先进方法不仅展现了强劲的离线性能，而且显著增强了从离线到在线的改进效果。

Conclusion: GAC提供了一个有效的方法来解决传统RL算法在结合线下预训练与线上经验更新时遇到的问题，通过其独特的架构设计，能够在不依赖于逐步骤奖励的情况下实现优异的表现。

Abstract: Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(τ, y)$, and \textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \textit{exploitation}, by optimizing latent plans to maximize expected returns, and \textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.

</details>


### [9] [AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2512.21544)
*Xinru Wen,Weizhong Lin,Xuan Xiao*

Main category: cs.LG

TL;DR: 提出了一种名为AVP-Fusion的两阶段深度学习框架，通过自适应特征融合和对比学习来提高抗病毒肽（AVPs）识别的准确性。实验表明该方法在基准数据集上显著优于现有技术，并能够对病毒家族进行精确的子类预测。


<details>
  <summary>Details</summary>
Motivation: 当前计算方法难以捕捉复杂的序列依赖关系并有效处理模糊、难以分类的样本，这对加速新药开发构成了挑战。

Method: AVP-Fusion结合了自适应特征融合与对比学习策略，使用10种不同的描述符构建全景特征空间，并引入了自适应门控机制以动态调节CNN提取局部基序及BiLSTM捕获全局依赖性的权重。此外，还采用了基于在线难例挖掘(OHEM)和BLOSUM62的数据增强方法来解决数据分布问题。

Result: 在Set 1基准数据集上，AVP-Fusion达到了0.9531的准确率和0.9064的MCC值，远超现有最佳方法。并且，在第二阶段利用迁移学习实现了即使在样本量有限的情况下也能对六个病毒家族和八个特定病毒进行精准的子类预测。

Conclusion: AVP-Fusion作为一种鲁棒且可解释性强的工具，适用于高通量抗病毒药物筛选。

Abstract: Accurate identification of antiviral peptides (AVPs) is critical for accelerating novel drug development. However, current computational methods struggle to capture intricate sequence dependencies and effectively handle ambiguous, hard-to-classify samples. To address these challenges, we propose AVP-Fusion, a novel two-stage deep learning framework integrating adaptive feature fusion and contrastive learning. Unlike traditional static feature concatenation, we construct a panoramic feature space using 10 distinct descriptors and introduce an Adaptive Gating Mechanism.This mechanism dynamically regulates the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Furthermore, to address data distribution challenges, we employ a contrastive learning strategy driven by Online Hard Example Mining (OHEM) and BLOSUM62-based data augmentation, which significantly sharpens the model's decision boundaries. Experimental results on the benchmark Set 1 dataset demonstrate that AVP-Fusion achieves an accuracy of 0.9531 and an MCC of 0.9064, significantly outperforming state-of-the-art methods. In the second stage, leveraging transfer learning, the model enables precise subclass prediction for six viral families and eight specific viruses, even under limited sample sizes. In summary, AVP-Fusion serves as a robust and interpretable tool for high-throughput antiviral drug screening.

</details>


### [10] [Discovering Sparse Recovery Algorithms Using Neural Architecture Search](https://arxiv.org/abs/2512.21563)
*Patrick Yubeaton,Sarthak Gupta,M. Salman Asif,Chinmay Hegde*

Main category: cs.LG

TL;DR: 本文提出了一种元学习框架，能够通过神经架构搜索等工具在信号处理领域自动发现算法。该框架可以重新发现ISTA和FISTA算法的关键元素，并且适用于不同的数据分布和其他算法。


<details>
  <summary>Details</summary>
Motivation: 设计解决信号处理中逆问题的新算法是一项极其困难、依赖启发式方法且耗时的任务。

Method: 使用了包括神经架构搜索在内的元学习工具开发了一个框架，旨在从超过50,000个变量的搜索空间中重新发现ISTA及其加速版本FISTA算法的一些关键组成部分。

Result: 成功地利用所提出的框架重新发现了ISTA和FISTA算法的核心部分，并证明了该框架同样适用于其他类型的数据分布及算法。

Conclusion: 展示了元学习技术在自动化算法发现方面的潜力，特别是在信号处理领域内针对复杂逆问题的解决方案。

Abstract: The design of novel algorithms for solving inverse problems in signal processing is an incredibly difficult, heuristic-driven, and time-consuming task. In this short paper, we the idea of automated algorithm discovery in the signal processing context through meta-learning tools such as Neural Architecture Search (NAS). Specifically, we examine the Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated Fast ISTA (FISTA) variant as candidates for algorithm rediscovery. We develop a meta-learning framework which is capable of rediscovering (several key elements of) the two aforementioned algorithms when given a search space of over 50,000 variables. We then show how our framework can apply to various data distributions and algorithms besides ISTA/FISTA.

</details>


### [11] [A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)](https://arxiv.org/abs/2512.21610)
*Jagaran Chakma,Zhiguang Zhou,Jyoti Chakma,Cao YuSen*

Main category: cs.LG

TL;DR: 本研究提出了一种数据驱动的多目标方法，用于预测超高性能混凝土（UHPC）的机械性能、流动性和孔隙率。在测试了21种机器学习算法后，选择了5种高性能模型，其中XGBoost在使用随机搜索和K折交叉验证进行超参数调整后表现出最佳准确性。通过两阶段过程建立模型：首先基于原始数据构建初步XGBoost模型；选定最终模型后，通过去除多重共线性特征、利用孤立森林识别异常值以及采用SHAP分析选择重要特征来清洗数据集。然后使用精炼后的数据集重新训练XGBoost，从而对所有输出达到高预测准确度。此外，还开发了一个图形用户界面以支持材料设计师的工作。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过减少对广泛实验测试的需求，同时提高对于UHPC混合设计中关键属性如机械性能、流动性和孔隙率等预测的准确性。

Method: 采用了包含21种不同机器学习算法的测试，最终选择了包括XGBoost在内的五种表现最好的模型，并通过随机搜索与K折交叉验证技术对其进行了超参数优化。整个过程分为两个阶段：第一阶段基于未经处理的数据建立初始XGBoost模型；第二阶段则是在确定最优模型后，通过对数据集进行清洗（移除具有多重共线性的特征、运用孤立森林算法识别并剔除异常值、借助SHAP分析挑选出最关键特征）再重新训练XGBoost模型。

Result: XGBoost被证明是最精确的预测模型，特别是在经过超参数调优之后。此外，在完成数据清洗步骤并对模型进行重训后，新版本的XGBoost模型对于所有考察指标均展现出了高度的预测准确性。

Conclusion: 提出的框架显著提升了UHPC相关属性预测的准确性，同时减少了实际实验工作的需求量。

Abstract: This study presents a data-driven, multi-objective approach to predict the mechanical performance, flow ability, and porosity of Ultra-High-Performance Concrete (UHPC). Out of 21 machine learning algorithms tested, five high-performing models are selected, with XGBoost showing the best accuracy after hyperparameter tuning using Random Search and K-Fold Cross-Validation. The framework follows a two-stage process: the initial XGBoost model is built using raw data, and once selected as the final model, the dataset is cleaned by (1) removing multicollinear features, (2) identifying outliers with Isolation Forest, and (3) selecting important features using SHAP analysis. The refined dataset as model 2 is then used to retrain XGBoost, which achieves high prediction accuracy across all outputs. A graphical user interface (GUI) is also developed to support material designers. Overall, the proposed framework significantly improves the prediction accuracy and minimizes the need for extensive experimental testing in UHPC mix design.

</details>


### [12] [Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms](https://arxiv.org/abs/2512.21638)
*Jagaran Chakma,Zhiguang Zhou,Badhan Chakma*

Main category: cs.LG

TL;DR: 本研究开发并评估了三种机器学习模型（ET-XGB、RF-LGBM和Transformer-XGB）来预测钢-聚丙烯纤维增强高性能混凝土的机械性能，包括抗压强度(CS)、抗折强度(FS)以及抗拉强度(TS)。结果表明，ET-XGB在总体准确率上表现最佳，而RF-LGBM对于FS提供了最稳定可靠的预测。此外，通过SHAP分析确定了影响强度的主要因素。


<details>
  <summary>Details</summary>
Motivation: 为了提高对钢-聚丙烯纤维增强高性能混凝土机械性能预测的准确性与可解释性，同时提供给工程应用中优化混凝土配比设计及结构性能评估的有效工具。

Method: 研究采用了三类机器学习模型：结合XGBoost的Extra Trees(ET-XGB)、结合LightGBM的随机森林(RF-LGBM)以及结合XGBoost的Transformer(Transformer-XGB)。使用k折交叉验证、超参数优化、SHAP值解释以及不确定性分析等方法训练模型，以确保模型既健壮又具有良好的解释能力。

Result: ET-XGB模型在整体准确度上表现最优，CS、FS和TS的测试R^2值分别为0.994、0.944和0.978；RF-LGBM模型在FS预测上最为稳定可靠；而Transformer-XGB尽管展现了强大的预测能力但在表现出最高的不确定性。SHAP分析进一步指出，纤维长径比(AR1和AR2)、硅灰(Sfu)及钢纤维含量(SF)是影响强度的关键因素。

Conclusion: 机器学习模型能够为高性能混凝土(HPC)的机械性能提供精确、可解释且具有良好泛化性的预测。这些模型为优化混凝土配合比设计及提高工程应用中的结构性能评估提供了有价值的工具。

Abstract: This research develops and evaluates machine learning models to predict the mechanical properties of steel-polypropylene fiber-reinforced high-performance concrete (HPC). Three model families were investigated: Extra Trees with XGBoost (ET-XGB), Random Forest with LightGBM (RF-LGBM), and Transformer with XGBoost (Transformer-XGB). The target properties included compressive strength (CS), flexural strength (FS), and tensile strength (TS), based on an extensive dataset compiled from published experimental studies. Model training involved k-fold cross-validation, hyperparameter optimization, Shapley additive explanations (SHAP), and uncertainty analysis to ensure both robustness and interpretability. Among the tested approaches, the ET-XGB model achieved the highest overall accuracy, with testing R^2 values of 0.994 for CS, 0.944 for FS, and 0.978 for TS and exhibited lowest uncertainty for CS and TS (approximately 13-16% and 30.4%, respectively). The RF-LGBM model provided the most stable and reliable predictions for FS (R^2 0.977), yielding the lowest uncertainty for FS (approximately 5-33%). The Transformer-XGB model demonstrated strong predictive capability (R^2 0.978 for TS and 0.967 for FS) but consistently showed the highest uncertainty, indicating reduced generalization reliability. SHAP analysis further indicated that fiber aspect ratios (AR1 and AR2), silica fume (Sfu), and steel fiber content (SF) were the most influential predictors of strength, whereas water content (W) and the water-binder ratio (w/b) consistently had negative effects. The findings confirm that machine learning models can provide accurate, interpretable, and generalizable predictions of HPC mechanical properties. These models offer valuable tools for optimizing concrete mix design and enhancing structural performance evaluation in engineering applications.

</details>


### [13] [Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search](https://arxiv.org/abs/2512.21648)
*Maximilian Weichart*

Main category: cs.LG

TL;DR: 本文提出了一种名为Inverse-RPO的新方法，该方法可以从任何无先验的UCB系统地推导出基于先验的UCT。通过将这种方法应用于方差感知的UCB-V，研究者得到了两种新的结合方差估计的基于先验的树策略，并且实验表明这些新策略在多个基准测试中优于PUCT，而无需增加额外的计算成本。


<details>
  <summary>Details</summary>
Motivation: 虽然有许多理论保证比UCB1更强的替代UCB存在，但将其扩展到基于先验的UCT一直具有挑战性，因为PUCT是经验上得出而非从第一原理推导而来。最近的工作通过将MCTS视为正则化策略优化（RPO）问题来回顾性地证明了PUCT的合理性。受到这一视角启发，作者们希望开发一种通用方法论，能够从任意无先验的UCB出发系统地推导出基于先验的UCT。

Method: 提出了Inverse-RPO方法论，它允许从任一无先验的UCB算法中派生出考虑先验信息的UCT策略。应用此方法于方差敏感型UCB-V之上，从而得到两个新的基于先验且能结合方差估计值的树搜索策略。

Result: 实验结果显示，所提出的这两个考虑方差因素的基于先验UCT策略，在多组基准测试下表现优于现有的PUCT策略，而且没有带来额外的计算负担。此外，还对mctx库进行了扩展以支持方差敏感型UCT，并展示了所需的代码更改非常少，旨在促进关于基于原则的先验UCT进一步研究。

Conclusion: 通过引入Inverse-RPO框架以及基于其对方差敏感型UCB-V的应用，本研究成功地为蒙特卡洛树搜索提供了一种有效利用先验信息并整合方差估计的新途径，这不仅提高了探索效率，也为后续研究奠定了基础。

Abstract: Monte Carlo Tree Search (MCTS) has profoundly influenced reinforcement learning (RL) by integrating planning and learning in tasks requiring long-horizon reasoning, exemplified by the AlphaZero family of algorithms. Central to MCTS is the search strategy, governed by a tree policy based on an upper confidence bound (UCB) applied to trees (UCT). A key factor in the success of AlphaZero is the introduction of a prior term in the UCB1-based tree policy PUCT, which improves exploration efficiency and thus accelerates training. While many alternative UCBs with stronger theoretical guarantees than UCB1 exist, extending them to prior-based UCTs has been challenging, since PUCT was derived empirically rather than from first principles. Recent work retrospectively justified PUCT by framing MCTS as a regularized policy optimization (RPO) problem. Building on this perspective, we introduce Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free UCB. Applying this method to the variance-aware UCB-V, we obtain two new prior-based tree policies that incorporate variance estimates into the search. Experiments indicate that these variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without incurring additional computational cost. We also provide an extension of the mctx library supporting variance-aware UCTs, showing that the required code changes are minimal and intended to facilitate further research on principled prior-based UCTs. Code: github.com/Max-We/inverse-rpo.

</details>


### [14] [Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation](https://arxiv.org/abs/2512.21650)
*Xiao Liu,Junchen Jin,Yanjie Zhao,Zhixuan Xing*

Main category: cs.LG

TL;DR: 本文提出了一种新的多模态无监督异常检测框架Causal-HM，该框架通过建模物理过程到结果的依赖性，并利用传感器信号指导视听特征提取，以及采用因果层次结构来识别违反物理一致性的异常。在新构建的Weld-4M基准测试中，Causal-HM达到了90.7%的I-AUROC，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态无监督异常检测方法往往忽视了制造过程中不同模态数据（如实时视频、音频与传感器数据）和结果模态数据（如焊接后图像）之间的固有物理生成逻辑，导致处理复杂过程时存在因果盲点。此外，高维视觉数据与低维传感器信号间的异质性差距也常常掩盖了关键的过程上下文信息。

Method: 提出了Causal-HM，一种统一的多模态无监督异常检测框架，它明确地对从物理过程到结果的依赖关系进行建模。该框架包括两个主要创新点：1) 传感器引导的CHM调制机制，使用低维传感器信号作为上下文来指导高维视听特征的提取；2) 因果层次架构，强制执行单向生成映射以识别违反物理一致性的异常情况。

Result: 在跨越四种模态的新构建Weld-4M基准测试中，Causal-HM实现了90.7%的I-AUROC得分，展示了其在无监督异常检测领域的最先进性能。

Conclusion: Causal-HM通过引入传感器信号作为上下文信息并采用因果层次架构，在处理复杂制造过程中的多模态无监督异常检测任务上取得了显著改进，为智能制造业的质量保证提供了强有力的支持。

Abstract: Multimodal Unsupervised Anomaly Detection (UAD) is critical for quality assurance in smart manufacturing, particularly in complex processes like robotic welding. However, existing methods often suffer from causal blindness, treating process modalities (e.g., real-time video, audio, and sensors) and result modalities (e.g., post-weld images) as equal feature sources, thereby ignoring the inherent physical generative logic. Furthermore, the heterogeneity gap between high-dimensional visual data and low-dimensional sensor signals frequently leads to critical process context being drowned out. In this paper, we propose Causal-HM, a unified multimodal UAD framework that explicitly models the physical Process to Result dependency. Specifically, our framework incorporates two key innovations: a Sensor-Guided CHM Modulation mechanism that utilizes low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction , and a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies that violate physical consistency. Extensive experiments on our newly constructed Weld-4M benchmark across four modalities demonstrate that Causal-HM achieves a state-of-the-art (SOTA) I-AUROC of 90.7%. Code will be released after the paper is accepted.

</details>


### [15] [Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2512.21651)
*Dung Anh Hoang,Cuong Pham,Cuong Nguyen,Trung le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 本文探讨了1比特大型语言模型(LLL)量化中输出匹配方法失败的原因及条件，并基于此提出了一种新的数据感知后训练量化(PTQ)方法，该方法能够有效考虑激活误差累积的同时保持优化效率。实验表明，与现有的1比特PTQ方法相比，新方法在几乎不增加额外开销的情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管1比特量化（将浮点权重转换为±1）能极大减少计算和内存需求，但现有1比特PTQ方法通常会导致性能显著下降。已有方法主要集中在权重对齐上，而直接对齐输出虽然直观且符合量化目标，但在1比特LLM上直接应用时往往导致性能明显降低。本文旨在探究为何以及在什么条件下输出匹配会失败，并据此提出改进方案。

Method: 通过分析发现输出匹配在特定条件下失败的原因后，研究者提出了一种针对1比特LLM的数据感知PTQ方法。该方法特别关注激活错误积累问题，同时确保优化过程高效进行。

Result: 实验证明，所提方法相较于现有1比特PTQ技术，在几乎没有引入额外开销的前提下，实现了持续的性能提升。

Conclusion: 本研究揭示了1比特LLM量化过程中采用输出匹配策略面临的挑战，并提出了有效的解决方案——一种新型的数据感知PTQ方法，该方法能够在维持低部署成本的同时提高模型性能。

Abstract: Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, and knowledge distillation. Among these, post-training quantization (PTQ) is widely adopted for its efficiency, as it requires no retraining and only a small dataset for calibration, enabling low-cost deployment. Recent advances for post-training quantization have demonstrated that even sub-4-bit methods can maintain most of the original model performance. However, 1-bit quantization that converts floating-point weights to \(\pm\)1, remains particularly challenging, as existing 1-bit PTQ methods often suffer from significant performance degradation compared to the full-precision models. Specifically, most of existing 1-bit PTQ approaches focus on weight alignment, aligning the full-precision model weights with those of the quantized models, rather than directly aligning their outputs. Although the output-matching approach objective is more intuitive and aligns with the quantization goal, naively applying it in 1-bit LLMs often leads to notable performance degradation. In this paper, we investigate why and under what conditions output-matching fails, in the context of 1-bit LLM quantization. Based on our findings, we propose a novel data-aware PTQ approach for 1-bit LLMs that explicitly accounts for activation error accumulation while keeping optimization efficient. Empirical experiments demonstrate that our solution consistently outperforms existing 1-bit PTQ methods with minimal overhead.

</details>


### [16] [An Information Theoretic Perspective on Agentic System Design](https://arxiv.org/abs/2512.21720)
*Shizhe He,Avanika Narayan,Ishan S. Khare,Scott W. Linderman,Christopher Ré,Dan Biderman*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的方法来评估压缩-预测系统中压缩器LM的表现，通过引入互信息估计器量化压缩质量，并独立于特定任务。研究结果表明，较大的压缩器不仅更准确而且在每个token上传递更多的信息位，同时扩大压缩器比扩大预测器更有效。


<details>
  <summary>Details</summary>
Motivation: 尽管压缩-预测系统很受欢迎，但其设计仍然非常随意，缺乏关于如何选择压缩器和预测器以影响下游性能的指导。为了改善这一情况，作者们从信息论的角度出发，试图找到一种方法来评估不同压缩器的选择对整体性能的影响。

Method: 作者们将压缩器LM视为一个噪声通道，提出了一个简单的互信息估计器来量化上下文与其压缩之间的关系。这种方法允许在不依赖于具体任务的情况下评估压缩的质量。通过对五个数据集和三个模型家族进行广泛的实证分析，他们展示了互信息与下游性能之间存在强相关性。

Result: 研究表明，更大的压缩器不仅准确性更高，而且在每个token上也能够传递更多的信息位。例如，一个7B参数的Qwen-2.5压缩器比1.5B参数版本精确度提高了1.6倍、紧凑度提高了4.6倍，且每token的信息量增加了5.5倍。此外，在不同数据集上的测试显示，相比于扩展预测器规模，增加压缩器规模的效果更为显著。

Conclusion: 该研究提供了一个新的视角来理解并优化压缩-预测架构中的组件选择问题。通过采用所提出的原理，即使是小至3B参数的本地压缩器也能恢复到最前沿大型语言模型99%的准确性，同时仅需支付26%的API成本。

Abstract: Agentic language model (LM) systems power modern applications like "Deep Research" and "Claude Code," and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller "compressor" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger "predictor" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is $1.6\times$ more accurate, $4.6\times$ more concise, and conveys $5.5\times$ more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover $99\%$ of frontier-LM accuracy at $26\%$ of API costs.

</details>


### [17] [A Comedy of Estimators: On KL Regularization in RL Training of LLMs](https://arxiv.org/abs/2512.21852)
*Vedant Shah,Johan Obando-Ceron,Vineet Jain,Brian Bartoldson,Bhavya Kailkhura,Sarthak Mittal,Glen Berseth,Pablo Samuel Castro,Yoshua Bengio,Nikolay Malkin,Moksh Jain,Siddarth Venkatraman,Aaron Courville*

Main category: cs.LG

TL;DR: 本文分析了在使用强化学习训练大型语言模型时，不同KL散度估计器配置对模型性能的影响。研究发现，在线策略设置中，使用导致无偏梯度的估计器配置可以提高模型在领域内和领域外任务上的表现，并且KL正则化有助于稳定异步设置下的离策略RL训练。


<details>
  <summary>Details</summary>
Motivation: 尽管KL散度估计器广泛应用于强化学习训练的大规模语言模型中，但目前缺乏系统性研究来分析各种KL估计器纳入目标函数的方式及其对下游性能的具体影响。最近的研究指出，当前采用的KL正则化方法未能为所声明的目标提供正确的梯度，从而造成了目标与实现之间的差异。

Method: 通过对几种估计器配置的梯度进行分析，揭示设计选择如何塑造梯度偏差。利用	exttt{Qwen2.5-7B}、	exttt{Llama-3.1-8B-Instruct} 和 	exttt{Qwen3-4B-Instruct-2507} 模型进行了RL微调实验，并在领域内和领域外的任务上评估其表现。

Result: 在线策略环境下，具有偏差梯度的估计器配置可能导致训练不稳定；而使用导致无偏梯度的估计器配置则能在领域内外任务上获得更好的性能。此外，在离策略环境下，KL正则化有助于稳定来自异步设置的RL训练。

Conclusion: 正确处理KL散度估计器的选择对于确保强化学习训练过程中的稳定性及提升大语言模型最终性能至关重要。

Abstract: The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.

</details>


### [18] [Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation](https://arxiv.org/abs/2512.21866)
*Yiming Qian,Thorsten Neumann,Xueyining Huang,David Hardoon,Fei Gao,Yong Liu,Siow Mong Rick Goh*

Main category: cs.LG

TL;DR: 提出了一种可解释的、保护隐私的数据集蒸馏框架，用于协作金融欺诈检测。通过将训练好的随机森林转换为透明的轴对齐规则区域，并在每个区域内均匀采样生成合成交易，从而创建一个紧凑且可审计的替代数据集，同时保留局部特征交互而不会暴露敏感原始记录。此方法不仅支持可解释性，还能够保持较高的模型性能指标，同时减少数据量高达85%至93%，并具有强大的隐私保护特性，适用于多机构环境和监管审查。


<details>
  <summary>Details</summary>
Motivation: 为了在保证数据隐私的前提下，提高金融欺诈检测系统的效率与准确性，并提供可解释的结果以满足监管要求。

Method: 1. 将训练完毕的随机森林模型转换成一系列透明的轴对齐规则区域（叶超矩形）。
2. 通过对每个规则区域进行均匀采样来生成合成交易。
3. 利用这些合成数据代替原始数据集，形成体积更小但仍然保留关键信息的新数据集。
4. 分析了生成的数据集在不同机构间共享时对模型性能的影响。
5. 对生成的数据进行了成员推断攻击测试，评估其隐私泄露风险。

Result: 1. 蒸馏后的数据集减少了85%到93%的数据量，同时保持了竞争力的精度和微F1值，AUC略有下降。
2. 在不同机构间共享及扩充合成数据可以提高跨集群的精度、召回率和AUC。
3. 合成数据与真实数据结构高度相似。
4. 成员推断攻击成功率接近随机水平，表明记忆化风险低。
5. 去除不确定性高的合成点后，AUC提升至最高0.687，同时改善了校准效果。
6. 敏感性测试显示，蒸馏比率对AUC影响较小。

Conclusion: 树区域蒸馏技术能够有效促进可信赖且易于部署的欺诈分析解决方案的发展，它提供了全局可解释规则、单个案例的理由说明以及量化不确定性的能力，同时还具备良好的隐私保护属性，非常适合于多机构场景下的应用及监管审核。

Abstract: We propose an explainable, privacy-preserving dataset distillation framework for collaborative financial fraud detection. A trained random forest is converted into transparent, axis-aligned rule regions (leaf hyperrectangles), and synthetic transactions are generated by uniformly sampling within each region. This produces a compact, auditable surrogate dataset that preserves local feature interactions without exposing sensitive original records. The rule regions also support explainability: aggregated rule statistics (for example, support and lift) describe global patterns, while assigning each case to its generating region gives concise human-readable rationales and calibrated uncertainty based on tree-vote disagreement.
  On the IEEE-CIS fraud dataset (590k transactions across three institution-like clusters), distilled datasets reduce data volume by 85% to 93% (often under 15% of the original) while maintaining competitive precision and micro-F1, with only a modest AUC drop. Sharing and augmenting with synthesized data across institutions improves cross-cluster precision, recall, and AUC. Real vs. synthesized structure remains highly similar (over 93% by nearest-neighbor cosine analysis). Membership-inference attacks perform at chance level (about 0.50) when distinguishing training from hold-out records, suggesting low memorization risk. Removing high-uncertainty synthetic points using disagreement scores further boosts AUC (up to 0.687) and improves calibration. Sensitivity tests show weak dependence on the distillation ratio (AUC about 0.641 to 0.645 from 6% to 60%).
  Overall, tree-region distillation enables trustworthy, deployable fraud analytics with interpretable global rules, per-case rationales with quantified uncertainty, and strong privacy properties suitable for multi-institution settings and regulatory audit.

</details>


### [19] [GQ-VAE: A gated quantized VAE for learning variable length tokens](https://arxiv.org/abs/2512.21913)
*Theo Datta,Kayla Huang,Sham Kakade,David Brandfonbrener*

Main category: cs.LG

TL;DR: 提出了一种新的架构GQ-VAE，可以作为现有分词器的替代品，能够学习编码可变长度的离散标记，并且在压缩和语言建模性能方面优于标准VQ-VAE分词器。


<details>
  <summary>Details</summary>
Motivation: 尽管大多数前沿模型仍然使用基于确定性频率的分词算法（如字节对编码BPE），但最近有许多工作致力于设计学习型神经分词器。然而，这些方案通常会增加底层语言模型的复杂性，并强制进行大规模架构变更，使其难以在大规模上实现。为了解决这些问题，研究者们提出了一个新的架构。

Method: 研究者们提出了一种名为门控量化变分自编码器（GQ-VAE）的新架构，该架构可以独立预训练以作为现有分词器的直接替换。其关键创新在于学习如何编码可变长度的离散标记。

Result: GQ-VAE在压缩率和语言建模表现上超过了标准VQ-VAE分词器，并接近了BPE的表现。当使用较小词汇量的BPE使得GQ-VAE与BPE之间的压缩效果相当时，发现GQ-VAE能改进下游的语言模型学习。

Conclusion: 结论是GQ-VAE提供了一种有潜力的方法来改进当前使用的分词技术，并为进一步的研究提供了几个令人兴奋的方向。

Abstract: While most frontier models still use deterministic frequency-based tokenization algorithms such as byte-pair encoding (BPE), there has been significant recent work to design learned neural tokenizers. However, these schemes generally add to underlying language model complexity and force large changes to architecture, making them hard to implement at large scales. To overcome these challenges, we propose the gated quantized variational autoencoder (GQ-VAE), a novel architecture that can be independently pre-trained to serve as a drop-in replacement for existing tokenizers. The key innovation of the architecture is to learn to encode variable-length discrete tokens. GQ-VAE improves compression and language modeling performance over a standard VQ-VAE tokenizer, and approaches the compression rate and language modeling performance of BPE. Interestingly, if we use BPE with a smaller vocabulary, such that the compression is equivalent between GQ-VAE and BPE, we find that GQ-VAE improves downstream language model learning. We conclude with a discussion of several exciting avenues for future work. Code can be found at https://github.com/Theo-Datta-115/gq-vae.

</details>


### [20] [Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs](https://arxiv.org/abs/2512.21915)
*Yafeng Tang,Xiaoou Ding,Jianzhuo Du,Zishuo Yan,Zhuang Ma,Zheng Liang,Zekai Qian,Hongzhi Wang*

Main category: cs.LG

TL;DR: DATE, a novel framework for generating high-quality and diverse tabular data, uses a combination of data partitioning, LLMs, and a Multi-Arm Bandit-based sampling algorithm to improve the diversity and quality of generated data. DATE outperforms current GAN-based and LLM-based methods, significantly reducing error rates and enhancing reasoning capabilities in LLMs.


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案难以处理自然异质且分布多样的真实世界数据，无法为多样化数据生成提供通用的好模型。

Method: DATE框架通过有效地将原始异构数据分割成多个多样化的子集来准备高质量且分布上不同的示例，并利用大型语言模型探索这些子集的多样性，同时采用基于决策树推理的反馈机制生成每个子集的高质量标记数据。此外，为了平衡生成数据的多样性与质量之间的权衡问题，设计了一种基于多臂老虎机的采样算法。

Result: 广泛的实验表明，DATE在表格分类和回归基准测试中始终优于最先进的基于GAN和LLM的方法。平均而言，仅使用100个生成的数据，DATE就能将错误率降低23.75%。实证显示，DATE生成的数据可以提高直接偏好优化（DPO）的准确性并增强目标数据上LLM的推理能力。

Conclusion: DATE提供了一种新的方法来解决现有技术在处理异构数据时面临的挑战，通过结合数据分割、大型语言模型以及基于多臂老虎机的抽样算法，能够在保持数据多样性的同时确保其质量，从而显著提高了机器学习应用的表现。

Abstract: Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with diverse distributions, making it challenging to obtain a universally good model for diverse data generation. To address this limitation, we introduce Diversity-Aware Tabular data gEnerator (DATE), a framework that (i) prepares high-quality and distributionally distinct examples for in-context learning by effectively partitioning the original heterogeneous data into multiple diverse subsets; (ii) harnesses Large Language Models (LLMs) to explore the diversity of the partitioned distribution with decision tree reasoning as feedback, generating high-quality labeled data for each subset. However, the massive generated data inherently involves a trade-off between diversity and quality. To integrate this issue, existing solutions greedily select the validation-best data. However, we prove that the selection in heterogeneous settings does not possess the greedy-choice property, and design a Multi-Arm Bandit-based sampling algorithm that balances the diversity and quality of generated data. Extensive experiments on tabular classification and regression benchmarks demonstrate that DATE consistently outperforms state-of-the-art GAN-based and LLM-based methods. On average, DATE achieves a 23.75% reduction in error rate with just 100 generated data. Empirically, we demonstrate that data generated by DATE can improve the accuracy of Direct Preference Optimization (DPO) and enhance the reasoning capability of LLMs on the target data. Code is available at https://github.com/windblow32/DATE.

</details>


### [21] [Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms](https://arxiv.org/abs/2512.21925)
*Kongchang Zhou,Tingyu Zhang,Wei Chen,Fang Kong*

Main category: cs.LG

TL;DR: 提出了一种新的框架——混合CMAB-T，它将离线数据与在线交互以一种有原则的方式结合起来。所提出的混合CUCB算法利用离线数据指导探索并加速收敛，同时策略性地结合在线交互来缓解离线数据集覆盖不足或分布偏差的问题。理论分析和实验结果均表明该算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 先前的工作主要集中在在线设置上，通过迭代交互学习未知环境，或者在离线设置中仅从日志数据中学习策略。然而，在线算法面临高昂的交互成本和缓慢适应问题，而离线方法受限于数据集质量和缺乏探索能力。为了解决这些互补的弱点，本文提出了一个新框架——混合CMAB-T。

Method: 提出了一种名为混合CUCB的新算法，该算法能够利用离线数据来引导探索过程，并加快收敛速度；同时，通过战略性地加入在线交互环节来解决离线数据集覆盖面不够或存在分布偏差的问题。

Result: 理论分析显示，当拥有高质量离线数据时，混合CUCB算法明显优于纯在线方法，并且当数据有限或不匹配时，能有效纠正离线方法中的偏差。实证研究进一步证明了我们算法的一致优势。

Conclusion: 本研究提出的混合CMAB-T框架及混合CUCB算法提供了一种有效的方法来整合离线与在线数据，以克服现有方法的局限性。

Abstract: The problem of combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) has been extensively studied. Prior work primarily focuses on either the online setting where an agent learns about the unknown environment through iterative interactions, or the offline setting where a policy is learned solely from logged data. However, each of these paradigms has inherent limitations: online algorithms suffer from high interaction costs and slow adaptation, while offline methods are constrained by dataset quality and lack of exploration capabilities. To address these complementary weaknesses, we propose hybrid CMAB-T, a new framework that integrates offline data with online interaction in a principled manner. Our proposed hybrid CUCB algorithm leverages offline data to guide exploration and accelerate convergence, while strategically incorporating online interactions to mitigate the insufficient coverage or distributional bias of the offline dataset. We provide theoretical guarantees on the algorithm's regret, demonstrating that hybrid CUCB significantly outperforms purely online approaches when high-quality offline data is available, and effectively corrects the bias inherent in offline-only methods when the data is limited or misaligned. Empirical results further demonstrate the consistent advantage of our algorithm.

</details>


### [22] [Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing](https://arxiv.org/abs/2512.22024)
*Wesley S. Leite,Rodrigo C. de Lamare,Yuriy Zakharov,Wei Liu,Martin Haardt*

Main category: cs.LG

TL;DR: 本文提出了一种可变窗口大小（VWS）的空间平滑框架，用于增强稀疏线性阵列的共阵到达方向（DOA）估计。通过压缩平滑孔径，VWS-CA-MUSIC和VWS-CA-rMUSIC算法用未扰动的低秩附加项替换部分扰动的秩一外积项，从而增加信号与噪声子空间之间的分离度，同时保持信号子空间跨度。此外，还推导了保证可识别性的界限，限制了压缩参数可以取的值。仿真结果表明，在稀疏几何结构下，相对于固定窗口共阵MUSIC方法，所提方法在性能上有显著提升且复杂度降低。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于共阵DOA估计技术在处理稀疏线性阵列时遇到的问题，特别是当传统固定窗口尺寸的方法无法有效区分信号与噪声子空间的情况。

Method: 引入一种新的VWS空间平滑框架，并在此基础上开发了VWS-CA-MUSIC及VWS-CA-rMUSIC两种算法。这些算法通过减小平滑孔径来替换某些受干扰的数据成分，采用更加准确的低秩信息代替，以此改善信号与噪声子空间间的差距。另外，研究者们还确定了确保系统可识别性的条件范围。

Result: 实验结果显示，在使用稀疏几何布局的情况下，新提出的VWS-CA-MUSIC/VWS-CA-rMUSIC方案比标准的固定窗口共阵MUSIC方法展现出更优的表现，并且计算成本更低。

Conclusion: 本研究成功地提出了一个基于可变窗口大小的空间平滑框架，该框架下的VWS-CA-MUSIC和VWS-CA-rMUSIC算法能够有效提高稀疏线性阵列中DOA估计的质量，同时降低了实现难度。

Abstract: In this work, we introduce a variable window size (VWS) spatial smoothing framework that enhances coarray-based direction of arrival (DOA) estimation for sparse linear arrays. By compressing the smoothing aperture, the proposed VWS Coarray MUSIC (VWS-CA-MUSIC) and VWS Coarray root-MUSIC (VWS-CA-rMUSIC) algorithms replace part of the perturbed rank-one outer products in the smoothed coarray data with unperturbed low-rank additional terms, increasing the separation between signal and noise subspaces, while preserving the signal subspace span. We also derive the bounds that guarantees identifiability, by limiting the values that can be assumed by the compression parameter. Simulations with sparse geometries reveal significant performance improvements and complexity savings relative to the fixed-window coarray MUSIC method.

</details>


### [23] [LibContinual: A Comprehensive Library towards Realistic Continual Learning](https://arxiv.org/abs/2512.22029)
*Wenbin Li,Shangge Liu,Borui Kang,Yiyang Chen,KaXuan Lew,Yang Chen,Yinghuan Shi,Lei Wang,Yang Gao,Jiebo Luo*

Main category: cs.LG

TL;DR: 本文提出了一种名为LibContinual的全面且可复现的库，旨在为现实中的持续学习提供一个基础平台。它解决了当前研究中缺乏统一框架的问题，并通过系统分析指出了主流评估方法中的三个隐含假设，这些假设往往高估了持续学习方法在实际应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 持续学习领域面临的一个基本挑战是灾难性遗忘，即适应新任务会降低对之前任务的表现。随着各种方法的快速发展，研究领域变得碎片化，缺乏统一的框架、不一致的实现方式、冲突的依赖关系和不同的评估协议，使得公平比较和可复现的研究越来越困难。

Method: 提出了LibContinual，这是一个基于高内聚低耦合模块架构构建的综合性和可复现性库，集成了五大方法类别中的19种代表性算法，提供了一个标准化的执行环境。利用这一统一框架，作者们还系统地识别并研究了主流评估中存在的三个隐含假设：（1）离线数据访问性，（2）无限制内存资源，以及（3）任务内语义同质性。

Result: 通过使用严格的在线持续学习设置、新的统一内存预算协议及提出的类别随机化设定进行综合分析后，发现当许多代表性的持续学习方法受到这些现实世界约束时，其性能显著下降。

Conclusion: 这项研究表明了资源意识强且语义鲁棒的持续学习策略的必要性，并提供了LibContinual作为未来在更贴近实际情况的持续学习研究中的基础工具包。

Abstract: A fundamental challenge in Continual Learning (CL) is catastrophic forgetting, where adapting to new tasks degrades the performance on previous ones. While the field has evolved with diverse methods, this rapid surge in diverse methodologies has culminated in a fragmented research landscape. The lack of a unified framework, including inconsistent implementations, conflicting dependencies, and varying evaluation protocols, makes fair comparison and reproducible research increasingly difficult. To address this challenge, we propose LibContinual, a comprehensive and reproducible library designed to serve as a foundational platform for realistic CL. Built upon a high-cohesion, low-coupling modular architecture, LibContinual integrates 19 representative algorithms across five major methodological categories, providing a standardized execution environment. Meanwhile, leveraging this unified framework, we systematically identify and investigate three implicit assumptions prevalent in mainstream evaluation: (1) offline data accessibility, (2) unregulated memory resources, and (3) intra-task semantic homogeneity. We argue that these assumptions often overestimate the real-world applicability of CL methods. Through our comprehensive analysis using strict online CL settings, a novel unified memory budget protocol, and a proposed category-randomized setting, we reveal significant performance drops in many representative CL methods when subjected to these real-world constraints. Our study underscores the necessity of resource-aware and semantically robust CL strategies, and offers LibContinual as a foundational toolkit for future research in realistic continual learning. The source code is available from \href{https://github.com/RL-VIG/LibContinual}{https://github.com/RL-VIG/LibContinual}.

</details>


### [24] [From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation](https://arxiv.org/abs/2512.22031)
*Nagham Osman,Vittorio Lembo,Giovanni Bottegoni,Laura Toni*

Main category: cs.LG

TL;DR: 本文探讨了生成模型是否可以用于替代药物发现流程中的一个步骤：类先导化合物的生成。研究结果表明，这些模型能够生成有效、多样且具有生物相关性的化合物，并对多个目标进行了评估。此外，文章还指出了当前评价指标和可用训练数据的一些局限性。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现过程中，通过大规模化合物库进行高通量筛选以识别潜在药物候选物是一个资源密集型过程。尽管虚拟筛选技术有所进步，但该方法仍然耗时且成本高昂。本研究旨在探索生成模型能否直接支持药物发现流程中的类先导化合物生成阶段，从而部分或完全取代传统的识别工作流。

Method: 研究者提出了一种专门针对类先导化合物生成任务设计的评估框架，该框架结合了物理化学、结构及生物活性相关标准，在一个多阶段过滤管道中定义了类先导化合物的化学空间。实验使用了两种自回归模型和一种基于扩散的生成模型，并在不同数据集和训练条件下进行了基准测试，利用标准度量和靶点特异性对接分数来评估输出。

Result: 研究表明，所测试的生成模型能够为多个靶标产生有效、多样化且生物学上相关的化合物；特别是对于GSK-3β靶标，一些选定的类先导化合物已被合成并在体外验证了其活性。同时，也发现了现有评估指标和可获取训练数据方面存在的主要限制。

Conclusion: 生成模型显示出作为药物发现流程中类先导化合物生成工具的巨大潜力，但仍需进一步改进评估体系以及增加高质量训练数据以克服当前挑战。

Abstract: Hit identification is a critical yet resource-intensive step in the drug discovery pipeline, traditionally relying on high-throughput screening of large compound libraries. Despite advancements in virtual screening, these methods remain time-consuming and costly. Recent progress in deep learning has enabled the development of generative models capable of learning complex molecular representations and generating novel compounds de novo. However, using ML to replace the entire drug-discovery pipeline is highly challenging. In this work, we rather investigate whether generative models can replace one step of the pipeline: hit-like molecule generation. To the best of our knowledge, this is the first study to explicitly frame hit-like molecule generation as a standalone task and empirically test whether generative models can directly support this stage of the drug discovery pipeline. Specifically, we investigate if such models can be trained to generate hit-like molecules, enabling direct incorporation into, or even substitution of, traditional hit identification workflows. We propose an evaluation framework tailored to this task, integrating physicochemical, structural, and bioactivity-related criteria within a multi-stage filtering pipeline that defines the hit-like chemical space. Two autoregressive and one diffusion-based generative models were benchmarked across various datasets and training settings, with outputs assessed using standard metrics and target-specific docking scores. Our results show that these models can generate valid, diverse, and biologically relevant compounds across multiple targets, with a few selected GSK-3$β$ hits synthesized and confirmed active in vitro. We also identify key limitations in current evaluation metrics and available training data.

</details>


### [25] [Scaling Adversarial Training via Data Selection](https://arxiv.org/abs/2512.22069)
*Youran Ye,Dejin Wang,Ajinkya Bhandare*

Main category: cs.LG

TL;DR: 本文提出了一种选择性对抗训练方法，通过仅对每个小批量中的关键样本子集生成对抗样本来提高效率。实验表明，该方法在减少对抗计算量的同时，可以达到或超过完全PGD对抗训练的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 作者注意到，在使用广泛的第一阶对抗攻击方法——投影梯度下降（PGD）时，所有训练样本都经历了相同的迭代内部循环优化过程，尽管它们对模型鲁棒性的贡献不同。这种做法导致了计算成本的问题。基于此观察，提出了一个更高效的方法来改善这一现状。

Method: 提出了选择性对抗训练策略，只针对每个小批次内的部分关键样本生成对抗样本。具体来说，引入了两种主要的选择标准：(1) 基于边距的采样，优先处理接近决策边界的样本；(2) 梯度匹配采样，挑选那些梯度与主批处理优化方向一致的样本。对于选定的子集生成对抗样本，而其余样本则通过混合目标函数进行清洁训练。

Result: 在MNIST和CIFAR-10数据集上的实验显示，所提出的方法能够以最多减少50%对抗计算的情况下，实现与全PGD对抗训练相媲美甚至更好的鲁棒性。

Conclusion: 结果证明，通过对样本进行有选择地生成对抗样本，可以在显著降低计算成本的同时保持或提升模型面对对抗样本时的鲁棒性。

Abstract: Projected Gradient Descent (PGD) is a strong and widely used first-order adversarial attack, yet its computational cost scales poorly, as all training samples undergo identical iterative inner-loop optimization despite contributing unequally to robustness. Motivated by this inefficiency, we propose \emph{Selective Adversarial Training}, which perturbs only a subset of critical samples in each minibatch. Specifically, we introduce two principled selection criteria: (1) margin-based sampling, which prioritizes samples near the decision boundary, and (2) gradient-matching sampling, which selects samples whose gradients align with the dominant batch optimization direction. Adversarial examples are generated only for the selected subset, while the remaining samples are trained cleanly using a mixed objective. Experiments on MNIST and CIFAR-10 show that the proposed methods achieve robustness comparable to, or even exceeding, full PGD adversarial training, while reducing adversarial computation by up to $50\%$, demonstrating that informed sample selection is sufficient for scalable adversarial robustness.

</details>


### [26] [Unifying Learning Dynamics and Generalization in Transformers Scaling Law](https://arxiv.org/abs/2512.22088)
*Chiwun Yang*

Main category: cs.LG

TL;DR: 本文通过将基于Transformer的语言模型的学习动态形式化为常微分方程系统，并近似为核行为，来理论分析了随着计算资源和数据规模的增加，模型泛化误差收敛到不可约风险的过程。研究建立了超额风险的理论上限，并发现了一个明显的相变：在初始优化阶段，超额风险相对于计算成本呈指数衰减；当超过特定资源分配阈值后，系统进入统计阶段，此时泛化误差遵循幂律衰减规律。此外，该理论还单独推导了模型大小、训练时间和数据集大小各自的缩放定律，阐明了每个变量如何独立地影响泛化性能的上限。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）开发中的缩放定律已经在经验上得到验证，但其背后的理论基础尚不明确。本文旨在通过理论分析填补这一空白，特别是针对多层Transformer在序列到序列数据上的随机梯度下降（SGD）训练过程，以更接近实际应用情况的方式进行研究。

Method: 本文首先将基于Transformer的语言模型学习动态建模为一个常微分方程(ODE)系统，并进一步将其行为近似于核方法。接着，对具有任意数据分布特性的序列到序列数据上的多层Transformer采用随机梯度下降(SGD)训练进行了严格的数学分析。

Result: 研究表明，在初始优化阶段，超额风险相对于计算成本呈现指数级衰减趋势；而当超出某一特定资源分配阈值时，则会进入一个统计阶段，在此期间，泛化误差则按照Θ(C^(-1/6))的形式表现出幂律衰减特征。此外，还分别就模型大小、训练时长及数据集规模三个方面提出了独立的缩放法则。

Conclusion: 本文提供了一种新的理论框架来理解大型语言模型的缩放定律及其背后的机制。通过建立超额风险与计算资源之间关系的理论上限，揭示了模型训练过程中存在从指数衰减向幂律衰减转变的现象。这些发现不仅有助于我们更好地理解现有模型的行为，也为未来设计更高效的语言模型提供了指导。

Abstract: The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.
  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Θ(\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.

</details>


### [27] [Explainable Multimodal Regression via Information Decomposition](https://arxiv.org/abs/2512.22102)
*Zhaozhao Ma,Shujian Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于部分信息分解（PID）的多模态回归框架，该框架能够分解每个模ality的独特、冗余和协同成分。通过引入高斯性假设作为归纳偏置解决了PID框架的不确定性问题，并且推导出一个闭式条件独立正则化项来促进每个模ality内独特信息的隔离。实验表明，该框架在预测准确性和可解释性方面均优于现有方法，并支持有效的模态选择以提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态回归方法缺乏有效的工具来解耦并量化每个模ality及其交互作用的个体贡献，限制了多模态融合的可解释性。

Method: 提出了一种新的基于部分信息分解（PID）的多模态回归框架，该框架将特定于模态的表示分解为独特、冗余和协同成分；通过强制执行潜在表示联合分布及响应变量转换后的高斯性来解决PID框架内在的不确定性问题；还推导出了一个闭形式的条件独立正则化器以促进每个模态内独特信息的隔离。

Result: 实验结果表明，所提框架不仅在六个真实世界数据集上的预测准确性上超过了最先进方法，而且提高了模型的可解释性；此外，在大规模脑年龄预测案例研究中表现优异，同时支持了有效率的模态选择用于推理过程。

Conclusion: 基于PID的新多模态回归框架不仅提高了预测性能，还增强了模型对不同模态贡献的理解能力，从而促进了更高效、更具解释性的多模态数据分析。

Abstract: Multimodal regression aims to predict a continuous target from heterogeneous input sources and typically relies on fusion strategies such as early or late fusion. However, existing methods lack principled tools to disentangle and quantify the individual contributions of each modality and their interactions, limiting the interpretability of multimodal fusion. We propose a novel multimodal regression framework grounded in Partial Information Decomposition (PID), which decomposes modality-specific representations into unique, redundant, and synergistic components. The basic PID framework is inherently underdetermined. To resolve this, we introduce inductive bias by enforcing Gaussianity in the joint distribution of latent representations and the transformed response variable (after inverse normal transformation), thereby enabling analytical computation of the PID terms. Additionally, we derive a closed-form conditional independence regularizer to promote the isolation of unique information within each modality. Experiments on six real-world datasets, including a case study on large-scale brain age prediction from multimodal neuroimaging data, demonstrate that our framework outperforms state-of-the-art methods in both predictive accuracy and interpretability, while also enabling informed modality selection for efficient inference. Implementation is available at https://github.com/zhaozhaoma/PIDReg.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [28] [Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks](https://arxiv.org/abs/2512.21345)
*Jasmin Saxer,Isabella Maria Aigner,Luise Linzmeier,Andreas Weiler,Kurt Stockinger*

Main category: cs.DB

TL;DR: 本文提出了一种名为Query Carefully的管道，它结合了基于大语言模型的SQL生成与不可回答输入的显式检测和处理方法，以解决文本到SQL系统在处理模糊、超出范围或无法回答的问题时可能产生的风险。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到SQL系统对于模糊、超出范围或无法回答的问题也倾向于生成可执行的SQL语句，这可能会导致输出被误认为是正确的，特别是在生物医学领域这样的高精度要求场景下这种风险尤为严重。为了解决这一问题，作者们提出了Query Carefully解决方案。

Method: 通过使用llama3.3:70b模型，并结合模式感知提示、明确的无答案规则（NAR）以及从可回答和不可回答的问题中抽取的少量示例来实现对不可回答查询的有效识别和处理。此外还构建了一个包含8个类别共计80个无答案问题的数据集OncoMX-NAQ用于评估。

Result: 在OncoMX开发集上，使用可回答示例进行少量提示提高了结果准确性，而添加不可回答示例并未降低性能；而在OncoMX-NAQ上，平衡提示实现了最高的不可回答检测准确性（0.8），对于结构定义明确的类别几乎达到了完美效果，但对于缺失值查询（0.5）和列歧义（0.3）仍存在持续挑战。

Conclusion: 研究展示了一种有效的方法来提高生物医学应用中文本到SQL转换过程中的透明度和可靠性，通过专门设计的用户界面展示了中间SQL、执行结果及弃权情况，支持更安全可靠的交互体验。

Abstract: Text-to-SQL systems allow non-SQL experts to interact with relational databases using natural language. However, their tendency to generate executable SQL for ambiguous, out-of-scope, or unanswerable queries introduces a hidden risk, as outputs may be misinterpreted as correct. This risk is especially serious in biomedical contexts, where precision is critical. We therefore present Query Carefully, a pipeline that integrates LLM-based SQL generation with explicit detection and handling of unanswerable inputs. Building on the OncoMX component of ScienceBenchmark, we construct OncoMX-NAQ (No-Answer Questions), a set of 80 no-answer questions spanning 8 categories (non-SQL, out-of-schema/domain, and multiple ambiguity types). Our approach employs llama3.3:70b with schema-aware prompts, explicit No-Answer Rules (NAR), and few-shot examples drawn from both answerable and unanswerable questions. We evaluate SQL exact match, result accuracy, and unanswerable-detection accuracy. On the OncoMX dev split, few-shot prompting with answerable examples increases result accuracy, and adding unanswerable examples does not degrade performance. On OncoMX-NAQ, balanced prompting achieves the highest unanswerable-detection accuracy (0.8), with near-perfect results for structurally defined categories (non-SQL, missing columns, out-of-domain) but persistent challenges for missing-value queries (0.5) and column ambiguity (0.3). A lightweight user interface surfaces interim SQL, execution results, and abstentions, supporting transparent and reliable text-to-SQL in biomedical applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [29] [Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum](https://arxiv.org/abs/2512.21340)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Nikolaos Tsironis,Souvik Sengupta,Kostas Ramantas,Jhofre Ojeda*

Main category: cs.DC

TL;DR: The abstract discusses the trend of smart cities moving towards data-centric architectures to improve urban services.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight how data-centric approaches can lead to more efficient, sustainable, and resilient urban service delivery in the context of smart city development.

Method: The method is not explicitly stated in the given content, as it is a high-level overview rather than a detailed methodology. It likely involves the analysis of current smart city initiatives and the role of data in these projects.

Result: The result is an understanding that data-centric architecture is a key component in the evolution of smart cities, contributing to better urban planning and service provision.

Conclusion: In conclusion, the adoption of data-centric architectures by smart cities is presented as a positive move towards enhancing the quality of urban services.

Abstract: Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.

</details>


### [30] [Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism](https://arxiv.org/abs/2512.21487)
*Xinglin Pan,Shaohuai Shi,Wenxiang Lin,Yuxin Wang,Zhenheng Tang,Wei Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: 提出了一种名为FinDEP的细粒度任务调度算法，通过最大化任务重叠来提高MoE推理吞吐量。实验表明，在32-GPU系统上相比先前方法最多可提升1.61倍吞吐量和1.24倍加速比。


<details>
  <summary>Details</summary>
Motivation: 混合专家（MoE）架构在增加模型规模的同时计算增长呈次线性，但由于KV缓存和稀疏专家激活导致推理时内存密集。最近的分布式专家并行技术虽然将注意力机制和专家分配给专用GPU组，但缺乏对共享专家的支持以及高效的任务调度能力，限制了性能表现。

Method: 提出了FinDEP，一种用于分布式专家并行技术的细粒度任务调度算法，旨在通过最大化任务重叠以增强MoE推理吞吐量。该算法包含三个创新点：1) 将计算/通信分割成更小的任务单元以实现细粒度流水线；2) 设计一个支持变化粒度与顺序的任务调度优化方案；3) 开发针对此大搜索空间的有效求解器。

Result: 在四个不同GPU系统上使用DeepSeek-V2和Qwen3-MoE进行实验的结果显示，相比于现有方法，FinDEP能够使吞吐量最高提升至1.61倍，并且在一个32-GPU系统上实现了高达1.24倍的速度增益。

Conclusion: FinDEP通过引入细粒度任务调度策略有效地解决了当前分布式专家并行技术中存在的问题，显著提高了MoE模型在大规模GPU集群中的推理效率。

Abstract: The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.
  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.
  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

</details>


### [31] [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)
*Hui Guo,Qihang Zheng,Chenghai Huo,Dongliang Guo,Haoqi Yang,Yang Zhang*

Main category: cs.DC

TL;DR: 介绍了一个名为nncase的开源编译框架，旨在统一优化各种目标上的大型语言模型部署。该框架通过三个关键模块：自动向量化、自动分布和自动调度，解决了传统编译器工作流程分散和适应成本高的问题。评估表明，nncase在某些模型上超越了主流框架，并且在CPU上达到了与手动优化相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的有效部署受到内存架构异质性的阻碍，传统的编译器面临工作流分散以及高昂的适应成本问题。

Method: 提出nncase，一个基于e图术语重写引擎的端到端编译框架，包含自动向量化、自动分布搜索并带有成本意识的通信优化、以及自动调度以最大化片上缓存局部性三大核心组件。

Result: nncase在Qwen3系列模型上的表现优于MLC LLM和Intel IPEX等主流框架，并且在CPU上的性能可以与手工优化的llama.cpp相媲美。

Conclusion: nncase证明了自动化编译对于高性能大型语言模型部署是可行的解决方案。

Abstract: The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.

</details>


### [32] [Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models](https://arxiv.org/abs/2512.21884)
*Tingyang Sun,Ting He,Bo Ji,Parimal Parag*

Main category: cs.DC

TL;DR: 本研究首次系统地探讨了分布式大语言模型推理中的资源分配问题，特别是块放置和请求路由的决策。通过实验验证的性能模型、离线优化方法及其在线适应算法，证明了所提解决方案在多种地理分布服务器设置下能显著减少推理时间。此外，还开发了一个轻量级的仅CPU模拟器，用于预测GPU服务器上分布式LLM推理的性能，为GPU访问受限的研究人员提供了便利。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在许多AI任务中表现出色，但其高昂的使用成本（即使是在训练后）成为一大障碍，主要是因为需要高端GPU的支持。最近出现了一种名为PETALS的分布式系统，通过将模型块分散到互联网上的多个低端GPU服务器上来降低部署门槛，这种方法比在GPU内存和其他更便宜但速度较慢的本地存储介质之间交换模型参数要快得多。然而，这样的分布式系统的性能严重依赖于资源分配策略，而如何最优地进行资源分配仍是一个未知数。

Method: 本研究针对分布式LLM推理中的资源分配问题进行了首次系统性探索，重点在于两个关键决策：块放置与请求路由。主要成果包括：开发了能够根据给定的块放置和请求路由决策预测推理性能的实验验证性能模型；将离线环境下块放置与请求路由优化定义为一个混合整数线性规划问题，并给出了NP难度证明及具有保证性能的多项式复杂度算法；以及提出了适用于在线环境下的调整版本，该版本在负载有限的情况下同样具备性能保障。

Result: 通过实验和实验验证的模拟表明，相比于当前最先进的方案，在不同地理位置分布的服务器环境中，所提出的解决方案可以大幅减少推理所需时间。此外，研究团队还开发了一个轻量级且仅需CPU支持的模拟工具，它能够预测基于GPU服务器实现的分布式LLM推理性能，这有助于那些GPU资源有限的研究者们对未来相关领域展开进一步探索。

Conclusion: 这项工作不仅为分布式LLM推理提供了一种有效的资源分配策略，而且通过开发易于获取的模拟工具促进了该领域的进一步研究。结果表明，适当的资源管理策略可以在不牺牲性能的前提下大幅度降低成本。

Abstract: Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.

</details>


### [33] [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)
*Deniz Elbek,Kamer Kaya*

Main category: cs.DC

TL;DR: BLEST是一个利用Tensor Cores加速的BFS框架，通过引入二值化虚拟切片集、图重排序策略和批量稀疏矩阵-稀疏向量乘法模式等技术，在多种真实世界图数据上实现了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 虽然现代GPU提供了具有极高吞吐量的专门矩阵乘累加单元（如Tensor Cores），但它们主要针对密集型操作设计，对于不规则且无结构的图计算来说难以有效利用。为了充分利用这些硬件特性进行广度优先搜索（BFS）算法，需要一种高效的方法来映射边缘操作到Tensor Cores上，并解决冗余、负载不平衡及同步问题。

Method: BLEST采用了一种基于位图的结构重新构建了pull-based BFS流水线，并引入了二值化虚拟切片集（BVSS）来实现warp级别负载均衡与避免前端无关的工作分配。此外，该框架还应用了两种互补的图重排序策略：一种面向社交类图的压缩导向排序，另一种是面向非社交图的带宽减少排序。在计算层面，开发了一种批量SpMSpV乘法模式，使用位级Tensor Core瓦片处理点积而不会浪费输出条目，减少了所需的MMA调用次数。最后，通过内核融合结合延迟顶点更新方案以减少主机端同步、减轻原子开销并提高缓存局部性。

Result: 实验表明，相较于BerryBees, Gunrock, 和 GSWITCH，BLEST在广泛的现实世界图数据集上分别平均达到了3.58倍、4.64倍以及4.9倍的速度提升。

Conclusion: 通过一系列创新技术，BLEST成功地将现代GPU中的Tensor Cores应用于BFS这样的图计算任务中，大幅提高了执行效率。

Abstract: Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\times$, $4.64\times$ and $4.9\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.

</details>


### [34] [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)
*Zhuoran Zhu,Chunyang Zhu,Hao Lin,Xu Fu,Yiming Zhou,Quanlu Zhang,Zhenhua Li,Feng Qian,Chao Yu,Boxun Li,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: FUSCO, a communication library optimized for Mixture-of-Experts (MoE) models, reduces data shuffling overhead and achieves significant speedups over existing solutions, improving both training and inference performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the inefficiency in data shuffling during the training and inference of large-scale Mixture-of-Experts (MoE) models, which is a major bottleneck caused by the mismatch between the data layout used by MoE and the one expected by the communication operations. Existing communication libraries do not handle this shuffling efficiently, leading to high overhead that can exceed half of the total runtime.

Method: The authors introduce FUSCO, a communication library specifically designed to be friendly with MoE models. It employs fused data transformation and communication, alongside a pipelined communication engine that understands fine-grained data layouts. The method also includes lightweight planning and load-balancing mechanisms to avoid unnecessary communication and balance the network traffic.

Result: Experiments show that FUSCO can achieve up to 3.84 times and 2.01 times speedup compared to NCCL and DeepEP, respectively, on benchmark tests. In practical end-to-end MoE tasks, FUSCO reduces training latency by 1.17 to 1.39 times and 1.10 to 1.19 times when compared to NCCL and DeepEP. For inference, FUSCO lowers the first-token generation latency by 1.09 to 1.25 times and 1.06 to 1.16 times against the same baselines.

Conclusion: FUSCO demonstrates its effectiveness as an MoE-tailored communication library, significantly reducing the overhead of data shuffling and improving the overall efficiency of both training and inference processes for MoE models.

Abstract: Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\times$ and 2.01$\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\times$ and 1.10-1.19$\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\times$ and 1.06-1.16$\times$.

</details>


### [35] [Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications](https://arxiv.org/abs/2512.22113)
*Shengkun Cui,Rahul Krishna,Saurabh Jha,Ravishankar K. Iyer*

Main category: cs.DC

TL;DR: 本文介绍了一种名为PRAXIS的协调器，它利用LLM驱动的结构化遍历服务依赖图和吊床块程序依赖图来诊断由代码和配置引起的问题。相较于现有技术，PRAXIS在提高根本原因分析准确性的同时显著降低了令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 生产中的云事件由于未解决的问题每小时平均损失超过200万美元，其中代码和配置相关问题是主要原因。因此，需要一种有效的方法来快速准确地诊断这类问题。

Method: PRAXIS使用基于大语言模型（LLM）的技术，通过服务依赖图（SDG）和吊床块程序依赖图（PDG）对微服务和代码级别的依赖关系进行结构化遍历，以定位并解释故障。

Result: 与最先进的ReAct基线相比，PRAXIS将根本原因分析（RCA）的准确性提高了最多3.1倍，同时减少了3.8倍的令牌消耗。

Conclusion: PRAXIS提供了一种高效的根本原因分析方法，能够更好地处理由代码和配置引起的云事件，并且在减少资源消耗的同时提高了准确性。

Abstract: Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [36] [Selective LLM-Guided Regularization for Enhancing Recommendation Models](https://arxiv.org/abs/2512.21526)
*Shanglin Yang,Zhan Shi*

Main category: cs.IR

TL;DR: 提出了一种选择性大语言模型引导正则化框架，该框架仅在可训练门控机制预测大语言模型可靠时激活基于大语言模型的成对排序监督。此方法在线下进行所有大语言模型评分，不增加推理成本，并在多个数据集上展示了其在整体准确性、冷启动和长尾场景中的显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型作为独立推荐器或应用全局知识蒸馏的方法存在成本高、偏见以及在用户-物品空间的大部分区域不可靠的问题。同时，已有研究表明，大语言模型特别擅长重新排序和处理挑战情境，而不是在所有情况下都表现良好。

Method: 提出了选择性大语言模型引导正则化（Selective LLM Guided Regularization）框架，通过一个可以根据用户历史长度、项目流行度及模型不确定性来判断LLM是否可靠的可训练门控机制，在适当时候激活基于LLM的成对排名监督。所有的LLM评分都在离线完成，这样可以在不增加推断成本的情况下转移知识。

Result: 实验结果表明，所提出的选择策略能够持续提高整体准确性，并在冷启动和长尾场景中取得显著收益，优于全局蒸馏基线模型。

Conclusion: 选择性大语言模型引导正则化为利用大语言模型于推荐系统提供了一种有效且计算效率高的方法，尤其在改善冷启动问题和优化长尾项推荐方面表现突出。

Abstract: Large language models provide rich semantic priors and strong reasoning capabilities, making them promising auxiliary signals for recommendation. However, prevailing approaches either deploy LLMs as standalone recommender or apply global knowledge distillation, both of which suffer from inherent drawbacks. Standalone LLM recommender are costly, biased, and unreliable across large regions of the user item space, while global distillation forces the downstream model to imitate LLM predictions even when such guidance is inaccurate. Meanwhile, recent studies show that LLMs excel particularly in re-ranking and challenging scenarios, rather than uniformly across all contexts.We introduce Selective LLM Guided Regularization, a model-agnostic and computation efficient framework that activates LLM based pairwise ranking supervision only when a trainable gating mechanism informing by user history length, item popularity, and model uncertainty predicts the LLM to be reliable. All LLM scoring is performed offline, transferring knowledge without increasing inference cost. Experiments across multiple datasets show that this selective strategy consistently improves overall accuracy and yields substantial gains in cold start and long tail regimes, outperforming global distillation baselines.

</details>


### [37] [CEMG: Collaborative-Enhanced Multimodal Generative Recommendation](https://arxiv.org/abs/2512.21543)
*Yuzhen Lin,Hongyi Chen,Xuanjing Chen,Shaowen Wang,Ivonne Xu,Dongming Jiang*

Main category: cs.IR

TL;DR: 提出了一种名为CEMG的新框架，通过多模态融合层和统一模态标记化阶段来改善生成式推荐模型中的协作信号整合和多模态特征融合问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决生成式推荐模型在协作信号浅层整合与多模态特征解耦合融合方面的问题，从而创建一个更全面的项目表示。

Method: 设计了CEMG框架，包括一个多模态融合层用于动态整合视觉和文本特征，以及一个使用RQ-VAE将融合表征转换成离散语义代码的统一模态标记化阶段。

Result: 广泛的实验表明，CEMG的表现显著优于最先进基线方法。

Conclusion: 提出的CEMG框架有效地解决了生成式推荐中存在的关键挑战，并通过实验证明了其优越性。

Abstract: Generative recommendation models often struggle with two key challenges: (1) the superficial integration of collaborative signals, and (2) the decoupled fusion of multimodal features. These limitations hinder the creation of a truly holistic item representation. To overcome this, we propose CEMG, a novel Collaborative-Enhaned Multimodal Generative Recommendation framework. Our approach features a Multimodal Fusion Layer that dynamically integrates visual and textual features under the guidance of collaborative signals. Subsequently, a Unified Modality Tokenization stage employs a Residual Quantization VAE (RQ-VAE) to convert this fused representation into discrete semantic codes. Finally, in the End-to-End Generative Recommendation stage, a large language model is fine-tuned to autoregressively generate these item codes. Extensive experiments demonstrate that CEMG significantly outperforms state-of-the-art baselines.

</details>


### [38] [LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model](https://arxiv.org/abs/2512.21595)
*Yinfu Feng,Yanjing Wu,Rong Xiao,Xiaoyi Zen*

Main category: cs.IR

TL;DR: 提出了一种名为LLM-I2I的数据中心框架，该框架利用大型语言模型(LLMs)来改善数据质量问题，包括一个基于LLM的生成器和一个基于LLM的判别器，以解决长尾物品的数据稀疏性和噪声问题。实验证明该方法能够提高推荐准确性，并在实际部署中增加了召回数和商品总值。


<details>
  <summary>Details</summary>
Motivation: 为了解决项目到项目（I2I）推荐模型中存在的数据稀疏性及噪声问题，同时不增加计算成本和部署复杂度。

Method: 提出了一个名为LLM-I2I的数据中心框架，其中包含一个基于大型语言模型（LLMs）的生成器用于合成用户-物品交互信息来缓解长尾物品的数据稀疏性问题，以及一个基于LLM的判别器用来过滤真实与合成数据中的噪声交互。

Result: 在工业界(AEDS)和学术界(ARD)数据集上进行了评估，结果显示LLM-I2I能持续提升推荐准确度，特别是在长尾物品方面。当部署在一个大规模跨境电子商务平台上时，相比现有I2I模型，它使召回数量提高了6.02%，商品总价值增长了1.22%。

Conclusion: 研究表明，通过使用大型语言模型改进数据中心的方法可以在不改变模型架构的情况下增强I2I推荐系统的性能。

Abstract: Item-to-Item (I2I) recommendation models are widely used in real-world systems due to their scalability, real-time capabilities, and high recommendation quality. Research to enhance I2I performance focuses on two directions: 1) model-centric approaches, which adopt deeper architectures but risk increased computational costs and deployment complexity, and 2) data-centric methods, which refine training data without altering models, offering cost-effectiveness but struggling with data sparsity and noise. To address these challenges, we propose LLM-I2I, a data-centric framework leveraging Large Language Models (LLMs) to mitigate data quality issues. LLM-I2I includes (1) an LLM-based generator that synthesizes user-item interactions for long-tail items, alleviating data sparsity, and (2) an LLM-based discriminator that filters noisy interactions from real and synthetic data. The refined data is then fused to train I2I models. Evaluated on industry (AEDS) and academic (ARD) datasets, LLM-I2I consistently improves recommendation accuracy, particularly for long-tail items. Deployed on a large-scale cross-border e-commerce platform, it boosts recall number (RN) by 6.02% and gross merchandise value (GMV) by 1.22% over existing I2I models. This work highlights the potential of LLMs in enhancing data-centric recommendation systems without modifying model architectures.

</details>


### [39] [KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering](https://arxiv.org/abs/2512.21799)
*Hung-Nghiep Tran,Atsuhiro Takasu*

Main category: cs.IR

TL;DR: 本文介绍了两个数据集KG20C和KG20C-QA，用于促进学术数据上的问答研究。KG20C是一个高质量的学术知识图谱，而KG20C-QA基于此构建以支持学术数据的QA任务，并提供了一组QA模板来生成自然语言问答对。


<details>
  <summary>Details</summary>
Motivation: 为了推进学术数据上的问答（QA）研究，作者构建了高质量的知识图谱KG20C以及基于其上用于QA任务的数据集KG20C-QA。目的是为研究社区提供一个可重用、可扩展的资源，以促进QA、推理和知识驱动的应用在学术领域的未来发展。

Method: 通过从Microsoft Academic Graph中选择特定的会议或期刊，基于质量过滤内容并定义模式来构建KG20C。对于KG20C-QA，则是定义了一组QA模板，将图中的三元组转换成自然语言形式的问题-答案对。此外还使用标准的知识图谱嵌入方法对该数据集进行了基准测试，并分析了不同关系类型下的表现。

Result: 创建了高质量的学术知识图谱KG20C及适用于QA任务的数据集KG20C-QA。提供了详尽的文档说明其构建过程与规格。同时，通过标准的知识图谱嵌入方法对KG20C-QA进行了评估，并给出了可重复使用的评估协议。

Conclusion: 正式发布这些带有完整文档的数据集，旨在为研究界贡献一个可用于QA、推理及知识驱动型应用等领域的可重用且可扩展资源。

Abstract: In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.

</details>


### [40] [Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion](https://arxiv.org/abs/2512.21863)
*Huatuan Sun,Yunshan Ma,Changguang Wu,Yanxin Zhang,Pengfei Wang,Xiaoyu Du*

Main category: cs.IR

TL;DR: 本文首次系统地研究了大型冻结视频语言模型在微视频推荐中的应用，通过对比不同的集成策略和特征提取范式，提出了双特征融合框架（DFF），该框架能够自适应地结合多层表示与项目ID嵌入，从而在真实世界的微视频推荐基准测试中达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前，在微视频推荐领域使用大型冻结视频语言模型（LVLMs）时缺乏系统性的实证评估。实际操作中往往将这些模型作为固定的黑盒特征提取器来部署，而没有系统性地比较替代的表征策略。

Method: 文章沿着两个关键设计维度进行了首次系统性实证研究：一是与ID嵌入的集成策略，特别是替换与融合；二是特征提取范式，比较了LVLM生成的字幕与中间解码器隐藏状态。基于这些发现，提出了一个轻量级且即插即用的方法——双特征融合（DFF）框架，该框架能自适应地融合来自冻结LVLM的多层次表示与物品ID嵌入。

Result: 实验结果表明，中间隐藏状态的表现始终优于基于字幕的表示，因为自然语言总结不可避免地会丢弃对于推荐至关重要的精细视觉语义；ID嵌入捕捉到了不可替代的合作信号，使得融合策略明显优于替换策略；不同层之间的解码器特征有效性差异显著。DFF框架在两个真实世界微视频推荐基准上达到了最先进水平，持续超越了强大的基线方法。

Conclusion: 研究表明，通过采用适当的集成策略和选择合适的特征提取方法，可以显著提高微视频推荐系统的性能。所提出的DFF框架为整合现成的大规模视觉-语言模型进入微视频推荐系统提供了一种有原则的方法。

Abstract: Frozen Large Video Language Models (LVLMs) are increasingly employed in micro-video recommendation due to their strong multimodal understanding. However, their integration lacks systematic empirical evaluation: practitioners typically deploy LVLMs as fixed black-box feature extractors without systematically comparing alternative representation strategies. To address this gap, we present the first systematic empirical study along two key design dimensions: (i) integration strategies with ID embeddings, specifically replacement versus fusion, and (ii) feature extraction paradigms, comparing LVLM-generated captions with intermediate decoder hidden states. Extensive experiments on representative LVLMs reveal three key principles: (1) intermediate hidden states consistently outperform caption-based representations, as natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation; (2) ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement; and (3) the effectiveness of intermediate decoder features varies significantly across layers. Guided by these insights, we propose the Dual Feature Fusion (DFF) Framework, a lightweight and plug-and-play approach that adaptively fuses multi-layer representations from frozen LVLMs with item ID embeddings. DFF achieves state-of-the-art performance on two real-world micro-video recommendation benchmarks, consistently outperforming strong baselines and providing a principled approach to integrating off-the-shelf large vision-language models into micro-video recommender systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 本研究通过调查46名行业专业人士，探讨了大型语言模型（LLMs）在软件工程中的应用情况。结果显示，尽管LLMs在解决技术问题、文档支持和代码标准化方面受到好评，但也存在对认知依赖、安全风险和技术自主性侵蚀的担忧。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）快速发展并对编码、文档编写及系统维护实践产生深远影响，理解这些工具如何被开发者使用变得至关重要。

Method: 基于对拥有不同教育背景与经验水平的46位行业专业人士进行的一项调查。

Result: 受访者对LLMs持积极态度，特别是在快速解答技术问题、改进文档支持以及提高源代码标准化程度方面。不过也表达了对于认知依赖增加、安全风险以及可能的技术自主性减弱等方面的顾虑。

Conclusion: 这项研究强调了以批判性和监督方式使用基于LLM工具的重要性，并为开发人员和研究人员提供了有效、负责任且更安全地采纳和发展LLM技术的见解，同时激发了对未来关于其认知、伦理和组织影响的研究兴趣。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [42] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种名为Cerberus的新颖预测性、无执行的覆盖率引导测试框架，旨在检测代码片段中的运行时错误和异常而无需实际执行代码。通过两阶段反馈循环，Cerberus能够生成高覆盖率的测试用例并更有效地发现运行时错误。


<details>
  <summary>Details</summary>
Motivation: 在软件开发过程中，不实际执行代码就能检测到代码片段中的运行时错误和异常是非常理想的。例如，在将在线代码片段整合进代码库之前检测运行时异常就显得尤为重要。

Method: 提出了Cerberus，一种利用大语言模型（LLMs）来生成触发运行时错误的输入，并进行代码覆盖率预测与错误检测的新型框架。它采用了两阶段反馈循环机制：首先同时提高代码覆盖率和检测运行时错误；当覆盖率到达100%或其最大值时，则转向专注于仅检测运行时错误。

Result: 实验证明，对于完整或不完整的代码片段，Cerberus比传统的基于学习的测试框架能更高效地生成高覆盖率的测试用例，从而发现更多的运行时错误。

Conclusion: Cerberus作为一种创新性的测试工具，能够有效提升代码覆盖率及检测运行时错误的能力，为软件开发过程中的质量保证提供了新的解决方案。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [43] [Code Clone Refactoring in C# with Lambda Expressions](https://arxiv.org/abs/2512.21511)
*Takuto Kawamoto,Yoshiki Higo*

Main category: cs.SE

TL;DR: 研究提出了一种针对C#语言的使用lambda表达式分析和合并代码克隆的技术，并通过NiCad克隆检测器检测到的2,217对克隆进行评估，发现35.0%的克隆对适合重构，其中28.9%成功进行了重构。


<details>
  <summary>Details</summary>
Motivation: 在不同的编程语言中，'提取方法'重构技术的有效性可能因语言特性而异。尽管行为参数化在Java程序中被广泛研究，但在其他编程语言中的应用较少。本研究旨在为C#开发一种特定的技术，利用lambda表达式来处理代码克隆问题。

Method: 研究者们设计了一个专用于C#的方法，该方法运用了lambda表达式来进行代码克隆的分析与合并。他们选取了由NiCad克隆检测工具识别出的一系列代码克隆作为实验对象，以此来测试所提方法的有效性。

Result: 通过对来自22个项目的2,217对代码克隆进行评估，结果显示有35.0%的克隆对被认为是可重构的；而在这些确定可以重构的克隆对中，实际成功重构的比例达到了28.9%。

Conclusion: 这项研究表明，所提出的基于C#的、利用lambda表达式的'提取方法'重构技术能够有效地应用于代码克隆的整合，且具有一定的成功率。

Abstract: "Extract Method" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.
  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal "Extract Method" approach may vary depending on the programming language.
  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.
  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.

</details>


### [44] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型的新型方法，通过构建实体依赖图(EDG)来实现仓库级别的类型推断，并通过迭代改进类型和依赖关系以提高准确性。该方法在12个复杂的Python仓库上进行了评估，相较于现有工作，在TypeSim得分、TypeExact得分以及减少新类型错误方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: Python的动态类型机制虽然提高了灵活性，但也是大型软件中运行时类型错误的主要来源。现有的类型推断工具主要针对孤立代码片段有效，但对于整个仓库级别的类型推断仍存在挑战，特别是处理复杂的跨过程依赖关系时。

Method: \methodName 提出了一种基于大型语言模型的方法，用于实现仓库级的类型推断。它通过创建一个实体依赖图（EDG）来表示存储库中的对象及其类型依赖性。在推理过程中，\methodName 采用迭代方式不断优化EDG中的类型和依赖关系，以确保准确的类型推断。此外，还引入了一个循环中的类型检查策略，实现实时验证与校正推断结果，从而减少了错误传播。

Result: 对12个复杂Python仓库进行评估后，\methodName 在TypeSim分数上达到了0.89，在TypeExact分数上达到了0.84，相比最强基线分别提高了27%和40%。更重要的是，\methodName 能够消除由工具引入的新类型错误达到92.7%。

Conclusion: 本研究介绍了一种名为\methodName 的新方法，能够有效地解决Python项目中仓库级别类型推断的问题。通过结合实体依赖图、迭代改进过程以及实时类型检查等创新点，\methodName 不仅在性能指标上超越了现有解决方案，而且极大地减少了新类型错误的发生率，为实际Python开发环境中的自动化且可靠的类型注解提供了重要进展。

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [45] [The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX](https://arxiv.org/abs/2512.21781)
*Abdul Ali Bangash,Tongxu Ge,Zhimin Zhao,Arshdeep Singh,Zitao Wang,Bram Adams*

Main category: cs.SE

TL;DR: 本文通过对比分析170个公开宣传的SBOM工具的使用案例，以及对CycloneDX和SPDX两种格式生态系统健康指标的定量比较，揭示了各自的优势与改进空间。研究发现，使用CycloneDX工具的项目显示出更高的开发者参与度及某些健康指标，而SPDX则受益于更成熟、工具更广泛的生态系统和行业认可。


<details>
  <summary>Details</summary>
Motivation: 软件材料清单（SBOM）对于提供软件组件元数据及其依赖关系的透明度至关重要，但其采用程度受到工具生态系统的影响。鉴于存在两种主要格式：SPDX和CycloneDX，它们在成熟度、工具支持和社区参与方面差异显著。本研究旨在通过量化对比不同工具生态系统的应用案例及健康状况，为开发者、贡献者及相关从业者提供关于这两种生态系统互补优势的信息，并指出相互增强的机会。

Method: 首先对170个公开宣传的SBOM工具进行使用案例的量化对比分析；接着评估CycloneDX（171个工具）与SPDX（470个工具）两大生态系统在稳健性和成熟度方面的健康指标；然后从开源工具中定量比较36,990份问题报告来识别挑战和发展机会；最后调查并比较了使用每种工具生态系统的前250个开源项目的健康指标。

Result: 研究表明，使用CycloneDX工具的项目表现出较高的开发者参与度和一些健康指标上的优势，而SPDX工具则得益于一个更加成熟且工具种类更为丰富的生态系统，同时拥有既定的行业采纳基础。

Conclusion: 这项研究为开发人员、贡献者和实践者提供了有关这两种生态系统互补优势的重要见解，并指出了促进双方共同进步的机会领域。

Abstract: A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.

</details>


### [46] [A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation](https://arxiv.org/abs/2512.21811)
*Qiaolin Qin,Jianchen Zhao,Heng Li,Weiyi Shang,Ettore Merlo*

Main category: cs.SE

TL;DR: 提出了一种新的无标签模板级度量PMSS，用于评估日志解析器性能，该方法与基于标签的度量FGA和FTA有显著正相关性，并在缺乏标签或真实数据不一致时提供了一个有价值的评估选项。


<details>
  <summary>Details</summary>
Motivation: 当前的日志解析器评价指标严重依赖于标记的数据集，这限制了研究只能使用固定的数据集进行，并且不同版本的真实数据可能导致性能结论不一致。

Method: 提出了一种名为PMSS的新颖无标签模板级度量，它通过medoid轮廓分析和Levenshtein距离来评估解析器分组和模板质量，通常具有接近线性的时间复杂度。

Result: 实验结果表明，在标准修正后的Loghub 2.0数据集上，达到最高PMSS或FGA的解析器表现出相似的性能；PMSS与FGA和FTA之间存在显著（p<1e-8）且正向的相关性。

Conclusion: PMSS为日志解析器的选择提供了新视角，特别是在面临不一致的真实数据或缺少标签的情况下。此外，还讨论了如何解释来自不同度量的结果，指出了使用PMSS的挑战，并给出了利用该度量选择解析器的指导方针。

Abstract: Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.

</details>


### [47] [Proceedings First Workshop on Adaptable Cloud Architectures](https://arxiv.org/abs/2512.22054)
*Giuseppe De Palma,Saverio Giallorenzo*

Main category: cs.SE

TL;DR: This abstract is an introduction to the post-proceedings of WACA 2025, a workshop on Adaptable Cloud Architectures, which took place in Lille, France, alongside DisCoTec 2025.


<details>
  <summary>Details</summary>
Motivation: The motivation for this volume is to provide a collection of papers that were presented at WACA 2025, which focuses on the latest research and developments in adaptable cloud architectures. The aim is to disseminate knowledge and foster discussions among researchers and practitioners in the field of distributed computing techniques.

Method: As this is an abstract for a proceedings volume, it does not describe a specific method. However, the included papers likely present various methodologies, case studies, and theoretical approaches related to the topic of adaptable cloud architectures.

Result: The result of this volume is the publication of peer-reviewed papers that contribute to the understanding and advancement of adaptable cloud architectures. It serves as a reference for current research and future directions in the area.

Conclusion: The conclusion can be inferred that the WACA 2025 post-proceedings offer valuable insights into the state-of-the-art of adaptable cloud architectures and are a significant contribution to the DisCoTec 2025 conference.

Abstract: This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.

</details>


### [48] [HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules](https://arxiv.org/abs/2512.22043)
*Zhangbo Long,Letian Sha,Jiaye Pan,Dongpeng Xu,Yifei Huang,Fu Xiao*

Main category: cs.SE

TL;DR: 本文提出了一种新的二进制程序分析框架，通过内核模块扩展传统动态二进制插桩的分析能力，并利用进程挖空技术构建分析环境，以提高细粒度分析的可用性和性能。该框架在Windows平台上实现并经过大量实验验证其有效性和性能。


<details>
  <summary>Details</summary>
Motivation: 由于部署性问题、高内存使用率以及性能开销，细粒度的二进制代码分析（如动态污点分析）需要不断研究来更好地适应新分析场景，例如内存破坏攻击和沙箱规避恶意软件。

Method: 提出了一种新的二进制程序分析框架，主要利用内核模块进一步拓展传统动态二进制插桩的分析能力；基于解耦分析的思想，采用进程挖空技术以一种新颖的方式在容器进程中构建了分析环境。

Result: 原型系统已在Windows平台实现，并通过基准测试和实际程序进行了大量实验，验证了该框架的有效性和性能。此外，通过对实际漏洞利用程序和恶意代码的分析，也证实了该框架的实际应用价值。

Conclusion: 新的二进制程序分析框架成功提高了细粒度分析方法的可用性和性能，在不影响目标程序执行的情况下重用了现有动态二进制插桩平台的功能，显示出了在实际安全威胁应对中的潜力。

Abstract: Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.

</details>
