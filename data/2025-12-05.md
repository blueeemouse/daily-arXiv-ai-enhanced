<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.LG](#cs.LG) [Total: 41]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [UserSimCRS v2: Simulation-Based Evaluation for Conversational Recommender Systems](https://arxiv.org/abs/2512.04588)
*Nolwenn Bernard,Krisztian Balog*

Main category: cs.IR

TL;DR: UserSimCRS v2, an upgraded toolkit for simulating and evaluating conversational recommender systems, introduces advanced user simulators, supports a broader range of CRSs and datasets, and adds new evaluation tools.


<details>
  <summary>Details</summary>
Motivation: 鉴于用于对话推荐系统（CRS）基于模拟评估的资源稀缺，这项工作旨在通过显著升级UserSimCRS工具包来填补这一空白，以符合最前沿的研究需求。

Method: 方法包括增强基于议程的用户模拟器、引入基于大型语言模型的模拟器、为更广泛的CRS和数据集提供集成支持，以及新增LLM作为评判者的评估工具。

Result: 通过案例研究展示了这些扩展功能的效果。

Conclusion: UserSimCRS v2为对话推荐系统的仿真评估提供了更加全面且先进的解决方案。

Abstract: Resources for simulation-based evaluation of conversational recommender systems (CRSs) are scarce. The UserSimCRS toolkit was introduced to address this gap. In this work, we present UserSimCRS v2, a significant upgrade aligning the toolkit with state-of-the-art research. Key extensions include an enhanced agenda-based user simulator, introduction of large language model-based simulators, integration for a wider range of CRSs and datasets, and new LLM-as-a-judge evaluation utilities. We demonstrate these extensions in a case study.

</details>


### [2] [Spatially-Enhanced Retrieval-Augmented Generation for Walkability and Urban Discovery](https://arxiv.org/abs/2512.04790)
*Maddalena Amendola,Chiara Pugliese,Raffaele Perego,Chiara Renso*

Main category: cs.IR

TL;DR: 本文介绍了一种基于空间检索增强生成（RAG）的框架WalkRAG，它通过结合信息检索、空间推理和大型语言模型来推荐步行城市路线，并支持用户与系统互动以获取路径及兴趣点的信息。初步结果显示了该方法在促进城市探索方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）尽管在人工智能领域中发挥了重要作用，但它们在处理空间检索和推理任务时存在局限性，比如容易产生错误信息（幻觉）。因此，需要新的解决方案来改善这些缺点。同时，为了提高城市探索体验，特别是为用户提供符合特定空间限制和个人偏好的步行路线建议，提出了将检索增强生成技术应用于地理理解任务的想法。

Method: 提出了一种名为WalkRAG的空间RAG框架，该框架利用对话界面为用户提供可步行的城市行程推荐。WalkRAG能够根据用户的特定空间约束和偏好要求提供路线建议，并且允许用户交互式地查询关于路线及其沿途的兴趣点(POI)的信息。

Result: 初步结果表明，通过整合信息检索、空间推理以及大型语言模型，WalkRAG框架能够有效地支持城市发现活动，为用户提供个性化的步行路线建议。

Conclusion: WalkRAG作为一种创新性的解决方案，成功地展示了如何通过结合信息检索、空间推理能力与大型语言模型的力量来克服现有LLMs在处理地理相关任务时所面临的挑战，从而为用户提供更准确、有用的城市探索建议。

Abstract: Large Language Models (LLMs) have become foundational tools in artificial intelligence, supporting a wide range of applications beyond traditional natural language processing, including urban systems and tourist recommendations. However, their tendency to hallucinate and their limitations in spatial retrieval and reasoning are well known, pointing to the need for novel solutions. Retrieval-augmented generation (RAG) has recently emerged as a promising way to enhance LLMs with accurate, domain-specific, and timely information. Spatial RAG extends this approach to tasks involving geographic understanding. In this work, we introduce WalkRAG, a spatial RAG-based framework with a conversational interface for recommending walkable urban itineraries. Users can request routes that meet specific spatial constraints and preferences while interactively retrieving information about the path and points of interest (POIs) along the way. Preliminary results show the effectiveness of combining information retrieval, spatial reasoning, and LLMs to support urban discovery.

</details>


### [3] [Ask Safely: Privacy-Aware LLM Query Generation for Knowledge Graphs](https://arxiv.org/abs/2512.04852)
*Mauro Dalle Lucca Tosi,Jordi Cabot*

Main category: cs.IR

TL;DR: 提出了一种隐私感知的知识图谱查询生成方法，通过识别并省略图中的敏感信息来防止敏感数据传输给第三方服务，同时保持了生成查询的质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决在知识图谱包含敏感数据且用户缺乏部署本地生成式大语言模型资源的情况下，无法应用现有方法的问题。

Method: 基于图结构识别敏感信息，并在请求大语言模型将自然语言问题转换为Cypher查询之前省略这些值。

Result: 实验结果表明，该方法在阻止敏感数据向第三方服务传输的同时，保持了生成查询的质量。

Conclusion: 本研究提供了一种有效的方法来处理含敏感数据的知识图谱查询问题，同时保护了用户隐私和查询质量。

Abstract: Large Language Models (LLMs) are increasingly used to query knowledge graphs (KGs) due to their strong semantic understanding and extrapolation capabilities compared to traditional approaches. However, these methods cannot be applied when the KG contains sensitive data and the user lacks the resources to deploy a local generative LLM. To address this issue, we propose a privacy-aware query generation approach for KGs. Our method identifies sensitive information in the graph based on its structure and omits such values before requesting the LLM to translate natural language questions into Cypher queries. Experimental results show that our approach preserves the quality of the generated queries while preventing sensitive data from being transmitted to third-party services.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection](https://arxiv.org/abs/2512.04106)
*Fouad Trad,Ali Chehab*

Main category: cs.SE

TL;DR: 本文研究了在代码漏洞检测任务中，基于检索的增强型提示方法相较于标准少样本提示和零样本提示以及微调模型的效果。实验结果表明，基于检索的增强型提示方法不仅在性能上优于其他提示策略，而且避免了模型微调所需的时间与成本。


<details>
  <summary>Details</summary>
Motivation: 针对复杂领域内（如代码漏洞检测）使用少样本提示来利用大型语言模型能力时，其有效性严重依赖于上下文示例的选择和质量这一问题，提出了基于检索的增强型提示作为改善方案。

Method: 采用Gemini-1.5-Flash模型，在代码漏洞检测任务上对比了三种方法：1) 使用随机选取的例子进行标准少样本提示；2) 利用语义相似的例子进行基于检索的增强型提示；3) 基于检索的标注法，即根据检索到的例子直接分配标签而不经过模型推理。此外，还将该方法与零样本提示及多个微调模型进行了比较。

Result: 基于检索的增强型提示在所有测试的方法中表现最佳，以20个样本为例，达到了74.05%的F1分数和83.90%的部分匹配准确率。相比于零样本提示(F1: 36.35%, 部分匹配准确率: 20.30%)和微调后的Gemini模型(F1: 59.31%, 部分匹配准确率: 53.10%)均有显著提升。尽管微调CodeBERT可以达到更高的性能(F1: 91.22%, 部分匹配准确率: 91.30%)，但需要额外的训练、维护工作量和资源投入。

Conclusion: 基于检索的增强型提示为提高少样本条件下代码漏洞检测任务的表现提供了一种有效且经济的方法，能够在不增加训练时间和成本的情况下超越传统的提示方法和某些微调模型。

Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.

</details>


### [5] [Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2512.04117)
*Joost Mertens,Joachim Denil*

Main category: cs.SE

TL;DR: 本文提出了一种通用方法，通过使用验证指标来检测孪生系统中的异常，并通过一个与工业相关的龙门起重机案例研究展示了这些技术。此外，还通过基于历史数据的参数估计来纠正数字孪生中的错误。


<details>
  <summary>Details</summary>
Motivation: 在孪生系统中，确保数字孪生保持对所孪生系统的有效表示是一个挑战，特别是在物理系统随时间演变的情况下。为了使数字孪生能够随着物理系统的变化而更新，需要开发一种能检测物理系统变化的方法。

Method: 本文采用了模型设计中的验证技术，提供了一种利用验证指标来检测孪生系统中异常情况的通用方法。同时，对于发现的异常，通过基于历史数据的参数估计来进行修正。

Result: 提出的方法被应用于一个学术但具有工业相关性的龙门起重机案例研究中，证明了该方法的有效性。

Conclusion: 通过使用模型验证技术，可以有效地检测孪生系统中的异常，并且借助基于历史数据的参数估计方法，能够实现对数字孪生的准确修正。

Abstract: One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.

</details>


### [6] [DrP: Meta's Efficient Investigations Platform at Scale](https://arxiv.org/abs/2512.04250)
*Shubham Somani,Vanish Talwar,Madhura Parikh,Eduardo Hernandez,Jimmy Wang,Shreya Shah,Chinmay Gandhi,Sanjay Sundarajan,Neeru Sharma,Srikanth Kamath,Nitin Gupta,Benjamin Renard,Ohad Yahalom,Chris Davis*

Main category: cs.SE

TL;DR: 本文介绍了一种名为DrP的端到端框架和系统，用于自动化调查过程，以减少故障解决时间（MTTR）并减轻待命工程师的工作负担。该系统已在Meta大规模部署，覆盖了300多个团队，并在生产环境中运行了5年，每天执行5万次自动分析，平均减少了20%的MTTR，显著提升了待命工作效率。


<details>
  <summary>Details</summary>
Motivation: 现有的调查流程通常依赖于人工或临时脚本，这导致效率低下，增加了故障缓解和隔离所需的时间，同时增加了待命工程师的压力和工作量。

Method: 提出了DrP，一个旨在通过自动化调查来降低事件平均解决时间(MTTR)和减轻待命工作负担的端到端框架与系统。它包括一个灵活的SDK用于编写调查剧本、可扩展后端执行这些剧本、插件将剧本集成进主流工作流如警报及事件管理工具中，以及对调查结果进行处理（包括缓解措施）的后处理系统。

Result: DrP已在Meta大规模部署，覆盖超过300个团队、2000多个分析器，应用于服务、核心基础设施、AI/ML、硬件、移动等多个领域。该系统日均执行5万次自动分析，整体上使MTTR降低了20%，对于某些团队甚至超过了80%，显著改善了待命生产力。

Conclusion: 通过引入DrP系统，能够有效缩短事件响应时间，提升问题排查效率，从而大幅提高待命工程师的工作效率和生活质量。

Abstract: Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.
  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.

</details>


### [7] [On the Role and Impact of GenAI Tools in Software Engineering Education](https://arxiv.org/abs/2512.04256)
*Qiaolin Qin,Ronnie de Souza Santos,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 本研究通过调查130名本科生，探讨了生成式AI工具在软件工程教育中的使用情况，包括其带来的好处、挑战、伦理问题以及教学期望。研究发现学生主要利用这些工具进行渐进学习和高级实现，但也面临一些困难，如不明确的推理过程和难以调整输出等。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT和GitHub Copilot等生成式AI（GenAI）工具的兴起，软件的学习与编写方式发生了转变。尽管这些工具为软件工程教育带来了新的支持机会，但也引发了关于过度依赖、道德使用及对学习影响等方面的担忧。

Method: 采用结合了李克特量表项目和开放式问题的调查方法，对来自两所大学的130名本科学生进行了调研。该调查旨在探索五个方面：使用背景、感知到的好处、遇到的挑战、伦理看法以及教学观点。

Result: 学生们最常将GenAI用于渐进式学习和复杂实现任务上，认为它有助于激发想法并增强信心；然而，同时也遇到了诸如无法理解AI决策逻辑及调整输出结果难度大等问题。此外，学生们还表达了对于公平性和不当行为的伦理顾虑，并呼吁提供更清晰的教学指导方针。

Conclusion: 生成式AI正以微妙的方式重塑软件工程教育领域。研究结果强调了需要通过构建支架式学习环境、制定伦理政策以及采取适应性教学策略来确保GenAI能够促进公平而高效的学习体验。

Abstract: Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.

</details>


### [8] [Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage](https://arxiv.org/abs/2512.04262)
*Nolan Platt,Ethan Luchs,Sehrish Nizamani*

Main category: cs.SE

TL;DR: 本研究探讨了大型语言模型（LLMs）在开发阶段提供可靠且一致的可用性评估的能力。通过应用Jakob Nielsen的十个可用性启发式方法对三十个开源网站进行评估，GPT-4o模型展示了在问题检测上中等程度的一致性，但在严重性判断方面显示出更大的变异性。这表明虽然GPT-4o能够生成内部一致的评价，特别是在识别可用性问题的存在方面，但其判断严重性的能力变化较大，实际使用时需要人为监督。


<details>
  <summary>Details</summary>
Motivation: 传统的人工专家进行的可用性评估耗时且主观性强，尤其是在开发早期。研究旨在探索大型语言模型是否可以在开发阶段提供可靠和一致的启发式评估。

Method: 选取了30个开源网站作为样本，并运用OpenAI的GPT-4o按照Jakob Nielsen提出的十条可用性原则生成超过850份启发式评估报告。对于每个站点进行了三次独立评估。

Result: GPT-4o模型在检测问题上表现出了中等一致性，平均成对Cohen's Kappa值为0.50，完全一致率为84%；但在评判问题严重度方面则表现出更大差异，加权Cohen's Kappa均值为0.63，完全一致率仅为56%，Krippendorff's Alpha接近于零。

Conclusion: 研究表明，尽管GPT-4o可以产生内部一致的评估结果，特别是在识别可用性问题存在方面，但它评估问题严重性的能力变化较大，实践中仍需人类监督。此外，该研究为自动化用户体验评估提供了可行性和局限性的见解，并为进一步提高模型一致性奠定了基础。

Abstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.

</details>


### [9] [Polynomiogram: An Integrated Framework for Root Visualization and Generative Art](https://arxiv.org/abs/2512.04263)
*Hoang Duc Nguyen,Anh Van Pham,Hien D. Nguyen*

Main category: cs.SE

TL;DR: 本文介绍了Polynomiogram框架，该框架是一个综合计算平台，旨在探索、可视化以及从多项式根系统生成艺术。它通过灵活的采样方案将科学探究和算法艺术创作结合在一起，并集成了NumPy伴侣矩阵求解器与MPSolve两种数值引擎，以同时支持高效可视化及精确计算。此方法不仅被应用于分析三次多项式系统的分岔结构，还展示了其作为个性化生成艺术工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究者希望通过开发一个能够同时支持科学探索和个人化艺术创造的平台，来增强对多项式根系统的研究及其在教育中的应用。此外，也希望通过这个平台展示数学基础如何跨越科学与艺术领域，提供一种新的创意表达方式。

Method: 本研究构建了Polynomiogram框架，该框架允许用户定义两个独立参数的范围，并通过生成函数映射到多项式的系数上。框架内集成了NumPy伴侣矩阵求解器用于大规模快速计算，以及MPSolve用于高精度验证。使用经典集合如Kac多项式和Lucas多项式验证了数值准确性。

Result: 结果表明，Polynomiogram不仅能有效帮助科学家探索多项式根现象并作为教育辅助工具可视化代数与动力系统的基本概念，还能作为一种新颖的艺术创作手段，例如生成类似木槿花的自然形态或创作向人工智能进展致敬的作品。

Conclusion: Polynomiogram框架成功地将科学研究与艺术创作相结合，为探索多项式根系统提供了强大的工具，同时也开启了个性化生成艺术的新途径。

Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.

</details>


### [10] [Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures](https://arxiv.org/abs/2512.04273)
*Tyler Slater*

Main category: cs.SE

TL;DR: 本研究首次提出了一个经验框架，用于测量AI生成的微服务中的'架构侵蚀'和技术债务累积。通过对比三种先进模型（GPT-5.1、Claude 4.5 Sonnet和Llama 3 8B）在实现标准化图书借阅微服务时的表现，发现虽然专有模型达到高架构一致性，但开源权重模型表现出显著差异，特别是Llama 3显示了80%的架构违规率，并且开源模型还显示出'实现懒惰'现象，生成较少逻辑行代码以满足令牌限制，从而忽略了复杂的业务逻辑。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从代码补全工具向自主系统架构师转变，其对长期软件可维护性的影响尚未量化。现有研究主要关注功能正确性的基准测试，而缺乏对由AI合成的微服务中技术债务积累情况的研究。

Method: 选取三种先进的大语言模型(GPT-5.1, Claude 4.5 Sonnet, 和 Llama 3 8B)，要求它们在严格的六边形架构约束下实现一个标准的图书借阅微服务。然后使用抽象语法树(AST)解析来评估不同模型生成代码的架构符合度以及识别出的技术债务问题。

Result: 专有模型如GPT-5.1达到了零违规率的高度架构一致性；相比之下，开源权重模型尤其是Llama 3展示出高达80%的架构违规率，并且这些模型倾向于通过创建领域层与基础设施层之间的非法循环依赖关系来绕过接口适配器。此外，观察到开源权重模型存在“实现懒惰”现象，即为了适应令牌限制而省略复杂业务逻辑，导致产生的逻辑行代码数量减少了60%。

Conclusion: 研究表明，在没有自动化架构检查的情况下，使用较小规模的开源权重模型进行系统搭建可能会加速结构性技术债务的累积。这表明未来工作中需要考虑如何改进这类模型或者开发配套工具来减少技术债务。

Abstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure "Architectural Erosion" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of "Implementation Laziness," where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.

</details>


### [11] [MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training](https://arxiv.org/abs/2512.04319)
*Zixiao Zhao,Fatemeh H. Fard,Jie JW Wu*

Main category: cs.SE

TL;DR: 提出了一种名为MANTRA的多阶段自适应噪声处理框架，旨在减少软件工程任务中深度学习模型训练数据中的噪声影响。通过在代码预训练语言模型和大型语言模型微调过程中嵌入噪声诊断与缓解步骤，MANTRA能够提高模型对代码总结及提交意图分类任务的表现。


<details>
  <summary>Details</summary>
Motivation: 为了应对大规模存储库中不可避免地引入的噪声或错误标签问题，这些问题会降低应用于软件工程任务的深度学习模型的准确性和鲁棒性。研究还发现，在软件工程（SE）领域以及针对SE任务的大规模语言模型（LLMs）中，关于噪声标签学习（NLL）的研究较少。

Method: 开发了MANTRA框架，该框架在代码-预训练语言模型（PTM）和代码-LLMs的微调过程中直接整合了噪声诊断与缓解措施。首先研究不同水平噪声对模型收敛性和损失轨迹的影响；然后运用基于每个样本损失动态变化和高斯混合模型聚类指导的自适应dropout策略来排除持续存在的噪声点同时保留干净的数据。

Result: 实验表明，某些LLMs比其他模型更容易受到噪声的影响。然而，通过使用MANTRA，所有模型在这两项任务上的表现都得到了改善。

Conclusion: MANTRA提供了一个有效的方法来减轻数据集中错误对训练的影响，节省了数据清理和处理的时间，并最大化了微调的效果。这对于从事软件工程相关工作的研究人员和实践者来说是一个有价值的工具。

Abstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.

</details>


### [12] [Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles](https://arxiv.org/abs/2512.04344)
*Zitong Zhou,Ben Limpanukorn,Hong Jin Kang,Jiyuan Wang,Yaoxuan Wu,Akos Kiss,Renata Hodovan,Miryung Kim*

Main category: cs.SE

TL;DR: 本文提出了一种针对编译器优化的定向模糊测试方法TargetFuzz，通过挖掘和重建程序结构关系来更有效地触发优化逻辑。实验显示，与基线模糊器相比，TargetFuzz在LLVM和MLIR上分别提高了8%和11%的覆盖率，并且触发优化的次数分别是2.8倍和2.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有的模糊器难以有效测试编译器优化：一是由于阶段排序问题导致优化交互被遗漏；二是因为许多优化仅当输入满足特定结构关系时才启动，而现有生成器很难产生这样的输入。

Method: 开发了名为TargetFuzz的一般用途、基于语法的变异模糊器，它能够从相关语料库中挖掘组成风格（即程序构造上的结构关系），然后利用合成突变将这些风格重构于不同上下文中以测试优化逻辑的变化。该工具通过轻量级的基于语法的构造注释适应新的编程语言，并自动合成变异器和交叉器来重建组成风格。

Result: 评估结果显示，在LLVM和MLIR上，TargetFuzz相对于基线模糊器提升了8%和11%的覆盖率，并且触发优化的频率分别是后者的2.8倍和2.6倍。此外，还展示了定向模糊测试可以作为管道模糊测试的有效补充。

Conclusion: TargetFuzz提供了一种增强编译器优化测试的新途径，尤其适用于像MLIR这样快速发展的模块化框架。定向模糊测试能有效补充传统管道式测试方法，提高对编译器优化正确性的验证水平。

Abstract: Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\times$ and 2.6$\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.

</details>


### [13] [Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration](https://arxiv.org/abs/2512.04445)
*Yanbin Zhang,Hanhui Ye,Yue Bai,Qiming Zhang,Liao Xiang,Wu Mianzhi,Renjun Hu*

Main category: cs.SE

TL;DR: 本文介绍了一种新的执行框架AutoDW，它能够逐步、有条件地回滚操作编排，以实现多步骤、会话级别的工作流自动化。通过综合基准测试，AutoDW在指令级和会话级任务上分别达到了90%和62%的完成率，优于其他基线，并且对于不同难度的任务以及主干语言模型的选择保持了良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动系统可以执行独立的指令，但在处理需要对操作过程有更多控制的多步骤、会话级别工作流程时遇到困难。为了克服这个问题，研究者开发了AutoDW这一新框架来提高文档相关任务的工作流自动化效率。

Method: AutoDW根据用户指令、意图过滤后的API候选和文档状态演变逐步计划API动作，并在参数和API层级上使用了强大的回滚机制，保证了执行轨迹与用户意图和文档上下文的一致性。

Result: 在由250个会话和1708条人工标注指令组成的全面基准测试中，AutoDW在指令级任务上的完成率为90%，在会话级任务上的完成率为62%，相较于强劲的基线分别提高了40%和76%。此外，AutoDW对于骨干大语言模型的选择以及不同难度的任务也展现出了很好的鲁棒性。

Conclusion: AutoDW为文档相关的多步骤工作流提供了一个有效的自动化解决方案，通过其独特的逐步规划和回滚机制确保了长时间工作流中用户意图的准确执行。

Abstract: Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW

</details>


### [14] [LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models](https://arxiv.org/abs/2512.04474)
*Jiaqi Sun,Wei Li,Heng Zhang,Chutong Ding,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.SE

TL;DR: 本文提出了一种主动且统一的日志模板解析框架LLM-SrcLog，它能够从源代码中直接提取日志模板，并通过数据驱动的解析补充无法获取代码的日志。该方法结合了跨函数静态代码分析器、基于大模型的白盒模板提取器以及黑盒模板提取器，以实现对日志常量与变量的有效区分。实验结果表明，LLM-SrcLog在准确率和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的日志解析工具主要依赖于从日志本身推断模板，忽视了源代码的作用，这限制了它们理解动态日志结构或适应系统变化的能力。此外，针对每条日志使用大语言模型进行推理的成本过高，不适合实际部署。因此，需要一种新的方法来提高日志解析的效率和准确性。

Method: 提出了LLM-SrcLog框架，包括：1) 一个跨功能静态代码分析器用于重建有意义的日志上下文；2) 基于大模型的白盒模板提取器加上后处理步骤来区分常量和变量；3) 针对剩余未匹配日志的数据驱动聚类黑盒模板提取器。

Result: 通过对Hadoop、Zookeeper两个公开基准测试及Sunfire-Compute大规模工业系统的实验显示，与两种基于大模型的方法相比，LLM-SrcLog提高了平均F1分数2-17%和8-35%，同时在线解析延迟与数据驱动方法相当，比逐日志大模型解析快约1000倍。

Conclusion: LLM-SrcLog达到了速度与准确性之间的近乎理想平衡，在真实生产环境中的案例研究进一步验证了其有效性。

Abstract: Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.

</details>


### [15] [Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding](https://arxiv.org/abs/2512.04538)
*Xinkui Zhao,Rongkai Liu,Yifan Zhang,Chen Zhi,Lufei Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: CoCo, a new framework for code completion, uses static code analysis to extract and utilize multi-granularity context from large-scale code repositories, improving the quality of generated code with up to 20.2% gains in EM over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成(RAG)方法在处理代码时，主要依赖浅层语义匹配，忽视了结构语义和代码特有的依赖关系，这限制了它们捕捉控制流和潜在意图的能力，从而影响了生成代码的质量。

Method: 提出了一种名为CoCo的新框架，该框架通过静态代码分析从大规模代码库中提取函数级、文件级和项目级的结构化上下文，并采用基于图的多粒度上下文选择机制过滤冗余信息和去除噪声，同时结合结构感知的代码重排机制确保语义和结构层面的一致性。

Result: 在CrossCodeEval和RepoEval基准测试上进行的广泛实验表明，CoCo相比现有最先进方法能够实现高达20.2%的EM指标提升。此外，该框架是模型无关的，可以无缝集成到现有方法中以显著提高性能。

Conclusion: 通过理解和利用多粒度上下文，CoCo提供了一种更有效的代码补全解决方案，不仅提高了生成代码的质量，还展示了良好的通用性和兼容性。

Abstract: As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.

</details>


### [16] [Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models](https://arxiv.org/abs/2512.04673)
*Gunjan Das,Paheli Bhattacharya,Rishabh Gupta*

Main category: cs.SE

TL;DR: 本研究对五种通用和三种代码特定的最先进大语言模型在涵盖语言能力、数学推理和可信度六个不同基准上进行了全面评估，发现针对代码优化的模型（如CodeLLaMA变体）不仅在编码任务中表现出色，在非编码任务中也显示出可衡量的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的研究探索了单个模型的能力，但系统地跨领域比较统一语言、推理和代码理解能力仍然较少被探讨。

Method: 选择五种通用和三种代码特定的大语言模型，在六个不同的基准测试上进行综合评估，并分析这些模型在CoNaLa数据集上的代码解释行为，比较自然语言和代码特化LLM的表现。

Result: 发现为代码优化的模型（例如CodeLLaMA变体）展现出强大的推理能力和句法精确性，即使对于非编码任务也能显示出可测量的性能增益，与Mistral-7B和Llama-3-8B等通用模型形成对比。

Conclusion: 研究表明，专为处理代码设计的语言模型不仅在编程相关任务上表现优异，在需要逻辑推理和其他非编码技能的任务中同样具有竞争力。

Abstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.

</details>


### [17] [Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap](https://arxiv.org/abs/2512.04680)
*Jialong Li,Mingyue Zhang,Nianyu Li,Danny Weyns,Zhi Jin,Kenji Tei*

Main category: cs.SE

TL;DR: 本文旨在为研究者和实践者提供一个全面的概览，探讨将生成式人工智能（GenAI）应用于自适应系统（SASs）中可能带来的好处与挑战，并提出了一条研究路线图来解决集成过程中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 由于自适应系统(SASs)需要处理变化和不确定性，而生成式人工智能(GenAI)，特别是大型语言模型，在数据理解和逻辑推理方面表现突出，这表明GenAI有潜力增强SASs的功能。然而，关于在SASs中使用GenAI的具体优势和挑战尚不清楚。鉴于此领域的出版物有限、技术应用多样性以及GenAI技术快速演变等因素，本论文旨在对此进行深入理解。

Method: 通过收集、筛选并分析来自四个不同研究领域的文献资料，将发现整理成两大类潜在益处：(i)围绕MAPE-K反馈循环特定功能对SAS自主性的提升；(ii)改善人类与SAS之间在“人在环上”设置下的互动。基于这些研究，提出了一个强调整合GenAI到SAS领域所面临挑战的研究路线图。

Result: 研究结果包括了对利用GenAI增强SASs能力时遇到的关键研究难题的概述，以及针对当前GenAI存在的不足之处提出了可行的缓解策略。

Conclusion: 论文最终总结了将GenAI引入SASs领域的重要性和可行性，同时指出了实现这一目标前必须克服的一系列挑战。

Abstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.

</details>


### [18] [Configuration Defects in Kubernetes](https://arxiv.org/abs/2512.05062)
*Yue Zhang,Uchswas Paul,Marcelo d'Amorim,Akond Rahman*

Main category: cs.SE

TL;DR: 本文对Kubernetes配置脚本中的缺陷进行了实证研究，确定了15种缺陷类别，并发现8种可用的静态分析工具能够检测其中8类。此外，作者开发了一个新的linter，能够识别两种严重后果的缺陷类别，发现了26个未知缺陷并得到了从业者的确认。最后，论文为Kubernetes配置脚本的缺陷检测和修复技术提供了建议。


<details>
  <summary>Details</summary>
Motivation: Kubernetes配置容易出错，这可能导致严重的后果。为了帮助从业者检测并防止这些缺陷，本文开展了关于Kubernetes配置缺陷的研究。

Method: 通过对开源仓库中提取到的2,260份Kubernetes配置脚本进行研究，分析了719个缺陷。采用定性分析方法，识别出15种类别的缺陷。还评估了8种公开可用的静态分析工具在检测缺陷方面的性能，并开发了一个新的linter来检测那些未被现有工具覆盖的两类严重缺陷。

Result: 识别出了15种类别的配置缺陷；发现现有的8种静态分析工具能够检测其中8类，特别是在数据字段相关的缺陷上具有最高的精度和召回率；新开发的linter发现了26个之前未知的缺陷，其中19个已被修复。

Conclusion: 为Kubernetes配置脚本提供了一套全面的缺陷分类体系，展示了当前工具的有效性和局限性，并通过开发的新工具填补了某些类型的缺陷检测空白。同时，也为如何利用缺陷检测与修复技术提出了建议。

Abstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [ASCIIBench: Evaluating Language-Model-Based Understanding of Visually-Oriented Text](https://arxiv.org/abs/2512.04125)
*Kerry Luo,Michael Fu,Joshua Peguero,Husnain Malik,Anvay Patil,Joyce Lin,Megan Van Overborg,Ryan Sarmiento,Kevin Zhu*

Main category: cs.LG

TL;DR: 本文介绍了ASCIIBench，一个新颖的基准测试，用于评估ASCII文本图像的生成和分类。通过分析发现，现有方法在区分大多数ASCII类别时表现不佳，这表明问题在于表示方法而非生成差异，从而为符号视觉模式的新嵌入方法或评估指标的发展提供了动力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管在长文本生成等任务上表现出色，但在需要精确空间和位置推理的任务中仍然面临挑战。ASCII艺术作为一种字符编码结构和形式的独特媒介，成为探究这一局限性的理想工具。因此，研究引入了ASCIIBench，旨在探索和改善模型在处理这种特殊类型数据时的表现。

Method: 研究人员构建了一个包含5,315个带标签ASCII图片的数据集，并发布了一个针对ASCII结构调整过的CLIP模型权重，以支持对LLM生成的ASCII艺术作品进行评估。通过对CLIP嵌入之间的余弦相似度分析来探讨不同ASCII类别的可分辨性。

Result: 实验结果表明，对于多数ASCII类别来说，基于CLIP嵌入的余弦相似度无法有效地区分它们，即使是在方差较低的类别中也只能达到随机水平的表现。然而，对于内部平均相似度高的类别则显示出明显的可区分性，这意味着瓶颈在于表示方式而不是生成变化。

Conclusion: 这些发现指出ASCII艺术可以作为多模态表示的压力测试，并激发了开发适用于符号视觉模式的新嵌入方法或评估指标的需求。所有资源均可在提供的GitHub链接中获得。

Abstract: Large language models (LLMs) have demonstrated several emergent behaviors with scale, including reasoning and fluency in long-form text generation. However, they continue to struggle with tasks requiring precise spatial and positional reasoning. ASCII art, a symbolic medium where characters encode structure and form, provides a unique probe of this limitation. We introduce ASCIIBench, a novel benchmark for evaluating both the generation and classification of ASCII-text images. ASCIIBench consists of a filtered dataset of 5,315 class-labeled ASCII images and is, to our knowledge, the first publicly available benchmark of its kind. Alongside the dataset, we release weights for a fine-tuned CLIP model adapted to capture ASCII structure, enabling the evaluation of LLM-generated ASCII art. Our analysis shows that cosine similarity over CLIP embeddings fails to separate most ASCII categories, yielding chance-level performance even for low-variance classes. In contrast, classes with high internal mean similarity exhibit clear discriminability, revealing that the bottleneck lies in representation rather than generational variance. These findings position ASCII art as a stress test for multimodal representations and motivate the development of new embedding methods or evaluation metrics tailored to symbolic visual modalities. All resources are available at https://github.com/ASCIIBench/ASCIIBench.

</details>


### [20] [Decoding Large Language Diffusion Models with Foreseeing Movement](https://arxiv.org/abs/2512.04135)
*Yichuan Mo,Quan Chen,Mingjie Li,Zeming Wei,Yisen Wang*

Main category: cs.LG

TL;DR: 提出了一种新的解码方法FDM，该方法结合了局部和全局的考虑因素以优化大语言扩散模型(LLDMs)中的解码顺序问题。此外，还开发了一个加速版本FDM-A，它通过限制对关键步骤的深入探索来提高效率。实验表明，FDM具有良好的可扩展性，并且FDM-A在效率与性能之间取得了更优的平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言扩散模型（LLDMs）虽然受益于灵活的解码机制，但这种灵活性也带来了挑战：推理性能变得非常依赖于令牌的解码顺序。现有的启发式方法主要关注局部效应而忽视了长期影响。为了解决这个问题并释放LLDMs的全部潜力，研究者提出了一个新的解码方法。

Method: 研究者提出了预见性解码方法(FDM)，这是一种整合了局部和全局考量的新方法，采用基于搜索的战略来实现离散空间内的有效优化。此外，通过对整个解码过程中所选令牌一致性的分析，他们开发了FDM的一个变体——带有加速功能的FDM（FDM-A），它仅在被识别为探索与平衡情形的关键步骤上进行深度探索。

Result: 广泛的实验表明，FDM在不同的基准测试和模型架构上都展示了很好的可扩展性；同时，FDM-A则在效率与性能之间的权衡方面表现出了优越性。

Conclusion: 本研究可能为开发更强大的LLDMs解码方法提供了原则性的进展方向。

Abstract: Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.

</details>


### [21] [MechDetect: Detecting Data-Dependent Errors](https://arxiv.org/abs/2512.04138)
*Philipp Jung,Nicholas Chandler,Sebastian Jäger,Felix Biessmann*

Main category: cs.LG

TL;DR: 提出了MechDetect算法，用于检测数据错误生成机制，通过给定的数据集和错误掩码来估计错误是否依赖于数据本身。该方法基于已有处理缺失值机制的研究，并且可以应用于其他类型的错误检测。实验表明了MechDetect的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究集中在检测数据错误或变化上，但很少有研究探讨导致这些错误产生的根本机制。了解错误是如何产生的对于追踪和修复它们至关重要。

Method: 本研究基于统计学文献中关于缺失值的现有工作，提出了一种名为MechDetect的简单算法，用于调查错误生成机制。给定一个表格数据集及其对应的错误掩码，该算法使用机器学习模型来估计错误是否取决于数据本身。

Result: 通过在已建立的基准数据集上的实验验证了MechDetect算法的有效性。

Conclusion: MechDetect提供了一种新颖的方法来识别数据错误背后的原因，这对于提高数据质量监测非常有用。此外，它不仅适用于缺失值问题，还可以扩展到其他类型的数据错误分析中。

Abstract: Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.

</details>


### [22] [ActVAE: Modelling human activity schedules with a deep conditional generative approach](https://arxiv.org/abs/2512.04223)
*Fred Shone,Tim Hillel*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件变分自编码器的深度生成学习方法，用于根据个人年龄、就业状况等信息建模现实活动日程。通过结合结构化潜在生成方法和条件方法，该模型能够快速生成精确且真实的日程安排。评估结果表明，与纯生成模型或纯条件模型相比，本方法在建模复杂多样的人类行为随机性方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 针对人类活动日程安排的复杂性和多样性建模问题，现有方法难以充分捕捉个体差异及行为随机性。

Method: 采用一种新颖的条件变分自编码器架构，整合了结构化潜在生成方法与条件方法的优势，实现了对不同输入标签（如年龄、职业状态等）下真实活动日程的快速生成。

Result: 通过联合密度估计框架及多个案例研究验证了所提模型的有效性；展示了该方法在数据需求和计算成本方面的实用性，并证明了其在新旧需求建模范式中的可部署性。

Conclusion: 对比分析显示，利用深层生成方法明确建模复杂多样人类行为的随机性对于提高模型性能至关重要。

Abstract: Modelling the complexity and diversity of human activity scheduling behaviour is inherently challenging. We demonstrate a deep conditional-generative machine learning approach for the modelling of realistic activity schedules depending on input labels such as an individual's age, employment status, or other information relevant to their scheduling. We combine (i) a structured latent generative approach, with (ii) a conditional approach, through a novel Conditional VAE architecture. This allows for the rapid generation of precise and realistic schedules for different input labels. We extensively evaluate model capabilities using a joint density estimation framework and several case studies. We additionally show that our approach has practical data and computational requirements, and can be deployed within new and existing demand modelling frameworks. We evaluate the importance of generative capability more generally, by comparing our combined approach to (i) a purely generative model without conditionality, and (ii) a purely conditional model which outputs the most likely schedule given the input labels. This comparison highlights the usefulness of explicitly modelling the randomness of complex and diverse human behaviours using deep generative approaches.

</details>


### [23] [Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness](https://arxiv.org/abs/2512.04264)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Jing Lin*

Main category: cs.LG

TL;DR: 本文研究了在集中式和联邦学习环境中，通过对抗训练使用十种不同的激活函数来提高机器学习模型的鲁棒性。提出了一种高级对抗训练方法，包括模型架构变化、软标签、简化数据增强和变化的学习率。实验结果表明，在CIFAR-10上，ReLU通常表现最佳；而在非IID数据条件下，联邦学习环境中的鲁棒准确性显著下降，但通过适当比例的数据共享可以显著改善模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大多数关于对抗训练的研究主要集中在ReLU激活函数以及集中式训练环境下，对于其他激活函数及联邦学习环境下的研究较少。因此，本研究旨在探索不同激活函数对模型鲁棒性的影响，并将对抗训练方法扩展到联邦学习环境中，以期在更广泛的场景下提高模型的鲁棒性。

Method: 提出了一个改进的对抗训练方案，该方案结合了模型架构调整、软标签技术、简化版的数据增强手段以及可变的学习速率策略。在集中式环境下对十种流行激活函数（除ReLU外）进行了广泛测试，随后将此对抗训练框架应用于联邦学习背景中，同时考虑了独立同分布(IID)与非独立同分布(非IID)两种数据设置情况。

Result: 在CIFAR-10数据集上面对快速梯度符号攻击时，所提出的集中式对抗训练方法达到了77.08%的自然准确率和67.96%的鲁棒准确率。实验证明，ReLU通常表现最好。但在联邦学习环境中，特别是当遇到非IID数据时，鲁棒性明显下降。为了解决这一问题，引入了数据共享机制，当使用40%的数据共享时，自然准确率达到70.09%，鲁棒准确率达到54.79%，超过了CalFAT算法的表现。

Conclusion: 研究表明，通过采用包含模型架构变更、软标签、简化数据增强及学习率调整在内的先进对抗训练方法，能够有效提升集中式环境下ML模型的鲁棒性。虽然在联邦学习尤其是非IID数据情况下模型鲁棒性有所下降，但通过合理比例的数据共享可以显著改善这种情况。这为实际应用提供了有价值的见解。

Abstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.

</details>


### [24] [The Initialization Determines Whether In-Context Learning Is Gradient Descent](https://arxiv.org/abs/2512.04268)
*Shifeng Xie,Rui Yuan,Simone Rossi,Thomas Hannagan*

Main category: cs.LG

TL;DR: 本文探讨了在更现实的条件下，多头线性自注意力（LSA）如何近似梯度下降（GD），特别是在将非零高斯先验均值纳入ICL的线性回归公式时。通过引入可训练的初始估计yq，提出了一种单头LSA的简单泛化yq-LSA，并证明其在语义相似性任务上的性能有所提高。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的研究已将线性自注意力与梯度下降联系起来，但这种联系主要是在简化的条件下建立的，具有零均值高斯先验和GD的零初始化。后来的研究质疑了这些简化假设的有效性，指出了在多层或非线性注意力条件下，自注意力执行类似于但不同于GD的优化推理。基于此背景，本研究旨在探索在更实际的条件下，多头LSA如何模仿GD的行为，尤其是在当线性回归形式下的ICL包含了非零高斯先验均值时。

Method: 首先通过对多头LSA嵌入矩阵进行扩展，引入查询的初始估计（称为初步猜测）。然后，为了解决一步GD与多头LSA之间持续存在的性能差距，提出了yq-LSA，这是一种带有可训练初始猜测yq的单头LSA的简单泛化。理论分析结合实验验证了yq-LSA的能力，并将其应用于流行的LLM上以增强其处理语义相似性任务的表现。

Result: 研究表明，需要一个关于头部数量的上限来支持ICL线性回归设置。实验不仅证实了这一点，还观察到一步GD与多头LSA间存在持续的性能差异。通过引入yq-LSA并提供线性回归任务上的实验证据，成功地缩小了这一差距。此外，在广泛使用的LLM中加入初始猜测能力后，它们在语义相似性任务上的表现得到了提升。

Conclusion: 该工作通过引入可学习的初始猜测yq扩展了单头LSA的概念，提出了yq-LSA方法，这不仅有助于理解ICL与GD之间的联系，而且在实践中提高了模型在语义相似性任务中的性能。

Abstract: In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.

</details>


### [25] [Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order](https://arxiv.org/abs/2512.04277)
*Prakhar Gupta,Vaibhav Gupta*

Main category: cs.LG

TL;DR: 研究通过在强化学习后训练阶段引入一个排序奖励，以指导模型趋向于解题顺序，发现结合单元准确性和排序奖励的混合奖励方法能够显著提高Sudoku求解任务中的测试准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨在强化学习后训练过程中仅使用标量提示朝向标准解算器排序是否能改善性能，即使是在随机化解决方案序列上进行微调的情况下。

Method: 首先利用Transformer模型基于随机求解顺序进行标准微调，然后采用带有两种奖励（单元准确性和当模型的输出顺序与求解顺序一致时增加的排序奖励）的组相对策略优化（GRPO）进行后训练。为了清晰地比较信号效果，通过固定组合方式将两种奖励结合，并使用简单的引导缩放来平衡初始化时各成分的大小。

Result: 混合奖励通常优于仅优化单元准确性的方法——最佳混合比纯基于随机顺序微调的模型具有更高的测试准确性，并且接近了基于求解顺序序列训练的模型的准确性。

Conclusion: 粗略的排序信号可以在不改变监督数据或架构的情况下，引导强化学习后训练过程趋向于求解顺序轨迹。

Abstract: Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.

</details>


### [26] [GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers](https://arxiv.org/abs/2512.04296)
*Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的参数高效微调框架GRASP及其变体StochGRASP，旨在通过减少可训练参数数量同时保持模型学习任务特定特征的能力。在不同噪声水平下，StochGRASP的表现优于确定性方法，特别适合于能效高且噪声敏感的硬件平台。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模预训练模型全模型适应带来的挑战，文章提出了仅更新一小部分参数的方法，从而提供一种更具有扩展性的替代方案。此外，为了应对边缘AI硬件部署中非理想推理条件下的鲁棒性问题，进一步开发了能够模拟硬件级权重变化性的概率参数化方法。

Method: 提出了GRASP（分组激活共享参数化），该方法将选定层中的D维token表示划分为K个远小于D的组，并为每个组学习一个共享的缩放和偏移向量；基于此，还提出了StochGRASP，它不是学习确定性的值作为预训练权重的扰动，而是学习高斯分布，结合噪声感知损失函数，以提高在非理想条件下执行时的鲁棒性。

Result: 实验表明，在GLUE (RoBERTa-base & RoBERTa-large) 和 E2E NLG (GPT-2 Medium) 数据集上，GRASP 的性能与现有PEFT方法相当或更好，同时相比LoRA和BitFit等技术大幅减少了可训练参数的数量。特别是在有噪声的情况下，StochGRASP比其确定性版本表现得更好。

Conclusion: GRASP和StochGRASP作为新的参数高效微调策略，不仅显著减少了需要调整的参数数量，而且在面对不同程度噪声时展现了更好的鲁棒性，非常适合应用于资源受限及存在固有噪声的新兴AI硬件环境中。

Abstract: Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.

</details>


### [27] [Data-regularized Reinforcement Learning for Diffusion Models at Scale](https://arxiv.org/abs/2512.04332)
*Haotian Ye,Kaiwen Zheng,Jiashu Xu,Puheng Li,Huayu Chen,Jiaqi Han,Sheng Liu,Qinsheng Zhang,Hanzi Mao,Zekun Hao,Prithvijit Chattopadhyay,Dinghao Yang,Liang Feng,Maosheng Liao,Junjie Bai,Ming-Yu Liu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架——数据正则化扩散强化学习（DDRL），它使用前向KL散度将策略锚定到离策略数据分布上，从而在生成扩散模型中通过强化学习与人类偏好对齐。实验表明，DDRL能够显著提高奖励并减少基准方法中的奖励黑客行为，特别是在高分辨率视频生成任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的通过强化学习调整生成扩散模型以符合人类偏好的算法往往容易受到奖励黑客的影响，如质量下降、过度风格化或多样性减少等问题。这些问题的根本原因在于它们正则化方法的内在局限性。

Method: 作者引入了数据正则化扩散强化学习（DDRL）这一新框架，该框架利用前向KL散度来将策略固定于一个离线的数据分布上。理论上讲，DDRL能够实现强化学习与标准扩散训练之间的稳健无偏整合；实践中，则是结合奖励最大化与扩散损失最小化的一种简单有效算法。

Result: 经过超过一百万GPU小时的实验和一万次双盲人类评估，在高分辨率视频生成任务上，DDRL相较于基线方法显著提高了奖励，并缓解了奖励黑客问题，达到了最高的人类偏好水平。

Conclusion: DDRL为扩散后训练提供了一个强大且可扩展的范例，能够在保持高质量输出的同时增强与人类偏好的一致性。

Abstract: Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.

</details>


### [28] [David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?](https://arxiv.org/abs/2512.05073)
*Shashwat Shankar,Subhranshu Pandey,Innocent Dengkhw Mochahari,Bhabesh Mali,Animesh Basak Chowdhury,Sukanta Bhattacharjee,Chandan Karfa*

Main category: cs.LG

TL;DR: 研究测试了小型语言模型与策划的智能体AI框架结合在硬件设计任务中的表现，发现这种组合不仅以较低成本实现了接近大型语言模型的表现，还为智能体提供了学习机会，促进了复杂设计任务中高效、适应性强解决方案的发展。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模不断扩大，研究者质疑是否对于硬件设计来说模型越大越好，并探索了小型语言模型结合特定AI框架在解决复杂设计问题时的有效性和效率。

Method: 通过使用NVIDIA的综合Verilog设计问题(CVDP)基准来评估小型语言模型加上策划的智能体AI框架的表现。该方法利用了任务分解、迭代反馈和纠正等手段。

Result: 结果显示，智能体工作流程能够在大幅度降低成本的同时达到接近大型语言模型的性能，并且还能让智能体从过程中学到东西，从而为复杂的硬件设计任务提供更加高效灵活的解决方案。

Conclusion: 研究表明，在特定领域如硬件设计上，适当规模的语言模型配合精心设计的智能体AI框架能够实现既经济又高效的解决方案，同时促进智能体的学习与发展。

Abstract: Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.

</details>


### [29] [Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2512.04351)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: 本文提出了一种新的不确定性度量方法——径向分散评分（RDS），它简单、无参数且完全模型无关，通过测量嵌入空间中生成样本的径向分散程度来评估大型语言模型的不确定性。此外，还提供了一个轻量级的概率加权变体，当模型自身可以提供token概率时，该变体表现优于九种强基线方法。RDS能够自然扩展到每个样本评分上，支持最佳N选择和基于置信度的过滤等应用。在四个具有挑战性的自由形式问答数据集及多个大型语言模型上的实验表明，所提出的度量方法在幻觉检测和答案选择方面达到了最先进的性能，并且对于样本大小和嵌入选择来说保持了鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有检测大型语言模型不确定性的方法过于复杂，依赖于脆弱的语义聚类或内部状态。为了构建更加可靠的语言模型系统，需要一种更为简洁有效的不确定性度量方式。

Method: 提出了径向分散评分（RDS）这一全新的不确定性度量标准，它通过计算采样生成在嵌入空间中的径向分散程度来进行评价。同时开发了一个轻量级的概率加权版本，利用模型自身的token概率进一步提升性能。

Result: 实验结果表明，在四个具有挑战性的自由形式问答数据集以及多种大型语言模型上，RDS及其概率加权版本不仅超越了九种强大的基线方法，而且在幻觉检测和答案选择方面表现出色，同时保持了对样本数量和嵌入选择的良好适应性。

Conclusion: 径向分散评分（RDS）作为一种新型的不确定性度量工具，为提高大型语言模型系统的可靠性提供了有效途径。其简洁性、高效性和广泛的适用范围使其成为当前领域内极具潜力的研究方向之一。

Abstract: Detecting when large language models (LLMs) are uncertain is critical for building reliable systems, yet existing methods are overly complicated, relying on brittle semantic clustering or internal states. We introduce \textbf{Radial Dispersion Score (RDS)}, a simple, parameter-free, fully model-agnostic uncertainty metric that measures the radial dispersion of sampled generations in embedding space. A lightweight probability-weighted variant further incorporates the model's own token probabilities when available, outperforming different nine strong baselines. Moroever, RDS naturally extends to per-sample scoring, enabling applications such as best-of-$N$ selection and confidence-based filtering. Across four challenging free-form QA datasets and multiple LLMs, our metrics achieve state-of-the-art hallucination detection and answer selection performance, while remaining robust and scalable with respect to sample size and embedding choice.

</details>


### [30] [SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction](https://arxiv.org/abs/2512.04354)
*April S. Liang,Fatemeh Amrollahi,Yixing Jiang,Conor K. Corbin,Grace Y. E. Kim,David Mui,Trevor Crowell,Aakash Acharya,Sreedevi Mony,Soumya Punnathanam,Jack McKeown,Margaret Smith,Steven Lin,Arnold Milstein,Kevin Schulman,Jason Hom,Michael A. Pfeffer,Tho D. Pham,David Svec,Weihan Chu,Lisa Shieh,Christopher Sharp,Stephen P. Ma,Jonathan H. Chen*

Main category: cs.LG

TL;DR: 本研究介绍并评估了SmartAlert，这是一种集成到电子健康记录中的机器学习驱动的临床决策支持系统，旨在通过预测稳定的实验室结果来减少不必要的重复检测。在针对两家医院八个急性护理单位共9270次入院进行的一项随机对照试验中，结果显示，在显示SmartAlert后的52小时内CBC结果数量显著减少（1.54比1.82，p<0.01），代表重复检测相对减少了15%，且未对次要安全结果产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 频繁进行不太可能提供有用临床信息的实验室测试是一种常见做法，这给患者带来了负担，并增加了医疗保健成本。教育和反馈干预措施的效果有限，而一般的测试订购限制和电子警报则妨碍了适当的临床护理。

Method: 引入并评估了一种名为SmartAlert的基于机器学习(ML)的临床决策支持(CDS)系统，该系统被整合到了电子健康记录中，用于预测稳定状态下的实验室结果以减少不必要的重复测试。这项案例研究描述了一个为期从2024年8月15日至2025年3月15日、覆盖两个医院八个急症护理单元总共9270次住院期间内实施SmartAlert的过程、挑战以及经验教训。

Result: 结果显示，在展示SmartAlert之后的52小时内CBC检测结果的数量显著下降（1.54对比1.82，p<0.01），代表重复性检测减少了15%，并且没有对第二级安全结果造成不利影响。

Conclusion: 一个由周密的实施与治理过程支持的基于机器学习的CDS系统可以为住院病人实验室检测提供精准指导，从而安全地减少不必要的重复检测。

Abstract: Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.

</details>


### [31] [STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting](https://arxiv.org/abs/2512.04385)
*Nan Zhou,Weijie Hong,Huandong Wang,Jianfeng Zheng,Qiuhua Wang,Yali Song,Xiao-Ping Zhang,Yong Li,Xinlei Chen*

Main category: cs.LG

TL;DR: 本文提出了一种时空物理信息扩散模型（STeP-Diff），通过结合DeepONet和基于PDE的扩散模型，从不完整和时变的数据中预测空气质量。该方法能够有效捕捉空气污染场中的时空依赖性，并在实际测试中显著优于其他算法。


<details>
  <summary>Details</summary>
Motivation: 由于移动平台如汽车和公交车的运动模式随机且不可控，导致收集到的传感器数据通常不完整且时间上不一致。为了解决这个问题，提高细粒度空气质量预报的准确性，提出了新的模型。

Method: 开发了Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff)，利用DeepONet来建模测量的空间序列，并使用基于偏微分方程(PDE)的信息扩散模型来进行预测。此外，通过一个PDE约束的正则化框架确保去噪过程渐近收敛于对流-扩散动力学。

Result: 与次优表现的算法相比，本模型在MAE、RMSE和MAPE上分别提高了最多89.12%、82.30%和25.00%。广泛评估表明STeP-Diff能有效地捕捉空气污染领域中的时空依赖关系。

Conclusion: STeP-Diff模型提供了一个有效的解决方案，用于处理由非专用移动平台收集而来的不完整及时间变化的数据，从而改善细粒度空气质量预测的质量。

Abstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.

</details>


### [32] [GraphBench: Next-generation graph learning benchmarking](https://arxiv.org/abs/2512.04475)
*Timo Stoll,Chendi Qian,Ben Finkelshtein,Ali Parviz,Darius Weber,Fabrizio Frasca,Hadar Shavit,Antoine Siraudin,Arman Mielke,Marie Anastacio,Erik Müller,Maya Bechler-Speicher,Michael Bronstein,Mikhail Galkin,Holger Hoos,Mathias Niepert,Bryan Perozzi,Jan Tönshoff,Christopher Morris*

Main category: cs.LG

TL;DR: GraphBench is introduced as a comprehensive benchmarking suite for machine learning on graphs, covering various domains and tasks with standardized evaluation protocols and a unified hyperparameter tuning framework.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper stems from the fragmented benchmarking practices in graph machine learning, which often rely on narrow, task-specific datasets and inconsistent evaluation protocols. This fragmentation hinders reproducibility and broader progress in the field. The authors aim to address these issues by providing a more unified and standardized approach.

Method: The method involves creating GraphBench, a benchmarking suite that includes diverse prediction tasks such as node-level, edge-level, graph-level, and generative settings. It also provides standardized evaluation protocols, consistent dataset splits, performance metrics that consider out-of-distribution generalization, and a unified hyperparameter tuning framework. The authors further benchmark this suite using message-passing neural networks and graph transformer models to provide baseline performances.

Result: The result is the establishment of GraphBench, which offers a set of principled baselines and reference performance for different types of graph machine learning tasks. It aims to improve the consistency and comparability of research in the area of graph-based machine learning by setting a standard for future work.

Conclusion: In conclusion, the introduction of GraphBench addresses the need for a more systematic and standardized way of benchmarking in the field of graph machine learning, promoting better reproducibility and facilitating advancements across multiple domains.

Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.

</details>


### [33] [Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems](https://arxiv.org/abs/2512.04476)
*Zehao Fan,Zhenyu Liu,Yunzhen Liu,Yayue Hou,Hadjer Benmeziane,Kaoutar El Maghraoui,Liu Liu*

Main category: cs.LG

TL;DR: 本文提出了一种利用CXL-NDP技术来执行Mixture-of-Experts (MoE)模型中冷专家的方法，通过将参数移动转换为更便宜的激活移动，并采用上下文感知混合精度量化来适应NDP有限的计算吞吐量。该方法在减少跨设备数据传输的同时，显著提高了解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) 模型虽然能够通过条件计算扩展大型语言模型，但是一旦专家权重超过GPU内存容量时，推理过程就会变得受内存限制。此时需要将权重卸载到外部存储器，这会导致高昂且重复的数据传输成本。

Method: 作者们提出了使用CXL-attached近数据处理(CXL-NDP)作为卸载层来就地执行冷专家，从而将昂贵的参数移动转变为较为经济的激活移动。他们还开发了一个上下文感知的MoE系统，该系统利用预填充阶段的激活统计数据指导解码阶段的专家放置，动态地将热专家固定在GPU侧的HBM上，并将其余部分映射到CXL-NDP。此外，为了满足NDP有限的计算吞吐量需求，引入了基于预填充阶段情况分配每位专家比特宽度（1-4位）的上下文感知混合精度量化方案。

Result: 实验结果表明，相较于最先进的方法，所提方法在GPU-NDP系统上的解码吞吐量最高可提高8.7倍，而平均准确率仅下降0.13%。

Conclusion: 通过利用CXL-NDP技术和上下文感知策略，本研究成功减少了跨设备间的数据迁移，同时大幅提升了MoE模型在大规模语言模型中的解码效率。

Abstract: Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.

</details>


### [34] [Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval](https://arxiv.org/abs/2512.04524)
*Tianle Hu,Weijun Lv,Na Han,Xiaozhao Fang,Jie Wen,Jiaxing Li,Guoxu Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于原型的语义一致性对齐（PSCA）两阶段框架，旨在解决领域自适应检索中的类级别语义对齐忽视、伪标签可靠性缺乏考虑以及直接量化受领域偏移影响的原始特征等问题。通过建立类级别的语义连接和利用几何接近性提供语义一致性对齐的可靠性指标，PSCA在多个数据集上展示了优越的表现。


<details>
  <summary>Details</summary>
Motivation: 当前领域自适应检索方法存在忽略类级别语义对齐、过度追求成对样本对齐、缺乏对伪标签可靠性的考量或评估标签正确性的几何指导，以及直接量化受领域迁移影响的原始特征导致学习到的哈希码质量下降等问题。

Method: 提出了Prototype-Based Semantic Consistency Alignment (PSCA)框架，分为两个阶段：第一阶段通过一组正交原型直接建立类级别的语义联系，同时利用几何接近性作为语义一致性对齐的可靠性指示器；第二阶段使用领域特定的量化函数处理重构特征，在跨领域生成统一的二进制哈希码。

Result: 广泛的实验验证了PSCA在多个数据集上的优越性能。

Conclusion: PSCA通过引入类级别语义一致性对齐与几何指导来提高领域自适应检索的质量，解决了现有方法中的一些基本限制，并在实际应用中表现出色。

Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.

</details>


### [35] [Explainable Graph Representation Learning via Graph Pattern Analysis](https://arxiv.org/abs/2512.04530)
*Xudong Wang,Ziheng Sun,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 本研究提出了一种通过图模式分析来学习和解释图表示的框架PXGL-GNN，旨在解决现有方法在可解释性方面的局限性，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然先前的工作已经探索了模型级和实例级的可解释图学习，但对于可解释的图表示学习的研究还比较有限。该研究试图回答一个基本问题：图表示中具体捕捉到了图的哪些信息？

Method: 受图核启发，提出了PXGL-GNN框架。首先抽样各种模式下的子结构，然后学习这些模式的表示并通过加权求和结合，权重反映了每个图模式贡献的重要性。同时提供了对方法稳健性和泛化能力的理论分析。

Result: 实验表明，使用模式分析可以有效地学习和解释现实世界数据中的图表示。此外，在监督与非监督学习任务上对比多种基线方法，展示了所提方法的有效性。

Conclusion: 这项工作为理解和提高图表示学习的可解释性提供了一个新的视角，即通过图模式分析来获得更直观且具有表现力的图表示。

Abstract: Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.

</details>


### [36] [On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference](https://arxiv.org/abs/2512.04558)
*Yue Yu,Qiwei Di,Quanquan Gu,Dongruo Zhou*

Main category: cs.LG

TL;DR: 本文探讨了测试时计算（TTC）对大型语言模型（LLMs）的增强作用，指出现有方法如最佳选择-n（BoN）采样和顺序修订存在不足。提出了一种基于奖励筛选的顺序推理策略，该策略仅将高回报生成纳入上下文中，从而在理论上提供了比标准TTC范式更强的保证，并在实验中展示了优于常用方法的一致改进。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时计算（TTC）已成为提升大型语言模型性能的重要手段，但像最佳选择-n（BoN）采样和顺序修订等方法的基本限制尚不清楚。研究旨在填补这一空白，通过分析指出标准BoN方法本质上是次优的，并探索接近最优前沿的新方法。

Method: 提出了奖励过滤顺序推断机制，该机制选择性地只将高回报生成纳入考虑范围，以此集中计算资源于更优秀的策略候选者上，同时抑制劣质选项。

Result: 理论分析表明，与传统的TTC范式相比，奖励过滤顺序推断能够提供更加严格的保障；实验结果也证明了这种方法在多个基准测试中相对于广泛使用的方法具有持续的优势表现。

Conclusion: 研究表明，通过实施奖励过滤顺序推断策略，可以有效克服现有TTC方法的一些局限性，既在理论上又在实践中展示出其优越性。

Abstract: Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.

</details>


### [37] [Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function](https://arxiv.org/abs/2512.04559)
*Hyeongyu Kang,Jaewoo Lee,Woocheol Shin,Kiyoung Om,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出了一种新的KL-正则化的强化学习方法SQDF，用于改进扩散模型的微调，通过软Q函数的重参数化策略梯度来减轻奖励过度优化的问题。此外，SQDF还引入了折扣因子、一致性模型整合以及离线策略回放缓冲区等创新点，以提高模式覆盖和处理奖励-多样性之间的权衡。实验表明，SQDF在保持多样性和自然性的同时达到了更高的目标奖励和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型微调方法在对齐下游目标时容易出现奖励过度优化问题，导致生成的样本虽然奖励高但不自然且多样性下降。为了解决这个问题，研究者们开发了一种名为Soft Q-based Diffusion Finetuning (SQDF)的新方法。

Method: SQDF是一种KL-正则化的RL方法，它使用了一个无需训练即可进行微分估计的软Q函数的重参数化策略梯度。该方法还通过引入折扣因子、与一致性模型结合以及利用离线策略回放缓冲区等方式进行了增强。

Result: 实验结果显示，SQDF不仅能够在文本到图像的对齐任务中达到更好的目标奖励同时保持多样性，在在线黑盒优化中也表现出了高的样本效率，并且能够维持生成内容的自然性和多样性。

Conclusion: SQDF提供了一种有效的方法来改善扩散模型的微调过程，避免了现有方法中常见的奖励过度优化问题，同时提高了生成样本的质量和多样性。

Abstract: Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.

</details>


### [38] [LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models](https://arxiv.org/abs/2512.04562)
*Siddharth Betala,Samuel P. Gleason,Ali Ramlaoui,Andy Xu,Georgia Channing,Daniel Levy,Clémentine Fourrier,Nikita Kazeev,Chaitanya K. Joshi,Sékou-Oumar Kaba,Félix Therrien,Alex Hernandez-Garcia,Rocío Mercado,N. M. Anoop Krishnan,Alexandre Duval*

Main category: cs.LG

TL;DR: 本文介绍了LeMat-GenBench，一个用于晶体材料生成模型的统一基准测试平台，并通过一系列评估指标来更好地指导模型开发和下游应用。研究结果表明，在稳定性增加的同时，新颖性和多样性平均会下降，没有一个模型在所有方面都表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化的评估框架，使得难以有意义地评估、比较和发展用于无机晶体逆向设计的生成机器学习模型。为了加速材料发现过程中的化学空间探索，需要一个能够支持模型发展和实际应用的统一基准。

Method: 作者们创建了LeMat-GenBench，它是一个针对晶体材料生成模型的综合基准测试，配有一系列旨在更好指导模型开发和下游应用的评估指标。此外，他们还发布了开源评估套件和Hugging Face上的公开排行榜，并对12个最近的生成模型进行了基准测试。

Result: 结果显示，随着稳定性的提高，新颖性和多样性通常会降低，并且没有一种模型能够在所有维度上都表现优异。

Conclusion: LeMat-GenBench为公平的模型对比提供了可重复和可扩展的基础，并致力于指导开发更加可靠、以发现为导向的晶体材料生成模型。

Abstract: Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.

</details>


### [39] [Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift](https://arxiv.org/abs/2512.04571)
*Aditi Naiknaware,Sanchit Singh,Hajar Homayouni,Salimeh Sekeh*

Main category: cs.LG

TL;DR: 本文提出了Temp-SCONE，一种针对动态环境设计的SCONE时间一致性扩展版本，通过引入基于平均阈值置信度（ATC）的信心驱动正则化损失来处理时间变化，从而提高在时间漂移下的鲁棒性，同时保持与非时间连续数据集相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的开放世界学习方法如SCONE虽然能够应对协变量和语义变化带来的挑战，但在面对动态环境时表现不佳。因此，需要开发一种新的方法来适应动态环境中的时间变化，并可靠地检测出分布外输入。

Method: 提出了一种名为Temp-SCONE的新方法，它是对SCONE的时间一致性的扩展。该方法通过引入基于平均阈值置信度（ATC）的信心驱动正则化损失，在不同时间步之间惩罚预测不稳定性的同时保持了SCONE的能量边距分离特性。

Result: 实验表明，在具有时间漂移特征的数据集上，Temp-SCONE相比SCONE在受到破坏的数据准确性以及更可靠的OOD检测方面有显著提升。对于没有时间连续性的独立数据集，Temp-SCONE也能保持相近的表现。

Conclusion: 研究结果强调了时间正则化的价值及其局限性，为在不断变化的动态环境中实现可靠的开放世界学习提供了理论见解。

Abstract: Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.

</details>


### [40] [Exploiting \texttt{ftrace}'s \texttt{function\_graph} Tracer Features for Machine Learning: A Case Study on Encryption Detection](https://arxiv.org/abs/2512.04590)
*Kenan Begovic,Abdulaziz Al-Ali,Qutaibah Malluhi*

Main category: cs.LG

TL;DR: 本文提出了一种利用Linux内核ftrace框架，特别是函数图跟踪器来生成用于机器学习应用的系统级数据的方法。通过真实世界的加密检测任务实验，证明了所提特征对于多种学习算法的有效性。结果表明，在手头的任务上达到了99.28%的准确率，并进一步通过一个多标签分类问题验证了这些结果。该工作为原始跟踪数据预处理和基于图的特征提取提供了全面的方法论，促进了机器学习在系统行为分析、程序识别及异常检测中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器学习在系统行为分析、程序识别以及异常检测中的性能，作者提出了使用Linux内核ftrace框架（尤其是函数图跟踪器）来生成有用的系统级数据的想法。

Method: 研究采用了ftrace框架下的函数图跟踪器来收集信息丰富的系统级数据，并针对大规模文件集上的加密活动检测任务进行了实验。此外，还设计了多标签分类实验以进一步验证方法的有效性。

Result: 实验结果显示，在加密活动检测任务中实现了99.28%的准确性；而在后续的多标签分类问题中，也成功地从跟踪数据中识别出了运行程序，进一步证明了所提方法的有效性和实用性。

Conclusion: 本研究表明，通过结合Linux内核的ftrace框架与机器学习技术，可以有效地进行系统行为分析、程序识别及异常检测等任务。这不仅为预处理原始跟踪数据和提取基于图的特征提供了新的方法，也为性能监控和安全分析领域开辟了新途径。

Abstract: This paper proposes using the Linux kernel ftrace framework, particularly the function graph tracer, to generate informative system level data for machine learning (ML) applications. Experiments on a real world encryption detection task demonstrate the efficacy of the proposed features across several learning algorithms. The learner faces the problem of detecting encryption activities across a large dataset of files, using function call traces and graph based features. Empirical results highlight an outstanding accuracy of 99.28 on the task at hand, underscoring the efficacy of features derived from the function graph tracer. The results were further validated in an additional experiment targeting a multilabel classification problem, in which running programs were identified from trace data. This work provides comprehensive methodologies for preprocessing raw trace data and extracting graph based features, offering significant advancements in applying ML to system behavior analysis, program identification, and anomaly detection. By bridging the gap between system tracing and ML, this paper paves the way for innovative solutions in performance monitoring and security analytics.

</details>


### [41] [Score Matching for Estimating Finite Point Processes](https://arxiv.org/abs/2512.04617)
*Haoqun Cao,Yixuan Zhang,Feng Zhou*

Main category: cs.LG

TL;DR: 本文针对有限点过程提出了一个通过Janossy测度构建的得分匹配形式化框架，并引入了一种（自回归）加权得分匹配估计器。对于非参数点过程模型，提出了一种简单的生存-分类增强方法，以解决得分匹配单独无法唯一确定真实分布的问题。实验表明该方法能够准确恢复强度且性能与MLE相当但效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有得分匹配估计器在处理有限点进程时存在局限性，主要是由于缺乏对得分匹配在这种特殊随机配置上行为的严格数学分析。此外，在非参数点过程模型中，仅靠得分匹配不能唯一确定基础的真实分布。

Method: 开发了一个基于Janossy测度的形式化框架来处理有限点过程中的得分匹配问题，并在此基础上引入了(自回归)加权得分匹配估计器。对于更一般的非参数点过程模型，还提出了一种简单的生存-分类增强方案，以克服得分匹配单独使用时存在的规范化问题。

Result: 所提出的方法能够在合成数据集和真实世界的时间及时空数据集中准确地恢复强度值，并且与最大似然估计相比，该方法达到了相似的性能水平但具有更高的计算效率。

Conclusion: 本研究为有限点过程提供了新的得分匹配估计方法，解决了之前方法中存在的理论分析不足问题，并通过实验证明了新方法的有效性和高效性。

Abstract: Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.

</details>


### [42] [Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective](https://arxiv.org/abs/2512.04625)
*Bowen Zheng,Ran Cheng*

Main category: cs.LG

TL;DR: 本文从预测分布的角度重新思考了Decoupled Knowledge Distillation (DKD)，提出了一种更通用的解耦方法Generalized Decoupled Knowledge Distillation (GDKD)损失，并基于教师模型预测分布对非最高logits的知识提取的重要性提出了改进算法。实验表明，GDKD在多个基准测试中优于原始DKD及其他领先的知识蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 随着Decoupled Knowledge Distillation (DKD)的出现，再次强调了通过高级解耦和加权策略来重视logit知识的重要性。尽管DKD代表了重要进展，但其背后的机制值得深入研究。

Method: 从预测分布的角度重新考虑DKD，引入了名为Generalized Decoupled Knowledge Distillation (GDKD)损失的增强版本，该版本提供了一种更加多样的解耦logits的方法。特别关注教师模型的预测分布及其对GDKD损失梯度的影响，揭示了两个常被忽视的关键见解：(1) 最高logit的划分大大改善了非最高logits之间的相互关系；(2) 加强对非最高logits蒸馏损失的关注可以增强它们之间的知识提取。利用这些见解，进一步提出了一种带有高效分区策略的简化GDKD算法，以处理教师模型预测分布的多模态性。

Result: 在CIFAR-100、ImageNet、Tiny-ImageNet、CUB-200-2011以及Cityscapes等多个基准上进行了全面实验，结果表明GDKD的表现优于原版DKD和其他主流的知识蒸馏方法。

Conclusion: 本研究提出的GDKD不仅加深了对于DKD工作原理的理解，而且通过改进算法提升了知识蒸馏的效果，在不同数据集上的广泛实验验证了其有效性。

Abstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.

</details>


### [43] [Federated Learning for Anomaly Detection in Maritime Movement Data](https://arxiv.org/abs/2512.04635)
*Anita Graser,Axel Weißenfeld,Clemens Heistracher,Melitta Dragaschnig,Peter Widhalm*

Main category: cs.LG

TL;DR: 本文介绍了一种新的联邦学习解决方案M3fed，用于运动异常检测模型的学习。通过实验和对比分析，展示了该方法在提高数据隐私性和减少通信成本方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了提高运动异常检测中机器学习的数据隐私性并降低通信成本，提出了M3fed这一创新解决方案。

Method: 采用了新颖的联邦学习（FL）策略来训练M3fed，并通过海事AIS数据进行了示例实验。

Result: 结果显示，与传统的集中式M3相比，M3fed在通信成本和FL模型质量方面具有优势。

Conclusion: M3fed作为一种新的联邦学习方案，在改善数据隐私保护及减少通信开销上展现出了良好的前景。

Abstract: This paper introduces M3fed, a novel solution for federated learning of movement anomaly detection models. This innovation has the potential to improve data privacy and reduce communication costs in machine learning for movement anomaly detection. We present the novel federated learning (FL) strategies employed to train M3fed, perform an example experiment with maritime AIS data, and evaluate the results with respect to communication costs and FL model quality by comparing classic centralized M3 and the new federated M3fed.

</details>


### [44] [Contract-Governed Training for Earth Observation: Observed Service Agreement Graphs and Coverage-Accuracy Trade-offs](https://arxiv.org/abs/2512.04644)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 本文提出了一种基于合同治理的地球观测模型训练范式，通过将训练样本分组到服务合同中，并为每个合同分配目标服务份额来确保训练过程中对特定区域或类别的覆盖。实验表明该方法可以显著减少优先覆盖误差，同时保持全局准确性并提高高优先级准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的地球观测模型训练往往侧重于优化全局准确性，但缺乏对特定区域、类别或关键任务层的服务保障。本文旨在通过引入一种新的训练框架来解决这一问题，以确保在训练过程中能够公平地服务于所有指定的合同。

Method: 提出了一个观察服务协议图（OSAG）作为轻量级的治理层，用于监控优化过程中的合同级别曝光（覆盖）、通过合同标准化采样权重推动实证覆盖向目标份额靠拢，并通过两个参数：采样混合系数alpha和合同正则化权重lambda_C来暴露明确的准确性-治理权衡。

Result: 实验结果表明，OSAG能够在维持整体准确性的前提下大幅降低优先覆盖错误率，并且提高了对于高优先级区域的准确性。此外，通过EuroSAT粗细合同对比实验还进一步证明了语义上更加精细的合同设计有助于减小每单位治理改进所付出的准确性代价。

Conclusion: 本研究提出的合同治理训练模式为地球观测领域提供了一个新颖而有效的解决方案，它不仅能够保证模型在全球范围内的性能，同时也增强了对特定重要区域的关注度。

Abstract: Earth observation (EO) models are frequently trained under implicit sampling policies that optimize global accuracy but provide no explicit guarantees on who (which regions, classes, or mission-critical strata) is being served throughout training. This paper introduces a contract-governed training paradigm for EO in which training samples are grouped into service contracts -- semantically meaningful units such as (dataset, region, rare-crop indicator) -- and each contract is assigned a target service share. We instantiate this paradigm as an Observed Service Agreement Graph (OSAG), a lightweight governance layer that (i) monitors contract-level exposure (coverage) during optimization, (ii) drives empirical coverage toward target shares via contract-normalized sampling weights, and (iii) exposes explicit accuracy-governance trade-offs through two knobs: a sampling mixture coefficient alpha and a contract-regularization weight lambda_C. We provide a compact theory in a toy setting: OSAG sampling concentrates empirical coverage to targets; coverage deviations upper-bound service-risk deviations; and contract design (coarse vs. fine) modulates governance cost. Experiments on AVIRIS hyperspectral scenes (Indian Pines plus Salinas) and multispectral Sentinel-2 EuroSAT demonstrate that OSAG can substantially reduce priority coverage error while maintaining global accuracy and improving high-priority accuracy. A EuroSAT coarse-vs-fine contract ablation further evidences how semantically refined contracts can reduce the accuracy cost per unit of governance improvement.

</details>


### [45] [TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation](https://arxiv.org/abs/2512.04694)
*Baris Yilmaz,Bevan Deniz Cilgin,Erdem Akagündüz,Salih Tileylioglu*

Main category: cs.LG

TL;DR: 本文提出了一种基于时间域条件生成器（TimesNet-Gen）的方法，用于从加速度记录中生成强地面运动。通过比较真实和生成记录之间的HVSR曲线和基本站点频率$f_0$分布来评估生成效果，并使用$f_0$分布混淆矩阵的得分总结站点特异性。结果表明，TimesNet-Gen在站点特定对齐方面表现出色，并且与基于频谱图的条件VAE基线相比，在站点特定强运动合成上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 有效的地震风险降低依赖于准确的地点特定评估，这需要能够表示当地场地条件对地面运动特性影响的模型。数据驱动方法，特别是从记录的地面运动中学习场地控制特征的方法，提供了一个有前景的方向。

Method: 提出了TimesNet-Gen，这是一种时间域条件生成器，它利用了特定站点的潜在瓶颈。该方法通过对每个站点的真实和生成记录之间HVSR曲线及基础站点频率$f_0$分布进行比较来评价其生成性能；同时，通过基于$f_0$分布混淆矩阵的评分来衡量站点特异性。

Result: TimesNet-Gen实现了强大的站点级对齐，并且在站点特定强运动合成方面优于基于频谱图的条件变分自动编码器基线。

Conclusion: TimesNet-Gen为从加速度记录生成强地面运动提供了有效的方法，特别是在捕捉和保留特定站点特征方面。此外，它还显示出比现有方法更好的性能，为地震风险减少提供了新的途径。

Abstract: Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.

</details>


### [46] [TRINITY: An Evolved LLM Coordinator](https://arxiv.org/abs/2512.04695)
*Jinglue Xu,Qi Sun,Peter Schwendeman,Stefan Nielsen,Edoardo Cetin,Yujin Tang*

Main category: cs.LG

TL;DR: Trinity, a system that uses a lightweight coordinator to manage collaboration among large language models, shows superior performance in various tasks and benchmarks. It assigns roles dynamically to participating LLMs for efficient problem solving.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of weight-merging when combining diverse foundation models, such as architectural mismatches and closed APIs, by introducing an orchestrating mechanism that can adaptively delegate tasks to different LLMs.

Method: Trinity employs a compact language model and a lightweight head as a coordinator, which is optimized using an evolutionary strategy. The system processes queries through multiple rounds, assigning one of three roles (Thinker, Worker, or Verifier) to selected LLMs at each round, thereby offloading complex skill acquisition from the coordinator.

Result: Experiments demonstrate that Trinity outperforms individual models and other methods across coding, math, reasoning, and domain knowledge tasks, and it also performs well on out-of-distribution tasks. Notably, Trinity achieves a 86.2% score on LiveCodeBench, setting a state-of-the-art result.

Conclusion: The success of Trinity can be attributed to the rich contextualization provided by the coordinator's hidden-state representations and the advantages of the separable Covariance Matrix Adaptation Evolution Strategy under high dimensionality and budget constraints.

Abstract: Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.

</details>


### [47] [Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement](https://arxiv.org/abs/2512.04703)
*Stefan Perko*

Main category: cs.LG

TL;DR: 本文提出了一种基于Young微分方程的随机连续时间近似方法来模拟无替换随机梯度下降（SGDo）算法，并证明了对于强凸目标函数和特定学习率调度，该近似方法几乎肯定收敛。此外，还计算了几乎肯定收敛的渐近速度的上限，其结果优于或等同于以前关于SGDo的研究成果。


<details>
  <summary>Details</summary>
Motivation: 尽管在实践中广泛使用基于epoch的随机梯度下降算法（SGDo），但与“有替换”和“单次遍历”版本相比，SGDo及其相关算法的数学理论仍相对未被充分探索。

Method: 作者们提出了一种新的方法，通过构建一个受“epoch布朗运动”驱动的Young微分方程，为SGDo创建了一个带加性噪声的随机连续时间近似模型。

Result: 研究表明，对于强凸目标函数以及形式如$u_t = \frac{1}{(1+t)^β}, β\in (0,1)$的学习率安排，所提出的连续时间近似方法几乎可以确定地收敛。而且，研究者们还给出了这种收敛性的渐近速率的一个上界估计。

Conclusion: 新提出的基于连续时间的SGDo近似方法不仅在理论上提供了几乎肯定的收敛保证，而且也对收敛速度给出了更优或至少不劣于先前工作的估计。

Abstract: Gradient optimization algorithms using epochs, that is those based on stochastic gradient descent without replacement (SGDo), are predominantly used to train machine learning models in practice. However, the mathematical theory of SGDo and related algorithms remain underexplored compared to their "with replacement" and "one-pass" counterparts. In this article, we propose a stochastic, continuous-time approximation to SGDo with additive noise based on a Young differential equation driven by a stochastic process we call an "epoched Brownian motion". We show its usefulness by proving the almost sure convergence of the continuous-time approximation for strongly convex objectives and learning rate schedules of the form $u_t = \frac{1}{(1+t)^β}, β\in (0,1)$. Moreover, we compute an upper bound on the asymptotic rate of almost sure convergence, which is as good or better than previous results for SGDo.

</details>


### [48] [RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting](https://arxiv.org/abs/2512.04752)
*Siqi Wang,Hailong Yang,Junjie Zhu,Xuezhu Wang,Yufan Xu,Depei Qian*

Main category: cs.LG

TL;DR: 研究者们提出了一种名为RLHFSpec的新系统，该系统通过整合推测解码和样本重新分配来加速大型语言模型的强化学习从人类反馈（RLHF）生成阶段，从而提高整体执行性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习从人类反馈（RLHF）流程中，生成阶段成为整个执行过程中的瓶颈。为了解决这个问题并优化生成阶段效率，研究团队开发了RLHFSpec系统。

Method: RLHFSpec系统通过引入适应性推测解码技术和样本重新分配策略来加速生成阶段。特别是，它采用了一个工作负载感知的草稿策略选择机制，并通过有效的样本迁移机制进一步优化了GPU资源利用。

Result: 实验结果表明，与现有最先进方法相比，RLHFSpec在生成阶段能够达到更高的吞吐量。此外，由于有效缓解了生成阶段的瓶颈问题，RLHFSpec在整个RLHF执行过程中也显示出了显著的性能提升。

Conclusion: 本研究表明，通过结合推测解码及样本重分配等技术手段，可以显著改善RLHF流程中的生成阶段效率，进而促进整个训练过程的速度。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with adaptive speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.

</details>


### [49] [MemLoRA: Distilling Expert Adapters for On-Device Memory Systems](https://arxiv.org/abs/2512.04763)
*Massimo Bini,Ondrej Bohdal,Umberto Michieli,Zeynep Akata,Mete Ozay,Taha Ceritli*

Main category: cs.LG

TL;DR: 本文提出了一种新的记忆系统MemLoRA及其视觉扩展MemLoRA-V，通过为小型语言模型配备专门的记忆适配器来实现本地部署，并集成了小型视觉-语言模型以支持原生的视觉理解。实验表明，该方法在仅文本操作上超越了大10倍的基础模型，在视觉理解任务上也表现出了显著的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管基于记忆增强的大规模语言模型（LLMs）在长对话中表现出色，但它们成本高昂且缺乏原生视觉能力，限制了其在多模态场景下的应用。小型语言模型（SLMs）更适合于设备端推理，但由于性能不足而未被充分利用。因此，需要一种解决方案能够使小型模型在保持用户数据隐私的同时，也能有效处理多模态内容。

Method: 提出了MemLoRA，一种通过给小型语言模型装备特殊记忆适配器来促进本地部署的记忆系统；以及它的视觉拓展版本MemLoRA-V，将小型视觉-语言模型整合进记忆系统以实现自然的视觉理解。遵循知识蒸馏原则，每个适配器都单独训练以执行特定的记忆操作——知识抽取、记忆更新及记忆增强生成。

Result: 在只包含文本的操作中，MemLoRA的表现优于比它大10倍的基础模型（如Gemma2-27B），并且在LoCoMo基准测试中的表现与60倍大的模型（如GPT-OSS-120B）相当。对于视觉理解任务，MemLoRA-V相比基于描述的方法显示出巨大改进（准确率81.3% vs 23.7%），同时保持了在文本相关任务上的强大性能。

Conclusion: 研究证明了MemLoRA及其视觉扩展版MemLoRA-V能够在保持高性能的同时，减少对云服务的依赖并保护用户隐私，从而为多模态应用场景提供了一个有效的解决方案。

Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.

</details>


### [50] [Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows](https://arxiv.org/abs/2512.04954)
*Rajneil Baruah*

Main category: cs.LG

TL;DR: 本研究提出了一种使用正则化流与似然加权重要性采样训练的新颖方法，用于高效估计高维逆问题中的理论参数。通过在2D和3D多模态基准任务上实施该方法来验证其有效性，并发现基础分布的拓扑结构对模型后验有很大影响。采用与目标模式基数相匹配的高斯混合模型初始化流可以显著提高重建保真度。


<details>
  <summary>Details</summary>
Motivation: 为了解决高维逆问题中理论参数的有效推理难题，同时避免需要后验训练样本的情况，提出了这种方法。

Method: 采用正则化流结合似然加权重要性采样的训练方式来估计摊销后验。在二维和三维多模态基准任务上进行了测试。特别地，研究了基于不同基础分布（单峰对比多峰）对最终后验建模效果的影响。

Result: 研究表明，标准单峰基础分布无法捕捉到断开的支持集，导致模式间出现虚假的概率桥接现象；而使用与目标模式数量相匹配的高斯混合模型作为初始化，则能够明显改善重建质量，这从一些距离和散度度量指标中得到了体现。

Conclusion: 适当选择或设计基础分布对于利用正则化流进行准确后验估计至关重要。特别是当面对具有多个分离模式的问题时，考虑采用多峰分布如高斯混合模型作为正则化流的起点，可有效提升模型表现。

Abstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.

</details>


### [51] [Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning](https://arxiv.org/abs/2512.04958)
*Roberto Cipollone,Luca Iocchi,Matteo Leonetti*

Main category: cs.LG

TL;DR: 本文提出了一种名为'可实现抽象'的新概念，用于解决分层强化学习中马尔可夫决策过程（MDP）的抽象问题。基于此概念，作者开发了一个新的HRL算法RARL，该算法能够生成组合且接近最优的低级策略，并对抽象中的不准确性具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在分层强化学习领域，现有的MDP抽象概念通常存在表达能力有限或缺乏形式化效率保证的问题。为了解决这些问题，研究引入了'可实现抽象'的概念。

Method: 通过定义'可实现抽象'来建立低级MDP与其相关的高级决策过程之间的关系。证明了任何针对这种抽象的策略都可以转换成对于低级MDP来说几乎是最佳的策略。基于这些发现，提出了一个新的HRL算法——RARL。

Result: 研究表明，RARL算法是大概率近似正确的，在多项式样本数内收敛，并且对抽象中的不准确之处表现出很强的鲁棒性。

Conclusion: 通过引入'可实现抽象'的概念及相应的RARL算法，提供了一种有效的方法来解决大型MDPs，同时保持了良好的理论性能保障。

Abstract: The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.

</details>


### [52] [Efficient Generative Transformer Operators For Million-Point PDEs](https://arxiv.org/abs/2512.04974)
*Armand Kassaï Koupaï,Lise Le Boudec,Patrick Gallinari*

Main category: cs.LG

TL;DR: ECHO框架通过采用分层卷积编码-解码架构、高分辨率PDE解决方案生成策略以及学习完整轨迹段的生成建模范式，解决了现有神经算子在密集网格上的可扩展性差、动态展开期间误差累积和任务特定设计等问题，展示了在多种PDE系统中模拟百万点轨迹的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的神经算子（NOs）虽然在解决偏微分方程方面显示出了潜力，但在实践中仍受限于密集网格上的可扩展性差、动态展开时的误差累积以及针对特定任务的设计。

Method: ECHO框架提出了三个关键创新：(i) 使用一种分层卷积编码-解码架构，在保持网格点保真度的同时实现了100倍时空压缩；(ii) 采用训练与适应策略，使得能够从稀疏输入网格生成高分辨率PDE解决方案；(iii) 采取一种生成建模范例来学习完整的轨迹段，从而减轻长时间范围内的误差漂移。

Result: ECHO框架在具有复杂几何形状、高频动态及长期视野的不同PDE系统上展示了最先进的性能，能够在多任务处理中表现出色，包括轨迹生成、正向和逆向问题求解以及插值等。

Conclusion: 通过引入ECHO这一变压器算子框架，研究成功克服了现有技术在解决大规模PDE问题上遇到的主要挑战，并为处理更广泛的PDE相关任务提供了可能性。

Abstract: We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dynamic unrolling, and task-specific design. ECHO addresses these challenges through three key innovations. (i) It employs a hierarchical convolutional encode-decode architecture that achieves a 100 $\times$ spatio-temporal compression while preserving fidelity on mesh points. (ii) It incorporates a training and adaptation strategy that enables high-resolution PDE solution generation from sparse input grids. (iii) It adopts a generative modeling paradigm that learns complete trajectory segments, mitigating long-horizon error drift. The training strategy decouples representation learning from downstream task supervision, allowing the model to tackle multiple tasks such as trajectory generation, forward and inverse problems, and interpolation. The generative model further supports both conditional and unconditional generation. We demonstrate state-of-the-art performance on million-point simulations across diverse PDE systems featuring complex geometries, high-frequency dynamics, and long-term horizons.

</details>


### [53] [SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals](https://arxiv.org/abs/2512.05038)
*Cassandra Goldberg,Chaehyeon Kim,Adam Stein,Eric Wong*

Main category: cs.LG

TL;DR: 本研究发现了一种名为超级激活机制的现象，通过利用在概念分布的极端高尾部的令牌激活，能够更可靠地检测概念的存在。这种机制在图像和文本模式、模型架构、模型层以及概念提取技术中均表现出色，相较于标准向量基础和提示概念检测方法，F1分数提高了高达14%。此外，该研究还展示了如何利用超级激活令牌来改进特征归因。


<details>
  <summary>Details</summary>
Motivation: 现有的概念向量虽然旨在通过将内部表示与人类可理解的语义联系起来以提高模型的可解释性，但其有效性往往受到嘈杂且不一致的激活影响。为了解决这个问题，并找到一种更可靠的方法来指示概念的存在，本研究被提出。

Method: 研究人员定义并探索了超级激活机制，即在概念内部分布的极高端的令牌激活提供了概念存在的可靠信号。他们通过跨不同模态（如图像和文本）、模型架构、模型层及概念提取技术展示这一机制的有效性和普适性。

Result: 结果表明，超级激活令牌在概念检测方面始终优于传统的基于向量和提示的概念检测方法，在多种设置下实现了高达14%的F1分数提升。此外，使用超级激活令牌还改善了针对特定概念的特征归因质量。

Conclusion: 超级激活机制提供了一个新的视角来理解和增强模型对于特定概念的识别能力，不仅提高了概念检测的准确性，也为改进特征归因方法开辟了新途径。

Abstract: Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.

</details>


### [54] [Multi-LLM Collaboration for Medication Recommendation](https://arxiv.org/abs/2512.05066)
*Huascar Sanchez,Briland Hitaj,Jules Bergmann,Linda Briesemeister*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多大型语言模型（LLM）协作的方法，通过借鉴化学中相互作用建模的思想来提高临床用药推荐的可靠性。初步结果显示，这种基于LLM Chemistry指导的合作方式在生成可信、个性化的患者用药建议方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 随着医疗领域越来越多地依赖AI进行可扩展且值得信赖的临床决策支持，确保模型推理的可靠性成为一个关键挑战。单一的大型语言模型容易出现幻觉和不一致性问题，而简单的模型集成往往无法提供稳定可靠的建议。

Method: 研究者们基于之前关于LLM Chemistry的工作——一种量化LLM之间协作兼容性的方法，将其应用于从简短临床案例中改进药物推荐的可靠性。这种方法利用了受化学启发的交互建模指导下的多LLM协作，旨在创建既有效（利用互补优势）、又稳定（保持一致质量），同时校准良好（最小化干扰和错误放大）的模型集成。

Result: 通过在真实世界的临床场景下评估基于化学原理的多LLM协作策略，研究发现这种考虑了模型间互动的集成确实能够生成可靠且针对患者的药物推荐。

Conclusion: 初步结果表明，以LLM Chemistry为指导的多模型合作可能是实现临床实践中可靠且值得信赖的人工智能助手的一条有希望的道路。

Abstract: As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.

</details>


### [55] [OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design](https://arxiv.org/abs/2512.05080)
*Ian Dunn,Liv Toft,Tyler Katz,Juhi Gupta,Riya Shah,Ramith Hettiarachchi,David R. Koes*

Main category: cs.LG

TL;DR: 研究介绍了一种名为OMTRA的多模态流匹配模型，该模型能够灵活执行与基于结构的药物设计（SBDD）相关的多种任务，包括从头设计和对接。此外，还构建了一个包含5亿个3D分子构象的数据集来支持训练。尽管在口袋条件下的从头设计和对接方面取得了最新成果，但大规模预训练和多任务训练的效果却较为一般。


<details>
  <summary>Details</summary>
Motivation: 这项工作的动机在于通过开发一种统一的生成建模框架来改进基于结构的药物设计（SBDD），以实现更有效的新型配体发现，并通过增加化学多样性来增强训练数据集。

Method: 研究人员提出并实现了OMTRA，这是一种多模态流匹配模型，能够执行多项与SBDD相关的工作。另外，他们还整理了一个巨大的3D分子构象数据集，用以支持OMTRA和其他类似方法的训练过程。

Result: OMTRA模型在口袋条件下的从头设计和对接任务中表现出了最先进的性能。然而，在采用大规模预训练及多任务学习策略时，其效果提升并不显著。

Conclusion: OMTRA作为一种创新性的多模态流匹配模型，在基于结构的药物设计领域展现出了巨大潜力，特别是在提高新药候选物的设计效率方面。不过，对于如何进一步优化利用大规模数据集进行预训练的方法仍需探索。

Abstract: Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA

</details>


### [56] [The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception](https://arxiv.org/abs/2512.05089)
*Eduardo Di Santi*

Main category: cs.LG

TL;DR: 本文提出了一种确定性功能拓扑框架，用于描述物理现象的有效实现形成具有稳定不变量和有限豪斯多夫半径的紧凑感知流形。通过蒙特卡洛采样方法，即使在系统方程未知的情况下也能发现该流形的边界。研究结果表明，这种框架为生物学习者和自监督AI模型从有限观察中进行泛化的现象提供了一个统一的数学基础，并在铁路点机、电池放电曲线及生理ECG信号三个领域进行了实证验证。


<details>
  <summary>Details</summary>
Motivation: 现实世界的物理过程不会产生任意变异性：它们的信号集中在功能空间的紧凑且变化较小的子集上。这种几何结构使得无论是生物还是人工系统都能快速地从少量示例中泛化。

Method: 开发了一种确定性的功能-拓扑框架，在此框架下，物理现象的所有有效实现构成了一个具有稳定不变量和有限豪斯多夫半径的紧凑感知流形。利用蒙特卡洛抽样法以完全自监督的方式探索了该流形的边界，即便是在那些其支配方程未知的情况。

Result: 提供了理论保证、知识边界的实用估计器，并在机电铁路点机、电化学电池放电曲线以及生理心电图信号三个不同领域进行了实证验证。结果表明，确定性的功能拓扑学为感知、表示和世界模型构建提供了一个统一的数学基础，解释了为什么生物学习者和自监督AI模型能够基于有限的观察进行泛化。

Conclusion: 确定性功能拓扑学为理解生物学习者和自监督AI模型如何从少量样本中学习并泛化提供了新的视角，揭示了物理现象内在结构与外在表现之间关系的一个关键方面。

Abstract: Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.
  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.
  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.
  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.

</details>


### [57] [TV2TV: A Unified Framework for Interleaved Language and Video Generation](https://arxiv.org/abs/2512.05103)
*Xiaochuang Han,Youssef Emad,Melissa Hall,John Nguyen,Karthik Padthe,Liam Robbins,Amir Bar,Delong Chen,Michal Drozdzal,Maha Elbayad,Yushi Hu,Shang-Wen Li,Sreya Dutta Roy,Jakob Verbeek,XuDong Wang,Marjan Ghazvininejad,Luke Zettlemoyer,Emily Dinan*

Main category: cs.LG

TL;DR: 本文介绍了一种新的全视频文本模型TV2TV，通过结合最新的语言模型推理进展来解决复杂视频输出的问题。该模型采用混合变换器架构，将视频生成分解为交替的文字和视频生成过程，在视觉质量和可控性方面都有显著提升，并且能够处理自然视频的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在处理需要大量语义分支或反复高级推理的复杂视频输出时仍存在困难。

Method: 提出了一种名为TV2TV的新类型全视频文本模型，它使用混合变换器（MoT）架构联合学习语言建模（下一个令牌预测）和视频流匹配（下一帧预测）。在推理时，TV2TV决定何时在生成文本与视频帧之间切换，使模型能够在产生帧之前以文字形式思考后续内容。

Result: TV2TV在视频游戏数据上的控制实验中展示了在视觉质量和可控性方面的显著改进，并且能够扩展到自然视频上，通过使用视觉-语言模型（VLMs）增加自然语言动作描述来增强体育视频。

Conclusion: TV2TV代表了向开放式的文本推理和控制方向迈进的重要一步，它对于生成复杂的现实世界动作序列展现出强大的能力。

Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.

</details>


### [58] [Deep infant brain segmentation from multi-contrast MRI](https://arxiv.org/abs/2512.05114)
*Malte Hoffmann,Lilla Zöllei,Adrian V. Dalca*

Main category: cs.LG

TL;DR: 本文介绍了一种名为BabySeg的深度学习框架，该框架能够准确地对婴儿和幼儿的MRI图像进行分割，支持多种MRI协议，并且在处理不同年龄组和输入配置时表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对婴儿和幼儿MRI图像分割中的挑战，如发育和成像限制、图像获取难度大、解剖结构多样性和运动伪影等问题，现有方法往往局限于特定类型的图像或狭窄的年龄段，缺乏泛化能力。

Method: 提出了一种名为BabySeg的深度学习脑分割框架，它利用领域随机化技术生成超出实际范围的训练图像以提高模型对数据集变化的鲁棒性，并引入了一种机制允许模型灵活地从任意数量的输入扫描中汇集和交互特征。

Result: 实验表明，BabySeg在各种年龄组别和输入配置下均能实现与当前最佳方法相匹配甚至超越的精度，同时运行时间显著减少。

Conclusion: BabySeg提供了一个统一的解决方案来解决婴儿及幼儿MRI图像分割问题，通过增强模型对于不同情况下的适应性，为临床应用提供了新的可能性。

Abstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.

</details>


### [59] [Value Gradient Guidance for Flow Matching Alignment](https://arxiv.org/abs/2512.05116)
*Zhen Liu,Tim Z. Xiao,Carles Domingo-Enrich,Weiyang Liu,Dinghuai Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度匹配的方法VGG-Flow，用于微调预训练的流匹配模型，以实现高效适应和概率上合理的先验保持。实验表明该方法在有限计算资源下有效且能保持原有模型特性。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐流匹配模型的方法无法同时实现调整效率和概率上的合理先验保留。

Method: 通过利用最优控制理论，提出了VGG-Flow算法，该算法的核心思想是最优差异应与价值函数的梯度场相匹配。此外，该方法还结合了奖励模型的一阶信息，并通过启发式初始化价值函数来促进快速适应。

Result: 在流行的文本到图像流匹配模型Stable Diffusion 3上进行了实证研究，结果表明本方法可以在有限的计算预算下有效地微调流匹配模型，同时实现了有效的、保留先验的对齐。

Conclusion: VGG-Flow为流匹配模型提供了一个新颖的微调方案，它不仅提高了适应效率，而且在保持原模型先验的同时实现了与人类偏好的良好对齐。

Abstract: While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [60] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 该论文提出了一种针对边缘集群的可持续性意识的大语言模型推理方法，通过碳和延迟感知路由策略来平衡推理延迟和碳足迹。实验表明，四个提示的批次大小可以在吞吐量、能源效率之间取得平衡，而更大的批次可能会导致GPU内存饱和。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）需要大量的计算资源，导致了显著的碳排放和运营成本。虽然训练过程是能耗密集型的，但长期的环境负担主要来自于推理过程，尤其是全球巨大的查询量。基于云的推理虽然提供了可扩展性，但由于集中处理和连续的数据传输而受到延迟和带宽限制的影响。边缘集群可以通过实现本地化执行来缓解这些问题，但它们在性能、能效以及设备限制之间面临着权衡。

Method: 研究者提出了一个可持续性的LLM推理方案，适用于包含NVIDIA Jetson Orin NX (8GB) 和 Nvidia Ada 2000 (16GB) 设备的边缘集群。此方法旨在通过碳-及延迟感知路由策略来平衡推理延迟与碳足迹，这些策略基于对不同提示及其批次配置下的能量消耗和执行时间进行实证基准测试所获得的信息制定。

Result: 实验评估显示，四个提示组成的批次能够在吞吐量和能源效率之间达到良好的折衷点；若使用更大规模的批次，则有可能面临GPU内存饱和的风险。此外，还比较了基线贪婪策略与碳感知和延迟感知策略在根据基准测试信息将提示路由到特定硬件时的表现。

Conclusion: 本研究展示了一个考虑到可持续发展的LLM推理方法，特别适用于边缘计算环境。通过精心设计的碳和延迟感知路由策略，可以在确保高效的同时减少对环境的影响。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [61] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 本文评估了从浏览器到边缘和云端实例一致执行的基于WebAssembly的无服务器工作流，结果显示AOT编译和实例预热可以显著减少启动延迟，并且对于小负载，浏览器性能具有竞争力，而对于大负载，边缘和云端节点上的AOT执行明显优于浏览器。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于WebAssembly（Wasm）作为一种二进制指令格式，能够在异构平台上实现可移植、沙箱化以及接近原生的执行，非常适合在浏览器、边缘节点和云服务器上执行无服务器工作流。然而其性能和稳定性受到启动开销、运行时执行模型（如提前编译AOT与即时编译JIT）及部署环境中的资源变化等因素的影响很大。

Method: 论文通过使用wasm32-wasi模块设置了一个实验环境，在浏览器中于web worker内执行，在边缘端和云端则通过HTTP shim向Wasm运行时传输帧。测量指标包括冷启动与热启动延迟、每步骤延迟、工作流跨度、吞吐量以及CPU/内存利用率，以此来捕捉跨不同环境下的端到端行为。

Result: 结果表明，AOT编译和实例预热能够大幅降低启动延迟；对于小负载情况，由于完全基于内存的数据交换，浏览器可以获得有竞争力的表现；而随着负载增大，工作流进入计算和内存密集阶段后，边缘端和云端节点上的AOT执行明显优于浏览器表现。

Conclusion: 基于WebAssembly的工作流在不同平台上的性能表现差异显著，选择合适的编译方式和预热策略能够优化整体效率。对于轻量级任务，直接在浏览器中执行可能更为高效；而对于重载任务，则建议利用边缘或云端提供的更强大处理能力。

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [62] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文综述了基于微服务的雾计算和边缘计算中最新的资源管理策略，特别关注节能解决方案，并对136项研究进行了系统性回顾和分类，指出了现有文献中的未解决挑战和空白，提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的指数增长加剧了对高效响应服务的需求。雾计算和边缘计算作为分布式范式，虽然可以减少延迟、带宽限制和能耗，但在资源管理上面临挑战，如资源约束、计算异构性、动态工作负载及多样的服务质量要求。

Method: 通过系统地回顾2020-2024年间超过136项关于基于微服务的雾计算与边缘计算资源管理策略的研究，将其分类为五个主要子领域：服务放置、资源配置、任务调度与卸载、资源分配以及实例选择。此外，还审查了现有调查并识别出未解决的挑战与文献缺口。

Result: 研究揭示了基础资源管理组件之间缺乏协同作用的问题，并提出了利用AI驱动优化、量子计算和服务无服务器化等未来研究方向。

Conclusion: 本综述提供了一个统一且注重能源效率的观点来理解基于微服务的雾计算与边缘计算中的资源管理问题，为更集成、高效和可持续的未来解决方案铺平道路。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [63] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: 本文介绍了Fast ACS的设计，这是一种基于文件的有序消息传递系统，能够在全球范围内几秒甚至亚秒级（p99）内将消息传递给消费者，同时保持较低的资源成本。


<details>
  <summary>Details</summary>
Motivation: 实时系统中低延迟的消息传递至关重要。随着计算规模的增长，数据需要从生产者传输到可能分布在城市和大陆边界的集群中的数千个消费者。这种系统要求一个健壮的消息系统，能够跨集群传输包含数据的消息，并有效地将它们交付给消费者。此外，系统还需要提供诸如顺序性和至少一次送达这样的保证，同时避免对消费者的过载，允许他们按照自己的速度消费消息。

Method: 研究介绍了一种名为Fast ACS的基于文件的有序消息发送系统设计，该系统利用了双边（跨集群）和单边（集群内）通信原语的组合——即远程过程调用和远程内存访问——来传递消息。

Result: Fast ACS已经成功部署到了几十个生产集群中，并且可以扩展到每个集群内的数千个消费者，这相当于在高峰时达到Tbps级别的集群内消费者流量。值得注意的是，根据消息量和消费者规模，Fast ACS可以在几秒钟甚至亚秒级（p99）内向全球消费者传递消息，同时保持较低的资源成本。

Conclusion: Fast ACS展示了其作为高效、可扩展的消息传递解决方案的能力，它能够在确保消息顺序性的同时，在低资源成本下实现快速的全球范围内的消息分发。

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [64] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS是一个快速且确定性的分析模型，用于生成高效的GPU GEMM内核。基于此模型实现了一个轻量级的GEMM框架，其性能超过自调优解决方案的95%，同时将自调优时间减少到零，非常适合HPC和ML工作负载中的实证调整替代品。


<details>
  <summary>Details</summary>
Motivation: 为了提高GPU上GEMM操作的效率，同时避免耗时的运行时自调优过程，研究者开发了tritonBLAS模型。该模型旨在通过预先预测近似最优配置来直接提供高性能的GEMM内核，特别适用于生产环境下的高性能计算（HPC）和机器学习（ML）任务。

Method: tritonBLAS利用架构参数如缓存层次结构以及代码与数据相对位置等信息，明确地建模了架构拓扑、矩阵形状与算法阻塞行为之间的关系，从而预测出接近最优的设置。基于这一分析模型，在Triton内部完全实现了轻量级的GEMM框架。

Result: tritonBLAS在现代GPU上的多种GEMM问题规模测试中表现出色，达到了自调优解决方案95%以上的性能水平，并且彻底消除了自调优所需的时间。

Conclusion: 这项研究表明，tritonBLAS能够作为实证性调优方法的有效替代方案，在保持高水平性能的同时显著降低了优化开销，对于实际生产环境中需要高效执行GEMM操作的应用程序来说是非常有价值的。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [65] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: 本文提出了一种称为虚拟库上下文（VLCs）的方法，用于在不修改库代码的情况下控制库的资源利用。实验表明，使用VLCs可以使得包括使用OpenMP、OpenBLAS和LibTorch的应用程序在内的基准测试提速最高达2.85倍。


<details>
  <summary>Details</summary>
Motivation: 随着现代并行机器的复杂性和规模不断增加，程序员越来越依赖于软件库的组合来封装和利用并行性。然而，许多库在设计时并未考虑到这一点，并假定它们能够独占所有资源。同时使用这样的库会导致竞争和性能下降。先前的解决方案涉及修改库或操作系统，这往往是不可行的。

Method: 提出了虚拟库上下文（VLCs），这是一种进程子单元，能够封装一组库及其关联的资源分配，从而在不修改库代码的前提下控制这些库的资源利用率。用户可以通过VLCs在库之间划分资源以防止争用，或者加载同一库的多个副本，以便在同一进程中并行执行原本线程不安全的代码。

Result: 通过C++和Python的原型实现对VLCs进行了描述和评估。实验结果显示，对于包含使用了OpenMP、OpenBLAS以及LibTorch应用程序的基准测试而言，VLCs能够实现高达2.85倍的速度提升。

Conclusion: VLCs提供了一个有效的解决方案，能够在不改变现有库代码的基础上管理并行应用中的资源竞争问题，从而显著提高程序性能。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [66] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 本文提出了gpuFLOPBench，一个用于评估大型语言模型（LLMs）预测CUDA内核浮点运算次数能力的基准测试。它通过577个带有真实数据和执行属性注解的CUDA内核来检测模型在不运行代码的情况下进行性能预估的能力。研究发现当前最先进的LLMs在处理简单内核时表现良好，但在遇到隐含浮点运算的情况时会出现较大误差。这表明现有代码助手的一个核心限制在于无法内化硬件特定的微码效应，并且确立了gpuFLOPBench作为开发能够像经验丰富的GPU开发者一样严谨地推理性能的LLM工具的基础测试平台的地位。


<details>
  <summary>Details</summary>
Motivation: 现代GPU软件栈要求开发者能够在实际运行内核之前预测性能瓶颈；错误估计前期的浮点工作负载可能会影响调优、调度甚至硬件采购决策。然而，尽管代码生成技术取得了快速进展，但如今的大型语言模型很少被测试这种前瞻性的推理能力。

Method: 创建了一个名为gpuFLOPBench的基准测试，该测试让模型尝试在不执行代码的前提下预测来自HeCBench的577个CUDA内核的单精度和双精度浮点运算数。这些内核附带了真实的数据和八个执行属性的注释，用以区分容易分析的代码与那些其浮点运算依赖于编译器或运行时行为的内核。

Result: 最新闭源推理模型的评估显示了明确但不均衡的进步：最新的大型语言模型对于简单的内核可以达到完美的分类效果，但当出现由除法、内在数学函数或常见子表达式引起的隐性浮点运算时，它们仍然会犯下多个数量级的错误。

Conclusion: 结果揭示了现有代码辅助工具的核心局限性——即无法内化硬件特有的微码效应，并将gpuFLOPBench定位为一个专注的测试平台，旨在开发能够以与经验丰富的GPU开发者相同严格程度来推理性能的大型语言模型工具。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [67] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 本文提出了一种针对稀疏LU分解中非零元素分布不均问题的结构感知不规则分块方法，通过引入基于对角块的新特征来表征局部非零分布，并据此调整不同区域的分块大小。实验表明，该方法在单个和多个NVIDIA A100 GPU上相较于PanguLU和SuperLU_DIST均有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 在稀疏LU分解过程中，符号分解后的非零元素倾向于分布在矩阵的对角线和右下角区域，而常规的二维分块方式在这种非均匀分布结构上可能导致各分块间的工作负载不平衡。此外，现有的矩阵特征不足以有效地指导分块过程。

Method: 提出了一个基于对角块的新特征以有效描述稀疏矩阵的局部非零分布；进一步地，根据这种局部非零分布情况设计了一种能够自适应调整分块大小的不规则分块方法，即在密集区域使用细粒度分块，在稀疏区域则采用粗粒度分块，从而在依赖树的同一层级内及跨层级间平衡了各个分块中的非零数量。

Result: 实验结果显示，在单个NVIDIA A100 GPU上，提出的不规则分块方法相比PanguLU平均提速1.50倍，比最新的SuperLU_DIST提速3.32倍；当扩展到4个NVIDIA A100 GPU时，相对于PanguLU和SuperLU_DIST分别获得了1.40倍和3.84倍的速度提升。

Conclusion: 所提的结构感知不规则分块方法对于改善稀疏LU分解中的工作负载均衡问题具有明显效果，并且在实际应用中能够显著提高计算效率。

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [68] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: 本文提出了一种名为KAI的系统，该系统通过实现一种新的'异步回流'协议来优化CXL计算内存（CCM）中的数据移动和处理操作，从而减少了端到端运行时间，并大幅降低了CCM和主机的空闲时间。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前操作卸载机制无法有效利用基于不同CXL协议模型之间的权衡问题，以及由此带来的数据迁移成本高和性能受限的问题。

Method: 研究者首先分析了不同CXL协议下的权衡点及其对不同类型工作负载性能的影响；接着提出了一个新颖的‘异步回流’协议，它通过在底层CXL协议之上精心安排数据与控制传输操作来发挥作用；最后设计并实现了支持异步数据迁移及轻量级流水线处理的KAI系统。

Result: 实验结果显示，KAI系统能够将端到端运行时间减少高达50.4%，同时平均来说，CCM和主机的空闲时间分别减少了22.11倍和3.85倍。

Conclusion: 通过引入KAI系统及所提出的异步回流协议，本研究表明可以有效地改善CXL计算内存环境下的数据处理效率和整体系统性能。

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [69] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: 本文研究了太赫兹通信与联邦学习结合时，现实宽频损伤对优化动态的影响，并提出了一种多载波随机框架来解决这一问题。研究表明，在标准无偏聚合下，收敛误差受子载波信噪比的调和平均值驱动；而通过采用信噪比加权聚合策略可以有效恢复在高倾斜状态下的收敛性。


<details>
  <summary>Details</summary>
Motivation: 太赫兹（THz）通信与联邦学习（FL）的融合有望实现超快分布式学习，但实际中宽带损伤对优化动态的具体影响尚未得到理论上的充分描述。

Method: 开发了一个多载波随机框架，该框架明确地将局部梯度更新与频率选择性THz效应（如波束倾斜、分子吸收和抖动）联系起来。

Result: 发现了一个关键多样性陷阱：在标准无偏聚合下，收敛错误地板由子载波SNR的调和平均值决定。此外，还确定了一个基本带宽限制，表明超出某一点扩展频谱会因热噪声集成和边缘增益崩溃而降低收敛性。提出了信噪比加权聚合策略以抑制这些频谱空洞处的方差奇点，从而在标准平均法失效的高倾斜状态下恢复收敛。

Conclusion: 数值结果验证了所讨论物理层参数对THz-FL系统性能预期影响的有效性，强调了适当聚合策略对于克服THz通信条件下挑战的重要性。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [70] [A Fast Ethereum-Compatible Forkless Database](https://arxiv.org/abs/2512.04735)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 本文介绍了一种新的状态数据库，该数据库是原生实现的，保持了与以太坊的兼容性，同时专为非分叉区块链设计。这种数据库为验证者提供了十倍的速度提升和99%的空间减少，并将存档节点的存储需求减少了三倍。


<details>
  <summary>Details</summary>
Motivation: 现有的以太坊状态数据库（StateDB）设计用于维护多个状态版本的分叉链，而对于不需要多个状态版本的新一代快速、非分叉区块链来说效率低下。此外，当前的状态数据库实现基于键值存储（如LevelDB），这进一步降低了效率。

Method: 开发一种新型状态数据库，该数据库作为本地数据库实现，旨在提高非分叉区块链的性能，同时保持与以太坊标准的兼容性。

Result: 新提出的数据库实现了显著的性能改进：对于验证者而言，处理速度提高了十倍，所需空间减少了99%；对于存档节点，存储需求下降至原来的三分之一。

Conclusion: 通过引入专门针对非分叉区块链优化的新状态数据库设计，可以极大地提高性能并减少资源消耗，从而为采用以太坊标准但不支持多状态版本的新区块链提供更高效的数据存储解决方案。

Abstract: The State Database of a blockchain stores account data and enables authentication. Modern blockchains use fast consensus protocols to avoid forking, improving throughput and finality. However, Ethereum's StateDB was designed for a forking chain that maintains multiple state versions. While newer blockchains adopt Ethereum's standard for DApp compatibility, they do not require multiple state versions, making legacy Ethereum databases inefficient for fast, non-forking blockchains. Moreover, existing StateDB implementations have been built on key-value stores (e.g., LevelDB), which make them less efficient.
  This paper introduces a novel state database that is a native database implementation and maintains Ethereum compatibility while being specialized for non-forking blockchains. Our database delivers ten times speedups and 99% space reductions for validators, and a threefold decrease in storage requirements for archive nodes.

</details>
