<div id=toc></div>

# Table of Contents

- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1] [MusicSem: A Semantically Rich Language--Audio Dataset of Natural Music Descriptions](https://arxiv.org/abs/2602.17769)
*Rebecca Salganik,Teng Tu,Fei-Yueh Chen,Xiaohao Liu,Keifeng Lu,Ethan Luvisia,Zhiyao Duan,Guillaume Salha-Galvan,Anson Kahng,Yunshan Ma,Jian Kang*

Main category: cs.MM

TL;DR: 本文介绍了一个名为MusicSem的数据集，该数据集包含32,493个语言-音频对，来源于Reddit上的自然音乐讨论。它覆盖了更广泛的音乐语义，并提出了一个五类语义分类法来结构化这些表达。通过使用MusicSem评估多种多模态模型，强调了建模细粒度语义的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的音乐与文本之间的多模态学习模型难以捕捉用户用自然语言描述音乐时所表达的意图，这表明用于训练和评估这些模型的数据集未能完全反映人类用来描述音乐的更广泛和更自然的形式。

Method: 创建了MusicSem数据集，其中包含了从Reddit社交平台获取的32,493个语言-音频配对，并提出了一种包括描述性、氛围性、情境性、元数据相关性和背景性在内的五种语义类别的分类体系。

Result: MusicSem能够捕捉到更加宽泛的音乐语义范围，反映了听众以细微且以人为中心的方式自然地描述音乐的方式。此外，通过利用MusicSem对一系列多模态模型进行检索和生成任务的评估，突出了对精细语义建模的重要性。

Conclusion: MusicSem作为一个新的语义感知资源，支持未来关于人类对齐的多模态音乐表示学习的研究。

Abstract: Music representation learning is central to music information retrieval and generation. While recent advances in multimodal learning have improved alignment between text and audio for tasks such as cross-modal music retrieval, text-to-music generation, and music-to-text generation, existing models often struggle to capture users' expressed intent in natural language descriptions of music. This observation suggests that the datasets used to train and evaluate these models do not fully reflect the broader and more natural forms of human discourse through which music is described. In this paper, we introduce MusicSem, a dataset of 32,493 language-audio pairs derived from organic music-related discussions on the social media platform Reddit. Compared to existing datasets, MusicSem captures a broader spectrum of musical semantics, reflecting how listeners naturally describe music in nuanced and human-centered ways. To structure these expressions, we propose a taxonomy of five semantic categories: descriptive, atmospheric, situational, metadata-related, and contextual. In addition to the construction, analysis, and release of MusicSem, we use the dataset to evaluate a wide range of multimodal models for retrieval and generation, highlighting the importance of modeling fine-grained semantics. Overall, MusicSem serves as a novel semantics-aware resource to support future research on human-aligned multimodal music representation learning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [2] [When & How to Write for Personalized Demand-aware Query Rewriting in Video Search](https://arxiv.org/abs/2602.17667)
*Cheng cheng,Chenxing Wang,Aolin Li,Haijun Wu,Huiyun Hu,Juyuan Wang*

Main category: cs.IR

TL;DR: WeWrite, a new Personalized Demand-aware Query Rewriting framework, improves video search by addressing when and how to apply personalization, using a hybrid training approach, and ensuring low latency, resulting in enhanced user engagement and reduced query reformulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the effectiveness of video search systems by leveraging user historical behaviors more efficiently, overcoming the limitations of traditional methods that use implicit history features, such as signal dilution and delayed feedback.

Method: WeWrite addresses three main challenges: (1) determining 'When to Write' through an automated posterior-based mining strategy for high-quality sample extraction; (2) 'How to Write' by integrating Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) for style alignment; and (3) 'Deployment' via a parallel 'Fake Recall' architecture to maintain low latency.

Result: Online A/B testing on a large-scale video platform showed that WeWrite increased the Click-Through Video Volume (VV>10s) by 1.07% and decreased the Query Reformulation Rate by 2.97%.

Conclusion: The proposed WeWrite framework effectively enhances the performance of video search systems by personalizing query rewriting based on user behavior, leading to better search outcomes and user experience.

Abstract: In video search systems, user historical behaviors provide rich context for identifying search intent and resolving ambiguity. However, traditional methods utilizing implicit history features often suffer from signal dilution and delayed feedback. To address these challenges, we propose WeWrite, a novel Personalized Demand-aware Query Rewriting framework. Specifically, WeWrite tackles three key challenges: (1) When to Write: An automated posterior-based mining strategy extracts high-quality samples from user logs, identifying scenarios where personalization is strictly necessary; (2) How to Write: A hybrid training paradigm combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to align the LLM's output style with the retrieval system; (3) Deployment: A parallel "Fake Recall" architecture ensures low latency. Online A/B testing on a large-scale video platform demonstrates that WeWrite improves the Click-Through Video Volume (VV$>$10s) by 1.07% and reduces the Query Reformulation Rate by 2.97%.

</details>


### [3] [IRPAPERS: A Visual Document Benchmark for Scientific Retrieval and Question Answering](https://arxiv.org/abs/2602.17687)
*Connor Shorten,Augustas Skaburskas,Daniel M. Jones,Charles Pierse,Roberto Esposito,John Trengrove,Etienne Dilocker,Bob van Luijt*

Main category: cs.IR

TL;DR: 本文介绍了IRPAPERS，一个包含3,230页来自166篇科学论文的数据集，旨在比较基于图像和基于文本的检索及问答系统的表现。研究表明，尽管在某些指标上文本检索表现稍好，但结合两种模态可以实现更优的结果。此外，还评估了多种多向量图像嵌入模型，并分析了单一模态表示的局限性以及需要特定模态的问题类型。


<details>
  <summary>Details</summary>
Motivation: 视觉文档处理相比文本和关系数据处理而言探索较少，而最近多模态基础模型的进步提供了直接从文档图像进行检索和生成的可能性。研究旨在探讨基于图像的系统与已建立的基于文本的方法相比如何。

Method: 创建了一个名为IRPAPERS的基准测试数据集，其中包含了每一页的图像及其OCR转录本。使用了180个‘针在草堆’问题来对比不同系统的表现。同时，通过MUVERA评价效率-性能折衷，并测试了多个多向量图像嵌入模型。

Result: 文本检索（利用Arctic 2.0嵌入、BM25和混合文本搜索）达到了46%的Recall@1、78%的Recall@5和91%的Recall@20；基于图像的检索则分别达到43%、78%和93%。结合两种模态的混合搜索超越了单独使用任一模态的效果。对于问答任务，基于文本的RAG系统比基于图像的系统具有更高的准确性（0.82 vs. 0.71）。

Conclusion: 研究表明，虽然文本检索在某些情况下略优于图像检索，但结合文本和图像信息的多模态方法能够提供最佳结果。此外，Cohere Embed v4页面图像嵌入在闭源模型中表现出色，超过了所有测试的开源模型。

Abstract: AI systems have achieved remarkable success in processing text and relational data, yet visual document processing remains relatively underexplored. Whereas traditional systems require OCR transcriptions to convert these visual documents into text and metadata, recent advances in multimodal foundation models offer retrieval and generation directly from document images. This raises a key question: How do image-based systems compare to established text-based methods? We introduce IRPAPERS, a benchmark of 3,230 pages from 166 scientific papers, with both an image and an OCR transcription for each page. Using 180 needle-in-the-haystack questions, we compare image- and text-based retrieval and question answering systems. Text retrieval using Arctic 2.0 embeddings, BM25, and hybrid text search achieved 46% Recall@1, 78% Recall@5, and 91% Recall@20, while image-based retrieval reaches 43%, 78%, and 93%, respectively. The two modalities exhibit complementary failures, enabling multimodal hybrid search to outperform either alone, achieving 49% Recall@1, 81% Recall@5, and 95% Recall@20. We further evaluate efficiency-performance tradeoffs with MUVERA and assess multiple multi-vector image embedding models. Among closed-source models, Cohere Embed v4 page image embeddings outperform Voyage 3 Large text embeddings and all tested open-source models, achieving 58% Recall@1, 87% Recall@5, and 97% Recall@20. For question answering, text-based RAG systems achieved higher ground-truth alignment than image-based systems (0.82 vs. 0.71), and both benefit substantially from increased retrieval depth, with multi-document retrieval outperforming oracle single-document retrieval. We analyze the complementary limitations of unimodal text and image representations and identify question types that require one modality over the other. The IRPAPERS dataset and all experimental code are publicly available.

</details>


### [4] [Enhancing Scientific Literature Chatbots with Retrieval-Augmented Generation: A Performance Evaluation of Vector and Graph-Based Systems](https://arxiv.org/abs/2602.17856)
*Hamideh Ghanadian,Amin Kamali,Mohammad Hossein Tekieh*

Main category: cs.IR

TL;DR: 本研究探讨了通过检索增强生成(RAG)来改进科学文献聊天机器人，比较了基于向量和基于图的检索系统，并展示了混合RAG系统在提高科学知识可访问性和支持循证决策方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何利用检索增强生成技术来提升科学文献聊天机器人的性能，特别是结合结构化（图）和非结构化（向量）数据库以更有效地根据研究目标筛选资源。

Method: 方法包括开发一个能够接入科学文章及灰色文献的聊天机器人，并对单文档上传与大规模语料库检索两种应用场景进行了系统评估。评估基准集由GPT模型生成，并对选定输出进行标注以便评价。

Result: 结果显示，混合型RAG系统在提高检索准确度和响应相关性方面具有优势，揭示了不同方法各自的强项与局限性。

Conclusion: 结论指出，混合RAG系统展现了改善科学知识获取途径和支持基于证据做决定的巨大潜力。

Abstract: This paper investigates the enhancement of scientific literature chatbots through retrieval-augmented generation (RAG), with a focus on evaluating vector- and graph-based retrieval systems. The proposed chatbot leverages both structured (graph) and unstructured (vector) databases to access scientific articles and gray literature, enabling efficient triage of sources according to research objectives. To systematically assess performance, we examine two use-case scenarios: retrieval from a single uploaded document and retrieval from a large-scale corpus. Benchmark test sets were generated using a GPT model, with selected outputs annotated for evaluation. The comparative analysis emphasizes retrieval accuracy and response relevance, providing insight into the strengths and limitations of each approach. The findings demonstrate the potential of hybrid RAG systems to improve accessibility to scientific knowledge and to support evidence-based decision making.

</details>


### [5] [SuiteEval: Simplifying Retrieval Benchmarks](https://arxiv.org/abs/2602.18107)
*Andrew Parry,Debasis Ganguly,Sean MacAvaney*

Main category: cs.IR

TL;DR: SuiteEval is introduced as a unified, end-to-end evaluation framework for information retrieval, particularly aimed at improving the reproducibility and comparability of foundation embedding models. It supports major benchmarks and streamlines the evaluation process.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the issues of fragmented practices in information retrieval (IR) evaluation, which hinder reproducibility and comparability, especially for foundation embedding models that need to perform well across different domains. The authors aim to provide a solution that standardizes IR evaluations and makes them more accessible, even as the need for evaluating against a wider range of benchmarks grows.

Method: The method presented in the paper is the development of SuiteEval, a comprehensive framework designed to automate the entire IR evaluation process. This includes handling data loading, dynamic indexing, ranking, metric computation, and result aggregation. SuiteEval also allows for the addition of new benchmark suites with minimal effort and reuses on-disk indices to reduce disk usage, making it efficient and easy to use.

Result: The results of implementing SuiteEval show that it successfully provides a standardized and streamlined approach to IR evaluation. It supports a variety of major benchmarks and reduces the amount of boilerplate code required, thereby facilitating more reproducible research in the field of information retrieval.

Conclusion: In conclusion, the introduction of SuiteEval offers a significant advancement in the way IR systems are evaluated, by providing a unified, user-friendly, and efficient tool that enhances reproducibility and comparability. It is a valuable resource for researchers and practitioners looking to evaluate their IR models against a broad set of benchmarks.

Abstract: Information retrieval evaluation often suffers from fragmented practices -- varying dataset subsets, aggregation methods, and pipeline configurations -- that undermine reproducibility and comparability, especially for foundation embedding models requiring robust out-of-domain performance. We introduce SuiteEval, a unified framework that offers automatic end-to-end evaluation, dynamic indexing that reuses on-disk indices to minimise disk usage, and built-in support for major benchmarks (BEIR, LoTTE, MS MARCO, NanoBEIR, and BRIGHT). Users only need to supply a pipeline generator. SuiteEval handles data loading, indexing, ranking, metric computation, and result aggregation. New benchmark suites can be added in a single line. SuiteEval reduces boilerplate and standardises evaluations to facilitate reproducible IR research, as a broader benchmark set is increasingly required.

</details>


### [6] [A Simple yet Effective Negative Sampling Plugin for Constructing Positive Sample Pairs in Implicit Collaborative Filtering](https://arxiv.org/abs/2602.18206)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Ronghua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 本文提出了一种简单而有效的负采样插件PSP-NS，旨在增强正面样本信号，通过建立用户-物品二分图、基于复制的重加权生成正样本对，并采用活动感知加权方案来更好地学习不活跃用户的偏好。实验结果表明，PSP-NS在多个真实数据集上表现优于现有方法，特别是在Yelp数据集上大幅提升了Recall@30和Precision@30指标。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式协同过滤模型大多依赖于负采样训练，但当前工作主要集中在提高负面样本质量，忽视了正面样本的作用。此外，虽然一些去噪推荐方法可用于处理正面样本噪声，但通常会导致监督信息稀疏化，并且忽略了用户活跃度偏差，造成对非活跃用户的学习不足。为了解决这些问题，提出了PSP-NS。

Method: PSP-NS首先构建一个反映全局与局部交互置信度的用户-物品二分图；接着，通过基于复制的重加权技术生成正样本对以强化正面信号；最后，引入一种活动感知权重调整机制，以便更有效地捕捉不活跃用户的偏好。

Result: 理论分析从改进边界的角度解释了PSP-NS为何能提升排名质量（如Precision@k/Recall@k）。广泛的实验证明，在四个真实世界的数据集上，PSP-NS相对于最强基线显著提高了性能，特别是在Yelp数据集中，Recall@30和Precision@30分别提高了32.11%和22.90%。

Conclusion: PSP-NS作为一款易于集成到多种隐式CF推荐器或负采样方法中的插件，不仅增强了正面样本信号，还有效解决了用户活跃度偏差问题，从而整体提升了推荐系统的性能。

Abstract: Most implicit collaborative filtering (CF) models are trained with negative sampling, where existing work designs sophisticated strategies for high-quality negatives while largely overlooking the exploration of positive samples. Although some denoising recommendation methods can be applied to implicit CF for denoising positive samples, they often sparsify positive supervision. Moreover, these approaches generally overlook user activity bias during training, leading to insufficient learning for inactive users. To address these issues, we propose a simple yet effective negative sampling plugin, PSP-NS, from the perspective of enhancing positive supervision signals. It builds a user-item bipartite graph with edge weights indicating interaction confidence inferred from global and local patterns, generates positive sample pairs via replication-based reweighting to strengthen positive signals, and adopts an activity-aware weighting scheme to effectively learn inactive users' preferences. We provide theoretical insights from a margin-improvement perspective, explaining why PSP-NS tends to improve ranking quality (e.g., Precision@k/Recall@k), and conduct extensive experiments on four real-world datasets to demonstrate its superiority. For instance, PSP-NS boosts Recall@30 and Precision@30 by 32.11% and 22.90% on Yelp over the strongest baselines. PSP-NS can be integrated with various implicit CF recommenders or negative sampling methods to enhance their performance.

</details>


### [7] [The Economical-Ecological Benefits of Matching Non-matching Socks](https://arxiv.org/abs/2602.18221)
*Teddy Lazebnik*

Main category: cs.IR

TL;DR: 本研究量化了配对不匹配'孤儿'袜子的经济和生态价值，以及阻止这种行为的社会成本。通过将袜子所有权形式化为一个不确定条件下的顺序决策问题，并结合实地研究和计算机模拟评估，研究发现严格配对看似节省资源实则导致许多没有袜子穿的日子，而控制不匹配容忍度则可以维持使用并减少浪费。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决由于单只袜子丢失而导致的浪费问题，探索通过配对不匹配袜子来减少浪费的可能性及其面临的挑战。

Method: 采用行为研究估计人们对袜子不匹配的敏感性和偏好多样性，并通过计算机模拟评估不同配对策略的效果。

Result: 结果显示，对于袜子不匹配的一定容忍度有助于减少浪费并提高袜子使用效率，而严格的配对要求虽然表面上看起来节约但实际造成了更多未使用的日子。

Conclusion: 研究表明，尽管存在一定的局限性和挑战，配对不匹配袜子在减少浪费方面是可行的。

Abstract: Socks are produced and replaced at a massive scale, yet their paired use makes them unusually vulnerable to waste, as the loss of a single sock can strand usable wear-capacity and trigger premature replacement. In this study, we quantify the economic and ecological value of pairing non-matching \say{orphan} socks, and the social cost that discourages this behaviour. We formalize sock ownership as a sequential decision problem under uncertainty in which socks wear out and disappear stochastically during laundering, while public exposure induces a person-specific mismatch penalty. We conducted an in-person study to estimate mismatch sensitivity and diversity preference, linking behavioural heterogeneity to optimal mixing strategies. Using these results and a computer simulation-based evaluation of interpretable pairing policies, we show that strict matching can appear resource-frugal largely because it generates many sockless days, whereas controlled tolerance for mismatch sustains service and reduces stranded capacity across loss regimes. This study establishes the feasibility of matching non-matching socks while outlining its limitations and challenges.

</details>


### [8] [Dual-Tree LLM-Enhanced Negative Sampling for Implicit Collaborative Filtering](https://arxiv.org/abs/2602.18249)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 提出了一种无需文本和微调的双树LLM增强负采样方法（DTL-NS），通过离线错误负样本识别模块和多视角难负采样模块，有效提高了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的推荐系统研究中，负采样技术对文本信息和任务特定微调的依赖限制了其实际应用性。为解决这一问题，提出了一个无需文本信息和微调过程的新方法。

Method: 开发了一个名为DTL-NS的方法，包括两部分：一是利用层次索引树将协同结构与潜在语义信息转换为条目ID编码以供LLM推理，从而准确识别错误负样本；二是结合用户-条目偏好分数及这些编码中的条目-条目层级相似度来挖掘高质量难负样本，进而提高模型区分能力。

Result: 在Amazon-sports数据集上，相对于最强基线，DTL-NS在Recall@20和NDCG@20指标上分别提升了10.64%和19.12%。此外，该方法可以集成到多种隐式CF模型和负采样方法中，持续提升它们的表现。

Conclusion: DTL-NS提供了一种创新性的解决方案，能够有效地减少对于文本信息及额外微调步骤的需求，同时显著增强了推荐系统的性能。

Abstract: Negative sampling is a pivotal technique in implicit collaborative filtering (CF) recommendation, enabling efficient and effective training by contrasting observed interactions with sampled unobserved ones.
  Recently, large language models (LLMs) have shown promise in recommender systems; however, research on LLM-empowered negative sampling remains underexplored.
  Existing methods heavily rely on textual information and task-specific fine-tuning, limiting practical applicability.
  To address this limitation, we propose a text-free and fine-tuning-free Dual-Tree LLM-enhanced Negative Sampling method (DTL-NS).
  It consists of two modules: (i) an offline false negative identification module that leverages hierarchical index trees to transform collaborative structural and latent semantic information into structured item-ID encodings for LLM inference, enabling accurate identification of false negatives; and (ii) a multi-view hard negative sampling module that combines user-item preference scores with item-item hierarchical similarities from these encodings to mine high-quality hard negatives, thus improving models' discriminative ability.
  Extensive experiments demonstrate the effectiveness of DTL-NS. For example, on the Amazon-sports dataset, DTL-NS outperforms the strongest baseline by 10.64% and 19.12% in Recall@20 and NDCG@20, respectively.
  Moreover, DTL-NS can be integrated into various implicit CF models and negative sampling methods, consistently enhancing their performance.

</details>


### [9] [A Topology-Aware Positive Sample Set Construction and Feature Optimization Method in Implicit Collaborative Filtering](https://arxiv.org/abs/2602.18288)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 提出了一种拓扑感知的正样本集构建与特征优化方法（TPSC-FO），通过识别和转化误负样本来更准确地学习用户偏好，同时引入了基于邻域的特征优化模块来精炼正样本特征。实验表明该方法在多个数据集上有效。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有负采样策略中存在的问题，如过度依赖模型当前的表示能力以及未能充分利用误负样本作为潜在正样本来指导模型更准确地学习用户偏好。

Method: 设计了一种考虑拓扑社区结构的误负样本识别方法，并开发了一个拓扑感知的正样本集构建模块，利用差异化的社区检测策略捕捉隐式反馈中的拓扑社区结构，结合个性化的噪声过滤技术可靠地识别误负样本并将其转换成正样本。此外，还引入了一个基于邻域的特征优化模块，在嵌入空间中结合邻居特征来细化正样本特征，有效减少正样本中的噪声。

Result: 在五个真实世界数据集和两个合成数据集上的广泛实验验证了TPSC-FO的有效性。

Conclusion: 本研究提出的TPSC-FO方法能够有效地通过识别并转化误负样本来改进模型对于用户偏好的学习过程，并且通过邻域特征优化进一步提升了模型性能。

Abstract: Negative sampling strategies are widely used in implicit collaborative filtering to address issues like data sparsity and class imbalance. However, these methods often introduce false negatives, hindering the model's ability to accurately learn users' latent preferences. To mitigate this problem, existing methods adjust the negative sampling distribution based on statistical features from model training or the hardness of negative samples. Nevertheless, these methods face two key limitations: (1) over-reliance on the model's current representation capabilities; (2) failure to leverage the potential of false negatives as latent positive samples to guide model learning of user preferences more accurately. To address the above issues, we propose a Topology-aware Positive Sample Set Construction and Feature Optimization method (TPSC-FO). First, we design a simple topological community-aware false negative identification (FNI) method and observe that topological community structures in interaction networks can effectively identify false negatives. Motivated by this, we develop a topology-aware positive sample set construction module. This module employs a differential community detection strategy to capture topological community structures in implicit feedback, coupled with personalized noise filtration to reliably identify false negatives and convert them into positive samples. Additionally, we introduce a neighborhood-guided feature optimization module that refines positive sample features by incorporating neighborhood features in the embedding space, effectively mitigating noise in the positive samples. Extensive experiments on five real-world datasets and two synthetic datasets validate the effectiveness of TPSC-FO.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [Multi-Attribute Group Fairness in $k$-NN Queries on Vector Databases](https://arxiv.org/abs/2602.17858)
*Thinh On,Senjuti Basu Roy,Baruch Schieber*

Main category: cs.DB

TL;DR: 本文研究了向量数据库中k-最近邻搜索的多属性组公平性问题，并提出了一种计算框架，该框架能够生成满足多个受保护属性联合计数约束的候选对象，并通过后处理阶段构建所有属性上的公平k-NN结果。


<details>
  <summary>Details</summary>
Motivation: 在k-最近邻搜索中引入公平性以确保由受保护属性定义的各组之间具有比例代表性。当公平性涉及多个属性时，这些约束必须同时满足，使得问题在计算上变得困难。

Method: 提出了一个计算框架，该框架调整局部敏感哈希（LSH）来加速候选对象生成，并在受保护属性值的笛卡尔积上建立轻量级索引。对于两个属性，提供了基于流的确切多项式时间算法；对于三个或更多属性，则制定了基于整数线性规划（ILP）的确切解决方案。

Result: 实验评估表明了所提框架的通用性和可扩展性，并且现有向量搜索方法无法直接适应于实现公平性。

Conclusion: 本研究为多属性组公平性的k-最近邻搜索提供了一种有效的解决方案，展示了在搜索时间、内存/索引成本和召回率之间的良好权衡。

Abstract: We initiate the study of multi-attribute group fairness in $k$-nearest neighbor ($k$-NN) search over vector databases. Unlike prior work that optimizes efficiency or query filtering, fairness imposes count constraints to ensure proportional representation across groups defined by protected attributes. When fairness spans multiple attributes, these constraints must be satisfied simultaneously, making the problem computationally hard. To address this, we propose a computational framework that produces high-quality approximate nearest neighbors with good trade-offs between search time, memory/indexing cost, and recall. We adapt locality-sensitive hashing (LSH) to accelerate candidate generation and build a lightweight index over the Cartesian product of protected attribute values. Our framework retrieves candidates satisfying joint count constraints and then applies a post-processing stage to construct fair $k$-NN results across all attributes. For 2 attributes, we present an exact polynomial-time flow-based algorithm; for 3 or more, we formulate ILP-based exact solutions with higher computational cost. We provide theoretical guarantees, identify efficiency--fairness trade-offs, and empirically show that existing vector search methods cannot be directly adapted for fairness. Experimental evaluations demonstrate the generality of the proposed framework and scalability.

</details>


### [11] [Efficient Filtered-ANN via Learning-based Query Planning](https://arxiv.org/abs/2602.17914)
*Zhuocheng Gan,Yifan Wang*

Main category: cs.DB

TL;DR: 本文提出了一种基于学习的查询规划框架，该框架能够为每个查询动态选择最有效的执行计划。通过使用从数据集和查询统计信息中得出的轻量级预测，包括维度、语料库大小、分布特征和谓词统计等，来支持多种过滤类型。实验表明，与强大的基线相比，该方法在保持90%以上召回率的同时，可实现高达4倍的加速。


<details>
  <summary>Details</summary>
Motivation: 在向量检索中，过滤后的近似最近邻搜索是一个越来越重要的问题。然而，系统面临着由于执行顺序导致的一个难题：预过滤（先过滤后进行近似最近邻搜索）需要昂贵的每谓词索引构建成本；而后期过滤（先进行近似最近邻搜索后再过滤候选者）可能因过滤后候选者不足而在低选择性下浪费计算资源并丧失召回率。

Method: 作者们引入了一种基于学习的查询规划框架，它能利用来自数据集和查询统计数据（如维度、语料库大小、分布特性及谓词统计）的轻量级预测，为每次查询动态地挑选出最优执行方案。此框架兼容多种类型的过滤器，例如分类/关键词以及范围谓词，并且可以通用任何后端ANN索引。

Result: 实验结果表明，相较于其他强劲基准方法，在保持至少90%召回率的前提下，所提出的方法最高可达至4倍的速度提升。

Conclusion: 研究介绍了一个灵活高效的查询规划解决方案，能够在保证高召回率的同时显著提高过滤式近似最近邻搜索任务的性能。

Abstract: Filtered ANN search is an increasingly important problem in vector retrieval, yet systems face a difficult trade-off due to the execution order: Pre-filtering (filtering first, then ANN over the passing subset) requires expensive per-predicate index construction, while post-filtering (ANN first, then filtering candidates) may waste computation and lose recall under low selectivity due to insufficient candidates after filtering. We introduce a learning-based query planning framework that dynamically selects the most effective execution plan for each query, using lightweight predictions derived from dataset and query statistics (e.g., dimensionality, corpus size, distribution features, and predicate statistics). The framework supports diverse filter types, including categorical/keyword and range predicates, and is generic to use any backend ANN index. Experiments show that our method achieves up to 4x acceleration with >= 90% recall comparing to the strong baselines.

</details>


### [12] [Seasoning Data Modeling Education with GARLIC: A Participatory Co-Design Framework](https://arxiv.org/abs/2602.18274)
*Viktoriia Makovska,Ihor Michurin,Mariia Tokhtamysh,George Fletcher,Julia Stoyanovich*

Main category: cs.DB

TL;DR: 本文介绍了一种名为GARLIC的教学方法，旨在通过角色扮演、协作综合、引导批判和迭代改进等环节，教授参与式实体关系（ER）建模。该方法不仅培养学生的建模技术能力，还增强了他们对数据表示中社会与伦理维度的认识。


<details>
  <summary>Details</summary>
Motivation: 当前数据库教育在教授参与式ER建模方面缺乏结构化的教学方法。鉴于ER建模在定义数据系统如何呈现人、过程和机构方面起着核心作用，引入更包容的数据表示方式是非常重要的。

Method: 提出了GARLIC方法论，这是一种基于工作坊的学习形式，结合了角色扮演、合作合成、指导性批评及反复精炼等元素来教授参与式ER建模。

Result: GARLIC方法降低了参与式ER建模的入门门槛，并为学生提供了协作设计包容性数据模型的实际技能。

Conclusion: GARLIC方法论提供了一个有效途径，以促进参与式ER建模的学习与实践，同时增强学生对于数据表示背后潜在的社会伦理问题的理解。

Abstract: Entity-Relationship (ER) modeling is commonly taught as a primarily technical activity, despite its central role in shaping how data systems represent people, processes, and institutions. Prior research in participatory design demonstrates that involving diverse stakeholders in modeling can surface tacit knowledge, challenge implicit assumptions, and produce more inclusive data representations. However, database education currently lacks structured pedagogical approaches for teaching participatory ER modeling in practice.
  We introduce the GARLIC methodology for teaching and learning participatory ER modeling. GARLIC adapts and extends the ONION participatory ER modeling framework of Makovska et al.(HILDA 2025) into a workshop-based learning format that combines role-playing, collaborative synthesis, guided critique, and iterative refinement. GARLIC is designed to develop both technical modeling skills and critical awareness of the social and ethical dimensions of data representation. GARLIC lowers the barrier to participatory ER modeling and equips students with practical skills for collaborative, inclusive data model design.

</details>


### [13] [Dichotomy for Axiomatising Inclusion Dependencies on K-Databases](https://arxiv.org/abs/2602.18390)
*Miika Hannula,Teymur Ismikhanov,Jonni Virtema*

Main category: cs.DB

TL;DR: 本文研究了K-数据库上包含依赖关系的蕴含问题，基于单子K是否为弱吸收或弱可约建立了二分法，并指出了在特定条件下需要引入平衡公理。


<details>
  <summary>Details</summary>
Motivation: 研究不同类型的单子K下K-数据库中包含依赖关系的蕴含问题及其公理化。

Method: 通过分析单子K的性质（弱吸收性或弱可约性），确定适用于K-数据库中包含依赖关系的标准公理或需附加的弱对称公理、平衡公理的有效性和完整性。

Result: 如果单子K是弱可约的，则标准的包含依赖公理对于蕴含问题是完备的；如果不是弱可约而是弱吸收的话，则需要加上弱对称公理来保证完备性。此外，在要求每个K-关系联合权重相同的情况下，还需加入平衡公理。

Conclusion: 基于单子K的不同特性，为K-数据库中的包含依赖关系提供了不同的公理系统以确保其蕴含问题的正确解决。

Abstract: A relation consisting of tuples annotated by an element of a monoid K is called a K-relation. A K-database is a collection of K-relations. In this paper, we study entailment of inclusion dependencies over K-databases, where K is a positive commutative monoid. We establish a dichotomy regarding the axiomatisation of the entailment of inclusion dependencies over K-databases, based on whether the monoid K is weakly absorptive or weakly cancellative. We establish that, if the monoid is weakly cancellative then the standard axioms of inclusion dependencies are sound and complete for the implication problem. If the monoid is not weakly cancellative, it is weakly absorptive and the standard axioms of inclusion dependencies together with the weak symmetry axiom are sound and complete for the implication problem. In addition, we establish that the so-called balance axiom is further required, if one stipulates that the joint weights of each K-relation of a K-database need to be the same; this generalises the notion of a K-relation being a distribution. In conjunction with the balance axiom, weak symmetry axiom boils down to symmetry.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving](https://arxiv.org/abs/2602.17677)
*Sutej Kulgod,Sean Ye,Sanchit Tanwar,Christoffer Heckman*

Main category: cs.LG

TL;DR: 该论文提出了一种方法来减少视觉语言模型在处理合成生成的多项选择题时对隐藏文本线索的依赖，通过将正确答案与语言特征解耦并采用课程学习策略，促使模型更多地依赖视觉信息。


<details>
  <summary>Details</summary>
Motivation: 观察到合成生成的多项选择题非常容易受到隐藏文本线索的影响，这使得模型能够利用语言模式而不是视觉上下文来作答，导致即使没有视觉输入的情况下也能达到接近人工验证基准的准确性。

Method: 提出的方法包括将正确答案从语言特征中解耦出来，并采用一种课程学习策略，以确保模型更多地依赖于视觉基础而非简单的语言捷径。

Result: 使用提出的方法后，盲目准确率从比随机高66.9%降低到了仅高出2.9%，表明大部分可利用的语言捷径已经被消除。

Conclusion: 通过所提出的方法有效地减少了模型对非视觉信息的依赖性，提高了其基于视觉理解的表现真实性。

Abstract: Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.

</details>


### [15] [BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs](https://arxiv.org/abs/2602.17680)
*Yujia Wang,Jihong Guan,Wengen Li,Shuigeng Zhou,Xuhong Wang*

Main category: cs.LG

TL;DR: 提出了BioBridge框架，通过领域增量持续预训练和跨模态对齐技术，结合了蛋白质语言模型的领域知识和大型语言模型的通用推理能力，实现了在多种蛋白质相关任务以及一般理解任务上的优秀表现。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质语言模型（PLM）适应多任务能力和泛化能力有限，而通用的大规模语言模型（LLM）缺乏解释蛋白质序列的能力及领域特定的知识，限制了其生物语义推理的有效性。为解决这些问题并结合两者优势。

Method: 采用领域增量持续预训练（DICP）方法，同时将蛋白质领域知识与通用推理语料库注入到一个大规模语言模型中；通过PLM-Projector-LLM管道实现跨模态对齐，将蛋白质序列嵌入映射到语言模型的语义空间；采取端到端优化策略支持多种任务执行。

Result: BioBridge在多个蛋白质基准测试如EC、BindingDB上表现出与主流PLM相当的性能，并且在一般理解任务如MMLU、RACE上也达到了与LLM相当的结果。

Conclusion: BioBridge成功地结合了领域特定的适应性和通用语言能力的优势，展示了其在处理各种任务时的创新优势。

Abstract: Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.

</details>


### [16] [LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs](https://arxiv.org/abs/2602.17681)
*Ofir Gordon,Lior Dikstein,Arnon Netzer,Idan Achituve,Hai Victor Habi*

Main category: cs.LG

TL;DR: 本文提出了一种名为LATMiX的方法，通过可学习的仿射变换来减少异常值，从而在MX低比特量化下提高了大语言模型的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 后训练量化（PTQ）是降低大型语言模型（LLMs）内存和计算成本的一种广泛使用的方法。已有研究表明，通过对激活应用可逆变换可以显著提高量化的鲁棒性，但现有方法主要限于旋转或Hadamard变换，并且大多数研究集中在传统量化方案上，而现代硬件越来越支持微缩放（MX）数据格式。尝试结合两者时遇到了严重的性能下降问题，因此先前的工作引入了关于变换的一些假设。本研究从互补的角度出发，旨在解决这一挑战。

Method: 首先，作者对MX量化下的变换进行了理论分析，得出了量化误差的一个界限，强调了考虑激活分布和底层量化结构的重要性。基于此分析，提出了LATMiX方法，该方法将异常值减少推广到利用标准深度学习工具优化的可学习的可逆仿射变换。

Result: 实验表明，在广泛的零样本基准测试中，对于多种模型规模，LATMiX在MX低比特量化下相对于强大的基线方法持续提高了平均准确率。

Conclusion: 这项工作通过引入LATMiX方法展示了如何有效结合可逆变换与MX量化格式，以改善大语言模型在低精度条件下的性能。

Abstract: Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.

</details>


### [17] [Duality Models: An Embarrassingly Simple One-step Generation Paradigm](https://arxiv.org/abs/2602.17682)
*Peng Sun,Xinyi Shang,Tao Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional "one input, one output" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a "one input, dual output" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo

</details>


### [18] [Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates](https://arxiv.org/abs/2602.17683)
*Irene Iele,Giulia Romoli,Daniele Molino,Elena Mulero Ayllón,Filippo Ruffini,Paolo Soda,Matteo Tortora*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的概率预测框架，用于在清晰天空条件下进行田间尺度的NDVI预测。该方法通过整合历史NDVI观测值与历史和未来气象协变量来处理不规则的重访模式和视界依赖性不确定性，并引入了累积和极端天气特征工程以更好地捕捉与植被响应相关的延迟气象效应。实验表明，该方法在点估计和概率评估指标上均优于多种统计、深度学习以及最近的时间序列基线模型。


<details>
  <summary>Details</summary>
Motivation: 准确的短期植被动态预测是精准农业中数据驱动决策支持的关键。然而，由于云层覆盖导致的稀疏和不规则采样以及作物生长过程中异质性的气候条件，从卫星观测数据中预测NDVI（归一化差异植被指数）仍然具有挑战性。

Method: 提出了一个特别设计用于在晴朗天空条件下进行田间级NDVI预测的概率预测框架。该方法利用了一个基于Transformer的架构，将历史植被动态建模与未来外生信息明确分开，并结合了历史NDVI观测值及历史和未来的气象协变量。为了解决不规则的重新访问模式和视界依赖性不确定性的问题，引入了一种时间距离加权分位数损失，使得训练目标与实际预测范围相匹配。此外，还加入了累积性和极端天气特征工程，以便更好地捕捉对植被响应有影响的延迟气象效应。

Result: 广泛的实验使用欧洲卫星数据显示，所提出的方法在点估计和概率评估度量方面始终优于一系列多样化的统计、深度学习以及最新的时间序列基准模型。消融研究进一步强调了目标历史的重要性，同时显示当联合利用时气象协变量提供了互补的增益。

Conclusion: 本研究表明，所提出的基于Transformer架构的概率预测框架能有效改善田间级别NDVI的预测精度，在面对由云层遮挡引起的数据缺失和复杂气候条件下表现尤为突出。

Abstract: Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.

</details>


### [19] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: 提出了一种无需执行的奖励模型CodeScaler，用于代码生成任务中的强化学习训练和测试时推断。该模型通过利用验证过的代码问题导出的偏好数据进行训练，并结合语法感知的代码提取和保持有效性的奖励塑形技术，从而在五个编码基准上提高了Qwen3-8B-Base的表现，并且在没有测试用例的情况下也能实现可扩展的强化学习。


<details>
  <summary>Details</summary>
Motivation: 当前基于单元测试反馈的强化学习方法（RLVR）虽然促进了大型语言模型在代码生成方面的进展，但其可扩展性受到高质量测试用例可用性和可靠性的限制。因此，研究者们旨在开发一种不依赖于执行反馈的方法来提高代码生成任务中强化学习的可扩展性。

Method: 提出了CodeScaler，一个专为代码生成设计的无需执行的奖励模型。该模型通过对从已验证代码问题中精心挑选的数据集进行训练，并采用语法感知代码提取与保持有效性的奖励塑形技术以确保优化过程的稳定性和鲁棒性。

Result: 实验表明，在五个编码基准上，CodeScaler平均提升了Qwen3-8B-Base 11.72个百分点的表现，相较于基于二进制执行的RL方法高出1.82个百分点；同时，即使是在没有任何测试用例的人造数据集上也能够支持可扩展的强化学习。此外，在推理阶段，CodeScaler作为有效的测试时间扩展方法，不仅达到了与单元测试方法相当的性能，还实现了延迟降低十倍的效果。

Conclusion: CodeScaler作为一种创新的无执行奖励模型，成功地解决了现有基于执行反馈的强化学习方法在代码生成领域面临的可扩展性挑战，同时在多个基准测试中表现出色，包括但不限于代码、通用及推理领域，显示出其广泛的应用潜力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [20] [Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling](https://arxiv.org/abs/2602.17685)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.LG

TL;DR: 本研究提出了一种统一的共椭圆机动框架，用于低地球轨道上的多目标主动碎片清除任务。通过在现实的轨道模拟环境中比较三种规划算法（贪婪启发式、蒙特卡洛树搜索和深度强化学习），实验结果表明基于掩码近端策略优化的深度强化学习方法在任务效率和计算性能上表现最佳，为未来自主化的空间任务规划指明了方向。


<details>
  <summary>Details</summary>
Motivation: 为了应对低地球轨道内日益增长的空间碎片问题，并提高主动碎片清除任务的安全性和资源利用效率。

Method: 开发了一个包含随机碎片场、禁区及ΔV限制的真实轨道模拟环境，并在此基础上评估了贪婪启发式、蒙特卡洛树搜索以及采用掩码近端策略优化的深度强化学习三种不同规划算法的表现。

Result: 实验显示，在100个测试场景中，使用掩码PPO的深度强化学习方法能够访问比贪婪算法多达两倍的目标数量，并且在运行时间上显著优于MCTS算法。

Conclusion: 现代强化学习方法特别是掩码PPO，在实现可扩展性、安全性及资源高效的空间任务规划方面展现出巨大潜力，为将来更先进的主动碎片清除自主化奠定了基础。

Abstract: This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.

</details>


### [21] [EXACT: Explicit Attribute-Guided Decoding-Time Personalization](https://arxiv.org/abs/2602.17695)
*Xin Yu,Hanwen Xing,Lingzhou Xue*

Main category: cs.LG

TL;DR: 本文提出了一种新的解码时个性化方法EXACT，它通过使用有限的成对偏好反馈和一组可解释属性来实现与用户的生成对齐。该方法首先离线识别用户特定的属性子集，然后在线推理时检索最相关的属性并将其注入上下文以指导生成。理论分析和实验证明了EXACT的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前解码时个性化方法主要依赖于隐式、较难解释的偏好表示，并且提供的用户表示缺乏灵活性，无法很好地适应不同情境下的偏好变化。为了克服这些限制，研究者们提出了EXACT，旨在利用更明确的属性定义以及更灵活的方法来处理用户在不同场景中的偏好转换问题。

Method: EXACT分为两个阶段：1) 离线阶段，通过最大化偏好的响应可能性来确定用户特有的属性子集；2) 在线推理阶段，根据接收到的问题选取语义上最相关的属性，并将这些属性加入到上下文中以指导模型生成更加符合个人偏好的文本。此外，还提供了关于所提算法的一些理论近似保证。

Result: 广泛的实验表明，在人类标注的偏好数据集上，EXACT在偏好建模准确性及个性化生成质量方面均优于强大的基线模型。

Conclusion: EXACT作为一种新颖的解码时个性化技术，能够有效地利用少量的偏好反馈信息和一组预定义的可解释属性来改善大型语言模型对于单个用户不断变化的情境下的适应能力。

Abstract: Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.

</details>


### [22] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 本文提出了一种名为AnchorTree的新框架，它通过使用代码特有的结构化、分层先验来明确锚定扩散过程，从而提高了代码生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理编程语言时往往忽略了其严格的结构特性，导致生成的程序常常无法执行。为了解决这个问题，研究者提出了一个能够尊重编程语言结构，并有效提升代码生成质量的新框架。

Method: 通过利用抽象语法树（AST），AnchorTree优先解决语法和语义上显著的标记，比如关键字（如if, while）和标识符（如变量名），以此建立一个结构化的支架来指导后续的生成过程。

Result: 实验验证了该框架的有效性，AnCoder模型家族表明，这种结构锚定的扩散方法为高质量代码生成提供了一条参数效率高的路径。

Conclusion: AnchorTree框架通过引入结构化先验知识到扩散过程中，成功地提高了代码生成任务中的程序完整性和可执行性。

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [23] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: 提出了一种新的自监督预训练框架Robust-MMR，通过整合非对称扰动感知掩码、领域一致性正则化和模态韧性约束来增强医学视觉-语言模型在跨领域任务中的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态预训练方法大多忽略了鲁棒性问题，导致医学图像与临床文本联合推理的模型在遇到设备、协议及报告风格变化时表现不佳。

Method: 提出了一个名为Robust-MMR的新框架，该框架将鲁棒性目标明确地纳入到遮蔽的视觉-语言学习中，包括非对称扰动意识掩蔽、域一致性正则化以及模态弹性限制等技术。

Result: Robust-MMR在多个医学视觉-语言基准测试上取得了显著成绩，如VQA-RAD交叉领域准确率达到78.9%，SLAKE和VQA-2019分别为74.6%和77.0%；同时，在受干扰条件下，VQA-RAD准确性从69.1%提升至75.6%，MELINDA交叉领域精度由70.3%升至75.2%，检索实验显示平均排名下降幅度从超过16减少到4.1。

Conclusion: 研究表明，在预训练过程中明确建模鲁棒性可以产生更可靠且可转移的医学视觉-语言表示，适用于现实世界的部署。

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [24] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: 本文提出了一种新的视角，将大型语言模型视为可配置系统，并应用变异性管理技术来系统地分析推理时的配置选择。通过评估Hugging Face Transformers库，使用基于特征的变异性模型表示生成超参数及其约束条件，抽样代表性配置，测量其能耗、延迟和准确性，并从收集的数据中学习预测模型。结果表明，变异性建模有效地管理了LLM推理配置的复杂性，支持从有限数量的测量中准确预测推理行为。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在各种任务中的广泛应用，其巨大的计算需求引发了关于训练和推理过程中的能源效率和可持续性的担忧。尤其是推理阶段占据了总计算量的主要部分，因此优化它至关重要。尽管已有研究探索了优化技术和配置选择对能耗的影响，但推理服务器庞大的配置空间使得全面的经验评估变得不可行。

Method: 本文采用变异性管理技术，将大型语言模型看作是可配置系统，以此新角度出发以系统地分析推理时的配置选择。通过Hugging Face Transformers库进行实验，利用基于特征的变异性模型来表示生成超参数及它们之间的约束关系，选取代表性的配置方案，测量这些配置下的能量消耗、延迟以及准确性，并根据收集到的数据训练预测模型。

Result: 研究发现，变异性建模能够有效处理大型语言模型推理配置的复杂性问题。它不仅有助于系统地分析超参数的效果与相互作用，揭示其中存在的权衡关系，还能基于有限次数的实际测量准确预测推理表现。

Conclusion: 本研究开启了一个结合软件工程与机器学习的新方向，通过利用变异性建模来实现大型语言模型更高效且可持续的配置。

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [25] [A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU](https://arxiv.org/abs/2602.17693)
*Yuchen Luo,Fangyue Zhu,Ruining Zhou,Mingzhe Huang,Jian Zhu,Fanyu Fan,Wei Shao*

Main category: cs.LG

TL;DR: 本文研究了后训练量化(PTQ)在面向推理的模型如DeepSeek-R1-Distill-Qwen系列和QwQ-32B上应用时，在升腾NPU上的有效性。通过评估AWQ、GPTQ、SmoothQuant及FlatQuant四种算法，发现4比特权重仅压缩对较大模型有效，但激进的4比特权重-激活方案会导致NPU上的层校准不稳定。而标准8比特量化保持数值稳定。此外，INT8的实际部署表明尽管优化内核减少了延迟，动态量化开销仍限制了端到端加速。


<details>
  <summary>Details</summary>
Motivation: 探讨后训练量化(PTQ)技术在Ascend NPU平台上的有效性，尤其是与GPU架构相比尚未充分探索的部分。

Method: 选取代表性PTQ基线应用于一系列推理导向模型（如DeepSeek-R1-Distill-Qwen系列和QwQ-32B），并评估了包括AWQ、GPTQ、SmoothQuant以及FlatQuant在内的四种不同算法，覆盖从仅权重压缩到高级旋转方法的范围。

Result: 实证结果揭示了显著的平台敏感性：对于大型模型而言，4位仅权重量化是可行的；然而，激进的4位权重-激活方案在NPU上遭遇逐层校准不稳定性问题，导致长时间上下文推理任务中的逻辑崩溃。相反地，标准8位量化保持了数值上的稳定性。另外，针对INT8的实际部署案例显示，虽然优化后的内核能够减少延迟，但是当前动态量化的开销限制了端到端加速的效果。

Conclusion: 研究结果为理解量化推理模型在Ascend NPU平台上部署时的可行性与局限性提供了实践参考。

Abstract: Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.

</details>


### [26] [Can LLM Safety Be Ensured by Constraining Parameter Regions?](https://arxiv.org/abs/2602.17696)
*Zongmin Li,Jian Su,Farah Benamara,Aixin Sun*

Main category: cs.LG

TL;DR: 研究评估了四种大型语言模型安全区域识别方法，发现当前技术难以可靠地识别一个稳定且与数据集无关的安全区域。


<details>
  <summary>Details</summary>
Motivation: 为了验证大型语言模型中是否存在通过修改某些参数直接改变安全行为的“安全区域”。

Method: 采用跨四个不同大小的语言模型家族的四种安全区域识别方法，并利用十个安全识别数据集以及额外的功能性数据集进行测试。

Result: 所识别出的安全区域之间只有较低到中等程度的重叠，当进一步使用功能性数据集细化时，重叠度显著下降。

Conclusion: 现有的安全区域识别技术未能一致地找到一个不受特定数据集影响的稳定安全区域。

Abstract: Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.

</details>


### [27] [ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs](https://arxiv.org/abs/2602.17698)
*Xinlin Li,Timothy Chou,Josh Fromm,Zichang Liu,Yunjie Pan,Christina Fragouli*

Main category: cs.LG

TL;DR: 提出了一种名为ScaleBITS的混合精度量化框架，通过硬件对齐的权重分区方案和双向通道重排序技术，实现了在内存预算下的自动化细粒度位宽分配。实验表明，ScaleBITS在极低比特率下优于均匀精度量化和其他基于敏感性的基线方法，且不增加运行时开销。


<details>
  <summary>Details</summary>
Motivation: 后训练权重量化对于减少大型语言模型（LLMs）的内存和推理成本至关重要，但将平均精度降至4比特以下仍面临挑战，主要原因是权重敏感性高度不均匀以及缺乏系统的精度分配原则。现有解决方案存在运行时开销大或依赖启发式、高度受限的精度分配策略等问题。

Method: 开发了ScaleBITS，一种支持在给定内存预算下进行自动、细粒度位宽分配的混合精度量化框架，并保持硬件效率。该方法包括新的敏感性分析指导下的硬件对齐、基于双向通道重排序的块级权重分区方案；同时，将全局位宽分配建模为一个约束优化问题，并提出了一种可扩展的近似贪心算法来解决这一问题。

Result: 实验结果显示，在超低比特率情况下，ScaleBITS相较于统一精度量化方法有显著提升（最高可达+36%），同时也优于最先进的基于敏感性的基准方法（最高可达+13%），并且没有引入额外的运行时开销。

Conclusion: 本研究提出的ScaleBITS框架有效地解决了大规模语言模型中后训练权重量化的难题，特别是在实现更低比特率的同时保证了性能和效率，为未来相关领域的发展提供了新思路。

Abstract: Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.

</details>


### [28] [Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure](https://arxiv.org/abs/2602.17699)
*Chandrasekhar Gokavarapu,Sudhakar Gadde,Y. Rajasekhar,S. R. Bhargava*

Main category: cs.LG

TL;DR: 本文提出了一种在分布偏移下对预测器风险进行显式上界估计的方法，并构建了一个统一框架，该框架支持风险验证、模型验证的合理性和通过可识别性条件而非事后解释来强制执行的可解释性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决当训练和评估预测器时遇到的数据分布不一致问题，通过提供一种方法来量化这种分布偏移对预测性能的影响，并确保即使在非平凡规模下也能可靠地验证所学习到的模型。

Method: 作者们开发了一个新框架，其中包含了用于认证分布偏移下的风险的显式不等式、保证了对于一定规模的学习模型其验证过程的有效性，以及通过设置可识别性条件来增强模型的可解释性而不是依赖于后验解释。

Result: 研究结果表明，在满足特定规则性和复杂度约束条件下，可以计算出一个与转移度量和模型参数相关的显式上界来估计分布偏移下的超额风险。此外，还明确了不可认证区域的特点。

Conclusion: 这项工作为理解和控制由于数据分布变化导致的机器学习模型性能下降提供了理论基础和技术手段。

Abstract: Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.

</details>


### [29] [MIDAS: Mosaic Input-Specific Differentiable Architecture Search](https://arxiv.org/abs/2602.17700)
*Konstanty Subbotko*

Main category: cs.LG

TL;DR: MIDAS is a new approach that enhances DARTS by using dynamic, input-specific parameters through self-attention, improving the robustness of architecture selection. It shows high performance on CIFAR-10 and CIFAR-100, and consistently finds optimal architectures in NAS-Bench-201.


<details>
  <summary>Details</summary>
Motivation: The motivation behind MIDAS is to modernize the Differentiable Architecture Search (DARTS) by addressing its limitations, such as static architecture parameters, and to improve the practicality and robustness of neural architecture search.

Method: MIDAS replaces static architecture parameters with dynamic, input-specific ones computed via self-attention. It also localizes architecture selection to each spatial patch of the activation map and introduces a topology-aware, parameter-free search space for better node connectivity and edge selection.

Result: MIDAS achieves 97.42% top-1 accuracy on CIFAR-10 and 83.38% on CIFAR-100 in the DARTS search space. In NAS-Bench-201, it finds globally optimal architectures, and in RDARTS, it sets new state-of-the-art results on two out of four search spaces on CIFAR-10.

Conclusion: MIDAS improves upon DARTS by incorporating dynamic, input-specific parameters and enhancing the robustness of the architecture selection process, leading to higher performance and more reliable guidance for decoding.

Abstract: Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.

</details>


### [30] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: 介绍了一种新的时间序列生成模型PaCoDi，通过在频域内进行解耦生成建模，解决了传统模型中的局部纠缠问题和计算效率问题。该模型利用傅里叶变换将局部耦合的时间信号转换为全局解耦的频谱组件，并通过均场理论近似及交互修正机制连接理论与实际数据。此外，PaCoDi还利用了实值信号的厄米对称性来压缩序列长度，减少了注意力机制的计算量，同时保持了信息完整。实验表明，PaCoDi在生成质量和推理速度上都优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 针对长时间依赖关系建模中表现力与计算效率之间的基本权衡问题，特别是传统时序扩散模型存在的局部纠缠现象以及注意力机制带来的高计算成本（$\mathcal{O}(L^2)$），本文提出一种创新解决方案。

Method: 提出了名为PaCoDi（Parallel Complex Diffusion）的新架构，该架构基于频谱原生设计，在频率域内实现了解耦生成建模。通过傅里叶变换作为对角化算子，将局部耦合的时间信号转变为全局解相关联的频谱分量。采用均场理论近似加上交互式校正机制来弥合理论与数据间的差距，并将离散DDPM推广到连续时间频率SDEs。

Result: PaCoDi能够有效减少注意力机制所需的FLOPs达50%，同时不会损失信息。此外，它还能处理压缩流形上的非等方差噪声分布。广泛的实验证明了PaCoDi在生成质量与推理速度两方面均优于现有的基线方法。

Conclusion: PaCoDi提供了一个理论基础扎实且计算高效的长时间依赖关系时间序列建模方案，不仅解决了传统方法中存在的局限性，而且在性能指标上取得了显著进步。

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [31] [Provable Adversarial Robustness in In-Context Learning](https://arxiv.org/abs/2602.17743)
*Di Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种分布鲁棒的元学习框架，为在Wasserstein分布变化下的情境学习提供了最坏情况性能保证。通过分析线性自注意力Transformer模型，得出了对抗扰动强度、模型容量与情境示例数量之间的非渐近界。研究结果表明，模型的鲁棒性随其容量的平方根增长，并且对抗环境下样本复杂度的惩罚与扰动幅度的平方成正比。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型通过情境学习适应新任务的能力解释假设测试任务来自预训练期间见到的相似分布，但这一假设忽略了可能威胁实际应用可靠性的对抗分布偏移问题。

Method: 引入了一个基于Wasserstein分布变化的情境学习分布鲁棒元学习框架，并针对线性自注意力Transformer推导了连接对抗扰动强度、模型容量及情境示例数量的非渐近边界。

Result: 发现模型鲁棒性以模型容量平方根的速度增长；同时，在对抗设置下，样本复杂度相对于标准情况增加的程度与扰动大小的平方成正比。实验验证了这些缩放法则。

Conclusion: 这项工作深化了我们对在对抗条件下情境学习限制的理解，并指出模型容量是实现分布鲁棒性的基本资源。

Abstract: Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($ρ$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($ρ_{\text{max}} \propto \sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_ρ- N_0 \propto ρ^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.

</details>


### [32] [Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs](https://arxiv.org/abs/2602.17778)
*Zachary Coalson,Bo Fang,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文提出了一种新的对话语言模型的失效模式：回合放大，即模型在未完成基本任务的情况下不断延长多轮交互。通过利用澄清请求行为，攻击者可以系统地延长交互。与之前的依赖每轮提示优化的成本放大攻击不同，这种攻击源于对话动态，并且在不同的提示和任务中持续存在。研究还发现了一个与查询无关的、通用的激活子空间，与寻求澄清响应相关联。无论是通过微调还是运行时低级参数篡改，该机制都为诱导回合放大提供了可扩展的途径。实验表明，这种攻击显著增加了对话轮数，而现有的防御措施对此类新兴故障提供的保护有限。


<details>
  <summary>Details</summary>
Motivation: 多轮交互长度是对话式大型语言模型运营成本的主要因素。文章旨在揭示一种新的失效模式，称为‘回合放大’，其中模型在没有完成基础任务的情况下不断地延长多轮对话。此外，它探讨了如何通过利用模型倾向于寻求澄清的行为来系统性地延长对话时间，从而增加操作成本。

Method: 首先定义并描述了“回合放大”这一新失效模式。接着，通过识别一个与寻求澄清响应相关的查询独立、普遍存在的激活子空间，从机制角度分析了为何以及如何会发生这种现象。然后展示了如何通过供应链攻击（如微调）和运行时攻击（如低级别参数破坏）来诱导此行为。最后，在多个指令调整后的LLM及基准上验证了所提攻击的有效性和现有防御措施的局限性。

Result: 研究表明，所提出的攻击方法能够有效地导致对话轮次显著增加，同时保持对现有合规要求的遵守。此外，实验结果表明当前已有的防护手段对于此类新型失效模式所提供的保护作用相当有限。

Conclusion: 本研究首次提出了对话型LLM中的‘回合放大’问题，揭示了其背后的机制，并演示了如何通过操纵模型行为来放大对话成本。研究指出，未来需要开发更有效的策略来抵御这类基于对话动态而非单轮提示优化的攻击方式。

Abstract: Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.

</details>


### [33] [Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds](https://arxiv.org/abs/2602.17798)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种新的专家混合模型路由框架GrMoE，该框架通过控制Bingham分布的浓度矩阵来连续调节路由熵，从而实现对稀疏性和利用率之间权衡的控制。


<details>
  <summary>Details</summary>
Motivation: 现有的专家混合模型依赖于学习到的路由器将令牌分配给专家，但标准的softmax门控机制无法提供一种原则性的方法来控制稀疏性与利用率之间的平衡。此外，缺乏一个正式理论来支持浓度控制下的稀疏性。

Method: 研究者们开发了Grassmannian MoE (GrMoE)这一基于Grassmannian流形上的子空间运作的新路由框架，其中门控权重来自于Matrix Bingham分布的浓度参数，并且还提出了针对后验路由分布的一种摊销变分推理过程。

Result: 实验结果表明，在多个不同规模的语言模型上，GrMoE能够达到0%的路由崩溃率，同时保持或优于原有模型的困惑度，并提高了15-30%的负载均衡效率。此外，GrMoE展示了浓度与有效稀疏度间的平滑单调关系，使得无需重新训练即可调整稀疏度。

Conclusion: GrMoE为解决Mixture-of-Experts模型中的稀疏性控制问题提供了有效的解决方案，不仅提升了模型性能还增强了路由行为的可解释性。

Abstract: Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $Λ$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\% routing collapse across all seeds, comparable or better perplexity with 15--30\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.

</details>


### [34] [Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.17809)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种名为Stiefel-Bayes Adapters (SBA)的贝叶斯参数高效微调框架，该方法在Stiefel流形上对正交适配器因子施加矩阵Langevin先验，并通过切空间拉普拉斯近似与测地线回缩执行近似后验推理。实验结果表明，SBA在保持与现有方法（如LoRA和DoRA）相当的任务性能的同时，显著提高了预测不确定性校准效果、选择性预测AUROC以及OOD检测性能，且参数成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法如LoRA虽然能够实现大型语言模型的有效适应，但缺乏原则性的不确定性估计，导致预测不准确及在领域迁移时行为不可靠。

Method: 提出了Stiefel-Bayes Adapters (SBA)，一种新的贝叶斯参数高效微调框架，它在Stiefel流形上为正交适配器因子引入了矩阵Langevin先验，并通过切空间中的拉普拉斯近似加上测地线回缩来进行近似后验推断。

Result: 实验涵盖了GLUE和SuperGLUE基准测试、领域迁移评估、选择性预测协议以及抽象摘要任务等场景，在这些场景下，SBA不仅达到了与LoRA和DoRA相似的任务表现水平，还减少了18%到34%的预期校准误差，提升了领域迁移情况下选择性预测的AUROC 12%至25%，并且在OOD检测方面优于由五个LoRA模型组成的深度集成，同时只消耗了少量的参数。

Conclusion: 研究表明，将不确定性置于正确的几何结构上比简单地向适配器添加任何贝叶斯处理更为重要，SBA为此提供了一个有效的解决方案。

Abstract: Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.

</details>


### [35] [Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models](https://arxiv.org/abs/2602.17829)
*Preetom Biswas,Giulia Pedrielli,K. Selçuk Candan*

Main category: cs.LG

TL;DR: 本文提出了一种名为ruleXplain的框架，该框架利用大型语言模型从模拟驱动的动态系统中提取输入-输出关系的形式化解释。通过引入具有时间操作符和延迟语义的约束符号规则语言，并结合模拟器生成多样化的反事实输入轨迹，使得LLM能够生成可验证的因果规则。此外，通过闭环精炼过程保证了规则的一致性和语义有效性。研究使用两种不同的模拟器进行了验证：PySIRTEM流行病模拟器和EnergyPlus建筑能源模拟器，并通过三类实验评估了方法的有效性、因果编码能力以及规则集对未见输出趋势的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的方法在处理具有延迟效应的时间序列数据时难以产生通用且可解释性强的因果关系解释，尤其是在系统表现出无法通过简单函数映射捕捉到的复杂动态情况下。为了解决这个问题，作者们提出了一个新框架ruleXplain。

Method: ruleXplain框架利用大型语言模型（LLMs）从模拟驱动的动力学系统中提取关于输入-输出关系的形式化解释。它引入了一种包含时间算子和延迟语义的受限符号规则语言，允许LLMs通过结构化的提示生成可验证的因果规则。此方法依赖于一个原则性的模型（如模拟器），该模型可以将多变量输入时间序列映射到输出时间序列。模拟器用于生成多种反事实输入轨迹作为候选解释，并通过聚类提供给LLM作为上下文，任务是生成编码导致输出时间序列模式联合时间趋势的符号规则。整个过程包括一个闭合循环细化步骤以确保规则一致性和语义正确性。

Result: 通过对PySIRTEM流行病模拟器（测试率输入与每日感染计数的关系）及EnergyPlus建筑能耗模拟器（温度和太阳辐射输入对电力需求的影响）的应用，验证了ruleXplain框架的有效性。实验分为三大类：(1) 通过输入重建来检验规则集的效果；(2) 通过消融研究评价规则集中因果关系的编码情况；(3) 对提取出的规则在不同相位动态下的未见输出趋势进行泛化测试。

Conclusion: 提出的ruleXplain框架成功地利用大型语言模型为复杂的动力学系统提供了可解释性强的因果规则。这些规则不仅能够准确描述已知数据中的因果关系，而且还能很好地泛化至新的场景下，显示出其强大的实用价值。

Abstract: Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.

</details>


### [36] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: 提出了一种基于多项式能量模型的新策略参数化方法MePoly，旨在解决现有随机最优控制中参数化策略难以表示解的多模态性和基于扩散的策略缺乏显式概率密度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的参数化策略在表示解的多模态性方面表现不佳，而基于扩散的策略虽然能够恢复多模态性但缺乏显式的概率密度函数，这给策略梯度优化带来了挑战。

Method: 本文提出了名为MePoly的新策略参数化方案，该方案基于多项式能量模型构建，提供了明确且可处理的概率密度，允许进行精确的熵最大化。其理论基础是经典的矩问题，并利用了对于任意分布具有通用逼近能力的特点。

Result: 实验结果表明，MePoly能够有效地捕捉复杂的非凸流形，并在各种基准测试中的性能优于基线方法。

Conclusion: 通过引入MePoly作为新的策略参数化手段，研究者们不仅解决了传统方法在处理多模态解时遇到的问题，还为实现更高效的策略梯度优化提供了一条可行路径。

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [37] [Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846)
*Nick Dodson,Xinyu Gao,Qingsong Wang,Yusu Wang,Zhengchao Wan*

Main category: cs.LG

TL;DR: 本文提出了一种几何框架，用于理解扩散模型在噪声调度过程中何时发生记忆化或泛化，并发现中等噪声水平是记忆化风险最高的'危险区'。对于这一区域，文章提出了基于几何条件的干预措施以减轻记忆化问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能够生成高质量样本，但同时也可能记住训练数据，引发隐私问题。目前尚不清楚记忆化与泛化发生的机制、沿噪声调度的记忆化位置、数据几何如何影响记忆化以及不同噪声尺度下现象之间的相互作用。

Method: 引入了一个基于高斯壳层覆盖特性和后验集中行为的几何框架，将噪声调度划分为三个阶段。

Result: 揭示了记忆化风险在不同噪声水平下的非均匀分布；识别出中等噪声水平为记忆化最显著的‘危险区’；小噪声和大噪声区域通过不同的机制抵抗记忆化。

Conclusion: 提出了一个针对中等噪声水平的记忆化缓解策略，该策略基于特定的几何条件。

Abstract: Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

</details>


### [38] [JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861)
*Ryan McKenna,Galen Andrew,Borja Balle,Vadym Doroshenko,Arun Ganesh,Weiwei Kong,Alex Kurakin,Brendan McMahan,Mikhail Pravilov*

Main category: cs.LG

TL;DR: JAX-Privacy是一个专注于简化部署差异隐私机器学习机制的库，它结合了近期大量相关研究，提供了包括批选择、梯度裁剪、噪音添加等功能在内的验证过的模块化组件。


<details>
  <summary>Details</summary>
Motivation: 为了简化强健且高效的差异隐私机器学习机制的部署过程，并同时满足研究人员对于深度定制的需求以及实践者对开箱即用体验的要求。

Method: 通过遵循可用性、灵活性和效率的设计原则开发出JAX-Privacy库，该库为机制设计的所有关键方面（如批处理选择、梯度裁剪、噪声添加等）提供经过验证的模块化基础组件。

Result: JAX-Privacy能够支持从高度定制到直接应用的不同需求，整合了大量的最新研究成果来促进差异隐私在机器学习中的应用。

Conclusion: JAX-Privacy作为一个综合性的解决方案，旨在使差异隐私技术更加易于使用并提高其性能，从而推动该领域的发展。

Abstract: JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

</details>


### [39] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: 本文提出了一种名为ADAPT的新方法，结合了束搜索初始化与自适应梯度引导的变异，旨在克服文本离散性及现有提示优化技术在局部极小值问题上的局限。通过在Gemma 2 2B的稀疏自动编码器潜在变量上进行评估，并提出基于数据集激活统计的新指标，证明ADAPT在不同层和潜在类型中均优于先前的方法。


<details>
  <summary>Details</summary>
Motivation: 理解LLM激活空间中学到的方向所编码的特征需要识别强烈激活这些方向的输入。由于文本固有的离散性质，为大型语言模型（LLMs）探索特征可视化技术具有挑战性。此外，现有的提示优化技术在这个领域容易陷入局部最优解。

Method: 研究者提出了ADAPT方法，它是一种混合方法，结合了束搜索初始化与根据梯度调整的突变策略，专门针对上述难题而设计。

Result: 通过对Gemma 2 2B模型中的稀疏自动编码器潜在变量进行测试，并使用基于数据集激活统计数据的新指标来比较，结果表明ADAPT在各种层次和不同类型潜在变量上始终优于之前的方法。

Conclusion: 本研究表明，对于LLMs而言，特征可视化是可行的，但需要针对该领域的特定假设来进行设计。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [40] [MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies](https://arxiv.org/abs/2602.17868)
*Vasilii Feofanov,Songkang Wen,Jianfeng Zhang,Lujia Pan,Ievgen Redko*

Main category: cs.LG

TL;DR: 本研究通过引入Mantis+和MantisV2模型、改进测试时方法以及采用自集成和跨模型嵌入融合技术，显著提高了时间序列分类基础模型的零样本特征提取能力，在多个基准数据集上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 尽管早期的时间序列分类基础模型如Mantis展示了作为通用特征提取器应用于各种下游任务的可能性，但在冻结编码器与微调编码器之间仍存在明显的性能差距。为了解决这个问题，并进一步提高零样本特征提取的能力，本研究提出了新的方法和技术。

Method: 1. 引入了Mantis+，一种完全基于合成时间序列预训练的Mantis变体。
2. 通过控制性消融研究精炼架构，开发出更轻量级的编码器MantisV2。
3. 提出了一种增强型测试时方法，利用中间层表示并优化输出标记聚合。
4. 展示了通过自我集成和跨模型嵌入融合可以进一步提升性能。

Result: 广泛的实验表明，MantisV2和Mantis+在UCR、UEA、人类活动识别（HAR）基准以及EEG数据集上一致地超越了先前的时间序列基础模型，达到了最先进水平的零样本性能。

Conclusion: 这项工作通过引入新型模型和技术，显著增强了时间序列分类中零样本特征提取的表现力，为该领域提供了更加高效且强大的解决方案。

Abstract: Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.

</details>


### [41] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 本研究评估了监督机器学习模型在预测慢性鼻窦炎(CRS)患者手术效益方面的应用，使用术前数据训练的模型能够以约85%的分类准确率识别可能不需要手术的患者。在30个不同难度级别的案例测试中，模型达到了80%的准确率，超过了临床专家75.6%的平均预测准确率，显示出其支持个性化CRS护理和增强临床决策潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在医学预后领域取得了显著进展，但对于前瞻性收集的标准观察性临床干预试验数据的应用探索仍不足，而这些技术有潜力降低成本并改善患者结果。特别是对于慢性鼻窦炎（CRS）这种持续时间超过三个月、严重影响生活质量与社会成本的疾病而言，手术决策因其需要权衡已知程序风险与个体化结果不确定性而变得复杂。

Method: 研究采用了监督机器学习模型来预测CRS患者的手术效益，主要依据是Sino-Nasal Outcome Test-22 (SNOT-22)这一患者报告的结果指标。所有参与者均接受了手术治疗；研究者试图通过仅基于术前信息训练出的模型来确定哪些患者或许可以避免手术。此外，还比较了多个算法的表现，并采用了一种集成方法以提高预测准确性。

Result: 最佳模型在多种算法中实现了大约85%的分类准确度，表明它能够提供精确且可解释的手术候选资格预测。进一步地，在一组包含混合难度水平的30个案例上进行验证时，该模型达到了80%的准确度，优于专家临床医生75.6%的平均水平。

Conclusion: 研究表明，利用监督机器学习模型预测CRS患者是否适合接受手术具有很高的准确性，并且该模型提供的预测结果甚至优于经验丰富的临床医生。这表明此类AI工具具备辅助临床决策和支持个性化医疗护理的巨大潜力。

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [42] [Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898)
*Jingquan Yan,Yuwei Miao,Peiran Yu,Junzhou Huang*

Main category: cs.LG

TL;DR: 本文首次对注意力回归模型在训练过程中出现的PCC停滞现象进行了理论分析，揭示了优化动态和模型容量的基本限制，并提出了新的Extrapolative Correlation Attention (ECA)机制来克服这些限制，在保持MSE性能的同时显著提高相关性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决注意力回归模型训练中常见的但理解不足的现象——PCC停滞问题，即尽管MSE持续下降，PCC却很早就停止改善。通过深入探讨这一现象背后的原因，文章希望为改进模型提供新的思路和技术。

Method: 通过对PCC停滞现象进行严格的理论分析，识别出优化动力学中的关键冲突以及模型能力上的限制。基于这些发现，提出了一种新的机制——Extrapolative Correlation Attention (ECA)，该机制设计用于超越输入数据的凸包限制，从而更好地优化PCC。

Result: ECA能够在不同的基准测试中打破PCC停滞现象，即使是在面对挑战性的同质数据集时也能显著提升相关性，同时不牺牲MSE的表现。

Conclusion: 本研究表明，通过引入ECA这种新方法可以有效解决传统注意力机制中存在的PCC优化难题，为开发更强大的回归模型提供了新的方向。

Abstract: Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

</details>


### [43] [Distribution-Free Sequential Prediction with Abstentions](https://arxiv.org/abs/2602.17918)
*Jialin Yu,Moïse Blanchard*

Main category: cs.LG

TL;DR: 本文研究了在半对抗环境下，学习者可以对可能被污染的实例选择不进行预测的问题。提出了一个名为AbstainBoost的算法，该算法基于弱学习者的提升过程，在未知分布的情况下也能保证VC类的一般性错误为次线性的。此外，对于自适应对手和结构化函数类（如线性分类器），这些算法也具有类似的保证。


<details>
  <summary>Details</summary>
Motivation: 作者们探索了在不知道干净样本分布$μ$的前提下，是否能够实现与已知分布时相似的学习保证。这一问题的意义在于，现实世界中往往无法事先得知数据的确切分布情况，因此寻找一种不需要先验分布知识的方法显得尤为重要。

Method: 提出了一种新的算法AbstainBoost，它利用弱学习者的提升过程来处理未知分布下的学习任务。该方法允许学习者在面对可能是被污染的数据时选择不作出预测，从而避免受到惩罚。

Result: 研究表明，对于一般VC类，在未知分布的放弃学习中，AbstainBoost算法能够保证次线性的错误率。同时，针对自适应对手以及包括线性分类器在内的结构化函数类，该算法同样提供了相应的性能保障。

Conclusion: 通过引入AbstainBoost算法，即使在没有先验分布知识的情况下，也可以有效地处理包含任意数量敌对实例的序列预测问题，并且对于不同的对手类型及函数类别都表现出了良好的适应性和鲁棒性。

Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $μ$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $μ$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.

</details>


### [44] [Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere](https://arxiv.org/abs/2602.17940)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: 本研究针对高斯过程（GP）bandit问题在频度设置下，特别是在超球输入域上，提供了算法无关的最坏情况下的下界。对于平方指数（SE）核函数，证明了任何算法都会遭受特定形式的累积遗憾，并且要找到一个ε-最优解需要特定数量的时间步骤。此外，还改进了SE核的最大信息增益的上界。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决GP bandit问题中关于维度依赖的对数因子之间的差距这一开放性问题，特别是对于广泛使用的SE核函数。

Method: 通过理论分析，在超球输入域上为使用SE核的GP bandit问题建立了新的下界，并改进了最大信息增益的上界估计。

Result: 证明了任何算法在给定条件下都将面临特定形式的累积遗憾和简单遗憾，并给出了改进后的最大信息增益上界。

Conclusion: 结果表明，在超球输入域的情况下，现有最佳算法在维度独立的对数因子方面是最优的。

Abstract: We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$ time steps to find an $ε$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.

</details>


### [45] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 本文分析了基于梯度的超参数优化中超梯度估计误差的偏差-方差分解，并提出了一种集成超梯度策略来有效减少方差。实验结果表明，所提出的方差减少策略提高了超梯度估计的效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于梯度的超参数优化方法主要关注于减少估计与真实值之间的偏差，而忽略了由于数据分布导致的误差（即方差），这会降低性能。为了解决这个问题，文章进行了超梯度估计误差的偏差-方差分解，并特别分析了之前被忽略的方差项。

Method: 作者首先对超梯度估计误差进行了偏差-方差分解，并提供了对方差项的详细分析。接着，提出了一个综合性的超梯度估计错误界限分析。受这些理论启发，他们提出了一种集成超梯度策略以在超参数优化算法中有效地减少方差。

Result: 实验结果显示，在包括正则化超参数学习、数据超清理和少样本学习的任务上，所提出的方差减少策略确实改善了超梯度估计的表现。此外，通过建立超额误差与超梯度估计间的联系，为实证观察提供了一些解释。

Conclusion: 通过考虑并处理超梯度估计中的方差问题，可以有效提高基于梯度的超参数优化算法的性能。新提出的集成超梯度策略证明是一种有效的方差减少方法。

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [46] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: 研究通过使用对称性破坏维度扩展（SBDE）来探索深度学习中清洁准确性和对抗鲁棒性之间权衡的机制。SBDE 能够提高清洁准确性，但会降低模型对迭代白盒攻击的鲁棒性。通过在测试时应用掩码投影技术，可以恢复模型的鲁棒性，揭示了模型通过沿辅助轴创建尖锐边界来实现高准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨深度学习领域内清洁准确度与对抗鲁棒性之间的权衡现象背后的几何学原因。

Method: 采用对称性破坏维度扩展（SBDE）作为受控探针，通过向输入图像插入常数值像素来打破平移对称性，并且通过训练值重置这些辅助像素来应用测试时间掩码投影。

Result: SBDE提高了CIFAR-10数据集上ResNet-18模型的清洁准确性，但是降低了对迭代白盒攻击的鲁棒性；通过掩码投影可以恢复鲁棒性，显示模型通过沿着辅助轴建立尖锐边界以达到高准确性。

Conclusion: 研究结果提供了一个具体的几何解释：优化景观加深了吸引盆地以提高准确性，但不可避免地在辅助自由度上建立了陡峭的壁，从而对离流形扰动产生了脆弱的敏感性。

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [47] [Bayesian Online Model Selection](https://arxiv.org/abs/2602.17958)
*Aida Afshar,Yuke Zhang,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 本文提出了一种新的贝叶斯算法，用于随机强盗中的在线模型选择，并证明了该算法在贝叶斯遗憾上有oracle式的保证。此外，还研究了基础学习者间共享数据对缓解先验误设的影响。


<details>
  <summary>Details</summary>
Motivation: 为了解决从先验分布中抽取环境实例时，在多个强盗学习器中探索并最终与最佳学习器竞争的问题。

Method: 通过引入一种新的贝叶斯算法来解决随机强盗中的在线模型选择问题。

Result: 证明了新算法在贝叶斯遗憾上具有$O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$的oracle式保障；实验验证表明该方法在多种随机强盗设置下性能与最佳基础学习器相当；研究发现基础学习器间共享数据有助于减轻先验误设的影响。

Conclusion: 提出的新贝叶斯算法有效解决了随机强盗中的在线模型选择挑战，同时展示了数据共享对于处理先验不确定性的重要性。

Abstract: Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.

</details>


### [48] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 本论文研究了机器学习中的三个不同项目：监督学习和流形学习的函数逼近问题、迁移学习中领域间知识转移的问题以及通过信号分离技术提出的一种新的分类方法。


<details>
  <summary>Details</summary>
Motivation: 为了改进现有机器学习方法在理论上的不足，作者致力于探索数学理论如何应用于机器学习问题上，以期提高模型性能及效率。

Method: 1. 提出了一种新的监督学习方法来解决函数逼近问题；2. 研究了部分已知数据条件下从一个领域到另一个领域的知识迁移机制；3. 基于信号分离技术开发了一个用于分类任务的新算法。

Result: 1. 新提出的监督学习方法理论上解决了当前范式的一些缺点；2. 对迁移学习中局部平滑性和功能提升之间的关系进行了探讨；3. 开发的新算法不仅准确性高，而且速度更快。

Conclusion: 这项工作为机器学习中的关键挑战提供了新颖且有效的解决方案，特别是在处理大规模数据集时展现出显著优势。

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [49] [PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting](https://arxiv.org/abs/2602.17998)
*Shubham Bhardwaj,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 本文提出了一种名为PHAST的结构化模型，用于从仅位置观测数据中学习物理系统的动力学。该模型在保证长期预测稳定性的同时，也能恢复具有物理意义的参数。


<details>
  <summary>Details</summary>
Motivation: 现实中的物理系统是耗散的，比如摆会减速，电路会因为热量而失去电荷。从部分观测数据中预测这些系统的动态是科学机器学习中的一个核心挑战。

Method: 本文引入了PHAST（Port-Hamiltonian Architecture for Structured Temporal dynamics），它将哈密顿量分解为势能、质量和阻尼三个部分，并根据知识体系的不同采用不同的参数化方法。此外，通过使用Strang分裂法推进动力学演化。

Result: 在跨越机械、电气、分子、热力学、重力和生态系统的十三个仅基于位置的数据集上，PHAST相比其他基线方法实现了最佳的长时域预测性能，并且当提供的信息足够时，能够恢复出具有物理意义的参数。

Conclusion: 研究显示，在没有足够的锚点情况下识别问题是本质上不适定的（存在规范自由度问题）。因此，建议采用两轴评估方法来区分预测稳定性和可识别性。

Abstract: Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.

</details>


### [50] [Asynchronous Heavy-Tailed Optimization](https://arxiv.org/abs/2602.18002)
*Junfei Sun,Dixi Yao,Xuchen Gong,Tahseen Rabbani,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: 该论文研究了在重尾梯度噪声存在下，两种处理延迟更新的通信方案，并提出了基于延迟感知学习率调度和延迟补偿的算法修改。这些方法在准确性和运行时间权衡方面优于现有的同步和异步方法，并且对超参数更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 当前工作主要集中在开发和理解集中式或分布式同步设置中解决重尾噪声的方法，而忽视了这种噪声与异步优化之间的相互作用。本文旨在探索在重尾梯度噪声条件下如何通过异步更新来处理延迟问题。

Method: 提出并从理论上分析了基于延迟感知学习率调度和延迟补偿的算法修改以增强异步算法性能。

Result: 所提方法在图像和语言任务中都表现出了更好的准确性和运行时间权衡，并且对于超参数更加稳健。此外，还证明了在重尾噪声下的收敛性保证可以与同步版本相匹配，同时相比现有异步方法提高了对延迟的容忍度。

Conclusion: 本研究表明，在面对重尾梯度噪声时，通过适当调整如延迟感知学习率调度等策略，异步更新机制能够有效提高训练效率和模型性能。

Abstract: Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.

</details>


### [51] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 本文提出了Neural-Integrated Mechanistic Modeling (NIMM)评估框架，用于在现实条件下评估由大型语言模型生成的机制模型，并基于发现设计了NIMMgen框架来通过迭代改进增强代码正确性和实际有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究中使用大语言模型自动从数据构建机制模型的问题设定过于简化，导致不清楚这些模型在实践中的可靠性。因此需要一种新的评估框架来测试这些模型在更接近真实世界条件下的表现。

Method: 提出了一种名为Neural-Integrated Mechanistic Modeling (NIMM)的评估框架，该框架能够在部分观测和多样化任务目标的情况下评估大语言模型生成的机制模型。基于NIMM框架下发现的基本挑战，进一步开发了NIMMgen框架，通过迭代优化提升模型的有效性和代码层面的准确性。

Result: 实验结果表明，在三个不同科学领域的数据集上，NIMMgen框架展现了强大的性能。此外，学习到的机制模型还支持反事实干预模拟。

Conclusion: NIMM评价框架揭示了当前基线方法面临的重要挑战，而NIMMgen框架则提供了解决这些问题的一种新途径，能够提高机制模型的实际应用价值。

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [52] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 该论文提出了一种新的方法，通过梯度正则化来训练语言模型，以减少奖励黑客行为。与传统的KL惩罚相比，梯度正则化能够促使策略更新偏向于奖励更准确的区域，从而提高整体表现。


<details>
  <summary>Details</summary>
Motivation: 针对强化学习中常见的奖励黑客问题，即策略可能利用奖励函数的不准确性学习到非预期的行为，本研究旨在探索一种新方法来改善这一状况。

Method: 首先从理论上建立了奖励模型准确性与最优解平坦性之间的联系，然后引入了梯度正则化（GR）技术，用于引导训练过程朝向更加平坦、奖励更准确的区域发展。此外，还提出了一种有效的有限差分估计方法来进行显式的梯度正则化处理。

Result: 实验结果表明，在多种基于语言模型的强化学习任务中，梯度正则化比KL惩罚表现得更好。它不仅提高了GPT评价下的胜率，避免了对格式的过度关注，并且防止了在使用大型语言模型作为裁判进行数学任务时出现的作弊行为。

Conclusion: 本文提出的梯度正则化方法为解决强化学习中的奖励黑客问题提供了一个有效的新视角。相较于现有的解决方案如KL惩罚，梯度正则化能够在保持甚至提高奖励准确性的同时，优化模型的表现。

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [53] [Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework](https://arxiv.org/abs/2602.18055)
*Jingyang Qiao,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yanyun Qu,Yuan Xie*

Main category: cs.LG

TL;DR: 本文提出了Continual-NExT框架和MAGE方法，以解决Dual-to-Dual MLLMs在持续学习过程中面临的知识遗忘、幻觉生成、指令不遵从以及跨模态知识转移失败等问题。实验表明，MAGE方法在持续学习能力上优于其他方法，并达到了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 尽管Dual-to-Dual MLLMs展示了强大的即时学习和泛化能力，但在长期演化方面仍存在不足，特别是在适应动态现实场景时。此外，这类模型在学习新任务时面临已学知识被破坏的问题，除了传统的灾难性遗忘外，还存在着幻觉生成、指令不遵从以及跨模态知识转移失败等挑战。目前还没有为Dual-to-Dual MLLMs建立标准的持续学习框架来探索这些挑战。

Method: 为了提高Dual-to-Dual MLLMs的持续学习能力，本文提出了名为MAGE（General LoRA与Expert LoRA的混合及聚合）的方法，旨在促进不同模态间的知识转移同时减轻遗忘现象。此外，还建立了名为Continual-NExT的持续学习框架，该框架配备有精心设计的评估指标。

Result: 广泛的实验证明了MAGE方法的有效性，在改善Dual-to-Dual MLLMs持续学习能力方面超越了现有的其他持续学习方法，并实现了领先的表现水平。

Conclusion: 通过提出新的持续学习框架Continual-NExT及MAGE方法，本研究为增强Dual-to-Dual MLLMs面对持续学习挑战的能力提供了有效解决方案。

Abstract: Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.

</details>


### [54] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: 本文研究了图生成模型中等变性带来的权衡问题，通过在训练过程中使用基于正弦位置编码和节点置换的可控对称性调节方案来放松等变性要求。实验表明，适度打破对称性能加速模型收敛并延迟过拟合，从而在仅使用基线训练周期19%的情况下达到更好的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索图生成模型中的等变性与计算成本及收敛速度之间的权衡。尽管等变性保证了模型对于图的置换对称性的尊重，但严格的等变性增加了架构上的约束，并可能因需要考虑大量可能的节点排列而减慢收敛速度。

Method: 从一个等变离散流匹配模型出发，通过引入一种基于正弦位置编码和节点置换的可控制对称性调制方案，在训练期间放松模型的等变性要求。

Result: 实验结果显示，适度打破对称性能够为模型提供更容易的学习信号以加速早期训练过程；然而，如果过度，则可能导致鼓励捷径解决方案进而引发过拟合现象，即模型反复生成训练集中的重复图。相反地，适当调整对称性信号可以在加快收敛的同时推迟过拟合的发生，使得模型能够在仅利用基线训练轮次19%的情况下实现更优的表现。

Conclusion: 研究表明，通过合理调节图生成模型中的对称性程度，可以在不显著增加计算负担的前提下有效提升模型效率和最终性能，避免过早陷入局部最优解或过拟合状态。

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [55] [Non-Stationary Online Resource Allocation: Learning from a Single Sample](https://arxiv.org/abs/2602.18114)
*Yiding Feng,Jiashuo Jiang,Yige Wang*

Main category: cs.LG

TL;DR: 本文研究了在非平稳需求下在线资源分配问题，提出了一种基于类型依赖分位数的元策略，并针对不同类型的信息样本设计了相应的策略。对于奖励观察样本，实现了近似$\tilde{O}(\sqrt{T})$的遗憾；而对于仅类型信息样本，在满足最小到达概率假设的前提下，设计了一种完全自适应解决策略，达到了首个多项式对数遗憾保证$O((\log T)^3)$。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于处理非平稳需求下的在线资源分配问题，同时要求算法只需要每个周期的历史数据样本来有效运作。该研究区分了两种不同信息量的样本情况：既包含查询类型也包括奖励实现的奖励观察样本，以及只提供查询类型信息的更具有挑战性的仅类型样本。

Method: 提出了一种新颖的基于类型依赖分位数的元策略，将问题分解为几个模块化组件：奖励分布估计、通过流体松弛优化目标服务概率、以及通过动态接受阈值进行实时决策。对于仅类型样本的情况，在轻微的最小到达概率假设下，设计了部分自适应策略和一种更为重要的全自适应解决策略。

Result: 对于奖励观察样本，静态阈值策略达成了近似$\tilde{O}(\sqrt{T})$的遗憾；对于仅类型样本，首次证明了没有额外结构时次线性遗憾是不可能的，但通过提出的策略获得了$O((\log T)^3)$的多项式对数遗憾保障。

Conclusion: 本研究推进了先前的工作，通过使用最少的离线数据（每个时期一个样本）、处理无变化预算假设下的任意非平稳性以及支持多个资源约束，为非平稳多资源分配问题提供了新的解决方案。

Abstract: We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.

</details>


### [56] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出了一种新的方法FINO，通过在策略训练中加入噪声来提高从离线到在线强化学习的样本效率，并结合了熵引导的采样机制以平衡探索和利用。实验表明，在有限的在线预算下，FINO能够一致地达到更好的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然生成模型在离线强化学习中表现出色，但它们向在线微调扩展时遇到了未解决的关键挑战。为了促进有效的探索并增强样本效率，研究者提出了FINO方法。

Method: FINO方法基于流匹配策略，并通过在策略训练过程中注入噪声来鼓励超出离线数据集观察范围的更广泛动作。此外，该方法还结合了熵引导采样机制，以在整个在线微调期间平衡探索与开发。

Result: 跨多个具有挑战性的任务的实验表明，FINO在有限的在线预算条件下始终表现出优越的性能。

Conclusion: FINO为从离线到在线强化学习提供了一条有效途径，通过改善探索过程中的样本效率以及动态调整探索与利用之间的平衡，从而在不同任务上实现卓越表现。

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [57] [A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction](https://arxiv.org/abs/2602.18168)
*Danning Jing,Xinhai Chen,Xifeng Pu,Jie Hu,Chao Huang,Xuguang Chen,Qinglin Wang,Jie Liu*

Main category: cs.LG

TL;DR: RGD-Blast, a deep learning-based model, is proposed to accurately and efficiently predict long-term blast wave propagation. It integrates multi-scale and dynamic-static feature coupling mechanisms to improve generalization and reduce error accumulation, achieving significant speedup over traditional methods without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to overcome the challenges of modeling blast wave propagation, such as high nonlinearity, sharp gradients, and computational cost, while also addressing the limitations of existing machine learning models in terms of accuracy, especially in complex or out-of-distribution scenarios, and their tendency for error accumulation over extended forecasting periods.

Method: The method involves developing RGD-Blast, a novel deep surrogate model that utilizes a multi-scale module to capture both global and local flow dynamics, and a dynamic-static feature coupling mechanism to integrate time-varying and static features, thus enhancing its capability to generalize to unseen conditions and maintain accuracy over longer forecasting horizons.

Result: RGD-Blast demonstrates a two-order-of-magnitude speedup compared to traditional numerical methods, with comparable accuracy. It achieves an average RMSE below 0.01 and an R2 score above 0.89 across 280 consecutive time steps on unseen building layouts, showing strong generalization capabilities even under varying blast source locations and explosive charge weights.

Conclusion: RGD-Blast represents a significant advancement in long-term blast wave modeling by providing a robust, accurate, and computationally efficient solution that can effectively handle complex urban environments and out-of-distribution scenarios, making it a promising tool for real-world applications.

Abstract: Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.

</details>


### [58] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 本文提出了一种新的正式框架，用于通过使用双逻辑公式来衡量AI倾向性，该公式将模型的成功概率归因于当其倾向性处于"理想区间"内时。此外，利用配备有新开发的任务无关评分标准的大语言模型估计了理想区间的界限。研究发现，通过这种方法可以测量倾向性的变化以及这种变化对任务的影响，并且使用一个基准估计的倾向性能成功预测在保留任务上的行为。结合倾向性和能力比单独使用任一者具有更强的预测力。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能评估主要集中在能力测量上，但倾向性——模型表现出特定行为的趋势——在决定性能和安全结果方面起着核心作用。然而，传统的项目反应理论（IRT）方法描述了模型在任务上的成功是模型能力和任务需求的单调函数，这不适合处理倾向性问题，因为在倾向性问题中，过剩和不足都可能是有问题的。因此，需要一个新的框架来更准确地衡量AI模型的倾向性。

Method: 作者引入了一种基于双逻辑公式的新方法来定义模型的成功概率，特别是当模型的倾向性落在某个“理想区间”内时。为了估计这个理想区间的边界，他们开发了新的、与任务无关的评分标准，并将其应用于大语言模型（LLMs）。通过对六个不同家族的语言模型进行实验，在这些模型中人为地激发了倾向性的变化，以此来验证这一框架的有效性。

Result: 研究结果表明，所提出的框架能够有效测量倾向性的变动及其对任务表现的影响。重要的是，利用单一基准测试估算出的倾向性可以成功预测在其他未见过的任务上的行为。此外，将倾向性与能力相结合比单独考虑任一方面提供了更强的行为预测能力。

Conclusion: 这项工作展示了如何严谨地进行倾向性测量，并且相对于仅依赖能力评估来预测AI行为而言，它带来了显著的优势。

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [59] [LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification](https://arxiv.org/abs/2602.18195)
*Hairong Chen,Yicheng Feng,Ziyu Jia,Samir Bhatt,Hengguan Huang*

Main category: cs.LG

TL;DR: 提出了一种名为LERD的贝叶斯电生理神经动力学系统，该系统能够直接从多通道EEG中推断出潜在的神经事件及其关系结构，而无需事件或交互注释。通过综合实验表明，LERD在合成基准和两个真实世界AD EEG队列上均优于强大的基线模型，并提供了与生理一致的潜在摘要，有助于表征组级动态差异。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）改变了大脑电生理并破坏了多通道EEG动态，使得基于EEG的准确且具有临床价值的诊断对于筛查和疾病监测变得越来越重要。然而，许多现有的方法依赖于黑箱分类器，并没有明确地建模生成观察信号的基本动态。为了解决这些限制，提出了一个新的解决方案。

Method: 提出了LERD，一个端到端的贝叶斯电生理神经动力学系统，可以从没有事件或交互注释的多通道EEG中直接推断出潜在神经事件及其关系结构。LERD结合了一个连续时间事件推理模块与随机事件生成过程来捕捉灵活的时间模式，同时融入了受电生理启发的动力学先验来指导学习过程。此外，还提供了理论分析，得到了训练用的可处理边界以及对推断出的关系动态的稳定性保证。

Result: 广泛的实验显示，在合成基准测试及两个真实世界的AD EEG数据集上，LERD始终优于强劲的基线，并生成了与生理相符的潜在总结，这有助于描述组级别的动态差异。

Conclusion: 本研究介绍了一种创新的方法——LERD，它能够在不需要额外标注的情况下直接从EEG数据中推断出潜在的神经活动及其相互作用。这种方法不仅提高了对AD等疾病的诊断准确性，而且提供了更深入理解大脑电活动背后机制的可能性。

Abstract: Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.

</details>


### [60] [RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference](https://arxiv.org/abs/2602.18196)
*Xiuying Wei,Caglar Gulcehre*

Main category: cs.LG

TL;DR: 本文提出了一种名为RAT+的新架构，它通过全序列递归和主动递归学习来增强注意力机制。RAT+模型在密集预训练后，可以在推理时灵活切换到稀疏模式（可选局部窗口）或混合层/头组合，而无需重新训练不同的稀疏模型。实验表明，在常识推理和LongBench任务上，RAT+能够接近于密集注意力的准确度，并且在扩展参数规模后保持了相同趋势。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化扩张注意力虽然在推理时提供了效率优势，但将预训练的注意力模型稀疏化为扩张模式会导致准确性严重下降。

Method: 引入RAT+架构，该架构通过增加全序列递归和主动递归学习来增强注意力机制。单个RAT+模型只需一次密集预训练，之后可以根据需要快速适应不同的稀疏模式或混合配置。

Result: 在1.5亿参数、100亿token的训练下，RAT+模型在常识推理任务上的表现几乎与密集型模型相当；当使用64的扩张因子时，性能略有2-3点的下降。此外，当转换为top-k块注意力模式时，RAT+的表现优于传统注意力机制。进一步将参数扩大至26亿并基于200亿tokens训练，观察到了类似的趋势。

Conclusion: RAT+提供了一个有效的解决方案，允许从单一密集预训练模型中灵活地导出多种稀疏模式，同时保持良好的性能。

Abstract: Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.

</details>


### [61] [Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction](https://arxiv.org/abs/2602.18227)
*Redwanul Karim,Changhun Kim,Timon Conrad,Nora Gourmelon,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 本文提出了一种基于LoRA+PHead的参数高效领域适应方法，用于物理信息自注意力GNN在电压制度转移下的AC-PF预测。该方法通过限制适应于低秩更新来鼓励符合基尔霍夫定律的行为，并且与全微调相比，在减少85.46%可训练参数的同时，几乎恢复了全微调的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于物理信息图神经求解器在跨体制迁移时通常依赖于完全微调，这导致了高昂的再训练成本，并且在目标域适应和源域保持之间的稳定性-可塑性权衡上提供了有限的控制。因此，需要一种参数高效的领域适应方法，能够在保证物理一致性的同时，降低重新训练的成本。

Method: 采用LoRA应用于注意投影，并选择性地解冻预测头部以调节适应能力。这种方法通过一个基于物理的损失函数促进符合基尔霍夫定律的行为，同时将适应限制为低秩更新。

Result: 提出的LoRA+PHead适应方法在多种电网拓扑中实现了接近全微调的准确性，目标域RMSE差距为$2.6\times10^{-4}$，同时减少了85.46%的可训练参数数量。相对于全微调，LoRA+PHead在领域转移下降低了MV源保留率4.7个百分点（从22.6%降至17.9%），但仍能实现参数高效且物理一致的AC-PF估计。

Conclusion: LoRA+PHead方法提供了一种有效途径，能够在不同电压制度之间进行参数高效且物理一致的AC-PF预测，同时允许对效率-准确性之间的权衡进行控制。

Abstract: Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.

</details>


### [62] [Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver](https://arxiv.org/abs/2602.18248)
*Pietro Sittoni,Emanuele Zangrando,Angelo A. Casulli,Nicola Guglielmi,Francesco Tudisco*

Main category: cs.LG

TL;DR: 本文提出了一种名为Neural-HSS的参数高效架构，该架构基于分层半可分离（HSS）矩阵结构，并在数据稀缺的情况下对一大类偏微分方程（PDEs）表现出色。通过理论分析和实验验证了Neural-HSS在三维泊松方程以及电磁学、流体动力学和生物学等不同领域中从PDE生成的数据学习的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管有高性能计算基础设施可用，但为解决偏微分方程（PDEs）而产生大规模高质量数据集并训练模型仍然面临显著的计算成本问题。受椭圆PDE格林函数结构研究的启发，旨在开发一种更高效的方法来处理这些问题。

Method: 提出了Neural-HSS架构，该架构利用了层次半可分离（HSS）矩阵结构的特点，设计了一个对于广泛类型PDE都具有数据效率的模型。此外，还对该架构进行了理论分析，探究了其与其他架构元素如傅里叶神经算子层和卷积层之间的联系。

Result: 实验表明，在两百万点网格上的三维泊松方程测试中，即使是在低数据条件下，Neural-HSS也能够有效地从由椭圆PDE产生的数据中学习，并且表现优于基线方法。同时，它还能很好地适应来自包括电磁学、流体动力学及生物学等领域内多种PDE的数据。

Conclusion: Neural-HSS架构提供了一种新的途径，能够在低数据环境下有效解决PDEs相关的问题，展现了其在跨学科应用中的潜力。

Abstract: Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.

</details>


### [63] [MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data](https://arxiv.org/abs/2602.18253)
*Xabier de Zuazo,Vincenzo Verbeni,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.LG

TL;DR: 本研究首次展示了基于MEG的语音模型在感知和产生任务之间的迁移学习和跨任务解码。通过预训练一个基于Conformer的模型，并对18名参与者进行微调，结果表明迁移学习不仅提高了每个任务内的性能，还实现了任务间的可靠解码，证实了学习到的表征反映了共享神经过程而非特定于任务的运动活动。


<details>
  <summary>Details</summary>
Motivation: 数据高效的神经解码是语音脑-机接口的核心挑战。这项研究旨在探索是否可以通过迁移学习提高基于MEG的语音模型效率，以及这些模型能否在不同的任务（如语音感知与产生）之间通用。

Method: 研究人员首先在一个受试者上使用50小时的听觉数据预训练了一个基于Conformer架构的模型，然后针对另外18名受试者，在每人仅5分钟的数据上进行了微调。

Result: 采用迁移学习方法后，同一任务内的准确率提高了1-4%，而跨任务间（即从生产到感知或反之）的增益则达到了5-6%。此外，专为语音产生训练的模型也能以高于随机水平的表现解码被动听力任务。

Conclusion: 该研究表明，通过迁移学习可以显著提升基于MEG的语音模型在不同任务中的表现，并且这些模型能够捕捉到跨越感知和产生的共同神经处理机制，而不是仅仅反映特定任务相关的运动活动。

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

</details>


### [64] [Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers](https://arxiv.org/abs/2602.18292)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 本文提出了一种新的解码框架，将解码视为一个优化过程，可以涵盖多种现有解码方法，并且基于此框架设计了名为Best-of-K的新解码器，该解码器在多样本处理中表现出色，特别是在高采样温度下显著提高了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 作者认为现有的解码方法缺乏理论基础，通常被视为一种启发式调整的过程。为了改进这一点，他们提出了一个新的解码框架，旨在通过解决概率单纯形上的正则化问题来平衡模型得分与结构偏好及约束条件。

Method: 论文介绍了一个通用模板，它能够统一包括贪婪解码、Softmax采样、Top-K、Top-P以及Sparsemax风格稀疏性在内的多种解码策略。更重要的是，基于这一框架，作者开发了名为Best-of-K (BoK) 的新解码算法，该算法针对多样本流程进行了优化，特别是自一致性、重排序和验证者选择等方面。

Result: 实验结果显示，使用BoK方法可以在固定的K样本预算内提高找到优质备选方案的概率，从而改善了整体性能。特别地，在Qwen2.5-Math-7B模型上对MATH500数据集进行测试时，当采样温度较高时，准确率提高了18.6%。

Conclusion: 通过将解码视为一个有原则的优化层，本研究不仅提供了一种理解不同解码技术共通结构的方法，还展示了如何利用这种框架轻松创建新的解码算法。所提出的Best-of-K方法在多样本应用场景中显示出巨大潜力。

Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.

</details>


### [65] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过信息论分析了思维链监控系统的有效性，指出了影响其性能的两大误差来源，并提出了两种改进方法以提高监控准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过理论和实践相结合的方法，解决基于大型语言模型的思维链监控系统在实际应用中遇到的问题，特别是如何更准确地检测代码生成过程中的作弊行为等特定属性。

Method: 使用信息论分析来探讨思维链与输出之间非零互信息作为监控可行性的必要但不充分条件；识别出信息差距和引出错误为两大近似误差源；提出直接奖励模型产生易于监控的思维链以及最大化输出与思维链间条件互信息这两种互补方法以改善监控效果。

Result: 实验证明，在不同环境下，所提两种方法都能显著提高监控准确性，同时避免了即使是在对抗性训练条件下思维链质量下降的问题，从而减少了当任务奖励定义不完全时可能出现的奖励作弊现象。

Conclusion: 研究表明，通过针对性的训练目标可以系统地提升思维链监控能力，这对于确保基于LLM的应用程序安全性和可靠性具有重要意义。

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [66] [On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction](https://arxiv.org/abs/2602.18301)
*Ivan Bondarenko,Egor Palkin,Fedor Tikunov*

Main category: cs.LG

TL;DR: 本文研究了冻结的大型语言模型（LLM）如何从两个学习到的基础token中一次性重建大量文本，并探讨了这些基础token所编码的信息以及它们在约束条件下的行为。通过实验，文章分析了语义和句法内容在两个基础token中的分离情况、e-token的稳定性属性及其在重建过程中的注意力模式。此外，测试了两种正则化方案来强制e-token具有语义结构。结果表明m-token比e-token更强烈地捕捉语义信息；基于锚点的约束与重建准确性之间存在显著权衡；关系蒸馏可以在不牺牲重建质量的情况下将批处理级别的语义关系转移到基础token空间中，这为未来非自回归seq2seq系统的发展提供了支持。


<details>
  <summary>Details</summary>
Motivation: 探索冻结的大型语言模型（LLM）能否以单次前向传递的方式从少量学习到的基础token中重建大量文本，从而超越传统的自回归范式。研究旨在理解这些基础token编码何种类型的信息以及它们在不同条件下如何表现。

Method: 进行了一系列实验，包括分离基础token中的语义与句法内容、分析e-token的稳定性特征及可视化重建过程中对e-token的关注模式。还测试了使用教师嵌入的两种正则化方法——基于锚点的损失函数和关系蒸馏目标函数——以考察如何将语义结构施加给e-token。

Result: m-token相较于e-token更能强有力地捕捉语义信息；当采用基于锚点的约束时，虽然能够促进e-token携带更多语义，但同时会显著降低重建准确性；而关系蒸馏方法能够在保持良好重建效果的同时，成功地将在批次级别上观察到的语义关联性转移到基础token的空间内。

Conclusion: 本研究表明，通过对基础token施加适当的约束，特别是利用关系蒸馏技术，可以在不影响重建质量的前提下增强其语义表达能力。这为开发非自回归序列生成模型开辟了一条新的道路，其中预测基础token作为中间表示可能是实现高效且高质量文本生成的关键。

Abstract: Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.

</details>


### [67] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: 本文提出了一种名为JPmHC的新框架，它通过引入可训练的线性混合器来替代身份跳过连接，并明确控制梯度条件，解决了Hyper-Connections在深度学习中导致的训练不稳定性、有限的可扩展性和增加的内存开销问题。该方法通过约束混合器于操作范数有界的流形上（如双随机、斯蒂费尔、格拉斯曼），防止了梯度问题并增强了稳定性。实验证明，JPmHC比基线方法收敛更快、准确率更高且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的进步，特别是Hyper-Connections (HC)的出现，虽然带来了显著的性能提升，但同时也破坏了残差连接的身份映射属性，从而引起训练不稳定、可扩展性受限以及内存开销增加等问题。为了解决这些问题，提出了新的框架。

Method: 作者们提出了一个名为JPmHC（Jacobian-spectrum Preserving manifold-constrained Hyper-Connections）的框架，该框架使用n个并行流上的可训练线性混合器代替身份跳跃连接，并显式地控制梯度调节。通过将混合器M约束在操作数范数边界流形上（例如双随机、斯蒂费尔、格拉斯曼），JPmHC能够避免梯度异常并增强稳定性。此外，还引入了三个关键贡献：(i) 自由概率分析预测结构化跳跃的雅可比谱，提供用于混合器选择的设计规则；(ii) 用于固定点投影的记忆效率隐式微分，减少激活内存和同步开销；(iii) 通过Cayley变换实现的斯蒂费尔约束混合器，确保正交性而无需后处理归一化。

Result: 实验评估表明，在ARC-AGI上，与双随机基线相比，JPmHC实现了更快的收敛速度、更高的准确性以及更低的计算成本。

Conclusion: 作为HC的一种灵活且可扩展的扩展，JPmHC促进了频谱感知、稳定且高效的深度学习发展，同时对拓扑架构设计和基础模型演变提供了见解。

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [68] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: 本文研究了自动聚类方法中元模型的可解释性问题，通过回顾22种现有方法并分类其元特征，运用全局和局部可解释性技术评估元特征的重要性及特定聚类决策，旨在为无监督学习自动化提供更透明的决策基础。


<details>
  <summary>Details</summary>
Motivation: 当前自动聚类方法虽然表现良好，但推荐结果难以解释：数据集元特征对算法选择与超参数设定的影响不明确，限制了可靠性、偏倚诊断以及有效的元特征工程能力。

Method: 1. 回顾分析现有的22种自动聚类方法，并将其元特征归类到一个结构化的分类体系中。
2. 应用全局可解释性技术（如决策谓词图）来评估选定框架内元模型中的特征重要性。
3. 利用局部可解释性工具例如SHAP（SHapley Additive exPlanations）分析具体的聚类决定。

Result: 研究发现揭示了元特征相关性的一致模式，识别出现有元学习策略中的结构性弱点可能导致建议偏差，并为设计更具解释性的自动机器学习系统提供了实用指导。

Conclusion: 该研究为提高无监督学习自动化过程中的决策透明度奠定了实践基础。

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


### [69] [FedZMG: Efficient Client-Side Optimization in Federated Learning](https://arxiv.org/abs/2602.18384)
*Fotios Zantalis,Evangelos Zervas,Grigorios Koulouras*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习优化算法FedZMG，通过将局部梯度投影到零均值超平面上来解决客户端漂移问题，无需额外通信或调整超参数。理论分析和实验证明了FedZMG在非独立同分布数据条件下相较于FedAvg和FedAdam具有更快的收敛速度和更高的最终验证准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许在边缘设备上进行分布式模型训练同时保护数据隐私。然而，客户端通常持有非独立同分布（non-IID）的数据，这往往导致客户端漂移，从而降低收敛速度和模型性能。虽然已经提出了自适应优化器来缓解这些影响，但它们经常引入计算复杂性或通信开销，这对于资源受限的物联网环境来说是不合适的。

Method: 本文介绍了Federated Zero Mean Gradients (FedZMG)，这是一种新颖的、无参数的客户端侧优化算法，旨在通过结构化规范优化空间来解决客户端漂移问题。FedZMG推进了梯度中心化的理念，它把本地梯度投射到一个零均值超平面上，有效地中和了异构数据分布中的'强度'或'偏置'偏移，而不需要额外的通信或超参数调节。

Result: 提供了理论分析，证明了FedZMG可以减少有效梯度方差，并保证比标准FedAvg更紧致的收敛界限。在EMNIST、CIFAR100和Shakespeare数据集上的广泛实验评估表明，与基准FedAvg和自适应优化器FedAdam相比，特别是在高度非IID设置下，FedZMG实现了更好的收敛速度和最终验证准确性。

Conclusion: FedZMG作为一种创新的联邦学习方法，在处理非IID数据时表现出色，能够在不增加通信负担的情况下改善模型训练效率和性能。

Abstract: Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.

</details>


### [70] [Assigning Confidence: K-partition Ensembles](https://arxiv.org/abs/2602.18435)
*Aggelos Semoglou,John Pavlopoulos*

Main category: cs.LG

TL;DR: 本文介绍了一种名为CAKE的新框架，该框架通过集合聚类来评估每个数据点的分配稳定性及局部几何一致性，最终给出一个可解释的[0,1]得分，用以指示聚类中个体分配的置信度。实验表明CAKE能够有效识别模棱两可的数据点以及稳定的中心成员，并提供可用于提高聚类质量的信心排名。


<details>
  <summary>Details</summary>
Motivation: 聚类广泛用于无监督结构发现，但对于单个分配的可靠性提供了有限的洞察力。全局质量指标如收敛行为或目标值并不能反映特定实例是否被自信地分配，尤其是对于对初始化敏感的算法如k-means。这种分配级别的不稳定性会损害准确性和鲁棒性。虽然集成方法可以通过聚合多次运行来改善全局一致性，但它们通常缺乏工具以结合跨运行一致性和从学习到的聚类结构获得的几何支持的方式量化逐点置信度。

Method: 提出了CAKE（通过K-分区集成进行分配置信度评估）框架，该框架利用聚类集成计算两个互补统计量：分配稳定性和局部几何拟合的一致性。这些统计量被合并成一个介于[0,1]之间的单一可解释分数。

Result: 理论分析显示，在噪声存在的情况下CAKE仍然有效，并且能够区分稳定与不稳定点。在合成数据集和真实世界数据集上的实验表明，CAKE能够有效地突出显示模棱两可的点和稳定的核成员，所提供的置信度排名可以指导过滤或优先级排序以提高聚类质量。

Conclusion: CAKE为评估聚类任务中个体分配的置信水平提供了一个新的途径，它不仅考虑了跨不同运行间的分配稳定性，还结合了局部几何特性的一致性。这一框架有助于识别出那些在聚类过程中可能引起不确定性的点，并为提升聚类结果的整体质量提供了可能性。

Abstract: Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [71] [It's Not Just Timestamps: A Study on Docker Reproducibility](https://arxiv.org/abs/2602.17678)
*Oreofe Solarin*

Main category: cs.DC

TL;DR: 研究构建了一个Docker测量管道，对2000个GitHub仓库中的Dockerfile进行了分析，发现仅有56%的仓库能生成可构建镜像，而其中只有2.7%是位级可再现的。通过调整基础设施配置后，位级可再现性提高了18.6%，但仍有78.7%的可构建Dockerfile不可再现。主要原因是开发者控制的选择如未清理的缓存、日志等。基于这些模式，研究提出了具体的Dockerfile指南以指导未来检查工具和持续集成(CI)检查的发展。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过简单的完整性检查（即从Dockerfile重建镜像并比较哈希值）来验证软件供应链中的可再现容器构建。目的是了解当前Dockerfiles在实际应用中的可再现性情况，并找出导致不可再现性的主要原因。

Method: 研究人员建立了一个Docker测量管道，并应用于包含Dockerfile的2,000个GitHub仓库样本中。他们首先尝试直接根据Dockerfile构建镜像，然后针对那些无法直接获得位级可再现结果的情况，通过调整基础设施配置来提高可再现性比例。对于仍然存在的差异，进一步分析了根本原因。

Result: 研究显示，仅56%的仓库能够生成任何可构建镜像，且其中只有2.7%是无需任何基础设施配置调整即可实现位级可再现的。通过修改基础设施配置，位级可再现性提升了18.6%，但依旧有78.7%的可构建Dockerfile未能达到完全可再现标准。此外，除了时间戳和元数据外，开发者控制的因素如未清理的缓存、日志文件、文档以及浮动版本号成为了非可再现性的主要因素。

Conclusion: 本研究表明，在现有实践中，Dockerfiles的位级可再现性存在较大问题。尽管通过特定配置调整可以改善部分情况下的可再现性，但要实现广泛意义上的可再现性还需要解决由开发者选择引起的问题。为此，研究提出了具体的Dockerfile编写建议，并讨论了如何利用这些发现来改进未来的linters及CI检查机制。

Abstract: Reproducible container builds promise a simple integrity check for software supply chains: rebuild an image from its Dockerfile and compare hashes. We build a Docker measurement pipeline and apply it to a stratified sample of 2,000 GitHub repositories that contained a Dockerfile. We found that only 56% produce any buildable image, and just 2.7% of those are bitwise reproducible without any infrastructure configurations. After modifying infrastructure configurations, we raise bitwise reproducibility by 18.6%, but 78.7% of buildable Dockerfiles remain non-reproducible. We analyze the root causes of the remaining differences, and find that beyond timestamps and metadata, developer-controlled choices such as uncleaned caches, logs, documentation, and floating versions are dominant causes of non-reproducibility. We derive concrete Dockerfile guidelines from these patterns and discuss how they can inform future linters and Continuous Integration (CI) checks for reproducible containers.

</details>


### [72] [Message-Oriented Middleware Systems: Technology Overview](https://arxiv.org/abs/2602.17774)
*Wael Al-Manasrah,Zuhair AlSader,Tim Brecht,Ahmed Alquraan,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: 该论文对十个开源面向消息的中间件系统进行了全面的特征研究，发现这些系统通过提供高灵活性和可配置性以及为复杂应用提供核心构建块而演进，以支持现代云应用。此外，作者还创建了一个带注释的数据集，并将其公开，以便验证其发现并帮助从业者和开发者理解及比较不同系统的特性。


<details>
  <summary>Details</summary>
Motivation: 为了全面了解开源面向消息的中间件（MOM）系统的特点和发展趋势，研究团队选择了十个流行的且多样化的MOM系统进行深入分析。

Method: 研究者采用了一种严谨的方法来选择和分析这十个MOM系统，针对每个系统考察了42项特征共计134个不同的选项。

Result: 研究表明，MOM系统已经发展成为能够为现代云应用程序提供框架的技术，通过提供高度的灵活性与可配置性，并且为复杂的应用程序提供了包括事务支持、主动消息传递、资源管理、流量控制以及多租户原生支持在内的核心构建模块。同时指出社区有机会集中精力于更少的开源项目上。

Conclusion: 这项工作不仅揭示了MOM系统在支持现代云应用方面的进步，而且通过创建一个公开可用的标注数据集促进了业界对于这些系统特性的理解和比较。

Abstract: We present a comprehensive characterization study of open-source message-oriented middleware (MOM) systems. We followed a rigorous methodology to select and study ten popular and diverse MOM systems. For each system, we examine 42 features with a total of 134 different options. We found that MOM systems have evolved to provide a framework for modern cloud applications through high flexibility and configurability and by offering core building blocks for complex applications including transaction support, active messaging, resource management, flow control, and native support for multi-tenancy. We also identify that there is an opportunity for the community to consolidate its efforts on fewer open-source projects.
  We have also created an annotated data set that makes it easy to verify our findings, which can also be used to help practitioners and developers understand and compare the features of different systems. For a wider impact, we make our data set publicly available.

</details>


### [73] [Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs](https://arxiv.org/abs/2602.17808)
*Nathan Ng,Walid A. Hanafy,Prashanthi Kadambi,Balachandra Sunil,Ayush Gupta,David Irwin,Yogesh Simmhan,Prashant Shenoy*

Main category: cs.DC

TL;DR: SwapLess系统通过自适应调整模型处理在CPU和TPU之间的分配，减少了边缘TPU上运行的AI应用的延迟，特别是在多租户环境下。


<details>
  <summary>Details</summary>
Motivation: 物联网应用越来越依赖于设备上的AI加速器来确保高性能，特别是在连接受限和安全关键场景中。然而，这些加速器有限的片上内存迫使推理运行时要在主机和加速器内存之间交换模型段，从而显著增加了延迟。虽然通过将模型处理在CPU和加速器资源之间进行分割协作处理可以减少加速器的内存压力和延迟，但是简单的分区可能会由于向CPU转移过多计算或未能充分减少交换而恶化端到端的延迟，这个问题在多租户和动态环境中进一步被放大。

Method: 提出了一种名为SwapLess的系统，该系统针对内存受限的边缘TPUs设计了自适应、多租户TPU-CPU协同推断方案。SwapLess利用了一个分析排队模型，该模型捕捉了根据分区变化的CPU/TPU服务时间以及不同工作负载混合和请求率下的跨模型内外交换开销。使用这个模型，SwapLess在线连续调整分区点和CPU核心分配，以最小化端到端响应时间，并保持低决策开销。

Result: 在配备Edge TPU的平台上实现表明，对于单租户工作负载，SwapLess相对于默认的Edge TPU编译器最多可减少63.8%的平均延迟；而对于多租户工作负载，则最多可减少77.4%的平均延迟。

Conclusion: SwapLess为解决边缘TPU在执行AI任务时面临的内存限制问题提供了一个有效的解决方案，通过智能地管理CPU与TPU之间的任务分配，实现了显著的性能提升。

Abstract: IoT applications are increasingly relying on on-device AI accelerators to ensure high performance, especially in limited connectivity and safety-critical scenarios. However, the limited on-chip memory of these accelerators forces inference runtimes to swap model segments between host and accelerator memory, substantially inflating latency. While collaborative processing by partitioning the model processing between CPU and accelerator resources can reduce accelerator memory pressure and latency, naive partitioning may worsen end-to-end latency by either shifting excessive computation to the CPU or failing to sufficiently curb swapping, a problem that is further amplified in multi-tenant and dynamic environments.
  To address these issues, we present SwapLess, a system for adaptive, multi-tenant TPU-CPU collaborative inference for memory-constrained Edge TPUs. SwapLess utilizes an analytic queueing model that captures partition-dependent CPU/TPU service times as well as inter- and intra-model swapping overheads across different workload mixes and request rates. Using this model, SwapLess continuously adjusts both the partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead. An implementation on Edge TPU-equipped platforms demonstrates that SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads relative to the default Edge TPU compiler.

</details>


### [74] [Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation](https://arxiv.org/abs/2602.17811)
*Guy Blelloch,Andrew Brady,Laxman Dhulipala,Jeremy Fineman,Kishen Gowda,Chase Hutton*

Main category: cs.DC

TL;DR: 本文提出了更快的并行批动态算法，用于保持无向图的低出度定向。第一种算法在摊销意义上实现了渐近最优的工作界限；第二种算法与已知的最佳序列最坏情况算法相匹配，但以期望值形式给出；第三种算法显著改进了最近的研究成果，在保持相似定向的同时大幅降低了每条边更新所需的工作量。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发更高效的并行批动态算法来处理无向图中边缘的批量插入或删除问题，旨在最小化最大出度的同时保证整个批次处理的深度为对数多项式级别，并且每个边缘的处理工作量尽可能小。

Method: 通过设计三种不同的算法：1）一种在摊销意义上达到渐近最优预期工作界限的方法；2）一种基于图形树性上界c的O(c log n)定向算法，其单边更新的预期最坏情况工作量为O(√log n)；3）一种O(c + log n)-定向算法，具有O(log^2 n)的预期最坏情况工作量。

Result: 所提出的方法不仅首次实现了渐近最优的定向和工作界限，还分别在不同条件下匹配或超过了现有最佳序列算法的表现，特别是在减少每条边更新时的工作量方面取得了显著进步。

Conclusion: 本研究表明，通过精心设计并行批动态算法，可以在保持低出度定向的同时大幅度降低处理大规模无向图变更时所需的计算资源。

Abstract: A low out-degree orientation directs each edge of an undirected graph with the goal of minimizing the maximum out-degree of a vertex. In the parallel batch-dynamic setting, one can insert or delete batches of edges, and the goal is to process the entire batch in parallel with work per edge similar to that of a single sequential update and with span (or depth) for the entire batch that is polylogarithmic. In this paper we present faster parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. All results herein achieve polylogarithmic depth, with high probability (whp); the focus of this paper is on minimizing the work, which varies across results.
  Our first result is the first parallel batch-dynamic algorithm to maintain an asymptotically optimal orientation with asymptotically optimal expected work bounds, in an amortized sense, improving over the prior best work bounds of Liu et al.~[SPAA~'22] by a logarithmic factor.
  Our second result is a $O(c \log n)$ orientation algorithm with expected worst-case $O(\sqrt{\log n})$ work per edge update, where $c$ is a known upper-bound on the arboricity of the graph. This matches the best-known sequential worst-case $O(c \log n)$ orientation algorithm given by Berglin and Brodal ~[Algorithmica~'18], albeit in expectation.
  Our final result is a $O(c + \log n)$-orientation algorithm with $O(\log^2 n)$ expected worst-case work per edge update. This algorithm significantly improves upon the recent result of Ghaffari and Koo~[SPAA~'25], which maintains a $O(c)$-orientation with $O(\log^9 n)$ worst-case work per edge whp.

</details>


### [75] [Distributed Triangle Enumeration in Hypergraphs](https://arxiv.org/abs/2602.17834)
*Duncan Adamson,Will Rosenbaum,Paul G. Spirakis*

Main category: cs.DC

TL;DR: 本论文系统地研究了超图中的分布式子超图枚举问题，提出了几种计算模型，并为这些模型设计了分布式三角形枚举算法，同时介绍了稀疏和"处处稀疏"的超图类别以及在其中进行有效三角形枚举的算法。


<details>
  <summary>Details</summary>
Motivation: 由于子图检测和枚举在理论挑战和实际应用方面的重要性，在过去十年中它们已经成为分布式图算法的核心问题。本文旨在对超图中的分布式子超图枚举展开系统性研究。

Method: 1. 提出几种概括了图的CONGEST模型的超图计算模型，并评估它们相对的计算能力。
2. 在提出的计算模型中设计分布式三角形枚举算法，并证明在两个这样的模型中这些算法的最优性。
3. 引入稀疏和“处处稀疏”的超图类别，并描述在这类超图中进行有效三角形枚举的分布式算法。
4. 描述了一般技术，认为这些技术对于在我们的超图模型中设计高效算法是有用的。

Result: 成功定义了新的超图计算模型，开发并验证了几种模型下的分布式三角形枚举算法的有效性和最优性，同时还针对特定类型的稀疏超图提出了有效的算法解决方案。

Conclusion: 通过引入新的超图计算模型与分布式算法设计，本研究为解决分布式子超图枚举问题提供了坚实的基础，并指出了一些未来研究可能采用的一般技术。

Abstract: In the last decade, subgraph detection and enumeration have emerged as a central problem in distributed graph algorithms. This is largely due to the theoretical challenges and practical applications of these problems. In this paper, we initiate the systematic study of distributed sub-hypergraph enumeration in hypergraphs. To this end, we (1)~introduce several computational models for hypergraphs that generalize the CONGEST model for graphs and evaluate their relative computational power, (2)~devise algorithms for distributed triangle enumeration in our computational models and prove their optimality in two such models, (3)~introduce classes of sparse and ``everywhere sparse'' hypergraphs and describe efficient distributed algorithms for triangle enumeration in these classes, and (4)~describe general techniques that we believe to be useful for designing efficient algorithms in our hypergraph models.

</details>


### [76] [Joint Training on AMD and NVIDIA GPUs](https://arxiv.org/abs/2602.18007)
*Jon Hu,Thomas Jia,Jing Zhu,Zhendong Yu*

Main category: cs.DC

TL;DR: 本文提出了一种在AMD-NVIDIA混合环境下进行异构混合训练的技术解决方案，通过Device-Direct Communication方法实现了接近NVIDIA同构系统98%的吞吐量，同时保持了训练稳定性和正确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模的不断扩大，对计算和系统容量的需求迅速增长，单一供应商的同构集群已变得不再足够。因此，研究团队旨在开发一种能够在不同硬件（如AMD和NVIDIA GPU）之间有效执行训练任务的方法。

Method: 首先采用了基于CPU转发通信的兼容性导向方法，并结合了跨并行组的不同通信后端选择以及多网卡并行数据传输技术。为进一步提高性能，还提出了直接设备间通信方法，集成了一个CPU卸载P2P机制来实现无需主机内存介入的跨厂商GPU直接数据传输。

Result: 实验结果表明，在LLaMA-8B和Qwen2-7B上，所提出的直接设备间通信方法达到了与NVIDIA同构系统几乎相同的吞吐量（高达98%），同时也确保了训练过程的稳定性和准确性。

Conclusion: 该研究成功地解决了AMD-NVIDIA混合环境中大规模语言模型训练的问题，为未来更高效、更具成本效益的异构计算方案铺平了道路。

Abstract: As large language models continue to scale, training demands on compute and system capacity grow rapidly, making single-vendor homogeneous clusters insufficient. This paper presents a technical solution for heterogeneous mixed training in AMD-NVIDIA environments. We first adopt a compatibility-oriented approach based on CPU-Forwarding Communication, with differentiated communication back-end selection across parallel groups and multi-NIC parallel data transfer. To achieve higher performance, we further propose another Device-Direct Communication approach, integrating a CPU-offloading P2P mechanism to enable direct cross-vendor GPU data transfer without host-memory staging. Experiments on LLaMA-8B and Qwen2-7B demonstrate that the proposed Device-Direct Communication approach achieves up to 98% of the throughput of an NVIDIA homogeneous system, while preserving training stability and correctness.

</details>


### [77] [A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum](https://arxiv.org/abs/2602.18158)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.DC

TL;DR: 提出了一种精确的多目标任务分配框架，用于优化边缘-中心-云架构中工作流应用的整体可靠性和延迟。该方法在实际案例和合成工作流测试中表现出了显著的性能改进。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的关键工作流应用采用简化的边缘-中心-云架构，如何在设备限制和多样化的操作条件下有效进行任务分配成为挑战。由于这些应用对可靠性和延迟有严格要求，但这两者之间存在冲突，因此需要一种能够同时优化这两个关键指标的方法。

Method: 通过开发一个综合性的二进制整数线性规划公式来实现，该公式考虑了每个目标的相对重要性，并且结合了时间冗余技术，同时还考虑到了相关研究中经常被忽视的重要约束条件。

Result: 在真实世界的工作流应用中，相比基线策略，在考虑的相关目标权衡下，本方法平均提高了84.19%的可靠性以及49.81%的延迟降低。实验结果表明，该方法对于所考虑系统架构下的多种工作流应用都具有良好的效果和可扩展性，运行时间从0.03秒到50.94秒不等。

Conclusion: 提出的精确多目标任务分配框架能够在边缘-中心-云架构内有效提升工作流应用程序的可靠性和减少延迟，显示出其在现实场景中的实用价值。

Abstract: A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.

</details>


### [78] [Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum](https://arxiv.org/abs/2602.18287)
*Andrea D'Iapico,Monica Vitali*

Main category: cs.DC

TL;DR: 本文提出了一种方法，通过持续分析能耗模式、组件间通信和基础设施的环境特性来生成绿色约束，从而指导调度器生成环保的部署计划。该方法能够自适应地学习并更新这些约束条件，以实现节能调度。


<details>
  <summary>Details</summary>
Motivation: 随着信息技术对减少能源消耗和温室气体排放的需求日益增加，在云-边缘连续体上部署云原生应用时，需要找到一种既考虑应用程序组件的计算需求又考虑执行节点环境影响的节能部署策略。

Method: 作者提出了一种自动产生由绿色约束引导的部署计划的方法论与架构。这些绿色约束是基于对能量消耗模式、组件间通信及底层基础设施环境特征的持续分析得出的，并可通过监控数据随时间自动学习和更新。

Result: 通过实际的云原生应用部署场景验证了所提方法的有效性，展示了其在减少能源使用及相关排放方面的成效。

Conclusion: 本研究为云原生应用提供了一个创新性的绿色感知调度解决方案，有助于提高IT系统的环境可持续性。

Abstract: The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [79] [Examining LLMs Ability to Summarize Code Through Mutation-Analysis](https://arxiv.org/abs/2602.17838)
*Lara Khatib,Micheal Pu,Bogdan Vasilescu,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 本文提出了一种基于变异的评估方法，用于测试大语言模型生成的代码摘要是否准确反映了代码的实际逻辑。通过在代码中注入特定变异并检查模型是否更新其摘要以反映新行为来验证该方法的有效性。研究发现，随着程序复杂性的增加，摘要准确性显著下降，并且模型往往描述的是算法意图而非实际变异后的行为。


<details>
  <summary>Details</summary>
Motivation: 随着开发人员越来越多地依赖大语言模型生成的代码摘要来进行文档编写、测试和审查，研究这些摘要是否准确反映了程序的实际功能变得尤为重要。大语言模型通常会自信地描述代码看起来应该做什么（意图），而忽略了定义其实际行为的细微边缘情况或逻辑更改。

Method: 本文介绍了一种基于变异的评估方法，直接测试摘要是否真正匹配代码逻辑。该方法包括生成一个摘要，在代码中注入一个有针对性的变异，并检查大语言模型是否会更新其摘要以反映新的行为。研究者通过对62个程序共计624次变异-摘要评估进行了验证。实验设计包括对12个受控合成程序进行324次不同类型（语句、值、决策）及不同位置（开始、中间、结束）的变异测试；以及对从Less Basic Python Problems (LBPP)数据集中选取的50个人类编写的程序进行150次变异样本测试。

Result: 研究结果表明，摘要准确性随着复杂性增加而急剧下降，单个函数的准确性为76.5%，而对于多线程系统则降至17.3%。突变类型和位置的影响较弱。此外，尽管GPT-4与GPT-5.2之间的性能有了显著提升（从49.3%提高到85.3%），并且识别突变为“错误”的能力有所改善，但两种模型仍然难以区分实现细节与标准算法模式。

Conclusion: 本工作确立了变异分析作为一种系统方法，用以评估大语言模型生成的摘要是否反映了程序行为而非表面的文字模式。

Abstract: As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as "bugs", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.

</details>


### [80] [DeCEAT: Decoding Carbon Emissions for AI-driven Software Testing](https://arxiv.org/abs/2602.18012)
*Pragati Kumari,Novarun Deb*

Main category: cs.SE

TL;DR: 本文提出了DeCEAT框架，用于系统评估小型语言模型(SLMs)在测试生成过程中的环境影响与性能之间的权衡。通过使用HumanEval基准和自适应提示变体进行实验，结果表明不同SLMs在可持续性方面表现出不同的优势，强调了提示设计对环境及性能结果的影响。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在自动化软件测试中使用的增加，其环境影响引起了关注，但现有的可持续性分析几乎只集中在大型语言模型上。小型语言模型（SLMs）在测试生成过程中的能源消耗和碳足迹特性仍待探索。

Method: 开发了DeCEAT框架，该框架利用HumanEval基准和基于Anthropic模板的自适应提示变体来评估SLMs的环境与性能折衷。使用CodeCarbon测量能耗和碳排放量，并通过单元测试覆盖率评估生成测试的质量。

Result: 研究发现显示，不同的SLMs展示出独特的可持续性优势：一些模型优先考虑较低的能耗和更快的执行速度，而另一些则在碳限制下保持更高的稳定性和准确性。这表明，在SLM驱动的测试生成过程中，可持续性是多维度的，并且受到提示设计的强烈影响。

Conclusion: 本研究为基于SLM的自动化测试生成提供了一个专门定制的可持续性评估框架，阐明了提示结构和模型选择如何共同作用于环境和性能结果。

Abstract: The increasing use of language models in automated software testing raises concerns about their environmental impact, yet existing sustainability analyses focus almost exclusively on large language models. As a result, the energy and carbon characteristics of small language models (SLMs) during test generation remain largely unexplored. To address this gap, this work introduces the DeCEAT framework, which systematically evaluates the environmental and performance trade-offs of SLMs using the HumanEval benchmark and adaptive prompt variants (based on the Anthropic template). The framework quantifies emission and time-aware behavior under controlled conditions, with CodeCarbon measuring energy consumption and carbon emissions, and unit test coverage assessing the quality of generated tests. Our results show that different SLMs exhibit distinct sustainability strengths: some prioritize lower energy use and faster execution, while others maintain higher stability or accuracy under carbon constraints. These findings demonstrate that sustainability in the generation of SLM-driven tests is multidimensional and strongly shaped by prompt design. This work provides a focused sustainability evaluation framework specifically tailored to automated SLM-based test generation, clarifying how prompt structure and model choice jointly influence environmental and performance outcomes.

</details>


### [81] [Role and Identity Work of Software Engineering Professionals in the Generative AI Era](https://arxiv.org/abs/2602.18190)
*Jorge Melegati*

Main category: cs.SE

TL;DR: 本文探讨了生成式AI（GenAI）在软件工程中的应用如何影响不同角色的软件专业人员的工作身份，并提出了一项研究议程来更好地理解角色对GenAI引发的身份工作的影响，旨在为这种技术的应用提供支持。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（GenAI）逐渐被引入到软件工程领域，它不仅改变了技术层面，也影响到了从事该领域的专业人士对自己工作的看法——即他们的工作身份以及围绕此身份形成、调整或拒绝的过程。现有研究表明GenAI的采用确实触发了软件专业人员的身份工作变化，但这些研究没有区分不同角色（如开发者与测试者）之间的差异。本文认为有必要将角色作为定义软件专业人员身份工作的一个因素来考虑。

Method: 为了支撑这一论点，作者们回顾了一些关于不同角色的研究以及最近关于如何在软件工程中采纳GenAI的研究。基于这些回顾，提出了一个研究议程以更深入地了解角色是如何影响由GenAI采用所引起的身份工作的，并据此建议开发新的工具或方法来促进GenAI的采纳过程。

Result: 文章指出了需要进一步探索的方向，包括但不限于：1) 不同角色对于GenAI的态度和反应有何不同；2) GenAI如何具体影响各个角色的专业发展路径和个人成长；3) 有哪些有效的方法可以帮助软件团队更好地适应因GenAI而产生的变化。

Conclusion: 通过认识到不同角色在面对GenAI时可能经历独特的职业身份转变过程，本研究强调了制定针对性策略的重要性，以便于帮助所有相关方顺利过渡至更加依赖人工智能辅助的新工作模式。此外，还讨论了预期研究成果对实践可能带来的潜在影响。

Abstract: The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.

</details>


### [82] [ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation](https://arxiv.org/abs/2602.18306)
*Dongming Jin,Zhi Jin,Zheng Fang,Linyu Li,XiaoTian Yang,Yuanpeng He,Xiaohong Chen*

Main category: cs.SE

TL;DR: 提出了一种新的评估环境ReqElicitGym，用于对话式需求获取中的访谈能力的互动和自动评估。通过使用ReqElicitGym，研究者对七种代表性大语言模型进行了系统性实证研究，发现当前的大语言模型在揭示隐含需求方面仍存在局限性，尤其是在风格相关的需求上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）编码能力的迅速提升，基于LLM的自动化软件开发的瓶颈正从生成正确的代码转向引出用户的需求。尽管对此兴趣日益增长，但大语言模型在对话式需求获取中的访谈能力尚未得到充分探索。现有的评估往往依赖于少数场景、真实用户交互及主观的人工评分，这阻碍了系统的定量比较。

Method: 提出了ReqElicitGym，一个针对对话式需求获取中访谈能力进行互动与自动化评价的新环境。该环境引入了一个新的评估数据集，并设计了互动式的标准用户和任务评估器。数据集包含101个网站需求获取情景，覆盖10种应用类型。标准用户和任务评估器都达到了与真实用户及专家判断的高度一致。

Result: 利用ReqElicitGym，对七种代表性大语言模型进行了系统性的实证研究。结果显示，当前的大语言模型在揭示用户隐含需求方面的能力有限，能够引出少于一半的用户隐含需求，且有效的获取问题通常出现在对话的后期。此外，还发现大语言模型能够引出交互性和内容上的隐含需求，但在处理风格相关需求时始终遇到困难。

Conclusion: 研究认为，ReqElicitGym将有助于促进自动化对话式需求获取方法的评估与发展。

Abstract: With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.

</details>


### [83] [VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean](https://arxiv.org/abs/2602.18307)
*Yutong Xin,Qiaochu Chen,Greg Durrett,Işil Dillig*

Main category: cs.SE

TL;DR: 本文介绍了一个名为VeriSoftBench的基准测试，它包含500个Lean 4证明义务，这些义务来自开源形式化方法开发，并且被封装以保持实际存储库上下文和跨文件依赖。评估显示，针对Mathlib风格数学优化的证明器在面向存储库设置中表现不佳；成功与传递性存储库依赖紧密相关；提供仅限于证明依赖闭包的精选上下文可以提高性能，但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在交互式定理证明方面取得了显著成果，特别是在Lean语言中。但是，大多数基于LLM的证明自动化基准测试都是从Mathlib生态系统中的数学内容中提取的，而软件验证中的证明则是在定义丰富的代码库中开发的，并且有大量的项目特定库。因此需要一个更贴近实际软件验证场景的基准测试来评估LLM和其他证明器的表现。

Method: 创建了VeriSoftBench，这是一个包含500个Lean 4证明义务的基准测试集，这些任务来源于开放源码的形式化方法发展，并且打包时保留了真实的仓库背景和跨文件依赖关系。通过这个基准测试来评估前沿的大规模语言模型（LLMs）以及专门的证明工具的表现。

Result: 研究发现，为适应Mathlib风格数学而调整的证明工具在这种以仓库为中心的环境中表现不佳；成功的概率与间接的仓库依赖性密切相关：那些证明过程涉及到大量多步依赖的任务更难解决；当提供的精选上下文限制在证明所需的依赖闭包内时，相较于暴露整个仓库的情况，性能有所提升，但仍有很大的改进空间。

Conclusion: 提出了一个新的基准测试VeriSoftBench用于评估自动证明工具在软件验证领域的表现。结果显示，当前针对Mathlib优化的证明工具在软件项目特有的背景下效果不理想，同时指出了依赖管理对于证明自动生成的重要性。此外，研究还表明通过适当裁剪输入上下文可以改善结果，不过仍存在不少挑战。

Abstract: Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.

</details>


### [84] [Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation](https://arxiv.org/abs/2602.18357)
*Wallace Albertini,Marina Condé Araújo,Júlia Condé Araújo,Antonio Pedro Santos Alves,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文提出了一种统计置信功能正确性（SCFC）方法来评估AI系统的质量，通过定义定量规范限值、进行分层和概率抽样、应用自助法估计性能度量的置信区间以及计算能力指数四个步骤。该方法在两个实际工业AI系统案例研究中得到验证，并得到了专家们关于其实用性、易用性和采用意图的积极反馈。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统质量评估标准如ISO/IEC 25059缺乏实用且统计上稳健的方法来评价功能性正确性。

Method: 本论文介绍了一种新的评估方法，即统计置信功能正确性（SCFC），它包括设定量化规格限制、执行分层和概率采样、利用自助法估算性能指标的置信区间，并最终计算出一个能力指数作为整体指标。

Result: 通过对两个现实世界中的工业AI系统案例研究，以及与AI专家访谈，证明了该方法的有效性、可用性及其潜在的实际应用场景。

Conclusion: 研究表明，提出的SCFC方法是一种可行且有价值的手段，可以将功能性正确的评估从点估计转变为具有统计置信度的声明。

Abstract: The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.

</details>
