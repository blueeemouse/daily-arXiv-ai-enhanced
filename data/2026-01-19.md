<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 41]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Multi-Artifact Analysis of Self-Admitted Technical Debt in Scientific Software](https://arxiv.org/abs/2601.10850)
*Eric L. Melin,Nasir U. Eisty,Gregory Watson,Addi Malviya-Thakur*

Main category: cs.SE

TL;DR: This study introduces and evaluates the concept of scientific debt, a specialized form of self-admitted technical debt (SATD) in scientific software (SSW), by analyzing multiple artifacts from 23 open-source SSW projects. It highlights the need for specific detection methods for scientific debt, which is not well captured by traditional SATD categories.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to identify, categorize, and evaluate a specialized form of SATD, termed 'scientific debt,' in the context of scientific software. This is driven by the unique risks such debt poses to the validity and reproducibility of scientific results, as well as the observation that traditional SATD categories may not adequately capture these domain-specific issues.

Method: The researchers conducted a multi-artifact analysis across code comments, commit messages, pull requests, and issue trackers from 23 open-source SSW projects. They created and validated a dataset of scientific debt, developed a classifier for detecting SATD from multiple sources, and performed a practitioner validation to assess the practical relevance of identifying scientific debt.

Result: The developed classifier demonstrated strong performance across over 900,000 artifacts from 23 SSW projects. The study found that SATD, particularly scientific debt, was most frequently documented in pull requests and issue trackers. Traditional SATD models often failed to detect scientific debt, underlining the necessity for its specific identification. The practitioner validation confirmed the recognition and utility of scientific debt in real-world scenarios.

Conclusion: The research concludes that scientific debt is a distinct form of SATD in SSW, requiring targeted approaches for its detection and management. The provided dataset, classification analysis, and practitioner feedback offer a foundational perspective on addressing scientific debt, advocating for the development of tailored SATD detection strategies in SSW.

Abstract: Context: Self-admitted technical debt (SATD) occurs when developers acknowledge shortcuts in code. In scientific software (SSW), such debt poses unique risks to the validity and reproducibility of results. Objective: This study aims to identify, categorize, and evaluate scientific debt, a specialized form of SATD in SSW, and assess the extent to which traditional SATD categories capture these domain-specific issues. Method: We conduct a multi-artifact analysis across code comments, commit messages, pull requests, and issue trackers from 23 open-source SSW projects. We construct and validate a curated dataset of scientific debt, develop a multi-source SATD classifier, and conduct a practitioner validation to assess the practical relevance of scientific debt. Results: Our classifier performs strongly across 900,358 artifacts from 23 SSW projects. SATD is most prevalent in pull requests and issue trackers, underscoring the value of multi-artifact analysis. Models trained on traditional SATD often miss scientific debt, emphasizing the need for its explicit detection in SSW. Practitioner validation confirmed that scientific debt is both recognizable and useful in practice. Conclusions: Scientific debt represents a unique form of SATD in SSW that that is not adequately captured by traditional categories and requires specialized identification and management. Our dataset, classification analysis, and practitioner validation results provide the first formal multi-artifact perspective on scientific debt, highlighting the need for tailored SATD detection approaches in SSW.

</details>


### [2] [Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation](https://arxiv.org/abs/2601.10942)
*Zitong Zhou,Matteo Paltenghi,Miryung Kim,Michael Pradel*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的测试增强技术Change And Cover (ChaCo)，专门针对代码修改（PRs）中未被覆盖的部分生成测试用例，以填补回归测试的最后一英里缺口。通过为开发人员提供相关测试上下文，并将生成的测试与现有测试套件无缝集成，ChaCo在SciPy、Qiskit和Pandas等开源项目上进行了评估，证明了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 软件不断演进过程中，开发者经常提交拉取请求（PRs）来引入新特性或修复错误。尽管很多项目都有全面的测试套件，但部分由PR修改的代码行仍然没有得到测试，形成了‘最后一英里’的回归测试缺口。现有的测试生成器通常旨在提高整体覆盖率，而没有特别关注PR中的未覆盖行。

Method: 提出了Change And Cover (ChaCo)这一基于大型语言模型（LLM）的测试增强技术，该技术考虑到了特定于PR的补丁覆盖率问题，能够为刚修改的代码提供增强的测试。此外，研究者还确定了向LLM提供合适的测试上下文作为生成有用测试的关键挑战，并提出了两种提取相关测试内容的技术。为了使增强后的测试对开发者来说是可接受的，ChaCo会仔细地将它们整合到现有的测试套件中，比如匹配测试结构和风格，并生成测试添加概要供开发者审阅。

Result: ChaCo在来自SciPy、Qiskit和Pandas三个流行且复杂的开源项目的145个PR上进行了评估。结果显示，该方法成功帮助30%的PR达到了完全的补丁覆盖率，每条PR的成本仅为$0.11，显示出其实效性和实用性。人类评审员发现这些测试值得添加（评分4.53/5.0）、很好地整合了（评分4.2/5.0）并且与PR相关（评分4.7/5.0）。消融实验表明，测试上下文对于情境感知的测试生成至关重要，可带来两倍的覆盖率。在提交的12个测试中，已有8个被合并，并揭露并修复了两个未知的bug。

Conclusion: 本研究表明，通过使用基于LLM的方法如ChaCo可以有效地针对PR中的未覆盖代码行生成测试用例，从而填补了回归测试中的最后一英里缺口。这种技术不仅提高了覆盖率，也得到了实际应用的认可，未来有望集成到持续集成工作流中自动化完成回归测试增强。

Abstract: Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a "last-mile" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.

</details>


### [3] [Patterns of Bot Participation and Emotional Influence in Open-Source Development](https://arxiv.org/abs/2601.11138)
*Matteo Vaccargiu,Riccardo Lai,Maria Ilaria Lunesu,Andrea Pinna,Giuseppe Destefanis*

Main category: cs.SE

TL;DR: 研究了机器人在以太坊生态系统开源讨论中的贡献及其对开发者情绪的影响。发现尽管机器人占比很小，但它们的参与改变了沟通的时间动态和情绪动态，使得人类评论者的情绪更倾向于感激、钦佩和乐观，而减少了困惑。


<details>
  <summary>Details</summary>
Motivation: 探索机器人如何影响开源项目中的讨论模式和参与者的情绪状态。

Method: 分析了来自十个仓库的36,875个账户的数据集，其中包含105个已验证的机器人。使用训练好的模型来识别27种情绪类别，并比较机器人与人类参与者之间的互动差异。

Result: 机器人虽然只占总参与者的0.28%，但它们能够更快地回应pull requests，在issues中则扮演较慢节奏的支持角色。机器人更加中立，但它们介入后人类评论变得更加积极正面。

Conclusion: 即使少量的机器人也能够显著改变开发者交流过程中的时间安排及情感氛围，促进更为积极向上的对话环境。

Abstract: We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.

</details>


### [4] [Automation and Reuse Practices in GitHub Actions Workflows: A Practitioner's Perspective](https://arxiv.org/abs/2601.11299)
*Hassan Onsori Delicheh,Guillaume Cardoen,Alexandre Decan,Tom Mens*

Main category: cs.SE

TL;DR: 该研究通过调查419名实践者，探讨了GitHub Actions在工作流自动化和重用实践中的应用情况，发现尽管核心CI/CD任务得到较多关注，但在安全分析等关键领域仍有不足。此外，可重用Actions被广泛依赖但可重用工作流采纳率较低，且存在版本控制与维护的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解工作流开发中的良好及不良实践，并识别支持工作流维护的机会点。当前，关于开发者偏好的自动化与重用实践的知识有限，而GitHub Actions作为支持工作流自动化的工具，在使用过程中遇到了不少维护上的难题。

Method: 采用问卷调查法，对419名使用GitHub Actions的工作流实践者进行了调研。研究具体探索了实践中倾向于自动化的任务类型、创建工作流的首选机制以及优先考虑的非功能性特征等方面。

Result: 结果显示，虽然核心CI/CD任务得到了广泛的自动化处理，但在安全性分析和性能监控等重要方面投入较少。实践者们非常依赖可复用的Actions，但对于可复用的工作流接受度不高。此外，还观察到了Action版本管理和维护方面的挑战。复制粘贴代码仍然是避免依赖复杂性的常见做法。

Conclusion: 基于上述发现，提出了改进工具的需求，包括加强对多种自动化任务的支持，以及改善发现、管理并信任可重复利用的工作流组件的方法。

Abstract: GitHub natively supports workflow automation through GitHub Actions. Yet, workflow maintenance is often considered a burden for software developers, who frequently face difficulties in writing, testing, debugging, and maintaining workflows. Little knowledge exists concerning the automation and reuse practices favoured by workflow practitioners. We therefore surveyed 419 practitioners to elucidate good and bad workflow development practices and to identify opportunities for supporting workflow maintenance. Specifically, we investigate the tasks that practitioners tend to automate using GitHub Actions, their preferred workflow creation mechanisms, and the non-functional characteristics they prioritise. We also examine the practices and challenges associated with GitHub's workflow reuse mechanisms. We observe a tendency to focus automation efforts on core CI/CD tasks, with less emphasis on crucial areas like security analysis and performance monitoring. Practitioners strongly rely on reusable Actions, but reusable workflows see less frequent adoption. Furthermore, we observed challenges with Action versioning and maintenance. Copy-pasting remains a common practice to have more control and avoid the complexity of depending on reusable components. These insights suggest the need for improved tooling, enhanced support for a wide range of automation tasks, and better mechanisms for discovering, managing, and trusting reusable workflow components.

</details>


### [5] [RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback](https://arxiv.org/abs/2601.11362)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: RITA is a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven requirements engineering (RE). It supports the transformation of raw user feedback into requirements artifacts, such as request classification, non-functional requirement identification, and natural-language requirements specification, with seamless integration into Jira.


<details>
  <summary>Details</summary>
Motivation: 在线用户反馈是需求工程中的宝贵资源，但由于其数量庞大且包含很多噪声，分析起来非常困难。虽然现有的工具可以支持单个的反馈分析任务，但它们的功能很少被整合成端到端的支持流程。这种缺乏端到端集成的情况限制了现有需求工程（RE）工具的实际应用，并使得评估这些工具在现实世界中的有用性变得困难。

Method: 为了解决这个问题，研究者们提出了RITA，这是一个将轻量级开源大型语言模型整合到统一工作流中的工具，用于基于反馈的需求工程。RITA通过友好的用户界面支持自动请求分类、非功能性需求识别以及从在线反馈生成自然语言需求规范，并与Jira集成以实现需求规范到开发工具的无缝转移。

Result: RITA利用先前评估过的基于LLM的需求工程技术来高效地将原始用户反馈转化为需求制品，有助于缩小研究与实践之间的差距。

Conclusion: RITA作为一个创新工具，能够有效地将用户反馈转换为具体的需求工件，从而提高了需求工程流程的效率和实用性。它不仅解决了现有工具缺乏端到端集成的问题，还促进了研究成果向实际应用的转化。

Abstract: Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.

</details>


### [6] [A Practical Guide to Establishing Technical Debt Management](https://arxiv.org/abs/2601.11430)
*Marion Wiese*

Main category: cs.SE

TL;DR: 该白皮书基于作者的博士论文成果，旨在将科学发现转化为实践指导，通过与三支不同公司的团队合作，开发了一套技术债务管理指南，适用于团队内部而非整个公司。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过转化其博士论文的研究成果来提供实际指导，帮助不同背景的团队建立适合自身需求的技术债务管理系统。

Method: 作者与其他研究人员合作，支持来自不同公司的三个团队调整并建立符合各自需求的技术债务管理系统，并在此过程中筛选出实用的研究结果形成指南。

Result: 产出了一份关于在团队内建立技术债务管理的指南，其中区分了所有团队都采用的“最佳实践”和至少一个团队使用的“可选项”。

Conclusion: 该指南旨在为团队提供方向性建议而非严格框架，在很多地方强调团队应共同决定如何设计流程；同时，虽然本指南主要针对团队层面，但也在最后部分提供了关于在整个公司范围内实施技术债务管理的一些建议。

Abstract: This white paper provides an overview of the topic of "technical debt" and presents an approach for managing technical debt in teams. The white paper is based on the results of my dissertation, which aimed to translate scientific findings into practical guidance. To this end, I collaborated with other researchers to support three teams from different companies in adapting and establishing a technical debt management system tailored to their specific needs. Research findings were supplemented with details or additional approaches. Research results that were less practical were discarded. The result is a guide on establishing technical debt management within a team. The guide is intended to provide orientation and not be a rigid framework. We distinguish between "best practices" and "nice-to-haves." "Best practices" are understood to be all approaches that were adopted by all three teams. "Nice-to-haves" were used by at least one team. In many places, it is explicitly mentioned that the team should decide together how to design the process. This also applies, of course, to all areas where this was not explicitly mentioned. This white paper explicitly does not cover the establishment of technical debt management across the entire company, but provides suggestions for this at the end.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning](https://arxiv.org/abs/2601.10998)
*Shinsuk Kang,Youngjae Kim*

Main category: cs.DC

TL;DR: 本文提出了一种名为AFLL（自适应反馈循环学习）的实时负载稳定系统，能够学习服务器发送消息与随后客户端请求之间的因果关系，并通过调整消息类型的权重来预测性地限制低优先级消息，从而在确保关键消息传递的同时防止过载。实验表明，AFLL能显著减少CPU时间、降低线程争用，并保持极高的可重复性。


<details>
  <summary>Details</summary>
Motivation: 针对大型多人在线游戏服务器面临的问题，即当服务器负载超出容量时，传统方法要么无差别地限制所有类型的消息传输（这会损害游戏体验），要么使用固定的启发式规则而无法适应动态的工作负载。文章旨在开发一种能够根据实际情况自动调整以维持服务性能的新方法。

Method: AFLL利用反向传播算法持续调节不同类型消息的权重值，实现对低重要性信息的事先拦截，同时保证核心通信不受影响。该方案通过后台计算和缓存优化达到了零学习开销的效果。

Result: 实验结果显示，在1000名并发玩家的情况下，AFLL将平均CPU处理时间减少了48.3%，峰值CPU时间降低了51.7%，并且线程竞争下降了64.4%。此外，所有测试指标下的变异系数均小于2%，证明了系统的高度一致性。

Conclusion: AFLL通过学习循环因果关系实现了对于延迟敏感型系统的实用实时适应能力，展示了其在维护高质量游戏体验方面的潜力。

Abstract: Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.

</details>


### [8] [Konflux: Optimized Function Fusion for Serverless Applications](https://arxiv.org/abs/2601.11156)
*Niklas Kowallik,Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: 本文提出了一种系统，能够分析复杂应用中所有可能的函数融合设置。通过模拟FaaS平台，该系统允许本地实验，无需重新配置实时平台，大大减少了相关的成本和时间。研究结果表明，在分析成本和延迟权衡时，只有一小部分融合配置代表了最优解，并且这些最优解受到所使用定价模型的强烈影响。


<details>
  <summary>Details</summary>
Motivation: 函数即服务（FaaS）已经成为无服务器云计算中的一个中心范式，但优化FaaS部署仍然具有挑战性。通过函数融合技术可以将多个功能合并成一个部署单元来减少由多个函数组成的应用的成本和延迟。然而，即使是在小型应用中，可能的融合配置数量也非常庞大，使得在生产环境中进行暴力基准测试既费时又费钱。

Method: 开发了一个系统，用于分析复杂应用程序中每一个潜在的融合设置。该系统模仿FaaS平台以支持本地实验，从而避免对实际运行平台的重新配置，并显著降低了相关联的成本与时间消耗。

Result: 评估了多个示例FaaS应用程序及其资源限制下的所有融合配置。发现只有少数几种融合配置能够在成本和延迟之间达到最佳平衡，并且这种最佳选择深受当前使用计价模式的影响。

Conclusion: 通过提出的系统可以在不增加额外成本的情况下，有效地探索出最佳的函数融合方案，这对于优化基于FaaS的服务有着重要意义。

Abstract: Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.
  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.

</details>


### [9] [Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering](https://arxiv.org/abs/2601.11487)
*Paulo Sérgio Almeida*

Main category: cs.DC

TL;DR: 本研究提出了一种新的基于SPS + FIFO的消息传递算法，该算法结合了发送者缓冲和接收者缓冲以强制执行因果顺序，克服了仅发送者缓冲方法的局限性，并实现了每条消息几乎恒定的元数据大小。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，遵循因果顺序的消息传递是一种经典且广泛有用的进程间通信抽象。现有的大多数方法都通过给消息打上因果信息标签并在接收方缓存直到可以安全地传递来实现这一点，但这种方法对于大量进程来说会产生禁止性的元数据开销。而仅在发送方进行消息缓存的方法虽然能够减少元数据开销，但由于其存在诸多缺点而不常用。

Method: 首先讨论了只在发送方缓冲消息的方法的局限性，并引入了发送方许可发送（SPS）强制策略，证明了SPS加上先进先出（FIFO）意味着因果关系。接着分析了一个最近采用SPS+FIFO的发送方缓冲算法Cykas，指出了它在吞吐量可扩展性和活性方面的问题。然后，提出了一种新颖的基于SPS+FIFO的算法，该算法采用了新的混合方法：通过结合发送方缓冲以实施SPS和接收方缓冲以确保FIFO来强制因果关系。

Result: 所提出的算法克服了仅发送方缓冲方法的限制，达到了实际上每条消息固定大小的元数据。通过精心选择数据结构，该算法还具有计算最优性，摊还后的处理开销实际上也是固定的。据我们所知，没有其他与拓扑无关的因果交付算法具备这些特性。

Conclusion: 这项工作介绍了一种创新的方法来改善分布式系统中的因果消息传递效率，特别是在降低元数据开销同时保持良好的吞吐量可扩展性和活性方面。

Abstract: Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [10] [Tail-Aware Data Augmentation for Long-Tail Sequential Recommendation](https://arxiv.org/abs/2601.10933)
*Yizhou Dang,Zhifu Wei,Minhan Huang,Lianbo Ma,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.IR

TL;DR: 本文提出了一种针对长尾序列推荐的尾部感知数据增强方法(TADA)，通过增强尾部项目/用户的交互频率同时保持头部性能，从而提高了模型对尾部的学习能力。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，大多数用户只能与少数项目进行互动，而大部分项目很少被消费。这种普遍存在的长尾挑战限制了模型学习用户偏好的能力。尽管之前的努力试图通过从头部部分或增加上下文信息来丰富尾部项目/用户的知识，但它们仍然面临两个问题：1) 难以改善尾部用户/项目的稀缺互动情况，导致尾部偏好学习不完整；2) 当提高尾部用户/项目的准确性时，现有方法往往降低整体或头部部分的表现，从而损害用户体验。

Method: 提出了Tail-Aware Data Augmentation (TADA) 方法，首先通过线性模型捕捉低流行度项目之间的共现和关联。基于此设计了两种尾部感知增强操作符T-Substitute和T-Insert。前者用相关项目替换头部项目，后者利用共现关系通过结合头部和尾部项目来扩展原始序列。将增强序列与原始序列在表示层面上混合以保留偏好知识，并进一步跨不同尾部用户序列及增强序列扩展混合操作生成更丰富的样本，从而提升尾部表现。

Result: 综合实验表明该方法具有优越性。

Conclusion: TADA有效地解决了长尾序列推荐中存在的问题，通过增强尾部项目/用户的交互频率同时保持头部性能，显著提升了模型对于尾部的学习能力。

Abstract: Sequential recommendation (SR) learns user preferences based on their historical interaction sequences and provides personalized suggestions. In real-world scenarios, most users can only interact with a handful of items, while the majority of items are seldom consumed. This pervasive long-tail challenge limits the model's ability to learn user preferences. Despite previous efforts to enrich tail items/users with knowledge from head parts or improve tail learning through additional contextual information, they still face the following issues: 1) They struggle to improve the situation where interactions of tail users/items are scarce, leading to incomplete preferences learning for the tail parts. 2) Existing methods often degrade overall or head parts performance when improving accuracy for tail users/items, thereby harming the user experience. We propose Tail-Aware Data Augmentation (TADA) for long-tail sequential recommendation, which enhances the interaction frequency for tail items/users while maintaining head performance, thereby promoting the model's learning capabilities for the tail. Specifically, we first capture the co-occurrence and correlation among low-popularity items by a linear model. Building upon this, we design two tail-aware augmentation operators, T-Substitute and T-Insert. The former replaces the head item with a relevant item, while the latter utilizes co-occurrence relationships to extend the original sequence by incorporating both head and tail items. The augmented and original sequences are mixed at the representation level to preserve preference knowledge. We further extend the mix operation across different tail-user sequences and augmented sequences to generate richer augmented samples, thereby improving tail performance. Comprehensive experiments demonstrate the superiority of our method. The codes are provided at https://github.com/KingGugu/TADA.

</details>


### [11] [Can Instructed Retrieval Models Really Support Exploration?](https://arxiv.org/abs/2601.10936)
*Piyush Maheshwari,Sheshera Mysore,Hamed Zamani*

Main category: cs.IR

TL;DR: 本文评估了遵循指令的检索模型在基于方面条件和种子引导探索中的应用，发现最佳的遵循指令的检索器在排名相关性上优于不考虑指令的方法，但在遵循指令的表现上存在不足，可能不适合需要高度敏感于指令的长期探索会话。


<details>
  <summary>Details</summary>
Motivation: 探索性搜索的特点是目标不明确且查询意图不断变化。在这种情况下，能够捕捉用户指定的查询意图细微差别并相应调整结果的检索模型是可取的——遵循指令的检索模型承诺提供这种能力。

Method: 使用专家注释的测试集对基于方面条件和种子引导探索这一常见但未充分探索的应用中的遵循指令检索器进行了评估。同时评估了最近为遵循指令检索而微调的大语言模型以及通过高效成对排序提示进行排名的一般用途大语言模型。

Result: 最佳的遵循指令检索器在排名相关性上相比不考虑指令的方法有所改进。然而，在与用户体验密切相关的指令遵循表现上，并没有反映出排名相关性的改进，反而显示出对指令的不敏感或反直觉行为。

Conclusion: 尽管用户可能会从当前的遵循指令检索器中受益，超过那些不考虑指令的模型，但对于需要更高指令敏感度的长时间探索会话来说，这些检索器可能并不适合。

Abstract: Exploratory searches are characterized by under-specified goals and evolving query intents. In such scenarios, retrieval models that can capture user-specified nuances in query intent and adapt results accordingly are desirable -- instruction-following retrieval models promise such a capability. In this work, we evaluate instructed retrievers for the prevalent yet under-explored application of aspect-conditional seed-guided exploration using an expert-annotated test collection. We evaluate both recent LLMs fine-tuned for instructed retrieval and general-purpose LLMs prompted for ranking with the highly performant Pairwise Ranking Prompting. We find that the best instructed retrievers improve on ranking relevance compared to instruction-agnostic approaches. However, we also find that instruction following performance, crucial to the user experience of interacting with models, does not mirror ranking relevance improvements and displays insensitivity or counter-intuitive behavior to instructions. Our results indicate that while users may benefit from using current instructed retrievers over instruction-agnostic models, they may not benefit from using them for long-running exploratory sessions requiring greater sensitivity to instructions.

</details>


### [12] [PruneRAG: Confidence-Guided Query Decomposition Trees for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11024)
*Shuguang Jiao,Xinyu Xiao,Yunfan Wei,Shuhan Qi,Chengkai Huang,Quan Z. Michael Sheng,Lina Yao*

Main category: cs.IR

TL;DR: 本文提出了一种名为PruneRAG的新框架，通过自适应节点扩展、基于置信度的决策以及细粒度检索机制来解决现有检索增强生成(RAG)系统中的证据遗忘和效率低下问题。实验表明，该方法在多跳问答基准测试中实现了比现有技术更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统在处理深层次推理链或搜索树扩张时面临两大主要问题：一是证据遗忘，即获取的知识未能得到有效利用；二是由于查询无限制扩展和重复检索导致的效率低下。这些问题揭示了当前RAG架构中检索与证据利用之间存在的重要差距。

Method: 为了解决上述挑战，作者们提出了PruneRAG，这是一个基于置信度指导的查询分解框架。它构建了一个结构化的查询分解树来进行稳定高效的推理过程。PruneRAG引入了三个关键机制：调节树宽和深度的自适应节点扩展、接受可靠答案并剪枝不确定分支的信心引导决策、以及通过提取实体级别锚点以提高检索精度的细粒度检索。

Result: 广泛的实验结果表明，在不同的多跳问答基准上，PruneRAG相比最先进基线模型达到了更高的准确率和效率。此外，研究者还定义了一个称为'证据遗忘率'的新指标，用于量化那些被成功检索但未正确使用的黄金证据的情况。

Conclusion: PruneRAG通过其独特的设计有效解决了传统RAG系统中存在的证据遗忘及低效问题，同时保持了多跳推理过程中重要证据的一致性，并大幅减少了检索开销。

Abstract: Retrieval-augmented generation (RAG) has become a powerful framework for enhancing large language models in knowledge-intensive and reasoning tasks. However, as reasoning chains deepen or search trees expand, RAG systems often face two persistent failures: evidence forgetting, where retrieved knowledge is not effectively used, and inefficiency, caused by uncontrolled query expansions and redundant retrieval. These issues reveal a critical gap between retrieval and evidence utilization in current RAG architectures. We propose PruneRAG, a confidence-guided query decomposition framework that builds a structured query decomposition tree to perform stable and efficient reasoning. PruneRAG introduces three key mechanisms: adaptive node expansion that regulates tree width and depth, confidence-guided decisions that accept reliable answers and prune uncertain branches, and fine-grained retrieval that extracts entity-level anchors to improve retrieval precision. Together, these components preserve salient evidence throughout multi-hop reasoning while significantly reducing retrieval overhead. To better analyze evidence misuse, we define the Evidence Forgetting Rate as a metric to quantify cases where golden evidence is retrieved but not correctly used. Extensive experiments across various multi-hop QA benchmarks show that PruneRAG achieves superior accuracy and efficiency over state-of-the-art baselines.

</details>


### [13] [Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings](https://arxiv.org/abs/2601.11124)
*Xiaoyu Liang,Yuchen Peng,Jiale Luo,Wenhao Wang,Haoji Hu,Xincheng Zhou*

Main category: cs.IR

TL;DR: 本文提出了一种新的两阶段框架Learn Before Represent (LBR)，旨在解决大型语言模型在专业领域如化学和法律中因缺乏特定领域知识而表现不佳的问题。通过首先注入领域知识，然后进行生成细化对比学习来保持语义一致性，从而显著提升了在医疗、化学和代码检索任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的“LLM+CL”范式虽然在一般表示学习方面表现出色，但在像化学和法律这样的垂直领域里表现欠佳，主要原因是缺乏特定领域的知识，特别是在专业术语处理上存在不足。

Method: 提出了Learn Before Represent (LBR)这一新颖的两阶段框架。首先通过信息瓶颈约束的生成学习阶段向模型注入领域知识，同时保持模型对因果注意力的关注以最大化知识获取并压缩语义；随后，在这些被压缩的表现形式上执行生成细化对比学习来进行对齐。

Result: 广泛的实验表明，LBR在医学、化学以及代码检索任务上显著优于其他强基线方法。

Conclusion: 这项工作为构建垂直领域内准确且鲁棒的表示建立了一个新范例。

Abstract: Large Language Models (LLMs) adapted via contrastive learning excel in general representation learning but struggle in vertical domains like chemistry and law, primarily due to a lack of domain-specific knowledge. This work identifies a core bottleneck: the prevailing ``LLM+CL'' paradigm focuses on semantic alignment but cannot perform knowledge acquisition, leading to failures on specialized terminology. To bridge this gap, we propose Learn Before Represent (LBR), a novel two-stage framework. LBR first injects domain knowledge via an Information Bottleneck-Constrained Generative Learning stage, preserving the LLM's causal attention to maximize knowledge acquisition while compressing semantics. It then performs Generative-Refined Contrastive Learning on the compressed representations for alignment. This approach maintains architectural consistency and resolves the objective conflict between generative and contrastive learning. Extensive experiments on medical, chemistry, and code retrieval tasks show that LBR significantly outperforms strong baselines. Our work establishes a new paradigm for building accurate and robust representations in vertical domains.

</details>


### [14] [Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration](https://arxiv.org/abs/2601.11144)
*Yuejie Li,Ke Yang,Tao Wang,Bolin Chen,Bowen Li,Chengjun Mao*

Main category: cs.IR

TL;DR: 提出了一种名为Deep GraphRAG的框架，通过分层检索策略和动态重排序模块来平衡全局搜索的全面性和局部搜索的效率，并使用一种新的强化学习方法DW-GRPO训练知识集成模块，以提高模型在相关性、准确性和简洁性方面的表现。实验表明该方法在准确率和效率上都优于基线图检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的检索增强生成（GraphRAG）框架在处理大规模层次图时面临着全局搜索全面性和局部搜索效率之间的权衡问题，同时缺乏稳健的多阶段重排序机制。

Method: Deep GraphRAG框架采用从宏观到微观的分层检索策略，结合了社区间的宏观上下文关系与社区内的微观上下文关系。整个过程分为三个阶段：社区间过滤、社区级细化以及实体级细粒度搜索。此外，还引入了一个基于束搜索优化的动态重排序模块，持续筛选候选者以确保效率与全局覆盖性的平衡。知识集成模块则利用紧凑型大语言模型(LLM)并通过动态加权奖励GRPO (DW-GRPO)进行训练。

Result: 评估结果表明，在Natural Questions和HotpotQA数据集上，Deep GraphRAG在准确性和效率方面显著优于基准图检索方法。

Conclusion: Deep GraphRAG通过其独特的分层检索方法及动态调整的重排序机制有效解决了现有GraphRAG框架所面临的挑战，为提高检索质量和效率提供了新思路。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.

</details>


### [15] [LLM-Assisted Pseudo-Relevance Feedback](https://arxiv.org/abs/2601.11238)
*David Otero,Javier Parapar*

Main category: cs.IR

TL;DR: 提出了一种结合大型语言模型(LLM)过滤阶段与经典伪相关反馈(PR)技术RM3的方法，以提高查询扩展的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的伪相关反馈方法如RM3容易受到主题漂移的影响，而基于大型语言模型生成查询扩展或变体的方法虽然有效但可能存在幻觉和术语不一致的问题。

Method: 在应用RM3估计之前加入一个基于LLM的过滤步骤：LLM对初始排名前k位的文档进行评估，仅接受被判断为相关的文档用于RM3计算。

Result: 该方法相较于传统的PRF以及强大的基线，在多个数据集和度量标准上均表现出改进。

Conclusion: 通过结合LLM的语义判断能力和经典PRF方法的优点，所提方法能够更有效地解决词汇不匹配问题并减少噪声内容影响。

Abstract: Query expansion is a long-standing technique to mitigate vocabulary mismatch in ad hoc Information Retrieval. Pseudo-relevance feedback methods, such as RM3, estimate an expanded query model from the top-ranked documents, but remain vulnerable to topic drift when early results include noisy or tangential content. Recent approaches instead prompt Large Language Models to generate synthetic expansions or query variants. While effective, these methods risk hallucinations and misalignment with collection-specific terminology. We propose a hybrid alternative that preserves the robustness and interpretability of classical PRF while leveraging LLM semantic judgement. Our method inserts an LLM-based filtering stage prior to RM3 estimation: the LLM judges the documents in the initial top-$k$ ranking, and RM3 is computed only over those accepted as relevant. This simple intervention improves over blind PRF and a strong baseline across several datasets and metrics.

</details>


### [16] [From SERPs to Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics](https://arxiv.org/abs/2601.11282)
*Junjie Wang,Gaole He,Alisa Rieger,Ujwal Gadiraju*

Main category: cs.IR

TL;DR: 本研究通过控制实验（N=483）探索了用户通过搜索引擎结果页面(SERPs)和AI生成的播客接收信息时态度变化的情况，特别是关注了信息接触顺序及方式对用户意见形成的影响。研究发现，信息接触的顺序确实影响态度改变，并且观点偏见以及话题争议性程度在塑造态度变化中起了一定作用，但个体调节因素没有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着搜索引擎结果页面(SERPs)与AI生成播客这两种媒介在日常信息搜寻行为中的日益融合，了解它们如何相互作用影响用户态度变得尤为重要，尤其是在涉及争议性、价值负载且常被讨论的话题背景下。

Method: 采用控制性用户研究方法(N=483)，考察了用户通过SERPs和AI生成播客获取信息时的态度效应，重点放在信息接触顺序及方式对用户意见形成的影响上。

Result: 大多数参与者表现出态度变化的结果；信息接触顺序对态度变化有影响；观点偏见及话题争议度在态度变化中扮演了一定角色，但未发现个体调节变量的影响。

Conclusion: 研究表明，在处理具有争议性的主题时，用户接触到的信息类型及其顺序可以显著影响其态度转变。此外，用户的初始偏见以及话题本身的争议程度也会影响这种转变过程。

Abstract: Compared to search engine result pages (SERPs), AI-generated podcasts represent a relatively new and relatively more passive modality of information consumption, delivering narratives in a naturally engaging format. As these two media increasingly converge in everyday information-seeking behavior, it is essential to explore how their interaction influences user attitudes, particularly in contexts involving controversial, value-laden, and often debated topics. Addressing this need, we aim to understand how information mediums of present-day SERPs and AI-generated podcasts interact to shape the opinions of users. To this end, through a controlled user study (N=483), we investigated user attitudinal effects of consuming information via SERPs and AI-generated podcasts, focusing on how the sequence and modality of exposure shape user opinions. A majority of users in our study corresponded to attitude change outcomes, and we found an effect of sequence on attitude change. Our results further revealed a role of viewpoint bias and the degree of topic controversiality in shaping attitude change, although we found no effect of individual moderators.

</details>


### [17] [Validating Search Query Simulations: A Taxonomy of Measures](https://arxiv.org/abs/2601.11412)
*Andreas Konstantin Kruff,Nolwenn Bernard,Philipp Schaer*

Main category: cs.IR

TL;DR: 本研究通过综合文献回顾，开发了一个分类法来结构化当前可用的度量方法，并通过实证分析验证了该分类法的有效性。研究还提供了关于在不同情境下验证用户模拟时应考虑哪些度量或度量组合的具体建议，并发布了一个包含最常用度量的专用库以促进未来的研究。


<details>
  <summary>Details</summary>
Motivation: 评估用户模拟器在信息检索系统评价中的有效性仍然是一个开放的问题，这限制了它们的有效使用及基于模拟结果的可靠性。

Method: 进行了全面的文献综述，特别关注于验证模拟用户查询与真实查询相关性的方法；基于综述开发了一个结构化现有度量方法的分类体系，并通过分析四个代表不同搜索场景的数据集之间不同度量的关系来实证支持这个分类体系。

Result: 提出了一个结构化的度量方法分类体系，且通过实证分析证明其有效性；给出了在不同情况下进行用户模拟验证时应考虑使用的度量或度量组合的具体建议；发布了包含最常用度量的专用库。

Conclusion: 研究为理解如何更好地验证用户模拟器提供了新的视角和实用工具，有助于提高基于模拟的信息检索系统评价的可靠性。

Abstract: Assessing the validity of user simulators when used for the evaluation of information retrieval systems remains an open question, constraining their effective use and the reliability of simulation-based results. To address this issue, we conduct a comprehensive literature review with a particular focus on methods for the validation of simulated user queries with regard to real queries. Based on the review, we develop a taxonomy that structures the current landscape of available measures. We empirically corroborate the taxonomy by analyzing the relationships between the different measures applied to four different datasets representing diverse search scenarios. Finally, we provide concrete recommendations on which measures or combinations of measures should be considered when validating user simulation in different contexts. Furthermore, we release a dedicated library with the most commonly used measures to facilitate future research.

</details>


### [18] [Isotropy-Optimized Contrastive Learning for Semantic Course Recommendation](https://arxiv.org/abs/2601.11427)
*Ali Khreis,Anthony Nasr,Yusuf Hilal*

Main category: cs.IR

TL;DR: 本文提出了一种基于BERT的自监督对比学习方法的语义课程推荐系统，通过数据增强和各向同性正则化改进了传统BERT嵌入表示的局限性，从而提高了课程推荐的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的BERT嵌入在处理课程描述时存在表示空间各向异性的问题，导致即使在语义相关性较低的情况下也显示出较高的余弦相似度。为了解决这一问题，并提高课程推荐系统的准确性和区分度，作者提出了新的解决方案。

Method: 采用自监督对比学习框架，结合数据增强技术和各向同性正则化手段来生成更具区分性的嵌入表示。该系统能够处理学生的文本查询，并从包含500多门跨多个学院工程课程的数据集中推荐前N个最相关的课程。

Result: 实验结果表明，与原始的BERT基线相比，经过微调后的模型在嵌入分离度以及课程推荐准确性方面都有所提升。

Conclusion: 通过引入对比学习、数据增强及各向同性正则化等技术手段，可以有效改善BERT在课程推荐任务中的表现，提供更精准的个性化课程推荐服务。

Abstract: This paper presents a semantic course recommendation system for students using a self-supervised contrastive learning approach built upon BERT (Bidirectional Encoder Representations from Transformers). Traditional BERT embeddings suffer from anisotropic representation spaces, where course descriptions exhibit high cosine similarities regardless of semantic relevance. To address this limitation, we propose a contrastive learning framework with data augmentation and isotropy regularization that produces more discriminative embeddings. Our system processes student text queries and recommends Top-N relevant courses from a curated dataset of over 500 engineering courses across multiple faculties. Experimental results demonstrate that our fine-tuned model achieves improved embedding separation and more accurate course recommendations compared to vanilla BERT baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Theoretically and Practically Efficient Resistance Distance Computation on Large Graphs](https://arxiv.org/abs/2601.11159)
*Yichun Yang,Longlong Lin,Rong-Hua Li,Meihao Liao,Guoren Wang*

Main category: cs.LG

TL;DR: 提出两种新的算法Lanczos Iteration和Lanczos Push，用于在大型图上高效计算电阻距离。这些算法减少了对图拉普拉斯矩阵条件数的依赖，提供了比现有方法更快的收敛速度，并且在效率和准确性方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的计算大型图上电阻距离的方法往往因为图拉普拉斯矩阵的条件数大而收敛缓慢。为了克服这个问题，论文提出了两个受经典Lanczos方法启发的新算法，旨在减少对条件数的依赖并提高计算效率。

Method: 论文提出了两种新算法：一种是全局算法Lanczos Iteration，其时间复杂度为$\tilde{O}(\sqrtκ m)$，与基于幂迭代的全局方法相比实现了$\sqrtκ$的速度提升；另一种是局部算法Lanczos Push，在某些温和条件下时间复杂度达到$\tilde{O}(κ^{2.75})$，相较于最先进的基于随机游走的局部算法有了显著改进。

Result: 通过在八个不同大小和统计特性的真实世界数据集上的广泛实验验证了所提算法的有效性，结果表明Lanczos Iteration和Lanczos Push在效率和准确性上明显优于现有最先进方法。

Conclusion: 提出的Lanczos Iteration和Lanczos Push算法有效解决了大型图上电阻距离计算的问题，不仅提高了计算效率还增强了准确性，为图分析领域中的多项应用提供了强有力的支持。

Abstract: The computation of resistance distance is pivotal in a wide range of graph analysis applications, including graph clustering, link prediction, and graph neural networks. Despite its foundational importance, efficient algorithms for computing resistance distances on large graphs are still lacking. Existing state-of-the-art (SOTA) methods, including power iteration-based algorithms and random walk-based local approaches, often struggle with slow convergence rates, particularly when the condition number of the graph Laplacian matrix, denoted by $κ$, is large. To tackle this challenge, we propose two novel and efficient algorithms inspired by the classic Lanczos method: Lanczos Iteration and Lanczos Push, both designed to reduce dependence on $κ$. Among them, Lanczos Iteration is a near-linear time global algorithm, whereas Lanczos Push is a local algorithm with a time complexity independent of the size of the graph. More specifically, we prove that the time complexity of Lanczos Iteration is $\tilde{O}(\sqrtκ m)$ ($m$ is the number of edges of the graph and $\tilde{O}$ means the complexity omitting the $\log$ terms) which achieves a speedup of $\sqrtκ$ compared to previous power iteration-based global methods. For Lanczos Push, we demonstrate that its time complexity is $\tilde{O}(κ^{2.75})$ under certain mild and frequently established assumptions, which represents a significant improvement of $κ^{0.25}$ over the SOTA random walk-based local algorithms. We validate our algorithms through extensive experiments on eight real-world datasets of varying sizes and statistical properties, demonstrating that Lanczos Iteration and Lanczos Push significantly outperform SOTA methods in terms of both efficiency and accuracy.

</details>


### [20] [Analytic Bijections for Smooth and Interpretable Normalizing Flows](https://arxiv.org/abs/2601.10774)
*Mathis Gerdes,Miranda C. N. Cheng*

Main category: cs.LG

TL;DR: 该论文提出三种新的解析双射——三次有理函数、sinh和三次多项式，这些双射在整个实数域上全局光滑($C^\infty$)且可封闭形式解析逆。此外，还开发了一种名为径向流的新架构，它在保持角度方向的同时变换径向坐标，具有训练稳定性高、参数量少等优点。实验表明，这些方法在一维和二维基准测试中表现优异，并能应用于高维物理问题如$\phi^4$晶格场论。


<details>
  <summary>Details</summary>
Motivation: 设计归一化流时面临的一个关键挑战是找到既具有表达力又保持可逆性且雅可比行列式易于计算的标量双射。现有方法存在权衡：仿射变换平滑且可解析逆但缺乏表达力；单调样条提供局部控制但仅逐段平滑且作用于有界域；残差流实现平滑但需要数值逆。

Method: 本文介绍了三类解析双射——三次有理、sinh以及三次多项式——它们在整个实数域上全局光滑（$C^\infty$），并且以封闭形式可解析地逆变。这些双射作为耦合流中的即插即用替代品，与样条性能相匹配或超越。除了耦合层外，还发展了径向流：一种利用直接参数化的新架构，它在保持角向不变的情况下转换径向坐标。

Result: 提出的双射在1D和2D基准测试中进行了全面评估，并通过$\phi^4$晶格场论实验展示了其对更高维度物理问题的应用能力。实验结果显示，所提出的双射不仅优于仿射基线，而且能够针对特定问题进行设计以解决模式崩溃问题。

Conclusion: 新提出的双射及径向流架构结合了之前所有方法的优点，提供了全局光滑性和封闭形式的解析逆，同时在实际应用中表现出色，特别是在处理具有径向结构的目标时，可以使用远少于传统方法的参数达到相似的质量。

Abstract: A key challenge in designing normalizing flows is finding expressive scalar bijections that remain invertible with tractable Jacobians. Existing approaches face trade-offs: affine transformations are smooth and analytically invertible but lack expressivity; monotonic splines offer local control but are only piecewise smooth and act on bounded domains; residual flows achieve smoothness but need numerical inversion. We introduce three families of analytic bijections -- cubic rational, sinh, and cubic polynomial -- that are globally smooth ($C^\infty$), defined on all of $\mathbb{R}$, and analytically invertible in closed form, combining the favorable properties of all prior approaches. These bijections serve as drop-in replacements in coupling flows, matching or exceeding spline performance. Beyond coupling layers, we develop radial flows: a novel architecture using direct parametrization that transforms the radial coordinate while preserving angular direction. Radial flows exhibit exceptional training stability, produce geometrically interpretable transformations, and on targets with radial structure can achieve comparable quality to coupling flows with $1000\times$ fewer parameters. We provide comprehensive evaluation on 1D and 2D benchmarks, and demonstrate applicability to higher-dimensional physics problems through experiments on $φ^4$ lattice field theory, where our bijections outperform affine baselines and enable problem-specific designs that address mode collapse.

</details>


### [21] [Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework](https://arxiv.org/abs/2601.10779)
*Qingyue Zhang,Chang Chu,Haohao Fu,Tianren Peng,Yanru Wu,Guanbo Huang,Yang Li,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 本文提出了一种名为UOWQ的理论框架，旨在解决多源迁移学习中的权重和样本量联合优化问题。通过将多源迁移学习表述为基于KL散度泛化误差度量的参数估计问题，UOWQ能够确定每个源任务的最佳权重和转移数量。实验结果表明该方法在多个实际基准测试中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的情况下，迁移学习对于提高模型性能至关重要。然而，从多个源任务直接进行均匀迁移可能导致负面迁移现象，这表明需要恰当地平衡不同来源的贡献。此外，现有的迁移学习方法通常只关注于优化源权重或迁移样本的数量，而忽略了两者的联合考虑。

Method: 提出了一个称为统一优化权重和数量（UOWQ）的理论框架，它基于KL散度泛化误差度量的渐近分析来公式化多源迁移学习问题。该框架共同决定了每个源任务的最佳源权重和最佳转移数量。首先证明了当适当调整权重时使用所有可用源样本总是最优的，并给出了这一现象的理论解释。此外，为了确定最佳转移权重，在单源设置下得出了闭式解，并为多源情况开发了一个基于凸优化的数值过程。

Result: 广泛的实验结果表明，无论是在DomainNet还是Office-Home等真实世界基准上，UOWQ都一致地超过了强大的基线方法。这些结果验证了我们的理论预测以及框架的实际有效性。

Conclusion: 通过引入UOWQ框架，本研究不仅提供了一种新的处理多源迁移学习的方法，还展示了其在实践中相对于其他方法的优势。

Abstract: Transfer learning plays a vital role in improving model performance in data-scarce scenarios. However, naive uniform transfer from multiple source tasks may result in negative transfer, highlighting the need to properly balance the contributions of heterogeneous sources. Moreover, existing transfer learning methods typically focus on optimizing either the source weights or the amount of transferred samples, while largely neglecting the joint consideration of the other. In this work, we propose a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), which formulates multi-source transfer learning as a parameter estimation problem grounded in an asymptotic analysis of a Kullback-Leibler divergence-based generalization error measure. The proposed framework jointly determines the optimal source weights and optimal transfer quantities for each source task. Firstly, we prove that using all available source samples is always optimal once the weights are properly adjusted, and we provide a theoretical explanation for this phenomenon. Moreover, to determine the optimal transfer weights, our analysis yields closed-form solutions in the single-source setting and develops a convex optimization-based numerical procedure for the multi-source case. Building on the theoretical results, we further propose practical algorithms for both multi-source transfer learning and multi-task learning settings. Extensive experiments on real-world benchmarks, including DomainNet and Office-Home, demonstrate that UOWQ consistently outperforms strong baselines. The results validate both the theoretical predictions and the practical effectiveness of our framework.

</details>


### [22] [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)
*Daniel Price,Prabhu Vellaisamy,John Shen,Di Wu*

Main category: cs.LG

TL;DR: 本研究扩展了值级并行性（VLP）在大型语言模型（LLMs）中的应用，不仅限于激活-权重GEMM，还涵盖了非线性近似和小批量不对称输入的优化，并设计了一种新的VLP架构Mugi，显著提高了吞吐量、能效以及减少了操作碳排放。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索值级并行性如何能够进一步提升大型语言模型的效率与性能，特别是在处理更复杂的操作时。

Method: 1. 对非线性近似进行了基于VLP的泛化，采用以价值为中心的方法来提高准确性。
2. 为小批量且具有不对称输入的GEMM优化VLP，利用最新的LLM优化技术如权重量化等。
3. 设计了一个名为Mugi的新VLP架构，整合上述创新点支持完整的LLM工作负载。

Result: 实验结果表明，Mugi架构对于非线性softmax操作，在吞吐量和能效方面分别达到了最高45倍和668倍的改进；对于整个LLM，则分别有2.07倍和3.11倍的改善。此外，该架构还能减少LLM运行过程中的操作碳足迹和固有碳足迹。

Conclusion: 通过将VLP应用于更广泛的场景并提出创新架构Mugi，这项研究证明了其在提高LLM性能及可持续性方面的巨大潜力。

Abstract: Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

</details>


### [23] [AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures](https://arxiv.org/abs/2601.10859)
*Dat Quoc Ha,Md Ferdous Alam,Markus J. Buehler,Faez Ahmed,Josephine V. Carstensen*

Main category: cs.LG

TL;DR: 本文提出了一种基于U-Net架构的机器学习模型，作为AI副驾来预测用户偏好的设计修改区域，从而减少拓扑优化（TO）中的人机交互迭代次数。该方法在提高可制造性或线性屈曲载荷方面表现出色，同时仅略微增加了总设计时间。


<details>
  <summary>Details</summary>
Motivation: 当前的拓扑优化工具虽然能够改进高性能工程结构的设计，但其广泛应用受到高计算时间和缺乏用户互动性的限制。现有的人机协作拓扑优化方法依赖于耗时的迭代区域选择过程。为了减少这种迭代尝试的数量，本研究开发了一个AI副驾系统，旨在通过机器学习技术预测用户可能感兴趣的修改区域。

Method: 研究采用了U-Net架构配置的预测模型，针对图像分割任务进行训练。该模型使用合成数据集进行训练，在这些数据集中，人类偏好被设定为识别最长的拓扑成员或最复杂的结构连接。经过训练后，模型能够向用户提供关于哪些区域适合修改的建议。

Result: 实验结果表明，所提出的AI辅助方法不仅能够成功推荐合理的修改区域，而且在多种不同类型的拓扑优化问题上都显示出了良好的泛化能力，并且在处理超出单个区域选择训练范围的情况时也表现出新行为。此外，与传统的简单拓扑优化相比，采用AI副驾的新方法可以将线性屈曲载荷提高39%，而总设计时间仅增加15秒。

Conclusion: 引入AI副驾显著提高了拓扑优化过程中的人机交互效率，减少了设计迭代次数，并且对于改善设计性能和加快设计流程具有潜在价值。

Abstract: Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.

</details>


### [24] [Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting](https://arxiv.org/abs/2601.10863)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: 本文提出了一种新的预测准确性与一致性评分（简称AC评分），用于评估概率多时间点预测的质量，同时考虑了多时间点准确性和稳定性。通过将此评分作为可微的目标函数应用于季节性ARIMA模型的训练，并在M4小时基准数据集上进行测试，结果显示，与传统的最大似然估计相比，显著减少了预测波动，同时保持或提高了点预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列预测方法仅优化准确性，而忽略了时间一致性，即当预测起点变化时，模型对未来同一事件预测的一致程度。为此，作者引入了一种新的度量标准——预测准确性与一致性评分（forecast AC score），旨在衡量概率多时间点预测的质量，同时考虑到准确性和稳定性。

Method: 提出了一种新的评价指标：预测准确性与一致性评分（forecast AC score），该指标能够同时衡量多时间点预测的准确性和稳定性。此外，还允许用户自定义权重来平衡准确性和一致性的需求。作为应用示例，作者将这一评分实现为一个可微分的目标函数，用于训练季节性ARIMA模型。

Result: 通过对M4 Hourly基准数据集的应用测试表明，使用AC评分优化的模型相比于采用传统最大似然估计方法训练的模型，在相同目标时间戳下实现了75%的预测波动减少，同时保持了相当甚至更优的点预测准确性。

Conclusion: 通过引入并应用预测准确性与一致性评分（forecast AC score）作为季节性ARIMA模型训练的目标函数，可以显著提高模型的时间一致性，大幅降低预测波动性，而不牺牲预测准确性。

Abstract: Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally provides for user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal ARIMA models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Our AC-optimized models achieve a 75\% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

</details>


### [25] [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905)
*Rajat Ghosh,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 本文提出了一种名为Action Shapley的度量标准，用于公正无偏地选择训练数据，并通过一个随机动态算法来计算它，该算法大幅降低了传统Shapley值计算的指数复杂性。实证研究表明，在五个实际案例中，此方法比传统的指数时间计算提高了超过80%的计算效率，且基于Action Shapley选择的训练数据策略始终优于随意选择的数据策略。


<details>
  <summary>Details</summary>
Motivation: 在离线和基于模型的强化学习系统中，世界模型用来模拟真实环境至关重要，尤其是在直接与真实环境交互成本高昂、危险或不切实际的情况下。这些世界模型的有效性和可解释性高度依赖于基础训练数据的质量。为了改善这一情况，提出了Action Shapley作为挑选高质量训练数据的新指标。

Method: 开发了一个随机化的动态算法以支持Action Shapley的计算，这个算法专门设计来解决传统Shapley值计算中存在的指数级复杂性问题。

Result: 通过对五个受限于数据的实际案例研究进行验证，提出的算法相比传统方法展现了超过80%的计算效率提升。此外，根据Action Shapley选择训练数据的方法始终优于非正式的选择方式。

Conclusion: 引入Action Shapley及其配套的随机化动态算法为高效选择训练数据提供了新的解决方案，这不仅提升了计算效率也增强了模型性能。

Abstract: Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

</details>


### [26] [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)
*Francis Ndikum Nji,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出了一种新的FAConvLSTM模型，用于处理高分辨率地球观测数据中的时空表示问题。该模型通过分解注意力机制和轻量级的[1x1]瓶颈等技术改进了传统ConvLSTM2D，在保持递归动力学的同时降低了计算复杂度，并且能够更好地捕捉长距离空间结构与多尺度交互过程。实验表明，FAConvLSTM相比标准ConvLSTM生成更稳定、可解释性更强及鲁棒性的潜在表示，同时显著减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 由于强烈的局部动态、远距离遥相关、多尺度相互作用以及非平稳性等因素，从高分辨率多变量地球观测数据中学习物理意义丰富的时空表示非常具有挑战性。常用的基线方法ConvLSTM2D存在计算成本高、建模长距离空间结构能力有限等问题。

Method: 提出了FAConvLSTM（Factorized-Attention ConvLSTM），这是一种旨在替代ConvLSTM2D的新层设计，它利用轻量级[1×1]瓶颈和共享深度方向的空间混合来分解循环门控计算，从而大幅降低通道复杂度而不牺牲递归动力学特性。此外，还引入了多尺度膨胀深度分支和挤压激励重新校准以有效建模跨空间尺度的物理过程交互，同时轴向空间注意机制有助于在不增加全局注意力成本的情况下捕捉远程依赖关系。

Result: 实验结果表明，FAConvLSTM在多变量时空气候数据上的表现优于标准ConvLSTM，不仅提供了更加稳定、易于理解且鲁棒性强的潜在表示，而且显著降低了计算负担。

Conclusion: FAConvLSTM为解决高分辨率地球观测数据中的时空表示难题提供了一个高效而强大的解决方案，它结合了因子化注意力机制和其他优化策略，实现了对长距离空间依赖性和多尺度物理过程的有效建模，同时保持较低的计算成本。

Abstract: Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convolutional gating incurs high computational cost and its strictly local receptive fields limit the modeling of long-range spatial structure and disentangled climate dynamics. To address these limitations, we propose FAConvLSTM, a Factorized-Attention ConvLSTM layer designed as a drop-in replacement for ConvLSTM2D that simultaneously improves efficiency, spatial expressiveness, and physical interpretability. FAConvLSTM factorizes recurrent gate computations using lightweight [1 times 1] bottlenecks and shared depthwise spatial mixing, substantially reducing channel complexity while preserving recurrent dynamics. Multi-scale dilated depthwise branches and squeeze-and-excitation recalibration enable efficient modeling of interacting physical processes across spatial scales, while peephole connections enhance temporal precision. To capture teleconnection-scale dependencies without incurring global attention cost, FAConvLSTM incorporates a lightweight axial spatial attention mechanism applied sparsely in time. A dedicated subspace head further produces compact per timestep embeddings refined through temporal self-attention with fixed seasonal positional encoding. Experiments on multivariate spatiotemporal climate data shows superiority demonstrating that FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

</details>


### [27] [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)
*Aakriti,Zhe Li,Dandan Liang,Chao Huang,Rui Li,Haibo Yang*

Main category: cs.LG

TL;DR: HOSL, a new hybrid-order split learning approach, combines zeroth-order (ZO) and first-order (FO) optimization to reduce memory usage on clients and improve convergence and performance on servers, showing significant improvements in memory efficiency and competitive accuracy for training large language models on edge devices.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the high memory overhead in split learning (SL) systems, which arises from the need to store intermediate activations for backpropagation on resource-constrained edge devices. While zeroth-order (ZO) optimization can reduce memory use, it often leads to slower convergence and lower performance. This paper aims to develop a solution that maintains the benefits of low memory usage offered by ZO while achieving the faster convergence and better performance typical of first-order (FO) methods, especially for large language models (LLMs).

Method: The proposed HOSL framework integrates zeroth-order (ZO) optimization for clients to minimize memory consumption and first-order (FO) optimization for servers to ensure fast convergence and good performance. The method employs ZO gradient estimation at the client end to avoid backpropagation and activation storage, thus reducing memory overhead. On the server side, FO optimization is used to maintain training speed and model quality. Theoretical analysis shows that HOSL's convergence rate depends more favorably on the client-side model dimension rather than the entire model, indicating improved convergence as more computations are offloaded to the server.

Result: Experimental results on OPT models (with 125M and 1.3B parameters) across six tasks show that HOSL can reduce client GPU memory usage by up to 3.7 times compared to FO methods, while still achieving accuracy within 0.20% to 4.23% of the baseline. Additionally, HOSL outperforms the ZO baseline by up to 15.55%, demonstrating its effectiveness in balancing memory efficiency and model performance on edge devices.

Conclusion: HOSL, a Hybrid-Order Split Learning framework, effectively balances memory efficiency and optimization performance by combining ZO optimization on the client side with FO optimization on the server side. It reduces client GPU memory usage significantly while maintaining or even improving model accuracy compared to both FO and ZO methods.

Abstract: Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which requires clients to store intermediate quantities such as activations for backpropagation. This results in substantial memory overhead, largely negating benefits of model partitioning. In contrast, zeroth-order (ZO) optimization eliminates backpropagation and significantly reduces memory usage, but often suffers from slow convergence and degraded performance. In this work, we propose HOSL, a novel Hybrid-Order Split Learning framework that addresses this fundamental trade-off between memory efficiency and optimization effectiveness by strategically integrating ZO optimization on the client side with FO optimization on the server side. By employing memory-efficient ZO gradient estimation at the client, HOSL eliminates backpropagation and activation storage, reducing client memory consumption. Meanwhile, server-side FO optimization ensures fast convergence and competitive performance. Theoretically, we show that HOSL achieves a $\mathcal{O}(\sqrt{d_c/TQ})$ rate, which depends on client-side model dimension $d_c$ rather than the full model dimension $d$, demonstrating that convergence improves as more computation is offloaded to the server. Extensive experiments on OPT models (125M and 1.3B parameters) across 6 tasks demonstrate that HOSL reduces client GPU memory by up to 3.7$\times$ compared to the FO method while achieving accuracy within 0.20%-4.23% of this baseline. Furthermore, HOSL outperforms the ZO baseline by up to 15.55%, validating the effectiveness of our hybrid strategy for memory-efficient training on edge devices.

</details>


### [28] [Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent](https://arxiv.org/abs/2601.10962)
*Ning Yang,Yikuan Zhang,Qi Ouyang,Chao Tang,Yuhai Tu*

Main category: cs.LG

TL;DR: 本研究通过分析SGD学习动态，揭示了一个非平衡机制，该机制影响了对较平坦、更泛化的解决方案的选择。实验表明，SGD轨迹在探索阶段会反复逃离尖锐的谷底并向损失景观中更平坦区域过渡。研究发现，SGD噪声将景观重塑为一种有效势能，有利于平坦解，并且随训练进行，能量障碍增加抑制了谷间转换，最终将动态限制在一个盆地内。增大SGD噪声强度可以延迟这一冻结过程，从而增强向更平坦最小值的收敛。


<details>
  <summary>Details</summary>
Motivation: 尽管随机梯度下降（SGD）对于深度学习至关重要，但其倾向于选择更为平坦且更具泛化能力的解决方案背后的动力学原理尚不清楚。

Method: 通过数值实验观察SGD轨迹的行为模式；利用物理模型展示SGD噪声如何改变损失景观的有效势能；分析随着训练进程SGD如何受到增长的能量屏障影响而逐渐冻结。

Result: 发现了SGD存在一个短暂的探索阶段，在此期间它能够从尖锐的山谷逃脱并朝向更平坦的区域移动；揭示了SGD噪声可以通过形成有利平坦解的有效势能来促进找到更好的解决方案；指出了随着时间推移，由于能量壁垒的增长，系统趋于冻结于某一特定盆地内；同时提出增加SGD噪声水平可推迟冻结现象发生，有助于达到更平坦的最小点。

Conclusion: 这项工作提供了一个统一的物理框架，连接了学习动态、损失景观几何以及泛化性能之间的关系，并为设计更有效的优化算法提供了指导原则。

Abstract: Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.

</details>


### [29] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 研究探讨了轻量级符号推理监督是否能提高紧凑自动程序修复模型中的修复类型分类。通过让大型教师模型提供结构化符号推理标签和修复类型标签，该方法在不增加模型大小或复杂度的情况下提高了性能，特别是在较少见的错误类别上。结果表明，正确的推理过程与正确的预测有很强的相关性，但并不完全决定预测结果。


<details>
  <summary>Details</summary>
Motivation: 小型代码模型在资源受限的环境中很有吸引力，但它们通常只能产生单一预测，这使得人们不清楚这些模型是学习到了有意义的程序结构还是仅仅依赖于浅层相关性。

Method: 提出了一种推理蒸馏方法，其中大型教师模型不仅提供修复类型标签，还提供捕捉错误高层次因果属性的结构化符号推理标签。然后，在IntroClass基准测试上使用CodeT5为基础的学生模型分别在仅有标签和含有推理蒸馏的信息下进行训练。

Result: 实验发现，加入推理监督后，宏观平均性能得到持续提升，尤其是在处理不太常见的错误类别时。此外，分析显示准确的推理轨迹与正确的修复类型预测之间存在强关联，尽管前者并不能完全决定后者。

Conclusion: 符号推理蒸馏是一种实用的方法，可以增强轻量级程序修复模型的可解释性和鲁棒性。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [30] [Constant Metric Scaling in Riemannian Computation](https://arxiv.org/abs/2601.10992)
*Kisung You*

Main category: cs.LG

TL;DR: 本文简要介绍了黎曼流形上常数度量缩放的影响，区分了在该操作下变化的量和不变的几何对象，并讨论了其对黎曼优化的意义。


<details>
  <summary>Details</summary>
Motivation: 作者旨在澄清在黎曼计算中引入全局度量缩放参数时，哪些量会发生变化而哪些几何结构保持不变，从而避免实践中可能出现的混淆。

Method: 文章通过理论分析的方式，列举了在进行常数度量缩放时会受到影响的数量（如范数、距离、体积元素及梯度大小）以及保持不变的几何对象（例如Levi-Civita联络、测地线、指数映射与对数映射和平行移动）。

Result: 结果表明，在黎曼优化过程中，常数度量缩放可以被看作是对步长的整体调整，而不是对底层几何结构的修改。

Conclusion: 本文提供了一个关于任意黎曼流形上常数度量缩放影响的简洁自包含说明，有助于理解如何在不影响依赖于这些方法的几何结构的情况下引入全局度量尺度参数。

Abstract: Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.

</details>


### [31] [Backdoor Attacks on Multi-modal Contrastive Learning](https://arxiv.org/abs/2601.11006)
*Simi D Kuniyilh,Rita Machacy*

Main category: cs.LG

TL;DR: 本文对对比学习中的后门攻击进行了全面且比较性的回顾，分析了威胁模型、攻击方法、目标领域以及可用的防御措施，并讨论了该领域的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于最近的研究表明，对比学习容易受到后门攻击和数据投毒攻击，因此本文旨在对该领域内存在的安全问题进行全面综述，并探讨其对工业及分布式环境系统安全部署的重要性。

Method: 通过文献综述的方式，本文分析了对比学习中针对不同应用场景（如视觉、多模态设置、图结构以及联邦学习）的后门攻击的威胁模型、具体攻击手段及其防御策略。

Result: 总结了对比学领域近期进展，强调了对比学习固有的特定脆弱性，并指出了当前面临的主要挑战与未来可能的研究路径。

Conclusion: 本研究发现对于确保基于对比学习的方法在实际应用中的安全性至关重要，特别是在工业级和分布式的环境中。

Abstract: Contrastive learning has become a leading self- supervised approach to representation learning across domains, including vision, multimodal settings, graphs, and federated learning. However, recent studies have shown that contrastive learning is susceptible to backdoor and data poisoning attacks. In these attacks, adversaries can manipulate pretraining data or model updates to insert hidden malicious behavior. This paper offers a thorough and comparative review of backdoor attacks in contrastive learning. It analyzes threat models, attack methods, target domains, and available defenses. We summarize recent advancements in this area, underline the specific vulnerabilities inherent to contrastive learning, and discuss the challenges and future research directions. Our findings have significant implications for the secure deployment of systems in industrial and distributed environments.

</details>


### [32] [AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2601.11028)
*Xinru Wen,Weizhong Lin,zi liu,Xuan Xiao*

Main category: cs.LG

TL;DR: 提出了一种名为AVP-Pro的两阶段预测框架，通过自适应特征融合和对比学习来提高抗病毒肽（AVPs）的识别精度。该模型在第一阶段达到了0.9531的准确率和0.9064的MCC值，超过了现有最先进方法；第二阶段则实现了对6个病毒家族及8种特定病毒的小样本条件下功能亚型预测。


<details>
  <summary>Details</summary>
Motivation: 现有的识别抗病毒肽的方法在捕捉复杂序列依赖性和区分高度相似的混淆样本方面存在局限性。为了克服这些挑战，并促进新药开发，需要一种更精确且强大的预测工具。

Method: AVP-Pro结合了自适应特征融合与对比学习技术。首先构建了一个包含10种不同描述符的全景特征空间，并设计了层次融合架构，该架构利用自注意力机制和自适应门控机制动态调整CNN提取的局部基序权重以及BiLSTM捕获的全局依赖关系。对于正负样本间高相似度导致决策边界模糊的问题，采用了基于BLOSUM62增强的在线难例挖掘(OHEM)驱动的对比学习策略。

Result: 在一般AVP识别的第一阶段中，模型取得了0.9531的准确率和0.9064的MCC值，优于现有最先进方法。第二阶段，在小样本条件下成功地对6个病毒家族及8种具体病毒进行了功能性亚型分类。

Conclusion: AVP-Pro为高效筛选抗病毒药物提供了强有力且可解释的新工具，并通过友好的网页界面增加了用户访问便利性。

Abstract: The accurate identification of antiviral peptides (AVPs) is crucial for novel drug development. However, existing methods still have limitations in capturing complex sequence dependencies and distinguishing confusing samples with high similarity. To address these challenges, we propose AVP-Pro, a novel two-stage predictive framework that integrates adaptive feature fusion and contrastive learning. To comprehensively capture the physicochemical properties and deep-seated patterns of peptide sequences, we constructed a panoramic feature space encompassing 10 distinct descriptors and designed a hierarchical fusion architecture. This architecture integrates self-attention and adaptive gating mechanisms to dynamically modulate the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Targeting the blurred decision boundary caused by the high similarity between positive and negative sample sequences, we adopted an Online Hard Example Mining (OHEM)-driven contrastive learning strategy enhanced by BLOSUM62. This approach significantly sharpened the model's discriminative power. Model evaluation results show that in the first stage of general AVP identification, the model achieved an accuracy of 0.9531 and an MCC of 0.9064, outperforming existing state-of-the-art (SOTA) methods. In the second stage of functional subtype prediction, combined with a transfer learning strategy, the model realized accurate classification of 6 viral families and 8 specific viruses under small-sample conditions. AVP-Pro provides a powerful and interpretable new tool for the high-throughput screening of antiviral drugs. To further enhance accessibility for users, we have developed a user-friendly web interface, which is available at https://wwwy1031-avp-pro.hf.space.

</details>


### [33] [OpFML: Pipeline for ML-based Operational Forecasting](https://arxiv.org/abs/2601.11046)
*Shahbaz Alvi,Giusy Fedele,Gabriele Accarino,Italo Epicoco,Ilenia Manco,Pasquale Schiano*

Main category: cs.LG

TL;DR: 本文介绍了一个名为OpFML的机器学习模型服务管道，用于周期性预测，并通过应用于每日火灾危险指数预测展示了其功能。


<details>
  <summary>Details</summary>
Motivation: 传统的野火风险评估方法往往高估了野火的风险，而基于数据驱动的方法和机器学习技术在气候与地球科学领域的应用越来越广泛。因此，开发一个可以适应不同需求且能有效进行周期性预测的机器学习系统变得尤为重要。

Method: 作者们开发出了OpFML（Operational Forecasting with Machine Learning），这是一个可配置、灵活的管道工具，专为部署机器学习模型以执行周期性预测任务设计。

Result: OpFML被成功应用于每日火灾危险指数的预报中，展示了该管道的强大功能及其多种特性。

Conclusion: OpFML提供了一种有效的方法来利用机器学习进行周期性的灾害风险评估，特别是对于野火危险指数的日常预测，显示出良好的潜力。

Abstract: Machine learning is finding its application in a multitude of areas in science and research, and Climate and Earth Sciences is no exception to this trend. Operational forecasting systems based on data-driven approaches and machine learning methods deploy models for periodic forecasting. Wildfire danger assessment using machine learning has garnered significant interest in the last decade, as conventional methods often overestimate the risk of wildfires. In this work, we present the code OpFML: Operational Forecasting with Machine Learning. OpFML is a configurable and adaptable pipeline that can be utilized to serve a machine learning model for periodic forecasting. We further demonstrate the capabilities of the pipeline through its application to daily Fire Danger Index forecasting and outline its various features.

</details>


### [34] [Soft Bayesian Context Tree Models for Real-Valued Time Series](https://arxiv.org/abs/2601.11079)
*Shota Saito,Yuta Nakahara,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 本文提出了软贝叶斯上下文树模型（Soft-BCT），这是一种针对实值时间序列的新模型，采用概率分割而非确定性分割，并基于变分推断提出了一种学习算法。在某些实际数据集上，Soft-BCT的表现与之前的BCT相当或更优。


<details>
  <summary>Details</summary>
Motivation: 为了改进处理实值时间序列的方法，作者开发了Soft-BCT模型，该模型利用概率性分割上下文空间来替代原有的确定性分割方式，旨在提高模型性能。

Method: 通过引入软（概率性的）上下文空间分割方法，创建了名为Soft-BCT的新模型，并且基于变分推断设计了相应的学习算法。

Result: 实验结果表明，在一些真实世界的数据集上，Soft-BCT模型展现出了与之前版本的BCT相媲美甚至更好的性能。

Conclusion: Soft-BCT作为一种新颖的处理实值时间序列的方法，通过采取软分割策略成功地提高了预测准确性，为相关领域提供了有价值的参考。

Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.

</details>


### [35] [Differentially Private Subspace Fine-Tuning for Large Language Models](https://arxiv.org/abs/2601.11113)
*Lele Zheng,Xiang Wang,Tao Zhang,Yang Cao,Ke Cheng,Yulong Shen*

Main category: cs.LG

TL;DR: 本文提出了一种名为DP-SFT的两阶段子空间微调方法，通过在任务特定的低维子空间中注入差分隐私噪声来减少噪声幅度并保持模型性能和训练稳定性。实验表明，该方法在严格的差分隐私约束下提高了准确性、稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在下游任务上的微调往往依赖敏感数据，这引发了隐私问题。尽管差分隐私技术能够提供强有力的隐私保护，并且已被广泛应用于微调过程中，但直接在整个高维参数空间内添加噪声会导致大的扰动范数，从而降低模型表现并破坏训练过程的稳定性。为了解决这个问题，作者们开发了新的方法。

Method: DP-SFT是一种两阶段的方法。第一阶段，通过分析主要梯度方向来确定与特定任务相关的更新信号所在的低维子空间；第二阶段，在这个子空间上投影全梯度，加入差分隐私噪声后，再将扰动后的梯度映射回原始参数空间进行模型更新。这样可以在不干扰无关参数的前提下有效保护隐私。

Result: 多个数据集上的实验证明了DP-SFT能够在严格遵守差分隐私约束的同时提升模型准确性和训练稳定性，加快收敛速度，并相对于基于差分隐私的传统微调基线取得了显著改进。

Conclusion: DP-SFT提供了一种有效的方法来解决大型语言模型微调中的隐私保护难题，通过专注于对任务相关参数子空间应用差分隐私技术，既保证了用户隐私又维持了良好的模型性能。

Abstract: Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.

</details>


### [36] [Optimized Algorithms for Text Clustering with LLM-Generated Constraints](https://arxiv.org/abs/2601.11118)
*Chaoqi Jia,Weihong Wu,Longkun Guo,Zhigang Lu,Chao Chen,Kok-Leong Ong*

Main category: cs.LG

TL;DR: 提出了一种新的约束生成方法，通过生成约束集而不是使用传统的成对约束来减少资源消耗，提高了查询效率和约束准确性，并引入了针对LLM生成约束特性的约束聚类算法。


<details>
  <summary>Details</summary>
Motivation: 为了提高聚类质量，研究人员试图利用大型语言模型（LLMs）自动生成约束条件来指导聚类过程。本文旨在通过一种新颖的约束生成方法减少资源消耗，并且改进现有的约束生成及聚类性能。

Method: 提出了一种新方法，该方法生成约束集而非单个的成对约束，同时设计了一个与LLM生成约束相适应的约束聚类算法，该算法包含置信度阈值和惩罚机制以处理潜在不准确的约束。

Result: 在五个文本数据集上的评估显示，所提出的方法实现了与最先进算法相当的聚类精度，同时减少了超过20倍的LLM查询次数。

Conclusion: 本研究展示了一种有效减少资源消耗并保持高聚类精度的新方法，为利用大型语言模型进行约束生成提供了一种高效途径。

Abstract: Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.

</details>


### [37] [Shape-morphing programming of soft materials on complex geometries via neural operator](https://arxiv.org/abs/2601.11126)
*Lu Chen,Gengxiang Chen,Xu Liu,Jingyan Su,Xuhao Lyu,Lihui Wang,Yingguang Li*

Main category: cs.LG

TL;DR: 本文提出了一种光谱和空间神经算子(S2NO)，它能够在复杂几何形状上实现高保真变形预测，结合进化算法可以对各种复杂几何形状进行体素级材料分布优化，从而显著提高了编程复杂形状变形的效率和能力。


<details>
  <summary>Details</summary>
Motivation: 尽管在简单几何形状的基本形状变形设计方面已经取得了进展，但要在复杂几何形状上实现准确且多样的变形设计仍然具有挑战性，这限制了形状变形软材料在诸如贴合植入物部署或空气动力学变形等高级应用中的潜力。

Method: 研究者开发了一种名为Spectral and Spatial Neural Operator (S2NO)的新方法，该方法通过整合Laplacian特征函数编码与空间卷积来捕捉不规则计算域上的全局和局部变形行为。此外，S2NO与进化算法相结合，实现了针对不同复杂几何形状（包括边界不规则形状、多孔结构和薄壁结构）的体素级材料分布优化。

Result: S2NO能够为复杂的几何形状提供高精度的变形预测，并允许进行超分辨率材料分布设计，进而增加了变形设计的多样性和复杂性。

Conclusion: 这项工作通过引入S2NO提升了对于复杂几何形状的高保真度变形预测能力，同时结合进化算法使得在多种复杂几何形状上进行精确的形状变形编程成为可能，极大地增强了形状变形设计的能力。

Abstract: Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.

</details>


### [38] [FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling](https://arxiv.org/abs/2601.11134)
*Sultan Amed,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: 本文提出了一种结合贝叶斯差分隐私的联邦生存学习框架（FSL-BDP），用于在不集中敏感数据的情况下建模违约时间轨迹。实验表明，在联邦设置下，贝叶斯差分隐私比传统的差分隐私更有效，接近非私有性能，并为多机构环境中的隐私保护决策支持系统设计提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着数据保护规则的加强，跨边界共享借款人数据变得越来越受限，但信用风险模型却需要跨机构学习来提高准确性。传统的方法存在两个问题：二分类忽略了违约时间的重要性，以及集中训练违反了新的监管限制。

Method: 提出了一个名为FSL-BDP的框架，该框架允许金融机构在遵守数据保护法规的同时，通过联合学习方式共同学习风险动态，同时提供贝叶斯差分隐私保证。

Result: 实验证明，在联邦环境下，与经典差分隐私相比，贝叶斯差分隐私从联合学习中获得的好处更多（+7.0% 对 +1.4%），并且对于大多数参与的客户端来说表现优于经典差分隐私。

Conclusion: 选择隐私机制时应考虑目标部署架构而非仅依赖于集中式基准测试结果。研究发现为受监管的多机构环境中设计隐私保护决策支持系统的实践者提供了可操作的指南。

Abstract: Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\% vs +1.4\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.

</details>


### [39] [Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction](https://arxiv.org/abs/2601.11135)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果推断的上下文感知图因果推理框架CaMol，用于解决分子属性预测中样本少的问题。通过引入上下文图编码化学知识、可学习的原子掩码策略以及分布干预者等方法，实现了对因果子结构的有效识别和分离。实验表明，该模型在少量样本任务上具有更高的准确性和样本效率，并且发现的因果子结构与化学知识中的功能基团高度一致，支持了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 在Web服务中，分子属性预测是一个重要的应用领域，如在线蛋白质结构预测和药物发现。当面临只有少量标记分子可用于预测未见属性的情况时，如何有效进行预测成为了一个关键挑战。尽管已有研究尝试利用上下文学习来捕捉分子和属性之间的关系，但这些方法在利用功能基团的先验知识及识别直接与属性相关的关键子结构方面存在局限性。

Method: 提出了一种名为CaMol的上下文感知图因果推理框架，该框架假设每个分子包含一个决定特定属性的潜在因果结构。首先，构建了一个上下文图来编码化学知识，将功能基团、分子和属性关联起来以指导因果子结构的发现；其次，设计了一种可学习的原子掩码策略来区分因果子结构与混淆子结构；最后，引入了一种分布干预者，通过结合因果子结构与基于化学原理的混淆因素来进行后门调整，从而将因果效应从实际化学变化中分离出来。

Result: 在多种分子数据集上的实验显示，CaMol在少量样本任务中达到了更高的准确度和样本效率，展示了其对于未知属性的良好泛化能力。此外，所发现的因果子结构与关于功能基团的化学知识高度吻合，这进一步验证了模型的解释能力。

Conclusion: CaMol通过采用因果推断视角解决了分子属性预测中面临的两大难题：一是利用与属性因果相连的功能基团的先验知识，二是识别直接与属性相关的关键子结构。实验证明了该方法不仅提高了预测准确性，还增强了模型对于新属性的适应性及可解释性。

Abstract: Molecular property prediction is becoming one of the major applications of graph learning in Web-based services, e.g., online protein structure prediction and drug discovery. A key challenge arises in few-shot scenarios, where only a few labeled molecules are available for predicting unseen properties. Recently, several studies have used in-context learning to capture relationships among molecules and properties, but they face two limitations in: (1) exploiting prior knowledge of functional groups that are causally linked to properties and (2) identifying key substructures directly correlated with properties. We propose CaMol, a context-aware graph causality inference framework, to address these challenges by using a causal inference perspective, assuming that each molecule consists of a latent causal structure that determines a specific property. First, we introduce a context graph that encodes chemical knowledge by linking functional groups, molecules, and properties to guide the discovery of causal substructures. Second, we propose a learnable atom masking strategy to disentangle causal substructures from confounding ones. Third, we introduce a distribution intervener that applies backdoor adjustment by combining causal substructures with chemically grounded confounders, disentangling causal effects from real-world chemical variations. Experiments on diverse molecular datasets showed that CaMol achieved superior accuracy and sample efficiency in few-shot tasks, showing its generalizability to unseen properties. Also, the discovered causal substructures were strongly aligned with chemical knowledge about functional groups, supporting the model interpretability.

</details>


### [40] [Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines](https://arxiv.org/abs/2601.11154)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 本研究比较了直升机发动机的两种预测性维护策略：有监督分类方法和基于自编码器的无监督异常检测方法。在故障数据稀缺或不完整的情况下，自编码器方法不需要故障标签即可实现有效检测，显示出在航空航天应用中早期故障检测的潜力。


<details>
  <summary>Details</summary>
Motivation: 为减少直升机发动机意外故障导致的操作中断、安全隐患及高昂维修成本，需要有效的预测性维护策略。

Method: 研究采用了两种方法：一种是有监督分类流程，依赖于正常和故障行为的标记示例；另一种是基于自编码器（AE）的无监督异常检测方法，仅使用健康发动机数据来学习正常运行模型，并将偏离视为潜在故障。

Result: 有监督模型在可以获得标注故障的情况下表现出色，而自编码器即使在缺乏故障标签时也能实现有效的故障检测。

Conclusion: 比较表明，在准确性、数据可用性和部署可行性之间存在实际权衡，强调了无监督学习作为航空航天应用中早期故障检测可行解决方案的潜力。

Abstract: Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.

</details>


### [41] [Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026](https://arxiv.org/abs/2601.11160)
*Claudia Plant,Lena G. M. Bauer,Christian Böhm*

Main category: cs.LG

TL;DR: 本文探讨了聚类算法在处理大规模真实数据集时如何平衡抽象与表示之间的关系，并介绍了子空间和深度聚类方法如何通过支持更丰富的表示来处理高维和复杂数据。同时指出，随着表示表达能力的增加，需要明确地在目标函数中加强抽象以确保执行的是聚类而非单纯的表示学习。最后展望了未来研究方向，即更加自适应地平衡抽象与表示，以提高性能、能效和可解释性。


<details>
  <summary>Details</summary>
Motivation: 面对大型实际数据集，寻找自然分组的方法变得尤为重要。有效的聚类不仅需要从个体对象的冗余细节中进行抽象，还必须具备能够突出群体间关键特征差异的丰富表现形式。

Method: 文章回顾了几种不同的聚类技术，包括经典的K-means算法以及新兴的子空间和深度聚类方法。特别强调了当前深度聚类方法是如何定义并通过基于中心点或密度的损失函数来实施抽象过程的。此外，还讨论了一种分离学习策略：一个潜在空间用于学习与聚类相关的信息，另一个则捕捉数据中的其他信息。

Result: 研究表明，通过采用更高级的表示方式（如子空间和深度学习），可以更好地处理高维度及复杂的数据类型。然而，为了保证这些方法确实是在执行聚类任务而不是单纯的学习数据表示，有必要在目标函数中直接加入促进抽象化的成分。

Conclusion: 未来的研究将致力于开发能够更灵活地调整抽象与表示之间平衡的新方法，从而提升聚类算法的整体性能、能源效率及其结果的可理解性。

Abstract: How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.
  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.
  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.

</details>


### [42] [LSTM VS. Feed-Forward Autoencoders for Unsupervised Fault Detection in Hydraulic Pumps](https://arxiv.org/abs/2601.11163)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 研究了两种无监督自编码器方案（前馈模型和LSTM模型）用于工业液压泵的早期故障检测，仅使用正常运行时的数据进行训练，在没有故障样本的情况下仍能实现高可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了解决工业液压泵意外故障可能导致生产中断和重大成本的问题，探索了能够及早发现故障的方法。

Method: 采用了两种自编码器（AE）方案：一种是分析单个传感器快照的前馈模型，另一种是捕捉短时间窗口的长短期记忆（LSTM）模型。这些模型仅使用来自52个传感器通道每分钟日志的健康数据进行训练，并通过包含七个已标注故障区间的单独数据集来评估。

Result: 尽管在训练过程中没有使用任何故障样本，但两种模型都展现出了很高的可靠性。

Conclusion: 研究表明，即使缺乏故障样本数据，利用无监督学习方法如自编码器也能有效实现工业液压泵的早期故障检测。

Abstract: Unplanned failures in industrial hydraulic pumps can halt production and incur substantial costs. We explore two unsupervised autoencoder (AE) schemes for early fault detection: a feed-forward model that analyses individual sensor snapshots and a Long Short-Term Memory (LSTM) model that captures short temporal windows. Both networks are trained only on healthy data drawn from a minute-level log of 52 sensor channels; evaluation uses a separate set that contains seven annotated fault intervals. Despite the absence of fault samples during training, the models achieve high reliability.

</details>


### [43] [TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation](https://arxiv.org/abs/2601.11184)
*Xiangyu Xu,Qingsong Zhong,Jilin Hu*

Main category: cs.LG

TL;DR: 提出了一种结构解缠的多尺度生成框架用于时间序列分析，通过双路径VQ-VAE分解趋势与季节性成分，并采用基于指导的重构策略来提高生成质量。实验表明该方法优于现有技术，特别是在参数量大幅减少的情况下仍能生成高质量长时间序列。


<details>
  <summary>Details</summary>
Motivation: 为了解决时间序列数据分析中数据稀缺性和隐私挑战的问题，特别是针对时间序列结构复杂性（如多尺度时间模式和异构组件）处理不足的问题。

Method: 开发了一个结构解缠的多尺度生成框架，使用双路径VQ-VAE分离趋势与季节性元素，并采取从粗到细的方式进行自回归生成；还引入了基于引导的重构策略，利用粗糙的季节信号作为先验来指导精细季节模式的重构。

Result: 在六个数据集上的实验显示，所提方法相较于现有技术能够产生更高质量的时间序列，在参数数量显著降低的同时表现出色，尤其是在生成长期高质量序列方面。

Conclusion: 本研究提供了一种有效应对时间序列中数据稀缺和隐私问题的新方法，它不仅能够更好地处理时间序列的结构复杂性，还能以较少的模型参数实现优秀的长序列生成效果。

Abstract: Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.

</details>


### [44] [FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization](https://arxiv.org/abs/2601.11200)
*Haiyang Xiao,Weiqing Li,Jinyue Guo,Guochao Jiang,Guohua Liu,Yuewei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FAQ（Family-Aware Quantization）的新方法，通过利用同一家族的大语言模型先验知识生成高保真校准样本，从而提高量化参数的准确性。实验结果表明，与使用原始校准数据相比，FAQ能够将精度损失减少高达28.5%。


<details>
  <summary>Details</summary>
Motivation: 后训练量化(PTQ)为在资源受限设备上部署大型语言模型提供了一种有效的数值压缩方案，但校准数据的代表性与通用性仍然是决定量化参数准确性的核心瓶颈。传统PTQ方法通常依赖于有限的样本集，难以捕捉推理阶段的激活分布，导致量化参数偏差。

Method: 提出了FAQ（Family-Aware Quantization），这是一种校准数据再生框架，它利用来自目标模型同一家族中的更大语言模型的先验知识来生成一系列高保真度的校准样本。具体来说，首先将原始校准样本输入到同家族的一个更大的LLM中，使用高度一致的知识体系重新生成一系列高保真校准数据。随后，这些携带链式思维并符合预期激活分布的数据会在专家指导下进行组间竞争以选出最佳样本，并经过重新归一化处理，以增强标准PTQ的有效性。

Result: 在包括Qwen3-8B在内的多个模型系列上的实验显示，相比于基于原始校准数据的基线，FAQ可以将精度损失减少至多28.5%，展示了其强大的潜力和贡献。

Conclusion: FAQ方法有效地解决了后训练量化过程中因校准数据不足而导致的精度损失问题，通过引入同家族大模型的先验知识提高了校准数据的质量，进而提升了量化模型的整体性能。

Abstract: Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.

</details>


### [45] [SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients](https://arxiv.org/abs/2601.11219)
*Zhikang Shen,Jianrong Lu,Haiyuan Wan,Jianhai Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为SDFLoRA的新方法，通过将每个客户端适配器分解为一个捕捉可转移知识的全局模块和一个保留客户端特定适应性的本地模块来解决联邦学习中由于不同客户端使用不同的低秩配置导致的直接聚合偏差和不稳定问题。该设计不仅支持在秩异质性下的鲁棒学习，还通过仅向全局模块注入差分隐私噪声来实现隐私保护优化。实验表明，SDFLoRA在GLUE基准测试上优于其他联邦LoRA基线方法，并实现了更好的效用-隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习（FL）部署中，由于不同客户端可能采用不同的低秩配置而导致直接聚合LoRA更新时出现偏差与不稳定性。当前解决方案要么强制统一秩、要么将异构更新对齐到共享子空间，这些做法限制了个性化并且在差分隐私噪音下对本地客户端信息提供较弱的保护。

Method: 提出了Selective Dual-module Federated LoRA (SDFLoRA)，将每个客户端适配器拆分为负责捕捉可迁移知识的全局模块以及保持客户端特定适应性的局部模块。全局模块被选择性地跨客户端对齐和聚合，而局部模块则保持私有化。此外，通过对全局模块专门注入差分隐私噪声来促进隐私意识优化。

Result: 在GLUE基准测试上的实验显示，SDFLoRA相比代表性联邦LoRA基线表现更佳，并且达到了更好的效用-隐私平衡。

Conclusion: SDFLoRA通过创新的设计解决了联邦学习场景下因秩异质性引起的挑战，同时增强了模型性能和个人数据隐私保护之间的平衡。

Abstract: Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.

</details>


### [46] [Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques](https://arxiv.org/abs/2601.11222)
*Haochen Wu,Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 本文提出了一种新的算子学习框架（MAD-BNO），仅依赖于域边界数据来学习线性偏微分方程的解，而无需全领域采样。通过使用数学人工数据方法生成训练数据，该框架能够有效处理不同类型的边界条件，并在多个基准测试中表现出色，同时显著减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于算子学习的方法需要对整个域进行采样的问题，作者提出了一种只利用边界数据的新框架。这种方法不仅减少了对额外测量或数值模拟的需求，还提供了一个完全数据驱动的解决方案。

Method: 提出了一个名为MAD-BNO的新方法，它结合了数学人工数据（MAD）技术与边界神经算子的概念。此方法通过从目标问题的基本解合成所有训练数据，专注于学习边界到边界的映射。训练完成后，可以利用边界积分公式高效地恢复任意位置的内部解。

Result: 实验结果表明，在二维拉普拉斯、泊松和亥姆霍兹方程等基准算子学习任务上，MAD-BNO方法达到甚至超过了现有神经算子方法的精度，同时大大缩短了训练所需的时间。

Conclusion: MAD-BNO作为一种创新的算子学习方案，展示了其在解决各种边界条件下线性偏微分方程方面的潜力。此外，该框架易于扩展至三维问题及复杂几何形状。

Abstract: For linear partial differential equations with known fundamental solutions, this work introduces a novel operator learning framework that relies exclusively on domain boundary data, including solution values and normal derivatives, rather than full-domain sampling. By integrating the previously developed Mathematical Artificial Data (MAD) method, which enforces physical consistency, all training data are synthesized directly from the fundamental solutions of the target problems, resulting in a fully data-driven pipeline without the need for external measurements or numerical simulations. We refer to this approach as the Mathematical Artificial Data Boundary Neural Operator (MAD-BNO), which learns boundary-to-boundary mappings using MAD-generated Dirichlet-Neumann data pairs. Once trained, the interior solution at arbitrary locations can be efficiently recovered through boundary integral formulations, supporting Dirichlet, Neumann, and mixed boundary conditions as well as general source terms. The proposed method is validated on benchmark operator learning tasks for two-dimensional Laplace, Poisson, and Helmholtz equations, where it achieves accuracy comparable to or better than existing neural operator approaches while significantly reducing training time. The framework is naturally extensible to three-dimensional problems and complex geometries.

</details>


### [47] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: 研究者们提出了一种名为Parametric Skill Transfer (PaST) 的框架，用于解决大型语言模型（LLMs）的知识截止问题。通过从源领域提取与领域无关的技能向量并将其线性注入目标模型中，PaST能够有效提升模型在新知识上的适应能力。实验表明，在知识整合问答和工具使用基准测试上，该方法优于现有的技术，并显示出强大的可扩展性和跨域迁移能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型由于其参数记忆是固定的，无法直接内化新信息，导致了“知识截止”的挑战。虽然监督微调(SFT)可以更新模型的知识，但它往往只更新事实内容而不改善模型利用这些新信息的能力。强化学习(RL)对于获得推理技能至关重要，但高昂的计算成本使其难以高效地在线适应。基于SFT和RL诱导的参数更新几乎正交这一观察结果，研究旨在开发一种更有效的知识适应解决方案。

Method: 提出了Parametric Skill Transfer (PaST) 框架，该框架允许从一个源领域提取出与领域无关的技能向量，并在线性地将这些技能转移到已经经过轻量级SFT处理以适应新数据的目标模型中。

Result: 实验显示，在SQuAD、LooGLE等知识整合型问答任务以及ToolBench这样的工具使用基准测试上，PaST表现优异。特别是在SQuAD上比当前最先进的自我编辑SFT基线高出至多9.9分；在LooGLE长上下文QA上绝对准确率提高了8.0分；同时在ToolBench上的零样本成功率平均提升了10.3个百分点，且这种改进在不同工具类别间保持一致。

Conclusion: PaST作为一种新颖的方法，不仅解决了大型语言模型面临的知识截止问题，还展示了出色的性能提升效果，特别是在需要快速适应新环境或任务时。此外，它还证明了所提出的技能向量具有良好的跨域迁移能力。

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [48] [Sample-Near-Optimal Agnostic Boosting with Improved Running Time](https://arxiv.org/abs/2601.11265)
*Arthur da Cunha,Miakel Møller Høgsgaard,Andrea Paudice*

Main category: cs.LG

TL;DR: 本文提出了一种新的无先验假设条件下的提升算法，该算法在样本大小上具有近似最优的样本复杂度，并且运行时间是多项式的。


<details>
  <summary>Details</summary>
Motivation: 尽管提升方法在经典设定下已经被很好地理解了，但在没有对数据做任何假设的情况下（即无先验情况下），它还没有得到充分的研究。最近关于无先验提升的研究虽然几乎解决了样本复杂度的问题，但已知达到这一界限的算法有着指数级的运行时间。

Method: 作者们设计了一个新的无先验提升算法，在考虑问题其他参数固定的情况下，其运行时间与样本量成多项式关系。

Result: 开发出首个在样本复杂度上接近最优且当其他问题参数固定时运行时间为多项式的无先验提升算法。

Conclusion: 这项工作为无先验学习环境提供了一个有效的解决方案，使得即使是在不对数据做出额外假设的情况下也能够高效地构建强学习器。

Abstract: Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.

</details>


### [49] [Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning](https://arxiv.org/abs/2601.11283)
*Nabil Belacel,Mohamed Rachid Boulassel*

Main category: cs.LG

TL;DR: 本研究通过结合尿液代谢组学与可解释的机器学习框架，识别出与ADHD相关的生物化学特征。使用最接近相似性分类器分析了来自52名ADHD患者和46名对照者的靶向代谢组学数据，基于14种代谢物实现了AUC>0.97的表现，这些发现为ADHD病理生理机制提供了见解，并支持开发面向未来的诊断平台。


<details>
  <summary>Details</summary>
Motivation: 注意缺陷多动障碍(ADHD)是一种常见的神经发育障碍，但缺乏客观的诊断工具。因此，迫切需要开发基于生物学的客观诊断方法以促进精准精神病学的发展。

Method: 采用尿液代谢组学联合一种具有内置特征选择功能的最接近相似性（CR）分类器来分析52名ADHD患者及46名对照个体的靶向代谢谱型。

Result: CR模型在表现上优于随机森林和K-最近邻分类算法，仅利用一组减少至14种的代谢物即达到了AUC>0.97的成绩。所涉及的代谢产物如多巴胺4-硫酸盐、N-乙酰天冬氨酰谷氨酸和瓜氨酸等，映射到了多巴胺能神经传递及氨基酸代谢路径上。

Conclusion: 这项工作展示了一个结合代谢组学与可解释机器学习的转化框架，旨在推动ADHD领域内更加客观且基于生物学信息的诊断策略发展。CR分类器以其透明的决策边界和较低的计算成本特点，适合集成到目标代谢组学检测以及未来即时护理诊断平台中。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.

</details>


### [50] [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)
*Zhihan Yang,Jiaqi Wei,Xiang Zhang,Haoyu Dong,Yiwen Wang,Xiaoke Guo,Pengkun Zhang,Yiwei Xu,Chenyu You*

Main category: cs.LG

TL;DR: 提出了一种名为FORESTLLM的新框架，它结合了决策森林的结构归纳偏置和大型语言模型（LLMs）的语义推理能力，以改善在少量样本设置下的表格数据学习效果。


<details>
  <summary>Details</summary>
Motivation: 在少数样本情况下，传统基于树的方法由于依赖于统计纯度指标而容易过拟合，直接应用大型语言模型则忽视了数据内在结构，导致性能不佳。

Method: 开发了FORESTLLM框架，该框架通过引入语义分割标准来评估候选分区的一致性，并采用一次性上下文推理机制稳定叶节点预测，将LLM用作训练阶段的设计者，将其丰富的上下文知识编码到轻量级可解释的森林模型中。

Result: FORESTLLM在多种少量样本分类和回归基准测试中达到了最先进的表现。

Conclusion: FORESTLLM通过结合决策森林与大型语言模型的优势，在处理少量标签样本时能够更有效地从表格数据中学习，为金融、医疗保健等领域提供了新的解决方案。

Abstract: Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.

</details>


### [51] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://arxiv.org/abs/2601.11342)
*Chuanyue Yu,Jiahui Wang,Yuhan Li,Heng Chang,Ge Lan,Qingyun Sun,Jia Li,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 本文探索了扩散语言模型(DLMs)在检索增强生成(RAG)框架下的表现，发现DLMs结合RAG虽然展现出依赖上下文信息的潜力，但存在生成精度不足的问题。为了解决响应语义漂移(RSD)问题，提出了保持语义的检索增强扩散(SPREAD)框架，通过引导去噪过程确保生成内容与查询语义一致，实验表明SPREAD显著提高了生成答案的精度并有效减少了RSD。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成(RAG)在提升大型语言模型(LLMs)方面取得了巨大成功，但由于LLM和DLM解码方式的根本差异，DLMs在此框架下的潜力尚未被充分探索。研究旨在填补这一空白，同时解决DLMs在RAG框架下遇到的生成精度有限及响应语义漂移(RSD)问题。

Method: 提出了Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD)框架，该框架引入了一种基于查询相关性的去噪策略，旨在通过主动引导去噪轨迹来保证生成的内容与查询语义相吻合，并有效抑制语义漂移。

Result: 实验结果显示，SPREAD能够显著提高生成答案的精确度，并有效地缓解了RAG框架内的响应语义漂移(RSD)问题。

Conclusion: 研究证明了DLMs与RAG结合具有利用更丰富上下文信息的能力，但也指出了其存在的挑战。通过引入SPREAD框架解决了关键的响应语义漂移问题，为未来进一步优化DLMs在RAG框架中的应用提供了新的方向。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.

</details>


### [52] [FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting](https://arxiv.org/abs/2601.11350)
*Jaehoon Lee,Seungwoo Lee,Younghwi Kim,Dohee Kim,Sunghyun Sim*

Main category: cs.LG

TL;DR: 提出了一种名为FEATHer的轻量级时间序列预测模型，专为边缘设备设计，能够在参数数量极低的情况下实现准确的长期预测。


<details>
  <summary>Details</summary>
Motivation: 随着工业系统向自动化发展，需要在具有严格延迟和内存限制的边缘设备上运行模型，但传统深度架构通常不适用这种场景。

Method: FEATHer模型通过引入超轻量多尺度分解、共享密集时间核、频谱感知分支门控以及稀疏周期核来解决这一问题，从而在保证模型紧凑性的同时提高预测精度。

Result: FEATHer仅用400个参数就能超越基准模型，在八个基准测试中获得最佳排名，并记录了60个第一名的成绩，平均排名为2.05。

Conclusion: 该研究表明，即使是在资源受限的边缘硬件上，也能够实现可靠的长距离预测，为工业实时推理提供了一个实用的方向。

Abstract: Time-series forecasting is fundamental in industrial domains like manufacturing and smart factories. As systems evolve toward automation, models must operate on edge devices (e.g., PLCs, microcontrollers) with strict constraints on latency and memory, limiting parameters to a few thousand. Conventional deep architectures are often impractical here. We propose the Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) for accurate long-term forecasting under severe limits. FEATHer introduces: (i) ultra-lightweight multiscale decomposition into frequency pathways; (ii) a shared Dense Temporal Kernel using projection-depthwise convolution-projection without recurrence or attention; (iii) frequency-aware branch gating that adaptively fuses representations based on spectral characteristics; and (iv) a Sparse Period Kernel reconstructing outputs via period-wise downsampling to capture seasonality. FEATHer maintains a compact architecture (as few as 400 parameters) while outperforming baselines. Across eight benchmarks, it achieves the best ranking, recording 60 first-place results with an average rank of 2.05. These results demonstrate that reliable long-range forecasting is achievable on constrained edge hardware, offering a practical direction for industrial real-time inference.

</details>


### [53] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: 本研究探讨了使用离线强化学习设计自主CPU电源控制器的方法，以提高并行应用程序运行时的能效而不严重影响其性能。通过结合在线应用无关性能数据和硬件性能计数器，并利用Intel的运行平均功率限制在实际系统上控制功率，证明了离线训练的智能体能够在可接受的性能损失下显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着计算基础设施设计中对能源效率的重视，CPU设计中已经加入了功率调节和感知能力，使得能够开发出可以实时监控和调整能源消耗与性能的系统软件。尽管强化学习看似是设计这种能源效率控制系统的好方法，但在线训练存在从缺乏适当模型来设置合适的模拟环境到如果在实时系统上部署训练则会出现扰动（噪音）和可靠性问题等多种挑战。因此，本文提出了一种替代方案：采用离线强化学习来避免在线RL训练带来的问题。

Method: 研究者们采取了一种灰色盒子方法，即结合在线应用无关性能数据（例如心跳）和硬件性能计数器，来确保科学目标得以实现的同时性能下降最小。该方法利用之前收集的各种策略下的状态转移数据集进行离线RL训练，而不是依赖于在线交互式学习。

Result: 通过对一系列计算密集型和内存密集型基准测试评估该方法，并且通过Intel的运行平均功率限制在实时系统上控制功率，结果显示离线训练的智能体可以在可容忍的性能降级成本下大幅减少能源消耗。

Conclusion: 离线强化学习为设计自主CPU电源控制器提供了一条可行路径，它能在保持应用程序性能的同时改善能源效率。这表明离线强化学习可能是解决数据中心等环境中能源效率问题的一个有前景的方向。

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [54] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: 本文提出了一种基于配对自编码器的数据驱动潜在空间推断框架，以处理求解逆问题时的观测不一致问题。通过在参数空间和观测空间之间学习映射，该框架能够在低维且信息丰富的潜在空间中实现正则化反演和优化。实验表明，该方法在处理部分、噪声或分布外数据时能保持与物理模型的一致性，并且在医学断层扫描和地球物理地震波形反演等成像示例中显示出比单独使用配对自编码器或端到端编解码器更准确的重建效果。


<details>
  <summary>Details</summary>
Motivation: 解决逆问题过程中遇到的观测数据不一致（如部分缺失、噪声污染或超出训练分布）挑战，同时保持与基础物理模型的一致性。

Method: 开发了一个基于配对自编码器的新颖框架，其中一个用于参数空间而另一个用于观测空间。通过学习两个自编码器间潜在空间的映射关系来实现在低维度潜在空间内的正则化反演及优化过程。

Result: 提出的框架能够有效地处理含有噪音、部分丢失或不在原始分布范围内的数据，同时维持与物理模型的一致性。相比单纯使用配对自编码器或相同架构下的端到端编码-解码方案，在存在数据不一致的情况下提供了更为精确的结果。

Conclusion: 本研究展示了一种利用配对自编码器进行潜在空间推理的有效策略，适用于多种科学与工程领域的逆问题解决场景，特别是在医学断层成像和地球物理地震波形反演等领域表现优异。

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


### [55] [Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families](https://arxiv.org/abs/2601.11428)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 本文系统地测试了Fourier神经算子（FNOs）在不同偏微分方程（PDEs）族上的鲁棒性问题，揭示了参数或边界条件分布变化可导致误差增加超过一个数量级，并提供了改进操作学习鲁棒性的见解。


<details>
  <summary>Details</summary>
Motivation: 尽管Fourier神经算子(FNOs)在学习偏微分方程(PDEs)的解映射方面表现出色，但其在面对分布偏移、长周期预测以及结构扰动时的鲁棒性尚未得到充分理解。研究旨在通过一系列受控的压力测试来探索FNOs在五种不同类型的PDE家族中的失效模式。

Method: 开发了一个系统性的压力测试框架，针对分散型、椭圆型、多尺度流体、金融及混沌系统这五类不同的PDE家族进行评估。设计了包括参数偏移、边界或终端条件变更、分辨率外推结合频谱分析及迭代预测在内的控制性压力测试，以揭示如频谱偏差、累积积分错误及对受限边界状态过拟合等脆弱点。

Result: 大规模评估显示，参数或边界条件的分布变化可以使错误放大超过一个数量级；而分辨率的变化主要集中在高频模式上引起错误。输入扰动通常不会放大错误，但在最坏的情况下（例如局部泊松扰动），挑战仍然存在。

Conclusion: 这些发现为比较失败模式提供了一份地图，并且对于提高操作学习中的鲁棒性提供了可行的见解。

Abstract: Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests--including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts--to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1{,}000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.

</details>


### [56] [When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models](https://arxiv.org/abs/2601.11444)
*Raphaël Razafindralambo,Rémy Sun,Frédéric Precioso,Damien Garreau,Pierre-Alexandre Mattei*

Main category: cs.LG

TL;DR: 研究探讨了集成方法在无条件分数基础扩散模型中的应用，发现尽管集成提高了分数匹配损失和模型似然性，但并未一致地改善图像数据集上的感知质量指标如FID。此外，在表格数据上发现了一种优于其他策略的聚合策略，并对分数模型求和提供了理论见解。


<details>
  <summary>Details</summary>
Motivation: 虽然集成是改进监督模型的一种众所周知的方法，但它在无条件分数基础扩散模型中的应用仍待探索。本研究旨在调查集成是否为生成建模带来实际好处。

Method: 通过使用深度集成、蒙特卡洛Dropout等技术，研究者们在CIFAR-10和FFHQ数据集上测试了多种聚合规则的效果。同时，也考察了表格数据，尝试理解不同聚合策略的表现差异及其背后的原因。

Result: 研究表明，尽管集成可以提高分数匹配损失及模型似然度，但对于像FID这样的感知质量度量却没有持续性的提升。另外，在处理表格数据时，发现特定的聚合策略表现更优。

Conclusion: 对于无条件分数基础扩散模型而言，简单的集成方法可能不足以显著提升生成样本的质量。研究还指出，关于分数估计与图像质量之间的关系需要进一步探究，同时也为分数模型的组合技术提供了理论上的见解。

Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).

</details>


### [57] [Low-Rank Key Value Attention](https://arxiv.org/abs/2601.11471)
*James O'Neill,Robert Clancy,Mariia Matskevichus,Fergal Reid*

Main category: cs.LG

TL;DR: 本文提出了一种名为低秩KV适应（LRKV）的多头注意力修改方法，通过利用注意力头之间的冗余性来减少KV缓存内存，同时保持完整的令牌级分辨率。实验表明，LRKV在大规模预训练中比标准注意力、MQA/GQA和MLA表现更好，特别是在减少训练计算量方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer预训练对内存和计算资源的需求不断增加，关键值（KV）缓存成为训练和自回归解码过程中的主要瓶颈。为了解决这个问题，作者提出了一个简单的改进方案——低秩KV适应（LRKV），旨在降低KV缓存所需内存的同时保留完整的令牌级别解析度。

Method: LRKV通过对每个层使用共享全秩KV投影并结合低秩头部特定残差的方式实现，从而提供了一个介于完全共享与完全独立注意力机制之间的连续权衡方案。这种方法能够作为标准多头注意力机制的直接替代品，并且自然地包含了查询共享方法如多查询或多组查询注意机制，但与潜变量压缩方法不同。

Result: 实验结果显示，在多个大规模预训练任务上，LRKV相比标准注意力、MQA/GQA以及MLA等方法，不仅损失下降更快、验证困惑度更低，而且下游任务性能也更强。特别地，在2.5B规模下，LRKV能够在使用大约一半KV缓存的情况下优于标准注意力，并且以累积FLOPs衡量时可以达到少至20-25%的训练计算需求。

Conclusion: 研究结果表明，LRKV作为一种实用有效的注意力机制，能够在受限于内存和计算资源条件下很好地扩展Transformer预训练。它几乎保留了相对于标准注意力的所有功能头多样性，而其他更激进的KV共享机制则依赖于补偿性的查询专业化。

Abstract: Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.

</details>


### [58] [Extractive summarization on a CMOS Ising machine](https://arxiv.org/abs/2601.11491)
*Ziqing Zeng,Abhimanyu Kumar,Chris H. Kim,Ulya R. Karpuzcu,Sachin S. Sapatnekar*

Main category: cs.LG

TL;DR: 该研究探索了在低功耗CMOS耦合振荡器Ising机（COBI）上实现McDonald风格的抽取式摘要生成的可行性。通过提出一种硬件感知的Ising公式化方法和开发一个完整的ES流程，包括随机舍入与迭代精化以及分解策略，使得仅使用整数耦合且精度有限的Ising硬件就能产生高质量摘要。实验结果显示，COBI相比暴力求解法实现了3-4.5倍的速度提升，并且能耗降低了两到三个数量级，同时保持了具有竞争力的摘要质量。


<details>
  <summary>Details</summary>
Motivation: 虽然现代抽取式摘要系统使用强大的神经模型达到了很高的准确度，但它们通常依赖于能源密集型的CPU或GPU基础设施，这些设施不适合资源受限环境中的实时推理。因此，研究者们探索了利用低功耗CMOS耦合振荡器Ising机器来实现实时、低能耗文本摘要的可能性。

Method: 1. 提出了一种硬件感知的Ising公式化方法，以减少局部场与耦合项之间的规模不平衡问题，从而提高了对系数量化的鲁棒性。
2. 开发了一个完整的抽取式摘要(ES)流程，包括随机舍入与迭代精化过程来补偿精度损失。
3. 设计了一种分解策略，将大的ES问题分割成较小的Ising子问题，这些问题可以在COBI上高效解决后再合并。

Result: 实验结果表明，在CNN/DailyMail数据集上，所提出的管道能够仅依靠整数耦合Ising硬件（有限精度条件下）生成高质量摘要。相比于暴力求解方法，COBI实现了3-4.5倍的速度提升；其能效比软件Tabu搜索算法高出几个数量级的同时，依然保持了竞争性的摘要质量。

Conclusion: 本研究表明，CMOS Ising求解器部署用于边缘设备上的实时、低能耗文本摘要具有巨大潜力。通过优化问题表述及处理流程，即便是在计算资源极为有限的情况下也能够有效执行复杂的自然语言处理任务。

Abstract: Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports integer-valued, all-to-all spin couplings. We first propose a hardware-aware Ising formulation that reduces the scale imbalance between local fields and coupling terms, thereby improving robustness to coefficient quantization: this method can be applied to any problem formulation that requires k of n variables to be chosen. We then develop a complete ES pipeline including (i) stochastic rounding and iterative refinement to compensate for precision loss, and (ii) a decomposition strategy that partitions a large ES problem into smaller Ising subproblems that can be efficiently solved on COBI and later combined. Experimental results on the CNN/DailyMail dataset show that our pipeline can produce high-quality summaries using only integer-coupled Ising hardware with limited precision. COBI achieves 3-4.5x runtime speedups compared to a brute-force method, which is comparable to software Tabu search, and two to three orders of magnitude reductions in energy, while maintaining competitive summary quality. These results highlight the potential of deploying CMOS Ising solvers for real-time, low-energy text summarization on edge devices.

</details>


### [59] [MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management](https://arxiv.org/abs/2601.11505)
*Miriam K. Wolff,Peter Calhoun,Eleonora Maria Aiello,Yao Qin,Sam F. Royston*

Main category: cs.LG

TL;DR: 本研究创建了一个统一且可访问的数据资源MetaboNet，用于1型糖尿病(T1D)的算法开发。该数据集整合了多个公开可用的T1D数据集，包括连续血糖监测(CGM)数据、胰岛素泵剂量记录以及其他辅助信息。MetaboNet是迄今为止规模最大的T1D管理数据集之一，旨在提高算法发展的可比性和普遍适用性，并提供了两种访问途径：完全公开子集和需要签署数据使用协议(DUA)的受限子集。


<details>
  <summary>Details</summary>
Motivation: 由于现有T1D管理数据集之间存在分散化和缺乏标准化的问题，限制了T1D算法的发展。当前数据集在结构上差异显著，获取与处理耗时较长，阻碍了数据集成并降低了算法发展成果之间的可比性和通用性。

Method: 通过整合多个公开可用的T1D数据集形成一个统一资源——MetaboNet数据集。要求纳入的数据集需同时包含连续血糖监测（CGM）数据及其对应的胰岛素泵给药记录；当存在时，还会保留诸如报告的碳水化合物摄入量和体力活动等辅助信息。

Result: 创建出的MetaboNet数据集包含了3135名受试者以及1228患者年的重叠CGM和胰岛素数据，这使得它比现有的独立基准数据集要大得多。此外，还为那些需要通过特定申请流程才能访问的数据集提供了自动转换成标准MetaboNet格式的数据处理管道。

Conclusion: 提出了一个综合性的公共T1D研究数据集MetaboNet，描述了其无限制部分和受DUA管辖部分的访问路径。该数据集覆盖了广泛的血糖谱和人口统计学特征，因此可以比单独的数据集提供更具有普遍性的算法性能。

Abstract: Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.

</details>
