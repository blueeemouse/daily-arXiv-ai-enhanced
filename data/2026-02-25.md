<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 10]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [cuRPQ: A High-Performance GPU-Based Framework for Processing Regular and Conjunctive Regular Path Queries](https://arxiv.org/abs/2602.20748)
*Sungwoo Park,Seohyeon Kim,Min-Soo Kim*

Main category: cs.DB

TL;DR: 提出了cuRPQ，一个针对RPQs和CRPQs的高性能GPU优化框架，通过新颖的遍历算法、高效的访问集合管理方案以及并发探索-实体化策略来解决关键的GPU挑战。实验表明cuRPQ比现有方法快了几个数量级且不会出现内存溢出错误。


<details>
  <summary>Details</summary>
Motivation: 正则路径查询（RPQs）对于路径约束可达性分析至关重要，而更复杂的变体如合取正则路径查询（CRPQs）在图分析中越来越被广泛使用。评估这些查询计算成本很高，但迄今为止还没有研究探索过GPU加速的可能性。

Method: 开发了一个名为cuRPQ的高性能GPU优化框架，该框架通过引入新的遍历算法、有效管理访问集的方法及同时进行探索与实体化的策略来应对主要的GPU挑战。

Result: 广泛的实验证明，cuRPQ相对于最先进方法有显著性能提升，速度提高了多个数量级，并且避免了内存不足的问题。

Conclusion: cuRPQ作为首个针对RPQs和CRPQs设计的GPU优化解决方案，不仅大幅提升了处理效率，还解决了传统方法中存在的内存限制问题，为图数据分析提供了强有力的支持。

Abstract: Regular path queries (RPQs) are fundamental for path-constrained reachability analysis, and more complex variants such as conjunctive regular path queries (CRPQs) are increasingly used in graph analytics. Evaluating these queries is computationally expensive, but to the best of our knowledge, no prior work has explored GPU acceleration. In this paper, we propose cuRPQ, a high-performance GPU-optimized framework for processing RPQs and CRPQs. cuRPQ addresses the key GPU challenges through a novel traversal algorithm, an efficient visited-set management scheme, and a concurrent exploration-materialization strategy. Extensive experiments show that cuRPQ outperforms state-of-the-art methods by orders of magnitude, without out-of-memory errors.

</details>


### [2] [RISK: Efficiently processing rich spatial-keyword queries on encrypted geo-textual data](https://arxiv.org/abs/2602.20952)
*Zhen Lv,Cong Cao,Hongwei Huo,Jiangtao Cui,Yanguo Peng,Hui Li,Yingfan Liu*

Main category: cs.DB

TL;DR: 提出了RISK，一种用于加密地理文本数据的丰富空间关键词查询模型，支持安全范围和k最近邻查询，具有较高的查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有的对称可搜索加密方案依赖于特定任务、不兼容的索引，限制了其实用性。

Method: 在文本优先然后是空间的方式下，RISK基于一种新颖的k最近邻四叉树(kQ-tree)构建，该树嵌入了代表性和区域最近邻居，并使用标准密码学工具进一步加密kQ-tree。

Result: 实验表明，对于1%范围查询和10-最近邻查询，RISK在响应时间上分别比最先进方法至少高出0.5和4个数量级。

Conclusion: RISK无缝地支持安全范围和k-最近邻查询，在IND-CKA2模型下被证明是安全的，并且可以扩展到多方场景和动态更新。

Abstract: Symmetric searchable encryption (SSE) for geo-textual data has attracted significant attention. However, existing schemes rely on task-specific, incompatible indices for isolated specific secure queries (e.g., range or k-nearest neighbor spatial-keyword queries), limiting practicality due to prohibitive multi-index overhead. To address this, we propose RISK, a model for rich spatial-keyword queries on encrypted geo-textual data. In a textual-first-then-spatial manner, RISK is built on a novel k-nearest neighbor quadtree (kQ-tree) that embeds representative and regional nearest neighbors, with the kQ-tree further encrypted using standard cryptographic tools (e.g., keyed hash functions and symmetric encryption). Overall, RISK seamlessly supports both secure range and k-nearest neighbor queries, is provably secure under IND-CKA2 model, and extensible to multi-party scenarios and dynamic updates. Experiments on three real-world and one synthetic datasets show that RISK outperforms state-of-the-art methods by at least 0.5 and 4 orders of magnitude in response time for 1% range queries and 10-nearest neighbor queries, respectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [The Tragedy of Chain Commons](https://arxiv.org/abs/2602.20341)
*Ignacio Amores-Sesar,Mirza Ahad Baig,Seth Gilbert,Ray Neiheiser,Michelle X. Yeo*

Main category: cs.DC

TL;DR: 本文首次系统地研究了共识与执行分离设计在区块链中的应用，指出了这种设计可能导致一种新的攻击方式——gaslighting，并证明了在这种分离模型中，抵御这种攻击和资源利用率之间存在一个基本的权衡。为了解决这个问题，文章讨论了一种基于领导者的中间模型，该模型既能抵抗gaslighting攻击，又能实现高吞吐量和低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术的发展，拜占庭容错（BFT）共识机制成为许多追求高性能和低延迟的现代区块链的基础。然而，在共识路径上进行交易执行和验证逐渐成为一个瓶颈问题。尽管将订单处理与执行分开的设计提高了性能，但这种方法可能会导致无效交易留在账本中，增加存储成本并促使新形式的战略行为出现。因此，需要对这种设计及其潜在风险有更深入的理解。

Method: 本文提出了一个正式框架来分析共识与执行之间的相互作用，并通过该框架揭示了分离开设计所允许的一种先前未被识别的攻击类型——gaslighting。接着，作者探讨了在分离模型下，抗此攻击能力与资源利用效率之间的根本性权衡关系。最后，提出并讨论了一种适用于基于领导者协议的中间模型，旨在同时保证对抗gaslighting攻击的能力以及保持系统的高吞吐量和低延迟特性。

Result: 研究表明，在完全分离的模型中，无法同时确定性地达到抵御gaslighting攻击和优化资源使用率的目标。提出的基于领导者的中间方案能够在提供足够安全保护的同时，也支持较高的交易处理速度和较低的响应时间。

Conclusion: 本文不仅揭示了当前一些高效区块链设计方案中存在的安全隐患，还提供了理论上的见解及实际可行的解决方案，对于促进更加安全且高效的区块链系统设计具有重要意义。

Abstract: Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.

</details>


### [4] [Circumventing the FLP Impossibility Result with Open Atomic Ethernet](https://arxiv.org/abs/2602.20444)
*Paul Borrill*

Main category: cs.DC

TL;DR: 本文介绍了一种名为开放原子以太网（OAE）的技术，它通过拒绝FLP不可能性结果所基于的异步模型，实现了确定性的原子协调。OAE的关键特性被描述为双同步，即在每个回合边界上双方都能达成共识的结果。


<details>
  <summary>Details</summary>
Motivation: 长期以来，分布式计算领域一直将Fischer-Lynch-Paterson (FLP) 不可能性结果视为设计中必须绕过的障碍，因为它表明在有哪怕只有一个故障进程的异步系统中，没有确定性协议能够保证达成一致。本文旨在挑战这一观点，提出一种新的系统模型来避开FLP的限制。

Method: 文章介绍了开放原子以太网（OEA），这是一种具备双同步特性的技术方案，意味着在每次交互结束时所有参与方都能够就结果形成共同知识。这种特性比单纯的同步提供了更强有力的保障。通过在第二层构建一个基于交换的双同步协议，OEA避开了FLP异步模型中的核心假设。

Result: 研究表明，通过采用双同步通信模式，OAE能够在不违反任何已知不可能性定理的情况下实现确定性的原子级协调。

Conclusion: 该文结论是，尽管FLP不可能性结果长期被视为分布式计算领域的基础约束之一，但通过引入像OAE这样具有创新性质的技术，并采用不同于传统异步模型的新系统模型，可以实现绕过这些限制的目的。

Abstract: The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result.

</details>


### [5] [Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning](https://arxiv.org/abs/2602.20450)
*Nihal Balivada,Shrey Gupta,Shashank Shreedhar Bhatt,Suyash Gupta*

Main category: cs.DC

TL;DR: 提出了一种名为Terraform的新客户端选择方法，通过使用梯度更新和确定性选择算法来选择异构客户端进行再训练，以提高联邦学习的全局模型准确性，比先前的工作提高了高达47%的准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）允许一种分布式的客户端-服务器架构，多个客户端可以协作训练一个全局机器学习模型而无需共享敏感的本地数据。然而，由于客户端之间的统计异质性，FL通常会导致比传统机器学习算法更低的准确性。

Method: Terraform，这是一种新颖的客户端选择方法，它利用梯度更新和一个确定性的选择算法来挑选出能够提升全局模型准确性的异构客户端来进行再训练。

Result: Terraform相较于之前的方法能够达到高达47%的额外准确性，并且通过全面的消融研究和训练时间分析进一步证明了其效率。

Conclusion: Terraform提供了一个有效的解决方案来解决联邦学习中的准确性下降问题，通过改进客户端选择过程，显著提升了模型性能。

Abstract: Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.

</details>


### [6] [A Granularity Characterization of Task Scheduling Effectiveness](https://arxiv.org/abs/2602.20561)
*Sana Taghipour Anvar,David Kaeli*

Main category: cs.DC

TL;DR: 该论文提出了一种基于任务图依赖拓扑的粒度特征框架，用以预测调度开销随并行度增加的增长情况。通过实验评估，证明了所提出的特征方法能够解释实践中观察到的各种强扩展性崩溃现象，并且能够准确预测强扩展性的极限，从而为运行时决定动态或静态执行提供了实用规则。


<details>
  <summary>Details</summary>
Motivation: 任务式运行时系统虽然为并行科学应用提供了灵活的负载均衡和可移植性，但其强扩展性对任务粒度非常敏感。随着并行度的提高，调度开销可能会从可以忽略不计转变为占据主导地位，导致某些算法性能急剧下降，而其他算法则影响不大。尽管这种效应在经验上被广泛观察到，但对于算法结构如何影响动态调度是否总是有益的理解却很缺乏。

Method: 本文引入了一个粒度特征框架，直接将调度开销的增长与任务图依赖拓扑联系起来。通过这一观察，研究者使用一个简单的粒度衡量标准来表征执行行为，指示何时可以通过并行计算摊销调度开销以及何时调度开销会成为性能瓶颈。

Result: 通过对具有不同依赖模式的代表性并行工作负载进行实验评估，展示了所提特征方法能够解释实际中观察到的渐进式和突然式的强扩展性失效现象。此外，还表明从依赖拓扑导出的开销模型能准确预测强扩展性的限制，并支持一种实用的运行时决策规则，用于选择动态或静态执行方式，而无需进行详尽的强扩展性研究或大量离线调整。

Conclusion: 这项工作提供了一种新的方法来理解并行应用程序中的调度开销问题，特别是它如何受到任务图结构的影响。提出的简单粒度测量方法不仅有助于预测特定算法的强扩展性限制，也为开发更有效的调度策略提供了指导方针。

Abstract: Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.

</details>


### [7] [Lagom: Unleashing the Power of Communication and Computation Overlapping for Distributed LLM Training](https://arxiv.org/abs/2602.20656)
*Guanbin Xu,ZhenGuo Xu,Yuzhe Li,Youhui Bai,Ping Gong,Chaoyi Ruan,Cheng Li*

Main category: cs.DC

TL;DR: Lagom, a system that optimizes overlapping communication with computation for distributed large-model training, achieves up to 1.33x speedup over existing solutions by co-tuning parameters and employing a unified cost model alongside a priority-based search algorithm.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of efficiently overlapping communication with computation in distributed large-model training, especially when computation acts as the bottleneck, through the development of a system capable of optimizing resource usage between the two processes.

Method: The method introduced in this paper involves the creation of Lagom, a system that uses a unified cost model and a priority-based search algorithm to co-tune communication parameters, thereby balancing the resource usage between computation and communication and reducing optimization complexity from exponential to linear.

Result: Results show that on both high- and low-bandwidth GPU clusters, Lagom achieves a speedup of 1.07-1.33x and 1.03-1.27x over NCCL and AutoCCL across various models and parallelization methods.

Conclusion: In conclusion, Lagom effectively enhances the efficiency of distributed large-model training by optimizing the overlap of communication and computation, leading to significant speedups compared to other state-of-the-art communication libraries.

Abstract: Overlapping communication with computation is crucial for distributed large-model training, yet optimizing it - especially when computation becomes the bottleneck-remains challenging. We present Lagom, a system that co-tunes communication parameters to balance resource usage between computation and communication. By introducing a unified cost model and a priority-based search algorithm, Lagom reduces optimization complexity from exponential to linear. Evaluations on high- and low-bandwidth GPU clusters show that Lagom achieves 1.07-1.33x and 1.03-1.27x speedup over NCCL and AutoCCL across diverse models and parallelizations.

</details>


### [8] [A Morton-Type Space-Filling Curve for Pyramid Subdivision and Hybrid Adaptive Mesh Refinement](https://arxiv.org/abs/2602.20887)
*David Knapp,Johannes Albrecht Holke,Thomas Spenke,Carsten Burstedde*

Main category: cs.DC

TL;DR: 本文介绍了一种新的功能元素类型——金字塔，用于连接四面体和六面体元素而无需悬空边，从而在三维混合元素网格中充分利用基于树的自适应网格细化（AMR）方法。通过定义空间填充曲线并解决与金字塔细化相关的独特挑战，文章提出支持新元素所需的功能设计，并将基本的全局并行算法推广到完全支持这种新元素，证实了该完整混合元素动态AMR框架的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了在三维混合元素网格中充分发挥基于树的自适应网格细化（AMR）的潜力，需要一种能够有效连接四面体和六面体元素而不产生悬空边的新元素类型。

Method: 引入金字塔作为新的功能元素类型；为金字塔定义一个明确的空间填充曲线；解决与金字塔细化相关的元素级别和森林级别的独特挑战；提出必要的功能设计来支持新元素；将基本的全局并行算法（包括细化、粗化、分区以及面鬼交换）推广至全面支持新元素。

Result: 成功地在包含金字塔元素的三维混合元素网格上实现了动态自适应网格细化；验证了该框架的效率和可扩展性。

Conclusion: 通过引入金字塔作为新的元素类型并相应调整算法，使得基于树的自适应网格细化技术能够在更广泛的三维混合元素网格场景下应用，展示了良好的性能表现。

Abstract: The forest-of-refinement-trees approach allows for dynamic adaptive mesh refinement (AMR) at negligible cost. While originally developed for quadrilateral and hexahedral elements, previous work established the theory and algorithms for unstructured meshes of simplicial and prismatic elements. To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes, this paper introduces the pyramid as a new functional element type; its primary purpose is to connect tetrahedral and hexahedral elements without hanging edges.We present a well-defined space-filling curve (SFC) for the pyramid and detail how the unique challenges on the element and forest level associated with the pyramidal refinement are resolved. We propose the necessary functional design and generalize the fundamental global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange to fully support this new element. Our demonstrations confirm the efficiency and scalability of this complete, hybrid-element dynamic AMR framework.

</details>


### [9] [Is a LOCAL algorithm computable?](https://arxiv.org/abs/2602.21022)
*Antonio Cruciani,Avinandan Das,Massimo Equi,Henrik Lievonen,Diep Luong-Le,Augusto Modanese,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文探讨了在LOCAL模型中节点更新状态时使用可计算函数与任意函数的区别，并展示了这种区别对于局部检查标签问题（LCLs）的重要性。此外，文章还揭示了解图大小n的信息如何影响这些模型下的问题复杂性。


<details>
  <summary>Details</summary>
Motivation: 作者想要澄清在“标准”LOCAL模型定义中的模糊之处，即节点是通过任意函数还是可计算函数来更新其状态的。同时，他们也想展示这个问题即使是在广泛研究的局部检查标签问题（LCLs）中也是重要的。

Method: 通过构造一个特定的LCL问题Π，作者们展示了当LOCAL模型为不可计算时、当我们知道图大小n的一个上界时以及当我们对n一无所知时解决该问题所需的不同轮数。进一步地，他们证明了对于任何LCL问题Π而言，在拥有n的界限的情况下，无论模型是否可计算，问题的轮复杂度都是相同的。

Result: 研究表明，对于所构造的LCL问题Π：1. 如果LOCAL模型不可计算，则可以在O(log n)轮内解决Π；2. 若模型可计算且我们掌握n的任一上限值，同样能在O(log n)轮内解决问题；3. 但若处于可计算模型下而我们对n毫无所知，则需要Ω(√n)轮才能解决。

Conclusion: 本研究强调了在讨论LOCAL模型时明确指定关于节点状态更新规则及对图规模n的先验知识的重要性。它表明即使是看似微不足道的假设差异也可能导致显著不同的算法效率结果。

Abstract: Common definitions of the "standard" LOCAL model tend to be sloppy and even self-contradictory on one point: do the nodes update their state using an arbitrary function or a computable function? So far, this distinction has been safe to neglect, since problems where it matters seem contrived and quite different from e.g. typical local graph problems studied in this context.
  We show that this question matters even for locally checkable labeling problems (LCLs), perhaps the most widely studied family of problems in the context of the LOCAL model. Furthermore, we show that assumptions about computability are directly connected to another aspect already recognized as highly relevant: whether we have any knowledge of $n$, the size of the graph. Concretely, we show that there is an LCL problem $Π$ with the following properties:
  1. $Π$ can be solved in $O(\log n)$ rounds if the \textsf{LOCAL} model is uncomputable.
  2. $Π$ can be solved in $O(\log n)$ rounds in the computable model if we know any upper bound on $n$.
  3. $Π$ requires $Ω(\sqrt{n})$ rounds in the computable model if we do not know anything about $n$.
  We also show that the connection between computability and knowledge of $n$ holds in general: for any LCL problem $Π$, if you have any bound on $n$, then $Π$ has the same round complexity in the computable and uncomputable models.

</details>


### [10] [ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments](https://arxiv.org/abs/2602.21140)
*Haley Li,Xinglu Wang,Cong Feng,Chunxu Zuo,Yanan Wang,Hei Lo,Yufei Cui,Bingji Wang,Duo Cui,Shuming Jing,Yizhou Shan,Ying Xiong,Jiannan Wang,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ReviveMoE is a method that allows for quick recovery from failures in large-scale LLM deployments, without the need to restart the service. It supports both traditional and disaggregated LLM architectures and is part of Huawei Cloud's MaaS, using xDeepServe and XCCL.


<details>
  <summary>Details</summary>
Motivation: 随着LLM部署在更多硬件上扩展，系统中发生单个故障的概率显著增加，云运营商必须考虑强大的对策来处理这些不可避免的故障。简单地重启LLM服务实例虽然是一种常见的恢复方法，但在模型即服务（MaaS）推理环境中成本很高，因为重新加载模型权重和重新编译计算图会为传入请求引入显著延迟。

Method: 提出了一种名为ReviveMoE的方法，用于在大规模LLM部署中快速从故障中恢复，而无需重启服务实例。ReviveMoE旨在支持传统的将MoE与注意力机制置于同一硬件上的LLM架构，以及分离MoE与注意力机制的分布式架构。该方法集成于华为云的MaaS之中，基于华为的xDeepServe服务平台和XCCL通信库构建。

Result: 论文没有提供具体的结果数据，但可以推测ReviveMoE通过减少因故障导致的服务中断时间提高了系统的可用性和效率。

Conclusion: ReviveMoE提供了一种有效的方法来提高大型语言模型部署中的容错能力，同时保持高性能服务水平。

Abstract: As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs](https://arxiv.org/abs/2602.20191)
*Dongwei Wang,Jinhee Kim,Seokho Han,Denis Gudovskiy,Yohei Nakata,Tomoyuki Okuno,KhayTze Peong,Kang Eun Jeon,Jong Hwan Ko,Yiran Chen,Huanrui Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoBiQuant的新颖混合位量化框架，该框架能够基于token敏感度调整权重精度以实现弹性LLM推理。通过迭代重建更高精度的权重和动态选择残差位片的数量，MoBiQuant能够在无需重复校准的情况下平滑地切换精度，并提高对于token异常值分布的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在云和边缘设备上改变运行时复杂性需要弹性大规模语言模型（LLM）部署，其中LLM可以根据可用计算资源使用不同的量化精度进行推断。然而，观察到量化校准参数通常与特定精度相关联，在弹性精度校准和运行时精度切换过程中带来了挑战。本文将不同校准参数的根源归因于由于精度依赖的异常迁移现象引起的token级别敏感度变化。

Method: 提出了MoBiQuant，一种新颖的混合位量化框架，它能够根据token敏感度调整权重精度以支持弹性LLM推理。特别地，引入了多合一递归残差量化方法，可以迭代地重建更高精度的权重；以及一个token感知路由器来动态选择残差位片数量。

Result: 实验结果显示，MoBiQuant表现出强大的弹性，可以在不需重复校准的情况下匹配针对LLaMA3-8B的比特特定校准PTQ的表现。

Conclusion: MoBiQuant提供了一种有效的方法来解决弹性精度校准和运行时精度切换的问题，通过改善对token异常值分布的泛化能力，实现了更加灵活且高效的LLM部署。

Abstract: Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.

</details>


### [12] [FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment](https://arxiv.org/abs/2602.20194)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出了一种联合框架，用于估计桥梁劣化的连续时间马尔可夫链（CTMC）风险模型，使得各组织能够在不转移原始检查记录的情况下协作训练共享基准模型。通过在合成数据上进行实验，结果表明该方法能够有效地收敛，并为参与者提供了一种自然的参与激励机制。


<details>
  <summary>Details</summary>
Motivation: 由于桥梁定期检查记录包含关于公共基础设施的敏感信息，在现有数据治理约束下，跨组织的数据共享变得不切实际。因此，研究者们希望开发一种方法，能够让不同的市政当局共同训练一个关于桥梁劣化过程的风险模型，而无需分享各自的原始数据。

Method: 研究者提出了一个基于联邦学习的框架来估计桥梁劣化的连续时间马尔可夫链(CTMC)风险模型。每个用户持有本地检查数据，并对三个劣化方向转变（好到轻微、好到严重、轻微到严重）训练一个log-linear风险模型，其中考虑了桥梁年龄、与海岸线距离及桥面面积作为协变量。本地优化是通过对CTMC log-likelihood使用小批量随机梯度下降完成的，每轮通信只上传一个12维伪梯度向量至中央服务器。服务器采用带动量和梯度裁剪的样本加权Federated Averaging (FedAvg)聚合用户更新。

Result: 所有实验均基于完全合成数据集进行，这些数据集具有特定区域异质性，允许对联邦收敛行为进行受控评估。模拟结果显示，在异构用户间，平均负对数似然一致收敛，且随着用户规模增加，聚合梯度范数减少。此外，联邦更新机制为用户提供了一种天然的参与激励：将本地检测数据集注册到共享技术标准平台上的用户可以定期获得更新后的全局基准参数——这是仅凭本地数据无法获得的信息——从而实现基于证据的生命期规划而不放弃数据主权。

Conclusion: 本研究表明，所提出的联邦学习框架能够有效支持跨组织之间的桥梁劣化建模合作，同时保护各自的数据隐私。它不仅促进了知识和技术的共享，还为参与者提供了明确的好处，即通过参与可以获得对于本地数据不可得的全球视角下的重要信息。

Abstract: Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\to$Minor, Good$\to$Severe, and Minor$\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.

</details>


### [13] [CaDrift: A Time-dependent Causal Generator of Drifting Data Streams](https://arxiv.org/abs/2602.20329)
*Eduardo V. L. Barboza,Jean Paul Barddal,Robert Sabourin,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 介绍了Causal Drift Generator (CaDrift)，一种基于结构因果模型的时间依赖型合成数据生成框架，用于产生具有可控变化事件和时间依赖性的数据流，以评估在不断演变的数据下的方法。实验结果表明该框架能够有效模拟分布变化。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够生成带有控制变化事件和时间依赖性数据的数据流工具，用来评估在数据演化过程中不同方法的表现。

Method: 通过基于结构因果模型（SCMs）构建的Causal Drift Generator (CaDrift)框架来实现，该框架能够通过改变SCM中的映射函数来合成各种分布性和协变量位移，并利用因果建模中的干预措施来模拟偶尔发生的扰动。

Result: 实验结果显示，在发生分布位移事件后，分类器的准确性往往会下降，随后逐渐恢复，这证实了生成器在模拟位移方面的有效性。

Conclusion: CaDrift为研究者提供了一个强大的工具，可以用来测试和验证在面对持续变化的数据时算法或模型的鲁棒性和适应能力。此外，此框架已在GitHub上公开可用。

Abstract: This work presents Causal Drift Generator (CaDrift), a time-dependent synthetic data generator framework based on Structural Causal Models (SCMs). The framework produces a virtually infinite combination of data streams with controlled shift events and time-dependent data, making it a tool to evaluate methods under evolving data. CaDrift synthesizes various distributional and covariate shifts by drifting mapping functions of the SCM, which change underlying cause-and-effect relationships between features and the target. In addition, CaDrift models occasional perturbations by leveraging interventions in causal modeling. Experimental results show that, after distributional shift events, the accuracy of classifiers tends to drop, followed by a gradual retrieval, confirming the generator's effectiveness in simulating shifts. The framework has been made available on GitHub.

</details>


### [14] [Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning](https://arxiv.org/abs/2602.20197)
*Zhuoxu Huang,Mengxi Jia,Hao Sun,Xuelong Li,Jungong Han*

Main category: cs.LG

TL;DR: 本文提出了一种名为CalibRL的混合策略RLVR框架，通过可控探索和专家指导来解决多模态大语言模型在强化学习训练过程中遇到的熵崩溃、策略退化或次优行为过度利用等问题。实验结果表明该方法有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在使用强化学习进行训练时面临巨大的状态空间与稀疏奖励挑战，导致熵崩溃、策略质量下降或对次优行为的过度开发。需要一种既能保持有益随机性又避免无控制随机采样缺陷的有效探索策略。

Method: 提出了CalibRL，一个支持基于专家指导下的可控探索的混合策略RLVR框架。该框架通过两个关键机制实现：1) 依据群体稀有度调整优势权重以校准分布并保持探索；2) 利用不对称激活函数(LeakyReLU)，将专家知识作为校准基线，在保持修正方向的同时调节过于自信的更新。此外，CalibRL还通过在线采样估计策略内分布，引导信息丰富的行为更新，防止错误模式收敛，并缓解了模型策略与专家轨迹之间的分布不匹配问题。

Result: 在八个基准测试中进行了广泛的实验验证，包括同域和跨域设置，结果一致显示了改进效果，证明了所提出的可控混合策略RLVR训练的有效性。

Conclusion: CalibRL提供了一种有效的方法来改善多模态大语言模型的强化学习过程中的探索-利用平衡问题，通过引入专家指导和特定机制提高了最终学到策略的质量和稳定性。

Abstract: Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.

</details>


### [15] [IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning](https://arxiv.org/abs/2602.20199)
*Soufiane Bacha,Laouni Djafri,Sahraoui Dhelim,Huansheng Ning*

Main category: cs.LG

TL;DR: 本文提出了一种名为IMOVNO+的两层框架，旨在同时提高二分类和多分类任务的数据质量和算法鲁棒性。该框架在数据层面通过条件概率量化样本信息量、划分数据集区域、采用结合Z分数与大跳跃间隙距离的重叠清理算法以及基于多重正则化的智能过采样算法来防止新重叠；在算法层面，通过元启发式方法修剪集成分类器以减少弱学习器的影响。IMOVNO+在35个数据集上的表现优于现有技术，特别是在处理多类数据时，在G-mean、F1-score等指标上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前对于类别不平衡、重叠及噪声问题的研究主要集中在二分类场景下，而在多分类设置中这些问题由于复杂的类间关系而未得到充分探索。传统的方法要么仅依赖几何距离可能移除有用样本或生成低质量合成数据，要么将不平衡局部化处理忽略了全局类间依赖性。此外，集成学习方法在整合弱分类器方面遇到困难，导致鲁棒性有限。

Method: IMOVNO+框架分为两个层次：数据层和算法层。在数据层面上，首先利用条件概率评估每个样本的信息量，接着将数据集划分为核心、重叠及噪声区域，并引入结合Z分数与大跳跃间隙距离的重叠清理算法。随后，实施一种基于多正则化的智能过采样策略，以控制合成样本之间的接近度，避免产生新的重叠现象。算法层面上，则是通过元启发式手段对集成分类器进行剪枝，降低弱学习者的影响。

Result: IMOVNO+在35个数据集（包括13个多分类和22个二分类）上进行了测试，结果表明其性能明显优于现有方法，在多个情况下接近100%准确率。对于多分类任务，IMOVNO+在G-mean、F1-score、精确率和召回率等方面分别实现了37-57%、25-44%、25-39% 和 26-43% 的提升。而在二分类任务中，也达到了近乎完美的效果，改进幅度为14-39%。

Conclusion: IMOVNO+框架能够有效应对类别不平衡、重叠及噪声问题，不仅提升了数据的质量还增强了算法的鲁棒性。它在处理多分类数据时特别有效，极大地提高了模型的泛化能力。

Abstract: Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits.

</details>


### [16] [Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling](https://arxiv.org/abs/2602.20210)
*Kiyoung Seong,Sungsoo Ahn,Sehui Han,Changyoung Park*

Main category: cs.LG

TL;DR: 提出了一种统一的多模态流模型Multimodal Crystal Flow (MCFlow)，通过独立的时间变量实现多种晶体生成任务，包括晶体结构预测和从头生成。该模型引入了成分和对称性感知的原子排序与层次置换增强，无需显式的结构模板即可注入强组合和晶体学先验。实验表明MCFlow在多个晶体生成任务中相对于特定任务基线表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度生成模型虽然在晶体建模任务上展示了良好的性能，但它们大多针对特定任务设计，缺乏一个可以跨不同生成任务共享晶体表示的统一框架。

Method: 提出了Multimodal Crystal Flow (MCFlow)模型，这是一种统一的多模态流模型，能够通过为原子类型和晶体结构设定独立的时间变量来作为不同的推理轨迹实现多种晶体生成任务。为了支持标准变压器模型中的多模态流动，研究者还引入了一种成分-及对称性意识的原子排序方法，并结合层次化排列增强技术，以不依赖于明确结构模板的方式注入强烈的组合性和晶体学先验知识。

Result: 在MP-20和MPTS-52基准测试上的实验结果显示，MCFlow相较于针对特定任务的基线方法，在多个晶体生成任务中均取得了具有竞争力的表现。

Conclusion: 本研究提出的MCFlow提供了一个有效且统一的方法来处理各种晶体生成任务，通过创新地整合成分-及对称性意识的原子排序与层次化排列增强技术，使得模型能够在没有明确结构模板的情况下依然能够很好地捕捉到晶体材料的关键特征。

Abstract: Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks.

</details>


### [17] [Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking](https://arxiv.org/abs/2602.21196)
*Ravi Ghadia,Maksim Abraham,Sergei Vorobyov,Max Ryabinin*

Main category: cs.LG

TL;DR: 本文介绍了一种名为UPipe的上下文并行技术，该技术通过在注意力头级别进行细粒度切分来显著减少自注意力的激活内存使用，从而打破激活内存限制，支持更长的上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有的处理长序列的方法虽然能够扩展上下文维度，但没有关注内存效率，这限制了它们可以支持的序列长度。而一些更先进的技术虽然能进一步延长可支持的上下文长度，但却牺牲了训练吞吐量。因此，需要一种既能提高内存效率又能保持训练速度的新方法。

Method: 提出了一种名为UPipe的简单却有效的上下文并行技术，它通过对注意力头级别的细粒度切分来显著降低自注意力层中中间张量的内存占用。

Result: 实验结果显示，对于32B大小的Transformer模型，UPipe可以将注意力层中的中间张量内存使用减少了高达87.5%。同时，在训练速度上与之前的上下文并行技术相当。此外，当在单个8×H100节点上训练Llama3-8B时，UPipe支持达到5M tokens的上下文长度，比之前的方法提高了超过25%。

Conclusion: UPipe作为一种新颖的上下文并行策略，不仅有效解决了现有方法中存在的内存效率问题，还能够在保持甚至提升训练性能的同时支持更长的上下文长度。

Abstract: Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\times$H100 node, improving upon prior methods by over 25$\%$.

</details>


### [18] [Coupled Cluster con MōLe: Molecular Orbital Learning for Neural Wavefunctions](https://arxiv.org/abs/2602.20232)
*Luca Thiede,Abdulrahman Aldossary,Andreas Burger,Jorge Arturo Campos-Gonzalez-Angulo,Ning Wang,Alexander Zook,Melisa Alkan,Kouhei Nakaji,Taylor Lee Patti,Jérôme Florian Gonthier,Mohammad Ghazi Vakili,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 本文提出了一种名为MōLe的等变机器学习模型，该模型能够直接从哈特里-福克分子轨道预测耦合簇(CC)理论的核心数学对象——激发振幅。尽管仅在小平衡几何结构上进行训练，MōLe展示了出色的数据效率和对较大分子及非平衡几何结构的良好泛化能力，并且能够减少CC计算收敛所需的循环次数，为基于波函数的高精度机器学习架构奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 密度泛函理论（DFT）虽然广泛用于计算分子性质，但其准确性往往不足以支持定量预测。另一方面，耦合簇（CC）理论虽然能提供与实验结果更接近的预测，但由于其高昂的计算成本限制了广泛应用。因此，研究者们旨在开发一种新的方法来提高计算效率的同时保持较高的预测精度。

Method: 提出了Molecular Orbital Learning (MōLe)架构，这是一种等变机器学习模型，能够直接根据输入的哈特里-福克分子轨道预测CC理论中的核心数学对象——激发振幅。

Result: MōLe显示出了极高的数据效率以及对更大分子和非平衡几何构型出色的泛化性能；此外，它还能有效减少达到CC计算收敛所需的迭代周期数。

Conclusion: 通过引入MōLe架构，本研究为实现基于波函数的高效准确的机器学习方法开辟了新途径，这将有助于加速分子设计过程并补充力场方法。

Abstract: Density functional theory (DFT) is the most widely used method for calculating molecular properties; however, its accuracy is often insufficient for quantitative predictions. Coupled-cluster (CC) theory is the most successful method for achieving accuracy beyond DFT and for predicting properties that closely align with experiment. It is known as the ''gold standard'' of quantum chemistry. Unfortunately, the high computational cost of CC limits its widespread applicability. In this work, we present the Molecular Orbital Learning (MōLe) architecture, an equivariant machine learning model that directly predicts CC's core mathematical objects, the excitation amplitudes, from the mean-field Hartree-Fock molecular orbitals as inputs. We test various aspects of our model and demonstrate its remarkable data efficiency and out-of-distribution generalization to larger molecules and off-equilibrium geometries, despite being trained only on small equilibrium geometries. Finally, we also examine its ability to reduce the number of cycles required to converge CC calculations. MōLe can set the foundations for high-accuracy wavefunction-based ML architectures to accelerate molecular design and complement force-field approaches.

</details>


### [19] [The Truthfulness Spectrum Hypothesis](https://arxiv.org/abs/2602.20273)
*Zhuofan Josh Ying,Shauli Ravfogel,Nikolaus Kriegeskorte,Peter Hase*

Main category: cs.LG

TL;DR: 本文通过系统评估五种真实性类型（定义性、经验性、逻辑性、虚构性和伦理性）以及奉承和逆预期撒谎，验证了真实性谱系假设。线性探针在大多数领域表现良好，但在奉承和逆预期撒谎方面失败。然而，当所有领域联合训练时，性能得到恢复。研究结果表明，在表示空间中同时存在不同广泛程度的真实性方向，并且后训练会重塑这些方向的几何结构。


<details>
  <summary>Details</summary>
Motivation: 先前的研究指出大型语言模型能够线性编码真实性，但最近的工作质疑这一发现的普遍性。为了解决这两种观点之间的矛盾，作者提出了真实性谱系假设：表示空间包含从广泛领域通用到狭窄领域特定的方向。这项研究旨在测试该假设的有效性。

Method: 研究人员对五种真实性类型（定义性、经验性、逻辑性、虚构性和伦理性）、奉承式谎言及逆向预期谎言进行了系统评估，并考察了现有诚实度基准。此外，他们还使用概念擦除方法来隔离不同类型的真实性方向，并通过因果干预揭示了领域特定方向比领域通用方向更有效。

Result: 线性探针在大多数领域内的泛化表现良好，但在处理奉承式谎言和逆向预期谎言时表现不佳。然而，当将所有领域一起训练时，可以恢复较强的性能。进一步分析显示，探针方向间的马哈拉诺比斯余弦相似度几乎完美地预测了跨域泛化能力。

Conclusion: 研究支持了真实性谱系假设：表示空间内共存着不同程度的一般性真实性方向，而且后训练过程会改变这些方向的几何形状。

Abstract: Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.

</details>


### [20] [Discrete Diffusion with Sample-Efficient Estimators for Conditionals](https://arxiv.org/abs/2602.20293)
*Karthik Elamvazhuthi,Abhijith Jayakumar,Andrey Y. Lokhov*

Main category: cs.LG

TL;DR: 本文提出了一种离散去噪扩散框架，该框架将单点条件概率的高效估计器与循环噪声和去噪动态相结合，用于离散状态空间上的生成建模。通过使用NeurISE方法估计这些条件概率，并在多个数据集上进行了实验验证，表明该方法在总变差、交叉相关性和核密度估计度量方面优于现有流行方法。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在开发一种新的离散去噪扩散框架，以提高在处理离散状态空间时生成模型的表现。此框架特别关注如何更有效地估计单点条件概率，从而改进反向扩散过程的参数化方式。

Method: 提出的方法利用了称为神经交互筛选估计器（NeurISE）的一种样本高效技术来估计扩散动力学中的单点条件概率。整个框架采用了循环方式的加噪与去噪策略。

Result: 通过对合成Ising模型、MNIST数据集以及由D-Wave量子退火机产生的科学数据集等进行控制实验，结果显示所提方法在二进制数据集上表现优于现有的基于比率的方法，在总变差、交叉相关性及核密度估计指标上均有提升。

Conclusion: 研究表明，提出的离散去噪扩散框架能够有效改善离散状态空间中生成模型的质量，特别是在处理二进制数据时，相比其他现有方法具有显著优势。

Abstract: We study a discrete denoising diffusion framework that integrates a sample-efficient estimator of single-site conditionals with round-robin noising and denoising dynamics for generative modeling over discrete state spaces. Rather than approximating a discrete analog of a score function, our formulation treats single-site conditional probabilities as the fundamental objects that parameterize the reverse diffusion process. We employ a sample-efficient method known as Neural Interaction Screening Estimator (NeurISE) to estimate these conditionals in the diffusion dynamics. Controlled experiments on synthetic Ising models, MNIST, and scientific data sets produced by a D-Wave quantum annealer, synthetic Potts model and one-dimensional quantum systems demonstrate the proposed approach. On the binary data sets, these experiments demonstrate that the proposed approach outperforms popular existing methods including ratio-based approaches, achieving improved performance in total variation, cross-correlations, and kernel density estimation metrics.

</details>


### [21] [Learning to Solve Complex Problems via Dataset Decomposition](https://arxiv.org/abs/2602.20296)
*Wanru Zhao,Lucas Caccia,Zhengyan Shi,Minseon Kim,Weijia Xu,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 本文提出了一种反向课程生成方法，通过递归地将复杂数据集分解为更简单、更易学习的组件。实验表明，使用这种方法训练的模型在数学和代码生成数据集上表现出优于标准训练的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的课程学习策略是按照从简单到复杂的顺序组织数据来训练模型。而本研究探索了一种新的反向课程生成方法，旨在通过递归地分解复杂数据集为更简单的部分，以帮助学生模型逐步掌握困难任务。

Method: 作者们提出了一种教师-学生框架，在该框架中，教师能够进行逐步推理，并利用此能力递归地生成较容易版本的例子。此外，他们还设计了一个新颖的评分系统，用于根据数据的结构复杂性和概念深度衡量其难度，从而实现对分解后数据的课程构建。

Result: 实验结果表明，在MATH和AIME等数学数据集以及代码生成数据集上，采用所提出方法生成的课程训练得到的模型比直接在原始数据集上进行标准训练的模型表现更好。

Conclusion: 这项研究表明，通过反向课程生成方法——即先将复杂问题分解成更小、更易于处理的部分再进行训练——可以有效提高模型解决难题的能力。

Abstract: Curriculum learning is a class of training strategies that organizes the data being exposed to a model by difficulty, gradually from simpler to more complex examples. This research explores a reverse curriculum generation approach that recursively decomposes complex datasets into simpler, more learnable components. We propose a teacher-student framework where the teacher is equipped with the ability to reason step-by-step, which is used to recursively generate easier versions of examples, enabling the student model to progressively master difficult tasks. We propose a novel scoring system to measure data difficulty based on its structural complexity and conceptual depth, allowing curriculum construction over decomposed data. Experiments on math datasets (MATH and AIME) and code generation datasets demonstrate that models trained with curricula generated by our approach exhibit superior performance compared to standard training on original datasets.

</details>


### [22] [In-context Pre-trained Time-Series Foundation Models adapt to Unseen Tasks](https://arxiv.org/abs/2602.20307)
*Shangqing Xu,Harshavardhan Kamarthi,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架In-Context Time-series Pre-training (ICTP)，通过为时间序列基础模型（TSFMs）添加上下文学习能力，使其能够在不进行微调的情况下适应未见过的任务。实验表明，这种方法在未见任务上提高了约11.4%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列基础模型虽然在特定任务上表现出色，但在面对未见过的任务时往往需要经过微调才能达到较好的表现。为了克服这一局限性，研究者们旨在开发一种能够使这些模型具备直接适应新任务的能力而无需额外训练的方法。

Method: 提出了一种名为In-Context Time-series Pre-training (ICTP)的新框架，该方法通过对原始预训练数据进行重构来赋予基础TSFM以In-Context Learning (ICL)能力，从而让模型可以根据给定上下文中的输入输出关系动态调整自身行为。

Result: 实验结果显示，在处理之前未见过的任务时，通过采用ICTP框架增强的时间序列基础模型相比现有最先进水平提高了大约11.4%的表现力，且这种改进是在没有对模型做任何进一步微调的前提下实现的。

Conclusion: 通过引入In-Context Learning到时间序列基础模型中，本研究表明可以显著提高模型对于未知任务的泛化能力，展示了In-Context Time-series Pre-training作为一种有效提升TSFMs灵活性与适用范围手段的巨大潜力。

Abstract: Time-series foundation models (TSFMs) have demonstrated strong generalization capabilities across diverse datasets and tasks. However, existing foundation models are typically pre-trained to enhance performance on specific tasks and often struggle to generalize to unseen tasks without fine-tuning. To address this limitation, we propose augmenting TSFMs with In-Context Learning (ICL) capabilities, enabling them to perform test-time inference by dynamically adapting to input-output relationships provided within the context. Our framework, In-Context Time-series Pre-training (ICTP), restructures the original pre-training data to equip the backbone TSFM with ICL capabilities, enabling adaptation to unseen tasks. Experiments demonstrate that ICT improves the performance of state-of-the-art TSFMs by approximately 11.4% on unseen tasks without requiring fine-tuning.

</details>


### [23] [QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.20309)
*Jingxuan Zhang,Yunta Hsieh,Zhongwei Wang,Haokun Lin,Xin Wang,Ziqi Wang,Yingtie Lei,Mi Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的后训练量化框架QuantVLA，专门针对视觉-语言-动作模型（VLA）设计，能够有效减少内存消耗并提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作(VLA)模型在实际部署中面临计算和内存需求快速增长的问题，特别是在处理更长的时间范围和更大的基础模型时。为了解决这些瓶颈问题，研究者们开发了QuantVLA框架。

Method: QuantVLA包含三个经过校准的关键部分：(1)选择性量化布局，将语言基础模型及扩散变换器(DiT)中的所有线性层整数化，同时保持注意力投影处于浮点状态以维持原始操作计划；(2)注意力温度匹配，一种轻量级的每头缩放机制，在推理时稳定注意力logits并通过反量化比例整合；(3)输出头平衡，一种逐层残差接口校准方法，用以缓解后期投影能量漂移。该框架无需额外训练，仅使用少量未标记的校准缓冲区，并支持低位权重与激活的整数内核，同时保持架构不变。

Result: 在LIBERO上的代表性VLA模型测试表明，QuantVLA不仅超越了全精度基线的任务成功率，还实现了约70%的相对内存节省以及1.22倍的端到端推理延迟加速，为低比特实体智能提供了一条实用路径。

Conclusion: QuantVLA提供了一种有效的解决方案来减轻VLA系统中的计算、内存和功耗限制，通过引入特定于VLA系统的量化技术，成功地提高了效率而不牺牲性能。

Abstract: Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.

</details>


### [24] [Emergent Manifold Separability during Reasoning in Large Language Models](https://arxiv.org/abs/2602.20338)
*Alexandre Polo,Chanwoo Chun,SueYeon Chung*

Main category: cs.LG

TL;DR: 本研究通过应用流形容量理论（MCT）到组合布尔逻辑任务上，揭示了链式思维提示（CoT）在大型语言模型中的推理过程表现为一个瞬时的几何脉冲。概念流形在计算前立即解开为线性可分的子空间，并在此后迅速压缩。这种现象被解释为动态流形管理机制，模型通过它动态调节表示能力以优化整个推理链中残差流的带宽。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维（CoT）提示显著提升了大型语言模型的推理性能，但对于其背后表征几何学的时间动态理解仍然不足。为了深入探究这一过程，并能够量化潜在表示的线性可分性而不受探测训练干扰因素的影响，本研究采用了流形容量理论（MCT）。

Method: 研究人员运用了流形容量理论（MCT）对一个组合布尔逻辑任务进行了分析。这种方法允许他们直接衡量潜在表示的线性可分性，而无需经过探测器训练这一环节。

Result: 研究表明，在推理过程中观察到了一种瞬态几何脉冲现象：就在实际计算之前，概念流形被解缠绕进入线性可分的子空间，随后又快速地被重新压缩。这与标准线性探针准确性在计算后长时间保持高水平的现象形成对比，表明了信息仅仅是可检索的和信息是几何上准备好进行处理之间存在根本区别。

Conclusion: 研究提出了一种称为“动态流形管理”的机制来解释上述现象，即模型能够动态调整表示容量以在整个推理链条中优化残差流的带宽。

Abstract: Chain-of-Thought (CoT) prompting significantly improves reasoning in Large Language Models, yet the temporal dynamics of the underlying representation geometry remain poorly understood. We investigate these dynamics by applying Manifold Capacity Theory (MCT) to a compositional Boolean logic task, allowing us to quantify the linear separability of latent representations without the confounding factors of probe training. Our analysis reveals that reasoning manifests as a transient geometric pulse, where concept manifolds are untangled into linearly separable subspaces immediately prior to computation and rapidly compressed thereafter. This behavior diverges from standard linear probe accuracy, which remains high long after computation, suggesting a fundamental distinction between information that is merely retrievable and information that is geometrically prepared for processing. We interpret this phenomenon as \emph{Dynamic Manifold Management}, a mechanism where the model dynamically modulates representational capacity to optimize the bandwidth of the residual stream throughout the reasoning chain.

</details>


### [25] [Hierarchical Molecular Representation Learning via Fragment-Based Self-Supervised Embedding Prediction](https://arxiv.org/abs/2602.20344)
*Jiele Wu,Haozhe Ma,Zhihan Guo,Thanh Vinh Vo,Tze Yun Leong*

Main category: cs.LG

TL;DR: 提出了一种新的图自监督学习框架GraSPNet，该框架能够同时建模原子级别和片段级别的语义信息，通过多层次的消息传递及掩码语义预测来学习节点和片段级别的表示。实验表明，GraSPNet在分子属性预测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图自监督学习方法主要关注节点或边级别的信息，而忽略了对分子性质有重要影响的化学相关子结构。因此，需要一种能捕捉到更细粒度以及更高层次语义信息的方法来改善分子图分析的效果。

Method: 提出了Graph Semantic Predictive Network (GraSPNet)，一个分层的自监督框架，它可以在没有预定义词汇表的情况下将分子图分解为化学上有意义的片段，并通过多级消息传递与两层面上的掩码语义预测来学习节点级和片段级的表示。

Result: 广泛的实验表明，GraSPNet能够学习到具有化学意义的表示，并且在迁移学习设置中始终优于最新的GSSL方法。

Conclusion: GraSPNet通过引入层级式的语义监督机制，在分子图分析领域展示了其优越性，提供了一种新的途径来生成既具表现力又可迁移的图嵌入。

Abstract: Graph self-supervised learning (GSSL) has demonstrated strong potential for generating expressive graph embeddings without the need for human annotations, making it particularly valuable in domains with high labeling costs such as molecular graph analysis. However, existing GSSL methods mostly focus on node- or edge-level information, often ignoring chemically relevant substructures which strongly influence molecular properties. In this work, we propose Graph Semantic Predictive Network (GraSPNet), a hierarchical self-supervised framework that explicitly models both atomic-level and fragment-level semantics. GraSPNet decomposes molecular graphs into chemically meaningful fragments without predefined vocabularies and learns node- and fragment-level representations through multi-level message passing with masked semantic prediction at both levels. This hierarchical semantic supervision enables GraSPNet to learn multi-resolution structural information that is both expressive and transferable. Extensive experiments on multiple molecular property prediction benchmarks demonstrate that GraSPNet learns chemically meaningful representations and consistently outperforms state-of-the-art GSSL methods in transfer learning settings.

</details>


### [26] [Momentum Guidance: Plug-and-Play Guidance for Flow Models](https://arxiv.org/abs/2602.20360)
*Runlong Liao,Jian Yu,Baiyu Su,Chi Zhang,Lizhang Chen,Qiang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的指导方法——动量指导（Momentum Guidance, MG），该方法利用了ODE轨迹本身，通过过去速度的指数移动平均来推断当前速度。MG在不增加额外计算成本的情况下匹配了标准引导的效果，并且当与分类器自由引导（CFG）结合使用时，还能进一步提高生成质量。实验表明，在各种基准测试中，MG有效提升了图像生成的质量，特别是在ImageNet-256数据集上实现了显著的FID分数改进。


<details>
  <summary>Details</summary>
Motivation: 基于流的生成模型虽然在高质量生成建模方面表现良好，但直接使用的条件形式往往导致生成样本缺乏细节和清晰度。现有的如分类器自由引导等技术虽能改善保真度，但会加倍推理成本并通常减少样本多样性。因此，作者旨在开发一种既能提高生成质量又不会显著增加计算负担的新方法。

Method: 提出了动量指导（MG）作为新的引导维度，它利用了ODE轨迹本身的特性。MG通过过去速度的指数移动平均值来外推当前速度，保持了每步一次评估的标准成本。这种方法能够在没有额外计算的情况下达到与标准引导相同的效果，并且可以与CFG结合以进一步提高质量。

Result: 实验结果表明，MG在不同基准测试中均有效提高了生成图像的质量。特别地，在ImageNet-256数据集上，MG单独使用或与CFG结合使用时，分别实现了36.68%和25.52%的FID得分平均改进。此外，对于大型基于流的模型如Stable Diffusion 3和FLUX.1-dev，MG也显示出一致的质量提升。

Conclusion: 动量指导为基于流的生成模型提供了一个有效的新工具，可以在不显著增加计算成本的前提下提高生成图像的质量。此方法不仅独立使用时效果良好，而且与现有技术相结合时也能进一步增强性能。

Abstract: Flow-based generative models have become a strong framework for high-quality generative modeling, yet pretrained models are rarely used in their vanilla conditional form: conditional samples without guidance often appear diffuse and lack fine-grained detail due to the smoothing effects of neural networks. Existing guidance techniques such as classifier-free guidance (CFG) improve fidelity but double the inference cost and typically reduce sample diversity. We introduce Momentum Guidance (MG), a new dimension of guidance that leverages the ODE trajectory itself. MG extrapolates the current velocity using an exponential moving average of past velocities and preserves the standard one-evaluation-per-step cost. It matches the effect of standard guidance without extra computation and can further improve quality when combined with CFG. Experiments demonstrate MG's effectiveness across benchmarks. Specifically, on ImageNet-256, MG achieves average improvements in FID of 36.68% without CFG and 25.52% with CFG across various sampling settings, attaining an FID of 1.597 at 64 sampling steps. Evaluations on large flow-based models like Stable Diffusion 3 and FLUX.1-dev further confirm consistent quality enhancements across standard metrics.

</details>


### [27] [cc-Shapley: Measuring Multivariate Feature Importance Needs Causal Context](https://arxiv.org/abs/2602.20396)
*Jörg Martin,Stefan Haufe*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法cc-Shapley（因果上下文Shapley），通过利用数据的因果结构知识来修正传统观测Shapley值，解决了由于碰撞偏差导致的数据中虚假关联问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Shapley值的多变量特征重要性度量方法在解释机器学习模型时可能会产生误导，因为仅考虑一个特征在其他特征的观察背景下的情况会导致虚假关联。为了更准确地识别和纠正这些误导性的特征归因，需要引入关于数据生成过程的因果知识。

Method: 提出了cc-Shapley（因果上下文Shapley）方法，这是一种对传统观测Shapley值进行干预修改的方法，它利用了数据因果结构的知识来分析特征在其余特征的因果背景下的重要性。

Result: 理论证明了该方法可以消除由碰撞偏差引起的虚假关联；通过对多种合成和真实世界数据集上Shapley值与cc-Shapley值行为的比较，观察到从观测到cc-Shapley时关联被消除或逆转。

Conclusion: 使用cc-Shapley能够有效避免传统Shapley值在处理多变量特征重要性时遇到的问题，特别是那些由碰撞偏差导致的错误关联，并且有助于更好地理解特征之间的真正关系。

Abstract: Explainable artificial intelligence promises to yield insights into relevant features, thereby enabling humans to examine and scrutinize machine learning models or even facilitating scientific discovery. Considering the widespread technique of Shapley values, we find that purely data-driven operationalization of multivariate feature importance is unsuitable for such purposes. Even for simple problems with two features, spurious associations due to collider bias and suppression arise from considering one feature only in the observational context of the other, which can lead to misinterpretations. Causal knowledge about the data-generating process is required to identify and correct such misleading feature attributions. We propose cc-Shapley (causal context Shapley), an interventional modification of conventional observational Shapley values leveraging knowledge of the data's causal structure, thereby analyzing the relevance of a feature in the causal context of the remaining features. We show theoretically that this eradicates spurious association induced by collider bias. We compare the behavior of Shapley and cc-Shapley values on various, synthetic, and real-world datasets. We observe nullification or reversal of associations compared to univariate feature importance when moving from observational to cc-Shapley.

</details>


### [28] [GeoPT: Scaling Physics Simulation via Lifted Geometric Pre-Training](https://arxiv.org/abs/2602.20399)
*Haixu Wu,Minghao Guo,Zongyi Li,Zhiyang Dou,Mingsheng Long,Kaiming He,Wojciech Matusik*

Main category: cs.LG

TL;DR: 介绍了一种名为GeoPT的预训练模型，该模型通过在几何形状上添加合成动力学来为物理模拟提供动力学感知的自监督，从而解决了静态几何形状忽略动力学的问题。它在超过一百万个样本上进行预训练，并在流体和固体力学等工业级基准测试中表现出色，减少了对标签数据的需求并加快了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 神经模拟器作为物理模拟的有效替代方案，但在生成高保真训练数据方面的高昂成本阻碍了其扩展。虽然使用现成的几何形状进行预训练提供了一个自然的选择，但仅基于静态几何形状的监督忽略了动力学特性，可能导致在物理任务中的负迁移。

Method: 提出了一种称为GeoPT的通用物理模拟预训练模型，采用增强几何预训练的方法。该方法通过向几何形状添加合成动力学来实现无需物理标签的动力学感知自监督。

Result: GeoPT在包含汽车、飞机和船舶的流体力学以及碰撞模拟中的固体力学等工业级基准测试中表现良好，与传统方法相比，减少了20-60%的标记数据需求，并将收敛速度提高了两倍。

Conclusion: 通过结合合成动力学，GeoPT成功地连接了几何与物理之间的差距，为可扩展的神经模拟开辟了一条道路，可能具有更广泛的应用潜力。

Abstract: Neural simulators promise efficient surrogates for physics simulation, but scaling them is bottlenecked by the prohibitive cost of generating high-fidelity training data. Pre-training on abundant off-the-shelf geometries offers a natural alternative, yet faces a fundamental gap: supervision on static geometry alone ignores dynamics and can lead to negative transfer on physics tasks. We present GeoPT, a unified pre-trained model for general physics simulation based on lifted geometric pre-training. The core idea is to augment geometry with synthetic dynamics, enabling dynamics-aware self-supervision without physics labels. Pre-trained on over one million samples, GeoPT consistently improves industrial-fidelity benchmarks spanning fluid mechanics for cars, aircraft, and ships, and solid mechanics in crash simulation, reducing labeled data requirements by 20-60% and accelerating convergence by 2$\times$. These results show that lifting with synthetic dynamics bridges the geometry-physics gap, unlocking a scalable path for neural simulation and potentially beyond. Code is available at https://github.com/Physics-Scaling/GeoPT.

</details>


### [29] [Three Concrete Challenges and Two Hopes for the Safety of Unsupervised Elicitation](https://arxiv.org/abs/2602.20400)
*Callum Canavan,Aditya Shrivastava,Allison Qi,Jonathan Michala,Fabien Roger*

Main category: cs.LG

TL;DR: 本文分析了现有的无监督引导和由易到难泛化技术在特定设计的数据集上的表现，发现这些方法在面对缺乏某些理想属性的数据集时表现不佳，并建议未来的研究应优先解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 作者指出，当前用于评估模型准确性的数据集可能过于理想化，导致对模型性能的评估结果过于乐观。为了更真实地测试模型能力，作者构建了一些具有挑战性的数据集，这些数据集不包含以往数据集中常见的有利特性（如真实性特征最为显著、训练集平衡、答案明确等）。

Method: 通过创建缺乏上述理想特性的新数据集来对一系列标准无监督引导及由易到难泛化技术进行压力测试；同时探索了组合不同技术和使用集成方法能否缓解因这些挑战而导致的表现下降。

Result: 研究结果显示，在面对缺乏理想属性的数据集时，没有一种技术能够可靠地表现出色。即使尝试结合多种技术或采用集成策略也只能部分减轻性能退化的问题。

Conclusion: 现有无监督引导与由易到难泛化技术在处理非理想条件下构造的数据集方面存在局限性，未来研究需要重点克服这些限制。

Abstract: To steer language models towards truthful outputs on tasks which are beyond human capability, previous work has suggested training models on easy tasks to steer them on harder ones (easy-to-hard generalization), or using unsupervised training algorithms to steer models with no external labels at all (unsupervised elicitation). Although techniques from both paradigms have been shown to improve model accuracy on a wide variety of tasks, we argue that the datasets used for these evaluations could cause overoptimistic evaluation results. Unlike many real-world datasets, they often (1) have no features with more salience than truthfulness, (2) have balanced training sets, and (3) contain only data points to which the model can give a well-defined answer. We construct datasets that lack each of these properties to stress-test a range of standard unsupervised elicitation and easy-to-hard generalization techniques. We find that no technique reliably performs well on any of these challenges. We also study ensembling and combining easy-to-hard and unsupervised techniques, and find they only partially mitigate performance degradation due to these challenges. We believe that overcoming these challenges should be a priority for future work on unsupervised elicitation.

</details>


### [30] [Wasserstein Distributionally Robust Online Learning](https://arxiv.org/abs/2602.20403)
*Guixian Chen,Salar Fattahi,Soroosh Shafiee*

Main category: cs.LG

TL;DR: 本文研究了分布鲁棒在线学习问题，提出了一种新的框架来解决其收敛性和计算上的挑战，并针对一类重要的分段凹损失函数提出了高效的算法。


<details>
  <summary>Details</summary>
Motivation: 在已有的离线设置中，通过Wasserstein分布鲁棒优化（DRO）可以很好地理解分布鲁棒学习范式，但其在线扩展在收敛和计算方面带来了重大挑战。

Method: 首先将问题形式化为决策者与选择最坏情况分布的对手之间的在线鞍点随机博弈；其次，对于重要的分段凹损失函数类，提出了一种利用问题几何特性的定制算法，以实现比现有求解器更快的速度。

Result: 提出的一般框架能够收敛到一个稳健的纳什均衡，该均衡与相应的离线Wasserstein DRO问题的解决方案相吻合。此外，所提出的算法通过对问题几何结构的巧妙运用，在解决最坏情况期望问题上实现了显著加速。

Conclusion: 本文成功解决了分布鲁棒在线学习中的关键挑战，包括理论上的收敛性保证以及实践中的高效计算方法。

Abstract: We study distributionally robust online learning, where a risk-averse learner updates decisions sequentially to guard against worst-case distributions drawn from a Wasserstein ambiguity set centered at past observations. While this paradigm is well understood in the offline setting through Wasserstein Distributionally Robust Optimization (DRO), its online extension poses significant challenges in both convergence and computation. In this paper, we address these challenges. First, we formulate the problem as an online saddle-point stochastic game between a decision maker and an adversary selecting worst-case distributions, and propose a general framework that converges to a robust Nash equilibrium coinciding with the solution of the corresponding offline Wasserstein DRO problem. Second, we address the main computational bottleneck, which is the repeated solution of worst-case expectation problems. For the important class of piecewise concave loss functions, we propose a tailored algorithm that exploits problem geometry to achieve substantial speedups over state-of-the-art solvers such as Gurobi. The key insight is a novel connection between the worst-case expectation problem, an inherently infinite-dimensional optimization problem, and a classical and tractable budget allocation problem, which is of independent interest.

</details>


### [31] [$κ$-Explorer: A Unified Framework for Active Model Estimation in MDPs](https://arxiv.org/abs/2602.20404)
*Xihe Gu,Urbashi Mitra,Tara Javidi*

Main category: cs.LG

TL;DR: 本文提出了一种新的主动探索算法$κ$-Explorer，该算法基于一个可参数化的、可分解的凹目标函数$U_κ$来优化状态-动作占用度量。通过结合内在估计复杂性和外在访问频率，$U_κ$能够统一处理各种全局目标，并且其递减回报结构有助于优先考虑未充分探索和高方差的转换。实验表明，与现有策略相比，$κ$-Explorer在基准MDP上表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 在具有完美状态可观测性的表格马尔可夫决策过程（MDPs）中，准确模型估计依赖于探索策略如何根据每个转移分布的内在复杂性分配访问频率。为了解决这一问题并提高探索效率，文章引入了新的方法来优化状态-动作对的访问模式。

Method: 文章定义了一个可参数化的目标函数族$U_κ$，它既考虑了内在估计难度也考虑了外在访问次数。利用$U_κ$梯度的闭式表征，提出了$κ$-Explorer算法，该算法通过对状态-动作占用度量执行Frank-Wolfe风格的优化来进行积极探索。此外，还介绍了一种完全在线且计算高效的替代算法以供实际应用。

Result: $κ$-Explorer算法能够在理论层面保证较小的遗憾界，并且在实践中的基准MDP测试里表现优于现有的探索策略。

Conclusion: 研究证明了$κ$-Explorer算法通过有效结合转移分布的内在复杂性与外在访问频率，在不同场景下均能实现更优的探索效果，为MDP中的探索问题提供了新的解决方案。

Abstract: In tabular Markov decision processes (MDPs) with perfect state observability, each trajectory provides active samples from the transition distributions conditioned on state-action pairs. Consequently, accurate model estimation depends on how the exploration policy allocates visitation frequencies in accordance with the intrinsic complexity of each transition distribution. Building on recent work on coverage-based exploration, we introduce a parameterized family of decomposable and concave objective functions $U_κ$ that explicitly incorporate both intrinsic estimation complexity and extrinsic visitation frequency. Moreover, the curvature $κ$ provides a unified treatment of various global objectives, such as the average-case and worst-case estimation error objectives. Using the closed-form characterization of the gradient of $U_κ$, we propose $κ$-Explorer, an active exploration algorithm that performs Frank-Wolfe-style optimization over state-action occupancy measures. The diminishing-returns structure of $U_κ$ naturally prioritizes underexplored and high-variance transitions, while preserving smoothness properties that enable efficient optimization. We establish tight regret guarantees for $κ$-Explorer and further introduce a fully online and computationally efficient surrogate algorithm for practical use. Experiments on benchmark MDPs demonstrate that $κ$-Explorer provides superior performance compared to existing exploration strategies.

</details>


### [32] [GauS: Differentiable Scheduling Optimization via Gaussian Reparameterization](https://arxiv.org/abs/2602.20427)
*Yaohui Cai,Vesal Bakhtazad,Cunxi Yu,Zhiru Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的可微框架GauS，通过使用高斯分布来建模操作符调度问题，有效捕捉时间的顺序性质，并大幅度减少优化空间。该方法在各种基准测试中达到了帕累托最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前基于梯度搜索的可微方法通常依赖于分类分布，这无法很好地捕捉时间的顺序特性，并且参数空间扩展性差。为解决这些问题，提出了GauS框架。

Method: GauS框架将调度表示为连续的高斯变量，利用了现代并行计算设备如GPU的优势，能够灵活地表示多种目标和约束条件，提供了复杂流水线调度问题的第一个可微公式。

Result: GauS在一系列基准测试中被评估，展示了其达到帕累托最优结果的能力。

Conclusion: 提出的GauS框架为操作符调度提供了一个高效、灵活的新方法，特别适用于需要考虑时间顺序性和大规模优化空间的问题。

Abstract: Efficient operator scheduling is a fundamental challenge in software compilation and hardware synthesis. While recent differentiable approaches have sought to replace traditional ones like exact solvers or heuristics with gradient-based search, they typically rely on categorical distributions that fail to capture the ordinal nature of time and suffer from a parameter space that scales poorly. In this paper, we propose a novel differentiable framework, GauS, that models operator scheduling as a stochastic relaxation using Gaussian distributions, which fully utilize modern parallel computing devices like GPUs. By representing schedules as continuous Gaussian variables, we successfully capture the ordinal nature of time and reduce the optimization space by orders of magnitude. Our method is highly flexible to represent various objectives and constraints, which provides the first differentiable formulation for the complex pipelined scheduling problem. We evaluate our method on a range of benchmarks, demonstrating that Gaus achieves Pareto-optimal results.

</details>


### [33] [Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference](https://arxiv.org/abs/2602.20449)
*Anna Hart,Chi Han,Jeonghwan Kim,Huimin Zhao,Heng Ji*

Main category: cs.LG

TL;DR: 本研究对比了蛋白质语言模型（PLM）和自然语言模型中注意力头层间信息分布的差异，并通过采用早期退出技术提高了蛋白质非结构性质预测任务上的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言只有20种氨基酸组成，但其功能空间丰富，与自然语言存在显著区别。这些差异促使研究者探索基于转换器的架构在蛋白质领域中的不同运作方式以及如何更好地利用PLM解决与蛋白质相关的问题。

Method: 直接比较了蛋白质与自然语言领域中注意力头层间存储的信息分布差异；适应了一种最初用于自然语言领域的简单早期退出技术，以提高效率的同时牺牲性能，在蛋白质非结构性质预测上实现了更高的准确性及实质性的效率提升。

Result: 在提高效率超过10%的同时，对于不同的模型和非结构性预测任务，性能提升了0.4到7.01个百分点。

Conclusion: 这项工作开启了直接比较语言模型从自然语言转移到蛋白质领域时行为变化的研究方向，并促进了生物领域内的语言建模发展。

Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.

</details>


### [34] [Oracle-Robust Online Alignment for Large Language Models](https://arxiv.org/abs/2602.20457)
*Zimeng Li,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文研究了在偏好反馈存在偏差的情况下，大型语言模型的在线对齐问题，并提出了一个鲁棒性目标函数。对于对数线性策略，该鲁棒目标可以被精确分解为原始损失函数加上显式敏感度惩罚项。作者开发了针对弱凸目标函数的投影随机复合更新方法，并证明了达到近似平稳状态的oracle复杂度为$\widetilde{O}(\varepsilon^{-2})$。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决当观察到的偏好反馈与理想的但未知的真实反馈不一致时，大型语言模型在线对齐的问题。这涉及到如何在面对可能有误的反馈信息时，依然能够有效地调整和改进模型。

Method: 引入了一个点态oracle不确定性集，并将问题表述为最坏情况下的优化问题。对于对数线性策略，提出了一种鲁棒性的在线对齐目标函数，它可以准确地分解成原有的损失函数加上一个明确的敏感度惩罚项。此外，还设计了适用于由此产生的弱凸目标函数的投影随机复合更新算法。

Result: 所提出的鲁棒目标函数允许通过闭式解进行精确分解；新提出的更新算法具有$\widetilde{O}(\varepsilon^{-2})$的oracle复杂度来达到近似平稳解。

Conclusion: 本研究提供了一种处理大型语言模型在线对齐中偏好反馈错误指定的方法，通过引入鲁棒性考虑增强了现有SAIL框架，并且展示了如何有效实现这一目标同时保持计算效率。

Abstract: We study online alignment of large language models under misspecified preference feedback, where the observed preference oracle deviates from an ideal but unknown ground-truth oracle. The online LLM alignment problem is a bi-level reinforcement problem due to the coupling between data collection and policy updates. Recently, the problem has been reduced to tractable single-level objective in the SAIL (Self-Improving Efficient Online Alignment) framework. In this paper, we introduce a pointwise oracle uncertainty set in this problem and formulate an oracle-robust online alignment objective as a worst-case optimization problem. For log-linear policies, we show that this robust objective admits an exact closed-form decomposition into the original loss function plus an explicit sensitivity penalty. We develop projected stochastic composite updates for the resulting weakly convex objective and prove $\widetilde{O}(\varepsilon^{-2})$ oracle complexity for reaching approximate stationarity.

</details>


### [35] [A Long-Short Flow-Map Perspective for Drifting Models](https://arxiv.org/abs/2602.20463)
*Zhiqi Li,Bo Zhu*

Main category: cs.LG

TL;DR: 该论文通过半群一致的长短期流图分解重新解释了漂移模型，提出了一种新的似然学习公式，并通过理论分析和实证评估验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过一种新的视角来重新解释漂移模型，即采用半群一致的长短期流图分解方法，从而为密度演化下的传输提供一个更贴合的描述方式。

Method: 基于半群一致的长短期流图分解方法，将全局传输过程分解成长期流图和允许闭式最优速度表示的短时终端流图；当终端区间长度趋近于零时，可以精确恢复漂移场及保持流图一致性所需的保守脉冲项。此外，提出了一个新的似然学习公式以使长短期流图分解与传输下的密度演化相匹配。

Result: 通过基准测试的理论分析和实证评估证实了所提框架的有效性，并且还提供了特征空间优化的理论解释，同时指出了未来研究中的一些开放问题。

Conclusion: 本文的工作不仅对理解漂移模型提供了新见解，而且也为密度演化的建模提出了新颖的方法论，为后续的研究打开了新的方向。

Abstract: This paper provides a reinterpretation of the Drifting Model~\cite{deng2026generative} through a semigroup-consistent long-short flow-map factorization. We show that a global transport process can be decomposed into a long-horizon flow map followed by a short-time terminal flow map admitting a closed-form optimal velocity representation, and that taking the terminal interval length to zero recovers exactly the drifting field together with a conservative impulse term required for flow-map consistency. Based on this perspective, we propose a new likelihood learning formulation that aligns the long-short flow-map decomposition with density evolution under transport. We validate the framework through both theoretical analysis and empirical evaluations on benchmark tests, and further provide a theoretical interpretation of the feature-space optimization while highlighting several open problems for future study.

</details>


### [36] [Wireless Federated Multi-Task LLM Fine-Tuning via Sparse-and-Orthogonal LoRA](https://arxiv.org/abs/2602.20492)
*Nuocheng Yang,Sihua Wang,Ouwen Huan,Mingzhe Chen,Tony Q. S. Quek,Changchuan Yin*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏正交LoRA、聚类拓扑设计和隐式专家混合机制的去中心化联邦学习方法，以解决异构数据集上微调大型语言模型时的知识遗忘、通信效率低下及多任务知识干扰问题。实验表明该方法可减少高达73%的通信资源消耗，并将平均性能提高5%。


<details>
  <summary>Details</summary>
Motivation: 针对在去中心化联邦学习中使用低秩适应（LoRA）技术对大型语言模型进行多任务数据集联合微调时出现的知识灾难性遗忘、通讯与收敛效率低以及推理过程中多任务知识干扰等问题，提出了改进方案。

Method: 1. 提出稀疏且正交的LoRA确保模型更新间的正交性，消除微调过程中的方向冲突。
2. 分析设备连接拓扑结构如何影响多任务表现，据此设计聚合阶段的基于集群的拓扑结构。
3. 引入一种隐式的专家混合机制来避免推理时互不兼容的知识共存问题。

Result: 通过模拟实验验证了所提方法的有效性：与传统LoRA方法相比，新方法能够降低多达73%的通信资源消耗，并将平均性能提升5%。

Conclusion: 本研究提出的解决方案成功缓解了去中心化联邦学习场景下遇到的主要挑战，包括因数据异质性导致的知识遗忘、带宽密集型冗余传输造成的低效沟通与收敛问题以及推理时的知识干扰现象。

Abstract: Decentralized federated learning (DFL) based on low-rank adaptation (LoRA) enables mobile devices with multi-task datasets to collaboratively fine-tune a large language model (LLM) by exchanging locally updated parameters with a subset of neighboring devices via wireless connections for knowledge integration.However, directly aggregating parameters fine-tuned on heterogeneous datasets induces three primary issues across the DFL life-cycle: (i) \textit{catastrophic knowledge forgetting during fine-tuning process}, arising from conflicting update directions caused by data heterogeneity; (ii) \textit{inefficient communication and convergence during model aggregation process}, due to bandwidth-intensive redundant model transmissions; and (iii) \textit{multi-task knowledge interference during inference process}, resulting from incompatible knowledge representations coexistence during inference. To address these issues in a fully decentralized scenario, we first propose a sparse-and-orthogonal LoRA that ensures orthogonality between model updates to eliminate direction conflicts during fine-tuning.Then, we analyze how device connection topology affects multi-task performance, prompting a cluster-based topology design during aggregation.Finally, we propose an implicit mixture of experts (MoE) mechanism to avoid the coexistence of incompatible knowledge during inference. Simulation results demonstrate that the proposed approach effectively reduces communication resource consumption by up to $73\%$ and enhances average performance by $5\%$ compared with the traditional LoRA method.

</details>


### [37] [A Generalized Apprenticeship Learning Framework for Capturing Evolving Student Pedagogical Strategies](https://arxiv.org/abs/2602.20527)
*Md Mirajul Islam,Xi Yang,Adittya Soukarjya Saha,Rajesh Debnath,Min Chi*

Main category: cs.LG

TL;DR: 本文介绍了一种名为THEMES的广义学徒学习框架，用于从少量专家演示中推断出潜在奖励函数，并生成能够复制最优行为的决策策略。通过与六种最先进基线比较，展示了其在教育技术中的潜力和优越性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）和深度强化学习（DRL）在智能辅导系统等电子学习环境中取得成功，但它们在更广泛的应用上受到样本效率低以及设计奖励函数难等问题限制。因此，研究转向了可以从少量专家示例中学到有效教学策略的方法。

Method: 采用一种名为THEMES的广义学徒学习方法，该方法能够根据专家学生的学习过程复杂性来推导有效的教学政策，其中考虑到了随时间动态变化的多个奖励函数。

Result: THEMES相对于六个最先进的基线表现出了优越的性能，在仅使用前一学期18条轨迹的情况下，预测后一学期学生教学决策时达到了AUC 0.899和Jaccard 0.653的成绩。

Conclusion: 研究表明THEMES作为一种强大的替代方案，在诱导有效教学策略方面展现了巨大潜力。

Abstract: Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) have advanced rapidly in recent years and have been successfully applied to e-learning environments like intelligent tutoring systems (ITSs). Despite great success, the broader application of DRL to educational technologies has been limited due to major challenges such as sample inefficiency and difficulty designing the reward function. In contrast, Apprenticeship Learning (AL) uses a few expert demonstrations to infer the expert's underlying reward functions and derive decision-making policies that generalize and replicate optimal behavior. In this work, we leverage a generalized AL framework, THEMES, to induce effective pedagogical policies by capturing the complexities of the expert student learning process, where multiple reward functions may dynamically evolve over time. We evaluate the effectiveness of THEMES against six state-of-the-art baselines, demonstrating its superior performance and highlighting its potential as a powerful alternative for inducing effective pedagogical policies and show that it can achieve high performance, with an AUC of 0.899 and a Jaccard of 0.653, using only 18 trajectories of a previous semester to predict student pedagogical decisions in a later semester.

</details>


### [38] [Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition](https://arxiv.org/abs/2602.20530)
*Ming Li,Yong-Jin Liu,Fang Liu,Huankun Sheng,Yeying Fan,Yixiang Wei,Minnan Luo,Weizhan Zhang,Wenping Wang*

Main category: cs.LG

TL;DR: 提出了一种记忆引导的原型共现学习（MPCL）框架，用于从多模态生理和行为信号中识别混合情绪。该框架通过多尺度关联记忆机制融合多模态信号，并构建特定情绪的原型记忆库来捕捉跨模态语义关系，同时引入记忆检索策略提取情绪类别间的语义级共现关联，从而在两个公开数据集上证明了其在混合情绪识别任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别模型大多局限于实验室环境中单一情绪的预测，而现实世界的人类情感体验往往涉及多种同时存在的情绪状态。当前方法未能充分考虑共存情绪之间的效价一致性和结构相关性。

Method: 提出记忆引导的原型共现学习（Memory-guided Prototypical Co-occurrence Learning, MPCL）框架；利用多尺度关联记忆机制融合多模态信号；构造特定情绪的原型记忆库以捕捉跨模态语义关系；采用原型关系蒸馏确保潜在原型空间内的跨模态对齐；基于人类认知记忆系统启发的记忆检索策略被用来提取情绪类别间语义级别的共现关联。

Result: 综合实验表明，在两个公共数据集上，所提方法MPCL在混合情绪识别方面定量与定性地优于最先进方法。

Conclusion: 本研究提出的MPCL框架能够有效建模情绪共现模式，并且在混合情绪分布预测上表现出色，为解决真实场景下复杂情绪识别问题提供了新的思路和技术支持。

Abstract: Emotion recognition from multi-modal physiological and behavioral signals plays a pivotal role in affective computing, yet most existing models remain constrained to the prediction of singular emotions in controlled laboratory settings. Real-world human emotional experiences, by contrast, are often characterized by the simultaneous presence of multiple affective states, spurring recent interest in mixed emotion recognition as an emotion distribution learning problem. Current approaches, however, often neglect the valence consistency and structured correlations inherent among coexisting emotions. To address this limitation, we propose a Memory-guided Prototypical Co-occurrence Learning (MPCL) framework that explicitly models emotion co-occurrence patterns. Specifically, we first fuse multi-modal signals via a multi-scale associative memory mechanism. To capture cross-modal semantic relationships, we construct emotion-specific prototype memory banks, yielding rich physiological and behavioral representations, and employ prototype relation distillation to ensure cross-modal alignment in the latent prototype space. Furthermore, inspired by human cognitive memory systems, we introduce a memory retrieval strategy to extract semantic-level co-occurrence associations across emotion categories. Through this bottom-up hierarchical abstraction process, our model learns affectively informative representations for accurate emotion distribution prediction. Comprehensive experiments on two public datasets demonstrate that MPCL consistently outperforms state-of-the-art methods in mixed emotion recognition, both quantitatively and qualitatively.

</details>


### [39] [Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training](https://arxiv.org/abs/2602.20532)
*Zhengyao Gu,Jonathan Light,Raul Astudillo,Ziyu Ye,Langzhou He,Henry Peng Zou,Wei Cheng,Santiago Paternain,Philip S. Yu,Yisong Yue*

Main category: cs.LG

TL;DR: 本文提出了一种名为ACTOR-CURATOR的自动课程学习框架，用于大规模语言模型（LLMs）在强化学习后训练中的问题选择。通过将问题选择视为非平稳随机多臂赌博机问题，并基于在线随机镜像下降法推导出一个原则性的损失函数，ACTOR-CURATOR能够有效地从大型题库中动态挑选训练题目以直接优化策略性能提升。实验表明，该方法相比均匀采样及其他强大的课程基准，在多个具有挑战性的推理基准上表现更优，特别是在AIME2024和ARC-1D测试集上获得了显著的相对增益与高达80%的速度提升。


<details>
  <summary>Details</summary>
Motivation: 针对使用强化学习对大型基础模型进行后训练时通常依赖于大量且异构的数据集这一现状，有效的课程学习变得既关键又具挑战性。为了克服这个问题，研究者们旨在开发一种可扩展且全自动化的课程学习框架，以提高大型语言模型（LLMs）在强化学习后训练过程中的效率与稳定性。

Method: 提出了ACTOR-CURATOR框架，该框架通过学习一个神经策展人来动态地从大型题库中选取训练题目，目的是直接优化预期的政策性能改进。问题的选择被形式化为一个非平稳的随机多臂赌博机问题，并基于在线随机镜像下降法设计了相应的损失函数。此外，还提供了部分反馈下的遗憾保证。

Result: 实验结果表明，ACTOR-CURATOR在各种具有挑战性的推理基准测试中始终优于均匀采样和其他强大的课程基线，特别是在AIME2024和ARC-1D数据集上分别实现了相对于最强基线28.6%和30.5%的相对增益，以及最高达80%的速度提升。

Conclusion: 研究表明，ACTOR-CURATOR是一种强大而实用的方法，适用于大规模语言模型（LLMs）的可扩展后训练。它不仅提高了训练过程的稳定性和效率，而且在多个复杂推理任务上展示了显著的性能优势。

Abstract: Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.

</details>


### [40] [Sample-efficient evidence estimation of score based priors for model selection](https://arxiv.org/abs/2602.20549)
*Frederic Wang,Katherine L. Bouman*

Main category: cs.LG

TL;DR: 本文提出了一种新的估计器，用于通过后验采样方法的时间边缘积分来估计扩散先验的模型证据。该方法利用反向扩散采样过程中自然获得的大量中间样本，仅使用少量后验样本即可准确估计模型证据，并且能够选择正确的扩散模型先验以及诊断不同高度病态、非线性逆问题下的先验不匹配情况，包括一个真实的黑洞成像问题。


<details>
  <summary>Details</summary>
Motivation: 在解决不适定的成像逆问题时，选择合适的先验至关重要，因为这可以帮助避免严重的偏差。尽管扩散模型是当前解决具有数据驱动先验的逆问题的最佳方法，但直接计算与扩散先验相关的模型证据是非常困难的。此外，大多数现有的模型证据估计器需要多次评估未标准化的先验密度或需要一个精确的干净先验分数。因此，研究者们提出了一个新的估计器来解决这个问题。

Method: 提出的方法是一种通过整合后验采样方法的时间边缘来估计扩散先验模型证据的新估计器。它利用了在反向扩散采样过程期间自然得到的大量中间样本，从而只需少量（例如20个）后验样本就能实现对模型证据的准确估计。此方法还展示了如何与最近的扩散后验采样方法一起实施。

Result: 实验证明，所提出的估计器在可以解析地计算模型证据的情况下与之相匹配，并且能够在多种高度病态和非线性的逆问题中选择正确的扩散模型先验并诊断出先验不匹配的问题，其中包括了一个现实世界的黑洞成像问题。

Conclusion: 这项工作介绍了一种有效的方法来估计扩散先验的模型证据，对于解决不适定的成像逆问题特别有用。它不仅减少了所需后验样本的数量，而且能够准确选择适当的先验并在真实世界的应用中证明了其有效性。

Abstract: The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.

</details>


### [41] [GENSR: Symbolic Regression Based in Equation Generative Space](https://arxiv.org/abs/2602.20557)
*Qian Li,Yuxiao Hu,Juncheng Liu,Yuntian Chen*

Main category: cs.LG

TL;DR: 提出了一种基于生成潜空间的符号回归框架GenSR，通过预训练双分支条件变分自编码器将符号方程重新参数化到一个具有良好结构的'地图'中，使得搜索过程更加高效且准确。


<details>
  <summary>Details</summary>
Motivation: 传统的符号回归方法在探索隐藏于观测数据背后的方程时面临挑战，因为它们通常在一个离散的方程空间中搜索，而该空间中方程的结构变化很少与其数值行为一致，导致拟合误差反馈过于嘈杂而不利于指导探索。

Method: 开发了GenSR框架，首先使用双分支条件变分自编码器(CVAE)预训练来将符号方程转换为一个同时具有符号连续性和局部数值平滑性的生成潜空间。在推理阶段，CVAE能够粗略地定位输入数据到潜在空间中的有希望区域，然后通过修改后的CMA-ES算法利用平滑的潜在梯度进一步细化候选区域。从贝叶斯视角来看，GenSR将符号回归任务重构为最大化条件分布$p(\mathrm{Equ.} \mid \mathrm{Num.})$的过程，并通过证据下界(ELBO)达成这一目标。

Result: 广泛的实验表明，GenSR不仅优化了预测准确性、表达式简洁性和计算效率，而且在噪声条件下仍保持稳健性。

Conclusion: GenSR提供了一个有效的方法来克服传统符号回归中存在的问题，通过创建一个可以被更有效地探索的方程空间映射，从而提高了寻找隐藏方程的能力。

Abstract: Symbolic Regression (SR) tries to reveal the hidden equations behind observed data. However, most methods search within a discrete equation space, where the structural modifications of equations rarely align with their numerical behavior, leaving fitting error feedback too noisy to guide exploration. To address this challenge, we propose GenSR, a generative latent space-based SR framework following the `map construction -> coarse localization -> fine search'' paradigm. Specifically, GenSR first pretrains a dual-branch Conditional Variational Autoencoder (CVAE) to reparameterize symbolic equations into a generative latent space with symbolic continuity and local numerical smoothness. This space can be regarded as a well-structured `map'' of the equation space, providing directional signals for search. At inference, the CVAE coarsely localizes the input data to promising regions in the latent space. Then, a modified CMA-ES refines the candidate region, leveraging smooth latent gradients. From a Bayesian perspective, GenSR reframes the SR task as maximizing the conditional distribution $p(\mathrm{Equ.} \mid \mathrm{Num.})$, with CVAE training achieving this objective through the Evidence Lower Bound (ELBO). This new perspective provides a theoretical guarantee for the effectiveness of GenSR. Extensive experiments show that GenSR jointly optimizes predictive accuracy, expression simplicity, and computational efficiency, while remaining robust under noise.

</details>


### [42] [GATES: Self-Distillation under Privileged Context with Consensus Gating](https://arxiv.org/abs/2602.20574)
*Alex Stein,Furong Huang,Tom Goldstein*

Main category: cs.LG

TL;DR: 研究在缺乏可靠监督的情况下，通过从导师共识中在线派生监督并进行轨迹蒸馏来改进文档无学生模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在没有真实标签、可验证奖励或外部评分者评估答案的情况下，研究自蒸馏方法以提高文档无问答模型的准确性。

Method: 通过采样多个基于文档的推理路径，并使用一致性来控制学习过程。该方法根据这种可靠性信号，通过完整的导师推理轨迹（而不仅仅是最终答案）来进行知识蒸馏。

Result: 与不对称评估下的保持集内域准确率从46.0%提高到62.0%，公共文档无数学基准上的平均(maj@8)准确率从20.2%提高到35.4%。

Conclusion: 共识门控轨迹蒸馏法显著提高了向文档无学生模型转移的效果。

Abstract: We study self-distillation in settings where supervision is unreliable: there are no ground truth labels, verifiable rewards, or external graders to evaluate answers. We focus on document-grounded question answering with asymmetric context, where a single model serves as both tutor (with access to a relevant source document during training) and student (answering from the question alone at test time). Rather than assuming tutor correctness, we derive supervision online from tutor consensus by sampling multiple document-grounded reasoning traces and using agreement to gate learning. Conditioned on this reliability signal, we distill knowledge through full tutor reasoning trajectories (not just final answers), providing a dense and stable learning signal. Empirically, this consensus-gated trajectory distillation substantially improves transfer to the document-free student. Held-out in-domain accuracy under asymmetric evaluation improves from 46.0\% to 62.0\%, and average (maj@8) accuracy on public document-free math benchmarks improves from 20.2\% to 35.4\%.

</details>


### [43] [Upper-Linearizability of Online Non-Monotone DR-Submodular Maximization over Down-Closed Convex Sets](https://arxiv.org/abs/2602.20578)
*Yiyang Lu,Haresh Jadav,Mohammad Pedramfar,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文研究了在线最大化非单调递减回报(DR)-次模函数的问题，通过新的结构结果，实现了将该问题转化为在线线性优化问题的转化，从而在各种反馈模型下都取得了优于现有技术的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的无投影在线方法在处理非单调递减回报(DR)-次模函数的最大化问题时存在次优后悔度和有限反馈保证的问题。

Method: 提出了一种新结构结果，证明了通过精心设计的指数重参数化、缩放参数及替代势函数，这类问题可以被$1/e$-线性化，并因此能够简化为在线线性优化问题。

Result: 得到了$O(T^{1/2})$静态遗憾度，并且每轮只需一次梯度查询；同时解锁了自适应和动态遗憾度保证，在半带状、带状以及零阶反馈下均有改进的表现率。

Conclusion: 本研究提供了一种有效的方法来解决特定条件下在线非单调DR-次模函数最大化问题，显著提升了不同反馈模型下的性能指标。

Abstract: We study online maximization of non-monotone Diminishing-Return(DR)-submodular functions over down-closed convex sets, a regime where existing projection-free online methods suffer from suboptimal regret and limited feedback guarantees. Our main contribution is a new structural result showing that this class is $1/e$-linearizable under carefully designed exponential reparametrization, scaling parameter, and surrogate potential, enabling a reduction to online linear optimization. As a result, we obtain $O(T^{1/2})$ static regret with a single gradient query per round and unlock adaptive and dynamic regret guarantees, together with improved rates under semi-bandit, bandit, and zeroth-order feedback. Across all feedback models, our bounds strictly improve the state of the art.

</details>


### [44] [Is the Trigger Essential? A Feature-Based Triggerless Backdoor Attack in Vertical Federated Learning](https://arxiv.org/abs/2602.20593)
*Yige Liu,Yiwei Lou,Che Wang,Yongzhi Cao,Hanpin Wang*

Main category: cs.LG

TL;DR: 本文揭示了在垂直联邦学习中的一种新型无触发器后门攻击方式，该方法即使在更严格的诚实但好奇的安全假设下也能有效执行。实验结果表明，与三种基线后门攻击相比，这种攻击的性能高出2到50倍，并且对主要任务的影响极小。此外，在面对不同防御策略时，该攻击仍表现出强健的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 尽管垂直联邦学习（VFL）以保护隐私著称，但它仍然面临来自后门攻击的重大隐私和安全威胁。本文旨在揭露一种新的基于特征的无触发器后门攻击途径，指出在VFL中实施后门攻击并不需要传统意义上的触发器。

Method: 提出了一种由标签推断、毒化生成（采用放大与扰动机制）以及后门执行三部分组成的无触发器后门攻击方法。此方法在更加严格的安全假设下工作——即攻击者在训练阶段是诚实但好奇而非恶意的。

Result: 通过五个基准数据集上的广泛实验验证，所提出的攻击方法相较于三种基线后门攻击表现出了显著的优势，性能提升幅度达到2至50倍之多；并且即便是在有32个被动参与者及仅有一组辅助数据的情况下，该攻击依旧保持了高效性。

Conclusion: 新发现的无触发器后门攻击路径不仅暴露了垂直联邦学习场景中存在的安全漏洞，同时也促使研究社区重新审视相关安全威胁，并激发研究人员开发出更为稳健实用的防御策略。

Abstract: As a distributed collaborative machine learning paradigm, vertical federated learning (VFL) allows multiple passive parties with distinct features and one active party with labels to collaboratively train a model. Although it is known for the privacy-preserving capabilities, VFL still faces significant privacy and security threats from backdoor attacks. Existing backdoor attacks typically involve an attacker implanting a trigger into the model during the training phase and executing the attack by adding the trigger to the samples during the inference phase. However, in this paper, we find that triggers are not essential for backdoor attacks in VFL. In light of this, we disclose a new backdoor attack pathway in VFL by introducing a feature-based triggerless backdoor attack. This attack operates under a more stringent security assumption, where the attacker is honest-but-curious rather than malicious during the training phase. It comprises three modules: label inference for the targeted backdoor attack, poison generation with amplification and perturbation mechanisms, and backdoor execution to implement the attack. Extensive experiments on five benchmark datasets demonstrate that our attack outperforms three baseline backdoor attacks by 2 to 50 times while minimally impacting the main task. Even in VFL scenarios with 32 passive parties and only one set of auxiliary data, our attack maintains high performance. Moreover, when confronted with distinct defense strategies, our attack remains largely unaffected and exhibits strong robustness. We hope that the disclosure of this triggerless backdoor attack pathway will encourage the community to revisit security threats in VFL scenarios and inspire researchers to develop more robust and practical defense strategies.

</details>


### [45] [QEDBENCH: Quantifying the Alignment Gap in Automated Evaluation of University-Level Mathematical Proofs](https://arxiv.org/abs/2602.20629)
*Santiago Gonzalez,Alireza Amiri Bavandpour,Peter Ye,Edward Zhang,Ruslans Aleksejevs,Todor Antić,Polina Baron,Sujeet Bhalerao,Shubhrajit Bhattacharya,Zachary Burton,John Byrne,Hyungjun Choi,Nujhat Ahmed Disha,Koppany István Encz,Yuchen Fang,Robert Joseph George,Ebrahim Ghorbani,Alan Goldfarb,Jing Guo,Meghal Gupta,Stefano Huber,Annika Kanckos,Minjung Kang,Hyun Jong Kim,Dino Lorenzini,Levi Lorenzo,Tianyi Mao,Giovanni Marzenta,Ariane M. Masuda,Lukas Mauth,Ana Mickovic,Andres Miniguano-Trujillo,Antoine Moulin,Wenqi Ni,Tomos Parry,Kevin Ren,Hossein Roodbarani,Mathieu Rundström,Manjil Saikia,Detchat Samart,Rebecca Steiner,Connor Stewart,Dhara Thakkar,Jeffrey Tse,Vasiliki Velona,Yunhai Xiang,Sibel Yalçın,Jun Yan,Ji Zeng,Arman Cohan,Quanquan C. Liu*

Main category: cs.LG

TL;DR: Researchers introduce QEDBench, a new benchmark for assessing AI judges' alignment with human experts on advanced math proofs, revealing varying degrees of accuracy and bias among leading LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the reliability of automated evaluation by Large Language Models (LLMs) in the context of upper-undergraduate to early graduate level mathematics, where existing benchmarks are insufficient.

Method: Introduction of QEDBench, a dual-rubric alignment benchmark, and deployment of a dual-evaluation matrix involving 7 judges and 5 solvers, evaluated against over 1,000 hours of human judgment on university-level math proofs.

Result: Certain LLMs display notable positive bias in scoring, while other models' performance drops significantly in specific areas like Discrete Math and Graph Theory. Gemini 3.0 Pro shows state-of-the-art performance, whereas GPT-5 Pro and Claude Sonnet 4.5 see their scores decline in discrete domains.

Conclusion: The study reveals a significant Alignment Gap in standard LLM-as-a-Judge protocols when evaluating university-level math proofs, with some models showing positive bias and others performing poorly in discrete domains. The QEDBench benchmark is introduced as a tool to measure and improve the alignment of AI judges with human experts.

Abstract: As Large Language Models (LLMs) saturate elementary benchmarks, the research frontier has shifted from generation to the reliability of automated evaluation. We demonstrate that standard "LLM-as-a-Judge" protocols suffer from a systematic Alignment Gap when applied to upper-undergraduate to early graduate level mathematics. To quantify this, we introduce QEDBench, the first large-scale dual-rubric alignment benchmark to systematically measure alignment with human experts on university-level math proofs by contrasting course-specific rubrics against expert common knowledge criteria. By deploying a dual-evaluation matrix (7 judges x 5 solvers) against 1,000+ hours of human evaluation, we reveal that certain frontier evaluators like Claude Opus 4.5, DeepSeek-V3, Qwen 2.5 Max, and Llama 4 Maverick exhibit significant positive bias (up to +0.18, +0.20, +0.30, +0.36 mean score inflation, respectively). Furthermore, we uncover a critical reasoning gap in the discrete domain: while Gemini 3.0 Pro achieves state-of-the-art performance (0.91 average human evaluation score), other reasoning models like GPT-5 Pro and Claude Sonnet 4.5 see their performance significantly degrade in discrete domains. Specifically, their average human evaluation scores drop to 0.72 and 0.63 in Discrete Math, and to 0.74 and 0.50 in Graph Theory. In addition to these research results, we also release QEDBench as a public benchmark for evaluating and improving AI judges. Our benchmark is publicly published at https://github.com/qqliu/Yale-QEDBench.

</details>


### [46] [TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer](https://arxiv.org/abs/2602.20643)
*Jiawei Wang,Chuang Yang,Jiawei Yong,Xiaohang Xu,Hongjun Wang,Noboru Koshizuka,Shintaro Fukushima,Ryosuke Shibasaki,Renhe Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于transformer的新型城市移动轨迹生成框架，通过两阶段预训练和微调过程来解决隐私问题，并利用逆强化学习捕捉轨迹奖励信号，以提高模型的可靠性和多样性。


<details>
  <summary>Details</summary>
Motivation: 城市移动轨迹对于理解城市动态和增强城市规划至关重要，但获取此类数据常常受到隐私问题的阻碍。

Method: 采用一种基于transformer的模型，首先将轨迹生成视为离线强化学习问题进行预训练，并通过减少词汇空间来简化处理；然后使用构建的奖励模型对预训练模型进行微调，结合逆向强化学习（IRL）技术从历史数据中推断个人移动偏好。

Result: 在多个数据集上的综合评估表明，所提出的框架在可靠性和多样性方面显著优于现有模型。

Conclusion: 这项研究不仅推进了城市移动建模领域的发展，也为模拟城市数据提供了强大的方法论支持，对交通管理和城市发展计划具有重要意义。

Abstract: Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.

</details>


### [47] [Bikelution: Federated Gradient-Boosting for Scalable Shared Micro-Mobility Demand Forecasting](https://arxiv.org/abs/2602.20671)
*Antonios Tziorvas,Andreas Tritsarolis,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的解决方案Bikelution，用于无桩共享单车系统的中期需求预测，在保护隐私的同时提供准确的预测。


<details>
  <summary>Details</summary>
Motivation: 无桩共享单车系统快速增长产生了大量时空数据集，但自行车需求受多种外部因素影响，传统的时序模型不足以应对。集中式机器学习虽然能提供高精度预测，但在边缘设备上分布的数据处理时会引发隐私和带宽问题。

Method: Bikelution是一种基于梯度提升树的有效联邦学习（FL）解决方案，旨在解决隐私保护问题同时提供长达六小时的准确中期需求预测。

Result: 在三个真实世界的共享单车系统数据集上的实验表明，Bikelution与基于集中式机器学习的方法相比具有可比性，并且优于当前最先进的方法。

Conclusion: 研究结果突出了隐私意识需求预测的可行性，并概述了联邦学习与集中式机器学习方法之间的权衡。

Abstract: The rapid growth of dockless bike-sharing systems has generated massive spatio-temporal datasets useful for fleet allocation, congestion reduction, and sustainable mobility. Bike demand, however, depends on several external factors, making traditional time-series models insufficient. Centralized Machine Learning (CML) yields high-accuracy forecasts but raises privacy and bandwidth issues when data are distributed across edge devices. To overcome these limitations, we propose Bikelution, an efficient Federated Learning (FL) solution based on gradient-boosted trees that preserves privacy while delivering accurate mid-term demand forecasts up to six hours ahead. Experiments on three real-world BSS datasets show that Bikelution is comparable to its CML-based variant and outperforms the current state-of-the-art. The results highlight the feasibility of privacy-aware demand forecasting and outline the trade-offs between FL and CML approaches.

</details>


### [48] [UrbanFM: Scaling Urban Spatio-Temporal Foundation Models](https://arxiv.org/abs/2602.20677)
*Wei Chen,Yuqian Wu,Junle Chen,Xiaofang Zhou,Yuxuan Liang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的方法UrbanFM，旨在通过解决城市时空数据的异质性、相关性和动态性问题，推动大规模城市时空基础模型的发展。为此，作者们构建了WorldST数据集，引入了MiniST单元，并创建了EvalST基准测试。实验表明，UrbanFM在未见过的城市和任务上展现了出色的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前城市计算领域由于依赖于特定场景的模型而显得碎片化，这些模型往往过度拟合特定区域或任务，限制了其通用性。本文旨在通过开发能够更好地理解城市时空数据的基础模型来解决这一挑战，从而提高模型的泛化能力。

Method: 1. 构建了一个名为WorldST的大规模语料库，统一了来自全球超过100个城市的物理信号如交通流量和速度的数据格式。
2. 引入了MiniST单元，这是一种新的分割机制，可以将连续的时空场离散为可学习的计算单元，以统一基于网格和传感器观测的表示方式。
3. 提出了UrbanFM架构，一种具有有限归纳偏置的简约自注意力架构，用于从大量数据中自主学习动态时空依赖关系。
4. 建立了EvalST，迄今为止最大的城市时空基准测试。

Result: 实验结果表明，UrbanFM能够在未见过的城市和地区以及不同任务上实现显著的零样本泛化性能，这标志着向开发大规模城市时空基础模型迈出了关键一步。

Conclusion: 通过系统地探索扩大规模的对象与方法，本研究成功地提高了城市时空模型的泛化能力，为未来城市科学研究提供了强有力的支持工具。UrbanFM不仅展示了处理城市复杂性的潜力，而且也为进一步研究奠定了坚实的基础。

Abstract: Urban systems, as dynamic complex systems, continuously generate spatio-temporal data streams that encode the fundamental laws of human mobility and city evolution. While AI for Science has witnessed the transformative power of foundation models in disciplines like genomics and meteorology, urban computing remains fragmented due to "scenario-specific" models, which are overfitted to specific regions or tasks, hindering their generalizability. To bridge this gap and advance spatio-temporal foundation models for urban systems, we adopt scaling as the central perspective and systematically investigate two key questions: what to scale and how to scale. Grounded in first-principles analysis, we identify three critical dimensions: heterogeneity, correlation, and dynamics, aligning these principles with the fundamental scientific properties of urban spatio-temporal data. Specifically, to address heterogeneity through data scaling, we construct WorldST. This billion-scale corpus standardizes diverse physical signals, such as traffic flow and speed, from over 100 global cities into a unified data format. To enable computation scaling for modeling correlations, we introduce the MiniST unit, a novel split mechanism that discretizes continuous spatio-temporal fields into learnable computational units to unify representations of grid-based and sensor-based observations. Finally, addressing dynamics via architecture scaling, we propose UrbanFM, a minimalist self-attention architecture designed with limited inductive biases to autonomously learn dynamic spatio-temporal dependencies from massive data. Furthermore, we establish EvalST, the largest-scale urban spatio-temporal benchmark to date. Extensive experiments demonstrate that UrbanFM achieves remarkable zero-shot generalization across unseen cities and tasks, marking a pivotal first step toward large-scale urban spatio-temporal foundation models.

</details>


### [49] [High-Dimensional Robust Mean Estimation with Untrusted Batches](https://arxiv.org/abs/2602.20698)
*Maryam Aliakbarpour,Vladimir Braverman,Yuhan Liu,Junze Yin*

Main category: cs.LG

TL;DR: 本文研究了在存在统计异质性和潜在恶意用户的情况下，如何通过一个两阶段的腐败模型来估计高维均值，并提出了两种基于和方根（SoS）的算法来解决这个问题。这些算法达到了最小最大最优误差率$O(\sqrt{\varepsilon/n} + \sqrt{d/nN} + \sqrt\alpha)$。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决在数据由多个用户贡献且可能存在敌对行为者的情况下，如何准确地从这些数据中恢复出真实分布的均值问题。

Method: 通过提出一种包含两个自然变体的双重腐败模型来处理数据偏差：一是好的批次来自具有均值偏移$\sqrt\alpha$的分布；二是每个好批次中有$\alpha$比例的样本被对手恶意篡改。为应对这种分层腐败情况，提供了两种基于和方根(SoS)的算法。

Result: 所提出的算法实现了最小最大最优误差率$O(\sqrt{\varepsilon/n} + \sqrt{d/nN} + \sqrt\alpha)$，表明虽然异质性$\alpha$代表了一种内在的统计难度，但由于批处理结构提供的内部平均效应，对手用户的影响被$1/\sqrt{n}$因子抑制。

Conclusion: 该研究表明，在面对统计异质性和潜在恶意用户的挑战时，可以通过设计适当的算法有效地估计高维均值，并且利用批处理结构可以减少恶意用户的负面影响。

Abstract: We study high-dimensional mean estimation in a collaborative setting where data is contributed by $N$ users in batches of size $n$. In this environment, a learner seeks to recover the mean $μ$ of a true distribution $P$ from a collection of sources that are both statistically heterogeneous and potentially malicious. We formalize this challenge through a double corruption landscape: an $\varepsilon$-fraction of users are entirely adversarial, while the remaining ``good'' users provide data from distributions that are related to $P$, but deviate by a proximity parameter $α$.
  Unlike existing work on the untrusted batch model, which typically measures this deviation via total variation distance in discrete settings, we address the continuous, high-dimensional regime under two natural variants for deviation: (1) good batches are drawn from distributions with a mean-shift of $\sqrtα$, or (2) an $α$-fraction of samples within each good batch are adversarially corrupted. In particular, the second model presents significant new challenges: in high dimensions, unlike discrete settings, even a small fraction of sample-level corruption can shift empirical means and covariances arbitrarily.
  We provide two Sum-of-Squares (SoS) based algorithms to navigate this tiered corruption. Our algorithms achieve the minimax-optimal error rate $O(\sqrt{\varepsilon/n} + \sqrt{d/nN} + \sqrtα)$, demonstrating that while heterogeneity $α$ represents an inherent statistical difficulty, the influence of adversarial users is suppressed by a factor of $1/\sqrt{n}$ due to the internal averaging afforded by the batch structure.

</details>


### [50] [Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty](https://arxiv.org/abs/2602.20729)
*Xu Wan,Chao Yang,Cheng Yang,Jie Song,Mingyang Sun*

Main category: cs.LG

TL;DR: 提出了一种名为Fuz-RL的新框架，它利用模糊测度来指导安全强化学习，通过创新的模糊贝尔曼算子估计鲁棒价值函数，并证明了解决Fuz-RL问题等同于解决分布鲁棒的安全RL问题，实验表明该方法在不同不确定性条件下提高了安全性和控制性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决实际环境中多种不确定因素对可解释风险评估和稳健决策带来的挑战，从而提高安全强化学习（RL）的表现与安全性。

Method: 提出了Fuz-RL框架，该框架采用模糊测度引导的方法进行安全强化学习；开发了新的模糊贝尔曼算子，利用Choquet积分来估算鲁棒价值函数；从理论上证明了Fuz-RL问题（以约束马尔可夫决策过程CMDP形式）的解决等价于分布鲁棒安全RL问题（以鲁棒CMDP形式）的解决。

Result: 实证分析显示，在safe-control-gym和safety-gymnasium场景下，Fuz-RL能够以无模型方式有效集成现有的安全RL基线方法，显著提升了面对观察、行动及动态中各种类型不确定性时的安全性和控制表现。

Conclusion: Fuz-RL提供了一个有效的途径来增强现实世界应用中的安全强化学习系统的性能与安全性，特别是在处理复杂多样的不确定性方面展现出优越性。

Abstract: Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.

</details>


### [51] [Rethink Efficiency Side of Neural Combinatorial Solver: An Offline and Self-Play Paradigm](https://arxiv.org/abs/2602.20730)
*Zhenxing Xu,Zeyuan Ma,Weidong Bao,Hui Yan,Yan Zheng,Ji Wang*

Main category: cs.LG

TL;DR: 提出了一种名为ECO的新学习范式，用于神经组合优化的高效离线自博弈。该方法通过转换为两阶段离线模式、设计基于Mamba的架构以及采用启发式引导的渐进式启动机制来提高效率和稳定性，并在TSP和CVRP问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有在线学习范式在神经组合优化中的效率低下问题，文章旨在开发一种更高效的离线自我博弈方法，以提升训练过程中的内存使用率和吞吐量。

Method: 1) 通过引入包含监督预热与迭代直接偏好优化(DPO)两个阶段的离线范式实现范式转变；2) 设计了基于Mamba的体系结构以进一步提高离线范式的效率；3) 利用基于启发式的渐进式启动机制来稳定训练过程，确保策略持续改进。

Result: 实验结果表明，在TSP（旅行商问题）和CVRP（容量约束车辆路径问题）任务上，ECO与最新基准相比具有竞争力，特别是在效率方面，如内存利用率和训练吞吐量有显著优势。

Conclusion: ECO作为一种新的学习范式，成功地提高了神经组合优化中离线自博弈的效率，通过特定的设计选择实现了性能上的突破。

Abstract: We propose ECO, a versatile learning paradigm that enables efficient offline self-play for Neural Combinatorial Optimization (NCO). ECO addresses key limitations in the field through: 1) Paradigm Shift: Moving beyond inefficient online paradigms, we introduce a two-phase offline paradigm consisting of supervised warm-up and iterative Direct Preference Optimization (DPO); 2) Architecture Shift: We deliberately design a Mamba-based architecture to further enhance the efficiency in the offline paradigm; and 3) Progressive Bootstrapping: To stabilize training, we employ a heuristic-based bootstrapping mechanism that ensures continuous policy improvement during training. Comparison results on TSP and CVRP highlight that ECO performs competitively with up-to-date baselines, with significant advantage on the efficiency side in terms of memory utilization and training throughput. We provide further in-depth analysis on the efficiency, throughput and memory usage of ECO. Ablation studies show rationale behind our designs.

</details>


### [52] [Transcoder Adapters for Reasoning-Model Diffing](https://arxiv.org/abs/2602.20904)
*Nathan Hu,Jake Ward,Thomas Icard,Christopher Potts*

Main category: cs.LG

TL;DR: 本研究引入了转码适配器，以学习模型在微调前后MLP计算差异的可解释近似。通过将此技术应用于Qwen2.5-Math-7B及其推理精馏变体DeepSeek-R1-Distill-Qwen-7B，发现适配器能够忠实再现目标模型内部计算和下一个令牌预测，并且在推理基准测试中通常能恢复因推理微调带来的50-90%准确率增益。此外，研究还深入探讨了犹豫令牌生成行为，指出仅约2.4%的适配器特征（总计5,600个）对此功能至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型越来越普遍，但关于推理训练对模型内部机制影响的研究仍然不足。这项工作的动机是更好地理解这一过程，并开发一种新的方法来近似表示微调前后模型内部变化。

Method: 研究人员开发了一种名为转码适配器的新技术，该技术旨在学习模型微调前后多层感知机(MLP)计算差异的一种可解释近似。他们将这种方法应用于分析Qwen2.5-Math-7B与其经过推理精馏后的版本DeepSeek-R1-Distill-Qwen-7B之间的区别。

Result: 实验表明，所学到的适配器能够很好地反映目标模型内部计算及下一个词预测情况，在进行推理任务时，这些适配器能够匹配推理模型的回答长度，并通常可以恢复由于进行推理微调而获得的50-90%准确性提升。进一步分析显示，大约只有8%的适配器特征直接与推理行为相关联；特别是对于犹豫令牌（如“等待”）的产生，仅有约2.4%（总共5,600个）的适配器特征发挥了关键作用。

Conclusion: 总体而言，研究结果为理解推理训练提供了新见解，并暗示转码适配器可能有助于更广泛地研究微调过程。

Abstract: While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to the target model's internal computation and next-token predictions. When evaluated on reasoning benchmarks, adapters match the reasoning model's response lengths and typically recover 50-90% of the accuracy gains from reasoning fine-tuning. Adapter features are sparsely activating and interpretable. When examining adapter features, we find that only ~8% have activating examples directly related to reasoning behaviors. We deeply study one such behavior -- the production of hesitation tokens (e.g., "wait"). Using attribution graphs, we trace hesitation to only ~2.4% of adapter features (5.6k total) performing one of two functions. These features are necessary and sufficient for producing hesitation tokens; removing them reduces response length, often without affecting accuracy. Overall, our results provide insight into reasoning training and suggest transcoder adapters may be useful for studying fine-tuning more broadly.

</details>


### [53] [From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2602.20911)
*Ruiqi Liu,Boyu Diao,Hangda Liu,Zhulin An,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: 提出了一种新的方法，即语义引导的自适应专家森林（SAEF），通过将适配器组织成结构化的层次来改进类增量学习中的知识共享问题。实验表明，SAEF在多个基准数据集上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法虽然能够防止遗忘旧的知识，但未能充分利用任务间的关系来进行更有效的知识共享。

Method: SAEF首先基于任务之间的语义关系将它们分组为概念集群，然后在每个集群内通过合并相似任务的适配器来构建平衡的专家树。在推理时，SAEF会根据输入激活一组相关的专家，并结合这些被激活专家的输出做出最终预测，其中每个专家的贡献度由其置信水平决定。

Result: 实验结果表明，所提出的SAEF方法在几个基准数据集上的表现优于现有技术。

Conclusion: 通过引入一种结构化的方式来组织和利用不同任务间的知识，SAEF为解决类增量学习挑战提供了一个有效途径。

Abstract: Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.

</details>


### [54] [Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels](https://arxiv.org/abs/2602.20932)
*Anupam Sharma,Harish Katti,Prajwal Singh,Shanmuganathan Raman,Krishna Miyapuram*

Main category: cs.LG

TL;DR: 本研究探讨了EEG是否能捕捉到跨越多个层次的对象表示，并提出了一种基于情景的分析方法，使用WordNet进行层次感知的情景采样。通过在PEERS数据集上应用此方法，研究表明模型在分类较高层次的类别时表现更好，强调了抽象深度作为EEG解码的一个未被充分探索的维度。


<details>
  <summary>Details</summary>
Motivation: 先前的研究已经探索了基于EEG的对象或概念分类，但这些研究通常局限于短暂呈现的图像或视频刺激的被动观看，且类别数量有限。由于EEG具有低信噪比的特点，在大量类别中识别精细表征仍然具有挑战性。这项工作旨在调查EEG是否能够捕获跨越多个层级的对象表示，并提出了一个基于情景（episode）的分析框架来评估机器学习模型在不同但相关分类任务上的表现。

Method: 采用了基于WordNet的层次感知情景采样方法生成具有多样化层级的情景，而非依赖于固定或随机抽取的相同数量类别的传统方式。此外，本研究还介绍了迄今为止EEG领域最大的情景框架之一，用于从PEERS数据集中检测观察文本。该数据集包含来自264名参与者执行受控认知任务时收集的931,538个EEG样本，覆盖了1,610个对象标签。

Result: 研究发现，当分类类别来源于层次结构中的更高级别时，模型的表现趋于改善，表明对抽象级别的敏感性。这为理解EEG信号如何编码关于物体的信息提供了新的视角，并指出抽象深度是EEG解码研究中值得进一步探索的方向。

Conclusion: 本研究表明，通过采用考虑语义抽象水平的方法可以提高EEG信号分类性能，揭示了抽象深度对于EEG解码的重要性，并为未来在此方向上的研究提供了动力。

Abstract: An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations may exist. In this work, we investigate whether EEG captures object representations across multiple hierarchical levels, and propose episodic analysis, in which a Machine Learning (ML) model is evaluated across various, yet related, classification tasks (episodes). Unlike prior episodic EEG studies that rely on fixed or randomly sampled classes of equal cardinality, we adopt hierarchy-aware episode sampling using WordNet to generate episodes with variable classes of diverse hierarchy. We also present the largest episodic framework in the EEG domain for detecting observed text from EEG signals in the PEERS dataset, comprising $931538$ EEG samples under $1610$ object labels, acquired from $264$ human participants (subjects) performing controlled cognitive tasks, enabling the study of neural dynamics underlying perception, decision-making, and performance monitoring.
  We examine how the semantic abstraction level affects classification performance across multiple learning techniques and architectures, providing a comprehensive analysis. The models tend to improve performance when the classification categories are drawn from higher levels of the hierarchy, suggesting sensitivity to abstraction. Our work highlights abstraction depth as an underexplored dimension of EEG decoding and motivates future research in this direction.

</details>


### [55] [Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers](https://arxiv.org/abs/2602.20937)
*Akshita Gupta,Marieme Ngom,Sam Foreman,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，用于为包括AdamW、ADOPT、LAMB、Sophia、Shampoo和Muon在内的更广泛优化器推导μP。通过在多个基准模型上的实现，展示了对于上述优化器，在增加模型宽度时的学习率零样本迁移，并提供了关于这些优化器深度缩放参数化的实证见解。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型的训练加速与扩展依赖于自适应一阶和二阶优化方法的各种变体。然而，这些优化例程的表现高度依赖于超参数的选择，这对大规模模型来说是计算成本高昂的调整过程。而最大更新参数化（μP）旨在使最优超参数独立于模型大小，允许将较小模型上调整好的超参数转移到较大目标模型的训练中。尽管对SGD和Adam有不错的结果，但为其他优化器推导μP仍具挑战性，因为底层张量编程方法难以掌握。

Method: 基于最近引入光谱条件作为张量程序替代方案的研究工作，作者们提出了一个新框架来为更广泛的优化器（如AdamW, ADOPT, LAMB, Sophia, Shampoo及Muon）推导μP。该方法不仅限于特定类型的优化算法，而是试图提供一种通用的方法论以支持不同种类的优化技术。

Result: 研究者们在多个基准模型上实现了他们提出的μP推导，并成功地展示了当模型宽度增加时，对于所讨论的优化器而言，学习率可以实现零样本迁移。此外，还提供了关于这些优化器深度缩放参数化的实证观察。

Conclusion: 本研究表明，通过采用新提出的框架，能够有效地为一系列流行的优化器开发出最大更新参数化规则，从而促进了从较小规模实验到更大规模实际应用中超参数的有效转移。这为未来设计更加高效且可扩展的语言模型训练流程奠定了基础。

Abstract: Several variations of adaptive first-order and second-order optimization methods have been proposed to accelerate and scale the training of large language models. The performance of these optimization routines is highly sensitive to the choice of hyperparameters (HPs), which are computationally expensive to tune for large-scale models. Maximal update parameterization $(μ$P$)$ is a set of scaling rules which aims to make the optimal HPs independent of the model size, thereby allowing the HPs tuned on a smaller (computationally cheaper) model to be transferred to train a larger, target model. Despite promising results for SGD and Adam, deriving $μ$P for other optimizers is challenging because the underlying tensor programming approach is difficult to grasp. Building on recent work that introduced spectral conditions as an alternative to tensor programs, we propose a novel framework to derive $μ$P for a broader class of optimizers, including AdamW, ADOPT, LAMB, Sophia, Shampoo and Muon. We implement our $μ$P derivations on multiple benchmark models and demonstrate zero-shot learning rate transfer across increasing model width for the above optimizers. Further, we provide empirical insights into depth-scaling parameterization for these optimizers.

</details>


### [56] [Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2602.20947)
*Thorbjørn Mosekjær Iversen,Zebin Duan,Frederik Hagelskjær*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于内核的方法——Wilson Score Kernel Density Classification，用于估计二元分类中的置信边界。该方法在选择性分类的背景下进行了评估，展示了与高斯过程分类相似的性能，但计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 随着基于深度学习的二元分类器的性能和易用性的显著提高，自动化关键检查任务成为可能。然而，在关键操作中应用二元分类器需要依赖于可靠置信边界的估计，以确保系统性能达到一定的统计显著性水平。

Method: 研究者们提出了Wilson Score Kernel Density Classification方法，这是一种新颖的基于内核的方法，用于在二元分类中估计置信边界。其核心是Wilson Score Kernel Density Estimator，它是一种函数估计器，用于估计具有条件变化成功概率的二项实验中的置信边界。

Result: 该方法在四个不同数据集上进行了评估，展示了它可以作为任何特征提取器（包括视觉基础模型）的分类头部使用。结果显示，所提方法与高斯过程分类相比表现相近，但计算复杂度较低。

Conclusion: Wilson Score Kernel Density Classification为二元分类提供了一种有效且高效的置信边界估计方法，适用于包括关键操作在内的多种场景。

Abstract: The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.

</details>


### [57] [Does Order Matter : Connecting The Law of Robustness to Robust Generalization](https://arxiv.org/abs/2602.20971)
*Himadri Mandal,Vishnu Varadarajan,Jaee Ponde,Aritra Das,Mihir More,Debayan Gupta*

Main category: cs.LG

TL;DR: The paper connects the law of robustness, which requires overparameterization for robust interpolation, with robust generalization by introducing a new notion of robust generalization error and providing a lower bound on the expected Rademacher complexity. The findings suggest that to achieve low robust generalization error, the Lipschitz constant must be within a certain range, and this is consistent with the scaling predicted by Wu et al. (2023) for the MNIST dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address an open problem posed by Bubeck and Sellke (2021) regarding the connection between the law of robustness and robust generalization, and to understand the implications of this connection for the required Lipschitz constant and model overparameterization.

Method: The authors introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. They also conduct experiments to test the scaling of the lower-bound Lipschitz constant with dataset size and model capacity, using the MNIST dataset as a case study.

Result: The results show that the lower-bound Lipschitz constant scales in the order predicted by Wu et al. (2023), and that to obtain low robust generalization error, the Lipschitz constant must lie within a specific range, with the allowable perturbation radius being linked to the Lipschitz scale.

Conclusion: The paper concludes that there is a clear connection between the law of robustness and robust generalization, and that the Lipschitz constant required for smooth interpolation does not change in order when considering robust generalization. The empirical evidence supports the theoretical bounds, indicating that the robustness of a model can be improved by controlling its Lipschitz constant.

Abstract: Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifically, we introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. Our bounds recover the $Ω(n^{1/d})$ regime of Wu et al.\ (2023) and show that, up to constants, robust generalization does not change the order of the Lipschitz constant required for smooth interpolation. We conduct experiments to probe the predicted scaling with dataset size and model capacity, testing whether empirical behavior aligns more closely with the predictions of Bubeck and Sellke (2021) or Wu et al.\ (2023). For MNIST, we find that the lower-bound Lipschitz constant scales on the order predicted by Wu et al.\ (2023). Informally, to obtain low robust generalization error, the Lipschitz constant must lie in a range that we bound, and the allowable perturbation radius is linked to the Lipschitz scale.

</details>


### [58] [Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning](https://arxiv.org/abs/2602.21020)
*Antoine Bergerault,Volkan Cevher,Negar Mehr*

Main category: cs.LG

TL;DR: 本文探讨了多智能体模仿学习（MA-IL）中学习接近纳什均衡策略的困难性，并提出通过假设专家均衡中的策略优势来克服这些挑战，为具有行为克隆误差$ε_{\text{BC}}$的情况提供了一个纳什模仿差距$\mathcal{O}\left(nε_{\text{BC}}/(1-γ)^2\right)$。


<details>
  <summary>Details</summary>
Motivation: 多智能体模仿学习的目标是从专家演示中学习互动领域的最优策略。然而，对于离线MA-IL来说，缺乏对所学策略与纳什均衡之间差距的描述。因此，研究者们试图探索在一般$n$玩家马尔可夫博弈中学习低被利用策略的不可能性和难度结果，并寻找克服这些难题的方法。

Method: 研究者通过给出即使精确匹配度量也失败的例子以及展示给定固定匹配度量误差下表征纳什差距的新难度结果来进行论证。接着，他们展示了如何使用关于专家均衡的战略优势假设来克服这些挑战，特别是针对主导策略专家均衡的情形，提出了一个新的最佳响应连续性的概念，并认为这是标准正则化技术隐含鼓励的。

Result: 对于具有行为克隆误差$ε_{\text{BC}}$的主导策略专家均衡情况，研究给出了一个纳什模仿差距$\mathcal{O}\left(nε_{\text{BC}}/(1-γ)^2\right)$，其中$γ$是折扣因子。此外，还推广了这一结果，提出了最佳响应连续性的新概念。

Conclusion: 尽管在一般的$n$玩家马尔可夫游戏中学习低被利用策略存在固有的困难，但通过采用关于专家均衡的战略优势假设，可以克服这些挑战并量化纳什模仿差距。此外，研究提出的最佳响应连续性概念可能有助于进一步理解如何通过正则化技术改善学习过程。

Abstract: Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $ε_{\text{BC}}$, this provides a Nash imitation gap of $\mathcal{O}\left(nε_{\text{BC}}/(1-γ)^2\right)$ for a discount factor $γ$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.

</details>


### [59] [T1: One-to-One Channel-Head Binding for Multivariate Time-Series Imputation](https://arxiv.org/abs/2602.21043)
*Dongik Park,Hyunwoo Ryu,Suahn Bae,Keondo Park,Hyung-Sin Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为T1的CNN-Transformer混合架构，通过通道-头绑定机制实现选择性信息传递，从而在多变量时间序列中实现鲁棒填补。实验显示，在11个基准数据集上，T1相比次优基线平均减少了46%的MSE，尤其在极端稀疏条件下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多变量时间序列中的缺失值时遇到挑战，特别是在面对多样化的缺失模式和严重的缺失情况下。这些方法往往无法同时有效地从每个变量内的稀疏观测中提取时间模式以及在变量间有选择地传递信息，导致重建误差增大。

Method: 提出的方法是T1（基于一对一通道-头绑定的时间序列填补），这是一种结合了CNN与Transformer特性的新型架构。它通过一种称为“通道-头绑定”的机制建立CNN通道与注意力头之间的一对一对应关系，允许当某些时间模式因缺失而被破坏时，其相应的注意力路径可以根据剩余可观察模式自适应地下调权重，同时通过未受影响的通道保持可靠的跨变量连接。

Result: 实验结果表明，在11个基准数据集上，T1达到了最先进的性能，相比于次优基线平均降低了46%的均方误差(MSE)，尤其在极高稀疏度(70%缺失率)的情况下表现尤为出色。此外，该模型无需重新训练即可泛化至未见过的缺失模式，并且在整个数据集中使用一致的超参数配置。

Conclusion: T1通过创新的通道-头绑定机制解决了多变量时间序列中缺失值填补的问题，显著提高了不同条件下尤其是高稀疏度下的填补准确性。

Abstract: Imputing missing values in multivariate time series remains challenging, especially under diverse missing patterns and heavy missingness. Existing methods suffer from suboptimal performance as corrupted temporal features hinder effective cross-variable information transfer, amplifying reconstruction errors. Robust imputation requires both extracting temporal patterns from sparse observations within each variable and selectively transferring information across variables--yet current approaches excel at one while compromising the other. We introduce T1 (Time series imputation with 1-to-1 channel-head binding), a CNN-Transformer hybrid architecture that achieves robust imputation through Channel-Head Binding--a mechanism creating one-to-one correspondence between CNN channels and attention heads. This design enables selective information transfer: when missingness corrupts certain temporal patterns, their corresponding attention pathways adaptively down-weight based on remaining observable patterns while preserving reliable cross-variable connections through unaffected channels. Experiments on 11 benchmark datasets demonstrate that T1 achieves state-of-the-art performance, reducing MSE by 46% on average compared to the second-best baseline, with particularly strong gains under extreme sparsity (70% missing ratio). The model generalizes to unseen missing patterns without retraining and uses a consistent hyperparameter configuration across all datasets. The code is available at https://github.com/Oppenheimerdinger/T1.

</details>


### [60] [Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2602.21072)
*Zhangjie Xia,Yu Yang,Pan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为LoDADA的新方法，通过识别和利用局部动态差异来改进跨域数据的重用策略，从而在不同环境下的离线强化学习任务中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习方法处理动态不匹配问题时，要么基于整个状态空间做出全局假设，要么采用逐点的数据过滤，这两种方式都可能忽略掉领域间的局部相似性或导致计算成本过高。研究旨在开发一种更精细且可扩展的数据选择策略，以有效利用源数据中的有用信息。

Method: 提出了Localized Dynamics-Aware Domain Adaptation (LoDADA) 方法，该方法首先对来自源和目标数据集的状态转移进行聚类，并通过领域判别估计集群级别的动态差异度。对于那些具有较小差异度的集群，其包含的状态转移将被保留；而对于较大差异度的，则会被过滤掉。

Result: 实验结果表明，在面对各种全局与局部动态变化的情况下，LoDADA能够持续超越当前最先进off-dynamics离线RL方法的表现，更有效地利用了局部分布不匹配的信息。

Conclusion: 通过引入一种新的、考虑局部动态差异的数据选择机制，LoDADA为解决跨域离线强化学习中的动态不匹配问题提供了有效的解决方案。

Abstract: Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.

</details>


### [61] [Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads](https://arxiv.org/abs/2602.21081)
*Huy Trinh,Rebecca Ma,Zeqi Yu,Tahsin Reza*

Main category: cs.LG

TL;DR: 本研究探索了使用DeepSpeed框架来提高视觉转换器(ViTs)的可扩展性和性能，通过在不同GPU配置和数据集上评估训练效率，并调整软件参数以识别影响分布式训练性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 视觉转换器（ViTs）在图像处理任务中表现出色，但其可扩展性受到计算和内存需求的限制，特别是在参数众多的大规模模型中。为解决这一问题，本研究旨在利用高效的分布式训练框架DeepSpeed来增强ViTs的可扩展性和性能表现。

Method: 研究方法包括跨多种GPU配置对CIFAR-10和CIFAR-100等数据集进行节点内及节点间训练效率评估，同时考察分布式数据并行对训练速度、通信开销以及整体可扩展性的影响；此外，还通过系统地改变批次大小和梯度累积等软件参数，来确定影响分布式训练表现的主要因素。

Result: 实验结果为将DeepSpeed应用于与图像相关的任务提供了基础依据，并为进一步理解DeepSpeed的局限性及探索优化视觉转换器分布式训练流程的策略指明了方向。

Conclusion: 通过本研究发现，采用DeepSpeed可以有效提升视觉转换器的训练效率与可扩展性，未来工作将继续深入探究如何克服现有挑战并进一步优化针对视觉转换器的分布式训练方案。

Abstract: Vision Transformers (ViTs) have demonstrated remarkable potential in image processing tasks by utilizing self-attention mechanisms to capture global relationships within data. However, their scalability is hindered by significant computational and memory demands, especially for large-scale models with many parameters. This study aims to leverage DeepSpeed, a highly efficient distributed training framework that is commonly used for language models, to enhance the scalability and performance of ViTs. We evaluate intra- and inter-node training efficiency across multiple GPU configurations on various datasets like CIFAR-10 and CIFAR-100, exploring the impact of distributed data parallelism on training speed, communication overhead, and overall scalability (strong and weak scaling). By systematically varying software parameters, such as batch size and gradient accumulation, we identify key factors influencing performance of distributed training. The experiments in this study provide a foundational basis for applying DeepSpeed to image-related tasks. Future work will extend these investigations to deepen our understanding of DeepSpeed's limitations and explore strategies for optimizing distributed training pipelines for Vision Transformers.

</details>


### [62] [Ski Rental with Distributional Predictions of Unknown Quality](https://arxiv.org/abs/2602.21104)
*Qiming Cui,Michael Dinitz*

Main category: cs.LG

TL;DR: 本文在分布预测的框架下重新审视了ski rental问题，提出了一种新算法，该算法在给定预测误差的情况下能够实现几乎最优的成本损失。同时证明了此算法的一致性和鲁棒性，并通过多种下界分析展示了其结果的紧致性。


<details>
  <summary>Details</summary>
Motivation: 作者旨在从分布预测的角度出发，在'带有预测的算法'框架中重新探讨ski rental这一核心在线问题。相比于简单地预测滑雪天数，将预测视为关于滑雪天数的一个分布更为自然且可能更强大。

Method: 设计了一种新的算法，当真实滑雪天数从某个未知的真实分布p中抽取时，保证了算法成本最多为OPT加上一个与购买成本b及预测误差{eta}相关的项。这里OPT指的是对于真实分布p而言最优策略的预期成本。

Result: 展示了所提算法的主要结果是存在一种算法，其预期成本最多为OPT + O(min(max({eta}, 1) * sqrt(b), b log b))。此外，还提供了多种下界来证明这些上界的紧致性。

Conclusion: 提出的算法不仅在预测误差小时具有一致性O(sqrt(b))，而且在预测误差大时具有鲁棒性O(b log b)，并且不需要知道预测误差的确切值。此外，文中提供的下界表明，这种一致性/鲁棒性的权衡以及特定损失函数均无法显著改进。

Abstract: We revisit the central online problem of ski rental in the "algorithms with predictions" framework from the point of view of distributional predictions. Ski rental was one of the first problems to be studied with predictions, where a natural prediction is simply the number of ski days. But it is both more natural and potentially more powerful to think of a prediction as a distribution p-hat over the ski days. If the true number of ski days is drawn from some true (but unknown) distribution p, then we show as our main result that there is an algorithm with expected cost at most OPT + O(min(max({eta}, 1) * sqrt(b), b log b)), where OPT is the expected cost of the optimal policy for the true distribution p, b is the cost of buying, and {eta} is the Earth Mover's (Wasserstein-1) distance between p and p-hat. Note that when {eta} < o(sqrt(b)) this gives additive loss less than b (the trivial bound), and when {eta} is arbitrarily large (corresponding to an extremely inaccurate prediction) we still do not pay more than O(b log b) additive loss. An implication of these bounds is that our algorithm has consistency O(sqrt(b)) (additive loss when the prediction error is 0) and robustness O(b log b) (additive loss when the prediction error is arbitrarily large). Moreover, we do not need to assume that we know (or have any bound on) the prediction error {eta}, in contrast with previous work in robust optimization which assumes that we know this error.
  We complement this upper bound with a variety of lower bounds showing that it is essentially tight: not only can the consistency/robustness tradeoff not be improved, but our particular loss function cannot be meaningfully improved.

</details>


### [63] [SOM-VQ: Topology-Aware Tokenization for Interactive Generative Models](https://arxiv.org/abs/2602.21133)
*Alessandro Londei,Denise Lanzieri,Matteo Benati*

Main category: cs.LG

TL;DR: 本文提出了一种结合矢量量化和自组织映射的分词方法SOM-VQ，该方法能够学习具有显式低维拓扑结构的离散码本。通过保持邻域结构，使得在学习到的网格上相邻的令牌对应于语义相似的状态，从而可以直接对潜在空间进行几何操作。SOM-VQ不仅产生了更易于学习的令牌序列，还提供了代码空间中可导航的明确几何形状，并且支持直观的人机交互控制。


<details>
  <summary>Details</summary>
Motivation: 现有的向量量化表示虽然强大，但缺乏令牌空间中的语义结构，这限制了人类可解释性的控制能力。为了解决这一问题，研究者们旨在开发一种既能保留矢量量化优势又能增强语义可控性的新方法。

Method: SOM-VQ方法结合了矢量量化技术与自组织映射(Self-Organizing Maps, SOM)，通过引入拓扑感知更新来维护邻居结构，确保学得的网格上彼此靠近的令牌代表语义相近的状态。这样做的目的是为了能够在潜空间内直接执行几何变换，同时让生成过程更加直观地受到用户指导。

Result: 实验结果表明，在评估领域内，SOM-VQ能产生更利于学习的令牌序列，并且为代码空间提供了一个明确可导航的几何形态。特别地，这种拓扑结构使得用户可以通过调整令牌空间中的距离来进行生成导向，实现了无需帧级约束条件下的语义对齐。此外，研究者们还展示了如何利用简单的基于网格抽样方法实现从参考序列出发的受控偏离与收敛。

Conclusion: SOM-VQ作为一种通用框架，为音乐、手势及其他交互式生成领域提供了可解释性强的离散表示方案。它不仅增强了生成模型的语义可控性，也促进了人机交互过程中对生成内容的直观操控。

Abstract: Vector-quantized representations enable powerful discrete generative models but lack semantic structure in token space, limiting interpretable human control. We introduce SOM-VQ, a tokenization method that combines vector quantization with Self-Organizing Maps to learn discrete codebooks with explicit low-dimensional topology. Unlike standard VQ-VAE, SOM-VQ uses topology-aware updates that preserve neighborhood structure: nearby tokens on a learned grid correspond to semantically similar states, enabling direct geometric manipulation of the latent space. We demonstrate that SOM-VQ produces more learnable token sequences in the evaluated domains while providing an explicit navigable geometry in code space. Critically, the topological organization enables intuitive human-in-the-loop control: users can steer generation by manipulating distances in token space, achieving semantic alignment without frame-level constraints. We focus on human motion generation - a domain where kinematic structure, smooth temporal continuity, and interactive use cases (choreography, rehabilitation, HCI) make topology-aware control especially natural - demonstrating controlled divergence and convergence from reference sequences through simple grid-based sampling. SOM-VQ provides a general framework for interpretable discrete representations applicable to music, gesture, and other interactive generative domains.

</details>


### [64] [Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma](https://arxiv.org/abs/2602.21168)
*Jingya Cheng,Alaleh Azhir,Jiazi Tian,Hossein Estiri*

Main category: cs.LG

TL;DR: 本文提出了一种顺序反事实框架，该框架考虑了电子健康记录中的时间依赖性，区分了不可变特征和可控特征，并展示了干预如何随着时间传播。通过对2,723名COVID-19患者的数据应用，表明在考虑慢性条件时，使用朴素方法将需要生物学上不可能的反事实情况。此外，还识别出一个心脏肾脏级联反应，并说明只有顺序反事实才能捕捉到这种时间上的传播效果。


<details>
  <summary>Details</summary>
Motivation: 临床医生希望通过反事实推理来探讨患者的“如果…会怎样”问题，但标准的方法假设特征独立且同时可修改，这些假设在纵向临床数据中被违反。为了更准确地反映实际情况，特别是考虑到时间因素对于电子健康记录的重要性，研究者们开发了新的框架。

Method: 引入了顺序反事实框架，该框架通过区分不可改变的特征（如慢性诊断）与可控制的特征（如实验室数值），并建模干预措施如何随时间传播，从而尊重电子健康记录中的时间依赖性。

Result: 当应用于2,723名COVID-19患者（包括383例长期新冠导致的心力衰竭病例及对照组）时，发现使用传统方法下，高达38%至67%具有慢性疾病的患者需要生物学上不可能实现的反事实情境。此外，研究还揭示了一个涉及慢性肾病、急性肾损伤以及心力衰竭之间相互作用的心脏肾脏级联效应，其相对风险分别为2.27和1.19。

Conclusion: 顺序反事实框架能够提供更具生物合理性的临床见解，转变了传统的反事实解释方式，从单纯考虑某个特征的变化转向思考早期干预及其后续影响的问题。

Abstract: Counterfactual inference enables clinicians to ask "what if" questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -> AKI -> HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from "what if this feature were different?" to "what if we had intervened earlier, and how would that propagate forward?" --  yielding clinically actionable insights grounded in biological plausibility.

</details>


### [65] [The Diffusion Duality, Chapter II: $Ψ$-Samplers and Efficient Curriculum](https://arxiv.org/abs/2602.21185)
*Justin Deschenaux,Caglar Gulcehre,Subham Sekhar Sahoo*

Main category: cs.LG

TL;DR: 本文提出了一种针对离散扩散的预测-校正（PC）采样器家族，该方法不仅适用于任意噪声过程，并且在语言和图像建模上优于传统的祖先采样。此外，还开发了一个内存高效的高斯松弛训练阶段课程，减少了训练时间和内存使用，同时保持了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的均匀状态离散扩散模型虽然在少量步骤生成和引导方面表现出色，但随着步骤数增加，其采样质量趋于平稳。因此，研究者们旨在开发一种新的采样器来解决这个问题，并提高模型在更多步骤下的表现。

Method: 引入了一类针对离散扩散的预测-校正（PC）采样器，能够应用于任意噪声过程中；发展了一个内存高效的训练课程用于高斯松弛训练阶段，以减少资源消耗而不牺牲性能。

Result: 所提出的PC采样器在OpenWebText数据集上的生成困惑度更低、CIFAR10上的FID/IS分数更好；新开发的训练课程相比Duo方法减少了25%的训练时间及33%的内存使用。

Conclusion: 通过引入新型PC采样器，研究挑战了Masked扩散是基于扩散的语言建模未来趋势的观点。这项工作不仅改进了采样过程，也优化了训练效率。

Abstract: Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2

</details>


### [66] [Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training](https://arxiv.org/abs/2602.21189)
*Anas Barakat,Souradip Chakraborty,Khushbu Pahwa,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: 本文探讨了Pass@k作为可验证大型语言模型任务性能指标时，直接优化Pass@k可能带来的Pass@1性能下降的问题，并通过理论分析和实验展示了这种权衡的根源。


<details>
  <summary>Details</summary>
Motivation: 研究发现，在直接优化Pass@k时经常会出现一种权衡：即随着Pass@k的提高，Pass@1反而会下降。鉴于Pass@1在实际操作中往往是一个硬性限制条件，因此理解这一现象背后的机制变得尤为重要。

Method: 采用理论分析的方法来描述当进行Pass@k策略优化时，由于提示干扰导致梯度冲突从而可能降低Pass@1的情况；同时使用大规模语言模型对数学推理任务进行了实验验证。

Result: 研究表明，Pass@$k$策略梯度可以与Pass@1梯度相冲突，原因是Pass@$k$优化隐式地重新分配了向低成功率提示的比例；对于所谓的负干扰提示来说，它们权重的增加可能会使Pass@k更新方向偏离Pass@1方向。

Conclusion: 本研究揭示了直接优化Pass@k可能导致Pass@1性能下降的原因，并提供了理论支持及实验证据，有助于更好地理解和解决这一重要问题。

Abstract: Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.

</details>


### [67] [Statistical Query Lower Bounds for Smoothed Agnostic Learning](https://arxiv.org/abs/2602.21191)
*Ilias Diakonikolas,Daniel M. Kane*

Main category: cs.LG

TL;DR: 本文研究了平滑不可知学习的复杂性，特别是在线性分割下子高斯分布的不可知学习。主要发现是提出了统计查询（SQ）下界，证明了目前已知的上界几乎是最佳可能的。


<details>
  <summary>Details</summary>
Motivation: 研究者们想要探索在输入数据有轻微高斯扰动的情况下，学习器与目标类别中最佳分类器竞争时的学习复杂度。

Method: 通过线性规划对偶找到一个矩匹配困难分布，并据此提出统计查询算法的下界。

Result: 证明了对于半空间的平滑不可知学习，任何统计查询算法都需要达到d^(Ω(1/σ^2 + log(1/ε)))的复杂度，这几乎与已知的最佳上界相匹配。

Conclusion: L_1-多项式回归应用于函数的平滑版本已经接近最优解；对于半空间类，给出了低阶逼近多项式的下界。

Abstract: We study the complexity of smoothed agnostic learning, recently introduced by~\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\tilde{O}(1/σ^2) \log(1/ε)}$, where $σ$ is the smoothing parameter and $ε$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Ω(1/σ^{2}+\log(1/ε))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.

</details>


### [68] [Test-Time Training with KV Binding Is Secretly Linear Attention](https://arxiv.org/abs/2602.21204)
*Junchen Liu,Sven Elflein,Or Litany,Zan Gojcic,Ruilong Li*

Main category: cs.LG

TL;DR: 本文重新定义了测试时训练（TTT）的概念，将其视为一种学习型线性注意力机制而非简单的记忆过程。这一视角不仅解释了之前令人困惑的模型行为，还带来了架构简化、效率提升等实际好处。


<details>
  <summary>Details</summary>
Motivation: 作者通过分析发现，现有的测试时训练（TTT）及其KV绑定作为序列建模层被广泛认为是一种在线元学习形式，在测试时记忆键值映射的观点与观察到的现象不符。基于这些发现，作者决定重新审视TTT的公式化表达。

Method: 研究者提出将一大类TTT架构表达为一种学习型线性注意力操作符的形式，并据此对TTT进行了重新表述。

Result: 这种新视角不仅解释了之前难以理解的模型行为，还支持了架构上的简化、完全并行化的实现以提高效率，同时保持性能不变，并且能够将多种TTT变体系统地归约为标准线性注意力形式。

Conclusion: 总的来说，这项工作改变了人们对TTT的理解，不再将其视作测试时的记忆过程，而是看作具有增强表示能力的学习型线性注意力机制。

Abstract: Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [69] [Mitigating "Epistemic Debt" in Generative AI-Scaffolded Novice Programming using Metacognitive Scripts](https://arxiv.org/abs/2602.20206)
*Sreecharan Sankaranarayanan*

Main category: cs.SE

TL;DR: The study investigates the impact of using AI in programming education, particularly focusing on how unrestricted use of AI can lead to 'Epistemic Debt' and create 'Fragile Experts' who lack deep understanding. A between-subjects experiment with 78 participants showed that while unrestricted AI users matched the productivity of those with scaffolded AI support, they performed significantly worse (77% failure rate) in a follow-up task without AI, compared to 39% for the scaffolded group. The findings suggest that incorporating metacognitive friction into learning systems is crucial for ensuring students truly learn and understand, not just produce code.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to explore the potential risks associated with the democratization of Large Language Models (LLMs) in programming, specifically the creation of 'Vibe Coding,' where novice programmers focus more on semantic intent than syntactic implementation. The concern is that, without proper educational guidance, such an approach might hinder cognitive skill development by encouraging the outsourcing of intrinsic cognitive load necessary for building foundational knowledge.

Method: To address the research question, a between-subjects experiment was conducted involving 78 participants, categorized as 'AI-Native' learners. These participants were divided into three groups: Manual (control), Unrestricted AI (allowing direct outsourcing of tasks to AI), and Scaffolded AI (using a custom IDE plugin with an 'Explanation Gate' feature to enforce a 'Teach-Back' protocol before integrating AI-generated code). The experiment aimed to measure the effects of these different conditions on learning and retention.

Result: The results indicated that while the Unrestricted AI group matched the productivity of the Scaffolded AI group, they demonstrated a significant drop in performance (77% failure rate) when faced with a maintenance task during an 'AI-Blackout' period, compared to only 39% for the Scaffolded group. This suggests that the unrestricted use of AI may lead to a superficial understanding of the material, whereas the scaffolded approach fosters deeper learning and better retention.

Conclusion: The conclusion drawn from this study highlights the importance of incorporating pedagogical guardrails, such as metacognitive friction, within AI-assisted learning environments. Without such measures, there's a risk of creating 'Fragile Experts' - individuals who can produce functional code but lack the fundamental understanding needed for long-term success and the ability to maintain or modify their work without AI assistance.

Abstract: The democratization of Large Language Models (LLMs) has given rise to ``Vibe Coding," a workflow where novice programmers prioritize semantic intent over syntactic implementation. While this lowers barriers to entry, we hypothesize that without pedagogical guardrails, it is fundamentally misaligned with cognitive skill acquisition. Drawing on the distinction between Cognitive Offloading and Cognitive Outsourcing, we argue that unrestricted AI encourages novices to outsource the Intrinsic Cognitive Load required for schema formation, rather than merely offloading Extraneous Load. This accumulation of ``Epistemic Debt" creates ``Fragile Experts" whose high functional utility masks critically low corrective competence.
  To quantify and mitigate this debt, we conducted a between-subjects experiment (N=78) using a custom Cursor IDE plugin backed by Claude 3.5 Sonnet. Participants represented "AI-Native" learners across three conditions: Manual (Control), Unrestricted AI (Outsourcing), and Scaffolded AI (Offloading). The Scaffolded condition utilized a novel ``Explanation Gate," leveraging a real-time LLM-as-a-Judge framework to enforce a ``Teach-Back" protocol before generated code could be integrated.
  Results reveal a ``Collapse of Competence": while Unrestricted AI users matched the productivity of the Scaffolded group (p < .001 vs. Manual), they suffered a 77% failure rate in a subsequent AI-Blackout maintenance task, compared to only 39% in the Scaffolded group. Qualitative analysis suggests that successful vibe coders naturally engage in self-scaffolding, treating the AI as a consultant rather than a contractor. We discuss the implications for the maintainability of AI-generated software and propose that future learning systems must enforce Metacognitive Friction to prevent the mass production of unmaintainable code.

</details>


### [70] [PhantomRun: Auto Repair of Compilation Errors in Embedded Open Source Software](https://arxiv.org/abs/2602.20284)
*Han Fu,Andreas Ermedahl,Sigrid Eldh,Kristian Wiklund,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 本文介绍了一个名为PhantomRun的自动化框架，它利用大型语言模型生成并验证CI编译失败的修复方案。该框架通过提供对GitHub Actions、GitLab CI和四种不同构建系统的适配层来解决嵌入式系统项目中多样化的构建基础设施和工具链的问题。评估表明，PhantomRun能够成功修复目标项目中高达45%的CI编译失败问题，证明了基于LLM的修复对于嵌入式系统CI流水线是可行的。


<details>
  <summary>Details</summary>
Motivation: 持续集成(CI)管道在嵌入式软件编译过程中有时会遇到失败，导致开发者需要花费大量时间进行调试。作者研究发现硬件依赖性问题是编译失败的主要原因，其次是语法错误和构建脚本问题。鉴于多数修复只需要相对较小的改动，因此提出了自动修复的可能性，特别是如果能够处理多样化设置以及缺乏测试数据的问题的话。

Method: 开发了PhantomRun这一自动化框架，其利用大型语言模型（LLMs）根据构建日志、源代码、历史修复记录及编译器错误信息合成修复方案。此外，该框架还提供了对GitHub Actions与GitLab CI的支持，并兼容四种不同的构建系统，以适应各种构建环境。

Result: 实验结果表明，在针对的研究项目中，PhantomRun能够成功修复高达45%的CI编译失败案例，显示出基于大型语言模型的自动修复方法在嵌入式系统CI流程中的有效性。

Conclusion: PhantomRun展示了一种有效的方法来自动化修复嵌入式系统CI流程中的编译失败问题，通过利用大型语言模型可以显著提高修复效率，减轻开发者负担。

Abstract: Continuous Integration (CI) pipelines for embedded software sometimes fail during compilation, consuming significant developer time for debugging. We study four major open-source embedded system projects, spanning over 4000 build failures from the project's CI runs. We find that hardware dependencies account for the majority of compilation failures, followed by syntax errors and build-script issues. Most repairs need relatively small changes, making automated repair potentially suitable as long as the diverse setups and lack of test data can be handled.
  In this paper, we present PhantomRun, an automated framework that leverages large language models (LLMs) to generate and validate fixes for CI compilation failures. The framework addresses the challenge of diverse build infrastructures and tool chains across embedded system projects by providing an adaptation layer for GitHub Actions and GitLab CI and four different build systems. PhantomRun utilizes build logs, source code, historical fixes, and compiler error messages to synthesize fixes using LLMs. Our evaluations show that PhantomRun successfully repairs up to 45% of CI compilation failures across the targeted projects, demonstrating the viability of LLM-based repairs for embedded-system CI pipelines.

</details>


### [71] [Quantifying the Expectation-Realisation Gap for Agentic AI Systems](https://arxiv.org/abs/2602.20292)
*Sebastian Lobentanzer*

Main category: cs.SE

TL;DR: 研究发现，AI系统在部署前的预期与实际部署后的效果之间存在显著差异。软件开发中，AI工具不仅没有带来预期的24%效率提升，反而导致了19%的减速；临床文档记录方面，供应商宣称的时间节省实际上并不明显；而在临床决策支持上，外部验证性能远低于开发者报告的数据。这些问题主要由工作流程整合摩擦、验证负担等因素引起，建议采用结构化规划框架，并考虑人类监督成本来明确量化收益期望。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨和量化在不同应用场景下（如软件工程、临床文档记录及临床决策支持）部署AI系统前后预期与现实之间的差距。

Method: 通过回顾控制试验和跨领域的独立验证，包括软件工程、临床文档以及临床决策支持等领域的案例，对AI系统部署前后的期望与实现情况进行对比分析。

Result: 结果显示，在软件开发领域，AI工具使用后出现了与预期相反的效果；临床文档记录中时间节省效果不明显；而临床决策支持系统的实际表现也远不如预期。这些差异主要归因于工作流整合问题、额外验证需求等多种因素。

Conclusion: 基于上述发现，建议采用一种结构化的规划方法，要求明确设定可量化的效益目标，并且在计划过程中充分考虑到人力监督的成本。

Abstract: Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic heterogeneity in treatment effects. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in.

</details>


### [72] [UAMTERS: Uncertainty-Aware Mutation Analysis for DL-enabled Robotic Software](https://arxiv.org/abs/2602.20334)
*Chengjie Lu,Jiahui Wu,Shaukat Ali,Malaika Din Hashmi,Sebastian Mathias Thomle Mason,Francois Picard,Mikkel Labori Olsen,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 提出了一种名为UAMTERS的不确定性感知突变分析框架，用于评估DL支持的机器人软件在不同不确定性水平下的测试套件有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试生成技术难以有效评估DL支持的软件在动态环境中的可靠性，特别是在面对内在不确定性时。

Method: 开发了UAMTERS框架，引入了不确定性感知突变操作符来向DL支持的机器人软件中注入随机不确定性，并提出了衡量测试套件检测失败能力的突变分数指标。

Result: 通过三个机器人案例研究验证了UAMTERS能够更有效地识别测试套件的质量以及捕捉由不确定性引起的失败。

Conclusion: UAMTERS提供了一个新的方法来评估DL支持的机器人软件测试的有效性，特别是在存在不确定性的条件下。

Abstract: Self-adaptive robots adjust their behaviors in response to unpredictable environmental changes. These robots often incorporate deep learning (DL) components into their software to support functionality such as perception, decision-making, and control, enhancing autonomy and self-adaptability. However, the inherent uncertainty of DL-enabled software makes it challenging to ensure its dependability in dynamic environments. Consequently, test generation techniques have been developed to test robot software, and classical mutation analysis injects faults into the software to assess the test suite's effectiveness in detecting the resulting failures. However, there is a lack of mutation analysis techniques to assess the effectiveness under the uncertainty inherent to DL-enabled software. To this end, we propose UAMTERS, an uncertainty-aware mutation analysis framework that introduces uncertainty-aware mutation operators to explicitly inject stochastic uncertainty into DL-enabled robotic software, simulating uncertainty in its behavior. We further propose mutation score metrics to quantify a test suite's ability to detect failures under varying levels of uncertainty. We evaluate UAMTERS across three robotic case studies, demonstrating that UAMTERS more effectively distinguishes test suite quality and captures uncertainty-induced failures in DL-enabled software.

</details>


### [73] [A Case Study on Runtime Verification of a Continuous Deployment Process](https://arxiv.org/abs/2602.20598)
*Shoma Ansai,Masaki Waga*

Main category: cs.SE

TL;DR: 研究者通过SyMon监控基于FluxCD的持续部署流程，发现FluxCD在新镜像推送到GHCR后，并非总能在5分钟内检测到更新，但10分钟内总是可以。同时证明了SyMon能够实现接近实时的监控。


<details>
  <summary>Details</summary>
Motivation: 为了评估运行时监控在基于FluxCD的持续部署过程中的有效性与性能，特别是针对从GitHub Actions到Kubernetes应用部署链路中各组件的行为表现。

Method: 采用了SyMon工具对包含GitHub Actions、GitHub Container Registry (GHCR)、FluxCD以及运行于Kubernetes上的应用程序在内的系统进行日志监控；通过分析FluxCD轮询日志来判断部署更新是否被正确识别。

Result: 结果显示，在新镜像被推送至GHCR之后，FluxCD并不总是能够在5分钟之内检测到这一变化，但在所有收集的日志数据中，它总是在10分钟内完成了检测。此外，SyMon展现了其实现近实时监控的能力。

Conclusion: 研究表明，尽管存在一定的延迟，FluxCD仍能有效支持持续部署流程；而SyMon则为这类场景下的高效监控提供了一个可行方案。

Abstract: We report our experience in applying runtime monitoring to a FluxCD-based continuous deployment (CD) process. Our target system consists of GitHub Actions, GitHub Container Registry (GHCR), FluxCD, and an application running on Kubernetes. We monitored its logs using SyMon. In our setting, we regard a deployment update as detected when FluxCD's polling log resolves the latest image tag. Through the case study, we found that FluxCD did not always detect a new image within five minutes after it was pushed to GHCR, whereas it always did so within ten minutes in the collected logs. Moreover, our results show that SyMon is fast enough for near-real-time monitoring in our setting.

</details>


### [74] [SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference](https://arxiv.org/abs/2602.20610)
*Cuong Chi Le,Minh V. T Pham,Tung Vu Duy,Cuong Duc Van,Huy N. Phan,Hoang N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 本论文介绍了一种名为SpecMind的新框架，通过反馈驱动的多轮提示方法生成后置条件，相比现有技术在准确性和完整性上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管规范对于确保程序正确性至关重要，但手动编写它们既困难又耗时。最近基于大型语言模型（LLM）的方法在生成如后置条件等规范方面取得了成功，但现有的单次提示方法往往产生不准确的结果。

Method: 本文提出了SpecMind框架，该框架将LLMs视为交互式和探索性的推理者，而不是一次性生成器。SpecMind采用反馈驱动的多轮提示方法，使模型能够通过整合隐性和显性的正确性反馈来迭代地完善候选后置条件，并自主决定何时停止这一过程。

Result: 实验评估表明，SpecMind在生成后置条件的准确性和完整性方面显著优于最先进方法。

Conclusion: 通过将LLMs作为交互式推理工具使用，SpecMind提供了一个更有效率且更加精准的后置条件生成解决方案。

Abstract: Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.

</details>


### [75] [PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring](https://arxiv.org/abs/2602.20717)
*Xiting Liu,Yuetong Liu,Yitong Zhang,Jia Li,Shi-Min Hu*

Main category: cs.SE

TL;DR: 本研究提出了一种名为PackMonitor的新方法，能够从根本上消除大型语言模型在软件开发中推荐不存在的包的问题。通过持续监控模型的解码过程并在必要时干预，PackMonitor成功地将幻觉包的发生率降至零，同时保持了低延迟推理和原有模型能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地融入到软件开发工作流程中，其可信度成为了一个关键问题。特别是在依赖推荐场景下，由于普遍存在的包幻觉现象（即模型推荐实际上并不存在的包），使得这些模型的可靠性受到质疑。尽管已有研究尝试解决这一问题，但通常只能减少而非彻底消除幻觉包的现象，给软件安全带来了持续的风险。

Method: 基于包的有效性可以通过有限且可枚举的权威包列表来决定的关键见解，研究者们提出了PackMonitor。这种方法通过持续监控模型的解码过程，并在生成安装命令期间选择性地激活干预机制，从而从根本上防止幻觉包的出现。具体来说，它解决了三个核心挑战：(1) 使用上下文感知解析器确定何时触发干预；(2) 通过包名干涉器严格限制解码空间至一个权威包列表内；(3) 借助DFA缓存机制确保即使面对数百万个包也能高效监控，且几乎不增加额外开销。

Result: 广泛的实验表明，PackMonitor作为一个无需训练、即插即用的解决方案，在五种广泛使用的大型语言模型上一致地将幻觉包发生率降为零，同时维持了低延迟推断并保留了原始模型的功能。

Conclusion: PackMonitor提供了一种有效的方法来完全消除大型语言模型中的幻觉包问题，不仅增强了模型在软件开发环境下的可靠性和安全性，还展示了在不影响性能的情况下实现这一目标的可能性。

Abstract: As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminate them, leaving persistent software security risks.
  In this work, we argue that package hallucinations are theoretically preventable based on the key insight that package validity is decidable through finite and enumerable authoritative package lists. Building on this, we propose PackMonitor, the first approach capable of fundamentally eliminating package hallucinations by continuously monitoring the model's decoding process and intervening when necessary. To implement this in practice, PackMonitor addresses three key challenges: (1) determining when to trigger intervention via a Context-Aware Parser that continuously monitors model outputs and selectively activates intervening only during installation command generation; (2) resolving how to intervene by employing a Package-Name Intervenor that strictly limits the decoding space to an authoritative package list; and (3) ensuring monitoring efficiency through a DFA-Caching Mechanism that enables scalability to millions of packages with negligible overhead. Extensive experiments on five widely used LLMs demonstrate that PackMonitor is a training-free, plug-and-play solution that consistently reduces package hallucination rates to zero while maintaining low-latency inference and preserving original model capabilities.

</details>


### [76] [Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs](https://arxiv.org/abs/2602.20799)
*Guangsheng Ou,Qiming Zhang,Sirong Chen,Anji Li,Dong Xu,Tiancheng Luo,Dekun Dai,Cuiyun Gao,Long Wang,Jun Zhou,Mingwei Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了一种名为UCD-Training的两阶段训练框架，用于从未见过的代码库中合成数据，旨在提高大型语言模型对新软件框架的理解和推理能力。同时引入了一个新的基准测试UnseenCodeBench来评估代码生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型在处理新发布的软件框架时表现不佳，容易产生幻觉，因为这些模型在训练过程中没有接触到类似的环境。尽管存在一些技术如检索增强生成可以在一定程度上减少幻觉的发生，但仅通过提示注入知识不足以让模型完全理解代码库内不同组件之间的内在联系或正确组合应用。对于未见过的代码库，由于缺乏高质量、面向使用的大量代码作为训练数据，现有的数据合成方法不足以充分捕捉使用场景。

Method: 提出了UCD-Training框架，该框架首先解析源代码以构建代码图，然后利用文件级依赖数据进行保持依赖关系的继续预训练（CPT），最后基于三种类型的数据进行图形基础监督微调（SFT）：单跳关系推理数据、组合API推理数据以及代码库使用数据。每种数据都增强了显式推理轨迹。此外，还引入了UnseenCodeBench作为新的基准，专门针对未见代码库中的代码生成任务。

Result: 通过在多个代码库上进行全面实验验证了所提方法的有效性。虽然具体结果摘要中未给出，但可以推测出UCD-Training有助于改善模型在处理未知代码库时的表现，并且能够更好地理解和推理代码间的复杂关系。

Conclusion: 研究证明了UCD-Training作为一种有效的手段，能够帮助大规模语言模型更准确地理解和生成基于未见过代码库的代码，特别是在需要高级别推理的任务中。

Abstract: In the context of newly release software frameworks, large language models (LLMs) often exhibit poor performance and a high rate of hallucination, as they are not exposed to such environments during training. Although inference-time augmentation techniques such as retrieval-augmented generation (RAG) can partially mitigate hallucinations, knowledge injection through prompting alone is insufficient to enable models to fully understand the intrinsic relationships among different components of a codebase, or to reason about the correct compositions and apply. Although explicit knowledge injection can be achieved through post-training, compared with public code domains, unseen codebases typically provide only source code and lack large volumes of high-quality, usage-oriented code that can be directly leveraged as training data. Consequently, existing data synthesis approaches are insufficient to adequately capture unseen codebases usage scenarios when restricted to source code alone. To address these challenges, we propose UCD-Training, a two-stage training framework for reasoning-aware data synthesis grounded in a code graph constructed from unseen codebases. UCD-Training first parses the source code to build a code graph, then conducts dependency-preserving continued pretraining (CPT) using file-level dependency data, followed by graph-grounded supervised fine-tuning (SFT) on three types of synthesized data augmented with explicit reasoning traces: (1) single-hop relation reasoning data, (2) compositional API reasoning data, and (3) codebase utilization data. We further introduce a new benchmark, UnseenCodeBench, for code generation on unseen codebases and conduct comprehensive experiments across multiple codebases.

</details>


### [77] [A Modular Multi-Document Framework for Scientific Visualization and Simulation in Java](https://arxiv.org/abs/2602.21026)
*David Heddle*

Main category: cs.SE

TL;DR: 本文介绍了一个为科学可视化和模拟设计的模块化多文档界面(MDI)框架，该框架在JVM生态系统中实现了可视化层、模拟引擎和可选硬件加速3D渲染之间的架构分离。通过一个实时3D气体膨胀模拟案例展示了其架构的一致性，并且这个框架可通过Maven Central公开获取，适用于长期使用的科学和工程桌面应用程序。


<details>
  <summary>Details</summary>
Motivation: 设计并实现一个模块化的多文档界面框架，用于支持科学可视化和模拟，在JVM环境中实现不同功能组件间的清晰分隔。

Method: 开发了一个MDI框架，其中包含了核心抽象、线程模型、模拟集成策略以及依赖隔离方法，同时将3D功能封装成独立模块来避免2D应用中的非必要依赖耦合。

Result: 通过结合实时3D气体膨胀模拟与同步2D熵图绘制的案例研究，验证了所提出框架的有效性和一致性。

Conclusion: 提出的MDI框架成功地在保持各功能组件间清晰边界的同时提供了良好的扩展性和灵活性，特别适合需要长期维护的科学及工程类桌面软件。

Abstract: This paper presents the design and implementation of a modular multi-document interface (MDI) framework for scientific visualization and simulation in the Java Virtual Machine (JVM) ecosystem. The framework emphasizes architectural separation between visualization layers, simulation engines, and optional hardware-accelerated 3D rendering. 3D functionality is isolated into a separate module to prevent unnecessary dependency coupling in 2D-only applications. We describe the core abstractions, threading model, simulation integration strategy, and dependency isolation approach. A case study involving a real-time 3D gas expansion simulation integrated with synchronized 2D entropy plotting demonstrates architectural cohesion. The framework is publicly available via Maven Central and targets long-lived scientific and engineering desktop applications.

</details>


### [78] [Automated Detection and Mitigation of Dependability Failures in Healthcare Scenarios through Digital Twins](https://arxiv.org/abs/2602.21037)
*Bruno Guindani,Matteo Camilli,Livia Lestingi,Marcello M. Bersani*

Main category: cs.SE

TL;DR: 本文提出了一种名为M-GENGAR的方法，该方法基于闭环数字孪生范式来确保医疗信息物理系统的可靠性。通过结合随机混合自动机建模、患者动态数据驱动学习以及统计模型检查等技术，M-GENGAR能够系统地识别并分类违反专家定义的可靠性要求的情景，并支持自动化生成缓解策略。在一项涉及肺部呼吸机的研究案例中，结果显示M-GENGAR合成的策略在87.5%的情况下至少能像人类决策一样有效地稳定患者生命体征指标，同时平均保持相关指标比正常健康值更接近20%。


<details>
  <summary>Details</summary>
Motivation: 现有的临床决策支持系统虽然支持临床实践，但仍然需要一种主动且注重可靠性的方法论，能够在患者安全受到威胁之前识别并减轻故障情况。此外，由于医疗信息物理系统（CPSs）中存在异构性和人在生理行为上的不确定性，这对形成安全关键PDP三元组的依赖性提出了挑战。

Method: M-GENGAR方法基于闭环数字孪生（DT）范式，结合了随机混合自动机构建、数据驱动的学习患者动态变化及统计模型检验技术。该方法还包括一个离线的关键情景检测阶段，集成了模型空间探索与多样性分析，以系统化方式识别和分类违背专家定义可靠性需求的情景。同时，M-GENGAR还支持通过正式博弈理论分析自动合成缓解策略。

Result: 通过对代表性的使用案例研究——肺部呼吸机的应用进行评估表明，在所评估的情景中，通过正式博弈理论分析合成的策略在87.5%的情况下至少能像人类决策一样有效稳定患者的生命体征指标，同时平均而言，这些策略维持的相关指标比正常健康值更加接近20%。

Conclusion: M-GENGAR提供了一种创新的方法来提高医疗信息物理系统的可靠性，通过提前识别潜在风险并自动生成缓解措施，它显示出了优于或相当于人类决策的有效性。这为未来开发更可靠的医疗设备和技术提供了新的方向。

Abstract: Medical Cyber-Physical Systems (CPSs) integrating Patients, Devices, and healthcare personnel (Physicians) form safety-critical PDP triads whose dependability is challenged by system heterogeneity and uncertainty in human and physiological behavior. While existing clinical decision support systems support clinical practice, there remains a need for proactive, reliability-oriented methodologies capable of identifying and mitigating failure scenarios before patient safety is compromised. This paper presents M-GENGAR, a methodology based on a closed-loop Digital Twin (DT) paradigm for dependability assurance of medical CPSs. The approach combines Stochastic Hybrid Automata modeling, data-driven learning of patient dynamics, and Statistical Model Checking with an offline critical scenario detection phase that integrates model-space exploration and diversity analysis to systematically identify and classify scenarios violating expert-defined dependability requirements. M-GENGAR also supports the automated synthesis of mitigation strategies, enabling runtime feedback and control within the DT loop. We evaluate M-GENGAR on a representative use case study involving a pulmonary ventilator. Results show that, in 87.5% of the evaluated scenarios, strategies synthesized through formal game-theoretic analysis stabilize patient vital metrics at least as effectively as human decision-making, while maintaining relevant metrics 20% closer to nominal healthy values on average.

</details>


### [79] [Validation of an analyzability model for quantum software: a family of experiments](https://arxiv.org/abs/2602.21074)
*Ana Díaz-Muñoz,José A. Cruz-Lemus,Moisés Rodríguez,Maria Teresa Baldassarre,Mario Piattini*

Main category: cs.SE

TL;DR: 本文通过一系列实验验证了一个先前提出的混合软件（包含经典和量子组件）的可分析性模型，特别是其量子组件部分。实验结果表明该模型能够有效区分不同可分析性水平的量子软件组件，并与人类对这些算法复杂性的感知相一致，从而加强了其在量子计算中的有效性。


<details>
  <summary>Details</summary>
Motivation: 确保混合软件（同时包含经典和量子元素）的可维护性和工业采用率的关键在于其可分析性。文章旨在通过实证研究来验证一个基于ISO/IEC 25010标准开发的、用于评估量子算法可分析性的模型的有效性及其与人类对这些算法复杂性认知之间的一致性。

Method: 采用了一种系列实验的方法，共包括四个针对不同背景参与者（学术界和专业领域）的研究。这些实验旨在测试所提议模型衡量量子算法可分析性的能力，并探索模型计算出的可分析性水平与参与者对于这些算法复杂度感受之间的关系。

Result: 结果显示，所提出的模型能有效地识别具有不同可分析性程度的量子软件组件，并且这种识别与人们对于算法复杂度的感觉相符，证明了模型在量子计算环境下的有效性。

Conclusion: 通过实验证明了基于ISO/IEC 25010标准提出的混合软件可分析性模型对于评估量子组件特别有效，这为提高混合软件系统的可维护性及促进其更广泛的应用提供了支持。

Abstract: The analyzability of hybrid software, which integrates both classical and quantum components, is a key factor in ensuring its maintainability and industrial adoption. This article presents the empirical validation, through a family of experiments, of the quantum component of a previously proposed hybrid software analyzability model based on the ISO/IEC 25010 standard. The experimental series consists of four studies involving participants with diverse profiles in both academic and professional settings. In these experiments, the model's ability to effectively measure the analyzability of quantum algorithms is assessed, and the relationship between the analyzability levels computed by the model and the participant's perceptions of the complexity of these algorithms is examined. The results indicate that the proposed model effectively distinguishes between quantum software components with varying levels of analyzability and aligns with human perception, reinforcing its validity in quantum computing.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [80] [SPP-SCL: Semi-Push-Pull Supervised Contrastive Learning for Image-Text Sentiment Analysis and Beyond](https://arxiv.org/abs/2602.20767)
*Jiesheng Wu,Shengrong Li*

Main category: cs.MM

TL;DR: 本文提出了一种半推拉监督对比学习（SPP-SCL）方法，旨在解决图像-文本情感分析中模态内和模态间情感关系不一致的问题。通过两步策略实现了模态内与模态间关系的平衡，并在跨模态特征融合前确保了这些关系的一致性。实验结果表明，该方法在三个公开的数据集上显著优于现有最先进方法，特别是在情感识别方面表现得更加突出。


<details>
  <summary>Details</summary>
Motivation: 当前的图像-文本情感分析方法面临着模态内部及模态之间存在的情感关系不一致问题。为了解决这一难题并实现更准确的情感分析，研究者们提出了新的解决方案。

Method: 提出了一种名为Semi-Push-Pull Supervised Contrastive Learning (SPP-SCL)的新方法，采用创新的两阶段策略执行：首先进行模态内的监督对比学习以增强同类样本间的相似度；接着依据特定条件决定是否继续执行第二阶段——模态间的监督对比学习，目的是增加不同模态下样本间的差异性。最终目标是达到模态内与模态间关系的平衡，并促进后续的跨模态特征融合用于情感分析。

Result: 通过对三个公开可用的图像-文本情感及讽刺检测数据集进行测试，结果显示SPP-SCL方法相比现有最先进的技术有了大幅度的性能提升，在情感识别方面表现出更强的区分能力。

Conclusion: 所提出的SPP-SCL方法有效解决了图像-文本情感分析中存在的模态不平衡问题，通过实施一种新颖的两步策略成功提升了模型对于复杂情感内容的理解准确性。

Abstract: Existing Image-Text Sentiment Analysis (ITSA) methods may suffer from inconsistent intra-modal and inter-modal sentiment relationships. Therefore, we develop a method that balances before fusing to solve the issue of vision-language imbalance intra-modal and inter-modal sentiment relationships; that is, a Semi-Push-Pull Supervised Contrastive Learning (SPP-SCL) method is proposed. Specifically, the method is implemented using a novel two-step strategy, namely first using the proposed intra-modal supervised contrastive learning to pull the relationships between the intra-modal and then performing a well-designed conditional execution statement. If the statement result is false, our method will perform the second step, which is inter-modal supervised contrastive learning to push away the relationships between inter-modal. The two-step strategy will balance the intra-modal and inter-modal relationships to achieve the purpose of relationship consistency and finally perform cross-modal feature fusion for sentiment analysis and detection. Experimental studies on three public image-text sentiment and sarcasm detection datasets demonstrate that SPP-SCL significantly outperforms state-of-the-art methods by a large margin and is more discriminative in sentiment.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [81] [Indaleko: The Unified Personal Index](https://arxiv.org/abs/2602.20507)
*William Anthony Mason*

Main category: cs.IR

TL;DR: 该论文提出了一种与人类记忆相匹配的个人信息检索架构——统一个人信息索引（UPI），并通过Indaleko原型系统展示了其在大规模数据集上的可行性。该系统通过整合时间、空间和活动元数据，实现了自然语言查询，并解决了跨平台搜索碎片化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的信息检索系统忽略了人类记忆的工作方式，主要依赖于关键词搜索，这与人们通过事件线索回忆信息的方式不一致。为了弥补这一差距，作者提出了一个与人类记忆模式对齐的新架构。

Method: 开发了一个名为Unified Personal Index (UPI) 的新架构，以及一个名为Indaleko的原型系统来实现这一架构。该方法包括集成多种类型的元数据到一个统一的图数据库中，利用记忆锚点索引来加速查询处理等技术手段。

Result: 实验表明，与商业系统相比，基于UPI的Indaleko能够有效地执行结合时间、地点及活动模式的多维度查询，并且保持了良好的精确度。此外，它还支持快速地集成新的数据源，同时通过UUID机制保障用户隐私。

Conclusion: 这项研究通过将个人信息检索从单纯的关键词匹配转变为与记忆相符的查找过程，为现有数据提供了即时的好处，并为未来的情境感知系统奠定了基础。

Abstract: Personal information retrieval fails when systems ignore how human memory works. While existing platforms force keyword searches across isolated silos, humans naturally recall through episodic cues like when, where, and in what context information was encountered. This dissertation presents the Unified Personal Index (UPI), a memory-aligned architecture that bridges this fundamental gap. The Indaleko prototype demonstrates the UPI's feasibility on a 31-million file dataset spanning 160TB across eight storage platforms. By integrating temporal, spatial, and activity metadata into a unified graph database, Indaleko enables natural language queries like "photos near the conference venue last spring" that existing systems cannot process. The implementation achieves sub-second query responses through memory anchor indexing, eliminates cross-platform search fragmentation, and maintains perfect precision for well-specified memory patterns. Evaluation against commercial systems (Google Drive, OneDrive, Dropbox, Windows Search) reveals that all fail on memory-based queries, returning overwhelming result sets without contextual filtering. In contrast, Indaleko successfully processes multi-dimensional queries combining time, location, and activity patterns. The extensible architecture supports rapid integration of new data sources (10 minutes to 10 hours per provider) while preserving privacy through UUID-based semantic decoupling. The UPI's architectural synthesis bridges cognitive theory with distributed systems design, as demonstrated through the Indaleko prototype and rigorous evaluation. This work transforms personal information retrieval from keyword matching to memory-aligned finding, providing immediate benefits for existing data while establishing foundations for future context-aware systems.

</details>


### [82] [PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization](https://arxiv.org/abs/2602.20676)
*Shuzhi Cao,Rong Chen,Ailong He,Shuguang Han,Jufeng Chen*

Main category: cs.IR

TL;DR: PRECTR-V2通过挖掘全局相关性偏好、构建硬负样本和预训练轻量级编码器来解决低活跃用户个性化建模难、排名模型泛化偏差及表示学习与CTR微调间不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 在搜索系统中，有效协调搜索相关性匹配和点击率（CTR）预测这两个核心目标对于发现用户兴趣和提高平台收入至关重要。然而，先前的工作PRECTR面临三个主要挑战：1. 低活跃用户和新用户的行为数据有限，难以实现有效的个性化相关性偏好建模；2. 排名模型的训练数据主要来自高相关性曝光，导致与粗排阶段候选空间存在分布不匹配问题，引起泛化偏差；3. 受延迟限制，原始模型采用Emb+MLP架构并冻结了BERT编码器，这阻碍了联合优化，并造成了表征学习与CTR微调之间的错位。

Method: 为了解决这些问题，本文提出了PRECTR-V2方法。首先，通过在特定查询下挖掘全局相关性偏好来缓解低活跃用户的稀疏行为问题，促进冷启动场景下的有效个性化相关性建模。其次，通过嵌入噪声注入和相关性标签重构构造硬负样本，并通过成对损失优化它们相对于正样本的相对排名，以此纠正曝光偏差。最后，基于文本相关性分类任务从大语言模型(LLM)和监督微调(SFT)的知识蒸馏预训练一个轻量级变压器编码器，替代冻结的BERT模块，以更好地适应CTR微调，并超越传统的Emb+MLP范式。

Result: PRECTR-V2能够更有效地处理低活跃用户的数据稀疏性问题，减少排名模型因数据分布不匹配而产生的泛化偏差，并通过引入新的编码器解决了原模型中表示学习与CTR微调之间存在的不一致性问题。

Conclusion: PRECTR-V2通过一系列创新措施成功克服了原有PRECTR框架中存在的局限性，包括改善低活跃用户个性化推荐质量、校正排名模型泛化偏差以及提升CTR预测准确性等，从而进一步提高了搜索系统的整体性能。

Abstract: In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.

</details>


### [83] [IntRR: A Framework for Integrating SID Redistribution and Length Reduction](https://arxiv.org/abs/2602.20704)
*Zesheng Wang,Longfei Xu,Weidong Deng,Huimin Yan,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 提出了一种新的框架IntRR，通过语义ID重分配和结构长度减少来解决当前生成式推荐系统中的问题，从而提高推荐准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的语义ID（SIDs）在索引目标与实际推荐目标之间存在不一致，并且由于这些标识符是静态的，模型缺乏适应用户交互复杂性的灵活性。此外，将层次化的SIDs扁平化为标记序列会导致序列长度膨胀，增加了计算开销和推理延迟。

Method: IntRR框架通过使用特定项目的唯一ID（UIDs）作为协作锚点，动态地跨层次代码簿层重新分配语义权重；同时递归处理SID层次结构，无需扁平化序列，确保每个项目固定成本为一个标记。

Result: 在基准数据集上的广泛实验表明，与代表性生成基线相比，IntRR在推荐准确性和效率方面都取得了显著改进。

Conclusion: IntRR提供了一个有效的方法来克服现有生成式推荐系统的局限性，特别是关于语义ID的管理和处理，提升了整体性能。

Abstract: Generative Recommendation (GR) has emerged as a transformative paradigm that reformulates the traditional cascade ranking system into a sequence-to-item generation task, facilitated by the use of discrete Semantic IDs (SIDs). However, current SIDs are suboptimal as the indexing objectives (Stage 1) are misaligned with the actual recommendation goals (Stage 2). Since these identifiers remain static (Stage 2), the backbone model lacks the flexibility to adapt them to the evolving complexities of user interactions. Furthermore, the prevailing strategy of flattening hierarchical SIDs into token sequences leads to sequence length inflation, resulting in prohibitive computational overhead and inference latency. To address these challenges, we propose IntRR, a novel framework that integrates objective-aligned SID Redistribution and structural Length Reduction. By leveraging item-specific Unique IDs (UIDs) as collaborative anchors, this approach dynamically redistributes semantic weights across hierarchical codebook layers. Concurrently, IntRR handles the SID hierarchy recursively, eliminating the need to flatten sequences. This ensures a fixed cost of one token per item. Extensive experiments on benchmark datasets demonstrate that IntRR yields substantial improvements over representative generative baselines, achieving superior performance in both recommendation accuracy and efficiency.

</details>


### [84] [RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition](https://arxiv.org/abs/2602.20735)
*Kun Ran,Marwah Alaofi,Danula Hettiachchi,Chenglong Ma,Khoi Nguyen Dinh Anh,Khoi Vo Nguyen,Sachin Pathiyan Cherumanal,Lida Rashidi,Falk Scholer,Damiano Spina,Shuoqi Sun,Oleg Zendel*

Main category: cs.IR

TL;DR: 本文介绍了RMIT-ADM+S系统，该系统在NeurIPS 2025 MMU-RAG竞赛的文本到文本赛道中获奖。系统采用名为Routing-to-RAG (R2RAG)的新架构，能够根据查询复杂度和证据充分性动态调整检索策略，并且能在单个消费级GPU上运行，支持复杂的科研任务。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在开发一种能够基于查询复杂性和证据充分性动态调整检索策略的轻量级组件组成的检索增强生成（RAG）架构。通过使用较小的语言模型，使系统能够在单一消费级GPU上运行的同时支持复杂的科研任务。

Method: 提出了Routing-to-RAG (R2RAG)架构，这是一种基于G-RAG系统（ACM SIGIR 2025 LiveRAG挑战赛的获胜者）的研究导向型检索增强生成(RAG)架构。R2RAG引入了根据输出质量评估反馈来优化的模块。

Result: R2RAG赢得了开源类别中的最佳动态评估奖，展示了其在设计精巧与资源高效利用方面的高度有效性。

Conclusion: R2RAG系统的成功表明，通过精心设计和有效利用资源，即使是基于较小语言模型的解决方案也能在诸如NeurIPS 2025 MMU-RAG竞赛这样的挑战中取得优异成绩。

Abstract: This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text
  track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG
  (R2RAG), a research-focused retrieval-augmented generation (RAG)
  architecture composed of lightweight components that dynamically adapt the
  retrieval strategy based on inferred query complexity and evidence
  sufficiency. The system uses smaller LLMs, enabling operation on a single
  consumer-grade GPU while supporting complex research tasks. It builds on the
  G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it
  with modules informed by qualitative review of outputs. R2RAG won the Best
  Dynamic Evaluation award in the Open Source category, demonstrating high
  effectiveness with careful design and efficient use of resources.

</details>


### [85] [Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking](https://arxiv.org/abs/2602.20800)
*Dalia Nahhas,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.IR

TL;DR: 本文提出了一种无泄漏的双评审框架，用于解决生成信息检索中文化相关性的评估问题。通过将监督和评估严格分离，并在一个新的包含33,052个文化背景故事的数据集上测试，发现从交叉编码器蒸馏出的密集双向编码器在无泄漏条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的评判方法存在循环性和偏好泄露的问题，导致性能被高估。为了解决生成信息检索中文化相关性评价面临的挑战，特别是针对规范性标准如文化相关性的候选选择瓶颈问题。

Method: 本文将文化相关性形式化为一个查询内排名任务，并引入了一个完全分离监督（Judge B）与评估（Judge A）角色的无泄漏双评审架构。使用了从Judge-B监督下的交叉编码器提炼出来的密集双向编码器来执行排名任务。

Result: 实验结果表明，虽然经典基线仅提供了适度的改进，但由Judge-B监督的交叉编码器提炼出的密集双向编码器非常有效。特别是在无泄漏条件下进行评估时，提炼出的BGE-M3模型明显优于原始交叉编码器。

Conclusion: 严格的评估者分离是实现可信生成信息检索评价的前提条件，证明了细微的文化偏好可以被提炼成高效的排名器而不会发生泄漏。

Abstract: In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.

</details>


### [86] [E-MMKGR: A Unified Multimodal Knowledge Graph Framework for E-commerce Applications](https://arxiv.org/abs/2602.20877)
*Jiwoo Kang,Yeon-Chang Lee*

Main category: cs.IR

TL;DR: 提出了一种名为E-MMKGR的框架，通过构建电商特定的多模态知识图谱并学习统一的商品表示，以提高推荐和产品搜索任务的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态推荐系统依赖于固定的一组模态和特定的任务目标，限制了模态扩展性和任务泛化能力。

Method: 开发E-MMKGR框架，该框架创建了一个电商专用的多模态知识图谱E-MMKG，并利用基于GNN的传播与面向知识图谱的优化来学习统一的商品表示。

Result: 在真实世界Amazon数据集上的实验表明，在推荐任务中Recall@10最多提高了10.18%，而在产品搜索方面相较于基于向量的检索方法提升了高达21.72%的表现。

Conclusion: E-MMKGR框架有效提高了推荐系统及产品搜索任务中的性能，展示了其有效性和可扩展性。

Abstract: Multimodal recommender systems (MMRSs) enhance collaborative filtering by leveraging item-side modalities, but their reliance on a fixed set of modalities and task-specific objectives limits both modality extensibility and task generalization. We propose E-MMKGR, a framework that constructs an e-commerce-specific Multimodal Knowledge Graph E-MMKG and learns unified item representations through GNN-based propagation and KG-oriented optimization. These representations provide a shared semantic foundation applicable to diverse tasks. Experiments on real-world Amazon datasets show improvements of up to 10.18% in Recall@10 for recommendation and up to 21.72% over vector-based retrieval for product search, demonstrating the effectiveness and extensibility of our approach.

</details>


### [87] [Naver Labs Europe @ WSDM CUP | Multilingual Retrieval](https://arxiv.org/abs/2602.20986)
*Thibault Formal,Maxime Louis,Hervé Déjean,Stéphane Clinchant*

Main category: cs.IR

TL;DR: 本报告介绍了我们参加2026年WSDM杯多语言文档检索任务的情况，该任务对跨语言泛化提出了挑战。通过评估五次逐步增强的运行结果，显示了SPLARE模型在与先进密集型基线如Qwen3-8B-Embed对比中的优势，强调了学习稀疏检索模型在非英语中心场景下的持续相关性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 参与WSDM Cup 2026共享任务，旨在探索并展示SPLARE（一种新提出的稀疏检索模型）在处理从英语查询到多语言文档检索时的有效性及优越性，特别是在跨语言泛化方面的能力。

Method: 采用了基于SPLARE-7B模型的五种不同配置实验，包括结合轻量级改进措施、使用Qwen3-Reranker-4B进行重排序以及实施简单的分数融合策略等方法来优化检索性能。

Result: 结果显示，即使面对复杂的跨语种检索挑战，SPLARE模型的表现依旧优于当前先进的密集型基准模型，例如Qwen3-8B-Embed，证明了其在多语言环境下的强大适应性和有效性。

Conclusion: 研究证实了SPLARE作为学习稀疏检索模型，在超越以英语为中心的应用场景中保持高度的相关性和竞争地位；同时为未来跨语言信息检索系统的设计提供了有价值的参考。

Abstract: This report presents our participation to the WSDM Cup 2026 shared task on multilingual document retrieval from English queries. The task provides a challenging benchmark for cross-lingual generalization. It also provides a natural testbed for evaluating SPLARE, our recently proposed learned sparse retrieval model, which produces generalizable sparse latent representations and is particularly well suited to multilingual retrieval settings.
  We evaluate five progressively enhanced runs, starting from a SPLARE-7B model and incorporating lightweight improvements, including reranking with Qwen3-Reranker-4B and simple score fusion strategies. Our results demonstrate the strength of SPLARE compared to state-of-the-art dense baselines such as Qwen3-8B-Embed. More broadly, our submission highlights the continued relevance and competitiveness of learned sparse retrieval models beyond English-centric scenarios.

</details>


### [88] [Generative Pseudo-Labeling for Pre-Ranking with LLMs](https://arxiv.org/abs/2602.20995)
*Junyu Bi,Xinting Niu,Daixuan Cheng,Kun Yuan,Tao Wang,Binbin Cao,Jian Wu,Yuning Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种名为GPL（Generative Pseudo-Labeling）的新框架，该框架利用大型语言模型为未曝光项目生成无偏、内容感知的伪标签，以解决预排名阶段存在的训练-服务不一致问题。实际应用表明，此方法能够提高点击率3.07%，同时增加推荐多样性和长尾项目的发现。


<details>
  <summary>Details</summary>
Motivation: 在工业推荐系统中，预排名阶段面临一个关键挑战：训练与服务之间的差异导致样本选择偏差严重，并且降低了泛化能力，特别是对于长尾内容。现有的去偏方法通常依赖于启发式方法或者从有偏排名器中进行知识蒸馏，这些方法要么将可能合适的未曝光项错误地标记为负样本，要么将曝光偏差传播到伪标签中。

Method: 提出了Generative Pseudo-Labeling (GPL)框架，通过利用大型语言模型来为未曝光物品生成无偏见的内容感知伪标签，旨在明确地使训练分布与在线服务空间对齐。此外，GPL通过离线生成特定用户的兴趣锚点并在冻结语义空间内匹配候选者的方式提供高质量监督，而不会增加在线延迟。

Result: 当被部署在一个大规模生产系统中时，GPL提高了点击率至3.07%，同时还显著提升了推荐多样性以及长尾商品的发现效率。

Conclusion: 本研究提出的GPL框架成功解决了预排名阶段由于训练和服务间差距造成的样本选择偏差问题，不仅有效提高了点击率，而且增强了推荐系统的多样性和对长尾内容的支持。

Abstract: Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debiasing approaches typically rely on heuristics (e.g., negative sampling) or distillation from biased rankers, which either mislabel plausible unexposed items as negatives or propagate exposure bias into pseudo-labels. In this work, we propose Generative Pseudo-Labeling (GPL), a framework that leverages large language models (LLMs) to generate unbiased, content-aware pseudo-labels for unexposed items, explicitly aligning the training distribution with the online serving space. By offline generating user-specific interest anchors and matching them with candidates in a frozen semantic space, GPL provides high-quality supervision without adding online latency. Deployed in a large-scale production system, GPL improves click-through rate by 3.07%, while significantly enhancing recommendation diversity and long-tail item discovery.

</details>


### [89] [Position-Aware Sequential Attention for Accurate Next Item Recommendations](https://arxiv.org/abs/2602.21052)
*Timur Nabiev,Evgeny Frolov*

Main category: cs.IR

TL;DR: 本文提出了一种基于可学习位置核的自注意力机制，该机制能够在纯位置空间中操作，与语义相似性解耦，并直接调节注意力权重，从而改善了序列顺序建模的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的序列自注意力模型依赖于加法位置嵌入来在输入时注入位置信息。然而，这种做法使得注意力机制仅表面上对序列顺序敏感：位置信息与项目嵌入语义纠缠在一起，在深层架构中传播较弱，并限制了捕捉丰富序列模式的能力。

Method: 为了解决上述局限性，作者引入了一种基于核函数的自注意力机制，其中可学习的位置核纯粹在位置空间中运作，与语义相似性分离，并且直接调整注意力权重。当每个注意力块都应用这种核时，它能够实现自适应多尺度序列建模。

Result: 实验表明，所提出的基于位置核的注意力方法在标准的下一个项目预测基准测试上始终优于强大的竞争基线。

Conclusion: 通过引入独立于语义相似性的可学习位置核，新的自注意力机制能够更好地捕捉序列中的时间顺序特征，进而提高了模型在序列预测任务上的表现。

Abstract: Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.

</details>


### [90] [Multi-Vector Index Compression in Any Modality](https://arxiv.org/abs/2602.21202)
*Hanxiang Qin,Alexander Martin,Rohan Jha,Chunsheng Zuo,Reno Kriz,Benjamin Van Durme*

Main category: cs.IR

TL;DR: 本文研究了多向量检索在不同模态下的高效压缩方法，特别是针对图像、视频和音频等大型语料库的成本问题。提出了一种注意力引导的聚类(AGC)方法，并在文本、视觉文档和视频检索任务中证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 晚期交互已成为文本、图像、视觉文档和视频信息检索的主要范式，但其计算和存储成本随文档长度线性增长，对于富含图像、视频和音频的语料库来说非常昂贵。

Method: 本文探索了几种与查询无关的方法来压缩多向量文档表示，包括序列调整大小、记忆令牌、分层池化以及一种新的注意力引导聚类(AGC)方法。其中，AGC利用注意力机制识别文档中最具有语义显著性的区域作为聚类中心点，并对token聚合进行加权。

Result: 评估结果显示，在文本（BEIR）、视觉文档（ViDoRe）和视频（MSR-VTT, MultiVENT 2.0）检索任务上，注意力引导的聚类方法始终优于其他参数化的压缩方法（如序列调整大小和记忆令牌），比非参数化的层次聚类提供了更大的索引大小灵活性，并且相比完整未压缩索引实现了竞争性或更优的表现。

Conclusion: 提出的注意力引导聚类方法为解决多向量检索中的效率问题提供了一个有效途径，特别是在处理包含大量多媒体内容的数据集时展现出优势。

Abstract: We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.

</details>
