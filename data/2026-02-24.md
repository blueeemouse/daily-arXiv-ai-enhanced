<div id=toc></div>

# Table of Contents

- [cs.MM](#cs.MM) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.LG](#cs.LG) [Total: 100]
- [cs.DB](#cs.DB) [Total: 7]


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1] [Health+: Empowering Individuals via Unifying Health Data](https://arxiv.org/abs/2602.19319)
*Sujaya Maiyya,Shantanu Sharma,Avinash Kumar*

Main category: cs.MM

TL;DR: Health+ 是一个以用户为中心的多模式健康数据管理系统，旨在让用户能够轻松上传、查询和分享他们的健康数据，同时确保数据的安全性和隐私性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗生态系统碎片化且以机构为中心，个人难以有效控制自己的医疗记录。这些记录分散在不兼容的系统和格式中。

Method: Health+ 通过提供直观的界面和智能推荐来增强个人对其健康信息的访问和分享能力，同时在系统层面解决存储、整合及保护异构健康记录所带来的复杂问题。

Result: Health+ 能够统一不同形式的数据，并优先考虑患者需求，为建立更加连接紧密、易于理解且由用户控制的健康信息生态系统奠定了基础。

Conclusion: Health+ 提供了一个解决方案，使个人（包括技术知识有限的人）能够更好地管理自己的健康数据，促进了更高效和个人化的健康管理方式。

Abstract: Managing personal health data is a challenge in today's fragmented and institution-centric healthcare ecosystem. Individuals often lack meaningful control over their medical records, which are scattered across incompatible systems and formats. This vision paper presents Health+, a user-centric, multimodal health data management system that empowers individuals (including those with limited technical expertise) to upload, query, and share their data across modalities (e.g., text, images, reports). Rather than aiming for institutional overhaul, Health+ emphasizes individual agency by providing intuitive interfaces and intelligent recommendations for data access and sharing. At the system level, it tackles the complexity of storing, integrating, and securing heterogeneous health records, ensuring both efficiency and privacy. By unifying multimodal data and prioritizing patients, Health+ lays the foundation for a more connected, interpretable, and user-controlled health information ecosystem.

</details>


### [2] [Tri-Subspaces Disentanglement for Multimodal Sentiment Analysis](https://arxiv.org/abs/2602.19585)
*Chunlei Meng,Jiabin Luo,Zhenglin Yan,Zhenyu Yu,Rong Fu,Zhongxue Gan,Chun Ouyang*

Main category: cs.MM

TL;DR: 本文提出了一种三子空间解缠(TSD)框架，用于多模态情感分析，通过将特征分解为三个互补的子空间来提高表示能力，并设计了子空间感知交叉注意力(SACA)融合模块以获得更丰富、更鲁棒的表示。实验表明该方法在关键指标上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感分析的方法主要集中在全局共享表示或模态特定特征上，忽略了仅由某些模态对共享的信号，限制了多模态表示的表现力和区分力。

Method: 提出了一个三子空间解缠（TSD）框架，该框架显式地将特征分解成三个互补子空间：捕获全局一致性的公共子空间、建模成对跨模态协同作用的子模态共享子空间以及保持模态特定线索的私有子空间。此外，引入了解耦监督器与结构化正则化损失以保证这些子空间的纯净性和独立性，并设计了一个子空间感知交叉注意力（SACA）融合模块来自适应地建模和整合来自三个子空间的信息。

Result: 在CMU-MOSI和CMU-MOSEI上的实验表明，TSD在所有关键指标上都达到了最先进性能，例如在CMU-MOSI上达到0.691 MAE，在CMU-MOSEI上达到54.9% ACC-7。此外，该方法也很好地迁移到了多模态意图识别任务中。消融研究表明，三子空间解缠与SACA共同增强了多层次跨模态情感线索的建模。

Conclusion: 通过提出TSD框架及其SACA融合模块，本研究有效提升了多模态情感分析中的表达能力和判别力，同时在多个基准数据集上取得了最先进的结果。

Abstract: Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic modalities to infer human sentiment. Most existing methods either focus on globally shared representations or modality-specific features, while overlooking signals that are shared only by certain modality pairs. This limits the expressiveness and discriminative power of multimodal representations. To address this limitation, we propose a Tri-Subspace Disentanglement (TSD) framework that explicitly factorizes features into three complementary subspaces: a common subspace capturing global consistency, submodally-shared subspaces modeling pairwise cross-modal synergies, and private subspaces preserving modality-specific cues. To keep these subspaces pure and independent, we introduce a decoupling supervisor together with structured regularization losses. We further design a Subspace-Aware Cross-Attention (SACA) fusion module that adaptively models and integrates information from the three subspaces to obtain richer and more robust representations. Experiments on CMU-MOSI and CMU-MOSEI demonstrate that TSD achieves state-of-the-art performance across all key metrics, reaching 0.691 MAE on CMU-MOSI and 54.9% ACC-7 on CMU-MOSEI, and also transfers well to multimodal intent recognition tasks. Ablation studies confirm that tri-subspace disentanglement and SACA jointly enhance the modeling of multi-granular cross-modal sentiment cues.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [The Category Mistake of Cislunar Time: Why NASA Cannot Synchronize What Doesn't Exist](https://arxiv.org/abs/2602.18641)
*Paul Borrill*

Main category: cs.DC

TL;DR: 本文分析了美国白宫指示NASA建立的协调月球时间（LTC）计划，指出该计划基于一个类别错误：将“同步时间”视为一种本体实体，而实际上它是一个认识论构造。通过前向时间唯一性假设、Spekkens的莱布尼茨操作主义等视角进行分析，提出了一种基于双边原子交互而非单向时间分配的交易替代方案。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示NASA的协调月球时间计划背后存在的哲学混淆，并提出更合理的替代方法。

Method: 采用哲学分析方法，特别是通过前向时间唯一性假设、Spekkens的莱布尼茨操作主义、Wood-Spekkens精细调节论证以及本体与认识论解释之间的区分来审视cislunar时间项目。

Result: 表明将量子力学中的概念转变应用于cislunar时间项目，可以澄清项目中关于时间同步性的误解，并指出了现有工程设计上的根本性问题。

Conclusion: 协调月球时间计划建立在一个哲学混淆之上，需要重新考虑其基础；建议探索一种基于双边原子交互的新方法作为可能的解决方案。

Abstract: In April 2024, the White House directed NASA to establish Coordinated Lunar Time (LTC) by December 2026. The programme assumes that a unified time standard can be constructed by deploying atomic clocks on the lunar surface, computing relativistic corrections, and distributing synchronized time via LunaNet. This paper argues that the entire enterprise rests on a category mistake in the sense introduced by Ryle and developed by Spekkens in quantum foundations: it treats "synchronized time" as an ontic entity -- something that exists independently and can be transmitted from authoritative sources to dependent receivers -- when it is in fact an epistemic construct: a model-dependent representation of observer-relative clock relationships. We analyze the cislunar time programme through the lens of Forward-In-Time-Only (FITO) assumptions, Spekkens' Leibnizian operationalism, the Wood-Spekkens fine-tuning argument, and the distinction between ontic and epistemic interpretations that has dissolved long-standing puzzles in quantum mechanics. We show that the same conceptual move that dissolves quantum "mysteries" -- recognizing what is epistemic versus what is ontic -- dissolves the apparent coherence of the cislunar time programme and reveals it as an engineering project built on a philosophical confusion. We sketch a transactional alternative grounded in bilateral atomic interactions rather than unidirectional time distribution.

</details>


### [4] [What Distributed Computing Got Wrong: The Category Mistake That Turned Design Choices into Laws of Nature](https://arxiv.org/abs/2602.18723)
*Paul Borrill*

Main category: cs.DC

TL;DR: 本文揭示了分布式计算中基础的不可能性结果（如FLP定理、二将军问题和CAP定理）并非自然界固有的物理限制，而是由于将仅向前时间流动的信息流（FITO）错误地视为自然法则。文章通过六个步骤论证了这一观点，并提出了基于双边交易的替代方案，指出分布式计算领域可能在过去五十年间一直在一个错误的设计空间内进行优化。


<details>
  <summary>Details</summary>
Motivation: 作者认为分布式计算中的经典不可能性结果是基于对信息流方向性的误解，即假设信息只能向前流动（FITO），而这一假设实际上是一个设计选择而非自然界的定律。因此，这些结果并不反映物理世界的真正限制。

Method: 通过引入范畴错误的概念框架，识别FITO作为隐藏公理的作用，应用Leibnizian原则分析FITO模型中的多余本体结构，探讨去除FITO后会发生什么变化，展示这些不可能性定理实际上是关于FITO系统而非物理学的结论，最后提出基于双边交互作用的替代方法。

Result: 证明了FLP定理等分布式计算领域的不可能性结果依赖于FITO假设；展示了当不采用FITO时，原有的不可能性可以被解决；提出了使用原子双边事务来代替单向消息传递的新方法论。

Conclusion: 分布式计算领域长期以来可能基于一个错误的前提进行研究和发展，即仅向前时间流动的信息流（FITO）。通过改变这一基本假设，有可能克服传统上被认为不可逾越的技术障碍。

Abstract: The foundational impossibility results of distributed computing -- the Fischer-Lynch-Paterson theorem, the Two Generals Problem, the CAP theorem -- are widely understood as discoveries about the physical limits of coordination. This paper argues that they are nothing of the sort. They are consequences of a category mistake: treating Forward-In-Time-Only (FITO) information flow as a law of nature rather than recognizing it as a design choice inherited from Shannon's channel model and Lamport's happened-before relation. We develop this argument in six steps. First, we introduce the category mistake framework from Ryle through Spekkens' ontic/epistemic distinction in quantum foundations. Second, we identify FITO as the hidden axiom that unifies the classical impossibility results. Third, we apply Spekkens' Leibnizian principle to show that FITO-based models contain surplus ontological structure. Fourth, we develop the counterfactual: what changes when FITO is dropped. Fifth, we demonstrate that the impossibility theorems are theorems about FITO systems, not about physics. Sixth, we sketch the transactional alternative -- bilateral interactions that dissolve the apparent impossibilities by replacing unidirectional message passing with atomic bilateral transactions. The implication is that distributed computing has spent fifty years optimizing within the wrong design space.

</details>


### [5] [BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS](https://arxiv.org/abs/2602.18755)
*Omar Basit,Yunzhao Liu,Z. Jonny Kong,Y. Charlie Hu*

Main category: cs.DC

TL;DR: 本文提出了一种名为BiScale的两层能源优化框架，专为解聚的大型语言模型服务设计。通过在粗略和精细时间尺度上分别进行放置与基线频率计算及GPU频率动态调整，BiScale能够在满足服务级别目标（SLO）的同时，在预填充阶段减少高达39%、在解码阶段减少高达48%的能量消耗。


<details>
  <summary>Details</summary>
Motivation: 随着LLM服务中越来越多地采用预填充/解码分离来改善延迟-吞吐量权衡并满足严格的TTFT和TPOT SLOs，如何有效管理能源消耗成为了一个挑战。现有方法如自动扩展过于粗糙，难以跟踪快速的工作负载波动；而解聚环境下的DVFS应用则因相位不对称动态以及供给与频率控制之间的耦合变得更加复杂。

Method: BiScale是一种为了解决上述问题而设计的两层能源优化框架。它结合了预测延迟和功耗模型来同时优化预填充和解码阶段的位置选择与DVFS。在较粗的时间尺度上，BiScale根据阶段意识计算出最优位置安排和基准频率，以最小化能耗并符合SLO约束条件；而在更细的时间尺度上，则通过阶段特定控制手段——对于预填充使用模型预测控制（MPC）考虑队列演变对未来TTFT的影响，对于解码采用轻量级松弛感知适应策略利用其较为平滑且受内存限制的动力学特性——来实现每迭代周期内的GPU频率自适应调整。

Result: 实验结果表明，在一个由16个H100 GPU组成的集群上运行Llama 3.3 70B模型时，相较于DistServe，BiScale不仅能够满足TTFT/TPOT SLOs要求，还能显著降低能量消耗：预填充阶段最多可节省39%，解码阶段最多可节省48%。

Conclusion: BiScale提供了一种有效的解决方案，通过跨时间尺度的协调控制实现了在保证严格服务级别目标的同时大幅度降低能源消耗的目标。

Abstract: Prefill/decode disaggregation is increasingly adopted in LLM serving to improve the latency-throughput tradeoff and meet strict TTFT and TPOT SLOs. However, LLM inference remains energy-hungry: autoscaling alone is too coarse-grained to track fast workload fluctuations, and applying fine-grained DVFS under disaggregation is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control.
  We present BiScale, a two-tier energy optimization framework for disaggregated LLM serving. BiScale jointly optimizes placement and DVFS across prefill and decode using predictive latency and power models. At coarse timescales, BiScale computes phase-aware placement and baseline frequencies that minimize energy while satisfying SLO constraints. At fine timescales, BiScale dynamically adapts GPU frequency per iteration using stage-specific control: model predictive control (MPC) for prefill to account for queue evolution and future TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics. This hierarchical design enables coordinated control across timescales while preserving strict serving SLOs.
  Evaluation on a 16x H100 cluster serving Llama 3.3 70B with production-style traces shows that BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe.

</details>


### [6] [WANSpec: Leveraging Global Compute Capacity for LLM Inference](https://arxiv.org/abs/2602.18931)
*Noah Martin,Fahad Dogar*

Main category: cs.DC

TL;DR: 本文探讨了通过WANSpec技术将部分大型语言模型的推理任务转移到利用率较低的数据中心，以缓解高需求数据中心的容量问题，并有效利用现场计算资源。实验表明，该方法可以在避免延迟增加的同时，减少高需求数据中心中推测解码草稿模型的前向传递超过50%。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）应用快速增长，导致高性能GPU需求激增，选择合适的运行位置对请求延迟有显著影响。为了解决这一问题，文章提出了一种新的方法来平衡不同数据中心之间的负载。

Method: 提出了WANSpec技术，它通过将部分LLM生成过程卸载到利用率较低的数据中心来进行工作。此过程采用了推测解码技术，这是一种广泛用于加速自回归解码的方法，通过将草稿模型移动到那些未充分利用的计算资源上执行。

Result: 实验结果证明，WANSpec能够在不增加延迟的前提下，减少高需求数据中心内推测解码草稿模型所需进行的前向传递次数超过一半。

Conclusion: WANSpec提供了一个有效的解决方案，可以减轻繁忙数据中心的压力，并且能够更有效地利用现有的计算资源。

Abstract: Data centers capable of running large language models (LLMs) are spread across the globe. Some have high end GPUs for running the most advanced models (100B+ parameters), and others are only suitable for smaller models (1B parameters). The most capable GPUs are under high demand thanks to the rapidly expanding applications of LLMs. Choosing the right location to run an LLM inference workload can have consequences on the latency of requests due to these high demands. In this work, we explore options to shift some aspects of inference to the under-utilized data centers. We first observe the varying delays affecting inference in AWS services from different regions, demonstrating that load is not spread evenly. We then introduce WANSpec, which offloads part of LLM generation to the under-utilized data centers. In doing so, WANSpec can mitigate capacity issues as well as effectively use on-site compute (ie at universities) to augment cloud providers. This is done with speculative decoding, a widely used technique to speed up auto-regressive decoding, by moving the draft model to the under-utilized compute resources. Our experiments in simulation and cloud deployments show that WANSpec can judiciously employ redundancy to avoid increases in latency while still reducing the forward passes of speculative decoding's draft model in high demand data centers by over 50%.

</details>


### [7] [ucTrace: A Multi-Layer Profiling Tool for UCX-driven Communication](https://arxiv.org/abs/2602.19084)
*Emir Gencer,Mohammad Kefah Taha Issa,Ilyas Turimbetov,James D. Trotter,Didem Unat*

Main category: cs.DC

TL;DR: 本文介绍了一种名为ucTrace的新工具，它能够提供细粒度的UCX级别通信追踪，帮助高性能计算环境中的系统管理员和开发者优化性能并调试通信模式。


<details>
  <summary>Details</summary>
Motivation: 现有的分析工具缺乏对UCX级别的精细通信追踪，无法捕捉传输层行为，或仅限于特定的MPI实现。为了填补这些空白，研究人员开发了ucTrace。

Method: 通过在UCX层面进行消息传递剖析，将主机与设备（如GPU和NIC）之间的操作直接与其源MPI函数关联起来，并通过交互式可视化展示进程和设备特定的交互。

Result: ucTrace展示了其特性，包括在不同UCX设置下的MPI点对点行为、跨MPI库的Allreduce比较、线性求解器的通信分析、NUMA绑定效果以及大规模带GPU加速的GROMACS分子动力学模拟的剖析。

Conclusion: ucTrace为HPC环境提供了重要的洞察力，有助于优化性能及调试大规模工作负载中的通信模式。

Abstract: UCX is a communication framework that enables low-latency, high-bandwidth communication in HPC systems. With its unified API, UCX facilitates efficient data transfers across multi-node CPU-GPU clusters. UCX is widely used as the transport layer for MPI, particularly in GPU-aware implementations. However, existing profiling tools lack fine-grained communication traces at the UCX level, do not capture transport-layer behavior, or are limited to specific MPI implementations.
  To address these gaps, we introduce ucTrace, a novel profiler that exposes and visualizes UCX-driven communication in HPC environments. ucTrace provides insights into MPI workflows by profiling message passing at the UCX level, linking operations between hosts and devices (e.g., GPUs and NICs) directly to their originating MPI functions. Through interactive visualizations of process- and device-specific interactions, ucTrace helps system administrators, library and application developers optimize performance and debug communication patterns in large-scale workloads. We demonstrate ucTrace's features through a wide range of experiments including MPI point-to-point behavior under different UCX settings, Allreduce comparisons across MPI libraries, communication analysis of a linear solver, NUMA binding effects, and profiling of GROMACS MD simulations with GPU acceleration at scale. ucTrace is publicly available at https://github.com/ParCoreLab/ucTrace.

</details>


### [8] [A Formal Framework for Predicting Distributed System Performance under Faults](https://arxiv.org/abs/2602.19088)
*Ziwei Zhou,Si Liu,Zhou Zhou,Peixin Wang,MIn Zhang*

Main category: cs.DC

TL;DR: 提出了一种新的形式化框架，能够系统地预测分布式系统在各种故障场景下的性能，并通过PERF工具实现了这一框架，该工具能够准确预测不同故障设置下的系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前的分布式系统运行在复杂的环境中，不可避免地会遇到故障甚至对抗行为。从形式化设计直接预测这种环境下的系统性能一直是一个长期存在的挑战。

Method: 开发了一个包含故障注入器和多种故障库的形式化框架，以及将系统与故障注入器集成到一个适合于统计分析统一模型中的模型组合。此框架使用Maude进行了形式化，并实现为自动化工具PERF。

Result: PERF工具被应用于代表性的分布式系统中，能够精确预测不同故障设置下的系统性能，其基于形式化设计的估计与实际部署评估结果一致。

Conclusion: 本文介绍的形式化框架和PERF工具提供了一种有效的方法来预测分布式系统在存在多样故障情况下的性能表现。

Abstract: Today's distributed systems operate in complex environments that inevitably involve faults and even adversarial behaviors. Predicting their performance under such environments directly from formal designs remains a longstanding challenge. We present the first formal framework that systematically enables performance prediction of distributed systems across diverse faulty scenarios. Our framework features a fault injector together with a wide range of faults, reusable as a library, and model compositions that integrate the system and the fault injector into a unified model suitable for statistical analysis of performance properties such as throughput and latency. We formalize the framework in Maude and implement it as an automated tool, PERF. Applied to representative distributed systems, PERF accurately predicts system performance under varying fault settings, with estimations from formal designs consistent with evaluations on real deployments.

</details>


### [9] [Semantic Conflict Model for Collaborative Data Structures](https://arxiv.org/abs/2602.19231)
*Georgii Semenov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文提出了一种新的冲突模型，用于支持显式、本地优先的冲突解决方式，无需中央协调。该模型通过操作之间的语义依赖来识别冲突，并通过复制日志上的三方合并将冲突的操作重新基于一个调和操作上进行解决。


<details>
  <summary>Details</summary>
Motivation: 现有的冲突自由复制数据类型（CRDTs）虽然确保了通过内置冲突解决方案的一致性，但这种解决方案通常是隐含且对用户不透明的；而现有的调解技术往往依赖于集中式协调。因此，需要一种能够实现显式且本地优先冲突解决的方法，同时避免中心化控制的需求。

Method: 研究者们开发了一个适用于协作数据结构的冲突模型，该模型利用操作间存在的语义依赖关系来检测冲突，并采用三方合并技术在复制的日志基础上将相互冲突的操作重基到一个可以用来调解的操作上。

Result: 展示了该方法如何应用于协作寄存器，包括最后写入者获胜寄存器的一个明确形式化定义以及一个支持半自动调解的多寄存器实体。

Conclusion: 所提出的冲突模型为数字协作系统中的异步工作提供了一种新的途径，允许以更透明的方式处理冲突，同时保持了去中心化的特性。

Abstract: Digital collaboration systems support asynchronous work over replicated data, where conflicts arise when concurrent operations cannot be unambiguously integrated into a shared history. While Conflict-Free Replicated Data Types (CRDTs) ensure convergence through built-in conflict resolution, this resolution is typically implicit and opaque to users, whereas existing reconciliation techniques often rely on centralized coordination. This paper introduces a conflict model for collaborative data structures that enables explicit, local-first conflict resolution without central coordination. The model identifies conflicts using semantic dependencies between operations and resolves them by rebasing conflicting operations onto a reconciling operation via a three-way merge over a replicated journal. We demonstrate our approach on collaborative registers, including an explicit formulation of the Last-Writer-Wins Register and a multi-register entity supporting semi-automatic reconciliation.

</details>


### [10] [Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement](https://arxiv.org/abs/2602.19338)
*Halit Uyanık,Tolga Ovatman*

Main category: cs.DC

TL;DR: 本研究通过约束编程优化方法平衡复杂事件处理(CEP)任务图中不同路径之间的执行成本，实现了一个Python库以帮助小型IoT设备自适应地优化代码和I/O分配，从而提高整体延迟和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着物联网(IoT)环境中输入数据的多样性和任务复杂性的增加，需要考虑到边缘设备硬件及计算能力有限的解决方案。复杂事件处理(CEP)，作为一种典型应用场景，涉及从多个源读取和聚合数据以推断重要事件的发生。

Method: 采用了一种约束编程优化方法来平衡CEP任务图中不同路径间的执行成本，并特别关注于改善关键路径性能。该方法被封装成一个Python库，允许小规模IoT设备动态调整代码执行与输入输出配置，并通过抽象通信细节实现设备间共享内存的虚拟化。

Result: 实验结果表明，通过对关键路径性能进行优化，可以在多设备间执行CEP操作时显著提高吞吐量并减少延迟。

Conclusion: 本文提出的方法有效地解决了IoT环境下复杂事件处理过程中遇到的挑战，为提高IoT系统性能提供了新的思路。

Abstract: The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations.

</details>


### [11] [GPU-Resident Gaussian Process Regression Leveraging Asynchronous Tasks with HPX](https://arxiv.org/abs/2602.19683)
*Henrik Möllmann,Dirk Pflüger,Alexander Strack*

Main category: cs.DC

TL;DR: 本文扩展了GPRat库，通过引入完全基于GPU的高斯过程预测管道来解决高斯过程回归中的可扩展性问题。利用优化后的CUDA库实现了分块算法，对于大于128个训练样本的数据集，GPU实现提供了加速效果。


<details>
  <summary>Details</summary>
Motivation: 高斯过程（GP）是一种广泛使用的回归工具，但精确求解器的立方复杂度限制了其可扩展性。为了解决这一挑战，研究者们希望通过引入完全基于GPU的GP预测流程来提高处理大规模数据集时的效率。

Method: 作者们通过使用优化过的CUDA库实现了针对GP预测的分块算法，从而充分利用了线性代数运算中的大规模并行性。同时，他们还评估了最佳CUDA流的数量，并将GPU实现与现有的基于CPU的实现进行了性能比较。

Result: 实验结果表明，对于超过128个训练样本的数据集，GPU实现能够提供加速效果；Cholesky分解本身最高可达4.3倍加速，而GP预测则能达到4.6倍加速。此外，结合HPX与多个CUDA流使得GPRat在大尺寸数据集上的表现甚至超过了cuSOLVER达11%。

Conclusion: 通过引入基于GPU的预测管道以及利用优化的CUDA库，本研究成功地提高了GPRat处理大规模数据集的能力，尤其是在训练样本数量较多的情况下表现出显著的速度提升优势。

Abstract: Gaussian processes (GPs) are a widely used regression tool, but the cubic complexity of exact solvers limits their scalability. To address this challenge, we extend the GPRat library by incorporating a fully GPU-resident GP prediction pipeline. GPRat is an HPX-based library that combines task-based parallelism with an intuitive Python API.
  We implement tiled algorithms for the GP prediction using optimized CUDA libraries, thereby exploiting massive parallelism for linear algebra operations. We evaluate the optimal number of CUDA streams and compare the performance of our GPU implementation to the existing CPU-based implementation. Our results show the GPU implementation provides speedups for datasets larger than 128 training samples. We observe speedups of up to 4.3 for the Cholesky decomposition itself and 4.6 for the GP prediction. Furthermore, combining HPX with multiple CUDA streams allows GPRat to match, and for large datasets, surpass cuSOLVER's performance by up to 11 percent.

</details>


### [12] [A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response](https://arxiv.org/abs/2602.19742)
*Yulun Huang,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 该论文提出了一种集成框架，用于优化无人机路线规划、机队规模和边缘服务提供，以实现野火监测。通过结合历史火灾加权聚类、QoS感知的边缘分配、2-opt路线优化以及动态紧急重路由机制，该框架显著减少了响应时间、能耗和所需机队规模，并且能够在不明显影响常规操作的情况下迅速应对紧急情况。


<details>
  <summary>Details</summary>
Motivation: 针对野火监测中及时数据收集与处理的需求，以及现有UAV辅助边缘计算方法在满足能源、重访时间和容量限制条件下同时最小化端到端服务响应时间方面存在的挑战。

Method: 提出了一个集成框架，该框架共同优化了UAV路径规划、舰队规模及边缘服务提供。此框架结合了基于火灾历史权重的聚类来优先考虑高风险区域、QoS感知的边缘分配以平衡接近度和计算负载、采用2-opt路线优化并具有自适应舰队规模调整能力，还包括一种动态紧急重新路由机制。

Result: 实验表明，与GA, PSO和贪婪基线相比，所提出的框架平均响应时间降低了70.6-84.2%，能量消耗降低了73.8-88.4%，舰队规模减少了26.7-42.1%。紧急机制能在233秒内作出反应，远低于300秒的截止期限，对正常运作的影响微乎其微。

Conclusion: 本研究提出的集成框架有效解决了野火监测中的关键问题，包括降低响应时间、减少能量消耗和优化舰队规模等，为未来灾害管理提供了强有力的支持。

Abstract: Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging. We propose an integrated framework that co-optimizes UAV route planning, fleet sizing, and edge service provisioning for wildfire monitoring. The framework combines fire-history-weighted clustering to prioritize high-risk areas, Quality of Service (QoS)-aware edge assignment balancing proximity and computational load, 2-opt route optimization with adaptive fleet sizing, and a dynamic emergency rerouting mechanism. The key insight is that these subproblems are interdependent: clustering decisions simultaneously shape patrol efficiency and edge workloads, while capacity constraints feed back into feasible configurations. Experiments show that the proposed framework reduces average response time by 70.6--84.2%, energy consumption by 73.8--88.4%, and fleet size by 26.7--42.1% compared to GA, PSO, and greedy baselines. The emergency mechanism responds within 233 seconds, well under the 300-second deadline, with negligible impact on normal operations.

</details>


### [13] [Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation](https://arxiv.org/abs/2602.20097)
*Pu Jiao,Sheng Di,Jiannan Tian,Mingze Xia,Xuan Wu,Yang Zhang,Xin Liang,Franck Cappello*

Main category: cs.DC

TL;DR: 本文研究了基于预量化的压缩器产生的伪影，并提出了一种新的算法来减轻这些伪影，从而提高解压缩数据的质量同时保持高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 基于预量化的压缩器虽然能实现极高吞吐量，但在中等或较大用户指定误差界限下通常会遇到数据质量低的问题。为了解决这个问题，作者们研究了由这种压缩方式导致的伪影并寻求改善方法。

Method: 1. 详细描述了基于预量化压缩器中的伪影特征，以理解量化索引与压缩错误之间的关系。
2. 提出了一种新颖的量化感知插值算法来改进解压缩后的数据质量。
3. 在共享内存和分布式内存环境中对提出的算法进行了并行化处理，以获得高性能表现。
4. 利用五个真实世界的数据集对算法进行了评估，并通过两种领先的基于预量化的压缩器验证了其有效性。

Result: 实验表明，所提出的伪影缓解算法能够有效地提高基于预量化压缩器生成的解压缩数据的质量，同时维持其较高的压缩吞吐量。

Conclusion: 这项工作为减少基于预量化压缩技术所产生的负面影响提供了一个有效的方法，在不影响压缩速度的前提下提升了数据质量。

Abstract: Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or large user-specified error bounds. In this paper, we investigate the artifacts generated by pre-quantization based compressors and propose a novel algorithm to mitigate them. Our contributions are fourfold: (1) We carefully characterize the artifacts in pre-quantization based compressors to understand the correlation between the quantization index and compression error; (2) We propose a novel quantization-aware interpolation algorithm to improve the decompressed data; (3) We parallelize our algorithm in both shared-memory and distributed-memory environments to obtain high performance; (4) We evaluate our algorithm and validate it with two leading pre-quantization based compressors using five real-world datasets. Experiments demonstrate that our artifact mitigation algorithm can effectively improve the quality of decompressed data produced by pre-quantization based compressors while maintaining their high compression throughput.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [14] [Adaptive Multi-Agent Reasoning for Text-to-Video Retrieval](https://arxiv.org/abs/2602.19040)
*Jiaxin Wu,Xiao-Yong Wei,Qing Li*

Main category: cs.IR

TL;DR: 提出了一种自适应多智能体检索框架，以解决现有零样本文本到视频检索系统在处理复杂查询时遇到的时序、逻辑或因果关系推理问题。该框架包括检索智能体、推理智能体和查询重构智能体，并通过一个协调智能体动态管理这些智能体间的合作。此外，还引入了一种新的通信机制来加强协作与决策。实验表明，此方法相比CLIP4Clip和其他最先进方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本跨模态对齐技术虽然有所进步，但在处理包含时间、逻辑或因果关系等复杂查询方面仍存在局限性。特别是对于依赖于查询的时间推理能力不足，限制了其在实际应用中的有效性。

Method: 设计了一个自适应多智能体检索框架，其中包括：1) 用于大规模视频库检索的检索智能体；2) 进行零样本上下文时间推理的推理智能体；3) 对模糊查询进行细化并恢复因迭代而性能下降的查询重构智能体。此外，还有一个协调智能体负责根据中间反馈和推理结果动态地协调上述智能体的工作。同时，开发了一种新的通信机制，该机制利用检索性能记忆和历史推理轨迹来提高协作与决策效率。

Result: 在三个TRECVid基准测试上进行了实验，涵盖了八年的时间跨度，结果显示提出的框架比CLIP4Clip提高了两倍的性能，并且远超当前最先进的方法。

Conclusion: 本研究提出的自适应多智能体检索框架能够有效应对复杂的查询任务，特别是在需要进行时间、逻辑或因果关系推理的情况下。通过引入专门的智能体及创新的通信机制，不仅增强了系统的灵活性与鲁棒性，而且在多个基准测试中取得了显著优于现有方法的表现。

Abstract: The rise of short-form video platforms and the emergence of multimodal large language models (MLLMs) have amplified the need for scalable, effective, zero-shot text-to-video retrieval systems. While recent advances in large-scale pretraining have improved zero-shot cross-modal alignment, existing methods still struggle with query-dependent temporal reasoning, limiting their effectiveness on complex queries involving temporal, logical, or causal relationships. To address these limitations, we propose an adaptive multi-agent retrieval framework that dynamically orchestrates specialized agents over multiple reasoning iterations based on the demands of each query. The framework includes: (1) a retrieval agent for scalable retrieval over large video corpora, (2) a reasoning agent for zero-shot contextual temporal reasoning, and (3) a query reformulation agent for refining ambiguous queries and recovering performance for those that degrade over iterations. These agents are dynamically coordinated by an orchestration agent, which leverages intermediate feedback and reasoning outcomes to guide execution. We also introduce a novel communication mechanism that incorporates retrieval-performance memory and historical reasoning traces to improve coordination and decision-making. Experiments on three TRECVid benchmarks spanning eight years show that our framework achieves a twofold improvement over CLIP4Clip and significantly outperforms state-of-the-art methods by a large margin.

</details>


### [15] [FineRef: Fine-Grained Error Reflection and Correction for Long-Form Generation with Citations](https://arxiv.org/abs/2602.18437)
*Yixing Peng,Licheng Zhang,Shancheng Fang,Yi Liu,Peijian Gu,Quan Wang*

Main category: cs.IR

TL;DR: 本文提出了一种名为FineRef的框架，通过细粒度错误反思来教导模型自我识别和纠正引用错误，包括不匹配和不相关的问题。该框架采用两阶段训练策略，结合了监督微调和过程级强化学习。实验结果表明，FineRef在引用性能和答案准确性上都有显著提升，并且在领域转移和噪声检索场景中表现出良好的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在生成带有引用的内容时，经常会出现引用与内容不匹配或不相关的情况。当前的方法过于优化引用的准确性而忽视了与用户查询的相关性，这导致了答案质量和鲁棒性的下降。此外，在需要多个引用的长文本生成中，单次处理的方式难以提供最佳答案。为了解决这些问题，作者提出了FineRef框架。

Method: FineRef框架基于细粒度错误反思设计，旨在明确地教会模型如何自我识别并逐个纠正两大类引用错误：不匹配和不相关。它采用了两阶段训练策略。第一阶段通过监督微调引入“尝试-反思-纠正”的行为模式，使用专门轻量级模型构建的精细可控反思数据进行；同时设计了一个在线自反思引导策略，通过迭代增加经过验证的、自我改进的例子来丰富训练数据以提高泛化能力。第二阶段则应用过程级别的强化学习，配备多维度奖励方案以促进反思准确性、答案质量以及修正增益。

Result: 在ALCE基准测试中的实验显示，FineRef在引用表现和答案准确率方面都有显著改进。相比GPT-4，7B模型在Citation F1上提高了最多18%，EM Recall上提高了4%，并且在关键评估指标上超过了最先进的模型。此外，FineRef还在领域转换设置及噪音检索情景下展现了强大的通用性和鲁棒性。

Conclusion: 综上所述，FineRef提供了一种有效的方法来解决大型语言模型生成过程中遇到的引用问题，不仅提高了引用的准确性和答案的质量，还增强了模型对于不同应用场景下的适应能力和稳定性。

Abstract: Generating with citations is crucial for trustworthy Large Language Models (LLMs), yet even advanced LLMs often produce mismatched or irrelevant citations. Existing methods over-optimize citation fidelity while overlooking relevance to the user query, which degrades answer quality and robustness in real-world settings with noisy or irrelevant retrieved content. Moreover, the prevailing single-pass paradigm struggles to deliver optimal answers in long-form generation that requiring multiple citations. To address these limitations, we propose FineRef, a framework based on Fine-grained error Reflection, which explicitly teaches the model to self-identify and correct two key citation errors, mismatch and irrelevance, on a per-citation basis. FineRef follows a two-stage training strategy. The first stage instills an "attempt-reflect-correct" behavioral pattern via supervised fine-tuning, using fine-grained and controllable reflection data constructed by specialized lightweight models. An online self-reflective bootstrapping strategy is designed to improve generalization by iteratively enriching training data with verified, self-improving examples. To further enhance the self-reflection and correction capability, the second stage applies process-level reinforcement learning with a multi-dimensional reward scheme that promotes reflection accuracy, answer quality, and correction gain. Experiments on the ALCE benchmark demonstrate that FineRef significantly improves both citation performance and answer accuracy. Our 7B model outperforms GPT-4 by up to 18% in Citation F1 and 4% in EM Recall, while also surpassing the state-of-the-art model across key evaluation metrics. FineRef also exhibits strong generalization and robustness in domain transfer settings and noisy retrieval scenarios.

</details>


### [16] [Altar: Structuring Sharable Experimental Data from Early Exploration to Publication](https://arxiv.org/abs/2602.18588)
*William Gaultier,Andrea Lodetti,Ian Coghill,David Colliaux,Maximilian Fleck,Alienor Lahlou*

Main category: cs.IR

TL;DR: 本文介绍了一种名为Altar的轻量级、领域无关框架，用于从项目开始就结构化实验数据，而无需施加严格的数据模型。它基于Sacred实验跟踪模型构建，旨在确保可重复性并防止在发表时需要追溯重建。


<details>
  <summary>Details</summary>
Motivation: 在实验项目的活跃开发阶段，尤其是在协作研究中，管理和元数据管理是一个重大挑战。尽管在这个阶段确保可重复性和避免在发表时进行追溯重建非常重要，但这一阶段经常被项目提案中的数据管理计划所忽视。

Method: 提出了Altar，一个轻量级且不受领域限制的框架，它围绕着Sacred实验追踪模型设计，用来捕捉并结构化实验（元）数据。参数、元数据、曲线和小文件存储在一个灵活的NoSQL数据库里，而大的原始数据则保存在专门的存储空间并通过唯一标识符链接起来，以保证效率和可追溯性。

Result: Altar为不同技能水平的用户提供了多种使用途径，并且可以与现有工作流程结合，最小程度地干扰工作习惯。虽然开始使用Altar不需要专门的基础设施，但该框架易于部署到服务器上，并且在扩展或准备发表数据时可以公开访问。

Conclusion: 通过解决研究的动态阶段问题，Altar为探索性实验和符合FAIR原则的数据共享之间提供了一个实用桥梁。

Abstract: Managing the data and metadata during the active development phase of an experimental project presents a significant challenge, particularly in collaborative research. This phase is frequently overlooked in Data Management Plans included in project proposals, despite its important role in ensuring reproducibility and preventing the need for retroactive reconstruction at the time of publication. Here we present Altar, a lightweight, domain-agnostic framework for structuring experimental data from the onset of a project without imposing rigid data models. Altar is built around the Sacred experiment-tracking model and captures experimental (meta)data and structures them. Parameters, metadata, curves and small files are stored in a flexible NoSQL database, while large raw data are maintained in dedicated storage and linked through unique identifiers, ensuring efficiency and traceability. This integration is composable with exiting workflows, allowing integration with minimial disruption of work habits. We document different pathways to use Altar based on users skillset (PhD students, Post-docs, Principal Investigators, Laboratory administrators, System administrators). While getting started with Altar does not require a specialized infrastructure, the framework can be easily deployed on a server and made publicly accessible when scaling up or preparing data for publication. By addressing the dynamic phase of research, Altar provides a practical bridge between exploratory experimentation and FAIR-aligned data sharing.

</details>


### [17] [Towards Reliable Negative Sampling for Recommendation with Implicit Feedback via In-Community Popularity](https://arxiv.org/abs/2602.18759)
*Chen Chen,Haobo Lin,Yuanbo Xu*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架ICPNS（In-Community Popularity Negative Sampling），利用用户社区结构来识别可靠的负样本。该方法基于物品曝光由潜在的用户社区驱动的洞察，通过识别这些社区并利用社区内流行度有效近似物品曝光概率。实验结果表明，ICPNS在基于图的推荐系统上表现出了持续改进，并且在MF（矩阵分解）模型上也有竞争力。


<details>
  <summary>Details</summary>
Motivation: 从隐式反馈中学习是现代推荐系统中的一个基本问题，在这种情况下只能观察到正面交互而没有明确的负面信号。在这种设定下，负采样对于通过构建负项来实现有效的偏好学习和排名优化起着关键作用。然而，设计可靠的负采样策略仍然具有挑战性，因为它们必须同时确保真实性、难度和可解释性。

Method: 提出了ICPNS（In-Community Popularity Negative Sampling）框架，该方法利用了用户社区结构来确定可靠且信息丰富的负样本。基于物品曝光是由潜在用户社区驱动的观点，通过识别这些社区并使用社区内部的受欢迎程度，ICPNS能够有效地估计出物品被曝光的概率。因此，那些在用户所属社区内很受欢迎但用户未点击的物品被视为更可信的真实负样本。

Result: 广泛的实验结果表明，ICPNS在四个基准数据集上的基于图的推荐器上取得了持续性的改进，并且在基于MF（矩阵分解）的模型上也表现出色，超过了代表性的负采样策略。

Conclusion: ICPNS提供了一种创新的方法来处理推荐系统中的负采样问题，通过考虑用户所属社区内的流行度来进行负样本的选择，从而提高了推荐系统的性能。

Abstract: Learning from implicit feedback is a fundamental problem in modern recommender systems, where only positive interactions are observed and explicit negative signals are unavailable. In such settings, negative sampling plays a critical role in model training by constructing negative items that enable effective preference learning and ranking optimization. However, designing reliable negative sampling strategies remains challenging, as they must simultaneously ensure realness, hardness, and interpretability. To this end, we propose \textbf{ICPNS (In-Community Popularity Negative Sampling)}, a novel framework that leverages user community structure to identify reliable and informative negative samples. Our approach is grounded in the insight that item exposure is driven by latent user communities. By identifying these communities and utilizing in-community popularity, ICPNS effectively approximates the probability of item exposure. Consequently, items that are popular within a user's community but remain unclicked are identified as more reliable true negatives. Extensive experiments on four benchmark datasets demonstrate that ICPNS yields consistent improvements on graph-based recommenders and competitive performance on MF-based models, outperforming representative negative sampling strategies under a unified evaluation protocol.

</details>


### [18] [Give Users the Wheel: Towards Promptable Recommendation Paradigm](https://arxiv.org/abs/2602.18929)
*Fuyuan Lyu,Chenglin Luo,Qiyuan Zhang,Yupeng Hou,Haolun Wu,Xing Tang,Xue Liu,Jin L. C. Guo,Xiuqiang He*

Main category: cs.IR

TL;DR: 本文提出了一种解耦提示可调序列推荐（DPR）框架，该框架使传统的序列推荐模型能够通过自然语言动态引导检索过程，同时保持协同信号。实验表明DPR在提示引导任务中显著优于现有最先进方法，并且在标准序列推荐场景中也保持了竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的序列推荐模型无法适应用户即时目标与历史习惯不一致的情况，而大型语言模型虽然可以提供语义推理能力来解释这种意图，但其集成方式要么牺牲了基于ID检索的效率和协作精度，要么受限于基础模型的召回能力。

Method: 提出了Decoupled Promptable Sequential Recommendation (DPR)框架，通过引入Fusion模块对协作和语义信号进行对齐、采用Mixture-of-Experts架构分离正负指导的梯度冲突以及实施三阶段训练策略逐步对齐提示的语义空间与协作空间。

Result: 在真实数据集上的广泛实验证明，DPR在提示导向任务中的表现明显优于最新的基准模型，同时在标准序列推荐场景下也保持了竞争性的性能。

Conclusion: DPR框架成功地为传统序列推荐模型赋予了通过自然语言提示动态调整检索过程的能力，同时保持了对协作信号的支持，在多种推荐场景下表现出色。

Abstract: Conventional sequential recommendation models have achieved remarkable success in mining implicit behavioral patterns. However, these architectures remain structurally blind to explicit user intent: they struggle to adapt when a user's immediate goal (e.g., expressed via a natural language prompt) deviates from their historical habits. While Large Language Models (LLMs) offer the semantic reasoning to interpret such intent, existing integration paradigms force a dilemma: LLM-as-a-recommender paradigm sacrifices the efficiency and collaborative precision of ID-based retrieval, while Reranking methods are inherently bottlenecked by the recall capabilities of the underlying model. In this paper, we propose Decoupled Promptable Sequential Recommendation (DPR), a model-agnostic framework that empowers conventional sequential backbones to natively support Promptable Recommendation, the ability to dynamically steer the retrieval process using natural language without abandoning collaborative signals. DPR modulates the latent user representation directly within the retrieval space. To achieve this, we introduce a Fusion module to align the collaborative and semantic signals, a Mixture-of-Experts (MoE) architecture that disentangles the conflicting gradients from positive and negative steering, and a three-stage training strategy that progressively aligns the semantic space of prompts with the collaborative space. Extensive experiments on real-world datasets demonstrate that DPR significantly outperforms state-of-the-art baselines in prompt-guided tasks while maintaining competitive performance in standard sequential recommendation scenarios.

</details>


### [19] [SIDEKICK: A Semantically Integrated Resource for Drug Effects, Indications, and Contraindications](https://arxiv.org/abs/2602.19183)
*Mohammad Ashhad,Olga Mashkova,Ricardo Henao,Robert Hoehndorf*

Main category: cs.IR

TL;DR: 研究人员开发了SIDEKICK，一个基于FDA结构化产品标签的知识图谱，用于标准化药物的适应症、禁忌症和不良反应信息。通过使用大型语言模型（LLM）提取和图检索增强生成（Graph RAG）进行本体映射的方法处理超过50,000种药物标签，并将术语映射到人类表型本体(HPO)、MONDO疾病本体和RxNorm上。该资源在基于副作用相似性的药物再利用任务中优于SIDER和ONSIDES数据库。此外，研究者还将数据集序列化为RDF图，并采用Semanticscience Integrated Ontology (SIO)作为顶层本体来提高互操作性，从而支持自动化安全性监测及基于表型的相似性分析。


<details>
  <summary>Details</summary>
Motivation: 当前的药物安全数据集常依赖于如MedDRA这样的术语体系，这限制了它们在语义推理能力以及与语义网本体和知识图谱间的互操作性。为解决这一问题，研究人员旨在创建一种新的解决方案，能够提供更好的语义集成能力和互操作性。

Method: 1. 从FDA结构化产品标签中提取药物相关信息；2. 使用大型语言模型(LLM)和图检索增强生成(Graph RAG)技术来进行本体映射；3. 将提取的数据映射至HPO、MONDO疾病本体论及RxNorm等标准术语库；4. 数据集被序列化为RDF格式，并采用了Semanticscience Integrated Ontology (SIO)作为高层级本体以增强不同系统之间的兼容性。

Result: SIDEKICK作为一个语义整合资源，在根据副作用相似度进行药物重新定位的任务中表现优于现有的SIDER和ONSIDES数据库。同时，它还支持自动化安全监控以及基于表型的相似性分析。

Conclusion: 通过构建SIDEKICK知识图谱，研究成功地提升了药物安全数据的语义推理能力和跨平台互操作性，为药物再利用提供了强有力的支持。

Abstract: Pharmacovigilance and clinical decision support systems utilize structured drug safety data to guide medical practice. However, existing datasets frequently depend on terminologies such as MedDRA, which limits their semantic reasoning capabilities and their interoperability with Semantic Web ontologies and knowledge graphs. To address this gap, we developed SIDEKICK, a knowledge graph that standardizes drug indications, contraindications, and adverse reactions from FDA Structured Product Labels. We developed and used a workflow based on Large Language Model (LLM) extraction and Graph-Retrieval Augmented Generation (Graph RAG) for ontology mapping. We processed over 50,000 drug labels and mapped terms to the Human Phenotype Ontology (HPO), the MONDO Disease Ontology, and RxNorm. Our semantically integrated resource outperforms the SIDER and ONSIDES databases when applied to the task of drug repurposing by side effect similarity. We serialized the dataset as a Resource Description Framework (RDF) graph and employed the Semanticscience Integrated Ontology (SIO) as upper level ontology to further improve interoperability. Consequently, SIDEKICK enables automated safety surveillance and phenotype-based similarity analysis for drug repurposing.

</details>


### [20] [SplitLight: An Exploratory Toolkit for Recommender Systems Datasets and Splits](https://arxiv.org/abs/2602.19339)
*Anna Volodkevich,Dmitry Anikin,Danil Gusak,Anton Klenitskiy,Evgeny Frolov,Alexey Vasilev*

Main category: cs.IR

TL;DR: 本文介绍了一个名为SplitLight的开源工具包，它可以帮助研究人员和从业者设计预处理和分割流程，并审查外部工件，从而使得决策过程可测量、可比较且可报告。


<details>
  <summary>Details</summary>
Motivation: 离线评估推荐系统时，数据准备过程中隐藏或记录不足的选择往往会影响结果。这些看似微小的决定，如过滤方式、重复处理、冷启动处理及分割策略的设计，都可能极大地改变模型排名，并削弱研究的可再现性和论文间的可比性。为了解决这些问题，作者提出了SplitLight工具包。

Method: SplitLight是一个探索性工具包，给定一个交互日志和由此产生的分割子集后，该工具能够分析核心与时间序列的数据统计信息，描述重复消费模式和时间戳异常，并诊断分割的有效性（包括时间泄漏、冷用户/项目暴露以及分布变化）。此外，SplitLight还支持通过综合汇总和交互式可视化来并排比较不同的分割策略。

Result: SplitLight作为Python工具包和交互式的无代码界面提供，可以生成审计摘要以证明评估协议的合理性，并支持在推荐系统的研究和行业中进行透明、可靠且可比较的实验。

Conclusion: SplitLight旨在提高推荐系统离线评估过程中数据准备阶段决策的透明度、可比性和报告能力，有助于改善研究重现性和跨论文对比。

Abstract: Offline evaluation of recommender systems is often affected by hidden, under-documented choices in data preparation. Seemingly minor decisions in filtering, handling repeats, cold-start treatment, and splitting strategy design can substantially reorder model rankings and undermine reproducibility and cross-paper comparability.
  In this paper, we introduce SplitLight, an open-source exploratory toolkit that enables researchers and practitioners designing preprocessing and splitting pipelines or reviewing external artifacts to make these decisions measurable, comparable, and reportable. Given an interaction log and derived split subsets, SplitLight analyzes core and temporal dataset statistics, characterizes repeat consumption patterns and timestamp anomalies, and diagnoses split validity, including temporal leakage, cold-user/item exposure, and distribution shifts. SplitLight further allows side-by-side comparison of alternative splitting strategies through comprehensive aggregated summaries and interactive visualizations. Delivered as both a Python toolkit and an interactive no-code interface, SplitLight produces audit summaries that justify evaluation protocols and support transparent, reliable, and comparable experimentation in recommender systems research and industry.

</details>


### [21] [DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework](https://arxiv.org/abs/2602.19702)
*Adamya Shyam,Venkateswara Rao Kagita,Bharti Rana,Vikas Kumar*

Main category: cs.IR

TL;DR: 本文提出了一种统一的多模态推荐框架DReX，该框架通过从多模态反馈中利用交互级别的特征逐步完善用户和项目表示。实验结果表明，该方法在所有评估的数据集上都优于最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推荐系统方法存在一些关键限制：单独处理不同的模态、在训练过程中需要每个交互都有完整的多模态数据、或者独立学习用户和项目的表示。这些问题导致了复杂性的增加以及用户与项目嵌入之间潜在的不一致。

Method: 提出了DReX，一个统一的多模态推荐框架，它通过利用来自多模态反馈的交互级特征来逐步改进用户和物品表示。模型使用门控循环单元有选择地将这些细粒度特征整合到全局表示中。

Result: 所提方法在考虑了包含评论文本为一种模态的三个真实世界数据集上的性能得到了评估。结果显示，我们的方法在所有被评估的数据集上均优于最先进的方法。此外，通过将评论文本视为一种模态，该方法还能自动生成用户和项目的可解释关键词配置文件。

Conclusion: DReX提供了一种有效的方法来克服当前多模态推荐系统中的挑战，包括能够同时建模细致的互动细节和更广泛的偏好模式，无需单独进行用户和项目特征提取过程，并且对变化或缺失的模态具有内在鲁棒性。

Abstract: Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.

</details>


### [22] [A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs](https://arxiv.org/abs/2602.19711)
*Krzysztof Kutt,Elżbieta Sroka,Oleksandra Ishchuk,Luiz do Valle Miranda*

Main category: cs.IR

TL;DR: 本文提出了一种混合推荐方法，该方法结合了知识图谱嵌入、近似最近邻搜索和SPARQL驱动的语义过滤技术。通过对JUHMP知识图谱进行实验，使用TransE、ComplEx、ConvE和CompGCN四种嵌入模型，并对ComplEx和HNSW进行了超参数选择。最终实现了三阶段神经-符号推荐器，在稀疏且异构的元数据条件下也能产生有用且可解释的推荐结果。


<details>
  <summary>Details</summary>
Motivation: 随着数字文化遗产资源数量的增长，需要开发先进的推荐方法来理解异构数据实体之间的语义关系。

Method: 本研究提出了一个完整的实现方法论，包括构建混合推荐流程，该流程整合了知识图谱嵌入、近似最近邻搜索及SPARQL驱动的语义筛选。在由CHExRISH项目创建的JUHMP知识图谱上评估了四种不同的嵌入模型（TransE, ComplEx, ConvE, CompGCN），并对ComplEx和HNSW执行了超参数调整。

Result: 尽管面临稀疏与异构的元数据挑战，所提出的方法仍能生成有价值且易于理解的推荐建议。这些推荐的有效性也得到了专家评审的支持。

Conclusion: 通过将知识图谱嵌入、近似最近邻搜索以及基于SPARQL的语义过滤相结合的方式，即使是在面对稀疏和多样化的元数据时，也能成功地为数字文化遗产提供有效而透明的推荐服务。

Abstract: The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowledge graph developed within the CHExRISH project, which at the time of experimentation contained ${\approx}3.2$M RDF triples describing people, events, objects, and historical relations affiliated with the Jagiellonian University (Kraków, PL). We evaluate four embedding families (TransE, ComplEx, ConvE, CompGCN) and perform hyperparameter selection for ComplEx and HNSW. Then, we present and evaluate the final three-stage neuro-symbolic recommender. Despite sparse and heterogeneous metadata, the approach produces useful and explainable recommendations, which were also proven with expert evaluation.

</details>


### [23] [GrIT: Group Informed Transformer for Sequential Recommendation](https://arxiv.org/abs/2602.19728)
*Adamya Shyam,Venkateswara Rao Kagita,Bharti Rana,Vikas Kumar*

Main category: cs.IR

TL;DR: 该论文提出了一种新的序列推荐系统方法，通过引入潜在的群组表示来捕捉用户随时间变化的群体特征和个人历史，以提高下一项推荐的准确性。通过对五个基准数据集进行广泛的实验，证明了该方法优于当前最新的序列推荐方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的序列推荐系统通常忽略了群体层面特征的影响，这些特征可以捕捉相似用户之间的集体行为。研究者假设，明确地建模随时间演变的群体特征与个人用户历史相结合，能够显著增强下一项推荐的效果。

Method: 该方法引入了潜在的群组表示，每个用户的群体归属是通过可学习的时间变化成员权重来建模的。在每个时间步骤计算的成员权重通过建模用户偏好变化而得出，同时考虑了短期和长期用户偏好。从用户行为中提取一组统计特征，并通过一系列转换进一步细化，产生最终的漂移感知成员权重。通过将学到的成员分数加权潜群嵌入，得到一个基于群组的表示。这个表示与用户在Transformer块内的序列表示相结合，共同捕捉个人和群体层面的时间动态，从而生成更丰富、上下文感知更强的嵌入。

Result: 通过在五个基准数据集上进行的广泛实验验证了所提方法的有效性，结果显示该方法始终优于目前最先进的序列推荐方法。

Conclusion: 结合用户个体历史和随时间演变的群体特征的方法可以显著改善序列推荐系统的性能。

Abstract: Sequential recommender systems aim to predict a user's future interests by extracting temporal patterns from their behavioral history. Existing approaches typically employ transformer-based architectures to process long sequences of user interactions, capturing preference shifts by modeling temporal relationships between items. However, these methods often overlook the influence of group-level features that capture the collective behavior of similar users. We hypothesize that explicitly modeling temporally evolving group features alongside individual user histories can significantly enhance next-item recommendation. Our approach introduces latent group representations, where each user's affiliation to these groups is modeled through learnable, time-varying membership weights. The membership weights at each timestep are computed by modeling shifts in user preferences through their interaction history, where we incorporate both short-term and long-term user preferences. We extract a set of statistical features that capture the dynamics of user behavior and further refine them through a series of transformations to produce the final drift-aware membership weights. A group-based representation is derived by weighting latent group embeddings with the learned membership scores. This representation is integrated with the user's sequential representation within the transformer block to jointly capture personal and group-level temporal dynamics, producing richer embeddings that lead to more accurate, context-aware recommendations. We validate the effectiveness of our approach through extensive experiments on five benchmark datasets, where it consistently outperforms state-of-the-art sequential recommendation methods.

</details>


### [24] [FairFS: Addressing Deep Feature Selection Biases for Recommender System](https://arxiv.org/abs/2602.20001)
*Xianquan Wang,Zhaocheng Du,Jieming Zhu,Qinglin Jia,Zhenhua Dong,Kai Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种名为FairFS的特征选择算法，旨在解决工业推荐系统中由于层偏置、基线偏置和近似偏置导致的特征重要性估计不准确的问题。通过在所有非线性变换层上规范化特征重要性、引入接近分类器决策边界的平滑基线特征以及采用聚合近似方法来缓解这些偏置问题，实验表明该方法能够有效减少偏置并达到最先进的特征选择性能。


<details>
  <summary>Details</summary>
Motivation: 在工业推荐系统中，特征的重要性评估对于提高在线服务表现同时降低成本至关重要。然而，现有的基于可训练门控和敏感性的深度学习特征选择方法存在层偏置、基线偏置及近似偏置等问题，导致特征重要性评估不够准确。

Method: 提出了FairFS算法，通过跨所有非线性转换层对特征重要性进行正则化处理以解决层偏置；引入靠近分类器决策边界的平滑基线特征；采用一种聚合近似方法来减轻基线偏置和近似偏置的影响。

Result: 广泛的实验证明了FairFS能够有效地缓解上述提到的各种偏置，并且在特征选择方面达到了最先进水平的表现。

Conclusion: FairFS提供了一种公平且准确的方法来进行特征选择，解决了现有方法中存在的几种偏置问题，从而提高了特征选择过程中的准确性。

Abstract: Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.

</details>


### [25] [ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation](https://arxiv.org/abs/2602.20093)
*Kun Yang,Yuxuan Zhu,Yazhe Chen,Siyao Zheng,Bangyang Hong,Kangle Wu,Yabo Ni,Anxiang Zeng,Cong Fu,Hui Li*

Main category: cs.IR

TL;DR: 提出了一种名为ManCAR的新框架，该框架通过在全局交互图的拓扑结构中约束推理过程来解决潜变量漂移问题，从而提高顺序推荐系统的性能。实验显示ManCAR在七个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序推荐系统在多步推理过程中容易出现潜变量漂移现象，导致推荐结果不合理。研究者认为推荐系统中的有效推理应该被视为在协作流形上的导航，而非自由形式的潜在改进。

Method: ManCAR（Manifold-Constrained Adaptive Reasoning）构建了一个基于用户最近行为合作邻域的局部意图先验，并以项目单纯形上的分布表示。训练时，模型逐步将其潜在预测分布与这个先验对齐，确保推理轨迹保持在有效的流形内。测试时，推理过程自适应进行直到预测分布稳定下来。

Result: 在七个基准数据集上进行了实验，结果显示ManCAR相对于最新基线方法在NDCG@10指标上最高可实现46.88%的相对提升。

Conclusion: ManCAR提供了一种有效的方法来避免潜变量漂移并提高了顺序推荐系统的推荐质量。此外，还提供了ManCAR变分解释，理论上验证了其防止漂移和自适应测试停止机制的有效性。

Abstract: Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [26] [Validated Code Translation for Projects with External Libraries](https://arxiv.org/abs/2602.18534)
*Hanliang Zhang,Arindam Sharma,Cristina David,Meng Wang,Brandon Paulsen,Daniel Kroening,Wenjia Ye,Taro Sekiyama*

Main category: cs.SE

TL;DR: 本文提出了一种结合检索机制和跨语言验证流程的框架，用于将具有外部依赖的Go项目翻译成Rust，并且能够显著提高编译成功率和等价性验证的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理依赖于外部库的源程序时表现不佳，会出现构造不存在的目标API以及无法生成调用所需的导入等问题；此外，在代码操作不透明的、由库定义的类型时，验证语义等价性也变得非常困难。

Method: 作者提出了一种针对有外部依赖的Go项目向Rust转换的翻译及验证框架。该方法结合了（i）一种映射Go库API到Rust API的检索机制，以及（ii）一种跨语言验证流水线，通过仅从公共库API合成适配器来建立存在不透明库类型时的语言互操作性，然后验证I/O等价性。

Result: 评估结果表明，该系统对六个具有非平凡外部依赖的实际Go仓库进行了测试，所提出的方法极大地提高了编译成功和等价性验证的成功率，在依赖最重的情况下可达100%，平均约为原来的两倍。

Conclusion: 本研究提出的翻译与验证框架有效解决了当源代码依赖外部库时大型语言模型进行程序翻译存在的问题，特别是在处理不透明库定义类型时，能够显著提升翻译后的代码质量与正确性。

Abstract: Large Language Models (LLMs) have shown promise for program translation, particularly for migrating systems code to memory-safe languages such as Rust. However, existing approaches struggle when source programs depend on external libraries: LLMs frequently hallucinate non-existent target APIs and fail to generate call-enabling imports; moreover, validating semantic equivalence is challenging when the code manipulates opaque, library-defined types. We present a translation and validation framework for translating Go projects with external dependencies to Rust. Our approach combines (i) a retrieval mechanism that maps Go library APIs to Rust APIs, and (ii) a cross-language validation pipeline that establishes language interoperability in the presence of opaque library types by synthesising adapters exclusively from public library APIs, prior to validating I/O equivalence. We evaluate our system on six real-world Go repositories with non-trivial external dependencies. Our approach significantly increases both the compilation and equivalence success rate (up to 100% in the most dependency-heavy case; approx. 2x on average) by enabling validated translation that manipulate opaque, library-defined types.

</details>


### [27] [Runtime-Augmented LLMs for Crash Detection and Diagnosis in ML Notebooks](https://arxiv.org/abs/2602.18537)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: 本文提出了一种名为CRANE-LLM的新方法，该方法通过结合静态代码上下文和运行时信息（如对象类型、张量形状和数据属性）来预测Jupyter笔记本中的目标单元格是否会崩溃，并解释其根本原因。实验结果表明，与仅使用大型语言模型相比，这种方法在崩溃检测和诊断准确性上提高了7到10个百分点，在F1分数上提高了8到11点。


<details>
  <summary>Details</summary>
Motivation: 尽管Jupyter笔记本因其支持交互式和迭代实验而被广泛用于机器学习开发，但它们很容易出现错误，其中崩溃是最具破坏性的。针对这一问题，目前尚缺乏系统的方法来进行崩溃检测和诊断。

Method: CRANE-LLM是一种新方法，它增强了大型语言模型的能力，通过从笔记本内核状态中提取结构化的运行时信息来实现。给定先前执行过的单元格和一个目标单元格，CRANE-LLM将静态代码内容与运行时信息相结合，包括对象类型、张量形状以及数据属性等，以预测目标单元格是否会崩溃及其背后的原因。

Result: 在包含222个ML笔记本的JunoBench基准测试中，CRANE-LLM展示了对于Gemini、Qwen和GPT-5三种最先进大型语言模型而言，运行时信息能够提高崩溃检测和诊断性能，准确率提升了7到10个百分点，F1分数增加了8到11点。不同机器学习库、崩溃原因以及大型语言模型之间观察到了不同的改进效果。

Conclusion: 研究表明，通过整合补充类别的运行时信息，CRANE-LLM能够显著改善现有大型语言模型在Jupyter笔记本崩溃检测与诊断方面的表现。

Abstract: Jupyter notebooks are widely used for machine learning (ML) development due to their support for interactive and iterative experimentation. However, ML notebooks are highly prone to bugs, with crashes being among the most disruptive. Despite their practical importance, systematic methods for crash detection and diagnosis in ML notebooks remain largely unexplored. We present CRANE-LLM, a novel approach that augments large language models (LLMs) with structured runtime information extracted from the notebook kernel state to detect and diagnose crashes before executing a target cell. Given previously executed cells and a target cell, CRANE-LLM combines static code context with runtime information, including object types, tensor shapes, and data attributes, to predict whether the target cell will crash (detection) and explain the underlying cause (diagnosis). We evaluate CRANE-LLM on JunoBench, a benchmark of 222 ML notebooks comprising 111 pairs of crashing and corresponding non-crashing notebooks across multiple ML libraries and crash root causes. Across three state-of-the-art LLMs (Gemini, Qwen, and GPT-5), runtime information improves crash detection and diagnosis by 7-10 percentage points in accuracy and 8-11 in F1-score, with larger gains for diagnosis. Improvements vary across ML libraries, crash causes, and LLMs, and depends on the integration of complementary categories of runtime information.

</details>


### [28] [1D-Bench: A Benchmark for Iterative UI Code Generation with Visual Feedback in Real-World](https://arxiv.org/abs/2602.18548)
*Qiao Xu,Yipeng Yu,Chengxiao Feng,Xu Liu*

Main category: cs.SE

TL;DR: 本文提出了1D-Bench，一个基于真实电商工作流程的设计转代码基准测试，它通过提供参考渲染和可能包含提取错误的导出中间表示来测试模型对中间表示缺陷而非字面遵循的鲁棒性。实验表明迭代编辑通常能提高最终性能，但在合成修复轨迹和基于强化学习的编辑后训练中观察到的效果有限且不稳定。


<details>
  <summary>Details</summary>
Motivation: 设计转代码技术将高保真UI设计转化为可执行的前端实现，但因为数据集、工具链和评估协议不一致，使得进展难以比较。因此，需要一个更加统一且贴近实际应用场景的基准测试方法来衡量不同解决方案的有效性。

Method: 作者引入了1D-Bench，这是一个基于真实电子商务工作流的基准测试，其中每个实例都提供了参考渲染图以及可能存在提取错误的导出中间表示。模型接受两者作为输入，并依据参考渲染进行评估，以检验模型对于中间表示中的缺陷是否具有鲁棒性。此外，还探讨了通过多轮迭代修改组件级编辑以及基于合成修复路径和强化学习的方法来改进模型表现。

Result: 实验结果表明，迭代编辑通常能够提高最终的表现，包括增加渲染成功率和改善视觉相似度。然而，在采用合成修复轨迹及基于强化学习的编辑策略进行后期训练时，观察到了有限且不稳定的效果提升，这可能是由于终端奖励稀疏以及文件级别更新的高方差所致。

Conclusion: 1D-Bench为设计转代码任务提供了一个新的基准测试框架，强调了在存在中间表示缺陷情况下模型鲁棒性的重要性。尽管迭代编辑被证明是有效的，但后续尝试通过特定技术进一步优化模型表现的努力显示出了局限性。

Abstract: Design-to-code translates high-fidelity UI designs into executable front-end implementations, but progress remains hard to compare due to inconsistent datasets, toolchains, and evaluation protocols. We introduce 1D-Bench, a benchmark grounded in real e-commerce workflows, where each instance provides a reference rendering and an exported intermediate representation that may contain extraction errors. 1D is short for one day, representing the efficient completion of design-to-code tasks in less than one day. Models take both as input, using the intermediate representation as structural cues while being evaluated against the reference rendering, which tests robustness to intermediate representation defects rather than literal adherence.
  1D-Bench requires generating an executable React codebase under a fixed toolchain with an explicit component hierarchy, and defines a multi-round setting in which models iteratively apply component-level edits using execution feedback. Experiments on commercial and open-weight multimodal models show that iterative editing generally improves final performance by increasing rendering success and often improving visual similarity. We further conduct a pilot study on post-training with synthetic repair trajectories and reinforcement learning based editing, and observe limited and unstable gains that may stem from sparse terminal rewards and high-variance file-level updates.

</details>


### [29] [Refactoring for Novices in Java: An Eye Tracking Study on the Extract vs. Inline Methods](https://arxiv.org/abs/2602.18579)
*José Aldo Silva da Costa,Rohit Gheyi,José Júnior Silva da Costa,Márcio Ribeiro,Rodrigo Bonifácio,Hyggo Almeida,Ana Carla Bibiano,Alessandro Garcia*

Main category: cs.SE

TL;DR: 本研究通过眼动追踪技术来分析内联方法与提取方法重构对Java初学者代码理解及导航的影响。结果显示，这两种重构方式的效果取决于任务难度：在较难的任务中，方法提取可以提高性能并减少视觉努力；而在较简单的任务中，则可能增加时间消耗和认知负担。此外，尽管参与者偏好于为可读性和复用性而进行的方法提取，但这种偏好并不总是与实际测量的性能一致。


<details>
  <summary>Details</summary>
Motivation: 以往基于静态度量的研究未能清晰地展示内联方法与提取方法重构之间的差异，并且对于人类理解和导航方面的探索不足。本研究旨在填补这一空白，利用眼动追踪技术深入探讨两种重构方式如何影响编程新手处理代码的方式。

Method: 研究采用了一种动态的方法——即使用眼动追踪技术记录参与者在阅读代码和解决问题时的行为。实验设计包括让32位Java新手解决八个简单任务，这些任务分别以内联版本和提取版本的形式呈现给参与者。除了主要实验外，还对另外58位新手进行了调查，以收集更多定量和定性数据。

Result: 研究发现，对于复杂一些的任务，提取方法能够显著改善表现并减少所需的视觉努力（如时间减少高达78.8%，回视次数减少84.6%）。然而，在处理更简单的任务时，提取方法反而会损害表现（如时间增加了最多166.9%，回视次数增加了200%）。即使方法名称具有意义，新手们也常常需要在调用点与被提取的方法之间来回切换，这增加了导航需求和认知负荷。

Conclusion: 研究表明，对于初学者而言，过早地模块化可能会带来负面影响。因此，教育者应当谨慎对待向新手介绍模块化的时机。同时，这项工作强调了眼动追踪作为补充静态度量的一种有用工具的价值。

Abstract: Developers often extract methods to improve readability, understanding, and reuse, while inlining keeps logic in one block. Prior work based on static metrics has not shown clear differences between these practices, and the human side of comprehension and navigation remains underexplored. We investigate Inline Method vs. Extract Method refactorings using a dynamic approach: eye tracking while participants read and solve tasks. We analyze key code areas and compare visual effort and reading behavior (fixation duration and count, regressions, revisits), alongside time and attempts. We ran a controlled experiment with 32 Java novices, followed by short interviews. Each participant solved eight simple tasks across four programs presented in an inlined version and four in an extracted version. We also surveyed 58 additional novices for complementary quantitative and qualitative data. Results show that effects depend on task difficulty. In two tasks, method extraction improved performance and reduced visual effort, with time decreasing by up to 78.8% and regressions by 84.6%. For simpler tasks (e.g., square area), extraction hurt performance: time increased by up to 166.9% and regressions by 200%. Even with meaningful method names, novices often switched back and forth between call sites and extracted methods, increasing navigation and cognitive load. Preferences frequently favored extraction for readability and reuse, but did not always match measured performance. These findings suggest educators should be cautious about premature modularization for novices and highlight eye tracking as a useful complement to static metrics.

</details>


### [30] [Automatic, Expressive, and Scalable Fuzzing with Stitching](https://arxiv.org/abs/2602.18689)
*Harrison Green,Fraser Brown,Claire Le Goues*

Main category: cs.SE

TL;DR: 提出了一种名为stitching的技术，通过在运行时动态组装API使用约束的片段来提高fuzzing技术的有效性和可扩展性。STITCH工具实现了这种技术，并在多个基准测试中取得了显著的效果，自动部署后已在开源项目中发现了许多新的bug。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化harness生成方法在合成时就固定了API序列，限制了每个harness可以测试的行为范围；而能够动态探索新序列的方法又缺乏表达现实世界使用约束的能力，导致因简单API误用而产生大量假阳性结果。

Method: 通过设计一种叫做stitching的技术，该技术允许将API使用约束编码成多个片段，然后由fuzzer根据覆盖率反馈在运行时动态组装这些片段。同时，利用静态类型系统控制对象如何在不同块之间流动，以及一个动态检查的外在类型状态来追踪任意元数据，从而支持描述丰富的语义约束如对象状态依赖和跨函数前置条件。

Result: 与四个最先进的工具对比，在33个基准测试中，STITCH在21个测试中达到了最高的代码覆盖率，并且发现了30个真正存在的bug，相比之下其他所有工具加起来仅找到10个。此外，STITCH还被自动部署到了1365个广泛使用的开源项目上，成功地在102个项目中发现了131个新的bug，其中73个已经被修复。

Conclusion: 通过引入stitching技术，STITCH工具不仅提高了fuzzing过程中发现真实bug的能力，同时也减少了由于API误用造成的假阳性问题。其在实际应用中的表现证明了这种方法的有效性和实用性。

Abstract: Fuzzing is a powerful technique for finding bugs in software libraries, but scaling it remains difficult. Automated harness generation commits to fixed API sequences at synthesis time, limiting the behaviors each harness can test. Approaches that instead explore new sequences dynamically lack the expressiveness to model real-world usage constraints leading to false positives from straightforward API misuse.
  We propose stitching, a technique that encodes API usage constraints in pieces that a fuzzer dynamically assembles at runtime. A static type system governs how objects flow between blocks, while a dynamically-checked extrinsic typestate tracks arbitrary metadata across blocks, enabling specifications to express rich semantic constraints such as object state dependencies and cross-function preconditions. This allows a single specification to describe an open-ended space of valid API interactions that the fuzzer explores guided by coverage feedback.
  We implement stitching in STITCH, using LLMs to automatically configure projects for fuzzing, synthesize a specification, triage crashes, and repair the specification itself. We evaluated STITCH against four state-of-the-art tools on 33 benchmarks, where it achieved the highest code coverage on 21 and found 30 true-positive bugs compared to 10 by all other tools combined, with substantially higher precision (70% vs. 12% for the next-best LLM-based tool). Deployed automatically on 1365 widely used open-source projects, STITCH discovered 131 new bugs across 102 projects, 73 of which have already been patched.

</details>


### [31] [Efficient Dynamic Test Case Generation for Path-Based Coverage Criteria](https://arxiv.org/abs/2602.18768)
*Jakub Zelek,Jakub Ruszil,Adam Roman,Artur Polański*

Main category: cs.SE

TL;DR: 本文提出了一种新的测试用例生成方法，满足四种基于路径的白盒覆盖标准，并且可以按需增量生成测试用例，而不需要预先计算整个测试套件。该方法不仅在执行时间和内存消耗方面优于现有技术，还为测试人员提供了更灵活高效的工具来实现高覆盖率并显著减少测试设计开销。


<details>
  <summary>Details</summary>
Motivation: 作者旨在开发一种新的测试用例生成方法，能够满足特定的路径覆盖标准（Prime Path, Simple Cycle, Simple Path, Edge-Acyclic Path），同时改进了现有方法要求一次性生成全部测试案例的不足，以提高测试设计效率和灵活性。

Method: 该研究基于Johnson算法的一个修改版本，允许测试用例根据需要逐步生成。此方法通过仅保留生成后续覆盖项所需的最小路径集来节省内存，适用于任意大的图。此外，它还允许测试者控制生成测试套件的大小。

Result: 实证评估表明，与现有技术相比，所提出的方法在执行时间、内存使用以及提供给测试者的灵活性方面均表现出色。

Conclusion: 新提出的测试用例生成方法在性能和灵活性上都超越了现有的解决方案，为软件测试领域带来了实质性的进步。

Abstract: We present a novel approach to test-case generation that satisfies four white-box, path-based coverage criteria: Prime Path, Simple Cycle, Simple Path, and Edge-Acyclic Path. Our method builds on a modified version of Johnson algorithm and enables test cases to be generated incrementally and on demand, rather than requiring the entire test suite to be computed upfront. This streaming capability represents a substantial advancement over existing approaches, as it allows testers to begin executing and refining tests immediately, thereby significantly improving the efficiency of test design. Our solution is inherently memory efficient, as it does not store all discovered coverage items; instead, it retains only the minimal set of paths required to generate subsequent coverage items on the fly. As a result, the approach scales to arbitrarily large graphs. In addition, the algorithm gives testers explicit control over the size of the generated test suite by allowing them to restrict the number of cycles permitted in a test path. The approach is grounded in new theoretical insights, most notably a novel characterization of prime paths in terms of the strongly connected components of control-flow graphs. We complement these theoretical contributions with a practical implementation and a comprehensive empirical evaluation. The results demonstrate that our method not only outperforms existing techniques in terms of execution time and memory consumption, but also provides testers with a more flexible and efficient tool for achieving high coverage while substantially reducing test design overhead.

</details>


### [32] [Operational Robustness of LLMs on Code Generation](https://arxiv.org/abs/2602.18800)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: 本文提出了一种新的鲁棒性评估方法——场景域分析，用于评估大型语言模型在生成代码时对任务描述变化的敏感度。通过实验，对四种最先进的LLMs（Gemini-pro, Codex, Llamma2 和 Falcon 7B）进行了排名，并发现复杂性和高级主题的任务会降低鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被应用于自动生成程序代码，有必要对其在此类应用中的鲁棒性进行评估，特别是这些模型对于编码任务描述变化的敏感程度。然而，现有的鲁棒性评估技术并不适用于代码生成场景，因为自然语言描述的数据空间是离散的。

Method: 提出了名为“场景域分析”的鲁棒性评价方法，旨在寻找导致LLMs产生错误输出的自然语言描述中预期最小的变化量。研究者还对该方法的理论性质进行了形式化证明，并通过广泛的实验来评估四款最先进LLM的鲁棒性表现。

Result: 成功地根据鲁棒性从好到差对四个顶尖LLM进行了排序；同时观察到，在面对更复杂的任务或涉及更高级的主题（如多线程和数据结构）时，这些模型的鲁棒性较差。

Conclusion: 场景域分析作为一种有效的方法揭示了不同条件下LLMs在代码生成任务中的鲁棒性差异，为理解及改进LLMs提供了一个新的视角。

Abstract: It is now common practice in software development for large language models (LLMs) to be used to generate program code. It is desirable to evaluate the robustness of LLMs for this usage. This paper is concerned in particular with how sensitive LLMs are to variations in descriptions of the coding tasks. However, existing techniques for evaluating this robustness are unsuitable for code generation because the input data space of natural language descriptions is discrete. To address this problem, we propose a robustness evaluation method called scenario domain analysis, which aims to find the expected minimal change in the natural language descriptions of coding tasks that would cause the LLMs to produce incorrect outputs. We have formally proved the theoretical properties of the method and also conducted extensive experiments to evaluate the robustness of four state-of-the-art art LLMs: Gemini-pro, Codex, Llamma2 and Falcon 7B, and have found that we are able to rank these with confidence from best to worst. Moreover, we have also studied how robustness varies in different scenarios, including the variations with the topic of the coding task and with the complexity of its sample solution, and found that robustness is lower for more complex tasks and also lower for more advanced topics, such as multi-threading and data structures.

</details>


### [33] [Narrowing the Complexity Gap in the Evaluation of Large Language Models](https://arxiv.org/abs/2602.18928)
*Yang Chen,Shuyang Liu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出了一种自动化技术GeneBench，用于给任何编程基准添加现实世界的复杂性。通过转换四个广泛使用的编程基准并评估13个大型语言模型（包括两个推理型）在这些基准上的表现，发现所有编程任务的性能都有显著下降，表明了LLMs在面对现实世界复杂性时遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在实际代码复杂性方面的表现至关重要。否则，仅基于简单的基准测试可能会高估LLMs的编程能力，在实际应用中导致失望。现有的解决方案通常是针对特定任务的，并且从真实项目控制数据质量可能耗时且容易出错。更重要的是，使用固定基准问题评估LLMs存在数据污染和过拟合的风险。

Method: 提出了GeneBench，一种利用多目标优化来增加编程问题复杂度的技术，同时保持代码的可读性与现实世界程序相似。对四个广泛使用的编程基准进行了转换，并用GeneBench对13个大型语言模型（包括2个推理专用模型）进行了评估。

Result: 所有编程任务的性能都出现了显著下降（范围为14.9%-60.5%，平均值=35.2%），这显示了LLMs在处理具有现实世界复杂性的任务时面临的困难。即使是在少量提示或微调后，这种挑战依然存在。此外，在错误修复方面，研究中的LLMs在GeneBench和SWE-Bench下的表现相似。

Conclusion: 研究表明，GeneBench能够有效揭示LLMs在应对真实世界编程复杂性方面的局限性，而无需构建成本高昂的真实世界基准。该技术对于评价LLMs的实际应用潜力具有重要意义。

Abstract: Evaluating Large Language Models (LLMs) with respect to real-world code complexity is essential. Otherwise, there is a risk of overestimating LLMs' programming abilities based on simplistic benchmarks, only to be disappointed when using them in real-world settings. Recently, researchers explored the construction of more realistic benchmarks by mining or augmenting open-source repositories. Such solutions are usually task-specific. Data quality control from real-world projects can also be time-consuming and error-prone. More importantly, evaluating LLMs on fixed benchmark problems is subject to data contamination and overfitting. We propose GeneBench, an automated technique to add real-world complexities to any programming benchmark. GeneBench leverages a multi-objective optimization to increase the complexity of programming problems while maintaining the readability of code similar to real-world programs. Transforming four widely-used programming benchmarks using GeneBench and evaluating 13 LLMs (including two reasoning LLMs) on them shows a notable performance drop across all programming tasks (14.9%-60.5%, avg=35.2%), demonstrating LLMs' struggle under real-world complexities. The struggle persists even when LLMs are few-shot prompted or fine-tuned with examples from different versions of GeneBench, demonstrating the challenging nature of the problems. Finally, we show that the performance of the studied LLMs in bug repair is similar under GeneBench and SWE-Bench. This, along with the consistent reproduction of performance drop of all studied LLMs across four tasks under different versions of GeneBench, makes the technique suitable to evaluate LLMs without costly construction of real-world benchmarks.

</details>


### [34] [A Systematic Evaluation of Environmental Flakiness in JavaScript Tests](https://arxiv.org/abs/2602.19098)
*Negar Hashemi,Amjed Tahir,August Shi,Shawn Rasheed,Rachel Blagojevic*

Main category: cs.SE

TL;DR: 本文系统地评估了环境因素（操作系统、Node.js版本和浏览器）对JavaScript测试波动性的影响，确定了65个受环境影响的波动项目，并提出了一种轻量级解决方案js-env-sanitizer来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 测试波动性是工业界的一个重要问题，它影响测试效率和产品质量。尽管已有大量研究探讨了波动测试的影响，但许多根本原因尚未被探索，特别是在像JavaScript这样的动态语言中。

Method: 首先在多种环境配置下执行测试套件，以确定环境变化是否会导致波动行为。选取了三个环境因素进行操作：操作系统、Node.js版本和浏览器。基于此分析，开发了一个名为js-env-sanitizer的工具，能够通过跳过并报告与环境相关的波动测试（而不是让它们失败），从而允许CI构建继续/成功而无需重新运行整个测试套件。

Result: 总共识别出65个因环境因素导致波动性的项目，其中28个与操作系统相关，5个与Node.js版本兼容性有关，16个同时涉及操作系统和Node.js问题，还有17个与浏览器兼容性有关。提出的js-env-sanitizer工具实现了高准确性，且性能或配置开销极小，当前支持三种流行的JavaScript测试框架（Jest、Mocha和Vitest）。

Conclusion: 研究表明，环境因素显著影响JavaScript项目的测试波动性。为此，js-env-sanitizer提供了一种有效的手段来减轻这类问题带来的负面影响，有助于提高持续集成流程中的测试稳定性和效率。

Abstract: Test flakiness is a significant issue in industry, affecting test efficiency and product quality. While extensive research has examined the impact of flaky tests, many root causes remain unexplored, particularly in the context of dynamic languages such as JavaScript. In this paper, we conduct a systematic evaluation of the impact of environmental factors on test flakiness in JavaScript. We first executed test suites across multiple environmental configurations to determine whether changes in the environment could lead to flaky behavior. We selected three environmental factors to manipulate: the operating system, the Node.js version, and the browser. We identified a total of 65 environmental flaky projects, with 28 related to operating system issues, five to Node.js version compatibility, 16 to a combination of operating system and Node.js issues, and 17 related to browser compatibility. To address environmental flakiness, we developed a lightweight mitigation approach, js-env-sanitizer, that can sanitize environmental-related flaky tests by skipping and reporting them (rather than failing), allowing CI builds to continue/succeed without rerunning entire test suites. The tool achieves high accuracy with minimal performance or configuration overhead, and currently supports three popular JavaScript testing frameworks (Jest, Mocha, and Vitest)

</details>


### [35] [Gecko: A Simulation Environment with Stateful Feedback for Refining Agent Tool Calls](https://arxiv.org/abs/2602.19218)
*Zeyu Zhang,Guohao Li,Zhenchang Xing,Alexandros Apostolopoulos,Yu Lin Lee,Liang Zheng*

Main category: cs.SE

TL;DR: 本研究引入了Gecko，一个综合环境，用于模拟工具响应，以改进大型语言模型（LLM）的工具调用，并解决使用真实工具进行迭代优化时出现的问题。通过提供三种类型的反馈，Gecko允许LLMs优化其工具调用，进而提出了一种名为GATS的有效测试时间缩放方法。实验表明，在不同基准上GATS显著提升了多个LLM版本的工具调用性能。


<details>
  <summary>Details</summary>
Motivation: 当前系统依赖于LLMs来计划和生成工具调用以完成任务，但这些调用容易出错且仅基于LLM自身能力。虽然让LLMs根据实际工具执行结果迭代地完善工具调用序列是有帮助的，但这过程可能成本高昂并导致不安全的结果。因此，需要一种更有效的方法来提高LLM工具调用的质量，同时减少对真实工具依赖所带来的问题。

Method: 研究者们提出了Gecko，这是一个结合规则与LLMs来模拟工具响应的环境。Gecko能够检查工具调用的有效性、合成符合输出模式的合理响应，并评估是否所有任务目标都已达成。基于Gecko提供的这三类反馈，形成了一个简单而有效的测试时间缩放方法GATS，用于帮助LLMs优化它们的工具调用。

Result: 在BFCLv3和$τ^2$-bench两个基准上的实验结果显示，GATS持续提高了包括GPT-4o, GPT-5以及Gemini-3.0-pro在内的多种LLM的工具调用表现。

Conclusion: Gecko及其相关方法GATS为改善LLM的工具调用提供了新的途径，不仅减少了对真实工具直接使用的依赖，而且通过提供详细的反馈信息促进了LLM工具调用准确性和效率的提升。此外，该工作还讨论了方法的工作机制，并展望了未来的发展方向。

Abstract: The ability to use tools is fundamental for large language model (LLM) agents. Given a task, existing systems use LLMs to plan and generate tool calls, which are executed by real-world tools to complete the task. However, tool calls are prone to errors because they are derived merely from LLM intrinsic capabilities. What is more, while it is useful to let LLMs iteratively refine the tool-call sequence using execution results from real tools, this process can be expensive and lead to unsafe results. To improve LLM tool calls and address issues caused by using real tools for refinement, we introduce Gecko, a comprehensive environment that simulates tool responses using a combination of rules and LLMs. Specifically, Gecko checks the validity of tool calls including input arguments and tool names, synthesizes reasonable responses that adhere to the output schema, and assesses whether all task objectives have been achieved. These three types of feedback provided by Gecko allow LLMs to refine their tool calls, forming a simple yet effective test-time scaling method named GATS. On BFCLv3 and $τ^2$-bench, GATS consistently improves the tool calling performance of various LLMs including GPT-4o, GPT-5, and Gemini-3.0-pro. We further discuss working mechanisms of our method and share future possibilities.

</details>


### [36] [ComUICoder: Component-based Reusable UI Code Generation for Complex Websites via Semantic Segmentation and Element-wise Feedback](https://arxiv.org/abs/2602.19276)
*Jingyu Xiao,Jiantong Qin,Shuoqi Li,Man Ho Lam,Yuxuan Wan,Jen-tse Huang,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了一种新的多页面复杂网页基准ComUIBench，以及一种基于组件的UI代码生成框架ComUICoder，旨在解决现有MLLMs在处理长且复杂的网站时遇到的问题。通过引入混合语义感知块分割、视觉感知图基块合并和基于优先级的元素反馈机制，ComUICoder显著提高了复杂多页面网站的整体生成质量和代码复用性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）虽然在UI-to-code任务上表现出色，但在处理长且复杂的网站时，存在片段化分割、重复组件冗余代码生成及频繁出现的UI不一致等问题。为了系统地研究并解决这些挑战，作者提出了新的解决方案。

Method: 1. 引入了ComUIBench，一个带有组件注释的新多页复杂网页基准，用于评估MLLMs在实际网站场景中生成可复用UI代码的能力。
2. 提出了ComUICoder框架，该框架强调语义感知分割、代码重用和细粒度优化。具体包括：
   - 混合语义感知块分割
   - 视觉感知图基块合并
   - 基于优先级的元素级反馈

Result: 广泛的实验表明，ComUICoder能够显著提高对复杂多页面网站的整体生成质量与代码可复用性。

Conclusion: 通过提出的ComUIBench基准测试集和ComUICoder框架，本文为改善MLLMs在处理复杂多页面网站时的表现提供了一个有效的方案，特别是在增强代码的一致性和可复用性方面取得了重要进展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance on the UI-to-code task, which aims to generate UI code from design mock-ups. However, when applied to long and complex websites, they often struggle with fragmented segmentation, redundant code generation for repetitive components, and frequent UI inconsistencies. To systematically investigate and address these challenges, we introduce ComUIBench, a new multi-page complex webpage benchmark with component annotations, designed to evaluate MLLMs' ability to generate reusable UI code in realistic website scenarios. Building upon this benchmark, we propose ComUICoder, a component-based UI code generation framework that emphasizes semantic-aware segmentation, code reuse, and fine-grained refinement. Specifically, ComUICoder incorporates (1) Hybrid Semantic-aware Block Segmentation for accurate UI semantic coherent block detection, (2) Visual-aware Graph-based Block Merge to consolidate structurally similar components within and across webpages for reusable implementation, and (3) Priority-based Element-wise Feedback to refine generated code and reduce element-level inconsistencies. Extensive experiments demonstrate that ComUICoder significantly improves overall generation quality and code reusability on complex multipage websites. Our datasets and code are publicly available at https://github.com/WebPAI/ComUICoder.

</details>


### [37] [Towards Automated Page Object Generation for Web Testing using Large Language Models](https://arxiv.org/abs/2602.19294)
*Betül Karagöz,Filippo Ricca,Matteo Biagiola,Andrea Stocco*

Main category: cs.SE

TL;DR: 本研究探索了使用大型语言模型（LLMs），特别是GPT-4o和DeepSeek Coder自动生成网页测试的页面对象（POs）的可能性。实验结果表明，这些模型能够以32.6%到54.0%的准确率生成语法正确且功能上有用的POs，并且在大多数情况下元素识别率超过70%。


<details>
  <summary>Details</summary>
Motivation: 尽管页面对象(POs)是提高自动化端对端网页测试可维护性和可扩展性的广泛采用的设计模式，但创建和维护POs仍然是一个主要依靠人工、劳动密集型的过程。而自动化的解决方案实际应用有限。在此背景下，大型语言模型(LLMs)在这方面的潜力还未得到充分探索。

Method: 通过一项实证研究，评估了特定的大型语言模型GPT-4o和DeepSeek Coder自动生成用于网页测试的POs的能力。基于五个已有手动编写POs作为基准的真实网页应用程序来评价生成物的质量，重点关注准确性（即正确识别的基准元素的比例）和元素识别率（即正确识别或标记修改的基准元素比例）。

Result: 结果显示，大型语言模型能够生成语法正确且功能上有用的POs，其准确性值范围从32.6%到54.0%，并且在大多数情况下元素识别率超过70%。

Conclusion: 这项研究表明，大型语言模型具有生成网页测试用页面对象的潜力，但也指出了当前存在的挑战。它为未来将大型语言模型集成进实际测试工作流程中的进一步研究提供了方向。

Abstract: Page Objects (POs) are a widely adopted design pattern for improving the maintainability and scalability of automated end-to-end web tests. However, creating and maintaining POs is still largely a manual, labor-intensive activity, while automated solutions have seen limited practical adoption. In this context, the potential of Large Language Models (LLMs) for these tasks has remained largely unexplored. This paper presents an empirical study on the feasibility of using LLMs, specifically GPT-4o and DeepSeek Coder, to automatically generate POs for web testing. We evaluate the generated artifacts on an existing benchmark of five web applications for which manually written POs are available (the ground truth), focusing on accuracy (i.e., the proportion of ground truth elements correctly identified) and element recognition rate (i.e., the proportion of ground truth elements correctly identified or marked for modification). Our results show that LLMs can generate syntactically correct and functionally useful POs with accuracy values ranging from 32.6% to 54.0% and element recognition rate exceeding 70% in most cases. Our study contributes the first systematic evaluation of LLMs strengths and open challenges for automated PO generation, and provides directions for further research on integrating LLMs into practical testing workflows.

</details>


### [38] [On the Variability of Source Code in Maven Package Rebuilds](https://arxiv.org/abs/2602.19383)
*Jens Dietrich,Behnaz Hassanshahi*

Main category: cs.SE

TL;DR: 本文测试了开源软件包重建过程中是否使用相同的源代码。研究发现，主要由于构建扩展在构建时生成代码难以重现，导致替代构建的源代码不等效，并提出了应对策略。


<details>
  <summary>Details</summary>
Motivation: 为了提高软件供应链的安全性，业界普遍采用从源代码重新构建开源软件包的做法。本文旨在验证这些独立构建是否真的基于相同的源代码进行。

Method: 比较Maven Central上发布的软件包随附的源代码与Google的Assured Open Source和Oracle的Build-from-Source项目提供的独立构建关联的源代码，分析非等效源代码的原因。

Result: 对于28个流行软件包的85个版本的替代构建中发现了源代码不等效的情况，主要原因在于构建时生成代码的构建扩展难以复制。

Conclusion: 虽然开源软件包的重建有助于提升安全性，但实现完全一致性的挑战依然存在，特别是当涉及到构建期间自动生成代码的技术时。为此，作者提出了一些缓解该问题的策略。

Abstract: Rebuilding packages from open source is a common practice to improve the security of software supply chains, and is now done at an industrial scale. The basic principle is to acquire the source code used to build a package published in a repository such as Maven Central (for Java), rebuild the package independently with hardened security, and publish it in some alternative repository. In this paper we test the assumption that the same source code is being used by those alternative builds. To study this, we compare the sources released with packages on Maven Central, with the sources associated with independently built packages from Google's Assured Open Source and Oracle's Build-from-Source projects. We study non-equivalent sources for alternative builds of 28 popular packages with 85 releases. We investigate the causes of non-equivalence, and find that the main cause is build extensions that generate code at build time, which are difficult to reproduce. We suggest strategies to address this issue.

</details>


### [39] ["Write in English, Nobody Understands Your Language Here": A Study of Non-English Trends in Open-Source Repositories](https://arxiv.org/abs/2602.19446)
*Masudul Hasan Masud Bhuiyan,Manish Kumar Bala Kumar,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: 该研究分析了从2015年到2025年间GitHub上的91.4亿个问题、拉取请求和讨论，以及跨越五种编程语言和30种自然语言的62,500个存储库，探讨开源软件（OSS）中多语言使用程度的增长。发现韩语、中文和俄语等语言的参与度稳步增长，不仅体现在交流中，在代码注释、字符串文字和文档文件中也有所体现。尽管这种变化反映了OSS中的包容性和语言多样性增强，但也带来了语言紧张，并且非英语或多种语言项目往往获得较少的关注和参与。


<details>
  <summary>Details</summary>
Motivation: 随着全球参与度的增加以及Unicode等标准对非拉丁文脚本支持的改善，开源软件社区正逐渐变得更加多语言化。本研究旨在探索开源软件中多语言使用的增长程度及其影响。

Method: 通过对2015年至2025年间GitHub上91.4亿个问题、拉取请求和讨论及62,500个仓库进行分析，涵盖五种编程语言和三十种自然语言，围绕沟通、代码和文档中的语言使用变化提出六个研究问题来追踪变化。

Result: 研究表明，特别是韩语、中文和俄语在内的多语言参与度在持续增长，这不仅表现在问题和讨论中，还体现在代码注释、字符串文本和文档文件里。虽然这种转变反映了开源软件领域更大的包容性和语言多样性，但同时也造成了语言间的张力。非英语或多种语言项目倾向于获得较低的关注度与参与度。

Conclusion: 开源软件正在变得更加多语言化，尤其是在韩语、中文和俄语方面。然而，这一趋势也导致了语言紧张感，并且非英语或多语言项目通常会收到较少的关注和贡献。这意味着语言既是一种资源也是一种障碍，影响着谁的声音被听到、谁作出贡献以及开放协作如何展开。

Abstract: The open-source software (OSS) community has historically been dominated by English as the primary language for code, documentation, and developer interactions. However, with growing global participation and better support for non-Latin scripts through standards like Unicode, OSS is gradually becoming more multilingual. This study investigates the extent to which OSS is becoming more multilingual, analyzing 9.14 billion GitHub issues, pull requests, and discussions, and 62,500 repositories across five programming languages and 30 natural languages, covering the period from 2015 to 2025. We examine six research questions to track changes in language use across communication, code, and documentation. We find that multilingual participation has steadily increased, especially in Korean, Chinese, and Russian. This growth appears not only in issues and discussions but also in code comments, string literals, and documentation files. While this shift reflects greater inclusivity and language diversity in OSS, it also creates language tension. The ability to express oneself in a native language can clash with shared norms around English use, especially in collaborative settings. Non-English or multilingual projects tend to receive less visibility and participation, suggesting that language remains both a resource and a barrier, shaping who gets heard, who contributes, and how open collaboration unfolds.

</details>


### [40] [Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering](https://arxiv.org/abs/2602.19614)
*Chih-Hong Cheng,Brian Hsuan-Cheng Liao,Adam Molin,Hasan Esen*

Main category: cs.SE

TL;DR: 本文提出了一种在安全关键的汽车工程中集成可信生成式AI的工作流级设计原则，通过从需求差异识别到SysML v2架构更新及重新测试的端到端流程进行了演示。采用分段处理与多样性采样等技术改进了对大型规范变更的识别准确性，并通过生成具体的测试案例来确保可追溯的回归测试，为在安全性要求高的场景下使用生成式AI提供了实用保障。


<details>
  <summary>Details</summary>
Motivation: 在安全关键系统工程领域应用大规模语言模型时面临信任度、可追溯性以及与现有验证实践一致性的挑战。为了克服这些障碍，作者们旨在开发一种能够增强生成式人工智能（GenAI）集成可信度的设计框架。

Method: 研究者们首先展示了传统的整体式提示方法在处理大篇幅规范文档时容易遗漏重要变更的问题；随后提出了基于章节分解结合多样性抽样和轻量级自然语言处理检查的新策略以提高信息完整性和正确率。接着，他们将需求变更反映到SysML v2模型中并通过编译和静态分析进行验证。此外，还通过建立从规范变量到架构端口和状态之间明确映射关系的方式自动生成测试用例，从而支持可追踪的回归测试。

Result: 实验结果表明，相较于传统的一次性提示方式，所提出的分段处理加多样本选取的方法显著提高了对于复杂规范文件中细微但关键变化的捕捉能力。同时，在SysML v2模型更新过程中引入的自动化验证手段有效保证了更改后的系统架构质量。最后，通过直接关联规范元素与测试脚本来实现的自动化测试用例生成机制，不仅简化了测试流程，也为后续维护工作提供了便利。

Conclusion: 本研究表明，通过遵循特定的设计原则并在整个开发周期内实施适当的控制措施，可以有效地将生成式AI技术应用于安全关键型汽车工程项目中。这不仅有助于提高软件定义车辆开发效率，同时也为确保最终产品的安全性和可靠性奠定了坚实基础。

Abstract: The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic ("big-bang") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering.

</details>


### [41] [Towards Understanding Views on Combining Videos and Gamification in Software Engineering Training](https://arxiv.org/abs/2602.19628)
*Pasan Peiris,Matthias Galster,Antonija Mitrovic,Sanna Malinen,Raul Vincent Lumapas,Jay Holland*

Main category: cs.SE

TL;DR: 研究了软件工程学生和行业从业者对于将游戏化元素加入到基于视频的培训中的看法，发现两者对此都持支持态度。


<details>
  <summary>Details</summary>
Motivation: 为了提高通过观看培训视频学习的效果，研究者希望通过添加游戏化元素来增加参与度，并探索软件工程学生和行业从业者对此的看法。

Method: 通过向学生和专业人士发放调查问卷的方式进行研究。

Result: 研究发现，学生和专业人士对基于视频的培训总体上持有相似的看法，并且支持将游戏化与基于视频的培训相结合。

Conclusion: 本研究的结果可以为设计针对软件工程师的游戏化培训解决方案提供信息。

Abstract: Watching training videos passively leads to superficial learning. Adding gamification can increase engagement. We study how software engineering students and industry practitioners view gamifying video-based training. We conducted a survey with students and professionals. Students and professionals share similar perceptions toward video-based training in general and support combining gamification and video-based training. Our findings can inform the design of gamified training solutions for software engineers.

</details>


### [42] [Carbon-Aware Governance Gates: An Architecture for Sustainable GenAI Development](https://arxiv.org/abs/2602.19718)
*Mateen A. Abbasi,Tommi J. Mikkonen,Petri J. Ihantola,Muhammad Waseem,Pekka Abrahamsson,Niko K. Mäkitalo*

Main category: cs.SE

TL;DR: 本文提出了一种名为碳意识治理门(CAGG)的架构扩展，旨在将碳预算、能源来源和可持续性验证编排整合到人机治理层中，以解决生成式AI在软件开发周期中增加的计算需求导致的碳足迹问题。CAGG包含三个组件：能量与碳来源账本、碳预算管理器以及绿色验证协调器，并通过治理政策和可重用设计模式实现运作。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能（GenAI）在软件开发生命周期（SDLC）中的快速采用，对于计算能力的需求也随之增长，这导致了开发活动的碳排放量上升。同时，组织正在将治理机制嵌入到基于GenAI的支持开发过程中，以促进信任、透明度和责任性。但是，这些治理机制引入了额外的计算工作负载，包括重复推理、再生循环以及扩大的验证流程，从而进一步增加了能源使用及碳足迹。为了解决这一问题，提出了新的解决方案。

Method: 论文提出了碳意识治理门（Carbon-Aware Governance Gates, CAGG），这是一种架构上的扩展，它将碳预算、能源来源跟踪以及可持续性导向的验证编排集成到了人类-人工智能交互的治理层级之中。CAGG由三个主要部分组成：(i) 能源与碳来源记录账本；(ii) 碳预算管理者；以及(iii) 绿色验证调度者。这些组成部分通过治理策略和可复用的设计模式来实现其功能。

Result: 虽然摘要中没有直接提到具体的结果数据或实验发现，但可以推测出CAGG的实施有望减少GenAI辅助开发过程中的碳足迹，同时保持必要的治理标准。通过有效管理碳预算和优化验证流程，该方法可能有助于降低整体能源消耗并提高软件开发流程的环境友好性。

Conclusion: 碳意识治理门(CAGG)作为一种创新性的解决方案，旨在通过整合碳预算控制、能源使用追踪以及环保型验证流程等手段，来应对GenAI在软件开发过程中所带来的碳排放挑战。这种方法不仅能够帮助组织维持其对于GenAI应用的信任、透明度和责任感，同时也促进了更加可持续的发展实践。

Abstract: The rapid adoption of Generative AI (GenAI) in the software development life cycle (SDLC) increases computational demand, which can raise the carbon footprint of development activities. At the same time, organizations are increasingly embedding governance mechanisms into GenAI-assisted development to support trust, transparency, and accountability. However, these governance mechanisms introduce additional computational workloads, including repeated inference, regeneration cycles, and expanded validation pipelines, increasing energy use and the carbon footprint of GenAI-assisted development. This paper proposes Carbon-Aware Governance Gates (CAGG), an architectural extension that embeds carbon budgets, energy provenance, and sustainability-aware validation orchestration into human-AI governance layers. CAGG comprises three components: (i) an Energy and Carbon Provenance Ledger, (ii) a Carbon Budget Manager, and (iii) a Green Validation Orchestrator, operationalized through governance policies and reusable design patterns.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Revisiting the Seasonal Trend Decomposition for Enhanced Time Series Forecasting](https://arxiv.org/abs/2602.18465)
*Sanjeev Panta,Xu Yuan,Li Chen,Nian-Feng Tzeng*

Main category: cs.LG

TL;DR: 本文通过分解时间序列并分别处理趋势和季节性成分，提出了一种改进的机器学习模型架构，以提高多变量时间序列预测的准确性。该方法在基准数据集上实现了约10%的MSE平均减少，并在美国地质调查局河流站点提供的水文数据集上展示了其实用价值。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，时间序列预测在许多领域都面临着重大挑战。为了解决这个问题，作者们希望通过改进机器学习模型的架构来提高多变量时间序列预测的表现。

Method: 通过对时间序列进行分解，特别关注趋势与季节性成分，并采用不同的策略来预测这两个部分：对于趋势成分使用可逆实例归一化技术，而对于季节性成分则直接应用骨干模型而无需任何归一化或缩放过程。基于这些策略，研究者提出了双MLP模型作为更高效的解决方案。

Result: 所提出的方法能够显著降低现有最先进模型的误差值，在四个基准数据集上平均减少了大约10%的MSE。此外，当应用于从美国地质调查局河流站提取的真实世界水文数据时，该方法不仅保持了线性时间复杂度还取得了显著改进。

Conclusion: 研究表明，通过分别对时间序列的趋势和季节性成分采取针对性措施，可以有效提升多变量时间序列预测性能。提出的双MLP模型不仅计算效率高，而且在实际应用场景中也表现出色。

Abstract: Time series forecasting presents significant challenges in real-world applications across various domains. Building upon the decomposition of the time series, we enhance the architecture of machine learning models for better multivariate time series forecasting. To achieve this, we focus on the trend and seasonal components individually and investigate solutions to predict them with less errors. Recognizing that reversible instance normalization is effective only for the trend component, we take a different approach with the seasonal component by directly applying backbone models without any normalization or scaling procedures. Through these strategies, we successfully reduce error values of the existing state-of-the-art models and finally introduce dual-MLP models as more computationally efficient solutions. Furthermore, our approach consistently yields positive results with around 10% MSE average reduction across four state-of-the-art baselines on the benchmark datasets. We also evaluate our approach on a hydrological dataset extracted from the United States Geological Survey (USGS) river stations, where our models achieve significant improvements while maintaining linear time complexity, demonstrating real-world effectiveness. The source code is available at https://github.com/Sanjeev97/Time-Series-Decomposition

</details>


### [44] [Decentralized Attention Fails Centralized Signals: Rethinking Transformers for Medical Time Series](https://arxiv.org/abs/2602.18473)
*Guoqi Yu,Juncheng Wang,Chen Yang,Jing Qin,Angelica I. Aviles-Rivero,Shujun Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为CoTAR的集中式多层感知器模块，用于改进医学时间序列数据（如EEG和ECG）的分析。通过引入全局核心令牌来促进令牌间的交互，这种方法不仅更好地适应了MedTS信号的集中特性，还降低了计算复杂度。实验结果表明，该方法在五个基准测试中均表现出色，在效率和效果上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的模型在处理医学时间序列数据时，虽然能够有效捕捉时间依赖性，但对于通道间依赖性的建模存在局限。这主要是由于医学时间序列信号本质上是集中的，而Transformer的注意力机制是分散式的，导致它在捕捉全局同步性和统一波形模式方面不够有效。因此，需要一种新的方法来解决这一结构上的不匹配问题。

Method: 提出了CoTAR（Core Token Aggregation-Redistribution），这是一种基于多层感知器(MLP)的集中式模块，旨在替代传统的分散式注意力机制。CoTAR通过引入一个全局核心令牌作为中介来促进令牌之间的交互作用，从而强制执行集中式的聚合与再分配策略。这种设计不仅更符合医学时间序列信号的集中特性，而且将计算复杂度从二次降低到线性。

Result: 在五个基准测试上的实验验证了所提方法的有效性和高效性。特别是，在APAVA数据集上达到了最高12.13%的性能提升，同时相比最先进方法仅使用了三分之一的内存以及五分之一的推理时间。

Conclusion: 通过引入CoTAR模块，研究成功地解决了现有深度学习模型在处理医学时间序列数据时面临的挑战，特别是在捕捉通道间依赖性方面的不足。实验结果证明了新方法不仅提高了准确性，也大幅提升了计算效率。

Abstract: Accurate analysis of medical time series (MedTS) data, such as electroencephalography (EEG) and electrocardiography (ECG), plays a pivotal role in healthcare applications, including the diagnosis of brain and heart diseases. MedTS data typically exhibit two critical patterns: temporal dependencies within individual channels and channel dependencies across multiple channels. While recent advances in deep learning have leveraged Transformer-based models to effectively capture temporal dependencies, they often struggle with modeling channel dependencies. This limitation stems from a structural mismatch: MedTS signals are inherently centralized, whereas the Transformer's attention mechanism is decentralized, making it less effective at capturing global synchronization and unified waveform patterns. To address this mismatch, we propose CoTAR (Core Token Aggregation-Redistribution), a centralized MLP-based module designed to replace decentralized attention. Instead of allowing all tokens to interact directly, as in standard attention, CoTAR introduces a global core token that serves as a proxy to facilitate inter-token interactions, thereby enforcing a centralized aggregation and redistribution strategy. This design not only better aligns with the centralized nature of MedTS signals but also reduces computational complexity from quadratic to linear. Experiments on five benchmarks validate the superiority of our method in both effectiveness and efficiency, achieving up to a 12.13% improvement on the APAVA dataset, while using only 33% of the memory and 20% of the inference time compared to the previous state of the art. Code and all training scripts are available at https://github.com/Levi-Ackman/TeCh.

</details>


### [45] [Support Vector Data Description for Radar Target Detection](https://arxiv.org/abs/2602.18486)
*Jean Pinsolle,Yadang Alexis Rouzoumka,Chengfang Ren,Chistèle Morisseau,Jean-Philippe Ovarlez*

Main category: cs.LG

TL;DR: 本文提出使用支持向量数据描述(SVDD)及其深度扩展版本Deep SVDD作为一类学习方法，来解决雷达检测中杂波和热噪声结合导致的传统自适应检测器性能下降的问题，并提出了两种新的基于SVDD的检测算法。


<details>
  <summary>Details</summary>
Motivation: 传统雷达检测技术在高斯环境下有效，但在遇到更适于用重尾分布如复数椭圆对称（CES）或复合高斯（CGD）系列建模的杂波时表现不佳。稳健协方差估计器虽能部分解决问题，但在热噪声与杂波同时存在的情况下仍显不足。

Method: 研究采用支持向量数据描述（SVDD）及其深度学习扩展版本Deep SVDD进行目标检测，这两种单类学习方法无需直接估计噪声协方差矩阵，并在此基础上开发了两种新的基于SVDD的检测算法。

Result: 通过模拟雷达数据验证了所提出的基于SVDD的检测算法的有效性。

Conclusion: 基于SVDD的方法为处理复杂环境下的雷达检测问题提供了新思路，尤其是当面临杂波与热噪声共存挑战时，显示出优于传统方法的潜力。

Abstract: Classical radar detection techniques rely on adaptive detectors that estimate the noise covariance matrix from target-free secondary data. While effective in Gaussian environments, these methods degrade in the presence of clutter, which is better modeled by heavy-tailed distributions such as the Complex Elliptically Symmetric (CES) and Compound-Gaussian (CGD) families. Robust covariance estimators like M-estimators or Tyler's estimator address this issue, but still struggle when thermal noise combines with clutter. To overcome these challenges, we investigate the use of Support Vector Data Description (SVDD) and its deep extension, Deep SVDD, for target detection. These one-class learning methods avoid direct noise covariance estimation and are adapted here as CFAR detectors. We propose two novel SVDD-based detection algorithms and demonstrate their effectiveness on simulated radar data.

</details>


### [46] [Measuring the Prevalence of Policy Violating Content with ML Assisted Sampling and LLM Labeling](https://arxiv.org/abs/2602.18518)
*Attila Dobi,Aravindh Manickavasagam,Benjamin Thompson,Xiaohan Yang,Faisal Farooq*

Main category: cs.LG

TL;DR: 本文提出了一种基于设计的测量系统，用于准确测量内容违规的普遍性。该系统通过机器学习辅助的权重从展示流中抽取每日概率样本，利用多模态大语言模型进行标签处理，并生成无偏且具有一致性的违规内容普遍性估计值及置信区间。


<details>
  <summary>Details</summary>
Motivation: 内容安全团队需要反映用户实际体验的指标，而不仅仅是报告的数据。然而，准确地测量违规内容的普遍性具有挑战性，因为违规情况通常很少见，而且人工标注成本高昂，这使得频繁进行代表性研究变得缓慢。

Method: 提出了一个基于设计的测量系统，它每天从展示流中使用机器学习辅助的权重抽取概率样本，以将标签预算集中在高曝光和高风险的内容上，同时保持无偏性；使用由政策提示和黄金集验证管理的多模态大语言模型来标记采样项；并生成与设计一致的普遍性估计值，包括置信区间和仪表板深入分析。

Result: 这个系统能够支持通过后分层估计的方式，对同一日常样本按界面、观看者地理位置、内容年龄等多个维度提供普遍性数据。此外，还描述了统计估计量、方差和置信区间的构建方法、标签质量监控以及使系统能够在不同政策下配置的工程工作流程。

Conclusion: 所提出的测量系统旨在实现一个全球样本支持多个视角分析的目标，即同样的日常样本可以支持根据不同维度（如界面、观众地理位置、内容年龄等）通过后分层估计得出的普遍性数据，从而为内容安全提供了更有效率和效果的解决方案。

Abstract: Content safety teams need metrics that reflect what users actually experience, not only what is reported. We study prevalence: the fraction of user views (impressions) that went to content violating a given policy on a given day. Accurate prevalence measurement is challenging because violations are often rare and human labeling is costly, making frequent, platform-representative studies slow. We present a design-based measurement system that (i) draws daily probability samples from the impression stream using ML-assisted weights to concentrate label budget on high-exposure and high-risk content while preserving unbiasedness, (ii) labels sampled items with a multimodal LLM governed by policy prompts and gold-set validation, and (iii) produces design-consistent prevalence estimates with confidence intervals and dashboard drilldowns. A key design goal is one global sample with many pivots: the same daily sample supports prevalence by surface, viewer geography, content age, and other segments through post-stratified estimation. We describe the statistical estimators, variance and confidence interval construction, label-quality monitoring, and an engineering workflow that makes the system configurable across policies.

</details>


### [47] [Wide Open Gazes: Quantifying Visual Exploratory Behavior in Soccer with Pose Enhanced Positional Data](https://arxiv.org/abs/2602.18519)
*Joris Bekkers*

Main category: cs.LG

TL;DR: 该研究提出了一种新的公式化连续随机视觉层，通过增强的姿态时空跟踪来量化足球运动员的视觉感知。利用头部和肩部旋转角度创建速度依赖的视野图，并结合球场控制和价值面分析球员等待接球及随后控球阶段的行为。研究结果表明，如等待传球时观察到的防守区域百分比等聚合视觉指标，可以预测2024年美洲杯32场比赛中盘带结束时获得的受控球场价值。


<details>
  <summary>Details</summary>
Motivation: 传统的足球视觉探索行为测量方法存在位置偏差、注释难题、二元测量限制等问题，并且无法很好地预测比赛中的短期成功或与基本足球分析模型兼容。

Method: 引入了一个新的基于概率的视场和遮挡模型，结合头部和肩膀旋转角度，在二维俯视平面上为每个球员创建速度依赖性的视觉地图。这些视觉地图再与球场控制及价值表面相结合，用于分析球员等待接球以及其后持球阶段的表现。

Result: 发现像等待传球期间观察到的防守面积比例这样的综合视觉度量能够有效预测在一系列盘带动作结束后所获得的控制下的球场价值。

Conclusion: 新方法不仅适用于所有位置的球员，无需手动注解，还能提供与现有足球分析框架无缝集成的连续性测量结果。此外，为了促进与现有足球分析框架的整合，研究人员开源了进行此类计算所需的工具。

Abstract: Traditional approaches to measuring visual exploratory behavior in soccer rely on counting visual exploratory actions (VEAs) based on rapid head movements exceeding 125°/s, but this method suffer from player position bias (i.e., a focus on central midfielders), annotation challenges, binary measurement constraints (i.e., a player is scanning, or not), lack the power to predict relevant short-term in-game future success, and are incompatible with fundamental soccer analytics models such as pitch control. This research introduces a novel formulaic continuous stochastic vision layer to quantify players' visual perception from pose-enhanced spatiotemporal tracking. Our probabilistic field-of-view and occlusion models incorporate head and shoulder rotation angles to create speed-dependent vision maps for individual players in a two-dimensional top-down plane. We combine these vision maps with pitch control and pitch value surfaces to analyze the awaiting phase (when a player is awaiting the ball to arrive after a pass for a teammate) and their subsequent on-ball phase. We demonstrate that aggregated visual metrics - such as the percentage of defended area observed while awaiting a pass - are predictive of controlled pitch value gained at the end of dribbling actions using 32 games of synchronized pose-enhanced tracking data and on-ball event data from the 2024 Copa America. This methodology works regardless of player position, eliminates manual annotation requirements, and provides continuous measurements that seamlessly integrate into existing soccer analytics frameworks. To further support the integration with existing soccer analytics frameworks we open-source the tools required to make these calculations.

</details>


### [48] [AdaptStress: Online Adaptive Learning for Interpretable and Personalized Stress Prediction Using Multivariate and Sparse Physiological Signals](https://arxiv.org/abs/2602.18521)
*Xueyi Wang,Claudine J. C. Lamoth,Elisabeth Wilhelm*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的、可解释的、个性化的压力预测方法，使用消费级智能手表的生理数据（如心率变异性、活动模式和睡眠指标）进行时间序列预测。通过与最先进的模型及传统基线比较，在最优条件下（5天输入，1天预测），本模型表现出了更优的性能，特别是在个人化模式捕捉方面。


<details>
  <summary>Details</summary>
Motivation: 连续的压力预测可能有助于生活方式干预。研究旨在开发一种利用消费级智能手表收集的生理数据来进行个性化且可解释的压力水平预测的方法。

Method: 开发了一个时间序列预测模型，利用多变量特征（包括心率变异性、活动模式和睡眠指标）来预测跨越16个时间范围的压力水平。对16名参与者进行了为期10-15周的监测，并将该方法与最先进的时间序列模型（Informer, TimesNet, PatchTST）以及传统基线（CNN, LSTM, CNN-LSTM）进行了对比评估。

Result: 在最佳设置下（5天输入，1天预测），该模型达到了MSE为0.053，MAE为0.190，RMSE为0.226的表现。相比于最好的基线模型，分别提高了36.9%、25.5%和21.5%。解释性分析显示，睡眠指标是最主要且一致的压力预测因素，而活动特征显示出高个体间差异。

Conclusion: 结合适应性和可解释性的深度学习技术，消费级穿戴设备可以提供根据个体生理反应调整的相关压力评估，为现实世界中的大规模、持续、可解释的心理健康监测奠定了基础。

Abstract: Continuous stress forecasting could potentially contribute to lifestyle interventions. This paper presents a novel, explainable, and individualized approach for stress prediction using physiological data from consumer-grade smartwatches. We develop a time series forecasting model that leverages multivariate features, including heart rate variability, activity patterns, and sleep metrics, to predict stress levels across 16 temporal horizons (History window: 3, 5, 7, 9 days; forecasting window: 1, 3, 5, 7 days). Our evaluation involves 16 participants monitored for 10-15 weeks. We evaluate our approach across 16 participants, comparing against state-of-the-art time series models (Informer, TimesNet, PatchTST) and traditional baselines (CNN, LSTM, CNN-LSTM) across multiple temporal horizons. Our model achieved performance with an MSE of 0.053, MAE of 0.190, and RMSE of 0.226 in optimal settings (5-day input, 1-day prediction). A comparison with the baseline models shows that our model outperforms TimesNet, PatchTST, CNN-LSTM, LSTM, and CNN under all conditions, representing improvements of 36.9%, 25.5%, and 21.5% over the best baseline. According to the explanability analysis, sleep metrics are the most dominant and consistent stress predictors (importance: 1.1, consistency: 0.9-1.0), while activity features exhibit high inter-participant variability (0.1-0.2). Most notably, the model captures individual-specific patterns where identical features can have opposing effects across users, validating its personalization capabilities. These findings establish that consumer wearables, combined with adaptive and interpretable deep learning, can deliver relevant stress assessment adapted to individual physiological responses, providing a foundation for scalable, continuous, explainable mental health monitoring in real-world settings.

</details>


### [49] [The Geometry of Multi-Task Grokking: Transverse Instability, Superposition, and Weight Decay Phase Structure](https://arxiv.org/abs/2602.18523)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文研究了多任务设置下的Grokking现象，揭示了不同算术任务间理解顺序的差异、优化轨迹的低维特性、权重衰减对学习动态的影响、最终解的压缩抗性和参数冗余性对模型恢复能力的作用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索多任务模块化算术背景下Grokking现象的本质，特别是共享架构变换器在处理加法、乘法和平方等多重任务时表现出的学习行为特征。

Method: 通过系统地改变权重衰减值，在双任务（模加+模乘）和三任务（模加+模乘+模平方）目标上训练共享架构变换器，并观察学习过程中的多种现象。

Result: 发现包括但不限于：1) 不同算术任务间存在固定的理解先后顺序；2) 优化路径保持在一个经验上不变的低维执行流形内；3) 权重衰减对学习时间尺度及缺陷出现时机有显著影响；4) 最终解决方案虽然只占据少数主成分方向但分布于全秩权重中且对小扰动敏感；5) 参数冗余性有助于模型部分恢复。

Conclusion: 研究表明，多任务Grokking在参数空间中构建了一个紧凑的叠加子空间，其中权重衰减扮演着压缩压力的角色，而多余的参数则提供了优化路径上的几何冗余。

Abstract: Grokking -- the abrupt transition from memorization to generalization long after near-zero training loss -- has been studied mainly in single-task settings. We extend geometric analysis to multi-task modular arithmetic, training shared-trunk Transformers on dual-task (mod-add + mod-mul) and tri-task (mod-add + mod-mul + mod-sq) objectives across a systematic weight decay sweep. Five consistent phenomena emerge. (1) Staggered grokking order: multiplication generalizes first, followed by squaring, then addition, with consistent delays across seeds. (2) Universal integrability: optimization trajectories remain confined to an empirically invariant low-dimensional execution manifold; commutator defects orthogonal to this manifold reliably precede generalization. (3) Weight decay phase structure: grokking timescale, curvature depth, reconstruction threshold, and defect lead covary systematically with weight decay, revealing distinct dynamical regimes and a sharp no-decay failure mode. (4) Holographic incompressibility: final solutions occupy only 4--8 principal trajectory directions yet are distributed across full-rank weights and destroyed by minimal perturbations; SVD truncation, magnitude pruning, and uniform scaling all fail to preserve performance. (5) Transverse fragility and redundancy: removing less than 10% of orthogonal gradient components eliminates grokking, yet dual-task models exhibit partial recovery under extreme deletion, suggesting redundant center manifolds enabled by overparameterization. Together, these results support a dynamical picture in which multi-task grokking constructs a compact superposition subspace in parameter space, with weight decay acting as compression pressure and excess parameters supplying geometric redundancy in optimization pathways.

</details>


### [50] [Audio-Visual Continual Test-Time Adaptation without Forgetting](https://arxiv.org/abs/2602.18528)
*Sarthak Kumar Maharana,Akshay Mehra,Bhavya Ramakrishna,Yunhui Guo,Guan-Ming Su*

Main category: cs.LG

TL;DR: 本文提出了一种名为AV-CTTA的方法，通过仅自适应调整模态融合层来改善模型在测试时的表现，并且利用一个选择性参数检索机制动态地从缓存中获取最佳的融合层参数，从而在不需要源数据的情况下提高了模型对非平稳域的适应能力，同时最小化了灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 先前解决音频-视觉连续测试时适应问题的方法存在灾难性遗忘的问题，即由于在测试时持续更新参数导致模型性能显著下降。因此，研究者们旨在找到一种方法，不仅能够提高模型在目标域上的表现，还能增强其在后续域中的性能，同时避免灾难性遗忘。

Method: 研究者首先展示了只针对模态融合层进行目标域适应不仅能够提升该域下的性能，还能够增强后续域中的性能。基于此发现，他们提出了AV-CTTA方法，该方法通过使用一个选择性的参数检索机制，根据一小批测试数据动态地从缓冲区中检索最优的融合层参数，然后将这些参数整合到模型中，使其适应当前的测试分布，并保存以供将来使用。

Result: 在包含单模态和双模态损坏的基准数据集上进行了广泛的实验，结果表明所提出的AV-CTTA方法相比现有方法表现出色，同时有效减少了灾难性遗忘现象。

Conclusion: 本研究表明，通过对模态融合层进行单独调整以及采用选择性参数检索机制，可以有效地提高模型在不同测试域上的适应性和准确性，同时缓解了灾难性遗忘的问题。

Abstract: Audio-visual continual test-time adaptation involves continually adapting a source audio-visual model at test-time, to unlabeled non-stationary domains, where either or both modalities can be distributionally shifted, which hampers online cross-modal learning and eventually leads to poor accuracy. While previous works have tackled this problem, we find that SOTA methods suffer from catastrophic forgetting, where the model's performance drops well below the source model due to continual parameter updates at test-time. In this work, we first show that adapting only the modality fusion layer to a target domain not only improves performance on that domain but can also enhance performance on subsequent domains. Based on this strong cross-task transferability of the fusion layer's parameters, we propose a method, $\texttt{AV-CTTA}$, that improves test-time performance of the models without access to any source data. Our approach works by using a selective parameter retrieval mechanism that dynamically retrieves the best fusion layer parameters from a buffer using only a small batch of test data. These parameters are then integrated into the model, adapted to the current test distribution, and saved back for future use. Extensive experiments on benchmark datasets involving unimodal and bimodal corruptions show our proposed $\texttt{AV-CTTA}$ significantly outperforms existing methods while minimizing catastrophic forgetting.

</details>


### [51] [Diagnosing LLM Reranker Behavior Under Fixed Evidence Pools](https://arxiv.org/abs/2602.18613)
*Baris Arat,Emre Sefer*

Main category: cs.LG

TL;DR: 本文提出了一种控制诊断方法，通过使用Multi-News集群作为固定的证据池来隔离重排序过程，从而独立于检索质量研究不同模型的排序行为。


<details>
  <summary>Details</summary>
Motivation: 传统的重排序评估方法将排序行为与检索质量结合在一起，使得输出差异不能单独归因于排序策略。为了能够独立地研究排序策略的影响，需要一种新的诊断方法来隔离重排序过程。

Method: 作者们采用Multi-News集群作为固定证据池，并限制每个池中恰好有八个文档，向所有排序器传递完全相同的输入。在该设置下，BM25和MMR被用作词汇匹配和多样性优化的可解释参考点。

Result: 实验结果表明，在345个集群上，不同模型表现出不同的冗余模式：一个大型语言模型在较大的选择预算下隐式地增加了多样性，而另一个则增加了冗余性。相比之下，当选择预算较小时，大型语言模型在词汇覆盖方面表现不佳。这些发现表明大型语言模型的排名与两种基准策略都存在显著差异。

Conclusion: 通过消除检索变量，可以直接将这些差异归因于排序策略本身。提出的这种诊断方法是模型无关的，适用于任何排序器，包括开源系统和专有API。

Abstract: Standard reranking evaluations study how a reranker orders candidates returned by an upstream retriever. This setup couples ranking behavior with retrieval quality, so differences in output cannot be attributed to the ranking policy alone. We introduce a controlled diagnostic that isolates reranking by using Multi-News clusters as fixed evidence pools. We limit each pool to exactly eight documents and pass identical inputs to all rankers. Within this setup, BM25 and MMR serve as interpretable reference points for lexical matching and diversity optimization. Across 345 clusters, we find that redundancy patterns vary by model: one LLM implicitly diversifies at larger selection budgets, while another increases redundancy. In contrast, LLMs underperform on lexical coverage at small selection budgets. As a result, LLM rankings diverge substantially from both baselines rather than consistently approximating either strategy. By eliminating retrieval variance, we can attribute these differences directly to the ranking policy. This diagnostic is model-agnostic and applicable to any ranker, including open source systems and proprietary APIs.

</details>


### [52] [CaliCausalRank: Calibrated Multi-Objective Ad Ranking with Robust Counterfactual Utility Optimization](https://arxiv.org/abs/2602.18786)
*Xikai Yang,Sebastian Sun,Yilin Li,Yue Xing,Ming Wang,Yang Wang*

Main category: cs.LG

TL;DR: 提出了一种名为CaliCausalRank的新框架，旨在解决广告排名系统中多目标优化问题，通过训练时的评分校准、基于约束的多目标优化及鲁棒反事实效用估计等方法，实现在不同流量段的一致性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的广告排名系统在同时优化点击率(CTR)、转化率(CVR)、收入和用户体验指标等多个目标时遇到了挑战，比如跨流量段的评分尺度不一致以及点击日志中的位置偏见导致的离线-在线度量差异。

Method: CaliCausalRank框架将评分校准视为首要训练目标之一，采用拉格朗日松弛法来满足约束条件，并利用方差减少的反事实估计器来进行可靠的离线评估。

Result: 在Criteo和Avazu数据集上的实验表明，与最佳基线(PairRank)相比，CaliCausalRank实现了1.1%的相对AUC提升、31.6%的校准误差降低以及3.2%的效用增长，同时保持了不同流量段间的一致性表现。

Conclusion: CaliCausalRank提供了一个统一的解决方案，能够有效地处理广告排名系统中的多目标优化难题，包括评分尺度不一致性和位置偏差等问题，从而提高了系统的整体性能。

Abstract: Ad ranking systems must simultaneously optimize multiple objectives including click-through rate (CTR), conversion rate (CVR), revenue, and user experience metrics. However, production systems face critical challenges: score scale inconsistency across traffic segments undermines threshold transferability, and position bias in click logs causes offline-online metric discrepancies. We propose CaliCausalRank, a unified framework that integrates training-time scale calibration, constraint-based multi-objective optimization, and robust counterfactual utility estimation. Our approach treats score calibration as a first-class training objective rather than post-hoc processing, employs Lagrangian relaxation for constraint satisfaction, and utilizes variance-reduced counterfactual estimators for reliable offline evaluation. Experiments on the Criteo and Avazu datasets demonstrate that CaliCausalRank achieves 1.1% relative AUC improvement, 31.6% calibration error reduction, and 3.2% utility gain compared to the best baseline (PairRank) while maintaining consistent performance across different traffic segments.

</details>


### [53] [Learning Beyond Optimization: Stress-Gated Dynamical Regime Regulation in Autonomous Systems](https://arxiv.org/abs/2602.18581)
*Sheng Ran*

Main category: cs.LG

TL;DR: 该论文提出了一种无明确目标函数的学习动态框架，系统通过评估自身内部动态的内在健康状况来调节结构可塑性。引入了双时间尺度架构，将快速状态演化与慢速结构适应区分开来，并通过内部生成的压力变量触发状态依赖性的结构修改事件，从而在不依赖外部定义目标的情况下产生自我组织的学习阶段。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统向真正的自主性迈进，在长时间跨度和不断变化的情境下运作时，目标可能会变得模糊、变化或完全缺失。研究者提出了一个核心问题：在没有明确目标函数的情况下，系统如何判断其正在进行的内部动态是有效的还是病态的？以及它应该如何在缺乏外部监督的情况下调节结构变化？

Method: 本文提出了一种新的学习框架，不需要显式的目标函数。系统不是最小化外部错误信号，而是评估自己内部动态的内在健康状况，并据此调整结构可塑性。为此，引入了一个双时间尺度架构，其中快速状态演变与缓慢的结构性适应过程相分离，并且这两个过程通过一个内部产生的压力变量联系起来，该变量累积了持续动力学功能障碍的证据。结构改变不再是连续发生的，而是作为一个状态依赖事件被触发。

Result: 通过一个极简玩具模型证明，这种由压力调节的机制能够在不依赖于外部定义目标的情况下产生分段的、自组织的学习阶段。

Conclusion: 结果表明，提出的基于压力调节机制的方法可以为开发能够自我评估并进行内部调节结构重组的自主学习系统提供一种可能路径。

Abstract: Despite their apparent diversity, modern machine learning methods can be reduced to a remarkably simple core principle: learning is achieved by continuously optimizing parameters to minimize or maximize a scalar objective function. This paradigm has been extraordinarily successful for well-defined tasks where goals are fixed and evaluation criteria are explicit. However, if artificial systems are to move toward true autonomy-operating over long horizons and across evolving contexts-objectives may become ill-defined, shifting, or entirely absent. In such settings, a fundamental question emerges: in the absence of an explicit objective function, how can a system determine whether its ongoing internal dynamics are productive or pathological? And how should it regulate structural change without external supervision? In this work, we propose a dynamical framework for learning without an explicit objective. Instead of minimizing external error signals, the system evaluates the intrinsic health of its own internal dynamics and regulates structural plasticity accordingly. We introduce a two-timescale architecture that separates fast state evolution from slow structural adaptation, coupled through an internally generated stress variable that accumulates evidence of persistent dynamical dysfunction. Structural modification is then triggered not continuously, but as a state-dependent event. Through a minimal toy model, we demonstrate that this stress-regulated mechanism produces temporally segmented, self-organized learning episodes without reliance on externally defined goals. Our results suggest a possible route toward autonomous learning systems capable of self-assessment and internally regulated structural reorganization.

</details>


### [54] [Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction](https://arxiv.org/abs/2602.19987)
*Ha-Anh Hoang Nguyen,Tri-Duc Phan Le,Duc-Hoang Pham,Huy-Son Nguyen,Cam-Van Thi Nguyen,Duc-Trong Le,Hoang-Quynh Le*

Main category: cs.LG

TL;DR: 本文提出了一种名为CURE的框架，用于解决时间到事件反事实生存预测问题。通过综合多模态嵌入和潜在子群检索，CURE能够集成临床、副临床、人口统计学及多组学信息，并通过交叉注意力机制进行对齐与融合。实验结果表明，CURE模型在METABRIC和TCGA-LUAD数据集上的生存分析中始终优于强大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 面对存在异质性和删失数据的情况下优化个体化生存结果的问题，需要开发一种新的方法来改进现有的生存分析技术。

Method: 提出了CURE框架，该框架通过全面的多模态嵌入和潜在子群检索来推进反事实生存建模。CURE整合了多种类型的信息，并利用交叉注意力机制对这些信息进行了对齐与融合。此外，还使用了一个专家混合架构来适应性地精炼复杂的多组学信号。

Result: 在METABRIC和TCGA-LUAD数据集上，CURE模型在生存分析中的表现优于强基线模型，这通过时间依赖一致性指数（$C^{td}$）和综合布里尔分数（IBS）进行评估得以证明。

Conclusion: CURE展示了其在增强多模态理解方面的潜力，并可作为未来治疗推荐模型的基础。所有代码及相关资源均已公开，以促进研究的可重复性。

Abstract: This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.

</details>


### [55] [GIST: Targeted Data Selection for Instruction Tuning via Coupled Optimization Geometry](https://arxiv.org/abs/2602.18584)
*Guanghui Min,Tianhao Huang,Ke Wan,Chen Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为GIST的新方法，用于在参数高效微调场景中选择有影响力的数据子集。GIST通过从验证梯度恢复任务特定子空间，并将训练梯度投影到该子空间来实现更准确的数据选择，从而在相同的预算下以极低的存储和计算成本匹配或超越现有最佳基线表现。


<details>
  <summary>Details</summary>
Motivation: 现有的数据选择方法通常假设参数之间是坐标独立的，这种假设在参数高效微调（如LoRA）中不成立，因为在这种情况下优化几何表现出强参数耦合以及非对角线交互作用。为了克服这一局限性，作者提出了GIST方法。

Method: GIST采用梯度等距子空间变换技术，首先通过奇异值分解(SVD)从验证梯度中提取任务相关的子空间；然后将训练梯度映射到这个子空间内；最后依据样本与目标方向的一致性对其进行评分。

Result: 广泛的实验表明，在相同的选择预算条件下，GIST不仅能够达到或优于当前最先进的基线水平，而且只需要后者0.29%的存储空间和25%的计算时间。

Conclusion: GIST提供了一个简单而原则性的替代方案，通过鲁棒子空间对齐代替轴向缩放，有效解决了参数高效微调过程中存在的跨参数耦合问题，同时大幅降低了所需的资源消耗。

Abstract: Targeted data selection has emerged as a crucial paradigm for efficient instruction tuning, aiming to identify a small yet influential subset of training examples for a specific target task. In practice, influence is often measured through the effect of an example on parameter updates. To make selection scalable, many approaches leverage optimizer statistics (e.g., Adam states) as an axis-aligned surrogate for update geometry (i.e., diagonal precondition), implicitly treating parameters as coordinate-wise independent. We show that this assumption breaks down in parameter-efficient fine-tuning (PEFT) methods such as LoRA. In this setting, the induced optimization geometry exhibits strong cross-parameter coupling with non-trivial off-diagonal interactions, while the task-relevant update directions are confined to a low-dimensional subspace. Motivated by this mismatch, we propose GIST (Gradient Isometric Subspace Transformation), a simple yet principled alternative that replaces axis-aligned scaling with robust subspace alignment. GIST recovers a task-specific subspace from validation gradients via spectral filtering (SVD), projects training gradients into this coupled subspace, and scores examples by their alignment with target directions.Extensive experiments have demonstrated that GIST matches or outperforms the state-of-the-art baseline with only 0.29% of the storage and 25% of the computational time under the same selection budget.

</details>


### [56] [Learning Invariant Visual Representations for Planning with Joint-Embedding Predictive World Models](https://arxiv.org/abs/2602.18639)
*Leonardo F. Toso,Davit Shadunts,Yunyang Lu,Nihal Sharma,Donglin Zhan,Nam H. Nguyen,James Anderson*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过增加一个双模拟编码器来改进最近的潜在预测架构（JEPAs），以提高对慢特征（如背景变化和视觉干扰）的鲁棒性。该模型在导航任务中表现出更好的鲁棒性，并且能够在比DINO世界模型小得多的潜在空间中运行。此外，它对于预训练视觉编码器的选择是不可知的，与DINOv2、SimDINOv2和iBOT特征搭配时仍能保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于高维视觉观察学习的世界模型（如DINO-WM）在测试时由于对‘慢特征’敏感而导致鲁棒性下降。这些慢特征包括背景变化和与解决任务无关的视觉干扰。为了解决这个问题，作者提出了一个新的方法来增强模型对这类变化的鲁棒性。

Method: 作者的方法是在预测目标上添加了一个双模拟编码器，该编码器实施了控制相关的状态等价性，将具有相似转移动态的状态映射到附近的潜在状态，同时限制来自慢特征的贡献。这样设计的目的是为了让模型能够更好地忽略那些不重要的视觉变化，专注于真正影响任务执行的因素。

Result: 通过对不同测试时间背景变化及视觉干扰下的简单导航任务进行评估，结果表明该模型在所有基准测试中一致地提高了对慢特征的鲁棒性，同时工作在一个显著减小的潜在空间内，这个空间最多可以比DINO-WM小10倍。

Conclusion: 本研究成功开发出一种更鲁棒于慢特征的世界模型，其不仅减少了所需的潜在空间大小，而且还能灵活地与多种预训练视觉编码器一起使用而不损失性能。

Abstract: World models learned from high-dimensional visual observations allow agents to make decisions and plan directly in latent space, avoiding pixel-level reconstruction. However, recent latent predictive architectures (JEPAs), including the DINO world model (DINO-WM), display a degradation in test time robustness due to their sensitivity to "slow features". These include visual variations such as background changes and distractors that are irrelevant to the task being solved. We address this limitation by augmenting the predictive objective with a bisimulation encoder that enforces control-relevant state equivalence, mapping states with similar transition dynamics to nearby latent states while limiting contributions from slow features. We evaluate our model on a simple navigation task under different test-time background changes and visual distractors. Across all benchmarks, our model consistently improves robustness to slow features while operating in a reduced latent space, up to 10x smaller than that of DINO-WM. Moreover, our model is agnostic to the choice of pretrained visual encoder and maintains robustness when paired with DINOv2, SimDINOv2, and iBOT features.

</details>


### [57] [Adaptive Time Series Reasoning via Segment Selection](https://arxiv.org/abs/2602.18645)
*Shvat Messica,Jiawen Zhang,Kevin Li,Theodoros Tsiligkaridis,Marinka Zitnik*

Main category: cs.LG

TL;DR: ARTIST, a model that uses reinforcement learning for adaptive temporal segment selection and reasoning over time series data, outperforms existing methods on several benchmarks, especially in rare event localization and multi-segment reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列推理方法通常在推断前将整个时间序列编码为固定表示，而不论整个序列是否都与问题相关。这导致模型无法根据问题选择性地检查时间序列中的相关信息。

Method: 提出了一种名为ARTIST的模型，该模型将时间序列推理视为一个顺序决策问题。它采用了控制器-推理器架构，并使用强化学习来训练控制器角色以选择信息丰富的片段，以及训练推理器角色生成基于片段条件的推理轨迹和最终答案。此外，还采用了一种新颖的层次策略优化方法来进行后训练，使模型在片段选择和问答行为方面表现出色。

Result: ARTIST在六个时间序列推理基准上进行了评估，并与大型语言模型、视觉-语言模型以及先前的时间序列推理系统进行了比较。结果显示，ARTIST比最强基线平均提高了6.46个百分点的准确性，在罕见事件定位和多片段推理任务中取得了最大的进步。监督微调提升了性能，而强化学习通过优化面向问题的片段选择进一步增强了效果。

Conclusion: 选择性数据利用对于有效的时间序列推理至关重要。ARTIST通过自适应选择时间序列片段并结合强化学习技术，在多个基准测试中展示了其优越性。

Abstract: Time series reasoning tasks often start with a natural language question and require targeted analysis of a time series. Evidence may span the full series or appear in a few short intervals, so the model must decide what to inspect. Most existing approaches encode the entire time series into a fixed representation before inference, regardless of whether or not the entire sequence is relevant. We introduce ARTIST, which formulates time-series reasoning as a sequential decision problem. ARTIST interleaves reasoning with adaptive temporal segment selection. It adopts a controller-reasoner architecture and uses reinforcement learning to train the controller role to select informative segments and the reasoner role to generate segment-conditioned reasoning traces and final answers. During inference, the model actively acquires task-relevant information instead of relying on a static summary of the full sequence. We use a novel hierarchical policy optimization approach for post-training that allows the model to excel in both segment selection and question-answering behavior. We evaluate ARTIST on six time-series reasoning benchmarks and compare it with large language models, vision-language models, and prior time-series reasoning systems. ARTIST improves average accuracy by 6.46 absolute percentage points over the strongest baseline. The largest gains appear on rare event localization and multi-segment reasoning tasks. Supervised fine-tuning improves performance, and reinforcement learning provides additional gains by optimizing question-adaptive segment selection. These results show that selective data use drives effective time-series reasoning.

</details>


### [58] [Information-Guided Noise Allocation for Efficient Diffusion Training](https://arxiv.org/abs/2602.18647)
*Gabriel Raya,Bac Nguyen,Georgios Batzolis,Yuhta Takida,Dejan Stancevic,Naoki Murata,Chieh-Hsin Lai,Yuki Mitsufuji,Luca Ambrogioni*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的数据自适应训练噪声调度方法InfoNoise，该方法通过估计去噪损失中的熵减少率来指导噪声采样分布，从而改进了扩散模型的训练效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型训练依赖于手动调整的噪声计划，这可能会在信息量较低的噪声区域浪费计算资源，并且限制了跨数据集、分辨率和表示法的迁移性。

Method: 作者从信息论的角度重新审视了噪声计划分配，并提出了前向过程条件熵率作为诊断现有计划中次优噪声级别分配的理论基础和数据依赖性指标。基于这些见解，他们引入了InfoNoise，这是一种基于原则的数据自适应训练噪声计划，它用从训练过程中已计算的去噪损失估算出的熵减少率得出的信息引导噪声采样分布来取代启发式计划设计。

Result: 在自然图像基准测试中，InfoNoise与经过调优的EDM风格计划相匹配或超越，在某些情况下（如CIFAR-10）训练速度显著加快（约1.4倍）。对于标准图像调谐计划表现出明显不匹配的离散数据集，它可以在少至3倍的训练步骤内达到更优的质量。

Conclusion: 总体而言，InfoNoise使噪声调度变得数据自适应，减少了随着扩散模型扩展到不同领域时对每个数据集进行计划设计的需求。

Abstract: Training diffusion models typically relies on manually tuned noise schedules, which can waste computation on weakly informative noise regions and limit transfer across datasets, resolutions, and representations. We revisit noise schedule allocation through an information-theoretic lens and propose the conditional entropy rate of the forward process as a theoretically grounded, data-dependent diagnostic for identifying suboptimal noise-level allocation in existing schedules. Based on these insight, we introduce InfoNoise, a principled data-adaptive training noise schedule that replaces heuristic schedule design with an information-guided noise sampling distribution derived from entropy-reduction rates estimated from denoising losses already computed during training. Across natural-image benchmarks, InfoNoise matches or surpasses tuned EDM-style schedules, in some cases with a substantial training speedup (about $1.4\times$ on CIFAR-10). On discrete datasets, where standard image-tuned schedules exhibit significant mismatch, it reaches superior quality in up to $3\times$ fewer training steps. Overall, InfoNoise makes noise scheduling data-adaptive, reducing the need for per-dataset schedule design as diffusion models expand across domains.

</details>


### [59] [Communication-Efficient Personalized Adaptation via Federated-Local Model Merging](https://arxiv.org/abs/2602.18658)
*Yinan Zou,Md Kamran Chowdhury Shisher,Christopher G. Brinton,Vishrant Tripathi*

Main category: cs.LG

TL;DR: 提出了一种名为Potara的框架，用于联邦个性化设置中参数高效微调方法的应用。该框架通过合并一个联邦模型（捕捉通用知识）和一个本地模型（捕捉个性化知识）来为每个客户端构建个性化模型。实验表明Potara在提高个性化性能的同时减少了通信成本。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法在面对联邦部署下的任务级异质性时面临挑战，需要一种平衡通用与个性化知识的方法，并且现有方法多基于启发式规则缺乏理论依据，同时先前的模型合并方法计算和通信开销大。

Method: Potara框架利用线性模式连通性的概念，证明了预期任务损失有一个方差迹上界，最小化此上界可得闭形式最优混合权重，从而确保合并后的模型比单独使用联邦或本地模型有更紧的界限。

Result: 实验结果表明，在视觉和语言基准测试中，Potara不仅持续改善了个性化表现，同时也减少了所需的通信量，实现了良好的性能-通信权衡。

Conclusion: Potara提供了一个原则性的解决方案，以解决联邦学习环境下面向个性化需求的任务级异构性问题，通过优化模型合并过程中的权重分配策略，有效提升了个性化适应能力并降低了资源消耗。

Abstract: Parameter-efficient fine-tuning methods, such as LoRA, offer a practical way to adapt large vision and language models to client tasks. However, this becomes particularly challenging under task-level heterogeneity in federated deployments. In this regime, personalization requires balancing general knowledge with personalized knowledge, yet existing approaches largely rely on heuristic mixing rules and lack theoretical justification. Moreover, prior model merging approaches are also computation and communication intensive, making the process inefficient in federated settings. In this work, we propose Potara, a principled framework for federated personalization that constructs a personalized model for each client by merging two complementary models: (i) a federated model capturing general knowledge, and (ii) a local model capturing personalized knowledge. Through the construct of linear mode connectivity, we show that the expected task loss admits a variance trace upper bound, whose minimization yields closed-form optimal mixing weights that guarantee a tighter bound for the merged model than for either the federated or local model alone. Experiments on vision and language benchmarks show that Potara consistently improves personalization while reducing communication, leading to a strong performance-communication trade-off.

</details>


### [60] [Large Causal Models for Temporal Causal Discovery](https://arxiv.org/abs/2602.18662)
*Nikolaos Kougioulis,Nikolaos Gkorgkolis,MingXue Wang,Bora Caglayan,Dario Simionato,Andrea Tonon,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 本文提出了一种大型因果模型（LCM）的原理框架，结合了多样的合成生成器和现实的时间序列数据集，能够在更大规模上进行学习。实验表明，LCMs能够有效地扩展到更高的变量数和更深的架构，同时保持强大的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的因果发现方法针对横截面和时间序列数据通常遵循特定数据集的范式，为每个单独的数据集拟合新模型。这种方法限制了多数据集预训练的潜力。

Method: 提出了一个大型因果模型（LCMs）的原理框架，该框架结合了多样化的合成生成器与实际时间序列数据集，支持大规模学习。

Result: 通过在合成、半合成及真实基准上的广泛实验表明，LCMs能够有效扩展至更高变量计数和更深层架构的同时保持出色表现。训练后的模型相比经典和神经基线，在分布外设置下达到了竞争性或更好的准确性，并且实现了快速单次推理。

Conclusion: 结果证明，LCMs作为时间因果发现的基础模型范例是很有前途的。

Abstract: Causal discovery for both cross-sectional and temporal data has traditionally followed a dataset-specific paradigm, where a new model is fitted for each individual dataset. Such an approach limits the potential of multi-dataset pretraining. The concept of large causal models (LCMs) envisions a class of pre-trained neural architectures specifically designed for temporal causal discovery. Prior approaches are constrained to small variable counts, degrade with larger inputs, and rely heavily on synthetic data, limiting generalization. We propose a principled framework for LCMs, combining diverse synthetic generators with realistic time-series datasets, allowing learning at scale. Extensive experiments on synthetic, semi-synthetic and realistic benchmarks show that LCMs scale effectively to higher variable counts and deeper architectures while maintaining strong performance. Trained models achieve competitive or superior accuracy compared to classical and neural baselines, particularly in out-of-distribution settings, while enabling fast, single-pass inference. Results demonstrate LCMs as a promising foundation-model paradigm for temporal causal discovery. Experiments and model weights are available at https://github.com/kougioulis/LCM-paper/.

</details>


### [61] [Transformers for dynamical systems learn transfer operators in-context](https://arxiv.org/abs/2602.18679)
*Anthony Bao,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 研究发现，基于注意力机制的模型可以通过上下文学习策略来预测未见过的动力系统。这种策略包括使用延迟嵌入提升低维时间序列以检测系统的高维动力学流形，并识别和预测表征该流形上全局流动的长期不变集。


<details>
  <summary>Details</summary>
Motivation: 探索在不重新训练的情况下，小规模的基于变压器的模型如何通过上下文学习适应并预测不同的动力系统，以及这一过程中表现出的性能权衡现象。

Method: 采用了一个小型两层、单头变压器进行单一动力系统的预测训练，然后评估其对不同动力系统进行预测的能力而无需进一步训练。

Result: 观察到了在分布内与分布外性能之间存在早期权衡的现象，表现为二次下降现象；揭示了基于注意力机制的模型能够利用全局吸引子信息来提高短期预测能力。

Conclusion: 本研究表明了预训练的大模型可以在未经重新训练的情况下预测未见物理系统背后的机制，并展示了注意力机制模型利用全局吸引子信息服务于短期预测的独特能力。

Abstract: Large-scale foundation models for scientific machine learning adapt to physical settings unseen during training, such as zero-shot transfer between turbulent scales. This phenomenon, in-context learning, challenges conventional understanding of learning and adaptation in physical systems. Here, we study in-context learning of dynamical systems in a minimal setting: we train a small two-layer, single-head transformer to forecast one dynamical system, and then evaluate its ability to forecast a different dynamical system without retraining. We discover an early tradeoff in training between in-distribution and out-of-distribution performance, which manifests as a secondary double descent phenomenon. We discover that attention-based models apply a transfer-operator forecasting strategy in-context. They (1) lift low-dimensional time series using delay embedding, to detect the system's higher-dimensional dynamical manifold, and (2) identify and forecast long-lived invariant sets that characterize the global flow on this manifold. Our results clarify the mechanism enabling large pretrained models to forecast unseen physical systems at test without retraining, and they illustrate the unique ability of attention-based models to leverage global attractor information in service of short-term forecasts.

</details>


### [62] [In-Context Planning with Latent Temporal Abstractions](https://arxiv.org/abs/2602.18694)
*Baiting Luo,Yunuo Zhang,Nathaniel S. Keplinger,Samir Gupta,Abhishek Dubey,Ayan Mukhopadhyay*

Main category: cs.LG

TL;DR: I-TAP是一种结合了上下文适应与在线规划的离线强化学习框架，通过从离线轨迹中学习观察条件下的残差量化VAE和时间Transformer，以在学习到的离义时间抽象空间中直接进行蒙特卡洛树搜索。它在确定性、随机性和部分可观测环境中的表现优于或匹配强基准方法。


<details>
  <summary>Details</summary>
Motivation: 基于规划的强化学习在连续控制任务中面临两个实际问题：在原始时间尺度上进行规划会导致分支过多及长期预测困难；而真实环境往往是部分可观察的，并且存在状态转换，这使得静态、完全观察的动态假设失效。

Method: I-TAP利用一种观察条件下的残差量化变分自编码器（VAE）将每个观察-宏动作段压缩成一系列粗细不等的离散残差令牌堆栈，并使用时间Transformer根据最近的历史数据自回归地预测这些令牌堆栈。测试时，I-TAP直接在令牌空间执行蒙特卡洛树搜索，采用简短历史隐式适应无需梯度更新，并将选定的令牌堆栈解码为可执行动作。

Result: 在包括确定性的MuJoCo、每集具有潜在动态机制的随机MuJoCo以及高维Adroit操控任务（含部分可观测版本）在内的多个环境中，I-TAP的表现一致地匹配或超越了强大的无模型和有模型离线基线方法，证明了其在随机动态和部分可观测条件下高效且稳健的上下文规划能力。

Conclusion: I-TAP展示了一种新颖的方法来解决连续控制任务中基于规划的强化学习所面临的挑战，通过整合上下文适应与在线规划于学习到的时间抽象空间内，实现了即使在部分可观测及存在状态转换的真实环境下也能有效工作的解决方案。

Abstract: Planning-based reinforcement learning for continuous control is bottlenecked by two practical issues: planning at primitive time scales leads to prohibitive branching and long horizons, while real environments are frequently partially observable and exhibit regime shifts that invalidate stationary, fully observed dynamics assumptions. We introduce I-TAP (In-Context Latent Temporal-Abstraction Planner), an offline RL framework that unifies in-context adaptation with online planning in a learned discrete temporal-abstraction space. From offline trajectories, I-TAP learns an observation-conditioned residual-quantization VAE that compresses each observation-macro-action segment into a coarse-to-fine stack of discrete residual tokens, and a temporal Transformer that autoregressively predicts these token stacks from a short recent history. The resulting sequence model acts simultaneously as a context-conditioned prior over abstract actions and a latent dynamics model. At test time, I-TAP performs Monte Carlo Tree Search directly in token space, using short histories for implicit adaptation without gradient update, and decodes selected token stacks into executable actions. Across deterministic MuJoCo, stochastic MuJoCo with per-episode latent dynamics regimes, and high-dimensional Adroit manipulation, including partially observable variants, I-TAP consistently matches or outperforms strong model-free and model-based offline baselines, demonstrating efficient and robust in-context planning under stochastic dynamics and partial observability.

</details>


### [63] [Insertion Based Sequence Generation with Learnable Order Dynamics](https://arxiv.org/abs/2602.18695)
*Dhruvesh Patel,Benjamin Rozonoyer,Gaurav Pandey,Tahira Naseem,Ramón Fernandez Astudillo,Andrew McCallum*

Main category: cs.LG

TL;DR: 本文提出了一种通过插入生成可变长度序列的方法，该方法利用了可训练的顺序动态来改善离散流匹配的目标率，并采用可变长度掩码扩散模型作为生成模型。实验表明，在图遍历任务和小分子生成任务上，所提出的方法在灵活性、训练稳定性和生成质量之间取得了良好的平衡。


<details>
  <summary>Details</summary>
Motivation: 作者旨在解决插入模型相比自回归模型（ARMs）在学习上的挑战，因为插入模型的动作空间更大。为了解决这个问题，文章引入了可训练的顺序动态到离散流匹配中，目的是让联合训练更加可行，无需进行数值模拟。

Method: 使用了可变长度掩码扩散模型作为生成性插入模型，该模型通过插入和填充遮罩标记来生成序列。同时，通过将可训练的顺序动态纳入目标率中，探索了不同参数化选择对灵活性、训练稳定性以及生成质量之间的权衡。

Result: 在图遍历任务中，对于已知局部最优插入顺序的情况，研究了参数化的实证选择并展示了灵活性、训练稳定性和生成质量之间的权衡。在从头开始的小分子生成方面，与均匀顺序动态相比，学习到的顺序动态导致生成的有效分子数量增加且质量提高。

Conclusion: 研究表明，通过结合可训练的顺序动态与可变长度掩码扩散模型，可以在不需要数值模拟的情况下实现有效且高质量的序列生成，尤其适用于图遍历和小分子生成等领域。

Abstract: In many domains generating variable length sequences through insertions provides greater flexibility over autoregressive models. However, the action space of insertion models is much larger than that of autoregressive models (ARMs) making the learning challenging. To address this, we incorporate trainable order dynamics into the target rates for discrete flow matching, and show that with suitable choices of parameterizations, joint training of the target order dynamics and the generator is tractable without the need for numerical simulation. As the generative insertion model, we use a variable length masked diffusion model, which generates by inserting and filling mask tokens. On graph traversal tasks for which a locally optimal insertion order is known, we explore the choices of parameterization empirically and demonstrate the trade-offs between flexibility, training stability and generation quality. On de novo small molecule generation, we find that the learned order dynamics leads to an increase in the number of valid molecules generated and improved quality, when compared to uniform order dynamics.

</details>


### [64] [Phase-Consistent Magnetic Spectral Learning for Multi-View Clustering](https://arxiv.org/abs/2602.18728)
*Mingdong Lu,Zhikui Chen,Meng Liu,Shubin Ma,Liang Zhao*

Main category: cs.LG

TL;DR: 提出了一种相位一致磁谱学习方法，通过显式建模视图间的定向一致性，并结合非负幅度骨架形成复数值磁亲和力，从而指导无监督多视图表示学习与聚类。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督多视图聚类方法在处理不同视图间具有相似强度但方向性趋势矛盾的关系时表现不稳定，这会导致全局谱几何失真并降低聚类效果。

Method: 引入了相位一致磁谱学习框架，该框架通过将跨视图方向一致性建模为一个相位项，并将其与非负幅度基础相结合以创建复数值磁亲和力；利用Hermitian磁拉普拉斯算子提取稳定的共享谱信号，并作为结构化自监督来引导无监督的多视图表示学习与聚类。同时，为了在大规模上获得鲁棒的谱提取输入，采用了基于锚点的高阶共识建模构建紧凑共享结构，并应用轻量级细化来抑制噪声或不一致关系。

Result: 在多个公开的多视图基准测试中进行了广泛的实验，结果表明所提出的方法能够持续优于强大的基线模型。

Conclusion: 所提出的相位一致磁谱学习方法有效地解决了因视图间存在矛盾的方向性趋势而导致的无监督多视图聚类问题，通过改进的谱学习策略提高了聚类性能。

Abstract: Unsupervised multi-view clustering (MVC) aims to partition data into meaningful groups by leveraging complementary information from multiple views without labels, yet a central challenge is to obtain a reliable shared structural signal to guide representation learning and cross-view alignment under view discrepancy and noise. Existing approaches often rely on magnitude-only affinities or early pseudo targets, which can be unstable when different views induce relations with comparable strengths but contradictory directional tendencies, thereby distorting the global spectral geometry and degrading clustering. In this paper, we propose \emph{Phase-Consistent Magnetic Spectral Learning} for MVC: we explicitly model cross-view directional agreement as a phase term and combine it with a nonnegative magnitude backbone to form a complex-valued magnetic affinity, extract a stable shared spectral signal via a Hermitian magnetic Laplacian, and use it as structured self-supervision to guide unsupervised multi-view representation learning and clustering. To obtain robust inputs for spectral extraction at scale, we construct a compact shared structure with anchor-based high-order consensus modeling and apply a lightweight refinement to suppress noisy or inconsistent relations. Extensive experiments on multiple public multi-view benchmarks demonstrate that our method consistently outperforms strong baselines.

</details>


### [65] [Prior Aware Memorization: An Efficient Metric for Distinguishing Memorization from Generalization in Large Language Models](https://arxiv.org/abs/2602.18733)
*Trishita Tiwari,Ari Trachtenberg,G. Edward Suh*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，称为先验感知记忆化（Prior-Aware Memorization），用于识别大型语言模型中真正的数据记忆现象。该方法无需重新训练模型且计算成本较低，通过评估候选后缀与其特定训练前缀之间是否存在强关联来区分真实记忆与统计常见序列。研究结果表明，在先前被标记为记忆化的序列中有55%到90%实际上是统计上常见的，这强调了在评估泄露风险时考虑模型先验的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有测量语言模型记忆化的方法往往混淆了真实的数据记忆和生成统计常见序列的现象，导致对记忆化的错误标签。此外，反事实记忆化虽然提供了解决方案但需要多次重训练模型，实际操作中成本高昂。因此，需要一种既能够准确区分这两种情况又不需要大量计算资源的新方法。

Method: 提出了先验感知记忆化标准，这是一种理论基础坚实、轻量级且无需训练的方法，用于识别大型语言模型中的真实记忆现象。该方法的核心思想是判断候选后缀是否仅与特定的训练前缀高度相关还是由于统计共性而可能出现在许多不相关的提示中。

Result: 实验结果显示，在两个预训练模型LLaMA和OPT的训练语料库以及SATML训练数据提取挑战数据集上，使用新方法发现之前被认为是由模型记忆引起的序列中很大一部分实际上是统计上常见的。对于版权风险模拟使用的长序列和PII泄露模拟使用的命名实体，这一比例分别为55%至90%和大约40%。

Conclusion: 研究证明了低频次本身不足以作为记忆化的证据，并强调了在评估潜在的数据泄露风险时考虑模型先验知识的重要性。先验感知记忆化提供了一个有效的手段来更准确地衡量真正由记忆引起的数据泄露问题。

Abstract: Training data leakage from Large Language Models (LLMs) raises serious concerns related to privacy, security, and copyright compliance. A central challenge in assessing this risk is distinguishing genuine memorization of training data from the generation of statistically common sequences. Existing approaches to measuring memorization often conflate these phenomena, labeling outputs as memorized even when they arise from generalization over common patterns. Counterfactual Memorization provides a principled solution by comparing models trained with and without a target sequence, but its reliance on retraining multiple baseline models makes it computationally expensive and impractical at scale.
  This work introduces Prior-Aware Memorization, a theoretically grounded, lightweight and training-free criterion for identifying genuine memorization in LLMs. The key idea is to evaluate whether a candidate suffix is strongly associated with its specific training prefix or whether it appears with high probability across many unrelated prompts due to statistical commonality.
  We evaluate this metric on text from the training corpora of two pre-trained models, LLaMA and OPT, using both long sequences (to simulate copyright risks) and named entities (to simulate PII leakage). Our results show that between 55% and 90% of sequences previously labeled as memorized are in fact statistically common. Similar findings hold for the SATML training data extraction challenge dataset, where roughly 40% of sequences exhibit common-pattern behavior despite appearing only once in the training data. These results demonstrate that low frequency alone is insufficient evidence of memorization and highlight the importance of accounting for model priors when assessing leakage.

</details>


### [66] [When World Models Dream Wrong: Physical-Conditioned Adversarial Attacks against World Models](https://arxiv.org/abs/2602.18739)
*Zhixiang Guo,Siyuan Liang,Andras Balogh,Noah Lunberry,Rong-Cheng Tu,Mark Jelasity,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出了物理条件世界模型攻击(PhysCond-WMA)，这是首个针对生成式世界模型的白盒攻击方法，通过扰动如HDMap嵌入和3D-box特征等物理条件通道，在保持感知真实性的同时诱导语义、逻辑或决策级扭曲。实验结果表明该方法有效，并揭示了生成式世界模型中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着生成式世界模型在合成可控、传感器条件下的驾驶视频中应用日益广泛，它们对物理先验的依赖性暴露出了新的攻击面。本文旨在探索并量化这些安全漏洞。

Method: 提出了一种名为Physical-Conditioned World Model Attack (PhysCond-WMA)的方法，分两个阶段优化：质量保留指导阶段确保反向扩散损失低于校准阈值；动量导向去噪阶段沿着去噪轨迹累积与目标一致的梯度，以实现稳定的时间一致性语义转换。

Result: 实验结果显示，本方法在提高FID约9%及FVD约3.9%的情况下依然有效。在目标攻击设定下，攻击成功率(ASR)达到0.55。进一步研究表明，使用被攻击视频进行训练会降低3D检测性能约4%，并对开环规划性能产生约20%的负面影响。

Conclusion: 这项研究首次揭示并量化了生成式世界模型中存在的安全漏洞，强调了开发更全面的安全检查机制的重要性。

Abstract: Generative world models (WMs) are increasingly used to synthesize controllable, sensor-conditioned driving videos, yet their reliance on physical priors exposes novel attack surfaces. In this paper, we present Physical-Conditioned World Model Attack (PhysCond-WMA), the first white-box world model attack that perturbs physical-condition channels, such as HDMap embeddings and 3D-box features, to induce semantic, logic, or decision-level distortion while preserving perceptual fidelity. PhysCond-WMA is optimized in two stages: (1) a quality-preserving guidance stage that constrains reverse-diffusion loss below a calibrated threshold, and (2) a momentum-guided denoising stage that accumulates target-aligned gradients along the denoising trajectory for stable, temporally coherent semantic shifts. Extensive experimental results demonstrate that our approach remains effective while increasing FID by about 9% on average and FVD by about 3.9% on average. Under the targeted attack setting, the attack success rate (ASR) reaches 0.55. Downstream studies further show tangible risk, which using attacked videos for training decreases 3D detection performance by about 4%, and worsens open-loop planning performance by about 20%. These findings has for the first time revealed and quantified security vulnerabilities in generative world models, driving more comprehensive security checkers.

</details>


### [67] [Vectorized Bayesian Inference for Latent Dirichlet-Tree Allocation](https://arxiv.org/abs/2602.18795)
*Zheng Wang,Nizar Bouguila*

Main category: cs.LG

TL;DR: 本文提出了Latent Dirichlet-Tree Allocation (LDTA)框架，它是LDA的泛化版本，能够使用任意Dirichlet-Tree分布替代原有的Dirichlet先验，从而更好地表示主题之间的丰富关联和层次关系。


<details>
  <summary>Details</summary>
Motivation: 传统的LDA模型虽然能够发现离散数据中的潜在主题结构，但其使用的Dirichlet先验无法充分表达主题之间复杂的关联性和层次性。为了克服这一局限性，作者引入了LDTA框架。

Method: 研究者开发了通用的平均场变分推断和期望传播方法来实现LDTA框架下的推理，并且提供了适用于所有Dirichlet-Tree分布的有效更新规则。此外，还展示了两种推理方法的向量化特性，并实现了完全向量化及GPU加速的算法执行。

Result: 通过理论分析与实验验证表明，LDTA不仅大大扩展了LDA的建模能力，同时保持了良好的可扩展性和计算效率。

Conclusion: LDTA作为一种新的文本主题模型，通过采用更加灵活的树状结构先验，有效增强了对复杂主题间关系的捕捉能力，为后续相关研究提供了一个有力的新工具。

Abstract: Latent Dirichlet Allocation (LDA) is a foundational model for discovering latent thematic structure in discrete data, but its Dirichlet prior cannot represent the rich correlations and hierarchical relationships often present among topics. We introduce the framework of Latent Dirichlet-Tree Allocation (LDTA), a generalization of LDA that replaces the Dirichlet prior with an arbitrary Dirichlet-Tree (DT) distribution. LDTA preserves LDA's generative structure but enables expressive, tree-structured priors over topic proportions. To perform inference, we develop universal mean-field variational inference and Expectation Propagation, providing tractable updates for all DT. We reveal the vectorized nature of the two inference methods through theoretical development, and perform fully vectorized, GPU-accelerated implementations. The resulting framework substantially expands the modeling capacity of LDA while maintaining scalability and computational efficiency.

</details>


### [68] [SGNO: Spectral Generator Neural Operators for Stable Long Horizon PDE Rollouts](https://arxiv.org/abs/2602.18801)
*Jiayi Li,Zhaonan Wang,Flora D. Salim*

Main category: cs.LG

TL;DR: 本文提出了一种新的神经算子SGNO，它通过在傅里叶空间中使用指数时间差分更新和学习的对角生成器来解决长时间预测中的误差累积和高频成分反馈增长问题。实验表明，SGNO在长时预测上比其他基线方法具有更低的误差和更稳定的预测性能。


<details>
  <summary>Details</summary>
Motivation: 神经算子能够快速求解偏微分方程（PDE）并通常能在参数和分辨率之间泛化。然而，在短训练长测试的情况下，自回归展开可能会变得不稳定。这主要是由于一步误差随时间累积以及高频成分反馈增强导致的。为了解决这些问题，作者提出了Spectral Generator Neural Operator (SGNO)。

Method: SGNO采用一种残差时间步进方法，针对上述两个问题设计了专门的解决方案。对于线性部分，SGNO利用傅里叶空间中的指数时间差分更新，并引入了一个学习到的对角生成器；通过限制该生成器实部为非正数，确保迭代过程中不会放大线性动态。对于非线性动态，SGNO增加了带有通道混合功能的门控强迫项以控制非线性更新。此外，还应用了谱截断及可选平滑掩模于强迫路径上，进一步减少高频反馈的影响。

Result: 理论分析给出了单步放大界限与有限范围滚动误差界限，将生成器近似误差从非线性不匹配中分离出来，并提供了保证潜变量$L^2$范数在整个滚动步骤中不增长的充分条件。实验结果表明，在跨越1D、2D和3D PDE家族的APEBench数据集上，SGNO相比强大的神经算子基线模型实现了更低的长期误差和更长的稳定滚动长度。消融实验验证了生成器约束、门控机制和过滤的作用。

Conclusion: 本研究提出的SGNO有效解决了传统神经算子在长序列预测中存在的稳定性问题，特别是在处理PDE问题时展现出了优越性能。通过合理设计线性和非线性部分的更新策略，SGNO能够在保持准确度的同时延长预测序列的稳定性。

Abstract: Neural operators provide fast PDE surrogates and often generalize across parameters and resolutions. However, in the short train long test setting, autoregressive rollouts can become unstable. This typically happens for two reasons: one step errors accumulate over time, and high frequency components feed back and grow.
  We introduce the Spectral Generator Neural Operator (SGNO), a residual time stepper that targets both effects. For the linear part, SGNO uses an exponential time differencing update in Fourier space with a learned diagonal generator. We constrain the real part of this generator to be nonpositive, so iterating the step does not amplify the linear dynamics. For nonlinear dynamics, SGNO adds a gated forcing term with channel mixing within each Fourier mode, which keeps the nonlinear update controlled. To further limit high frequency feedback, SGNO applies spectral truncation and an optional smooth mask on the forcing pathway.
  We derive a one step amplification bound and a finite horizon rollout error bound. The bound separates generator approximation error from nonlinear mismatch and gives sufficient conditions under which the latent $L^2$ norm does not grow across rollout steps. On APEBench spanning 1D, 2D, and 3D PDE families, SGNO achieves lower long horizon error and longer stable rollout lengths than strong neural operator baselines. Ablations confirm the roles of the generator constraint, gating, and filtering.The code is available at https://github.com/lijy32123-cloud/SGNO.

</details>


### [69] [Exact Attention Sensitivity and the Geometry of Transformer Stability](https://arxiv.org/abs/2602.18849)
*Seyed Morteza Emadi*

Main category: cs.LG

TL;DR: 本文提出了一种稳定性理论，解释了为什么pre-LayerNorm有效、DeepNorm为何使用$N^{-1/4}$缩放以及预热的必要性。通过引入块-$\infty$/RMS几何学来对齐逐标记计算，从而获得与序列长度无关的Lipschitz界。研究发现，transformer的稳定性完全来源于架构上的梯度流，而非注意力动态变化。


<details>
  <summary>Details</summary>
Motivation: 尽管transformer推动了现代AI的发展，但在训练过程中表现出脆弱性。为了理解这一现象并提高其稳定性，作者开发了一个新的理论框架，旨在从基本原理出发解答几个关键问题：pre-LayerNorm为何有效、DeepNorm使用的特定缩放比例的原因及预热阶段的重要性。

Method: 该研究基于两大支柱构建了框架：首先精确导出了softmax Jacobian的操作范数；其次，提出了一个与tokenwise计算相一致的块-$\infty$/RMS几何结构，以得到独立于序列长度的Lipschitz界限。利用此框架证明了pre-LN保持恒等梯度路径，而post-LN则随着深度增加指数级累积LayerNorm雅可比矩阵，并展示了DeepNorm中$N^{-1/4}$比例因子的来源。

Result: 在774M参数模型上验证了理论的有效性，发现与直觉相反，在整个训练过程中$θ(p) \approx 1$保持不变。这表明transformer的稳定性完全来自架构上的梯度流动特性，而不是学习到的注意力模式的作用。

Conclusion: 本研究表明，transformer模型的训练稳定性主要由其架构决定，特别是梯度流动方式，而非注意力机制本身的动态变化。这意味着未来在设计和优化transformer时，需要更加关注架构层面如何处理敏感性问题。

Abstract: Despite powering modern AI, transformers remain mysteriously brittle to train. We develop a stability theory that explains why pre-LayerNorm works, why DeepNorm uses $N^{-1/4}$ scaling, and why warmup is necessary, all from first principles. Our framework has two pillars: (1) We derive the \emph{exact} operator norm of the softmax Jacobian, $\|J_{softmax}(u/τ)\|_{\infty\to 1} = θ(p)/τ$, where the balanced-mass factor $θ(p)\in[0,1]$ quantifies attention sensitivity. (2) We introduce a block-$\infty$/RMS geometry aligned with tokenwise computation, yielding Lipschitz bounds independent of sequence length. Using this framework, we prove that pre-LN preserves identity gradient paths while post-LN compounds LayerNorm Jacobians exponentially with depth, and we show that DeepNorm's $N^{-1/4}$ emerges from the quartic structure of attention's four projection matrices. We validate our theory on 774M-parameter models and find that, contrary to the intuition that attention sharpens during training to reduce sensitivity, $θ(p) \approx 1$ persists throughout. Transformer stability arises entirely from architectural gradient flow, not from attention dynamics. This finding changes how we reason about training: the architecture itself must handle sensitivity, not learned attention patterns.

</details>


### [70] [Rank-Aware Spectral Bounds on Attention Logits for Stable Low-Precision Training](https://arxiv.org/abs/2602.18851)
*Seyed Morteza Emadi*

Main category: cs.LG

TL;DR: 本文提出了一种基于秩的注意力分数集中不等式，该不等式在低精度训练中对控制溢出风险有更强的约束力。通过这一结果，作者为FP8训练导出了几何感知的比例因子，这些因子可以提供原则性的溢出保证而无需观察激活值。这种方法计算每层的比例因子，并且与融合注意力内核兼容。实验表明，几何感知缩放在消除延迟缩放失败场景中的溢出方面有效，同时保持了下游任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决变压器模型在低精度训练时由于注意力分数的最大幅度所导致的溢出风险问题。传统的方法使用秩无关的边界估计最大注意力分数的尾概率，但这种方法对于现代架构来说过于宽松。

Method: 本文首先推导了一个秩相关的注意力分数集中不等式，该不等式展示了当交互矩阵$M = W^Q W^{K\top}$具有较低秩$r \ll d$时，最大绝对注意力分数的尾概率衰减得更快。接着，利用这个结论为FP8训练开发了几何感知比例因子，它们可以通过隐式幂迭代从谱范数$\|W^Q W^{K\top}\|_2$中获得每层的尺度因子。此外，还引入了一种分组查询注意力公式来避免键扩展。

Result: 新方法在GPT-2 XL到Llama-2-70B等多种模型上进行了测试，结果显示几何感知缩放能够在延迟缩放失败的瞬态场景中消除溢出，同时保持了与延迟缩放方法相当的下游MMLU准确性。

Conclusion: 本文介绍的新方法——基于秩的注意力分数集中不等式和几何感知比例因子，有效地解决了低精度训练过程中潜在的溢出风险问题，同时维持或提高了模型性能。

Abstract: Attention scores in transformers are bilinear forms $S_{ij} = x_i^\top M x_j / \sqrt{d_h}$ whose maximum magnitude governs overflow risk in low-precision training. We derive a \emph{rank-aware concentration inequality}: when the interaction matrix $M = W^Q W^{K\top}$ has rank $r \ll d$, tail probabilities for $\max_{i,j}|S_{ij}|$ decay as $\exp(-d^{2}α^{2}/(γr))$ rather than $\exp(-dα^{2})$, where $γ> 1$ is a typicality parameter. For transformer attention where $r = d_h$, this yields $8$--$28\times$ tighter concentration than rank-agnostic bounds in modern architectures. We apply this result to FP8 training, deriving \emph{geometry-aware scale factors} that provide principled overflow guarantees without observing activations. The method computes per-layer scales from the spectral norm $\|W^Q W^{K\top}\|_2$ via implicit power iteration, includes a grouped query attention formulation that avoids key expansion, and remains compatible with fused attention kernels. Across GPT-2 XL to Llama-2-70B, geometry-aware scaling eliminates overflows in transient scenarios where delayed scaling fails, while achieving comparable downstream MMLU accuracy.

</details>


### [71] [Issues with Measuring Task Complexity via Random Policies in Robotic Tasks](https://arxiv.org/abs/2602.18856)
*Reabetswe M. Nkhumise,Mohamed S. Talamali,Aditya Gilra*

Main category: cs.LG

TL;DR: 本研究评估了基于随机权重猜测（RWG）的策略信息容量（PIC）和最优策略信息容量（POIC）这两种方法在非表格领域内衡量任务复杂度的有效性。通过使用难度递增的机器人操作设置进行测试，结果表明PIC和POIC与典型的理解和经验结果相矛盾，这表明需要开发出比基于RWG的方法更可靠的新指标来衡量非表格强化学习中的任务复杂度。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，测量任务复杂度对于创建有意义的基准和设计有效的课程至关重要。然而，在非表格领域中，有效衡量任务复杂度的方法相对较少。本文旨在通过实证分析现有的基于随机权重猜测（RWG）的信息论度量——策略信息容量（PIC）和最优策略信息容量（POIC），来探讨这些方法是否能够准确地反映实际任务复杂度。

Method: 研究者选取了一系列已知相对复杂度逐步增加的机器人操作场景，并分别采用密集奖励和稀疏奖励两种形式对任务进行了设定。随后，利用PIC和POIC两种方法对这些任务的复杂度进行了评估。

Result: 实验结果显示，根据PIC度量，在相同奖励公式下两连杆机械臂被认为比单连杆机械臂简单，这与机器人控制及强化学习的经验不符；而根据POIC度量，对于同一配置而言，稀疏奖励的任务被认为比密集奖励的任务要容易。这些发现都与常规认知相反。

Conclusion: 基于现有评估，PIC和POIC作为衡量非表格强化学习任务复杂性的指标存在不足之处，它们给出的结果往往与实际情况相悖。因此，未来的研究需要探索和发展新的、更加可靠的复杂性度量方法。

Abstract: Reinforcement learning (RL) has enabled major advances in fields such as robotics and natural language processing. A key challenge in RL is measuring task complexity, which is essential for creating meaningful benchmarks and designing effective curricula. While there are numerous well-established metrics for assessing task complexity in tabular settings, relatively few exist in non-tabular domains. These include (i) Statistical analysis of the performance of random policies via Random Weight Guessing (RWG), and (ii) information-theoretic metrics Policy Information Capacity (PIC) and Policy-Optimal Information Capacity (POIC), which are reliant on RWG. In this paper, we evaluate these methods using progressively difficult robotic manipulation setups, with known relative complexity, with both dense and sparse reward formulations. Our empirical results reveal that measuring complexity is still nuanced. Specifically, under the same reward formulation, PIC suggests that a two-link robotic arm setup is easier than a single-link setup - which contradicts the robotic control and empirical RL perspective whereby the two-link setup is inherently more complex. Likewise, for the same setup, POIC estimates that tasks with sparse rewards are easier than those with dense rewards. Thus, we show that both PIC and POIC contradict typical understanding and empirical results from RL. These findings highlight the need to move beyond RWG-based metrics towards better metrics that can more reliably capture task complexity in non-tabular RL with our task framework as a starting point.

</details>


### [72] [Boosting for Vector-Valued Prediction and Conditional Density Estimation](https://arxiv.org/abs/2602.18866)
*Jian Qian,Shu Ge*

Main category: cs.LG

TL;DR: 本文研究了在一般散度下进行向量值和条件密度预测的稳定性条件，并提出了一个基于指数重加权和几何中位数聚合的通用提升框架GeoMedBoost。该框架在弱学习器条件下实现了经验散度超标误差的指数衰减，同时统一了几种经典提升算法的几何视角。


<details>
  <summary>Details</summary>
Motivation: 尽管提升方法在结构化预测中被广泛使用，但对超出标量损失之外的聚合的一般理论理解仍不完整。本文旨在研究向量值及条件密度预测在更广泛的散度下的稳定性条件，并探索如何通过聚合来增强性能保证。

Method: 定义了(α,β)-可提升性作为稳定性的形式化表述；证明了几何中位数聚合对于一大类散度达到了(α,β)-可提升性；分析了常见散度（如ℓ1, ℓ2, TV, 和Hel）下向量值预测与条件密度估计的可提升性；提出了一种基于指数重加权和几何中位数聚合的新提升框架GeoMedBoost。

Result: 发现几何中位数聚合能够实现多种散度下的(α,β)-可提升性，揭示了维度依赖与维度无关机制之间的明显区别；虽然KL散度不能直接通过几何中位数聚合变得可提升，但可以通过Hellinger距离间接处理；所提出的GeoMedBoost框架在满足弱学习者条件和(α,β)-可提升性时，能获得经验散度超标误差的指数衰减。

Conclusion: 本文为结构化预测中的提升提供了一个新的几何视角，不仅统一了几种经典算法还展示了如何利用几何中位数聚合来改进预测模型的表现。

Abstract: Despite the widespread use of boosting in structured prediction, a general theoretical understanding of aggregation beyond scalar losses remains incomplete. We study vector-valued and conditional density prediction under general divergences and identify stability conditions under which aggregation amplifies weak guarantees into strong ones.
  We formalize this stability property as \emph{$(α,β)$-boostability}. We show that geometric median aggregation achieves $(α,β)$-boostability for a broad class of divergences, with tradeoffs that depend on the underlying geometry. For vector-valued prediction and conditional density estimation, we characterize boostability under common divergences ($\ell_1$, $\ell_2$, $\TV$, and $\Hel$) with geometric median, revealing a sharp distinction between dimension-dependent and dimension-free regimes. We further show that while KL divergence is not directly boostable via geometric median aggregation, it can be handled indirectly through boostability under Hellinger distance.
  Building on these structural results, we propose a generic boosting framework \textsc{GeoMedBoost} based on exponential reweighting and geometric-median aggregation. Under a weak learner condition and $(α,β)$-boostability, we obtain exponential decay of the empirical divergence exceedance error. Our framework recovers classical algorithms such as \textsc{MedBoost}, \textsc{AdaBoost}, and \textsc{SAMME} as special cases, and provides a unified geometric view of boosting for structured prediction.

</details>


### [73] [PCA-VAE: Differentiable Subspace Quantization without Codebook Collapse](https://arxiv.org/abs/2602.18904)
*Hao Lu,Onur C. Koyun,Yongxin Guo,Zhengjie Zhu,Abbas Alili,Metin Nafi Gurcan*

Main category: cs.LG

TL;DR: 本文提出了一种新的PCA-VAE模型，它使用在线PCA瓶颈通过Oja规则训练，以解决向量量化自编码器（VQ-VAE）的非微分性、直通技巧需求以及潜在崩溃问题。该模型在CelebAHQ数据集上的重建质量优于VQ-GAN和SimVQ，并且使用的潜变量位数少10到100倍。此外，PCA-VAE自然地生成可解释的维度，而无需对抗正则化或解缠目标。


<details>
  <summary>Details</summary>
Motivation: 现有的向量量化自编码器存在几个内在缺陷：量化器不可微分，需要直通技巧，并且容易发生坍塌。为了解决这些问题，作者提出了一种基于在线PCA瓶颈的新方法，这种方法是完全可微分的并且更加简单合理。

Method: 提出了PCA-VAE模型，该模型采用在线PCA瓶颈并通过Oja规则进行训练。这种方法不需要代码本、承诺损失或查找噪声，并学习一个正交的、按方差排序的潜变量基。

Result: PCA-VAE在CelebAHQ数据集上实现了比VQ-GAN和SimVQ更高的重建质量，同时使用了少得多的潜变量位数。此外，该模型能够产生自然可解释的维度，如姿态、光照和性别线索，而无需额外的对抗正则化或解缠目标。

Conclusion: 研究表明，PCA可以作为一种替代向量量化的方法，具有数学基础、稳定性、比特效率高以及语义结构化的优点，为生成模型的发展提供了新的方向。

Abstract: Vector-quantized autoencoders deliver high-fidelity latents but suffer inherent flaws: the quantizer is non-differentiable, requires straight-through hacks, and is prone to collapse. We address these issues at the root by replacing VQ with a simple, principled, and fully differentiable alternative: an online PCA bottleneck trained via Oja's rule. The resulting model, PCA-VAE, learns an orthogonal, variance-ordered latent basis without codebooks, commitment losses, or lookup noise. Despite its simplicity, PCA-VAE exceeds VQ-GAN and SimVQ in reconstruction quality on CelebAHQ while using 10-100x fewer latent bits. It also produces naturally interpretable dimensions (e.g., pose, lighting, gender cues) without adversarial regularization or disentanglement objectives. These results suggest that PCA is a viable replacement for VQ: mathematically grounded, stable, bit-efficient, and semantically structured, offering a new direction for generative models beyond vector quantization.

</details>


### [74] [TRUE: A Trustworthy Unified Explanation Framework for Large Language Model Reasoning](https://arxiv.org/abs/2602.18905)
*Yujiao Yang*

Main category: cs.LG

TL;DR: 提出了一种名为TRUE的框架，旨在通过多层次的方法来提高大型语言模型推理过程的可解释性和可靠性，包括实例级的可执行验证、局部结构级的可行区域DAG建模以及类别级的因果失效模式分析。


<details>
  <summary>Details</summary>
Motivation: 现有的解释方法缺乏可信的结构洞察力，并且局限于单个实例分析，无法揭示推理稳定性和系统性失败机制。因此，需要一种新的方法来解决这些问题，从而提高大型语言模型决策过程的透明度和理解度。

Method: TRUE框架结合了可执行推理验证、基于可行区域的有向无环图（DAG）建模以及因果失效模式分析。该框架在实例层面重新定义了推理轨迹并引入盲执行验证；在局部结构层面通过构建一致性的扰动形成可行区域DAG；在类别层面，则采用因果失效模式分析法识别重复出现的结构性失败模式并通过Shapley值量化其影响。

Result: 实验表明，所提出的框架能够提供多层级的、可验证的解释，包括个体实例的可执行推理结构、邻近输入的可行区域表示以及具有重要性量化的类别级别可解释失败模式。这些成果为增强大型语言模型推理系统的可解释性和可靠性奠定了统一而原则性的范式。

Conclusion: TRUE框架有效提高了大型语言模型推理过程中的可解释性与可靠性问题，通过提供多层次的解释方式，使得对于模型如何做出特定决策有了更清晰的理解，并且能够更好地定位和理解模型的系统性错误。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in complex reasoning tasks, yet their decision-making processes remain difficult to interpret. Existing explanation methods often lack trustworthy structural insight and are limited to single-instance analysis, failing to reveal reasoning stability and systematic failure mechanisms. To address these limitations, we propose the Trustworthy Unified Explanation Framework (TRUE), which integrates executable reasoning verification, feasible-region directed acyclic graph (DAG) modeling, and causal failure mode analysis. At the instance level, we redefine reasoning traces as executable process specifications and introduce blind execution verification to assess operational validity. At the local structural level, we construct feasible-region DAGs via structure-consistent perturbations, enabling explicit characterization of reasoning stability and the executable region in the local input space. At the class level, we introduce a causal failure mode analysis method that identifies recurring structural failure patterns and quantifies their causal influence using Shapley values. Extensive experiments across multiple reasoning benchmarks demonstrate that the proposed framework provides multi-level, verifiable explanations, including executable reasoning structures for individual instances, feasible-region representations for neighboring inputs, and interpretable failure modes with quantified importance at the class level. These results establish a unified and principled paradigm for improving the interpretability and reliability of LLM reasoning systems.

</details>


### [75] [DeepInterestGR: Mining Deep Multi-Interest Using Multi-Modal LLMs for Generative Recommendation](https://arxiv.org/abs/2602.18907)
*Yangchen Zeng*

Main category: cs.LG

TL;DR: DeepInterestGR提出了三项创新：多LLM兴趣挖掘、奖励标记的深度兴趣和兴趣增强的商品离散化，以解决浅层兴趣问题。实验表明该方法在HR@K和NDCG@K指标上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐框架主要依赖于表面行为信号（如标题和描述等文本特征）来编码项目，这导致了浅层兴趣问题：模型无法捕捉用户互动背后的潜在语义丰富的兴趣，限制了个性化深度和推荐解释性。

Method: DeepInterestGR通过三个关键创新来解决这个问题：(1) 多LLM兴趣挖掘(MLIM)，利用多个前沿LLM及其多模态变体通过思维链提示提取深层文本和视觉兴趣表示；(2) 奖励标记的深度兴趣(RLDI)，使用轻量级二分类器为挖掘的兴趣分配奖励标签，提供有效的监督信号给强化学习；(3) 兴趣增强的商品离散化(IEID)，将精选的深度兴趣编码成语义嵌入并通过RQ-VAE量化为SID令牌。采用两阶段训练管道：首先通过监督微调使生成模型与深度兴趣信号及协同过滤模式对齐，然后是基于我们兴趣感知奖励优化的GRPO进行强化学习。

Result: 在三个Amazon Review基准测试上的实验显示，DeepInterestGR在HR@K和NDCG@K指标上持续优于最先进基线。

Conclusion: DeepInterestGR通过引入新的机制解决了当前生成式推荐系统中存在的浅层兴趣问题，并在多个评估指标上展示了其相对于现有解决方案的优势。

Abstract: Recent generative recommendation frameworks have demonstrated remarkable scaling potential by reformulating item prediction as autoregressive Semantic ID (SID) generation. However, existing methods primarily rely on shallow behavioral signals, encoding items solely through surface-level textual features such as titles and descriptions. This reliance results in a critical Shallow Interest problem: the model fails to capture the latent, semantically rich interests underlying user interactions, limiting both personalization depth and recommendation interpretability. DeepInterestGR introduces three key innovations: (1) Multi-LLM Interest Mining (MLIM): We leverage multiple frontier LLMs along with their multi-modal variants to extract deep textual and visual interest representations through Chain-of-Thought prompting. (2) Reward-Labeled Deep Interest (RLDI): We employ a lightweight binary classifier to assign reward labels to mined interests, enabling effective supervision signals for reinforcement learning. (3) Interest-Enhanced Item Discretization (IEID): The curated deep interests are encoded into semantic embeddings and quantized into SID tokens via RQ-VAE. We adopt a two-stage training pipeline: supervised fine-tuning aligns the generative model with deep interest signals and collaborative filtering patterns, followed by reinforcement learning with GRPO optimized by our Interest-Aware Reward. Experiments on three Amazon Review benchmarks demonstrate that DeepInterestGR consistently outperforms state-of-the-art baselines across HR@K and NDCG@K metrics.

</details>


### [76] [SLDP: Semi-Local Differential Privacy for Density-Adaptive Analytics](https://arxiv.org/abs/2602.18910)
*Alexey Kroshnin,Alexandra Suvorikova*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，即半局部差分隐私（SLDP），该框架根据局部密度为每个用户分配一个隐私区域，并通过迭代细化来估计这些区域。重要的是，此框架将隐私成本与细化迭代次数解耦，从而允许在不增加额外隐私预算的情况下实现高分辨率网格。


<details>
  <summary>Details</summary>
Motivation: 在本地差分隐私（LDP）下进行密度自适应域离散化对于高实用性隐私保护分析至关重要，但由于与迭代细化相关的隐私预算成本而仍然具有挑战性。

Method: 提出了Semi-Local Differential Privacy (SLDP) 框架，它基于局部密度给每个用户分配隐私区域，并通过点在其隐私区域内可能的移动来定义邻接关系。还介绍了一个交互式的(ε, δ)-SLDP协议，由一个诚实但好奇的服务端通过公共渠道协调，以私密方式估计这些区域。

Result: 实验结果表明，在合成数据集和真实世界数据集上的估计任务中，所提出的框架是有效的。

Conclusion: 新提出的SLDP框架有效地解决了在保持隐私的同时实现高质量数据分析的问题，特别是它能够在不消耗更多隐私预算的前提下支持多次迭代优化。

Abstract: Density-adaptive domain discretization is essential for high-utility privacy-preserving analytics but remains challenging under Local Differential Privacy (LDP) due to the privacy-budget costs associated with iterative refinement. We propose a novel framework, Semi-Local Differential Privacy (SLDP), that assigns a privacy region to each user based on local density and defines adjacency by the potential movement of a point within its privacy region. We present an interactive $(\varepsilon, δ)$-SLDP protocol, orchestrated by an honest-but-curious server over a public channel, to estimate these regions privately. Crucially, our framework decouples the privacy cost from the number of refinement iterations, allowing for high-resolution grids without additional privacy budget cost. We experimentally demonstrate the framework's effectiveness on estimation tasks across synthetic and real-world datasets.

</details>


### [77] [From Human-Level AI Tales to AI Leveling Human Scales](https://arxiv.org/abs/2602.18911)
*Peter Romero,Fernando Martínez-Plumed,Zachary R. Tyler,Matthieu Téhénan,Sipeng Chen,Álvaro David Gómez Antón,Luning Sun,Manuel Cebrian,Lexin Zhou,Yael Moros Daval,Daniel Romero-Alvarado,Félix Martí Pérez,Kevin Wei,José Hernández-Orallo*

Main category: cs.LG

TL;DR: 本文提出了一种将AI模型与'全球人口'进行校准的框架，通过构建多级能力量表并使用公开的人类测试数据来校准每个量表，从而实现对AI性能的标准化评估。


<details>
  <summary>Details</summary>
Motivation: 当前比较AI模型与'人类水平'时，基准分数往往无法直接比较或人类基线来自狭隘的人群样本。为了更准确地衡量AI的表现，作者提出了一个基于全球人口校准的新框架。

Method: 该研究通过构建一套针对不同能力（如推理、理解、知识等）的多级量表，并利用公开发布的教育和推理基准测试数据（PISA, TIMSS, ICAR, UKBioBank, ReliabilityBench）来校准每项能力的量表。此外，还采用大型语言模型(LLMs)来估计基础值B，假设这些模型能够浓缩关于人类群体的丰富信息。

Result: 新方法允许相对于全球人口重新校准和标准化量表，提高了AI模型与人类表现对比的准确性。

Conclusion: 通过引入一种新的校准框架，这项工作为AI模型提供了一个更加公平且广泛适用的评价标准，有助于更好地理解AI相对于整个人类社会的能力水平。

Abstract: Comparing AI models to "human level" is often misleading when benchmark scores are incommensurate or human baselines are drawn from a narrow population. To address this, we propose a framework that calibrates items against the 'world population' and report performance on a common, human-anchored scale. Concretely, we build on a set of multi-level scales for different capabilities where each level should represent a probability of success of the whole world population on a logarithmic scale with a base $B$. We calibrate each scale for each capability (reasoning, comprehension, knowledge, volume, etc.) by compiling publicly released human test data spanning education and reasoning benchmarks (PISA, TIMSS, ICAR, UKBioBank, and ReliabilityBench). The base $B$ is estimated by extrapolating between samples with two demographic profiles using LLMs, with the hypothesis that they condense rich information about human populations. We evaluate the quality of different mappings using group slicing and post-stratification. The new techniques allow for the recalibration and standardization of scales relative to the whole-world population.

</details>


### [78] [Exponential Convergence of (Stochastic) Gradient Descent for Separable Logistic Regression](https://arxiv.org/abs/2602.18946)
*Sacchit Kale,Piyushi Manupriya,Pierre Marion,Francis bach,Anant Raj*

Main category: cs.LG

TL;DR: 该论文证明了通过使用简单且非自适应的递增步长计划，梯度下降可以在完全稳定的优化状态下实现可分离逻辑回归下的指数收敛。此外，论文还展示了使用轻量级自适应步长规则的随机梯度下降能够获得指数加速，从而改进了现有的多项式速率保证。


<details>
  <summary>Details</summary>
Motivation: 先前的工作表明，在接近稳定性边缘时，经常会出现加速现象，此时优化轨迹变得不稳定且难以分析。本文旨在展示加速并不一定需要依赖于不稳定的优化过程，并试图证明通过精心设计的步长增长方案，即使在稳定区域内也能为梯度下降和随机梯度下降方法获得指数加速效果。

Method: 研究者采用了一个简单的、非自适应的步长递增策略来证明梯度下降法能够在满足一定间隔条件下对可分离逻辑回归问题达到指数级别的快速收敛，并且整个过程中保持在稳定状态。对于随机梯度下降法则提出了一种不需要线性搜索或特殊程序的轻量级自适应步长规则，也实现了指数级收敛。

Result: 实验结果表明，所提出的基于非自适应递增步长的方法确实能够让梯度下降算法在处理可分离逻辑回归任务时达到预期的指数级加速效果；同时，对于随机梯度下降而言，新引入的自适应步长调整机制同样有效地提高了其收敛速度至指数级别。

Conclusion: 本文的研究成果表明，仅仅通过对步长增长方式进行恰当的设计，就足以使得梯度下降和随机梯度下降这两种广泛应用于现代机器学习领域中的优化算法，在处理特定类型的问题时（如符合一定条件的可分离逻辑回归），无需进入不稳定区域就能实现显著的速度提升。

Abstract: Gradient descent and stochastic gradient descent are central to modern machine learning, yet their behavior under large step sizes remains theoretically unclear. Recent work suggests that acceleration often arises near the edge of stability, where optimization trajectories become unstable and difficult to analyze. Existing results for separable logistic regression achieve faster convergence by explicitly leveraging such unstable regimes through constant or adaptive large step sizes. In this paper, we show that instability is not inherent to acceleration. We prove that gradient descent with a simple, non-adaptive increasing step-size schedule achieves exponential convergence for separable logistic regression under a margin condition, while remaining entirely within a stable optimization regime. The resulting method is anytime and does not require prior knowledge of the optimization horizon or target accuracy. We also establish exponential convergence of stochastic gradient descent using a lightweight adaptive step-size rule that avoids line search and specialized procedures, improving upon existing polynomial-rate guarantees. Together, our results demonstrate that carefully structured step-size growth alone suffices to obtain exponential acceleration for both gradient descent and stochastic gradient descent.

</details>


### [79] [Toward Manifest Relationality in Transformers via Symmetry Reduction](https://arxiv.org/abs/2602.18948)
*J. François,L. Ravera*

Main category: cs.LG

TL;DR: 该论文提出了一种基于对称性减少的框架，通过使用不变的关系量来重新表述表示、注意力机制和优化动力学，从而直接在关系结构上操作，以减少参数冗余并提供一个有原则的几何框架来分析优化。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer模型中存在的坐标依赖表示和连续对称性导致了内部冗余问题，本文旨在通过一种新的方法来解决这一问题。

Method: 提出了一个基于对称性减少的新框架，该框架利用不变的关系量来重构表示、注意力机制以及优化过程，从而去除构造中的多余自由度。

Result: 开发出能够直接处理关系结构的架构，这为减少参数冗余提供了理论基础，并且对于优化过程的分析也有了更加几何化的理解方式。

Conclusion: 通过对称性减少的方法，可以有效降低Transformer模型的参数冗余，同时提供了一个新的视角来理解和改进模型的优化过程。

Abstract: Transformer models contain substantial internal redundancy arising from coordinate-dependent representations and continuous symmetries, in model space and in head space, respectively. While recent approaches address this by explicitly breaking symmetry, we propose a complementary framework based on symmetry reduction. We reformulate representations, attention mechanisms, and optimization dynamics in terms of invariant relational quantities, eliminating redundant degrees of freedom by construction. This perspective yields architectures that operate directly on relational structures, providing a principled geometric framework for reducing parameter redundancy and analyzing optimization.

</details>


### [80] [Incremental Transformer Neural Processes](https://arxiv.org/abs/2602.18955)
*Philip Mortimer,Cristiana Diaconu,Tommy Rochussen,Bruno Mlodozeniec,Richard E. Turner*

Main category: cs.LG

TL;DR: 本文介绍了一种名为增量TNP（incTNP）的新模型，通过采用因果遮罩、键值缓存以及一种数据高效自回归训练策略，在保持与标准TNPs相同预测性能的同时，将更新的计算成本从二次时间复杂度降低到线性。实验结果表明，incTNP不仅在合成和真实世界任务上表现出色，甚至优于非因果TNPs，并且在顺序推理中实现了显著的速度提升。此外，通过调整“隐式贝叶斯性”指标来评估模型更新的一致性，发现incTNP保持了与标准非因果TNPs一样隐式的贝叶斯预测规则，证明了它能够在不牺牲流式推理所需一致性的情况下获得因果遮罩带来的计算优势。


<details>
  <summary>Details</summary>
Motivation: 许多应用本质上是顺序性的，涉及连续的数据流如实时传感器读数或数据库更新。在这些场景下，模型应该支持廉价的增量更新而非每次新观察时都重新计算内部表示——这是现有TNP变体所缺乏的能力。

Method: 受到大型语言模型的启发，研究者们提出了Incremental TNP (incTNP)。该方法利用因果掩码、键值(KV)缓存和一种数据高效的自回归训练策略，使得incTNP能够匹配标准TNPs的预测性能，同时将更新的计算成本从二次时间复杂度减少至线性。

Result: 通过一系列合成和实际任务的实证评估，包括表格回归和温度预测等，结果显示incTNP不仅能提供与非因果TNPs相当甚至更好的表现，而且为顺序推理带来了数量级上的加速。另外，通过调整一种衡量‘隐式贝叶斯性’的指标来检验模型更新的一致性，研究表明incTNP维持了一个与标准非因果TNPs同样隐含贝叶斯特性的预测规则。

Conclusion: incTNP通过引入因果遮罩、键值缓存及自回归训练策略，在保持高性能的同时显著降低了处理连续数据流时的计算开销，适用于需要快速响应最新数据变化的应用场景。

Abstract: Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference.

</details>


### [81] [Learning to Detect Language Model Training Data via Active Reconstruction](https://arxiv.org/abs/2602.19020)
*Junjie Oscar Yin,John X. Morris,Vitaly Shmatikov,Sewon Min,Hannaneh Hajishirzi*

Main category: cs.LG

TL;DR: 本研究提出了一种新的成员推断攻击(ADRA)，通过主动诱导模型重建给定文本来进行。利用在线策略强化学习，设计了重建度量和对比奖励来提高数据重建和检测效果。实验表明，该方法在检测预训练、后训练和蒸馏数据方面优于现有技术，平均提高了10.7%的性能。


<details>
  <summary>Details</summary>
Motivation: 传统成员推断攻击（MIA）通常基于固定模型权重被动地进行，使用对数似然或文本生成等方式。而这项工作假设训练数据比非成员更易于被重建，并试图通过一种新型的主动数据重建攻击（ADRA）来利用这种差异进行成员推断。受到强化学习可以增强权重中已编码行为的研究发现启发，研究人员开发了这种方法。

Method: 提出了Active Data Reconstruction Attack (ADRA) 方法，这是一种新的成员推断攻击方式，它通过训练积极诱导模型重建特定文本。为有效运用强化学习执行MIA任务，设计了专门的重建指标和对比奖励机制。基于这些设计理念，形成了ADRA及其自适应版本ADRA+两种算法。

Result: 实验结果显示，与现有的MIA相比，所提出的方法在识别预训练、后训练以及蒸馏数据方面表现出显著优势，总体上实现了相对于次优方案约10.7%的性能提升。特别是，在BookMIA测试中的预训练数据检测上，ADRA+相较于Min-K%++提升了18.8%，而在AIME上的后训练数据检测上则提高了7.6%。

Conclusion: 通过引入主动式的数据重建攻击（ADRA），结合在线策略强化学习及专门设计的重建评价标准与奖惩系统，研究者们成功开发出一种能更有效地从大型语言模型中推断其训练数据集成员的新方法。

Abstract: Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \textsc{ADRA} and its adaptive variant \textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\% over the previous runner-up. In particular, \MethodPlus~improves over Min-K\%++ by 18.8\% on BookMIA for pre-training detection and by 7.6\% on AIME for post-training detection.

</details>


### [82] [Pushing the Limits of Inverse Lithography with Generative Reinforcement Learning](https://arxiv.org/abs/2602.19027)
*Haoyu Yang,Haoxing Ren*

Main category: cs.LG

TL;DR: 本研究提出了一种结合生成式AI和逆向光刻技术（ILT）的新方法，通过训练一个生成器来学习给定设计条件下的掩模分布，并提出多个候选方案。该方法首先使用WGAN加上重建损失预训练生成器，然后通过带有ILT引导模仿损失的组相对策略优化（GRPO）进行微调。实验结果表明，此方法在减少EPE违规、提高处理速度及最终掩模质量方面优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 逆向光刻技术对于现代半导体制造至关重要，但其高度非凸的目标函数经常导致优化陷入局部最小值。尽管已经有人尝试用生成式AI为ILT提供初始估计，但大多数方法只能模仿次优数据集，无法有效帮助优化过程避开非凸陷阱。

Method: 研究人员将掩模合成问题重新定义为条件采样任务，其中生成器学习了基于特定设计条件下的掩模分布并能提出多个候选方案。首先，采用WGAN加上重建损失对生成器进行预训练；随后，利用组相对策略优化（GRPO）配合一种基于ILT指导的模仿损失对该模型进行微调。在推理阶段，从生成器中抽取一小批掩模样本，经过快速批量ILT细化后，根据光刻度量（如EPE、工艺窗口等）选择最佳候选者。

Result: 在LithoBench数据集上，所提出的混合框架相比强大的数值ILT基线减少了3nm容差下的EPE违规情况，并且大约提高了两倍的吞吐量，同时改善了最终掩模的质量。此外，在ICCAD13竞赛案例中实现了超过20%的EPE改进以及相对于当前最先进数值ILT求解器3倍的速度提升。

Conclusion: 通过学习提出有利于ILT处理的初始化方案，该方法能够缓解由非凸性带来的挑战，并超越传统求解器或单纯依靠生成式AI所能达到的效果。

Abstract: Inverse lithography (ILT) is critical for modern semiconductor manufacturing but suffers from highly non-convex objectives that often trap optimization in poor local minima. Generative AI has been explored to warm-start ILT, yet most approaches train deterministic image-to-image translators to mimic sub-optimal datasets, providing limited guidance for escaping non-convex traps during refinement. We reformulate mask synthesis as conditional sampling: a generator learns a distribution over masks conditioned on the design and proposes multiple candidates. The generator is first pretrained with WGAN plus a reconstruction loss, then fine-tuned using Group Relative Policy Optimization (GRPO) with an ILT-guided imitation loss. At inference, we sample a small batch of masks, run fast batched ILT refinement, evaluate lithography metrics (e.g., EPE, process window), and select the best candidate. On \texttt{LithoBench} dataset, the proposed hybrid framework reduces EPE violations under a 3\,nm tolerance and roughly doubles throughput versus a strong numerical ILT baseline, while improving final mask quality. We also present over 20\% EPE improvement on \texttt{ICCAD13} contest cases with 3$\times$ speedup over the SOTA numerical ILT solver. By learning to propose ILT-friendly initializations, our approach mitigates non-convexity and advances beyond what traditional solvers or GenAI can achieve.

</details>


### [83] [A Markovian View of Iterative-Feedback Loops in Image Generative Models: Neural Resonance and Model Collapse](https://arxiv.org/abs/2602.19033)
*Vibhas Kumar Vats,David J. Crandall,Samuel Goree*

Main category: cs.LG

TL;DR: 研究发现，AI训练数据集中的AI生成样本会导致模型间反馈，这种反馈可能导致模型崩溃。通过将迭代反馈建模为马尔可夫链，研究人员识别出导致神经共振现象的两个条件，并通过不同实验展示了这种现象，提出了八种崩溃行为模式。神经共振为生成模型中的长期退化行为提供了一个统一的解释，并提供了诊断和最终缓解崩溃的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的数据不可避免地被包含进训练数据集中，由此产生的模型间反馈可能会导致模型性能下降甚至崩溃。然而，这种退化背后的机制尚未得到充分理解。

Method: 通过将迭代反馈过程建模为马尔可夫链，并在MNIST、ImageNet数据集上使用扩散模型，在CycleGAN以及音频反馈实验中观察局部与全局流形几何的变化情况，从而定义了引发神经共振所需的条件，并提出了一套关于崩溃行为的分类体系。

Result: 研究表明，只要反馈过程具有遍历性且潜在表示经历方向收缩，则广泛类别的反馈过程会在潜在空间中收敛到低维不变结构，即所谓的神经共振。此外，还基于研究结果提出了八种不同的崩溃行为模式。

Conclusion: 神经共振现象为理解生成模型中由反馈引起的长期退化行为提供了一个新的视角，并且为识别、描述乃至减轻这种崩溃提供了实用的诊断工具。

Abstract: AI training datasets will inevitably contain AI-generated examples, leading to ``feedback'' in which the output of one model impacts the training of another. It is known that such iterative feedback can lead to model collapse, yet the mechanisms underlying this degeneration remain poorly understood. Here we show that a broad class of feedback processes converges to a low-dimensional invariant structure in latent space, a phenomenon we call neural resonance. By modeling iterative feedback as a Markov Chain, we show that two conditions are needed for this resonance to occur: ergodicity of the feedback process and directional contraction of the latent representation. By studying diffusion models on MNIST and ImageNet, as well as CycleGAN and an audio feedback experiment, we map how local and global manifold geometry evolve, and we introduce an eight-pattern taxonomy of collapse behaviors. Neural resonance provides a unified explanation for long-term degenerate behavior in generative models and provides practical diagnostics for identifying, characterizing, and eventually mitigating collapse.

</details>


### [84] [Back to Blackwell: Closing the Loop on Intransitivity in Multi-Objective Preference Fine-Tuning](https://arxiv.org/abs/2602.19041)
*Jiahao Zhang,Lujing Zhang,Keltin Grimes,Zhuohao Yu,Gokul Swamy,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 提出了一种新的解决方案——最大熵布莱克威尔赢家（MaxEntBW），以及一种有效的偏好微调算法PROSPER，专门用于处理多目标不传递性偏好问题。该方法在大型语言模型的微调上表现优于基线方法，并且无需将多个目标标量化。


<details>
  <summary>Details</summary>
Motivation: 解决偏好微调过程中遇到的不传递性偏好问题，这些问题可能源自单一目标上的不一致排序或多个目标的标量化。这类问题会导致没有明确的最佳策略，破坏了标准偏好微调流程的核心假设。

Method: 提出了一个基于博弈论的新解概念——最大熵布莱克威尔赢家(MaxEntBW)，并开发了名为PROSPER的有效偏好微调算法来支持大规模计算MaxEntBWs。PROSPER直接处理多个目标，无需进行标量化处理。

Result: 当应用于根据多目标LLM作为评判者反馈来微调大型语言模型时，PROSPER在指令遵循和通用聊天基准测试中均超过了所有考虑的基线方法。

Conclusion: 研究展示了一种创新的方法来应对偏好微调中的不传递性挑战，通过引入MaxEntBW概念及其实现算法PROSPER，为处理多目标优化问题提供了有效途径。

Abstract: A recurring challenge in preference fine-tuning (PFT) is handling $\textit{intransitive}$ (i.e., cyclic) preferences. Intransitive preferences often stem from either $\textit{(i)}$ inconsistent rankings along a single objective or $\textit{(ii)}$ scalarizing multiple objectives into a single metric. Regardless of their source, the downstream implication of intransitive preferences is the same: there is no well-defined optimal policy, breaking a core assumption of the standard PFT pipeline. In response, we propose a novel, game-theoretic solution concept -- the $\textit{Maximum Entropy Blackwell Winner}$ ($\textit{MaxEntBW}$) -- that is well-defined under multi-objective intransitive preferences. To enable computing MaxEntBWs at scale, we derive $\texttt{PROSPER}$: a provably efficient PFT algorithm. Unlike prior self-play techniques, $\texttt{PROSPER}$ directly handles multiple objectives without requiring scalarization. We then apply $\texttt{PROSPER}$ to the problem of fine-tuning large language models (LLMs) from multi-objective LLM-as-a-Judge feedback (e.g., rubric-based judges), a setting where both sources of intransitivity arise. We find that $\texttt{PROSPER}$ outperforms all baselines considered across both instruction following and general chat benchmarks, releasing trained model checkpoints at the 7B and 3B parameter scales.

</details>


### [85] [IDLM: Inverse-distilled Diffusion Language Models](https://arxiv.org/abs/2602.19066)
*David Li,Nikita Gushchin,Dmitry Abulkhanov,Eric Moulines,Ivan Oseledets,Maxim Panov,Alexander Korotin*

Main category: cs.LG

TL;DR: 该论文通过将逆向蒸馏技术扩展到离散环境中，解决了扩散语言模型(DLMs)在文本生成中推理速度慢的问题。尽管面临理论和实际挑战，但通过证明逆向公式存在唯一解并引入梯度稳定松弛法，IDLM方法成功减少了4到64倍的推理步骤，同时保持了教师模型的熵和生成困惑度。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）在文本生成方面取得了显著成果，但由于其多步采样导致推理速度缓慢，限制了实际应用。为了解决这一问题，研究者们试图将原本用于加速连续扩散模型的逆向蒸馏技术应用于离散环境下的DLMs。

Method: 研究首先对逆向蒸馏目标缺乏唯一性保证的问题进行了理论分析，并证明所提出的逆向公式确实存在唯一的解决方案。此外，针对离散空间中的反向传播难题，提出了梯度稳定的松弛方法来促进有效训练。

Result: 实验结果表明，提出的Inverse-distilled Diffusion Language Models (IDLM)方法能够将不同DLMs的推理步骤减少4至64倍，同时维持与原教师模型相近的熵值和生成困惑度。

Conclusion: 本研究表明，通过适当的理论分析和改进措施，可以有效地利用逆向蒸馏技术来加速离散环境下的扩散语言模型，从而提高其实用价值。

Abstract: Diffusion Language Models (DLMs) have recently achieved strong results in text generation. However, their multi-step sampling leads to slow inference, limiting practical use. To address this, we extend Inverse Distillation, a technique originally developed to accelerate continuous diffusion models, to the discrete setting. Nonetheless, this extension introduces both theoretical and practical challenges. From a theoretical perspective, the inverse distillation objective lacks uniqueness guarantees, which may lead to suboptimal solutions. From a practical standpoint, backpropagation in the discrete space is non-trivial and often unstable. To overcome these challenges, we first provide a theoretical result demonstrating that our inverse formulation admits a unique solution, thereby ensuring valid optimization. We then introduce gradient-stable relaxations to support effective training. As a result, experiments on multiple DLMs show that our method, Inverse-distilled Diffusion Language Models (IDLM), reduces the number of inference steps by 4x-64x, while preserving the teacher model's entropy and generative perplexity.

</details>


### [86] [TimeRadar: A Domain-Rotatable Foundation Model for Time Series Anomaly Detection](https://arxiv.org/abs/2602.19068)
*Hui He,Hezhe Qiao,Yutong Chen,Kun Yi,Guansong Pang*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列基础模型TimeRadar，它在分数时间-频率域内构建，旨在通过自适应区分正常和异常信号来改进时间序列异常检测（TSAD）任务。


<details>
  <summary>Details</summary>
Motivation: 当前的时间序列基础模型主要关注于学习预定义时间或频率域内的普遍和规则模式以支持监督下游任务，如预测。但它们对于本质上无监督的下游任务，比如时间序列异常检测（TSAD），往往表现不佳。这是因为异常模式在相同的时间/频率域中可能与常规模式非常相似。

Method: 为了解决这个问题，作者们引入了名为TimeRadar的新颖TSFM，该模型建立在一个分数时间-频率域上，以支持跨不同未见数据集的一般性TSAD。其核心见解是将时间序列旋转到一个依赖于数据的分数时间-频率表示可以自适应地根据不同数据集区分正常和异常信号。为此，在TimeRadar中提出了一个新的组件——分数调制时频重建(FTFRecon)，利用可学习的分数阶数将时间序列旋转至连续时间与频率域之间最显著的角度，从而实现精确的数据重建。此外，为了使TimeRadar能够建模全局数据重建未能捕捉到的局部异常，还引入了上下文偏差学习(CDL)组件，用于建模输入相对于其上下文时间序列数据在可旋转域中的局部偏差。

Result: 通过这种方法，TimeRadar能够在最优的时间-频率域内为每个数据输入提供自适应的数据重建，有效地区分出包括未见数据集在内的各种数据集中无限定边界的异常模式与常规模式。

Conclusion: 总之，TimeRadar通过创新地使用分数时间-频率域及自适应方法增强了对异常信号的识别能力，为解决时间序列异常检测问题提供了新途径。

Abstract: Current time series foundation models (TSFMs) primarily focus on learning prevalent and regular patterns within a predefined time or frequency domain to enable supervised downstream tasks (e.g., forecasting). Consequently, they are often ineffective for inherently unsupervised downstream tasks-such as time series anomaly detection (TSAD), which aims to identify rare, irregular patterns. This limitation arises because such abnormal patterns can closely resemble the regular patterns when presented in the same time/frequency domain. To address this issue, we introduce TimeRadar, an innovative TSFM built in a fractional time-frequency domain to support generalist TSAD across diverse unseen datasets. Our key insight is that rotating a time series into a data-dependent fractional time-frequency representation can adaptively differentiate the normal and abnormal signals across different datasets. To this end, a novel component, namely Fractionally modulated Time-Frequency Reconstruction (FTFRecon), is proposed in TimeRadar to leverage a learnable fractional order to rotate the time series to the most pronounced angle between a continuous time and frequency domain for accurate data reconstruction. This provides adaptive data reconstruction in an optimal time-frequency domain for each data input, enabling effective differentiation of the unbounded abnormal patterns from the regular ones across datasets, including unseen datasets. To allow TimeRadar to model local abnormality that is not captured by the global data reconstruction, we further introduce a Contextual Deviation Learning (CDL) component to model the local deviation of the input relative to its contextual time series data in the rotatable domain.

</details>


### [87] [Learning from Complexity: Exploring Dynamic Sample Pruning of Spatio-Temporal Training](https://arxiv.org/abs/2602.19113)
*Wei Chen,Junle Chen,Yuqian Wu,Yuxuan Liang,Xiaofang Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的训练效率技术ST-Prune，通过动态样本剪枝来智能识别最具信息量的样本，从而加速收敛并提高时空预测中的训练效率。实验表明，该方法在保持或甚至提高模型性能的同时，显著加快了训练速度，并具有可扩展性和普遍性。


<details>
  <summary>Details</summary>
Motivation: 时空预测对于交通、气候科学和城市规划等领域的智能系统至关重要，但在这些领域的大规模且常常冗余的数据集上训练深度学习模型存在显著的计算瓶颈。现有解决方案通常专注于优化模型架构或优化器，而忽视了训练数据本身的内在低效性。传统的每次迭代整个静态数据集的方法浪费了大量的资源在易于学习或重复的样本上。

Method: 提出了一种名为ST-Prune的新颖训练效率技术，通过动态样本剪枝来根据模型实时学习状态智能地识别最具信息量的样本，从而加速收敛并提高训练效率。

Result: 在真实世界的时空数据集上进行了广泛的实验，结果表明ST-Prune显著加快了训练速度，同时保持或甚至提高了模型性能，并且展示了良好的可扩展性和普遍适用性。

Conclusion: ST-Prune提供了一种有效解决时空预测中训练效率问题的方法，通过动态调整训练过程中的样本选择实现了更快的收敛速度而不牺牲准确性。此外，其展示出的可扩展性和普遍性意味着这项技术可以广泛应用于各种需要处理大规模数据集的场景。

Abstract: Spatio-temporal forecasting is fundamental to intelligent systems in transportation, climate science, and urban planning. However, training deep learning models on the massive, often redundant, datasets from these domains presents a significant computational bottleneck. Existing solutions typically focus on optimizing model architectures or optimizers, while overlooking the inherent inefficiency of the training data itself. This conventional approach of iterating over the entire static dataset each epoch wastes considerable resources on easy-to-learn or repetitive samples. In this paper, we explore a novel training-efficiency techniques, namely learning from complexity with dynamic sample pruning, ST-Prune, for spatio-temporal forecasting. Through dynamic sample pruning, we aim to intelligently identify the most informative samples based on the model's real-time learning state, thereby accelerating convergence and improving training efficiency. Extensive experiments conducted on real-world spatio-temporal datasets show that ST-Prune significantly accelerates the training speed while maintaining or even improving the model performance, and it also has scalability and universality.

</details>


### [88] [Robust Predictive Uncertainty and Double Descent in Contaminated Bayesian Random Features](https://arxiv.org/abs/2602.19126)
*Michele Caprio,Katerina Papagiannouli,Siu Lun Chau,Sayan Mukherjee*

Main category: cs.LG

TL;DR: 本文提出了一种鲁棒的贝叶斯随机特征回归方法，通过Huber风格的污染集明确考虑了先验和似然性的错误指定。推导出上下后验预测密度的显式和可处理边界，并引入了不精确最高密度区域（IHDR）以进行稳健的预测不确定性量化。研究结果表明，即使在存在有界先验和似然性错误指定的情况下，预测不确定性仍然具有计算上的可处理性，并且保留了经典的双下降相结构。


<details>
  <summary>Details</summary>
Motivation: 为了应对先验和似然性可能存在的错误指定问题，文章旨在开发一种更鲁棒的贝叶斯随机特征回归方法。通过引入对先验和似然性不确定性的直接建模，该方法能够提供更可靠、更具抗扰动能力的预测结果。

Method: 基于经典等价性——即带有高斯先验和似然性的贝叶斯推理与使用岭正则化的随机特征训练之间的关系，作者将单一的先验和似然性替换为ε-和η-污染可信集，并利用悲观广义贝叶斯更新执行推理。

Result: 得到的结果包括：显式且易于处理的下上后验预测密度界限；提出了一个有效的外近似方法来估计不精确最高密度区域；获得了预测方差的界限，并证明这些界限保持了已知的RF模型成比例增长渐近特性。

Conclusion: 本研究表明，即使在先验和似然性存在一定程度错配的情况下，所提出的鲁棒贝叶斯方法仍能保持预测不确定性的计算可行性，并且这种不确定性继承了经典的双下降阶段结构。此外，它还提供了在先验和似然性错配情况下的最坏情况保证。

Abstract: We propose a robust Bayesian formulation of random feature (RF) regression that accounts explicitly for prior and likelihood misspecification via Huber-style contamination sets. Starting from the classical equivalence between ridge-regularized RF training and Bayesian inference with Gaussian priors and likelihoods, we replace the single prior and likelihood with $ε$- and $η$-contaminated credal sets, respectively, and perform inference using pessimistic generalized Bayesian updating. We derive explicit and tractable bounds for the resulting lower and upper posterior predictive densities. These bounds show that, when contamination is moderate, prior and likelihood ambiguity effectively acts as a direct contamination of the posterior predictive distribution, yielding uncertainty envelopes around the classical Gaussian predictive. We introduce an Imprecise Highest Density Region (IHDR) for robust predictive uncertainty quantification and show that it admits an efficient outer approximation via an adjusted Gaussian credible interval. We further obtain predictive variance bounds (under a mild truncation approximation for the upper bound) and prove that they preserve the leading-order proportional-growth asymptotics known for RF models. Together, these results establish a robustness theory for Bayesian random features: predictive uncertainty remains computationally tractable, inherits the classical double-descent phase structure, and is improved by explicit worst-case guarantees under bounded prior and likelihood misspecification.

</details>


### [89] [Detecting labeling bias using influence functions](https://arxiv.org/abs/2602.19130)
*Frida Jørgensen,Nina Weng,Siavash Bigdeli*

Main category: cs.LG

TL;DR: 本研究探讨了影响函数在检测标签偏差中的应用，通过在MNIST和CheXpert数据集上进行实验，展示了影响函数能够有效识别错误标记样本的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于资源限制或无意识偏见，在数据收集过程中会出现标签偏差问题，导致子群体间标签错误率不等或子群体代表性失真。大多数公平性约束假设训练标签反映了真实分布，这使得它们在存在标签偏差时无效；因此留下了一个挑战性的问题：我们如何检测这种标签偏差？

Method: 研究者们开发了一种样本价值评估流程，并首先在MNIST数据集上进行了测试，然后扩展到了更复杂的CheXpert医学影像数据集。为了检验标签噪声，他们通过翻转数据集中一个类别的20%标签来引入控制错误。使用对角Hessian近似方法，研究人员尝试利用影响函数来估计每个训练样本对模型预测的影响程度。

Result: 实验结果显示，在MNIST数据集中成功地检测到了接近90%的误标样本。而在CheXpert数据集上，误标样本始终表现出更高的影响分数。这些结果突出了影响函数在识别标签错误方面的潜力。

Conclusion: 研究表明，影响函数可以作为检测标签偏差的有效工具，为解决因标签错误而导致的数据偏差问题提供了新的思路。

Abstract: Labeling bias arises during data collection due to resource limitations or unconscious bias, leading to unequal label error rates across subgroups or misrepresentation of subgroup prevalence. Most fairness constraints assume training labels reflect the true distribution, rendering them ineffective when labeling bias is present; leaving a challenging question, that \textit{how can we detect such labeling bias?} In this work, we investigate whether influence functions can be used to detect labeling bias. Influence functions estimate how much each training sample affects a model's predictions by leveraging the gradient and Hessian of the loss function -- when labeling errors occur, influence functions can identify wrongly labeled samples in the training set, revealing the underlying failure mode. We develop a sample valuation pipeline and test it first on the MNIST dataset, then scaled to the more complex CheXpert medical imaging dataset. To examine label noise, we introduced controlled errors by flipping 20\% of the labels for one class in the dataset. Using a diagonal Hessian approximation, we demonstrated promising results, successfully detecting nearly 90\% of mislabeled samples in MNIST. On CheXpert, mislabeled samples consistently exhibit higher influence scores. These results highlight the potential of influence functions for identifying label errors.

</details>


### [90] [Test-Time Learning of Causal Structure from Interventional Data](https://arxiv.org/abs/2602.19131)
*Wei Chen,Rui Ding,Bojun Huang,Yang Zhang,Qiang Fu,Yuxuan Liang,Han Shi,Dongmei Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为TICL的新方法，通过在测试时生成实例特定的训练数据，并结合联合因果推断，以改善监督因果学习中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 监督因果学习在因果发现中显示出潜力，但在干预目标未知的情况下，在不同干预设置中的泛化能力较差。

Method: TICL（测试时间干预因果学习），结合了测试时间训练与联合因果推理。采用自增强策略在测试时生成实例特定的训练数据，并开发了一个受PC启发的两阶段监督学习方案。

Result: 在bnlearn基准测试上的广泛实验表明，TICL在因果发现和干预目标检测的多个方面表现优越。

Conclusion: TICL通过避免分布偏移并在理论上保证可识别性的同时有效利用自增强训练数据，从而提高了跨不同干预环境下的泛化性能。

Abstract: Supervised causal learning has shown promise in causal discovery, yet it often struggles with generalization across diverse interventional settings, particularly when intervention targets are unknown. To address this, we propose TICL (Test-time Interventional Causal Learning), a novel method that synergizes Test-Time Training with Joint Causal Inference. Specifically, we design a self-augmentation strategy to generate instance-specific training data at test time, effectively avoiding distribution shifts. Furthermore, by integrating joint causal inference, we developed a PC-inspired two-phase supervised learning scheme, which effectively leverages self-augmented training data while ensuring theoretical identifiability. Extensive experiments on bnlearn benchmarks demonstrate TICL's superiority in multiple aspects of causal discovery and intervention target detection.

</details>


### [91] [Celo2: Towards Learned Optimization Free Lunch](https://arxiv.org/abs/2602.19142)
*Abhinav Moudgil,Boris Knyazev,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出了一种简单的标准化优化器架构和增强元训练方法，使得在极小的计算资源下（4.5 GPU小时）就能元训练出一个性能良好的通用学习更新规则。该更新规则能够稳定地扩展到十亿规模的预训练任务，并且在不同分布的任务上表现出色，兼容现代优化框架。


<details>
  <summary>Details</summary>
Motivation: 现有学习型优化器由于在超出其训练分布时往往无法泛化，且元训练成本高，导致实际应用受限。例如，先前的工作VeLO尽管使用了大量计算资源（约4000 TPU月），但未能超越6亿参数的任务范围。

Method: 通过设计一种简单的标准化优化器架构以及加强元训练过程，实现了在一个非常小的计算量（具体为4.5 GPU小时）下完成高效、泛用的学习更新规则的元训练。

Result: 所提出的学习更新规则不仅能够稳定应用于高达十亿参数规模的任务中（如GPT-3 XL 1.3B），而且对于多种不同于训练时分布的任务也表现出了强大的适应性。此外，它还与包括正交化、对输入输出及隐藏权重采用不同的更新规则等在内的现代优化技术相兼容。

Conclusion: 本研究表明，在有限的计算资源条件下，通过特定的设计策略可以开发出既高效又具有良好泛化能力的学习型优化算法，为未来进一步探索更丰富的元训练方法和数据管理策略以提升性能铺平了道路。

Abstract: Learned optimizers are powerful alternatives to hand-designed update rules like Adam, yet they have seen limited practical adoption since they often fail to meta-generalize beyond their training distribution and incur high meta-training cost. For instance, prior work, VeLO, scaled meta-training to 4,000 TPU months ($\sim$10$\times$ GPT-3 compute) to meta-train a general-purpose optimizer but it failed to generalize beyond 600M parameters tasks. In this work, we present a surprising finding: by crafting a simple normalized optimizer architecture and augmenting meta-training, it becomes feasible to meta-train a performant general-purpose learned update rule on a tiny fraction of VeLO compute, 4.5 GPU hours to be precise. Our learned update rule scales stably to a billion-scale pretraining task (GPT-3 XL 1.3B) which is six orders of magnitude larger than its meta-training distribution. Furthermore, it shows strong performance across diverse out-of-distribution tasks and is compatible with modern optimization harness that includes orthogonalization, distinct update rules for input-output and hidden weights, and decoupled weight decay. In all, this work paves the way for practically applicable learnable optimization algorithms, unlocking exploration of richer meta-training and data curation recipes to further improve performance.

</details>


### [92] [Incremental Learning of Sparse Attention Patterns in Transformers](https://arxiv.org/abs/2602.19143)
*Oğuz Kaan Yüksel,Rodrigo Alvarez Lucendo,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本研究通过引入高阶马尔可夫链任务，探讨了transformers如何学习整合来自多个过去位置的信息。研究表明，transformers通过稀疏的注意力模式逐步学会此任务，并且学习动力学从竞争性转向合作性。利用简化的微分方程建模这些动态并证明了阶段收敛结果。此外，研究还指出早期停止作为隐式正则化器促使模型倾向于更简单的假设类别。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索transformers如何根据统计显著性的不同来整合多源历史信息，特别是理解其学习过程中的阶段性以及复杂行为背后的机制。

Method: 通过设计一个高阶马尔可夫链任务，使用稀疏注意模式分析transformers的学习路径；运用简化后的微分方程对学习动力学进行建模，并验证每个阶段的学习成果。

Result: 发现transformers在学习过程中表现出先竞争后合作的特点，能够逐步掌握更复杂的任务；同时，早期停止有助于防止过拟合，使模型偏向于较简单的假设类型。

Conclusion: 该研究为理解transformers中的阶段性学习和复杂行为提供了理论基础，并为自然语言处理及算法推理领域的泛化能力带来了新的见解。

Abstract: This paper introduces a high-order Markov chain task to investigate how transformers learn to integrate information from multiple past positions with varying statistical significance. We demonstrate that transformers learn this task incrementally: each stage is defined by the acquisition of specific information through sparse attention patterns. Notably, we identify a shift in learning dynamics from competitive, where heads converge on the most statistically dominant pattern, to cooperative, where heads specialize in distinct patterns. We model these dynamics using simplified differential equations that characterize the trajectory and prove stage-wise convergence results. Our analysis reveals that transformers ascend a complexity ladder by passing through simpler, misspecified hypothesis classes before reaching the full model class. We further show that early stopping acts as an implicit regularizer, biasing the model toward these simpler classes. These results provide a theoretical foundation for the emergence of staged learning and complex behaviors in transformers, offering insights into generalization for natural language processing and algorithmic reasoning.

</details>


### [93] [Virtual Parameter Sharpening: Dynamic Low-Rank Perturbations for Inference-Time Reasoning Enhancement](https://arxiv.org/abs/2602.19169)
*Saba Kublashvili*

Main category: cs.LG

TL;DR: 本文提出了一种名为虚拟参数锐化（VPS）的技术，该技术在推理时通过基于激活条件的动态低秩扰动来增强冻结的变压器线性层。与静态学习低秩适配器的方法不同，VPS利用批处理激活统计和可选梯度信号即时构建其扰动因素，从而实现无需持久参数更新的测试时间适应。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种新的方法，即虚拟参数锐化（VPS），它能够在不进行持续参数更新的情况下，在测试时根据输入数据动态调整模型权重。这种方法试图克服现有参数高效微调方法如LoRA只能学习静态低秩适配器的局限性。

Method: VPS通过公式Delta W = gamma * W^T V U^T W引入了动态、基于激活条件的低秩扰动到冻结的变压器线性层中。选择矩阵U和V是通过稀疏激活引导的选择或Sylvester耦合回归构建的。此外，还提供了一个自适应策略系统，该系统可以根据激活能量和令牌级熵调节扰动幅度，并结合多目标验证和迭代优化以适应具有真实监督的任务。

Result: 论文提供了关于扰动谱特性的理论分析以及数学基础的详细探讨，并讨论了基于激活条件计算如何可能提高大型语言模型的推理能力。

Conclusion: VPS作为一种新颖的推理时技术，能够有效增强冻结变换器线性层的表现力，同时保持参数效率。通过动态地适应输入数据特性，VPS展示了在没有永久性参数更改的情况下改善模型性能的可能性。

Abstract: I introduce Virtual Parameter Sharpening (VPS), an inference-time technique that augments frozen transformer linear layers with dynamic, activation-conditioned low-rank perturbations. Unlike parameter-efficient fine-tuning methods such as LoRA, which learn static low-rank adapters, VPS constructs its perturbation factors on the fly from batch activation statistics and optional gradient signals, enabling test-time adaptation without persistent parameter updates. The perturbation takes the form Delta W = gamma * W^T V U^T W, where selector matrices U and V are constructed via sparse activation-guided selection or Sylvester-coupled regression. We provide a theoretical analysis of the perturbation's spectral properties and describe an adaptive policy system that modulates perturbation magnitude based on activation energy and token-level entropy. This system incorporates multi-objective verification with iterative refinement for tasks with ground-truth supervision. We present the complete algorithmic framework, analyze its mathematical foundations, and discuss the mechanisms by which activation-conditioned computation may enhance reasoning capabilities in large language models. Implementation and experimental code are available at https://github.com/Saba-Kublashvili/vps-virtual-parameter-synthesis .

</details>


### [94] [HybridFL: A Federated Learning Approach for Financial Crime Detection](https://arxiv.org/abs/2602.19207)
*Afsana Khan,Marijn ten Thij,Guangzhi Tang,Anna Wilbik*

Main category: cs.LG

TL;DR: 本文提出了一种混合联邦学习（HybridFL）方法，旨在解决数据在用户间水平分割以及特征集上垂直分割的问题。通过结合横向聚合和纵向特征融合技术，HybridFL能够在严格保持数据本地化的同时实现联合学习。实验表明，HybridFL在金融犯罪检测中表现优于仅基于交易的本地模型，并且达到了与集中式基准相当的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习通常只处理水平或垂直的数据分区问题，但在很多实际场景中数据分布更为复杂。为了应对这种同时存在横跨不同用户（水平）和补充特征集合（垂直）的数据分割情况，提出了混合联邦学习方案。

Method: HybridFL通过整合横向聚合与纵向特征融合的技术手段，设计了一个能够适应复杂数据分布的学习框架。该框架允许在不直接共享原始信息的前提下，多个参与者共同训练模型。

Result: 使用AMLSim和SWIFT数据集进行的实验显示，HybridFL不仅明显优于单独利用交易数据的本地模型，在性能上也接近于中心化的学习方法。

Conclusion: HybridFL为解决现实世界中复杂的混合数据分布提供了一种有效的解决方案，特别是在需要高度隐私保护的应用场景下，如金融犯罪检测领域。

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm that enables multiple parties to collaboratively train models on privately owned data without sharing raw information. While standard FL typically addresses either horizontal or vertical data partitions, many real-world scenarios exhibit a complex hybrid distribution. This paper proposes Hybrid Federated Learning (HybridFL) to address data split both horizontally across disjoint users and vertically across complementary feature sets. We evaluate HybridFL in a financial crime detection context, where a transaction party holds transaction-level attributes and multiple banks maintain private account-level features. By integrating horizontal aggregation and vertical feature fusion, the proposed architecture enables joint learning while strictly preserving data locality. Experiments on AMLSim and SWIFT datasets demonstrate that HybridFL significantly outperforms the transaction-only local model and achieves performance comparable to a centralized benchmark.

</details>


### [95] [Evaluating SAP RPT-1 for Enterprise Business Process Prediction: In-Context Learning vs. Traditional Machine Learning on Structured SAP Data](https://arxiv.org/abs/2602.19237)
*Amit Lal*

Main category: cs.LG

TL;DR: 本文从实践者的角度首次独立评估了SAP的检索预训练转换器（RPT-1），一个仅64.6MB大小但基于大量结构化数据预训练的模型，并将其与几种调优后的梯度提升决策树在SAP商业场景下的表现进行了对比。结果显示，在无任何特定任务训练的情况下，RPT-1能够达到接近于这些模型的表现，特别是在数据量较少时甚至优于XGBoost。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过首次独立地从实践者视角评估SAP的检索预训练转换器（RPT-1）来探讨表格基础模型是否可以在没有特定任务训练的情况下使机器学习对企业数据变得易于访问。

Method: 研究者将RPT-1与经过调整的梯度增强决策树（包括XGBoost、LightGBM和CatBoost）进行比较，测试场景覆盖了SAP业务中的需求预测、预测性数据完整性和财务风险分类三个领域。采用五折交叉验证方法对规模介于2,500至3,200行的数据集进行评估。

Result: 结果表明，在没有任何训练样本的情况下，RPT-1能达到调优后GBDT准确性的91-96%。对于分类任务而言，AUC-ROC上的差距相对较小，为3.6-4.1个百分点；而对于回归任务，则显示出更大的差距，R-squared值相差8.9-11.1个百分点。值得注意的是，在大约75-100行上下文数据点处，RPT-1实际上超过了XGBoost的表现。

Conclusion: 基于上述发现，作者提出了一种实用的混合工作流程：首先使用RPT-1快速筛选，然后根据需要选择性地训练GBDT以提高预测准确性。所有实验均可通过公开可用的Hugging Face Spaces重现。

Abstract: Tabular foundation models aim to make machine learning accessible for enterprise data without task-specific training. This paper presents the first independent evaluation of SAP's Retrieval Pretrained Transformer (RPT-1) from a practitioner perspective. RPT-1 is a compact 64.6 MB model pretrained on 1.34 TB of structured data across 3.1 million tables. We benchmark it against tuned gradient-boosted decision trees (XGBoost, LightGBM, CatBoost) on three SAP business scenarios: demand forecasting across SD/MM/PP modules, predictive data integrity in BC/MM/QM, and financial risk classification in FI/CO/AR. Across five-fold cross-validation on datasets ranging from 2,500 to 3,200 rows, RPT-1 reaches 91-96% of tuned GBDT accuracy without any training examples. The classification gap is modest at 3.6-4.1 percentage points on AUC-ROC, though regression tasks show wider gaps of 8.9-11.1 percentage points on R-squared. An interesting finding is a crossover at roughly 75-100 context rows where RPT-1 actually outperforms XGBoost under limited data. Based on these results, we propose a practical hybrid workflow: use RPT-1 for rapid screening, then train GBDT selectively where prediction accuracy justifies the effort. All experiments are reproducible through publicly available Hugging Face Spaces.

</details>


### [96] [Alternating Bi-Objective Optimization for Explainable Neuro-Fuzzy Systems](https://arxiv.org/abs/2602.19253)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

TL;DR: 本文提出了一种名为X-ANFIS的双目标梯度优化方案，用于可解释的自适应神经模糊推理系统。该方法通过交替梯度传递来解耦性能目标和可解释性目标，并在九个UCI回归数据集上进行了约5000次实验验证，结果表明X-ANFIS能够在保持竞争力的预测准确性的同时达到目标区分度。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么通过计算成本高昂的进化多目标优化（MOO）来平衡精度与可解释性之间的权衡，要么采用无法恢复非凸Pareto区域的基于梯度的标量化方法。因此，需要一种新的方法来解决这一问题，即同时提高模糊系统的准确性和可解释性。

Method: 研究者们提出了X-ANFIS，这是一种交替双目标梯度优化方案，适用于可解释性的自适应神经模糊推理系统。该方法使用Cauchy隶属函数以实现语义控制初始化下的稳定训练，并引入了一个可微分的可解释性目标，它通过交替梯度传递从性能目标中分离出来。

Result: 经过对九个UCI回归数据集的大约5,000次实验验证，X-ANFIS能够一致地达到目标区分度，同时保持了具有竞争力的预测准确性，并且找到了超出MOO Pareto前沿凸包的解决方案。

Conclusion: X-ANFIS提供了一种有效的方法来增强自适应神经模糊推理系统的可解释性，而不会牺牲太多预测准确性。此方法通过引入可微分的可解释性目标并采用交替梯度下降策略，成功地解决了精度与可解释性之间的权衡问题。

Abstract: Fuzzy systems show strong potential in explainable AI due to their rule-based architecture and linguistic variables. Existing approaches navigate the accuracy-explainability trade-off either through evolutionary multi-objective optimization (MOO), which is computationally expensive, or gradient-based scalarization, which cannot recover non-convex Pareto regions. We propose X-ANFIS, an alternating bi-objective gradient-based optimization scheme for explainable adaptive neuro-fuzzy inference systems. Cauchy membership functions are used for stable training under semantically controlled initializations, and a differentiable explainability objective is introduced and decoupled from the performance objective through alternating gradient passes. Validated in approximately 5,000 experiments on nine UCI regression datasets, X-ANFIS consistently achieves target distinguishability while maintaining competitive predictive accuracy, recovering solutions beyond the convex hull of the MOO Pareto front.

</details>


### [97] [DGPO: RL-Steered Graph Diffusion for Neural Architecture Generation](https://arxiv.org/abs/2602.19261)
*Aleksei Liuliakov,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种名为DGPO的方法，该方法通过拓扑节点排序和位置编码扩展了离散图扩散模型的强化学习微调以处理有向无环图。实验表明，该方法在NAS-Bench-101和NAS-Bech-201上取得了与基准相当的结果，并且能够学习可迁移的结构先验，在仅使用7%搜索空间预训练后，生成接近最佳的架构。


<details>
  <summary>Details</summary>
Motivation: 现有的图扩散方法设计用于无向结构，忽略了神经架构中边的方向所编码的功能语义信息。因此，需要一种新的方法来解决这个问题，特别是在处理神经架构搜索（NAS）等组合结构生成任务时。

Method: 提出了Directed Graph Policy Optimization (DGPO)，这是一种通过拓扑节点排序和位置编码将离散图扩散模型的强化学习微调扩展到有向无环图（DAGs）的方法。

Result: DGPO在NAS-Bench-101和NAS-Bench-201上得到了验证，与所有三个NAS-Bench-201任务的基准最优结果相匹配。此外，研究还发现该模型能够学习可转移的结构先验，即在只对搜索空间的7%进行预训练之后，经过微调能够生成接近于全数据模型表现的架构。

Conclusion: 强化学习引导下的离散扩散一旦扩展以处理方向性问题，就能为有向组合结构提供一个可控的生成框架。

Abstract: Reinforcement learning fine-tuning has proven effective for steering generative diffusion models toward desired properties in image and molecular domains. Graph diffusion models have similarly been applied to combinatorial structure generation, including neural architecture search (NAS). However, neural architectures are directed acyclic graphs (DAGs) where edge direction encodes functional semantics such as data flow-information that existing graph diffusion methods, designed for undirected structures, discard. We propose Directed Graph Policy Optimization (DGPO), which extends reinforcement learning fine-tuning of discrete graph diffusion models to DAGs via topological node ordering and positional encoding. Validated on NAS-Bench-101 and NAS-Bench-201, DGPO matches the benchmark optimum on all three NAS-Bench-201 tasks (91.61%, 73.49%, 46.77%). The central finding is that the model learns transferable structural priors: pretrained on only 7% of the search space, it generates near-oracle architectures after fine-tuning, within 0.32 percentage points of the full-data model and extrapolating 7.3 percentage points beyond its training ceiling. Bidirectional control experiments confirm genuine reward-driven steering, with inverse optimization reaching near random-chance accuracy (9.5%). These results demonstrate that reinforcement learning-steered discrete diffusion, once extended to handle directionality, provides a controllable generative framework for directed combinatorial structures.

</details>


### [98] [Taming Preconditioner Drift: Unlocking the Potential of Second-Order Optimizers for Federated Learning on Non-IID Data](https://arxiv.org/abs/2602.19271)
*Junkang Liu,Fanhua Shang,Hongying Liu,Jin Liu,Weixin An,Yuanyuan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦二阶优化框架FedPAC，通过预处理器对齐和校正来解决在非IID数据上训练时的几何不匹配问题，从而提高稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦二阶优化方法在处理非独立同分布（non-IID）数据时表现不稳定甚至发散，主要原因是客户端的局部训练导致了不同的曲率定义几何结构（即预处理坐标系），而服务器端模型平均更新使用的是这些不兼容的度量标准，破坏了全局下降方向。

Method: FedPAC框架包括两个关键步骤：(i) 对齐：将本地预处理器聚合为一个全局参考，并通过全局预处理器热启动客户端；(ii) 校正：利用全局预处理方向引导本地预处理更新以抑制长期漂移。

Result: FedPAC能够在视觉与语言任务中持续改善稳定性与准确性，例如，在CIFAR-100数据集上使用ViTs实现了高达5.8%的绝对准确率提升。此外，研究还提供了在部分参与条件下具有线性加速效果的非凸收敛保证。

Conclusion: FedPaac提供了一个有效的解决方案来应对联邦学习场景下由于数据非独立同分布而导致的优化难题，显著提升了模型性能。

Abstract: Second-order optimizers can significantly accelerate large-scale training, yet their naive federated variants are often unstable or even diverge on non-IID data.
  We show that a key culprit is \emph{preconditioner drift}: client-side second-order training induces heterogeneous \emph{curvature-defined geometries} (i.e., preconditioner coordinate systems), and server-side model averaging updates computed under incompatible metrics, corrupting the global descent direction.
  To address this geometric mismatch, we propose \texttt{FedPAC}, a \emph{preconditioner alignment and correction} framework for reliable federated second-order optimization.
  \texttt{FedPAC} explicitly decouples parameter aggregation from geometry synchronization by:
  (i) \textbf{Alignment} (i.e.,aggregating local preconditioners into a global reference and warm-starting clients via global preconditioner); and
  (ii) \textbf{Correction} (i.e., steering local preconditioned updates using a global preconditioned direction to suppress long-term drift).
  We provide drift-coupled non-convex convergence guarantees with linear speedup under partial participation.
  Empirically, \texttt{FedPAC} consistently improves stability and accuracy across vision and language tasks, achieving up to $5.8\%$ absolute accuracy gain on CIFAR-100 with ViTs.
  Code is available at https://anonymous.4open.science/r/FedPAC-8B24.

</details>


### [99] [AdsorbFlow: energy-conditioned flow matching enables fast and realistic adsorbate placement](https://arxiv.org/abs/2602.19289)
*Jiangjie Qiu,Wentao Li,Honghao Chen,Leyi Zhao,Xiaonan Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为AdsorbFlow的确定性生成模型，用于在催化表面上识别低能量吸附几何结构。该模型通过条件流匹配学习吸附物平移和旋转刚体配置空间中的能量条件向量场。与现有方法相比，AdsorbFlow使用更少的生成步骤，并且在准确性和速度上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 在计算异质催化中，识别催化表面上的低能量吸附几何结构是一个实际瓶颈问题。不仅密度泛函理论（DFT）的成本高，而且要提出能够松弛到正确能量盆地的初始放置也很困难。尽管条件去噪扩散提高了成功率，但每样本仍需约100次迭代步骤。

Method: 研究者们引入了AdsorbFlow，这是一种确定性的生成模型，它通过条件流匹配学习吸附物平移和旋转刚体配置空间中的能量条件向量场。能量信息是通过无分类器引导条件输入的，而不是通过能量梯度引导。采样过程简化为求解一个ODE，只需最少5步即可完成。

Result: 在OC20-Dense数据集上的全DFT单点验证表明，基于EquiformerV2骨干的AdsorbFlow达到了61.4% SR@10和34.1% SR@1的成功率，超过了AdsorbDiff (SR@1 31.8%, SR@10 41.0%) 和 AdsorbML (SR@10 47.7%) 在每个评估级别上的表现。此外，AdsorbFlow使用的生成步骤减少了20倍，并且在所有生成方法中具有最低的异常率(6.8%)。对于50个分布外系统，AdsorbFlow保持了58.0% SR@10 的成功率，MLFF到DFT之间的差距仅为约4个百分点。

Conclusion: 结果表明，对于吸附物放置而言，确定性传输比随机去噪更快且更准确。

Abstract: Identifying low-energy adsorption geometries on catalytic surfaces is a practical bottleneck for computational heterogeneous catalysis: the difficulty lies not only in the cost of density functional theory (DFT) but in proposing initial placements that relax into the correct energy basins. Conditional denoising diffusion has improved success rates, yet requires $\sim$100 iterative steps per sample.
  Here we introduce AdsorbFlow, a deterministic generative model that learns an energy-conditioned vector field on the rigid-body configuration space of adsorbate translation and rotation via conditional flow matching. Energy information enters through classifier-free guidance conditioning -- not energy-gradient guidance -- and sampling reduces to integrating an ODE in as few as 5 steps.
  On OC20-Dense with full DFT single-point verification, AdsorbFlow with an EquiformerV2 backbone achieves 61.4% SR@10 and 34.1% SR@1 -- surpassing AdsorbDiff (31.8% SR@1, 41.0% SR@10) at every evaluation level and AdsorbML (47.7% SR@10) -- while using 20 times fewer generative steps and achieving the lowest anomaly rate among generative methods (6.8%). On 50 out-of-distribution systems, AdsorbFlow retains 58.0% SR@10 with a MLFF-to-DFT gap of only 4~percentage points. These results establish that deterministic transport is both faster and more accurate than stochastic denoising for adsorbate placement.

</details>


### [100] [Soft Sequence Policy Optimization: Bridging GMPO and SAPO](https://arxiv.org/abs/2602.19327)
*Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 该论文提出了一种新的目标函数，称为软序列策略优化（Soft Sequence Policy Optimization），它结合了序列级重要性权重中的token级别概率比的软门控函数，旨在促进有效的策略探索同时保持训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究集中在基于组相对策略优化（GRPO）开发新的策略优化方法上，主要方向包括向更好地与任务中使用的序列级奖励对齐的序列级重要性采样权重转变，以及寻找PPO风格剪切的替代方案以避免相关的训练信号损失和熵崩溃。本文在此基础上，进一步提出了一个新的目标函数来提高策略探索效率并维持训练过程的稳定性。

Method: 本文引入了软序列策略优化（SSPO），这是一种离线强化学习目标，它通过在序列级重要性权重内加入token级别的概率比率上的软门控函数，从而实现更有效的策略探索。

Result: 虽然摘要中没有直接提供实验结果，但可以推测这种方法可能在促进策略探索的同时，也能够解决之前方法中存在的训练不稳定问题。

Conclusion: 这项工作提出了一种新颖的方法——软序列策略优化，旨在通过改进的重要性权重机制来增强大型语言模型的策略探索能力，并且保证了训练过程中的稳定性。

Abstract: A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. Recent work, such as Soft Adaptive Policy Optimization (SAPO), reformulates the Scopic objective within the GRPO framework and achieves both sequence coherence and token adaptivity. Geometric-Mean Policy Optimization (GMPO) leverages token-wise ratio clipping within sequence importance sampling weights. Building on these ideas, this work proposes a new objective that promotes effective policy exploration while maintaining training stability. Specifically, we introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights.

</details>


### [101] [Smooth Gate Functions for Soft Advantage Policy Optimization](https://arxiv.org/abs/2602.19345)
*Egor Denisov,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 本研究在GRPO的基础上提出了使用不同门函数来提高训练稳定性和模型性能的方法，并通过实验分析了这些门函数对Qwen2.5-7B-Instruct模型在数学推理任务上的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管组相对策略优化（GRPO）显著推进了大型语言模型的训练并增强了其推理能力，但它由于使用硬裁剪而容易出现不稳定性。为了克服这一限制，软自适应策略优化（SAPO）采用平滑的基于Sigmoid的门函数代替裁剪，以实现更稳定的更新。研究者希望进一步探讨不同门函数对训练稳定性和最终模型性能的影响。

Method: 本文正式定义了可接受门函数应满足的关键属性，并识别出几个这样的函数家族用于实证评估。研究通过对Qwen2.5-7B-Instruct模型在数学推理任务上进行实验来分析这些发现。

Result: 实验结果为设计更平滑和更鲁棒的策略优化目标提供了实用指导，有助于改善大型语言模型的训练过程。

Conclusion: 选择合适的门函数可以提高大型语言模型训练时的稳定性及最终性能；通过明确良好门函数需具备的特点，并经由实验验证了几类函数的表现，该研究为后续工作指出了方向。

Abstract: Group Relative Policy Optimization (GRPO) has significantly advanced the training of large language models and enhanced their reasoning capabilities, while it remains susceptible to instability due to the use of hard clipping. Soft Adaptive Policy Optimization (SAPO) addresses this limitation by replacing clipping with a smooth sigmoid-based gate function, which leads to more stable updates. We have decided to push this theory further and investigate the impact of different gate functions on both training stability and final model performance. We formalize the key properties that admissible gates should satisfy and identify several families of such functions for empirical evaluation. This paper presents an analysis of our findings based on experiments conducted with the Qwen2.5-7B-Instruct model on mathematical reasoning tasks. These results provide practical guidance for designing smoother and more robust policy optimization objectives for large language model training.

</details>


### [102] [Active perception and disentangled representations allow continual, episodic zero and few-shot learning](https://arxiv.org/abs/2602.19355)
*David Rawlinson,Gideon Kowadlo*

Main category: cs.LG

TL;DR: 本文提出了一种互补学习系统(Complementary Learning System, CLS)，其中快速学习器完全放弃泛化以实现持续的零样本和少样本学习。这种架构表明，快速、基于上下文的推理可以与缓慢、结构化的泛化共存，为稳健的持续学习提供了一条路径。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决机器学习系统中泛化需求与快速学习之间存在的矛盾问题。传统的模型训练方法倾向于产生实体或类别边界上的纠缠表示，这在需要进行快速大幅度更新（如持续学习或少样本学习）时可能导致破坏性干扰。虽然存在一些能够实现非干扰表示的快速学习技术，但它们通常缺乏泛化能力。

Method: 作者提出了一种新的互补学习系统(Complementary Learning System, CLS)框架，在该框架下，快速学习组件完全不追求泛化性能，而是专注于实现连续性的零样本及少量样本学习任务。不同于以往多数CLS方法主要依赖情景记忆来进行重播和巩固，这里提出的快速解缠学习者作为一个并行推理系统运行。通过利用传统慢速统计学习者提供的上下文偏差，快速学习者促使慢速学习者以熟悉且广泛适用的方式来编码新刺激，从而支持零样本和少量样本学习。

Result: 实验结果证明了所提架构的有效性，即快速、基于上下文的推理确实可以与慢速、结构化的泛化共存，并且这种方法为实现更加鲁棒的连续学习提供了可能性。

Conclusion: 本研究展示了通过结合快速无泛化学习与慢速泛化学习机制，可以有效促进鲁棒的持续学习过程。这一发现为未来开发更高效灵活的人工智能系统指明了方向。

Abstract: Generalization is often regarded as an essential property of machine learning systems. However, perhaps not every component of a system needs to generalize. Training models for generalization typically produces entangled representations at the boundaries of entities or classes, which can lead to destructive interference when rapid, high-magnitude updates are required for continual or few-shot learning. Techniques for fast learning with non-interfering representations exist, but they generally fail to generalize. Here, we describe a Complementary Learning System (CLS) in which the fast learner entirely foregoes generalization in exchange for continual zero-shot and few-shot learning. Unlike most CLS approaches, which use episodic memory primarily for replay and consolidation, our fast, disentangled learner operates as a parallel reasoning system. The fast learner can overcome observation variability and uncertainty by leveraging a conventional slow, statistical learner within an active perception system: A contextual bias provided by the fast learner induces the slow learner to encode novel stimuli in familiar, generalized terms, enabling zero-shot and few-shot learning. This architecture demonstrates that fast, context-driven reasoning can coexist with slow, structured generalization, providing a pathway for robust continual learning.

</details>


### [103] [LLMs Can Learn to Reason Via Off-Policy RL](https://arxiv.org/abs/2602.19362)
*Daniel Ritter,Owen Oertell,Bradley Guo,Jonathan Chang,Kianté Brantley,Wen Sun*

Main category: cs.LG

TL;DR: 本文提出了一种新的离策略强化学习算法OAPL，该算法在竞争数学基准上优于使用重要性采样的GRPO，并且在LiveCodeBench上与公开可用的编码模型DeepCoder表现相当，同时训练期间生成次数减少了3倍。此外，OAPL允许即使在训练和推理策略之间存在超过400个梯度步骤延迟的情况下也能进行有效的后训练。


<details>
  <summary>Details</summary>
Motivation: 当前针对大型语言模型（LLMs）的强化学习方法通常采用在线策略算法如PPO或GRPO，但分布式训练架构导致的策略滞后以及训练与推理策略间的差异使得数据本质上是离策略的。为了解决这个问题，先前的工作集中在通过重要性采样或者更紧密地调整训练和推理策略来使这些离策略数据看起来更像是在线策略。本文则拥抱离策略性并提出了一个不需要这些修改的新算法。

Method: 提出了一种名为OAPL的新离策略RL算法，它不依赖于对推理引擎的显式修改或重要性采样。

Result: OAPL在竞争数学基准测试中表现出色，超过了采用重要性采样的GRPO，并且能够以DeepCoder在LiveCodeBench上的性能相匹配，同时仅需后者三分之一的训练生成次数。此外，实验还表明，通过OAPL训练的模型在Pass@k指标下的测试时间扩展性有所提高。

Conclusion: OAPL提供了一种有效处理大规模语言模型强化学习中固有离策略问题的方法，能够在训练和推理策略间存在显著延迟时仍保持高效训练。

Abstract: Reinforcement learning (RL) approaches for Large Language Models (LLMs) frequently use on-policy algorithms, such as PPO or GRPO. However, policy lag from distributed training architectures and differences between the training and inference policies break this assumption, making the data off-policy by design. To rectify this, prior work has focused on making this off-policy data appear more on-policy, either via importance sampling (IS), or by more closely aligning the training and inference policies by explicitly modifying the inference engine. In this work, we embrace off-policyness and propose a novel off-policy RL algorithm that does not require these modifications: Optimal Advantage-based Policy Optimization with Lagged Inference policy (OAPL). We show that OAPL outperforms GRPO with importance sampling on competition math benchmarks, and can match the performance of a publicly available coding model, DeepCoder, on LiveCodeBench, while using 3x fewer generations during training. We further empirically demonstrate that models trained via OAPL have improved test time scaling under the Pass@k metric. OAPL allows for efficient, effective post-training even with lags of more than 400 gradient steps between the training and inference policies, 100x more off-policy than prior approaches.

</details>


### [104] [In Defense of Cosine Similarity: Normalization Eliminates the Gauge Freedom](https://arxiv.org/abs/2602.19393)
*Taha Bouhsine*

Main category: cs.LG

TL;DR: 本文澄清了在单位球面上受约束的嵌入中使用余弦相似度的有效性，指出Steck等人的结论不应被广泛应用于反对所有情况下的余弦相似度使用。当嵌入在训练过程中或之后被正规化到单位球面时，由对角'规范'矩阵D引起的问题消失，并且余弦距离等同于欧几里得距离的一半平方。


<details>
  <summary>Details</summary>
Motivation: 作者旨在回应Steck等人关于从矩阵分解模型学到的嵌入的余弦相似度可能由于对角'规范'矩阵D而变得任意的观点。虽然承认原研究结果对于特定条件下的实践者是正确的和重要的，但作者认为不应将这一发现推广为对余弦相似度普遍使用的警告。

Method: 通过证明如果嵌入被限制在单位球面上（无论是训练期间还是之后通过适当的目标函数），则可以消除由D-矩阵引起的歧义。此外，展示了在这种情况下，余弦距离与欧几里得距离之间存在单调等价关系。

Result: 研究表明，在正规化后的嵌入上，基于余弦的距离与基于欧几里得的距离之间的邻居排名是完全相同的。这表明，只要正确地应用了规范化步骤，那么余弦相似度计算中的所谓问题实际上并不存在。

Conclusion: 结论强调了嵌入向量正规化的重要性，以确保余弦相似度测量的有效性和准确性。文章还指出，余弦相似度本身并不是问题所在；真正的问题在于缺乏适当的规范化处理。

Abstract: Steck, Ekanadham, and Kallus [arXiv:2403.05440] demonstrate that cosine similarity of learned embeddings from matrix factorization models can be rendered arbitrary by a diagonal ``gauge'' matrix $D$. Their result is correct and important for practitioners who compute cosine similarity on embeddings trained with dot-product objectives. However, we argue that their conclusion, cautioning against cosine similarity in general, conflates the pathology of an incompatible training objective with the geometric validity of cosine distance on the unit sphere. We prove that when embeddings are constrained to the unit sphere $\mathbb{S}^{d-1}$ (either during or after training with an appropriate objective), the $D$-matrix ambiguity vanishes identically, and cosine distance reduces to exactly half the squared Euclidean distance. This monotonic equivalence implies that cosine-based and Euclidean-based neighbor rankings are identical on normalized embeddings. The ``problem'' with cosine similarity is not cosine similarity, it is the failure to normalize.

</details>


### [105] [RAmmStein: Regime Adaptation in Mean-reverting Markets with Stein Thresholds -- Optimal Impulse Control in Concentrated AMMs](https://arxiv.org/abs/2602.19419)
*Pranay Anchuri*

Main category: cs.LG

TL;DR: 本文将去中心化交易所中的流动性提供问题视为一个最优控制问题，并提出了基于深度强化学习的解决方案RAmmStein。实验结果表明，该方法能够显著提高资本效率，同时降低再平衡频率。


<details>
  <summary>Details</summary>
Motivation: 现有的流动性管理策略通常采用启发式或阈值策略，未能充分考虑市场动态变化。为了更有效地在最大化费用累积和最小化再平衡摩擦成本（如gas费、交易滑点）之间取得平衡，研究者尝试通过建立数学模型来寻找最优解。

Method: 首先，将流动性管理建模为一个最优控制问题，并推导出对应的Hamilton-Jacobi-Bellman准变分不等式(HJB-QVI)。接着，提出了一种近似解法——RAmmStein，这是一种深度强化学习方法，它将Ornstein-Uhlenbeck过程的均值回归速度以及其他特征作为输入。

Result: 实验评估显示，RAmmStein相比被动和激进策略实现了更高的净回报率(0.72%)。此外，与贪婪再平衡策略相比，该方法减少了67%的再平衡次数，同时保持了88%的有效时间。

Conclusion: 研究表明，通过采取具有制度意识的懒惰策略，可以在很大程度上提高资本效率，从而保留原本可能因操作成本而被侵蚀的收益。

Abstract: Concentrated liquidity provision in decentralized exchanges presents a fundamental Impulse Control problem. Liquidity Providers (LPs) face a non-trivial trade-off between maximizing fee accrual through tight price-range concentration and minimizing the friction costs of rebalancing, including gas fees and swap slippage. Existing methods typically employ heuristic or threshold strategies that fail to account for market dynamics. This paper formulates liquidity management as an optimal control problem and derives the corresponding Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI). We present an approximate solution RAmmStein, a Deep Reinforcement Learning method that incorporates the mean-reversion speed (theta) of an Ornstein-Uhlenbeck process among other features as input to the model. We demonstrate that the agent learns to separate the state space into regions of action and inaction. We evaluate the framework using high-frequency 1Hz Coinbase trade data comprising over 6.8M trades. Experimental results show that RAmmStein achieves a superior net ROI of 0.72% compared to both passive and aggressive strategies. Notably, the agent reduces rebalancing frequency by 67% compared to a greedy rebalancing strategy while maintaining 88% active time. Our results demonstrate that regime-aware laziness can significantly improve capital efficiency by preserving the returns that would otherwise be eroded by the operational costs.

</details>


### [106] [PIS: A Physics-Informed System for Accurate State Partitioning of $Aβ_{42}$ Protein Trajectories](https://arxiv.org/abs/2602.19444)
*Qianfeng Yu,Ningkang Peng,Yanhui Gu*

Main category: cs.LG

TL;DR: 本文介绍了一种名为PIS的物理信息系统，用于稳健的亚稳态划分，并通过集成预计算的物理先验如回转半径和溶剂可及表面积来改进对蛋白质轨迹中微妙状态转换的理解。该系统还提供了一个交互平台，可以动态监测物理特性并进行多维结果验证。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端深度学习模型在捕捉蛋白质轨迹中的细微状态转变方面存在不足，因为缺乏明确的物理约束。而理解β-淀粉样蛋白（Aβ），特别是Aβ42异构体的构象演变对于阐明阿尔茨海默病的发病机制至关重要。

Method: 提出了PIS（Physics-Informed System）系统，它通过将诸如回转半径和溶剂可及表面积等预先计算好的物理先验融入到拓扑特征提取过程中，从而实现对Aβ42数据集上更优的表现。

Result: PIS系统不仅在Aβ42数据集上展示了优越性能，而且为生物研究人员提供了具有物理基础解释性的强大分析工具集合。此外，PIS还配备了一个支持动态监控物理特性和多维度结果验证的互动平台。

Conclusion: PIS作为一款结合了物理信息的系统，在处理Aβ42蛋白质构象变化时表现出色，为研究者们理解和探索阿尔茨海默病相关机制提供了新的手段。

Abstract: Understanding the conformational evolution of $β$-amyloid ($Aβ$), particularly the $Aβ_{42}$ isoform, is fundamental to elucidating the pathogenic mechanisms underlying Alzheimer's disease. However, existing end-to-end deep learning models often struggle to capture subtle state transitions in protein trajectories due to a lack of explicit physical constraints. In this work, we introduce PIS, a Physics-Informed System designed for robust metastable state partitioning. By integrating pre-computed physical priors, such as the radius of gyration and solvent-accessible surface area, into the extraction of topological features, our model achieves superior performance on the $Aβ_{42}$ dataset. Furthermore, PIS provides an interactive platform that features dynamic monitoring of physical characteristics and multi-dimensional result validation. This system offers biological researchers a powerful set of analytical tools with physically grounded interpretability. A demonstration video of PIS is available on https://youtu.be/AJHGzUtRCg0.

</details>


### [107] [SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning](https://arxiv.org/abs/2602.19455)
*Zelin He,Boran Han,Xiyuan Zhang,Shuai Zhang,Haotian Lin,Qi Zhu,Haoyang Fang,Danielle C. Maddix,Abdul Fatir Ansari,Akash Chandrayan,Abhinav Pradhan,Bernie Wang,Matthew Reimherr*

Main category: cs.LG

TL;DR: 提出了一种混合知识注入框架，将时间序列语言模型（TSLM）生成的见解直接注入到通用推理大语言模型（GRLM）中，通过强化学习方法无需人工监督即可获取富含知识的思考轨迹，从而实现强时间序列推理与领域内知识结合。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案在处理时间序列诊断推理时存在一个持续的问题：通用推理大语言模型具有强大的推理能力但缺乏对复杂时间序列模式的理解；而经过微调的时间序列语言模型虽然理解这些模式，但在面对更复杂问题时缺乏泛化推理的能力。

Method: 本文提出了一个混合知识注入框架，该框架能够将TSLM产生的洞察直接整合到GRLM的推理过程中。此外，还采用基于强化学习的方法，利用可验证奖励来无监督地激发富有知识含量的思维路径，并将其转移到GRLM中以促进有效知识注入。

Result: 实验表明，在SenTSR-Bench及其他公开数据集上，所提方法相比TSLMs和GRLMs分别提高了9.1%-26.1%及7.9%-22.4%，提供了强大且情境感知的时间序列诊断见解。

Conclusion: 本研究提出的混合知识注入框架成功地弥补了通用推理模型与专门时间序列模型之间的差距，不仅增强了模型对于复杂时间序列模式的理解能力，也提升了其解决更复杂问题时的泛化推理水平。

Abstract: Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.

</details>


### [108] [Making Conformal Predictors Robust in Healthcare Settings: a Case Study on EEG Classification](https://arxiv.org/abs/2602.19483)
*Arjun Chatterjee,Sayeed Sajjad Razin,John Wu,Siddhartha Laghuvarapu,Jathurshan Pradeepkumar,Jimeng Sun*

Main category: cs.LG

TL;DR: 本文评估了几种保形预测方法在EEG癫痫分类任务中的表现，发现个性化校准策略可以将覆盖率提高20个百分点以上，同时保持相当的预测集大小。


<details>
  <summary>Details</summary>
Motivation: 临床预测中的不确定性量化对于高风险诊断任务至关重要。保形预测提供了一种有理论覆盖保证的方法。然而，在实践中，患者分布的变化违反了标准保形方法所基于的独立同分布假设，导致在医疗环境下的覆盖率不佳。

Method: 研究者们在EEG癫痫分类任务上评估了几种保形预测方法的表现，该任务具有已知的分布变化挑战和标签不确定性。

Result: 通过采用个性化的校准策略，能够在保持相近预测集大小的同时，将覆盖率提升超过20个百分点。

Conclusion: 针对存在分布偏移及标签不确定性的EEG癫痫分类问题，个性化校准策略能显著提高保形预测的性能。

Abstract: Quantifying uncertainty in clinical predictions is critical for high-stakes diagnosis tasks. Conformal prediction offers a principled approach by providing prediction sets with theoretical coverage guarantees. However, in practice, patient distribution shifts violate the i.i.d. assumptions underlying standard conformal methods, leading to poor coverage in healthcare settings. In this work, we evaluate several conformal prediction approaches on EEG seizure classification, a task with known distribution shift challenges and label uncertainty. We demonstrate that personalized calibration strategies can improve coverage by over 20 percentage points while maintaining comparable prediction set sizes. Our implementation is available via PyHealth, an open-source healthcare AI framework: https://github.com/sunlabuiuc/PyHealth.

</details>


### [109] [Federated Learning Playground](https://arxiv.org/abs/2602.19489)
*Bryan Guanrong Shan,Alysa Ziying Tan,Han Yu*

Main category: cs.LG

TL;DR: 提出了一个名为Federated Learning Playground的互动浏览器平台，用于教授联邦学习的核心概念，用户无需编码或系统设置即可直接在浏览器中实验不同的数据分布、模型超参数和聚合算法，并通过实时可视化了解这些因素对客户端模型和全局模型的影响。


<details>
  <summary>Details</summary>
Motivation: 为了降低分布式AI新手的学习门槛，同时为快速原型设计和比较联邦学习方法提供一个沙盒环境，从而促进对这一重要范式的更广泛理解和采用。

Method: 开发了一个基于浏览器的交互式平台，该平台允许用户直接在浏览器内操作而无需编写代码或进行系统配置，支持调整异构客户端的数据分布、模型超参数及聚合算法，并通过实时可视化展现其对客户与全局模型的影响。

Result: 创建了Federated Learning Playground，作为易于使用的教育工具，有助于用户理解联邦学习中的挑战（如非独立同分布数据、局部过拟合及可扩展性问题）。

Conclusion: Federated Learning Playground不仅降低了联邦学习领域的入门难度，还为快速测试和比较不同联邦学习方法提供了可能，促进了该技术的理解与应用。

Abstract: We present Federated Learning Playground, an interactive browser-based platform inspired by and extends TensorFlow Playground that teaches core Federated Learning (FL) concepts. Users can experiment with heterogeneous client data distributions, model hyperparameters, and aggregation algorithms directly in the browser without coding or system setup, and observe their effects on client and global models through real-time visualizations, gaining intuition for challenges such as non-IID data, local overfitting, and scalability. The playground serves as an easy to use educational tool, lowering the entry barrier for newcomers to distributed AI while also offering a sandbox for rapidly prototyping and comparing FL methods. By democratizing exploration of FL, it promotes broader understanding and adoption of this important paradigm.

</details>


### [110] [Softmax is not Enough (for Adaptive Conformal Classification)](https://arxiv.org/abs/2602.19498)
*Navid Akhavan Attar,Hesam Asadollahzadeh,Ling Luo,Uwe Aickelin*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过利用预softmax logit空间的信息并使用Helmholtz自由能作为模型不确定性和样本难度的度量，重新加权非一致性分数，从而提高对输入难度的敏感性。实验表明，这种基于能量的方法提高了预测集的适应性，在效率和适应性方面都比基线有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的深度共形分类器从softmax输出中导出非一致性分数，这可能成为不可靠的指标，导致过度自信的误分类或不必要的犹豫。作者认为这种不可靠性可能会被CP生成的预测集继承，限制了它们的适应能力。

Method: 本文提出的方法是利用预softmax logit空间中的信息，并采用Helmholtz自由能作为衡量模型不确定性及样本难度的指标。通过对每个样本的能量得分进行单调变换来重新加权非一致性分数，以提高其对输入难度的敏感性。

Result: 在多个数据集和深度架构上使用四种最先进的评分函数进行的实验显示，该基于能量的增强方法改善了预测集的适应性，与基准非一致性分数相比，在效率和适应性方面都有显著提升。

Conclusion: 提出的基于能量的方法能够有效提高预测集的适应性，同时保持高效性，为解决传统深度共形分类器中存在的问题提供了一个新的思路。

Abstract: The merit of Conformal Prediction (CP), as a distribution-free framework for uncertainty quantification, depends on generating prediction sets that are efficient, reflected in small average set sizes, while adaptive, meaning they signal uncertainty by varying in size according to input difficulty. A central limitation for deep conformal classifiers is that the nonconformity scores are derived from softmax outputs, which can be unreliable indicators of how certain the model truly is about a given input, sometimes leading to overconfident misclassifications or undue hesitation. In this work, we argue that this unreliability can be inherited by the prediction sets generated by CP, limiting their capacity for adaptiveness. We propose a new approach that leverages information from the pre-softmax logit space, using the Helmholtz Free Energy as a measure of model uncertainty and sample difficulty. By reweighting nonconformity scores with a monotonic transformation of the energy score of each sample, we improve their sensitivity to input difficulty. Our experiments with four state-of-the-art score functions on multiple datasets and deep architectures show that this energy-based enhancement improves the adaptiveness of the prediction sets, leading to a notable increase in both efficiency and adaptiveness compared to baseline nonconformity scores, without introducing any post-hoc complexity.

</details>


### [111] [Less is More: Convergence Benefits of Fewer Data Weight Updates over Longer Horizon](https://arxiv.org/abs/2602.19510)
*Rudrajit Das,Neel Patel,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 本文探讨了数据混合在训练鲁棒机器学习模型中的重要性，通过双层优化任务来解决这一问题。作者们分析了有限内层更新步数T下的收敛行为，并证明了实践中常用的T=1的贪婪方法在简单二次示例中可能失败。对于给定的参数更新预算N和每域损失强凸假设下，使用全梯度（或随机梯度）时最优T值分别以Θ(log N) (或Θ((N log N)^{1/2}))的比例增长。


<details>
  <summary>Details</summary>
Motivation: 数据混合是训练鲁棒机器学习模型的关键组成部分，但当前最先进的方法由于计算限制，在更新权重之前仅采用有限且通常较小数量的内层更新步骤，这导致对这种近似的理论影响尚不完全理解。

Method: 本研究通过严格的数学分析探讨了在仅有有限次内层更新步骤T情况下的数据混合收敛行为。同时，基于固定的参数更新预算N以及假设每域损失强凸条件下，确定了最优T值的增长比例。

Result: 研究表明，实际应用中常用的单步更新策略（T=1）即使是在简单的二次示例中也可能失效；而针对全梯度访问场景，当T按Θ(log N)增长时效果最佳；对于只能访问随机梯度的情况，则T应按照Θ((N log N)^{1/2})增长。

Conclusion: 选择合适的内层迭代次数T对于提高数据混合效率至关重要，特别是当考虑整体计算资源限制时。该工作为如何在给定计算预算下有效设置T提供了理论指导。

Abstract: Data mixing--the strategic reweighting of training domains--is a critical component in training robust machine learning models. This problem is naturally formulated as a bilevel optimization task, where the outer loop optimizes domain weights to minimize validation loss, and the inner loop optimizes model parameters to minimize the weighted training loss. Classical bilevel optimization relies on hypergradients, which theoretically require the inner optimization to reach convergence. However, due to computational constraints, state-of-the-art methods use a finite, often small, number of inner update steps before updating the weights. The theoretical implications of this approximation are not well understood. In this work, we rigorously analyze the convergence behavior of data mixing with a finite number of inner steps $T$. We prove that the "greedy" practical approach of using $T=1$ can fail even in a simple quadratic example. Under a fixed parameter update budget $N$ and assuming the per-domain losses are strongly convex, we show that the optimal $T$ scales as $Θ(\log N)$ (resp., $Θ({(N \log N)}^{1/2})$) for the data mixing problem with access to full (resp., stochastic) gradients. We complement our theoretical results with proof-of-concept experiments.

</details>


### [112] [Beyond Accuracy: A Unified Random Matrix Theory Diagnostic Framework for Crash Classification Models](https://arxiv.org/abs/2602.19528)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本研究引入了一种基于随机矩阵理论(RMT)和重尾自正则化(HTSR)的频谱诊断框架，用于评估交通事故分类模型的质量。该框架适用于多种机器学习模型，并通过幂律指数α来区分模型是否过拟合。研究发现，良好的正则化模型通常具有2到4之间的α值，而过拟合模型则表现出α<2或频谱塌陷。此外，还提出了基于α的提前停止标准和模型选择协议。


<details>
  <summary>Details</summary>
Motivation: 现有的交通事故分类模型评价指标（如准确率、F1分数或AUC）无法揭示模型是否存在静默过拟合的问题。为了解决这一局限性，研究人员开发了一个新的频谱诊断框架，旨在提供更深层次的模型质量洞察。

Method: 采用基于随机矩阵理论(RMT)与重尾自正则化(HTSR)的方法构建了一个频谱诊断框架，该框架可应用于广泛的机器学习模型类别，包括但不限于BERT/ALBERT/Qwen2.5等权重矩阵、XGBoost/随机森林的折外增量矩阵、逻辑回归的经验海森矩阵、决策树的诱导亲和力矩阵以及KNN的图拉普拉斯算子。通过对爱荷华州交通部提供的两个事故分类任务的数据集进行分析，使用了稀疏Lanczos近似方法确保了框架在大规模数据集上的可扩展性。

Result: 研究结果表明，良好正则化的模型其幂律指数α值稳定位于2至4之间（平均2.87±0.34），而过拟合模型则显示α<2或出现频谱塌缩现象。此外，幂律指数α与专家意见之间存在显著的相关性（Spearman ρ= 0.89, p < 0.001）。基于这些发现，提出了一个以α为基础的早期停止准则及频谱模型选择流程，并通过交叉验证F1分数基准进行了验证。

Conclusion: 新提出的频谱诊断框架能够有效地区分正常与过拟合的交通事故分类模型，且通过幂律指数α与专家判断的一致性证明了其有效性。这为提高模型鲁棒性和可靠性提供了新的视角和技术手段。

Abstract: Crash classification models in transportation safety are typically evaluated using accuracy, F1, or AUC, metrics that cannot reveal whether a model is silently overfitting. We introduce a spectral diagnostic framework grounded in Random Matrix Theory (RMT) and Heavy-Tailed Self-Regularization (HTSR) that spans the ML taxonomy: weight matrices for BERT/ALBERT/Qwen2.5, out-of-fold increment matrices for XGBoost/Random Forest, empirical Hessians for Logistic Regression, induced affinity matrices for Decision Trees, and Graph Laplacians for KNN. Evaluating nine model families on two Iowa DOT crash classification tasks (173,512 and 371,062 records respectively), we find that the power-law exponent $α$ provides a structural quality signal: well-regularized models consistently yield $α$ within $[2, 4]$ (mean $2.87 \pm 0.34$), while overfit variants show $α< 2$ or spectral collapse. We observe a strong rank correlation between $α$ and expert agreement (Spearman $ρ= 0.89$, $p < 0.001$), suggesting spectral quality captures model behaviors aligned with expert reasoning. We propose an $α$-based early stopping criterion and a spectral model selection protocol, and validate both against cross-validated F1 baselines. Sparse Lanczos approximations make the framework scalable to large datasets.

</details>


### [113] [A Statistical Approach for Modeling Irregular Multivariate Time Series with Missing Observations](https://arxiv.org/abs/2602.19531)
*Dingyi Nie,Yixing Wu,C. -C. Jay Kuo*

Main category: cs.LG

TL;DR: 本文提出了一种处理不规则多变量时间序列数据的简单方法，通过提取与时间无关的统计特征来消除时间轴的影响。该方法在四个生物医学数据集上达到了最先进的性能，并且计算复杂度较低。


<details>
  <summary>Details</summary>
Motivation: 针对医疗保健等领域中存在缺失值的不规则多变量时间序列数据给预测建模带来的挑战，本文旨在寻找一种比现有深度学习方法更简单但同样有效的解决方案。

Method: 本文的方法是为每个变量计算四个关键特征——观测值的均值和标准差、以及连续观察之间变化的均值和可变性，以此创建一个固定维度的表示。这些特征随后被用于标准分类器如逻辑回归和XGBoost中。

Result: 在四个生物医学数据集上的评估表明，所提方法超越了最近的transformer和基于图模型的表现，在AUROC/AUPRC上提高了0.5-1.7%，在准确率/F1分数上提高了1.1-1.7%，同时减少了计算复杂性。消融研究表明，性能提升主要归功于特征提取而非分类器的选择。

Conclusion: 研究结果挑战了当任务目标允许使用与时间无关的表示时采用复杂时间建模的必要性，提供了一个高效且可解释的不规则时间序列分类方案。

Abstract: Irregular multivariate time series with missing values present significant challenges for predictive modeling in domains such as healthcare. While deep learning approaches often focus on temporal interpolation or complex architectures to handle irregularities, we propose a simpler yet effective alternative: extracting time-agnostic summary statistics to eliminate the temporal axis. Our method computes four key features per variable-mean and standard deviation of observed values, as well as the mean and variability of changes between consecutive observations to create a fixed-dimensional representation. These features are then utilized with standard classifiers, such as logistic regression and XGBoost. Evaluated on four biomedical datasets (PhysioNet Challenge 2012, 2019, PAMAP2, and MIMIC-III), our approach achieves state-of-the-art performance, surpassing recent transformer and graph-based models by 0.5-1.7% in AUROC/AUPRC and 1.1-1.7% in accuracy/F1-score, while reducing computational complexity. Ablation studies demonstrate that feature extraction-not classifier choice-drives performance gains, and our summary statistics outperform raw/imputed input in most benchmarks. In particular, we identify scenarios where missing patterns themselves encode predictive signals, as in sepsis prediction (PhysioNet, 2019), where missing indicators alone can achieve 94.2% AUROC with XGBoost, only 1.6% lower than using original raw data as input. Our results challenge the necessity of complex temporal modeling when task objectives permit time-agnostic representations, providing an efficient and interpretable solution for irregular time series classification.

</details>


### [114] [The Sample Complexity of Replicable Realizable PAC Learning](https://arxiv.org/abs/2602.19552)
*Kasper Green Larsen,Markus Engelund Mathiasen,Chirag Pabbaraju,Clement Svendsen*

Main category: cs.LG

TL;DR: 本文研究了可复制的PAC学习问题，为某一特定的学习难题构建了一个样本复杂度下界，并且这个下界几乎与假设类H的大小呈(˜log|H|)^{3/2}的关系。同时，还证明了对于该下界实例，存在一个几乎匹配的上界。


<details>
  <summary>Details</summary>
Motivation: 作者旨在探索PAC学习中的可复制性问题，并试图通过构造特别困难的学习问题来发现样本复杂度的下界。

Method: 研究者定义了一个与假设类H相关的特定Cayley图，并通过分析此图上的适当随机游走以及考察其邻接矩阵的谱性质来进行证明。

Result: 得出了一个关于样本复杂度的近似下界，该下界大约是(log|H|)^{3/2}，并且展示了一个与此下界实例几乎匹配的上界。

Conclusion: 研究表明在考虑给定的问题实例时，已经找到了几乎最优的样本复杂度界限。如果要找到更强的下界，则需要考虑不同的情形。

Abstract: In this paper, we consider the problem of replicable realizable PAC learning. We construct a particularly hard learning problem and show a sample complexity lower bound with a close to $(\log|H|)^{3/2}$ dependence on the size of the hypothesis class $H$. Our proof uses several novel techniques and works by defining a particular Cayley graph associated with $H$ and analyzing a suitable random walk on this graph by examining the spectral properties of its adjacency matrix.
  Furthermore, we show an almost matching upper bound for the lower bound instance, meaning if a stronger lower bound exists, one would have to consider a different instance of the problem.

</details>


### [115] [Advantage-based Temporal Attack in Reinforcement Learning](https://arxiv.org/abs/2602.19582)
*Shenghong He*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，即基于优势的对抗性变换器(AAT)，通过引入多尺度因果自注意力机制和加权优势机制来生成具有更强时间相关性的对抗样本，从而提高攻击性能。实验表明AAT在Atari、DeepMind控制套件和谷歌足球任务上的表现与主流对抗攻击基线相当或更好。


<details>
  <summary>Details</summary>
Motivation: 现有的基于奖励的攻击方法在生成对抗扰动时无法捕捉不同时间步骤之间的依赖关系，导致当前扰动与先前扰动之间的时间关联较弱。为了克服这一局限，研究者们旨在开发一种能够生成更具有时间相关性的对抗样本来增强攻击效果的新方法。

Method: 提出了Advantage-based Adversarial Transformer (AAT) 方法，该方法利用多尺度因果自注意力(MSCSA)机制动态捕捉不同时期的历史信息与当前状态间的依赖关系，并引入了加权优势机制来量化给定状态下扰动的有效性，引导生成过程朝向高效益区域采样以产生高效对抗样本。

Result: 广泛的实验结果显示，AAT 在 Atari 游戏、DeepMind 控制套件以及 Google 足球等任务上展现出了与主流对抗攻击基准相匹配甚至超越的表现。

Conclusion: 通过采用MSCSA机制和加权优势机制，AAT能够生成具备更强时间相关性的对抗样本，从而有效地提高了对深度强化学习模型的攻击性能。

Abstract: Extensive research demonstrates that Deep Reinforcement Learning (DRL) models are susceptible to adversarially constructed inputs (i.e., adversarial examples), which can mislead the agent to take suboptimal or unsafe actions. Recent methods improve attack effectiveness by leveraging future rewards to guide adversarial perturbation generation over sequential time steps (i.e., reward-based attacks). However, these methods are unable to capture dependencies between different time steps in the perturbation generation process, resulting in a weak temporal correlation between the current perturbation and previous perturbations.In this paper, we propose a novel method called Advantage-based Adversarial Transformer (AAT), which can generate adversarial examples with stronger temporal correlations (i.e., time-correlated adversarial examples) to improve the attack performance. AAT employs a multi-scale causal self-attention (MSCSA) mechanism to dynamically capture dependencies between historical information from different time periods and the current state, thus enhancing the correlation between the current perturbation and the previous perturbation. Moreover, AAT introduces a weighted advantage mechanism, which quantifies the effectiveness of a perturbation in a given state and guides the generation process toward high-performance adversarial examples by sampling high-advantage regions. Extensive experiments demonstrate that the performance of AAT matches or surpasses mainstream adversarial attack baselines on Atari, DeepMind Control Suite and Google football tasks.

</details>


### [116] [Variational Inference for Bayesian MIDAS Regression](https://arxiv.org/abs/2602.19610)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 本研究开发了一种用于贝叶斯混合数据采样回归的坐标上升变分推断(CAVI)算法，该算法能够有效处理线性权重参数化的问题。通过归一化约束分离影响系数和权重函数参数，形成了一个双线性结构，使得通用哈密顿蒙特卡洛采样器变得不可靠，但保持了CAVI可利用的条件共轭性。在包含多达50个预测变量的21种数据生成配置的蒙特卡洛研究中，与块吉布斯采样器基准相比，CAVI产生的后验均值几乎相同，同时实现了107倍至1,772倍的速度提升。此外，对S&P 500日收益率的实际波动率预测应用表明，CAVI与吉布斯抽样产生的点预测基本相同，而CAVI每月估计完成时间不到10毫秒。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决现有方法在处理具有线性权重参数化的贝叶斯混合数据采样(MIDAS)回归问题时遇到的挑战，特别是当模型呈现出双线性结构时，通用哈密顿蒙特卡洛采样器变得不再可靠。作者希望通过引入坐标上升变分推断(CAVI)算法来克服这一限制，该算法能够在保持条件共轭性的同时，有效地对模型参数进行估计。

Method: 研究人员开发了一种适用于贝叶斯MIDAS回归模型（带有线性权重参数化）的坐标上升变分推断(CAVI)算法。此方法通过归一化约束将影响系数从权重函数参数中分离出来，从而形成一种虽然对于通用哈密顿蒙特卡洛采样器来说不可靠但却保留了可用于CAVI的条件共轭性的双线性结构。每一步变分更新都允许闭式解：回归系数和权重参数为高斯分布，误差方差为逆伽马分布。

Result: 实验结果表明，在涉及最多50个预测变量的21种不同数据生成配置下，所提出的CAVI算法与作为基准的块吉布斯采样器相比，能够产生近乎相同的后验均值估计，同时速度提升了107倍到1,772倍。另一方面，通用自动微分变分推断(ADVI)方法不仅偏差更大，而且计算效率显著较低。此外，权重函数参数在所有配置下都表现出优秀的校准度；尽管影响系数可信区间的覆盖范围随着预测变量数量增加而有所下降，但仍然体现了速度与区间校准之间的一种权衡。

Conclusion: 本研究表明，针对贝叶斯MIDAS回归模型提出的坐标上升变分推断(CAVI)算法，在不牺牲精度的前提下大大提高了计算效率。它不仅能够快速准确地估计出模型参数，而且在实际应用如S&P 500日收益波动率预测上也表现出了优异性能。这表明，对于需要高效处理大量数据并要求实时响应的应用场景而言，CAVI是一个极具潜力的选择。

Abstract: We develop a Coordinate Ascent Variational Inference (CAVI) algorithm for Bayesian Mixed Data Sampling (MIDAS) regression with linear weight parameteri zations. The model separates impact coe cients from weighting function parameters through a normalization constraint, creating a bilinear structure that renders generic Hamiltonian Monte Carlo samplers unreliable while preserving conditional conju gacy exploitable by CAVI. Each variational update admits a closed-form solution: Gaussian for regression coe cients and weight parameters, Inverse-Gamma for the error variance. The algorithm propagates uncertainty across blocks through second moments, distinguishing it from naive plug-in approximations. In a Monte Carlo study spanning 21 data-generating con gurations with up to 50 predictors, CAVI produces posterior means nearly identical to a block Gibbs sampler benchmark while achieving speedups of 107x to 1,772x (Table 9). Generic automatic di eren tiation VI (ADVI), by contrast, produces bias 714 times larger while being orders of magnitude slower, con rming the value of model-speci c derivations. Weight function parameters maintain excellent calibration (coverage above 92%) across all con gurations. Impact coe cient credible intervals exhibit the underdispersion characteristic of mean- eld approximations, with coverage declining from 89% to 55% as the number of predictors grows a documented trade-o between speed and interval calibration that structured variational methods can address. An empirical application to realized volatility forecasting on S&P 500 daily returns con rms that CAVI and Gibbs sampling yield virtually identical point forecasts, with CAVI completing each monthly estimation in under 10 milliseconds.

</details>


### [117] [Is Your Diffusion Sampler Actually Correct? A Sampler-Centric Evaluation of Discrete Diffusion Language Models](https://arxiv.org/abs/2602.19619)
*Luhan Tang,Longxuan Yu,Shaorong Zhang,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出了一种基于采样器为核心的oracle框架，用以解决离散扩散语言模型（dLLMs）在评估时遇到的挑战。该方法通过将学习到的去噪器替换为从真实马尔可夫链导出的确切隐马尔可夫模型后验，从而隔离了由于采样动态引起的误差。研究发现即使在oracle去噪器下，少数步骤的离散扩散采样器也不是分布正确的，并且负对数似然、生成困惑度或MAUVE等指标的改进并不意味着正确的采样。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（dLLMs）提供了一种快速灵活的方式来替代自回归模型（ARMs），但它们的评估面临挑战：现有指标混淆了去噪器近似误差与由采样动态引起的误差。而这个问题在自回归模型中不会出现，因为其自回归采样精确反映了所学的概率模型。

Method: 引入了一种基于采样器为核心的oracle框架，该框架通过使用从真实马尔可夫链得出的确切隐藏马尔可夫模型后验来代替学习到的去噪器，从而能够在受控环境中单独考察由采样过程引发的误差。

Result: 研究表明，在oracle去噪器下，只有当步数接近序列长度时，少数几步的离散扩散采样器才不会表现出转换级别的不匹配现象；此外，诸如负对数似然、生成困惑度或者MAUVE等评价标准的改善并不能保证采样的正确性。

Conclusion: 本研究揭示了离散扩散语言模型中存在的一个关键问题——即即使在理想条件下，少量迭代次数下的采样结果也未必能够准确反映目标分布。这表明当前常用的性能指标可能不足以全面衡量此类模型的有效性。

Abstract: Discrete diffusion language models (dLLMs) provide a fast and flexible alternative to autoregressive models (ARMs) via iterative denoising with parallel updates. However, their evaluation is challenging: existing metrics conflate denoiser approximation error with sampler-induced error from the sampling dynamics, a problem that does not arise for ARMs whose autoregressive sampling exactly reflects the learned probability model. We introduce a sampler-centric oracle framework that replaces learned denoisers with an exact Hidden Markov Model posterior derived from a ground-truth Markov chain, isolating sampler-induced error in a controlled setting. We show that few-step discrete diffusion samplers are not distributionally correct even under an oracle denoiser, with transition-level mismatch that vanishes only as the number of steps approaches the sequence length. Moreover, improvements in negative log-likelihood, generative perplexity, or MAUVE do not imply correct sampling. Code is available at https://luhantang.github.io/dllm_sampler

</details>


### [118] [VecFormer: Towards Efficient and Generalizable Graph Transformer with Graph Token Attention](https://arxiv.org/abs/2602.19622)
*Jingbo Zhou,Jun Xia,Siyuan Li,Yunfan Liu,Wenjun Wang,Yufei Huang,Changxi Chi,Mutian Hong,Zhuoli Ouyang,Shu Wang,Zhongqi Wang,Xingyu Wu,Chang Yu,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为VecFormer的新模型，它通过两阶段训练范式解决了现有图变换器在处理大规模图时计算复杂度高以及泛化性能差的问题。实验表明，VecFormer在性能和速度上都优于现有的图变换器。


<details>
  <summary>Details</summary>
Motivation: 目前的图变换器方法面临两个关键挑战：一是大多数模型的计算复杂度呈指数增长，难以扩展到大型图；二是基于节点级别操作的注意力机制限制了模型灵活性，并导致在分布外（OOD）场景下的泛化性能较差。

Method: 提出了VecFormer（向量量化图变换器），采用两阶段训练策略。第一阶段使用两个码本来重建节点特征与图结构，学习丰富的语义Graph Codes。第二阶段则基于转换后的跨码本在Graph Token层面执行注意力机制，以减少计算复杂度并提高模型泛化能力。

Result: 广泛的实验显示，在不同规模的数据集上，VecFormer相较于现有的图变换器在性能和处理速度方面均表现出色。

Conclusion: VecFormer为解决图表示学习中的可扩展性和泛化问题提供了一个有效且高效的解决方案，特别是在OOD设置下的节点分类任务中。

Abstract: Graph Transformer has demonstrated impressive capabilities in the field of graph representation learning. However, existing approaches face two critical challenges: (1) most models suffer from exponentially increasing computational complexity, making it difficult to scale to large graphs; (2) attention mechanisms based on node-level operations limit the flexibility of the model and result in poor generalization performance in out-of-distribution (OOD) scenarios. To address these issues, we propose \textbf{VecFormer} (the \textbf{Vec}tor Quantized Graph Trans\textbf{former}), an efficient and highly generalizable model for node classification, particularly under OOD settings. VecFormer adopts a two-stage training paradigm. In the first stage, two codebooks are used to reconstruct the node features and the graph structure, aiming to learn the rich semantic \texttt{Graph Codes}. In the second stage, attention mechanisms are performed at the \texttt{Graph Token} level based on the transformed cross codebook, reducing computational complexity while enhancing the model's generalization capability. Extensive experiments on datasets of various sizes demonstrate that VecFormer outperforms the existing Graph Transformer in both performance and speed.

</details>


### [119] [Compositional Planning with Jumpy World Models](https://arxiv.org/abs/2602.19634)
*Jesse Farebrother,Matteo Pirotta,Andrea Tirinzoni,Marc G. Bellemare,Alessandro Lazaric,Ahmed Touati*

Main category: cs.LG

TL;DR: 本文提出了一种基于跳跃世界模型的组合规划方法，通过学习多步动态预测模型来提高长时间范围任务的预测准确性，并在多种基础策略上实现了显著优于原始动作规划的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决组合规划中长时序预测误差累积导致难以估计由序列化策略引起的访问分布的问题，受几何策略组合框架启发，研究者们致力于开发一种能够捕捉状态占据情况的跳跃世界模型。

Method: 通过学习多步动态预测模型（跳跃世界模型），该模型能在非策略方式下捕捉预训练策略在多个时间尺度上的状态占据情况；并结合TD流提出了新的跨时间尺度一致性目标函数以提高长时间预测精度。此外还展示了如何利用生成性预测来评估执行任意策略序列的价值。

Result: 实验结果表明，在具有挑战性的操作和导航任务中，使用跳跃世界模型进行组合规划相比基于原始动作的规划平均提高了200%的相对表现。

Conclusion: 本文提出的跳跃世界模型和相应的组合规划方法为解决复杂任务提供了一个有效途径，特别是在需要长期决策的情境下展现出了优越性。

Abstract: The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.

</details>


### [120] [Evaluating the Impact of Data Anonymization on Image Retrieval](https://arxiv.org/abs/2602.19641)
*Marvin Chen,Manuel Eberhardinger,Johannes Maucher*

Main category: cs.LG

TL;DR: 本文研究了匿名化对基于内容的图像检索（CBIR）系统性能的影响，通过提出一个评估框架来比较匿名化前后检索结果的一致性。实验涵盖了三种匿名化方法、四个匿名化程度和四种训练策略，并揭示了使用原始数据训练的模型在匿名化后仍能产生最相似的检索结果，为开发既符合隐私规定又能保持性能的CBIR系统提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR等隐私法规的重要性日益增加，视觉数据的匿名化变得越来越重要，但其可能对依赖于视觉特征的计算机视觉系统如CBIR造成负面影响。然而，对于匿名化如何影响CBIR的研究还不充分。受DOKIQ项目启发，该项目是巴登-符腾堡州刑事警察局积极使用的一个基于人工智能的文件验证系统，本研究旨在填补这一空白。

Method: 文章提出了一个简单的评估框架，即匿名化后的检索结果应该尽可能接近匿名化前的结果。为此，研究人员系统地评估了两种公开数据集以及内部DOKIQ数据集中匿名化的影响。实验包括了三种匿名化方法、四个匿名化程度级别、以及基于当前最先进的自蒸馏无标签技术(DINO)v2的四种训练策略。

Result: 结果显示，使用原始数据训练的模型显示出明显的检索偏好，在经过匿名化处理后能够产生最为相似的检索结果。这意味着即使是在数据被匿名化的情况下，这些模型仍然能够较好地维持其性能表现。

Conclusion: 该论文的研究发现为开发既能遵守隐私合规要求又可有效保持检索性能的CBIR系统提供了宝贵的实践指导意义。

Abstract: With the growing importance of privacy regulations such as the General Data Protection Regulation, anonymizing visual data is becoming increasingly relevant across institutions. However, anonymization can negatively affect the performance of Computer Vision systems that rely on visual features, such as Content-Based Image Retrieval (CBIR). Despite this, the impact of anonymization on CBIR has not been systematically studied. This work addresses this gap, motivated by the DOKIQ project, an artificial intelligence-based system for document verification actively used by the State Criminal Police Office Baden-Württemberg. We propose a simple evaluation framework: retrieval results after anonymization should match those obtained before anonymization as closely as possible. To this end, we systematically assess the impact of anonymization using two public datasets and the internal DOKIQ dataset. Our experiments span three anonymization methods, four anonymization degrees, and four training strategies, all based on the state of the art backbone Self-Distillation with No Labels (DINO)v2. Our results reveal a pronounced retrieval bias in favor of models trained on original data, which produce the most similar retrievals after anonymization. The findings of this paper offer practical insights for developing privacy-compliant CBIR systems while preserving performance.

</details>


### [121] [Spectral Phase Encoding for Quantum Kernel Methods](https://arxiv.org/abs/2602.19644)
*Pablo Herrero Gómez,Antonio Jimeno Morenilla,David Muñoz-Hernández,Higinio Mora Mora*

Main category: cs.LG

TL;DR: 本文研究了量子核方法在受控加性噪声下的性能，提出了一种新的特征构建方法SPE，并通过实验表明基于DFT的预处理在噪声增加时具有最小的退化率，且与经典基线相比表现出了可比或更稳定的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子核方法作为近期量子机器学习的一个有前景的方向，但其在数据损坏情况下的行为尚未得到充分理解。

Method: 引入了SPE（谱相位编码），一种结合离散傅里叶变换前端和对角相位嵌入的方法；在统一框架下对比了QK-DFT与其他量子变体及经典SVM基线在相同清洁数据超参数选择下的性能；采用固定效应回归和野群集自助法来量化跨异构真实世界数据集的鲁棒性。

Result: 结果表明，在所有量子方法中，基于DFT的预处理随着噪声增加表现出最小的退化率，且相对于PCA和RP有统计学支持的斜率差异；与经典基线相比，QK-DFT的退化程度与线性SVM相当，但比RBF SVM更为稳定。硬件实验进一步证实了SPE在重叠估计中的执行性和数值稳定性。

Conclusion: 量子核方法的鲁棒性很大程度上依赖于结构对齐的预处理及其与对角嵌入的交互作用，这为NISQ时代量子机器学习提供了一个以鲁棒性为先的观点。

Abstract: Quantum kernel methods are promising for near-term quantum ma- chine learning, yet their behavior under data corruption remains insuf- ficiently understood. We analyze how quantum feature constructions degrade under controlled additive noise. We introduce Spectral Phase Encoding (SPE), a hybrid construc- tion combining a discrete Fourier transform (DFT) front-end with a diagonal phase-only embedding aligned with the geometry of diagonal quantum maps. Within a unified framework, we compare QK-DFT against alternative quantum variants (QK-PCA, QK-RP) and classi- cal SVM baselines under identical clean-data hyperparameter selection, quantifying robustness via dataset fixed-effects regression with wild cluster bootstrap inference across heterogeneous real-world datasets. Across the quantum family, DFT-based preprocessing yields the smallest degradation rate as noise increases, with statistically sup- ported slope differences relative to PCA and RP. Compared to classical baselines, QK-DFT shows degradation comparable to linear SVM and more stable than RBF SVM under matched tuning. Hardware exper- iments confirm that SPE remains executable and numerically stable for overlap estimation. These results indicate that robustness in quan- tum kernels depends critically on structure-aligned preprocessing and its interaction with diagonal embeddings, supporting a robustness-first perspective for NISQ-era quantum machine learning.

</details>


### [122] [NEXUS : A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi Nationa Capital Region](https://arxiv.org/abs/2602.19654)
*Rampunit Kumar,Aditya Maheshwari*

Main category: cs.LG

TL;DR: 本文提出了一种名为NEXUS的架构，用于预测德里国家首都区的二氧化碳、氮氧化物和二氧化硫。该架构使用了四年的大气数据，在参数量远少于其他模型的情况下，实现了高精度的预测，并能够实时部署用于空气质量监测系统。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染对公众健康构成重大挑战，特别是在德里国家首都区这样的大都市中影响着数百万人。为了解决这一问题，研究人员开发了NEXUS架构来更准确地预测主要污染物水平。

Method: NEXUS架构集成了块嵌入、低秩投影和自适应融合机制，以解码复杂的大气化学模式。它基于2018年至2021年间来自十六个空间网格的数据进行工作。

Result: NEXUS对于CO、NO和SO2的R^2值分别超过了0.94、0.91和0.95，且使用的参数数量显著少于SCINet、Autoformer和FEDformer等现有方法。此外，研究还揭示了明显的昼夜节律与季节性变化规律。

Conclusion: NEXUS不仅在预测性能上表现出色，同时具有很高的计算效率，适合实时部署到空气质量监控系统中。

Abstract: Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems.

</details>


### [123] [PaReGTA: An LLM-based EHR Data Encoding Approach to Capture Temporal Information](https://arxiv.org/abs/2602.19661)
*Kihyuk Yoon,Lingchao Mao,Catherine Chong,Todd J. Schwedt,Chia-Chun Chiang,Jing Li*

Main category: cs.LG

TL;DR: 提出了PaReGTA，一种基于大型语言模型（LLM）的编码框架，用于处理电子健康记录（EHRs）中的时间信息。该方法通过将纵向EHR事件转换为带有明确时间线索的访问级别模板文本、学习领域适应的访问嵌入以及使用混合时间池化来汇总患者表示，以保留和利用时间信息。此外，PaReGTA不需要从头开始训练，并且可以与未来专门针对EHR的句子嵌入模型兼容。在39,088名偏头痛患者的测试中，PaReGTA在偏头痛类型分类上优于稀疏基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的EHRs表示方法要么丢失了重要的时间信息（如one-hot或计数为基础的表示），要么需要大量数据和计算资源（如序列模型）。因此，研究者旨在开发一种新的方法，既能有效地利用时间信息，又能在数据有限的情况下表现良好。

Method: PaReGTA采用以下步骤：1. 将纵向EHR事件转换为含有明确时间提示的访问级模板文本；2. 通过对预训练的句子嵌入模型进行轻量级对比微调来学习领域自适应的访问嵌入；3. 利用结合近期性和全局信息性的混合时间池化技术将访问嵌入聚合为固定维度的患者表示。此外，为了提高可解释性，还引入了PaReGTA-RSS指标，通过移除特定临床定义因素后重新计算表示并测量变化来量化这些因素的重要性。

Result: 在来自All of Us Research Program的39,088名偏头痛患者的数据集上进行了实验。结果表明，相比于稀疏基线方法，PaReGTA在偏头痛类型的分类任务上表现更优。同时指出，在当前数据集规模下，深层顺序模型的表现不够稳定。

Conclusion: PaReGTA提供了一种有效利用时间信息的新途径，尤其适用于数据有限的情况。它不仅能够改善医疗预测任务的结果，而且还具有良好的扩展性，能受益于未来的EHR专用句子嵌入模型发展。

Abstract: Temporal information in structured electronic health records (EHRs) is often lost in sparse one-hot or count-based representations, while sequence models can be costly and data-hungry. We propose PaReGTA, an LLM-based encoding framework that (i) converts longitudinal EHR events into visit-level templated text with explicit temporal cues, (ii) learns domain-adapted visit embeddings via lightweight contrastive fine-tuning of a sentence-embedding model, and (iii) aggregates visit embeddings into a fixed-dimensional patient representation using hybrid temporal pooling that captures both recency and globally informative visits. Because PaReGTA does not require training from scratch but instead utilizes a pre-trained LLM, it can perform well even in data-limited cohorts. Furthermore, PaReGTA is model-agnostic and can benefit from future EHR-specialized sentence-embedding models. For interpretability, we introduce PaReGTA-RSS (Representation Shift Score), which quantifies clinically defined factor importance by recomputing representations after targeted factor removal and projecting representation shifts through a machine learning model. On 39,088 migraine patients from the All of Us Research Program, PaReGTA outperforms sparse baselines for migraine type classification while deep sequential models were unstable in our cohort.

</details>


### [124] [PerturbDiff: Functional Diffusion for Single-Cell Perturbation Modeling](https://arxiv.org/abs/2602.19685)
*Xinyu Yuan,Xixian Liu,Ya Shi Zhang,Zuobai Zhang,Hongyu Guo,Jian Tang*

Main category: cs.LG

TL;DR: 本文提出了一种名为PerturbDiff的新方法，该方法通过将细胞分布嵌入希尔伯特空间，并定义一个直接作用于概率分布的扩散生成过程，从而捕捉隐藏因素下的群体水平响应变化。PerturbDiff在单细胞响应预测上达到了最先进的性能，并且对于未见过的扰动具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于高通量单细胞测序是破坏性的，同一样本不能同时观察到扰动前后的情况，因此需要一种方法来映射未配对的对照和扰动群体。现有的模型通常假设给定观察条件（如细胞类型）和扰动类型时只有一个固定的响应分布，但实际中响应会因不可观测的潜在因素而系统地变化，形成同一观察条件下可能的分布流形。

Method: 提出了PerturbDiff，它将建模从单个细胞转移到整个分布上，通过将这些分布作为希尔伯特空间中的点进行嵌入，并定义了一个直接作用于概率分布上的基于扩散的生成过程。这种方法允许PerturbDiff捕捉跨隐藏因子的群体级响应变化。

Result: 基准测试表明，PerturbDiff在单细胞响应预测方面取得了最先进性能，并且对于未曾遇到过的扰动有显著更强的泛化能力。

Conclusion: PerturbDiff提供了一种有效的方法来模拟细胞对不同扰动的响应，尤其适用于存在不可见或复杂环境因素的情况下。此外，它展示了良好的泛化性，为未来的研究提供了坚实的基础。

Abstract: Building Virtual Cells that can accurately simulate cellular responses to perturbations is a long-standing goal in systems biology. A fundamental challenge is that high-throughput single-cell sequencing is destructive: the same cell cannot be observed both before and after a perturbation. Thus, perturbation prediction requires mapping unpaired control and perturbed populations. Existing models address this by learning maps between distributions, but typically assume a single fixed response distribution when conditioned on observed cellular context (e.g., cell type) and the perturbation type. In reality, responses vary systematically due to unobservable latent factors such as microenvironmental fluctuations and complex batch effects, forming a manifold of possible distributions for the same observed conditions. To account for this variability, we introduce PerturbDiff, which shifts modeling from individual cells to entire distributions. By embedding distributions as points in a Hilbert space, we define a diffusion-based generative process operating directly over probability distributions. This allows PerturbDiff to capture population-level response shifts across hidden factors. Benchmarks on established datasets show that PerturbDiff achieves state-of-the-art performance in single-cell response prediction and generalizes substantially better to unseen perturbations. See our project page (https://katarinayuan.github.io/PerturbDiff-ProjectPage/), where code and data will be made publicly available (https://github.com/DeepGraphLearning/PerturbDiff).

</details>


### [125] [Understanding the Curse of Unrolling](https://arxiv.org/abs/2602.19733)
*Sheheryar Mehmood,Florian Knoll,Peter Ochs*

Main category: cs.LG

TL;DR: 本文探讨了算法展开在机器学习中的应用，特别是针对解映射的雅可比矩阵计算时出现的"展开诅咒"现象。通过非渐近分析，揭示了该现象的根源，并提出截断早期迭代和使用暖启动方法可以缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 算法展开在机器学习领域被广泛应用，尤其是在超参数优化和元学习中。尽管在适当条件下，展开法能产生渐近正确的雅可比矩阵，但最近的研究指出，在迭代算法过程中直接求导可能会导致导数迭代初期与真实雅可比矩阵相偏离，这种现象被称为“展开诅咒”。本研究旨在深入理解此现象背后的机制，并探索减轻其负面影响的方法。

Method: 采用非渐近分析方法来解释展开诅咒现象的发生原因，并确定影响其行为的算法因素。此外，研究还展示了通过截断导数计算的早期迭代不仅可以减轻展开诅咒，还能减少内存需求；同时发现，在双层优化中使用暖启动策略会自然地引入一种隐式的截断形式，为解决实际问题提供了有效途径。

Result: 研究表明，通过特定方式截断早期迭代过程确实有助于缓解展开诅咒现象，同时减少了所需的内存资源。实验结果表明，在代表性示例上，所提出的理论发现得到了验证。

Conclusion: 对于算法展开过程中遇到的展开诅咒问题，通过适当的截断技术及暖启动策略能够有效地进行缓解。这些方法不仅有助于提高计算效率，也为进一步优化基于迭代算法的学习模型提供了新的视角。

Abstract: Algorithm unrolling is ubiquitous in machine learning, particularly in hyperparameter optimization and meta-learning, where Jacobians of solution mappings are computed by differentiating through iterative algorithms. Although unrolling is known to yield asymptotically correct Jacobians under suitable conditions, recent work has shown that the derivative iterates may initially diverge from the true Jacobian, a phenomenon known as the curse of unrolling. In this work, we provide a non-asymptotic analysis that explains the origin of this behavior and identifies the algorithmic factors that govern it. We show that truncating early iterations of the derivative computation mitigates the curse while simultaneously reducing memory requirements. Finally, we demonstrate that warm-starting in bilevel optimization naturally induces an implicit form of truncation, providing a practical remedy. Our theoretical findings are supported by numerical experiments on representative examples.

</details>


### [126] [Bayesian Meta-Learning with Expert Feedback for Task-Shift Adaptation through Causal Embeddings](https://arxiv.org/abs/2602.19788)
*Lotta Mäkinen,Jorge Loría,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出了一种基于因果关系的贝叶斯元学习方法，通过利用任务间潜在的因果嵌入来提高对分布外任务的适应性，减少负迁移，并在仿真和真实临床预测场景中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的元学习方法在处理新的同分布任务时表现良好，但在遇到分布外的任务时容易出现负迁移现象，即从源任务学到的知识反而对目标任务产生负面影响。为了克服这一问题，研究者提出了一种考虑因果关系的方法，旨在促进基于机制相似性的知识迁移而非表面相关性。

Method: 该研究引入了一种因果感知的贝叶斯元学习技术，它依赖于预先计算好的、反映不同任务间因果关系的潜隐式表示（即因果嵌入）。这种方法允许根据专家提供的关于源任务与目标任务之间因果相似性的噪声判断来进行有限的数据访问条件下的模型调整。

Result: 理论分析表明，通过条件化这些因果嵌入可以控制先验不匹配的问题并减轻任务转移过程中的负迁移效应。实验结果也显示，在控制实验及大规模现实世界临床疾病跨域预测应用中，所提方法能够有效降低负迁移的发生率，同时提升模型对于未知或未见任务类型的适应能力。

Conclusion: 本研究提出的因果意识贝叶斯元学习框架成功地解决了传统方法在面对分布外任务时易遭受负迁移影响的问题，为开发更加健壮且具有良好泛化性能的学习系统提供了新思路。

Abstract: Meta-learning methods perform well on new within-distribution tasks but often fail when adapting to out-of-distribution target tasks, where transfer from source tasks can induce negative transfer. We propose a causally-aware Bayesian meta-learning method, by conditioning task-specific priors on precomputed latent causal task embeddings, enabling transfer based on mechanistic similarity rather than spurious correlations. Our approach explicitly considers realistic deployment settings where access to target-task data is limited, and adaptation relies on noisy (expert-provided) pairwise judgments of causal similarity between source and target tasks. We provide a theoretical analysis showing that conditioning on causal embeddings controls prior mismatch and mitigates negative transfer under task shift. Empirically, we demonstrate reductions in negative transfer and improved out-of-distribution adaptation in both controlled simulations and a large-scale real-world clinical prediction setting for cross-disease transfer, where causal embeddings align with underlying clinical mechanisms.

</details>


### [127] [Stop Preaching and Start Practising Data Frugality for Responsible Development of AI](https://arxiv.org/abs/2602.19789)
*Sophia N. Wilson,Guðrún Fjóla Guðmundsdóttir,Andrew Millard,Raghavendra Selvan,Sebastian Mair*

Main category: cs.LG

TL;DR: 本文认为机器学习社区需要从倡导转向实践数据节俭，以实现负责任的人工智能发展。通过估计ImageNet-1K使用相关的能源消耗和碳排放，并提供实证证明数据节俭既实用又有益，最后提出具体建议来促进AI的负责任开发。


<details>
  <summary>Details</summary>
Motivation: 面对日益增长的数据集导致性能提升逐渐减少、能源使用与碳排放增加的问题，文章旨在强调数据节俭方法的重要性，并指出尽管对这种方法的认识有所提高，但其实际采用仍不足。

Method: 文章首先估算ImageNet-1K下游使用的能耗与碳排放量；接着，通过展示基于coreset的子集选择方法能够显著降低训练时的能耗同时保持准确性且减少数据集偏见，提供了数据节俭可行性和益处的实证证据。

Result: 研究结果表明，采用coreset为基础的方法选择数据子集可以大幅度减少训练过程中的能量消耗，而不会显著影响模型的准确性，同时也能够减轻数据集偏差问题。

Conclusion: 为了推进人工智能负责任的发展，必须缩小在提倡与实践中关于数据节约之间的差距。文章为此提出了将数据节约从理论上的讨论转化为实际行动的具体建议。

Abstract: This position paper argues that the machine learning community must move from preaching to practising data frugality for responsible artificial intelligence (AI) development. For long, progress has been equated with ever-larger datasets, driving remarkable advances but now yielding increasingly diminishing performance gains alongside rising energy use and carbon emissions. While awareness of data frugal approaches has grown, their adoption has remained rhetorical, and data scaling continues to dominate development practice. We argue that this gap between preach and practice must be closed, as continued data scaling entails substantial and under-accounted environmental impacts. To ground our position, we provide indicative estimates of the energy use and carbon emissions associated with the downstream use of ImageNet-1K. We then present empirical evidence that data frugality is both practical and beneficial, demonstrating that coreset-based subset selection can substantially reduce training energy consumption with little loss in accuracy, while also mitigating dataset bias. Finally, we outline actionable recommendations for moving data frugality from rhetorical preach to concrete practice for responsible development of AI.

</details>


### [128] [Drift Localization using Conformal Predictions](https://arxiv.org/abs/2602.19790)
*Fabian Hinder,Valerie Vaquet,Johannes Brinkrolf,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种基于保形预测的新方法来解决概念漂移定位问题，特别是在高维、低信号环境中现有局部测试方案可能失效的情况下。通过在最先进的图像数据集上进行实验，展示了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的概念漂移定位方法在处理高维低信号数据时存在局限性，因此需要一种新的方法来更有效地识别受漂移影响的样本。

Method: 采用保形预测的方法，与依赖局部测试的传统方案不同，为概念漂移定位提供了一个全新的视角。

Result: 所提出的方法在最新的图像数据集上进行了测试，并证明了其相对于传统方法的优势。

Conclusion: 基于保形预测的概念漂移定位新方法在高维低信号场景下表现良好，为监测和理解概念漂移提供了有效工具。

Abstract: Concept drift -- the change of the distribution over time -- poses significant challenges for learning systems and is of central interest for monitoring. Understanding drift is thus paramount, and drift localization -- determining which samples are affected by the drift -- is essential. While several approaches exist, most rely on local testing schemes, which tend to fail in high-dimensional, low-signal settings. In this work, we consider a fundamentally different approach based on conformal predictions. We discuss and show the shortcomings of common approaches and demonstrate the performance of our approach on state-of-the-art image datasets.

</details>


### [129] [Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing](https://arxiv.org/abs/2602.19805)
*Wall Kim,Chaeyoung Song,Hanul Kim*

Main category: cs.LG

TL;DR: 提出了一种名为Decision MetaMamba (DMM)的新结构，通过采用基于密集层的序列混合器和修改位置结构来解决Mamba模型在离线强化学习中的信息丢失问题。实验表明DMM在多种RL任务中表现出色且参数量小，具有很强的实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在离线强化学习（offline RL）中受到关注，但其选择机制经常导致当RL序列中的关键步骤被省略时出现不利影响。为了解决这些问题并防止由于选择性扫描和残差门控引起的信息损失，提出了新的解决方案。

Method: 本文介绍了一种称为Decision MetaMamba (DMM)的新架构，它用基于密集层的序列混合器替换了Mamba的令牌混合器，并修改了位置结构以保持局部信息。这种设计允许在进行Mamba操作之前考虑所有通道的同时执行序列混合。

Result: 广泛的实验证明了DMM在不同强化学习任务中达到了最先进水平的表现。此外，DMM还以其紧凑的参数量实现了这些结果，显示出在实际应用中的强大潜力。

Conclusion: Decision MetaMamba (DMM)作为一种简单而有效的改进方法，在克服Mamba模型局限性方面取得了显著成效，不仅提高了性能而且减少了模型复杂度，为离线强化学习提供了新的研究方向。

Abstract: Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications.

</details>


### [130] [Generalized Random Direction Newton Algorithms for Stochastic Optimization](https://arxiv.org/abs/2602.19893)
*Soumen Pachal,Prashanth L. A.,Shalabh Bhatnagar,Avinash Achar*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机方向随机逼近(RDSA)的广义Hessian估计器族，仅使用噪声函数测量。随着函数测量数量增加，估计偏差降低。分析了估计器的渐近无偏性以及结合这些估计器的随机牛顿方法的收敛性，并通过数值实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过开发新的Hessian矩阵估计技术来改善在仅有噪声函数值可用的情况下优化算法的表现。

Method: 采用随机方向随机逼近方法构建不同形式的广义Hessian估计器；对所提出的估计器进行了渐近与非渐近收敛性分析；实施了数值实验以测试理论发现的有效性。

Result: 展示了具有更多函数测量值的估计器表现出更低阶的估计偏差；证明了估计器的渐近无偏性质；提供了关于使用所提议估计器的随机牛顿法的收敛性分析。

Conclusion: 所提出的基于RDSA的广义Hessian估计器能够有效减少估计偏差，并且当应用于随机牛顿方法时表现良好，为处理含噪目标函数优化问题提供了一个有力工具。

Abstract: We present a family of generalized Hessian estimators of the objective using random direction stochastic approximation (RDSA) by utilizing only noisy function measurements. The form of each estimator and the order of the bias depend on the number of function measurements. In particular, we demonstrate that estimators with more function measurements exhibit lower-order estimation bias. We show the asymptotic unbiasedness of the estimators. We also perform asymptotic and non-asymptotic convergence analyses for stochastic Newton methods that incorporate our generalized Hessian estimators. Finally, we perform numerical experiments to validate our theoretical findings.

</details>


### [131] [DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning](https://arxiv.org/abs/2602.19895)
*Zhongwei Wan,Yun Shen,Zhihao Dou,Donghao Zhou,Yu Zhang,Xin Wang,Hui Shen,Jing Xiong,Chaofan Tao,Zixuan Zhong,Peizhou Huang,Mi Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为DSDR的双尺度多样性正则化强化学习框架，旨在改进大型语言模型（LLM）推理过程中的探索问题。该方法通过全局和局部两个层面促进推理路径的多样性，既保证了不同解决方案模式的探索，也防止了每个模式内的熵塌陷。理论分析与实验结果均表明，DSDR能够保持最优正确性，并在基于群体的优化中提供有效的学习信号。


<details>
  <summary>Details</summary>
Motivation: 现有的增强学习方法用于提高大型语言模型推理时，往往存在探索不足的问题，导致策略倾向于收敛于少数几种推理模式，过早停止深度探索。传统熵正则化仅引入局部随机性，无法诱导有意义的路径级多样性，这使得基于群体的策略优化过程中学习信号弱且不稳定。

Method: 提出了DSDR（Dual-Scale Diversity Regularization），一种新的强化学习框架，它将LLM推理中的多样性分解为全局和耦合两部分。在全球层面上，DSDR鼓励正确的推理轨迹之间具有多样性以探索不同的解决方案；在局部层面上，则对正确的轨迹应用长度不变、词元级别的熵正则化，这样既能避免每个模式内发生熵塌陷又保持了正确性。此外，通过一个全局到局部的分配机制连接这两个尺度，对于更独特的正确轨迹加强局部正则化。

Result: 理论分析显示，在有界正则化条件下，DSDR可以维持最优正确性，并在基于群体的优化中持续提供信息量丰富的学习信号。同时，给出了一种原则性的全局至局部耦合规则。实验方面，在多个推理基准测试上，DSDR展示了在准确性和pass@k指标上的持续提升，强调了双尺度多样性对于RLVR中深入探索的重要性。

Conclusion: 研究提出并验证了一种新颖的强化学习框架——DSDR，有效解决了现有方法在增强LLM推理能力时遇到的探索不足问题。通过促进推理路径的双重尺度多样性，DSDR不仅提高了模型解决问题的能力，还增强了学习过程的稳定性和效率。

Abstract: Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.

</details>


### [132] [De novo molecular structure elucidation from mass spectra via flow matching](https://arxiv.org/abs/2602.19912)
*Ghaith Mqawass,Tuan Le,Fabian Theis,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 本文介绍了一种名为MSFlow的两阶段编码-解码流匹配生成模型，用于从小分子的质量谱中推断出分子结构。该模型在将质量谱转换为对应的分子表示方面达到了最先进的性能，准确率高达45%，比现有最佳方法提高了十四倍。


<details>
  <summary>Details</summary>
Motivation: 质谱法是一种强大的工具，能够识别复杂的样品中的分子结构，但将光谱转化为完整的分子结构是一个未充分定义且困难的问题。解决这个问题对于生物研究、发现新的代谢物以及推进多个领域的化学研究至关重要。

Method: 研究人员开发了MSFlow，这是一个两阶段的编码-解码流匹配生成模型。第一阶段采用公式限定的Transformer模型将质量谱编码成一个连续而具有化学信息的嵌入空间；第二阶段则训练一个解码器流匹配模型，从质量谱的潜在嵌入中重建分子。

Result: 通过严格的评估显示，MSFlow可以准确地将高达45%的分子质量谱转换为其相应的分子表示，这一成就相较于当前最先进水平提高了最多达十四倍。此外，还提供了消融研究来证明使用信息保持的分子描述符对质量谱编码的重要性，并支持采用基于离散流的解码器。

Conclusion: MSFlow显著提高了从小分子质量谱到分子结构解析的准确性，为生物学见解、新代谢物发现及跨领域化学研究的进步提供了强有力的支持。此外，非商业用户可以在GitHub上获得训练好的MSFlow版本。

Abstract: Mass spectrometry is a powerful and widely used tool for identifying molecular structures due to its sensitivity and ability to profile complex samples. However, translating spectra into full molecular structures is a difficult, under-defined inverse problem. Overcoming this problem is crucial for enabling biological insight, discovering new metabolites, and advancing chemical research across multiple fields. To this end, we develop MSFlow, a two-stage encoder-decoder flow-matching generative model that achieves state-of-the-art performance on the structure elucidation task for small molecules. In the first stage, we adopt a formula-restricted transformer model for encoding mass spectra into a continuous and chemically informative embedding space, while in the second stage, we train a decoder flow matching model to reconstruct molecules from latent embeddings of mass spectra. We present ablation studies demonstrating the importance of using information-preserving molecular descriptors for encoding mass spectra and motivate the use of our discrete flow-based decoder. Our rigorous evaluation demonstrates that MSFlow can accurately translate up to 45 percent of molecular mass spectra into their corresponding molecular representations - an improvement of up to fourteen-fold over the current state-of-the-art. A trained version of MSFlow is made publicly available on GitHub for non-commercial users.

</details>


### [133] [Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models](https://arxiv.org/abs/2602.19926)
*Jin Liu,Yinbin Miao,Ning Xi,Junkang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为LA-LoRA的新方法，解决了在差分隐私联邦学习环境中使用低秩适应（LoRA）进行大规模视觉模型和语言模型微调时遇到的性能下降问题。通过解耦梯度交互并使客户端之间的更新方向一致，LA-LoRA提高了在严格隐私限制下的鲁棒性，并在Swin Transformer和RoBERTa模型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私联邦学习(DPFL)环境下对大型视觉模型(LVMs)和大型语言模型(LLMs)进行微调面临基本的隐私-效用权衡问题。直接将低秩适应(LoRA)应用于DPFL设置会导致性能下降，特别是在LVMs中。

Method: 提出了LA-LoRA（本地交替LoRA），一种新方法，旨在解决三个关键挑战：1) 由于两个非对称低秩矩阵同时更新导致的梯度耦合；2) 在差分隐私下复合噪声放大；3) 参数空间中全局聚合模型的尖锐度。LA-LoRA通过解耦梯度交互并调整客户机间的更新方向来增强在严格隐私约束下的鲁棒性。

Result: 广泛的实验表明，LA-LoRA在Swin Transformer和RoBERTa模型上达到了最先进(SOTA)的表现，展示了对抗DP噪声的强大能力和对LVMs及LLMs的广泛适用性。例如，在Tiny-ImageNet数据集上对Swin-B模型进行微调且隐私预算ε=1的情况下，LA-LoRA比最佳基线RoLoRA高出16.83%的测试准确率。

Conclusion: LA-LoRA不仅有效解决了现有LoRA方法在差分隐私联邦学习场景下的局限性，而且显著提升了模型性能与鲁棒性，为未来的研究提供了新的思路。

Abstract: Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\textbf{L}ocal \textbf{A}lternating \textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($ε= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\% in test accuracy. Code is provided in \repolink.

</details>


### [134] [Expanding the Role of Diffusion Models for Robust Classifier Training](https://arxiv.org/abs/2602.19931)
*Pin-Han Huang,Shang-Tse Chen,Hsuan-Tien Lin*

Main category: cs.LG

TL;DR: 本研究探索了将扩散模型的内部表示作为对抗训练（AT）中的辅助学习信号，结果表明这可以提高分类器的鲁棒性，并且扩散表示与合成数据在塑造表示中起互补作用。


<details>
  <summary>Details</summary>
Motivation: 除了生成合成数据外，探究扩散模型的内部表示是否能够为鲁棒分类器训练提供更多益处。

Method: 通过系统实验分析扩散模型提供的多样化且部分鲁棒的表示，并在对抗训练过程中明确地将这些扩散表示作为辅助学习信号整合进去。

Result: 实验证明，在AT中加入扩散模型表示始终能提高不同设置下的鲁棒性；同时，扩散模型和合成数据在形成更解耦特征方面发挥着互补作用。

Conclusion: 联合利用扩散模型表示和合成数据在对抗训练中被证明是有效的策略，有助于提升图像分类器的鲁棒性。

Abstract: Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.

</details>


### [135] [DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models](https://arxiv.org/abs/2602.19945)
*Jin Liu,Yinbin Miao,Ning Xi,Junkang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的优化器DP-FedAdamW，旨在解决在差分隐私联邦学习(DPFL)中直接应用AdamW时遇到的三个主要问题：数据异质性和隐私噪声共同放大了二阶矩估计器的方差、差分隐私扰动导致的二阶矩估计偏差以及对局部过拟合敏感度增加引发的客户端漂移。通过稳定二阶矩方差、消除差分隐私引起的偏差，并使本地更新与全局下降方向一致来抑制客户端漂移，从而恢复DP下的AdamW表现。理论分析表明，该方法能够提供一个无偏的二阶矩估计器，并在线性加速收敛率方面取得进步，同时提供了更严格的(ε,δ)-差分隐私保证。实验结果证明了DP-FedAdamW在语言和视觉Transformer及ResNet-18上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习（FL）中，在保持差分隐私的同时平衡收敛效率和鲁棒性是一个核心挑战。虽然AdamW能加速大规模模型的训练与微调，但在差分隐私联邦学习（DPFL）环境下直接使用它存在若干问题：数据异质性和隐私噪声加剧了二阶矩估计器的方差；差分隐私扰动引入了对二阶矩估计的偏差；并且增加了对于局部过拟合的敏感度，恶化了客户端漂移现象。

Method: 为了解决这些问题，作者提出了名为DP-FedAdamW的新优化器。该优化器通过稳定二阶矩估计器的方差、去除由差分隐私引入的偏差以及调整局部更新以匹配全局下降方向来减少客户端漂移，从而改善了DPFL场景下AdamW的表现。

Result: 理论分析显示，DP-FedAdamW能够实现无偏的二阶矩估计，并且在没有任何异质性假设的情况下证明了线性加速的收敛速度。此外，还提供了更严格的(ε,δ)-差分隐私保证。实验上，在Tiny-ImageNet (Swin-Base, ε=1) 数据集上，DP-FedAdamW相比当前最优技术提高了5.83%。

Conclusion: 研究表明，DP-FedAdamW作为一种基于AdamW的优化器，有效解决了DPFL环境中存在的关键问题，不仅提升了模型训练过程中的性能，而且增强了隐私保护能力。

Abstract: Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity to local overfitting, worsening client drift. We propose DP-FedAdamW, the first AdamW-based optimizer for DPFL. It restores AdamW under DP by stabilizing second-moment variance, removing DP-induced bias, and aligning local updates to the global descent to curb client drift. Theoretically, we establish an unbiased second-moment estimator and prove a linearly accelerated convergence rate without any heterogeneity assumption, while providing tighter $(\varepsilon,δ)$-DP guarantees. Our empirical results demonstrate the effectiveness of DP-FedAdamW across language and vision Transformers and ResNet-18. On Tiny-ImageNet (Swin-Base, $\varepsilon=1$), DP-FedAdamW outperforms the state-of-the-art (SOTA) by 5.83\%. The code is available in Appendix.

</details>


### [136] [Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks](https://arxiv.org/abs/2602.19980)
*Itamar Trainin,Shauli Ravfogel,Omri Abend,Amir Feder*

Main category: cs.LG

TL;DR: 研究发现，非自回归模型（NAR）在处理前瞻任务时，通过利用未来标记反向解码来避免学习复杂的遍历机制，从而比自回归模型（AR）需要更少的训练样本和更浅层的架构即可达到相同或更好的准确率。


<details>
  <summary>Details</summary>
Motivation: 探讨自回归(AR)与非自回归(NAR)模型在处理前瞻任务时的不同机制，特别是解决规划问题的能力差异。

Method: 通过要求模型提前计划以得出正确结论，分析了AR与NAR两种范式在解决问题上的根本区别，并通过机制分析训练和推理动态展示了NAR模型如何利用未来标记进行反向解码。

Result: 结果显示，虽然AR和NAR模型都能在前瞻任务上实现完美准确度，但NAR模型只需要指数级更少的训练示例以及更浅的架构；相比之下，AR模型往往需要特定课程调整才能收敛。

Conclusion: 本研究表明，在处理需要多步前瞻的规划任务时，NAR模型提供了一种效率更高的解决方案，它能够有效利用反向生成的优势来简化复杂决策过程。

Abstract: While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct conclusion, we analyze how these two paradigms fundamentally differ in their approach to the problem. We identify a critical asymmetry in planning problems: while forward generation requires complex lookahead at branching junctions, reverse generation is often deterministic. This asymmetry creates an opportunity for NAR models. Through mechanistic analysis of training and inference dynamics, we demonstrate that NAR models learn to solve planning tasks by utilizing future tokens to decode backwards, avoiding the need to learn complex traversal mechanisms entirely. Consequently, we report that both AR and NAR models are able to achieve perfect accuracy on the lookahead task. However, NAR models require exponentially fewer training examples and shallower architectures compared to AR models, which often fail to converge without specific curriculum adjustments.

</details>


### [137] [A Computationally Efficient Multidimensional Vision Transformer](https://arxiv.org/abs/2602.19982)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.LG

TL;DR: 本文提出了一种基于张量余弦积的新型视觉变换器框架（TCP-ViT），通过利用图像数据中的多线性结构和余弦变换的正交性，实现了高效的注意力机制和结构化的特征表示。实验表明该方法在减少参数的同时保持了竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉变换器虽然在计算机视觉任务中取得了最先进的性能，但其实际部署受到高计算和内存成本的限制。

Method: 本文介绍了一种基于张量余弦积(Cproduct)的新框架来构建视觉变换器。研究开发了张量余弦积的理论基础，分析了它的代数性质，并将其整合到新的基于Cproduct的视觉变换器架构(TCP-ViT)中。

Result: 在标准分类和分割基准上的数值实验表明，所提出的方法达到了一致的1/C参数减少（其中C是通道数量），同时保持了竞争性的准确性。

Conclusion: 通过引入基于张量余弦积的新框架，本文为视觉变换器提供了一个有效降低计算和内存成本的解决方案，同时维持了模型的准确度。

Abstract: Vision Transformers have achieved state-of-the-art performance in a wide range
  of computer vision tasks, but their practical deployment is limited by high
  computational and memory costs. In this paper, we introduce a novel tensor-based
  framework for Vision Transformers built upon the Tensor Cosine Product
  (Cproduct). By exploiting multilinear structures inherent in image data and the
  orthogonality of cosine transforms, the proposed approach enables efficient
  attention mechanisms and structured feature representations. We develop the
  theoretical foundations of the tensor cosine product, analyze its algebraic
  properties, and integrate it into a new Cproduct-based Vision Transformer
  architecture (TCP-ViT). Numerical experiments on standard classification and
  segmentation benchmarks demonstrate that the proposed method achieves a uniform
  1/C parameter reduction (where C is the number of channels) while
  maintaining competitive accuracy.

</details>


### [138] [Learning Discriminative and Generalizable Anomaly Detector for Dynamic Graph with Limited Supervision](https://arxiv.org/abs/2602.20019)
*Yuxing Tian,Yiyan Qi,Fengran Mo,Weixu Zhang,Jian Guo,Jian-Yun Nie*

Main category: cs.LG

TL;DR: 本文提出了一种新的动态图异常检测框架，该框架能够从正常/未标记的数据中学习区分边界，并在有标签的异常数据可用时利用这些数据，同时不过度拟合，以保持对未知异常的良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的动态图异常检测方法要么是无监督的，导致边界模糊；要么是半监督的，容易过度拟合有限的标记异常数据，且对于未见过的异常泛化能力差。为了解决这个问题，研究者们探索了如何从正常或未标记数据中学习一个具有区分性的边界，同时在有标记的异常数据时有效利用这些信息，而不牺牲对未知异常的泛化性能。

Method: 提出了一个有效的、可推广的且模型无关的框架，包含三个主要部分：1）残差表示编码，用于捕捉当前交互与其历史上下文之间的偏差，提供与异常相关的信号；2）限制损失函数，通过两个共中心超球体限定正常表示范围，确保尺度一致的同时使异常点分离；3）双边界优化策略，使用正则流建模的正常似然分布来学习一个区分性和鲁棒性强的边界。

Result: 广泛的实验表明，在多样化的评估设置下，所提出的框架表现出优越性。

Conclusion: 通过引入创新的方法论来解决动态图异常检测中的挑战，这项工作不仅提高了检测准确性，还增强了对未知异常的泛化能力。

Abstract: Dynamic graph anomaly detection (DGAD) is critical for many real-world applications but remains challenging due to the scarcity of labeled anomalies. Existing methods are either unsupervised or semi-supervised: unsupervised methods avoid the need for labeled anomalies but often produce ambiguous boundary, whereas semi-supervised methods can overfit to the limited labeled anomalies and generalize poorly to unseen anomalies. To address this gap, we consider a largely underexplored problem in DGAD: learning a discriminative boundary from normal/unlabeled data, while leveraging limited labeled anomalies \textbf{when available} without sacrificing generalization to unseen anomalies. To this end, we propose an effective, generalizable, and model-agnostic framework with three main components: (i) residual representation encoding that capture deviations between current interactions and their historical context, providing anomaly-relevant signals; (ii) a restriction loss that constrain the normal representations within an interval bounded by two co-centered hyperspheres, ensuring consistent scales while keeping anomalies separable; (iii) a bi-boundary optimization strategy that learns a discriminative and robust boundary using the normal log-likelihood distribution modeled by a normalizing flow. Extensive experiments demonstrate the superiority of our framework across diverse evaluation settings.

</details>


### [139] [BarrierSteer: LLM Safety via Learning Barrier Steering](https://arxiv.org/abs/2602.20102)
*Thanh Q. Tran,Arun Verma,Kiwan Wong,Bryan Kian Hsiang Low,Daniela Rus,Wei Xiao*

Main category: cs.LG

TL;DR: 本文提出了一种名为BarrierSteer的新框架，该框架通过将学习到的非线性安全约束直接嵌入模型的潜在表示空间来形式化响应安全性。它使用基于控制屏障函数(CBFs)的引导机制，在推理过程中高效且精确地检测和防止不安全的响应轨迹。实验表明，BarrierSteer能够显著降低对抗攻击成功率、减少不安全生成，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在各种任务中表现出色，但它们容易受到对抗性攻击并产生不安全内容的问题仍然是部署的主要障碍，特别是在高风险场景下。解决这一挑战需要既实用又具备严格理论支持的安全机制。

Method: 本文引入了BarrierSteer框架，该框架通过将学习到的非线性安全约束直接嵌入到模型的潜在表示空间中来形式化响应的安全性。BarrierSteer采用基于控制屏障函数(CBFs)的导向机制，在推断时以高精度有效地检测和阻止不安全响应轨迹。通过有效合并多个安全约束而不改变底层LLM参数，保持了模型原有的能力和性能。

Result: 实验结果表明，BarrierSteer大大降低了对抗攻击的成功率，减少了不安全的内容生成，并且表现优于现有的方法。此外，研究还提供了理论依据，证明在潜在空间应用CBFs是一种有原则且计算效率高的确保安全性的方法。

Conclusion: BarrierSteer为增强大型语言模型的安全性提供了一个新颖而有效的解决方案，通过在模型的潜在空间内实施非线性安全约束，实现了对不安全响应的有效管理和控制。

Abstract: Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.

</details>


### [140] [Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds](https://arxiv.org/abs/2602.20111)
*Ezra Edelman,Surbhi Goel*

Main category: cs.LG

TL;DR: 研究了在对抗注入模型中的在线学习，其中数据流主要从未知分布中独立同分布抽取，但也可能混入对手选择的实例。本文证明了对于VC维1的情况存在Ω(√T)的下界，并引入了一个基于鲁棒见证的框架来改进算法性能。


<details>
  <summary>Details</summary>
Motivation: 探索在对抗注入模型中，在线学习者如何在面对未知分布的数据和潜在的恶意干扰时仍能保持高效的学习表现。

Method: 提出了一种基于“鲁棒见证”的潜在框架，该框架利用小规模标记样本集来验证预测同时抵抗对抗性污染的影响。通过两种组合维度——推理维度和证书维度（新提出的放松条件）——实现了该框架的具体应用。

Result: 证明了当VC维为1时，存在Ω(√T)的下界，表明不同信息状态下存在显著差异；并且对于具有特定推理维度k的类，达到了~O(T^(1-1/k))的综合误差；特别地，发现二维实数空间中的半空间具有3的证书维度，从而首次得到了这类问题的一个与分布无关的~O(T^(2/3))界。

Conclusion: 本文工作揭示了在对抗环境中，拥有对底层分布的先验知识与没有这种知识之间存在根本性的差距，并且提出了新的方法论以改善在缺乏完整信息情况下的学习效果。

Abstract: We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\ from an unknown distribution $\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\tilde{O}(\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.
  We resolve this question by proving a matching $Ω(\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \emph{inference dimension}, yielding combined error $\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.

</details>


### [141] [Adaptation to Intrinsic Dependence in Diffusion Language Models](https://arxiv.org/abs/2602.20126)
*Yunxiao Zhao,Changxiao Cai*

Main category: cs.LG

TL;DR: 本文提出了一种与分布无关的解码时间表方法，该方法能够适应目标数据分布（未知）的依赖结构，而无需任何先验知识或超参数调整。通过随机化每次迭代中揭示的标记数量，作者展示了对于两种特定参数选择，采样收敛保证分别以$\widetilde O(\mathsf{TC}/K)$和$\widetilde O(\mathsf{DTC}/K)$缩放，其中$K$是迭代次数，$\mathsf{TC}$和$\mathsf{DTC}$代表了目标分布的总相关性和双重总相关性。这些结果对实际并行抽样方案$K<L$（$L$为标记序列长度）下显著提高了先前的收敛理论，并且对于低复杂度分布提供了实质性的抽样加速。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型(DLMs)在实证上取得了成功，但关于解码时间表——即在采样过程中指定被揭开的标记顺序和大小的方式——如何影响生成质量的理论理解仍然有限。因此，研究旨在探索一种新的解码策略，该策略不仅能够适应数据内在的依赖结构，还能避免对先验知识或超参数调节的需求。

Method: 提出的方法是一种随机化的解码时间表，它不预先设定每次迭代中要揭露的标记数量，而是根据目标数据分布的未知依赖结构自适应地决定。这种方法不需要任何先验信息或额外的超参数调整。

Result: 研究表明，所提出的随机化解码时间表可以在两个具体的参数配置下提供改进的采样收敛保障。对于具有较低复杂度的数据分布，这种新方法可以显著加快抽样过程。此外，该方法在实践中相关的并行抽样情形下也表现出色，即当迭代次数少于标记序列长度时。

Conclusion: 本研究揭示了扩散语言模型对于内在数据结构的适应性，并强调了在推理计划设计中采用随机化解码规模的好处。这为理解及优化扩散语言模型的性能提供了新的视角。

Abstract: Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\widetilde O(\mathsf{TC}/K)$ and $\widetilde O(\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\mathsf{TC}$ and $\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.

</details>


### [142] [Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data](https://arxiv.org/abs/2602.20152)
*Zhenyao Ma,Yue Liang,Dongxu Li*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习框架——行为学习（BL），它可以从数据中学习可解释和可识别的优化结构，适用于从单个优化问题到分层组合的各种情况。BL不仅统一了预测性能、内在可解释性和可识别性，还具有广泛的科学领域应用潜力。


<details>
  <summary>Details</summary>
Motivation: 受到行为科学启发，旨在创建一个能够从数据中学习可解释和可识别优化结构的新颖通用机器学习框架。这样的框架可以广泛应用于涉及优化的科学领域。

Method: 通过参数化由本质上可解释模块构建的复合效用函数来实现，每个模块都可以被写成符号形式作为效用最大化问题(UMP)。BL支持从单一UMP到层次组合的不同架构，其中平滑且单调的变体(IBL)保证了可识别性。

Result: 理论方面，确立了BL的通用逼近性质，并分析了IBL的M-估计特性。实验表明，BL展示了强大的预测性能、内在可解释性以及对高维数据的可扩展性。

Conclusion: 行为学习(BL)为从数据中提取优化结构提供了一个新颖而有效的途径，同时保持了良好的预测性能与内在可解释性，适用于多种科学领域中的优化问题。

Abstract: Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [143] [Vibe Coding on Trial: Operating Characteristics of Unanimous LLM Juries](https://arxiv.org/abs/2602.18492)
*Muhammad Aziz Ullah,Abdul Serwadda*

Main category: cs.DB

TL;DR: 研究了使用大型语言模型（LLM）委员会来审查由其他模型生成的SQL查询的安全性。通过基准测试确定了最强的六个模型，并基于这些模型构建了一致性委员会，以减少错误接受率同时保持高通过率。结果表明，小规模的一致性委员会在确保安全性方面表现良好，但委员会的具体组成对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在编程方面的表现越来越出色，开发者可以使用自然语言描述需求并让工具自动生成代码初稿。然而，缺乏一种可靠的方法来判断哪些由模型生成的查询可以直接接受而不必人工检查。本研究旨在探索如何利用LLM委员会来进行这一审核步骤，特别是在安全第一的应用场景中，错误接受比错误拒绝更需避免。

Method: 首先对15个开源模型在82项MySQL文本到SQL任务上进行了基准测试，采用基于执行的协议来获得一个清晰的基线，找出其中最强的六个模型。然后，从这六个模型中构建了大小为1至6的不同一致性委员会，只有当所有成员都认为给定的SQL查询正确时才予以接受。

Result: 结果显示，单一模型作为裁判的表现参差不齐；而由强模型组成的小规模一致性委员会能够有效降低错误接受率，同时仍能通过大量有效的查询；委员会的具体构成对其性能有着重要影响。

Conclusion: 本研究表明，使用由几个强大模型组成的一致性委员会可以作为一种有效的方法来提高自动代码生成过程中SQL查询的安全性，但需要仔细考虑委员会成员的选择。

Abstract: Large Language Models (LLMs) are now good enough at coding that developers can describe intent in plain language and let the tool produce the first code draft, a workflow increasingly built into tools like GitHub Copilot, Cursor, and Replit. What is missing is a reliable way to tell which model written queries are safe to accept without sending everything to a human. We study the application of an LLM jury to run this review step. We first benchmark 15 open models on 82 MySQL text to SQL tasks using an execution grounded protocol to get a clean baseline of which models are strong. From the six best models we build unanimous committees of sizes 1 through 6 that see the prompt, schema, and candidate SQL and accept it only when every member says it is correct. This rule matches safety first deployments where false accepts are more costly than false rejects. We measure true positive rate, false positive rate and Youden J and we also look at committees per generator. Our results show that single model judges are uneven, that small unanimous committees of strong models can cut false accepts while still passing many good queries, and that the exact committee composition matters significantly.

</details>


### [144] [RDBLearn: Simple In-Context Prediction Over Relational Databases](https://arxiv.org/abs/2602.18495)
*Yanlin Zhang,Linjie Xu,Quan Gan,David Wipf,Minjie Wang*

Main category: cs.DB

TL;DR: 该论文提出了一种将表格上下文学习（ICL）扩展到关系型预测的方法，通过自动特征化每个目标行并使用关系聚合其链接记录来实现。开发了名为RDBLearn的工具包，它在多个数据集上表现出色，有时甚至优于强大的监督基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的表格上下文学习方法主要针对单一平面表中的小样本适应新任务，而现实世界中许多任务的数据分布在多个关联表中，需要一种能够处理这种关系型数据库的新方法。

Method: 本文介绍的方法是首先对每个目标行进行自动特征化，利用与其相关联的记录进行关系聚合，生成一个扩增后的表格；然后使用现成的表格基础模型在这个扩增后的表格上运行。这一过程被封装在一个易于使用的工具包RDBLearn中，支持以scikit-learn风格的估计器接口轻松切换不同的表格ICL后端。

Result: 实验结果表明，在RelBench和4DBInfer等多个基准数据集上，RDBLearn的表现超过了评估中的其他基础模型方法，并且在某些情况下甚至优于那些经过专门训练或微调的强大监督学习基线。

Conclusion: 本研究成功地将表格上下文学习扩展到了关系型数据环境中，为直接从复杂的关系数据库结构中进行高效预测提供了一个新的解决方案。

Abstract: Recent advances in tabular in-context learning (ICL) show that a single pretrained model can adapt to new prediction tasks from a small set of labeled examples, avoiding per-task training and heavy tuning. However, many real-world tasks live in relational databases, where predictive signal is spread across multiple linked tables rather than a single flat table. We show that tabular ICL can be extended to relational prediction with a simple recipe: automatically featurize each target row using relational aggregations over its linked records, materialize the resulting augmented table, and run an off-the-shelf tabular foundation model on it. We package this approach in \textit{RDBLearn} (https://github.com/HKUSHXLab/rdblearn), an easy-to-use toolkit with a scikit-learn-style estimator interface that makes it straightforward to swap different tabular ICL backends; a complementary agent-specific interface is provided as well. Across a broad collection of RelBench and 4DBInfer datasets, RDBLearn is the best-performing foundation model approach we evaluate, at times even outperforming strong supervised baselines trained or fine-tuned on each dataset.

</details>


### [145] [The Human Factor in Data Cleaning: Exploring Preferences and Biases](https://arxiv.org/abs/2602.19368)
*Hazim AbdElazim,Shadman Islam,Mostafa Milani*

Main category: cs.DB

TL;DR: 本研究通过一项控制调查研究，探讨了数据清理过程中的人类判断偏差。研究发现，在数据清理任务中存在框架效应、锚定与调整偏差、代表性启发式等认知偏差机制。此外，参与者在修复数据时倾向于保持数据缺失而不是填充合理值。这些偏差模式在技术经验丰富的参与者和技术多样的工作流程实践中依然存在。


<details>
  <summary>Details</summary>
Motivation: 数据清理通常被视为一个技术预处理步骤，但实际上它严重依赖于人类的判断。为了更好地理解这一过程中的潜在偏差，研究人员设计了一系列基于人口普查情景的任务，以探索数据清理过程中存在的认知偏差。

Method: 研究采用了一项控制调查研究的方法，参与者需要完成错误检测、数据修复和插补以及实体匹配等任务。这些任务的设计灵感来源于具有已知语义有效性的普查场景。

Result: 研究发现了数据清理过程中存在几种系统性的认知偏差机制，包括：由于表面格式差异导致的框架效应、专家提示引起的锚定与调整偏差、典型性但有效的属性组合被频繁标记为错误的代表性启发式偏差。此外，参与者在面对数据修复时表现出对保持数据缺失而非进行合理插值的偏好。

Conclusion: 数据清理中的偏差反映了普遍的认知倾向，而不仅仅是缺乏专业知识的结果。这些发现促使开发更加人性化且能明确区分表示与语义的数据清理系统，并以非规定性方式呈现专家或算法建议，同时支持对非典型但有效案例的反思性评估。

Abstract: Data cleaning is often framed as a technical preprocessing step, yet in practice it relies heavily on human judgment. We report results from a controlled survey study in which participants performed error detection, data repair and imputation, and entity matching tasks on census-inspired scenarios with known semantic validity. We find systematic evidence for several cognitive bias mechanisms in data cleaning. Framing effects arise when surface-level formatting differences (e.g., capitalization or numeric presentation) increase false-positive error flags despite unchanged semantics. Anchoring and adjustment bias appears when expert cues shift participant decisions beyond parity, consistent with salience and availability effects. We also observe the representativeness heuristic: atypical but valid attribute combinations are frequently flagged as erroneous, and in entity matching tasks, surface similarity produces a substantial false-positive rate with high confidence. In data repair, participants show a robust preference for leaving values missing rather than imputing plausible values, consistent with omission bias. In contrast, automation-aligned switching under strong contradiction does not exceed a conservative rare-error tolerance threshold at the population level, indicating that deference to automated recommendations is limited in this setting. Across scenarios, bias patterns persist among technically experienced participants and across diverse workflow practices, suggesting that bias in data cleaning reflects general cognitive tendencies rather than lack of expertise. These findings motivate human-in-the-loop cleaning systems that clearly separate representation from semantics, present expert or algorithmic recommendations non-prescriptively, and support reflective evaluation of atypical but valid cases.

</details>


### [146] [Breaking the Barriers of Database-Agnostic Transactions](https://arxiv.org/abs/2602.19440)
*Toshihiro Suzuki,Hiroyuki Yamada*

Main category: cs.DB

TL;DR: 本文提出了一种新的数据库抽象概念——原子性单元（AU），以解决联邦事务管理中性能优化难和需要模式迁移的问题。通过利用AU，可以充分利用数据库的性能，并在不进行模式迁移的情况下实现高效的元数据分离。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦事务管理方法在数据库抽象层上执行时面临性能优化困难以及需要将应用程序数据及其关联的事务元数据共存于同一记录中的问题，这通常需要对现有数据库进行模式迁移。

Method: 提出了原子性单元（AU）的概念，它允许联邦事务管理更有效地推送数据库操作，并且能够高效地分离事务元数据与应用程序数据。

Result: 实验结果表明，采用AU的ScalarDB实现了显著更好的性能和有效的元数据分离。

Conclusion: 通过引入AU这一新概念，联邦事务管理系统可以在不牺牲性能或要求模式迁移的前提下，更好地处理跨多个数据库的一致性保证。

Abstract: Federated transaction management has long been used as a method to virtually integrate multiple databases from a transactional perspective, ensuring consistency across the databases. Modern approaches manage transactions on top of a database abstraction to achieve database agnosticism; however, these approaches face several challenges. First, managing transactions on top of a database abstraction makes performance optimization difficult because the abstraction hides away the details of underlying databases, such as database-specific capabilities. Additionally, it requires that application data and the associated transaction metadata be colocated in the same record to allow for efficient updates, necessitating a schema migration to run federated transactions on top of existing databases. This paper introduces a new concept in such database abstraction called Atomicity Unit (AU) to address these challenges. AU enables federated transaction management to aggressively pushdown database operations by making use of the knowledge about the scope within which they can perform operations atomically, fully harnessing the performance of the databases. Moreover, AU enables efficient separation of transaction metadata from application data, allowing federated transactions to run on existing databases without requiring a schema migration or significant performance degradation. In this paper, we describe AU, how AU addresses the challenges, and its implementation within ScalarDB, an open-sourced database-agnostic federated transaction manager. We also present evaluation results demonstrating that ScalarDB with AU achieves significantly better performance and efficient metadata separation.

</details>


### [147] [FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing](https://arxiv.org/abs/2602.19490)
*Yongxin Chen,Zhiyuan Jiang,Chao Zhang,Haoran Xu,Shenglin Xu,Jianping Tang,Zheming Li,Peidai Xie,Yongjun Wang*

Main category: cs.DB

TL;DR: 本文介绍了一种新的基于大语言模型的自适应模糊测试框架FuzzySQL，旨在发现数据库管理系统(DBMS)特殊功能中的微妙漏洞。通过结合语法引导的SQL生成与逻辑转换渐进突变技术，并采用混合错误修复管道，FuzzySQL能够创建结构和语义上多样化的测试案例，从而更深入地覆盖DBMS后端执行路径。实验结果表明，该方法在MySQL、MariaDB等五种DBMS上发现了37个漏洞，证明了其在发现复杂数据库系统深层隐藏错误方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统数据库模糊测试技术主要关注语法正确性和通用SQL结构，对于系统级模式（如GTID）、程序化构造（如PROCEDURE）及高级进程命令（如KILL）等较为隐蔽但重要的DBMS特性探索不足。这些特性虽然不常被典型输入触发，但在特定边缘条件下可能引发严重崩溃或安全问题。

Method: 提出FuzzySQL，一种利用大型语言模型(LLM)支持的自适应模糊测试框架。它结合了语法指导的SQL生成与一种名为逻辑转移渐进变异的新技术，后者通过否定条件和重构执行逻辑来探索替代控制路径，同时采用规则基础修补与LLM驱动语义修复相结合的混合错误修复流程，以自动修正语法和上下文敏感故障。

Result: 在包括MySQL、MariaDB、SQLite、PostgreSQL和Clickhouse在内的多个DBMS上评估FuzzySQL，共发现37个漏洞，其中7个与测试不足的DBMS特殊功能相关。截至撰写时，已有29个案例得到确认，分配了9个CVE标识符，14个已被供应商修复，更多漏洞计划在未来版本中修复。

Conclusion: 研究结果显示，传统模糊器在语义特征覆盖方面存在局限性，而基于LLM的模糊测试方法具有发现复杂数据库系统中深层次隐藏错误的巨大潜力。

Abstract: Traditional database fuzzing techniques primarily focus on syntactic correctness and general SQL structures, leaving critical yet obscure DBMS features, such as system-level modes (e.g., GTID), programmatic constructs (e.g., PROCEDURE), advanced process commands (e.g., KILL), largely underexplored. Although rarely triggered by typical inputs, these features can lead to severe crashes or security issues when executed under edge-case conditions. In this paper, we present FuzzySQL, a novel LLM-powered adaptive fuzzing framework designed to uncover subtle vulnerabilities in DBMS special features. FuzzySQL combines grammar-guided SQL generation with logic-shifting progressive mutation, a novel technique that explores alternative control paths by negating conditions and restructuring execution logic, synthesizing structurally and semantically diverse test cases. To further ensure deeper execution coverage of the back end, FuzzySQL employs a hybrid error repair pipeline that unifies rule-based patching with LLM-driven semantic repair, enabling automatic correction of syntactic and context-sensitive failures. We evaluate FuzzySQL across multiple DBMSs, including MySQL, MariaDB, SQLite, PostgreSQL and Clickhouse, uncovering 37 vulnerabilities, 7 of which are tied to under-tested DBMS special features. As of this writing, 29 cases have been confirmed with 9 assigned CVE identifiers, 14 already fixed by vendors, and additional vulnerabilities scheduled to be patched in upcoming releases. Our results highlight the limitations of conventional fuzzers in semantic feature coverage and demonstrate the potential of LLM-based fuzzing to discover deeply hidden bugs in complex database systems.

</details>


### [148] [The Climate Change Knowledge Graph: Supporting Climate Services](https://arxiv.org/abs/2602.19786)
*Miguel Ceriani,Fiorela Ciroku,Alessandro Russo,Massimiliano Schembri,Fai Fung,Neha Mittal,Vito Trianni,Andrea Giovanni Nuzzolese*

Main category: cs.DB

TL;DR: 本文介绍了气候变化知识图谱，它通过整合与气候模拟相关的多样化数据源，形成一个连贯且可互操作的知识图，以支持复杂查询和更明智的决策。


<details>
  <summary>Details</summary>
Motivation: 面对气候变化对人类资源和活动的广泛影响，当前依赖传统搜索接口和API来检索气候模型生成的数据集存在局限性。为了克服这些挑战，并提供一种更有效的方法来探索气候数据，从而支持更明智的决策制定过程，研究者们开发了气候变化知识图谱。

Method: 气候变化知识图谱通过将来自不同来源的气候模拟相关数据整合到一个统一的知识框架中，利用领域专家的意见构建了一个开放访问的知识图谱及其本体论基础，旨在增强对气候数据的探索能力。

Result: 该知识图谱能够执行涉及气候模型、模拟、变量、时空域以及粒度等多方面的复杂查询，为应对气候变化问题提供了更加全面的信息支持。

Conclusion: 气候变化知识图谱作为一种创新资源，通过促进气候数据的更好理解与利用，在解决气候变化带来的挑战方面扮演着重要角色。

Abstract: Climate change impacts a broad spectrum of human resources and activities, necessitating the use of climate models to project long-term effects and inform mitigation and adaptation strategies. These models generate multiple datasets by running simulations across various scenarios and configurations, thereby covering a range of potential future outcomes. Currently, researchers rely on traditional search interfaces and APIs to retrieve such datasets, often piecing together information from metadata and community vocabularies. The Climate Change Knowledge Graph is designed to address these challenges by integrating diverse data sources related to climate simulations into a coherent and interoperable knowledge graph. This innovative resource allows for executing complex queries involving climate models, simulations, variables, spatio-temporal domains, and granularities. Developed with input from domain experts, the knowledge graph and its underlying ontology are published with open access license and provide a comprehensive framework that enhances the exploration of climate data, facilitating more informed decision-making in addressing climate change issues.

</details>


### [149] [Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)](https://arxiv.org/abs/2602.19811)
*Laurent Bindschaedler*

Main category: cs.DB

TL;DR: 该论文提出了一种用于星型模式OLAP的中间件缓存，通过将SQL和自然语言(NL)查询统一转换为一种称为OLAP意图签名的形式，以提高查询复用率。在严格的模式验证下，只有完全匹配的意图才能被重用，并且引入了两种保持正确性的派生方法（汇总、过滤）来扩展覆盖率。实验表明，这种方法比基于文本或抽象语法树（AST）的缓存方法具有更高的命中率，同时避免了错误命中。


<details>
  <summary>Details</summary>
Motivation: 分析工作负载表现出显著的语义重复性，但大多数生产缓存系统根据SQL表面形式（文本或AST）进行键入，这导致跨BI工具、笔记本和自然语言接口的复用碎片化。为了克服这个问题，本文提出了一种新的缓存机制，旨在提升查询复用效率的同时保证结果的准确性。

Method: 文章介绍了一种安全优先的中间件缓存方案，专为星型模式上的OLAP设计。该方案将SQL及自然语言查询标准化至统一的键空间——即所谓的“OLAP意图签名”，它捕捉度量、分组级别、过滤器以及时间窗口等信息。此外，还提出了两种保持正确性的衍生技术（向上汇总、向下过滤），以进一步扩大覆盖范围而不依赖近似匹配。

Result: 通过对TPC-DS, SSB, 和 NYC TLC三个数据集共1,395个查询进行测试，该方法实现了82%的命中率，相比之下，基于文本的缓存方法仅为28%，而基于AST的方法则为56%。更重要的是，没有出现任何错误命中情况。对于层次结构查询，利用提出的衍生技术能够使命中率翻倍。

Conclusion: 研究证明了通过引入统一的OLAP意图签名作为缓存键值，可以大幅度提高分析查询的缓存效率与准确性。即使是在非常严格的条件下也能够有效地识别并重用相同意图的不同表达形式，从而减少了不必要的计算开销。

Abstract: Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.

</details>
