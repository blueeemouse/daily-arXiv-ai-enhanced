<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 42]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 本文提出了三种改进粒子FRNN物理模拟的方法，包括实时更新/重建比率优化器、新的RT核心使用方法以及支持周期性边界条件的技术。实验表明这些方法可以显著提高模拟速度和能效比，并且在不同GPU代际上具有良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了进一步提升基于RT核心的粒子FRNN物理模拟性能，研究者们提出了一系列新方法来解决现有技术中存在的问题，如BVH结构管理效率低、内存限制等。

Method: 1. 引入了一个实时更新/重建比率优化器用于调整BVH结构；
2. 提出了两种新的RT核心使用方式，旨在消除对邻居列表的需求；
3. 开发了一种使RT核心能够处理具有周期性边界条件的FRNN模拟的技术。

Result: 通过Lennard-Jones FRNN相互作用模型进行测试发现：
- 更新/重建比率优化器可以使RT核心管道的速度最高提升约3.4倍。
- 新方法对于小半径到对数正态分布半径的情况，分别实现了约1.3倍至2.0倍的加速。
- 所提方法还能处理因使用邻居列表而无法装入内存的大规模粒子群模拟情况。
- 支持周期性边界条件的技术没有引入明显的性能损失。
- 该方法在不同代际GPU上的表现良好，同时指出了更适合传统GPU计算的场景。

Conclusion: 所提出的改进措施有效提升了基于RT核心执行的粒子FRNN物理模拟的效率与适用范围，为未来相关研究提供了有价值的参考。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: 研究发现，大型语言模型对低资源语言的开放性问题提供的答案质量较低，并且所使用的语言显著影响了模型采用的文化背景，进而影响了回答的质量。


<details>
  <summary>Details</summary>
Motivation: 确保用户在使用不同语言与大型语言模型（LLMs）交互时不应系统性地处于劣势，即不论使用何种语言，用户都应该得到相似质量的回答。

Method: 基于WildChat数据集分析创建了一组现实世界的开放式问题来评估语言变化是否会影响回答质量；通过LLM-as-a-Judge方法识别回应中存在的文化背景；并在多种语言上对CulturalBench基准测试的一个翻译子集进行评估以进一步探讨这个问题。

Result: 结果表明，对于低资源语言提出的开放性问题，大型语言模型给出的答案质量普遍较低；而且选择不同的语言会改变模型响应中使用的文化信息和背景，这对下游答案的质量产生了影响。

Conclusion: 本研究表明，为了提高公平性和准确性，需要解决大型语言模型在处理不同语言特别是低资源语言时存在的回答质量差异以及文化背景适应性问题。

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [3] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 该论文提出了一种在保持因果关系的同时实现数据稀疏性的方法，通过在路由池中利用零计算（空）专家来提高混合专家层的计算效率。这种方法在视觉-语言模型训练上进行了评估，证明了结合权重稀疏性和数据稀疏性比单独使用权重稀疏性能够更有效地提升计算效率和下游性能表现。


<details>
  <summary>Details</summary>
Motivation: 为了提高混合专家（MoE）层的计算效率，除了已有的权重稀疏性外，还希望引入数据稀疏性，即让每个专家仅处理部分令牌。但是直接实现数据稀疏性的方法如专家选择路由会破坏自回归模型中的因果关系，导致训练与推理不匹配的问题。

Method: 研究者们提出了一种新的方法，在保持因果关系的前提下引入数据稀疏性：通过在路由池中增加零计算（空）专家，当令牌被路由到这些空专家时，实际上并不消耗任何计算资源。通过标准负载均衡目标训练模型，使得所有真实专家和空专家都能被均匀地使用，从而间接实现了预期的数据稀疏性。

Result: 在视觉-语言模型训练任务上的实验表明，相比只采用权重稀疏性的方法，结合权重稀疏性和新提出的这种形式的数据稀疏性可以形成一个更加高效的计算边界，并且能够在训练损失以及下游任务性能上取得更好的结果。此外，模型还学会了隐式的模态感知分配策略，对于图像令牌更倾向于将其路由至空专家，而对于文本令牌则较少这样做，这表明无需显式指定模态信息也能达到良好效果。

Conclusion: 通过引入零计算（空）专家并结合现有权重稀疏性技术，可以在保持因果一致性的同时有效实现数据稀疏性，进而显著提升了混合专家架构下的计算效率及模型性能。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [4] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: 本文通过熵最优传输的视角对注意力机制进行了泛化，提出了带有可训练先验的广义最优传输注意力（GOAT），该机制不仅解决了注意力陷阱问题，还结合了学习位置嵌入的灵活性和固定编码的长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过熵最优传输理论来泛化现有的注意力机制，并指出标准注意力机制实际上是基于一种隐式的均匀先验进行正则化的运输问题。为了克服这种简单假设带来的限制，特别是关于注意力陷阱的问题以及在表示上的折衷，提出了新的注意力模型。

Method: 提出了一种名为Generalized Optimal Transport Attention with Trainable Priors (GOAT)的新注意力机制，它使用可学习的连续先验代替了标准注意力中的隐式均匀先验。此外，GOAT能够吸收空间信息到核心注意力计算中，从而学习到可以外推的先验。

Result: 实验结果表明，GOAT提供了一个基于EOT的注意力陷阱解释，并实现了针对它们的解决方案，同时保持与优化后的内核如FlashAttention完全兼容。

Conclusion: 通过引入GOAT，研究者们不仅为注意力机制提供了一个新的理论框架，而且还开发出了解决注意力陷阱的有效方法，同时增强了对于序列长度变化的适应性。

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [5] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: 本文提出了FedUMM，一种基于非独立同分布多模态数据的统一多模态模型联邦学习框架，通过轻量级LoRA适配器微调减少通信成本，并在VQA v2和GenEval生成基准上进行了评估。结果显示，尽管随着客户端数量和异质性的增加性能略有下降，但仍然与集中式训练保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前的统一多模态模型通常在集中式环境中训练，这限制了它们在隐私敏感及地理分布场景中的应用。为了克服这一局限性，研究旨在开发一种能够处理非独立同分布（non-IID）多模态数据且具有低通信成本的联邦学习框架。

Method: 采用NVIDIA FLARE构建了一个名为FedUMM的联邦学习框架，该框架利用参数高效微调技术，让客户端仅需训练轻量级的LoRA适配器而无需调整基础模型本身；服务器端则只聚合这些适配器更新信息。

Result: 实验结果表明，在面对多达16个客户端以及不同程度的数据异质性时，FedUMM方法虽然表现出轻微的性能衰退，但总体上依然能够与传统集中式训练方法相媲美。此外，通过仅传输适配器更新而非整个模型权重，每轮通信开销大幅度降低。

Conclusion: 本研究表明，FedUMM为未来探索保护隐私的联邦统一多模态模型提供了实证经验，证明了在确保用户隐私的同时，也能够实现高效的模型训练。

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [6] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: 提出了一种名为Ambient Dataloops的迭代框架，用于优化数据集质量，从而帮助扩散模型更准确地学习数据分布。通过数据-模型协同进化过程，在每次迭代中逐步提高数据质量和模型性能。采用Ambient Diffusion技术处理噪声，避免自我消耗循环。该方法在无条件和文本条件图像生成及从头蛋白质设计上达到顶尖表现，并提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 现代数据集中样本质量参差不齐，直接用这些异质数据训练会导致模型效果不佳。为了解决这个问题，研究提出了一个数据集-模型共同进化的流程来逐步提升数据质量和模型性能。

Method: 提出的Ambient Dataloops框架利用了数据集-模型共进化的方法。在每次迭代过程中，数据集的质量得到提升，同时模型也随之改进。为了避免出现破坏性的自消耗循环，在每一代中，将合成改善后的样本视为带有轻微噪音的数据点，并使用Ambient Diffusion技术来学习如何在有损情况下进行有效学习。

Result: 实验结果表明，Ambient Dataloops在无条件图像生成、基于文本的图像生成以及全新蛋白质设计方面均取得了当前最佳的表现。此外，还为所提框架提供了一个理论基础，解释了数据循环过程带来的好处。

Conclusion: 通过引入Ambient Dataloops框架，可以有效地解决由于原始数据集质量差异导致的学习问题，不仅提高了模型训练的效果，而且在多个应用场景下都展示了其优越性。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [7] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: 本文提出了一种名为Lattice的混合顺序预测系统，该系统通过二进制置信度门控有条件地激活学习到的行为结构。实验验证了Lattice在推荐系统、科学时间序列和金融市场上的有效性，特别是在不确定时能够避免错误激活行为原型。


<details>
  <summary>Details</summary>
Motivation: 为了提高预测系统的准确性和可靠性，尤其是在数据分布发生变化或存在不确定性的情况下，作者提出了Lattice系统。它旨在通过仅在足够自信时才激活基于行为原型的评分来改善基础模型的表现，并且在遇到不确定性时能够回落到基线预测以避免错误决策。

Method: Lattice系统首先将行为窗口聚类为行为原型，然后利用二进制置信度门控机制决定是否激活特定于原型的评分。只有当置信度超过设定阈值时才会激活相关逻辑；否则，系统会依赖于常规预测方法。研究中使用了LSTM和Transformer作为骨干模型，在多个领域进行了测试。

Result: 在MovieLens推荐系统上与LSTM结合使用时，Lattice相对于LSTM基准提高了31.9%的HR@10指标；相比SASRec和BERT4Rec等Transformer基线也有显著优势。对于LIGO和金融市场的数据集，Lattice能够在分布变化发生时正确拒绝激活原型，表明其有效防止了误激活。而在已经具有良好结构的Transformer模型上，Lattice表现中立，没有造成性能下降。

Conclusion: Lattice系统通过引入置信度门控机制成功地管理了认识论不确定性，既能在适用情况下增强模型性能，也能在不适用时避免不必要的干预，这为安全关键应用中的不确定性管理提供了一个有前景的设计原则。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [8] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为CASL的新框架，旨在将扩散模型的稀疏潜在维度与语义概念对齐。通过训练稀疏自动编码器(SAE)并学习一个轻量级线性映射来关联每个概念和相关的潜在维度，从而实现了监督对齐。此外，还提出了CASL-Steer作为控制潜变量干预手段以及编辑精度比(EPR)度量标准，以验证这些对齐方向的语义意义。实验表明，该方法在编辑精度和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于稀疏自动编码器(SAE)的方法在理解扩散模型时依赖于无监督方式，这导致稀疏特征无法很好地与人类可理解的概念相匹配，因此限制了它们提供可靠语义控制生成图像的能力。

Method: 首先使用冻结的U-Net激活训练一个SAE来获得解纠缠的潜在表示，然后学习一种轻量级线性映射，将每个概念与一小部分相关的潜在维度相关联。为了验证所学得概念轴的语义含义，研究者们提出了CASL-Steer，这是一种控制潜变量干预方法，可以沿着学习到的概念轴移动激活值。

Result: 实验结果表明，所提出的方法在编辑精度和可解释性方面都优于现有方法，并且这是首次在扩散模型中实现监督下潜在表示与语义概念之间的对齐工作。

Conclusion: CASL框架能够有效地将扩散模型中的稀疏潜在维度与具体的语义概念进行对齐，为生成内容提供了更可靠的语义控制能力。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [9] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 本文探讨了在自然内容与合成内容混合的场景下，经验风险最小化(ERM)方法及其局限性，并提出了一种对不同类型的数据点赋予非均匀权重的算法，在估计分布均值及PAC学习设定中表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）生成的合成内容日益增多，这些内容混入了从评论网站到法庭文件等各类‘自然’数据之中。本文旨在研究在这种普遍存在的背景下，基础学习理论问题如何被重新审视。

Method: 文章通过建立一个包含自然和合成数据的学习任务序列模型来模拟这一场景，其中学习算法并不知道每个示例的具体来源。研究了在这样的环境下，经验风险最小化(ERM)的可能性和限制。对于任意d维分布均值估计问题，对比了ERM与一种给不同代数据点分配非均匀权重的算法的表现；在PAC学习框架下，进一步考察了ERM是否总是收敛于真实概念的能力。

Result: 发现虽然ERM能够收敛至真实均值，但采用非均匀加权策略的算法在性能上超越了它。而在PAC学习设置中，ERM并不总能收敛至正确概念，不过存在一些算法能够在任意VC类和任意程度污染下学会正确的假设。

Conclusion: 研究表明，在自然与合成数据共存的情况下，传统ERM方法存在局限性，而通过对数据点施加差异化权重或使用其他特定设计的学习算法，则可以克服这些挑战并取得更好的学习效果。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [10] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther是一个与PyTorch兼容的库，它将随机数值线性代数算法整合到一个高性能框架中，提供高效的替代组件，如草图线性层、二维卷积、多头注意力和随机矩阵分解。通过使用自定义C++/CUDA后端，Panther能在CPU和GPU上运行，并且在BERT模型上实现了高达75%的内存节省，同时保持了相当的损失。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型训练受限于GPU内存和计算能力的限制。尽管随机数值线性代数（RandNLA）提供了压缩这些模型的有效技术，但由于缺乏统一的生产级库而阻碍了这些方法的广泛应用。

Method: 开发了一个名为Panther的PyTorch兼容库，该库将已建立的RandNLA算法整合进一个高性能框架中。Panther通过实现定制的C++/CUDA后端（pawX），提供了可在CPU和GPU上运行的优化实现。

Result: 通过用Panther提供的组件替换标准PyTorch线性层（仅需几行代码修改），研究人员在BERT模型上展示了显著的内存节省（高达75%），同时保持了可比较的损失水平。

Conclusion: Panther作为一个有效的工具，能够帮助开发者轻松地采用RandNLA技术来减少深度学习模型的内存占用，从而使得在有限硬件资源下也能进行更大型或更复杂的模型训练成为可能。

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [11] [Multi-Targeted Graph Backdoor Attack](https://arxiv.org/abs/2601.15474)
*Md Nabi Newaz Khan,Abdullah Arafat Miah,Yu Bi*

Main category: cs.LG

TL;DR: 本文提出了首个针对图分类任务的多目标后门攻击方法，通过子图注入而非替换的方式，在保持原始图结构的同时向干净图中植入多个触发器，从而将预测重定向到不同的目标标签。实验表明，该攻击方法在五个数据集上对四种GNN模型均表现出色，并且对抗当前最先进的防御手段（如随机平滑和精细剪枝）时也显示出了强大的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的针对图分类的后门攻击研究局限于使用基于子图替换机制的单一目标攻击，其中攻击者仅向GNN模型植入一个触发器。为了探索更加复杂和隐蔽的攻击方式，作者提出了一种新的多目标后门攻击方法，旨在同时利用多个触发器来影响模型对于不同目标标签的预测结果。

Method: 提出了一种名为“子图注入”的新方法，用于在不改变原始图结构的情况下向图中添加恶意信息；设计了能够同时针对多个目标标签发起攻击的策略；通过广泛的实验证明所提方法的有效性和普遍适用性，包括但不限于考察不同注入方法、连接数、触发器大小等因素的影响；评估了最新防御技术对该攻击方法的效果。

Result: 实验结果显示，所提出的多目标后门攻击框架相比传统的基于子图替换的方法具有明显优势，在不影响正常准确率的前提下实现了高成功率的攻击；此外，该攻击方法被证明对多种GNN架构及训练参数设置都有效；面对最新的防御措施时仍能保持较好的攻击效果。

Conclusion: 本研究表明，GNN在执行图分类任务时面临着来自多目标后门攻击的安全威胁。所提出的新型攻击框架不仅展示了高效性和鲁棒性，还揭示了现有防御措施存在的不足之处。

Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.

</details>


### [12] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: 本研究评估并比较了SARIMAX、XGBoost和LSTM三种不同的预测模型，用于预测一家澳大利亚三级转诊医院每日急诊科入院人数。研究发现XGBoost在预测总日入院量方面准确性最高，而SARIMAX在预测高复杂性病例方面略胜一筹。所有模型对于常规日常模式的重现表现良好，但在估计突发且不频繁的患者数量激增时存在不足。


<details>
  <summary>Details</summary>
Motivation: 急诊科过度拥挤对病人安全和运营效率构成威胁，因此需要准确的需求预测来进行有效的资源配置。

Method: 研究使用2017年1月至2021年12月间来自澳大利亚一家三级转诊医院的数据，并将需求分解为八个特定病房类别，同时根据临床复杂性对患者进行分层。为了处理新冠疫情导致的数据扭曲，研究采用Prophet模型生成异常期间的合成反事实值。

Result: 实验结果表明，提出的三种模型都持续优于季节性朴素基线。XGBoost在预测总日入院数方面表现出最高的准确性（平均绝对误差为6.63），而对于主要复杂性案例的预测，统计SARIMAX模型则略微优越（MAE为3.77）。

Conclusion: 虽然这些技术能够成功再现常规的日复一日模式，但它们共同的局限在于低估了突然且不经常发生的患者数量激增情况。

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [13] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于Martingale理论的前瞻性采样框架（MFS），用于改进大型语言模型的解码过程。通过将推理路径的质量建模为随机过程，并利用概率论中的原则来评估和选择路径，MFS在六个推理基准测试中展现了超越现有技术的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码在大型语言模型中由于逐个生成令牌的过程，往往无法找到全局最优的推理路径。尽管一些策略如前瞻性采样试图通过模拟未来步骤来缓解这个问题，但它们通常依赖于临时性的启发式方法来评估路径和修剪搜索空间。

Method: 引入了Martingale前瞻性采样（MFS）这一原则性框架，它将LLM解码重新定义为识别最优随机过程的问题。使用Doob分解定理来衡量路径的可预测优势，采用Optional Stopping Theory进行原则性的次优候选路径修剪，并根据Martingale收敛定理设计了一个自适应停止规则以确保路径质量已证明收敛时结束探索。

Result: 实验结果表明，在六个推理基准上，MFS不仅在准确性上超过了最先进的方法，还显著提高了计算效率。

Conclusion: 通过结合Martingale理论与概率论原则，MFS为改善大型语言模型的解码提供了新的视角，既提升了推理准确性也增强了计算效率。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [14] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 本文提出了一种Margin-Aware Speculative Verification方法，该方法通过调整验证规则以适应目标模型的局部决策稳定性，从而在保持生成质量的同时显著加速了从8B到235B不同规模模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有投机解码技术中的验证机制依赖于严格的token级拒绝采样，在现代大型语言模型操作的低边际情况下效率低下。这是因为当目标模型对于顶级候选者表现出弱偏好时，拒绝看似合理的次优选项带来的信息增益微乎其微，但回滚成本却非常高。

Method: Margin-Aware Speculative Verification是一种无需训练、与领域无关的验证策略，它基于目标logits直接测量的决策稳定性来调节验证过程，并且仅在严格验证带来极少好处时放松拒绝标准。

Result: 实验表明，所提方法能够为8B至235B范围内的各种模型提供一致且显著的推理速度提升，同时保持了多样的基准测试下的生成质量。

Conclusion: 通过引入Margin-Aware Speculative Verification, 可以有效提高大型语言模型推测解码过程中验证阶段的效率，而不会牺牲生成文本的质量。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [15] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 本研究通过使用多重插补链式方程（MICE）处理缺失数据，并采用标准化平均绝对误差（nMAE）度量，对缅因州30个湖泊的Secchi盘深度（SDD）进行预测。研究表明，在六种候选方法中，岭回归提供了最佳的平均测试性能。进一步分析发现，仅需约176个训练样本和一个预测因子即可达到接近完整历史数据集95%的准确性，从而为湖泊研究人员提供了一种简化且高效的采样努力与测量优先级设定规则。


<details>
  <summary>Details</summary>
Motivation: 志愿者主导的湖泊监测产生了一系列不规则、季节性的数据，其中存在许多由于冰盖、天气相关访问限制以及偶尔的人为错误导致的数据缺失，这使得有害藻华的预测和预警变得复杂。因此，需要一种方法来处理这些缺失值并提高预测准确性。

Method: 本研究从缅因州湖泊三十年的现场记录中抽取了30个数据丰富的湖泊子集，用于研究Secchi盘深度(SDD)预测。对于缺失数据，采用了多重插补链式方程(MICE)的方法；模型性能则通过标准化平均绝对误差(nMAE)指标评估，以实现跨湖比较。在几种预测方法中选择了表现最好的岭回归法来进行进一步分析。

Result: 研究发现，岭回归法在六个候选方法中表现最优。通过岭回归分析确定了最小样本量：在最近历史协议下，平均每湖约176个训练样本可使模型达到完整历史精度的95%以内。同时识别出一个包含四个特征的最小特征集，其性能与十三特征基准相当。综合这些结果后提出了联合可行性函数，用以确定满足特定精度目标所需的最短训练历史长度和最少预测因子数量。

Conclusion: 该研究提出了一种联合可行性策略，它将最近的历史长度和特征选择统一在一个固定的准确度目标之下，为湖泊研究人员提供了一个简单而有效的规则来设置采样工作量和测量优先级。结果显示，只需大约64个近期样本和每个湖泊的一个预测因子就能满足95%的准确度要求，突出了有针对性监测的实用性。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [16] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 本研究提出了一种机器学习方法，通过临床测试和MRI数据来提高非典型阿尔茨海默病（atAD）患者的诊断准确性。相较于仅使用海马体积作为特征，该方法通过结合额外重要的MRI特征实现了更好的性能，并且能够显著提高atAD病例的正确诊断率。


<details>
  <summary>Details</summary>
Motivation: 为了改善非典型阿尔茨海默病（atAD）患者的诊断情况，因为这部分患者常常被误诊。尽管临床评估与海马萎缩量对于典型的阿尔茨海默病具有较高的诊断精度，但对于呈现不典型症状的AD患者来说，常规诊断方法并不准确。

Method: 采用机器学习的方法，利用从一个私人数据集及两个公开数据集（NACC和ADNI）收集到的1410名受试者的数据进行分析。这些受试者分为四组：273例典型AD、184例非典型AD、235例非AD认知障碍以及685例认知正常个体。除了基于临床测试结果和海马体积外，还综合了大脑其他区域的MRI特征用于分类实验。

Result: 通过加入更多关键性的MRI特征，模型在区分非典型AD与非AD认知障碍方面表现优于仅使用海马体积的情况。此外，利用Boruta统计方法识别出了对不同诊断群体有显著区分作用的大脑区域。对于NACC数据集，非典型AD的正确诊断比例从52%提升到了69%；而对于ADNI数据集，则是从34%增加至77%，同时保持了高精确度。

Conclusion: 提出的机器学习方法能够在临床环境中仅依靠标准护理流程下的临床测试电池和MRI数据，有效提高非记忆性atAD的诊断准确性。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [17] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 本文探讨了量化过程如何影响机器遗忘效果，并提出了一种新的量化感知遗忘方法来解决这个问题，该方法在4比特量化条件下能够有效保持对特定数据的遗忘。


<details>
  <summary>Details</summary>
Motivation: 研究者们发现，在实际应用中，模型为了部署往往会被量化（例如，降至4比特），但这种做法可能会导致已经通过机器遗忘技术移除的知识被灾难性地恢复。

Method: 作者首先分析了低比特量化为何会削弱遗忘的效果，接着提出了一个基于logits空间铰链损失的新方法：对于每一个需要遗忘的例子，他们强制要求未学习模型输出的logits与原模型至少相差半个量化步长的边界值，以此确保即使经过量化处理后，被遗忘的例子仍然可以被区分出来。

Result: 实验结果表明，在语言和分类任务上（包括一个关于Twitter错误信息的数据集），所提出的方法能够在4比特量化条件下成功维持对特定数据点的遗忘状态；相比之下，现有方法几乎完全无法阻止已遗忘知识的恢复。

Conclusion: 本研究表明，通过采用量化感知的遗忘策略，可以在不完全重训练的情况下有效防止因模型量化而导致的信息泄露问题。

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [18] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于注意力机制的白盒架构Prism，该架构源自最大化编码率减少（MCR^2）的原则。通过引入两个物理约束：过完备字典和无理数频率分离（π-RoPE），使得信号与噪声子空间之间不相干。实验表明，这些几何归纳偏置足以单独诱导无监督的功能解缠。使用TinyStories作为验证频谱动态的受控测试平台，观察到Prism能够自发地将其注意力头专门化为频谱上不同的区域。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型，特别是Transformer，经常被批评为“黑箱”且缺乏可解释性。因此，作者们旨在开发一个更具有可解释性的架构，同时保持或提高模型性能。

Method: Prism架构基于最大编码率降低(MCR^2)原则设计，并将注意力机制建模为特定信号-噪声流形上的梯度上升过程。此外，还引入了过完备字典来扩展表示相空间以及π-RoPE以确保信号和噪声子空间之间的不相干性。

Result: 研究表明，仅凭这些几何归纳偏差就足以独立地引起无监督功能解缠。利用TinyStories作为控制测试平台来验证频谱动态，发现Prism能够自然地将其注意力头分为低频和高频两种类型，分别用于捕捉长距离因果依赖关系（信号）和处理局部语法约束（噪声）。

Conclusion: 结果表明，通过有原则的几何构建，可解释性和性能并不是互相排斥的目标，而是可以统一起来的。

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [19] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: 本文提出了一种名为RDumb++的方法，通过引入基于熵的漂移评分和KL散度漂移评分两种漂移检测机制以及自适应重置策略，解决了在长时序下或快速变化测试分布中持续测试时间适应(CTTA)的问题。实验表明，在整个数据流中，相比RDumb，RDumb++能够保持稳定的适应性，并且平均准确率提高了约3%。


<details>
  <summary>Details</summary>
Motivation: 现有的CTTA方法如Tent、EATA等在面对快速变化或极长时间跨度的测试分布迁移时表现不佳，特别是在CCC基准测试中模型需要处理7.5百万样本量级的数据流，其中包含不断变化的损坏类型和严重程度。为了解决这一问题，研究提出了改进版的RDumb算法，即RDumb++。

Method: RDumb++是RDumb的一个原则性扩展，它引入了两种漂移检测机制——基于熵的漂移评分与KL散度漂移评分，以及自适应重置策略。这些机制使得模型能够在累积适应变得有害之前检测到这种情况，并采取措施恢复以避免预测崩溃。

Result: 在包含三个速度设置和三种随机种子配置（共九次运行，每次包含一百万个样本）的CCC-medium数据集上进行的实验显示，RDumb++在整个数据流过程中都优于RDumb，平均绝对准确率提高了大约3%。此外，关于漂移阈值和重置强度的消融实验进一步证明了漂移感知重置对于防止崩溃及实现可靠的长期CTTA至关重要。

Conclusion: 研究结果表明，通过引入特定的漂移检测机制与自适应重置策略，RDumb++能够在长时序或者快速变化的测试环境中有效提升模型性能并维持稳定，从而克服了现有CTTA方法的一些局限。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [20] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 本文通过两个受控实验展示了在医疗保健领域的机器学习中，使用临床定制指标相比验证损失能更优地优化模型，从而更好地满足临床任务需求。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习依赖于验证损失来指导模型优化，但对于医疗保健领域而言，模型需要根据具体的临床要求来表现良好。因此，使用与临床相关的定制化指标可以更精确地捕捉这些要求，并允许更多非微分选项来进行优化。

Method: 进行了两项控制实验，比较了采用临床相关指标与采用验证损失进行模型优化的效果。

Result: 结果显示，使用临床定制指标进行优化的模型在临床任务上表现更好。

Conclusion: 尽管定义和编码临床相关指标需要额外的努力，但这样做可以获得更适合临床使用的模型，从而更好地达到医疗保健中机器学习的核心目标。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [21] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，用于从部分观测数据中学习神经算子，以解决实际科学应用中遇到的不完整观测数据问题。通过引入掩码预测训练策略和物理感知潜在传播器来处理核心困难，并在POBench-PDE基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于传感器限制、地理约束或测量成本等原因，现实世界中的科学应用经常遇到不完整的观测数据。尽管神经算子在提高PDE求解的计算效率和准确性方面取得了显著进展，但其完全观测空间输入的基本假设严重限制了其在实际应用中的适用性。

Method: 提出了潜自回归神经算子（Latent Autoregressive Neural Operator），它包括两个新组件：(i) 一种掩码到预测的训练策略，通过战略性地遮盖观察区域来创建人工监督；(ii) 一个物理感知的潜在传播器，通过边界优先的自回归生成在潜在空间中重建解决方案。此外，开发了POBench-PDE，一个专门针对评估在部分观测条件下神经算子表现的综合基准。

Result: 该方法在所有基准测试下，对于少于50%缺失率的块状缺失情况实现了18-69%的相对L2误差减少，包括现实世界的气候预测任务。此外，该方法能够有效应对高达75%的缺失率，某种程度上缩小了理想化研究环境与现实世界科学计算复杂性之间的差距。

Conclusion: 所提出的方法为处理部分观测数据提供了有效的解决方案，特别是在高缺失率情况下表现出色，表明了其在实际科学应用中的潜力。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [22] [BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations](https://arxiv.org/abs/2601.15552)
*Phuc Nguyen,Benjamin Zelditch,Joyce Chen,Rohit Patra,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文介绍了一种名为BanditLP的多利益相关者上下文强盗框架，该框架结合了神经汤普森采样和大规模线性规划以在服务时进行受限动作选择。通过公共基准测试和合成数据实验表明其优于强大的基线，并且在LinkedIn的电子邮件营销系统中应用时展示了业务上的成功。


<details>
  <summary>Details</summary>
Motivation: 为了解决在存在多个利益相关者的情况下，如何有效地进行上下文决策的问题，同时确保所选动作满足特定约束条件。

Method: 提出了一种名为BanditLP的方法，它利用神经汤普森采样来学习目标特定的结果，并使用一个能够处理数十亿变量的大规模线性程序来进行受限制的动作选择。

Result: 在公开的数据集和合成数据上进行了实验，显示出相对于强大基线的一致改进；此外，在LinkedIn的实际电子邮件营销系统中部署后也取得了积极的业务成果。

Conclusion: BanditLP提供了一个通用且可扩展性强的解决方案，适用于需要整合探索与受限优化的各种应用场景。

Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.

</details>


### [23] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 本文研究了在随机前置时间和未知需求过程条件下易腐品库存系统的管理问题。通过采用边际成本会计方案和深度学习策略，开发了两种端到端的学习方法：纯黑盒方法（E2E-BB）和结构引导方法（E2E-PIL）。进一步地，通过运营数据分析中的增强技术发展出更优的策略（E2E-BPIL）。实验结果表明，在合成数据和真实数据上，E2E-PIL的表现优于E2E-BB，而E2E-BPIL又进一步改进了E2E-PIL。此外，嵌入启发式政策结构可以减少模型的有效复杂度并提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 管理有限寿命的易腐品是库存管理中的一个基本挑战，因为错误的订购决策很快会导致缺货或过度浪费。特别是在订单下达时仅能依赖有限的历史数据、观察协变量及当前系统状态的情况下，如何有效利用这些信息来改善学习效率，成为了研究的重点。

Method: 本研究采用了边际成本会计方案为每个订单分配单一生命周期成本，并创建了一个统一的损失函数以支持端到端学习。基于此，训练了一个能够直接将观察到的协变量和系统状态映射到订货量的深度学习策略。此外，还开发了两个变体：一种是纯黑盒方法（E2E-BB），另一种是结构引导的方法（E2E-PIL），后者通过显式计算而不是额外学习来捕捉库存效应。对于E2E-PIL，研究显示其目标是由一次齐次性诱导的，从而允许使用来自运营数据分析的一种增强技术来产生一个改进的策略（E2E-BPIL）。

Result: 实验结果表明，在合成数据集和实际数据集上，E2E-PIL的表现优于E2E-BB，而通过运用增强技术得到的E2E-BPIL表现最佳。此外，通过过剩风险分解分析发现，嵌入启发式政策结构能够在牺牲少量灵活性的同时降低模型的有效复杂度并提高学习效率。

Conclusion: 研究表明，当深度学习为基础的决策工具与人类知识相结合时，它们变得更加有效且稳健。这强调了将先进的分析方法与库存理论相整合的价值所在。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [24] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文研究了1-identification问题，提出了一个新的下界估计方法，并设计了一个新算法，该算法在所有问题实例中与下界的差距仅限于对数因子的多项式。


<details>
  <summary>Details</summary>
Motivation: 为了在至少存在一个合格臂的情况下，找到期望总拉次数的新下界，并解决历史文献中遗留的关于多个合格臂时$\mathbb{E}τ$分析的问题。

Method: 通过优化公式化来推导新的$\mathbb{E}τ$下界；设计了一种新的算法，其上界与下界的差距最多为对数因子的多项式。

Result: 成功推导出当至少有一个合格臂时$\mathbb{E}τ$的新下界；所设计的新算法对于所有问题实例都具有紧密的上界。

Conclusion: 本文的研究补充了当存在多个合格臂时$\mathbb{E}τ$的分析，解决了先前文献中的开放问题。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [25] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: 本文提出了一种名为Dualformer的新框架，旨在解决基于Transformer的模型在长时间序列预测中因低通滤波效应而逐渐衰减高频信息的问题。Dualformer通过同时在时域和频域建模互补的时间模式、层次化的频率采样模块以及周期性感知的加权机制，实现了结构化频率建模与自适应时间-频率特征整合，从而提高了模型性能特别是对于异构或弱周期性的数据。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的模型在长期时间序列预测中存在固有的低通滤波效应问题，导致重要的高频信息逐渐衰减，影响了对细粒度时间变化的捕捉能力。为了解决这一限制，提出了新的方法来改善频率成分的传播方式。

Method: 提出了Dualformer框架，包含三个主要组成部分：1) 双分支架构，在时域和频域同时建模互补的时间模式；2) 层次化的频率采样模块，将不同的频率带分配给不同层，以保持较低层中的高频细节并模拟较深层中的低频趋势；3) 周期性感知的权重机制，根据输入的谐波能量比动态平衡双分支贡献。

Result: 通过对八个广泛使用的基准测试进行的大量实验表明，Dualformer表现出稳健性和优越性能，特别是在处理异构或弱周期性数据时。

Conclusion: Dualformer通过引入双域视角下的频率建模策略，有效解决了基于Transformer模型在长期时间序列预测中存在的低通滤波效应问题，提升了模型对于复杂时间序列数据的泛化能力和准确性。

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [26] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: 本文提出了一种针对长序列编辑的递归最小二乘编辑器RLSEdit，它通过在线二次优化和软约束来平衡模型更新中的可塑性和稳定性问题。实验表明该方法在处理大量编辑时能够保持早期编辑效果，并且在GLUE等基准测试中保留了通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有的模型编辑方法在面对长时间连续到达的编辑请求时，容易遇到可塑性与稳定性之间的矛盾：直接定位并修改的方式可能随时间累积干扰，而空空间风格的‘硬保存’只能保护明确约束的部分，导致先前的编辑被覆盖或非约束行为偏离，从而在多次编辑后降低模型的一般性能。

Method: 提出了RLSEdit，一种用于长序列编辑的递归最小二乘编辑器。该方法将编辑视为具有软约束的在线二次优化问题，旨在最小化累计键值拟合目标函数的同时加入两个正则项，以控制相对于预训练权重以及指定锚点映射的偏差。此更新策略利用Woodbury恒等式实现了高效的在线递归计算，每次编辑的成本与历史长度无关，仅依赖于当前编辑规模。

Result: 实验证明，在多个模型家族上，RLSEdit能够稳定地扩展至10,000次编辑，相较于其他强大的基线方法，在编辑成功率及整体稳定性方面表现更优——特别是能有效保留早期编辑内容，并在GLUE、推理/代码基准测试中保持模型的一般能力。

Conclusion: RLSEdit为解决长期连续模型编辑场景下的可塑性-稳定性难题提供了一个有效的解决方案，能够在不影响模型原有功能的前提下高效整合新信息。

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [27] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 本文提出了零错误范围（ZEH）的概念，用于评估最先进的大语言模型在无错误条件下能够解决的最大问题范围。通过评估GPT-5.2和Qwen2.5的ZEH，研究发现即使是这些表现优异的模型，在处理简单问题时也会出错，这提醒了人们在关键安全领域应用大语言模型时需谨慎。此外，虽然计算ZEH需要大量计算资源，但通过使用树结构和在线softmax方法可以有效减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了提高大语言模型（LLMs）的信任度并更好地理解其局限性，特别是当它们被应用于对安全性要求极高的场景时。

Method: 定义了零错误范围（ZEH）来衡量模型在没有任何错误的情况下能够解决问题的最大范围；通过具体实例如判断字符串奇偶性和括号匹配等任务来测试GPT-5.2和Qwen2.5的ZEH；分析了ZEH与准确率之间的关系及算法能力出现的线索；探讨了如何利用树结构和在线softmax技术来降低计算ZEH的成本。

Result: 揭示了即使是最先进的LLMs也可能在看似简单的任务上犯错；发现了ZEH与模型准确性之间存在关联但行为细节有所不同；提出了通过特定技术手段可显著降低ZEH计算所需的时间和资源消耗的方法。

Conclusion: ZEH为理解和改善大语言模型提供了新的视角，强调了在敏感领域部署前对其能力进行彻底评估的重要性。同时指出，尽管面临计算效率挑战，仍可通过优化策略减轻负担。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [28] [Communication-efficient Federated Graph Classification via Generative Diffusion Modeling](https://arxiv.org/abs/2601.15722)
*Xiuling Wang,Xin Huang,Haibo Hu,Jianliang Xu*

Main category: cs.LG

TL;DR: CeFGC, a new Federated GNN (FGNN) paradigm, reduces communication rounds to three and tackles non-IID data challenges by using generative diffusion models. This approach enables clients to train local GNNs on synthetic and real graphs, leading to an efficient global model. Theoretical and experimental results show CeFGC's effectiveness and efficiency over existing methods, particularly in handling non-IID data.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the high communication overhead and non-IID (non-Independent and Identically Distributed) data challenges faced by Federated GNNs (FGNNs). These issues limit the practicality and performance of FGNNs when dealing with decentralized data. By introducing a novel paradigm that minimizes direct client-server communication, the research aims to improve the efficiency and effectiveness of GNN training in federated settings, especially for non-IID data distributions.

Method: CeFGC, or Communication-efficient Federated Graph Collaboration, is the proposed method. It involves each client training a generative diffusion model to capture its local graph distribution, which is then shared with a server. The server redistributes these models back to all clients, who use them to generate synthetic graphs. These synthetic graphs, combined with local real graphs, are used to train local GNN models. Finally, the updated model weights from each client are aggregated at the server to form a global GNN model. This process limits the communication between the server and clients to just three rounds, significantly reducing the overall communication cost.

Result: Theoretical analysis shows that CeFGC reduces the I/O complexity of communication volume to a constant of three rounds. Experimental evaluations on multiple real-world graph datasets demonstrate that CeFGC outperforms state-of-the-art FGNN approaches in terms of both effectiveness and efficiency, especially in scenarios with non-IID data. The method effectively aligns local and global model objectives and enriches the training set with diverse graphs, leading to better generalization and performance.

Conclusion: CeFGC presents a significant advancement in the field of Federated GNNs by providing a solution that not only reduces the communication overhead but also handles the challenge of non-IID data more efficiently. Through the use of generative diffusion models, it successfully enriches the training process, leading to improved performance of the global GNN model. This work opens up new possibilities for more effective and efficient learning from decentralized and heterogeneous graph-structured data.

Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.

</details>


### [29] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 提出了一种新的药物-药物相互作用（DDI）预测框架GenRel-DDI，该框架通过将DDI预测重新定义为以关系为中心的学习问题来提高模型对未见药物的泛化能力。实验表明，GenRel-DDI在多个基准测试中显著优于现有方法，特别是在严格实体不重叠评估上表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前的药物-药物相互作用预测模型虽然在标准基准测试中表现出色，但在实际应用情景下，面对大多数候选药物对涉及之前未见过的药物且验证过的交互数据稀缺的情况时，其泛化性能不佳。此外，仅增加模型容量并不能有效改善这一问题。

Method: 提出了GenRel-DDI框架，它将DDI预测视为一个关系学习问题，独立于具体的药物身份来学习交互表示，从而能够捕捉到可以转移到未见过药物和新药物组合上的交互模式。

Result: 广泛的实验证明了GenRel-DDI相对于最先进方法的一致性和显著优越性，尤其是在严格的实体不重叠评价中优势明显。

Conclusion: GenRel-DDI框架通过引入关系级抽象有效地解决了现有DDI预测方法难以泛化至未见药物的问题，并展示了其在鲁棒DDI预测中的有效性及实用性。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [30] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 提出了一种新的主动学习框架，该框架通过混合多个大型语言模型来生成标签，以提高标注的鲁棒性，并采用注释差异性和负学习方法来减少噪声标签的影响。实验表明，该框架的表现与人工标注相当，优于单一LLM基线和其他基于LLM集成的方法，且能在实际应用中完全在本地机器上运行。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）快速发展和强大的泛化能力，它们越来越多地被纳入主动学习流程作为标注者以降低成本。然而，这些模型产生的标签质量往往达不到实际应用的要求。

Method: 提出了一个名为“循环中的LLM混合”主动学习框架，用基于混合LLM的标注模型生成的标签代替人类标注者，旨在通过集合多个LLM的优点来增强基于LLM标注的鲁棒性。此外，为了进一步减轻噪声标签的影响，引入了标注差异性和负学习来识别不可靠的标注并增强学习效果。

Result: 广泛的实验证明，所提出的框架达到了与人工标注相媲美的性能，并且始终优于单个LLM基准以及其他基于LLM集成的方法。

Conclusion: 这个新框架不仅提高了基于LLM的标注质量，还使得整个过程能够在现实世界的应用程序中完全于本地机器上执行。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [31] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为GOSV的框架，通过全局优化识别出对安全至关重要的注意力头，并使用两种激活重贴片策略：有害贴片和零消融。这些方法帮助识别了两个空间上不同的安全向量集，揭示了对齐的大语言模型在安全目的上的独立功能路径。研究发现，当约30%的总头部被重新贴片时，会发生完全的安全故障。基于这些见解，开发了一种新的白盒越狱方法，在所有测试模型中显著优于现有的白盒攻击，证明了GOSV框架对于LLM安全可解释性的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖局部、贪婪归因，假设组件贡献是独立的，忽略了大语言模型中不同组件（如注意力头）之间的合作交互作用，这对安全机制有着共同贡献。这种局限性导致了大型语言模型的安全防护措施面对越狱攻击时仍然脆弱。

Method: 提出了GOSV框架，该框架通过同时对所有注意力头进行全局优化来识别对安全关键的注意力头。采用两种互补的激活重贴片策略：有害贴片和零消融。这些策略用于识别两组空间上明显不同的安全向量集合，称为恶意注入向量和安全抑制向量。

Result: 研究发现，当大约30%的总注意力头被重新贴片时，会发生完全的安全故障。基于此洞察，研究人员开发了一种新的推理时间白盒越狱方法，该方法利用识别出的安全向量通过激活重贴片进行攻击。与现有技术相比，这种新方法在所有测试模型中表现出色。

Conclusion: 本研究表明，通过GOSV框架能够有效地识别出对LLM安全性起关键作用的注意力头，并且基于此框架开发的新白盒越狱方法比现有方法更加有效。这为提高LLM安全可解释性提供了强有力的证据。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [32] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 本文提出了一种基于大规模AI模型训练后参数矩阵的非均匀结构特性，通过统计标准和结构退火过程去除不支持的依赖关系，并揭示稳定独立子结构的方法，从而实现无需修改模型功能或接口的结构化并行推理。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模AI模型在进行推理时通常使用密集参数矩阵，导致推理成本和系统复杂度随着模型规模增长而不可持续地增加。这种限制并非由于模型容量不足，而是因为在处理训练后的推理系统时将其视为单一操作符，忽略了学习过程中形成的内部结构。

Method: 研究者观察到大型模型中的梯度更新事件高度局部化和选择性，使得许多参数依赖在训练后与初始化分布无统计学差异。基于这一发现，他们引入了一个训练后的统计标准和一个结构退火程序，用来移除不受支持的依赖关系，同时暴露稳定的、独立的子结构。

Result: 该方法确立了训练后模型无关的推理系统的结构视图，并允许执行结构化的并行推理，且不需要改变模型的功能或接口。

Conclusion: 通过利用训练后模型中参数依赖性的固有结构，可以优化推理过程，减少不必要的计算开销，同时保持模型性能不变。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [33] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: 提出了IA-HVAE，结合了初始摊销猜测和迭代细化，通过在变换域中创建线性可分离解码器实现了实时应用。与传统HVAE相比，迭代推理速度提高了35倍，并且在去模糊和降噪等逆问题上表现出更好的重建质量。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合初始摊销猜测与基于解码器梯度的迭代细化来改进变分自编码器的推断过程，同时保持高模型深度下的实时性能。

Method: 开发了Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE)，其特征在于使用一个线性可分离的解码器于变换域（如傅里叶空间），以支持非常高的模型深度下的实时应用程序。

Result: IA-HVAE在准确性和速度方面分别优于全摊销和全迭代方法；在诸如去模糊和降噪之类的逆问题中提供了比普通HVAE更好的重建质量；相较于传统的HVAE，迭代推理的速度提升了35倍。

Conclusion: 研究显示，通过引入混合方案——即结合初始摊销估计与迭代细化——IA-HVAE不仅能够显著提高处理速度，而且还能增强模型在解决某些类型逆问题时的表现。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [34] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: 本文提出了一种名为$k$-lazyGD的在线学习算法，该算法在贪婪在线梯度下降（OGD）和懒惰梯度下降/对偶平均之间架起了一座桥梁，并且在平滑在线凸优化(SOCO)场景中分析了这一谱系。研究表明，对于任何不超过$Θ(√{T/P_T})$的懒惰松弛$k$，$k$-lazyGD可以实现最优动态遗憾$\mathcal{O}(√{(P_T+1)T})$，其中$P_T$是比较路径长度。这意味着可以在不牺牲命中性能的情况下实现懒惰更新。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索一种方法，它能够在保持懒惰方法固有的小移动的同时，不损害其跟踪能力。通过引入$k$-lazyGD，研究者们希望找到一种平衡，使得算法既能在需要时保持稳定，又能在必要时变得敏捷。

Method: 本文基于Follow the Regularized Leader (FTRL)框架来分析$k$-lazyGD算法，并且推导了一个匹配的下界。此外，由于松弛程度依赖于$P_T$，因此采用了具有不同松弛程度的学习者集合，以此来确保当可能时算法能够证明是稳定的，而在必须时则能变得敏捷。

Result: 研究表明，对于直到$Θ(√{T/P_T})$的任意懒惰松弛$k$，$k$-lazyGD均可达到最优动态遗憾$\mathcal{O}(√{(P_T+1)T})$。这表明，只要适当选择$k$值，就有可能在不牺牲命中表现的情况下实现懒惰更新。

Conclusion: 通过$k$-lazyGD算法的研究，正式地将允许的懒惰程度与比较器的变化联系起来，表明$k$-lazyGD能够在不影响跟踪能力的前提下保留懒惰方法内在的小幅度变动。

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [35] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 本文提出了一种条件灵活性指数(CFI)，它通过从历史数据中学习参数化的可接受不确定性集，并使用上下文信息来使这些集合有条件，从而扩展了传统的灵活性指数。CFI利用标准化流从高斯基础分布到数据分布学习双射映射，以在更可能相关的条件下定义可接受的不确定性集，从而提供更灵活的信息估计。


<details>
  <summary>Details</summary>
Motivation: 随着过程日益灵活化，确定稳健的调度决策变得越来越重要。传统上，灵活性指数通过使用简单的可接受不确定性集（如超立方体）来近似可接受的不确定性区域，以识别安全的操作计划。然而，目前可用的上下文信息，如预测，尚未被用于在确定灵活性指数时定义可接受的不确定性集。因此，研究提出了条件灵活性指数（CFI），旨在通过结合历史数据和上下文信息改进现有方法。

Method: 本文引入了条件灵活性指数(CFI)的概念，该指数通过从历史数据中学习参数化的可接受不确定性集并结合情境信息使之成为条件性集合，从而扩展了传统的灵活性指数。这通过运用一种称为归一化流的技术实现，该技术能够从高斯基础分布至数据分布之间建立双向映射。基于此，在潜在空间内构造了一个超球体作为可接受的潜在不确定性集合，并将其映射回数据空间。

Result: 研究表明，不能笼统地说数据驱动的可接受不确定性集优于简单集合，或是条件集合优于非条件集合。但是，两者都确保只考虑包含实际实现的不确定参数空间区域。此外，通过将CFI应用于一个安全约束下的机组承诺示例，展示了CFI可以通过整合时间信息来提高调度质量。

Conclusion: 条件灵活性指数(CFI)通过结合历史数据和上下文信息，为给定条件下的相关区域提供了更准确的灵活性估计，从而改进了传统的灵活性指数。尽管不是所有情况下都能证明数据驱动或条件性集合的优势，但它们确实有助于确保仅考虑含有真实情况发生的不确定性参数空间区域。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [36] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: 本文提出了CLASP算法，用于解决约束在线凸优化问题。对于凸损失，CLASP在任意$β∈(0,1)$时达到遗憾$O(T^{\max\{β,1-β\}})$和累积平方惩罚$O(T^{1-β})$；而对于强凸问题，CLASP首次提供了对遗憾和累积平方惩罚的对数级保证，即两者的上界均为$O(\log T)$。


<details>
  <summary>Details</summary>
Motivation: 研究者们关注的是约束在线凸优化（COCO）问题，在这个问题中学习者需要迭代地选择行动，并且观察到未预期的凸损失和凸约束，同时因违反约束而遭受处罚。本文旨在开发一种既能最小化累积损失又能减少约束违规平方和的新算法。

Method: 提出了一种名为CLASP (Convex Losses And Squared Penalties) 的新算法，该方法通过充分利用凸投影算子的坚定非扩张性来进行分析，这是之前的研究没有采用过的策略。

Result: 对于凸损失函数，CLASP实现了$O(T^{\max\{β,1-β\}})$级别的遗憾以及$O(T^{1-β})$级别的累积平方惩罚；而在强凸情形下，CLASP能够提供首次对遗憾和累积平方惩罚都具有对数级别($O(\log T)$)的保证。

Conclusion: CLASP算法为解决约束在线凸优化问题提供了一个有效的方法，特别是在处理强凸问题时展现了优越性能，通过实现对数级别的遗憾与累积平方惩罚上界来改善现有技术。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [37] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 本文应用了可解释的人工智能（XAI）来提高工业信息物理系统（CPS）中机器学习模型的预测性能。通过SHAP值分析时间序列数据分解组件对模型预测的影响，发现训练时缺乏足够的上下文信息。基于此发现，作者通过增加数据实例窗口大小来改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统对于安全性和经济性而言都是敏感基础设施，因此其可靠性至关重要。尽管深度学习越来越多地被集成到这些系统中，但模型本身的复杂性导致操作不透明。为了防止模型在未来的未知数据上表现出意外行为，需要严格的评估。可解释的人工智能(XAI)能够揭示模型推理过程，从而允许进行更全面的行为分析。

Method: 采用XAI技术，特别是利用SHAP(Shapley Additive Explanations)值，分析了时间序列数据分解的不同组成部分如何影响机器学习模型的预测结果。基于XAI提供的见解，研究者识别出模型训练过程中存在的问题，即缺乏充足的背景信息，并据此调整方法，以期提高模型表现。

Result: 研究表明，通过使用XAI方法确实可以识别出影响模型预测的关键因素，即训练期间缺少足够的时间背景信息。根据这一发现，通过增加用于训练的数据实例窗口大小，成功提升了模型的整体性能。

Conclusion: 将XAI应用于工业CPS中的ML模型不仅有助于理解模型决策背后的逻辑，而且还能指导模型优化，例如通过扩大训练数据窗口大小来增强模型对未来未见数据的预测能力。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [38] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 本文提出了一种可能近似正确的（PAC）算法来解决MAP推断问题，该算法在有限计算资源下能够提供最优解，并通过信息理论度量来表征可处理性条件。


<details>
  <summary>Details</summary>
Motivation: 由于最大后验（MAP）估计通常难以处理，即使是在许多常见的结构约束和近似方案下也仍然困难，因此需要找到一种方法能够在有限的计算资源条件下提供最优或接近最优的解决方案。

Method: 研究者们引入了可能近似正确（PAC）算法来进行MAP推断，这些算法在变量和固定计算预算下都能提供理论上最优的解答。他们使用信息理论度量来定义PAC-MAP的可处理性条件，并且通过概率电路的有效实现来完成PAC-MAP求解器的设计。此外，还开发了随机化策略以作为独立的MAP推理技术或改进流行的启发式方法。

Result: 实验表明，在一系列基准测试中，所提出的方法具有优势。这包括在有限样本中估算的信息理论度量可以帮助确定问题的可处理性，以及所设计的PAC-MAP求解器可以有效地实现。

Conclusion: 提出的PAC算法为MAP推断提供了一种新的途径，它不仅能在给定的计算预算内给出最优解，还能与现有的启发式方法相结合，从而增强其解决方案并提供严格的保证。

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [39] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 本研究首次系统地比较了三种或以上针对拉曼光谱分析设计的深度学习分类器，在多个开源拉曼数据集上进行了基准测试，采用统一的训练和超参数调整协议，并报告了分类准确率和宏平均F1分数，为基于拉曼光谱的分类提供了公平且可复现的模型对比。


<details>
  <summary>Details</summary>
Motivation: 目前，虽然深度学习分类器在拉曼光谱分析中的表现优于经典化学计量方法，但它们之间的直接比较很少，尤其是那些专门为拉曼光谱分析开发的深度学习模型。此外，现有的评估通常是孤立进行的，或者与传统的机器学习方法或简单修改的视觉基础架构相比，这些架构原本不是为拉曼光谱提出的。因此，需要一个系统性的基准来比较专门用于拉曼光谱分析的深度学习模型。

Method: 选取了三个支持标准评估、微调以及明确分布偏移测试的开源拉曼数据集，对五种代表性的深度学习架构按照统一的训练和超参数调整协议进行了评价。

Result: 研究报告了分类准确率和宏平均F1分数，旨在提供一个公平且可重复的比较框架，以评估专为拉曼光谱设计的深度学习模型。

Conclusion: 通过本研究提出的系统性基准测试，能够更好地理解不同深度学习模型在处理拉曼光谱数据时的表现差异，为未来相关研究选择合适的方法论提供了参考依据。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [40] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 本文探讨了在核岭回归(KRR)背景下，两种内在维度（$d_ρ$和$d_K$）的概念及其对泛化性能的影响。通过分析与给定概率测度相关的Kolmogorov $n$-宽度与积分算子的特征值之间的关系，得出了一个关于约束KRR过度误差界限的新结果，并提出了一种利用有限样本估计$n$-宽度上限的算法。此外，还展示了对于某些分形集合，有效维度$d_K$可以远小于Minkowski维度$d_ρ$。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索数据集固有维度如何影响机器学习方法特别是核岭回归(KRR)的泛化能力，以及如何通过理解这些维度来改进模型性能。

Method: 文章定义并比较了两种不同的内在维度：基于核函数诱导的标准度量上的上Minkowski维数($d_ρ$)及由Kolmogorov $n$-宽度衰减速率导出的有效维度($d_K$)。接着，通过分析给定概率测度下Kolmogorov $n$-宽度与积分算子特征值间的关系，推导出针对特定条件下KRR的过剩错误边界。同时，提出一种仅需从给定分布中抽取有限样本来估计所有$n$-宽度上限的方法。

Result: 研究表明，对于固定区域Ω，Kolmogorov $n$-宽度能够刻画所有支撑于该区域的概率测度下的最坏情况特征值衰减。这有助于理解受约束KRR的泛化行为，并据此推导出一个关于训练集大小n足够大时的过剩误差界。此外，实验表明对于接近均匀分布的情况，使用有限数量的样本即可高概率地计算出所有$n$-宽度的ε-精确上限。最后，通过对各种分形集合计算有效维度$d_K$发现，在某些情况下它明显小于Minkowski维度$d_ρ$。

Conclusion: 结论是，通过考虑不同类型的内在维度可以帮助更好地理解和提高机器学习模型如KRR的泛化性能。特别地，有效维度$d_K$可能比直观上的几何维度（如Minkowski维度）更小，这对于设计更有效的学习算法具有重要意义。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [41] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: 提出了一种新的对比学习框架Beat-SSL，通过在节奏级和心跳级使用软目标进行双重上下文学习，用于ECG信号的分析。在多标签分类任务中，该模型表现接近广泛预训练的基础模型；在分割任务上则超越了其他方法4%。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习（CL）框架要么只关注全局上下文，要么未能充分利用ECCG特有的特征，并且依赖于硬对比目标，可能无法充分捕捉ECG信号特征相似性的连续性。因此需要一种能更有效地利用有限标注数据并适应ECG特性的新方法。

Method: 提出了Beat-SSL，这是一种对比学习框架，它通过节奏级别和心跳级别的软目标对比来执行双上下文学习。

Result: 在多标签分类任务中，尽管基础模型进行了更广泛的预训练，但Beat-SSL达到了其性能的93%；在分割任务上，Beat-SSL比所有其他方法高出4%。

Conclusion: Beat-SSL证明了在处理ECG数据时，采用考虑特定ECG特性的双上下文学习策略的有效性，特别是在分割任务上的表现尤为突出。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [42] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: 本文介绍了一种名为TTT-Discover的新方法，它通过在测试时进行强化学习来让大语言模型继续训练，并专注于解决特定问题。该方法在数学、GPU内核工程、算法设计和生物学等领域的问题上取得了新的最佳成果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索如何使用AI为科学问题发现新的技术前沿。与之前依赖于提示冻结的大语言模型（LLM）进行搜索的工作不同，作者提出了一种允许LLM在测试时继续训练的方法，以期针对具体问题产生更优解。

Method: 提出了Test-Time Training to Discover (TTT-Discover) 方法，该方法结合了持续学习的特点，但其目标是为特定问题找到最优解而非泛化的解决方案。为此，特别设计了学习目标和搜索子程序来优先考虑最有希望的解决方案。

Result: TTT-Discover 方法几乎在所有尝试过的问题上都设定了新的技术水平，包括Erdős最小重叠问题及自相关不等式、GPUMode内核竞赛(比现有技术快达2倍)、过去的AtCoder算法竞赛以及单细胞分析中的去噪问题。

Conclusion: 研究显示，TTT-Discover不仅能够有效提升多种类型问题的解决效率，而且相比需要封闭前沿模型才能达成的先前最佳结果，它利用开源模型OpenAI gpt-oss-120b即实现了所有成就，并且代码公开可复现，成本低廉。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [43] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 提出了一种新的训练方法，即反事实训练，通过在训练阶段利用反事实解释来增加模型的解释能力，这种方法不仅有助于生成更合理、可操作的解释，还提高了模型对对抗性攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 反事实解释作为一种后验解释方法，对于不透明的机器学习模型来说变得越来越受欢迎。为了使反事实解释在现实世界的决策系统中有用，它们应当与基础数据相符且考虑到特征变异性的限制条件是可行的。本研究旨在让模型直接负责达到这一最终目标，即通过反事实训练促进学习到的表示与合理、可行动的解释之间的差异最小化。

Method: 采用一种名为反事实训练的新颖训练机制，在训练过程中利用反事实解释来减少学习到的表示与合理、可执行解释之间的一致性差距。

Result: 实证和理论分析表明，所提出的方法有助于训练出能够自然地提供理想的反事实解释的模型，并且这些模型还显示出了更好的对抗鲁棒性。

Conclusion: 反事实训练是一种有效的手段，可以提高模型的内在解释能力和对抗鲁棒性。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [44] [ToolCaching: Towards Efficient Caching for LLM Tool-calling](https://arxiv.org/abs/2601.15335)
*Yi Zhai,Dian Shen,Junzhou Luo,Bin Yang*

Main category: cs.SE

TL;DR: 本文提出了一种名为ToolCaching的高效特性驱动和自适应缓存框架，专门针对大型语言模型（LLM）工具调用系统设计。通过集成语义与系统级特性来评估请求的可缓存性和估算缓存价值，并采用VAAC算法结合基于多臂赌博机的准入策略与以价值为导向的多因素淘汰机制。实验表明，相比标准策略，ToolCaching能够实现高达11%的缓存命中率提升及34%的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的研究通过采用传统的计算机系统技术如并行和异步执行改善了工具调用性能，但重复或冗余的工具调用请求问题仍未得到充分解决。对于LLM工具调用应用缓存解决方案时遇到的新挑战包括异构请求语义、动态工作负载以及变化的新鲜度需求，这使得传统缓存策略效果不佳。

Method: 提出了ToolCaching框架，该框架通过整合语义和系统级别的特征来评估请求的可缓存性并估计缓存的价值。核心部分是VAAC算法，它将基于多臂赌博机的准入控制与基于价值的多因素淘汰相结合，同时考虑请求频率、最近访问时间和缓存价值等因素。

Result: 在合成和公开的工具调用工作负载上进行的广泛实验显示，与标准策略相比，使用VAAC的ToolCaching实现了高达11%的更高缓存命中率以及降低了34%的延迟。

Conclusion: ToolCaching为解决LLM工具调用中的冗余请求问题提供了有效的途径，通过其创新的设计不仅提高了缓存效率还显著减少了响应时间，在实际应用中展现了加速LLM工具调用的能力。

Abstract: Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.

</details>


### [45] [Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding](https://arxiv.org/abs/2601.15339)
*Jayant Havare,Ashish Mittal,Srikanth Tamilselvam,Ganesh Ramakrishnan*

Main category: cs.SE

TL;DR: 本文开发了一个多语言语音驱动的代码理解框架，能够接受用户母语的口语查询，通过自动语音识别（ASR）转录，并利用大型语言模型（LLMs）进行代码感知的ASR输出精炼，以执行诸如代码问答和代码检索等任务。研究聚焦于四种广泛使用的印度语言及英语，指出转录错误如何影响下游任务性能，并展示出LLM引导的精炼显著提升了转录和代码理解阶段的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的代码理解系统主要为使用键盘的英语用户设计，这限制了在多语言环境和以语音为主的场景下的可访问性，特别是在像印度这样的地区。为了提供更包容的交互方式，需要解决与代码相关的口语查询所带来的独特挑战，包括非标准英语用法、领域特定词汇以及自定义标识符的混合表达。

Method: 研究人员开发了一种多语言语音驱动的框架来促进代码理解。该框架首先通过自动语音识别技术将用户的口语查询转化为文本，然后运用大型语言模型对转换结果进行优化处理，特别是针对代码相关内容进行精细化调整。最后，该框架能够与代码模型相连接，执行包括代码问答和代码检索在内的多种任务。

Result: 实验集中在四种流行的印度语言加上英语上，系统地分析了转录错误对后续任务表现的影响。此外，还发现了ASR处理代码时的关键失败模式，并证明了由大型语言模型指导的细化过程可以显著提高从转录准确性到代码理解效率的整体性能。

Conclusion: 研究表明，在语音接口中加入针对代码特性的适应措施是必要的，并且提出了一个实用的解决方案，用于构建健壮的、支持多语言的语音驱动编程工具。

Abstract: Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.

</details>


### [46] [A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs](https://arxiv.org/abs/2601.15352)
*Adeyemi Adeseye,Aisvarya Adeseye*

Main category: cs.SE

TL;DR: 本文提出了一种基于本地大语言模型（LLM）的提示框架，用于检测Python 3.7+代码中的循环漏洞。该框架针对三类与循环相关的问题，并通过迭代提示引导模型行为。实验结果表明，在准确率、召回率和F1分数方面，Phi模型优于LLaMA模型。


<details>
  <summary>Details</summary>
Motivation: 循环漏洞是软件开发中一个主要的风险来源，可能导致无限循环或执行、资源耗尽或引入降低性能和危害安全的逻辑错误。传统的静态分析工具由于依赖于语法模式而难以发现语义缺陷。大语言模型因其能够理解代码上下文为漏洞检测提供了新的可能性。此外，本地部署的大语言模型相比商业模型解决了隐私、延迟和依赖性问题，支持高效的离线分析。

Method: 设计了一个通用且结构化的基于提示的框架，利用本地部署的大语言模型来检测Python 3.7+代码中的循环漏洞。该框架专注于控制与逻辑错误、循环内部的安全风险以及资源管理效率低下这三类问题。通过迭代提示指导模型的行为，并包含了特定语言意识、代码感知基础、版本敏感性和幻觉预防等关键保护特性。

Result: 使用两个本地部署的大语言模型（LLaMA 3.2; 3B 和 Phi 3.5; 4B）对设计的基于提示的框架进行了测试。将大语言模型的结果与手动建立的基准真值进行了验证，结果显示在精度、召回率和F1分数方面Phi优于LLaMA。

Conclusion: 研究结果强调了为本地大语言模型设计有效提示以执行安全准确的代码漏洞分析的重要性。

Abstract: Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.

</details>


### [47] [Evaluating and Achieving Controllable Code Completion in Code LLM](https://arxiv.org/abs/2601.15879)
*Jiajun Zhang,Zeyu Cui,Lei Zhang,Jian Yang,Jiaxi Yang,Qiang Liu,Zilei Wang,Binyuan Hui,Liang Wang,Junyang Lin*

Main category: cs.SE

TL;DR: 提出了首个指令引导的代码补全基准C3-Bench，通过评估40多个主流大语言模型在C3-Bench和传统基准上的表现，揭示了开源与高级专有模型在遵循用户指令方面的能力差距，并通过一个简单的数据合成管道生成高质量的指令-完成对进行监督微调，得到的新模型Qwen2.5-Coder-C3在C3-Bench上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的代码补全评价方法主要关注基于给定上下文的功能正确性，而忽略了模型在补全过程中遵循用户指令的能力，这是LLM辅助编程中的常见场景。

Method: 开发了一个名为Controllable Code Completion Benchmark (C3-Bench)的基准测试集，包含2,195个精心设计的补全任务；并对超过40种主流大语言模型进行了全面评估；还提出了一种利用Qwen2.5-Coder生成高质量指令-完成对的数据合成方法，用于监督微调以提高模型性能。

Result: 研究发现开源与先进的专有模型之间在代码补全时遵循指示的能力存在显著差异；经过特定数据集训练后的新模型Qwen2.5-Coder-C3在C3-Bench上表现最优。

Conclusion: 这项工作为提升大语言模型在代码补全及遵从指令方面的性能提供了有价值的见解，并为未来关于代码大语言模型的研究指明了新的方向。同时，所有代码、数据集和模型均被开源以促进可重复性和进一步的研究。

Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.

</details>


### [48] [The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations](https://arxiv.org/abs/2601.16009)
*Giovanna Broccia,Sira Vegas,Alessio Ferrari*

Main category: cs.SE

TL;DR: 该研究探讨了UML序列图与基于文本的需求结合使用是否能比单独使用基于文本的需求更准确地进行需求检查，并研究了认知能力（如工作记忆和心理旋转技能）对两种处理方式（仅文本与UML辅助的文本）下表现差异的影响。实验结果显示，表示类型、工作记忆容量和心理旋转能力之间存在显著的三向交互作用，表明UML支持的有效性因人而异：在同时具备高水平两种认知能力的参与者中，使用UML进行违规检测时表现反而下降；然而，在UML辅助检查时，同样的认知特征却提高了理由准确性，说明高认知能力可能有助于处理图表和文本这类多模态信息时进行更深层次的推理过程。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估UML序列图与基于文本的需求一起使用是否能够提高需求检查的准确性，以及探索认知能力（例如工作记忆和心理旋转技能）是否会影响两种处理方式（纯文本与UML辅助文本）下的表现差异。

Method: 通过一个包含38名参与者的交叉实验来评估两种处理条件下需求检查的准确性，包括发现问题的数量及提供的理由。采用线性和广义线性混合效应模型分析处理方式、时期、顺序以及认知能力的影响。

Result: 结果揭示了表示形式、工作记忆能力和心理旋转能力之间存在显著的三方交互效应。这意味着对于那些在这两项认知能力上得分较高的参与者来说，利用UML进行违规检测时的表现反而有所降低；但是，相同的认知特质却能在UML辅助检验时提升理由的准确性。

Conclusion: 结论是UML支持在需求审查中的有效性并不是普遍适用的，而是取决于个体的认知能力。具有较高工作记忆和心理旋转能力的人在使用UML识别违规行为时表现较差，但这些人在提供基于UML的支持的理由时显示出更高的准确性。

Abstract: The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.

</details>


### [49] [Towards a Goal-Centric Assessment of Requirements Engineering Methods for Privacy by Design](https://arxiv.org/abs/2601.16080)
*Oleksandr Kosenkov,Ehsan Zabardast,Jannik Fischbach,Tony Gorschek,Daniel Mendez*

Main category: cs.SE

TL;DR: 本文提出了一种以目标为中心的方法来评估GDPR下的隐私设计(PbD)要求工程(RE)方法，通过文献回顾、访谈和实践者验证来完成研究，建议根据组织目标而非仅过程特征来评估PbD的RE方法。


<details>
  <summary>Details</summary>
Motivation: 当前，尽管有越来越多的要求工程（RE）方法来实现《通用数据保护条例》(GDPR)下的隐私设计(PbD)，但哪种RE方法最适合组织的目标仍是一个挑战。

Method: 采用文献综述、访谈以及与从业者的验证等手段，综合出一种以目标为中心的PbD方法评估方式。

Result: 研究表明，从业人员对于PbD并没有系统性的方法；因此，建议对PbD的RE方法应根据组织目标进行评估，而不仅仅是基于过程特点。

Conclusion: 当进一步发展时，这种以目标为中心的方法可能有助于开发、选择和调整PbD的需求工程实践。

Abstract: Implementing privacy by design (PbD) according to the General Data Protection Regulation (GDPR) is met with a growing number of requirements engineering (RE) approaches. However, the question of which RE method for PbD fits best the goals of organisations remains a challenge. We report our endeavor to close this gap by synthesizing a goal-centric approach for PbD methods assessment. We used literature review, interviews, and validation with practitioners to achieve the goal of our study. As practitioners do not approach PbD systematically, we suggest that RE methods for PbD should be assessed against organisational goals, rather than process characteristics only. We hope that, when further developed, the goal-centric approach could support the development, selection, and tailoring of RE practices for PbD.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: 本文提出了一种名为NL4ST的交互工具，该工具支持用户使用自然语言查询时空数据库，并通过三层架构来实现从自然语言理解到生成物理计划的过程。


<details>
  <summary>Details</summary>
Motivation: 随着移动计算设备和定位技术的进步，时空数据在数据库中的管理需求激增。然而，非专家用户往往难以掌握领域特定的专业知识及可执行查询语言，这使得他们很难独立完成相关查询任务。因此，迫切需要一种能够支持自然语言查询的方法来降低使用门槛。

Method: 作者开发了一个叫做NL4ST的工具，它采用了三层架构设计：(i) 知识库与语料库用于准备必要的背景信息；(ii) 自然语言理解层负责实体链接等任务；(iii) 物理计划生成层根据前两步的结果创建有效的查询执行方案。

Result: 通过四个真实及合成数据集测试表明，NL4ST能够有效地提供时空数据库查询服务。此外，研究团队还提供了在线版本的NL4ST以及演示视频以供进一步了解。

Conclusion: NL4ST作为连接非专业用户与复杂时空数据库查询之间的桥梁，其通过自然语言处理技术简化了查询过程，为广泛用户群体访问和利用时空数据提供了便利。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [51] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: 提出了一种新的增量函数依赖发现算法EAIFD，通过维护差集的部分超图和引入多属性哈希表等技术，显著提高了运行速度并减少了内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的静态算法在处理增量更新时效率低下，而增量算法则面临严重的性能和内存瓶颈问题。

Method: EAIFD算法通过维护差集的部分超图，并将增量FD发现问题重新定义为超图上的最小打击集枚举问题来避免完全重跑。此外，该方法还设计了多属性哈希表（MHT）用于有效FD的高频键值映射，并开发了两步验证策略以高效地验证候选者。

Result: 实验结果表明，与现有算法相比，EAIFD在实际数据集上实现了高达一个数量级的运行时间加速，同时内存使用量减少了两个数量级以上。

Conclusion: EAIFD是一种高效且可扩展的解决方案，适用于增量函数依赖发现。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [52] [Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia and Grokipedia on Controversial Topics](https://arxiv.org/abs/2601.15484)
*Philipp Eibl,Erica Coppolillo,Simone Mungari,Luca Luceri*

Main category: cs.IR

TL;DR: 本研究对比分析了维基百科和Grokipedia在政治争议话题上的语义框架、政治倾向及内容优先级，发现两者在文章不同部分的语义相似性逐渐减弱，并且在争议话题上差异更加明显。尽管Grokipedia展现出更多右倾内容，但两平台主要呈现左倾框架。


<details>
  <summary>Details</summary>
Motivation: 鉴于在线百科全书作为现代信息基础设施的核心地位及其意识形态偏见的争议焦点，特别是关于维基百科被认为具有左倾偏见而Grokipedia被视作右倾替代品的讨论，这项研究旨在通过比较这两个平台在已知政治争议话题上的表现来探索它们之间存在的具体差异。

Method: 本研究采用比较分析方法，针对维基百科与Grokipedia平台上有关政治争议话题的文章进行考察。研究特别关注了语义框架、政治立场以及内容优先排序等方面的差异。此外，还通过公开实验代码增强了研究透明度。

Result: 研究结果显示，在有争议的话题中，维基百科与Grokipedia之间的语义相似度随着文章部分的不同而递减；相较于随机选取的话题，争议性更强的主题显示出更大的差异。尽管Grokipedia表现出更明显的双峰分布特征，含有较多右翼观点的内容，但两大百科全书整体上仍然偏向于左翼框架。

Conclusion: 尽管存在一些差异，特别是在处理争议性较强的话题时，维基百科和Grokipedia均倾向于以左倾的方式构建其内容。这表明，即便是在试图提供一个相对平衡视角的新平台如Grokipedia上，也难以完全摆脱某种形式的政治倾向。

Abstract: Online encyclopedias are central to contemporary information infrastructures and have become focal points of debates over ideological bias. Wikipedia, in particular, has long been accused of left-leaning bias, while Grokipedia, an AI-generated encyclopedia launched by xAI, has been framed as a right-leaning alternative. This paper presents a comparative analysis of Wikipedia and Grokipedia on well-established politically contested topics. Specifically, we examine differences in semantic framing, political orientation, and content prioritization. We find that semantic similarity between the two platforms decays across article sections and diverges more strongly on controversial topics than on randomly sampled ones. Additionally, we show that both encyclopedias predominantly exhibit left-leaning framings, although Grokipedia exhibits a more bimodal distribution with increased prominence of right-leaning content. The experimental code is publicly available.

</details>


### [53] [DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking](https://arxiv.org/abs/2601.15518)
*Wenxin Zhou,Ritesh Mehta,Anthony Miyaguchi*

Main category: cs.IR

TL;DR: 本文开发了一个两阶段检索系统，结合了多种互补的检索方法以及学习排序器和基于大语言模型的重排序，以解决TREC Tip-of-the-Tongue (ToT) 任务。最佳系统通过将混合检索与Gemini-2.5-flash重排序相结合，在测试集上达到了0.66的召回率和0.41的NDCG@1000。


<details>
  <summary>Details</summary>
Motivation: 为了解决TREC Tip-of-the-Tongue (ToT) 任务，需要一个更有效的检索系统。

Method: 采用了混合检索策略，包括基于大语言模型的检索、稀疏（BM25）和密集（BGE-M3）检索方法，并引入了主题感知多索引密集检索；第二阶段使用训练过的LambdaMART重排序器和基于大语言模型的重排序进行评估。

Result: 最佳配置下，该系统在测试集上的召回率为0.66，NDCG@1000为0.41。

Conclusion: 结合混合检索与Gemini-2.5-flash重排序的方法证明了融合检索的有效性。

Abstract: We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.

</details>


### [54] [Enhancing guidance for missing data in diffusion-based sequential recommendation](https://arxiv.org/abs/2601.15673)
*Qilong Yan,Yifei Xing,Dugang Liu,Jingpu Duan,Jian Yin*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的反事实注意力调节扩散模型CARD，旨在通过放大关键兴趣转折点项目的信号并同时识别和抑制用户序列中的噪音，来提高生成质量。该方法包括双重汤普森采样法以识别经历显著兴趣转移的序列，以及一种反事实注意力机制来量化每个项目的重要性。实验表明，该方法在实际数据上表现良好且计算成本不高。


<details>
  <summary>Details</summary>
Motivation: 现有的序列推荐方法由于观察序列中缺失的数据导致用户信息指导的质量下降，影响了生成质量。虽然现有方法试图通过移除局部相似项来解决这一问题，但它们忽略了用户兴趣中的‘关键转折点’，这对于准确预测后续用户意图至关重要。

Method: 本文提出了一个名为CARD的新模型，它使用双重汤普森采样技术来识别出经历重要兴趣转变的序列，并采用反事实注意力机制来衡量这些序列中各个条目的重要性。这样做的目的是为扩散模型提供高质量的引导信号，即动态重新加权的交互向量，从而实现有效的生成。

Result: 实验证明，所提出的方法在真实世界的数据集上表现出色，而且没有带来过高的计算成本。

Conclusion: CARD模型通过其独特的设计有效地解决了因用户兴趣变化而产生的序列推荐挑战，提高了生成结果的相关性和准确性。

Abstract: Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.

</details>


### [55] [CoNRec: Context-Discerning Negative Recommendation with LLMs](https://arxiv.org/abs/2601.15721)
*Xinda Chen,Jiawei Wu,Yishuang Liu,Jialin Zhu,Shuwen Xiao,Junjun Zheng,Xiangheng Kong,Yuning Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种新的大语言模型框架，专门用于处理负面反馈建模，通过特别设计的上下文辨别模块、语义ID表示和渐进式GRPO训练范式来提高对负面反馈背后语义背景的理解，并引入了基于多天未来负面反馈及其协同信号的新奖励函数和评估指标，以解决传统方法中正面反馈主导导致的上下文理解偏差问题。


<details>
  <summary>Details</summary>
Motivation: 用户不喜欢什么比喜欢什么更难理解，且在现代推荐系统中，用户的负面偏好研究变得越来越重要。尽管许多平台已经引入了明确的负面反馈机制来改进推荐模型，但大多数现有方法主要将负面反馈作为增强正面推荐的辅助信号，很少直接对负面兴趣进行建模。此外，由于负面反馈数据本身的稀疏性，模型往往受到由正面反馈主导引起的情境理解偏见的影响。

Method: 提出了首个带有特殊设计的上下文辨别模块的大语言模型框架，使用语义ID表示替换基于文本的项目描述，并引入了项目级对齐任务以增强LLM对负面反馈背后语义情境的理解。同时，设计了一种渐进式GRPO训练范式，使模型能够动态平衡正负行为情境的利用。

Result: 所提方法不仅解决了传统方法中存在的上下文理解偏差问题，还揭示了传统下一次负面项预测目标与用户真实负面偏好之间的根本错位现象，并通过提出一种新奖励函数和评估指标来缓解这一问题。

Conclusion: 本研究提出的针对负面反馈建模的方法为推荐系统领域提供了新颖有效的解决方案，特别是在改善用户体验驱动度量如负面反馈率方面显示出了潜在价值。

Abstract: Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.

</details>


### [56] [CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval](https://arxiv.org/abs/2601.15849)
*Tsung-Hsiang Chou,Chen-Jui Yu,Shui-Hsiang Hsu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: 本文提出了一种新的训练框架CGPT，通过使用大语言模型生成的监督来增强表格检索。该方法通过对表格实例进行聚类并跨簇采样以构建语义多样化的部分表格，并由大语言模型为这些部分表格生成合成查询，进而通过硬负例对比微调来优化嵌入模型。实验表明，CGPT在四个公开基准上始终优于包括QGpT在内的检索基线，在R@1指标上平均提升了16.54%。此外，CGPT展示了强大的跨域泛化能力，即使使用较小的大语言模型生成合成查询也依然有效。


<details>
  <summary>Details</summary>
Motivation: 通用嵌入模型在文本检索中表现出色，但在表格检索方面表现不佳，因为高度结构化的内容导致了语义压缩和查询-表格不匹配的问题。最近基于大型语言模型（LLM）的检索增强方法通过生成合成查询来缓解这一问题，但它们往往依赖于启发式的部分表格选择，并且很少利用这些合成查询作为监督来改进嵌入模型。

Method: CGPT是一种旨在通过LLM生成的监督来增强表格检索的训练框架。它首先通过K-means算法对表格实例进行聚类，并从不同簇中抽样以构建语义多样性高的部分表格；接着，利用LLM为这些部分表格生成合成查询；最后，使用这些合成查询进行硬负例对比微调，从而精细化调整嵌入模型。

Result: 在四个公开基准测试（MimoTable、OTTQA、FetaQA 和 E2E-WTQ）上的实验结果表明，CGPT相较于其他检索基线，包括QGpT，在R@1指标上平均提高了16.54%。同时，在统一的多领域语料库设置下，CGPT还展示了良好的跨域泛化能力，即便使用规模较小的LLM生成合成查询时也能保持高效。

Conclusion: 研究结果表明，通过语义引导的部分表格构建与基于LLM生成监督的对比学习相结合，提供了一种对于大规模表格检索既有效又可扩展的方法论。

Abstract: General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.

</details>


### [57] [STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion](https://arxiv.org/abs/2601.15860)
*Shui-Hsiang Hsu,Tsung-Hsiang Chou,Chen-Jui Yu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: 提出了STAR框架，通过语义聚类和加权融合改进表格的语义表示，从而在五个基准测试中比QGpT方法获得了更高的召回率。


<details>
  <summary>Details</summary>
Motivation: 现有方法如QGpT尝试通过生成合成查询来丰富表格语义，但它们依赖于粗略的部分表格采样和简单的融合策略，这限制了语义多样性并阻碍了有效的查询-表格对齐。

Method: STAR首先应用header-aware K-means聚类将语义相似的行分组，并选择代表性的中心实例构建一个多样化的部分表格；接着生成特定于簇的合成查询以全面覆盖表格的语义空间；最后采用加权融合策略整合表格和查询嵌入，实现细粒度的语义对齐。

Result: 实验显示，在五个基准数据集上，STAR相比QGpT达到了一致更高的召回率。

Conclusion: 通过引入语义聚类与自适应加权融合技术，STAR能够捕捉来自结构化和文本源的互补信息，提高表格表示的表现力。

Abstract: Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.

</details>


### [58] [MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging](https://arxiv.org/abs/2601.15930)
*Tianjun Wei,Enneng Yang,Yingpeng Du,Huizhong Guo,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 该论文首次系统地研究了生成推荐（GR）中的模型合并（MM），提出了一个统一框架MMGRid，通过结构化的上下文网格组织在不同上下文中训练的GR检查点。研究揭示了从大型语言模型（LLMs）训练GR模型时可能遇到的参数冲突问题以及如何通过基础模型替换来缓解这些问题，并且提出了一种基于加权上下文合并的方法以平衡增量训练带来的近期偏差。


<details>
  <summary>Details</summary>
Motivation: 随着生成推荐（GR）在推荐系统中的兴起，模型规模迅速增长和计算成本显著增加，使得模型合并（MM）成为一种有吸引力的技术，特别是在对成本敏感的部署场景中。然而，在不同的实际应用场景和随时间变化的用户行为下，如何有效地合并专门化到不同现实世界上下文的生成推荐器成为一个基本但未被充分探索的挑战。

Method: 论文提出了一种名为MMGRid的统一框架，该框架构建了一个由共享基础大型语言模型衍生出但在特定上下文数据上微调过的GR模型组成的结构化上下文网格。通过对这个受控模型空间的研究，作者们分析了跨GR范式和合并算法下的模型合并过程。

Result: 研究发现，从大型语言模型训练GR模型可能导致由于标记分布偏移和目标差异而产生的参数冲突；通过区分任务感知与上下文特定参数的变化并采用基础模型替换的方式可以缓解这类冲突。此外，还发现跨上下文的增量训练会引入最近性偏差，这可以通过加权上下文合并得到有效平衡。最优合并权重的选择与上下文依赖的交互特征相关联。

Conclusion: 本文的工作为理解和改进生成推荐系统中的模型合并提供了新的视角，展示了如何通过设计合理的合并策略来应对不同上下文间模型整合所面临的挑战，为实际应用中的模型合并实践提供了有价值的指导。

Abstract: Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.

</details>


### [59] [Unveiling and Simulating Short-Video Addiction Behaviors via Economic Addiction Theory](https://arxiv.org/abs/2601.15975)
*Chen Xu,Zhipeng Yi,Ruizi Wang,Wenjie Wang,Jun Xu,Maarten de Rijke*

Main category: cs.IR

TL;DR: 研究结合经济学成瘾理论和推荐系统捕捉到的用户隐性行为，分析短视频成瘾现象，并提出一种新的训练框架AddictSim来模拟这些模式。实验结果表明，AddictSim在两个大规模数据集上的表现优于现有策略，并且整合多样性意识算法能够有效缓解成瘾行为。


<details>
  <summary>Details</summary>
Motivation: 短视频应用吸引了大量用户流量，但同时也导致了对用户健康及平台可持续发展构成威胁的成瘾使用模式。以往针对此问题的研究主要依赖问卷调查或基于志愿者的数据收集方式，存在样本量小和人口偏差的问题。而短视频平台拥有大规模的行为数据，为分析成瘾行为提供了宝贵的基础。

Method: 研究将经济成瘾理论与通过推荐系统捕捉到的用户隐含行为相结合，分析短视频成瘾的功能模式，并介绍了一种新的训练框架AddictSim用于学习和建模这些模式。为了考虑个性化的成瘾模式，AddictSim采用了一种均值适应策略与群体相对策略优化训练方法。

Result: 研究表明，短视频成瘾遵循类似于传统形式（如物质滥用）的功能模式，并且其强度与先前社会科学研究所发现的结果一致。实验结果证明，在两个大规模数据集上，AddictSim的表现持续优于现有的训练策略。模拟结果显示，整合多样性意识算法可以很好地减轻成瘾行为。

Conclusion: 通过结合经济学成瘾理论和用户行为数据分析短视频成瘾现象是可行的。提出的AddictSim框架不仅能够有效地模拟个性化成瘾模式，而且展示了通过引入多样性意识算法减少成瘾行为的可能性。

Abstract: Short-video applications have attracted substantial user traffic. However, these platforms also foster problematic usage patterns, commonly referred to as short-video addiction, which pose risks to both user health and the sustainable development of platforms. Prior studies on this issue have primarily relied on questionnaires or volunteer-based data collection, which are often limited by small sample sizes and population biases. In contrast, short-video platforms have large-scale behavioral data, offering a valuable foundation for analyzing addictive behaviors. To examine addiction-aware behavior patterns, we combine economic addiction theory with users' implicit behavior captured by recommendation systems. Our analysis shows that short-video addiction follows functional patterns similar to traditional forms of addictive behavior (e.g., substance abuse) and that its intensity is consistent with findings from previous social science studies. To develop a simulator that can learn and model these patterns, we introduce a novel training framework, AddictSim. To consider the personalized addiction patterns, AddictSim uses a mean-to-adapted strategy with group relative policy optimization training. Experiments on two large-scale datasets show that AddictSim consistently outperforms existing training strategies. Our simulation results show that integrating diversity-aware algorithms can mitigate addictive behaviors well.

</details>
