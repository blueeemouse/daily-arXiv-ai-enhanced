<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 在仅使用过去室内温度值进行长期外生温度预测的研究中，线性模型（如DLinear）的表现优于更复杂的Transformer系列模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索在具有挑战性的单变量环境中，即只使用过去室内温度值进行预测的情况下，线性模型与Transformer系列模型的性能对比。

Method: 采用了一系列模型包括线性模型（Linear、NLinear、DLinear）和Transformer家族模型（Transformer、Informer、Autoformer），并在标准化的训练、验证和测试数据集上进行了评估。

Result: 结果表明，在所有数据集划分中，线性基线模型（特别是DLinear）比更复杂的Transformer家族架构表现得更好。

Conclusion: 精心设计的线性模型在面对仅有外生变量的时间序列预测难题时，仍然是非常强大的基准方法。

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [2] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: 该论文提出了一种针对离散扩散模型的引导式迁移学习方法（GTL），可以在不修改预训练去噪器的情况下从目标分布中采样，并且引入了一种高效的引导采样器，以减少大规模词汇和长序列情况下的计算时间和资源。


<details>
  <summary>Details</summary>
Motivation: 现有的离散扩散模型虽然在语言和其他离散领域表现出色，但其高性能依赖于大量训练数据集，这在新领域的适应上成本高昂或风险较大。当前的迁移学习方法需要对大型扩散模型进行微调，这在计算上既昂贵又常常不切实际。

Method: 基于连续扩散的比率基础迁移学习，提出了适用于离散扩散模型的引导式迁移学习方法（GTL）。该方法允许从目标分布中采样而不需调整预训练的去噪器。同时，为了处理大词汇量和长序列时遇到的效率问题，开发了一种高效引导采样器，通过集中评估规划选择的位置和顶级候选令牌来降低采样时间和计算量。

Result: GTL 方法在合成马尔可夫链和语言建模等顺序数据上进行了评估，并提供了关于其行为的经验分析。结果显示这种方法使得在大规模词汇及长序列上的引导式语言建模变得可行。

Conclusion: 本研究为离散扩散模型提供了一个新的迁移学习途径，不仅提高了模型对于新领域的适应性，还通过优化采样过程降低了计算需求，从而促进了此类模型在更多应用场景中的实用性。

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [3] [Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields](https://arxiv.org/abs/2512.10886)
*Stefan Matthes,Markus Schramm*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的学习框架，可以从常规运行数据中推断出槽式聚光太阳能电站回路级质量流比和时变接收器传热系数。该方法通过夜间均质化期间的数据来隔离水力和热损失效应，并使用Andasol 3太阳能场的历史数据进行了优化。模型能够准确重建回路温度并估计回路不平衡和接收器热损失，与无人机红外热成像结果高度一致。


<details>
  <summary>Details</summary>
Motivation: 槽式聚光太阳能电站需要在空间上不均匀的光学性能、热量损失和压力下降的情况下保持输出温度的一致性。然而，由于缺乏对回路级别的流量以及接收器热损失参数的直接观测，使用标准监控工具难以诊断水力失衡或接收器退化情况。

Method: 提出了一个结合了物理信息的学习框架，该框架可以直接从日常操作数据中推断出回路级别的质量流比及时变接收器热传递系数。此方法利用非光照条件下热油循环的夜间均一化时段来单独分析水力状况及热损耗影响。采用可微分的共轭热传递模型，并将其嵌入到端到端的学习流程中，使用来自50兆瓦安达索尔3号太阳能田的历史工厂数据进行优化。

Result: 模型能够以小于2°C的均方根误差（RMSE）精确重建回路温度，并提供具有物理意义的回路失衡和接收器热损失估计。与基于无人机的红外热成像(QScan)比较显示了强烈的相关性，正确识别了所有高损耗接收器区域。

Conclusion: 研究证明，即使是在噪声较大的实际CSP运营数据中，只要结合适当的建模和可微分优化技术，也足以恢复潜在的物理参数。

Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.

</details>


### [4] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 本文通过BlueROV2平台研究了基于深度强化学习（PPO算法）的水下自主导航方法，该方法结合了目标导向导航信息、虚拟占用网格和边界光线投射。实验结果表明，在高度杂乱的环境中，所提出的方法优于传统的DWA规划器，并且能够很好地从模拟环境转移到现实世界应用中。


<details>
  <summary>Details</summary>
Motivation: 由于GPS信号缺失、能见度低以及存在水下障碍物，水下环境中的自主导航仍然是一个重大挑战。为了解决这些问题，本研究以开放性科学实验平台BlueROV2为例进行了探讨。

Method: 采用了基于近端策略优化(PPO)算法的深度强化学习方法，其观测空间包括了朝向目标的导航信息、虚拟占据网格及沿操作区域边界的光线投射数据。此外，还利用了一个逼真的仿真环境进行评估，并通过与3D数字孪生监督下的物理BlueROV2验证来降低实际测试的风险。

Result: 实验结果显示，相比于作为鲁棒基线的动态窗口方法(DWA)，PPO策略在充满障碍物的环境中表现更优，特别是在局部适应性和减少碰撞方面。同时，实验也证明了从仿真到真实世界的迁移能力。

Conclusion: 研究表明，深度强化学习特别是PPO算法非常适合于解决水下机器人领域内复杂的自主导航问题，提供了比传统方法更好的解决方案。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [5] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种新的算法，通过优化策略以对抗从原始分块评论家乐观地回溯构建的蒸馏评论家，从而解决在使用分块评论家时遇到的开环次优性和学习长动作序列策略的困难。该方法保留了多步价值传播的优点，并在长时域离线目标条件任务中优于先前的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的TD方法由于自举机制容易产生自举偏差，导致价值估计有偏。虽然已有工作提出了分块评论家来加速价值备份，但从中提取策略存在挑战，因为策略必须以开环方式输出整个动作序列，这可能对于需要策略响应性的环境来说是次优的，特别是当动作序列长度增加时建模变得困难。

Method: 提出的新算法将批评者的动作序列长度与策略的动作序列长度解耦，允许策略基于较短的动作序列操作。通过针对部分动作序列的蒸馏评论家进行优化，该蒸馏评论家通过从原始分块评论家乐观地回溯构建而成，以近似当部分动作序列扩展为完整序列时可达到的最大值。

Result: 所提方法在具有挑战性的、长时域的离线目标条件下任务上进行了评估，并显示出比之前的方法更可靠的表现。

Conclusion: 新算法有效解决了使用分块评论家时遇到的问题，同时保持了多步价值传播的好处，在处理长时域任务时表现优异。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [6] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 本研究介绍了一种无需额外训练即可使大型语言模型（LLM）在接收新信息的同时进行思考和生成输出的方法，通过利用旋转嵌入的特性，使得原本设计用于顺序交互的LLM能够同时听、想、说。该方法在数学、常识以及安全性推理任务上进行了评估，结果显示它能实时生成准确的答案，并将首个非思考token出现的时间从几分钟减少到不超过5秒，整体实时延迟降低了6-11倍。


<details>
  <summary>Details</summary>
Motivation: 当前许多先进的大型语言模型虽然具备了思考能力以增强其性能和安全性，但这也导致它们在互动性方面表现较差：面对新的输入时，模型必须先完成思考才能作出回应。这与语音助手或嵌入式助手等实际应用场景中要求模型能够即时响应并适应附加信息的需求相矛盾。因此，研究旨在开发一种技术，让这些模型能够在不牺牲即时响应能力的前提下保持思考能力。

Method: 研究者们提出了一种基于现有大型语言模型的技术，通过利用旋转嵌入（rotary embeddings）的特性来实现。这种方法允许模型在处理连续输入的同时进行思考和生成答案，从而模仿人类边听边想边回答的能力。整个过程不需要对模型进行额外训练。

Result: 实验结果表明，在数学问题解决、常识推理及安全相关的推理任务中，所提出的方法能够显著提高模型响应的速度，将产生首个非思考token所需时间从几分钟缩短至不超过5秒钟；同时，总体上的实时延迟也得到了大幅度降低，减少了6到11倍。

Conclusion: 这项工作展示了一种有效提升大型语言模型实时互动能力而不牺牲其思考能力的方法。通过采用旋转嵌入技术，模型可以在接收新信息的过程中同步地进行思考和响应，这对于需要快速反应的应用场景特别有用。

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [7] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 本研究探索了点态函数的内在属性如何影响训练和性能，并通过大规模搜索发现了比现有方法（如LayerNorm、RMSNorm和DyT）更有效的函数设计Derf(x) = erf(αx + s)，该函数在视觉、语音表示和DNA序列建模等多个领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 鉴于Dynamic Tanh (DyT)作为归一化层替代方案的成功，这项工作旨在进一步探索能够超越DyT表现的函数设计方案。

Method: 首先研究了点态函数的固有特性对训练及性能的影响；基于这些发现进行了大规模的有效函数设计搜索，最终引入了Derf(x) = erf(αx + s)这一新函数设计，其中erf(x)代表重缩放的高斯累积分布函数。

Result: 实验结果表明，与LayerNorm、RMSNorm以及DyT相比，Derf在包括图像识别与生成、语音表示及DNA序列建模在内的多个领域都展现出更好的性能。此外，Derf带来的性能提升主要源于其更强的泛化能力而非拟合能力。

Conclusion: 由于其简单性和卓越的表现，Derf成为无归一化Transformer架构中的实用选择。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [8] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: 本文提出了一种名为DaSH的方法，用于从大量异构数据集中选择整个数据集以提高下游性能。实验表明，DaSH在准确性和效率上都优于现有的数据选择基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独选择样本，并假设所有数据都是等同相关的，忽略了数据集及其来源之间的差异。因此，需要一种能够考虑数据集和组级别实用性、并能有效从有限观察中泛化的数据集选择方法。

Method: 提出了Dataset Selection via Hierarchies (DaSH)方法，该方法在数据集和组（例如集合、机构）级别建模实用性，从而能够在资源约束下通过有限的探索步骤来改进模型训练表现。

Result: DaSH在两个公开基准测试（Digit-Five 和 DomainNet）上比最先进的数据选择基线最多提高了26.2%的准确性，同时所需探索步骤明显减少。此外，消融研究表明DaSH对于低资源设置以及缺乏相关数据集的情况具有鲁棒性。

Conclusion: DaSH提供了一种有效的解决方案，适用于实际多源学习工作流程中的可扩展和自适应数据集选择。

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [9] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架BiFlow，该框架不需要精确的解析逆过程，而是学习一个近似从噪声到数据逆映射的反向模型。实验表明，与因果解码方法相比，BiFlow在提高生成质量的同时，采样速度提高了两个数量级，并且在基于NF的方法中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 标准的归一化流（NFs）通常受限于显式可逆性要求，这确保了反向过程可以作为其确切的解析逆。然而，最近TARFlow及其变体的发展虽然通过结合Transformers和自回归流重振了NF方法，但也暴露了解码为一个重要瓶颈。因此，作者提出了BiFlow框架以去除对精确解析逆的需求，允许更灵活的损失函数和架构设计。

Method: BiFlow框架通过学习一个反向模型来近似底层从噪声到数据的逆映射，从而消除了对精确分析逆的需求。这种方法不仅使得损失函数和架构选择更加灵活，还能够加速采样过程。

Result: ImageNet上的实验显示，与因果解码对应物相比，BiFlow提高了生成质量，并且采样速度加快了多达两个数量级。此外，BiFlow在基于NF的方法中达到了最先进水平，在单次评估('1-NFE')方法中也表现出竞争力。

Conclusion: BiFlow作为一种新型的双向标准化流框架，通过学习近似的反向模型而非依赖于精确的解析逆过程，实现了更高效的图像生成。它不仅提高了生成图像的质量，而且显著减少了采样时间，为NF领域带来了新的发展方向。

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>
