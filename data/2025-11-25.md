<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 19]
- [cs.MM](#cs.MM) [Total: 3]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Token-Controlled Re-ranking for Sequential Recommendation via LLMs](https://arxiv.org/abs/2511.17913)
*Wenxi Dai,Wujiang Xu,Pinhuan Wang,Dimitris N. Metaxas*

Main category: cs.IR

TL;DR: COREC 是一种新型重排序框架，通过用户参与的细粒度控制，平衡个性化与需求，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的重排序方法缺乏细粒度的用户控制机制，导致用户无法主动参与推荐过程，且推荐结果可能过于受限或次优。

Method: COREC 是一种基于令牌增强的重排序框架，通过显式的属性信号让用户参与推荐结果的共创，同时学习平衡用户指令与潜在偏好。

Result: 实验表明，COREC 在推荐效果和满足特定属性需求方面均优于现有基线方法。

Conclusion: COREC 框架通过引入细粒度的用户控制机制，成功弥补了现有重排序方法的不足，实现了用户需求与个性化推荐的平衡，并在实验中表现出色。

Abstract: The widespread adoption of Large Language Models (LLMs) as re-rankers is shifting recommender systems towards a user-centric paradigm. However, a significant gap remains: current re-rankers often lack mechanisms for fine-grained user control. They struggle to balance inherent user preferences with multiple attribute-based constraints, often resorting to simplistic hard filtering that can excessively narrow the recommendation pool and yield suboptimal results. This limitation leaves users as passive recipients rather than active collaborators in the recommendation process. To bridge this gap, we propose COREC, a novel token-augmented re-ranking framework that incorporates specific user requirements in co-creating the recommendation outcome. COREC empowers users to steer re-ranking results with precise and flexible control via explicit, attribute-based signals. The framework learns to balance these commands against latent preferences, yielding rankings that adhere to user instructions without sacrificing personalization. Experiments show that COREC: (1) exceeds state-of-the-art baselines on standard recommendation effectiveness and (2) demonstrates superior adherence to specific attribute requirements, proving that COREC enables fine-grained and predictable manipulation of the rankings.

</details>


### [2] [Save, Revisit, Retain: A Scalable Framework for Enhancing User Retention in Large-Scale Recommender Systems](https://arxiv.org/abs/2511.18013)
*Weijie Jiang,Armando Ordorica,Jaewon Yang,Olafur Gudmundsson,Yucheng Tu,Huizhong Duan*

Main category: cs.IR

TL;DR: A novel framework for modeling revisitation behavior improved Pinterest's user retention by 0.1% through better attribution and scalable analysis.


<details>
  <summary>Details</summary>
Motivation: User retention is critical for online platforms, but existing methods struggle with accurate attribution and the complexity of revisitation behavior.

Method: The framework introduces a surrogate attribution process to link saves to revisitations and employs a scalable event aggregation pipeline for large-scale analysis.

Result: The framework achieved a 0.1% increase in active users on Pinterest's Related Pins surface without additional computational costs.

Conclusion: The proposed lightweight and interpretable framework successfully addresses the challenges of modeling and optimizing revisitation behavior, leading to a measurable improvement in user retention on Pinterest.

Abstract: User retention is a critical objective for online platforms like Pinterest, as it strengthens user loyalty and drives growth through repeated engagement. A key indicator of retention is revisitation, i.e., when users return to view previously saved content, a behavior often sparked by personalized recommendations and user satisfaction. However, modeling and optimizing revisitation poses significant challenges. One core difficulty is accurate attribution: it is often unclear which specific user actions or content exposures trigger a revisit, since many confounding factors (e.g., content quality, user interface, notifications, or even changing user intent) can influence return behavior. Additionally, the scale and timing of revisitations introduce further complexity; users may revisit content days or even weeks after their initial interaction, requiring the system to maintain and associate extensive historical records across millions of users and sessions. These complexities render existing methods insufficient for robustly capturing and optimizing long-term revisitation. To address these gaps, we introduce a novel, lightweight, and interpretable framework for modeling revisitation behavior and optimizing long-term user retention in Pinterest's search-based recommendation context. By defining a surrogate attribution process that links saves to subsequent revisitations, we reduce noise in the causal relationship between user actions and return visits. Our scalable event aggregation pipeline enables large-scale analysis of user revisitation patterns and enhances the ranking system's ability to surface items with high retention value. Deployed on Pinterest's Related Pins surface to serve 500+ million users, the framework led to a significant lift of 0.1% in active users without additional computational costs.

</details>


### [3] [Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems](https://arxiv.org/abs/2511.18024)
*Dor Arviv,Yehonatan Elisha,Oren Barkan,Noam Koenigstein*

Main category: cs.IR

TL;DR: A method using Sparse Autoencoder extracts interpretable monosemantic neurons from recommender system embeddings, enabling controllable personalization.


<details>
  <summary>Details</summary>
Motivation: To reveal semantic structure in pretrained representations and preserve interactions between user and item embeddings for interpretable and controllable recommendation.

Method: The approach uses a Sparse Autoencoder (SAE) with a prediction-aware training objective to align latent structure with user-item affinity predictions.

Result: The extracted neurons capture properties like genre, popularity, and temporal trends, supporting post hoc control operations.

Conclusion: The method successfully extracts monosemantic neurons from user and item embeddings in recommender systems, enabling interpretable and controllable personalization without modifying the base model.

Abstract: We present a method for extracting \emph{monosemantic} neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a \emph{prediction aware} training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.

</details>


### [4] [Fidelity-Aware Recommendation Explanations via Stochastic Path Integration](https://arxiv.org/abs/2511.18047)
*Oren Barkan,Yahlly Schein,Yehonatan Elisha,Veronika Bogina,Mikhail Baklanov,Noam Koenigstein*

Main category: cs.IR

TL;DR: SPINRec是一种模型无关的方法，通过随机基线采样提高推荐系统解释的忠实性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的解释忠实性（即解释准确反映模型真实推理的程度）尚未得到充分研究，SPINRec旨在填补这一空白。

Method: SPINRec采用随机基线采样方法，从经验数据分布中采样多个可能的用户配置文件，并选择最忠实的归因路径，以克服先前方法的局限性。

Result: SPINRec在三个模型（MF、VAE、NCF）、三个数据集（ML1M、Yahoo! Music、Pinterest）和一系列反事实指标上均优于所有基线方法。

Conclusion: SPINRec通过随机基线采样和路径集成技术，显著提高了推荐系统中解释的忠实性，为推荐系统的可解释性设立了新的基准。

Abstract: Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.

</details>


### [5] [ProHD: Projection-Based Hausdorff Distance Approximation](https://arxiv.org/abs/2511.18207)
*Jiuzhou Fu,Luanzheng Guo,Nathan R. Tallent,Dongfang Zhao*

Main category: cs.IR

TL;DR: ProHD是一种快速且高精度的Hausdorff距离近似算法，适用于大规模高维数据集。


<details>
  <summary>Details</summary>
Motivation: 在大规模高维数据集上精确计算Hausdorff距离（HD）成本过高，因此需要一种快速且可靠的近似方法。

Method: ProHD通过将数据投影到少量信息方向（如质心轴和主成分）来识别候选极端点子集，并在该子集上计算HD。这种方法保证了真实HD的有界低估，并显著降低了计算成本。

Result: 在图像、物理和合成数据集（高达200万点，维度D=256）上的实验表明，ProHD比精确算法快10-100倍，且比基于随机采样的近似方法误差低5-20倍。

Conclusion: ProHD是一种高效的投影引导近似算法，能够显著加速Hausdorff距离（HD）的计算，同时保持高精度。该方法通过投影数据到少量信息方向并计算候选极端点的HD，实现了对真实HD的有界低估，并在大规模高维数据集上表现出色。

Abstract: The Hausdorff distance (HD) is a robust measure of set dissimilarity, but computing it exactly on large, high-dimensional datasets is prohibitively expensive. We propose \textbf{ProHD}, a projection-guided approximation algorithm that dramatically accelerates HD computation while maintaining high accuracy. ProHD identifies a small subset of candidate "extreme" points by projecting the data onto a few informative directions (such as the centroid axis and top principal components) and computing the HD on this subset. This approach guarantees an underestimate of the true HD with a bounded additive error and typically achieves results within a few percent of the exact value. In extensive experiments on image, physics, and synthetic datasets (up to two million points in $D=256$), ProHD runs 10--100$\times$ faster than exact algorithms while attaining 5--20$\times$ lower error than random sampling-based approximations. Our method enables practical HD calculations in scenarios like large vector databases and streaming data, where quick and reliable set distance estimation is needed.

</details>


### [6] [LLM Reasoning for Cold-Start Item Recommendation](https://arxiv.org/abs/2511.18261)
*Shijun Li,Yu Wang,Jin Wang,Ying Li,Joydeep Ghosh,Anne Cocos*

Main category: cs.IR

TL;DR: 本文提出针对Netflix领域冷启动项目推荐的新推理策略，利用LLM的推理能力推断用户偏好，显著提升冷启动推荐性能，在某些情况下比Netflix生产排名模型表现提升8%。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注有丰富用户-项目交互数据的暖启动场景，而冷启动场景中稀疏的交互数据限制了传统协同过滤方法的效果，这一问题尚未得到充分探索。

Method: 利用LLM的高级推理能力推断用户偏好，系统评估监督微调、基于强化学习的微调以及结合两者的混合方法，以优化推荐性能。

Result: 在真实数据上的广泛实验表明，在冷启动推荐场景中，方法效果和实际性能均有显著提升。基于推理的微调模型在某些情况下比Netflix生产排名模型表现提升8%。

Conclusion: LLM通过其推理能力和广泛知识库在改善推荐系统方面具有显著潜力，特别是在冷启动场景中，提出的推理策略能有效解决稀疏交互数据带来的挑战。

Abstract: Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.

</details>


### [7] [Democratic Recommendation with User and Item Representatives Produced by Graph Condensation](https://arxiv.org/abs/2511.18279)
*Jiahao Liang,Haoran Yang,Xiangyu Zhao,Zhiwen Yu,Guandong Xu,Wanyu Wang,Kaixiang Yang*

Main category: cs.IR

TL;DR: DemoRec通过图压缩和聚类技术优化推荐系统，显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 大规模用户-物品交互图在计算效率和信息传播方面存在挑战，现有方法存在泛化能力不足或信息丢失的问题。

Method: DemoRec利用图压缩生成用户和物品代表，构建紧凑的交互图，并通过聚类共享特征的节点来减少图规模和计算复杂度。

Result: 在四个公开数据集上的实验表明，DemoRec在推荐性能、计算效率和鲁棒性方面均有显著提升。

Conclusion: DemoRec通过图压缩技术显著提升了推荐系统的性能、计算效率和鲁棒性，优于现有方法。

Abstract: The challenges associated with large-scale user-item interaction graphs have attracted increasing attention in graph-based recommendation systems, primarily due to computational inefficiencies and inadequate information propagation. Existing methods provide partial solutions but suffer from notable limitations: model-centric approaches, such as sampling and aggregation, often struggle with generalization, while data-centric techniques, including graph sparsification and coarsening, lead to information loss and ineffective handling of bipartite graph structures. Recent advances in graph condensation offer a promising direction by reducing graph size while preserving essential information, presenting a novel approach to mitigating these challenges. Inspired by the principles of democracy, we propose \textbf{DemoRec}, a framework that leverages graph condensation to generate user and item representatives for recommendation tasks. By constructing a compact interaction graph and clustering nodes with shared characteristics from the original graph, DemoRec significantly reduces graph size and computational complexity. Furthermore, it mitigates the over-reliance on high-order information, a critical challenge in large-scale bipartite graphs. Extensive experiments conducted on four public datasets demonstrate the effectiveness of DemoRec, showcasing substantial improvements in recommendation performance, computational efficiency, and robustness compared to SOTA methods.

</details>


### [8] [BioArtlas: Computational Clustering of Multi-Dimensional Complexity in Bioart](https://arxiv.org/abs/2511.19162)
*Joonhyung Bae*

Main category: cs.IR

TL;DR: BioArtlas 通过多维度分析和聚类技术，揭示了生物艺术作品的四种组织模式，并提供了交互式网页界面供公众探索。


<details>
  <summary>Details</summary>
Motivation: 生物艺术的混合性质跨越艺术、科学、技术、伦理和政治，难以用传统的单轴分类方法进行分析。

Method: 采用基于代码本的方法，将相关概念统一聚类，解决了文化术语中的多义性问题。通过评估多达 800 种表示空间-算法组合，确定了最优的聚类方法（Agglomerative clustering at k=15 on 4D UMAP）。

Result: 揭示了四种组织模式：艺术家特定的方法一致性、基于技术的分割、时间上的艺术演变以及跨时间的概念亲和性。

Conclusion: BioArtlas 提供了一种新颖的多维度分析方法，通过轴感知表示和聚类技术，成功揭示了生物艺术作品的四种组织模式，同时通过交互式网页界面实现了严谨分析与公众可访问性的平衡。

Abstract: Bioart's hybrid nature spanning art, science, technology, ethics, and politics defies traditional single-axis categorization. I present BioArtlas, analyzing 81 bioart works across thirteen curated dimensions using novel axis-aware representations that preserve semantic distinctions while enabling cross-dimensional comparison. Our codebook-based approach groups related concepts into unified clusters, addressing polysemy in cultural terminology. Comprehensive evaluation of up to 800 representation-space-algorithm combinations identifies Agglomerative clustering at k=15 on 4D UMAP as optimal (silhouette 0.664 +/- 0.008, trustworthiness/continuity 0.805/0.812). The approach reveals four organizational patterns: artist-specific methodological cohesion, technique-based segmentation, temporal artistic evolution, and trans-temporal conceptual affinities. By separating analytical optimization from public communication, I provide rigorous analysis and accessible exploration through an interactive web interface (https://www.bioartlas.com) with the dataset publicly available (https://github.com/joonhyungbae/BioArtlas).

</details>


### [9] [Large Language Model Enhanced Graph Invariant Contrastive Learning for Out-of-Distribution Recommendation](https://arxiv.org/abs/2511.18282)
*Jiahao Liang,Haoran Yang,Xiangyu Zhao,Zhiwen Yu,Mianjie Li,Chuan Shi,Kaixiang Yang*

Main category: cs.IR

TL;DR: InvGCLLM结合数据驱动和LLM知识，通过因果学习和图优化提升图推荐系统的分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决图推荐系统中分布外泛化问题，传统方法因学习虚假环境相关性而失效，LLM的潜力尚未充分挖掘。

Method: 提出了一种创新的因果学习框架InvGCLLM，结合了数据驱动的因果置信度评分和LLM的图优化能力，通过对比学习目标学习稳健表示。

Result: 在四个公开数据集上，InvGCLLM显著优于现有基线方法，提升了分布外推荐性能。

Conclusion: InvGCLLM通过结合数据驱动模型和知识驱动的大型语言模型，显著提升了图推荐系统在分布外场景下的性能，实验证明了其优越性。

Abstract: Out-of-distribution (OOD) generalization has emerged as a significant challenge in graph recommender systems. Traditional graph neural network algorithms often fail because they learn spurious environmental correlations instead of stable causal relationships, leading to substantial performance degradation under distribution shifts. While recent advancements in Large Language Models (LLMs) offer a promising avenue due to their vast world knowledge and reasoning capabilities, effectively integrating this knowledge with the fine-grained topology of specific graphs to solve the OOD problem remains a significant challenge. To address these issues, we propose {$\textbf{Inv}$ariant $\textbf{G}$raph $\textbf{C}$ontrastive Learning with $\textbf{LLM}$s for Out-of-Distribution Recommendation (InvGCLLM)}, an innovative causal learning framework that synergistically integrates the strengths of data-driven models and knowledge-driven LLMs. Our framework first employs a data-driven invariant learning model to generate causal confidence scores for each user-item interaction. These scores then guide an LLM to perform targeted graph refinement, leveraging its world knowledge to prune spurious connections and augment missing causal links. Finally, the structurally purified graphs provide robust supervision for a causality-guided contrastive learning objective, enabling the model to learn representations that are resilient to spurious correlations. Experiments conducted on four public datasets demonstrate that InvGCLLM achieves significant improvements in out-of-distribution recommendation, consistently outperforming state-of-the-art baselines.

</details>


### [10] [UFO: Unfair-to-Fair Evolving Mitigates Unfairness in LLM-based Recommender Systems via Self-Play Fine-tuning](https://arxiv.org/abs/2511.18342)
*Jiaming Zhang,Yuyuan Li,Xiaohua Feng,Zhifei Ren,Li Zhang,Chaochao Chen*

Main category: cs.IR

TL;DR: 论文提出了一种名为UFO的框架，通过自我博弈机制解决大语言模型推荐系统中的不公平问题，同时保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注监督微调阶段的不公平问题，忽视了预训练阶段的偏见，且难以平衡公平性与推荐性能。

Method: UFO框架采用自我博弈机制，通过两个角色（judger和corrector）交替优化，识别并纠正预训练和监督微调阶段的不公平。

Result: 实验表明，UFO能有效缓解不公平问题，同时提升推荐性能。

Conclusion: UFO框架通过自我博弈机制，从根源上解决了推荐系统中的不公平问题，并保持了良好的推荐效果。

Abstract: Large language model-based Recommender Systems (LRSs) have demonstrated superior recommendation performance by integrating pre-training with Supervised Fine-Tuning (SFT). However, this approach introduces item-side unfairness. Existing studies primarily attribute this issue to the absence of fairness constraints during SFT and attempt to mitigate unfairness via re-weighting and re-ranking methods. In this paper, we find that unfairness arises not only from SFT but also from pre-training, where inherent biases are further amplified during SFT. This finding underscores the failure of current methods to address the root causes of unfairness. Moreover, current methods struggle to preserve satisfactory recommendation performance. To tackle these issues, we propose an Unfair-to-Fair evOlving (UFO) framework using a self-play mechanism, formulating unfairness mitigation as a two-player game. UFO alternates between two player roles: the \textit{judger}, which identifies unfairness from both pre-training and SFT, and the \textit{corrector}, which adjusts the LRS to address identified unfairness while preserving recommendation performance. Iterative optimization between these roles enables UFO to completely resolve unfairness. Extensive experiments demonstrate that UFO effectively mitigates unfairness while improving recommendation performance.

</details>


### [11] [Time Matters: Enhancing Sequential Recommendations with Time-Guided Graph Neural ODEs](https://arxiv.org/abs/2511.18347)
*Haoyan Fu,Zhida Qin,Shixiao Yang,Haoyao Zhang,Bin Lu,Shuang Li,Tianyu Huang,John C. S. Lui*

Main category: cs.IR

TL;DR: TGODE通过时间引导扩散生成器和广义图神经ODE，解决了序列推荐中用户兴趣不连续和物品分布不均的问题，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐方法常忽略两个关键因素：用户兴趣在交互间的不规则性以及物品分布随时间的高度不均。前者导致历史交互可能与当前行为无关，后者则因外部驱动分布与用户兴趣不匹配而影响推荐准确性。

Method: TGODE首先构建用户时间图和物品演化图，分别利用用户个性化偏好和全局物品分布信息。为解决用户交互不规则性带来的时间稀疏性，设计了时间引导扩散生成器以自动获取增强的时间感知用户图，并引入用户兴趣截断因子以高效识别稀疏时间间隔。随后，增强的用户图和物品图被输入广义图神经ODE，以匹配用户偏好和物品分布的演化。

Result: 实验结果表明，TGODE在五个数据集上均优于基线方法，性能提升范围在10%至46%之间。

Conclusion: TGODE通过构建用户时间图和物品演化图，结合时间引导扩散生成器和用户兴趣截断因子，有效解决了现有方法中忽略的用户兴趣不连续性和物品分布不均问题，显著提升了推荐性能。

Abstract: Sequential recommendation (SR) is widely deployed in e-commerce platforms, streaming services, etc., revealing significant potential to enhance user experience. However, existing methods often overlook two critical factors: irregular user interests between interactions and highly uneven item distributions over time. The former factor implies that actual user preferences are not always continuous, and long-term historical interactions may not be relevant to current purchasing behavior. Therefore, relying only on these historical interactions for recommendations may result in a lack of user interest at the target time. The latter factor, characterized by peaks and valleys in interaction frequency, may result from seasonal trends, special events, or promotions. These externally driven distributions may not align with individual user interests, leading to inaccurate recommendations. To address these deficiencies, we propose TGODE to both enhance and capture the long-term historical interactions. Specifically, we first construct a user time graph and item evolution graph, which utilize user personalized preferences and global item distribution information, respectively. To tackle the temporal sparsity caused by irregular user interactions, we design a time-guided diffusion generator to automatically obtain an augmented time-aware user graph. Additionally, we devise a user interest truncation factor to efficiently identify sparse time intervals and achieve balanced preference inference. After that, the augmented user graph and item graph are fed into a generalized graph neural ordinary differential equation (ODE) to align with the evolution of user preferences and item distributions. This allows two patterns of information evolution to be matched over time. Experimental results demonstrate that TGODE outperforms baseline methods across five datasets, with improvements ranging from 10% to 46%.

</details>


### [12] [A Recommender System Based on Binary Matrix Representations for Cognitive Disorders](https://arxiv.org/abs/2511.18645)
*Raoul H. Kutil,Georg Zimmermann,Christian Borgelt*

Main category: cs.IR

TL;DR: A Python-based recommender system using binary matrices helps diagnose cognitive disorders by identifying potential disorders and recommending next symptoms, showing promise as a clinical tool.


<details>
  <summary>Details</summary>
Motivation: Diagnosing cognitive disorders is complex due to symptom overlap and the need to identify the most informative next symptoms. The research aims to develop a tool to assist in this process.

Method: The system uses a binary matrix representation of disorders and their symptom combinations, filtering based on patient symptoms to identify potential disorders and recommend further symptoms. A prototype was implemented in Python.

Result: The system successfully identified plausible disorders from initial symptoms and recommended further symptoms for diagnosis refinement, providing context on symptom-disorder relationships.

Conclusion: The prototype recommender system demonstrates potential as a clinical support tool, aiding mental health professionals in efficiently identifying disorders and improving diagnostic accuracy through symptom-specific follow-up investigations.

Abstract: Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy.

</details>


### [13] [When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation](https://arxiv.org/abs/2511.18717)
*Jin Chai,Xiaoxiao Ma,Jian Yang,Jia Wu*

Main category: cs.IR

TL;DR: PASRec is a diffusion-based framework for active recommendation, jointly predicting Time of Interest and Item of Interest, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing sequential recommendation systems are passive, missing opportunities to engage users after they close the application. Active recommendation aims to predict the next interaction time and deliver items proactively.

Method: The paper proposes PASRec, a diffusion-based framework that jointly optimizes the prediction of ToI and IoI through a unified objective.

Result: Experiments on five benchmarks show PASRec outperforms eight state-of-the-art baselines under leave-one-out and temporal splits.

Conclusion: PASRec, a diffusion-based framework, effectively addresses the challenges of active recommendation by jointly aligning Time of Interest (ToI) and Item of Interest (IoI), demonstrating superior performance over existing baselines.

Abstract: Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.

</details>


### [14] [Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation](https://arxiv.org/abs/2511.18740)
*Yu Wang,Yonghui Yang,Le Wu,Yi Zhang,Richang Hong*

Main category: cs.IR

TL;DR: HaNoRec是一种多模态LLM框架，通过硬度感知和噪声正则化优化，解决了推荐任务中的样本硬度不平衡和跨模态语义偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本模态中运行，忽略了用户的细粒度兴趣，尤其是由视觉信号塑造的兴趣。此外，随机负采样导致样本硬度不平衡，而DPO中的固定参考模型限制了跨模态语义偏差的修正。

Method: 提出了一种名为HaNoRec的多模态LLM框架，结合了硬度感知和噪声正则化的偏好优化方法，动态调整优化权重并引入高斯扰动分布优化。

Result: HaNoRec能够动态优化训练样本的权重，增强跨模态语义一致性，减少模态偏差，从而提升推荐性能。

Conclusion: HaNoRec框架通过动态调整优化权重和高斯扰动分布优化，有效解决了样本硬度不平衡和跨模态语义偏差问题，提升了多模态大语言模型在推荐任务中的性能。

Abstract: Recent advances in Large Language Models (LLMs) have opened new avenues for sequential recommendation by enabling natural language reasoning over user behavior sequences. A common approach formulates recommendation as a language modeling task, where interaction histories are transformed into prompts and user preferences are learned via supervised fine-tuning. However, these methods operate solely in the textual modality and often miss users' fine-grained interests, especially when shaped by rich visual signals such as product images or movie posters. Multimodal Large Language Models (MLLMs) offer a promising alternative by aligning text and vision in a shared semantic space. A prevalent training paradigm applies Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) to model user preferences. Yet, two core challenges remain: 1) Imbalanced sample hardness, where random negative sampling causes overfitting on easy examples and under-training on hard ones; 2) Cross-modal semantic bias, where the fixed reference model in DPO prevents the policy model from correcting modality misalignments--especially over long sequences. To address these issues, we propose a Multimodal LLM framework that integrates Hardness-aware and Noise-regularized preference optimization for Recommendation (HaNoRec). Specifically, HaNoRec dynamically adjusts optimization weights based on both the estimated hardness of each training sample and the policy model's real-time responsiveness, prioritizing harder examples. It further introduces Gaussian-perturbed distribution optimization on output logits to enhance cross-modal semantic consistency and reduce modality bias inherited from the reference model.

</details>


### [15] [STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models](https://arxiv.org/abs/2511.18805)
*Yi Xu,Chaofan Fan,Jinxin Hu,Yu Zhang,Zeng Xiaoyi,Jing Zhang*

Main category: cs.IR

TL;DR: STORE框架通过语义标记化、正交旋转变换和高效注意力机制，解决了推荐系统中高基数稀疏特征带来的表示和计算瓶颈，显著提升了预测准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现代个性化推荐系统中的排序模型面临高基数、异构和稀疏特征空间带来的挑战，存在表示瓶颈（低秩表示导致模型可扩展性受限）和计算瓶颈（特征标记爆炸导致传统注意力机制计算量大且易分散）。

Method: 提出STORE框架，包含三个核心创新：(1) 语义标记化：将高基数稀疏特征分解为紧凑的稳定语义标记；(2) 正交旋转变换：旋转低基数静态特征子空间以促进更高效的特征交互；(3) 高效注意力：过滤低贡献标记以提高计算效率同时保持模型准确性。

Result: 离线实验和在线A/B测试显示，该框架持续提升预测准确性（在线CTR提升2.71%，AUC提升1.195%）和训练效率（吞吐量提升1.84倍）。

Conclusion: STORE框架有效解决了推荐系统中的表示和计算瓶颈，为处理高基数异构稀疏特征提供了可扩展的解决方案，显著提升了模型性能和效率。

Abstract: Ranking models have become an important part of modern personalized recommendation systems. However, significant challenges persist in handling high-cardinality, heterogeneous, and sparse feature spaces, particularly regarding model scalability and efficiency. We identify two key bottlenecks: (i) Representation Bottleneck: Driven by the high cardinality and dynamic nature of features, model capacity is forced into sparse-activated embedding layers, leading to low-rank representations. This, in turn, triggers phenomena like "One-Epoch" and "Interaction-Collapse," ultimately hindering model scalability.(ii) Computational Bottleneck: Integrating all heterogeneous features into a unified model triggers an explosion in the number of feature tokens, rendering traditional attention mechanisms computationally demanding and susceptible to attention dispersion. To dismantle these barriers, we introduce STORE, a unified and scalable token-based ranking framework built upon three core innovations: (1) Semantic Tokenization fundamentally tackles feature heterogeneity and sparsity by decomposing high-cardinality sparse features into a compact set of stable semantic tokens; and (2) Orthogonal Rotation Transformation is employed to rotate the subspace spanned by low-cardinality static features, which facilitates more efficient and effective feature interactions; and (3) Efficient attention that filters low-contributing tokens to improve computional efficiency while preserving model accuracy. Across extensive offline experiments and online A/B tests, our framework consistently improves prediction accuracy(online CTR by 2.71%, AUC by 1.195%) and training effeciency (1.84 throughput).

</details>


### [16] [Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation](https://arxiv.org/abs/2511.18997)
*Chenhao Zhai,Chang Meng,Xueliang Wang,Shuchang Liu,Xiaolong Hu,Shisong Tang,Xiaoqiang Feng,Xiu Li*

Main category: cs.IR

TL;DR: HMUM框架通过离线建模和在线决策优化短视频推荐，显著提升性能并已部署于快手平台。


<details>
  <summary>Details</summary>
Motivation: 短视频推荐系统中用户偏好多样，现有提升模型难以处理多策略的异质性，且传统固定权重方法缺乏个性化，导致决策偏差。

Method: HMUM框架包括离线混合提升建模（HUM）模块和在线动态决策（DDM）模块，分别用于捕捉多策略的协同与个体效应，以及实时估计用户响应价值权重。

Result: 在两个公共数据集、一个工业数据集及快手平台的在线A/B测试中，HMUM表现出优越的离线性能和关键指标的显著提升。

Conclusion: HMUM框架在快手平台上成功部署，显著提升了关键指标，并为数亿用户带来了实际效益。

Abstract: The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users.

</details>


### [17] [What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models](https://arxiv.org/abs/2511.19324)
*Roksana Goworek,Olivia Macmillan-Scott,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 本文系统评估了跨语言信息检索（CLIR）中的四种干预方法，发现针对CLIR专门训练的密集检索模型优于词汇匹配方法，且对比学习能有效缓解语言偏见，特别是在低资源和跨文字对中表现显著。


<details>
  <summary>Details</summary>
Motivation: 跨语言信息检索面临资源差异、文字差异和嵌入模型跨语言语义对齐弱等挑战，现有方法依赖翻译和单语检索启发式方法，增加了计算开销和噪声，降低了性能。

Method: 系统评估了四种干预类型：文档翻译、使用预训练编码器的多语言密集检索、在词、短语和查询-文档级别的对比学习，以及交叉编码器重排序，并在三个基准数据集上进行测试。

Result: 针对CLIR专门训练的密集检索模型一致优于词汇匹配方法，且从文档翻译中获益甚微。对比学习缓解了语言偏见，对初始对齐较弱的编码器带来显著改进，重排序有效但依赖于交叉编码器训练数据的质量。

Conclusion: 跨语言搜索系统应优先考虑语义多语言嵌入和有针对性的基于学习的对齐，而不是基于翻译的流程，特别是对于跨文字和资源不足的语言。

Abstract: Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.

</details>


### [18] [Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval](https://arxiv.org/abs/2511.19325)
*Olivia Macmillan-Scott,Roksana Goworek,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: Query expansion via mLLMs improves retrieval, but performance varies by query length and language script. Fine-tuning helps only with similar data formats, highlighting the need for balanced multilingual resources.


<details>
  <summary>Details</summary>
Motivation: To identify factors that drive cross-lingual retrieval performance and understand the impact of query expansion techniques.

Method: The study evaluates recent multilingual large language models (mLLMs) and fine-tuned variants across several generative expansion strategies.

Result: Query length determines effective prompting techniques, and elaborate prompts often do not yield further gains. Linguistic disparities persist, with cross-lingual query expansion benefiting languages with weak baselines but performing poorly between languages with different scripts. Fine-tuning only improves performance when training and test data formats are similar.

Conclusion: The study highlights the need for more balanced multilingual and cross-lingual training and evaluation resources to address linguistic disparities and improve retrieval performance.

Abstract: Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.

</details>


### [19] [Revisiting Feedback Models for HyDE](https://arxiv.org/abs/2511.19349)
*Nour Jedidi,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究发现，结合传统反馈模型（如Rocchio）可以显著提升HyDE方法的效果，为基于LLM的伪相关反馈提供了简单有效的改进方案。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过结合传统反馈模型（如Rocchio和RM3）来优化基于LLM的伪相关反馈方法（如HyDE）的效果。

Method: 在HyDE方法中引入并系统评估传统反馈模型（如Rocchio），用于提取和加权扩展词。

Result: 实验表明，结合Rocchio等反馈算法可以显著提升HyDE方法的有效性。

Conclusion: 传统反馈模型能够有效改进基于LLM的伪相关反馈方法，为相关研究提供了简单且实用的优化方向。

Abstract: Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [20] [Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation](https://arxiv.org/abs/2511.18415)
*Wei Yang,Yiran Zhu,Zilin Li,Xunjia Zhang,Hongtao Wang*

Main category: cs.MM

TL;DR: SEKD通过自蒸馏方法提升视觉语言模型在层次理解任务中的性能，无需人工标注，显著提高了路径一致性和零样本表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在层次理解任务中表现不佳，主要问题在于无法维持跨层次状态，而非缺乏分类知识。

Method: 提出了自蒸馏知识蒸馏（SEKD）方法，通过让同一视觉语言模型逐步推理并作为教师，暴露其硬标签、软分布和解码器隐藏状态，供单次推理的学生模型蒸馏学习。

Result: SEKD显著提升了模型在领域内路径一致性（HCA）和零样本任务上的表现，分别提高了29.50个百分点和从4.15%到42.26%。

Conclusion: SEKD是一种无需人工标注或外部工具的自蒸馏方法，能够显著提升视觉语言模型在层次理解任务中的性能，尤其是在路径一致性和零样本任务上表现突出。

Abstract: Vision-language models (VLMs) possess rich knowledge but often fail on hierarchical understanding tasks, where the goal is to predict a coarse-to-fine taxonomy path that remains consistent across all levels. We compare three inference paradigms for hierarchical VQA and find that stepwise reasoning, when conditioned on prior answers, significantly outperforms single-pass prompting. Further analysis indicates that the main limitation of current VLMs is their inability to maintain cross-level state, rather than a lack of taxonomic knowledge. Motivated by this diagnosis, we propose Self-Elicited Knowledge Distillation (SEKD), which requires no human labels or external tools: the same VLM is prompted to reason step by step and act as a teacher by exposing its hard labels, soft distributions, and decoder hidden states, while a single-pass student distills these signals. The student VLM remains efficient while approaching the accuracy of its multi-step teacher. It improves in-domain path consistency (HCA) by up to +29.50 percentage points, raises zero-shot HCA on an unseen taxonomy from 4.15% to 42.26%, and yields gains on challenging mathematical benchmarks. Because all supervision is self-elicited, SEKD scales to new taxonomies and datasets without annotation cost, providing a practical route to imbue compact VLMs with dependency-aware multi-step reasoning.

</details>


### [21] [When Top-ranked Recommendations Fail: Modeling Multi-Granular Negative Feedback for Explainable and Robust Video Recommendation](https://arxiv.org/abs/2511.18700)
*Siran Chen,Boyu Chen,Chenyun Yu,Yi Ouyang,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.MM

TL;DR: A new framework (ENF) and algorithm (S-GRPO) improve video recommendations by better predicting negative feedback and explaining user dislikes, validated by significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing systems fail to capture deep video semantics and struggle with biased user behaviors, resulting in inaccurate interest modeling and negative feedback.

Method: The ENF framework integrates three agents (Profile, Video, and Reason) for comprehensive user and video analysis, while the S-GRPO algorithm enables progressive task handling during reinforcement fine-tuning.

Result: The method outperforms baselines, achieving an 8.6% improvement over GPT-4o in reason classification, increasing watch time by 6.2%, and reducing fast-skip rate by 9.4%.

Conclusion: The proposed ENF framework and S-GRPO algorithm effectively address the limitations of existing video recommendation systems by improving negative feedback prediction and reason explanation, leading to enhanced user satisfaction and engagement.

Abstract: Existing video recommendation systems, relying mainly on ID-based embedding mapping and collaborative filtering, often fail to capture in-depth video content semantics. Moreover, most struggle to address biased user behaviors (e.g., accidental clicks, fast skips), leading to inaccurate interest modeling and frequent negative feedback in top recommendations with unclear causes. To tackle this issue, we collect real-world user video-watching sequences, annotate the reasons for users' dislikes, and construct a benchmark dataset for personalized explanations. We then introduce the Agentic Explainable Negative Feedback (ENF) framework, which integrates three core components: (1) the Profile Agent, extracting behavioral cues from users' historical data to derive psychological and personality profiles; (2) the Video Agent, performing comprehensive multimodal video analysis; and (3) the Reason Agent, synthesizing information from the other two agents to predict user engagement and generate explanations. Additionally, we propose the S-GRPO algorithm, enabling the model to progressively address complex tasks during reinforcement fine-tuning. Experimental results on the collected dataset show that our method significantly outperforms state-of-the-art baselines in negative feedback prediction and reason explanation. Notably, it achieves an 8.6% improvement over GPT-4o in reason classification. Deployment on the business platform further validates its benefits: increasing average user watch time by 6.2%, reducing the fast-skip rate by 9.4%, and significantly enhancing user satisfaction.

</details>


### [22] [Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach](https://arxiv.org/abs/2511.19080)
*Fan Nie,Jiangqun Ni,Jian Zhang,Bin Zhang,Weizhe Zhang,Bin Li*

Main category: cs.MM

TL;DR: FoVB introduces a variational Bayesian framework for deepfake detection, using advanced feature extraction and correlation learning to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address security concerns posed by audio-visual deepfakes, requiring a generalizable and effective detection method that leverages cross-modal inconsistencies.

Method: The method involves using variational Bayesian estimation to approximate audio-visual correlation as a Gaussian distributed latent variable, combined with difference convolutions and a high-pass filter to extract forgery-aware features. The latent variable is further factorized into modality-specific and correlation-specific components with orthogonality constraints.

Result: Extensive experiments show that FoVB surpasses state-of-the-art methods in various benchmarks, demonstrating its effectiveness in detecting multi-modal deepfakes.

Conclusion: The proposed FoVB framework effectively addresses the challenge of multi-modal deepfake detection by leveraging variational Bayesian estimation and advanced feature extraction techniques, outperforming existing methods in benchmarks.

Abstract: The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.

</details>
