<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 15]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.LG](#cs.LG) [Total: 35]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [GCRank: A Generative Contextual Comprehension Paradigm for Takeout Ranking Model](https://arxiv.org/abs/2601.02361)
*Ziheng Ni,Congcong Liu,Cai Shang,Yiming Sun,Junjie Li,Zhiwei Fang,Guangpeng Chen,Jian Li,Zehua Zhang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.IR

TL;DR: 提出了一种新的生成式框架来改进广告系统中的排名阶段，该框架通过统一架构处理异构信号，并包括两个核心组件：生成上下文编码器（GCE）和生成上下文融合模块（GCF）。实验表明该方法在关键业务指标上取得了显著增益。


<details>
  <summary>Details</summary>
Motivation: 当前的排名模型依赖于分散的模块和手工设计的特征，限制了它们解释复杂用户意图的能力。特别是在基于位置的服务中，用户的决策受到动态的空间、时间和个人背景的影响。为了解决这些局限性，作者提出了一个新颖的生成式框架，将排名视为上下文理解任务。

Method: 新框架包含两个核心部分：生成上下文编码器（GCE）和生成上下文融合（GCF）。其中GCE由三个专门模块组成：个性化上下文增强器（PCE）、集体上下文增强器（CCE）以及动态上下文增强器（DCE），用于分别处理用户特定建模、群体级模式及实时情境适应；GCF则通过低秩适应无缝集成这些上下文表示。

Result: 广泛的实验证明，所提出的方法在点击率和平台收入等关键业务指标上实现了显著增长。

Conclusion: 这项工作开创了生成式推荐的新视角，并展示了其在工业广告系统中的实际潜力。该方法已成功部署在一个大规模食品配送广告平台上，证明了其实质性的实践影响。

Abstract: The ranking stage serves as the central optimization and allocation hub in advertising systems, governing economic value distribution through eCPM and orchestrating the user-centric blending of organic and advertising content. Prevailing ranking models often rely on fragmented modules and hand-crafted features, limiting their ability to interpret complex user intent. This challenge is further amplified in location-based services such as food delivery, where user decisions are shaped by dynamic spatial, temporal, and individual contexts. To address these limitations, we propose a novel generative framework that reframes ranking as a context comprehension task, modeling heterogeneous signals in a unified architecture. Our architecture consists of two core components: the Generative Contextual Encoder (GCE) and the Generative Contextual Fusion (GCF). The GCE comprises three specialized modules: a Personalized Context Enhancer (PCE) for user-specific modeling, a Collective Context Enhancer (CCE) for group-level patterns, and a Dynamic Context Enhancer (DCE) for real-time situational adaptation. The GCF module then seamlessly integrates these contextual representations through low-rank adaptation. Extensive experiments confirm that our method achieves significant gains in critical business metrics, including click-through rate and platform revenue. We have successfully deployed our method on a large-scale food delivery advertising platform, demonstrating its substantial practical impact. This work pioneers a new perspective on generative recommendation and highlights its practical potential in industrial advertising systems.

</details>


### [2] [The Impact of LLM-Generated Reviews on Recommender Systems: Textual Shifts, Performance Effects, and Strategic Platform Control](https://arxiv.org/abs/2601.02362)
*Itzhak Ziv,Moshe Unger,Hilah Geva*

Main category: cs.IR

TL;DR: 研究了AI生成的评论如何影响基于内容的推荐系统（RS）的表现和业务结果。通过分析用户中心和平台中心两种途径，发现虽然AI生成的评论可以提高RS性能，但人类撰写的评论训练出的模型表现更优。此外，语气框架策略能显著提高平台生成评论的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的兴起，基于内容的推荐系统越来越多地遇到AI生成的内容与人类撰写的内容并存的情况。本研究旨在探讨引入AI生成的评论对推荐系统性能及业务成果的影响。

Method: 使用TripAdvisor上的大量酒店评论数据集，利用大型语言模型生成合成评论，并评估这些评论在推荐系统的训练和部署阶段产生的影响。同时比较了用户中心（个人使用AI工具改进评论）和平台中心（平台直接从结构化元数据生成合成评论）两种方式下AI内容进入推荐系统的路径。

Result: AI生成的评论在多个文本维度上与人类撰写的评论存在系统性差异；尽管两种类型的AI评论相比没有文本数据的模型都能提升推荐系统性能，但基于人类评论训练的模型始终表现出色；人类训练的模型能够稳健地泛化到AI内容，而AI训练的模型在两种内容类型上均表现不佳；基于语气的框架策略（鼓励、建设性或批判性）大大提高了平台生成评论的效果。

Conclusion: 研究强调了平台控制在管理AI生成评论的产生和整合方面具有战略重要性，以确保合成内容能够补充推荐系统的健壮性和可持续商业价值。

Abstract: The rise of generative AI technologies is reshaping content-based recommender systems (RSes), which increasingly encounter AI-generated content alongside human-authored content. This study examines how the introduction of AI-generated reviews influences RS performance and business outcomes. We analyze two distinct pathways through which AI content can enter RSes: user-centric, in which individuals use AI tools to refine their reviews, and platform-centric, in which platforms generate synthetic reviews directly from structured metadata. Using a large-scale dataset of hotel reviews from TripAdvisor, we generate synthetic reviews using LLMs and evaluate their impact across the training and deployment phases of RSes. We find that AI-generated reviews differ systematically from human-authored reviews across multiple textual dimensions. Although both user- and platform-centric AI reviews enhance RS performance relative to models without textual data, models trained on human reviews consistently achieve superior performance, underscoring the quality of authentic human data. Human-trained models generalize robustly to AI content, whereas AI-trained models underperform on both content types. Furthermore, tone-based framing strategies (encouraging, constructive, or critical) substantially enhance platform-generated review effectiveness. Our findings highlight the strategic importance of platform control in governing the generation and integration of AI-generated reviews, ensuring that synthetic content complements recommendation robustness and sustainable business value.

</details>


### [3] [Towards Trustworthy LLM-Based Recommendation via Rationale Integration](https://arxiv.org/abs/2601.02364)
*Chung Park,Taesan Kim,Hyeongjun Yun,Dongjoon Hong,Junui Hong,Kijung Park,MinCheol Cho,Mira Myong,Jihoon Oh,Min sung Choi*

Main category: cs.IR

TL;DR: 提出了一种基于大语言模型的推荐系统（LLM-Rec），该系统不仅能预测项目还能生成逻辑合理的解释。通过使用自注释的解释数据集和指导调整，以先解释后推荐的方式工作，并采用链式思维风格表示解释，从而提高了解释性和推荐性能。在亚马逊评论数据集的时尚和科学领域上的实验表明了相对于现有基准方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统主要关注准确性和短期参与度，而忽视了透明度和可信度。最近，一些平台如亚马逊和Instagram开始向用户提供推荐理由，认识到它们在建立信任和提高用户参与度方面的重要性。然而，现有的大多数系统仍然将推荐理由视为事后补充的内容。

Method: 提出了一个基于大型语言模型的推荐系统（LLM-Rec），它不仅能够预测商品，还能生成具有逻辑基础的理由。该方法利用了一个自我标注的理由数据集，并采用了理由优先格式下的指令调优，其中模型在输出推荐商品之前先生成解释。通过采取这种策略并以链式思维(CoT)风格表示理由，LLM-Rec增强了可解释性和推荐性能。

Result: 在Amazon Review数据集的时尚与科学领域进行了实验，结果显示LLM-Rec相对于已有的基准方法有显著提升。

Conclusion: 研究证明了LLM-Rec在提高推荐系统的解释性同时也能增强推荐效果。此外，为了促进研究的可重复性和未来的研究，作者公开发布了一个包含用户历史、理由以及推荐项目的理由增强型推荐数据集。

Abstract: Traditional recommender systems (RS) have been primarily optimized for accuracy and short-term engagement, often overlooking transparency and trustworthiness. Recently, platforms such as Amazon and Instagram have begun providing recommendation rationales to users, acknowledging their critical role in fostering trust and enhancing engagement; however, most existing systems still treat them as post-hoc artifacts. We propose an LLM-based recommender (LLM-Rec) that not only predicts items but also generates logically grounded rationales. Our approach leverages a self-annotated rationale dataset and instruction tuning in a rationale-first format, where the model generates an explanation before outputting the recommended item. By adopting this strategy and representing rationales in a chain-of-thought (CoT) style, LLM-Rec strengthens both interpretability and recommendation performance. Experiments on the Fashion and Scientific domains of the Amazon Review dataset demonstrate significant improvements over well-established baselines. To encourage reproducibility and future research, we publicly release a rationale-augmented recommendation dataset containing user histories, rationales, and recommended items.

</details>


### [4] [TextBridgeGNN: Pre-training Graph Neural Network for Cross-Domain Recommendation via Text-Guided Transfer](https://arxiv.org/abs/2601.02366)
*Yiwen Chen,Yiqing Wu,Huishi Luo,Fuzhen Zhuang,Deqing Wang*

Main category: cs.IR

TL;DR: 提出了一种名为TextBridgeGNN的预训练和微调框架，通过文本作为语义桥梁连接不同领域，以解决基于ID嵌入的图推荐模型在跨域迁移时面临的挑战。实验表明该方法在跨域、多域及无需训练场景下优于现有方法，并能有效结合预训练语言模型驱动的语义与基于图的协同过滤。


<details>
  <summary>Details</summary>
Motivation: 基于ID嵌入的经典图推荐模型在跨域迁移时遇到难题，主要由于ID嵌入的不可转移性和跨域间异构交互图结构上的不兼容性。为了解决这些问题并构建一个可预训练的图推荐模型，提出了新的解决方案。

Method: 提出了TextBridgeGNN框架，利用文本信息作为不同领域间的语义桥梁。在预训练阶段，使用文本打破由多个领域形成的数据孤岛，并设计层次化的GNN来学习包含文本特征的领域特定知识和全局知识；在微调阶段，则引入相似性转移机制，通过对语义相关节点之间的转移初始化目标领域的ID嵌入，实现ID嵌入和图模式的有效迁移。

Result: 实验结果表明，在跨域、多域以及无需额外训练的情况下，TextBridgeGNN的表现均优于现有的方法，能够很好地将预训练语言模型（PLM）驱动的语义与基于图的协同过滤相结合，同时避免了昂贵的语言模型微调或实时推理开销。

Conclusion: TextBridgeGNN提供了一种有效的途径来克服基于ID的图推荐系统在跨域应用中的局限性，通过利用文本作为媒介实现了更好的跨域知识迁移效果。

Abstract: Graph-based recommendation has achieved great success in recent years. The classical graph recommendation model utilizes ID embedding to store essential collaborative information. However, this ID-based paradigm faces challenges in transferring to a new domain, making it hard to build a pre-trained graph recommendation model. This phenomenon primarily stems from two inherent challenges: (1) the non-transferability of ID embeddings due to isolated domain-specific ID spaces, and (2) structural incompatibility between heterogeneous interaction graphs across domains.
  To address these issues, we propose TextBridgeGNN, a pre-training and fine-tuning framework that can effectively transfer knowledge from a pre-trained GNN to downstream tasks. We believe the key lies in how to build the relationship between domains. Specifically, TextBridgeGNN uses text as a semantic bridge to connect domains through multi-level graph propagation. During the pre-training stage, textual information is utilized to break the data islands formed by multiple domains, and hierarchical GNNs are designed to learn both domain-specific and domain-global knowledge with text features, ensuring the retention of collaborative signals and the enhancement of semantics. During the fine-tuning stage, a similarity transfer mechanism is proposed. This mechanism initializes ID embeddings in the target domain by transferring from semantically related nodes, successfully transferring the ID embeddings and graph pattern.
  Experiments demonstrate that TextBridgeGNN outperforms existing methods in cross-domain, multi-domain, and training-free settings, highlighting its ability to integrate Pre-trained Language Model (PLM)-driven semantics with graph-based collaborative filtering without costly language model fine-tuning or real-time inference overhead.

</details>


### [5] [A Lay User Explainable Food Recommendation System Based on Hybrid Feature Importance Extraction and Large Language Models](https://arxiv.org/abs/2601.02374)
*Melissa Tessa,Diderot D. Cidjeu,Rachele Carli,Sarah Abchiche,Ahmad Aldarwishd,Igor Tchappi,Amro Najjar*

Main category: cs.IR

TL;DR: 本文提出了一种利用大型语言模型和SHAP混合提取关键变量的方法，为食品推荐系统的结果提供更加详细、动态且有说服力的解释，从而增强用户对系统的信任度和透明度。


<details>
  <summary>Details</summary>
Motivation: 为了提高食品推荐系统结果的可理解性，并增加用户对推荐系统的信任和透明度。

Method: 结合大型语言模型（LLM）与SHAP技术进行关键变量的混合提取，以生成针对推荐结果的详尽解释。

Result: 相比现有文献中的方法，所提方法能够为普通用户提供更全面、更具说服力以及更易于理解的解释内容。

Conclusion: 通过将大型语言模型与SHAP相结合来改进食品推荐系统的后处理过程，可以显著提升推荐结果对于非专业用户的清晰度和接受度，进而加强了用户对该系统的信任感。

Abstract: Large Language Models (LLM) have experienced strong development in recent years, with varied applications. This paper uses LLMs to develop a post-hoc process that provides more elaborated explanations of the results of food recommendation systems. By combining LLM with a hybrid extraction of key variables using SHAP, we obtain dynamic, convincing and more comprehensive explanations to lay user, compared to those in the literature. This approach enhances user trust and transparency by making complex recommendation outcomes easier to understand for a lay user.

</details>


### [6] [TAG-HGT: A Scalable and Cost-Effective Framework for Inductive Cold-Start Academic Recommendation](https://arxiv.org/abs/2601.02381)
*Zhexiang Li*

Main category: cs.IR

TL;DR: 本文提出了一种成本效益高的神经符号框架TAG-HGT，旨在解决学术平台上的归纳冷启动推荐问题。通过采用“语义优先，结构精炼”的分离范式，并利用冻结的大规模语言模型作为离线语义工厂，将知识提炼到轻量级异构图变压器中。实验结果表明，与仅使用结构信息的方法相比，TAG-HGT在系统召回率上提高了20.7%，同时极大地降低了推理延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 当前工业学术平台面临的一个主要挑战是处理大量新加入学者的冷启动推荐问题，这些用户没有历史互动记录。虽然最近的生成性图模型显示出良好的语义能力，但它们存在高推理延迟和巨大的计算成本问题，使得其实时大规模应用变得不切实际。

Method: 提出了TAG-HGT，一种结合了大型语言模型（DeepSeek-V3）提供的全局召回能力和结构信号提供的局部区分能力的成本效益型神经符号框架。该方法通过跨视图对比学习(CVCL)将LLM的知识提炼到一个轻量级的异构图变换器(HGT)中。

Result: TAG-HGT在严格的Time-Machine协议下，在庞大的OpenAlex数据集上实现了91.97%的SOTA System Recall@10，比仅基于结构的方法高出20.7%。此外，它还将推理延迟从780秒减少到了1.73毫秒，每千次查询的成本从约1.50美元降低至不到0.001美元。

Conclusion: TAG-HGT不仅显著改善了冷启动场景下的学术推荐性能，还极大降低了部署成本，为实现高精度学术推荐服务的普及奠定了基础。

Abstract: Inductive cold-start recommendation remains the "Achilles' Heel" of industrial academic platforms, where thousands of new scholars join daily without historical interaction records. While recent Generative Graph Models (e.g., HiGPT, OFA) demonstrate promising semantic capabilities, their prohibitive inference latency (often exceeding 13 minutes per 1,000 requests) and massive computational costs render them practically undeployable for real-time, million-scale applications. To bridge this gap between generative quality and industrial scalability, we propose TAG-HGT, a cost-effective neuro-symbolic framework. Adopting a decoupled "Semantics-First, Structure-Refined" paradigm, TAG-HGT utilizes a frozen Large Language Model (DeepSeek-V3) as an offline semantic factory and distills its knowledge into a lightweight Heterogeneous Graph Transformer (HGT) via Cross-View Contrastive Learning (CVCL). We present a key insight: while LLM semantics provide necessary global recall, structural signals offer the critical local discrimination needed to distinguish valid collaborators from semantically similar but socially unreachable strangers in dense embedding spaces. Validated under a strict Time-Machine Protocol on the massive OpenAlex dataset, TAG-HGT achieves a SOTA System Recall@10 of 91.97%, outperforming structure-only baselines by 20.7%. Most significantly, from an industrial perspective, TAG-HGT reduces inference latency by five orders of magnitude ($4.5 \times 10^{5}\times$) compared to generative baselines (from 780s down to 1.73 ms), and slashes inference costs from $\sim$$1.50 to $<$$0.001 per 1k queries. This 99.9% cost reduction democratizes high-precision academic recommendation.

</details>


### [7] [Tree of Preferences for Diversified Recommendation](https://arxiv.org/abs/2601.02386)
*Hanyang Yuan,Ning Tang,Tongya Zheng,Jiarong Xu,Xintong Hu,Renhong Huang,Shunyu Liu,Jiacong Hu,Jiawei Chen,Mingli Song*

Main category: cs.IR

TL;DR: 本文提出了一种利用大型语言模型(LLMs)来揭示用户未被充分探索的偏好的新方法，通过构建偏好树(ToP)结构系统地推理用户的偏好，并采用数据为中心的方法生成合成交互以训练推荐系统，从而实现多样化和相关性更高的推荐。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要从观察到的用户反馈中推断用户偏好的多样性，但由于固有的数据偏差，观察到的数据可能无法完全反映用户兴趣，导致未被充分探索的偏好被忽视或未能显现。这可能导致推荐的多样性不足。

Method: 1. 提出了一种名为偏好树（ToP）的新颖结构，用于从粗到细建模用户偏好。
2. 利用大型语言模型（LLMs）的专业知识来揭示基于用户行为的未被充分探索的偏好。
3. 采用数据为中心的方法识别匹配用户偏好的候选项目，并生成反映未被充分探索偏好的合成交互。
4. 将这些交互整合起来训练一个通用的推荐器以增加多样性。
5. 在优化过程中动态选择有影响力的用户以提高整体效率。

Result: 广泛的评估表明，在大多数情况下，该方法在多样性和相关性方面优于现有方法，而在其他情况下接近最优性能，同时保持合理的推理延迟。

Conclusion: 本研究提出的方法能够有效解决因数据偏差而导致的推荐多样性不足问题，通过利用LLMs的能力挖掘用户未被充分探索的兴趣点，进而提供更加多样化且相关的推荐结果。

Abstract: Diversified recommendation has attracted increasing attention from both researchers and practitioners, which can effectively address the homogeneity of recommended items. Existing approaches predominantly aim to infer the diversity of user preferences from observed user feedback. Nonetheless, due to inherent data biases, the observed data may not fully reflect user interests, where underexplored preferences can be overwhelmed or remain unmanifested. Failing to capture these preferences can lead to suboptimal diversity in recommendations. To fill this gap, this work aims to study diversified recommendation from a data-bias perspective. Inspired by the outstanding performance of large language models (LLMs) in zero-shot inference leveraging world knowledge, we propose a novel approach that utilizes LLMs' expertise to uncover underexplored user preferences from observed behavior, ultimately providing diverse and relevant recommendations. To achieve this, we first introduce Tree of Preferences (ToP), an innovative structure constructed to model user preferences from coarse to fine. ToP enables LLMs to systematically reason over the user's rationale behind their behavior, thereby uncovering their underexplored preferences. To guide diversified recommendations using uncovered preferences, we adopt a data-centric approach, identifying candidate items that match user preferences and generating synthetic interactions that reflect underexplored preferences. These interactions are integrated to train a general recommender for diversification. Moreover, we scale up overall efficiency by dynamically selecting influential users during optimization. Extensive evaluations of both diversity and relevance show that our approach outperforms existing methods in most cases and achieves near-optimal performance in others, with reasonable inference latency.

</details>


### [8] [A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance](https://arxiv.org/abs/2601.02428)
*Okan Bursa*

Main category: cs.IR

TL;DR: 本文提出了自适应检索增强生成(ARM)框架，该框架利用动态记忆基质取代静态向量索引，模仿认知巩固和遗忘原理，以达到高效检索性能。在轻量级检索基准测试中，ARM仅使用约2200万参数就接近了最先进水平，并且在超高效模型中表现出最佳效率。此外，研究还比较了Llama 3.1与GPT-4o在静态与动态RAG组合上的表现差异，指出前者在适度延迟下实现最高关键词覆盖率（67.2%），而后者则通过动态选择性检索策略获得最快响应速度（平均8.2秒）及竞争性的覆盖范围（58.7%）。


<details>
  <summary>Details</summary>
Motivation: 为了提高检索增强生成系统的效率和性能，同时解决传统方法中因使用固定向量索引而导致的记忆资源浪费问题。

Method: 引入了一种名为Adaptive RAG Memory (ARM)的新框架，它采用了受认知科学启发的动态记忆机制来替代原有的静态索引方式。该机制允许频繁访问的信息得到加强并免于被遗忘，而不常用的信息则会逐渐衰退。

Result: 实验结果显示，在轻量级检索任务上，ARM能够以较少的参数数量（约22M）接近最先进的检索效果；并且在不同类型的模型（如Llama 3.1和GPT-4o）上结合使用静态或动态RAG时，能够根据实际需求平衡质量、延迟和内存效率。

Conclusion: ARM框架不仅提供了与现有解决方案相媲美的准确性，而且通过其独特的记忆增长自我调节机制以及可解释的记忆保持动态特性，为生产和研究中的RAG系统提供了一个实用的选择。

Abstract: We introduce \emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\approx$ 0.940, Recall@5 $=1.000$) with only $\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings.
  ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.

</details>


### [9] [CREAM: Continual Retrieval on Dynamic Streaming Corpora with Adaptive Soft Memory](https://arxiv.org/abs/2601.02708)
*HuiJeong Son,Hyeongu Kang,Sunho Kim,Subeen Ho,SeongKu Kang,Dongha Lee,Susik Yoon*

Main category: cs.IR

TL;DR: 提出了CREAM，一种自监督框架，用于基于记忆的持续检索，以更有效地学习新语料库中未见主题而无需真实标签。通过细粒度相似性估计、正则化集群原型设计和分层核心集采样等技术，在无标签设置下实现了优于最强方法27.79% (Success@5) 和44.5% (Recall@10) 的平均检索准确率，并且性能与甚至超过有监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于记忆的连续学习方法依赖于固定的一组带有真实相关文档的查询，这限制了它们对未见过的查询和文档的泛化能力，使得这些方法在实际应用中不太实用。为了改善这一点，研究旨在开发一种能够有效处理流数据中未见话题的学习方法，而不需要依赖于标签。

Method: 提出了一种名为CREAM（持续检索增强记忆）的自监督框架，该框架利用动态结构化的软记忆来捕捉流查询和文档不断变化的语义，并适应已见和未见话题。此框架通过三个关键技术实现：细粒度相似性估计、规范化集群原型化以及分层核心集抽样。

Result: 实验表明，CREAM在两个基准数据集上的表现优于其他方法，特别是在没有标签的情况下，其在Success@5指标上平均提高了27.79%，在Recall@10指标上平均提高了44.5%，并且达到了可以与甚至超越监督方法的性能水平。

Conclusion: CREAM提供了一个有效的解决方案来解决动态数据流中的信息检索挑战，特别是对于那些没有标签的新出现的话题。这种方法不仅提高了适应性和检索准确性，还显示出即使在缺乏标注数据的情况下也能达到或超越传统监督方法的潜力。

Abstract: Information retrieval (IR) in dynamic data streams is emerging as a challenging task, as shifts in data distribution degrade the performance of AI-powered IR systems. To mitigate this issue, memory-based continual learning has been widely adopted for IR. However, existing methods rely on a fixed set of queries with ground-truth relevant documents, which limits generalization to unseen queries and documents, making them impractical for real-world applications. To enable more effective learning with unseen topics of a new corpus without ground-truth labels, we propose CREAM, a self-supervised framework for memory-based continual retrieval. CREAM captures the evolving semantics of streaming queries and documents into dynamically structured soft memory and leverages it to adapt to both seen and unseen topics in an unsupervised setting. We realize this through three key techniques: fine-grained similarity estimation, regularized cluster prototyping, and stratified coreset sampling. Experiments on two benchmark datasets demonstrate that CREAM exhibits superior adaptability and retrieval accuracy, outperforming the strongest method in a label-free setting by 27.79\% in Success@5 and 44.5\% in Recall@10 on average, and achieving performance comparable to or even exceeding that of supervised methods.

</details>


### [10] [Netflix Artwork Personalization via LLM Post-training](https://arxiv.org/abs/2601.02764)
*Hyunji Nam,Sejoon Oh,Emma Kong,Yesu Feng,Moumita Bhattacharya*

Main category: cs.IR

TL;DR: 本研究探索了根据用户的多样化偏好提供个性化艺术作品推荐的问题，通过对预训练的大规模语言模型进行后训练来实现。实验结果显示，在使用Llama 3.1 8B模型对110K数据点进行训练并在5K用户-标题对上评估后，与Netflix生产模型相比，这种方法提高了3-5%的性能，表明了利用大规模语言模型进行细粒度个性化推荐的一个有前景的方向。


<details>
  <summary>Details</summary>
Motivation: 鉴于用户在娱乐平台上与各种各样的标题互动，并且每个标题都由一个艺术品代表，但不同的用户可能因个人偏好不同而对同一艺术品产生不同的共鸣。因此，存在一种需求，即开发出能够考虑这种用户异质性的个性化艺术品推荐系统，以提高用户满意度和参与度。

Method: 采用预训练的大规模语言模型（LLMs），然后针对个性化艺术品推荐任务进行后训练。方法旨在为每个用户挑选最符合其偏好的视觉表现形式，从而实现更加个性化的推荐。

Result: 通过使用Llama 3.1 8B模型进行实验，在包含110K数据点的数据集上训练，并在5K保留用户-标题对上测试，结果表明，经过后训练的LLM比Netflix现有的生产模型表现更好，具体表现为性能提升3-5%。

Conclusion: 研究结果表明，通过对预训练的大规模语言模型进行后训练以适应个性化艺术品推荐的需求是一种有效的方法，可以显著提高用户满意度与参与度。这为将来使用大规模语言模型来实现更细致的个性化推荐提供了新的方向。

Abstract: Large language models (LLMs) have demonstrated success in various applications of user recommendation and personalization across e-commerce and entertainment. On many entertainment platforms such as Netflix, users typically interact with a wide range of titles, each represented by an artwork. Since users have diverse preferences, an artwork that appeals to one type of user may not resonate with another with different preferences. Given this user heterogeneity, our work explores the novel problem of personalized artwork recommendations according to diverse user preferences. Similar to the multi-dimensional nature of users' tastes, titles contain different themes and tones that may appeal to different viewers. For example, the same title might feature both heartfelt family drama and intense action scenes. Users who prefer romantic content may like the artwork emphasizing emotional warmth between the characters, while those who prefer action thrillers may find high-intensity action scenes more intriguing. Rather than a one-size-fits-all approach, we conduct post-training of pre-trained LLMs to make personalized artwork recommendations, selecting the most preferred visual representation of a title for each user and thereby improving user satisfaction and engagement. Our experimental results with Llama 3.1 8B models (trained on a dataset of 110K data points and evaluated on 5K held-out user-title pairs) show that the post-trained LLMs achieve 3-5\% improvements over the Netflix production model, suggesting a promising direction for granular personalized recommendations using LLMs.

</details>


### [11] [COFFEE: COdesign Framework for Feature Enriched Embeddings in Ads-Ranking Systems](https://arxiv.org/abs/2601.02807)
*Sohini Roychowdhury,Doris Wang,Qian Ge,Joy Mu,Srihari Reddy*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的三维框架，用于增强用户-广告表示，不增加模型推理或服务复杂性。通过整合多源事件、延长用户历史记录和丰富数据属性及多模态嵌入，该方法在提高AUC、CTR预测以及改善用户-广告表示方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了准确评估用户在接触内容前后的兴趣，多样且丰富的数据来源对商业广告推荐模型至关重要。尽管扩展用户互动历史可以改进用户兴趣预测，但根据规模法则原则，从多个来源嵌入活动序列以确保用户和广告表示的新鲜度同样重要。

Method: 本文介绍了一种新的三维框架，旨在增强用户-广告表示而不增加模型推理或服务复杂性的负担。第一维度考察了整合多样化事件来源的影响；第二维度探讨了较长用户历史的好处；第三维度则集中于通过附加事件属性和多模态嵌入来丰富数据。

Result: 所提方法能够将广告曝光源的AUC和缩放曲线斜率提升至原始有机使用源的1.56到2倍，即使在线序列长度仅为100至10,000也是如此。此外，当使用富化过的广告曝光事件源时，点击率(CTR)预测相比基线生产广告推荐系统提高了0.56% AUC，从而改善了更长和离线用户-广告表示的序列缩放分辨率。

Conclusion: 研究结果表明，通过引入多源事件、延长用户历史记录并结合额外事件属性与多模态嵌入，可以在不影响模型推理和服务复杂度的情况下显著提高用户-广告交互的质量。

Abstract: Diverse and enriched data sources are essential for commercial ads-recommendation models to accurately assess user interest both before and after engagement with content. While extended user-engagement histories can improve the prediction of user interests, it is equally important to embed activity sequences from multiple sources to ensure freshness of user and ad-representations, following scaling law principles. In this paper, we present a novel three-dimensional framework for enhancing user-ad representations without increasing model inference or serving complexity. The first dimension examines the impact of incorporating diverse event sources, the second considers the benefits of longer user histories, and the third focuses on enriching data with additional event attributes and multi-modal embeddings. We assess the return on investment (ROI) of our source enrichment framework by comparing organic user engagement sources, such as content viewing, with ad-impression sources. The proposed method can boost the area under curve (AUC) and the slope of scaling curves for ad-impression sources by 1.56 to 2 times compared to organic usage sources even for short online-sequence lengths of 100 to 10K. Additionally, click-through rate (CTR) prediction improves by 0.56% AUC over the baseline production ad-recommendation system when using enriched ad-impression event sources, leading to improved sequence scaling resolutions for longer and offline user-ad representations.

</details>


### [12] [HarmonRank: Ranking-aligned Multi-objective Ensemble for Live-streaming E-commerce Recommendation](https://arxiv.org/abs/2601.02955)
*Boyang Xia,Zhou Yu,Zhiliang Zhu,Hanxiao Sun,Biyun Han,Jun Wang,Runnan Liu,Wenwu Ou*

Main category: cs.IR

TL;DR: 本文提出了一种新的多目标集成框架HarmonRank，旨在解决直播电商推荐系统中的排名任务与目标间对齐问题。通过将AUC转化为可微分的排序技术，并采用两步关系感知集成方案，该方法在离线和在线实验中均表现出色，已在拥有4亿日活跃用户的快手直播电商平台全面部署，带来了超过2%的购买增长。


<details>
  <summary>Details</summary>
Motivation: 直播电商推荐系统越来越受到关注，但现有的多目标集成模型存在两个主要问题：一是二分类任务的优化方向与实际的排名任务不一致；二是忽略了不同目标之间的关联性。为了解决这些问题，需要一种既能对齐到排名任务又能实现目标间对齐的新方法。

Method: 提出了名为HarmonRank的新型多目标集成框架。对于排名任务对齐，将AUC度量转换成一个排序求和问题并通过可微分的排序技术进行优化。针对目标间的对齐，则是将传统的一步式集成方式改为两步的关系感知集成方案。

Result: 在两个工业数据集上进行了广泛的离线实验以及线上测试，结果表明所提方法显著优于当前最先进的方法。此外，该方法已经在快手直播电商平台全面实施，服务着4亿的日活跃用户群，并且贡献了超过2%的购买增长。

Conclusion: HarmonRank作为一种创新性的多目标集成框架，在解决直播电商推荐系统的挑战方面展现出了卓越性能，不仅有效提升了用户体验，还为企业带来了可观的商业价值提升。

Abstract: Recommendation for live-streaming e-commerce is gaining increasing attention due to the explosive growth of the live streaming economy. Different from traditional e-commerce, live-streaming e-commerce shifts the focus from products to streamers, which requires ranking mechanism to balance both purchases and user-streamer interactions for long-term ecology. To trade off multiple objectives, a popular solution is to build an ensemble model to integrate multi-objective scores into a unified score. The ensemble model is usually supervised by multiple independent binary classification losses of all objectives. However, this paradigm suffers from two inherent limitations. First, the optimization direction of the binary classification task is misaligned with the ranking task (evaluated by AUC). Second, this paradigm overlooks the alignment between objectives, e.g., comment and buy behaviors are partially dependent which can be revealed in labels correlations. The model can achieve better trade-offs if it learns the aligned parts of ranking abilities among different objectives.
  To mitigate these limitations, we propose a novel multi-objective ensemble framework HarmonRank to fulfill both alignment to the ranking task and alignment among objectives. For alignment to ranking, we formulate ranking metric AUC as a rank-sum problem and utilize differentiable ranking techniques for ranking-oriented optimization. For inter-objective alignment, we change the original one-step ensemble paradigm to a two-step relation-aware ensemble scheme.
  Extensive offline experiments results on two industrial datasets and online experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods. The proposed method has been fully deployed in Kuaishou's live-streaming e-commerce recommendation platform with 400 million DAUs, contributing over 2% purchase gain.

</details>


### [13] [Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation](https://arxiv.org/abs/2601.02962)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 本研究提出了一种新的搜索查询建议偏差识别方法，通过递归算法审问技术创建建议树，以获取更深层次的搜索建议，并基于此研究了与政治领域人物相关的搜索中的话题群体偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管搜索查询建议在在线信息搜索中扮演着重要角色，但对其的研究远少于搜索引擎的其他方面。主要原因是查询建议上下文稀疏且每个查询最多只有十个建议的数据基础有限，这使得识别搜索查询建议中的偏差变得困难。

Method: 采用递归算法审问技术并构建建议树来增加对较不显眼的搜索建议的访问，以此深化偏差分析的数据基础。

Result: 该方法能够提供更多的搜索建议用于分析，特别是对于与政治领域相关的人物搜索，可以用来调查话题群体偏差。

Conclusion: 这项工作展示了一种新的搜索查询建议偏差识别方法，它允许访问更多隐含的搜索建议，从而为偏差分析提供了更丰富的数据基础，特别是在政治领域人物相关的搜索中发现了话题群体偏差。

Abstract: Despite their important role in online information search, search query suggestions have not been researched as much as most other aspects of search engines. Although reasons for this are multi-faceted, the sparseness of context and the limited data basis of up to ten suggestions per search query pose the most significant problem in identifying bias in search query suggestions. The most proven method to reduce sparseness and improve the validity of bias identification of search query suggestions so far is to consider suggestions from subsequent searches over time for the same query. This work presents a new, alternative approach to search query bias identification that includes less high-level suggestions to deepen the data basis of bias analyses. We employ recursive algorithm interrogation techniques and create suggestion trees that enable access to more subliminal search query suggestions. Based on these suggestions, we investigate topical group bias in person-related searches in the political domain.

</details>


### [14] [Parallel Latent Reasoning for Sequential Recommendation](https://arxiv.org/abs/2601.03153)
*Jiakai Tang,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 提出了一种新的框架Parallel Latent Reasoning (PLR)，通过同时探索多个多样化的推理路径来解决现有方法在序列推荐中随着推理深度增加而回报递减的问题，从而提高推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在推理方法仅依赖于单一轨迹上的深度级扩展，导致随着推理深度的增加出现收益递减的问题。为了解决这一局限性，并且更有效地捕捉用户偏好，研究提出了Parallel Latent Reasoning (PLR)框架。

Method: PLR通过在连续潜在空间中学习触发令牌来构建平行推理流，使用全局推理正则化保持不同流之间的多样性，并通过多流合成聚合自适应地综合多流输出。

Result: 在三个真实世界数据集上进行的广泛实验表明，与最先进的基线相比，PLR显著提高了性能，同时保持了实时推断效率。此外，理论分析也进一步验证了并行推理对于提高泛化能力的有效性。

Conclusion: 本研究表明，通过引入宽度级别的计算扩展，PLR能够超越当前基于深度扩展的方法，在序列推荐任务上表现出色，开启了增强序列推荐中推理能力的新方向。

Abstract: Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose \textbf{Parallel Latent Reasoning (PLR)}, a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.

</details>


### [15] [Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers](https://arxiv.org/abs/2601.03211)
*Yue Kang,Zhuoyi Huang,Benji Schussheim,Diana Licon,Dina Atia,Shixing Cao,Jacob Danovitch,Kunho Kim,Billy Norcilien,Jonah Karpman,Mahmound Sayed,Mike Taylor,Tao Sun,Pavel Metrikov,Vipul Agarwal,Chris Quirk,Ye-Yi Wang,Nick Craswell,Irene Shaffer,Tianwei Chen,Sulaiman Vesal,Soundar Srinivasan*

Main category: cs.IR

TL;DR: 提出了一种有效的方法，通过微调小型语言模型来进行准确的相关性标注，使用合成数据生成克服了企业领域高质量和可访问数据集的缺乏问题。


<details>
  <summary>Details</summary>
Motivation: 在企业搜索中，由于获取标记数据的难度，构建大规模高质量的数据集仍然是一个核心挑战。

Method: 该方法利用大型语言模型合成现实的企业查询，应用BM25检索难例，并使用教师大型语言模型分配相关性分数。最终将所得数据集提炼成一个小型语言模型（SLM），产生一个紧凑的相关性标注器。

Result: 在由训练有素的人类注释者标注的923对企业查询-文档对的高质量基准上评估了该方法，结果显示提炼出的小型语言模型与人类判断的一致性达到了或超过了教师大型语言模型。此外，微调后的标注器显著提高了吞吐量，实现了17倍的增长，同时成本效益提高了19倍。

Conclusion: 这种方法为企业规模的检索应用程序提供了可扩展且具有成本效益的相关性标注解决方案，支持现实世界环境下的快速离线评估和迭代。

Abstract: In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [Proceedings of the 1st International Workshop on Low Carbon Computing (LOCO 2024)](https://arxiv.org/abs/2601.02898)
*Wim Vanderbauwhede,Lauritz Thamsen,José Cano*

Main category: cs.DC

TL;DR: 这是2024年第一届低碳计算国际研讨会（LOCO 2024）的会议记录。


<details>
  <summary>Details</summary>
Motivation: 该论文集旨在汇集关于低碳计算领域的最新研究和进展，促进学术交流和技术发展，以应对全球气候变化挑战。

Method: 通过组织国际研讨会，邀请来自世界各地的研究人员提交他们在低碳计算方面的研究成果，并通过同行评审来选择高质量的文章收录进会议记录中。

Result: 会议记录包含了经过严格筛选后的多篇论文，涵盖了低碳计算的各种主题，如能源效率、绿色IT解决方案等。

Conclusion: LOCO 2024为低碳计算领域内的研究人员提供了一个宝贵的交流平台，有助于推动相关技术的发展和创新。

Abstract: This is the proceedings of the 1st International Workshop on Low Carbon Computing (LOCO 2024).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [Case Count Metric for Comparative Analysis of Entity Resolution Results](https://arxiv.org/abs/2601.02824)
*John R. Talburt,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Onais Khan Mohammed,Mahboob Khan Mohammed,Khizer Syed,Leon Claasssens*

Main category: cs.DB

TL;DR: 本文介绍了一种新的过程和软件系统——案例计数度量系统(CCMS)，用于在不知道真实链接（标签）的情况下，比较和分析两个不同的ER聚类过程对同一数据集的结果。CCMS能够生成一系列计数来描述第一过程中产生的聚类如何通过第二过程基于四种可能的转换情景进行转换，并且在分析模式下还可以帮助用户评估这些变化。


<details>
  <summary>Details</summary>
Motivation: 在不知道真实链接（标签）的情况下，需要一种方法来比较和分析两个不同实体解析(ER)聚类过程对于相同数据集的结果。

Method: 开发了案例计数度量系统(CCMS)，该系统能够计算第一过程形成的每个聚类经过第二过程后保持不变、合并成更大聚类、被分割为更小聚类或与多个新聚类重叠的情况的数量。此外，CCMS还提供了分析功能以帮助用户审查这些变化。

Result: CCMS能够有效提供两组ER聚类结果之间转换情况的详细信息，并且已经在学术界和工业研究中得到了应用。

Conclusion: 案例计数度量系统(CCMS)是一个有用的工具，它能够系统地比较没有已知真实链接时两个ER聚类过程的结果，从而支持研究人员更好地理解聚类算法的行为及其影响。

Abstract: This paper describes a new process and software system, the Case Count Metric System (CCMS), for systematically comparing and analyzing the outcomes of two different ER clustering processes acting on the same dataset when the true linking (labeling) is not known. The CCMS produces a set of counts that describe how the clusters produced by the first process are transformed by the second process based on four possible transformation scenarios. The transformations are that a cluster formed in the first process either remains unchanged, merges into a larger cluster, is partitioned into smaller clusters, or otherwise overlaps with multiple clusters formed in the second process. The CCMS produces a count for each of these cases, accounting for every cluster formed in the first process. In addition, when run in analysis mode, the CCMS program can assist the user in evaluating these changes by displaying the details for all changes or only for certain types of changes. The paper includes a detailed description of the CCMS process and program and examples of how the CCMS has been applied in university and industry research.

</details>


### [18] [SpANNS: Optimizing Approximate Nearest Neighbor Search for Sparse Vectors Using Near Memory Processing](https://arxiv.org/abs/2601.03229)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.DB

TL;DR: 提出了SpANNS，一种用于稀疏近似最近邻搜索的近内存处理架构，结合了混合倒排索引与高效的查询管理和运行时优化，在CXL Type-2近内存平台上实现，相比最新的CPU基线性能提升了15.2倍至21.6倍。


<details>
  <summary>Details</summary>
Motivation: 随着结合稀疏和密集嵌入的混合检索系统在信息检索流程中变得越来越普遍，现有基于CPU实现的稀疏近似最近邻搜索（ANNS）成为扩展性的瓶颈。

Method: 通过设计名为SpANNS的近内存处理架构来解决这个问题，该架构利用混合倒排索引、有效查询管理以及运行时优化，并且是在支持计算功能的DIMM上实现索引遍历和距离计算等操作。

Result: SpANNS在执行速度上比当前最先进的CPU基线快15.2倍到21.6倍，为稀疏向量搜索提供了可扩展且高效的解决方案。

Conclusion: SpANNS架构显著提高了稀疏向量数据库中近似最近邻搜索的速度和效率，表明它是未来处理高维空间相似性搜索的有效方法。

Abstract: Approximate Nearest Neighbor Search (ANNS) is a fundamental operation in vector databases, enabling efficient similarity search in high-dimensional spaces. While dense ANNS has been optimized using specialized hardware accelerators, sparse ANNS remains limited by CPU-based implementations, hindering scalability. This limitation is increasingly critical as hybrid retrieval systems, combining sparse and dense embeddings, become standard in Information Retrieval (IR) pipelines. We propose SpANNS, a near-memory processing architecture for sparse ANNS. SpANNS combines a hybrid inverted index with efficient query management and runtime optimizations. The architecture is built on a CXL Type-2 near-memory platform, where a specialized controller manages query parsing and cluster filtering, while compute-enabled DIMMs perform index traversal and distance computations close to the data. It achieves 15.2x to 21.6x faster execution over the state-of-the-art CPU baselines, offering scalable and efficient solutions for sparse vector search.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Physical Transformer](https://arxiv.org/abs/2601.02433)
*Tao Xu,Zhixin Hu,Li Luo,Momiao Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种物理变换器，将现代变换器风格的计算与几何表示和物理动力学相结合，在简单的玩具问题上表现出色，并为统一数字推理与物理基础流形的AI模型提供了可能。


<details>
  <summary>Details</summary>
Motivation: 现有的数字AI系统主要在虚拟空间中操作，尽管取得了显著进展，但缺乏与现实世界的直接互动。本文旨在通过结合现代变换器计算、几何表示及物理动力学来解决这一局限性，推动向物理AI的发展。

Method: 设计了一个多层次架构：微观层面采用有效哈密顿量加非哈密顿浴项来模拟注意力头和前馈块；介观层面状态演化遵循学习到的神经微分流形（NDM）上的哈密顿流和HJB最优控制；宏观层面上维持生成语义工作区以及二维信息-相位图谱，跟踪推理轨迹中的不确定性和信息增益。

Result: 在涉及数值积分和动力系统的简单玩具问题上，物理变换器相较于简单基线方法展现了更好的稳定性和长期准确性。

Conclusion: 该框架展示了尊重底层几何与哈密顿结构的优势，为开发更可解释且可能统一的推理、控制及与现实世界交互模型开辟了道路。

Abstract: Digital AI systems spanning large language models, vision models, and generative architectures that operate primarily in symbolic, linguistic, or pixel domains. They have achieved striking progress, but almost all of this progress lives in virtual spaces. These systems transform embeddings and tokens, yet do not themselves touch the world and rarely admit a physical interpretation. In this work we propose a physical transformer that couples modern transformer style computation with geometric representation and physical dynamics. At the micro level, attention heads, and feed-forward blocks are modeled as interacting spins governed by effective Hamiltonians plus non-Hamiltonian bath terms. At the meso level, their aggregated state evolves on a learned Neural Differential Manifold (NDM) under Hamiltonian flows and Hamilton, Jacobi, Bellman (HJB) optimal control, discretized by symplectic layers that approximately preserve geometric and energetic invariants. At the macro level, the model maintains a generative semantic workspace and a two-dimensional information-phase portrait that tracks uncertainty and information gain over a reasoning trajectory. Within this hierarchy, reasoning tasks are formulated as controlled information flows on the manifold, with solutions corresponding to low cost trajectories that satisfy geometric, energetic, and workspace-consistency constraints. On simple toy problems involving numerical integration and dynamical systems, the physical transformer outperforms naive baselines in stability and long-horizon accuracy, highlighting the benefits of respecting underlying geometric and Hamiltonian structure. More broadly, the framework suggests a path toward physical AI that unify digital reasoning with physically grounded manifolds, opening a route to more interpretable and potentially unified models of reasoning, control, and interaction with the real world.

</details>


### [20] [Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth](https://arxiv.org/abs/2601.02609)
*Arjun S. Nair*

Main category: cs.LG

TL;DR: 本文提出了一种名为Chronicals的开源训练框架，通过四个协同优化技术实现了对大型语言模型微调速度的显著提升，相比Unsloth方法提速达到3.51倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在进行微调时面临内存瓶颈问题，使得即使是最先进的硬件也无法满足其需求。为了解决这个问题，并提高训练效率，研究者开发了Chronicals框架。

Method: Chronicals框架采用了四种主要的技术来实现优化：(1) 融合Triton内核以减少内存流量；(2) 通过在线softmax计算减少logit所需的内存；(3) 使用改进版LoRA方法调整适配器矩阵之间的学习率；(4) 应用最佳拟合递减序列打包技术回收由于填充而浪费的计算资源。

Result: 实验结果表明，在使用A100-40GB GPU对Qwen2.5-0.5B模型进行全量微调时，Chronicals能够达到41,184 tokens/秒的速度，而Unsloth只能达到11,736 tokens/秒。对于LoRA方式，当rank设置为32时，Chronicals的速度可以达到11,699 tokens/秒，远高于Unsloth MAX的2,857 tokens/秒。此外，研究还发现Unsloth报告中的一个高基准实际上没有产生有效的梯度更新。

Conclusion: Chronicals通过一系列创新性优化大幅提升了大型语言模型微调过程中的性能表现，不仅有效解决了内存限制问题，而且提供了完整的数学基础支持以及易于使用的开源实现。

Abstract: Large language model fine-tuning is bottlenecked by memory: a 7B parameter model requires 84GB--14GB for weights, 14GB for gradients, and 56GB for FP32 optimizer states--exceeding even A100-40GB capacity. We present Chronicals, an open-source training framework achieving 3.51x speedup over Unsloth through four synergistic optimizations: (1) fused Triton kernels eliminating 75% of memory traffic via RMSNorm (7x), SwiGLU (5x), and QK-RoPE (2.3x) fusion; (2) Cut Cross-Entropy reducing logit memory from 5GB to 135MB through online softmax computation; (3) LoRA+ with theoretically-derived 16x differential learning rates between adapter matrices; and (4) Best-Fit Decreasing sequence packing recovering 60-75% of compute wasted on padding.
  On Qwen2.5-0.5B with A100-40GB, Chronicals achieves 41,184 tokens/second for full fine-tuning versus Unsloth's 11,736 tokens/second (3.51x). For LoRA at rank 32, we reach 11,699 tokens/second versus Unsloth MAX's 2,857 tokens/second (4.10x). Critically, we discovered that Unsloth's reported 46,000 tokens/second benchmark exhibited zero gradient norms--the model was not training.
  We provide complete mathematical foundations: online softmax correctness proofs, FlashAttention IO complexity bounds O(N^2 d^2 M^{-1}), LoRA+ learning rate derivations from gradient magnitude analysis, and bin-packing approximation guarantees. All implementations, benchmarks, and proofs are available at https://github.com/Ajwebdevs/Chronicals with pip installation via https://pypi.org/project/chronicals/.

</details>


### [21] [Polynomial Convergence of Riemannian Diffusion Models](https://arxiv.org/abs/2601.02499)
*Xingyu Xu,Ziyi Zhang,Yorie Nakahira,Guannan Qu,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文在非欧几里得空间上对扩散模型进行了更深入的理论分析，证明了在$L_2$准确度下的分数估计情况下，只需要多项式小步长即可保证总变差距离中的小采样误差，而不需要数据分布的平滑性或正性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数扩散模型假设底层空间是欧几里得的，但在很多实际应用中，数据被限制在欧几里得空间的一个子流形上。为了解决这一问题，De Bortoli等人（2022年）引入了黎曼扩散模型，并证明了使用指数级小的步长可以在Wasserstein距离中获得较小的采样误差。但是，这些结论要求数据分布是平滑且严格正的。

Method: 通过利用Li-Yau对于热核对数梯度的估计以及Minakshisundaram-Pleijel关于扰动热方程的参数展开方法，作者们证明了仅需$L_2$准确度下的分数估计和多项式小步长就能确保总变差距离中的小采样误差，同时对基础流形仅做温和的标准曲率假设。

Result: 结果表明，在不需要数据分布具有平滑性或正性的条件下，采用多项式小步长足以保证总变差距离中的小采样误差。这为非欧几里得空间上的扩散模型提供了更为精确的分析途径。

Conclusion: 本研究显著加强了现有理论，为非欧几里得空间中扩散模型的分析奠定了基础，尤其是当面对不满足平滑性或正性条件的数据分布时。

Abstract: Diffusion models have demonstrated remarkable empirical success in the recent years and are considered one of the state-of-the-art generative models in modern AI. These models consist of a forward process, which gradually diffuses the data distribution to a noise distribution spanning the whole space, and a backward process, which inverts this transformation to recover the data distribution from noise. Most of the existing literature assumes that the underlying space is Euclidean. However, in many practical applications, the data are constrained to lie on a submanifold of Euclidean space. Addressing this setting, De Bortoli et al. (2022) introduced Riemannian diffusion models and proved that using an exponentially small step size yields a small sampling error in the Wasserstein distance, provided the data distribution is smooth and strictly positive, and the score estimate is $L_\infty$-accurate. In this paper, we greatly strengthen this theory by establishing that, under $L_2$-accurate score estimate, a {\em polynomially small stepsize} suffices to guarantee small sampling error in the total variation distance, without requiring smoothness or positivity of the data distribution. Our analysis only requires mild and standard curvature assumptions on the underlying manifold. The main ingredients in our analysis are Li-Yau estimate for the log-gradient of heat kernel, and Minakshisundaram-Pleijel parametrix expansion of the perturbed heat equation. Our approach opens the door to a sharper analysis of diffusion models on non-Euclidean spaces.

</details>


### [22] [GEM-Style Constraints for PEFT with Dual Gradient Projection in LoRA](https://arxiv.org/abs/2601.02500)
*Brian Tekmen,Jason Yin,Qianqian Tong*

Main category: cs.LG

TL;DR: 本研究提出了一种名为I-GEM的方法，该方法在低秩适配器（LoRA）子空间内重新审视了梯度情景记忆（GEM），以实现大型语言模型的持续学习。I-GEM通过仅在适配器参数内部限制非干扰性，保持了类似GEM的稳定性同时大幅减少了平均投影开销。实验表明，I-GEM在准确率上接近GEM，并且相较于A-GEM有所提升，更重要的是，它将投影时间相比GEM降低了约1000倍。


<details>
  <summary>Details</summary>
Motivation: 由于对大型语言模型进行全面微调计算成本高昂，因此需要探索能够利用参数高效适配器的持续学习方法来降低这一成本。

Method: 研究者们提出了I-GEM，这是一种基于固定预算、GPU驻留的双重投影梯度近似技术，专门针对GEM中的二次投影问题而设计。通过只在适配器参数中施加非干扰约束，I-GEM能够在保持性能的同时极大地减少计算需求。

Result: 在使用GPT-2 (355M) 和 LoRA ($r=8$) 对具有诱导域漂移的3任务AG News数据集进行测试时，I-GEM达到了与GEM相近的平均精度(相差约0.04分)，并且比A-GEM高出约1.4分。此外，相对于GEM而言，I-GEM的投影时间减少了大约1000倍。

Conclusion: 研究结果表明，在LoRA子空间应用GEM约束为大规模语言模型上的持续学习提供了一条实用途径。

Abstract: Full fine-tuning of Large Language Models (LLMs) is computationally costly, motivating Continual Learning (CL) approaches that utilize parameter-efficient adapters. We revisit Gradient Episodic Memory (GEM) within the Low-Rank Adapter (LoRA) subspace and introduce I-GEM: a fixed-budget, GPU-resident dual projected-gradient approximation to GEM's quadratic projection. By constraining non-interference solely within the adapter parameters, I-GEM preserves GEM-like stability with orders-of-magnitude lower mean projection overhead. On a 3-task AG News split with induced domain drift, using GPT-2 (355M) and LoRA ($r=8$), I-GEM matches GEM's average accuracy (within $\sim\!0.04$ pts) and outperforms A-GEM by $\sim\!1.4$ pts. Crucially, it reduces projection time vs.\ GEM by a factor of $\sim\!10^3$. These results suggest that applying GEM constraints in the LoRA subspace is a practical pathway for continual learning at the LLM scale.

</details>


### [23] [hdlib 2.0: Extending Machine Learning Capabilities of Vector-Symbolic Architectures](https://arxiv.org/abs/2601.02509)
*Fabio Cumbo,Kabir Dhillon,Daniel Blankenberg*

Main category: cs.LG

TL;DR: hdlib, a Python library for Vector-Symbolic Architectures (VSA), has been extended to include advanced machine learning models such as supervised classification with feature selection, regression, clustering, and graph-based learning. Additionally, it introduces Quantum Hyperdimensional Computing and a Quantum Machine Learning model. The library is open-source, available on GitHub, and can be installed via pip or Conda.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the extension of hdlib is to meet the increasing demand for more sophisticated, data-driven modeling within the VSA framework, which represents and processes information using high-dimensional vectors.

Method: The method involves adding four new extensions to hdlib: enhanced supervised classification with feature selection, a new regression model, a clustering model, and a graph-based learning model. Furthermore, they have implemented Quantum Hyperdimensional Computing with quantum-powered arithmetic operations and a Quantum Machine Learning model for supervised learning.

Result: The result is an expanded version of hdlib that now supports a wider range of machine learning tasks, including both classical and quantum computing approaches. This makes it a more versatile tool for researchers and practitioners in the field of VSA and machine learning.

Conclusion: The conclusion is that this major update to hdlib significantly broadens its applicability and utility for the design and implementation of VSA and machine learning solutions, while maintaining its open-source status and accessibility through common package managers.

Abstract: Following the initial publication of hdlib, a Python library for designing Vector-Symbolic Architectures (VSA), we introduce a major extension that significantly enhances its machine learning capabilities. VSA, also known as Hyperdimensional Computing, is a computing paradigm that represents and processes information using high-dimensional vectors. While the first version of hdlib established a robust foundation for creating and manipulating these vectors, this update addresses the growing need for more advanced, data-driven modeling within the VSA framework. Here, we present four extensions: significant enhancements to the existing supervised classification model also enabling feature selection, and a new regression model for predicting continuous variables, a clustering model for unsupervised learning, and a graph-based learning model. Furthermore, we propose the first implementation ever of Quantum Hyperdimensional Computing with quantum-powered arithmetic operations and a new Quantum Machine Learning model for supervised learning. hdlib remains open-source and available on GitHub at https://github.com/cumbof/hdlib under the MIT license, and distributed through the Python Package Index (pip install hdlib) and Conda (conda install -c conda-forge hdlib). Documentation and examples of these new features are available on the official Wiki at https://github.com/cumbof/hdlib/wiki.

</details>


### [24] [Prioritized Replay for RL Post-training](https://arxiv.org/abs/2601.02648)
*Mehdi Fatemi*

Main category: cs.LG

TL;DR: 本文提出了一种针对大型语言模型RL后训练的问题级优先级框架，通过基于模型的成功统计数据来选择问题，从而自然地将训练重点放在既不总是成功也不总是失败的问题上，同时降低了对梯度信息贡献小的问题的优先级。该方法不需要预定义难度等级、辅助预测器或外部标签，并引入了轻量级机制以实现实际部署。


<details>
  <summary>Details</summary>
Motivation: 传统的课程策略在训练初期强调较简单的任务，而作者观察到中等成功率的任务往往能产生更强的学习信号。因此，本研究旨在开发一种自动且持续适应的优先级排序过程，直接与基于GRPO的后训练动态相匹配。

Method: 提出的方法基于深度RL中的优先重放缓冲区理念以及先前的研究发现，即通过一个简单的模型驱动优先级分数从经验成功统计数据中选择问题。此外，还引入了堆基优先采样和定期重新测试已解决及未解决问题的轻量级机制，以缓解饥饿和遗忘现象。

Result: 所提出的方法能够在无需预定义难度层次、辅助预测器或外部标签的情况下，自动调整训练过程中不同问题的优先级，更加关注那些对于学习有帮助但又不是始终可以被解决的问题。

Conclusion: 这种方法为手动设计的课程提供了一个原则性和可扩展的替代方案，同时使得数据选择能够直接与基于GRPO的后训练动态保持一致。

Abstract: We introduce a problem-level prioritization framework for RL post-training of large language models. Building on insights from prioritized replay in deep RL, as well as prior observations that rollouts with intermediate success rates tend to produce stronger learning signals under methods such as GRPO, our approach selects problems according to a simple, model-driven priority score derived from empirical success statistics. In contrast to conventional curriculum strategies that emphasize easier tasks early in training, the resulting schedule naturally focuses training on problems that are neither consistently solved nor consistently failed, while deprioritizing those that contribute little gradient information. The method yields a continuously adapting and automatic prioritization process that requires no predefined difficulty tiers, auxiliary predictors, or external labels. We further introduce lightweight mechanisms for practical deployment, including heap-based prioritized sampling and periodic retesting of solved and unsolved problems to mitigate starvation and forgetting. Overall, the approach offers a principled and scalable alternative to manually designed curricula while aligning data selection directly with the dynamics of GRPO-based post-training.

</details>


### [25] [When Prompting Meets Spiking: Graph Sparse Prompting via Spiking Graph Prompt Learning](https://arxiv.org/abs/2601.02662)
*Bo Jiang,Weijun Zhao,Beibei Wang,Jin Tang*

Main category: cs.LG

TL;DR: 本文首次提出了一种基于脉冲神经元机制的稀疏图提示特征学习方法（SpikingGPF），通过学习每个节点的选择性特征上的稀疏提示向量，实现了更紧凑轻量的设计，并提高了对节点噪声的鲁棒性。此外，该方法还引入了基于稀疏表示理论的新颖提示表示学习模型，进一步促进了紧凑性和高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有的图提示特征（GPF）学习方法在处理节点的所有特征维度时显得冗余且容易受到节点特征噪声的影响。为了克服这个问题，本文提出了利用脉冲神经元机制来学习稀疏图提示的方法。

Method: SpikingGPF 方法主要包括两个方面：首先，它采用脉冲神经元架构为每个节点学习一个稀疏提示向量，仅对选择性的节点特征进行提示，从而实现更加紧凑和轻量级的设计同时提高对抗节点噪声的能力；其次，SpikingGPF 基于稀疏表示理论引入了一个新的提示表示学习模型，将每个节点提示表达为提示原子的稀疏组合，这不仅鼓励了更为紧凑的表示形式，也促进了高效计算。

Result: 广泛的实验表明，SpikingGPF 在多个基准测试中表现出色，既有效又具有很强的鲁棒性。

Conclusion: 通过引入脉冲神经元机制以实现稀疏图提示特征学习，SpikingGPF 提供了一种创新的方式解决现有 GPF 方法中的冗余问题以及对噪声敏感的问题，证明了其在提升模型性能与鲁棒性方面的潜力。

Abstract: Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node's all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.

</details>


### [26] [MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods](https://arxiv.org/abs/2601.02668)
*Xiaoyan Sun,Qingyu Meng,Yalu Wen*

Main category: cs.LG

TL;DR: 本文提出了一种结合统计先验与深度学习能力的多头注意力特征选择方法（MAFS），该方法在处理高维生物医学数据时，能够有效捕捉复杂的非线性关系和交互作用，同时提供可解释的重要性评分。实验结果表明，相较于现有的基于过滤器和深度学习的方法，MAFS在覆盖范围、稳定性和可扩展性方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法在处理高维生物医学数据时面临挑战，包括无法捕捉复杂关系或消除冗余、缺乏稳定性及可解释性等问题。因此需要一种既具有统计可解释性又能利用深度学习表征能力的新方法。

Method: 开发了MAFS框架，首先使用基于过滤器的先验知识进行稳定的初始化并指导学习过程；然后通过多头注意力机制从多个角度并行检查特征，以捕获复杂的非线性关系；最后，重新排序模块整合来自不同注意力头的结果，解决冲突并最小化信息损失，生成稳健一致的特征排名。

Result: 在模拟数据集以及实际癌症基因表达和阿尔茨海默病数据集上，MAFS相比于其他基于过滤器和深度学习的方法，在覆盖范围、稳定性和一致性方面表现出色。

Conclusion: MAFS为高维生物医学数据中的特征选择提供了一个可扩展、可解释且稳健的解决方案。

Abstract: Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attention improves interpretability but is limited in capturing multi-level dependencies and remains sensitive to initialization, reducing reproducibility. Most existing methods rarely combine statistical interpretability with the representational power of deep learning, particularly in ultra-high-dimensional settings. Here, we introduce MAFS (Multi-head Attention-based Feature Selection), a hybrid framework that integrates statistical priors with deep learning capabilities. MAFS begins with filter-based priors for stable initialization and guide learning. It then uses multi-head attention to examine features from multiple perspectives in parallel, capturing complex nonlinear relationships and interactions. Finally, a reordering module consolidates outputs across attention heads, resolving conflicts and minimizing information loss to generate robust and consistent feature rankings. This design combines statistical guidance with deep modeling capacity, yielding interpretable importance scores while maximizing retention of informative signals. Across simulated and real-world datasets, including cancer gene expression and Alzheimer's disease data, MAFS consistently achieves superior coverage and stability compared with existing filter-based and deep learning-based alternatives, offering a scalable, interpretable, and robust solution for feature selection in high-dimensional biomedical data.

</details>


### [27] [Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies](https://arxiv.org/abs/2601.02754)
*Mingming Zhang,Na Li,Zhuang Feiqing,Hongyang Zheng,Jiangbing Zhou,Wang Wuyin,Sheng-jie Sun,XiaoWei Chen,Junxiong Zhu,Lixin Zou,Chenliang Li*

Main category: cs.LG

TL;DR: 提出了一种新的Q值正则化生成式自动出价方法QGA，通过结合双Q学习策略与决策转换器来优化广告表现。该方法不仅能够模仿数据集中的经验，还能减少次优轨迹的负面影响，并通过Q值引导的双重探索机制安全地探索超出数据分布的策略空间。实验表明，QGA在公共基准测试和模拟环境中均优于现有方法，在大规模实际A/B测试中实现了广告GMV增长3.27%及广告ROI提升2.49%。


<details>
  <summary>Details</summary>
Motivation: 随着电子商务快速发展，自动出价成为优化不同广告主环境下广告表现的关键资产。当前基于强化学习(RL)和生成模型的方法主要通过复杂结构模仿离线历史行为，但存在高昂的超参数调优成本问题，且次优路径进一步增加了策略学习难度。为解决这些问题，提出了新方法。

Method: 提出名为QGA的新方法，将带有双Q学习策略的Q值正则化引入决策转换器(Decision Transformer)框架内。此外，还设计了一个由Q值模块动态指导、基于多个预期回报目标及局部扰动动作条件下的双重探索机制，以安全地探索超出原始数据分布范围内的策略空间。

Result: 在公开基准测试与模拟环境中的实验结果表明，QGA相比现有方案能持续取得更优或极具竞争力的表现。特别地，在大规模现实世界A/B测试场景下，使用QGA后广告GMV提升了3.27%，广告投资回报率（Ad ROI）提高了2.49%。

Conclusion: QGA作为一种创新的自动出价解决方案，成功解决了传统方法中存在的问题，如高成本的超参数调整以及次优路径带来的负面影响等。它通过联合优化策略模仿与行动价值最大化，同时利用一种新颖的探索机制，证明了其在提高广告效果方面的有效性。

Abstract: With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning.
  To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.

</details>


### [28] [Uni-FinLLM: A Unified Multimodal Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment](https://arxiv.org/abs/2601.02677)
*Gongao Zhang,Haijiang Zeng,Lu Jiang*

Main category: cs.LG

TL;DR: 提出了一种名为Uni-FinLLM的统一多模态大型语言模型，能够处理金融文本、数值时间序列、基本面和视觉数据，用于从微观到宏观层面的风险评估。该模型在股票预测、信用风险评估和系统性风险检测方面显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 金融机构和监管机构需要一种能够整合异构数据以评估从股票波动到系统脆弱性的风险的系统。现有方法通常将这些任务分开处理，未能捕捉跨尺度依赖关系。

Method: 提出了Uni-FinLLM，这是一个统一的多模态大语言模型，使用共享Transformer骨干和模块化任务头来联合处理金融文本、数值时间序列、基本面和视觉数据。通过跨模态注意力和多任务优化学习一个连贯的表现形式，适用于微观、中观和宏观级别的预测。

Result: 在股票预测、信用风险评估和系统性风险检测方面进行了评估，Uni-FinLLM显著优于基线。它将股票方向准确性提高到了67.4%（从61.7%），信贷风险准确性提高到了84.1%（从79.6%），以及宏观经济预警准确性达到了82.3%。

Conclusion: 结果验证了统一的多模态大语言模型可以共同建模资产行为和系统脆弱性，为金融提供了一个可扩展的决策支持引擎。

Abstract: Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified multimodal large language model that uses a shared Transformer backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified multimodal LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.

</details>


### [29] [CRoPE: Efficient Parametrization of Rotary Positional Embedding](https://arxiv.org/abs/2601.02728)
*Beicheng Lou,Zifei Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于复数线性变换的旋转位置编码方法，该方法比现有实现方式更自然，能够节省近50%的注意力模块参数，并且对模型性能几乎没有影响。


<details>
  <summary>Details</summary>
Motivation: 当前的旋转位置编码在Transformer模型中虽然简洁地用复数线性代数表达，但实际实现中的$Q/K/V$-投影并不等同于复数线性变换。作者认为采用复数线性变换作为参数化方式更加自然，可以显著减少参数数量而不损失模型性能。

Method: 通过将复数线性变换应用于$Q/K/V$-投影来替代现有的实现方式，以达到简化模型结构和提高参数效率的目的。

Result: 实验证明，所提出的修改几乎不影响样本内外的模型性能，同时实现了更高效的参数使用以及表示空间的更清晰解释。

Conclusion: 使用复数线性变换作为旋转位置编码的新参数化方法，不仅能够大幅度减少模型参数量，而且保持了良好的模型表现力。

Abstract: Rotary positional embedding has become the state-of-the-art approach to encode position information in transformer-based models. While it is often succinctly expressed in complex linear algebra, we note that the actual implementation of $Q/K/V$-projections is not equivalent to a complex linear transformation. We argue that complex linear transformation is a more natural parametrization and saves near 50\% parameters within the attention block. We show empirically that removing such redundancy has negligible impact on the model performance both in sample and out of sample. Our modification achieves more efficient parameter usage, as well as a cleaner interpretation of the representation space.

</details>


### [30] [Scalable Tree Ensemble Proximities in Python](https://arxiv.org/abs/2601.02735)
*Adrien Aumon,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: 本文提出了一种可分离加权叶碰撞邻近度框架，通过稀疏矩阵分解技术，实现高效、低内存的树集成邻近度计算，显著提高了处理大规模数据集时的运行时间和内存使用效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于树集成方法（如随机森林）的邻近度计算通常具有二次时间或内存复杂度，这限制了它们在大规模数据集上的应用。因此，需要一种更高效的邻近度计算方法来解决这个问题。

Method: 作者定义了一类可分离加权叶碰撞邻近度，并证明了该类别中的任何邻近度测量都可以通过精确的稀疏矩阵分解来表示。这种方法只关注叶子节点层面的碰撞，避免了显式的成对比较，从而利用Python中的稀疏线性代数进行低内存、可扩展的邻近度计算。

Result: 实验结果表明，与传统方法相比，所提出的方法在运行时间和内存消耗方面都有显著改进，能够在标准CPU硬件上有效扩展至包含数十万个样本的数据集。

Conclusion: 这项工作为基于树集成模型的大规模邻近度计算提供了一个新的框架，不仅提高了计算效率，还减少了所需内存，使得处理更大规模的数据成为可能。

Abstract: Tree ensemble methods such as Random Forests naturally induce supervised similarity measures through their decision tree structure, but existing implementations of proximities derived from tree ensembles typically suffer from quadratic time or memory complexity, limiting their scalability. In this work, we introduce a general framework for efficient proximity computation by defining a family of Separable Weighted Leaf-Collision Proximities. We show that any proximity measure in this family admits an exact sparse matrix factorization, restricting computation to leaf-level collisions and avoiding explicit pairwise comparisons. This formulation enables low-memory, scalable proximity computation using sparse linear algebra in Python. Empirical benchmarks demonstrate substantial runtime and memory improvements over traditional approaches, allowing tree ensemble proximities to scale efficiently to datasets with hundreds of thousands of samples on standard CPU hardware.

</details>


### [31] [Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction](https://arxiv.org/abs/2601.02884)
*Hana Yahia,Bruno Figliuzzi,Florent Di Meglio,Laurent Gerbaud,Stephane Menand,Mohamed Mahjoub*

Main category: cs.LG

TL;DR: 本文比较了应用于钻井时间序列数据中的领域泛化技术，重点是预测连续的粘滑指数（SSI）。通过对抗域泛化（ADG）和不变风险最小化（IRM）模型与基线模型对比，发现ADG和IRM模型分别比基线模型提高了10%和8%的性能，尤其在检测严重事件方面表现更佳。此外，迁移学习的应用进一步提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种能够跨域泛化的稳健回归模型，用于预测SSI，以评估钻头处的扭转井下振动情况。

Method: 使用60秒标记的1Hz表面钻井数据序列来训练模型，并在不同于训练井的井中测试该模型。采用网格搜索方法优化关键超参数，并对比分析了ADG、IRM及基线模型的表现。同时评估了迁移学习对模型性能提升的效果。

Result: ADG和IRM模型相较于基线模型分别实现了10%和8%的性能提升；对于严重事件的检测率，ADG/IRM模型达到了60%，而基线模型仅为20%。应用迁移学习到预训练模型后，性能得到进一步改善。

Conclusion: 结果表明，ADG和IRM模型均优于基线模型，其中ADG略胜一筹。这证明了领域泛化方法在钻井应用中的潜力，尤其是ADG方法最为有效。

Abstract: This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.

</details>


### [32] [RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance](https://arxiv.org/abs/2601.02888)
*Xuanyu Wang,Haisen Su,Jingtao Zhang,Xiangxiang Wang,Yongbin Yu,Manping Fan,Bo Gong,Siqi Chen,Mingsheng Cao,Liyong Ren*

Main category: cs.LG

TL;DR: 本研究提出了一种新的量化框架RPIQ，旨在解决视觉障碍用户在使用智能辅助系统时遇到的模型内存消耗大和推理成本高的问题。该方法通过采用基于单实例校准和高斯-赛德尔迭代量化的多协作闭环补偿方案，能够在保持接近全精度模型性能的同时，显著降低峰值内存消耗（相比原始全精度模型减少约60%-75%），并已在多种大规模模型上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 视觉受损用户在日常信息访问和实时环境感知方面面临巨大挑战，需要具有准确识别能力的智能辅助系统。尽管大规模模型为感知和推理提供了有效解决方案，但其实际部署受限于过大的内存消耗和高昂的推理成本。此外，现有量化策略常忽略块间误差累积问题，导致模型稳定性下降。

Method: 本文提出了残差投影多协作闭环及单实例量化(RPIQ)这一新型量化框架，其量化过程采用了基于单实例校准与高斯-赛德尔迭代量化的多协作闭环补偿方案。

Result: 实验表明，RPIQ能够将模型压缩至4位表示形式，并且相比于原始全精度模型大约减少了60%-75%的峰值内存消耗。同时，在多个语言和视觉任务中保持了非常接近全精度模型的表现，并在文本理解和复杂场景下的视觉问答等关键应用中展现了出色的识别与推理能力。

Conclusion: RPIQ不仅验证了其实现在真实辅助系统中的有效性，还提高了大型模型的计算效率和可靠性，使它们能够快速准确地向视障用户提供所需信息。

Abstract: Visually impaired users face significant challenges in daily information access and real-time environmental perception, and there is an urgent need for intelligent assistive systems with accurate recognition capabilities. Although large-scale models provide effective solutions for perception and reasoning, their practical deployment on assistive devices is severely constrained by excessive memory consumption and high inference costs. Moreover, existing quantization strategies often ignore inter-block error accumulation, leading to degraded model stability. To address these challenges, this study proposes a novel quantization framework -- Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization(RPIQ), whose quantization process adopts a multi-collaborative closed-loop compensation scheme based on Single Instance Calibration and Gauss-Seidel Iterative Quantization. Experiments on various types of large-scale models, including language models such as OPT, Qwen, and LLaMA, as well as vision-language models such as CogVLM2, demonstrate that RPIQ can compress models to 4-bit representation while significantly reducing peak memory consumption (approximately 60%-75% reduction compared to original full-precision models). The method maintains performance highly close to full-precision models across multiple language and visual tasks, and exhibits excellent recognition and reasoning capabilities in key applications such as text understanding and visual question answering in complex scenarios. While verifying the effectiveness of RPIQ for deployment in real assistive systems, this study also advances the computational efficiency and reliability of large models, enabling them to provide visually impaired users with the required information accurately and rapidly.

</details>


### [33] [Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control](https://arxiv.org/abs/2601.02896)
*Harshvardhan Saini,Yiming Tang,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，通过适应梯度上升到大型语言模型（LLMs），实现有针对性的提示发现。该方法旨在控制LLMs中出现的行为特征（如谄媚、幻觉），并提出了两种方法RESGA和SAEGA来优化随机初始化的提示，以更好地与已识别的角色方向对齐。此外，还引入了流畅的梯度上升来控制发现的角色引导提示的流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案在手动提示工程与自动优化方法之间存在两难选择：前者直观但不可扩展且不够精确，后者有效但作为“黑箱”运行，与模型内部没有可解释的联系。因此，需要一种既可控又可解释的方法来控制大型语言模型中的行为特征。

Method: 提出了一种将梯度上升应用于大型语言模型的新框架，开发了两种方法RESGA和SAEGA，用于优化随机初始化的提示，以便与特定角色方向更好地对齐。同时，介绍了流畅梯度上升技术来保证所发现提示的流畅性。

Result: 在Llama 3.1, Qwen 2.5, 和Gemma 3上展示了RESGA和SAEGA的有效性，对于操控三种不同的人物性格，特别是谄媚方面，自动发现的提示达到了显著改进（从49.90%提升至79.24%）。

Conclusion: 通过基于机制上有意义的特征进行提示发现，本方法为可控且可解释的行为修改提供了一个新范式。

Abstract: Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as "black boxes" with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt discovery. In specific, we propose two methods, RESGA and SAEGA, that both optimize randomly initialized prompts to achieve better aligned representation with an identified persona direction. We introduce fluent gradient ascent to control the fluency of discovered persona steering prompts. We demonstrate RESGA and SAEGA's effectiveness across Llama 3.1, Qwen 2.5, and Gemma 3 for steering three different personas,sycophancy, hallucination, and myopic reward. Crucially, on sycophancy, our automatically discovered prompts achieve significant improvement (49.90% compared with 79.24%). By grounding prompt discovery in mechanistically meaningful features, our method offers a new paradigm for controllable and interpretable behavior modification.

</details>


### [34] [Multi-Distribution Robust Conformal Prediction](https://arxiv.org/abs/2601.02998)
*Yuqi Yang,Ying Jin*

Main category: cs.LG

TL;DR: 本文提出了一种max-p聚合方案，用于在多个异构分布上构建一致性预测集，确保无论测试数据来自哪个分布，预测集的覆盖率都能超过预设水平。该方法不仅提供了有限样本、多分布覆盖，还通过优化程序提高了效率，并减少了预测集的大小。


<details>
  <summary>Details</summary>
Motivation: 研究如何构建一个在多个不同分布上一致有效的预测集，以应对公平性和分布鲁棒性问题中的挑战，即测试数据可能来自任意单个或混合源分布的情况。

Method: 提出了一个max-p聚合方案，给定每个分布相关的任何一致性分数，可以提供有限样本、多分布覆盖。接着研究了几个在均匀覆盖约束下的效率优化程序，证明了所提出的聚合方案的最优性和紧致性，并提出了一般算法来学习聚合后能导致有效预测集的一致性分数。

Result: 实验表明，该方法能够跨多个分布提供有效的最坏情况覆盖率，同时与单纯应用max-p聚合到单源一致性分数相比，大大减小了集合大小，且与使用流行标准一致性分数的单源预测集大小相当。

Conclusion: 本研究为解决多分布环境下的公平性和分布鲁棒性问题提供了一个新的视角和解决方案，通过max-p聚合方案有效提升了预测集在保证覆盖率的同时减少其大小的能力。

Abstract: In many fairness and distribution robustness problems, one has access to labeled data from multiple source distributions yet the test data may come from an arbitrary member or a mixture of them. We study the problem of constructing a conformal prediction set that is uniformly valid across multiple, heterogeneous distributions, in the sense that no matter which distribution the test point is from, the coverage of the prediction set is guaranteed to exceed a pre-specified level. We first propose a max-p aggregation scheme that delivers finite-sample, multi-distribution coverage given any conformity scores associated with each distribution. Upon studying several efficiency optimization programs subject to uniform coverage, we prove the optimality and tightness of our aggregation scheme, and propose a general algorithm to learn conformity scores that lead to efficient prediction sets after the aggregation under standard conditions. We discuss how our framework relates to group-wise distributionally robust optimization, sub-population shift, fairness, and multi-source learning. In synthetic and real-data experiments, our method delivers valid worst-case coverage across multiple distributions while greatly reducing the set size compared with naively applying max-p aggregation to single-source conformity scores, and can be comparable in size to single-source prediction sets with popular, standard conformity scores.

</details>


### [35] [In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior](https://arxiv.org/abs/2601.03015)
*Anaïs Berkes,Vincent Taboga,Donna Vakalis,David Rolnick,Yoshua Bengio*

Main category: cs.LG

TL;DR: 提出了一种名为SPICE的贝叶斯ICRL方法，通过深度集成学习Q值的先验，并在测试时利用上下文信息进行贝叶斯更新。该方法即使仅在次优轨迹上预训练也能实现最优行为，减少了遗憾并快速适应未见过的任务。


<details>
  <summary>Details</summary>
Motivation: 当前的ICRL方法要么无法超越训练分布进行改进，要么需要接近最优的数据，这限制了其实用性。因此，研究旨在开发一种能够从次优数据中恢复、并在测试时利用上下文信息来探索和适应的新方法。

Method: 提出了SPICE，一种使用深度集成学习Q值先验并通过贝叶斯更新在测试时利用这些先验的方法。此外，在线推理遵循鼓励探索与适应的上置信界规则。

Result: 理论证明SPICE在随机多臂赌博机问题及有限时间MDP中均能达到遗憾最优的行为表现；实验验证表明，相比于先前的ICRL和元强化学习方法，SPICE能在未知任务上做出接近最优决策，显著减少遗憾，并且对分布变化保持鲁棒性。

Conclusion: SPICE提供了一种有效的解决方案来克服现有ICRL方法的局限性，它能够在没有参数更新的情况下快速适应新环境，同时减少了遗憾并提高了对于未见任务的适应速度。

Abstract: In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. We prove that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.

</details>


### [36] [Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning](https://arxiv.org/abs/2601.03032)
*Vidhi Rathore*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架——因果流形公平性（CMF），该框架结合了因果推断和几何深度学习，通过在潜在表示中保持局部黎曼几何不变来解决机器学习中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习公平性方法通常将数据视为高维空间中的静态点，忽略了生成这些数据的基础结构。作者认为敏感属性不仅会改变数据分布，还会因果地扭曲数据流形本身的几何形状。

Method: CMF通过约束解码器的雅可比矩阵和海森矩阵，确保潜在空间中的规则（距离和形状）在不同人口群体之间得以保持。这种方法旨在学习一个潜在表示，在这个表示中，由度量张量和曲率定义的局部黎曼几何在对敏感属性进行反事实干预时保持不变。

Result: 通过合成结构因果模型（SCMs）验证了CMF的有效性，表明它能够有效地解开敏感的几何扭曲，同时保持任务效用，并通过几何度量提供了公平性-效用权衡的严格量化。

Conclusion: CMF为处理机器学习中的公平性问题提供了一种新颖的方法，能够在保证任务性能的同时减少因敏感属性导致的数据几何变形带来的不公平影响。

Abstract: Fairness in machine learning is increasingly critical, yet standard approaches often treat data as static points in a high-dimensional space, ignoring the underlying generative structure. We posit that sensitive attributes (e.g., race, gender) do not merely shift data distributions but causally warp the geometry of the data manifold itself. To address this, we introduce Causal Manifold Fairness (CMF), a novel framework that bridges causal inference and geometric deep learning. CMF learns a latent representation where the local Riemannian geometry, defined by the metric tensor and curvature, remains invariant under counterfactual interventions on sensitive attributes. By enforcing constraints on the Jacobian and Hessian of the decoder, CMF ensures that the rules of the latent space (distances and shapes) are preserved across demographic groups. We validate CMF on synthetic Structural Causal Models (SCMs), demonstrating that it effectively disentangles sensitive geometric warping while preserving task utility, offering a rigorous quantification of the fairness-utility trade-off via geometric metrics.

</details>


### [37] [When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability](https://arxiv.org/abs/2601.03047)
*Raphael Ronge,Markus Maier,Frederick Eberhardt*

Main category: cs.LG

TL;DR: 该论文通过复制Anthropic使用稀疏自动编码器(SAE)对Llama 3.1进行的研究，初步测试了其关于机制可解释性的主张。虽然成功再现了一些基本特性和控制能力，但研究发现这些方法在泛化性方面存在显著的脆弱性，并且难以区分主题相似的特征。这表明，当前的方法在安全关键应用中往往缺乏系统可靠性，需要从强调内部表征的可解释性转向可靠地预测和控制模型输出。


<details>
  <summary>Details</summary>
Motivation: 动机在于验证Anthropic提出的通过稀疏自动编码器从大型语言模型的神经激活模式中提取人类可理解特征的方法的有效性和广泛适用性，特别是考虑到这种方法可能为人工智能安全领域的人类监督提供一条重要途径。

Method: 方法包括利用开源的稀疏自动编码器(SAEs)来复制Anthropic的主要研究成果于Llama 3.1上，同时考察了特性引导过程中对于层选择、引导幅度以及上下文条件下的敏感度。

Result: 结果表明，尽管能够复现基础的功能提取与操控能力，但这些方法显示出相当大的脆弱性；具体表现为对不同层面的选择、操控程度及情境高度敏感。此外，还观察到了非标准的激活行为，并证明了区分主题相近特征之间的难度。

Conclusion: 结论是基于SAE的可解释性技术虽然在某些选定案例中表现出色，但现有方法通常未能达到安全关键应用所需的一致可靠性。这提示我们有必要将注意力从优先考虑内部表示形式的可解释性转移到模型输出的可靠预测与控制上。

Abstract: Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature extraction and steering capabilities, our investigation suggests that major caution is warranted regarding the generalizability of these claims. We find that feature steering exhibits substantial fragility, with sensitivity to layer selection, steering magnitude, and context. We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another. While SAE-based interpretability produces compelling demonstrations in selected cases, current methods often fall short of the systematic reliability required for safety-critical applications. This suggests a necessary shift in focus from prioritizing interpretability of internal representations toward reliable prediction and control of model output. Our work contributes to a more nuanced understanding of what mechanistic interpretability has achieved and highlights fundamental challenges for AI safety that remain unresolved.

</details>


### [38] [Joint Encoding of KV-Cache Blocks for Scalable LLM Serving](https://arxiv.org/abs/2601.03067)
*Joseph Kampeas,Emir Haleva*

Main category: cs.LG

TL;DR: 提出了一种KV缓存块联合编码的方法，通过融合请求和输入块中的相似块来共享表示，同时保持标准缓存结构。这种方法显著缓解了内存瓶颈问题，支持高并发服务而无需特殊硬件，并且在不同LLM模型和基准测试中实现了高达4.38倍的KV缓存压缩率，仅有微不足道的准确性损失。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）由于关键-值（KV）缓存的增长导致内存使用量大增，限制了在并发负载下的实时吞吐量。现有KV缓存压缩方法存在一些缺陷，如依赖于固定启发式、破坏张量布局或需要专门计算等，阻碍了可扩展性和部署。

Method: 提出了一种新的KV缓存块联合编码技术，该技术可以将跨请求和输入块中的相似块融合成共享表示，同时保持传统的缓存结构不变。此外，还从理论上分析了在一个泊松过程模型下融合缓存块的速率失真权衡。

Result: 实验结果显示，在多种LLM模型及基准测试中，所提方法能够实现最高达4.38倍的KV缓存压缩比，且对精度影响极小。相比最近的结构化与自适应压缩基线方案表现更优。实际应用中，单机vLLM基准测试显示联合编码提高了约40%的token处理速度。

Conclusion: 本研究提出的KV缓存联合编码方法有效解决了现有KV缓存增长带来的内存瓶颈问题，为实现无须特制硬件支持下的高并发服务提供了新途径。

Abstract: Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment.
  We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\sim$40\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion  kv_joint_encoding.

</details>


### [39] [ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning](https://arxiv.org/abs/2601.03093)
*Tuc Nguyen,Thai Le*

Main category: cs.LG

TL;DR: 提出了一种名为ATLAS的新方法，该方法通过在推理时利用轻量级的潜在验证器动态控制引导决策，从而有效提高大型语言模型（LLMs）在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的激活和潜在引导方法依赖于固定的策略和静态干预强度，这限制了它们处理不同问题实例时的鲁棒性，并可能导致过度或不足引导。为了克服这些问题，提出了ATLAS框架，它能够根据具体任务动态调整引导决策。

Method: ATLAS是一个特定于任务的框架，它使用外部轻量级潜在验证器，在给定中间隐藏状态的情况下预测当前推理的质量，并自适应地决定是否以及如何强烈地应用引导。这种方法允许针对每个示例和步骤进行调整，同时保持最小的开销。

Result: 在多个数学推理基准测试中，ATLAS不仅比纯解码和固定引导基线提高了准确性，还显著减少了测试时的令牌使用量。

Conclusion: 研究结果表明，由验证器指导的潜在适应为控制推理效率提供了一种有效且可扩展的方法，同时不牺牲解决方案的质量。

Abstract: Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task- specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available.

</details>


### [40] [From Muscle to Text with MyoText: sEMG to Text via Finger Classification and Transformer-Based Decoding](https://arxiv.org/abs/2601.03098)
*Meghna Roy Chowdhury,Shreyas Sen,Yi Ding*

Main category: cs.LG

TL;DR: MyoText, a hierarchical framework, decodes sEMG signals into text through finger activation classification, ergonomic typing priors, and sentence reconstruction using a T5 transformer. It shows superior performance in accuracy and error rates on the emg2qwerty dataset, suggesting its potential for keyboard-free typing in wearable and mixed-reality systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to develop a more efficient and accurate method for translating surface electromyography (sEMG) signals into text, which can be used as a neural interface for keyboard-free text input in wearable and mixed-reality systems. The authors aim to build upon previous work by creating a physiologically grounded, hierarchical approach that better mimics the natural process of typing and reduces the complexity of decoding muscle activity into language.

Method: The method introduced in this paper, named MyoText, consists of three main steps: 1) Classifying finger activations from multichannel sEMG data using a CNN-BiLSTM-Attention model, 2) Inferring letters based on ergonomic typing priors, and 3) Reconstructing full sentences with a fine-tuned T5 transformer. This modular design aims to link muscle intent directly to language output, while also narrowing down the possible solutions during the decoding process, thereby improving overall accuracy.

Result: Evaluations conducted on 30 users from the emg2qwerty dataset showed that MyoText achieved an 85.4% finger-classification accuracy, a 5.4% character error rate (CER), and a 6.5% word error rate (WER). These results indicate significant improvements over baseline methods, demonstrating the effectiveness of the proposed hierarchical framework in accurately translating sEMG signals into text.

Conclusion: In conclusion, MyoText represents a significant advancement in the field of sEMG-to-text translation, offering not only improved accuracy but also a structured, physiological approach to decoding muscle activity. Its success points towards the feasibility of implementing virtual and augmented-reality typing interfaces without the need for physical keyboards, paving the way for more intuitive and seamless neural input methods in future computing environments.

Abstract: Surface electromyography (sEMG) provides a direct neural interface for decoding muscle activity and offers a promising foundation for keyboard-free text input in wearable and mixed-reality systems. Previous sEMG-to-text studies mainly focused on recognizing letters directly from sEMG signals, forming an important first step toward translating muscle activity into text. Building on this foundation, we present MyoText, a hierarchical framework that decodes sEMG signals to text through physiologically grounded intermediate stages. MyoText first classifies finger activations from multichannel sEMG using a CNN-BiLSTM-Attention model, applies ergonomic typing priors to infer letters, and reconstructs full sentences with a fine-tuned T5 transformer. This modular design mirrors the natural hierarchy of typing, linking muscle intent to language output and reducing the search space for decoding. Evaluated on 30 users from the emg2qwerty dataset, MyoText outperforms baselines by achieving 85.4% finger-classification accuracy, 5.4% character error rate (CER), and 6.5% word error rate (WER). Beyond accuracy gains, this methodology establishes a principled pathway from neuromuscular signals to text, providing a blueprint for virtual and augmented-reality typing interfaces that operate entirely without physical keyboards. By integrating ergonomic structure with transformer-based linguistic reasoning, MyoText advances the feasibility of seamless, wearable neural input for future ubiquitous computing environments.

</details>


### [41] [Time-Aware Synthetic Control](https://arxiv.org/abs/2601.03099)
*Saeyoung Rho,Cyrus Illick,Samhitha Narasipura,Alberto Abadie,Daniel Hsu,Vishal Misra*

Main category: cs.LG

TL;DR: 提出了时间感知合成控制(TASC)方法，该方法通过结合状态空间模型和低秩信号结构来处理具有强趋势的时间序列面板数据。TASC利用卡尔曼滤波器和Rauch-Tung-Striebel平滑器进行生成模型拟合及反事实推断，并在模拟和真实世界数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有的合成控制(SC)方法通常不考虑干预前时间指标的顺序，这可能导致它们未能充分利用存在强烈趋势时的时间结构。

Method: TASC采用了一种状态空间模型，保持信号的低秩结构的同时引入了一个常数趋势。它首先使用期望最大化算法拟合一个生成时间序列模型，然后执行反事实推理。

Result: TASC在具有强烈时间趋势和高观测噪声水平的情况下提供了优势。这些结论基于对模拟数据集以及来自政策评估和体育预测等领域的实际数据集的评价得出。

Conclusion: 对于包含显著时间趋势的数据集而言，TASC提供了一种改进的方法来进行因果推断。

Abstract: The synthetic control (SC) framework is widely used for observational causal inference with time-series panel data. SC has been successful in diverse applications, but existing methods typically treat the ordering of pre-intervention time indices interchangeable. This invariance means they may not fully take advantage of temporal structure when strong trends are present. We propose Time-Aware Synthetic Control (TASC), which employs a state-space model with a constant trend while preserving a low-rank structure of the signal. TASC uses the Kalman filter and Rauch-Tung-Striebel smoother: it first fits a generative time-series model with expectation-maximization and then performs counterfactual inference. We evaluate TASC on both simulated and real-world datasets, including policy evaluation and sports prediction. Our results suggest that TASC offers advantages in settings with strong temporal trends and high levels of observation noise.

</details>


### [42] [One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling](https://arxiv.org/abs/2601.03111)
*Yiyuan Li,Zhen Huang,Yanan Wu,Weixun Wang,Xuefeng Li,Yijia Luo,Wenbo Su,Bo Zheng,Pengfei Liu*

Main category: cs.LG

TL;DR: 本文介绍了一种名为通才学习的框架，通过精心设计单个训练样本以激发跨学科影响。研究表明，一个策略性选择的数学推理样本可以显著提高多个领域的性能，包括物理、化学和生物学。提出的方法在各种推理基准测试中优于使用更大数据集进行训练的表现，表明样本的质量与设计比数量更重要。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于挑战现有的关于大型语言模型（LLMs）中强化学习（RL）所需数据量的基本假设，展示一次性学习的有效性。

Method: 提出了一个称为通才学习的新框架，用于设计能够引起多学科影响的单一训练样本，并通过实验证明了这种精心设计的综合样本在不同领域中的表现优于自然出现的个别样本。

Result: 发现一个经过策略性挑选的数学推理样本可以在多个领域带来显著的性能提升；确定了对于推理至关重要的数学技能特征；并且展示了人工合成的包含多学科元素的样本优于单独使用各个领域自然样本的训练效果。

Conclusion: 本研究结果表明，精准设计训练样本（即样本工程），而不是简单地增加数据量，可能是解锁语言模型增强推理能力的关键。

Abstract: The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.

</details>


### [43] [PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback](https://arxiv.org/abs/2601.03149)
*Dehao Yuan,Tyler Farnan,Stefan Tesliuc,Doron L Bergman,Yulun Wu,Xiaoyu Liu,Minghui Liu,James Montgomery,Nam H Nguyen,C. Bayan Bruss,Furong Huang*

Main category: cs.LG

TL;DR: 介绍了PersonaLedger，一种结合了大型语言模型和可配置程序引擎的生成引擎，用于创建多样化且逻辑正确的金融交易数据。该工具通过闭环交互方式生成符合财务规则的数据，并提供了一个包含3000万笔交易的公开数据集及基准测试套件，旨在加速金融AI领域的创新并支持严格的评估。


<details>
  <summary>Details</summary>
Motivation: 由于严格的隐私法规限制了对真实交易数据的访问，阻碍了金融AI领域的开放研究。尽管合成数据可以解决这一问题，但现有的生成器要么缺乏行为多样性，要么不能保证逻辑上的准确性。基于规则的模拟器依赖于手工设计的工作流程和浅层随机性，无法完全反映人类行为的丰富性；而基于学习的生成器如GANs虽然能捕捉到相关性，却常常违反硬性的财务约束，并且仍然需要在私人数据上训练。

Method: 提出了PersonaLedger，这是一种新的生成引擎，它利用大型语言模型（LLM）根据丰富的用户角色来产生多样化的交易流，并与一个专家可配置的程序化引擎相结合以保持正确性。LLM与引擎之间形成了一个闭环互动：每次事件后，引擎都会更新用户状态、执行财务规则，并返回一个上下文感知的“下一个提示”，引导LLM走向可行的下一步行动。

Result: 使用此引擎，研究人员创建了一个由23,000名用户的3000万笔交易组成的公共数据集以及一套包含两个任务（流动性分类与身份盗窃分割）的基准测试。

Conclusion: PersonaLedger为社区提供了一种既现实又保护隐私的资源——包括代码、规则和生成日志——以促进金融AI领域的创新，并使严格且可重复的评估成为可能。

Abstract: Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware "nextprompt" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.

</details>


### [44] [Prompt-Counterfactual Explanations for Generative AI System Behavior](https://arxiv.org/abs/2601.03156)
*Sofie Goethals,Foster Provost,João Sedoc*

Main category: cs.LG

TL;DR: 本文探讨了输入提示如何影响基于大语言模型的生成式AI系统输出特定特征（如毒性、负面情绪或政治偏见）的问题。为此，提出了一种适应于非确定性生成式AI系统的反事实解释框架，并引入了生成提示-反事实解释（PCEs）的算法。通过三个案例研究展示了该方法在抑制不希望出现的输出特性以及增强红队测试中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统被越来越多地集成到实际应用中，决策者需要理解是什么导致这些系统表现出特定的输出特征。本文旨在解决这一问题，特别是探索输入提示对输出的影响。

Method: 本文采用了来自可解释AI领域的反事实解释技术，并针对生成式AI系统的特点进行了调整，提出了一个适用于非确定性生成式AI系统的灵活框架。基于此框架，作者开发了一种用于生成提示-反事实解释(PCEs)的算法。

Result: 通过三个案例研究证明了所提方法的有效性，包括分析政治倾向、毒性及情感等不同输出特征。结果表明PCEs不仅有助于优化提示工程以减少不良输出，还能加强红队测试来发现更多可能引发不良输出的提示。

Conclusion: 这项工作为生成式AI中基于提示的可解释性奠定了基础，这对于提高模型透明度和责任性至关重要，尤其是在这些模型承担更高风险任务时。

Abstract: As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -the prompt- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.

</details>


### [45] [Rapid Augmentations for Time Series (RATS): A High-Performance Library for Time Series Augmentation](https://arxiv.org/abs/2601.03159)
*Wadie Skaf,Felix Kern,Aryamaan Basu Roy,Tejas Pradhan,Roman Kalkreuth,Holger Hoos*

Main category: cs.LG

TL;DR: 本文介绍了一种名为RATS的高性能时间序列增强库，它使用Rust编写并提供了Python接口（RATSpy）。与常用的tsaug库相比，RATSpy在143个数据集上的平均加速达到74.5%，且峰值内存使用量减少高达47.9%。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列增强库主要用Python编写，在处理大规模数据时存在性能瓶颈问题，限制了它们在大型生产系统中的应用。而时间序列增强对于训练鲁棒的深度学习模型尤其重要，特别是在标签数据稀缺且获取成本高的领域。

Method: 开发了RATS，一个使用Rust语言编写的高性能时间序列增强库，并提供Python绑定（RATSpy）。RATS实现了多种增强方法，包括基础变换、频域操作和时间扭曲技术，通过内置并行化的统一管道接口访问。

Result: 通过对143个数据集进行综合基准测试，发现相比于常用库tsaug，RATSpy能够实现平均74.5%的速度提升（在大型数据集上可达94.8%），同时最高可减少47.9%的峰值内存使用。

Conclusion: RATS为时间序列增强提供了一个高效解决方案，解决了现有库在处理大规模数据集时遇到的性能问题，适合应用于需要快速处理大量时间序列数据的场景。

Abstract: Time series augmentation is critical for training robust deep learning models, particularly in domains where labelled data is scarce and expensive to obtain. However, existing augmentation libraries for time series, mainly written in Python, suffer from performance bottlenecks, where running time grows exponentially as dataset sizes increase -- an aspect limiting their applicability in large-scale, production-grade systems. We introduce RATS (Rapid Augmentations for Time Series), a high-performance library for time series augmentation written in Rust with Python bindings (RATSpy). RATS implements multiple augmentation methods spanning basic transformations, frequency-domain operations and time warping techniques, all accessible through a unified pipeline interface with built-in parallelisation. Comprehensive benchmarking of RATSpy versus a commonly used library (tasug) on 143 datasets demonstrates that RATSpy achieves an average speedup of 74.5\% over tsaug (up to 94.8\% on large datasets), with up to 47.9\% less peak memory usage.

</details>


### [46] [Dynamic Hyperparameter Importance for Efficient Multi-Objective Optimization](https://arxiv.org/abs/2601.03166)
*Daphne Theodorakopoulos,Marcel Wever,Marius Lindauer*

Main category: cs.LG

TL;DR: 提出了一种新的动态优化方法，该方法在搜索过程中根据目标之间的权衡优先考虑最具影响力的超参数，从而加速经验收敛并找到更优解。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标优化（MOO）方法通常将所有超参数视为同等重要，忽略了超参数的重要性可能依据不同目标间的权衡而显著变化。

Method: 基于先前关于MOO后分析中超参数重要性（HPI）的研究工作，现在通过HyperSHAP计算的HPI集成到了优化过程中。利用ParEGO算法自然产生的目标权重，并通过固定不重要的超参数来调整配置空间，使搜索能够集中在重要的超参数上。

Result: 通过PyMOO和YAHPO-Gym中的多样化任务验证了所提方法的有效性。实证结果表明，在收敛速度和Pareto前沿质量方面相比基线有所提高。

Conclusion: 提出的动态优化方法能够有效改善多目标优化问题中模型选择过程的表现，特别是在处理相互竞争的目标时。

Abstract: Choosing a suitable ML model is a complex task that can depend on several objectives, e.g., accuracy, model size, fairness, inference time, or energy consumption. In practice, this requires trading off multiple, often competing, objectives through multi-objective optimization (MOO). However, existing MOO methods typically treat all hyperparameters as equally important, overlooking that hyperparameter importance (HPI) can vary significantly depending on the trade-off between objectives. We propose a novel dynamic optimization approach that prioritizes the most influential hyperparameters based on varying objective trade-offs during the search process, which accelerates empirical convergence and leads to better solutions. Building on prior work on HPI for MOO post-analysis, we now integrate HPI, calculated with HyperSHAP, into the optimization. For this, we leverage the objective weightings naturally produced by the MOO algorithm ParEGO and adapt the configuration space by fixing the unimportant hyperparameters, allowing the search to focus on the important ones. Eventually, we validate our method with diverse tasks from PyMOO and YAHPO-Gym. Empirical results demonstrate improvements in convergence speed and Pareto front quality compared to baselines.

</details>


### [47] [Predicting Time Pressure of Powered Two-Wheeler Riders for Proactive Safety Interventions](https://arxiv.org/abs/2601.03173)
*Sumit S. Shevtekar,Chandresh K. Maurya,Gourab Sil,Subasish Das*

Main category: cs.LG

TL;DR: 研究了时间压力对摩托车骑行者风险行为的影响，并提出了一种名为MotoTimePressure的深度学习模型，用于预测时间压力状态。该模型在碰撞风险预测方面表现出色，有助于提高两轮车行驶安全。


<details>
  <summary>Details</summary>
Motivation: 时间压力显著影响摩托车骑行者的危险操作和事故倾向，但其预测在智能交通系统中尚未得到充分探索。

Method: 构建了一个包含153次骑行、51名参与者在无、低和高时间压力条件下的超过129,000条标记多变量时间序列数据集。基于此数据集提出了MotoTimePressure模型，该模型结合了卷积预处理、双阶段时间注意力机制及Squeeze-and-Excitation特征重校准技术。

Result: MotoTimePressure模型达到了91.53%的准确率和98.93%的ROC AUC值，优于八个基准模型。此外，利用MTPS预测的时间压力作为特征，提高了基于Informer的碰撞风险预测准确性，从91.25%提升至93.51%。

Conclusion: 通过阈值化的时间压力状态可以捕捉到骑手的认知压力情况，从而实现包括自适应警报、触觉反馈、V2I信号传输和速度引导在内的主动ITS干预措施，支持更安全的两轮车出行。

Abstract: Time pressure critically influences risky maneuvers and crash proneness among powered two-wheeler riders, yet its prediction remains underexplored in intelligent transportation systems. We present a large-scale dataset of 129,000+ labeled multivariate time-series sequences from 153 rides by 51 participants under No, Low, and High Time Pressure conditions. Each sequence captures 63 features spanning vehicle kinematics, control inputs, behavioral violations, and environmental context. Our empirical analysis shows High Time Pressure induces 48% higher speeds, 36.4% greater speed variability, 58% more risky turns at intersections, 36% more sudden braking, and 50% higher rear brake forces versus No Time Pressure. To benchmark this dataset, we propose MotoTimePressure, a deep learning model combining convolutional preprocessing, dual-stage temporal attention, and Squeeze-and-Excitation feature recalibration, achieving 91.53% accuracy and 98.93% ROC AUC, outperforming eight baselines. Since time pressure cannot be directly measured in real time, we demonstrate its utility in collision prediction and threshold determination. Using MTPS-predicted time pressure as features, improves Informer-based collision risk accuracy from 91.25% to 93.51%, approaching oracle performance (93.72%). Thresholded time pressure states capture rider cognitive stress and enable proactive ITS interventions, including adaptive alerts, haptic feedback, V2I signaling, and speed guidance, supporting safer two-wheeler mobility under the Safe System Approach.

</details>


### [48] [Decentralized Autoregressive Generation](https://arxiv.org/abs/2601.03184)
*Stepan Maschan,Haoxuan Qu,Jun Liu*

Main category: cs.LG

TL;DR: 本文提出了一个关于自回归生成去中心化的理论分析，并定义了去中心化离散流匹配目标。通过实验表明，在多模式语言模型中，去中心化和集中化训练设置是等效的。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索自回归生成过程中的去中心化方法的有效性及其与传统集中式方法相比的表现。

Method: 作者定义了去中心化离散流匹配目标，将概率生成速度表达为专家流的线性组合，并通过实验比较了LLaVA和InternVL 2.5-1B两种不同的范例在多种基准测试下的表现。

Result: 研究表明，在多模式语言模型上，去中心化训练方式可以达到与集中式训练相似的效果。

Conclusion: 此研究提供了一种新的视角来理解自回归生成任务中的去中心化处理，为未来相关领域的发展提供了有价值的参考。

Abstract: We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrat- ing the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and per- forms full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.

</details>


### [49] [Sparse Knowledge Distillation: A Mathematical Framework for Probability-Domain Temperature Scaling and Multi-Stage Compression](https://arxiv.org/abs/2601.03195)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文提出了一个基于概率域软化算子的稀疏知识蒸馏统一理论框架，该框架包含四个核心组成部分：与算子无关的偏差-方差分解、多阶段剪枝的同伦路径形式化、n阶段蒸馏的收敛性保证以及等价类特征描述。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为稀疏知识蒸馏提供一个更全面且通用的理论基础，通过构建一个基于概率域软化算子的操作级分析框架来实现这一点。这不仅有助于理解在何种情况下稀疏模型能够优于密集型教师模型，还解释了迭代压缩成功的原因，并提供了对不同概率域算子下学生模型一致性的洞察。

Method: 研究者们首先定义了一套基于排名保持、连续性、熵单调性、恒等性和边界行为的概率域软化算子公理系统。接着，他们基于这些原则开发了一个四部分组成的理论框架，其中包括操作无关的偏差-方差分解、函数空间中多阶段修剪的同伦路径正式化、n阶段蒸馏过程中的收敛性保障以及识别产生相同学生模型的不同概率域操作符的等效类别。

Result: 研究表明，存在多个满足所提公理系统的非等效算子族。所有学习理论保证对于这个算子类都是统一有效的，独立于具体实现细节。此外，该框架为黑盒教师蒸馏、包括top-k截断和纯文本输出在内的部分访问设置以及保护隐私的模型压缩提供了坚实的理论依据。

Conclusion: 本研究为理解和应用稀疏知识蒸馏奠定了坚实的理论基础，特别是通过引入新的概率域软化算子概念及其相关属性，使得我们能够更好地掌握稀疏学生模型如何在多种场景下优于或等同于其教师模型。

Abstract: We develop a unified theoretical framework for sparse knowledge distillation based on probability-domain softening operators. While the equivalence $p^{1/T} \propto \mathrm{softmax}(z/T)$ is well known, our contribution is an operator-level analytical framework built on this foundation rather than the equivalence itself.
  The framework comprises four core components: (i) operator-agnostic bias--variance decompositions that characterize when sparse students outperform dense teachers, (ii) a homotopy path formalization of multi-stage pruning in function space explaining why iterative compression succeeds where one-shot pruning fails, (iii) convergence guarantees establishing $O(1/n)$ rates for $n$-stage distillation with explicit parameter dependence, and (iv) equivalence class characterizations identifying distinct probability-domain operators that yield identical student models under capacity constraints.
  We introduce an axiomatic definition of probability-domain softening operators based on ranking preservation, continuity, entropy monotonicity, identity, and boundary behavior, and show that multiple non-equivalent operator families satisfy these axioms. All learning-theoretic guarantees are shown to hold uniformly across this operator class, independent of implementation details. These results provide theoretical grounding for black-box teacher distillation, partial-access settings such as top-$k$ truncation and text-only outputs, and privacy-preserving model compression.

</details>


### [50] [Empowering Reliable Visual-Centric Instruction Following in MLLMs](https://arxiv.org/abs/2601.03198)
*Weilei He,Feng Ju,Zhiyuan Fan,Rui Min,Minhao Cheng,Yi R. Fung*

Main category: cs.LG

TL;DR: 本文提出了一种新的基准VC-IFEval及其配套的数据集，用于评估多模态大语言模型在视觉和文本指令下的跟随能力。通过微调模型，提高了模型在视觉指令跟随上的准确性，并对现有模型的优缺点提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型(IF)能力评估主要集中在文本模态中的口头指令上，忽略了视觉模态中蕴含的丰富语义信息所带来的隐性约束。为了解决这个问题，作者引入了VC-IFEval来全面评估这些模型在多模态环境下的指令跟随性能。

Method: 提出了一个名为VC-IFEval的新基准及相应数据集，该基准系统地将视觉依赖性约束纳入指令设计中，从而能够更严格且细致地评估MLLMs输出与视觉输入以及文本指令的一致程度。此外，通过对所提出的数据集进行微调，研究者们观察到了模型在遵循视觉指令方面的显著进步。

Result: 实验结果表明，在代表性的多模态大语言模型上使用VC-IFEval后，模型在视觉指令跟随准确性上有显著提升。同时，这项工作还揭示了当前模型的优势与局限性。

Conclusion: VC-IFEval作为一个专门针对多模态场景设计的基准测试工具，不仅填补了现有评估体系中的空白，而且促进了对于如何提高多模态大语言模型按照用户指定意图生成内容这一问题的理解。

Abstract: Evaluating the instruction-following (IF) capabilities of Multimodal Large Language Models (MLLMs) is essential for rigorously assessing how faithfully model outputs adhere to user-specified intentions. Nevertheless, existing benchmarks for evaluating MLLMs' instruction-following capability primarily focus on verbal instructions in the textual modality. These limitations hinder a thorough analysis of instruction-following capabilities, as they overlook the implicit constraints embedded in the semantically rich visual modality. To address this gap, we introduce VC-IFEval, a new benchmark accompanied by a systematically constructed dataset that evaluates MLLMs' instruction-following ability under multimodal settings. Our benchmark systematically incorporates vision-dependent constraints into instruction design, enabling a more rigorous and fine-grained assessment of how well MLLMs align their outputs with both visual input and textual instructions. Furthermore, by fine-tuning MLLMs on our dataset, we achieve substantial gains in visual instruction-following accuracy and adherence. Through extensive evaluation across representative MLLMs, we provide new insights into the strengths and limitations of current models.

</details>


### [51] [Counterfactual Fairness with Graph Uncertainty](https://arxiv.org/abs/2601.03203)
*Davi Valério,Chrysoula Zerva,Mariana Pinto,Ricardo Santos,André Carreiro*

Main category: cs.LG

TL;DR: 本文提出了一种新的偏见评估方法CF-GU，该方法将因果图的不确定性纳入了反事实公平性（CF）审计中，通过实验表明即使在最小领域知识约束下也能高置信度地识别出已知偏见。


<details>
  <summary>Details</summary>
Motivation: 由于现有的反事实公平性审计依赖于单一的因果图，而这种图在现实世界中很少能够被确定，因此需要一种可以考虑因果图不确定性的模型偏见评估方法。

Method: 1. 采用因果发现算法，并结合领域知识限制生成一组可能的有向无环图（DAGs）。 2. 利用归一化香农熵来量化图形不确定性。 3. 提供关于CF指标的信心界限。

Result: 合成数据上的实验证明了不同的领域知识假设如何支持或反驳CF审计的结果；而在真实世界数据集（COMPAS和Adult数据集）上的实验则显示，即使是在提供最少领域知识约束的情况下，也能够以很高的置信度识别出广为人知的偏见。

Conclusion: 提出的CF-GU方法能够在考虑到因果图不确定性的前提下有效评价机器学习模型的偏见，有助于构建更加可信和稳健的ML系统。

Abstract: Evaluating machine learning (ML) model bias is key to building trustworthy and robust ML systems. Counterfactual Fairness (CF) audits allow the measurement of bias of ML models with a causal framework, yet their conclusions rely on a single causal graph that is rarely known with certainty in real-world scenarios. We propose CF with Graph Uncertainty (CF-GU), a bias evaluation procedure that incorporates the uncertainty of specifying a causal graph into CF. CF-GU (i) bootstraps a Causal Discovery algorithm under domain knowledge constraints to produce a bag of plausible Directed Acyclic Graphs (DAGs), (ii) quantifies graph uncertainty with the normalized Shannon entropy, and (iii) provides confidence bounds on CF metrics. Experiments on synthetic data show how contrasting domain knowledge assumptions support or refute audits of CF, while experiments on real-world data (COMPAS and Adult datasets) pinpoint well-known biases with high confidence, even when supplied with minimal domain knowledge constraints.

</details>


### [52] [From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence](https://arxiv.org/abs/2601.03220)
*Marc Finzi,Shikai Qiu,Yiding Jiang,Pavel Izmailov,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文探讨了数据中可学习内容的问题，指出了信息论中的三个悖论，并引入了一个新的概念——epiplexity，用于量化计算受限观察者能从数据中学到的信息。通过这个概念，文章展示了如何通过计算创造信息、信息如何依赖于数据的顺序以及似然建模如何产生比数据生成过程更复杂的程序。此外，还提出了估计epiplexity的实际方法，并讨论了其在数据选择方面的应用。


<details>
  <summary>Details</summary>
Motivation: 作者旨在解决现有信息度量（如香农信息和柯尔莫哥洛夫复杂性）无法充分评估对有限计算能力观察者有用的信息内容的问题。文章希望通过引入新概念来更好地理解信息是如何被创建和利用的，特别是考虑到现代实践中遇到的挑战。

Method: 本文首先定义并举例说明了信息论中的三个悖论，然后提出epiplexity作为衡量给定数据集对于有界计算资源观察者来说可学习信息量的一种方式。接着，介绍了几种实用的方法来估算不同数据源之间的epiplexity差异，并展示了这些估计值与下游性能的相关性。

Result: 研究表明，使用epiplexity可以有效地区分不同类型的数据源，它能够很好地反映数据集在下游任务上的表现，并且指出了一些可以提高分布外泛化能力的数据干预措施。

Conclusion: 通过引入epiplexity的概念，本研究为理解和量化数据价值提供了新的视角，特别是在数据选择方面，指导了如何挑选、生成或转换数据以优化学习系统的表现。

Abstract: Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.

</details>


### [53] [PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters](https://arxiv.org/abs/2601.03237)
*Javier Salazar Cavazos*

Main category: cs.LG

TL;DR: 本文提出了一种改进的深度聚类算法PET-TURTLE，它通过引入幂律先验和稀疏logits来优化不平衡数据集上的聚类表现。实验表明，PET-TURTLE提高了对不平衡数据源的准确性，减少了对少数群体的过度预测，并整体上提升了聚类效果。


<details>
  <summary>Details</summary>
Motivation: 传统的TURTLE算法在处理平衡集群时表现出色，但面对数据不平衡的情况时会导致非理想的超平面选择，从而增加聚类错误率。因此，研究者们旨在开发一种能够有效应对数据分布不均情况下的聚类方法。

Method: PET-TURTLE通过修改成本函数以适应不平衡的数据分布，并采用幂律先验假设来解决这一问题。此外，通过引入稀疏logits简化了标签搜索空间，这不仅有助于提高不平衡数据集上的性能，也增强了对于平衡数据集的准确性。

Result: 实验证明，无论是合成数据还是真实世界中的数据，PET-TURTLE都能显著改善不平衡数据来源的聚类准确性，同时减少对较小集群的过预测现象，从而达到更好的整体聚类结果。

Conclusion: PET-TURTLE作为一种新颖且有效的聚类算法，在处理不平衡数据集方面显示出了明显的优势。其通过调整成本函数及采用稀疏logits技术，为不平衡数据提供了一个更优解。

Abstract: Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [54] [Talks that Builds: Exploring Communication factors for the Success of Emerging Professional in Product Teams](https://arxiv.org/abs/2601.02421)
*Nyan Lin Zaw*

Main category: cs.SE

TL;DR: 本研究关注18-27岁的年轻新兴专业人士组成的产品团队，探讨影响其成功的新因素，如好奇心、地理位置接近度、文档记录和资源获取等，补充了现有文献中关于这些新因素如何塑造团队生产力和项目成果的空白。


<details>
  <summary>Details</summary>
Motivation: 大部分组织沟通的研究集中于具有五年以上经验且年龄超过27岁的成熟专业人士。这项研究旨在填补针对较年轻（18-27岁）新兴专业人士组成的团队在哪些因素影响下能够成功的相关研究空白。

Method: 通过案例分析或调查研究的方法，识别出对于年轻产品团队而言重要的成功因素，包括但不限于好奇心、成员间的地理距离、文档化程度以及对资源的访问能力。

Result: 发现了一些新的关键成功要素，比如好奇心、团队成员之间的物理距离、文档管理和资源可用性等，在年轻的新兴专业人员组成的团队中扮演着重要角色。同时指出，部分传统上认为重要的因素在这个群体中的影响力有所减弱。

Conclusion: 该研究表明，与更成熟的专业人士相比，年轻新兴专业人士所在的产品团队受一系列独特因素的影响较大。这些发现有助于更好地理解如何支持这类团队以提高它们的工作效率和项目成功率。

Abstract: This paper recognizes that most organizational communication study focuses on established professionals aged above 27 with more than five years of experience. In contrast, this study examines product teams with younger emerging professionals aged 18-27 and explores which factors influence their success. While some established factors still apply, others become less relevant, and new ones such as curiosity, locational proximity, documentation, access to resources were identified in the study. Overall, this study fills a gap in the literature on how these newer factors shape team productivity and project outcomes based on the success rate of the product the team developed.

</details>


### [55] [Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support](https://arxiv.org/abs/2601.02504)
*Elizaveta Artser,Daniil Karol,Anna Potriasaeva,Aleksei Rostovskii,Katsiaryna Dzialets,Ekaterina Koshchenko,Xiaotian Su,April Yi Wang,Anastasiia Birillo*

Main category: cs.SE

TL;DR: 该论文介绍了一种集成在IDE中的人工智能辅助调试工具，旨在通过提供实时支持、建议断点和上下文提示来帮助编程教育中的调试技能学习。它利用RAG与大语言模型、程序切片以及自定义启发式方法减少大语言模型调用次数并提高准确性，并通过技术分析、用户体验研究及课堂测试三个层次的评估展示了其在教学上的潜力。


<details>
  <summary>Details</summary>
Motivation: 编程教育和软件开发中调试是一项关键技能，但在计算机科学课程中往往被忽视。为了解决这个问题，本文提出了一种集成在IDE中的人工智能辅助调试助手。

Method: 该工具结合了RAG（Retrieval-Augmented Generation）与大型语言模型、程序切片技术和特定于情境的启发式规则，以减少对大型语言模型的依赖同时提高推荐准确性。

Result: 通过技术层面的分析、用户体验调研及实际课堂教学测试三方面的综合评价表明，此AI辅助调试工具在教授学生如何有效进行代码调试方面展现出巨大潜力。

Conclusion: 基于AI的调试助手能够有效地补充现有CS课程内容，在提高学生解决编程问题能力的同时也提升了他们对于调试过程的理解。

Abstract: Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.

</details>


### [56] [Green LLM Techniques in Action: How Effective Are Existing Techniques for Improving the Energy Efficiency of LLM-Based Applications in Industry?](https://arxiv.org/abs/2601.02512)
*Pelin Rabia Kuran,Rumbidzai Chitakunye,Vincenzo Stoico,Ilja Heitlager,Justus Bogner*

Main category: cs.SE

TL;DR: 研究了四种技术以减少大型语言模型在工业应用中的能源消耗，发现小型和大型模型协作通过Nvidia的Prompt Task and Complexity Classifier (NPCC)使用提示复杂度阈值是唯一能在不显著损害其他质量的情况下大幅降低能耗的技术。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速采用，其大量的能源消耗引起了关注，特别是在行业规模部署时。尽管已经提出了一些解决方法，但关于这些方法在基于LLM的行业应用中有效性的确凿证据有限。本研究旨在填补这一空白。

Method: 选取了一款荷兰IT服务公司Schuberg Philis的聊天机器人应用程序作为分析对象，并选择了四种技术：小大模型协作、提示优化、量化以及批处理。将这四种技术以八种不同组合应用于该应用程序上，并通过实验研究它们对能源消耗、准确性和响应时间的影响。

Result: 结果表明，诸如提示优化和2比特量化等几种技术能够显著降低能耗，有时甚至高达90%。然而，这些技术尤其对准确性产生了负面影响，达到了实践中不可接受的程度。唯一能够在不严重损害其他品质的情况下实现显著且强有力的能量减少的技术是通过Nvidia的Prompt Task and Complexity Classifier (NPCC)使用提示复杂度阈值的小型和大型模型协作。

Conclusion: 虽然在实践中减少基于LLM的应用程序的能源消耗并不难，但提高其能效，即在不损害其他质量的前提下减少能源使用，仍然具有挑战性。这项研究为朝着这个目标前进提供了实用见解。

Abstract: The rapid adoption of large language models (LLMs) has raised concerns about their substantial energy consumption, especially when deployed at industry scale. While several techniques have been proposed to address this, limited empirical evidence exists regarding the effectiveness of applying them to LLM-based industry applications. To fill this gap, we analyzed a chatbot application in an industrial context at Schuberg Philis, a Dutch IT services company. We then selected four techniques, namely Small and Large Model Collaboration, Prompt Optimization, Quantization, and Batching, applied them to the application in eight variations, and then conducted experiments to study their impact on energy consumption, accuracy, and response time compared to the unoptimized baseline.
  Our results show that several techniques, such as Prompt Optimization and 2-bit Quantization, managed to reduce energy use significantly, sometimes by up to 90%. However, these techniques especially impacted accuracy negatively, to a degree that is not acceptable in practice. The only technique that achieved significant and strong energy reductions without harming the other qualities substantially was Small and Large Model Collaboration via Nvidia's Prompt Task and Complexity Classifier (NPCC) with prompt complexity thresholds. This highlights that reducing the energy consumption of LLM-based applications is not difficult in practice. However, improving their energy efficiency, i.e., reducing energy use without harming other qualities, remains challenging. Our study provides practical insights to move towards this goal.

</details>


### [57] [On the Effectiveness of Proposed Techniques to Reduce Energy Consumption in RAG Systems: A Controlled Experiment](https://arxiv.org/abs/2601.02522)
*Zhinuan,Guo,Chushu Gao,Justus Bogner*

Main category: cs.SE

TL;DR: 本研究通过控制实验评估了五种旨在减少RAG系统能耗的实用技术的影响，揭示了提高相似性检索阈值、减小嵌入大小、应用向量索引和使用BM25S重排序器等方法可以显著降低能量消耗，在某些情况下可达60%。值得注意的是，找到最优检索阈值和减小嵌入大小能够大幅降低能耗和延迟且不损失准确性，是真正节能的方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习（如RAG系统）对能源需求的增长，其环境可持续性引起了广泛关注。尽管已有绿色策略被提出，但它们在RAG系统中的实际效果尚未得到充分探索。

Method: 采用由合作方软件改进组开发的一个类生产环境下的RAG系统，通过总计9种配置、超过200小时的CRAG数据集试验来评估五种节能技术对能耗、延迟及准确度的影响。

Result: 一些技术如增加相似性检索阈值、减少嵌入尺寸、实施向量索引以及运用BM25S重排工具能显著降低能量使用，最高达60%，但部分策略也导致了不可接受的准确率下降，比如对于索引策略来说降幅可达30%。特别地，优化检索阈值与缩小嵌入规模不仅大大减少了能耗和等待时间，并且没有牺牲精度。

Conclusion: 这是首次全面实证研究针对RAG系统的节能设计技术，为希望构建可持续RAG应用程序的开发者和研究人员提供了指导。

Abstract: The rising energy demands of machine learning (ML), e.g., implemented in popular variants like retrieval-augmented generation (RAG) systems, have raised significant concerns about their environmental sustainability. While previous research has proposed green tactics for ML-enabled systems, their empirical evaluation within RAG systems remains largely unexplored. This study presents a controlled experiment investigating five practical techniques aimed at reducing energy consumption in RAG systems. Using a production-like RAG system developed at our collaboration partner, the Software Improvement Group, we evaluated the impact of these techniques on energy consumption, latency, and accuracy.
  Through a total of 9 configurations spanning over 200 hours of trials using the CRAG dataset, we reveal that techniques such as increasing similarity retrieval thresholds, reducing embedding sizes, applying vector indexing, and using a BM25S reranker can significantly reduce energy usage, up to 60% in some cases. However, several techniques also led to unacceptable accuracy decreases, e.g., by up to 30% for the indexing strategies. Notably, finding an optimal retrieval threshold and reducing embedding size substantially reduced energy consumption and latency with no loss in accuracy, making these two techniques truly energy-efficient. We present the first comprehensive, empirical study on energy-efficient design techniques for RAG systems, providing guidance for developers and researchers aiming to build sustainable RAG applications.

</details>


### [58] [PerspectiveCoach: Exploring LLMs for Developer Reflection](https://arxiv.org/abs/2601.02559)
*Lauren Olson,Emitzá Guzmán,Florian Kunneman*

Main category: cs.SE

TL;DR: 本研究介绍了一种名为PerspectiveCoach的大型语言模型驱动的对话工具，旨在帮助开发者通过结构化的视角转换练习来深入反思软件设计决策对边缘化社区的影响。通过一项有18名前端开发者参与的对照研究显示，该工具能够支持伦理推理、提高用户视角的参与度，并有助于开发者增强自我意识、拓宽视野及更细致地表达伦理观点。此外，参与者对工具的易用性和相关性给予了高度评价。


<details>
  <summary>Details</summary>
Motivation: 尽管对于软件开发中的伦理挑战有了越来越多的认识，但实践者们仍然缺乏结构化的工具来帮助他们批判性地考虑边缘化用户的实际体验。

Method: 本研究采用了一种名为PerspectiveCoach的大规模语言模型（LLM）驱动的对话工具，通过一个关于在线性别骚扰的真实案例，让18名前端开发者与之互动，以检验该工具如何支持伦理推理和用户视角的参与。研究同时进行了定性和定量分析，包括文本相似性分析以及人类-人类交流作为比较。

Result: 研究发现，PerspectiveCoach促进了开发者们的自我意识提升、视野拓宽以及更细腻的伦理表达。文本相似性分析表明，参与者在多次尝试中提高了他们重述内容的真实性，捕捉到了用户关切的表面层次和语义层面。不过，人与PerspectiveCoach之间的重述基线低于人际间对话，这突显了非个人化与人际视角转换之间的上下文差异。

Conclusion: 这项工作为支持批判性、伦理自我反思的大规模语言模型驱动的终端用户视角转换提供了一个探索性的设计，并提供了实证见解（即增强适应性、突出多元性），展示了此类工具如何帮助从业者构建更加包容和社会响应的技术。

Abstract: Despite growing awareness of ethical challenges in software development, practitioners still lack structured tools that help them critically engage with the lived experiences of marginalized users. This paper presents PerspectiveCoach, a large language model (LLM)-powered conversational tool designed to guide developers through structured perspective-taking exercises and deepen critical reflection on how software design decisions affect marginalized communities. Through a controlled study with 18 front-end developers (balanced by sex), who interacted with the tool using a real case of online gender-based harassment, we examine how PerspectiveCoach supports ethical reasoning and engagement with user perspectives. Qualitative analysis revealed increased self-awareness, broadened perspectives, and more nuanced ethical articulation, while a complementary human-human study contextualized these findings. Text similarity analyses demonstrated that participants in the human-PerspectiveCoach study improved the fidelity of their restatements over multiple attempts, capturing both surface-level and semantic aspects of user concerns. However, human-PerspectiveCoach's restatements had a lower baseline than the human-human conversations, highlighting contextual differences in impersonal and interpersonal perspective-taking. Across the study, participants rated the tool highly for usability and relevance. This work contributes an exploratory design for LLM-powered end-user perspective-taking that supports critical, ethical self-reflection and offers empirical insights (i.e., enhancing adaptivity, centering plurality) into how such tools can help practitioners build more inclusive and socially responsive technologies.

</details>


### [59] [Compressed code: the hidden effects of quantization and distillation on programming tokens](https://arxiv.org/abs/2601.02563)
*Viacheslav Siniaev,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: cs.SE

TL;DR: 本文通过分析编程语言词汇的分布和关键字覆盖模式，研究了大型语言模型（特别是压缩模型）在代码生成中的token级机制，并提出了一种新的冷启动概率分析方法。此外，还全面评估了量化、蒸馏、模型缩放及任务特定微调等不同模型优化技术对token级表示和代码生成质量的影响。实验结果提供了关于如何在各种优化约束下保持代码生成质量的经验验证指导方针。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展示出了卓越的代码生成能力，但其token级别的机制，尤其是在被压缩后的模型中，还没有得到充分探索。研究旨在理解编程语言是如何在这些模型的分词器中编码的，以及不同的模型优化技术如何影响token级别的表示和代码生成的质量。

Method: 通过对编程语言token表示进行系统性分析，包括词汇分布与关键字覆盖模式的研究；引入了一种新颖的冷启动概率分析法来洞察模型行为而无需明确提示；同时对多种模型优化技术如量化、蒸馏、模型缩放及针对具体任务的微调进行了全面评估。

Result: 实验揭示了关键性的洞见，即不同优化手段如何影响token级别行为及代码生成品质；提供了经实证验证的指导原则，以帮助在面临各类优化限制时维持良好的代码产出水平。

Conclusion: 本研究不仅加深了对LLM代码生成机制理论层面的理解，也为实际生产环境中优化模型的应用提供了实用指南。

Abstract: Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints. These findings advance both theoretical understanding of LLM code generation and practical implementation of optimized models in production environments.

</details>


### [60] [State of the Quantum Software Engineering Ecosystem](https://arxiv.org/abs/2601.02601)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 该论文研究了量子软件工程(QSE)生态系统的现状，特别是学术界和工业界的成就、活动以及成功的创业努力。采用了一种新颖的研究方法，利用先进的人工智能技术（如大型语言模型GPT-5）来识别在QSE领域活跃且取得显著成果的机构和公司。


<details>
  <summary>Details</summary>
Motivation: 探索量子软件工程领域的最新进展与成功案例，特别是那些通过同行评审出版物或在风险投资市场上筹集资金而表现出色的企业和机构。

Method: 使用基于最先进人工智能技术的方法论，具体来说是利用大型语言模型（尤其是GPT-5）来进行研究。

Result: 确定了一系列在量子软件工程领域非常活跃并取得了显著成就的机构和企业。

Conclusion: 通过应用先进的AI技术，本研究能够有效地映射出量子软件工程生态系统中关键参与者的情况，为理解该领域的发展提供了新的视角。

Abstract: We study the current state of the Quantum Software Engineering (QSE) ecosystem, focusing on the achievements, activities, and engagements from academia and industry, with a special focus on successful entrepreneurial endeavors in this arena. Our research methodology is a novel one, featuring the state-of-the-art in Artificial Intelligence (AI), namely Large Language Models (LLMs), especially Generative Pretrained Transformers (GPT). We use one of such models, namely the OpenAI GPT-5 model, through the ChatGPT tool. The goal is to identify institutions and companies that are highly active and have achieved distinguished results in QSE, evidenced by peer-reviewed publications or raised capital in the venture capital market.

</details>


### [61] [TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs](https://arxiv.org/abs/2601.02632)
*Alireza Ezaz,Ghazal Khodabandeh,Majid Babaei,Naser Ezzati-Jivan*

Main category: cs.SE

TL;DR: 本文提出了一种名为TAAF的新方法，它结合了时间索引、知识图谱(KGs)和大型语言模型(LLMs)，将原始跟踪数据转化为可操作的见解。通过构建一个基于时间索引的知识图谱来捕捉线程、CPU和系统资源等实体之间的关系，并使用LLM解释特定于查询的子图以回答自然语言问题。实验表明，TAAF在多跳和因果推理任务中提高了高达31.2%的答案准确性。


<details>
  <summary>Details</summary>
Motivation: 执行跟踪是理解、调试和优化复杂软件系统的重要信息来源。然而，来自操作系统内核或像Chrome或MySQL这样的大规模应用程序的跟踪非常庞大且难以分析。现有工具依赖预定义的分析，而定制化的洞察通常需要编写领域特定的脚本，这是一项容易出错且耗时的任务。因此，需要一种新的方法来更有效地从执行跟踪中提取有用的信息。

Method: TAAF (Trace Abstraction and Analysis Framework) 是一种新方法，它首先构建一个基于时间索引的知识图谱，该图谱能够捕捉到如线程、CPU和系统资源等实体间的关系。然后，利用大型语言模型(LLMs)去解读针对具体查询的知识子图，从而减少手动检查的需求以及对深度系统专业知识的要求。

Result: 为了评估TAAF的有效性，研究者引入了一个名为TraceQA-100的基准测试集，包含基于真实内核跟踪的100个问题。实验结果表明，在使用三种不同的大型语言模型进行跨多个时间设置测试后，TAAF在多跳和因果推理任务中的答案准确率提高了最高达31.2%。此外，还探讨了图形基础推理的优势及其局限性。

Conclusion: TAAF框架通过结合时间索引、知识图谱与大型语言模型提供了一种有效的方法来处理复杂的执行跟踪数据，显著提高了特定类型问题解答的准确性。这一成果为下一代跟踪分析工具的发展奠定了基础。

Abstract: Execution traces are a critical source of information for understanding, debugging, and optimizing complex software systems. However, traces from OS kernels or large-scale applications like Chrome or MySQL are massive and difficult to analyze. Existing tools rely on predefined analyses, and custom insights often require writing domain-specific scripts, which is an error-prone and time-consuming task. This paper introduces TAAF (Trace Abstraction and Analysis Framework), a novel approach that combines time-indexing, knowledge graphs (KGs), and large language models (LLMs) to transform raw trace data into actionable insights. TAAF constructs a time-indexed KG from trace events to capture relationships among entities such as threads, CPUs, and system resources. An LLM then interprets query-specific subgraphs to answer natural-language questions, reducing the need for manual inspection and deep system expertise. To evaluate TAAF, we introduce TraceQA-100, a benchmark of 100 questions grounded in real kernel traces. Experiments across three LLMs and multiple temporal settings show that TAAF improves answer accuracy by up to 31.2%, particularly in multi-hop and causal reasoning tasks. We further analyze where graph-grounded reasoning helps and where limitations remain, offering a foundation for next-generation trace analysis tools.

</details>


### [62] [Enterprise Identity Integration for AI-Assisted Developer Services: Architecture, Implementation, and Case Study](https://arxiv.org/abs/2601.02698)
*Manideep Reddy Chinthareddy*

Main category: cs.SE

TL;DR: 本文提出了一种将OAuth 2.0和OpenID Connect集成到支持MCP的开发环境中的实用架构，以确保AI辅助开发工具在企业使用时符合身份验证、访问控制及治理要求。


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助开发服务越来越多地嵌入现代IDE中，企业需要确保这些工具能够在现有的身份验证、访问控制和治理框架内运行。尽管模型上下文协议（MCP）允许AI助手获取结构化的内部上下文信息，但其规范仅提供了最小化的授权模型，并且缺乏关于如何整合企业单点登录(SSO)的指导。

Method: 文章介绍了一种将OAuth 2.0和OpenID Connect (OIDC)整合进支持MCP的开发者环境的实际架构。该方法描述了IDE扩展如何获得并展示令牌、MCP服务器如何通过身份提供商验证这些令牌以及如何利用范围和声明来实施最低权限访问。

Result: 通过使用Visual Studio Code、基于Python的MCP服务器和一个符合OIDC的身份提供商(IdP)进行原型实现，证明了这种方法的可行性。案例研究评估了认证延迟、令牌验证开销、操作考虑因素以及与AI相关的特定风险。

Conclusion: 所提出的方法为企业采用AI辅助开发工具提供了一种可部署模式，同时保持了身份保证和可审计性。

Abstract: AI-assisted developer services are increasingly embedded in modern IDEs, yet enterprises must ensure these tools operate within existing identity, access control, and governance requirements. The Model Context Protocol (MCP) enables AI assistants to retrieve structured internal context, but its specification provides only a minimal authorization model and lacks guidance on integrating enterprise SSO. This article presents a practical architecture that incorporates OAuth 2.0 and OpenID Connect (OIDC) into MCP-enabled developer environments. It describes how IDE extensions obtain and present tokens, how MCP servers validate them through an identity provider, and how scopes and claims can enforce least-privilege access. A prototype implementation using Visual Studio Code, a Python-based MCP server, and an OIDC-compliant IdP demonstrates feasibility. A case study evaluates authentication latency, token-validation overhead, operational considerations, and AI-specific risks. The approach provides a deployable pattern for organizations adopting AI-assisted developer tools while maintaining identity assurance and auditability.

</details>


### [63] [Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism](https://arxiv.org/abs/2601.02736)
*Lingzhe Zhang,Tong Jia,Yunpeng Zhai,Leyi Pan,Chiming Duan,Minghua He,Pei Xiao,Ying Li*

Main category: cs.SE

TL;DR: 提出了一种名为SpecRCA的推测性根因分析框架，用于微服务系统中更准确高效地进行故障定位。该框架采用先假设后验证的方法，能够在保证准确性的同时提高分析效率。


<details>
  <summary>Details</summary>
Motivation: 随着微服务架构在企业级云原生应用中的普及，其内在复杂性和动态运行时交互导致了异常情况频发。现有的基于大规模语言模型（LLM）的智能根因分析技术虽然展示了良好的潜力，但在存在探索多样性不足以及对大型模型依赖严重的问题，这限制了它们在实际场景中的应用。

Method: 设计并实现了一个名为SpecRCA的新框架，它通过一个假设起草模块快速生成候选根原因，并使用并行根原因验证器来有效验证这些假设。这种方法旨在提高根因分析过程中的准确性与效率。

Result: 初步实验表明，在AIOps 2022数据集上，SpecRCA相比现有方法能够达到更高的准确性和效率。

Conclusion: SpecRCA为复杂的微服务环境提供了一个实用性强、可扩展且易于理解的根因分析解决方案。

Abstract: Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.

</details>


### [64] [CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation](https://arxiv.org/abs/2601.02868)
*Peiding Wang,Li Zhang,Fang Liu,Chongyang Tao,Yinghao Zhu*

Main category: cs.SE

TL;DR: 该论文提出了一种基于抽象语法树（AST）指导的动态内存管理系统CodeMEM，用于仓库级别的迭代代码生成。通过Code Context Memory组件和Code Session Memory机制，CodeMEM能够持续更新仓库上下文，并通过基于AST的分析来检测和缓解遗忘问题。实验结果表明，CodeMEM在指令跟随及代码生成方面均达到了领先水平，同时减少了交互轮次，保持了竞争力的推理延迟和令牌效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在仓库级代码生成中通过互动协作显著提高开发者生产力的同时，需要不断保存并更新仓库上下文以整合新验证的信息。然而，会话历史的增长增加了认知负担，导致遗忘以及先前已解决错误的再次出现。现有的内存管理方法主要集中在自然语言表示上，不足以应对这些挑战。

Method: 提出了CodeMEM系统，它是一种针对仓库级别迭代代码生成定制的、由抽象语法树（AST）引导的动态内存管理系统。该系统包括两个核心部分：Code Context Memory，负责通过AST引导的LLM操作动态维护和更新仓库上下文；Code Session Memory，则构建了一个以代码为中心的交互历史表示，并通过基于AST的分析明确地检测和减轻遗忘现象。

Result: 实验评估显示，在遵循指令基准测试CodeIF-Bench与代码生成基准测试CoderEval上，CodeMEM实现了最先进性能，分别提高了当前回合12.2%和会话级别11.5%的指令遵循率，并将交互轮次减少了2到3轮，同时还保持了有竞争力的推理延迟和令牌效率。

Conclusion: 研究介绍了一种创新的方法——CodeMEM，旨在解决LLMs在仓库级代码生成过程中遇到的上下文管理和遗忘问题。通过引入AST导向的记忆管理机制，CodeMEM不仅有效提升了代码生成的质量，还优化了用户与系统的交互体验。

Abstract: Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.

</details>


### [65] [Few-shot learning for security bug report identification](https://arxiv.org/abs/2601.02971)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: 本文提出了一种基于少量学习的技术，使用SetFit框架来有效识别安全漏洞报告，即使在标签数据有限的情况下也能表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习技术依赖大量标记数据来分类和识别安全漏洞报告，但实际中这类数据往往稀缺，导致模型性能差且实用性受限。

Method: 研究采用了名为SetFit的最新少量学习框架，该框架结合了句子转换器、对比学习和参数高效微调。通过在一个小规模标记的数据集上训练模型，以区分安全相关与非安全相关的错误报告。

Result: 提出的这种方法达到了0.865的最佳AUC值，在所有评估的数据集上均优于传统机器学习方法（基线）。

Conclusion: 基于SetFit的少量学习为识别安全漏洞报告提供了一个有前景的选择，它能够在极少标注努力下实现高效的模型开发，非常适合于标签数据稀缺的情况。

Abstract: Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.

</details>


### [66] [A Dataset of Low-Rated Applications from the Amazon Appstore for User Feedback Analysis](https://arxiv.org/abs/2601.03009)
*Nek Dil Khan,Javed Ali Khan,Darvesh Khan,Jianqiang Li,Mumrez Khan,Shah Fahad Khan*

Main category: cs.SE

TL;DR: 本研究构建了一个新的数据集，该数据集来源于亚马逊软件应用商店中64个低评分应用程序的79,821条用户评论。此外，还手动注释了其中的6000条评论，将其分类为六个不同的问题类别。此数据集旨在帮助识别和分类影响用户体验的关键问题，支持基于机器学习的方法开发，并为软件改进提供依据。


<details>
  <summary>Details</summary>
Motivation: 虽然许多研究集中在高评分应用程序上，但低评分应用程序往往未被充分探索，尽管它们有可能揭示有价值的见解来改善软件质量。

Method: 从亚马逊软件应用商店选取了64款低评分应用，收集到了79,821条用户评论。对这些评论中的6000条进行了人工标注，分为六大类问题：用户界面（UI）与用户体验（UX）、功能与特性、兼容性与设备特定性、性能与稳定性、客户服务与响应速度以及安全性和隐私问题。

Result: 创建了一个包含大量低评分应用用户反馈的数据集，并且通过人工标注部分数据提供了详细的分类信息。该数据集有助于自动分类用户反馈至不同类型的问题，从而指导软件质量改进。

Conclusion: 通过对低评分应用程序用户反馈进行系统分析并公开相关数据集，为基于用户反馈改进软件质量奠定了基础。同时，也为软件供应商及研究人员探索软件演化活动提供了机会，比如缺失的功能点、讽刺语气及情感分析等，以更好地理解较低应用评分背后的原因。

Abstract: In todays digital landscape, end-user feedback plays a crucial role in the evolution of software applications, particularly in addressing issues that hinder user experience. While much research has focused on high-rated applications, low-rated applications often remain unexplored, despite their potential to reveal valuable insights. This study introduces a novel dataset curated from 64 low-rated applications sourced from the Amazon Software Appstore (ASA), containing 79,821 user reviews. The dataset is designed to capture the most frequent issues identified by users, which are critical for improving software quality. To further enhance the dataset utility, a subset of 6000 reviews was manually annotated to classify them into six district issue categories: user interface (UI) and user experience (UX), functionality and features, compatibility and device specificity, performance and stability, customer support and responsiveness, and security and privacy issues. This annotated dataset is a valuable resource for developing machine learning-based approaches aiming to automate the classification of user feedback into various issue types. Making both the annotated and raw datasets publicly available provides researchers and developers with a crucial tool to understand common issues in low-rated apps and inform software improvements. The comprehensive analysis and availability of this dataset lay the groundwork for data-derived solutions to improve software quality based on user feedback. Additionally, the dataset can provide opportunities for software vendors and researchers to explore various software evolution-related activities, including frequently missing features, sarcasm, and associated emotions, which will help better understand the reasons for comparatively low app ratings.

</details>


### [67] [NavAI: A Generalizable LLM Framework for Navigation Tasks in Virtual Reality Environments](https://arxiv.org/abs/2601.03251)
*Xue Qin,Matthew DiGiovanni*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的导航框架NavAI，旨在解决虚拟现实环境中自动化探索的问题。通过在三个不同的VR环境中进行目标导向和探索性任务测试，证明了NavAI能够实现高精度导航，但在需要动态目标评估的情况下也暴露了一些限制。


<details>
  <summary>Details</summary>
Motivation: 现有的导航技术主要集中在360度图像数据集和3D模拟器中的路径优化，无法直接应用于沉浸式虚拟现实环境。为了解决这一差距，提出了一个通用的大语言模型（LLM）基础的导航框架NavAI，支持多样化的VR应用中基本动作与复杂的目标导向任务。

Method: 开发了一个名为NavAI的导航框架，该框架基于大语言模型设计，能够在多种VR应用之间提供导航服务。研究者通过设置目标导向型和探索型任务，在三种不同的虚拟现实环境中对NavAI进行了评估。

Result: 实验结果表明，在目标导向的任务中NavAI达到了89%的成功率。然而，当面临需要实时调整目标的情况时，完全依赖于大语言模型的方法表现出一定的局限性。

Conclusion: NavAI作为一个基于大语言模型的导航解决方案，在处理虚拟现实环境下的导航问题方面展现出了良好的性能。尽管如此，对于那些要求动态目标评价的任务来说，仍然存在挑战。此外，文章还讨论了实验过程中发现的一些限制，并对未来的研究方向给出了建议。

Abstract: Navigation is one of the fundamental tasks for automated exploration in Virtual Reality (VR). Existing technologies primarily focus on path optimization in 360-degree image datasets and 3D simulators, which cannot be directly applied to immersive VR environments. To address this gap, we present NavAI, a generalizable large language model (LLM)-based navigation framework that supports both basic actions and complex goal-directed tasks across diverse VR applications. We evaluate NavAI in three distinct VR environments through goal-oriented and exploratory tasks. Results show that it achieves high accuracy, with an 89% success rate in goal-oriented tasks. Our analysis also highlights current limitations of relying entirely on LLMs, particularly in scenarios that require dynamic goal assessment. Finally, we discuss the limitations observed during the experiments and offer insights for future research directions.

</details>
