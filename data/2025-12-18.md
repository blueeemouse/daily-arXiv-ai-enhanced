<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.LG](#cs.LG) [Total: 56]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems](https://arxiv.org/abs/2512.14733)
*Qiang Chen,Venkatesh Ganapati Hegde*

Main category: cs.IR

TL;DR: 本文提出了一种通过优化内容级探索的位置来安全高效地提供探索内容的方法，该方法在大规模流媒体平台上部署后，能够在保持商业指标的同时收集无偏的交互数据，并且这些数据被证明能够显著提高用户参与度。


<details>
  <summary>Details</summary>
Motivation: 在以电视为主的远程环境中，虽然探索对于提高长期推荐质量至关重要，但它通常会降低短期业务表现。因为在这种环境下，用户被动参与、期望即时相关性并且很少有机会进行纠正。因此，需要一种既能保证探索内容的安全有效投放又能最小化对平台整体观看时间目标影响的方法。

Method: 本文介绍的方法是通过确定滚动深度区域中参与度较低的部分并战略性地引入一个名为“Something Completely Different”的行来包含随机内容，而不是在整个用户界面(UI)上均匀地强制执行探索。这种方法根据实证低成本、高覆盖的位置来调整其出现，确保对平台级别的观看时间目标的影响最小。

Result: 通过广泛的A/B测试表明，这种策略能够在保持商业指标的同时收集无偏见的交互数据。此外，所收集的数据整合到下游候选生成中后，显著提高了用户的参与度，验证了它对于推荐系统的价值。

Conclusion: 本研究提出的基于行为信息的内容探索机制，不仅为大规模表面探索内容提供了可行方案，而且与现有行列多样化和基于强盗算法的探索技术相辅相成。结果表明，这种方法能够有效地平衡探索带来的短期负面影响与长期收益，同时改善了推荐系统的性能。

Abstract: Exploration is essential to improve long-term recommendation quality, but it often degrades short-term business performance, especially in remote-first TV environments where users engage passively, expect instant relevance, and offer few chances for correction. This paper introduces an approach for delivering content-level exploration safely and efficiently by optimizing its placement based on reach and opportunity cost. Deployed on a large-scale streaming platform with over 100 million monthly active users, our approach identifies scroll-depth regions with lower engagement and strategically introduces a dedicated container, the "Something Completely Different" row containing randomized content. Rather than enforcing exploration uniformly across the user interface (UI), we condition its appearance on empirically low-cost, high-reach positions to ensure minimal tradeoff against platform-level watch time goals. Extensive A/B testing shows that this strategy preserves business metrics while collecting unbiased interaction data. Our method complements existing intra-row diversification and bandit-based exploration techniques by introducing a deployable, behaviorally informed mechanism for surfacing exploratory content at scale. Moreover, we demonstrate that the collected unbiased data, integrated into downstream candidate generation, significantly improves user engagement, validating its value for recommender systems.

</details>


### [2] [MedNuggetizer: Confidence-Based Information Nugget Extraction from Medical Documents](https://arxiv.org/abs/2512.15384)
*Gregor Donabauer,Samy Ateia,Udo Kruschwitz,Maximilian Burger,Matthias May,Christian Gilfrich,Maximilian Haas,Julio Ruben Rodas Garzaro,Christoph Eckl*

Main category: cs.IR

TL;DR: MedNuggetizer是一个基于大型语言模型的工具，可以从医学文档中提取和聚类信息片段，以帮助临床医生探索基础医学证据。通过使用泌尿科主要指南和PubMed近期研究作为信息源，该工具在'前列腺活检前的抗生素预防'这一临床用例上展示了其有效性。领域专家的评估表明，MedNuggetizer为临床医生及研究人员提供了一种高效的方法来浏览长文档，并轻松提取可靠的、以查询为中心的医学证据。


<details>
  <summary>Details</summary>
Motivation: 开发MedNuggetizer的主要动机是为了解决临床医生在探索支持决策所需的基础医学证据时遇到的困难，特别是当需要从大量冗长的医学文献中快速准确地提取相关信息时。

Method: MedNuggetizer利用大型语言模型(LLM)执行重复的信息片段提取任务，随后将这些片段进行分组，从而跨多个文档生成可靠证据。它采用了针对特定查询驱动的方式，使得用户能够聚焦于他们感兴趣的特定医学话题或问题。

Result: 通过采用有关‘前列腺活检前的抗生素预防’的真实世界案例，MedNuggetizer展示了其有效性和实用性。使用来自主要泌尿学指南和最近的PubMed研究的信息作为输入，该工具能够成功地提炼出关键点并将其组织成易于理解的形式。

Conclusion: 根据领域专家的评价，MedNuggetizer被证实是一种非常有用的工具，可以帮助医疗专业人员更有效地处理长篇幅的医学资料，并从中抽取与特定查询高度相关的可靠证据。

Abstract: We present MedNuggetizer, https://mednugget-ai.de/; access is available upon request.}, a tool for query-driven extraction and clustering of information nuggets from medical documents to support clinicians in exploring underlying medical evidence. Backed by a large language model (LLM), \textit{MedNuggetizer} performs repeated extractions of information nuggets that are then grouped to generate reliable evidence within and across multiple documents. We demonstrate its utility on the clinical use case of \textit{antibiotic prophylaxis before prostate biopsy} by using major urological guidelines and recent PubMed studies as sources of information. Evaluation by domain experts shows that \textit{MedNuggetizer} provides clinicians and researchers with an efficient way to explore long documents and easily extract reliable, query-focused medical evidence.

</details>


### [3] [BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems](https://arxiv.org/abs/2512.15526)
*Abdullah Al Munem,Sumona Yeasmin,Mohammad Rezwanul Huq*

Main category: cs.IR

TL;DR: 该实验提出了一种结合BERT和CNN的神经协同过滤推荐系统，能够处理数值、类别和图像数据。在MovieLens数据集上的测试表明，与简单的NCF模型及基于BERT的NCF模型相比，所提模型表现更优，证明了同时考虑类别和图像信息可以提高推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 为了增加用户与网站的互动从而为网站所有者带来收益，需要一个强大的推荐系统来根据用户的独特偏好推荐物品。

Method: 提出了集成BERT和CNN的神经协同过滤（NCF）模型，该模型能够接收用户和物品配置文件作为输入，并能处理数值、类别以及图像数据以提取潜在特征。使用小样本的MovieLens数据集对该模型进行了25个周期的训练和验证。

Result: 所提出的模型在召回率上达到了0.72，在799名用户上Hit Ratio @ 10达到了0.486，超过了对比中的简单NCF模型和基于BERT的NCF模型的表现。

Conclusion: 实验结论是，通过同时考虑类别和图像数据可以提升推荐系统的性能。

Abstract: Every day, a significant number of users visit the internet for different needs. The owners of a website generate profits from the user interaction with the contents or items of the website. A robust recommendation system can increase user interaction with a website by recommending items according to the user's unique preferences. BERT and CNN-integrated neural collaborative filtering (NCF) have been proposed for the recommendation system in this experiment. The proposed model takes inputs from the user and item profile and finds the user's interest. This model can handle numeric, categorical, and image data to extract the latent features from the inputs. The model is trained and validated on a small sample of the MovieLens dataset for 25 epochs. The same dataset has been used to train and validate a simple NCF and a BERT-based NCF model and compared with the proposed model. The proposed model outperformed those two baseline models. The obtained result for the proposed model is 0.72 recall and 0.486 Hit Ratio @ 10 for 799 users on the MovieLens dataset. This experiment concludes that considering both categorical and image data can improve the performance of a recommendation system.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: 本文介绍了一个名为LLMQ的端到端CUDA/C++实现，它能够使用性价比高的普通GPU来训练中等规模的语言模型（例如30亿到320亿参数）。通过一系列针对内存和通信瓶颈的优化措施，如激活检查点、卸载以及基于复制引擎的集合操作，使得在单个16GB中端游戏显卡上训练或微调70亿参数模型成为可能，或者在配备4块RTX 4090的工作站上训练320亿参数模型。这一切都是在执行标准8位训练流程且不引入额外算法近似的情况下完成的，并保持了约50%的FLOP利用率。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型变得越来越大，对于那些没有能力负担昂贵数据中心级GPU的研究者来说，在成本效益较高的普通GPU上进行有效训练变得尤为重要。然而，这些设备通常具有较低的内存可用性和较慢的通信速度。因此，开发一种能够在这些限制条件下高效训练大语言模型的方法显得十分必要。

Method: 研究者们提出了一个名为LLMQ的解决方案，该方案利用了一系列针对内存不足和通信延迟问题的优化技术，包括但不限于：
- 激活检查点：减少内存占用
- 卸载：将部分计算任务转移到其他资源上去
- 基于复制引擎的集合操作：加快数据传输效率
通过这些方法，即使是在家用级别的GPU上也能够支持大规模语言模型的训练。

Result: 实验结果表明，LLMQ能够在单张16GB显存的游戏级别GPU上成功训练70亿参数的语言模型，或者在装有4块RTX 4090显卡的工作站上处理320亿参数的大模型。整个过程中采用了标准8位训练管道，未做任何额外简化处理，同时保持了大约50%左右的浮点运算性能。

Conclusion: 本研究表明，通过精心设计与优化，即使是相对低端的GPU也可以胜任大型语言模型的训练任务。LLMQ不仅降低了高性能计算的门槛，还达到了与使用昂贵云端GPU相当甚至更优的效率水平。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [5] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 本文探索了GPU上布隆过滤器的设计空间，通过向量化、线程协作和计算延迟三个维度的优化，实现了在保持高精度的同时达到高吞吐量的效果。相比现有技术，在B200 GPU上批量查询（构建）性能分别提高了11.35倍（15.4倍）。


<details>
  <summary>Details</summary>
Motivation: 虽然针对CPU优化的布隆过滤器实现已经得到了广泛研究，但在GPU上的设计仍相对较少。鉴于GPU具有大规模线程级并行性和高带宽内存的特点，它们非常适合加速布隆过滤器变体。本文旨在填补这一空白，探索如何利用GPU特性来提高布隆过滤器处理速度同时保持准确性。

Method: 研究者们沿着向量化、线程合作及计算延迟这三个方面探索了GPU上的布隆过滤器设计空间。通过对不同参数配置下硬件响应情况的分析，他们发现这些优化点组合对吞吐量有着显著影响，尤其是在过滤器能够适应GPU缓存域的情况下效果更佳。

Result: 实验结果表明，所提出的优化设计方案能够在维持高精度配置优越性的同时，提供通常只有高误差变体才能达到的吞吐量水平。在相同的错误率条件下，新方法相较于当前最先进技术，在B200 GPU上进行批量过滤器查找（构造）时分别提升了11.35倍（15.4倍），并且在多种配置下均能实现接近理论最大值92%以上的实际性能。

Conclusion: 本研究成功地解决了速度与精度之间的传统权衡问题，通过特定于GPU的优化策略大幅提升了布隆过滤器的性能。这不仅为未来的研究提供了有价值的参考，同时也准备发布一个模块化的CUDA/C++实现版本供社区使用。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [6] [LeaseGuard: Raft Leases Done Right](https://arxiv.org/abs/2512.15659)
*A. Jesse Jiryu Davis,Murat Demirbas,Lingzhi Deng*

Main category: cs.DC

TL;DR: LeaseGuard, a new lease algorithm for Raft-based systems, improves read and write performance while maintaining consistency. It reduces the overhead of consistent reads and significantly increases write throughput, while also improving read availability during leader transitions.


<details>
  <summary>Details</summary>
Motivation: The need to ensure read consistency in distributed databases using the Raft consensus algorithm is challenged by the high communication cost or the vague and potentially harmful prior lease protocols. This leads to either inefficient implementations or lack of proper implementation in most Raft systems.

Method: LeaseGuard, a novel lease algorithm, leverages specific guarantees from Raft elections to enhance system availability. The method includes two optimizations: one for quick restoration of write throughput and another for improved read availability. LeaseGuard's design is rigorously specified using TLA+ and was evaluated through Python simulations and an implementation within LogCabin, a C++ reference of Raft.

Result: By implementing LeaseGuard, the overhead for consistent reads is reduced from one to zero network roundtrips, and write throughput is increased from around 1000 to approximately 10,000 writes per second. Additionally, LeaseGuard allows 99% of reads to succeed immediately on a new leader, as opposed to traditional leases which prevent all reads until a lease is acquired.

Conclusion: LeaseGuard offers a significant improvement over existing methods for ensuring read consistency in Raft-based distributed databases, with notable gains in both read and write performance without compromising consistency.

Abstract: Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.

</details>


### [7] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: 本文提出了一种名为Dynamic Rebatching的解决方案，用于解决早期退出（EE）大型语言模型在传统批处理框架下存在的问题。通过动态重组批次，该方法能够提高吞吐量2-12%，同时保持输出质量，并且完全消除了非自愿退出的情况。


<details>
  <summary>Details</summary>
Motivation: 传统的批处理框架不适用于具有早期退出功能的大型语言模型（LLM），因为同一批次中的请求可能不会同时准备好退出。现有解决方案要么对整个批次强制执行统一决策，忽视了早期退出的机会；要么通过迫使提前退出而降低输出质量。

Method: 提出了Dynamic Rebatching，这是一种动态重组批次的方法，在每个早期退出点处重新组织批次。满足退出条件的请求被立即处理，而继续进行的请求则被暂时存储、重新分组为新的批次并转发至更深层。此外，还引入了DREX系统来实现这一方法，它包含两个关键优化：无数据移动重批缓冲区和考虑到早期退出及服务水平协议（SLA）的调度器。DREX还能高效处理因跳过层而缺失的KV缓存问题。

Result: 评估显示，与基线方法相比，DREX提高了2-12%的吞吐量，同时保持了输出质量。重要的是，DREX彻底消除了非自愿退出的情况，确保了由EE模型意图提供的输出质量得以保存。

Conclusion: 通过实施Dynamic Rebatching及其相关优化措施，DREX证明了其在提高具备早期退出能力的大规模语言模型推理效率方面的有效性，同时保证了高质量的结果。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [8] [One Size Doesn't Fit All: Age-Aware Gamification Mechanics for Multimedia Learning Environments](https://arxiv.org/abs/2512.15630)
*Sarah Kaißer,Markus Kleffmann,Kristina Schaaff*

Main category: cs.MM

TL;DR: 本文探讨了如何根据年龄差异设计游戏化元素，以满足不同学习者在动机和认知上的需求。通过文献综述，提出了年龄组、机制与效果之间的映射，并总结了五个针对特定年龄段的游戏化设计原则及三种技术模式。研究结果表明，游戏化并非普遍有效，而是需要差异化设计来促进所有年龄段的学习参与度和包容性。


<details>
  <summary>Details</summary>
Motivation: 当前许多数字学习系统在使用游戏化时忽略了年龄相关的差异，未能充分考虑学习者多样化的动机和认知需求。

Method: 基于目标文献回顾，分析了不同年龄段对游戏化机制的响应，并据此提出了一套适用于多媒体学习环境中的年龄特异性游戏化设计方案。

Result: 研究发现，为不同年龄段设计定制化的游戏化方案能够更有效地支持学习者的参与度和包容性；同时，还提炼出了五项设计原则以及三种可用于实现的技术模式。

Conclusion: 游戏化在数字学习中的应用需要考虑到年龄因素的影响，采用差异化的设计策略才能更好地服务于各个年龄段的学习者。

Abstract: Gamification is widely used in digital learning. However, most systems neglect age-related differences. This paper investigates how gamification can be designed in an age-aware way to address learners' diverse motivational and cognitive needs. Based on a targeted literature review, we present a mapping of age groups, mechanics, and effects. Furthermore, we derive five design principles for age-specific gamification and identify three technical patterns for implementation in multimedia learning environments. The results indicate that gamification is not universally effective, but rather requires a differentiated design to support engagement and inclusivity across the lifespan.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [9] [MS-Index: Fast Top-k Subsequence Search for Multivariate Time Series under Euclidean Distance](https://arxiv.org/abs/2512.14723)
*Jens E. d'Hondt,Teun Kortekaas,Odysseas Papapetrou,Themis Palpanas*

Main category: cs.DB

TL;DR: 本文提出了一种新的算法——多变量子序列索引（MS-Index），用于在欧几里得距离下执行最近邻多变量时间序列子序列搜索，支持查询时自定义选择通道。实验表明，该方法对于原始和归一化的子序列，在性能上比现有技术高出一到两个数量级。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，不是所有多变量时间序列(MTS)的通道都与每个查询相关。比如飞机传感器可能收集大量组件和子系统的数据，但只有少数与特定查询相关，如确定故障起落架的原因或特定飞行操作。因此，相关查询通道通常是在查询时指定的。

Method: 引入了多变量子序列索引（MS-Index）算法，它是一种用于在欧几里得距离下进行精确最近邻MTS子序列搜索的新方法，并且允许在查询时临时选择查询通道。

Result: 通过34个数据集上的全面实验评估显示，MS-Index算法在处理原始和标准化子序列方面比现有最先进方法快一至两个数量级。

Conclusion: MS-Index为多变量时间序列子序列搜索提供了一个有效的方法，特别是当只需要考虑某些特定通道的数据时，其表现优于现有的解决方案。

Abstract: Modern applications frequently collect and analyze temporal data in the form of multivariate time series (MTS) -- time series that contain multiple channels. A common task in this context is subsequence search, which involves identifying all MTS that contain subsequences highly similar to a query time series. In practical scenarios, not all channels of an MTS are relevant to every query. For instance, airplane sensors may gather data on a plethora of components and subsystems, but only a few of these are relevant to a specific query, such as identifying the cause of a malfunctioning landing gear, or a specific flight maneuver. Consequently, the relevant query channels are often specified at query time. In this work, we introduce the Multivariate Subsequence Index (MS-Index), a novel algorithm for nearest neighbor MTS subsequence search under Euclidean distance that supports ad-hoc selection of query channels. The algorithm is exact and demonstrates query performance that scales sublinearly to the number of query channels. We examine the properties of \name with a thorough experimental evaluation over 34 datasets, and show that it outperforms the state-of-the-art one to two orders of magnitude for both raw and normalized subsequences.

</details>


### [10] [Extracting node comparison insights for the interactive exploration of property graphs](https://arxiv.org/abs/2512.15157)
*Cristina Aguiar,Jacques Chabin,Alexandre Chanson,Mirian Halfeld-Ferrari,Nicolas Hiot,Nicolas Labroche,Patrick Marcel,Verónika Peralta,Felipe Vasconcelos*

Main category: cs.DB

TL;DR: 本文提出了一种自动提取属性图中节点比较的方法，以支持这些图的交互式探索性分析。通过使用上下文来设计比较指标，并利用这些指标对节点进行分组，从而提取出有意义且非显而易见的比较结果。实验表明简单启发式方法能在几分钟内提供见解，但为了获得更高质量的见解，则需要使用较慢的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 虽然几十年来一直在研究如何评分图中的节点以理解其重要性（例如，从中心性的角度），但是基于属性在属性图中比较节点的问题，据我们所知，尚未得到解决。

Method: 首先提出了一种方法，即根据要比较节点的上下文来设计比较指标；然后正式定义了如何使用这些指标对节点进行分组的问题，以便所提取的比较既重要又不那么直接；最后提出了多种解决该问题的启发式方法。

Result: 在真实属性图数据库上的测试显示，简单的启发式方法能够在几分钟内提供见解，而对于更高质量的见解，则需要使用速度较慢的启发式方法。

Conclusion: 本研究为属性图中的节点比较提供了一种新的方法论，能够有效支持图数据的交互式探索分析。

Abstract: While scoring nodes in graphs to understand their importance (e.g., in terms of centrality) has been investigated for decades, comparing nodes in property graphs based on their properties has not, to our knowledge, yet been addressed. In this paper, we propose an approach to automatically extract comparison of nodes in property graphs, to support the interactive exploratory analysis of said graphs. We first present a way of devising comparison indicators using the context of nodes to be compared. Then, we formally define the problem of using these indicators to group the nodes so that the comparisons extracted are both significant and not straightforward. We propose various heuristics for solving this problem. Our tests on real property graph databases show that simple heuristics can be used to obtain insights within minutes while slower heuristics are needed to obtain insights of higher quality.

</details>


### [11] [Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting](https://arxiv.org/abs/2512.15308)
*Basil Ell*

Main category: cs.DB

TL;DR: 本文介绍了基于图模式的关联规则(GPARs)，用于处理有向标记多图如RDF图，支持生成性和评估性任务，并定义了概率空间来推导置信度、提升度等指标。


<details>
  <summary>Details</summary>
Motivation: 为了更好地处理有向标记多图（例如RDF图）中的扩展和评估任务，同时考虑到图的拓扑结构，提出了比现有方法更有效的框架。

Method: 提出了一种新的框架GPARs，该框架在没有重复任何语义的情况下对图模式进行评估，并定义了一个概率空间来从概率角度推导出置信度、提升度、杠杆率和信念度等度量标准。

Result: 新提出的度量标准与经典项集基础上的对应度量标准之间的关系得到了分析，并确定了保持这些特性特征属性的条件。

Conclusion: GPARs为处理有向标记多图提供了一种有效的方法，不仅适用于生成任务也适合于评估任务，并且通过引入概率空间使得可以更准确地衡量规则的有效性。

Abstract: We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.

</details>


### [12] [ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata](https://arxiv.org/abs/2512.15365)
*Gajendra Doniparthi,Shashank Balu Pandhare,Stefan Deßloch,Timo Mühlhaus*

Main category: cs.DB

TL;DR: 本文介绍了一种基于大型语言模型的系统ArcBERT，它能够理解自然语言查询并进行语义匹配，同时还能理解元数据中的结构和层次关系，从而有效处理多样化的用户查询模式。


<details>
  <summary>Details</summary>
Motivation: 传统的研究数据管理（RDM）生态系统中的搜索应用通常要求用户提交基于关键词的查询而非自然语言，这限制了用户体验。随着领域特定内容训练的大规模语言模型在专业自然语言处理任务中变得越来越普遍，开发一种能够理解自然语言查询且能识别元数据结构的新系统成为必要。

Method: 提出了ArcBERT，这是一个基于大规模语言模型（LLMs）设计的系统，用于集成元数据探索。该系统通过理解自然语言查询来进行语义匹配，并且能够理解和利用元数据中存在的结构和层级关系来改善搜索体验。

Result: ArcBERT展示了其理解自然语言查询以及元数据结构与层次的能力，使得它能够有效地应对各种用户查询模式，相较于传统基于关键词的搜索应用有了显著改进。

Conclusion: ArcBERT作为一款专为研究数据管理环境设计的新型搜索工具，通过采用先进的自然语言处理技术，不仅提高了对复杂查询的理解能力，而且加强了对元数据内部结构的认识，从而为用户提供了一个更高效、更直观的数据探索平台。

Abstract: Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [How Deep Does Your Dependency Tree Go? An Empirical Study of Dependency Amplification Across 10 Package Ecosystems](https://arxiv.org/abs/2512.14739)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 本研究对十个主要软件包生态系统中的500个项目进行了实证研究，发现不同生态系统间依赖放大倍数存在显著差异，其中Maven的平均放大倍数最高，为24.70倍；而CocoaPods最低，仅为0.32倍。基于此，建议针对不同的生态系统采取特定的安全策略。


<details>
  <summary>Details</summary>
Motivation: 随着现代软件开发越来越依赖于包生态系统，单一声明的依赖项可能会引入许多额外的传递性包，这种依赖放大现象对软件供应链安全具有重大影响。然而，跨生态系统的放大模式尚未在大规模上进行比较。

Method: 选择了十个主要生态系统（包括Maven Central、npm Registry等）中的500个项目作为样本，分析了这些项目中直接依赖与传递依赖的比例，并通过统计方法对比了不同生态系统之间的差异。

Result: 研究发现，Maven表现出最高的平均放大倍数，达到24.70倍，而Go Modules和npm分别为4.48倍和4.32倍，CocoaPods最低只有0.32倍。此外，28%的Maven项目超过10倍放大率，相比之下RubyGems为14%，npm为12%，而Cargo、PyPI、Packagist、CocoaPods及Pub则无一超过该阈值。

Conclusion: 不同生态系统的设计选择如依赖解析行为、标准库完整性以及平台限制等因素导致了依赖放大现象的差异。研究结果表明需要根据各个生态系统的特点制定特定的安全策略，比如对于Maven环境进行系统审计，对于npm和RubyGems实施针对性的异常检测，而对于那些控制得当的生态系统则继续沿用现有做法。

Abstract: Modern software development relies on package ecosystems where a single declared dependency can pull in many additional transitive packages. This dependency amplification, defined as the ratio of transitive to direct dependencies, has major implications for software supply chain security, yet amplification patterns across ecosystems have not been compared at scale. We present an empirical study of 500 projects across ten major ecosystems, including Maven Central for Java, npm Registry for JavaScript, crates io for Rust, PyPI for Python, NuGet Gallery for dot NET, RubyGems for Ruby, Go Modules for Go, Packagist for PHP, CocoaPods for Swift and Objective C, and Pub for Dart. Our analysis shows that Maven exhibits mean amplification of 24.70 times, compared to 4.48 times for Go Modules, 4.32 times for npm, and 0.32 times for CocoaPods. We find significant differences with large effect sizes in 22 of 45 pairwise comparisons, challenging the assumption that npm has the highest amplification due to its many small purpose packages. We observe that 28 percent of Maven projects exceed 10 times amplification, indicating a systematic pattern rather than isolated outliers, compared to 14 percent for RubyGems, 12 percent for npm, and zero percent for Cargo, PyPI, Packagist, CocoaPods, and Pub. We attribute these differences to ecosystem design choices such as dependency resolution behavior, standard library completeness, and platform constraints. Our findings suggest adopting ecosystem specific security strategies, including systematic auditing for Maven environments, targeted outlier detection for npm and RubyGems, and continuation of current practices for ecosystems with controlled amplification. We provide a full replication package with data and analysis scripts.

</details>


### [14] [VDMN: A Graphical Notation for Modelling Value Driver Trees](https://arxiv.org/abs/2512.14740)
*Benjamin Matthies*

Main category: cs.SE

TL;DR: 本文介绍了价值驱动建模符号（VDMN），这是一种图形化符号，旨在系统地指导价值驱动树（VDTs）的建模。通过案例研究和专家访谈评估表明，该符号支持一致且易于理解的价值驱动树建模，是实现VDT建模系统化和标准化的重要步骤。


<details>
  <summary>Details</summary>
Motivation: 尽管价值驱动树(VDTs)在管理决策和支持基于价值的管理方面越来越受到重视，但目前还没有系统性的指南来指导这些概念模型的建模过程。为了填补这一空白，本研究提出了一个专门为此目的设计的新方法。

Method: 本研究开发了一种新的图形化符号——价值驱动建模符号(VDMN)，它包含了一系列全面的语义构造元素以及直观的图形句法。此外，通过两个案例研究应用了VDMN，并通过专家访谈对其实际效用进行了评估。

Result: 结果表明，VDMN能够支持价值驱动树的一致性和可理解性建模。

Conclusion: VDMN代表了朝着VDT建模系统化和标准化迈出的重要一步。

Abstract: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.

</details>


### [15] [Revisiting the Reliability of Language Models in Instruction-Following](https://arxiv.org/abs/2512.14754)
*Jianshuo Dong,Yutong Zhang,Yan Liu,Zhenyu Zhong,Tao Wei,Chao Zhang,Han Qiu*

Main category: cs.SE

TL;DR: 本文研究了高级语言模型在细微差别提示下的可靠性问题，引入了新的度量标准reliable@k，并开发了一个自动生成高质量相似提示的数据增强管道。通过构建IFEval++进行系统评估，发现当前模型在处理细微差别时表现出显著不足，性能下降高达61.8%。此外还探索了三种可能的改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管先进的语言模型在如IFEval等基准测试中取得了接近满分的表现，但这些成绩并不总能转化为现实世界应用中的可靠服务。用户在实际使用过程中经常改变他们的措辞、上下文框架和任务表述。本文旨在研究模型是否能在传达类似用户意图但带有微妙差异的提示下展现出一致的能力。

Method: 提出了一种新的度量指标reliable@k，并且创建了一个能够自动生成高质量‘表亲’提示（即那些表达相似用户意图但含有微小差别的提示）的数据增强流程。基于此，进一步构建了IFEval++以支持系统的评估工作。

Result: 实验涵盖了20个私有及26个开源的语言模型，结果显示现有的模型在处理具有细微变化的提示时表现出了显著的不足——其性能最多可降低61.8%。

Conclusion: 本研究表明，面向细微差别的可靠性是实现更加可靠和值得信赖的语言模型行为的关键步骤之一，但目前仍处于探索阶段。同时，文章还探讨了几种潜在的改进方案。

Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.

</details>


### [16] [Examining Software Developers' Needs for Privacy Enforcing Techniques: A survey](https://arxiv.org/abs/2512.14756)
*Ioanna Theophilou,Georgia M. Kapitsaki*

Main category: cs.SE

TL;DR: 本文通过调查68名开发者，探讨了他们在数据隐私法规遵从性方面的需求，发现大多数开发者需要更多的自动化工具，并且随着隐私经验的增加，从业者对隐私工具的关注也在增加。研究结果指出了对隐私促进工具的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 由于GDPR和CCPA/CPRA等数据隐私立法要求所有软件系统必须遵守数据隐私法，开发者在实现法律需求时面临挑战。尽管先前的研究关注于开发者对隐私原则的理解和技术应用，但对于能够帮助开发者更好地遵守隐私法的新出现的需求尚未被充分研究。

Method: 采用问卷调查的方式，收集并分析了68名开发者的反馈，以了解他们对于辅助隐私法遵从性的具体需求及影响因素。

Result: 结果显示，多数开发者希望能够获得更多的自动化工具来帮助他们满足隐私法规的要求；同时，那些拥有更多隐私保护经验的开发者更加重视相关工具的支持。

Conclusion: 本研究表明，为了使开发过程中的合规工作变得更加简单流畅，有必要开发出更先进的自动化工具来支持隐私法的遵从。此外，该研究还强调了针对隐私促进解决方案的迫切需求。

Abstract: Data privacy legislation, such as GDPR and CCPA/CPRA, has rendered data privacy law compliance a requirement of all software systems. Developers need to implement various kinds of functionalities to cover law needs, including user rights and law principles. As data compliance is tightly coupled with legal knowledge, it is not always easy to perform such integrations in software systems. Prior studies have focused on developers' understanding of privacy principles, such as Privacy by Design, and have examined privacy techniques used in the software industry. Nevertheless, emerging developer needs that can assist in privacy law compliance have not been examined but are useful in understanding what development automation tools, such as Generative AI, need to cover to make the compliance process more straightforward and seamless within the development process. In this work, we present a survey that examines the above needs with the participation of 68 developers, while we have examined which factors affect practitioners' needs. Most developers express a need for more automated tools, while privacy experience increases practitioners' concerns for privacy tools. Our results can assist practitioners in better positioning their development activities within privacy law compliance and point to an urgent need for privacy facilitators.

</details>


### [17] [CAPE: Capability Achievement via Policy Execution](https://arxiv.org/abs/2512.14761)
*David Ball*

Main category: cs.SE

TL;DR: 本文介绍了一种名为Capability Engineering的新方法，通过CAPE协议实现将需求转化为可执行的规范，并训练模型默认满足这些规范。这种方法基于两个实证发现：上下文客观性和验证保真度缩放。在六个领域中，相对于DPO，CAPE能够减少81%的违规率，同时显著降低成本和时间。


<details>
  <summary>Details</summary>
Motivation: 当前的AI系统缺乏一种表达和强制执行需求的方法，导致即使是在基准测试中表现良好的高智能模型，在实际部署时也常常失败。为了解决这个问题，提出了Capability Engineering。

Method: 提出并实践了名为CAPE（通过策略执行实现能力）的协议，该协议实现了指定->验证->修正->训练的循环。基于两个关键发现：(1) 上下文客观性，即当上下文固定时，看似主观的属性变得客观；(2) 验证保真度随模型规模增加而提高。

Result: 在六个不同领域的109,500个例子中，相较于DPO，CAPE能减少81%的违规率，成本降低至原来的五分之一到二十分之一，并且开发周期从几个月缩短到了几周。

Conclusion: CAPE提供了一个有效框架来确保AI模型能够可靠地满足特定要求，同时减少了成本和时间。此外，还发布了CapabilityBench作为公共注册表，用于评估模型对抗社区贡献策略的能力。

Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.

</details>


### [18] [Workflows vs Agents for Code Translation](https://arxiv.org/abs/2512.14762)
*Henry Gray,Tom Yotam,Octavian Udrea*

Main category: cs.SE

TL;DR: 本文比较了两种基于大语言模型（LLM）的方法，用于MATLAB到硬件描述语言（HDL）转换过程中的语法修复阶段。研究发现，在三种不同规模的模型上，自主式方法比专家设计的结构化流程更有效地解决了初始语法错误问题，并且对于中小型模型特别有效，提高了整个转换流程的成功率。


<details>
  <summary>Details</summary>
Motivation: 从高级语言如MATLAB向硬件描述语言（HDLs）转化算法是一个资源密集但必要的步骤，尤其是在FPGA和ASIC部署方面。尽管大型语言模型提供了自动化的机会，但由于它们在HDL代码上的训练有限，导致端到端转译变得脆弱且容易出现语法错误。因此，有必要探索更有效的语法修复方法以提高转换效率。

Method: 研究采用了两种基于大语言模型的方法来解决MATLAB至HDL转换过程中遇到的语法修复问题：一种是遵循固定操作序列的结构化、专家设计流程；另一种则是利用Model Context Protocol (MCP)协议动态选择工具的更加自主的方法。通过对42个MATLAB信号处理函数的研究，专注于语法修复阶段的表现。

Result: 结果显示，在三个不同规模级别的模型测试中，自主式方法在解决初步语法错误上表现得更为出色，能够解锁更多候选者继续通过转换流程。这种上游改进带来了显著的下游效益，特别是在中等规模模型上，模拟到达率提高了超过20个百分点。

Conclusion: 研究结果表明，当正确设计时，这些自主框架最能有效地弥补小型和中型模型的能力限制。此外，条件检索对8B和30B模型有帮助；而在235B模型上，最终成功的增益较小，一个简单的RAG变体达到了最高的最终成功率。

Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.

</details>


### [19] [Let the Barbarians In: How AI Can Accelerate Systems Performance Research](https://arxiv.org/abs/2512.14806)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Shubham Agarwal,Mert Cemri,Bowen Wang,Alexander Krentsel,Tian Xia,Jongseok Park,Shuo Yang,Jeff Chen,Lakshya Agrawal,Ashwin Naren,Shulu Li,Ruiying Ma,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.SE

TL;DR: 本文介绍了AI驱动的系统研究（ADRS）的概念，通过几个开源ADRS实例展示了在十个案例研究中，ADRS生成的解决方案可以匹配甚至超越人类设计的最先进水平，并提出了有效使用ADRS的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能开始通过自动化发现新方案来改变研究过程，特别是在需要改进系统性能的研究领域，因为这类问题自然允许验证器的存在：候选方案可以在真实系统或模拟器中实现，并根据预定义的工作负载进行评估。

Method: 研究人员使用了数个开源ADRS实例（例如OpenEvolve、GEPA和ShinkaEvolve），并在十个案例研究中进行了演示，这些案例包括多区域云调度、混合专家负载均衡、基于LLM的SQL以及事务调度等。

Result: ADRS产生的解决方案能够在多个案例中与人类顶尖设计方案相匹敌，甚至超越它们。此外，还基于研究结果概述了有效运用ADRS的一些最佳实践，如提示规格的程度、反馈量及稳健评估等。

Conclusion: 尽管还没有一个通用的方法将ADRS应用于所有系统研究，但本文提供的初步发现和所识别的挑战为未来工作提供了有意义的指导，随着研究者努力逐渐转向问题构建和战略监督。

Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight. Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.

</details>


### [20] [Industry Expectations and Skill Demands in Quantum Software Testing](https://arxiv.org/abs/2512.14861)
*Ronnie de Souza Santos,Teresa Baldassarre,Cesar França*

Main category: cs.SE

TL;DR: 本研究通过分析110份来自量子软件和硬件开发组织的职位发布，探讨了量子软件测试领域的角色定义以及所需技能。结果表明，量子环境下的测试结合了传统软件质量保证与实验验证，并强调校准、控制及混合量子-经典验证的重要性。雇主寻求具备编程自动化专业知识、量子特定技术知识及跨学科合作能力的专业人士。


<details>
  <summary>Details</summary>
Motivation: 量子软件测试带来了与经典软件工程根本不同的新挑战。本研究旨在探索量子软件行业如何定义测试角色以及这些岗位对专业人员有何技能要求。

Method: 研究方法包括分析来自参与量子软件和硬件开发的组织发布的110个职位信息，以识别与测试相关的活动、能力和技能需求。

Result: 研究发现显示，在量子背景下进行测试需要将传统的软件质量保证与实验验证相结合，特别强调了校准、控制以及混合量子-经典验证的重要性。企业正在寻找能够将编程与自动化专业知识同量子特有技术知识以及跨学科协作技巧结合起来的人才。

Conclusion: 量子软件测试仍处于早期但快速发展的阶段，它连接了软件工程与实验物理学领域，这突显出调整教育与研究努力以使测试实践符合工业现实需求的重要性。

Abstract: Quantum software testing introduces new challenges that differ fundamentally from those in classical software engineering. Aims: This study investigates how the quantum software industry defines testing roles and what skills are expected from professionals in these positions. Method: We analyzed 110 job postings from organizations involved in quantum software and hardware development, identifying activities, competencies, and skill requirements related to testing. Results: The findings show that testing in quantum contexts combines traditional software quality assurance with experimental validation, emphasizing calibration, control, and hybrid quantum-classical verification. Employers seek professionals who integrate programming and automation expertise with quantum-specific technical knowledge and interdisciplinary collaboration skills. Conclusions: Quantum software testing remains at an early but rapidly evolving stage that bridges software engineering and experimental physics, highlighting the need for educational and research efforts that align testing practices with industrial realities.

</details>


### [21] [Evaluating Code Reasoning Abilities of Large Language Models Under Real-World Settings](https://arxiv.org/abs/2512.14917)
*Changshu Liu,Alireza Ghazanfari,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: This paper introduces RE2-Bench, a more complex and realistic benchmark for evaluating large language models (LLMs) in code reasoning. It includes 1,101 problems, with some drawn from real-world projects, and categorizes them into Easy or Hard based on nine complexity metrics. Evaluation of six LLMs shows a significant performance decrease from Easy to Hard problems, indicating that previous benchmarks may overestimate LLMs' reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准仅涉及简单的程序，未能体现现实世界中代码的复杂性。这导致在评估大型语言模型（LLM）时对其实际通用性的假设存在重大威胁。为了实现更真实的代码推理评估，本文提出了一种新的基准RE2-Bench。

Method: 提出了RE2-Bench，一个包含1,101个推理问题的基准，其中195个来自成熟的现实世界项目。该基准利用静态和动态程序分析技术来自动序列化和反序列化组合、复杂及自定义类型的真实世界代码，并通过九个可解释的代码复杂度指标对每个推理问题分类为简单或困难。

Result: 六个通用型和面向推理的大型语言模型在两个广泛使用的代码推理任务——输入预测和输出预测上的综合评估表明，从简单到困难的问题性能显著下降（输入预测下降51.50%，输出预测下降42.15%），证实了先前的评估可能高估了这些模型的推理能力。

Conclusion: RE2-Bench提供了一个更加严格且贴近现实世界的测试环境来评估大型语言模型的代码推理能力。实验结果证明，在处理更复杂的代码推理问题时，当前模型的表现远低于预期，指出了现有评测方法的局限性。

Abstract: Code reasoning tasks are becoming prevalent in large language model (LLM) assessments. Existing benchmarks involve simple programs, failing to represent real-world complexities such as inter- or intra-procedural dependencies, core or third-party API calls, highly nested constructs, and non-primitive complex types. Evaluating LLMs under such a simplistic setting poses a significant threat to assumptions about their generalizability in practice. To enable a more realistic evaluation of code reasoning, this paper proposes RE2-Bench, a benchmark of 1,101 reasoning problems, including 195 drawn from mature real-world projects. RE2-Bench leverages static and dynamic program analysis to automatically serialize and deserialize compound, complex, and custom types in real-world code, going far beyond the primitive-only settings used in prior work.
  A key feature of RE2-Bench is categorizing each reasoning problem as Easy or Hard via a principled majority-vote mechanism over nine interpretable code complexity metrics, resulting in two well-separated and semantically meaningful difficulty categories suitable for precise calibration of LLM reasoning ability. A comprehensive evaluation of six general-purpose and reasoning-oriented LLMs on two widely used code reasoning tasks -- input prediction and output prediction -- using RE2-Bench reveals a significant performance drop from Easy to Hard problems (51.50\% for input prediction and 42.15\% for output prediction), confirming that prior evaluations substantially overestimate the reasoning capabilities of LLMs.

</details>


### [22] [Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent](https://arxiv.org/abs/2512.14990)
*Mehil B Shah,Mohammad Masudur Rahman,Foutse Khomh*

Main category: cs.SE

TL;DR: 提出了一种名为RepGen的新方法，用于自动重现深度学习中的bug。该方法通过构建学习增强的上下文、制定全面的bug重现计划并利用迭代生成-验证-优化机制来自动生成可重现特定bug的代码。在106个真实世界的深度学习bug上测试，达到了80.19%的重现率，并且根据开发者研究显示，这种方法提高了23.35%的成功率，减少了56.8%的重现时间，同时降低了参与者的认知负担。


<details>
  <summary>Details</summary>
Motivation: 深度学习（DL）应用虽然被广泛采用，但它们容易出现许多错误、故障和漏洞。由于DL模型本身的不确定性以及与硬件和软件环境紧密结合的特点，手动方法仅能可靠地重现约3%的DL错误，这使得重现这些错误极其具有挑战性。为了解决这个问题，文章提出了一个新的自动化智能解决方案。

Method: 开发了RepGen，一种新的自动化智能方法来重现深度学习错误。该方法首先从项目中构造一个学习加强的情境；接着，它会为错误重现设计一个详尽的计划；最后，通过使用大型语言模型（LLM）以迭代生成-验证-优化的方式生成能够重现当前错误的代码。

Result: 在对106个现实世界中的深度学习错误进行评估后，RepGen实现了80.19%的再现率，比现有最先进技术提升了19.81个百分点。此外，一项包括27名参与者的研究表明，RepGen将DL错误再现成功率提高了23.35%，并将再现所需时间缩短了56.8%，同时也减轻了参与者的认知负荷。

Conclusion: RepGen作为一种创新的方法，在提高深度学习错误重现效率方面展现出了显著优势。它不仅大幅提高了错误重现的成功率，还有效减少了重现过程所需的时间及人力成本，显示出其在促进深度学习系统可靠性方面的巨大潜力。

Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.

</details>


### [23] [Toxicity Ahead: Forecasting Conversational Derailment on GitHub](https://arxiv.org/abs/2512.15031)
*Mia Mohammad Imran,Robert Zita,Rahat Rizvi Rahman,Preetha Chatterjee,Kostadin Damevski*

Main category: cs.SE

TL;DR: 研究通过构建一个基于大型语言模型（LLM）的框架来预测GitHub上对话偏离正轨的可能性，该框架使用两步提示策略生成对话动态摘要并估计偏离的可能性。在Qwen和Llama模型上的测试表明，此方法比现有的NLP基线更有效，能够支持开源软件社区中对话偏离的早期检测与主动管理。


<details>
  <summary>Details</summary>
Motivation: 开源软件(OSS)社区中的有害互动会降低贡献者的参与度，并威胁项目的持续性。为了防止毒性言论的出现，需要清楚地了解有害对话是如何发展的。然而，大多数主动式调节策略是手动的，这要求社区维护者投入大量时间和精力。因此，本研究旨在开发一种可扩展的方法来支持对这些毒性对话的早期识别。

Method: 研究人员整理了一个包含159个脱轨有毒讨论串和207个非有毒讨论串的数据集。他们提出了一个新的基于大型语言模型（LLM）的框架，用于通过两步提示流水线预测GitHub上的对话脱轨。首先，利用从少到多(LtM)提示技术生成《对话动态总结》(SCDs)，然后使用这些总结来估算脱轨的可能性。

Result: 在Qwen和Llama模型上评估时，LtM策略分别达到了0.901和0.852的F1分数，在决策阈值为0.3的情况下超过了既定的NLP基线对于对话脱轨的表现。外部验证在一个由308个GitHub问题串组成的数据集（其中65个有毒，243个无毒）上进行，F1分数高达0.797。

Conclusion: 研究发现证明了结构化LLM提示在OSS中对话偏离早期检测方面的有效性，使得可以采取主动且可解释性的调节措施成为可能。

Abstract: Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.
  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.

</details>


### [24] [Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering](https://arxiv.org/abs/2512.15148)
*Hang Yu,Yuzhou Lai,Li Zhang,Xiaoli Lian,Fang Liu,Yanrui Dong,Ting Zhang,Zhi Jin,David Lo*

Main category: cs.SE

TL;DR: 本研究通过分析2022年至2025年间在FSE、ASE和ICSE上发表的1,367篇论文，并对17个组织进行实证调查，收集了关于程序分析、自动化测试等六个主题的282份反馈。研究揭示了学术成果与工业需求之间的差距，并提出了七个关键启示，旨在指导未来软件工程研究更加关注实际工业影响。


<details>
  <summary>Details</summary>
Motivation: 尽管顶级软件工程会议继续关注自动化测试和程序修复等领域，但学术进步是否真正满足了工业需求仍不清楚。为了填补这一空白，有必要对比学术能力与工业反馈，从而为未来的软件工程研究指明方向。

Method: 首先系统地分析了2022至2025年期间在FSE、ASE和ICSE发表的1,367篇文章，识别主要研究领域、常用基准、行业相关性及开源可用性；然后通过对17家机构进行实证调查，采用结构化问卷形式，在六个显著话题上获得了282份回复。

Result: 得出了七个重要启示，强调了软件需求与架构中未被充分解决的问题、智能软件工程方法的可靠性和可解释性、学术研究中的输入假设、实践评估中的紧张关系以及伦理考量等问题。

Conclusion: 研究建议重新调整学术界对于这些重要但探索不足问题的关注，并指导未来软件工程研究朝着更大的工业影响力迈进。

Abstract: The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.

</details>


### [25] [Automating Execution and Verification of BPMN+DMN Business Processes](https://arxiv.org/abs/2512.15214)
*Giuseppe Della Penna,Igor Melatti*

Main category: cs.SE

TL;DR: 本文提出了一种名为BDTransTest的工具，该工具可以将BPMN+DMN流程转换为Java程序，并生成和执行测试计划，以帮助验证业务流程的正确性。此外，还提供了对文献中BPMN+DMN流程的方法论实验评估。


<details>
  <summary>Details</summary>
Motivation: 随着BPMN业务流程（包括DMN表）的日益普及，需要有工具和方法来验证它们的正确性。然而，大多数用于构建BPMN+DMN模型的框架仅允许设计者检测语法错误，而忽略了语义（行为）故障。这迫使业务流程设计人员使用专有工具手动运行其BPMN+DMN流程的单次执行以检测故障。此外，专有工具如何将BPMN+DMN流程转化为计算机模拟尚未明确说明。

Method: 开发了名为BDTransTest的工具，该工具能够：i) 将BPMN+DMN流程B翻译成Java程序P；ii) 为B合成并执行测试计划，可能需要业务设计师澄清某些输入域；iii) 根据B中的节点和边分析测试计划所达到的覆盖率。

Result: 通过文献中的BPMN+DMN流程对本方法进行了实验评估。

Conclusion: BDTransTest工具提供了一种从BPMN+DMN到Java程序的转化方式，以及一套测试计划的生成与执行机制，从而帮助识别和修复业务流程中的潜在问题。

Abstract: The increasing and widespread use of BPMN business processes, also embodying DMN tables, requires tools and methodologies to verify their correctness. However, most commonly used frameworks to build BPMN+DMN models only allow designers to detect syntactical errors, thus ignoring semantic (behavioural) faults. This forces business processes designers to manually run single executions of their BPMN+DMN processes using proprietary tools in order to detect failures. Furthermore, how proprietary tools translate a BPMN+DMN process to a computer simulation is left unspecified. In this paper, we advance this state of the art by designing a tool, named BDTransTest providing: i) a translation from a BPMN + DMN process B to a Java program P ; ii) the synthesis and execution of a testing plan for B, that may require the business designer to disambiguate some input domain; iii) the analysis of the coverage achieved by the testing plan in terms of nodes and edges of B. Finally, we provide an experimental evaluation of our methodology on BPMN+DMN processes from the literature.

</details>


### [26] [Heterogeneous Model Alignment in Digital Twin](https://arxiv.org/abs/2512.15281)
*Faima Abbasi,Jean-Sébastien Sottet,Cedric Pruski*

Main category: cs.SE

TL;DR: 本文提出了一种针对多层次、模型驱动的数字孪生技术的异构模型对齐方法，该方法通过自适应一致性机制和基于大语言模型的对齐过程来确保模型之间的语义一致性，并通过空气质量案例研究及OAEI测试用例验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 在多层模型驱动的数字孪生中，跨抽象层次对齐异构模型时存在语义不匹配、不一致性和同步问题。现有方法依赖静态映射和手动更新，不够灵活且容易出错，可能危及数据完整性。

Method: 提出了一个包含灵活性机制的框架，允许元模型适应并无缝互连，同时保持跨抽象层的语义一致性。该框架集成了自适应一致性机制将元模型与演化的模型链接起来，并利用大型语言模型（LLM）验证对齐过程，以确保结构保真度和概念一致性。

Result: 所提出的方法能够自动化发现语义对应关系，减少手动映射工作量，并提高跨不同类型模型的可扩展性。通过空气质量使用案例展示了该方法的应用，并利用来自Ontology Alignment Evaluation Initiative (OAEI)不同轨道的测试用例对其性能进行了验证。

Conclusion: 新的异构模型对齐方法为解决多层模型驱动数字孪生中的关键挑战提供了有效手段，提高了决策制定和运营效率的同时保证了数据完整性和一致性。

Abstract: Digital twin (DT) technology integrates heterogeneous data and models, along with semantic technologies to create multi-layered digital representation of physical systems. DTs enable monitoring, simulation, prediction, and optimization to enhance decision making and operational efficiency. A key challenge in multi-layered, model-driven DTs is aligning heterogeneous models across abstraction layers, which can lead to semantic mismatches, inconsistencies, and synchronization issues. Existing methods, relying on static mappings and manual updates, are often inflexible, error-prone, and risk compromising data integrity. To address these limitations, we present a heterogeneous model alignment approach for multi-layered, model-driven DTs. The framework incorporates a flexibility mechanism that allows metamodels to adapt and interconnect seamlessly while maintaining semantic coherence across abstraction layers. It integrates: (i) adaptive conformance mechanisms that link metamodels with evolving models and (ii) a large language model (LLM) validated alignment process that grounds metamodels in domain knowledge, ensuring structural fidelity and conceptual consistency throughout the DT lifecycle. This approach automates semantic correspondences discovery, minimizes manual mapping, and enhances scalability across diverse model types. We illustrate the approach using air quality use case and validate its performance using different test cases from Ontology Alignment Evaluation Initiative (OAEI) tracks.

</details>


### [27] [Can AI Generate more Comprehensive Test Scenarios? Review on Automated Driving Systems Test Scenario Generation Methods](https://arxiv.org/abs/2512.15422)
*Ji Zhou,Yongqi Zhao,Yixian Hu,Hexuan Li,Zhengguo Gu,Nan Xu,Arno Eichberger*

Main category: cs.SE

TL;DR: 该综述分析了31篇主要研究和10篇调查，聚焦于2023至2025年间基于场景测试的最新进展，特别是AI辅助及多模态方法。文章指出当前存在标准化评估指标缺失、伦理与人为因素整合不足以及对多模态和特定操作设计领域(ODD)场景覆盖不够三大问题，并提出了一套改进的分类体系以支持更透明的基准测试。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶系统（ADS）的安全性和可靠性是一个关键挑战，传统验证方法如大规模道路测试成本高昂且耗时。基于场景的测试作为一种可扩展且高效的替代方案被提出，但现有综述未能全面覆盖近期的方法和技术进步。

Method: 通过系统性地分析2015年至2025年间识别出的31篇主要研究和10篇综述文献，重点关注2023至2025年期间出现的新框架，这些新框架采用了生成模型等AI技术来合成多样化且安全关键的情景。

Result: 发现当前基于场景测试的研究中存在三个持续存在的空白：缺乏标准化评价指标、对伦理和人类因素考虑不足、以及针对多模态和特定操作设计域(ODD)情景的覆盖率低。

Conclusion: 为解决上述挑战，本文贡献了一个包含多模态扩展的细化分类法、一个负责任情景设计的伦理与安全检查表、以及带有情景难度模式的ODD覆盖图，旨在促进透明度更高的基准测试。整体而言，这些贡献为研究人员提供了方法上的清晰度，同时为业界提供了实用指南，支持可重复评估并加速高级别ADS的安全部署。

Abstract: Ensuring the safety and reliability of Automated Driving Systems (ADS) remains a critical challenge, as traditional verification methods such as large-scale on-road testing are prohibitively costly and time-consuming.To address this,scenario-based testing has emerged as a scalable and efficient alternative,yet existing surveys provide only partial coverage of recent methodological and technological advances.This review systematically analyzes 31 primary studies,and 10 surveys identified through a comprehensive search spanning 2015~2025;however,the in-depth methodological synthesis and comparative evaluation focus primarily on recent frameworks(2023~2025),reflecting the surge of Artificial Intelligent(AI)-assisted and multimodal approaches in this period.Traditional approaches rely on expert knowledge,ontologies,and naturalistic driving or accident data,while recent developments leverage generative models,including large language models,generative adversarial networks,diffusion models,and reinforcement learning frameworks,to synthesize diverse and safety-critical scenarios.Our synthesis identifies three persistent gaps:the absence of standardized evaluation metrics,limited integration of ethical and human factors,and insufficient coverage of multimodal and Operational Design Domain (ODD)-specific scenarios.To address these challenges,this review contributes a refined taxonomy that incorporates multimodal extensions,an ethical and safety checklist for responsible scenario design,and an ODD coverage map with a scenario-difficulty schema to enable transparent benchmarking.Collectively,these contributions provide methodological clarity for researchers and practical guidance for industry,supporting reproducible evaluation and accelerating the safe deployment of higher-level ADS.

</details>


### [28] [Insecure Ingredients? Exploring Dependency Update Patterns of Bundled JavaScript Packages on the Web](https://arxiv.org/abs/2512.15447)
*Ben Swierzy,Marc Ohm,Michael Meier*

Main category: cs.SE

TL;DR: 本文提出了一种名为Aletheia的新方法，该方法能够更准确地从JavaScript包中识别出使用的软件包版本。通过分析Tranco排名前100,000的网站，研究发现5%到20%的域名在16周内更新了它们的依赖项，并且捆绑的软件包相比CDN包含的软件包更新速度明显更快，但也有迹象表明少数广泛分布的供应商可能是及时更新的主要推动力。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中可重用软件组件的安全性问题受到关注，尤其是在JavaScript生态系统中，存在高比例易受攻击的软件包版本。然而，对于这些易受攻击版本在实际生产网站中的流行程度缺乏了解。现有检测机制要么针对特定流行的包，要么只适用于使用全局命名空间的单个文件资源，这不足以全面分析大规模现代Web应用程序的依赖更新行为。

Method: 研究人员提出了Aletheia，这是一种与软件包无关的方法，它利用源自抄袭检测领域的算法来解剖JavaScript包并识别其中的软件包版本。

Result: 研究表明，Aletheia方法在实践中明显优于现有的方法。通过对Tranco前10万个域名的爬取分析显示，大约5%至20%的域名会在16周内更新其依赖项。令人惊讶的是，长期来看，捆绑包比通过CDN引入的同类产品更新得快得多，因此包含的已知脆弱包版本数量最多可以减少10倍。

Conclusion: 尽管定量数据显示出积极的趋势，但观察结果也暗示着少数广泛使用的供应商可能在推动及时更新方面起着关键作用。这意味着仅仅依靠数字指标可能无法全面反映实际情况。

Abstract: Reusable software components, typically distributed as packages, are a central paradigm of modern software development. The JavaScript ecosystem serves as a prime example, offering millions of packages with their use being promoted as idiomatic. However, download statistics on npm raise security concerns as they indicate a high popularity of vulnerable package versions while their real prevalence on production websites remains unknown. Package version detection mechanisms fill this gap by extracting utilized packages and versions from observed artifacts on the web. Prior research focuses on mechanisms for either hand-selected popular packages in bundles or for single-file resources utilizing the global namespace. This does not allow for a thorough analysis of modern web applications' dependency update behavior at scale. In this work, we improve upon this by presenting Aletheia, a package-agnostic method which dissects JavaScript bundles to identify package versions through algorithms originating from the field of plagiarism detection. We show that this method clearly outperforms the existing approaches in practical settings. Furthermore, we crawl the Tranco top 100,000 domains to reveal that 5% - 20% of domains update their dependencies within 16 weeks. Surprisingly, from a longitudinal perspective, bundled packages are updated significantly faster than their CDN-included counterparts, with consequently up to 10 times fewer known vulnerable package versions included. Still, we observe indicators that few widespread vendors seem to be a major driving force behind timely updates, implying that quantitative measures are not painting a complete picture.

</details>


### [29] [A Container-based Approach For Proactive Asset Administration Shell Digital Twins](https://arxiv.org/abs/2512.15452)
*Carsten Ellwein,Jingxi Zhang,Andreas Wortmann,Antony Ayman Alfy Meckhael*

Main category: cs.SE

TL;DR: 本文提出了一种基于子模型的架构，通过将行为定义引入资产管理壳（AAS）中，使其能够动态地与容器化服务互动并适应。该方法通过扩展具有行为定义的子模型实现，并以一个三轴铣床案例研究为例进行了展示。


<details>
  <summary>Details</summary>
Motivation: 现有的资产管理壳（AAS）被当作静态信息模型处理，缺乏对动态服务集成和系统适应性的支持。文献尚未深入探讨如何将可执行行为特别是容器化服务融入或从AAS中导出的可能性，这限制了AAS在主动功能方面的潜力。

Method: 提出了一种基于子模型的方法，在AAS中引入结构化的服务概念，允许服务在运行时动态地与AAS实例交互和适应。这种概念是通过扩展包含行为定义的子模型来实现的，形成了一个模块化的事件驱动架构，可以根据嵌入的触发条件部署容器化服务。

Result: 通过一个三轴铣床的案例研究展示了这种方法的有效性。所提出的贡献使AAS不仅可以作为被动的数字表示，而且可以作为执行增值业务的活跃接口，为未来AI驱动的自适应和系统级智能奠定了基础。

Conclusion: 本文介绍了一种新的架构，旨在通过将行为定义整合到AAS中，促进制造环境中数字孪生的动态服务集成及系统适应能力的发展，从而为更高级别的智能化铺平道路。

Abstract: In manufacturing, digital twins, realized as Asset Administration Shells (AAS), have emerged as a prevalent practice. These digital replicas, often utilized as structured repositories of asset-related data, facilitate interoperability across diverse systems. However, extant approaches treat the AAS as a static information model, lacking support for dynamic service integration and system adaptation. The existing body of literature has not yet thoroughly explored the potential for integrating executable behavior, particularly in the form of containerized services, into or from the AAS. This integration could serve to enable proactive functionality. In this paper, we propose a submodel-based architecture that introduces a structured service notion to the AAS, enabling services to dynamically interact with and adapt AAS instances at runtime. This concept is implemented through the extension of a submodel with behavioral definitions, resulting in a modular event-driven architecture capable of deploying containerized services based on embedded trigger conditions. The approach is illustrated through a case study on a 3-axis milling machine. Our contribution enables the AAS to serve not only as a passive digital representation but also as an active interface for executing added-value services.%, thereby laying the foundation for future AI-driven adaptation and system-level intelligence in digital twin environments.

</details>


### [30] [On Assessing the Relevance of Code Reviews Authored by Generative Models](https://arxiv.org/abs/2512.15466)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 本文提出了一种基于多主观排序的新评估方法，用于评估大型语言模型如ChatGPT在代码审查中的表现。通过与CodeReview StackExchange上的顶尖人类回复进行比较，结果显示ChatGPT生成的评论质量显著优于人类回复，并且超越了StackExchange上被采纳的答案。此外，该方法促进了对生成式AI在代码审查中表现更有意义的评估，同时也提醒人们注意未经检查地将其集成到审查流程中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现有的代码审查生成评价方法要么依赖于与单一真实答案的自动比较，这无法捕捉到人类视角的变化性；要么依赖于对“有用性”的主观评价，这是一个非常模糊的概念。因此，需要一种新的评估方法来更准确、全面地衡量生成式AI在代码审查中的性能。

Method: 研究者们提出了一种称为多主观排序的新评估方法，利用来自CodeReview StackExchange的280份独立代码审查请求及其对应评论的数据集，由多名人类评审员对ChatGPT生成的评论与平台上最优秀的人类回复的质量进行了排名。

Result: 结果表明，ChatGPT生成的评论质量明显高于人类提供的评论，甚至超过了StackExchange上被接受的答案。

Conclusion: 所提出的方法不仅激励了对生成式AI在代码审查领域表现进行更加有意义的评估，还提高了对于将这类技术无限制地整合进审查过程可能带来的风险的认识。

Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.

</details>


### [31] [How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?](https://arxiv.org/abs/2512.15468)
*Hua Yang,Alejandro Velasco,Thanh Le-Cong,Md Nazmul Haque,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 研究探讨了语义等价代码转换规则是否可以用来规避成员推断（MI）检测。结果显示，单个转换规则对模型准确性的影响有限，但变量重命名规则显著降低了MI的成功率。结合多种转换并未进一步削弱MI的有效性，揭示了在训练大型语言模型时许可合规执行的一个关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的训练依赖于大量的代码数据，包括开源和私有代码，这引发了关于知识产权遵守及许可证受限代码潜在未授权使用的问题。尽管已有方法如成员推断技术尝试解决这些问题，但语义等价代码转换技术可能削弱这些解决方案的效果。

Method: 系统地研究了不同语义等价代码转换规则对于规避成员推断检测的效果，并通过因果分析验证了特定规则（如变量重命名）对降低MI成功率的具体影响。

Result: 发现单独应用每种转换规则对模型准确性的负面影响较小，但'重命名变量'这一规则特别有效，能够将MI成功率降低10.19%。同时指出，组合多种转换并不额外增加对抗MI检测的优势。

Conclusion: 研究结果表明，基于转换的混淆技术可显著减弱针对代码的成员推断检测效果，揭示了在执行代码许可合规性方面存在的一个重要漏洞。

Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.

</details>


### [32] [WuppieFuzz: Coverage-Guided, Stateful REST API Fuzzing](https://arxiv.org/abs/2512.15554)
*Thomas Rooijakkers,Anne Nijsten,Cristian Daniele,Erieke Weitenberg,Ringo Groenewegen,Arthur Melissen*

Main category: cs.SE

TL;DR: 本文介绍了一种名为WuppieFuzz的开源REST API模糊测试工具，它基于LibAFL构建，支持白盒、灰盒和黑盒模糊测试。该工具利用OpenAPI规范自动生成初始请求序列，并通过REST特定和LibAFL提供的变异器来探索软件中的不同代码路径。WuppieFuzz能够根据覆盖情况指导选择发送的下一个请求序列，以达到软件测试中的复杂状态，并且自动化了测试套件创建的过程以减少人工工作量。此外，还提供了多种报告类型帮助修复漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着业务流程越来越多地依赖于使用REST API进行通信的Web服务，充分测试这些暴露端点的安全性变得十分重要，因为它们可能带来安全风险。由于端点数量通常很大，因此需要像模糊测试这样的自动化测试技术。

Method: WuppieFuzz是一个基于LibAFL框架开发的REST API模糊测试工具，它能依据OpenAPI定义生成初始化请求语料库，并运用专门针对REST以及LibAFL自带的变异算法来产生新的测试用例，从而探索待测软件的不同执行路径。该工具还实现了基于覆盖率反馈的策略来决定后续发送哪些请求序列，旨在深入测试软件更复杂的内部状态。

Result: 作者们通过对Petstore API的应用评估了WuppieFuzz工具，特别是其白盒方法的稳健性和不同调度策略的有效性。他们还随着时间监测了端点覆盖率和代码覆盖率，以此来衡量这种方法的效果。

Conclusion: WuppieFuzz提供了一个强大的REST API模糊测试解决方案，能够有效发现潜在的安全问题。它的自动测试套件生成功能降低了对人工干预的需求，同时多种报告功能有助于快速定位并修复漏洞。

Abstract: Many business processes currently depend on web services, often using REST APIs for communication. REST APIs expose web service functionality through endpoints, allowing easy client interaction over the Internet. To reduce the security risk resulting from exposed endpoints, thorough testing is desired. Due to the generally vast number of endpoints, automated testing techniques, like fuzzing, are of interest.
  This paper introduces WuppieFuzz, an open-source REST API fuzzer built on LibAFL, supporting white-box, grey-box and black-box fuzzing. Using an OpenAPI specification, it can generate an initial input corpus consisting of sequences of requests. These are mutated with REST-specific and LibAFL-provided mutators to explore different code paths in the software under test. Guided by the measured coverage, WuppieFuzz then selects which request sequences to send next to reach complex states in the software under test. In this process, it automates harness creation to reduce manual efforts often required in fuzzing. Different kinds of reporting are provided by the fuzzer to help fixing bugs.
  We evaluated our tool on the Petstore API to assess the robustness of the white-box approach and the effectiveness of different power schedules. We further monitored endpoint and code coverage over time to measure the efficacy of the approach.

</details>


### [33] [A High-level Synthesis Toolchain for the Julia Language](https://arxiv.org/abs/2512.15679)
*Benedict Short,Ian McInerney,John Wickerson*

Main category: cs.SE

TL;DR: 本文提出了一种基于MLIR的编译器工具链，该工具链可以自动将用Julia编程语言编写的内核编译成SystemVerilog，无需额外的指令或语言定制。它支持动态和静态调度，并且可以直接与AXI4-Stream协议集成来与片上和片外存储器等子系统接口，生成与供应商无关的RTL。此原型工具链能够合成在真实FPGA设备上以100MHz运行的一组信号处理/数学基准测试，其吞吐量达到使用仅从C或C++等低级语言编译的设计所生成的最新工具链的59.71%到82.6%。


<details>
  <summary>Details</summary>
Motivation: 随着计算问题规模的显著增长以及对通用硬件加速器（如GPU和TPU）的需求增加，对于设计特定于问题的加速器（使用FPGA）的兴趣也重新燃起。然而，这些特定问题加速器的开发过程面临着“双语言问题”：算法在一个（通常是更高级的）语言中开发，但内核却在另一个抽象级别完全不同的语言中实现，这需要根本不同的专业知识。为了解决这个问题，研究者提出了一个新的解决方案。

Method: 提出了一种新的基于MLIR的编译器工具链，它可以统一开发过程，通过自动将用Julia编写的内核转换成SystemVerilog，而不需要任何额外的指令或语言定制。该工具链支持动态和静态调度，直接与AXI4-Stream协议集成以便与片上和片外内存等子系统进行接口，并生成不依赖于具体厂商的RTL代码。

Result: 这个原型工具链能够在实际FPGA设备上以100MHz的速度运行一组信号处理/数学基准测试，达到了使用像C或C++这样的低级语言编译而成的设计所产生的最先进工具链吞吐量的59.71%到82.6%之间。

Conclusion: 这种新工具链允许领域专家按照他们通常的方式用Julia编写计算内核，然后将其重定向到FPGA上，而无需添加额外的pragma或修改，从而简化了FPGA加速器的开发流程。

Abstract: With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the "two-language problem": algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Autonomous Source Knowledge Selection in Multi-Domain Adaptation](https://arxiv.org/abs/2512.14710)
*Keqiuyin Li,Jie Lu,Hua Zuo,Guangquan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为AutoS的多域适应方法，该方法能够自主选择源训练样本和模型，以利用更相关且可迁移的源信息来预测目标任务。通过基于密度的选择策略挑选源样本，并决定哪些源模型应参与目标预测。同时采用预训练的多模态模型构建伪标签增强模块，减少目标标签噪声并提高自我监督效果。实验表明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 无监督多域适应在迁移学习中起着关键作用，它利用来自多个源域的丰富信息解决未标记的目标域任务。然而，多个源域通常包含大量冗余或无关的信息，这可能会损害迁移性能，特别是在大规模源域设置下。因此，迫切需要开发有效策略来从大规模源域中识别并选择最可迁移的知识以应对目标任务。

Method: 提出了名为AutoS（自主源知识选择）的方法，该方法通过基于密度的选择策略在训练期间选择源样本，并确定哪些源模型应该对目标预测做出贡献。同时，使用基于预训练多模态模型构建的伪标签增强模块来减轻目标标签噪声并改善自我监督。

Result: 在真实世界数据集上的实验表明，所提出的方法优于现有方法。

Conclusion: 本文提出的AutoS方法为无监督多域适应提供了一种新途径，通过智能地选择更具相关性和可转移性的源信息及模型，显著提高了处理未标记目标域任务的能力。

Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.

</details>


### [35] [Topological Metric for Unsupervised Embedding Quality Evaluation](https://arxiv.org/abs/2512.15285)
*Aleksei Shestov,Anton Klenitskiy,Daria Denisova,Amurkhan Dzagkoev,Daniil Petrovich,Andrey Savchenko,Maksim Makarenko*

Main category: cs.LG

TL;DR: 本研究提出了一种基于持续同调的拓扑感知度量方法——Persistence，用于量化嵌入空间的几何结构和拓扑丰富性。实验结果表明，该方法能够与下游性能高度相关，优于现有的无监督度量方法，并且支持可靠的模型和超参数选择。


<details>
  <summary>Details</summary>
Motivation: 现代表示学习越来越依赖于大规模无标签数据训练的无监督和自监督方法。尽管这些方法在任务和领域间展示了令人印象深刻的泛化能力，但如何在没有标签的情况下评估嵌入质量仍然是一个未解决的问题。

Method: 本文介绍了一种名为Persistence的新度量标准，它利用持续同调来度量嵌入空间的几何结构及拓扑多样性，完全以无监督的方式进行。与那些假设线性可分或依赖协方差结构的度量不同，Persistence能够捕捉全局性和多尺度组织特性。

Result: 通过跨多个领域的实证研究表明，Persistence始终能够与下游表现保持顶尖水平的相关性，其表现超过了现有的无监督度量，并且可以实现可靠的模型和超参数选择。

Conclusion: Persistence作为一种新的度量标准，为无监督环境下评价嵌入空间的质量提供了一个有效的方法，它不仅能够反映嵌入空间的复杂结构，还能帮助改进模型训练流程中的决策制定。

Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.

</details>


### [36] [A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour](https://arxiv.org/abs/2512.14713)
*Georges Sfeir,Stephane Hess,Thomas O. Hancock,Filipe Rodrigues,Jamal Amani Rad,Michiel Bliemer,Matthew Beck,Fayyaz Khan*

Main category: cs.LG

TL;DR: 本文提出了一种潜在类别强化学习（LCRL）模型，能够捕捉旅行决策中的个体偏好形成过程及其异质性。通过应用该模型于驾驶模拟数据集，并采用变分贝叶斯方法估计参数，研究者识别出了三种具有明显不同偏好适应策略的个体类型。


<details>
  <summary>Details</summary>
Motivation: 旅行决策过程中存在显著的个体间差异，这不仅体现在个人的基本偏好上，还体现在这些偏好随时间变化的方式上。为了更好地理解和预测这种复杂的行为模式，需要一种能够同时考虑偏好形成过程及个体间差异的方法。

Method: 提出了一个名为Latent Class Reinforcement Learning (LCRL)的新模型，并将其应用于驾驶模拟器数据集上。使用了Variational Bayes方法来估计模型参数。

Result: 研究发现了三类具有明显不同偏好调整策略的驾驶者：第一类人的偏好依赖于情境且倾向于在特定情境下进行利用；第二类人无论情境如何都坚持一种持续性的利用策略；第三类人则采取探索性策略并结合情境特定的偏好。

Conclusion: 本研究提出的LCRL模型有效地揭示了旅行决策中个体偏好的形成过程及其异质性，为理解复杂的旅行行为提供了新的视角。

Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.

</details>


### [37] [How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection](https://arxiv.org/abs/2512.14715)
*Zafaryab Haider,Md Hafizur Rahman,Shane Moeykens,Vijay Devabhaktuni,Prabuddha Chakraborty*

Main category: cs.LG

TL;DR: 本研究首次探讨了大型语言模型（用于图像描述）权重中的低位翻转如何影响其生成描述的语义意义，同时保持语法结构不变。通过提出一种基于梯度敏感性估计的可微故障分析框架BLADE，研究人员能够定位对语义至关重要的位，并通过标题级别的语义-流畅性目标进一步优化选择。


<details>
  <summary>Details</summary>
Motivation: 以往的研究表明，硬件比特翻转会让转换器在非生成任务中变得脆弱，但这些方法忽略了生成系统中的语义和语言维度。本研究旨在理解意义是如何在比特级别上被编码、分布以及可变的，揭示即使是不易察觉的低级变化也能引导生成式视觉-语言模型的高级语义。

Method: 设计了一种名为BLADE（Bit-level Fault Analysis via Differentiable Estimation）的可微故障分析框架，利用基于梯度的敏感性估计来识别对语义至关重要比特，并通过一个描述层级的语义-流畅性目标进一步精炼这些比特的选择。

Result: 研究表明，即使是非常细微的位级扰动也足以显著改变AI对世界的叙述方式，而不会破坏语法结构或流畅性。此外，还发现通过模型自身的梯度可以预测哪些比特如果受到干扰将最强烈地影响意义。

Conclusion: 这项工作不仅揭示了生成模型中意义的编码方式及其如何受低层次变化的影响，同时也为鲁棒性测试、对抗防御及可解释人工智能开辟了新的途径。

Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.

</details>


### [38] [Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox](https://arxiv.org/abs/2512.14717)
*Ziqian Bi,Danyang Zhang,Junhao Song,Chiung-Yi Tseng*

Main category: cs.LG

TL;DR: 本文对GPT-OSS模型家族及当代大型语言模型在十个不同的金融NLP任务上进行了全面评估。通过实验发现，参数较少的GPT-OSS-20B模型不仅在准确性上与更大规模的模型相当，而且在计算效率方面表现更优。研究还引入了新的效率指标来衡量模型性能和资源利用之间的权衡，为生产环境中的部署决策提供了关键见解。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在金融服务领域的快速应用，需要严格的评估框架来评价这些模型的表现、效率以及实际应用性。本研究旨在通过对比不同规模的GPT-OSS模型及其竞争对手，在多个金融相关自然语言处理任务上的表现，探讨模型大小与任务性能之间关系的传统假设是否成立。

Method: 通过对120B和20B参数版本的GPT-OSS进行广泛测试，并使用真实世界金融数据集（如Financial PhraseBank, FiQA-SA, 和FLARE FINERORD）执行情感分析、问答和实体识别等任务。此外，还开发了一套新颖的效率度量标准，用于综合考虑模型性能与资源消耗之间的平衡点。

Result: 结果显示，较小的GPT-OSS-20B模型在保持较高准确率的同时展现了出色的计算效率；具体而言，其Token效率得分达到198.4，每秒可处理159.80个token。此外，GPT-OSS系列模型在多项任务中均优于包括Qwen3-235B在内的更大规模竞争者。

Conclusion: 研究表明，通过架构创新和训练策略，小规模的GPT-OSS模型能够在大幅降低计算成本的情况下实现与大规模模型相媲美的性能，这为金融应用场景下的可持续且经济高效的LLM部署提供了一个可行路径。

Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.

</details>


### [39] [SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting](https://arxiv.org/abs/2512.14718)
*Feng Xiong,Zongxia Xie,Yanru Sun,Haoyu Wang,Jianhong Lin*

Main category: cs.LG

TL;DR: 本文提出了一种基于谱熵引导的时空依赖性建模框架SEED，旨在解决现有方法在处理多变量时间序列预测时面临的三个主要问题：强时间自相关被不相关变量干扰、softmax归一化忽略和反转负相关、变量难以感知其时间位置。通过引入依赖评估器、基于谱熵的融合器、带符号图构造器以及上下文空间提取器，SEED能够在12个真实数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力或图的方法在进行有效的多变量时间序列预测时面临几个关键问题：（a）强烈的时序自相关经常被无关变量破坏；（b）softmax归一化忽略了并逆转了负相关；（c）变量难以感知它们的时间位置。为了解决这些问题，作者提出了一个新的框架来改进模型对于空间-时序依赖性的理解与利用。

Method: 提出的SEED框架包括几个核心组件：
- 依赖评估器：使用谱熵动态地对每个变量的空间和时间依赖性进行初步评估，允许模型自适应地平衡通道独立性和通道依赖性策略。
- 基于谱熵的融合器：进一步细化评估出的依赖权重，有效区分外部因素影响与内在动力学引起的时间规律。
- 带符号图构造器：通过支持边权重的正负值来保持负相关关系，克服了传统softmax方法的局限性。
- 上下文空间提取器：利用局部上下文窗口提取空间特征，帮助变量更好地感知自身的时间位置。

Result: 在来自不同应用领域的12个真实世界数据集上的广泛实验表明，SEED实现了最先进的性能，验证了其有效性和通用性。

Conclusion: SEED提供了一种新的方法来改善多变量时间序列预测中的复杂变量间依赖关系建模，通过创新设计解决了现有技术中存在的几个关键挑战，并且在多个实际应用场景中展示了优越的表现。

Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.

</details>


### [40] [Hybrid Attribution Priors for Explainable and Robust Model Training](https://arxiv.org/abs/2512.14719)
*Zhuoran Zhang,Feng Zhang,Shangyuan Li,Yang Shi,Yuanxing Zhang,Wei Chen,Tengjiao Wang,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出了一种新的归因先验提取框架——类感知归因先验（CAP），以及其扩展版本CAP Hybrid，通过引导语言模型捕捉细粒度的类别差异来生成更加显著和区分性的归因先验，从而提高模型在全数据、少量样本和对抗性场景下的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLM）在需要低延迟和轻量级部署的任务中被广泛应用，尤其是在分类任务上。随着可解释性和鲁棒性变得越来越重要，基于解释的学习已经成为一种有效的框架，它通过在训练过程中引入基于归因的监督来发挥作用；然而，推导出通用且可靠的归因先验仍然是一个重大挑战。现有归因方法虽然能够可靠地突出显示与类别相关的标记，但往往集中在语义相似类别共享的关键词上，这限制了它们改进模型区分能力的作用。

Method: 提出了类感知归因先验(CAP)，这是一种新的归因先验提取框架，旨在指导语言模型捕捉细粒度的类别区别，并产生更显著、更具区分性的归因先验。此外，还进一步介绍了CAP Hybrid，它结合了来自CAP和其他现有归因技术的先验，形成一个更为全面而平衡的监督信号。

Result: 广泛的实验表明，在全数据集、少量样本和对抗性情境下，该方法一致地提高了模型的可解释性和鲁棒性。

Conclusion: 通过采用CAP及CAP Hybrid方法，研究成功地增强了小型语言模型在多种情境下的性能，特别是在提升模型决策过程中的特征多样性以及面对挑战时保持稳健的能力方面。

Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.

</details>


### [41] [Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example](https://arxiv.org/abs/2512.14721)
*Arno Appenzeller,Nick Terzer,André Hohmeyer,Jan-Philipp Redlich,Sabine Luttmann,Friedrich Feuerhake,Nadine S. Schaadt,Timm Intemann,Sarah Teuber-Hanselmann,Stefan Nikolin,Joachim Weis,Klaus Kraywinkel,Pascal Birnstill*

Main category: cs.LG

TL;DR: 本文介绍了一种基于表格数据统计自动生成Synthea规则的方法，用于合成胶质母细胞瘤的数据集。生成的合成数据能够再现已知疾病进程，并保留大部分统计特性，为隐私保护研究提供了潜在价值。


<details>
  <summary>Details</summary>
Motivation: 生成合成数据是使医疗数据能够以符合隐私要求的方式进行二次利用的一种有前景的技术。虽然现有的Synthea数据生成器能基于规则创建逼真的患者数据，但这些规则的创建过程复杂且需要专业知识和现实样本数据。为此，文章旨在提出一种自动从表格数据中提取统计数据来生成Synthea规则的新方法。

Method: 作者们提出了一个新方法，通过从癌症报告中抽取的表格数据统计信息来自动化生成Synthea规则。他们还特别针对胶质母细胞瘤开发了一个Synthea模块，并使用真实世界的数据集生成了相应的合成数据集。

Result: 与原始数据集相比，所生成的合成数据成功重现了已知的疾病进程，并在很大程度上保持了统计属性。这表明该方法生成的合成数据具有良好的实用性和可靠性。

Conclusion: 合成患者数据在隐私保护研究方面展现出巨大潜力，可用于假设制定及原型开发。然而，在医学解释时仍需考虑其特定局限性，正如对待任何现有方法一样。

Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.

</details>


### [42] [HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers](https://arxiv.org/abs/2512.14722)
*Mohamed Malhou,Ludovic Perret,Kristin Lauter*

Main category: cs.LG

TL;DR: 本文通过应用分层注意力变换器（HATs）来计算Groebner基，改进了之前使用变换器处理多元多项式方程组的方法。HAT架构利用树状归纳偏置有效建模数据中的层次关系，相较于传统的平面注意力模型大幅节省了计算成本，并且能够解决比先前工作更大规模的问题实例。


<details>
  <summary>Details</summary>
Motivation: 为了解决计算Groebner基时遇到的效率问题，尤其是当面对大规模的数据集时。研究者们希望通过引入具有树结构归纳偏好的分层注意力变换器(Hierarchical Attention Transformers, HATs)，以更有效地捕捉数据间的层次关系，从而实现对传统方法在处理速度和能力上的超越。

Method: 采用了一种名为分层注意力变换器（Hierarchical Attention Transformers, HATs）的新方法来计算Groebner基。该方法通过引入树状结构的归纳偏置，能够更好地模拟存在于数据中的层级关系，进而达到与传统扁平注意力模型相比显著减少计算开销的目的。此外，还结合了课程学习技术进一步优化解题过程。

Result: 实验结果表明，所提出的方法不仅在计算效率上优于现有方案，而且能够成功求解比Kera等人(2024)研究中更大规模的问题实例。

Conclusion: 通过引入HATs及相应优化策略，本研究为基于Groebner基的大规模多元多项式方程组求解提供了一个高效的新途径。

Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)

</details>


### [43] [A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications](https://arxiv.org/abs/2512.14727)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Lisa Koch,Christian F. Baumgartner,Michael Muehlebach*

Main category: cs.LG

TL;DR: 本文探讨了在医疗领域中，尽管共形预测（CP）能为任意大小的校准样本提供统计保证，但这些保证的实际效用高度依赖于校准集的规模。研究通过一个医学图像分类任务的实证演示来支持这一观点。


<details>
  <summary>Details</summary>
Motivation: 作者质疑共形预测(CP)能够在任意大小的校准样本下提供实用统计保证的说法，特别是在数据稀缺的医疗领域，小规模校准集可能无法达到预期效果。

Method: 通过对医学图像分类任务进行实证分析，展示了即使共形预测理论对任意大小的校准样本有效，但在实际应用价值受校准集大小显著影响。

Result: 研究结果表明，在医疗领域里，当只有小规模校准集可用时，共形预测提供的不确定性估计的实用性确实受限。

Conclusion: 共形预测虽然能够为不同大小的校准样本提供统计学上的保证，但在其实际应用价值在很大程度上取决于所使用校准集的大小；对于数据有限的情况，如医疗领域，这可能构成挑战。

Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.

</details>


### [44] [A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems](https://arxiv.org/abs/2512.14728)
*Jie He,Yong Qin,Jianyuan Guo,Xuan Sun,Xuanchuan Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种基于AFC和AVL系统数据的全数据驱动方法，用于推断城市轨道交通中的个人出行轨迹。通过建立基于时空约束的列车备选集、数据驱动自适应轨迹推断以及行程轨迹构建，实现了高精度的乘客轨迹推断。采用KL散度结合EM算法的数据驱动参数估计法（KLEM）提升了模型的鲁棒性和适用性。使用真实个体出行轨迹数据进行验证的结果表明，在高峰时段，该方法的城市轨道交通出行轨迹推断准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 精细化的城市轨道交通轨迹推断对于运营组织具有重要意义。

Method: 开发了一种完全数据驱动的方法来推断城市轨道交通系统中个人的出行轨迹，利用自动售检票(AFC)和自动车辆定位(AVL)系统的数据来推断关键轨迹元素如选定列车、进出站时间及换乘时间等。提出了基于KL散度与EM算法相结合的数据驱动参数估计方法(KLEM)，消除了对外部或调查数据进行参数拟合的依赖。

Result: 所开发的方法能够实现高精度的乘客轨迹推断，在高峰时段的城市轨道交通出行轨迹推断准确率达到90%以上。

Conclusion: 本研究提出的全数据驱动方法有效提高了城市轨道交通中个人出行轨迹推断的准确性，并且通过实际数据验证了其在高峰时段应用的有效性。

Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.

</details>


### [45] [Semantic Geometry for policy-constrained interpretation](https://arxiv.org/abs/2512.14731)
*Nikit Phadke*

Main category: cs.LG

TL;DR: 提出了一个几何框架，用于策略约束的语义解释，确保在高风险领域中不会产生幻觉承诺。通过将语义意义、证据以及可接受的解释表示为单位球面上的方向或区域，并引入策略约束作为独立于证据几何学的先验，该框架能够处理矛盾或政策排除的情况。此方法与信息理论、贝叶斯推理和层论语义相关联，证明了其复杂度边界是信息理论上最优的。大规模受监管金融数据上的实证验证表明，在多种政策制度下实现了零幻觉批准。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域内，避免语义解释过程中出现幻觉承诺是非常重要的。为此，需要一种方法来确保解释符合特定的策略约束，同时保持对证据的准确解读。

Method: 采用几何方法，将语义意义映射到单位球面的方向上，证据则被建模为见证向量集，而可接受的解释对应于球面凸区域。策略约束被定义为与证据几何相分离的同一流形上的明确先验。解释过程转化为在允许区域内进行受限优化，当遇到矛盾或违反政策时，拒绝成为必要结果。

Result: 建立了与信息理论、贝叶斯推断及层论语义之间的联系，并证明了所提出的复杂性界限达到了信息理论上的最优。基于大规模受监管金融数据集的经验验证显示，在多个政策体系下成功实现了无幻觉批准的目标。

Conclusion: 本研究提出了一种新的几何框架，有效防止了高风险领域中的幻觉承诺问题，并且在实际应用中表现出了良好的效果。

Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.

</details>


### [46] [Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness](https://arxiv.org/abs/2512.14734)
*Qiang Chen,Venkatesh Ganapati Hegde,Hongfei Li*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级、与模型无关的方法，通过在推理时选择性地注入最近的观看历史来实现日内的个性化推荐，无需重新训练模型。这种方法能够即时适应用户偏好的变化，并显著提高了用户参与度指标。


<details>
  <summary>Details</summary>
Motivation: 现有的长视频流媒体推荐系统依赖于批量训练的模型和每日更新的特征，无法及时反映用户的最新行为，导致推荐内容过时。为了克服这一局限，研究者们希望开发一种能够在日内就根据用户最新行为进行调整的个性化推荐方法。

Method: 该文介绍了一种新颖且轻量级的方法，它能够在不重新训练模型的情况下，通过选择性地覆盖陈旧的用户特征以利用最近的观看历史记录，从而实现日内个性化。

Result: 实验结果表明，通过将个性化反馈循环从每天缩短到日内，关键用户参与度指标有了统计学上显著的0.47%增长，这是近期实验周期中观察到的最大参与度提升之一。

Conclusion: 这是首次有证据表明，在长视频流媒体服务中实施日内个性化能够产生重要影响，为需要重新训练模型的全实时架构提供了一个吸引人的替代方案。

Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.

</details>


### [47] [NoveltyRank: Estimating Conceptual Novelty of AI Papers](https://arxiv.org/abs/2512.14738)
*Zhengxu Yan,Han Li,Yuming Feng*

Main category: cs.LG

TL;DR: 本项目旨在开发一种模型，用于评估和排名AI论文的概念新颖性，通过文章的标题、摘要及与先前文献的语义相似度来评价新颖性。我们探索了两种任务形式：（1）二分类任务，预测论文的绝对新颖性；（2）成对新颖性比较，学习区分论文相对于其他论文的新颖性。我们使用Qwen3-4B-Instruct-2507和SciBERT微调这两种任务，并以GPT-5.1为基准进行性能分析。


<details>
  <summary>Details</summary>
Motivation: 随着学术出版变得越来越容易，特别是在AI相关领域，研究论文的数量急剧增加。这使得真正新颖且有影响力的工作难以脱颖而出，而手动评估新颖性往往不稳定且耗时。因此，需要一个数据驱动且可扩展的方式来评估研究的原创性，帮助研究人员高效识别出真正创新的想法，并为会议审稿人提供关于新颖性的定量一致信号。

Method: 我们的方法主要通过评估论文的标题、摘要以及与之前文献的语义相似度来衡量新颖性。基于新颖性评估的目标，我们探讨了具有不同建模目标的两种任务形式：一是二分类任务，它从先前新颖作品的学习模式中预测论文的绝对新颖性；二是成对新颖性比较，该任务学习根据相对新颖性来区分论文。

Result: 我们对Qwen3-4B-Instruct-2507和SciBERT在两个任务上进行了微调，并与GPT-5.1进行了基准测试，以分析任务形式和建模选择如何影响性能。

Conclusion: 所提出的系统能够有效地帮助识别出具有真正创新想法的研究提交，同时也为审稿人提供了关于论文新颖性的量化指标。

Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.

</details>


### [48] [Guided Discrete Diffusion for Constraint Satisfaction Problems](https://arxiv.org/abs/2512.14765)
*Justin Jung*

Main category: cs.LG

TL;DR: The paper introduces a method called discrete diffusion guidance for solving constraint satisfaction problems, with Sudoku puzzles being used as an example to show the method's effectiveness in unsupervised learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a novel approach that can solve complex constraint satisfaction problems, such as Sudoku, without the need for labeled data or supervision, which could be more efficient and applicable to a wider range of problems.

Method: The method involves using discrete diffusion guidance, which is a technique that likely pertains to a form of probabilistic or heuristic search, to explore the solution space of CSPs effectively and find solutions that satisfy all constraints.

Result: The result is that the proposed method successfully solves Sudoku puzzles, demonstrating its potential for tackling other constraint satisfaction problems in an unsupervised manner.

Conclusion: The conclusion would be that discrete diffusion guidance shows promise as an effective strategy for solving CSPs, particularly when supervised data is not available, and it opens up new possibilities for unsupervised problem-solving in similar domains.

Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.

</details>


### [49] [Evaluating Weather Forecasts from a Decision Maker's Perspective](https://arxiv.org/abs/2512.14779)
*Kornelius Raeth,Nicole Ludwig*

Main category: cs.LG

TL;DR: 本研究引入了决策校准框架来评估预报性能，重点关注决策层面而非预报层面。通过比较机器学习与经典数值天气预报模型在不同天气依赖的决策任务中的表现，发现预报水平的模型性能并不总是能可靠地转化为下游决策制定中的性能，且不同决策任务中模型排名可能会发生变化。


<details>
  <summary>Details</summary>
Motivation: 传统天气预报评估主要从预报员的角度出发，进行统计性评估，但实际应用中预报被用来支持决策。因此，有必要从决策者的角度出发，以预报改善决策的能力作为评价其价值的标准。

Method: 采用决策校准的新框架，对比分析机器学习和经典数值天气预测模型在多种天气相关决策任务上的表现。

Result: 研究发现，仅在预报层面上表现出色的模型未必能在后续的实际决策过程中同样有效；有些性能差异只有在决策层面才显现出来，并且对于不同的决策任务，模型之间的优劣排序也可能发生改变。

Conclusion: 传统的预报评估方法不足以选出对特定决策任务最优的预报模型。

Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.

</details>


### [50] [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699)
*Qiuyang Mang,Wenhao Chai,Zhifei Li,Huanzhi Mao,Shang Zhou,Alexander Du,Hanchen Li,Shu Liu,Edwin Chen,Yichuan Wang,Xieting Chu,Zerui Cheng,Yuan Xu,Tian Xia,Zirui Wang,Tianneng Shi,Jianzhu Yao,Yilong Zhao,Qizheng Zhang,Charlie Ruan,Zeyu Shen,Kaiyuan Liu,Runyuan He,Dong Xing,Zerui Li,Zirong Zeng,Yige Jiang,Lufeng Cheng,Ziyi Zhao,Youran Sun,Wesley Zheng,Meiyuwang Zhang,Ruyi Ji,Xuechang Tu,Zihan Zheng,Zexing Chen,Kangyang Zhou,Zhaozi Wang,Jingbang Chen,Aleksandra Korolova,Peter Henderson,Pramod Viswanath,Vijay Ganesh,Saining Xie,Zhuang Liu,Dawn Song,Sewon Min,Ion Stoica,Joseph E. Gonzalez,Jingbo Shang,Alvin Cheung*

Main category: cs.LG

TL;DR: 本文介绍了一个名为FrontierCS的基准测试，它包含156个开放性问题，涵盖了计算机科学的多个领域。这些问题由专家设计并审查，旨在解决那些最优解未知但解决方案质量可以客观评估的问题。与现有基准不同的是，FrontierCS侧重于通过实现可执行程序来解决问题而非直接给出答案，并且提供了专家参考解答和自动评估工具。实验结果表明，当前前沿推理模型在算法和研究任务上仍远落后于人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注已知最优解的任务，而本文作者认为有必要创建一个能够评估尚未找到最优解但可以通过某些标准衡量其质量的问题集。这样的一个问题集合对于推动计算机科学研究边界以及评估AI系统在处理复杂问题时的能力至关重要。

Method: 研究者们开发了FrontierCS，这是一个包含156个来自不同计算机科学领域的开放性问题的集合。每个问题都配有专家提供的参考解决方案及自动评分机制。该方法强调通过编写可运行的程序来完成任务，而不是简单地输出答案。

Result: 实验结果显示，即使是最先进的推理模型，在面对FrontierCS中的算法与研究挑战时，其表现也远远不及人类专家。此外，单纯增加计算资源并不能有效缩小这一差距；模型往往倾向于生成‘可用’代码而非探索高质量算法或系统设计方案。

Conclusion: FrontierCS为计算机科学难题设立了一个新的基准，它结合了开放式设计、可度量的进步以及专家策划的优点。然而，当前的前沿模型在解决这类问题上仍存在显著不足，表明需要进一步研究以提高模型理解和创造高级算法的能力。

Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.

</details>


### [51] [Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse](https://arxiv.org/abs/2512.14879)
*Jingwei Chen*

Main category: cs.LG

TL;DR: 本文提出了一种信息几何框架——熵水库布雷格曼投影（ERBP），用于统一并解释自引用学习中的模型崩溃现象及其修复方法。通过引入高熵分布来控制熵流，从而稳定系统动力学。实验验证了该理论，并表明不同的稳定化启发式方法对应于特定的水库选择和耦合系数。


<details>
  <summary>Details</summary>
Motivation: 自引用学习尽管具有无限可扩展性，但经常遭受模型崩溃的问题，例如语言模型生成重复文本、GAN丢失模式以及强化学习策略过度利用等。尽管实践者采用了一些临时解决方案，如真实数据混合、熵奖励、知识蒸馏或增强生成检索，但尚未有一个原则可以同时解释这些问题及这些解决方法的成功之处。

Method: 作者提出了熵水库布雷格曼投影（ERBP）这一信息几何框架，将上述现象进行了统一描述。他们将闭环建模为分布空间中的随机布雷格曼投影序列。在没有外部耦合的情况下，有限样本噪声迫使系统投射到一个不断缩小的经验支持集上，导致熵指数衰减直至最终崩溃。通过引入熵水库——一种混入每次投影的高熵分布——注入可控的熵流，从理论上证明了这种方法能够稳定系统动力学。

Result: 理论分析给出了（i）崩溃发生的必要条件；(ii) 保证非零熵底限的充分条件；(iii) 只依赖于样本大小以及布雷格曼生成器的强凸/利普希茨常数的闭形式速率。实验部分，在大规模语言模型自我训练、强化学习中的软演员-评论家算法以及GAN优化等方面验证了预测结果，并展示了不同稳定化启发式方法与特定水库选择及耦合系数之间的关系。

Conclusion: ERBP将一系列民间疗法转化为单一的量化设计规则：监控并预算你的熵流。这为理解和解决自引用学习中遇到的模型崩溃问题提供了新的视角。

Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.

</details>


### [52] [Task Matrices: Linear Maps for Cross-Model Finetuning Transfer](https://arxiv.org/abs/2512.14880)
*Darrin O' Brien,Dhikshith Gajulapalli,Eric Xia*

Main category: cs.LG

TL;DR: 研究发现，通过任务矩阵增强的基础模型在视觉和文本模型以及十个不同数据集上达到了超越线性探针的结果，有时甚至接近微调水平，证明了预训练和微调架构之间存在跨层线性编码。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究指出，在上下文提示影响下，大型视觉和语言模型会学习到隐含的线性编码，但这种线性表示在更广泛的适应机制中的存在尚未得到证实。

Method: 本研究提出了任务矩阵的概念，这是一种从基础到微调嵌入状态的线性转换方法。研究人员展示了对于视觉和文本模型以及十个不同的数据集来说，使用任务矩阵增强的基础模型能够实现优于线性探测器的表现，并且有时可以达到与完全微调模型相近的效果。

Result: 实验结果验证了预训练和微调架构之间确实存在着跨层线性编码；此外，还表明基于数据的方法来近似这些编码是高效且可泛化至多个领域的。

Conclusion: 该研究表明，通过引入任务矩阵概念，能够在多种场景下有效利用预训练模型中隐藏的线性结构，从而以更低的成本获得接近于完整微调过程的性能表现。

Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.

</details>


### [53] [OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams](https://arxiv.org/abs/2512.14892)
*Mohammad Abu-Shaira,Alejandro Rodriguez,Greg Speegle,Victor Sheng,Ishfaq Ahmad*

Main category: cs.LG

TL;DR: 本文提出了一种新的在线线性回归模型OLR-WA，该模型在处理数据漂移、快速收敛以及基于置信度的情景方面表现出色，同时与批处理回归及其他先进在线模型相比具有竞争力或更优的表现。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统方法在存储需求和模型重计算成本上的局限性，论文旨在开发一种能够有效应对数据模式随时间演变（即漂移）的新型在线学习模型，并通过保守更新策略来优先考虑高置信度的历史数据点。

Method: 提出了名为OLR-WA的新颖且多功能的多元在线线性回归模型；对涉及漂移的情况进行了研究，其中数据的基本模式会随着时间而变化；进行了收敛性分析；并与现有在线回归模型进行了比较。

Result: OLR-WA展示了可与批量回归相媲美的性能，并且在与其他最先进的在线模型比较时也表现出相当甚至更好的性能。特别是在快速收敛方面，即使从非常少的数据点开始，它也能持续达到较高的r2值。此外，它是唯一能够有效管理基于置信度挑战情景的模型。

Conclusion: OLR-WA不仅在处理时间相关漂移场景上表现出色，而且通过其保守更新机制有效地处理了基于置信度的复杂情况，证明了其作为在线线性回归任务解决方案的价值和通用性。

Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.

</details>


### [54] [Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections](https://arxiv.org/abs/2512.14895)
*Niklas Lauffer,Xiang Deng,Srivatsa Kundurthy,Brad Kenstler,Jeff Da*

Main category: cs.LG

TL;DR: The paper introduces on-policy expert corrections (OECs) to address the covariate shift problem in training multi-turn language model (LM) agents, which improves performance on software engineering tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the covariate shift issue in imitation learning for multi-turn LM agents, where the student policy's behavior diverges from the expert's, leading to ineffective fine-tuning due to unseen states.

Method: The method involves a new data generation technique called on-policy expert corrections (OECs), which generates partially on-policy data by starting with a student model and switching to an expert model during the trajectory. The OECs are then used along with rejection sampling and supervised fine-tuning to train the models.

Result: The results show that using OEC trajectories leads to a 14% and 13% improvement over traditional imitation learning in 7 billion and 32 billion parameter settings, respectively, as measured on SWE-bench verified tasks.

Conclusion: The conclusion is that combining expert demonstrations with on-policy data, such as OECs, is necessary for the effective training of multi-turn LM agents, especially in complex task domains like software engineering.

Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.

</details>


### [55] [Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective](https://arxiv.org/abs/2512.14932)
*Daniel Gomes de Pinho Zanco,Leszek Szczecinski,Jacob Benesty,Eduardo Vinicius Kuhn*

Main category: cs.LG

TL;DR: 提出了一种基于Kronecker积表示的有效寻找低秩MMSE滤波器正则化参数的方法，并通过仿真验证了该方法相比常用方法有显著优势。


<details>
  <summary>Details</summary>
Motivation: 为了在低秩设置中有效找到合适的正则化参数，这与秩选择问题紧密相关。

Method: 基于Kronecker-乘积表示来确定低秩MMSE滤波器的正则化参数。

Result: 所提方法经过仿真验证，在对比常用方法时表现出显著增益。

Conclusion: 正确选择正则化参数对于低秩MMSE滤波器至关重要，本文提出的方法能更有效地实现这一目标。

Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.

</details>


### [56] [Softly Constrained Denoisers for Diffusion Models](https://arxiv.org/abs/2512.14980)
*Victor M. Yeom Song,Severi Rissanen,Arno Solin,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 本文提出了一种将引导启发式调整集成到去噪器中的方法，以软性地偏向符合约束条件的样本生成，这种方法在约束错误指定时也能保持足够的灵活性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成满足特定约束条件的样本方面存在困难，而现有的解决方法会导致生成模型偏离真实数据分布。特别是当约束条件定义不准确时，这种问题尤为明显。

Method: 研究者们不是通过改变损失函数或采样循环来解决问题，而是将一种受引导启发的调整直接集成到了去噪器中，赋予其一个软性的归纳偏置，促使生成更符合约束要求的样本。

Result: 实验表明，采用软约束去噪器的方法能够利用约束知识提高样本对约束的遵守程度，并且在遇到与观察数据不符的情况时，仍能保持一定的灵活性，从而避免过度偏差。

Conclusion: 通过将基于引导的思想融入去噪过程，而非直接修改训练目标或采样策略，该方法提供了一条新的路径来处理科学应用中常见的约束满足问题，同时保持了模型对于潜在数据分布的良好适应性。

Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.

</details>


### [57] [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
*Yaniv Leviathan,Matan Kalman,Yossi Matias*

Main category: cs.LG

TL;DR: Repeating the input prompt enhances the performance of several popular models, including Gemini, GPT, Claude, and Deepseek, without affecting the number of tokens generated or the latency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to explore methods for improving the performance of AI models without increasing computational costs or response time. This is particularly relevant as efficiency and speed are critical factors in the deployment and usability of AI technologies.

Method: The study involves a direct comparison of model performance under two conditions: with and without the repetition of the input prompt. The models chosen (Gemini, GPT, Claude, and Deepseek) represent a range of popular AI tools. Performance metrics, such as accuracy and quality of responses, were evaluated while keeping the number of generated tokens and latency constant.

Result: Results indicate that repeating the input prompt significantly improves the performance of all tested models. This improvement is achieved without any increase in the number of generated tokens or the latency, suggesting an efficient way to boost model performance.

Conclusion: The conclusion drawn from this research is that simply by repeating the input prompt, one can enhance the effectiveness of AI models like Gemini, GPT, Claude, and Deepseek, making them more powerful without compromising on speed or resource usage. This finding opens up new possibilities for optimizing AI applications.

Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.

</details>


### [58] [Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes](https://arxiv.org/abs/2512.14991)
*Hanqing Jin,Renyuan Xu,Yanzhao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于模型的自适应分区算法，用于处理具有无限连续状态空间、有界连续动作和多项式增长奖励的受控扩散过程中的强化学习问题。该方法通过在每个分区内估计漂移、波动性和奖励，并根据需要细化离散化来平衡探索与近似。理论分析表明，该方法能够对更广泛的扩散型问题提供有效的学习保证，并通过数值实验进行了验证，包括应用于多资产均值-方差投资组合选择等高维问题。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在解决金融、经济及运筹学中自然出现的、具有无限连续状态空间、有界连续动作以及多项式增长奖励的控制扩散过程中的强化学习挑战。面对连续且高维度领域带来的难题，需要开发一种新的算法来有效应对这些复杂情况。

Method: 提出了一个基于模型的算法，该算法能够自适应地划分联合状态-动作空间，并在每个划分内保持漂移、波动率和奖励的估计器。当估计偏差超过统计置信度时，会自动细化离一化程度。这种自适应方案能够在探索和近似之间取得良好平衡，使得在无边界域内的高效学习成为可能。

Result: 分析得出了取决于问题时间范围、状态维度、奖励增长顺序以及为无界扩散过程量身定制的新定义缩放维度概念的遗憾界。所得到的结果不仅复现了有限设定下的现有发现作为特殊情况，而且将理论保证扩展到了更广泛的扩散类型问题上。此外，通过数值实验进一步证实了所提方法的有效性，特别是在解决如多资产均值-方差投资组合选择这样的高维问题方面表现突出。

Conclusion: 本研究成功地开发并验证了一种适用于无限连续状态空间下强化学习任务的新颖算法框架，它不仅克服了传统方法在处理高维数据时面临的局限性，还为实际应用提供了坚实的理论基础。

Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.

</details>


### [59] [DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding](https://arxiv.org/abs/2512.15000)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: 本文提出了一种专注于编码的Process Reward Model (PRM)，称为DreamPRM-Code，它通过Chain-of-Function提示策略将函数视为推理步骤，以促进模块化代码生成，并引入基于元学习的校正机制来解决标签噪声问题。在应用测试时扩展，DreamPRM-Code在LiveCodeBench上实现了80.9%的pass@1率，超过了OpenAI o4-mini的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的Process Reward Models (PRMs)虽然对于通过测试时间缩放改进大型语言模型（LLMs）非常重要，但在编程方面的效果有限，主要因为代码中缺乏有意义的步骤分解以及蒙特卡洛生成的部分标签带来的噪音。

Method: 提出了DreamPRM-Code，一种针对编码设计的PRM，采用Chain-of-Function提示策略诱导模块化代码生成，并且开发了一个基于元学习的修正机制来利用干净的最终解决方案单元测试标签进行两级优化，从而提高中间标签的质量。

Result: 实验表明，在使用测试时间缩放的情况下，DreamPRM-Code在LiveCodeBench上的表现达到了最先进的水平，其pass@1率为80.9%，优于OpenAI的o4-mini版本。

Conclusion: 通过引入特定于编码领域的改进措施，包括新的提示策略和有效的标签噪声处理方法，DreamPRM-Code证明了能够显著提升LLMs在编程任务中的性能。

Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.

</details>


### [60] [Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets](https://arxiv.org/abs/2512.15008)
*Sandeep Neela*

Main category: cs.LG

TL;DR: 本文介绍了一种名为股票模式助手（SPA）的确定性框架，该框架旨在提取单调价格走势，通过对称相关窗口附加相关公共事件，并生成基于事实、历史和有约束条件的解释。SPA仅依赖于每日OHLCV数据和标准化事件流，使得流程易于审计和再现。通过对AAPL、NVDA、SCHW和PGR四种股票的评估表明，SPA能够持续产生稳定的结构分解和上下文叙述。


<details>
  <summary>Details</summary>
Motivation: 现有的许多用于理解价格随时间演变的工具，如技术指标、图表启发式方法或复杂的预测模型，往往不能充分回答重要问题，且缺乏透明度或可审计性。

Method: 提出了一种新的确定性框架——股票模式助手（SPA），它使用每日OHLCV数据和标准化事件流来提取单调的价格走势，并通过一个对称的相关窗口与相关的公共事件进行关联，同时生成具有事实依据的历史解释。

Result: SPA在不同波动性和行业特性的四只股票上进行了测试，结果证明了SPA能够一致地提供稳定的价格结构分解以及情境化的叙述。消融实验进一步显示了确定性分割、事件对齐和受限制的解释如何共同提高了解释性。

Conclusion: SPA作为一个非预测系统，其价值在于为历史价格结构提供了透明、可复制的观点，这可以补充分析师的工作流程、风险审查以及更广泛的可解释AI管道。

Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.

</details>


### [61] [Epistemic diversity across language models mitigates knowledge collapse](https://arxiv.org/abs/2512.15011)
*Damian Hodel,Jevin D. West*

Main category: cs.LG

TL;DR: 研究发现，在AI模型生态系统中增加认知多样性可以减缓知识崩溃，但这种效果存在一个最佳水平。太少的多样性或太多的模型都会导致性能下降。建议监测AI系统的多样性，并鼓励开发更多领域和社区特定的模型。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）使用的增长，人们越来越担心会出现知识崩溃的问题，即想法变得过于单一化。先前的研究已经证明了单个模型在训练于自身输出时性能会下降。受生态学启发，本研究探讨了模型之间的多样性是否能够缓解这种崩溃现象。

Method: 研究者们构建了一个基于多个模型组成的生态系统，这些模型是在它们集体输出的基础上进行训练的。通过将训练数据分割给不同的语言模型，并在十个自我训练迭代周期内评估由此产生的生态系统的效果，来研究多样性对模型表现的影响。

Result: 研究结果表明，增加认识上的多样性确实有助于减轻知识崩溃的现象，但是这种正面影响只在达到某个最优水平之前有效。如果生态系统中只有少数几个多样化的模型，则无法充分表达出完整的真实分布，从而导致性能迅速下降；而如果把数据分散到过多的模型上，又会导致每个模型对于真实分布的近似能力降低，在第一个迭代步骤中就表现出较差的表现。

Conclusion: 为了防止AI系统中的知识崩溃，需要对跨AI系统的多样性进行监控，并制定政策激励创建更多针对特定领域和社区的模型。

Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.

</details>


### [62] [The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems](https://arxiv.org/abs/2512.15068)
*Debu Sinha*

Main category: cs.LG

TL;DR: 本文研究了基于检索的生成系统（RAG）中存在的幻觉问题，并提出了一种新的检测方法，即应用保形预测来提供有限样本覆盖保证，从而精确量化检测能力。尽管在合成数据上取得了高覆盖率和零假阳性率的好成绩，但在真实世界的数据集上，基于嵌入的方法表现不佳，显示出较高的假阳性率。GPT-4作为大型语言模型法官，在相同任务上的假阳性率显著降低，证明通过推理可以解决这个问题。作者将此现象称为“语义幻象”，指出基于嵌入的检测方法对于生产环境中RAG系统的部署是不够充分的。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检索增强生成(RAG)系统虽然依赖于检索到的证据，但仍然容易产生幻觉。当前用于检测这些幻觉的方法主要依靠语义相似性和自然语言推理(NLI)，但它们的基本局限性尚未得到严格定义。因此，需要一种更有效的检测手段来准确地识别出由RAG系统产生的幻觉内容。

Method: 本文采用了保形预测技术应用于幻觉检测中，旨在为检测能力提供有限样本覆盖保证。通过使用约600个例子组成的校准集进行测试，对比了不同方法包括最新版OpenAI文本嵌入3大模型和交叉编码器模型在合成与实际幻觉基准测试中的表现。

Result: 实验结果显示，在自然问题数据集上对合成幻觉实现了94%的覆盖率且无假阳性；然而，在跨越多个大型语言模型的真实幻觉基准测试中，基于嵌入的方法表现出极高的假阳性率。相反地，GPT-4作为评判标准时，其假阳性率仅为7%，表明该任务可通过推理有效解决。

Conclusion: 研究表明，存在一种‘语义幻象’现象，即看似合理的幻觉保持了与源文档的相似性，同时引入了嵌入方法无法察觉的事实错误。这表明，无论是在不同的嵌入架构、大型语言模型生成器还是任务类型下，单纯依赖基于嵌入的方法不足以支持生产级RAG系统的部署。

Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.

</details>


### [63] [The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks](https://arxiv.org/abs/2512.15082)
*Wanfu Gao,Zebin He,Jun Gao*

Main category: cs.LG

TL;DR: 提出了一种新的自动特征工程方法FEAML，专为多标签分类任务设计，利用大型语言模型（LLMs）的代码生成功能，并通过反馈机制不断优化生成的特征。实验结果显示FEAML在多个多标签数据集上的表现优于其他特征工程方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的特征工程方法尚未应用于多标签学习任务中，缺乏建模复杂标签依赖关系的能力，并且没有专门针对多标签任务的特点进行调整。

Method: 提出了Feature Engineering Automation for Multi-Label Learning (FEAML)，一种用于多标签分类的自动化特征工程技术，它利用了大型语言模型的代码生成功能。该方法通过使用元数据和标签共现矩阵来指导LLM理解数据特征与任务目标之间的关系，从而生成高质量特征。新生成的特征根据模型准确性评估其有效性，并使用皮尔逊相关系数检测冗余性。此外，FEAML将评估结果作为反馈，推动LLM在后续迭代中持续优化代码生成。

Result: 在不同多标签数据集上的实证研究显示，所提出的FEAML方法比其他特征工程方法表现更好。

Conclusion: 通过结合大型语言模型与反馈机制，FEAML实现了高效、可解释且自我改进的特征工程范式，特别适用于解决多标签分类问题中的挑战。

Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.

</details>


### [64] [FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation](https://arxiv.org/abs/2512.15116)
*Runze Li,Hanchen Wang,Wenjie Zhang,Binghao Li,Yu Zhang,Xuemin Lin,Ying Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的框架FADTI，通过可学习的傅里叶偏置投影模块注入频率信息特征调制，并结合自注意力和门控卷积进行时序建模。实验表明，FADTI在多种基准测试中，尤其是在高缺失率下，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列插补在医疗保健、交通预测和生物建模等领域至关重要，但现有的基于Transformer和扩散模型的方法缺乏明确的归纳偏置和频率意识，这限制了它们在结构化缺失模式和分布变化下的泛化能力。

Method: 提出了FADTI（Frequency-Aware Diffusion-based Time series Imputation），该框架通过一个可学习的傅里叶偏置投影(FBP)模块引入频率信息特征调节，并与通过自注意力机制和门控卷积实现的时间建模相结合。FBP支持多个频谱基，能够适应性地编码平稳和非平稳模式。

Result: 在多个基准数据集上进行了实验，包括一个新引入的生物时间序列数据集，结果显示FADTI在高缺失率情况下始终优于最先进方法。

Conclusion: FADTI通过引入频域归纳偏置到生成插补过程中，显著提高了多变量时间序列插补任务中的性能，特别是在面对高比例数据缺失时。

Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF

</details>


### [65] [Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany](https://arxiv.org/abs/2512.15140)
*Roland Baatz*

Main category: cs.LG

TL;DR: 本研究评估了用于预测德国NUTS-3地区作物产量和产量异常的机器学习模型（如XGBoost、随机森林及深度学习方法LSTM、TCN）的泛化性能和可解释性。尽管这些模型在传统测试集上表现良好，但在时间独立验证年份上的表现显著下降，暴露出泛化能力的持续局限。此外，即使基础模型未能很好地泛化，某些模型仍能产生看似可信的SHAP特征重要性值，揭示了解释性方法中的一个重要漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决农业与环境系统中机器学习预测的验证意识解读问题，并强调只有当模型明确显示出对未见的时间和空间条件具有泛化能力时，才能接受其特征重要性的表面价值。

Method: 采用高质量长期数据集，系统地比较了基于集成树的方法（XGBoost, 随机森林）和深度学习方法（LSTM, TCN）在评估与时间验证行为方面的表现。

Result: 所有模型在空间分割的传统测试集上表现良好，但它们在时间独立验证年份上的性能大幅下降；同时发现，即使模型泛化能力弱，也能生成看似可靠的SHAP特征重要性值。

Conclusion: 需要采取领域感知验证、混合建模策略以及加强对农业数据驱动中解释方法的严格审查。最终目标是找到一种足够稳健的方法来评估泛化能力，以确保能够信任模型解释。

Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?

</details>


### [66] [An Efficient Gradient-Based Inference Attack for Federated Learning](https://arxiv.org/abs/2512.15143)
*Pablo Montaña-Fernández,Ines Ortega-Fernandez*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于梯度的成员推理攻击方法，适用于联邦学习场景。该方法利用最后一层梯度在多个联邦回合中的时间演变，并通过影子技术学习训练记录的轮次梯度模式，无需访问私有数据集。此外，还提供了对离散属性推理攻击的自然扩展。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习减少了直接的数据暴露，提高了机器学习模型的隐私保证，但参与者与聚合者之间交换的模型更新仍可能泄露敏感信息。为了应对这一挑战，研究者们提出了一种新的针对联邦学习情境下的基于梯度的成员推理攻击方法。

Method: 采用一种新颖的方法来利用跨多个联邦回合中最后层梯度的时间演变。通过影子技术学习训练记录的每轮梯度模式，且不需要访问私人数据集。此外，该方法还能扩展到通过对不同属性假设下梯度响应进行对比来进行离散属性推断。

Result: 实验结果表明，所提出的攻击方法在成员推断方面表现出强大的性能，在计算和内存开销上与文献中的另一种攻击相当。研究发现多轮联邦学习可能会增加对推断攻击的脆弱性，聚合者比数据所有者构成更大的威胁，而且训练数据集的性质对攻击性能有很大影响，丰富、高维的数据会导致更强的信息泄露。

Conclusion: 这项研究表明，即使是在联邦学习这样的隐私保护框架内，仍然存在潜在的安全风险。特别地，随着联邦学习轮数的增加，系统对推理攻击变得更加脆弱。因此，未来的工作需要进一步探索如何在不牺牲模型性能的情况下加强隐私保护措施。

Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.

</details>


### [67] [Understanding NTK Variance in Implicit Neural Representations](https://arxiv.org/abs/2512.15169)
*Chengguang Ou,Yixin Zhuang*

Main category: cs.LG

TL;DR: 本文通过分析几种常见的INR组件对神经切线核(NTK)特征值方差的影响，揭示了不同INR架构如何通过改善NTK条件来缓解频谱偏置问题。实验表明，这些方法能够减少预测的方差，从而实现更快更稳定的收敛和更好的重建质量。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）经常由于频谱偏置而收敛缓慢且难以恢复高频细节。尽管先前的工作将这种现象与神经切线核（NTK）联系起来，但特定架构选择如何影响NTK条件仍不清楚。

Method: 研究者们展示了多种INR机制可以通过它们对一小部分成对相似性因子和缩放项的影响来理解，这些因子共同决定了NTK特征值方差。通过对常见INR组件进行封闭形式的方差分解，研究指出位置编码重塑输入相似性、球面标准化通过逐层缩放减少方差、以及哈达玛调制引入额外的低于1的相似性因子以达到乘法方差减少的效果。

Result: 研究表明，不同的INR架构通过改进NTK条件来减轻频谱偏置。在多个任务上的实验验证了预期的方差减少，并显示出更快、更稳定的收敛性和提高的重建质量。

Conclusion: 本研究提供了一个统一的观点来解释各种INR架构如何通过改善NTK条件来解决频谱偏置问题，进而促进模型的快速稳定收敛及高质量重建。

Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.

</details>


### [68] [DEER: Draft with Diffusion, Verify with Autoregressive Models](https://arxiv.org/abs/2512.15176)
*Zicong Cheng,Guo-Wei Yang,Jia Li,Zhijie Deng,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新的高效推测解码框架DEER，通过使用扩散大语言模型作为起草者和自回归模型进行验证，克服了现有方法中的不确定性累积和顺序解码问题，实现了高达32个token的草稿接受长度，并在HumanEval测试中与Qwen3-30B-A3B配合使用时达到了5.54倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法依赖于自回归（AR）起草模型，这导致了逐步积累的不确定性以及固有的顺序解码限制，从而限制了速度提升。因此，需要一种新的方法来提高LLM驱动系统中的解码效率。

Method: 作者引入了DEER，这是一种利用扩散大语言模型(dLLM)作为起草者、自回归模型作为验证者的高效推测解码框架。该框架采用两阶段训练流程来对齐基于dLLM的起草者与目标AR模型，并采用了单步解码技术生成长段草稿文本。

Result: 实验结果显示，DEER能够达到高达32个token的草稿接受长度，远超EAGLE-3实现的10个token。此外，在HumanEval测试中结合Qwen3-30B-A3B使用时，DEER实现了5.54倍的速度提升，而EAGLE-3仅能达到2.41倍。

Conclusion: DEER通过引入扩散大语言模型作为起草者并结合自回归模型验证的方法，有效解决了现有推测解码方案中存在的两大根本性问题，显著提高了解码效率。

Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/

</details>


### [69] [Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT](https://arxiv.org/abs/2512.15206)
*Liyu Zhang,Yejia Liu,Kwun Ho Liu,Runxi Huang,Xiaomin Ouyang*

Main category: cs.LG

TL;DR: 提出了一种名为Chorus的方法，该方法能够自适应地调整模型以应对未见过的部署条件，并且不需要目标域的数据。通过学习有效的上下文表示并根据上下文变化程度进行自适应集成，Chorus在多种传感任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物联网应用中，传感器数据受到多变环境因素的影响，而传统的方法往往忽略了这些上下文信息或采用简单的集成策略，导致处理部署后未预见的上下文变化时效果不佳。

Method: Chorus首先执行无标签传感器数据和基于语言的上下文嵌入之间的无监督跨模态重建，并对上下文嵌入空间进行正则化以学习鲁棒且可泛化的上下文表示；接着训练一个轻量级门控头来动态平衡传感器与上下文贡献；最后引入上下文缓存机制减少推理延迟。

Result: 实验表明，在IMU、语音和WiFi感知任务下，面对多样化的上下文变化，Chorus相比最先进基准方法在未见上下文中最高提高了11.3%的表现，同时保持了智能手机和边缘设备上的较低延迟。

Conclusion: Chorus为解决IoT应用场景下的模型自适应问题提供了新的视角，通过利用上下文信息有效提升了模型对于未知部署条件的适应能力。

Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.

</details>


### [70] [O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization](https://arxiv.org/abs/2512.15229)
*Elio Gruttadauria,Mathieu Fontaine,Jonathan Le Roux,Slim Essid*

Main category: cs.LG

TL;DR: 本文提出了一种基于EEND-EDA的端到端在线说话人日志系统O-EENC-SD，它采用了新颖的基于RNN的拼接机制来进行在线预测，并开发了新的中心点精炼解码器。该系统在无需超参数调整的情况下优于无监督聚类方法，并且比现有的在线端到端方法更高效。实验结果表明，O-EENC-SD在CallHome数据集上与最先进水平相当，并且在处理独立片段时提供了一个很好的错误检测率（DER）和复杂性之间的折衷。


<details>
  <summary>Details</summary>
Motivation: 为了改进现有的在线说话人日志技术，特别是解决无监督聚类方法需要调优超参数以及当前在线端到端方法计算成本高的问题。

Method: 提出了一个名为O-EENC-SD的新系统，该系统基于EEND-EDA，并引入了基于RNN的拼接机制用于在线预测；同时设计了一个新的中心点精炼解码器来改善预测性能。

Result: O-EENC-SD系统在两个说话人的对话电话语音领域中表现良好，在CallHome数据集上的测试结果表明其能够很好地平衡错误检测率（DER）和计算复杂度。

Conclusion: O-EENC-SD作为一种高效且性能优越的在线说话人日志解决方案，为未来的研究提供了新的方向。

Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.

</details>


### [71] [Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis](https://arxiv.org/abs/2512.15250)
*Youssef Ghallab,Omar Iraqy,Mohamed Kandil,Mohamed Ashraf,Saadeldine Eletter,Morougue Ghazal,Ayman Khalafallah,Nagwa El-Makky*

Main category: cs.LG

TL;DR: 本研究通过适应CBraMod编码器进行大规模自监督ECG预训练，并引入双掩码策略来捕捉导联内和导联间依赖性，克服了由于多模态标记数据有限以及模态特定差异带来的挑战。利用预训练的CBraMod编码器处理EEG，并为ECG预训练一个对称编码器，使每个模态都具有丰富的基础表示。这些表示通过简单的嵌入级联融合，使得分类头能够学习跨模态交互，从而在有限的多模态监督下实现有效的下游学习。实验结果表明，在情绪识别上该方法接近最先进水平，显示出精心设计的生理编码器即使采用直接融合也能显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 针对生理信号如心电图（ECG）和脑电图（EEG）提供的互补信息难以整合的问题，尤其是当面临多模态标记数据稀缺及不同模态间存在差异时，提出了一种新的解决方案。

Method: 采用了CBraMod编码器进行大规模自监督ECG预训练，并引入双掩码策略；同时，使用预训练的CBraMod编码器处理EEG并为ECG预训练一个对称编码器，通过简单的嵌入连接融合两种模态的表示，让分类头部能够学习到跨模态间的相互作用。

Result: 在情绪识别任务上，所提出的方法达到了接近最先进水平的表现，证明了即使采用简单的融合技术，经过精心设计的生理信号编码器也能够显著提高下游任务的表现。

Conclusion: 研究表明，通过基础模型方法可以有效利用生理信号的整体性质，提供可扩展、标签高效且泛化能力强的健康护理和情感计算解决方案。

Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.

</details>


### [72] [A Regime-Aware Fusion Framework for Time Series Classification](https://arxiv.org/abs/2512.15378)
*Honey Singh Chauhan,Zahraa S. Abdallah*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级框架Fusion-3 (F3)，该框架自适应地融合了Rocket、Sax和Sfa表示方法，用于单变量时间序列分类。通过将UCR数据集根据元特征聚类成六个组别，并分析在不同数据结构环境下融合方法的表现，发现对于具有结构性变化或丰富频域内容的数据集，融合方法通常优于强基线。此外，研究还结合了非参数配对统计、消融研究以及SHAP归因等方法，揭示了融合如何通过纠正特定错误来提高性能。实验结果表明，在113个UCR数据集上使用5折交叉验证时，F3相比Rocket有小但一致的改进。


<details>
  <summary>Details</summary>
Motivation: 尽管基于内核的方法如Rocket是处理单变量时间序列分类的有效手段之一，但它们在不同数据集上的表现并不均衡。本文旨在探索不同的表示方法是否能够捕捉到互补的信息结构，并且通过选择性地融合这些表示方法能否在系统可识别的特定类型数据集上带来一致性的提升。

Method: 提出了Fusion-3 (F3)框架，该框架能够自适应地整合Rocket, Sax, 和 Sfa三种表示方法。通过对UCR数据集进行元特征分析并将其划分为六个类别（考虑了序列长度、频谱结构、粗糙度及类别不平衡等因素），研究者们确定了哪些数据集结构适合应用融合策略。进一步地，采用非参数配对统计、消融研究以及SHAP值分析等方法探讨了融合技术如何改善模型性能。

Result: 实验结果显示，在具有结构性变化或频率成分丰富的数据集上，F3相较于单独使用Rocket或其他表示方法表现出更好的性能；而在非常不规则或者异常值较多的情况下，融合带来的增益则相对较小。利用5折交叉验证评估113个UCR数据集后发现，F3相比Rocket获得了轻微但稳定的整体提升。

Conclusion: 选择性地应用表示方法的融合可以为强大的基于内核的方法提供可靠且可解释性的扩展，特别是在数据支持的地方纠正其弱点。

Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.

</details>


### [73] [Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection](https://arxiv.org/abs/2512.15385)
*Julian Oelhaf,Mehran Pashaei,Georg Kordowich,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 本文提出了一种统一框架，用于系统地评估电力系统保护中机器学习模型的鲁棒性。通过高保真EMT仿真模拟实际退化场景，并为未来的鲁棒性感知设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和分布式发电的普及，传统的基于固定设置和本地测量的保护方案面临挑战。虽然机器学习提供了一个数据驱动的替代方案，但其实用部署取决于其鲁棒性，即在面对缺失、噪声或劣化的传感器数据时仍需保持可靠。

Method: 研究引入了一个统一框架来系统地评估电力系统保护中ML模型的鲁棒性，使用高保真的电磁暂态(EMT)仿真来建模现实中的退化情景，包括传感器故障、采样率降低以及短暂通信丢失等。该框架提供了一种一致的方法来比较不同模型的表现，量化有限可观测性的影响，并识别对于实现韧性运行至关重要的测量通道。

Result: 结果表明，在大多数退化类型下故障分类(FC)保持了高度稳定，但在单相损失的情况下性能下降约13%；而故障定位(FL)总体上更为敏感，电压损失使得定位误差增加了超过150%。

Conclusion: 这些发现为未来考虑鲁棒性的ML辅助保护系统的设计提供了可操作的指导。

Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.

</details>


### [74] [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](https://arxiv.org/abs/2512.15405)
*Jianfei Ma,Wee Sun Lee*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯强化学习算法EUBRL，该算法利用知识性指导进行有原则的探索，有效减少了估计误差导致的每步遗憾。在理论上，为一类足够表达性的先验提供了接近最优的遗憾和样本复杂度保证。实验结果表明，EUBRL在稀疏奖励、长周期及随机性任务中表现出更好的样本效率、可扩展性和一致性。


<details>
  <summary>Details</summary>
Motivation: 在已知与未知之间，智能体面临探索还是利用的困境。知识不确定性反映了这种边界，代表了由于知识有限而产生的系统不确定性。为了更好地处理这一问题，提出了新的方法来实现更有效的探索。

Method: 提出了一种名为EUBRL（Epistemic Uncertainty Bayesian Reinforcement Learning）的贝叶斯强化学习算法，该算法通过利用知识不确定性作为指引来进行探索，旨在减少因估计错误造成的每步遗憾，并且对于无限期折扣马尔可夫决策过程中的特定类型充分表达性先验条件，建立了几乎是最小最大最优的遗憾与样本复杂度保证。

Result: 通过在具有稀疏奖励、长时间范围以及随机性的任务上评估EUBRL，实验证明该算法能够实现更高的样本效率、更好的可扩展性和一致性表现。

Conclusion: EUBRL算法通过利用知识性指导实现了高效的探索，在理论分析和实际应用中都显示出了其优越性，特别是在处理具有挑战性的强化学习环境时。

Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.

</details>


### [75] [FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows](https://arxiv.org/abs/2512.15420)
*Yeonwoo Cha,Semin Kim,Jinhyeon Kwon,Seunghoon Hong*

Main category: cs.LG

TL;DR: 提出了一种名为FlowBind的新框架，用于任意模态间的生成任务。该方法通过学习一个共享潜在空间来捕捉跨模态信息，并利用特定于模态的可逆流将此潜在空间与每个模态连接起来，从而简化了训练过程、减少了数据需求和计算成本。实验表明，相比先前的方法，FlowBind在保持生成质量的同时，参数量减少至1/6，训练速度提高了10倍。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的方法在处理任意到任意生成任务时存在效率低下问题，包括需要大规模且配对受限的数据集、建模联合分布导致高计算成本以及依赖复杂的多阶段训练。

Method: FlowBind框架通过构建一个能够捕捉跨模态信息的共享潜在空间，并使用针对每种模态设计的可逆流作为桥梁来解决上述挑战。整个系统在一个单一的流匹配目标下共同优化，而在推理阶段，这些可逆流则充当编码器和解码器的角色直接实现模态间转换。

Result: 在文本、图像及音频等领域的实验显示，相较于之前的技术方案，FlowBind不仅达到了相近或更好的生成效果，还大幅降低了模型参数数量（至多减少6倍）并加速了训练过程（快达10倍）。

Conclusion: FlowBind提供了一个更高效灵活的解决方案来进行任意模态间的生成工作，它通过精简架构减少了对大数据集的需求，并显著降低了计算资源消耗，同时保持了良好的生成性能。

Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.

</details>


### [76] [Statistics of Min-max Normalized Eigenvalues in Random Matrices](https://arxiv.org/abs/2512.15427)
*Hyakka Nakada,Shu Tanaka*

Main category: cs.LG

TL;DR: 本研究探讨了随机矩阵中最小-最大归一化特征值的统计特性，提出了有效分布来评估累积分布的比例定律，并推导了随机矩阵在进行矩阵分解时产生的残差误差。通过数值实验验证了这些理论预测。


<details>
  <summary>Details</summary>
Motivation: 鉴于随机矩阵理论在纯数学、数学物理和机器学习等领域的关键作用，以及数据科学实践中输入数据通常需要在处理前进行归一化的现实情况，本研究旨在探索经过最小-最大归一化后的随机矩阵特征值的统计属性。

Method: 研究者们首先提出了适用于归一化特征值的有效分布模型，然后利用该模型来评价累积分布的比例法则；此外还从理论上推导了与随机矩阵因子分解相关的残差误差公式。

Result: 通过数值实验确认了所提出的关于归一化特征值累积分布比例法则及矩阵分解过程中残差误差的理论预期。

Conclusion: 研究表明，对于随机矩阵中的最小-最大归一化特征值，其统计特性的新发现有助于我们更好地理解数据标准化对后续分析的影响，同时为基于随机矩阵的数据处理方法提供了新的见解。

Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.

</details>


### [77] [Double Horizon Model-Based Policy Optimization](https://arxiv.org/abs/2512.15439)
*Akihiro Kubo,Paavo Parmas,Shin Ishii*

Main category: cs.LG

TL;DR: 本文提出了一种双地平线模型基础策略优化方法（DHMBPO），通过长的'分布滚动'生成同策略状态样本以减轻分布偏移，同时利用短的'训练滚动'提供准确的价值梯度估计和稳定的梯度更新，从而在连续控制基准上优于现有方法，在样本效率和运行时间方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习（MBRL）通过从学习到的动力学模型中生成合成轨迹来减少真实环境采样的成本，但选择轨迹长度存在两个难题：较长的轨迹虽然有助于保持同策略训练却放大了模型偏差；此外，较长的模型轨迹可能降低价值估计偏差，但会因多步反向传播而增加策略梯度的方差。这两个最优轨迹长度可能不同，为解决这一冲突，作者提出了双地平线模型基础策略优化方法。

Method: 提出的双地平线模型基础策略优化(DHMBPO)将轨迹过程分为长“分布滚动”(DR)和短“训练滚动”(TR)两部分。DR生成同策略状态样本用于缓解分布偏移问题，而较短的TR则利用可微转换提供精确的价值梯度估计与稳定梯度更新，减少了所需更新次数并降低了整体运行时间。

Result: 实验表明，该双地平线方法有效地平衡了分布偏移、模型偏差及梯度不稳定性，并且在连续控制基准测试中，无论是样本效率还是运行时长方面都超过了现有的MBRL方法。

Conclusion: 双地平线模型基础策略优化方法(DHMBPO)通过巧妙设计长短不同的滚动过程解决了MBRL中关于轨迹长度选取的内在矛盾，提高了算法性能。

Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.

</details>


### [78] [Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting](https://arxiv.org/abs/2512.15442)
*Neeraj Sarna,Yuanyuan Li,Michael von Gablenz*

Main category: cs.LG

TL;DR: 本文探讨了通过结合思维链和任务指令提示与两种版权缓解策略（否定提示和提示重写）来减少大规模文本到图像生成模型中版权内容产生的可能性，并通过实验研究了这些技术在不同复杂度模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模的文本到图像生成模型有可能记忆并复制其训练数据集中的内容，由于训练数据集中通常包含受版权保护的材料，这种复制行为可能会导致侵犯版权的风险，给AI用户和开发者带来法律纠纷和经济损失。因此，寻找有效的方法减少版权内容的生成变得尤为重要。

Method: 提出了一种将思维链、任务指令提示与否定提示及提示重写相结合的方法来尝试减轻版权内容的生成问题。通过对多种模型进行数值实验，评估所提方法在减少生成内容与原始版权图片相似性的同时保持用户输入相关性的能力。

Result: 实验结果提供了关于所提出的几种技术在不同复杂程度模型上减少版权内容生成方面效果如何的一些见解。

Conclusion: 结合使用思维链、任务指令提示以及否定提示和提示重写等技术可以作为一种潜在手段用于降低大型文本到图像生成模型中版权内容的产生风险。

Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.

</details>


### [79] [From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2512.15460)
*Xiangrui Xu,Zhize Li,Yufei Han,Bin Wang,Jiqiang Liu,Wei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量方法——可逆性损失（InvLoss），用于量化联邦学习系统中数据重建攻击的最大可能效果，并基于此开发了风险评估器InvRE和两种自适应噪声扰动防御措施，通过实验证明了该框架在系统地评估和缓解联邦学习中的DRA风险方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统面临数据重建攻击（DRA）的威胁，这类攻击让对手能够从本地客户端推断出敏感训练数据。目前缺乏一种理论基础的风险量化框架来表征和评估FL系统中的DRA风险。

Method: 引入可逆性损失（InvLoss）以量化给定数据实例和FL模型下DRA所能达到的最大效能；导出了InvLoss的一个紧致且可计算的上界；基于对雅可比矩阵谱性质的研究解释了防御方法的有效性；开发了基于InvLoss的DRA风险估计器InvRE；提出了两种自适应噪声扰动防御策略。

Result: 实验结果表明，所提出的框架能够有效地对不同数据样本及模型架构进行全面的风险评估，并且新提出的防御措施能够在不损害分类准确性的前提下增强联邦学习的隐私保护能力。

Conclusion: 通过引入InvLoss及其衍生工具，为联邦学习环境中数据重建攻击的风险评估与防御提供了系统化的方法论支持。

Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.

</details>


### [80] [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](https://arxiv.org/abs/2512.15492)
*Adrián Detavernier,Jasper De Bock*

Main category: cs.LG

TL;DR: 本文探讨了两种评估分类器个体预测可靠性的方法：鲁棒性量化（RQ）和不确定性量化（UQ），并通过实验表明两者结合可以形成一种优于单独使用任一方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于比较并结合鲁棒性量化与不确定性量化这两种不同的方法，以提高分类器预测结果的可靠性评估准确性。

Method: 通过在多个基准数据集上对比鲁棒性量化和不确定性量化的方法，并提出了一种将两者结合起来的新方法。

Result: 实验结果显示，在不同数据集中，鲁棒性和不确定性作为不可靠性的来源的重要性有所不同；结合使用RQ和UQ的方法表现优于单独采用其中任何一种方法。

Conclusion: 结论指出，鲁棒性量化与不确定性量化各有优势且互为补充，它们相结合形成的混合方法能够更有效地评估分类器预测的可靠性。

Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.

</details>


### [81] [Tracking Temporal Dynamics of Vector Sets with Gaussian Process](https://arxiv.org/abs/2512.15538)
*Taichi Aida,Mamoru Komachi,Toshinobu Ogiso,Hiroya Takamura,Daichi Mochihashi*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，利用无限维高斯过程建模每个向量集的底层分布，并通过随机傅里叶特征近似高斯过程中的潜在函数，以在低维空间中跟踪和可视化向量集随时间的变化。该方法被应用于社会学（犯罪分布）和语言学（词嵌入）数据，结果表明所提方法能够有效地捕捉时间动态，并为分析不同领域的时间索引向量集结构变化提供了强大的框架。


<details>
  <summary>Details</summary>
Motivation: 理解向量集合的时间演变是生态学、犯罪分析和语言学等各个领域的基本挑战。然而，由于这些向量集合复杂的结构及其随时间演化的特性，对其进行分析十分具有挑战性。因此，需要开发一种有效的方法来处理这种时间变化的向量集合，以便更好地理解和分析它们。

Method: 本文提出了一种新颖的方法，使用无限维高斯过程对每组向量背后的分布进行建模。通过用随机傅里叶特征逼近高斯过程中的潜函数，我们能够在时间上获得紧凑且可比较的向量表示。这使得我们能够在低维空间中追踪并可视化向量集的时间过渡。

Result: 本研究将该方法应用于社会学数据（犯罪分布）和语言学数据（词嵌入），展示了其在捕捉时间动态方面的有效性。结果显示，所提出的方法提供了可解释性和鲁棒性的表示，为跨越不同领域的时间索引向量集结构变化分析提供了一个强有力框架。

Conclusion: 综上所述，本文提出的基于无限维高斯过程及随机傅里叶特征的新方法成功地解决了时间变化向量集合的分析难题，不仅能够有效地捕捉到时间动态，还为跨学科的应用提供了有力支持。

Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.

</details>


### [82] [Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation](https://arxiv.org/abs/2512.15574)
*Yuxin Cai,Yanyong Huang,Jinyuan Chang,Dongjie Wang,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出了一种名为JUICE的新方法，该方法在处理未标记的不完整多视图数据时，通过将缺失数据恢复与特征和实例共选择结合在一个统一框架中，并利用跨视图邻域信息来学习样本间关系，从而提高了共选择的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的特征和实例共选择方法在处理未标记且不完整的多视图数据时存在不足，通常会先填补缺失的数据然后合并所有视图为一个单一数据集进行后续共选择。这种方法忽略了共选择与缺失数据填补之间的潜在互动，以及简单地合并多视图数据无法捕捉到视图间的互补信息，限制了共选择的有效性。

Method: 提出了Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE) 方法。JUICE首先使用可用观测值重建不完整的多视图数据，把缺失数据恢复与特征及实例共选择整合于一个统一框架内；接着，JUICE利用跨视图邻居信息学习样本间的关系，并在重建过程中进一步优化缺失值的填补。

Result: 广泛的实验表明，JUICE优于现有最先进方法。

Conclusion: JUICE方法通过有效结合跨视图信息和共选择过程，解决了未标记不完整多视图数据处理中的挑战，提高了特征和实例选择的表现。

Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.

</details>


### [83] [Corrective Diffusion Language Models](https://arxiv.org/abs/2512.15596)
*Shuibai Zhang,Fred Zhangzhi Peng,Yiheng Zhang,Jin Pan,Grigorios G. Chrysos*

Main category: cs.LG

TL;DR: 通过引入一种面向纠正的后训练原则，扩散语言模型在错误定位和就地修正方面表现出色，超过了标准掩码扩散语言模型的表现。此外，还提出了一种新的评估基准——代码修订基准（CRB），用于衡量这种纠错行为。


<details>
  <summary>Details</summary>
Motivation: While diffusion language models have the potential for iterative error correction, standard training methods do not reliably induce this behavior, as models struggle to identify unreliable tokens within a complete input. This limitation hinders the effectiveness of confidence-guided refinement, motivating the need for a new method that can improve the model's ability to correct errors while preserving correct content.

Method: A correction-oriented post-training principle that explicitly supervises visible incorrect tokens, aiming to enable error-aware confidence and targeted refinement in diffusion language models. This approach is compared against standard masked diffusion language model (MDLM) training through the use of the newly introduced Code Revision Benchmark (CRB).

Result: Models trained with the proposed correction-oriented post-training principle show significant improvements over standard MDLMs in tasks involving error correction, including code revision. Additionally, these models also demonstrate enhanced performance in pure completion tasks.

Conclusion: The proposed correction-oriented post-training principle for diffusion language models enables better error localization and in-place correction, outperforming standard MDLMs in correction scenarios while also improving pure completion performance. The Code Revision Benchmark (CRB) is introduced as a tool for evaluating this corrective behavior.

Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.

</details>


### [84] [How Smoothing is N-simplicial Attention?](https://arxiv.org/abs/2512.15600)
*Alexandre Dussolle,Pietro Liò*

Main category: cs.LG

TL;DR: 本文介绍了N-单纯形注意力，这是一种从成对token相似性到更高阶交互的转变，并将其与旋转位置嵌入(RoPE)相适应。此外，提出了一种有效的单纯形选择方法来管理增加的复杂度，并研究了N-单纯形注意力的平滑特性及其过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 尽管存在计算上的权衡（例如GATs或Transformers），但从纯多层感知器(MLP)到每层可学习图消息传递机制的转变对于达到最先进结果是基础性的。为了进一步推进这一领域，作者引入了N-单纯形注意力，旨在处理更高阶的交互作用。

Method: 本文提出了N-单纯形注意力的概念，它允许模型超越传统的成对token相似性而考虑更复杂的交互形式，并且通过与旋转位置嵌入(RoPE)结合来增强其应用范围。同时，为了解决由此带来的复杂度提升问题，还设计了一种成本效益高的单纯形选择策略。

Result: 研究表明，N-单纯形注意力能够有效捕捉数据中的高阶依赖关系，但同时也面临过平滑的问题。通过对N-单纯形注意力进行Lipschitz上界分析，证明了即使在开放了更高阶交互的消息传递后，该方法仍然可能遭遇过平滑现象。

Conclusion: N-单纯形注意力为理解数据中的复杂交互提供了一个新的视角，但其实际应用中需要注意控制过平滑的风险。通过适当的设计和调整，这种方法有望在保持高效的同时提高模型的表现力。

Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.

</details>


### [85] [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)
*Mathieu Blondel,Michael E. Sander,Germain Vivier-Ardisson,Tianlin Liu,Vincent Roulet*

Main category: cs.LG

TL;DR: 本文提供了一种将自回归模型(ARMs)和基于能量的模型(EBMs)统一起来的观点，并通过概率链规则建立了两者在函数空间中的明确双射关系。进一步探讨了ARMS与EBMs监督学习的等价性以及EBMs向ARMS蒸馏的理论误差界限，揭示了基于下一个令牌预测范式的ARMS具备前瞻性规划的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）主要采用自回归模型(ARMs)，而基于能量的模型(EBMs)虽然在历史上的应用较少，但在后训练对齐方面自然地表征了最优策略。为了提供这两种模型类别的统一视角并探索它们之间的联系及其在最大熵强化学习中的意义，本文展开了研究。

Method: 从概率链规则出发，建立自回归模型(ARMs)与基于能量的模型(EBMs)在函数空间内的明确双射关系；分析这种双射关系对应于最大熵强化学习中软贝尔曼方程的一种特殊情况；基于该双射，推导出ARMS与EBMs监督学习的等价性；同时，对于EBMs到ARMS的知识蒸馏过程提供了理论上的误差界分析。

Result: 发现了一个连接自回归模型(ARMs)与基于能量的模型(EBMs)的明确数学框架，证明了两者在监督学习任务上的等价性，并为EBMs向ARMs的知识蒸馏给出了理论误差界限。这些结果增进了我们对于尽管基于下一个token预测机制但ARMS仍能进行前瞻性规划的理解。

Conclusion: 本研究为理解自回归模型(ARMs)与基于能量的模型(EBMs)之间的关系提供了新的视角，并揭示了即使是在以下一个token预测为基础的情况下，ARMS也能够执行有效的前瞻性规划。此外，还为将来如何更有效地利用这两种类型的模型提供了理论基础。

Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.

</details>


### [86] [Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary](https://arxiv.org/abs/2512.15614)
*Xinshun Feng,Mingzhe Liu,Yi Qiao,Tongyu Zhu,Leilei Sun,Shuai Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为BEAT的框架，该框架将用户和项目的行为转化为离散且可解释的序列，并通过多级语义监督来连接行为信号与语言空间。实验表明，BEAT不仅提高了零样本推荐的表现，还生成了连贯且信息丰富的解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于基于ID的表示方式，这掩盖了语义含义并对语言模型施加结构限制，限制了其在开放式场景中的应用性。此外，现实世界中复杂的交互特性也加剧了这些挑战。

Method: 提出了BEAT框架，它利用向量量化自编码过程构建行为词汇表，从基于图的表示中解耦宏观兴趣和微观意图。接着引入多层次语义监督机制来缩小行为信号与语言空间之间的差距，并设计了一个语义对齐正则化机制，以直接将行为标记嵌入冻结的语言模型输入空间。

Result: 在三个公开数据集上的实验显示，BEAT改善了零样本推荐性能，同时产生了连贯且信息丰富的内容。进一步分析证明，所提出的行为标记能够捕捉细粒度语义，并为大型语言模型提供即插即用接口来整合复杂的行为模式。

Conclusion: BEAT作为一种统一且可转移的框架，在提高推荐系统性能的同时增强了其解释能力，为开放场景下的自然语言处理提供了新的思路。

Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.

</details>


### [87] [SoFlow: Solution Flow Models for One-Step Generative Modeling](https://arxiv.org/abs/2512.15657)
*Tianze Luo,Haotian Yuan,Zhuang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Solution Flow Models (SoFlow) 的框架，用于从零开始进行一步生成。通过引入流匹配损失和解一致性损失来训练模型，并且这种方法不需要计算雅可比-向量积（JVP），实验表明该方法在ImageNet 256x256数据集上达到了比MeanFlow模型更好的FID-50K分数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和流匹配模型中的多步去噪过程导致了效率问题，这促使研究者探索少步甚至一步生成的方法。

Method: 提出了Solution Flow Models (SoFlow) 框架，分析了速度函数与其对应的常微分方程(ODE)解函数之间的关系，并基于此提出了流匹配损失和解一致性损失来训练模型。特别地，所提出的解一致性损失无需计算雅可比-向量积（JVP）。

Result: 实验结果表明，在使用相同的Diffusion Transformer (DiT) 架构和相同数量的训练周期的情况下，本文提出的模型在ImageNet 256x256数据集上的FID-50K得分优于MeanFlow模型。

Conclusion: Solution Flow Models (SoFlow) 提供了一个有效的一步生成框架，能够改善生成性能而不需额外计算雅可比-向量积，显示出在图像生成任务中具有竞争力的结果。

Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.

</details>


### [88] [Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2512.15687)
*Zhenwen Liang,Sidi Lu,Wenhao Yu,Kishan Panaganti,Yujun Zhou,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度引导的强化学习框架G2RL，通过模型自身的更新几何来指导探索过程。实验表明，该方法在多个推理基准测试中优于基于熵和外部嵌入的方法，并且能够保持语义一致性的同时扩展到更正交甚至相反的梯度方向上。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的探索机制与大型语言模型的学习方式不完全匹配，导致虽然表面变化多样但无法确保样本轨迹在优化过程中产生不同的更新方向。

Method: G2RL根据模型最终层敏感性构建序列级特征，并通过比较这些特征来衡量每个轨迹如何重塑策略。新颖梯度方向的轨迹会获得有界乘法奖励缩放器，而冗余或偏离流形的更新则被减弱。

Result: 在Qwen3基础1.7B和4B模型上的数学和一般推理基准测试（MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro）中，G2RL在pass@1、maj@16和pass@k指标上持续优于基于熵的GRPO和外部嵌入方法。

Conclusion: G2RL提供了一种更忠实有效的探索导向基础，能够在维持语义连贯性的同时促进大型语言模型强化学习中向更加多样化且有时相对立的方向进行探索。

Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.

</details>


### [89] [Multi-Modal Semantic Communication](https://arxiv.org/abs/2512.15691)
*Matin Mortaheb,Erciyes Karakaya,Sennur Ulukus*

Main category: cs.LG

TL;DR: 本文提出了一种多模态语义通信框架，通过结合基于文本的用户查询来指导信息提取过程，使用跨模态注意力机制将视觉特征与语言嵌入融合，根据所得的相关性分数和即时信道带宽自适应地传输图像块。


<details>
  <summary>Details</summary>
Motivation: 现有的基于变换器的方法在处理复杂场景中的多个对象时表现不佳，因为自我注意力缺乏明确的任务导向。为了提高通信效率并解决这一问题，研究者们提出了一个能够整合文本用户查询以引导信息提取的新框架。

Method: 该方法引入了跨模态注意力机制，将视觉特征与语言嵌入相结合，为视觉数据生成软相关性评分。随后，依据这些评分及当前信道带宽情况，采用特定算法并通过独立训练的编码-解码对来自适应地调整分辨率传送图像片段，总比特率与信道容量相匹配。接收端则负责重构并组合这些片段，以保持任务关键信息。

Result: 提出的系统能够在复杂且带宽受限的环境中实现高效语义通信，并能更好地保留任务关键信息。

Conclusion: 本研究展示了一种灵活且目标驱动的设计方案，其能够显著改善复杂场景下的语义通信效率，特别是在带宽有限的情况下。

Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.

</details>
