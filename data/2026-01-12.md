<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 20]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 35]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Parallel Dynamic Spatial Indexes](https://arxiv.org/abs/2601.05347)
*Ziyang Men,Bo Huang,Yan Gu,Yihan Sun*

Main category: cs.DB

TL;DR: 本文系统地研究了并行空间索引，特别关注在高度动态的工作负载下实现高性能的更新性能。提出了两种数据结构：P-Orth树和SPaC树族，它们在批量更新方面表现出色，并保持了良好的或具有竞争力的查询性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，空间数据集往往是高度动态的，需要低延迟地批量更新点信息。但是，目前很少有工作能够有效达成这一目标。

Method: 选择两种被认为优化了低延迟更新的空间索引类型：Orth-tree和R-tree/BVH。基于此，提出了两种新的数据结构：P-Orth树（一种并行Orth-tree）和SPaC树族（一种并行R-tree/BVH）。

Result: P-Orth树和SPaC树族在批量更新方面的表现优于现有的并行kd-trees和Orth-trees，同时与对应的Orth-tree和R-tree相比，保留了更好或相当的查询性能。

Conclusion: 通过综合实验比较了各种并行空间索引的性能，并在论文末尾分享了发现。

Abstract: Maintaining spatial data (points in two or three dimensions) is crucial and has a wide range of applications, such as graphics, GIS, and robotics. To handle spatial data, many data structures, called spatial indexes, have been proposed, e.g. kd-trees, oct/quadtrees (also called Orth-trees), R-trees, and bounding volume hierarchies (BVHs). In real-world applications, spatial datasets tend to be highly dynamic, requiring batch updates of points with low latency. This calls for efficient parallel batch updates on spatial indexes. Unfortunately, there is very little work that achieves this.
  In this paper, we systematically study parallel spatial indexes, with a special focus on achieving high-performance update performance for highly dynamic workloads. We select two types of spatial indexes that are considered optimized for low-latency updates: Orth-tree and R-tree/BVH. We propose two data structures: the P-Orth tree, a parallel Orth-tree, and the SPaC-tree family, a parallel R-tree/BVH. Both the P-Orth tree and the SPaC-tree deliver superior performance in batch updates compared to existing parallel kd-trees and Orth-trees, while preserving better or competitive query performance relative to their corresponding Orth-tree and R-tree counterparts. We also present comprehensive experiments comparing the performance of various parallel spatial indexes and share our findings at the end of the paper.

</details>


### [2] [RISE: Rule-Driven SQL Dialect Translation via Query Reduction](https://arxiv.org/abs/2601.05579)
*Xudong Xie,Yuwei Zhang,Wensheng Dou,Yu Gao,Ziyu Cui,Jiansen Song,Rui Yang,Jun Wei*

Main category: cs.DB

TL;DR: 本文提出了一种新的基于大语言模型的SQL方言翻译方法RISE，该方法通过简化查询来提高对复杂和长SQL查询的翻译准确性。在TPC-DS和SQLProcBench两个基准测试中，RISE的表现优于传统基于规则的工具和其他基于大语言模型的方法。


<details>
  <summary>Details</summary>
Motivation: 由于将基于RDBMS的应用迁移到云端时需要跨不同数据库管理系统转换SQL方言，而传统的转换工具依赖于手工编写的规则，这要求大量的手动工作来支持新的RDBMS和方言。尽管大型语言模型可以帮助翻译SQL方言，但它们通常难以处理长且复杂的SQL查询。

Method: RISE方法首先使用一种方言感知的查询简化技术从复杂源查询$Q_c$中移除与特定方言$d$无关的SQL元素以得到简化后的查询$Q_s$，然后利用大语言模型将$Q_s$翻译成$Q_{s^{'}}$，并根据$Q_s$和$Q_{s^{'}}$之间的关系自动提取针对方言$d$的翻译规则$r_d$。最后，通过将$r_d$应用于$Q_c$，可以有效地翻译其中的方言$d$，从而绕过了源查询$Q_c$的复杂性。

Result: 在TPC-DS和SQLProcBench两个真实世界基准测试上评估了RISE，并将其性能与传统基于规则的工具以及基于大语言模型的方法进行了比较。结果显示，RISE在TPC-DS上的准确率为97.98%，在SQLProcBench上的准确率达到100%，分别比基线平均提高了24.62%和238.41%。

Conclusion: 提出的RISE方法能够准确地处理长且复杂的SQL查询，在实际应用中展现出优越的性能，为迁移RDBMS应用程序到云端提供了有效的解决方案。

Abstract: Translating SQL dialects across different relational database management systems (RDBMSs) is crucial for migrating RDBMS-based applications to the cloud. Traditional SQL dialect translation tools rely on manually-crafted rules, necessitating significant manual effort to support new RDBMSs and dialects. Although large language models (LLMs) can assist in translating SQL dialects, they often struggle with lengthy and complex SQL queries.
  In this paper, we propose RISE, a novel LLM-based SQL dialect translation approach that can accurately handle lengthy and complex SQL queries. Given a complex source query $Q_c$ that contains a SQL dialect $d$, we first employ a dialect-aware query reduction technique to derive a simplified query $Q_{s}$ by removing $d$-irrelevant SQL elements from $Q_c$. Subsequently, we utilize LLMs to translate $Q_{s}$ into $Q_{s^{'}}$, and automatically extract the translation rule $r_d$ for dialect $d$ based on the relationship between $Q_{s}$ and $Q_{s^{'}}$. By applying $r_d$ to $Q_c$, we can effectively translate the dialect $d$ within $Q_c$, thereby bypassing the complexity of the source query $Q_c$. We evaluate RISE on two real-world benchmarks, i.e., TPC-DS and SQLProcBench, comparing its performance against both the traditional rule-based tools and the LLM-based approaches with respect to translation accuracy. RISE achieves accuracies of 97.98% on TPC-DS and 100% on SQLProcBench, outperforming the baselines by an average improvement of 24.62% and 238.41%, respectively.

</details>


### [3] [The Importance of Parameters in Ranking Functions](https://arxiv.org/abs/2601.06001)
*Christoph Standke,Nikolaos Tziavelis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 该论文探讨了列权重在决定表中元组排名时的重要性，通过计算SHAP分数来量化不同排名函数和效应函数下的影响。研究发现，尽管所有情况都允许使用完全多项式时间随机近似方案（FPRAS），但精确计算的复杂性在不同情况下有所不同，有些可以在多项式时间内解决而有些则是#P-hard问题。此外，这些复杂性结果也适用于计算整个列的Shapley值。


<details>
  <summary>Details</summary>
Motivation: 为了回答关于排名函数解释的问题，特别是列权重对元组排名的影响程度，本文采用Grohe等人[ICDT'24]提出的框架来计算列权重的SHAP分数。

Method: 本文分析了基于不同基本排名函数（字典序、基于求和/最小/最大得分的顺序）以及从全局、top-k和局部视角考虑的效果函数下，该框架实例化的计算复杂度。特别地，研究集中在个体列的概率独立有限分布上。

Result: 研究发现，虽然对于所有情形都可以使用一种加性完全多项式时间随机近似方案(FPRAS)，但对于精确计算而言，某些情形是多项式时间内可解的，而另一些则为#P-hard问题。此外，所有复杂度结果（包括下界和上界）同样适用于计算整列（不考虑其权重）的Shapley值的任务。

Conclusion: 本研究表明，在不同排名与效果函数设置下，评估列权重重要性的计算复杂度存在显著差异。尽管有高效的近似方法可用，但精确计算的难度随具体情况而异。此外，这些结论也扩展到了整列Shapley值的计算领域。

Abstract: How important is the weight of a given column in determining the ranking of tuples in a table? To address such an explanation question about a ranking function, we investigate the computation of SHAP scores for column weights, adopting a recent framework by Grohe et al.[ICDT'24]. The exact definition of this score depends on three key components: (1) the ranking function in use, (2) an effect function that quantifies the impact of using alternative weights on the ranking, and (3) an underlying weight distribution. We analyze the computational complexity of different instantiations of this framework for a range of fundamental ranking and effect functions, focusing on probabilistically independent finite distributions for individual columns.
  For the ranking functions, we examine lexicographic orders and score-based orders defined by the summation, minimum, and maximum functions. For the effect functions, we consider global, top-k, and local perspectives: global measures quantify the divergence between the perturbed and original rankings, top-k measures inspect the change in the set of top-k answers, and local measures capture the impact on an individual tuple of interest. Although all cases admit an additive fully polynomial-time randomized approximation scheme (FPRAS), we establish the complexity of exact computation, identifying which cases are solvable in polynomial time and which are #P-hard. We further show that all complexity results, lower bounds and upper bounds, extend to a related task of computing the Shapley value of whole columns (regardless of their weight).

</details>


### [4] [Database Theory in Action: Direct Access to Query Answers](https://arxiv.org/abs/2601.06013)
*Jiayin Hu,Nikolaos Tziavelis*

Main category: cs.DB

TL;DR: 本文实现了一个支持多种查询和排序的直接访问数据结构，并研究了其实用性能，包括数据库系统的比较性能以及直接访问与其单次访问对应项之间的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管支持直接访问的数据结构的时间复杂度已经得到了深入研究，并且对于许多查询和常见排序已经有了高效的算法，但其实用性能却很少受到关注。

Method: 通过提供一个涵盖广泛查询和排序的实现来研究直接访问的实际表现方面，包括数据库系统之间的性能比较以及直接访问与单一访问形式间的关系。

Result: 研究表明，所实现的数据结构在处理各种查询和排序时具有良好的实用性能，同时揭示了不同数据库系统间的性能差异及直接访问和单次访问方式之间的联系。

Conclusion: 这项工作不仅提供了对直接访问数据结构实际性能的新见解，也为进一步优化这类数据结构的设计提出了可能的方向。

Abstract: Direct access asks for the retrieval of query answers by their ranked position, given a query and a desired order. While the time complexity of data structures supporting such accesses has been studied in depth, and efficient algorithms for many queries and common orders are known, their practical performance has received little attention. We provide an implementation covering a wide range of queries and orders; it allows us to investigate intriguing practical aspects, including the comparative performance of database systems and the relationship between direct access and its single-access counterpart.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [5] [SP-Rank: A Dataset for Ranked Preferences with Secondary Information](https://arxiv.org/abs/2601.05253)
*Hadi Hosseini,Debmalya Mandal,Amrit Puhan*

Main category: cs.IR

TL;DR: This paper introduces SP-Rank, a large-scale, publicly available dataset for benchmarking ranking algorithms that utilize both first-order preferences and second-order predictions. The dataset is evaluated across three domains with nine elicitation formats, showing that incorporating second-order signals improves accuracy over traditional vote-only methods. It supports applications in learning-to-rank, expert knowledge extraction, and preference-based AI fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to provide a new, rich dataset (SP-Rank) that allows for the evaluation of ranking algorithms capable of leveraging both individual votes (first-order signals) and predictions about how others will vote (second-order signals). Traditional datasets only capture individual preferences, whereas SP-Rank aims to support more complex modeling by including both types of signals, enabling better analysis of preference aggregation and supporting various downstream applications.

Method: The authors created SP-Rank, a dataset containing over 12,000 human-generated data points covering three domains: geography, movies, and paintings, with data collected using nine different elicitation formats. They then benchmarked the dataset by comparing the performance of traditional aggregation methods, which use only first-order votes, against SP-Voting, a method that combines both first- and second-order signals to infer ground-truth rankings. The evaluation focused on three core tasks: full ground-truth rank recovery, subset-level rank recovery, and probabilistic modeling of voter behavior.

Result: Results indicated that incorporating second-order signals into the ranking process significantly enhances the accuracy of inferring ground-truth rankings compared to using only first-order votes. This improvement was observed across all three core tasks examined in the study. Additionally, the dataset's structure and diversity enable its application in various fields such as learning-to-rank, extracting expert knowledge from crowds, and training reward models in AI fine-tuning pipelines.

Conclusion: The introduction of SP-Rank provides researchers and practitioners with a valuable resource for developing and evaluating more sophisticated ranking algorithms that can effectively integrate both first- and second-order signals. By demonstrating the benefits of combining these two types of information, the work paves the way for advancements in human preference modeling, aggregation theory, and human-AI alignment, while also supporting practical applications in areas like expert knowledge extraction and preference-based AI training.

Abstract: We introduce $\mathbf{SP-Rank}$, the first large-scale, publicly available dataset for benchmarking algorithms that leverage both first-order preferences and second-order predictions in ranking tasks. Each datapoint includes a personal vote (first-order signal) and a meta-prediction of how others will vote (second-order signal), allowing richer modeling than traditional datasets that capture only individual preferences. SP-Rank contains over 12,000 human-generated datapoints across three domains -- geography, movies, and paintings, and spans nine elicitation formats with varying subset sizes. This structure enables empirical analysis of preference aggregation when expert identities are unknown but presumed to exist, and individual votes represent noisy estimates of a shared ground-truth ranking. We benchmark SP-Rank by comparing traditional aggregation methods that use only first-order votes against SP-Voting, a second-order method that jointly reasons over both signals to infer ground-truth rankings. While SP-Rank also supports models that rely solely on second-order predictions, our benchmarks emphasize the gains from combining both signals. We evaluate performance across three core tasks: (1) full ground-truth rank recovery, (2) subset-level rank recovery, and (3) probabilistic modeling of voter behavior. Results show that incorporating second-order signals substantially improves accuracy over vote-only methods. Beyond social choice, SP-Rank supports downstream applications in learning-to-rank, extracting expert knowledge from noisy crowds, and training reward models in preference-based fine-tuning pipelines. We release the dataset, code, and baseline evaluations (available at https://github.com/amrit19/SP-Rank-Dataset ) to foster research in human preference modeling, aggregation theory, and human-AI alignment.

</details>


### [6] [TagRAG: Tag-guided Hierarchical Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05254)
*Wenbiao Tao,Yunshi Lan,Weining Qian*

Main category: cs.IR

TL;DR: 提出了TagRAG，一种标签引导的层次知识图谱RAG框架，用于提高检索增强生成模型在查询聚焦摘要任务中的效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法依赖片段级检索，这限制了它们处理查询聚焦摘要请求的能力；而GraphRAG虽然引入了基于图的方法来进行全局知识推理，但存在信息提取效率低下、资源消耗大以及对增量更新适应性差的问题。

Method: TagRAG包含两个主要组成部分：(1) 标签知识图构建，从文档中提取对象标签及其关系，并将其组织成层次化的领域标签链以实现结构化知识表示；(2) 标签引导的检索增强生成，通过检索领域中心的标签链来定位并合成相关知识。

Result: 在涵盖农业、计算机科学、法律等多个领域的UltraDomain数据集上进行的广泛实验表明，与基线相比，TagRAG达到了平均95.41%的胜率，同时在构建和检索效率方面分别比GraphRAG提高了约14.6倍和1.9倍。

Conclusion: TagRAG显著改善了检索增强生成对于较小语言模型的支持，提升了检索粒度，并且支持高效的知识增量更新。

Abstract: Retrieval-Augmented Generation enhances language models by retrieving external knowledge to support informed and grounded responses. However, traditional RAG methods rely on fragment-level retrieval, limiting their ability to address query-focused summarization queries. GraphRAG introduces a graph-based paradigm for global knowledge reasoning, yet suffers from inefficiencies in information extraction, costly resource consumption, and poor adaptability to incremental updates. To overcome these limitations, we propose TagRAG, a tag-guided hierarchical knowledge graph RAG framework designed for efficient global reasoning and scalable graph maintenance. TagRAG introduces two key components: (1) Tag Knowledge Graph Construction, which extracts object tags and their relationships from documents and organizes them into hierarchical domain tag chains for structured knowledge representation, and (2) Tag-Guided Retrieval-Augmented Generation, which retrieves domain-centric tag chains to localize and synthesize relevant knowledge during inference. This design significantly adapts to smaller language models, improves retrieval granularity, and supports efficient knowledge increment. Extensive experiments on UltraDomain datasets spanning Agriculture, Computer Science, Law, and cross-domain settings demonstrate that TagRAG achieves an average win rate of 95.41\% against baselines while maintaining about 14.6x construction and 1.9x retrieval efficiency compared with GraphRAG.

</details>


### [7] [CourtNav: Voice-Guided, Anchor-Accurate Navigation of Long Legal Documents in Courtrooms](https://arxiv.org/abs/2601.05255)
*Sai Khadloya,Kush Juvekar,Arghya Bhattacharya,Utkarsh Saxena*

Main category: cs.IR

TL;DR: CourtNav是一个语音引导的法律PDF导航器，通过法官的语音命令快速定位并高亮显示相关段落，大大减少了查找时间。


<details>
  <summary>Details</summary>
Motivation: 司法工作需要仔细阅读大量的文档，但实际操作中由于人力有限，详尽阅读变得不切实际。特别是在印度，判决书和交叉审问记录非常长。

Method: CourtNav使用语音转录、意图分类（基于语法优先和少量示例支持的LLM）、布局感知混合索引检索等技术，实现了从语音命令到文档内特定段落的快速定位与高亮。

Result: 在代表性的起诉书、诉状和命令文件上的试点表明，使用CourtNav后，找到相关信息的时间从手动导航时的3-5分钟减少到了10-15秒；如果包括快速视觉验证，则为30-45秒。

Conclusion: 通过采用以导航为中心的设计，CourtNav能够在保持控制性和透明度的同时增加咨询记录的实际范围。

Abstract: Judicial work depends on close reading of long records, charge sheets, pleadings, annexures, orders, often spanning hundreds of pages. With limited staff support, exhaustive reading during hearings is impractical. We present CourtNav, a voice-guided, anchor-first navigator for legal PDFs that maps a judge's spoken command (e.g., "go to paragraph 23", "highlight the contradiction in the cross-examination") directly to a highlighted paragraph in seconds. CourtNav transcribes the command, classifies intent with a grammar-first(Exact regex matching), LLM-backed router classifying the queries using few shot examples, retrieves over a layout-aware hybrid index, and auto-scrolls the viewer to the cited span while highlighting it and close alternates. By design, the interface shows only grounded passages, never free text, keeping evidence verifiable and auditable. This need is acute in India, where judgments and cross-examinations are notoriously long.In a pilot on representative charge sheets, pleadings, and orders, median time-to-relevance drops from 3-5 minutes (manual navigation) to 10-15 seconds; with quick visual verification included, 30-45 seconds. Under fixed time budgets, this navigation-first design increases the breadth of the record actually consulted while preserving control and transparency.

</details>


### [8] [From Events to Trending: A Multi-Stage Hotspots Detection Method Based on Generative Query Indexing](https://arxiv.org/abs/2601.05258)
*Kaichun Wang,Yanguang Chen,Ting Zhang,Mengyao Bao,Keyu Chen,Xu Hu,Yongliang Wang,Jingsheng Yang,Jinsong Zhang,Fei Lu*

Main category: cs.IR

TL;DR: 提出了一种针对对话系统场景优化的多阶段趋势查询检测框架，通过生成索引查询和在线检索匹配机制提高了检测效率与准确性，并在离线评估和在线A/B测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的聊天机器人难以有效处理与新闻相关的热门查询，而为传统搜索引擎设计的方法在对话环境中表现不佳。因此，需要一种专门针对对话系统场景的趋势查询检测方法来改善用户体验。

Method: 本研究提出一个多阶段框架，首先利用选定的热点事件生成索引查询，然后采用检索匹配机制实现实时在线检测趋势查询。该框架还引入了级联召回与排序架构以平衡检测效率和准确性，并采用单一召回模块作为冷启动策略收集在线数据以微调重排器。

Result: 实验表明，所提出的框架在离线评估和在线A/B测试中均显著优于基线方法，用户满意度（正负反馈比率）相对提高了27%。

Conclusion: 这项工作成功地填补了对话系统场景下趋势检测研究的空白，提供了一个有效的解决方案来识别和处理热门查询，从而改善了基于LLM的对话系统的用户体验。

Abstract: LLM-based conversational systems have become a popular gateway for information access, yet most existing chatbots struggle to handle news-related trending queries effectively. To improve user experience, an effective trending query detection method is urgently needed to enable differentiated processing of such target traffic. However, current research on trending detection tailored to the dialogue system scenario remains largely unexplored, and methods designed for traditional search engines often underperform in conversational contexts due to radically distinct query distributions and expression patterns. To fill this gap, we propose a multi-stage framework for trending detection, which achieves systematic optimization from both offline generation and online identification perspectives. Specifically, our framework first exploits selected hot events to generate index queries, establishing a key bridge between static events and dynamic user queries. It then employs a retrieval matching mechanism for real-time online detection of trending queries, where we introduce a cascaded recall and ranking architecture to balance detection efficiency and accuracy. Furthermore, to better adapt to the practical application scenario, our framework adopts a single-recall module as a cold-start strategy to collect online data for fine-tuning the reranker. Extensive experiments demonstrate that our framework significantly outperforms baseline methods in both offline evaluations and online A/B tests, and user satisfaction is relatively improved by 27\% in terms of positive-negative feedback ratio.

</details>


### [9] [A Technical Report on the Second Place Solution for the CIKM 2025 AnalytiCup Competition](https://arxiv.org/abs/2601.05259)
*Haotao Xie,Ruilin Chen,Yicheng Wu,Zhan Zhao,Yuanyuan Liu*

Main category: cs.IR

TL;DR: 本文提出了一种基于提示工程和思维链任务分解的简化框架，用于解决电子商务搜索中的多语言类别相关性判断问题。通过将相关性判断过程分解为翻译、意图理解、类别匹配和相关性判断四个可解释子任务，并使用低秩适应（LoRA）微调基础模型Qwen2.5-14B，该方法在保持竞争力的同时显著提高了推理效率并降低了计算与存储开销。实验结果表明，所提单模型框架不仅在CIKM 2025 AnalytiCup竞赛中取得了优异成绩，而且展示了结构化提示结合轻量级微调作为可扩展工业AI应用新范式的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对传统集成系统虽然能够提高多语言类别相关性判断准确性但同时带来训练、推理及维护复杂度高的问题，研究旨在开发一种既简化又高效的解决方案。

Method: 采用提示工程技术配合思维链任务分解策略，将整个相关性评估流程拆分为翻译、意图理解、类别匹配以及最终的相关性评判四个步骤，并通过低秩适应技术对大规模语言模型Qwen2.5-14B进行高效调整。

Result: 实验显示，提出的单一模型架构实现了与现有复杂集成系统相当甚至更优的准确性，同时具备更高的推理速度（单个A100 GPU上每秒处理20个样本）。此外，在CIKM 2025 AnalytiCup比赛提案中分别获得了公共排行榜0.8902分和私人排行榜0.8889分的好成绩。

Conclusion: 研究表明，通过结构化的提示工程联合轻量化微调可以有效替代复杂的集成系统，为实现可扩展的工业级人工智能应用提供了新的思路。

Abstract: In this work, we address the challenge of multilingual category relevance judgment in e-commerce search, where traditional ensemble-based systems improve accuracy but at the cost of heavy training, inference, and maintenance complexity. To overcome this limitation, we propose a simplified yet effective framework that leverages prompt engineering with Chain-of-Thought task decomposition to guide reasoning within a single large language model. Specifically, our approach decomposes the relevance judgment process into four interpretable subtasks: translation, intent understanding, category matching, and relevance judgment -- and fine-tunes a base model (Qwen2.5-14B) using Low-Rank Adaptation (LoRA) for efficient adaptation. This design not only reduces computational and storage overhead but also enhances interpretability by explicitly structuring the model's reasoning path. Experimental results show that our single-model framework achieves competitive accuracy and high inference efficiency, processing 20 samples per second on a single A100 GPU. In the CIKM 2025 AnalytiCup Competition Proposals, our method achieved 0.8902 on the public leaderboard and 0.8889 on the private leaderboard, validating the effectiveness and robustness of the proposed approach. These results highlight that structured prompting combined with lightweight fine-tuning can outperform complex ensemble systems, offering a new paradigm for scalable industrial AI applications.

</details>


### [10] [Quantifying Document Impact in RAG-LLMs](https://arxiv.org/abs/2601.05260)
*Armin Gerami,Kazem Faghih,Ramani Duraiswami*

Main category: cs.IR

TL;DR: 本文提出了一种新的度量标准——影响分数(IS)，用于量化检索文档对最终输出的影响，从而提高RAG系统的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前的RAG评估缺乏衡量单个检索文档对最终输出贡献的指标，这导致了事实不一致、来源冲突、偏见传播以及安全漏洞等问题。

Method: 基于部分信息分解提出了影响分数(IS)这一新指标，并通过毒害攻击模拟实验与消融研究两种方式验证其有效性。

Result: 在毒害攻击模拟中，IS能够在86%的情况下正确识别出最具影响力的恶意文档；而在消融研究中，仅使用IS排名靠前的文档生成的回答被持续认为比使用剩余文档生成的回答更接近原始回答。

Conclusion: 影响分数(IS)有效地隔离并量化了文档的影响，为提高RAG系统的透明度和可靠性提供了有价值的工具。

Abstract: Retrieval Augmented Generation (RAG) enhances Large Language Models (LLMs) by connecting them to external knowledge, improving accuracy and reducing outdated information. However, this introduces challenges such as factual inconsistencies, source conflicts, bias propagation, and security vulnerabilities, which undermine the trustworthiness of RAG systems. A key gap in current RAG evaluation is the lack of a metric to quantify the contribution of individual retrieved documents to the final output. To address this, we introduce the Influence Score (IS), a novel metric based on Partial Information Decomposition that measures the impact of each retrieved document on the generated response. We validate IS through two experiments. First, a poison attack simulation across three datasets demonstrates that IS correctly identifies the malicious document as the most influential in $86\%$ of cases. Second, an ablation study shows that a response generated using only the top-ranked documents by IS is consistently judged more similar to the original response than one generated from the remaining documents. These results confirm the efficacy of IS in isolating and quantifying document influence, offering a valuable tool for improving the transparency and reliability of RAG systems.

</details>


### [11] [LiveVectorLake: A Real-Time Versioned Knowledge Base Architecture for Streaming Vector Updates and Temporal Retrieval](https://arxiv.org/abs/2601.05270)
*Tarun Prajapati*

Main category: cs.IR

TL;DR: 本文提出了一种名为LiveVectorLake的双层时间知识库架构，它能够实现实时语义搜索的同时保持完整的版本历史记录。该系统通过内容寻址的块级同步、双层存储分离以及时间查询路由等核心架构贡献，优化了查询延迟和存储成本，并支持跨层级的ACID一致性。实验结果表明，与完全重新索引相比，更新时的内容再处理减少了10-15%，当前知识检索延迟低于100毫秒，历史版本的时间查询延迟控制在2秒内。


<details>
  <summary>Details</summary>
Motivation: 现代检索增强生成（RAG）系统面临着一个基本架构上的矛盾：向量索引虽然对查询延迟进行了优化，但在持续的知识更新方面表现不佳；而数据湖虽然擅长版本控制，但会引入查询延迟的问题。为了解决这一问题，本文提出了LiveVectorLake。

Method: LiveVectorLake采用了三个主要的架构创新：
1. 使用SHA-256哈希进行内容寻址的块级同步，实现无需外部状态跟踪的确定性变更检测。
2. 采用双层存储结构，将热层向量索引（Milvus搭配HNSW算法）与冷层列式版本管理（Delta Lake配合Parquet格式）分开，从而独立优化查询延迟和存储成本。
3. 实现了时间查询路由功能，允许通过带有ACID一致性的delta-versioning机制来进行指定时间点的知识检索。

Result: 评估结果显示，在一个包含五个时间点版本的100文档语料库上，LiveVectorLake展示了以下性能指标：(i) 更新期间仅需重处理10-15%的内容，远低于完全重新索引所需的100%；(ii) 对当前知识的检索延迟低于100毫秒；(iii) 针对整个版本历史的时间查询延迟保持在2秒以内；(iv) 通过对热/冷层的分离实现了存储成本的优化（只有最新块位于昂贵的向量索引中）。

Conclusion: LiveVectorLake方法能够在需要同时优化查询性能、更新效率和法规遵从性的生产RAG部署中发挥作用。

Abstract: Modern Retrieval-Augmented Generation (RAG) systems struggle with a fundamental architectural tension: vector indices are optimized for query latency but poorly handle continuous knowledge updates, while data lakes excel at versioning but introduce query latency penalties. We introduce LiveVectorLake, a dual-tier temporal knowledge base architecture that enables real-time semantic search on current knowledge while maintaining complete version history for compliance, auditability, and point-in-time retrieval. The system introduces three core architectural contributions: (1) Content-addressable chunk-level synchronization using SHA-256 hashing for deterministic change detection without external state tracking; (2) Dual-tier storage separating hot-tier vector indices (Milvus with HNSW) from cold-tier columnar versioning (Delta Lake with Parquet), optimizing query latency and storage cost independently; (3) Temporal query routing enabling point-in-time knowledge retrieval via delta-versioning with ACID consistency across tiers. Evaluation on a 100-document corpus versioned across five time points demonstrates: (i) 10-15% re-processing of content during updates compared to 100% for full re-indexing; (ii) sub-100ms retrieval latency on current knowledge; (iii) sub-2s latency for temporal queries across version history; and (iv) storage cost optimization through hot/cold tier separation (only current chunks in expensive vector indices). The approach enables production RAG deployments requiring simultaneous optimization for query performance, update efficiency, and regulatory compliance. Code and resources: [https://github.com/praj-tarun/LiveVectorLake]

</details>


### [12] [Improving User Experience with Personalized Review Ranking and Summarization](https://arxiv.org/abs/2601.05261)
*Muhammad Mufti,Omar Hammad,Mahfuzur Rahman*

Main category: cs.IR

TL;DR: 本文提出了一种结合评论排序和抽象总结的个性化框架，通过分析用户的历史评价来形成情感和个人偏好模型，并据此匹配并总结出最相关的评论。实验表明，该方法提高了用户满意度、相关性和决策信心，同时减少了阅读时间。


<details>
  <summary>Details</summary>
Motivation: 随着用户生成评论数量的增加，消费者面临着信息过载的问题，难以找到符合个人偏好的内容。现有的评论排名系统主要依据有用性投票、星级评分等指标，未能充分考虑个体用户的兴趣，且通常将文本情绪与评分信号分开处理。为了解决这些问题，研究提出了一个旨在提高决策效率的新框架。

Method: 该研究首先通过综合分析星级评分和评论内容来建模每个用户的情感。同时利用句子嵌入和聚类技术从历史评论中提取用户偏好，构建与主题和情感维度对齐的语义档案。然后采用相关性评分算法根据情感和方面相似度将这些档案与未见过的评论进行匹配。最后对最高匹配度的评论进行摘要处理以反映个人兴趣。

Result: 一项包含70名参与者的研究显示，个性化的方法在提升满意度、感知相关性和决策自信方面表现良好，同时也减少了用户花在阅读上的时间。

Conclusion: 结果强调了该方法在缓解信息过载及提供定制化内容方面的有效性，对于改善富含评论的决策环境中的用户体验具有重要价值。

Abstract: Online consumer reviews play a crucial role in guiding purchase decisions by offering insights into product quality, usability, and performance. However, the increasing volume of user-generated reviews has led to information overload, making it difficult for consumers to identify content that aligns with their specific preferences. Existing review ranking systems typically rely on metrics such as helpfulness votes, star ratings, and recency, but these fail to capture individual user interests and often treat textual sentiment and rating signals separately. This research addresses these limitations by proposing a personalized framework that integrates review ranking and abstractive summarization to enhance decision-making efficiency. The proposed system begins by modeling each user's sentiment through a hybrid analysis of star ratings and review content. Simultaneously, user preferences were derived from historical reviews using sentence embeddings and clustering, forming semantic profiles aligned with thematic and sentiment dimensions. A relevance scoring algorithm matched these profiles with unseen reviews based on sentiment and aspect similarity. Top-matched reviews were then summarized to reflect individual interests. A user study with 70 participants demonstrated that the personalized approach improved satisfaction, perceived relevance, and decision-making confidence, while reducing time spent reading. The results highlight the method's effectiveness in alleviating information overload and delivering content tailored to user-specific preferences, emphasizing its value in enhancing user experience in review-rich decision-making environments.

</details>


### [13] [LLM2IR: simple unsupervised contrastive learning makes long-context LLM great retriever](https://arxiv.org/abs/2601.05262)
*Xiaocong Yang*

Main category: cs.IR

TL;DR: 本研究提出了一种名为LLM2IR的无监督对比学习框架，能够将任何仅解码器的大规模语言模型转化为信息检索模型。该方法不仅在多个基准测试中展现出了有效性，而且揭示了模型上下文长度与信息检索能力之间的关系。


<details>
  <summary>Details</summary>
Motivation: 现代密集型信息检索模型通常依赖于成本高昂的大规模预训练。为了提供一种更有效的构建信息检索模型的方法，并探索信息检索能力与模型特性间的关系。

Method: 通过引入LLM2IR这一高效的无监督对比学习框架，将各种仅解码器的大规模语言模型转换为信息检索模型。

Result: 不同大规模语言模型在LoCo、LongEmbed和BEIR等多个信息检索基准上展示了其有效性；同时发现，在相同模型家族内比较时，具有更长上下文长度的模型往往表现出更强的信息检索能力。

Conclusion: 这项工作不仅为利用最先进的大规模语言模型构建信息检索模型提供了有效途径，还揭示了信息检索能力和模型上下文长度之间的联系，有助于设计更好的信息检索器。

Abstract: Modern dense information retrieval (IR) models usually rely on costly large-scale pretraining. In this paper, we introduce LLM2IR, an efficient unsupervised contrastive learning framework to convert any decoder-only large language model (LLM) to an information retrieval model. Despite its simplicity, the effectiveness is proven among different LLMs on multiple IR benchmarks including LoCo, LongEmbed and BEIR. We also find that models with a longer context length tend to have a stronger IR capacity by comparing task performances of models in the same model family. Our work not only provides an effective way to build IR models on the state-of-the-art LLMs, but also shed light on the relationship between information retrieval ability and model context length, which helps the design of better information retrievers.

</details>


### [14] [A General Metric-Space Formulation of the Time Warp Edit Distance (TWED)](https://arxiv.org/abs/2601.05263)
*Zhen Yi Lau*

Main category: cs.IR

TL;DR: 本文提出了一种广义的时间扭曲编辑距离（GTWED），它可以应用于任意度量空间，并且在温和的假设下保持真正的度量性质。通过将观察域和时间域视为度量空间，该方法扩展了传统的TWED到更广泛的应用场景，如符号数据、流形或嵌入等。


<details>
  <summary>Details</summary>
Motivation: 为了将时间扭曲编辑距离（TWED）的概念推广至任意度量空间，使得这一度量工具不仅限于时间序列分析，也能被用于处理其他类型的数据集，比如符号数据、流形或者嵌入向量等。

Method: 作者通过定义两个度量空间 $(X, d)$ 和 $(T, Δ)$ 分别代表观察域和时间域，提出了广义的时间扭曲编辑距离（GTWED）。证明了在一些温和条件下，GTWED 满足度量属性。并且展示当 $X = \mathbb{R}^d$、$T \subset \mathbb{R}$ 以及 $g(x) = x$ 时，GTWED 能够退化为经典 TWED 的特例。

Result: 研究结果表明，所提出的 GTWED 在适当条件下确实是一个有效的度量方式，它能够作为传统 TWED 的一般化形式，适用于更加多样化的数据类型。

Conclusion: 本技术说明介绍了广义时间扭曲编辑距离（GTWED）的概念及其理论基础，证明了其作为一种通用度量手段的有效性，从而为弹性距离度量提供了新的可能性，使其超越时间序列领域，可应用于更多类型的序列数据。

Abstract: This short technical note presents a formal generalization of the Time Warp Edit Distance (TWED) proposed by Marteau (2009) to arbitrary metric spaces. By viewing both the observation and temporal domains as metric spaces $(X, d)$ and $(T, Δ)$, we define a Generalized TWED (GTWED) that remains a true metric under mild assumptions. We provide self-contained proofs of its metric properties and show that the classical TWED is recovered as a special case when $X = \mathbb{R}^d$, $T \subset \mathbb{R}$, and $g(x) = x$. This note focuses on the theoretical structure of GTWED and its implications for extending elastic distances beyond time series, which enables the use of TWED-like metrics on sequences over arbitrary domains such as symbolic data, manifolds, or embeddings.

</details>


### [15] [Engineering the RAG Stack: A Comprehensive Review of the Architecture and Trust Frameworks for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2601.05264)
*Dean Wampler,Dave Nielson,Alireza Seddighi*

Main category: cs.IR

TL;DR: 本文提供了一个关于检索增强生成（RAG）架构的系统性文献综述，涵盖了从2018年至2025年的学术研究、工业应用及实际部署情况。文章对现有RAG技术进行了分类整理，并提出了定量评估框架，讨论了信任与一致性问题，旨在为构建适应性强、安全且可定制领域的RAG系统提供实用指南和技术参考。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型系统的扩展，如何在不增加模型容量的情况下整合外部知识成为一个挑战。同时，由于RAG方法多样性导致的研究和工程实践碎片化现象日益严重，需要一个统一的框架来整合现有的RAG技术并指导其未来发展。

Method: 通过综合分析2018年至2025年间关于RAG的学术论文、行业报告和技术实现指南，本研究开发了一套定量评估体系，并基于此将不同RAG技术归类到一个统一的分类体系中。此外，还探讨了这些技术对于建立可信度高、领域适应性强的RAG系统的重要性。

Result: 成功地为RAG技术创建了一个全面的分类学，并提出了一系列定量评价指标来帮助理解各种RAG方法的优势与局限性。同时也强调了在设计RAG解决方案时考虑安全性和领域适应性的必要性。

Conclusion: 本文不仅为理解和评估当前的RAG技术提供了宝贵的资源，而且也为未来该领域内新方法的发展指明了方向。它作为一份实用指南和技术参考手册，支持着更加健壮、安全以及能够适应特定应用场景需求的RAG系统的设计与实施。

Abstract: This article provides a comprehensive systematic literature review of academic studies, industrial applications, and real-world deployments from 2018 to 2025, providing a practical guide and detailed overview of modern Retrieval-Augmented Generation (RAG) architectures. RAG offers a modular approach for integrating external knowledge without increasing the capacity of the model as LLM systems expand. Research and engineering practices have been fragmented as a result of the increasing diversity of RAG methodologies, which encompasses a variety of fusion mechanisms, retrieval strategies, and orchestration approaches. We provide quantitative assessment frameworks, analyze the implications for trust and alignment, and systematically consolidate existing RAG techniques into a unified taxonomy. This document is a practical framework for the deployment of resilient, secure, and domain-adaptable RAG systems, synthesizing insights from academic literature, industry reports, and technical implementation guides. It also functions as a technical reference.

</details>


### [16] [Cross-Document Topic-Aligned Chunking for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05265)
*Mile Stankovic*

Main category: cs.IR

TL;DR: 本文提出了一种跨文档主题对齐(CDTA)切块方法，以解决知识碎片化问题。实验显示，在HotpotQA和UAE法律文本上，CDTA在忠实度和引用准确性方面均优于现有方法，尽管索引成本较高，但对于高查询量应用来说，信息密集的切块减少了查询时的检索需求。


<details>
  <summary>Details</summary>
Motivation: 当前的方法单独分割文档，但复杂的查询需要从多个来源获取信息，这导致了知识碎片化问题。为了解决这一问题，并提高RAG系统的性能，提出了跨文档主题对齐(CDTA)切块方法。

Method: 首先识别跨文档的主题，然后将段落映射到每个主题下，最后将它们综合成统一的切块。通过这种方法，在语料库级别上重建了知识。

Result: 在HotpotQA多跳推理任务中，与上下文检索(0.83)和语义切块(0.78)相比，该方法达到了0.93的忠实度；在UAE法律文本上，达到了0.94的忠实度以及0.93的引用准确性。即使k=3时，其忠实度仍保持在0.91，而语义方法降至0.68。

Conclusion: 虽然CDTA方法的索引成本更高，但它生成的信息密集型切块能够减少查询时间的检索需求，对于具有分布式知识的高查询量应用场景而言，跨文档合成相较于单文档优化有显著改进。

Abstract: Chunking quality determines RAG system performance. Current methods partition documents individually, but complex queries need information scattered across multiple sources: the knowledge fragmentation problem. We introduce Cross-Document Topic-Aligned (CDTA) chunking, which reconstructs knowledge at the corpus level. It first identifies topics across documents, maps segments to each topic, and synthesizes them into unified chunks.
  On HotpotQA multi-hop reasoning, our method reached 0.93 faithfulness versus 0.83 for contextual retrieval and 0.78 for semantic chunking, a 12% improvement over current industry best practice (p < 0.05). On UAE Legal texts, it reached 0.94 faithfulness with 0.93 citation accuracy. At k = 3, it maintains 0.91 faithfulness while semantic methods drop to 0.68, with a single CDTA chunk containing information requiring multiple traditional fragments.
  Indexing costs are higher, but synthesis produces information-dense chunks that reduce query-time retrieval needs. For high-query-volume applications with distributed knowledge, cross-document synthesis improves measurably over within-document optimization.

</details>


### [17] [Retrieval-Augmented Multi-LLM Ensemble for Industrial Part Specification Extraction](https://arxiv.org/abs/2601.05266)
*Muzakkiruddin Ahmed Mohammed,John R. Talburt,Leon Claasssens,Adriaan Marais*

Main category: cs.IR

TL;DR: 本文提出了一种名为RAGsemble的多大型语言模型集成框架，旨在从非结构化文本中提取工业部件规格。该框架通过整合九种先进的大型语言模型，并结合FAISS基于语义检索来提高输出准确性。实验结果表明，与领先的单一模型基线相比，该方法在提取精度、技术完整性和结构化输出质量方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 工业部件规格从非结构化文本中的抽取仍然是制造、采购和维护领域的一个持续挑战，手动处理既耗时又容易出错。为了克服这一问题以及单一模型系统的局限性，提出了本研究解决方案。

Method: 引入了RAGsemble框架，它是一个三阶段管道结构，集成了Gemini、OpenAI、Mistral Large和Gemma等家族的九个先进大型语言模型。此框架利用FAISS进行基于语义的检索以增强事实数据支持，并通过三个步骤执行：（1）由不同LLM并行提取；（2）利用高性能模型进行有针对性的研究增强；（3）智能合成具有冲突解决和置信度感知评分的结果。

Result: 使用真实工业数据集的实验结果显示，在提取准确性、技术完备性和结构化输出质量上，RAGsemble相对于领先的单一大型语言模型基准有显著改进。

Conclusion: 这项工作为工业领域提供了一个可扩展的集成架构，实现了整个流程中无缝的RAG集成，建立了全面的质量评估机制，并准备就绪可用于知识密集型制造业环境部署的解决方案。

Abstract: Industrial part specification extraction from unstructured text remains a persistent challenge in manufacturing, procurement, and maintenance, where manual processing is both time-consuming and error-prone. This paper introduces a retrieval-augmented multi-LLM ensemble framework that orchestrates nine state-of-the-art Large Language Models (LLMs) within a structured three-phase pipeline. RAGsemble addresses key limitations of single-model systems by combining the complementary strengths of model families including Gemini (2.0, 2.5, 1.5), OpenAI (GPT-4o, o4-mini), Mistral Large, and Gemma (1B, 4B, 3n-e4b), while grounding outputs in factual data using FAISS-based semantic retrieval. The system architecture consists of three stages: (1) parallel extraction by diverse LLMs, (2) targeted research augmentation leveraging high-performing models, and (3) intelligent synthesis with conflict resolution and confidence-aware scoring. RAG integration provides real-time access to structured part databases, enabling the system to validate, refine, and enrich outputs through similarity-based reference retrieval. Experimental results using real industrial datasets demonstrate significant gains in extraction accuracy, technical completeness, and structured output quality compared to leading single-LLM baselines. Key contributions include a scalable ensemble architecture for industrial domains, seamless RAG integration throughout the pipeline, comprehensive quality assessment mechanisms, and a production-ready solution suitable for deployment in knowledge-intensive manufacturing environments.

</details>


### [18] [Separating Semantic Expansion from Linear Geometry for PubMed-Scale Vector Search](https://arxiv.org/abs/2601.05268)
*Rob Koopman*

Main category: cs.IR

TL;DR: This paper introduces a PubMed scale retrieval system that uses a large language model to expand natural language queries into biomedical phrases, and then performs retrieval in a fixed, approximately isotropic embedding space without training any parameters. The evaluation is based on geometric properties.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to develop an efficient and scalable retrieval framework for the biomedical literature, which can interpret natural language queries and retrieve relevant information from a vast corpus such as MEDLINE, while separating semantic interpretation from the metric geometry of the document space.

Method: The method involves using a large language model to expand natural language queries into concise biomedical phrases. The retrieval operates in a pre-defined, mean-free, and nearly isotropic embedding space. Each document and query vector is created by averaging token embeddings, removing nuisance axes, and compressing with a Johnson-Lindenstrauss transform. Retrieval is done using exact cosine search on 256-dimensional int8 vectors across the entire MEDLINE corpus.

Result: The results show that the system can retrieve coherent biomedical clusters from the full MEDLINE corpus (around 40 million records) using exact cosine search. The evaluation, which focuses on geometric properties like head cosine, compactness, centroid closure, and isotropy, indicates that the proposed method outperforms random vector baselines. Recall is not applicable due to the nature of the language-model expansion.

Conclusion: The conclusion is that the proposed retrieval framework, which does not require training and separates semantic processing from geometric representation, can effectively handle PubMed scale retrieval tasks, demonstrating its potential for large-scale biomedical information retrieval.

Abstract: We describe a PubMed scale retrieval framework that separates semantic interpretation from metric geometry. A large language model expands a natural language query into concise biomedical phrases; retrieval then operates in a fixed, mean free, approximately isotropic embedding space. Each document and query vector is formed as a weighted mean of token embeddings, projected onto the complement of nuisance axes and compressed by a Johnson Lindenstrauss transform. No parameters are trained. The system retrieves coherent biomedical clusters across the full MEDLINE corpus (about 40 million records) using exact cosine search on 256 dimensional int8 vectors. Evaluation is purely geometric: head cosine, compactness, centroid closure, and isotropy are compared with random vector baselines. Recall is not defined, since the language-model expansion specifies the effective target set.

</details>


### [19] [Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach](https://arxiv.org/abs/2601.05269)
*Yoav Evron,Michal Bar-Asher Siegal,Michael Fire*

Main category: cs.IR

TL;DR: 本文介绍了一种快速且可扩展的AI方法，用于检测、提取和描述数字化手稿中的插图，通过三阶段流程实现了对大规模手稿图像内容的有效分析与检索。


<details>
  <summary>Details</summary>
Motivation: 随着数字档案为历史文献提供了前所未有的访问机会，但系统性地大规模研究插图仍然具有挑战性。因此，作者们希望通过开发一种新的AI方法来解决这一问题，使得学者能够更高效地进行视觉材料的研究。

Method: 该研究采用了三阶段的方法：首先使用微调过的图像分类模型过滤掉纯文本页面；其次利用高效的对象检测模型识别并裁剪出插图；最后运用多模态图像标注模型生成简洁的人类可读描述。这些信息被存储在一个可搜索数据库中。

Result: 应用此管道处理超过三百万页的数字化手稿后，自动识别并提取了超过20万张独特的插图。每页处理时间少于0.06秒，远超传统分割技术在效率和可访问性方面的表现。

Conclusion: 这项工作展示了尖端AI工具如何深刻改变学术工作流，并为数字手稿时代的跨学科研究开辟新途径。

Abstract: The recent Artificial Intelligence (AI) revolution has opened transformative possibilities for the humanities, particularly in unlocking the visual content embedded in historical manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically study illustrations at a large scale remains challenging. Our study presents a fast and scalable AI approach for detecting, extracting, and describing illustrations in digitized manuscripts. Focusing on collections like the Vatican Library, our system enables efficient visual analysis across millions of pages. Our pipeline consists of three stages: (1) a fine-tuned image classification model filters out text-only pages; (2) an efficient object detection model identifies and crops illustrations; and (3) a multimodal image captioning model generates concise, human-readable descriptions. These are stored in a searchable database, allowing scholars to retrieve relevant visual materials through keyword queries. By harnessing the power of recent AI advancements, we enable large-scale visual research that was previously impractical, empowering scholars in historical studies, art history, and cultural heritage to explore visual motifs, artistic styles, and cross-cultural influences with new precision and speed. Applying our pipeline to over three million digitized manuscript pages, we automatically identified and extracted more than 200,000 unique illustrations. This scale of processing in under 0.06 seconds per page, dramatically outperforms traditional segmentation techniques in both efficiency and accessibility for visual scholarship. Our work demonstrates how cutting-edge AI tools can profoundly reshape scholarly workflows and open new avenues for multidisciplinary research in the age of digital manuscripts.

</details>


### [20] [RECOR: Reasoning-focused Multi-turn Conversational Retrieval Benchmark](https://arxiv.org/abs/2601.05461)
*Mohammed Ali,Abdelrahman Abdallah,Amit Agarwal,Hitesh Laxmichand Patel,Adam Jatowt*

Main category: cs.IR

TL;DR: 本文提出了一种基于推理的对话信息检索基准，包含11个领域中的707次对话（2,971轮），并通过分解与验证框架保证质量。综合评估表明结合对话历史和推理可以显著提高检索性能，但隐式推理仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准将多轮对话和密集推理检索分开处理，然而实际的信息寻求需要两者兼备。为了解决这一问题，研究者们提出了一个旨在整合这两方面的新的基准。

Method: 研究者引入了一个名为“分解与验证”的框架来转变复杂查询为基于事实的多轮对话，并通过多层次验证确保质量，在每个对话回合中生成显式的检索推理。

Result: 综合评估显示，当结合对话历史和推理时，检索性能翻倍（基线.236 → 历史+推理.479 nDCG@10），并且专注于推理的模型明显优于密集编码器。不过，对于文本中未明确陈述逻辑联系的情况下的隐式推理仍然具有挑战性。

Conclusion: 这项研究表明，通过结合对话上下文与明确的推理过程可以大幅提高信息检索的效果；但是，对于那些没有直接在文本中表达出来的逻辑关系进行推理依然是未来工作的一个重要方向。

Abstract: Existing benchmarks treat multi-turn conversation and reasoning-intensive retrieval separately, yet real-world information seeking requires both. To bridge this gap, we present a benchmark for reasoning-based conversational information retrieval comprising 707 conversations (2,971 turns) across eleven domains. To ensure quality, our Decomposition-and-Verification framework transforms complex queries into fact-grounded multi-turn dialogues through multi-level validation, where atomic facts are verified against sources and explicit retrieval reasoning is generated for each turn. Comprehensive evaluation reveals that combining conversation history with reasoning doubles retrieval performance (Baseline .236 $\rightarrow$ History+Reasoning .479 nDCG@10), while reasoning-specialized models substantially outperform dense encoders. Despite these gains, further analysis highlights that implicit reasoning remains challenging, particularly when logical connections are not explicitly stated in the text.

</details>


### [21] [Efficient Temporal-aware Matryoshka Adaptation for Temporal Information Retrieval](https://arxiv.org/abs/2601.05549)
*Tuan-Luc Huynh,Weiqing Wang,Trung Le,Thuy-Trang Vu,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: 提出了一种名为TMRL的方法，通过引入时间子空间来提高检索器在时间相关上下文中的检索能力，从而改善RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统中检索器无法有效检索时间相关上下文的问题，这会影响下游生成任务的表现。

Method: 采用一种叫做Temporal-aware Matryoshka Representation Learning (TMRL)的新方法，该方法利用Matryoshka嵌入的嵌套结构来添加一个时间子空间，同时保持一般的语义表示。

Result: 实验表明，TMRL能够有效地适应多种文本嵌入模型，并且在时间检索和时间RAG表现上与之前基于Matryoshka但不考虑时间的方法以及先前的时间感知方法相比具有竞争力，同时还允许灵活调整准确性和效率之间的平衡。

Conclusion: TMRL提供了一种改进检索器处理时间相关信息能力的有效手段，有助于提升整体RAG系统的性能。

Abstract: Retrievers are a key bottleneck in Temporal Retrieval-Augmented Generation (RAG) systems: failing to retrieve temporally relevant context can degrade downstream generation, regardless of LLM reasoning. We propose Temporal-aware Matryoshka Representation Learning (TMRL), an efficient method that equips retrievers with temporal-aware Matryoshka embeddings. TMRL leverages the nested structure of Matryoshka embeddings to introduce a temporal subspace, enhancing temporal encoding while preserving general semantic representations. Experiments show that TMRL efficiently adapts diverse text embedding models, achieving competitive temporal retrieval and temporal RAG performance compared to prior Matryoshka-based non-temporal methods and prior temporal methods, while enabling flexible accuracy-efficiency trade-offs.

</details>


### [22] [Autoregressive Ranking: Bridging the Gap Between Dual and Cross Encoders](https://arxiv.org/abs/2601.05588)
*Benjamin Rozonoyer,Chong You,Michael Boratko,Himanshu Jain,Nilesh Gupta,Srinadh Bhojanapalli,Andrew McCallum,Felix Yu*

Main category: cs.IR

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的点生成排名方法，并证明了其在多标记文档ID上的表达能力优于双编码器。此外，作者还引入了一种新的损失函数SToICaL，该函数可以在点式设置中同时在项目和标记级别上整合排名感知监督。实验结果表明，SToICaL能够抑制无效文档ID生成的概率，并且在常见的排名指标上超越了顶级检索表现。


<details>
  <summary>Details</summary>
Motivation: 尽管有大量证据表明预训练的大型语言模型非常适合排名任务，但大多数基于LLM的方法依赖于下一个令牌预测，这是一种本质上不考虑排名的损失函数。为了解决这个问题，研究者们提出了一个名为点生成排名的新方法，旨在结合效率与表达力的优势，并利用因果变换器的上下文能力来实现排名。

Method: 首先，论文通过理论分析证明了当使用多令牌文档ID时，点生成排名方法比对偶编码器具有更高的表达力。接着，研究者设计了一种简化的令牌-项目校准损失函数SToICaL，它能够在单个项目的框架下同时提供针对项目级和令牌级的排名敏感监督。

Result: 实验部分采用了WordNet以及ESCI数据集进行测试。结果显示，SToICaL的两种变体都能够有效减少错误文档ID生成的可能性，并且相比传统的仅关注第一项检索效果的评估标准，在多个排名度量上表现出色。

Conclusion: 这项研究表明，通过采用适当的损失函数如SToICaL，可以显著提高基于LLM的点生成排名系统的性能，特别是在处理需要高精度排名的应用场景中。

Abstract: Dual and cross encoders have long been mainstays of information retrieval (IR), but are being challenged by the emergent capabilities of LLMs. An LLM-based approach we term pointwise generative ranking - generating tokens the length of a single docID as opposed to a list in order to enable ranking via beam search - combines efficiency and expressivity benefits while leveraging the in-context capabilities of Causal Transformers. Although there is ample evidence to suggest that pretrained LLMs are well-suited for ranking, we find that the vast majority of LLM-based approaches rely on next-token prediction, a loss function which is fundamentally rank-agnostic (and especially so with pointwise supervision). In this paper, we first prove that the expressivity of pointwise generative ranking with multi-token docIDs is superior to that of dual encoders. We then propose SToICaL - a Simple Token-Item Calibrated Loss - which can incorporate rank-aware supervision at both the item and token levels within the pointwise setup. We run a suite of experiments on ranking tasks derived from WordNet (Fellbaum, 1998) and ESCI (Reddy et al., arXiv:2206.06588). Two variants of SToICaL successfully suppress the probability of invalid docID generations and improve on common ranking metrics beyond top-1 retrieval.

</details>


### [23] [Revisiting Human-vs-LLM judgments using the TREC Podcast Track](https://arxiv.org/abs/2601.05603)
*Watheq Mansour,J. Shane Culpepper,Joel Mackenzie,Andrew Yates*

Main category: cs.IR

TL;DR: 本文研究了在音频文件转录片段中，大型语言模型（LLMs）与人类专家之间的一致性，并探讨了分歧对系统排名的影响。结果表明，在存在较大分歧的情况下，人类专家更倾向于同意LLMs的判断，而非TREC评估员的原始标注。


<details>
  <summary>Details</summary>
Motivation: 鉴于使用大型语言模型进行相关性标注的重要性日益增加，但不同研究对于LLMs能否达到与真实人类判断高度一致性的结论存在争议。本研究旨在分析LLMs与人类专家之间的一致性问题，并特别关注于由音频文件转录而来的文本数据集上这一现象的表现。

Method: 选取TREC 2020和2021年播客追踪项目中的音频文件转录为两分钟片段作为研究对象；利用五种不同的大型语言模型重新评估所有查询-片段对；对LLMs与TREC评估员意见分歧最大的一小部分样本进行了再次评估。

Result: 发现当LLMs与TREC评估员间存在显著分歧时，人类专家实际上更赞同LLMs的观点。这支持了Sormunen于2002年提出的观点——依赖单一评估者会导致较低的一致性水平。

Conclusion: 研究表明，在处理特定类型的多媒体内容如音频转录文本时，大型语言模型可以提供有价值的补充或替代传统人工标注方法。同时，它也强调了在评估过程中考虑多个来源意见的重要性。

Abstract: Using large language models (LLMs) to annotate relevance is an increasingly important technique in the information retrieval community. While some studies demonstrate that LLMs can achieve high user agreement with ground truth (human) judgments, other studies have argued for the opposite conclusion. To the best of our knowledge, these studies have primarily focused on classic ad-hoc text search scenarios. In this paper, we conduct an analysis on user agreement between LLM and human experts, and explore the impact disagreement has on system rankings. In contrast to prior studies, we focus on a collection composed of audio files that are transcribed into two-minute segments -- the TREC 2020 and 2021 podcast track. We employ five different LLM models to re-assess all of the query-segment pairs, which were originally annotated by TREC assessors. Furthermore, we re-assess a small subset of pairs where LLM and TREC assessors have the highest disagreement, and found that the human experts tend to agree with LLMs more than with the TREC assessors. Our results reinforce the previous insights of Sormunen in 2002 -- that relying on a single assessor leads to lower user agreement.

</details>


### [24] [Statistical Foundations of DIME: Risk Estimation for Practical Index Selection](https://arxiv.org/abs/2601.05649)
*Giulio D'Erasmo,Cesare Campagnano,Antonio Mallia,Pierpaolo Brutti,Nicola Tonellotto,Fabrizio Silvestri*

Main category: cs.IR

TL;DR: 本文提出了一种统计学方法，可以在推理时直接为每个查询确定最佳维度集，从而在保持效果的同时平均减少约50%的嵌入大小。


<details>
  <summary>Details</summary>
Motivation: 现有的高维密集嵌入技术虽然对于现代信息检索至关重要，但存在很多噪声或冗余维度。DIME（Dimension Importance Estimation）虽能提供查询依赖性的评分来识别嵌入中的信息成分，但其需要通过昂贵的网格搜索预先选定所有查询语料库嵌入的维度。

Method: 本研究开发了一个基于统计的方法，在推理时间直接确定每个查询的最佳维度集合，无需事先对整个查询语料库进行维度选择。

Result: 实验表明，该方法不仅能够达到与现有方法相当的有效性，而且能够在不同模型和数据集上将嵌入大小平均减少约50%。

Conclusion: 新提出的统计学标准能够在保证检索效果的前提下显著降低嵌入向量的维度，进而提高效率。

Abstract: High-dimensional dense embeddings have become central to modern Information Retrieval, but many dimensions are noisy or redundant. Recently proposed DIME (Dimension IMportance Estimation), provides query-dependent scores to identify informative components of embeddings. DIME relies on a costly grid search to select a priori a dimensionality for all the query corpus's embeddings. Our work provides a statistically grounded criterion that directly identifies the optimal set of dimensions for each query at inference time. Experiments confirm achieving parity of effectiveness and reduces embedding size by an average of $\sim50\%$ across different models and datasets at inference time.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [DafnyPro: LLM-Assisted Automated Verification for Dafny Programs](https://arxiv.org/abs/2601.05385)
*Debangshu Banerjee,Olivier Bouissou,Stefan Zetzsche*

Main category: cs.SE

TL;DR: 介绍了DafnyPro框架，它通过包含差异检查器、剪枝器和提示增强系统三个关键组件来提高LLM在生成Dafny验证注释方面的能力。该框架显著提高了Claude Sonnet 3.5和3.7模型在四个基准测试中的表现，并且经过DafnyPro增强的大模型训练数据微调后的较小Qwen模型也达到了较高的验证准确率。


<details>
  <summary>Details</summary>
Motivation: 为了改进大型语言模型（LLMs）在生成Dafny程序验证注解时的表现，提出了一种名为DafnyPro的新框架。

Method: 开发了DafnyPro框架，其中包括：防止修改基础程序逻辑的差异检查器、去除不必要不变量的剪枝器以及应用预定义证明策略的提示增强系统。此外，还对两个Qwen模型进行了基于DafnyPro增强后大模型尝试验证的数据集进行微调。

Result: 实验结果表明，在最具挑战性的DafnyBench基准测试中，使用DafnyPro增强的Claude Sonnet 3.5模型比基础模型正确证明的比例提高了16个百分点，达到86%。另外，经过微调的7B和14B Qwen模型分别实现了68%和70%的正确证明率。

Conclusion: DafnyPro有效地提升了不同规模模型在Dafny程序验证任务上的性能，即使是较小规模的模型也能保持较高的验证准确性。

Abstract: We present DafnyPro, an inference-time framework that enhances LLMs for generating verification annotations in Dafny. DafnyPro comprises three key components: a diff-checker that prevents modifications to base program logic, a pruner that removes unnecessary invariants, and a hint-augmentation system that retrieves and applies predefined, problem-independent proof strategies. We evaluate DafnyPro using Claude Sonnet 3.5 and 3.7 on four benchmarks: Clover, MBPP-Dafny, HumanEval-Dafny, and DafnyBench, achieving consistent performance gains in all cases. Notably, on DafnyBench, the most challenging benchmark, Claude Sonnet 3.5 enhanced with DafnyPro achieves 86% correct proofs, a 16 pp improvement over the base model. We also fine-tune two Qwen models on training data derived from verification attempts by larger models enhanced with DafnyPro. Our 7B and 14B models achieve 68% and 70% correct proofs on DafnyBench, respectively, demonstrating that smaller models can maintain high verification accuracy.

</details>


### [26] [Uncovering Failures in Cyber-Physical System State Transitions: A Fuzzing-Based Approach Applied to sUAS](https://arxiv.org/abs/2601.05449)
*Theodore Chambers,Arturo Miguel Russell Bernal,Michael Vierhauser,Jane Cleland-Huang*

Main category: cs.SE

TL;DR: 本研究介绍了一种名为SaFUZZ的状态感知模糊测试流程，用于验证小型无人驾驶航空系统(sUAS)在不同时间和环境条件下的核心行为。通过创建模糊测试规范来检测行为偏差，并生成故障树以帮助分析失败原因。经由高保真模拟环境和实际sUAS实地测试验证，SaFUZZ能够有效揭示真实世界应用中的多种状态转换失败情况。


<details>
  <summary>Details</summary>
Motivation: 随着小型无人驾驶航空系统(sUAS)在各种安全关键环境中越来越多地部署，对其机载决策逻辑在不同条件下的严格验证变得尤为重要。

Method: 提出了SaFUZZ，一种状态感知的模糊测试流程，它能够验证与状态转换、自动故障保护以及人类操作员交互相关的核心行为；创建了用于检测行为偏差的模糊测试规范，并动态生成相关的故障树来可视化导致失败的状态、模式及环境因素。

Result: 通过对一个现实世界的sUAS系统进行SaFUZZ验证，发现了开发团队之前未检测到的多个故障点。模糊测试在一个高保真度的模拟环境中执行，并且结果在物理sUAS的真实世界场地测试中得到了验证。

Conclusion: 研究表明，SaFUZZ能够为发现真实世界sUAS应用程序中的多样化状态转换失败提供实用且可扩展的方法。

Abstract: The increasing deployment of small Uncrewed Aerial Systems (sUAS) in diverse and often safety-critical environments demands rigorous validation of onboard decision logic under various conditions. In this paper, we present SaFUZZ, a state-aware fuzzing pipeline that validates core behavior associated with state transitions, automated failsafes, and human operator interactions in sUAS applications operating under various timing conditions and environmental disturbances. We create fuzzing specifications to detect behavioral deviations, and then dynamically generate associated Fault Trees to visualize states, modes, and environmental factors that contribute to the failure, thereby helping project stakeholders to analyze the failure and identify its root causes. We validated SaFUZZ against a real-world sUAS system and were able to identify several points of failure not previously detected by the system's development team. The fuzzing was conducted in a high-fidelity simulation environment, and outcomes were validated on physical sUAS in a real-world field testing setting. The findings from the study demonstrated SaFUZZ's ability to provide a practical and scalable approach to uncovering diverse state transition failures in a real-world sUAS application.

</details>


### [27] [Rethinking Basis Path Testing: Mixed Integer Programming Approach for Test Path Set Generation](https://arxiv.org/abs/2601.05463)
*Chao Wei,Xinyi Peng,Yawen Yan,Mao Luo,Ting Cai*

Main category: cs.SE

TL;DR: 本文提出了一种混合整数规划（MIP）框架来生成全局最优的基础路径集，通过两种策略：整体MIP模型和可扩展的增量MIP策略，后者在保证计算效率的同时实现了100%的成功率。


<details>
  <summary>Details</summary>
Motivation: 基础路径测试是结构化测试的核心，但传统的自动方法依赖于贪婪图遍历算法（如DFS/BFS），通常会生成次优路径。这种结构性缺陷直接影响了下游测试活动，增加了自动生成测试数据的复杂性和工程师的认知负担。

Method: 作者将基础路径生成从一个过程性搜索任务重新定义为一个声明式的优化问题，并引入了一个旨在产生结构上最简化的完整基础路径集的MIP框架。该框架包含两种互补策略：一种确保理论最优路径集的整体MIP模型，以及针对大型复杂拓扑结构设计的可扩展增量MIP策略。增量方法采用一个多目标函数，优先考虑路径简洁性，并加入新颖性惩罚以最大化线性独立路径的生成。

Result: 通过对真实代码和大规模合成控制流图的实证评估表明，所提出的增量MIP策略在生成完整的基路径集方面达到了100%的成功率，同时保持了较高的计算效率。

Conclusion: 这项工作提供了一种基础方法，用于生成高质量的结构“支架”，这可以提高后续测试生成努力的效率和效果。

Abstract: Basis path testing is a cornerstone of structural testing, yet traditional automated methods, relying on greedy graph-traversal algorithms (e.g., DFS/BFS), often generate sub-optimal paths. This structural inferiority is not a trivial issue; it directly impedes downstream testing activities by complicating automated test data generation and increasing the cognitive load for human engineers. This paper reframes basis path generation from a procedural search task into a declarative optimization problem. We introduce a Mixed Integer Programming (MIP) framework designed to produce a complete basis path set that is globally optimal in its structural simplicity. Our framework includes two complementary strategies: a Holistic MIP model that guarantees a theoretically optimal path set, and a scalable Incremental MIP strategy for large, complex topologies. The incremental approach features a multi-objective function that prioritizes path simplicity and incorporates a novelty penalty to maximize the successful generation of linearly independent paths. Empirical evaluations on both real-code and large-scale synthetic Control Flow Graphs demonstrate that our Incremental MIP strategy achieves a 100\% success rate in generating complete basis sets, while remaining computationally efficient. Our work provides a foundational method for generating a high-quality structural "scaffold" that can enhance the efficiency and effectiveness of subsequent test generation efforts.

</details>


### [28] [STELP: Secure Transpilation and Execution of LLM-Generated Programs](https://arxiv.org/abs/2601.05467)
*Swapnil Shinde,Sahil Wadhwa,Andy Luo,Emily Chen*

Main category: cs.SE

TL;DR: 本文讨论了大型语言模型生成代码的安全性和可靠性问题，并提出了一种名为STELP的解决方案，该方案能够以受控且安全的方式执行由LLM生成的代码。通过人类验证的数据集和公开数据集上的基准测试，结果表明此方法在安全执行风险代码片段方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）快速发展，在推理、规划及函数调用能力上取得了重大进步。然而，直接将这些模型生成的代码应用于生产软件开发系统中存在诸多问题，如代码不稳定或错误、包含数据污染、恶意攻击等漏洞，这阻碍了这类代码在生产AI系统中的应用。传统安全测试工具与人工审查在此情境下既不实用也不可靠。

Method: 提出了一个名为STELP（Secure Transpiler and Executor of LLM-Generated Program）的系统，旨在以可控且安全的方式执行由LLM生成的代码。此外，还贡献了一个经过人工验证的不安全代码片段数据集，并基于公共可用数据集对正确性、安全性和延迟进行了基准测试。

Result: 实验结果显示，提出的STELP方法在安全性执行风险代码片段方面显著优于现有的方法。

Conclusion: STELP为解决使用LLM自动生成代码时遇到的安全性和可靠性挑战提供了一条新途径，适用于无需人工监督即可自动产生并立即执行代码的应用场景。

Abstract: Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.

</details>


### [29] [Readability-Robust Code Summarization via Meta Curriculum Learning](https://arxiv.org/abs/2601.05485)
*Wenhao Zeng,Yitian Chai,Hao Zhou,Fandong Meng,Jie Zhou,Xiaodong Gu*

Main category: cs.SE

TL;DR: 本文研究了当前代码摘要模型在处理可读性差的代码时的表现，并提出了一种新的微调方法RoFTCodeSum，以增强模型对低质量代码的鲁棒性。实验结果表明，RoFTCodeSum不仅能提高模型对语义扰动的鲁棒性，同时也能改善其在原始代码上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的代码语言模型主要针对高可读性的代码进行优化，但在面对结构不良或混淆过的实际代码时，性能显著下降。为了提升模型在处理这类代码时的鲁棒性和效果，作者展开了本项研究。

Method: 通过实证评估现有先进模型（如GPT-4o和DeepSeek-V3）在处理低可读性代码时的表现，分析了提示工程的作用及不同变体的稳定性。基于发现的问题，提出了结合课程学习与元学习概念的新微调方法RoFTCodeSum，该方法通过对训练数据集逐步增加难度来优化模型参数。

Result: 实验结果显示，虽然最新的模型在遭遇低可读性代码时表现不佳且提示工程带来的改进有限，但采用RoFTCodeSum方法后，模型不仅提高了对语义扰动的抵抗能力，还在处理正常代码方面展现了更好的性能。

Conclusion: RoFTCodeSum提供了一种有效增强代码摘要模型鲁棒性的新途径，特别是在处理难以理解的代码时展现出优越性，为未来的研究提供了有价值的方向。

Abstract: Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.

</details>


### [30] [Empirical Characterization of Logging Smells in Machine Learning Code](https://arxiv.org/abs/2601.05540)
*Patrick Loic Foalem,Leuson Da Silva,Foutse Khomh,Ettore Merlo,Heng Li*

Main category: cs.SE

TL;DR: 本研究旨在通过大规模挖掘GitHub上开源的机器学习仓库，识别并描述机器学习系统中的常见日志记录问题（logging smells），并通过调查问卷的形式评估这些问题的相关性、严重性和频率。尽管研究结果可能无法推广到闭源工业项目，但该研究为理解并改进机器学习开发中的日志记录实践提供了重要步骤。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习组件越来越多地融入软件系统中，有效的日志记录对于确保模型训练和部署过程中的可重复性、可追溯性和可观测性变得至关重要。然而，目前尚不清楚实际操作中这些工具是如何被使用的，或者机器学习从业者是否采用了统一且有效的日志策略。此外，还没有实证研究系统地描述了机器学习系统中反复出现的日志记录不佳做法。

Method: 研究方法包括对GitHub上托管的开源机器学习库进行大规模挖掘以编目常见的日志记录问题，并随后针对机器学习工程师开展一项从业者调查，以评估所识别出的问题的重要性、严重程度及发生频率。

Result: 研究预计会发现机器学习系统中存在的不同类型的日志记录问题，并基于调查结果给出这些问题在实际工作中的相关性、严重性以及出现频率的评估。

Conclusion: 虽然这项研究的结果可能不适用于封闭源代码的工业项目，但它为理解和改善机器学习开发中的日志记录实践提供了一个重要的起点。

Abstract: \underline{Context:} Logging is a fundamental yet complex practice in software engineering, essential for monitoring, debugging, and auditing software systems. With the increasing integration of machine learning (ML) components into software systems, effective logging has become critical to ensure reproducibility, traceability, and observability throughout model training and deployment. Although various general-purpose and ML-specific logging frameworks exist, little is known about how these tools are actually used in practice or whether ML practitioners adopt consistent and effective logging strategies. To date, no empirical study has systematically characterized recurring bad logging practices--or logging smells--in ML System. \underline{Goal:} This study aims to empirically identify and characterize logging smells in ML systems, providing an evidence-based understanding of how logging is implemented and challenged in practice. \underline{Method:} We propose to conduct a large-scale mining of open-source ML repositories hosted on GitHub to catalogue recurring logging smells. Subsequently, a practitioner survey involving ML engineers will be conducted to assess the perceived relevance, severity, and frequency of the identified smells. \underline{Limitations:} % While The study's limitations include that While our findings may not be generalizable to closed-source industrial projects, we believe our study provides an essential step toward understanding and improving logging practices in ML development.

</details>


### [31] [Understanding LLM-Driven Test Oracle Generation](https://arxiv.org/abs/2601.05542)
*Adam Bodicoat,Gunel Jahangirova,Valerio Terragni*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型（LLMs）在生成测试预言以暴露软件故障方面的有效性，并探讨了不同的提示策略和上下文输入水平如何影响LLM生成预言的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化单元测试生成技术主要依赖于被测类的实现行为来生成回归预言，但它们未能解决预言问题：即区分正确与不正确的程序行为。随着基础模型特别是大型语言模型的兴起，存在一个新机遇去生成反映预期行为的测试预言。

Method: 通过实证研究的方法，探索不同提示策略及上下文输入程度对LLM生成预言质量的影响。

Result: 研究结果提供了关于基于LLM预言生成的优点和局限性的见解，增强了我们对其能力的理解，并促进了该领域的未来研究。

Conclusion: 研究表明，大型语言模型在生成能够揭示软件缺陷的测试预言方面展现出了潜力，但其效果受到提示策略和所提供上下文信息量的影响。

Abstract: Automated unit test generation aims to improve software quality while reducing the time and effort required for creating tests manually. However, existing techniques primarily generate regression oracles that predicate on the implemented behavior of the class under test. They do not address the oracle problem: the challenge of distinguishing correct from incorrect program behavior. With the rise of Foundation Models (FMs), particularly Large Language Models (LLMs), there is a new opportunity to generate test oracles that reflect intended behavior. This positions LLMs as enablers of Promptware, where software creation and testing are driven by natural-language prompts. This paper presents an empirical study on the effectiveness of LLMs in generating test oracles that expose software failures. We investigate how different prompting strategies and levels of contextual input impact the quality of LLM-generated oracles. Our findings offer insights into the strengths and limitations of LLM-based oracle generation in the FM era, improving our understanding of their capabilities and fostering future research in this area.

</details>


### [32] [An Empirical Study of Policy-as-Code Adoption in Open-Source Software Projects](https://arxiv.org/abs/2601.05555)
*Patrick Loic Foalem,Foutse Khomh,Leuson Da Silva,Ettore Merlo*

Main category: cs.SE

TL;DR: 本研究通过对399个使用了九种广泛采用的政策即代码（PaC）工具的GitHub仓库进行分析，结合定量和定性方法以及大型语言模型辅助分类，揭示了PaC工具在开源软件中的多样性和具体应用模式。研究发现为实践者和工具开发者提供了关于如何改进工具互操作性的见解，并为进一步研究PaC实践及其在确保可信、合规软件系统中的作用奠定了实证基础。


<details>
  <summary>Details</summary>
Motivation: 随着组织越来越多地采用政策即代码(PaC)工具，将治理、合规性和安全要求直接嵌入到软件系统中已经成为一种基础方法。然而，软件工程社区缺乏对这些工具在实际开发实践中如何使用的实证理解。因此，本研究旨在通过开展首个大规模PaC使用情况的研究来填补这一空白，以了解PaC工具是如何被采纳的、它们服务于什么目的以及它们支持哪些治理活动。

Method: 研究者们分析了利用九种广泛应用的PaC工具的399个GitHub仓库。他们采用了混合方法论，结合了工具使用与项目特征的定量分析及政策文件的定性调查。此外，还使用了一种经过专家验证的大规模语言模型(LLM)辅助分类流程，以此发展出一个由5个类别和15个子类别组成的PaC使用分类法。

Result: 研究表明，在PaC采纳方面存在显著多样性。PaC工具经常被用于早期阶段项目，并且主要关注于治理、配置控制和文档化。同时观察到了在MLOps管道中新兴的PaC使用趋势，以及明显的共用模式，比如OPA与Gatekeeper之间的搭配使用。所发展的分类法突出了重复出现的治理意图。

Conclusion: 该研究的结果为从业者和工具开发者提供了可操作的见解，强调了具体的PaC使用模式，并激发了提高工具间互操作性的机会。这项工作为未来关于PaC实践及其在确保可信、合规软件系统中角色的研究奠定了实证基础。

Abstract: \textbf{Context:} Policy-as-Code (PaC) has become a foundational approach for embedding governance, compliance, and security requirements directly into software systems. While organizations increasingly adopt PaC tools, the software engineering community lacks an empirical understanding of how these tools are used in real-world development practices.
  \textbf{Objective:} This paper aims to bridge this gap by conducting the first large-scale study of PaC usage in open-source software. Our goal is to characterize how PaC tools are adopted, what purposes they serve, and what governance activities they support across diverse software ecosystems.
  \textbf{Method:} We analyzed 399 GitHub repositories using nine widely adopted PaC tools. Our mixed-methods approach combines quantitative analysis of tool usage and project characteristics with a qualitative investigation of policy files. We further employ a Large Language Model (LLM)--assisted classification pipeline, refined through expert validation, to derive a taxonomy of PaC usage consisting of 5 categories and 15 sub-categories.
  \textbf{Results:} Our study reveals substantial diversity in PaC adoption. PaC tools are frequently used in early-stage projects and are heavily oriented toward governance, configuration control, and documentation. We also observe emerging PaC usage in MLOps pipelines and strong co-usage patterns, such as between OPA and Gatekeeper. Our taxonomy highlights recurring governance intents.
  \textbf{Conclusion:} Our findings offer actionable insights for practitioners and tool developers. They highlight concrete usage patterns, emphasize actual PaC usage, and motivate opportunities for improving tool interoperability. This study lays the empirical foundation for future research on PaC practices and their role in ensuring trustworthy, compliant software systems.

</details>


### [33] [Package-Aware Approach for Repository-Level Code Completion in Pharo](https://arxiv.org/abs/2601.05617)
*Omar Abedelkader,Stéphane Ducasse,Oleksandr Zaitsev,Romain Robbes,Guillermo Polito*

Main category: cs.SE

TL;DR: 本文提出了一种新的启发式方法，用于改进Pharo的代码补全引擎。该方法优先考虑同一包或项目中的类，从而提高了建议的相关性和准确性。


<details>
  <summary>Details</summary>
Motivation: Pharo现有的代码补全系统虽然强大，但在没有考虑到仓库结构，在推荐全局名称如类名、类变量或全局变量时不优先考虑同包或同项目的类。

Method: 新提出的启发式方法首先在请求类所在的包内搜索变量名，然后扩展到同一仓库内的其他包，最后才考虑全局命名空间。

Result: 初步结果表明，采用这种方法后平均倒数排名（MRR）有所提高，证明了基于包意识的补全比之前的纯全局方法提供了更准确和相关的建议。

Conclusion: 通过引入这种新的启发式方法，Pharo的代码补全功能得到了增强，能够更好地反映软件项目的实际结构，从而为开发者提供更加有用的代码补全建议。

Abstract: Pharo offers a sophisticated completion engine based on semantic heuristics, which coordinates specific fetchers within a lazy architecture. These heuristics can be recomposed to support various activities (e.g., live programming or history usage navigation). While this system is powerful, it does not account for the repository structure when suggesting global names such as class names, class variables, or global variables. As a result, it does not prioritize classes within the same package or project, treating all global names equally. In this paper, we present a new heuristic that addresses this limitation. Our approach searches variable names in a structured manner: it begins with the package of the requesting class, then expands to other packages within the same repository, and finally considers the global namespace. We describe the logic behind this heuristic and evaluate it against the default semantic heuristic and one that directly queries the global namespace. Preliminary results indicate that the Mean Reciprocal Rank (MRR) improves, confirming that package-awareness completions deliver more accurate and relevant suggestions than the previous flat global approach.

</details>


### [34] [A Large Scale Empirical Analysis on the Adherence Gap between Standards and Tools in SBOM](https://arxiv.org/abs/2601.05622)
*Chengjie Wang,Jingzheng Wu,Hao Lyu,Xiang Ling,Tianyue Luo,Yanjun Wu,Chen Zhao*

Main category: cs.SE

TL;DR: 本文首次大规模两阶段实证分析了SBOM工具与标准规范之间的遵守差距，揭示了当前SBOM工具在合规支持、工具一致性及软件信息准确性方面的根本性局限，并提供了实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管已有组织提出了SBOM标准并开发了相关工具，但对于这些工具是否严格遵循标准规范的研究较少，这可能导致合规问题和SBOM使用中的中断。

Method: 采用自动化评估框架SAP进行基准评估和为期一年的纵向跟踪研究，涵盖由六种SBOM工具从3,287个真实世界存储库生成的55,444个SBOM。

Result: 发现现有SBOM工具存在对政策要求的支持不足、工具间一致性低（跨语言包检测的一致率仅为7.84%至12.77%）以及随时间变化的一致性差等问题；此外，在详细软件信息如许可证准确度上表现不佳（低于20%）。

Conclusion: 指出了导致这些问题的根本原因，并为改进现状提出了解决方案。所有代码、复制docker镜像及评估结果已开源发布于GitHub和Zenodo平台供进一步研究。

Abstract: A Software Bill of Materials (SBOM) is a machine-readable artifact that systematically organizes software information, enhancing supply chain transparency and security. To facilitate the exchange and utilization of SBOMs, organizations such as the Linux Foundation and OWASP have proposed SBOM standards. Following standards, organizations have developed tools for generating and utilizing SBOMs. However, limited research has examined the adherence of these SBOM tools to standard specifications, a gap that could lead to compliance failures and disruptions in SBOM utilization. This paper presents the first large-scale, two-stage empirical analysis of the adherence gap, using our automated evaluation framework, SAP. The evaluation, comprising a baseline evaluation and a one-year longitudinal follow-up, covers 55,444 SBOMs generated by six SBOM tools from 3,287 real-world repositories. Our analysis reveals persistent, fundamental limitations in current SBOM tools: (1) inadequate compliance support with policy requirements; (2) poor tool consistencies, including inter-tool consistency rates as low as 7.84% to 12.77% for package detection across languages, and significant longitudinal inconsistency, where tools show low consistency with their own prior versions; and (3) mediocre to poor accuracy for detailed software information, e.g., accuracy of package licenses below 20%. We analyze the root causes of these gaps and provide practical solutions. All the code, replication docker image, evaluation results are open sourced at [GitHub](https://github.com/dw763j/SAP) and [Zenodo](https://doi.org/10.5281/zenodo.14998624) for further researches.

</details>


### [35] [Tracing Stereotypes in Pre-trained Transformers: From Biased Neurons to Fairer Models](https://arxiv.org/abs/2601.05663)
*Gianmario Voria,Moses Openja,Foutse Khomh,Gemma Catolino,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文探讨了基于Transformer的语言模型在软件工程中的应用及其潜在的偏见问题，通过构建一个包含九种偏见类型的有偏关系数据集，并调整神经元归属策略来追踪和抑制BERT模型中的有偏神经元。研究发现，有偏知识局限于少量神经元子集中，抑制这些神经元能够在最小化性能损失的情况下大幅减少偏见，为软件工程领域提供了一种可解释的方法来提升公平性。


<details>
  <summary>Details</summary>
Motivation: 随着基于Transformer的语言模型重塑了AI系统处理和生成文本的方式，在软件工程中这些模型支持着各种活动并加速自动化与决策过程。然而，证据表明这些模型可能会复制或放大社会偏见，引发公平性担忧。受近期关于神经元编辑工作的启发，本研究假设存在捕捉预训练Transformer内刻板印象关联的有偏神经元。

Method: 首先，建立了一个包含九种偏见类型的数据集，用于编码刻板印象的三元组。接着，调整神经元归因策略以追踪并在BERT模型中抑制有偏神经元。最后，评估了抑制操作对软件工程任务的影响。

Result: 研究结果表明，有偏知识集中在一小部分神经元子集中，而抑制这些特定神经元能够显著降低偏见，同时对模型整体性能影响极小。

Conclusion: 研究表明，通过对特定神经元进行干预可以在不显著牺牲性能的前提下有效减轻基于Transformer的语言模型中存在的偏见问题，为软件工程领域内的公平性提供了新的视角。

Abstract: The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.

</details>


### [36] [Drivora: A Unified and Extensible Infrastructure for Search-based Autonomous Driving Testing](https://arxiv.org/abs/2601.05685)
*Mingfei Cheng,Lionel Briand,Yuan Zhou*

Main category: cs.SE

TL;DR: Drivora, a unified and extensible testing infrastructure for autonomous driving systems (ADSs) built on CARLA, introduces a standardized scenario definition, decouples the testing engine, scenario execution, and ADS integration, and supports 12 ADSs through a unified interface.


<details>
  <summary>Details</summary>
Motivation: 鉴于现有基于搜索的测试方法在不同框架（如不同的场景空间、模拟器和自动驾驶系统）间重用与适应时需要大量工作，因此开发了一个统一且可扩展的基础设施来解决这些问题。

Method: 提出了Drivora，一个基于广泛使用的CARLA模拟器构建的统一且可扩展的ADS测试基础设施。它引入了使用低级可执行参数指定场景的统一场景定义OpenScenario，解耦了测试引擎、场景执行以及ADS集成，并通过进化计算探索新场景。

Result: Drivora能够支持灵活定制核心组件，利用并行执行机制最大化硬件利用率以进行大规模批量仿真，并通过统一接口提供对12个ADS的支持，从而简化配置和新ADS的整合。

Conclusion: Drivora为基于搜索的ADS测试提供了统一且易于扩展的基础架构，促进了不同设置下的重用性和适应性，同时支持新的测试设计。

Abstract: Search-based testing is critical for evaluating the safety and reliability of autonomous driving systems (ADSs). However, existing approaches are often built on heterogeneous frameworks (e.g., distinct scenario spaces, simulators, and ADSs), which require considerable effort to reuse and adapt across different settings. To address these challenges, we present Drivora, a unified and extensible infrastructure for search-based ADS testing built on the widely used CARLA simulator. Drivora introduces a unified scenario definition, OpenScenario, that specifies scenarios using low-level, actionable parameters to ensure compatibility with existing methods while supporting extensibility to new testing designs (e.g., multi-autonomous-vehicle testing). On top of this, Drivora decouples the testing engine, scenario execution, and ADS integration. The testing engine leverages evolutionary computation to explore new scenarios and supports flexible customization of core components. The scenario execution can run arbitrary scenarios using a parallel execution mechanism that maximizes hardware utilization for large-scale batch simulation. For ADS integration, Drivora provides access to 12 ADSs through a unified interface, streamlining configuration and simplifying the incorporation of new ADSs. Our tools are publicly available at https://github.com/MingfeiCheng/Drivora.

</details>


### [37] [AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training](https://arxiv.org/abs/2601.05703)
*Wiebe Vandendriessche,Jordi Thijsman,Laurens D'hooge,Bruno Volckaert,Merlijn Sebrechts*

Main category: cs.SE

TL;DR: 本文介绍了一种名为AI Bill of Materials (AIBOM)的工具，作为训练过的AI模型及其环境的标准化、可验证记录。通过自动化平台AIBoMGen生成签名的AIBOMs，该平台在训练过程中捕获数据集、模型元数据和环境细节，并利用加密哈希、数字签名等技术确保完整性，防止不诚实模型创建者的篡改。评估显示AIBoMGen能够可靠地检测到所有人工制品未经授权的修改，并以几乎可以忽略不计的性能开销生成AIBOMs，为构建安全透明的人工智能生态系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着复杂人工智能系统的快速采用，对于确保这些系统透明度、安全性及符合监管要求的工具的发展相对滞后。因此，开发一种能提供AI模型及其运行环境详细信息的标准化记录变得至关重要，以便于提高透明度、增强安全性并支持合规性。

Method: 引入了AI Bill of Materials (AIBOM)，它是软件材料清单(SBOM)的一种扩展形式，用于作为已训练AI模型及其环境的标准化、可验证记录。开发了一个概念验证平台AIBoMGen，该平台能够在训练过程中自动收集数据集、模型元数据以及环境详情来生成签名版的AIBOM文件。此外，该系统还采用了加密哈希算法、数字签名以及in-toto认证机制来保障整个过程中的完整性与安全性。

Result: 实验结果表明，AIBoMGen能够有效识别未经许可对各种组件所做的任何改动，并且在不影响性能的前提下成功生成AIBOM文档。这证明了AIBoMGen在促进更加安全和透明的人工智能生态体系建设方面具有巨大潜力。

Conclusion: AIBoMGen展示了其作为构建安全透明AI生态系统基础步骤的潜力，同时也有助于满足如欧盟AI法案等监管框架的要求。

Abstract: The rapid adoption of complex AI systems has outpaced the development of tools to ensure their transparency, security, and regulatory compliance. In this paper, the AI Bill of Materials (AIBOM), an extension of the Software Bill of Materials (SBOM), is introduced as a standardized, verifiable record of trained AI models and their environments. Our proof-of-concept platform, AIBoMGen, automates the generation of signed AIBOMs by capturing datasets, model metadata, and environment details during training. The training platform acts as a neutral, third-party observer and root of trust. It enforces verifiable AIBOM creation for every job. The system uses cryptographic hashing, digital signatures, and in-toto attestations to ensure integrity and protect against threats such as artifact tampering by dishonest model creators. Our evaluation demonstrates that AIBoMGen reliably detects unauthorized modifications to all artifacts and can generate AIBOMs with negligible performance overhead. These results highlight the potential of AIBoMGen as a foundational step toward building secure and transparent AI ecosystems, enabling compliance with regulatory frameworks like the EUs AI Act.

</details>


### [38] [From Issues to Insights: RAG-based Explanation Generation from Software Engineering Artifacts](https://arxiv.org/abs/2601.05721)
*Daniel Pöttgen,Mersedeh Sadeghi,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文首次应用检索增强生成(RAG)方法从问题跟踪数据中生成解释，实现了与人工编写的解释90%的一致性，并展示了在提高软件系统可解释性和透明度方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着现代软件系统的复杂性不断增加，理解其行为变得越来越困难，因此需要提高可解释性以改善透明度和用户信任。传统文档往往过时或不完整，难以从中获得准确且具有上下文特异性的解释。与此同时，虽然问题跟踪系统能够捕捉到丰富且不断更新的开发知识，但这些知识在可解释性方面的潜力尚未被发掘。

Method: 本文提出了一种基于检索增强生成（RAG）的方法来利用问题跟踪数据自动生成解释。该概念验证系统使用开源工具和语言模型实现，旨在展示如何有效利用结构化的问题数据生成解释。

Result: 通过在一个示例项目的GitHub问题集上评估所提方法，研究发现生成的解释与人工编写的解释有90%的一致性。此外，系统表现出很强的真实性和指令遵循能力，保证了所提供解释的可靠性和基础性。

Conclusion: 研究表明，基于RAG的方法不仅可以在黑盒机器学习模型之外扩展可解释性，而且当问题跟踪数据可用时，还能应用于更广泛的软件系统，使得系统行为更加易于理解和解读。

Abstract: The increasing complexity of modern software systems has made understanding their behavior increasingly challenging, driving the need for explainability to improve transparency and user trust. Traditional documentation is often outdated or incomplete, making it difficult to derive accurate, context-specific explanations. Meanwhile, issue-tracking systems capture rich and continuously updated development knowledge, but their potential for explainability remains untapped. With this work, we are the first to apply a Retrieval-Augmented Generation (RAG) approach for generating explanations from issue-tracking data. Our proof-of-concept system is implemented using open-source tools and language models, demonstrating the feasibility of leveraging structured issue data for explanation generation. Evaluating our approach on an exemplary project's set of GitHub issues, we achieve 90% alignment with human-written explanations. Additionally, our system exhibits strong faithfulness and instruction adherence, ensuring reliable and grounded explanations. These findings suggest that RAG-based methods can extend explainability beyond black-box ML models to a broader range of software systems, provided that issue-tracking data is available - making system behavior more accessible and interpretable.

</details>


### [39] [StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection](https://arxiv.org/abs/2601.05772)
*Qingyuan Li,Chenchen Yu,Chuanyi Li,Xin-Cheng Wen,Cheryl Lee,Cuiyun Gao,Bin Luo*

Main category: cs.SE

TL;DR: 提出了一种名为StriderSPD的框架，它结合了图分支和大型语言模型（LLM），利用结构信息来指导LLM识别安全补丁。通过创新设计的适配器在LLM的令牌级别上有效对齐汇编代码和伪代码之间的表示，并采用两阶段训练策略解决了由两个分支间参数差异大导致的优化不平衡问题。此外，还构建了一个与先前数据集在项目和领域上均不重叠的二进制SPD基准以进行更真实的评估。


<details>
  <summary>Details</summary>
Motivation: 现有软件中漏洞的存在严重威胁系统安全，及时应用安全补丁对于缓解攻击至关重要。但软件供应商通常会静默修补漏洞而披露有限，特别是在闭源软件中，补丁以二进制形式分发而不提供源代码访问权限。当前的二进制SPD方法往往将二进制提升到抽象层次如汇编或伪代码层面处理，这限制了语义表达并缺乏可用于提取结构的语法解析兼容性，从而阻碍准确的学习漏洞修复表示。此外，以往研究多从同一项目获取训练与测试数据，未能反映闭源条件下的实际情况。

Method: 提出了StriderSPD框架，该框架将图分支集成到大型语言模型中，利用结构化信息引导模型识别安全补丁。设计了新的适配器，在LLM的token级别上有效地对齐汇编代码和伪代码之间的表示。采用两阶段训练策略解决由于StriderSPD两个分支之间参数数量巨大差异引起的优化不平衡问题。

Result: StriderSPD能够在新构建的、与先前数据集在项目和领域上都不重叠的二进制SPD基准上进行广泛的评估，展示了其在真实世界条件下检测闭源软件中安全补丁的有效性。

Conclusion: StriderSPD提供了一种有效的解决方案，能够克服现有二进制SPD方法中的局限性，尤其是在处理闭源软件时。通过结合图分支与LLM以及两阶段训练策略，StriderSPD能够更准确地学习和识别安全补丁。

Abstract: Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \textbf{\textit{StriderSPD}}, a \underline{Str}ucture-gu\underline{ide}d joint \underline{r}epresentation \underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.

</details>


### [40] [SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking](https://arxiv.org/abs/2601.05827)
*Zewei Lin,Jiachi Chen,Jingwen Zhang,Zexu Wang,Yuming Feng,Weizhe Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文首次研究了去中心化金融(DeFi)质押中的逻辑缺陷问题，定义并检测了六种不同类型的逻辑缺陷，并基于此开发了一个名为SSR的静态分析工具来检测DeFi质押合约中的逻辑缺陷。实验结果表明SSR在准确率、召回率和F1分数上表现优秀。此外，通过分析大规模数据集，发现超过20%的DeFi质押合同至少包含一种逻辑缺陷。


<details>
  <summary>Details</summary>
Motivation: 鉴于DeFi质押中存在的逻辑缺陷可能让攻击者不当获利，比如通过操纵奖励金额或重复领取奖励等方式，本研究旨在识别这些逻辑缺陷并通过开发专门工具来帮助缓解此类威胁。

Method: 通过对64起安全事件及144份审计报告进行分析，确定了六种不同的逻辑缺陷类型；基于此研究开发了SSR工具，该工具使用大型语言模型（LLM）提取关于质押逻辑的基本信息，并构建一个DeFi质押模型以识别其中存在的逻辑缺陷。

Result: SSR工具达到了92.31%的准确率、87.92%的召回率以及88.85%的F1分数。另外，在对15,992个DeFi质押合同进行分析时，发现有3,557个（占总数的22.24%）合同至少存在一种逻辑缺陷。

Conclusion: 本研究表明，DeFi质押领域内存在着显著比例的逻辑缺陷，而所提出的SSR工具能够有效地识别这些问题，为提高DeFi应用的安全性提供了有力支持。

Abstract: Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [41] [Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization](https://arxiv.org/abs/2601.05955)
*Yuliang Chen,Xi Lin,Jun Wu,Xiangrui Cai,Qiaolun Zhang,Xichun Fan,Jiapeng Xu,Xiu Su*

Main category: cs.DC

TL;DR: 提出了一种新的联邦领域泛化框架FaST-PT，通过轻量级多模态风格转换和双提示模块设计来解决跨客户端数据异质性和计算通信开销大的问题。在PACS和DomainNet等四个跨域基准数据集上的实验表明，该方法比现有的SOTA FDG方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦领域泛化(FDG)方法难以处理跨客户端的数据异质性，并且会带来显著的通信与计算开销。为了解决这些挑战，本文提出了一个新的FDG框架。

Method: 本文提出的新框架名为FaST-PT，它包括一个轻量级多模态风格转换(MST)方法用于图像嵌入变换以扩展训练数据分布并缓解领域迁移；以及一个双提示模块，将提示分解为全局提示和领域提示，其中全局提示从增强后的嵌入中捕获通用知识，而领域提示则从本地数据中捕获特定领域的知识。此外，引入了领域感知提示生成(DPG)，以自适应地为每个样本生成合适的提示，促进未见领域的适应性。

Result: 在PACS、DomainNet等四个跨域基准数据集上进行了广泛的实验，结果表明FaST-PT的表现优于FedDG-GA和DiPrompt等当前最先进的FDG方法。消融研究进一步验证了FaST-PT的有效性和效率。

Conclusion: 本研究提出的FaST-PT框架成功解决了现有FDG方法中存在的跨客户端数据异质性和高开销问题，为联邦学习中的领域泛化提供了一个有效的新途径。

Abstract: Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [MoEBlaze: Breaking the Memory Wall for Efficient MoE Training on Modern GPUs](https://arxiv.org/abs/2601.05296)
*Jiyuan Zhang,Yining Liu,Siqi Yan,Lisen Deng,Jennifer Cao,Shuqi Yang,Min Ni,Bi Xue,Shen Li*

Main category: cs.LG

TL;DR: MoEBlaze, a memory-efficient MoE training framework, addresses the memory wall bottleneck in large-scale Mixture-of-Experts (MoE) architectures by eliminating intermediate buffers and optimizing data structures, achieving over 4x speedups and over 50% memory savings.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the significant 'memory wall' bottleneck that is amplified in modern large-scale Mixture-of-Experts (MoE) architectures, which limits the maximum batch size and sequence length on GPUs and results in excessive data movements, hindering performance and efficient model scaling.

Method: The authors present MoEBlaze, a memory-efficient MoE training framework. The method involves an end-to-end token dispatch and MoE training approach with optimized data structures to remove the need for intermediate buffers and activation materialization. Additionally, it uses co-designed kernels with smart activation checkpointing to reduce memory footprint while improving performance.

Result: The result of using MoEBlaze is a significant improvement in both speed and memory usage. Specifically, the framework achieves over 4 times the speed and more than 50% reduction in memory consumption compared to existing MoE frameworks.

Conclusion: In conclusion, the MoEBlaze framework effectively tackles the memory pressure and performance issues associated with large-scale MoE models, offering substantial improvements in computational efficiency and scalability.

Abstract: The pervasive "memory wall" bottleneck is significantly amplified in modern large-scale Mixture-of-Experts (MoE) architectures. MoE's inherent architectural sparsity leads to sparse arithmetic compute and also introduces substantial activation memory overheads -- driven by large token routing buffers and the need to materialize and buffer intermediate tensors. This memory pressure limits the maximum batch size and sequence length that can fit on GPUs, and also results in excessive data movements that hinders performance and efficient model scaling. We present MoEBlaze, a memory-efficient MoE training framework that addresses these issues through a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materializing, and (ii) co-designed kernels with smart activation checkpoint to mitigate memory footprint while simultaneously achieving better performance. We demonstrate that MoEBlaze can achieve over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

</details>


### [43] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: 本文介绍了一种名为TIME的新框架，该框架通过引入可选的ISO 8601时间标签、表示沉默间隔的tick turns以及可以在回复中任意位置出现的短<think>块来增强对话模型。TIME能够以更紧凑的方式触发明确推理，并且在不同规模下都提高了TIMEBench基准测试得分，同时减少了约一个数量级的推理标记使用量。


<details>
  <summary>Details</summary>
Motivation: 当前面向推理的大语言模型通常在每个响应开始时暴露显式“思考”过程，这虽然有助于算术、编程和解决问题，但成本高昂、模糊了声明级别的可审计性，且一旦模型开始展示便无法重新触发显式推理。此外，对话模型对时间结构不够敏感。为解决这些问题，研究者开发了TIME框架。

Method: TIME框架通过引入可选的ISO 8601 <time>标签、代表静默间隙的tick turns及可在任何位置出现的短<think>块来增强对话内容。采用四阶段课程训练Qwen3密集模型，包括一个小批量最大多样化全批次对齐步骤，使模型能够在需要的地方调用简短的推理爆发并保持用户可见文本紧凑。

Result: 跨4B到32B规模，TIME相比基础Qwen3在思考与非思考模式下均提高了TIMEBench分数，同时将推理标记减少了一个数量级左右。

Conclusion: TIME框架提供了一种新方法，通过上下文敏感和时间线索驱动的显式推理机制改进了对话模型的表现，使其更加高效和灵活。

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [44] [When the Server Steps In: Calibrated Updates for Fair Federated Learning](https://arxiv.org/abs/2601.05352)
*Tianrun Yu,Kaixiang Zhao,Cheng Zhang,Anjun Gao,Yueyang Quan,Zhuqing Liu,Minghong Fang*

Main category: cs.LG

TL;DR: 本文提出了一种新的服务器端去偏方法EquFL，用于减少联邦学习系统中的偏置。通过允许服务器在接收客户端模型更新后生成一个校准更新，并将其与聚合的客户端更新集成，从而产生一个调整后的全局模型以减少偏置。理论和实验结果表明，EquFL能够有效减少公平性损失并显著缓解系统内的偏置问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然具有明显优势，但在确保不同人群组之间的公平性方面面临挑战。现有的许多公平感知去偏方法要么需要修改客户端的训练协议，要么在聚合策略上缺乏灵活性。

Method: 提出了一种名为EquFL的新方法，该方法是一种服务器端去偏技术，旨在通过生成一个经过校准的更新来减轻联邦学习系统中的偏置。此校准更新随后与来自客户端的聚合更新相结合，以创建一个旨在减少偏置的调整后全球模型。

Result: 理论分析证明了EquFL可以收敛到FedAvg所达到的最佳全局模型，并且能够在训练轮次中有效地减少公平性损失。实验证明，EquFL能够显著地减轻系统内部的偏置问题。

Conclusion: EquFL作为一种创新的服务器端解决方案，在不需要对客户端进行额外修改的情况下，成功解决了联邦学习中存在的偏置问题，并展示了其在实际应用中的有效性。

Abstract: Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.

</details>


### [45] [The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection](https://arxiv.org/abs/2601.05371)
*Md Shafiqul Islam,Shakti Prasad Padhy,Douglas Allaire,Raymundo Arróyave*

Main category: cs.LG

TL;DR: 本文提出了一种基于核间几何的贝叶斯优化框架，通过预期散度距离探索核空间，使用多维缩放（MDS）将离散核库映射到连续欧几里得流形上，从而实现高效的高斯过程回归模型中的核选择。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归的表现严重依赖于协方差核的选择，而选择合适的核是提高模型质量的关键，但也是概率建模中最具有挑战性和计算成本的部分之一。

Method: 作者开发了一个基于核之间几何形状的贝叶斯优化框架，利用GP先验之间的预期散度距离来有效地探索核空间。采用多维尺度分析(MDS)嵌入技术，将这个距离矩阵转化为一个连续的欧几里得流形，使得原本离散的核库可以被映射到这样一个连续的空间中，进而支持平滑的贝叶斯优化(BO)。

Result: 通过合成基准、真实世界的时间序列数据集以及增材制造案例研究预测熔池几何形状的应用实例，该方法在预测准确性和不确定性校准方面均优于基线方法，包括大型语言模型(LLM)引导的搜索。

Conclusion: 这一框架为核搜索提供了一个可重复使用的概率几何学方法，并直接关联到高斯过程建模和深度核学习领域。

Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.

</details>


### [46] [Imitation Learning for Combinatorial Optimisation under Uncertainty](https://arxiv.org/abs/2601.05383)
*Prakash Gawas,Antoine Legrain,Louis-Martin Rousseau*

Main category: cs.LG

TL;DR: 本文提出了一种系统分类法，用于在不确定性下的组合优化中模仿学习的专家，并基于此分类提出了一个广义的数据集聚合算法。通过动态医生-患者分配问题的计算实验表明，从随机专家那里学习到的策略优于确定性或全信息专家，而交互式学习则能在使用更少的专家演示的情况下提高解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 模仿学习为大规模组合优化问题提供了一个数据驱动框架，但现有研究缺乏对生成训练示例的'专家'角色的统一框架来描述其建模假设、计算属性及对学习表现的影响。

Method: 本文介绍了一种针对不确定性下组合优化中模仿学习专家的系统分类方法，根据他们处理不确定性的方法、最优程度以及与学习者的互动模式将专家分为三类。基于这一分类，作者提出了一种支持多专家查询、专家聚合和灵活互动策略的广义数据集聚合(DAgger)算法。

Result: 通过对具有随机到达和容量限制的动态医生-患者分配问题进行评估，实验证明了从随机专家处学到的策略始终优于从确定性或全信息专家处学到的策略；同时，交互式学习能够以较少的专家演示次数提升解的质量。

Conclusion: 本研究表明，在不确定性环境下，利用随机专家可以显著改善模仿学习的效果；当随机优化变得计算上难以处理时，聚合确定性专家提供了有效的替代方案。

Abstract: Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance.
  This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies.
  The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.

</details>


### [47] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出了一种名为HINT的新知识蒸馏框架，用于多智能体强化学习中的集中训练、分散执行设置。通过利用层次化RL和伪非策略RL，HINT解决了复杂领域内合成高效教学策略的挑战、教师在处理分布外状态时遇到的困难以及学生与教师观察空间不匹配的问题。在具有挑战性的合作领域测试中，HINT相较于基线方法，在成功率上实现了60%到165%的提升。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏（KD）在多智能体强化学习（MARL）中通过使用集中的教师来加速分散的学生的学习过程有着潜在价值，但面临着几个关键瓶颈：难以在复杂领域合成高性能的教学策略；当教师需要在分布外(OOD)状态下进行推理时存在难度；以及分散的学生与集中的教师之间观察空间的不匹配问题。

Method: 提出了HINT（分层互动式教师转移），一种新的针对MARL的知识蒸馏框架，特别适用于集中训练、分散执行的场景。该方法利用了层次化强化学习来提供一个可扩展且性能高的教师模型，并通过伪非策略强化学习技术使教师策略能够基于教师和学生的经验共同更新，从而提高对OOD情况的适应能力。此外，HINT还采用了基于表现的过滤机制，只保留与结果相关的指导信息，减少了观察空间上的差异。

Result: 在多个具有挑战性的合作任务环境（如FireCommander资源分配任务和MARINE战术战斗任务）中评估了HINT的有效性。实验结果显示，HINT在这些基准测试中显著优于其他基线方法，特别是在成功率方面取得了60%至165%的改进。

Conclusion: HINT作为一种创新的知识蒸馏方法，有效地解决了多智能体强化学习中存在的几个关键问题，包括但不限于合成高效教学策略、改善教师对于未见过状态的适应能力以及减少由于观察空间不同而导致的信息丢失等问题。它不仅证明了自己在提高学习效率方面的潜力，而且也为未来研究提供了新方向。

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [48] [Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion](https://arxiv.org/abs/2601.05431)
*Xiaowen He,Su Jiang,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分自编码器（VAE）的数据空间反演（DSI）框架，用于预测CO2储存项目中的压力、应力、应变场以及断层滑移倾向。通过使用先验地质模型模拟结果和观测数据直接推断感兴趣量的后验分布，无需生成后验地质模型。该方法能准确预测压力、应变、应力场及断层滑移倾向，并减少关键地质力学和断层参数的不确定性。


<details>
  <summary>Details</summary>
Motivation: 在许多地下作业中，准确评估断层滑动的可能性至关重要。然而，在涉及断层的耦合流动-地质力学问题上应用传统的基于模型的历史匹配方法具有挑战性。因此，本研究旨在开发一种新的方法来克服这一难题。

Method: 本研究实施了一个基于变分自编码器（VAE）的数据空间反演（DSI）框架，用来预测CO2储存项目中的压力、应力、应变场以及断层滑移倾向。首先生成了包含两个断层的合成三维系统下的异质渗透率和孔隙度场实现，并对每个实现从先验分布中抽样不确定的地质力学和断层参数。接着，利用GEOS进行了这些地质模型的耦合流动-地质力学模拟。然后训练了一个带有堆叠卷积长短期记忆层的VAE，以根据潜在变量表示压力、应变、有效正应力和剪应力场。最后，结合监测井提供的观测压力和应变数据，利用VAE参数化与DSI进行后验预测。

Result: 实验结果显示，对于合成的真实模型而言，DSI-VAE框架能够准确地预测压力、应变、应力场以及断层滑移倾向，并且减少了关键地质力学和断层参数的不确定性。

Conclusion: 提出的基于VAE的数据空间反演框架为解决复杂地质条件下的断层滑移预测问题提供了一个有效的解决方案，展示了其在提高预测准确性及减少不确定性方面的潜力。

Abstract: Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.

</details>


### [49] [RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models](https://arxiv.org/abs/2601.05451)
*Marko Sterbentz,Kevin Cushing,Cameron Barrie,Kristian J. Hammond*

Main category: cs.LG

TL;DR: RingSQL, a hybrid data generation framework, combines query templates with LLM-based paraphrasing to produce high-quality, correct, and linguistically diverse training data for text-to-SQL models, leading to improved accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The development of text-to-SQL systems faces challenges due to the limited availability of high-quality training data. Traditional methods of creating such data are either costly (manual creation) or have limitations in terms of quality and scalability (template-based or LLM-based approaches).

Method: RingSQL innovatively merges schema-independent query templates with natural language question paraphrasing using large language models (LLMs), aiming to generate SQL queries that are both correct and exhibit a wide range of linguistic diversity.

Result: Models trained on data generated by RingSQL show an average improvement in accuracy of +2.3% across six different text-to-SQL benchmarks, compared to those trained on other synthetic data sets.

Conclusion: RingSQL presents a promising solution to enhance the quality and diversity of training data for text-to-SQL tasks, thereby improving the performance of these systems. The approach is made accessible through public code sharing.

Abstract: Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at https://github.com/nu-c3lab/RingSQL.

</details>


### [50] [Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning](https://arxiv.org/abs/2601.05474)
*Pingchuan Ma,Qixin Zhang,Shuai Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出了一种新的可微因果发现方法ALVGL，通过稀疏和低秩分解学习数据的精度矩阵，并构建一个超级结构来初始化标准的可微因果发现方法，从而提高优化效率和准确性。实验表明ALVGL在合成和真实数据集上均达到了最先进的准确度并显著提高了优化效率。


<details>
  <summary>Details</summary>
Motivation: 当前的可微分因果发现方法在处理高维数据或存在潜在混淆变量的数据时面临搜索空间巨大、目标函数复杂以及图论约束非平凡等问题。因此，研究者们开始关注利用超级结构来指导优化过程，但如何高效地学习到合适的超级结构仍是一个挑战。

Method: 本文提出了ALVGL方法，该方法采用稀疏性和低秩性分解来学习数据的精度矩阵，并设计了ADMM程序来优化这种分解，以识别与底层因果结构最相关的精度矩阵组件。这些组件随后被组合起来构造一个超级结构，该结构是真实因果图的一个超集。这个超级结构用于初始化具有更集中搜索空间的标准可微因果发现方法。

Result: 通过一系列结构因果模型中的实例化展示ALVGL的多功能性，包括高斯和非高斯设置，以及有无未测量混杂因素的情况。在综合和实际数据集上的广泛实验表明，ALVGL不仅达到了最新的准确度水平，而且显著提高了优化效率。

Conclusion: ALVGL作为可微因果发现流程的一种新颖且通用的增强手段，通过有效学习数据的精度矩阵并构造有助于初始化后续因果发现步骤的超级结构，证明了其自身在提高不同场景下优化效率及准确性方面的可靠性和有效性。

Abstract: Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.
  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.
  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.

</details>


### [51] [MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization](https://arxiv.org/abs/2601.05475)
*Jiefu Ou,Sapana Chaudhary,Kaj Bostrom,Nathaniel Weir,Shuai Zhang,Huzefa Rangwala,George Karypis*

Main category: cs.LG

TL;DR: 本研究提出了一种名为MaxCode的方法，通过在执行反馈基础上的迭代改进来指导大型语言模型发现更优的代码解决方案。该方法整合了自然语言批评模型以增强观察空间，并使用生成式奖励模型来改进搜索过程中的探索。实验结果表明，与基线相比，MaxCode在绝对加速值和相对加速排名上分别提高了20.3%和10.1%。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在一般编码任务中表现出色，但在优化代码时面临两大挑战：一是编写高效代码（如高性能CUDA内核和竞赛级CPU代码）需要系统、算法及特定语言方面的专业知识；二是除了二进制正确性外，还需要理解诸如时间性能和设备利用率等度量指标。

Method: 提出了MaxCode方法，它基于最大奖励强化学习框架统一了现有的搜索方法，并使观察函数和动作价值函数变得模块化易于修改。为了增加观察空间的信息量，集成了一个自然语言批评模型，可以将原始执行反馈转化为关于错误和性能瓶颈的诊断见解。此外，还训练了一个生成式的剩余奖励模型，利用回放中的动作值对潜在解决方案进行重新排序，以此改善搜索过程中的探索效率。

Result: 在KernelBench (CUDA) 和 PIE (C++) 优化基准测试上的结果显示，相较于基线方法，MaxCode 在绝对加速值和相对加速排名方面分别实现了 20.3% 和 10.1% 的提升。

Conclusion: MaxCode 方法成功地解决了大型语言模型在代码优化过程中遇到的一些难题，通过结合自然语言批评模型和生成式奖励模型有效提升了代码性能。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.

</details>


### [52] [Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection](https://arxiv.org/abs/2601.05501)
*Feihu Jin,Ying Tan*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hi-ZFO的混合优化框架，结合了零阶方法的探索能力和一阶方法的精确度，通过自适应地对模型进行分层处理，实现了在大语言模型微调中的优越性能和更短的训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统的细调大型语言模型的方法存在一些问题：一阶（FO）优化通常会导致训练趋向于尖锐、泛化能力差的极小值；而零阶（ZO）方法虽然提供了更强的探索行为，但收敛速度较慢，并且在生成任务中由于输出空间和搜索空间巨大，导致估计方差显著增加，使得ZO方法既嘈杂又低效。为了解决这些问题，研究者提出了一个新框架。

Method: Hi-ZFO是一个结合了零阶与一阶优化技术的层次化混合优化框架。该方法基于层的重要性分析来自适应地划分模型，对于关键层使用精确的一阶更新，而对于不太敏感的层则利用零阶优化。值得注意的是，在Hi-ZFO中引入零阶优化不仅仅是为了节省内存，而是作为一种有益随机性的来源，帮助模型逃离纯一阶优化容易停滞的地方性最小值。

Result: 通过在各种生成、数学以及代码推理任务上的验证，Hi-ZFO不仅持续表现出更好的性能，同时也大幅减少了训练所需的时间。

Conclusion: 结果表明，这种层次化的混合优化策略对于提高大型语言模型微调的有效性和效率具有重要意义。

Abstract: Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.

</details>


### [53] [Over-Searching in Search-Augmented Large Language Models](https://arxiv.org/abs/2601.05503)
*Roy Xie,Deepak Gopinath,David Qiu,Dong Lin,Haitian Sun,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 本文系统地评估了搜索增强型大语言模型中的过度搜索问题，指出了其在不同维度上的表现，并提出了一种新的评估指标——Tokens Per Correctness (TPC)，旨在衡量性能与成本之间的权衡。此外，还探讨了解决这一问题的方法，并发布了OverSearchQA以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前的搜索增强型大语言模型虽然在知识密集型任务中表现出色，但存在过度搜索的问题，即不必要地调用搜索工具，不仅导致计算效率低下还会因引入无关上下文而产生错误信息。为解决此问题，需要对过度搜索现象进行全面评估并探索有效的缓解策略。

Method: 通过系统性地从查询类型、模型类别、检索条件以及多轮对话等多个维度出发，评价了搜索增强型大语言模型中过度搜索的现象。定义了新度量标准Tokens Per Correctness (TPC)来量化搜索操作的成本效益比。同时，在查询和检索两个层面探索了减轻过度搜索影响的方法。

Result: 研究表明，对于可回答的问题，搜索通常能提高答案准确性，但对于无法回答的问题，则会降低拒绝作答的能力；复杂推理模型和深度研究系统中的过度搜索情况更为严重，且容易受到噪音检索的影响，在多轮对话中这种效应还会累积；证据组合非常关键，负面证据的存在有助于提高模型正确拒绝作答的能力。

Conclusion: 本研究揭示了搜索增强型大语言模型中过度搜索的具体表现形式及其影响因素，并提出了一种新的评估方法TPC来帮助理解搜索带来的性能增益与额外开销之间的平衡点。此外，还提供了初步的解决方案框架，并公开了数据集OverSearchQA以支持未来的研究工作。

Abstract: Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.

</details>


### [54] [Buffered AUC maximization for scoring systems via mixed-integer optimization](https://arxiv.org/abs/2601.05544)
*Moe Shiina,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 本文提出了一种基于混合整数线性优化（MILO）的方法，直接最大化缓冲AUC（bAUC），用于构建具有高可解释性的评分系统。通过限制评分系统中的问题数量来实现组稀疏性约束，并且实验结果表明该方法在AUC值上优于基于正则化和逐步回归的基线方法。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的许多研究已经使用了混合整数优化(MIO)技术来开发用于二分类的评分系统，但它们没有专注于直接最大化AUC（即接收者操作特征曲线下的面积）。鉴于AUC被认为是评价评分系统的一个重要指标，本研究旨在建立一个有效的MIO框架，以直接最大化作为AUC最紧凸下界的缓冲AUC(bAUC)。

Method: 提出了一个混合整数线性优化(MILO)模型，该模型在考虑组稀疏性约束以限制评分系统中问题数量的同时，最大化bAUC。

Result: 利用公开的真实世界数据集进行的计算实验证明，所提出的MILO方法能够构建出比基于正则化和逐步回归的基线方法具有更高AUC值的评分系统。

Conclusion: 这项研究促进了开发高度可解释分类模型的MIO技术的进步。

Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.

</details>


### [55] [Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow](https://arxiv.org/abs/2601.05583)
*Xue Feng,Li Wang,Deanna Needell,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出了一种自监督方法来学习JKO解算子，无需数值求解任何JKO轨迹。通过交替进行轨迹生成和算子更新，该方法能够同时学习JKO算子及其引起的轨迹，并且随着训练的进行，生成的数据越来越接近真实的JKO轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统的JKO方案虽然提供了一个稳定的变分框架用于计算Wasserstein梯度流，但其实际应用常常受到反复解决JKO子问题带来的高计算成本限制。

Method: 提出了一种自监督的学习方法，直接将输入密度映射到相应JKO子问题的最小值点。此外，还引入了Learn-to-Evolve算法，该算法通过在轨迹生成和算子更新之间交替来进行，以克服训练数据有限的问题。

Result: 实验结果表明，所提出的方法对于不同能量形式及初始条件都表现出准确性、稳定性和鲁棒性。

Conclusion: 本研究提出的自监督学习方法能有效降低JKO方案的应用难度，提高了计算效率，并保持了良好的泛化能力。

Abstract: The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.

</details>


### [56] [PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning](https://arxiv.org/abs/2601.05593)
*Jingcheng Hu,Yinmin Zhang,Shijie Shang,Xiaobo Yang,Yue Peng,Zhewei Huang,Hebin Zhou,Xin Wu,Jie Cheng,Fanqi Wan,Xiangwen Kong,Chengyuan Yao,Kaiwen Yan,Ailin Huang,Hongyu Zhou,Qi Han,Zheng Ge,Daxin Jiang,Xiangyu Zhang,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: 介绍了一种新的训练和推理框架Parallel Coordinated Reasoning (PaCoRe)，旨在克服当前语言模型在测试时计算量上的限制，通过大规模并行探索与消息传递架构相结合的方式，实现了在不超出上下文限制的情况下，扩展至数百万标记的有效测试时计算量。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型面临的一个核心局限是它们难以将测试时的计算量显著扩大到固定上下文窗口之外的顺序推理之上。

Method: 提出了Parallel Corored Reasoning（PaCoRe），一个采用多轮次消息传递架构协调的大规模并行探索训练与推理框架，能够通过每轮生成许多并行推理路径，并将结果压缩成上下文有限的消息来指导下一轮直至得到最终答案。整个系统通过大规模基于结果的强化学习端到端地进行训练。

Result: 该方法在多个领域中取得了显著进步，特别是在数学推理方面表现优异：80亿参数的模型在HMMT 2025上达到了94.5%的准确率，超过了GPT-5的93.2%，同时有效测试时计算量达到了大约两百万个令牌。

Conclusion: PaCoRe提供了一种新颖的方法来解决现有语言模型面临的挑战，通过利用并行处理和消息传递机制，成功地将有效的测试时计算量扩展到了前所未有的水平，从而在包括数学在内的多个领域内实现了性能提升。此外，研究团队还开源了模型检查点、训练数据以及完整的推理流程以促进后续研究。

Abstract: We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.

</details>


### [57] [Good Allocations from Bad Estimates](https://arxiv.org/abs/2601.05597)
*Sílvia Casacuberta,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，可以在自然处理效应分布下仅使用$O(M/ε)$样本达到与CATE相同的总体治疗效果。此外，预算灵活性还可以进一步减少分配所需的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统的CATE估计方法虽然能够有效地针对异质性人群进行治疗目标定位，但需要大量的样本（$O(M/ε^2)$）。本研究旨在通过较少的样本来实现近似最优的治疗分配，同时保持总体治疗效果。

Method: 利用粗略估计足以支持近乎最优的治疗分配这一关键见解，并且考虑了预算灵活性对进一步减少样本复杂度的影响。

Result: 该算法在各种真实世界的RCT数据集上进行了评估，结果表明，在所有情况下，它都能以惊人的少量样本找到接近最优的治疗分配方案。

Conclusion: 这项工作强调了治疗效果估计和治疗分配之间的根本区别：后者需要的样本要少得多。

Abstract: Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $ε> 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/ε^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $ε$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/ε)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.

</details>


### [58] [Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR](https://arxiv.org/abs/2601.05607)
*Zijun Min,Bingshuai Liu,Ante Wang,Long Zhang,Anxiang Zeng,Haibo Zhang,Jinsong Su*

Main category: cs.LG

TL;DR: 本文提出了动态混合策略优化（DHPO），结合了GRPO和GSPO的优点，通过加权机制融合了token级别和序列级别的重要性比率，并采用分支特定的裁剪策略来稳定训练。实验表明，在七个具有挑战性的数学推理基准测试中，DHPO的表现优于GRPO和GSPO。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR算法在不同粒度上工作，各有优势与局限。GRPO虽然保持了细粒度的信用分配但常面临高方差和不稳定性问题；而GSPO虽更好地匹配序列级奖励，却牺牲了token级别的信用分配。为解决这些问题，提出了一种新的方法以综合两者优点。

Method: 提出了Dynamic Hybrid Policy Optimization (DHPO)，它在一个单一裁剪替代目标内结合了GRPO和GSPO的特点。DHPO使用加权机制结合了token级和序列级的重要性比率，并探索了两种混合机制变体：平均混合与熵指导混合。此外，还采用了分支特异性裁剪策略来进一步稳定训练过程。

Result: 在七个具有挑战性的数学推理基准测试中，针对Qwen3系列中的密集型模型和MoE模型进行的实验表明，DHPO相比GRPO和GSPO表现出一致的优势。

Conclusion: DHPO通过有效整合GRPO与GSPO的优势，在多个数学推理任务上证明了其优越性，为强化学习领域提供了新的优化方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.

</details>


### [59] [PiXTime: A Model for Federated Time Series Forecasting with Heterogeneous Data Structures Across Nodes](https://arxiv.org/abs/2601.05613)
*Yiming Zhou,Mingyue Cheng,Hao Wang,Enhong Chen*

Main category: cs.LG

TL;DR: 提出了一种名为PiXTime的新型时间序列预测模型，专门用于联邦学习环境，解决了不同节点间因采样标准不同导致的时间粒度和变量集差异问题。通过个性化Patch Embedding与全局VE表，该模型能够处理多粒度、异构变量集的数据，并在多个实际基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据因其高价值而难以跨节点共享，联邦学习提供了一种利用分布式时间数据的方法。然而，不同的采样标准造成了各节点间时间粒度及变量集合的不同，这对传统的联邦学习方法构成了挑战。

Method: PiXTime采用个性化的Patch Embedding技术将特定于节点的时间粒度时间序列转换为统一维度的令牌序列供后续共享模型处理，并使用全局VE表来对齐跨节点的变量类别语义，从而提高跨节点可转移性。此外，基于Transformer的共享模型能够捕捉具有任意数量变量的辅助序列表示，并利用交叉注意力机制来增强目标序列的预测效果。

Result: 实验表明，PiXTime在联邦设置下达到了最先进的性能，并且在八个广泛使用的现实世界传统基准上展示了优越的表现。

Conclusion: PiXTime作为专为联邦学习设计的时间序列预测模型，有效地解决了跨节点存在多粒度和异构变量集的问题，通过创新性的方法提高了预测准确性，并在多种场景下验证了其有效性。

Abstract: Time series are highly valuable and rarely shareable across nodes, making federated learning a promising paradigm to leverage distributed temporal data. However, different sampling standards lead to diverse time granularities and variable sets across nodes, hindering classical federated learning. We propose PiXTime, a novel time series forecasting model designed for federated learning that enables effective prediction across nodes with multi-granularity and heterogeneous variable sets. PiXTime employs a personalized Patch Embedding to map node-specific granularity time series into token sequences of a unified dimension for processing by a subsequent shared model, and uses a global VE Table to align variable category semantics across nodes, thereby enhancing cross-node transferability. With a transformer-based shared model, PiXTime captures representations of auxiliary series with arbitrary numbers of variables and uses cross-attention to enhance the prediction of the target series. Experiments show PiXTime achieves state-of-the-art performance in federated settings and demonstrates superior performance on eight widely used real-world traditional benchmarks.

</details>


### [60] [Transformer Is Inherently a Causal Learner](https://arxiv.org/abs/2601.05647)
*Xinyue Wang,Stephen Wang,Biwei Huang*

Main category: cs.LG

TL;DR: 该论文发现，以自回归方式训练的transformer能够自然地在其学习到的表示中编码时间延迟的因果结构。通过分析这些模型对于过去输入的梯度敏感性可以直接恢复底层的因果图，无需明确的因果目标或结构约束。这一方法在处理非线性动力学、长期依赖关系及非平稳系统等复杂情况时表现出色，特别是在数据异质性增加的情况下，其性能远超现有最先进的发现算法。此外，随着数据量和异质性的增加，该方法显示出因果准确度提高的潜力，这是传统方法所缺乏的。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索自回归训练的transformer是否能够在没有明确因果指导的情况下，自然地学习并表达出多变量时间序列中的因果结构。

Method: 通过对以自回归方式训练用于预测未来值的transformer模型输出相对于过去输入的梯度敏感性进行分析，直接恢复出底层因果图。基于标准可识别性条件下的理论证明，并开发了一种使用聚合梯度归因的实际提取方法。

Result: 结果显示，在处理非线性动态、长依赖关系及非平稳系统等具有挑战性的场景时，本方法显著优于现有的最先进发现算法，特别是在数据异质性较高的情况下表现尤为突出。而且，随着数据量与异质性的增加，因果准确性也得到了提升，这一点是传统方法不具备的优势。

Conclusion: 结论指出，这种统一的观点为未来的范式奠定了基础：一方面，因果发现可以通过基础模型的角度来实现；另一方面，基础模型也可以通过因果视角获得可解释性和增强。

Abstract: We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.

</details>


### [61] [From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation](https://arxiv.org/abs/2601.05650)
*Miguel Matey-Sanz,Joaquín Torres-Sospedra,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类的方法来结构化Wi-Fi指纹数据集，以提高室内定位精度。通过空间或无线电特征对指纹进行分组，并且可以在建筑或楼层级别应用聚类。定位阶段使用最强接入点的聚类估计程序将未见的指纹分配给最相关的聚类中。然后仅在选定的聚类内执行定位，从而允许学习模型操作于更小且更一致的数据子集上。实验结果表明该方法有效降低了定位误差，特别是在建筑物级别的策略下，但代价是降低了楼层检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的Wi-Fi指纹室内定位技术性能受限于指纹数据集的大小和异质性、信号强度指示器的可变性以及大范围多层环境中的模糊性。这些因素显著降低了定位精度，特别是当全局模型不考虑结构约束时直接应用的情况下。

Method: 引入一种基于聚类的方法，在定位之前先对指纹数据集进行结构化处理。根据空间或无线电特性对指纹进行分组，并可在建筑或楼层层面实施聚类。对于新出现的指纹，基于最强接入点的聚类估计过程将其分配给最相关的簇内。随后，仅在选定的簇内执行定位任务。

Result: 通过对三个公开数据集及多种机器学习模型上的评估显示，该方法能够持续减少定位错误，特别是在采用建筑级策略时效果明显；不过，这同时也导致了楼层识别准确度有所下降。

Conclusion: 明确地通过聚类来结构化数据集是一种有效且灵活的方法，适用于可扩展的室内定位系统。尽管在某些情况下可能会影响楼层检测准确性，但总体上提高了整体定位精度。

Abstract: Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.

</details>


### [62] [Do Sparse Autoencoders Identify Reasoning Features in Language Models?](https://arxiv.org/abs/2601.05679)
*George Ma,Zhongyuan Liang,Irene Y. Chen,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 研究发现，稀疏自动编码器通过对比方法识别出的推理特征主要捕捉的是推理的语言相关性，而非实际的推理计算过程本身。这些特征对词汇层面的干预非常敏感，且操纵这些特征对基准性能的影响微乎其微。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨稀疏自动编码器（SAEs）在大型语言模型（LLMs）中是否能够识别出真正的推理特征。

Method: 采用了一种以证伪为导向的方法论，结合因果令牌注入实验和由LLM指导的证伪过程来测试特征激活是反映了推理过程还是仅仅是对表面语言关联性的反映。

Result: 结果表明，在20种不同的配置下，被识别为推理相关的特征对于词汇级别的干预非常敏感；并且，没有一个分析过的特征满足真正推理行为的标准。操控这些特征仅导致基准性能的小幅变化或轻微下降。

Conclusion: 结论指出，通过对比方法识别出的稀疏自动编码器特征主要捕获的是与推理相关的语言特性，而不是底层的推理计算。

Abstract: We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.

</details>


### [63] [AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces](https://arxiv.org/abs/2601.05680)
*Yeonsang Shin,Insoo Kim,Bongkeun Kim,Keonwoo Bae,Bohyung Han*

Main category: cs.LG

TL;DR: 提出了一种新的框架AGDC，用于同时建模离散和连续值的变长序列，通过结合分类预测与基于扩散的建模方法解决了现有基于离散化方法在高精度领域生成混合离散-连续序列时存在的可扩展性限制。


<details>
  <summary>Details</summary>
Motivation: 现有的基于离散化的模型在生成混合离散-连续序列方面存在局限性，尤其是在需要高精度的领域如半导体电路设计中，这种局限可能导致功能性失败。

Method: 开发了AGDC框架，该框架采用混合方法来联合处理离散值（通过分类预测）与连续值（通过基于扩散的建模）。此外，还引入了EOS logit调整机制及长度正则化项，并建立了大规模基准测试ContLayNet用于评估。

Result: 实验结果表明，在半导体布局、图形布局以及SVGs等多样化领域内，AGDC相比基于离散化的方法能够生成更高保真度的混合向量表示，实现了跨领域的可扩展高精度生成。

Conclusion: AGDC提供了一个有效解决高精度要求下混合离散-连续序列生成问题的新途径。

Abstract: Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.

</details>


### [64] [FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching](https://arxiv.org/abs/2601.05684)
*Hongyaoxing Gul,Lijuan Hu,Shuzi Niu,Fangfang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的量化方法FLRQ，旨在通过快速识别最佳秩并将其聚合以实现最小存储组合，从而在大型语言模型的后训练量化中有效减少模型大小和加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩后训练量化方法需要昂贵的微调来为大规模模型中的不同数据和层确定折衷秩，并且当前基于SVD的低秩近似增加了计算开销。

Method: 提出的方法FLRQ包括两个主要部分：基于Rank1-Sketch的灵活秩选择（R1-FLR）和剪辑下的最佳低秩逼近（BLC）。R1-FLR使用带有高斯投影的R1-Sketch进行快速低秩近似，允许为每一层提取异常值感知的秩；BLC则通过迭代法在缩放和剪辑策略下最小化低秩量化误差。

Result: FLRQ在全面实验中表现出强大的有效性与鲁棒性，在量化质量和算法效率两方面均达到了最先进的性能。

Conclusion: FLRQ作为一种新颖的解决方案，能够快速确定最佳秩并通过聚集这些秩来达到最小存储需求，同时保持了良好的准确性和高效性，解决了现有低秩后训练量化方法存在的问题。

Abstract: Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.

</details>


### [65] [mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations](https://arxiv.org/abs/2601.05732)
*Yongyi Yang,Jianyang Gao*

Main category: cs.LG

TL;DR: 提出了一种新的方法mHC-lite，通过将双随机矩阵显式构造为置换矩阵的凸组合来解决mHC中由于有限次Sinkhorn-Knopp迭代导致的近似误差以及实现效率问题。实验表明mHC-lite在保持或超越mHC性能的同时提高了训练吞吐量并消除了残差不稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法mHC使用有限次Sinkhorn-Knopp迭代来近似投影到Birkhoff多面体上，但这种方法无法保证严格的双随机性，并且需要专门的CUDA内核实现，增加了工程难度和移植性问题。

Method: 基于Birkhoff-von Neumann定理，提出mHC-lite，通过置换矩阵的凸组合直接构建双随机矩阵，从而确保了严格的双随机性，并且仅依赖于原生矩阵操作即可实现。

Result: 广泛的实验证明了mHC-lite不仅能够匹配甚至超过mHC的表现，而且即使采用朴素实现也能够达到更高的训练吞吐量，并且避免了HC和mHC中存在的残差不稳定现象。

Conclusion: mHC-lite提供了一种简洁有效的替代方案，解决了mHC因有限SK迭代带来的稳定性和实现复杂度问题，同时保持了良好的性能。

Abstract: Hyper-Connections (HC) generalizes residual connections by introducing dynamic residual matrices that mix information across multiple residual streams, accelerating convergence in deep neural networks. However, unconstrained residual matrices can compromise training stability. To address this, DeepSeek's Manifold-Constrained Hyper-Connections (mHC) approximately projects these matrices onto the Birkhoff polytope via iterative Sinkhorn--Knopp (SK) normalization. We identify two limitations of this approach: (i) finite SK iterations do not guarantee exact doubly stochasticity, leaving an approximation gap that can accumulate through network depth and undermine stability; (ii) efficient SK implementation requires highly specialized CUDA kernels, raising engineering barriers and reducing portability. Motivated by the Birkhoff--von Neumann theorem, we propose mHC-lite, a simple reparameterization that explicitly constructs doubly stochastic matrices as convex combinations of permutation matrices. This approach guarantees exact doubly stochasticity by construction and can be implemented using only native matrix operations. Extensive experiments demonstrate that mHC-lite matches or exceeds mHC in performance while achieving higher training throughput with a naive implementation and eliminating the residual instabilities observed in both HC and mHC. The code is publicly available at https://github.com/FFTYYY/mhc-lite.

</details>


### [66] [Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms](https://arxiv.org/abs/2601.05759)
*Turkan Simge Ispak,Salih Tileylioglu,Erdem Akagunduz*

Main category: cs.LG

TL;DR: 本研究将P波到达检测重新定义为自监督异常检测任务，通过对比不同变分自编码器配置发现注意力机制相比跳跃连接更能提高检测性能，特别是在近源范围内，表明了在自监督P波检测中全局上下文优先于像素级重建的重要性。


<details>
  <summary>Details</summary>
Motivation: 准确的P波检测对地震预警至关重要，但强震记录因高噪声水平、有限的标记数据和复杂的波形特征而带来挑战。

Method: 本研究将P波到达检测问题重构为一个自监督异常检测任务，并通过全面的网格搜索评估了492种不同的变分自编码器（VAE）架构配置，旨在探索不同架构如何平衡重建精度与异常区分能力之间的关系。

Result: 实验结果表明，虽然跳跃连接可以最小化重建误差（平均绝对误差约为0.0012），但它会导致“过度泛化”，使得模型能够重建噪声并掩盖检测信号；相比之下，注意力机制更注重全局背景而非局部细节，从而获得了最高的检测性能，其曲线下面积达到0.875。基于注意力机制的变分自编码器在0到40公里的近源范围内的曲线下面积达到了0.91。

Conclusion: 研究表明，在自监督P波检测任务中，倾向于全局上下文理解而非追求完美像素级重建的架构设计对于实现鲁棒性检测至关重要。

Abstract: Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce "overgeneralization", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.

</details>


### [67] [Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer](https://arxiv.org/abs/2601.05770)
*Yifan Zhang,Wei Bi,Kechi Zhang,Dongming Jin,Jie Fu,Zhi Jin*

Main category: cs.LG

TL;DR: 本文提出了一种名为离散Transformer的新架构，旨在解决从连续表示到离抽象逻辑转换的问题。通过功能解耦和温度退火采样技术，该方法能够促进人类可读程序的提取，并在保持与基于RNN基准相当性能的同时增强了对连续变量域的可解释性。


<details>
  <summary>Details</summary>
Motivation: 算法提取的目标是从特定算法任务训练的模型中直接合成可执行程序，但将这种方法扩展到Transformer时遇到了超位置问题，即纠缠特征以重叠方向编码阻碍了符号表达式的提取。

Method: 提出了一种称为Discrete Transformer的新架构，通过对数值注意力机制和数值MLP的功能进行严格分离，并采用温度退火采样策略来促进人类可读程序的提取。

Result: 实验证明，Discrete Transformer不仅达到了与基于RNN基线相当的性能水平，而且关键地将其可解释性扩展到了连续变量领域。此外，对退火过程的分析显示，高效的离散搜索经历了一个从探索到利用的明确阶段转变。

Conclusion: 这些发现共同确立了Discrete Transformer作为无示例算法发现的强大框架的地位，为提高Transformer可解释性提供了一条严谨的路径。

Abstract: Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.

</details>


### [68] [Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning](https://arxiv.org/abs/2601.05792)
*Manel Gil-Sorribes,Júlia Vilalta-Mor,Isaac Filella-Mercè,Robert Soliva,Álvaro Ciudad,Víctor Guallar,Alexis Molina*

Main category: cs.LG

TL;DR: 本文提出了一种名为Tensor-DTI的对比学习框架，通过整合来自分子图、蛋白质语言模型和结合位点预测的多模态嵌入来提高药物-靶标相互作用（DTI）建模的准确性。该方法在多个基准测试中优于现有基于序列和基于图的模型，并且即使在训练时排除了CDK2的情况下，也能够产生化学上合理的命中分布。此外，研究还探索了其在蛋白质-RNA和肽-蛋白相互作用中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前用于计算药物发现的药物-靶标相互作用（DTI）预测模型通常依赖于单一模态的预定义分子描述符或基于序列的嵌入，这些方法在表征能力上存在局限性。为了克服这一挑战并提高DTI预测的准确性，作者提出了一个新的框架。

Method: Tensor-DTI采用了一种孪生双编码器架构，能够同时捕捉化学与结构上的交互特征，并区分相互作用对与非相互作用对。它利用对比学习的方式，将从分子图、蛋白质语言模型以及结合位点预测得到的信息进行融合，以增强模型的学习能力。

Result: 在多种DTI基准数据集上的评估表明，Tensor-DTI的表现超过了现有的基于序列和基于图的方法。特别是在大规模化学库中针对CDK2进行推理时，即使是在训练过程中未包含CDK2的情况下，Tensor-DTI依然能够生成具有化学合理性的结果。此外，在与Glide对接及Boltz-2共折叠器相比的研究中，Tensor-DTI对于CDK2以及其他家族外目标展现了竞争力。

Conclusion: 本研究表明，通过对比学习整合多模态信息可以显著提高药物-靶标相互作用预测的准确性，同时也为虚拟筛选提供了更加可解释且可靠性更高的模型。此外，这种方法还有望应用于更广泛的生物分子相互作用类型。

Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.

</details>


### [69] [Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers](https://arxiv.org/abs/2601.05807)
*Mohamed Amine Hallam,Kuo-Kun Tseng*

Main category: cs.LG

TL;DR: 该论文研究了在长序列设置中，位置编码与token嵌入融合机制对Transformer性能的影响。实验结果表明，在处理长文本时选择合适的融合策略可以带来一致的性能提升，并且这种好处对于多种位置编码家族都是通用的。此外，还探讨了一种轻量级卷积门控机制，为长文档的融合层面引入局部归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 虽然大多数先前的工作都集中在设计新的位置编码上，但很少有人研究位置信息是如何与token嵌入相结合的。本文旨在探索不同的融合机制是否会影响模型的表现，特别是在处理长序列的情况下。

Method: 通过控制实验变量如相同的Transformer架构、数据分割和随机种子，比较了三种经典的融合策略：逐元素相加、带投影的串联以及标量门控融合。另外，为了验证这些增益是结构性而非随机性的，进行了配对种子分析和跨数据集比较。最后，探索了一种轻量级卷积门控机制以在融合级别引入局部归纳偏差。

Result: 实验显示，对于短文本来说，融合方式的选择影响不大；但对于长文档，则能够产生一致性的性能改进。进一步的实验指出，可学习融合的优势跨越了多个位置编码家族。

Conclusion: 位置编码融合是针对长序列Transformers的一个非平凡的设计选择，应该被视为一个明确的建模决策而不是固定的默认选项。

Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.

</details>


### [70] [Detecting Autism Spectrum Disorder with Deep Eye Movement Features](https://arxiv.org/abs/2601.05812)
*Zhanpei Huang,Taochen chen,Fangqing Gu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 研究提出了一种针对自闭症谱系障碍(ASD)的诊断工具，利用眼动数据并通过一种新的离散短期序列(DSTS)建模框架来区分ASD与正常发展个体。实验表明，该方法优于传统机器学习和复杂的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 鉴于眼动数据为检测ASD提供了非侵入性的手段，并且具有短期时间依赖性特征，研究旨在开发更有效的模型来捕捉这些细微的行为标记，以提高ASD诊断准确性。

Method: 设计了离散短期序列（DSTS）建模框架，结合类别感知表示和不平衡感知机制，专门用于高效捕捉复杂的眼动模式，区别于现有的基于Transformer模型中常用的堆叠注意力层方法。

Result: 通过在多个眼动数据集上的广泛测试，DSTS框架在识别ASD相关模式方面表现优于传统的机器学习技术和先进的深度学习模型。

Conclusion: 研究证明了DSTS框架能够有效利用眼动数据中的短期局部依赖性特征，为ASD诊断提供了一种新的、高效的途径。

Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.

</details>


### [71] [A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link](https://arxiv.org/abs/2601.05845)
*Eric Weine,Peter Carbonetto,Rafael A. Irizarry,Matthew Stephens*

Main category: cs.LG

TL;DR: 本文提出了带移位对数链接函数的泊松非负矩阵分解方法，以放宽现有方法中部分组合为加性的假设。新方法通过一个调参参数可以实现从加性到更接近乘性的组合方式变化，并提供了最大似然拟合算法及针对大规模稀疏数据集的近似计算方法，以提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的泊松非负矩阵分解方法假定在分解过程中“部分”是以加性方式组合的，这一假设在某些场景下并不自然。因此，作者提出了一种新的泊松NMF方法，采用移位对数链接函数来放松这种加性组合的限制，从而提供更加灵活的数据解释模型。

Method: 文章介绍了一种基于移位对数链接函数的泊松非负矩阵分解方法。该方法引入了一个可调节参数，随着参数的变化，模型从认为部分以加性方式结合（即标准泊松NMF）转变为更倾向于乘性方式结合。此外，还给出了适合此模型的最大似然估计拟合算法以及针对大型稀疏数据集设计的一种近似方法，用以显著降低计算时间。

Result: 通过在多个真实数据集上的应用示例表明，选择不同的链接函数会对泊松NMF的结果产生实质性影响；在某些情况下，使用移位对数链接函数相比传统的加性链接能够提高结果的可解释性。

Conclusion: 本研究提出的带有移位对数链接函数的泊松NMF不仅提供了超越传统加性假设的灵活性，而且在特定应用场景下还能增强分析结果的可解释性。此外，对于处理大规模稀疏数据时，所提供的近似计算方法有效提高了运算效率。

Abstract: Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable "parts-based" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the "parts" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.

</details>


### [72] [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](https://arxiv.org/abs/2601.05870)
*Huilin Deng,Hongchen Luo,Yue Zhu,Long Li,Zhuoyue Chen,Xinghao Zhao,Ming Li,Jihai Zhang,Mengchang Wang,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法IIB-LPO，通过在高熵状态触发潜在分支来多样化推理路径，并使用信息瓶颈原则作为轨迹过滤器和自我奖励机制，从而改善了大型语言模型在强化学习中的探索崩溃问题。实验结果表明，该方法在准确性和多样性指标上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在处理大型语言模型的推理时遇到了探索崩溃的问题，即随机展开的语义同质性导致模型陷入狭窄、过度优化的行为模式中。尽管当前的方法试图通过策略熵鼓励探索，但它们存在固有的局限性，如全局熵正则化容易受到奖励欺骗的影响，而局部令牌选择性更新难以克服预训练模型的强大归纳偏置。

Method: 提出了名为IIB-LPO的新方法，该方法将探索从对令牌分布进行统计扰动转变为推理轨迹的拓扑分支。IIB-LPO在高熵状态下触发潜变量分支以多样化推理路径，并且同时利用信息瓶颈原则作为轨迹筛选器和自我奖励机制，确保探索过程既简洁又具有信息量。

Result: 跨四个数学推理基准测试的实证结果显示，IIB-LPO达到了最先进的性能，在准确性上比以前的方法高出最多5.3%，在多样性度量上提高了7.4%。

Conclusion: IIB-LPO为解决大型语言模型在强化学习中遇到的探索崩溃问题提供了一个有效的方法，不仅提高了任务完成的准确性，还增强了推理路径的多样性。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.

</details>


### [73] [GlueNN: gluing patchwise analytic solutions with neural networks](https://arxiv.org/abs/2601.05889)
*Doyoung Kim,Donghee Lee,Hye-Sung Lee,Jiheon Lee,Jaeok Yi*

Main category: cs.LG

TL;DR: 提出了一种学习框架，通过将渐近解析解的积分常数提升为尺度依赖函数，并利用原始微分方程在整个域上约束这些系数函数，从而学习得到一个全局有效的解。该方法在化学动力学和宇宙学中的代表性问题中表现出色，能够准确再现全局解，并优于传统的边界匹配程序。


<details>
  <summary>Details</summary>
Motivation: 对于物理和工程中遇到的具有强烈尺度依赖项的复杂微分方程，当前通过区域划分简化方程然后匹配局部解来构造全局解的方法可能会失败，因为近似形式可能在匹配边界附近失效。

Method: 开发了一个学习框架，在这个框架内，渐近解析解的积分常数被视作尺度依赖函数，并且通过整个域上的原始微分方程来限制这些系数函数，以学习到不需要任意边界匹配就能平滑插值于渐近区间之间的全局有效解。

Result: 所提出的框架在化学动力学和宇宙学中的示例问题中成功地重现了全局解，并且其性能超过了传统匹配程序。

Conclusion: 这种基于学习的方法提供了一种新的途径来解决包含强尺度依赖性的复杂微分方程问题，能够在不依赖人工边界条件的情况下找到更准确的全局解。

Abstract: In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.

</details>


### [74] [Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates](https://arxiv.org/abs/2601.05909)
*Ayoub Ajarra,Debabrota Basu*

Main category: cs.LG

TL;DR: 本文研究了在任意更新下进行群体公平性审核的问题，提出了一种基于经验属性优化（EPO）oracle的PAC审核通用框架，并为统计平等性建立了与SP维度相关的无分布审核边界，该框架还能够自然地扩展到其他审核目标上。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型越来越多地嵌入社会基础设施中，对它们进行偏见审计变得越来越重要。然而，在现实世界的部署中，由于模型所有者可能会根据环境变化自适应地更新其模型，这使得审计变得更加复杂。这些更新可以改变基础模型类别同时保持某些感兴趣的属性不变，这就提出了在这种变动下什么可以被可靠审计的基本问题。

Method: 本文提出了一种基于经验属性优化（EPO）oracle的PAC审核通用框架来解决上述问题。对于统计平等性，通过引入一种新的组合度量——SP维度，来捕捉可接受策略更新的复杂性，并据此建立无分布审核边界。

Result: 研究表明，所提出的框架不仅能够有效估计使用最少标记样本的审核属性，如群体公平性，而且还能自然地扩展至诸如预测误差和鲁棒风险等其他审核目标。

Conclusion: 这项工作为在面对模型可能经历的战略性更改时如何有效地执行群体公平性审计提供了新的见解。它不仅有助于理解允许更新的信息复杂性，还提供了一种实用的方法来估计仅需少量标记样本就能完成的审核属性。

Abstract: As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.
  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.
  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.

</details>


### [75] [Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics](https://arxiv.org/abs/2601.05929)
*Sidney Shapiro,Burhanuddin Panvelwala*

Main category: cs.LG

TL;DR: 本文探讨了Meta开发的开源预测框架Prophet如何通过其可加性结构、开源实现和标准化工作流程来促进透明且可复制的预测实践，特别是在商业和金融分析领域。研究通过与ARIMA和随机森林模型在公开财务和零售数据集上的对比，评估了Prophet的表现及其在可解释性和可复制性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 在商业和金融分析领域，预测结果对决策至关重要，但传统预测方法需要大量手动调整且难以在专有环境中复制；机器学习方法虽然灵活，但在可解释性、随机训练过程及跨环境重现方面存在挑战。本文旨在探索一个既能保证可解释性又能确保标准化工作流以及易于访问的解决方案，以解决预测中的可再现性问题。

Method: 采用Meta开发的开源预测工具Prophet，并基于公开可用的金融和零售数据集，将其性能和可解释性与多种ARIMA设定（自动选择、手动指定及季节性变体）以及随机森林进行比较。整个实验设计受到严格控制并被完整记录下来。

Result: 研究表明，Prophet通过其独特的加法结构、开源实现方式及标准化的工作流程，在保持良好预测性能的同时提高了预测工作的透明度和可重复性。此外，还展示了使用Python的具体示例来说明Prophet如何简化预测工作流程并与分析管道无缝集成。

Conclusion: 本研究将Prophet定位为支持验证、审计能力和方法严谨性的基石，为基于Python的研究工作流提供了一个实用的参考框架，促进了可复制性预测的发展。

Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.

</details>


### [76] [LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection](https://arxiv.org/abs/2601.06016)
*Þór Sverrisson,Steinn Guðmundsson*

Main category: cs.LG

TL;DR: 本文介绍了一种基于变压器的癫痫检测器LookAroundNet，该检测器利用更宽的时间窗口来建模癫痫活动，并结合了感兴趣的段落前后的脑电图信号。通过在多种临床环境和记录模式下对多个脑电图数据集进行评估，结果显示LookAroundNet在不同数据分布中表现出色，具有良好的泛化能力和适合临床部署的计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于患者、记录条件和临床环境间癫痫动态的巨大差异，从脑电图（EEG）自动检测癫痫仍然很困难。为了克服这些挑战并推动自动化癫痫检测模型向临床可行解决方案发展。

Method: 提出了一种名为LookAroundNet的基于变压器的癫痫检测器，它使用较宽的时间窗口来建模癫痫活动，并且整合了感兴趣片段前后时期的EEG信号，以模仿临床医生在解读EEG记录时所使用的周围背景。

Result: LookAroundNet在跨数据集方面表现优异，对于之前未见过的记录条件也显示出很好的泛化能力，并且其运行成本与现实世界中的临床部署相兼容。研究结果表明，扩展的时间上下文、增加训练数据多样性以及模型集成是提高性能的关键因素。

Conclusion: 这项工作通过引入能够处理多样化数据分布并具有良好泛化能力的新型癫痫检测方法，为将自动癫痫检测模型推向临床应用迈出了重要一步。

Abstract: Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.

</details>
