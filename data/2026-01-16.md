<div id=toc></div>

# Table of Contents

- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LG](#cs.LG) [Total: 48]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1] [Subjective evaluation of UHD video coded using VVC with LCEVC and ML-VVC](https://arxiv.org/abs/2601.10448)
*Naeem Ramzan,Muhammad Tufail Khan*

Main category: cs.MM

TL;DR: 本文介绍了在VVC基础层上应用LCEVC作为增强层的多层视频编码配置的主观质量评估结果。测试比较了从HD VVC基础层生成并带有LCEVC增强的UHD输出与两个参考案例：上采样的VVC基础层解码和多层VVC(ML-VVC)。实验考虑了两种操作点，对应于增强层大约代表总比特率的10%和50%的情况。


<details>
  <summary>Details</summary>
Motivation: 为了评估在VVC基础层上使用LCEVC作为增强层的多层视频编码配置下的主观视频质量。

Method: 通过使用LCEVC Test Model (LTM)版本8.1对增强层进行编码，并遵循MPEG多层视频编码评估之前定义的测试方法和条件。采用退化类别评分(DCR)方法进行主观评价，共有25名参与者参与，数据集包括15个SDR和HDR序列。

Result: 报告的结果包括平均意见分数(MOS)及其95%置信区间，允许在所定义的测试范围内比较不同编码方法和操作点之间的感知质量。

Conclusion: 本研究提供了关于在VVC基础上叠加LCEVC增强层后的视频质量表现的见解，有助于理解该组合方案在不同比特率分配下的性能。

Abstract: This paper presents the results of a subjective quality assessment of a multilayer video coding configuration in which Low Complexity Enhancement Video Coding (LCEVC) is applied as an enhancement layer on top of a Versatile Video Coding (VVC) base layer. The evaluation follows the same test methodology and conditions previously defined for MPEG multilayer video coding assessments, with the LCEVC enhancement layer encoded using version 8.1 of the LCEVC Test Model (LTM). The test compares reconstructed UHD output generated from an HD VVC base layer with LCEVC enhancement against two reference cases: upsampled VVC base layer decoding and multilayer VVC (ML-VVC). Two operating points are considered, corresponding to enhancement layers representing approximately 10% and 50% of the total bitrate. Subjective assessment was conducted using the Degradation Category Rating (DCR) methodology with twenty five participants, across a dataset comprising fifteen SDR and HDR sequences. The reported results include Mean Opinion Scores (MOS) with associated 95% confidence intervals, enabling comparison of perceptual quality across coding approaches and operating points within the defined test scope.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [2] [STCRank: Spatio-temporal Collaborative Ranking for Interactive Recommender System at Kuaishou E-shop](https://arxiv.org/abs/2601.10027)
*Boyang Xia,Ruilin Bao,Hanjun Jiang,Jun Wang,Wenwu Ou*

Main category: cs.IR

TL;DR: 本文提出了一种新的时空协作排名框架（STCRank），旨在解决快手电商互动推荐系统中由于全屏UI和沉浸式下拉功能带来的目标间干扰及时间贪婪陷阱问题。通过多目标协作与多槽位协作模块，该系统实现了购买量和日活跃用户数的共同增长，并已在2025年6月上线使用。


<details>
  <summary>Details</summary>
Motivation: 为了解决快手电商平台主页推荐系统中存在的实时用户反馈响应不足的问题，同时应对由全屏界面设计导致的不同排序目标间的显式干扰以及序列推荐槽位转换中的时间贪婪陷阱挑战。

Method: 提出了一个名为Spatio-temporal collaborative ranking (STCRank)的新框架，包含两个主要模块：多目标协作(MOC)模块用于减少目标间的重叠和冲突；多槽位协作(MSC)模块则采用双阶段前瞻排名机制来实现全局最优解。

Result: 广泛实验表明，所提方法能够促进购买量和DAU的同时增长。

Conclusion: STCRank框架成功解决了因全屏UI和沉浸式浏览特性引起的目标干扰及时序贪婪陷阱问题，有效提升了用户体验和平台效益。

Abstract: As a popular e-commerce platform, Kuaishou E-shop provides precise personalized product recommendations to tens of millions of users every day. To better respond real-time user feedback, we have deployed an interactive recommender system (IRS) alongside our core homepage recommender system. This IRS is triggered by user click on homepage, and generates a series of highly relevant recommendations based on the clicked item to meet focused browsing demands. Different from traditional e-commerce RecSys, the full-screen UI and immersive swiping down functionality present two distinct challenges for regular ranking system. First, there exists explicit interference (overlap or conflicts) between ranking objectives, i.e., conversion, view and swipe down. This is because there are intrinsic behavioral co-occurrences under the premise of immersive browsing and swiping down functionality. Second, the ranking system is prone to temporal greedy traps in sequential recommendation slot transitions, which is caused by full-screen UI design. To alleviate these challenges, we propose a novel Spatio-temporal collaborative ranking (STCRank) framework to achieve collaboration between multi-objectives within one slot (spatial) and between multiple sequential recommondation slots. In multi-objective collaboration (MOC) module, we push Pareto frontier by mitigating the objective overlaps and conflicts. In multi-slot collaboration (MSC) module, we achieve global optima on overall sequential slots by dual-stage look-ahead ranking mechanism. Extensive experiments demonstrate our proposed method brings about purchase and DAU co-growth. The proposed system has been already deployed at Kuaishou E-shop since 2025.6.

</details>


### [3] [Development of Ontological Knowledge Bases by Leveraging Large Language Models](https://arxiv.org/abs/2601.10436)
*Le Ngoc Luyen,Marie-Hélène Abel,Philippe Gouspillou*

Main category: cs.IR

TL;DR: 本文介绍了一种利用大型语言模型(LLMs)优化本体知识库(OKB)开发的新方法，通过案例研究展示了该方法在加速本体构建、提高一致性、减少偏见和增强透明度方面的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的本体知识库（OKB）开发面临着扩展性、一致性和适应性等方面的挑战。随着生成式AI特别是大型语言模型（LLMs）的发展，为自动化和改进OKB的开发提供了新的可能。

Method: 提出了一种结构化、迭代的方法论，利用LLMs来优化知识获取、自动化本体工件生成，并支持持续的精细化循环。并通过一个专注于车辆销售领域用户情境概况本体发展的详细案例研究来展示这种方法的应用。

Result: 显著加快了本体构造过程，提高了本体的一致性，有效缓解了偏见问题，并增强了本体工程过程中的透明度。

Conclusion: 将LLMs集成到本体开发中具有变革性的潜力，可以显著改善知识管理系统中的可扩展性、集成能力和整体效率。

Abstract: Ontological Knowledge Bases (OKBs) play a vital role in structuring domain-specific knowledge and serve as a foundation for effective knowledge management systems. However, their traditional manual development poses significant challenges related to scalability, consistency, and adaptability. Recent advancements in Generative AI, particularly Large Language Models (LLMs), offer promising solutions for automating and enhancing OKB development. This paper introduces a structured, iterative methodology leveraging LLMs to optimize knowledge acquisition, automate ontology artifact generation, and enable continuous refinement cycles. We demonstrate this approach through a detailed case study focused on developing a user context profile ontology within the vehicle sales domain. Key contributions include significantly accelerated ontology construction processes, improved ontological consistency, effective bias mitigation, and enhanced transparency in the ontology engineering process. Our findings highlight the transformative potential of integrating LLMs into ontology development, notably improving scalability, integration capabilities, and overall efficiency in knowledge management systems.

</details>


### [4] [iTIMO: An LLM-empowered Synthesis Dataset for Travel Itinerary Modification](https://arxiv.org/abs/2601.10609)
*Zhuoxuan Huang,Yunshan Ma,Hongyu Zhang,Hua Ma,Zhu Sun*

Main category: cs.IR

TL;DR: 研究提出了行程修改任务的正式定义，并引入了iTIMO数据集，旨在解决现有研究中对行程修改探索不足的问题。通过一种通用流程生成需要修改的行程数据，该流程将数据生成视为基于意图的扰动任务，利用大型语言模型对实际行程进行替换、添加和删除三种原子编辑操作。此外，还设计了一种混合评估指标以保证扰动的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的旅行规划研究主要集中在固定的行程安排上，对于旅途中常见的行程修改需求关注较少。研究者认为缺乏需要修改的行程数据是阻碍这一领域研究的关键瓶颈。

Method: 1. 正式定义了行程修改任务，并为此创建了一个专门的数据集iTIMO。
2. 提出一个通用流程来生成需要修改的行程数据，这个过程被视为基于意图的扰动任务。
3. 利用大型语言模型（LLMs）执行三种基本编辑操作：替换（REPLACE）、增加（ADD）以及删除（DELETE），每项变动都基于三个意图：受欢迎程度的变化、空间距离的调整以及类别多样性的改变。
4. 设计了一个混合评价指标体系以确保扰动的效果。

Result: 通过对iTIMO数据集上的全面实验，揭示了当前大型语言模型在处理行程修改任务时存在的局限性，并为未来的研究指出了几个有价值的方向。

Conclusion: 本研究不仅填补了行程修改相关研究领域的空白，而且通过开发iTIMO数据集及提出一套有效的数据生成与评估方法论，为促进该领域的进一步发展奠定了基础。

Abstract: Addressing itinerary modification is crucial for enhancing the travel experience as it is a frequent requirement during traveling. However, existing research mainly focuses on fixed itinerary planning, leaving modification underexplored. To bridge this gap, we formally define the itinerary modification task and introduce iTIMO, a dataset specifically tailored for this purpose. We identify the lack of {\itshape need-to-modify} itinerary data as the critical bottleneck hindering research on this task and propose a general pipeline to overcome it. This pipeline frames the generation of such data as an intent-driven perturbation task. It instructs large language models to perturb real world itineraries using three atomic editing operations: REPLACE, ADD, and DELETE. Each perturbation is grounded in three intents, including disruptions of popularity, spatial distance, and category diversity. Furthermore, a hybrid evaluation metric is designed to ensure perturbation effectiveness. We conduct comprehensive experiments on iTIMO, revealing the limitations of current LLMs and lead to several valuable directions for future research. Dataset and corresponding code are available at https://github.com/zelo2/iTIMO.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Putting green software principles into practice](https://arxiv.org/abs/2601.09741)
*James Uther*

Main category: cs.SE

TL;DR: 本文通过一个实际运行在公共云上的产品案例，探讨了实现绿色软件的实践解决方案，特别是利用无服务器系统的成本影响来提高效率，并总结了一些有效的'绿色软件'原则。


<details>
  <summary>Details</summary>
Motivation: 尽管测量和减少计算系统排放的二氧化碳的需求及理论方法已经明确，但现实世界的例子仍然有限。

Method: 作者描述了一个针对正在公共云上运行的实际产品的绿色软件之旅，讨论了发现的实际解决方案，尤其是利用无服务器系统的成本效应来推动效率提升。

Result: 通过采取这些措施，提高了软件在公共云上的运行效率，并且基于成本控制的角度促进了环境友好型软件的发展。

Conclusion: 文章总结了几条适用于该项目的‘绿色软件’原则，为其他希望实施类似策略以减少碳足迹的项目提供了参考。

Abstract: The need and theoretical methods for measuring and reducing CO2 emitted by computing systems are well understood, but real-world examples are still limited. We describe a journey towards green software for a live product running on a public cloud. We discuss practical solutions found, in particular using the cost implications of serverless systems to drive efficiency. We end with some `green software' principles that worked well in this project.

</details>


### [6] [A Governance Model for IoT Data in Global Manufacturing](https://arxiv.org/abs/2601.09744)
*Vignesh Alagappan*

Main category: cs.SE

TL;DR: 本文提出了一种联邦治理模型，以应对全球制造业环境中工业物联网平台在大规模管理IoT数据时遇到的系统性挑战。该模型强调基于合同的互操作性、代码化策略执行和以资产为中心的责任制，并解决了在不需要集中控制操作技术系统的情况下实现语义一致性、质量保证和法规遵从的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管数据摄入和存储能力已经显著成熟，但企业在大规模管理IoT数据方面仍然面临系统性挑战。这些挑战并非源于工具限制，而是由于缺乏与分布式运营所有权、异构源系统以及边缘持续变化实际情况相一致的治理模型。

Method: 论文提出了一个联邦治理模型，重点在于合同驱动的互操作性、政策即代码的强制执行以及全球制造组织中的资产中心责任制。

Result: 该模型能够在不需要对运营技术系统进行集中控制的情况下，在架构边界上实施治理，从而实现语义一致性、质量保证和法规遵从。

Conclusion: 这项工作贡献了一个基于制造物联网需求和约束分析的系统架构和设计框架；实证验证将是未来的工作。

Abstract: Industrial IoT platforms in global manufacturing environments generate continuous operational data across production assets, utilities, and connected products. While data ingestion and storage capabilities have matured significantly, enterprises continue to face systemic challenges in governing IoT data at scale. These challenges are not rooted in tooling limitations but in the absence of a governance model that aligns with the realities of distributed operational ownership, heterogeneous source systems, and continuous change at the edge. This paper presents a federated governance model that emphasizes contract-driven interoperability, policy-as-code enforcement, and asset-centric accountability across global manufacturing organizations. The model addresses governance enforcement at architectural boundaries, enabling semantic consistency, quality assurance, and regulatory compliance without requiring centralized control of operational technology systems. This work contributes a systems architecture and design framework grounded in analysis of manufacturing IoT requirements and constraints; empirical validation remains future work

</details>


### [7] [Enhancing Formal Software Specification with Artificial Intelligence](https://arxiv.org/abs/2601.09745)
*Antonio Abu Nassar,Eitan Farchi*

Main category: cs.SE

TL;DR: 该论文提出了一种利用自然语言和轻量级数学符号作为中间规范语言，并通过AI进行审查和优化的方法，从而在保留形式化规范优点的同时显著降低了开发成本。


<details>
  <summary>Details</summary>
Motivation: 尽管形式化软件规范能够实现早期错误检测和明确不变量，但由于其高昂的标记开销以及使用传统形式化语言所需的专业知识，它在工业界的采用受到了限制。

Method: 研究者们采用了自然语言加上轻量级数学符号，并用\LaTeX编写作为中间形式的语言，在代码生成之前由AI对这些描述进行审阅与精炼。

Result: 当应用于组织知识增长的一个非平凡模拟时，这种方法使得项目能够在首次尝试时就获得正确的实现，并且实现了早期验证、明确不变量及设计上的正确性，同时大幅度减少了开发工作量。

Conclusion: 最新的人工智能进展让保持形式化规范许多好处的同时大幅降低成本成为可能，这表明了区分系统分析师控制的部分（可以从形式化规范中极大受益）和其他部分的重要性。

Abstract: Formal software specification is known to enable early error detection and explicit invariants, yet it has seen limited industrial adoption due to its high notation overhead and the expertise required to use traditional formal languages. This paper presents a case study showing that recent advances in artificial intelligence make it possible to retain many of the benefits of formal specification while substantially reducing these costs. The necessity of a clear distinction between what is controlled by the system analyst and can highly benefits from the rigor of formal specification and what need not be controlled is demonstrated. We use natural language augmented with lightweight mathematical notation and written in \LaTeX\ as an intermediate specification language, which is reviewed and refined by AI prior to code generation. Applied to a nontrivial simulation of organizational knowledge growth, this approach enables early validation, explicit invariants, and correctness by design, while significantly reducing development effort and producing a correct implementation on the first attempt.

</details>


### [8] [SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments](https://arxiv.org/abs/2601.09750)
*Robert K. Strehlow,Tobias Küster,Oskar F. Kupke,Brandon Llanque Kurps,Fikret Sivrikaya,Sahin Albayrak*

Main category: cs.SE

TL;DR: 本文介绍了一种名为SAGE的对话AI接口，它基于OPACA框架用于工具发现和执行。SAGE提供了丰富的可扩展性和模块性，使得添加新工具或服务变得容易，并且能够无缝切换不同的模型。研究中实现了多种任务解决策略，并通过一系列基准服务进行了评估，展示了不同策略的独特优势与劣势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在问答场景中表现出色，但实际应用通常需要访问实时信息或执行操作的工具。现有的方法虽然允许对特定用例进行微调，但在快速变化的软件环境和个人化服务需求下，不断开发和集成新工具成为必要。此外，领域或公司特有工具虽能显著提升LLM的实用性，但其整合过程存在挑战。因此，需要一种策略来动态定义并整合新工具至LLM中，同时还需要稳健且可扩展的零样本提示方法以高效利用这些工具。

Method: 提出了SAGE系统，这是一个基于OPACA框架的专业对话AI界面，旨在简化为LLM添加新工具或服务的过程。SAGE具备高度可扩展性和模块化设计，支持轻松更换底层模型及选择不同的提示方法。研究还探索了多种复杂度各异的任务解决方案，并通过综合基准服务集对其进行评估。

Result: 研究结果表明，所提出的几种任务解决策略各有优劣，在不同应用场景下表现不一。SAGE与OPACA框架，以及各种基准服务和实验结果，都已作为开源/开放数据发布于GitHub上。

Conclusion: SAGE作为一个灵活、可扩展的对话AI接口，成功地解决了将新工具动态集成到LLM中的问题，同时也提供了一系列有效的任务解决策略。这不仅增强了LLM处理现实世界问题的能力，也为未来的研究提供了宝贵的资源。

Abstract: Large language models (LLMs) have proven to work well in question-answering scenarios, but real-world applications often require access to tools for live information or actuation. For this, LLMs can be extended with tools, which are often defined in advance, also allowing for some fine-tuning for specific use cases. However, rapidly evolving software landscapes and individual services require the constant development and integration of new tools. Domain- or company-specific tools can greatly elevate the usefulness of an LLM, but such custom tools can be problematic to integrate, or the LLM may fail to reliably understand and use them. For this, we need strategies to define new tools and integrate them into the LLM dynamically, as well as robust and scalable zero-shot prompting methods that can make use of those tools in an efficient manner. In this paper, we present SAGE, a specialized conversational AI interface, based on the OPACA framework for tool discovery and execution. The integration with OPACA makes it easy to add new tools or services for the LLM to use, while SAGE itself presents rich extensibility and modularity. This not only provides the ability to seamlessly switch between different models (e.g. GPT, LLAMA), but also to add and select prompting methods, involving various setups of differently prompted agents for selecting and executing tools and evaluating the results. We implemented a number of task-solving strategies, making use of agentic concepts and prompting methods in various degrees of complexity, and evaluated those against a comprehensive set of benchmark services. The results are promising and highlight the distinct strengths and weaknesses of different task-solving strategies. Both SAGE and the OPACA framework, as well as the different benchmark services and results, are available as Open Source/Open Data on GitHub.

</details>


### [9] [Investigating Tool-Memory Conflicts in Tool-Augmented LLMs](https://arxiv.org/abs/2601.09760)
*Jiali Cheng,Rui Pan,Hadi Amiri*

Main category: cs.SE

TL;DR: 本文提出了一种新的知识冲突类型——工具-记忆冲突（TMC），即工具增强的大语言模型的内部参数化知识与外部工具知识之间的矛盾，并发现现有解决冲突的技术对此类冲突效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨工具增强的大语言模型中出现的一种新类型的知识冲突，即工具-记忆冲突(TMC)，并寻找有效的解决方法。

Method: 通过定义和分析工具-记忆冲突(TMC)的概念，评估现有冲突解决技术（包括基于提示和RAG的方法）在处理TMC时的有效性。

Result: 结果表明，尽管现有的大语言模型非常强大，但在STEM相关任务上尤其容易遭受TMC的影响；同时，不同的情况下，工具知识和参数化知识可能被以不同方式优先考虑；而且，现有的冲突解决技术对于工具-记忆冲突并不十分有效。

Conclusion: 提出了工具-记忆冲突作为工具增强型大语言模型面临的一个新挑战，并指出当前解决这类冲突的方法存在不足之处。

Abstract: Tool-augmented large language models (LLMs) have powered many applications. However, they are likely to suffer from knowledge conflict. In this paper, we propose a new type of knowledge conflict -- Tool-Memory Conflict (TMC), where the internal parametric knowledge contradicts with the external tool knowledge for tool-augmented LLMs. We find that existing LLMs, though powerful, suffer from TMC, especially on STEM-related tasks. We also uncover that under different conditions, tool knowledge and parametric knowledge may be prioritized differently. We then evaluate existing conflict resolving techniques, including prompting-based and RAG-based methods. Results show that none of these approaches can effectively resolve tool-memory conflicts.

</details>


### [10] [Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize Requirements for Compliance Test Case Generation](https://arxiv.org/abs/2601.09762)
*Zhiyi Xue,Xiaohong Chen,Min Zhang*

Main category: cs.SE

TL;DR: 本文提出了RAFT框架，用于通过从多个大型语言模型中明确隐性监管知识来自动化生成合规性测试。RAFT在金融、汽车和电力领域的实验表明其性能达到专家级别，并且显著优于最先进方法，同时减少了总的生成和审查时间。


<details>
  <summary>Details</summary>
Motivation: 高度监管领域中的合规性测试至关重要，但主要依赖人工进行，需要领域专家将复杂的法规转化为可执行的测试用例。虽然大型语言模型（LLMs）展示了自动化的潜力，但它们容易产生幻觉限制了可靠的应用。现有的混合方法通过使用形式化模型约束LLMs来缓解这一问题，但仍依赖于成本高昂的手动建模。

Method: 本文提出了一种名为RAFT的新框架，该框架能够通过从多个LLMs中提取隐性的监管知识并将其整合到三个产物中：领域元模型、正式的需求表示以及可测试性约束条件。这些产物随后被动态地注入提示中，以指导高精度的需求形式化和自动化测试生成。

Result: 跨金融、汽车和电力领域的实验证明，RAFT达到了专家级的表现，大幅超越了现有最先进的方法，并且整体上减少了生成与复核所需的时间。

Conclusion: RAFT框架为自动化合规性测试提供了一种有效的解决方案，不仅提高了测试生成的准确性和效率，还减少了对人工干预的需求，从而降低了成本。

Abstract: Compliance testing in highly regulated domains is crucial but largely manual, requiring domain experts to translate complex regulations into executable test cases. While large language models (LLMs) show promise for automation, their susceptibility to hallucinations limits reliable application. Existing hybrid approaches mitigate this issue by constraining LLMs with formal models, but still rely on costly manual modeling. To solve this problem, this paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation via explicating tacit regulatory knowledge from multiple LLMs. RAFT employs an Adaptive Purification-Aggregation strategy to explicate tacit regulatory knowledge from multiple LLMs and integrate it into three artifacts: a domain meta-model, a formal requirements representation, and testability constraints. These artifacts are then dynamically injected into prompts to guide high-precision requirement formalization and automated test generation. Experiments across financial, automotive, and power domains show that RAFT achieves expert-level performance, substantially outperforms state-of-the-art (SOTA) methods while reducing overall generation and review time.

</details>


### [11] [Adoption and Evolution of Code Style and Best Programming Practices in Open-Source Projects](https://arxiv.org/abs/2601.09832)
*Alvari Kupari,Nasser Giacaman,Valerio Terragni*

Main category: cs.SE

TL;DR: 本文分析了GitHub上1,036个流行的开源Java项目，研究代码风格和编程实践的采纳情况及其随时间的变化。研究发现Javadoc和命名违规是最常见的问题，并且许多项目违反了Google Java Style Guide中的规则。遵循编码规范声明的仓库显示出略高的整体合规性。


<details>
  <summary>Details</summary>
Motivation: 维持软件项目中的代码风格对于保持整体代码质量至关重要。遵循这些约定可以提高可维护性、可理解性和可扩展性。此外，在软件开发过程中遵循最佳实践可以提高性能并减少错误发生的可能性。

Method: 通过分析GitHub上1,036个流行的开源Java项目来研究代码风格和编程实践的采纳及演变情况，同时定期追踪活跃仓库中遵守编码标准的变化情况。

Result: 在所研究的仓库中发现了广泛的违规行为，其中Javadoc和命名方面的违规最为常见。此外，还发现在现代静态分析工具经常忽略的一些类别中，存在大量违反Google Java Style Guide的情况。声称遵循代码风格实践的仓库展示出略高一点的整体遵循度。

Conclusion: 研究结果为采用代码风格和编程实践提供了有价值的见解，强调了开源开发社区需要改进的关键领域。此外，论文还指出了重要的经验教训，并提出了未来提高Java项目代码质量的方向。

Abstract: Following code style conventions in software projects is essential for maintaining overall code quality. Adhering to these conventions improves maintainability, understandability, and extensibility. Additionally, following best practices during software development enhances performance and reduces the likelihood of errors. This paper analyzes 1,036 popular open-source JAVA projects on GITHUB to study how code style and programming practices are adopted and evolve over time, examining their prevalence and the most common violations. Additionally, we study a subset of active repositories on a monthly basis to track changes in adherence to coding standards over time. We found widespread violations across repositories, with Javadoc and Naming violations being the most common. We also found a significant number of violations of the GOOGLE Java Style Guide in categories often missed by modern static analysis tools. Furthermore, repositories claiming to follow code-style practices exhibited slightly higher overall adherence to code-style and best-practices. The results provide valuable insights into the adoption of code style and programming practices, highlighting key areas for improvement in the open-source development community. Furthermore, the paper identifies important lessons learned and suggests future directions for improving code quality in JAVA projects.

</details>


### [12] [On Fun for Teaching Large Programming Courses](https://arxiv.org/abs/2601.09842)
*Walid Maalej*

Main category: cs.SE

TL;DR: 本文介绍了一种通过趣味活动提高大型软件开发基础课程学生参与度和理解力的方法。该方法包括十种物理趣味活动，旨在帮助学生更好地记忆和反思抽象的软件开发概念。研究结果表明，这些活动有助于学生保持专注、记住关键概念，并在课后进行反思。


<details>
  <summary>Details</summary>
Motivation: 在传统的大学软件开发基础教学中，由于教室规模大，学生容易感到无聊、分心以及失去兴趣；同时，讲师也难以与学生有效互动。为了改善这种情况，作者提出了一系列有趣的活动来激发学生的积极性，并帮助他们理解和记忆软件开发中的基本概念。

Method: 研究者们设计并实施了包含十个物理趣味活动的教学方案，如模拟LA-OLA算法、使用纸飞机模拟对象消息传递等。这些活动被连续三年应用于超过500名学生的课程中。此外，还对15名曾经参加过该课程的学生及全球范围内的14位有经验的教育工作者进行了访谈调查。

Result: 研究发现，这些趣味性活动能够有效地让学生保持注意力集中，并且对于加深学生对关键知识点的记忆具有积极作用。同时，活动后的反思也有助于巩固学习成果。不过，要使这些活动被广泛接受并且有效果，关键在于保证它们简明扼要并与所教授的内容紧密相关。

Conclusion: 通过引入精心设计的趣味性活动，可以显著提升大规模软件开发入门课程的教学质量。这样的方法不仅增加了课堂上的互动性，还有利于学生更好地掌握重要概念。但需要注意的是，活动的设计应当简洁明了，直接关联到具体的学习目标上。

Abstract: Teaching software development basics to hundreds of students in a frontal setting is cost-efficient and thus still common in universities. However, in a large lecture hall, students can easily get bored, distracted, and disengaged. The frontal setting can also frustrate lecturers since interaction opportunities are limited and hard to scale. Fun activities can activate students and, if well designed, can also help remember and reflect on abstract software development concepts. We present a novel catalogue of ten physical fun activities, developed over years to reflect on basic programming and software development concepts. The catalogue includes the execution of a LA-OLA algorithm as in stadiums, using paper planes to simulate object messages and pointers, and traversing a lecture hall as a tree or a recursive structure. We report our experience of using the activities in a large course with 500+ students three years in a row. We also conducted an interview study with 15 former students of the course and 14 experienced educators from around the globe. The results suggest that the fun activities can enable students to stay focused, remember key concepts, and reflect afterwards. However, keeping the activities concise and clearly linked to the concepts taught seems to be key to their acceptance and effectiveness.

</details>


### [13] [Beyond Strict Rules: Assessing the Effectiveness of Large Language Models for Code Smell Detection](https://arxiv.org/abs/2601.09873)
*Saymon Souza,Amanda Santana,Eduardo Figueiredo,Igor Muzetti,João Eduardo Montandon,Lionel Briand*

Main category: cs.SE

TL;DR: 本研究评估了四种大型语言模型（LLMs）在30个Java项目中检测九种代码异味的有效性，并提出了一种结合LLMs和静态分析工具的检测策略。结果显示，对于结构简单的代码异味，LLMs表现良好；而对于复杂异味，则生成更多假阳性结果。因此，最佳策略取决于在代码异味检测中是更重视召回率还是精确度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已在多种软件工程活动中展现出卓越的能力，但其在代码异味检测方面的应用仍待进一步探索。与静态分析工具相比，LLMs能够支持更加灵活和适应性强的检测方法，以应对代码异味的独特属性。

Method: 研究选择了DeepSeek-R1、GPT-5 mini、Llama-3.3和Qwen2.5-Code这四款LLMs，对30个Java项目中的九种代码异味进行了评测。为此创建了一个基准数据集，该数据集由76位开发者手动检查268个潜在代码异味样本后形成。此外，还提出并测试了一种结合使用LLMs与静态分析工具的新检测策略。

Result: 结果显示，对于像大类（Large Class）和长方法（Long Method）这样结构相对简单的代码异味，LLMs表现得非常好。不过，不同LLMs及工具在识别不同类型代码异味时效果各异。所提出的结合LLMs与静态分析工具的策略，在九种代码异味中有五种按照F1分数衡量优于单独使用任一方。然而，这种组合策略也增加了复杂代码异味的误报率。

Conclusion: 研究表明，对于代码异味检测而言，最佳策略的选择取决于优先考虑召回率还是精确度。结合LLMs与静态分析工具的方法在某些情况下可以提高检测性能，但也可能增加复杂代码异味的假阳性结果。

Abstract: Code smells are symptoms of potential code quality problems that may affect software maintainability, thus increasing development costs and impacting software reliability. Large language models (LLMs) have shown remarkable capabilities for supporting various software engineering activities, but their use for detecting code smells remains underexplored. However, unlike the rigid rules of static analysis tools, LLMs can support flexible and adaptable detection strategies tailored to the unique properties of code smells. This paper evaluates the effectiveness of four LLMs -- DeepSeek-R1, GPT-5 mini, Llama-3.3, and Qwen2.5-Code -- for detecting nine code smells across 30 Java projects. For the empirical evaluation, we created a ground-truth dataset by asking 76 developers to manually inspect 268 code-smell candidates. Our results indicate that LLMs perform strongly for structurally straightforward smells, such as Large Class and Long Method. However, we also observed that different LLMs and tools fare better for distinct code smells. We then propose and evaluate a detection strategy that combines LLMs and static analysis tools. The proposed strategy outperforms LLMs and tools in five out of nine code smells in terms of F1-Score. However, it also generates more false positives for complex smells. Therefore, we conclude that the optimal strategy depends on whether Recall or Precision is the main priority for code smell detection.

</details>


### [14] [Self-reflection in Automated Qualitative Coding: Improving Text Annotation through Secondary LLM Critique](https://arxiv.org/abs/2601.09905)
*Zackary Okun Dunivin,Mobina Noori,Seth Frey,Curtis Atkinson*

Main category: cs.SE

TL;DR: 提出了一种两阶段工作流程，通过使用大型语言模型（LLM）进行初步编码和随后的自我反思来改进对大数据集的定性编码。该方法在处理Apache软件基金会项目评估讨论中的3000封高内容电子邮件时表现良好，能够有效降低错误率并提高F1分数。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）为大规模数据集提供了先进的定性编码能力，但在零样本或少量样本情况下，这些分类器可能会产生大量错误。即使经过精心设计和验证提示也是如此。研究旨在找到一种解决方案，以减少这类错误并提高分类准确性。

Method: 开发了一个通用的两阶段工作流：首先由一个LLM应用人类设计且适应于LLM的代码本；然后另一个LLM作为批评者，基于重新阅读源文本以及第一阶段模型给出的理由，对每个正标签进行自我反思，并作出最终决定。通过对Apache Software Foundation项目评估讨论中抽取的大约3000封信息丰富的邮件进行测试，评估了这种方法的有效性。

Result: 实验结果表明，单凭第一阶段的LLM标注，其假阳性率在8%到54%之间，尽管在测试中F1得分达到了0.74至1.00。引入第二阶段即自我反思步骤后，所有第一阶段注释的重新编码使得F1得分提高了0.04至0.25，特别是对于两个表现较差的代码而言，从0.52和0.55分别提升到了0.69和0.79。

Conclusion: 通过结合第一阶段LLM的高召回率与第二阶段的批判性审查，可以实现优先考虑精确度的同时保持计算成本较低的目标。此外，在人类指导和验证下，这种自我反思机制可以融入现有的LLM辅助注释流程中，有助于减少噪音并可能挽救原本不可用的分类器。

Abstract: Large language models (LLMs) allow for sophisticated qualitative coding of large datasets, but zero- and few-shot classifiers can produce an intolerable number of errors, even with careful, validated prompting. We present a simple, generalizable two-stage workflow: an LLM applies a human-designed, LLM-adapted codebook; a secondary LLM critic performs self-reflection on each positive label by re-reading the source text alongside the first model's rationale and issuing a final decision. We evaluate this approach on six qualitative codes over 3,000 high-content emails from Apache Software Foundation project evaluation discussions. Our human-derived audit of 360 positive annotations (60 passages by six codes) found that the first-line LLM had a false-positive rate of 8% to 54%, despite F1 scores of 0.74 and 1.00 in testing. Subsequent recoding of all stage-one annotations via a second self-reflection stage improved F1 by 0.04 to 0.25, bringing two especially poor performing codes up to 0.69 and 0.79 from 0.52 and 0.55 respectively. Our manual evaluation identified two recurrent error classes: misinterpretation (violations of code definitions) and meta-discussion (debate about a project evaluation criterion mistaken for its use as a decision justification). Code-specific critic clauses addressing observed failure modes were especially effective with testing and refinement, replicating the codebook-adaption process for LLM interpretation in stage-one. We explain how favoring recall in first-line LLM annotation combined with secondary critique delivers precision-first, compute-light control. With human guidance and validation, self-reflection slots into existing LLM-assisted annotation pipelines to reduce noise and potentially salvage unusable classifiers.

</details>


### [15] [S$^2$F: Principled Hybrid Testing With Fuzzing, Symbolic Execution, and Sampling](https://arxiv.org/abs/2601.10068)
*Lianjing Wang,Yufeng Zhang,Kenli Li,Zhenbang Chen,Xu Zhou,Pengfei Wang,Guangning Song,Ji Wang*

Main category: cs.SE

TL;DR: 提出了一种新的混合测试架构S$^2$F，它结合了传统符号执行的精确性和定制符号执行引擎的可扩展性，以解决现有混合测试工具在分支过度修剪和采样应用不当方面的问题。实验表明，S$^2$F在边缘覆盖率上平均提高了6.14%，在发现崩溃方面提升了32.6%。


<details>
  <summary>Details</summary>
Motivation: 现有的最先进（SOTA）混合测试工具未能充分利用符号执行和采样的能力，导致分支过度修剪及采样未被恰当地应用于合适的分支上。

Method: 开发了一种新型混合测试框架S$^2$F，该框架旨在通过结合常规符号执行的准确性与定制化符号执行引擎的可拓展性来克服上述问题，并提出了若干原则用于指导模糊测试、符号执行和抽样的整合使用。

Result: 对15个实际程序进行了广泛测试，结果表明S$^2$F相比SOTA工具，在边缘覆盖率上平均增长了6.14%，对于已发现的崩溃数量则有32.6%的增长。此外，S$^2$F还揭露了三个先前未知的实际程序崩溃案例。

Conclusion: 通过引入一种改进的混合测试方法S$^2$F，可以显著提高软件测试效率和质量，特别是在增加代码覆盖范围和识别更多潜在错误方面表现突出。

Abstract: Hybrid testing that integrates fuzzing, symbolic execution, and sampling has demonstrated superior testing efficiency compared to individual techniques. However, the state-of-the-art (SOTA) hybrid testing tools do not fully exploit the capabilities of symbolic execution and sampling in two key aspects. First, the SOTA hybrid testing tools employ tailored symbolic execution engines that tend to over-prune branches, leading to considerable time wasted waiting for seeds from the fuzzer and missing opportunities to discover crashes. Second, existing methods do not apply sampling to the appropriate branches and therefore cannot utilize the full capability of sampling. To address these two limitations, we propose a novel hybrid testing architecture that combines the precision of conventional symbolic execution with the scalability of tailored symbolic execution engines. Based on this architecture, we propose several principles for combining fuzzing, symbolic execution, and sampling. We implement our method in a hybrid testing tool S$^2$F. To evaluate its effectiveness, we conduct extensive experiments on 15 real-world programs. Experimental results demonstrate that S$^2$F outperforms the SOTA tool, achieving an average improvement of 6.14% in edge coverage and 32.6% in discovered crashes. Notably, our tool uncovers three previously unknown crashes in real-world programs.

</details>


### [16] [Mark My Works Autograder for Programming Courses](https://arxiv.org/abs/2601.10093)
*Yiding Qiu,Seyed Mahdi Azimi,Artem Lensky*

Main category: cs.SE

TL;DR: 介绍了一个名为Mark My Works的本地自动评分系统，该系统结合了传统的单元测试与大语言模型生成的解释，用于为大型编程课程提供及时详细的反馈。尽管AI评分与人工评分之间没有线性相关性，但两者在识别作品质量层次方面表现出相似性。AI系统评分更加保守，但提供了更为详尽的技术反馈。


<details>
  <summary>Details</summary>
Motivation: 大规模编程课程难以向学生代码提供及时且详细的反馈。

Method: 开发了一种名为Mark My Worlds的本地自动评分系统，该系统通过角色提示来分析提交的作品、批评代码质量并生成教育反馈，同时保持其推理过程透明。研究者在一个191人的工程课程中试用了该系统，并将AI生成的评估结果与人工评分进行了对比。

Result: AI评分与人工评分间未显示出线性相关性（r=-0.177, p=0.124），但两者都表现出类似的左偏分布，表明尽管评分理念不同，它们能够识别出相似的质量层级。AI系统打分更为保守（平均分为59.95分，而人工评分为80.53分），但能产生显著更详细的技术反馈。

Conclusion: Mark My Works系统为大规模编程课程中的学生代码提供了即时且详细的反馈解决方案。虽然AI评分比人工评分要保守得多，但它可以提供更丰富和具体的技术建议，有助于提高教学效果。

Abstract: Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.
  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.

</details>


### [17] [Towards Online Malware Detection using Process Resource Utilization Metrics](https://arxiv.org/abs/2601.10164)
*Themistoklis Diamantopoulos,Dimosthenis Natsos,Andreas L. Symeonidis*

Main category: cs.SE

TL;DR: 本文提出了一种在线学习方法用于动态恶意软件检测，该方法通过整合时间信息并使用行为特征（特别是进程资源利用率指标）来持续更新模型，从而能够适应新出现的威胁，并有效检测零日恶意软件。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和物联网的快速发展，计算资源之间的互联显著增加，为恶意软件的快速传播创造了环境。现有基于机器学习的方法依赖于大规模标记数据集、固定模型训练，并假设训练后的模型能长期有效，但这些方法往往无法检测到随时间演变的新型恶意软件攻击。

Method: 本文提出了一种在线学习方法，该方法通过结合时间信息及行为特征（具体指进程资源利用度量）对模型进行连续更新，以逐步适应新出现的安全威胁。

Result: 与传统的批处理算法相比，所提出的方法在检测零日恶意软件方面表现出了有效性；此外，在数据可用性有限的情况下，该方法也展示了其优越性。

Conclusion: 研究证明了在线学习方法在动态恶意软件检测中的优势，特别是在应对不断进化且难以预测的新威胁时展现出更好的适应性和准确性。

Abstract: The rapid growth of Cloud Computing and Internet of Things (IoT) has significantly increased the interconnection of computational resources, creating an environment where malicious software (malware) can spread rapidly. To address this challenge, researchers are increasingly utilizing Machine Learning approaches to identify malware through behavioral (i.e. dynamic) cues. However, current approaches are limited by their reliance on large labeled datasets, fixed model training, and the assumption that a trained model remains effective over time-disregarding the ever-evolving sophistication of malware. As a result, they often fail to detect evolving malware attacks that adapt over time. This paper proposes an online learning approach for dynamic malware detection, that overcomes these limitations by incorporating temporal information to continuously update its models using behavioral features, specifically process resource utilization metrics. By doing so, the proposed models can incrementally adapt to emerging threats and detect zero-day malware effectively. Upon evaluating our approach against traditional batch algorithms, we find it effective in detecting zero-day malware. Moreover, we demonstrate its efficacy in scenarios with limited data availability, where traditional batch-based approaches often struggle to perform reliably.

</details>


### [18] [Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges](https://arxiv.org/abs/2601.10220)
*Simin Sun,Miroslaw Staron*

Main category: cs.SE

TL;DR: 本文通过与四家公司中的十位资深专家进行定性研究，探讨了生成式AI在嵌入式软件开发中的应用实践和挑战。研究揭示了11种新兴实践和14个相关挑战，这些发现有助于团队重新思考工作流程、角色及工具链，以支持向AI增强型开发的可持续转变。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在软件开发流程中快速普及，正推动着软件工程领域的新变革。然而，对于嵌入式软件工程组织来说，这标志着它们首次尝试将AI整合到安全关键且资源受限的环境中去。面对确定性、可靠性和可追溯性的严格要求，采用生成技术面临着独特的挑战。

Method: 本研究采用了定性研究方法，包括与来自四个不同公司的十名资深专家进行半结构化焦点小组访谈以及结构化的头脑风暴会议。

Result: 研究识别出了与生成式AI工具的编排、负责任治理及可持续采用相关的11种新兴实践和14个挑战。结果显示，嵌入式软件工程团队正在重新考虑其工作流程、角色分配和技术栈，以便能够顺利过渡到更加自主化的流水线和AI增强型开发模式。

Conclusion: 文章结论表明，尽管存在诸多挑战，但通过调整现有工作流程、角色定义及工具使用方式，嵌入式软件工程团队可以实现向利用生成式AI辅助开发的有效转型。

Abstract: A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies.
  In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development.

</details>


### [19] [Evolving with AI: A Longitudinal Analysis of Developer Logs](https://arxiv.org/abs/2601.10258)
*Agnia Sergeyuk,Eric Huang,Dariia Karaeva,Anastasiia Serova,Yaroslav Golubev,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 本研究通过结合800名开发者的两年细粒度遥测数据与62名专业人士的调查，探讨了AI助手在IDE中的长期使用对日常编码实践的影响。结果显示，AI用户虽然生成了更多的代码但也删除了更多，而受访者报告生产力有所提高，并认为其他方面变化不大。


<details>
  <summary>Details</summary>
Motivation: 尽管AI驱动的编码助手正在成为专业IDE的标准配置，但它们对日常开发的持续影响仍不够清晰。以往的研究主要集中在短期使用或自我报告的认知上，关于长期持续使用AI如何重塑实际日常编码实践的问题仍有待解答。

Method: 采用混合方法研究IDE中AI的采纳情况，该研究结合了来自800名开发者为期两年的详细遥测数据以及62位专业人士参与的一项调查。研究分析了工作流程变化的五个维度：生产率、代码质量、代码编辑、代码复用和上下文切换。

Result: 遥测数据显示，AI使用者产生了大量代码的同时也删掉了相当多的部分；而根据调查反馈，这些用户感受到了生产效率上的提升，但在其他维度上感知到的变化很小。

Conclusion: 本研究提供了关于软件工作流程静默重组的经验见解，并对未来设计AI增强工具提出了启示。

Abstract: AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.

</details>


### [20] [Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs](https://arxiv.org/abs/2601.10496)
*Ali Al-Kaswan,Claudio Spiess,Prem Devanbu,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 研究引入了一个暴露意识评估框架，以量化先前接触错误代码与修复代码对模型偏好的影响。结果表明，大多数示例（67%）在训练数据中既没有错误版本也没有修复版本；当只有一个版本存在时，修复版本比错误版本更常见。模型生成过程中，模型更常复制错误行而非修复行，并且接触到错误代码的例子会放大这种倾向，而接触到修复代码的例子则只有边际改善。在似然评分中，最小和最大令牌概率度量始终偏好修复代码，显示出对正确修复的稳定偏差。相反，像基尼系数这样的度量在仅看到错误版本时会反转偏好。这些发现指出了暴露可能扭曲错误修复评估，并强调了LLM在实践中可能传播记忆中的错误的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被用于代码生成和调试，但它们的输出仍然可能包含从训练数据中继承的错误。本研究旨在探究模型是偏好正确的代码还是熟悉但错误的版本，这取决于它在训练期间接触的内容。

Method: 研究人员提出了一个暴露意识评估框架，该框架能够量化先前接触过错误代码与修复后的代码如何影响模型的选择。通过使用ManySStuBs4J基准，并借助Data Portraits技术对Stack-V2语料库进行成员测试来估计每个错误及修复版本是否出现在训练数据中。接着，根据曝光情况对例子进行分层，并通过代码补全任务以及多种基于似然性的评分指标比较模型偏好。

Result: 研究发现，大部分样本(67%)在训练资料里既没有出现过错误版本也没有出现过修正版本; 当仅有一个版本存在于训练集时, 修正版比错误版更常见。在模型生成方面, 模型重现出错行远多于修正行, 并且对于那些只见过错误版本的例子来说这种情况更加明显, 而对于那些见过修正版本的例子则仅有轻微改进。至于似然性评分, 最小和最大标记概率度量无论在哪种情况下都持续偏好修正代码, 这表明了一种朝向正确修复方案的稳定偏向。相对地, 像基尼系数这类度量在仅仅见到错误版本的情况下则会反转其偏好。

Conclusion: 暴露于特定类型的代码可以显著影响模型对于bug修复的评价方式，并揭示出大型语言模型在实际应用中有传播已记住错误的风险。

Abstract: Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 本研究探索了使用推理模型和传统大型语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，通过利用现有的ICD-9代码来预测入院情况，达到了89%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素与患者结果相关联，但在结构化数据中很少被记录下来。最近的研究关注于从临床文本中自动提取这些标记，以补充对患者社会环境了解的诊断系统。虽然大型语言模型在识别句子中的社会健康决定因素标签方面表现出色，但处理大量入院或纵向笔记时存在长距离依赖性的问题。

Method: 本研究采用推理模型和传统的大型语言模型，在MIMIC-III数据集上进行了医院入院多标签的社会健康决定因素ICD-9代码分类。研究还利用了现有的ICD-9代码来进行入院预测。

Result: 该方法在医院入院多标签社会健康决定因素ICD-9代码分类任务上取得了89%的F1分数，并且发现了139个入院案例中缺失的社会健康决定因素代码。

Conclusion: 研究表明，通过结合推理模型和大型语言模型可以有效提高社会健康决定因素的识别准确率，同时提供了可重复实验的代码。

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [22] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: 提出了一种名为FaTRQ的系统，该系统使用分层内存来避免从存储中获取完整向量，通过渐进距离估计器和高效的残差编码方式，结合CXL Type-2设备中的定制加速器，在提高存储效率的同时显著提升了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前的近似最近邻搜索（ANNS）引擎虽然利用预构建索引及压缩后的向量量化表示加快了检索速度，但在第二阶段的精炼过程中仍需从较慢的存储介质如SSD读取全精度向量，这已成为查询延迟的主要瓶颈。对于现代文本和多模态嵌入而言，这个问题尤为突出。

Method: FaTRQ系统采用分级内存架构，并引入了一个渐进距离估计器，能够使用从远端内存流式传输的紧凑残差对粗略分数进行精炼。当确定某个候选者不在前k名之内时，会提前停止精炼过程。为了支持这一机制，提出了分级残差量化方法，将残差编码为三元值并高效地存储于远端内存中。此外，还在CXL Type-2设备上部署了一个自定义加速器以实现低延迟本地精炼。

Result: 实验结果表明，与最先进的GPU ANNS系统相比，FaTRQ在存储效率上提高了2.4倍，吞吐量最高可提升至9倍。

Conclusion: FaTRQ通过消除从慢速存储中读取全精度向量的需求，显著减少了查询延迟，同时提高了存储效率和处理能力，为检索增强生成等应用提供了更加高效的支持方案。

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [23] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出了一种针对基于内容的推荐系统（CRSs）的噪声感知核心集选择框架NaCS，通过子模块优化和渐进式训练模型纠正标签噪声，并通过不确定性量化筛选出高质量的核心集样本，从而在减少训练开销的同时保持了模型质量。实验表明，仅使用1%的训练数据，NaCS就能恢复93-95%的全数据集训练性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高基于内容的推荐系统的效率并减少计算成本和资源需求，同时解决现有核心集选择方法中因用户-项目交互中的普遍噪声导致的问题，特别是当核心集规模极小时。

Method: Noise-aware Coreset Selection (NaCS)框架结合了基于训练梯度的子模块优化来构建核心集、利用逐步训练的模型修正噪声标签以及通过不确定性量化去除低置信度样本的方法。

Result: NaCS能够生成更高质量的核心集，相比现有的核心集选择技术，在效率上有所提升；仅需1%的训练数据即可达到接近于完整数据集训练时93-95%的表现。

Conclusion: NaCS为基于内容的推荐系统提供了一个有效的解决方案，能够在极大程度上降低训练所需的数据量而不牺牲模型的质量，对于改善推荐系统的实际应用具有重要意义。

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [24] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架TimeSAE，结合稀疏自动编码器和因果关系来解释时间序列数据中的黑盒模型预测。通过综合评估，TimeSAE相比现有方法提供了更准确且稳健的解释。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒模型和预训练模型在时间序列应用中的普及，在高风险领域理解并解释它们的预测变得尤为重要，特别是在需要可解释性和信任的情况下。然而，现有的大多数解释方法仅限于分布内解释，并不具有泛化到训练支持之外的能力，这要求学习具备泛化的性能。

Method: 本文基于稀疏自动编码器的概念，引入了TimeSAE框架，用于解释时间序列数据中的黑盒模型。

Result: 通过对合成和真实世界时间序列数据集上的广泛评估显示，与领先基准相比，TimeSAE能够提供更加忠实和稳健的解释。这些结果得到了定量指标和定性见解的支持。

Conclusion: 研究证明了TimeSAE作为一种新型的时间序列数据黑盒模型解释工具的有效性，它能够在面对分布偏移时保持较高的敏感度和有效性。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [25] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 该论文建立了广义线性模型类的eluder维度的下界，表明基于标准eluder维度的分析无法导致一阶遗憾界限。为了解决这个问题，作者引入了一种针对eluder维度的局部化方法；这种分析不仅恢复并改进了伯努利多臂老虎机的经典结果，还首次为具有有界累积收益的有限时间强化学习任务提供了一阶界限。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在解决当前基于eluder维度分析在广义线性模型中无法达到一阶遗憾界限的问题。

Method: 提出了一个针对eluder维度的新局部化方法，并通过该方法对广义线性模型进行了重新评估。

Result: 所提出的方法不仅复现并且改进了关于伯努利多臂老虎机问题的传统结论，而且对于具有有界累积回报的有限期强化学习任务提供了首个真正的一阶界限。

Conclusion: 本文介绍的新方法为解决广义线性模型中遇到的挑战开辟了道路，特别是在提高强化学习算法性能方面。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [26] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: 本文提出了在先验不匹配情况下，即去噪器训练所用的数据分布与当前推理任务所需数据分布不同的条件下，插件式近端梯度下降（PnP-PGD）的新收敛理论。这是已知的首次针对先验不匹配条件下的PnP-PGD收敛性证明，并且移除了对若干限制性强且难以验证假设的需求。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决现有PnP算法在先验不匹配情况下的收敛性问题，特别是当用于实际推理任务中的去噪器是在不同数据分布上训练而成时。通过提出新的收敛理论来放宽先前理论结果中的一些过于严格和无法验证的前提条件。

Method: 采用了一种新颖的方法论来分析PnP-PGD在先验不匹配情形下的行为特性及收敛性质，具体包括但不限于：1) 提出更宽松、更具实用性的假设条件；2) 为PnP-PGD开发了新的数学框架以支持其在给定情境下的应用。

Result: 成功地建立了对于PnP-PGD方法在先验不匹配条件下运行时的收敛性理论。这不仅标志着在该领域内的一次重要进步，而且也为未来相关研究奠定了坚实的基础。

Conclusion: 本研究为PnP-PGD在处理先验不匹配问题时提供了坚实的理论基础，消除了之前理论中一些不必要的限制条件，为实际应用开辟了新路径。

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [27] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 本研究提出了一种通用的因果公平模型训练流程，旨在解决医疗环境中直接和间接偏见的问题。通过将结构化的公平模型映射到观察性医疗环境，该流程能够明确考虑具体的医疗背景与差异，定义一个目标'公平'模型。此外，还探讨了在保持准确性的前提下如何平衡公平性，并展示了如何利用未受公平约束训练的基础模型生成具有因果公平性的下游预测结果。


<details>
  <summary>Details</summary>
Motivation: 在医疗环境中部署机器学习模型时，确保这些模型不会复制或加剧现有的医疗偏见至关重要。本文关注路径特定的因果公平性，旨在更好地理解社会和医疗背景下偏见的发生（例如，直接由临床医生或模型引起的歧视与由于医疗系统访问不均等导致的偏见），并描述这些偏见可能如何出现在学习模型中。

Method: 研究人员开发了一个适用于观察性健康数据的通用管道来训练因果公平模型。此管道特别考虑到了具体的医疗情境及其中存在的差异性，以定义理想的“公平”模型为目标。它不仅扩展了对于‘公平-准确性’权衡特征的理解，而且展示了一个未经公平约束训练的基础模型如何被用来为存在已知社会与医学差异的任务生成因果上公平的下游预测。

Result: 研究填补了两个主要空白：一是通过对直接与间接偏见来源的区分，以及在广泛认知偏见的情境下同时呈现这些公平考量与准确性考量，深化了对‘公平-准确性’权衡的理解；二是证明了可以使用没有公平限制条件下训练的基础模型来产生针对已知社会和医学差异任务的因果公平下游预测。

Conclusion: 这项工作提出了一种模型无关的管道，用于训练解决直接和间接形式医疗偏见的因果公平机器学习模型。

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [28] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 本文提出了一种结合GPTQ量化、低秩适应（LoRA）以及专门的数据蒸馏过程的集成框架，旨在显著减小大型语言模型的大小和复杂性，同时保持或提高任务特定性能。通过这一流程，实现了高达2倍的内存压缩，并在特定任务上实现了高效的推理。实证结果表明，与仅使用GPTQ量化相比，该方法在标准LLM基准测试中表现出色，特别是Muon优化器明显增强了量化过程中微调模型对准确率下降的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能够实现先进的自然语言处理能力，但在资源受限的边缘设备上部署时面临高计算、内存及能耗需求的挑战。因此，需要解决获取任务特定数据、为性能进行微调以及压缩模型以加速推理并减少资源需求这三个关键问题。

Method: 提出了一个综合框架，包括基于GPTQ的量化、低秩适应（LoRa）、以及一种专门的数据蒸馏过程，以大幅降低模型大小和复杂度的同时保持甚至增强任务特定表现。此框架利用了数据蒸馏、通过Kullback-Leibler散度的知识蒸馏、贝叶斯超参数优化以及Muon优化器来达成目标。

Result: 实验结果表明，相比于单独使用GPTQ量化，所提出的方案在标准大型语言模型基准测试中展现出了更好的性能，特别是在量化过程中，Muon优化器显著提高了微调后模型对抗精度衰减的能力。此外，还实现了最高达2倍的内存压缩效果，例如将6GB的模型缩减至3GB。

Conclusion: 本研究开发了一种有效的集成框架，用于优化大型语言模型的部署，使其能够在资源有限的边缘设备上运行。通过结合多种技术手段，不仅大大减少了模型的尺寸和复杂性，而且确保了模型在特定任务上的高效执行与准确性。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [29] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: 提出了一种基于插值的框架，用于在细粒度或连续设置中优化lp范数度量差分隐私（mDP），通过在锚点优化扰动分布，并通过log-convex组合在非锚点位置进行插值，从而有效保持mDP。此外，还探讨了对扰动分布和跨维度隐私预算分配的联合优化。实验证明，该方法在保证严格隐私的同时，提供了与现有机制相比具有竞争力的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于优化的方法虽然能够有效地减少粗粒度领域中的效用损失，但在细粒度或连续设置下优化mDP仍然存在挑战，主要是因为构建密集扰动矩阵及满足逐点约束条件所带来的计算成本。为了解决这一问题，本文提出了新的解决方案。

Method: 采用了一种基于插值的方法来优化lp范数mDP。首先，在一组稀疏选定的锚点上优化扰动分布；然后，通过log-convex组合的方式在非锚点处进行分布插值，同时确保mDP得以保持。对于高维空间中由于简单插值导致的隐私违规问题，将插值过程分解为一系列一维步骤，并推导出一个修正公式以设计上强制执行lp范数mDP。进一步地，研究了扰动分布与跨维度隐私预算分配之间的联合优化。

Result: 实验结果表明，在真实世界的位置数据集上，所提出的方法不仅提供了严格的隐私保障，而且在细粒度领域内表现出优于基线机制的竞争性实用性。

Conclusion: 本文介绍了一种新颖的基于插值的框架，用于解决细粒度或连续域中lp范数mDP优化难题。通过精心选择锚点、利用log-convex组合插值以及采取分步一维插值策略，成功实现了既定目标。此外，提出的联合优化方案也为提高隐私保护效率提供了一条新途径。

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [30] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: 本文提出了一种基于优化的连续时间表示方法——运动学标记化，该方法能够从噪声测量中重建显式样条，并将局部样条系数（位置、速度、加速度、急动度）进行标记化。在金融时间序列数据上应用时，与使用离散基线相比，这种方法在面对诱导放弃损失的情况下能维持校准的非平凡动作分布和稳定的策略。


<details>
  <summary>Details</summary>
Motivation: 由于许多现实世界的信号是通过噪声采样观察到的连续过程，而变换器是为离舍符号设计的，在低信噪比情况下，离散符号化可能表现脆弱，特别是在下游目标施加不对称惩罚时。因此，作者引入了运动学标记化来解决这个问题。

Method: 通过开发一种名为Kinematic Tokenization的新技术，它是一种基于优化的连续时间表示法，能够从噪声测量值中重构出一个明确的样条，并对局部样条系数（如位置、速度、加速度和急动度）进行编码。

Result: 在包含资产价格和交易量概况的多资产日常股票测试平台上，采用风险厌恶型的不对称分类目标作为学习能力的压力测试。结果显示，多种离散基线崩溃为吸收现金策略（清算均衡），而连续样条标记则支持校准的非平凡动作分布和稳定政策。

Conclusion: 实验结果表明，显式的连续时间标记可以提高在噪音时间序列下选择性决策策略的学习能力和校准效果，尤其是在那些鼓励采取放弃策略的损失函数条件下。

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [31] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 本文指出机器学习价值链由于经济数据处理不平等而结构上不可持续，并通过分析73个公开数据交易案例，揭示了大部分价值流向聚合者而非数据生成者的问题。提出了三个结构性问题：缺失的来源、不对称的议价能力和非动态定价，并提出了一种公平的数据-价值交换框架（EDVEX）来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 作者认为当前的机器学习价值链存在结构性问题，即随着数据及其衍生物成为经济资产，现有的从输入到模型权重再到合成输出的数据循环过程中的每个阶段都精炼了技术信号但剥夺了数据生产者的经济权益。这种状况不仅是一个经济福利问题，也威胁到了支持当前学习算法反馈循环的稳定性。

Method: 通过对73个公开数据交易进行分析，展示了大部分价值如何主要积累给聚合者而不是创造者，并且强调了交易条款普遍缺乏透明度的问题。此外，还确定了导致这种不平等现象存在的三个关键结构性缺陷：缺少出处、谈判能力不对等以及价格机制僵化。基于此，提出了一个名为“公平数据-值交换”(EDVEX) 的框架作为解决方案。

Result: 研究发现，在已记录的数据交易中，创作者的版税几乎可以忽略不计，同时交易条件普遍存在不透明性。这表明现有体系下大部分收益都被中间商所获得。为了解决上述提到的问题，文章提出了一个新的框架——公平数据-价值交换（EDVEX），旨在创建一个能够让所有参与者受益的基本市场环境。

Conclusion: 为了确保机器学习生态系统的长期可持续发展，需要重新考虑数据的价值分配方式。通过引入EDVEX框架，可以促进更加公平合理的数据交易实践，从而惠及所有利益相关方。此外，论文还指出了未来的研究方向，鼓励社区成员为此类议题做出具体贡献。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [32] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: 本研究探索了将专门的时间序列编码器与冻结的大型语言模型（LLM）主干结合的混合架构在时间序列分类问题中的应用，发现Inception模型是唯一一种当与LLM主干集成时能够持续提供正面性能增益的编码器架构。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）因其强大的推理和泛化能力而被重新用于时间序列分类（TSC），目前的研究主要集中在将时间序列数据明确映射到文本领域的对齐策略上；然而，对于时间序列编码器架构的选择仍缺乏深入探讨。

Method: 本工作进行了一项探索性研究，评估了包括Inception、卷积、残差、基于变压器以及多层感知机在内的多种编码器家族，这些编码器与冻结的大型语言模型（LLM）主干相结合形成混合架构。

Result: 实验结果表明，在所有测试的编码器中，只有Inception模型在与LLM主干集成时能够一致地产生积极的性能提升。

Conclusion: 这项研究表明，选择合适的时间序列编码器对于基于LLM的混合架构至关重要，并指出基于Inception的模型是未来LLM驱动的时间序列学习的一个有前途的方向。

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [33] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 本文提出了概率测度空间中的上下文操作学习方法，用于最优传输问题。目标是仅使用来自每个分布的少量样本作为提示，学习一个解算子将一对分布映射到OT映射，而无需在推理时进行梯度更新。研究中参数化了解算子，并在非参数和参数设置下开发了比例定律理论。实验结果验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决最优传输（OT）问题，作者提出了一种新的方法——上下文操作学习法。这种方法旨在通过少量样本示例来学习如何将一对分布映射至OT映射，且在实际应用中不需要执行梯度更新。此方法尝试克服传统方法需要大量训练数据以及实时计算成本高的局限性。

Method: 研究者首先定义了一个解算子，并将其参数化。接着，在非参数场景下，当任务集中在源-目标对的一个低内在维数流形上时，他们建立了泛化界限，这些界限量化了上下文准确性与提示大小、内在任务维度及模型容量之间的关系。而在参数场景下，如高斯族，他们提供了一种显式架构，能够在上下文中恢复精确的OT映射，并给出了有限样本超额风险界限。

Result: 通过对合成传输和生成模型基准测试的数值实验表明，所提出的框架能够有效工作，验证了其理论分析的正确性和实用性。

Conclusion: 本文介绍的方法为最优传输问题提供了新视角，通过上下文学习方式减少了对于大量训练数据的需求，并且在某些情况下能够准确地恢复OT映射。这为解决相关问题开辟了新的路径。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [34] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出了一种混合变压器架构，通过将离散中间层替换为连续深度的神经常微分方程(ODE)块，实现了通过学习的控制信号在推理时对生成属性的控制。该架构证明了其在梯度流稳定性、语义控制、连续插值和效率基准测试中的有效性，并展示了自适应ODE求解器揭示了学习动态中的几何结构，控制信号可将向量场划分为具有不同曲率特性的动力学状态。


<details>
  <summary>Details</summary>
Motivation: 为了克服标准变换器中固定离散层处理表示的限制，论文旨在引入一种新的方法，使得深度可以被视为由学习到的向量场管理的连续变量，从而允许通过注入低维控制信号来实现在推理时对生成属性的控制。

Method: 研究者设计了一种混合变换器架构，其中包含了连续深度的神经常微分方程(ODE)模块。该模型的关键在于利用一个学习到的向量场$F_θ(H, τ, u)$来处理信息，其中$u$是通过显式连接方式注入的一个低维控制信号。此外，还使用了伴随方法以确保训练过程中内存消耗与积分深度无关。

Result: 实验结果表明，所提出的架构在四个方面表现良好：1）梯度流稳定，没有发生梯度爆炸或消失事件；2）语义指导上正/负情绪控制准确率达到98%/88%；3）固定求解器与自适应求解器之间轨迹发散极小，仅为0.068%；4）与标准离散基线相比，延迟相当。进一步分析显示，自适应ODE求解器能够揭示学习动态中的几何结构。

Conclusion: 连续深度动态结合学习控制信号提供了一种可行且高效的可控语言生成机制。这不仅提高了生成过程中的灵活性，还保证了计算效率，为未来研究开辟了新方向。

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [35] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 该论文研究了XGBoost模型在点击率预测中的时间聚合特征，发现使用跟踪窗口规格相较于仅使用目标编码能显著提高ROC AUC和PR AUC指标。


<details>
  <summary>Details</summary>
Motivation: 探讨不同时间聚合策略对XGBoost模型点击率预测性能的影响，特别是针对Avazu点击率预测数据集。

Method: 通过严格的跨时间分割以及不前瞻特征约束来比较强的时间感知目标编码基线与增加了实体历史时间聚合特性的模型，并尝试了几种不同的窗口设计。

Result: 结果显示，在两个滚动尾部折叠上，跟踪窗口规范相比单独的目标编码提高了大约0.0066至0.0082的ROC AUC和大约0.0084至0.0094的PR AUC。事件计数窗口是唯一一个在所有时间聚合设计方案中能够一致优于跟踪窗口的方法，但其提升幅度较小。

Conclusion: 对于给定的数据集和协议，建议采用跟踪窗口作为实际默认设置；当边际ROC AUC增益重要时，可以考虑添加事件计数窗口。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [36] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的集成学习框架BPE，它通过构建每个模型的内在'行为特征'来确定集成权重，而非依赖于不同模型之间的差异性。实验表明，该方法在预测准确性、计算效率和存储资源利用方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统的静态集成学习方法（如堆叠）通常将每个基学习器视为一个整体分配权重，忽略了单个模型在实例空间不同区域表现出的能力差异。而动态集成选择虽然试图解决这一问题，但仍然主要依靠不同模型之间的差异作为集成的基础，忽视了模型本身的内在特性，并且需要大量验证集来估计能力。为了解决这些问题，作者提出了行为特征集成(BPE)框架。

Method: BPE框架的核心在于为每个模型构建一个固有的'行为特征'，并根据特定测试实例下模型响应与其既定行为特征之间的偏差来导出集成权重。

Result: 综合实验结果表明，基于BPE框架的算法在合成数据集和真实世界数据集上都取得了显著改进，不仅提高了预测准确性，还在多种场景下提升了计算效率和存储资源利用率。

Conclusion: BPE提供了一种新颖的方法来提升集成学习的表现，通过考虑模型自身的特性而不是仅仅依赖于模型间的差异，从而在保持高效的同时提高了性能。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [37] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 本研究提出了一种新的增强型上下文学习（ICL）框架，该框架通过在提示中加入少量标记示例和大量未标记输入，利用链式思维（CoT）提示让多层Transformer能够模拟期望最大化（EM）算法，从而有效从未标记数据中提取有用信息，提高ICL的准确性。实验结果表明，这种增强型ICL框架相比传统的少样本ICL表现更好。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)具有令人印象深刻的上下文学习能力，但其预测质量受到可放入提示中的少数昂贵标记演示的限制。同时，存在大量的、不断增长的与ICL任务密切相关的未标记数据。如何利用这些未标记数据来证明可以提升ICCL的表现成为了一个新兴的基本问题。

Method: 提出了一个增强型ICL框架，在此框架下，提示不仅包含一小部分标记示例，还包含一大块未标记输入。研究人员专注于多类线性分类设置，并展示了通过使用链式思维(CoT)提示方法，一个多层变压器能够有效地模仿期望最大化(EM)算法，从标记和未标记的数据中隐含地提取有用信息。此外，他们还表明这样的变压器可以通过教师强制训练，参数以线性速率收敛到所需的解决方案。

Result: 实验证明了增强型ICL框架比传统的少量样本ICL更有一致性的优越表现，为理论发现提供了经验支持。

Conclusion: 这项工作首次对未标记数据影响变压器ICL性能进行了理论研究，表明通过结合未标记数据，可以显著改善ICL的准确性。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [38] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 本研究提出了一种基于图像的深度学习模型（HuSHeM）来评估精子形态质量，并与结合了系统性炎症反应指数的世界卫生组织标准（WHO(+SIRI)）进行了对比。结果显示，HuSHeM在区分性能、校准分析以及临床实用性方面均优于WHO(+SIRI)，表明基于图像的深度学习方法可能提供比传统规则基础和炎症增强标准更高的预测可靠性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 精子形态质量评估是男性生育力评价中的一个重要但主观的部分，常受限于观察者间的变化性和资源限制。为了提高这一过程的客观性和可重复性，研究提出了一个生物医学人工智能框架，旨在比较基于图像的深度学习模型与现有临床标准。

Method: 开发了一个名为HuSHeM的深度学习模型，该模型使用高分辨率精子形态图像进行训练，并在一个独立的临床队列上进行评估。通过区分度、校准度及临床实用性分析来评估模型的表现。

Result: HuSHeM模型展示了较高的区分能力，ROC曲线下面积更大且置信区间较窄；精度-召回率分析显示，在处理类别不平衡时表现更优；校准分析表明预测概率与实际结果之间有较好的一致性；决策曲线分析则指出在临床上相关阈值概率下具有更大的净临床效益。

Conclusion: 基于图像的深度学习方法为精子形态学的质量评估提供了改进的预测可靠性和临床实用性，可以作为生育筛查和转诊工作流程中的决策支持工具，但不打算取代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [39] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Sparse-RL的方法，旨在通过稀疏采样减少长时序展开中存储键值缓存所需的大内存开销问题，同时引入了稀疏感知拒绝采样和基于重要性的重加权方法来解决由于压缩引起的信息丢失导致的策略不匹配问题。实验结果表明，与密集基线相比，该方法减少了展开开销并保持了性能，并且在稀疏推理部署期间提高了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）对于激发大型语言模型（LLMs）中的复杂推理能力变得至关重要，但长时序展开过程中存储键值（KV）缓存的巨大内存开销成为了关键瓶颈，限制了在有限硬件上的高效训练。尽管现有的KV压缩技术可以缓解推理阶段的问题，但直接应用于RL训练会导致严重的策略不匹配，从而引发性能崩溃。

Method: 提出了Sparse-RL方法，它通过稀疏回放实现稳定的RL训练。为了解决由压缩引起的信息损失所导致的离策略偏差问题，Sparse-RL结合了稀疏感知拒绝采样和基于重要性的重新加权两种机制。

Result: 实验结果显示，相比于密集型基准方案，Sparse-RL能够在降低展开开销的同时维持良好的表现。此外，Sparse-RL还天然地实现了稀疏意识训练，这极大地增强了模型在进行稀疏推理部署时的鲁棒性。

Conclusion: Sparse-RL提供了一种有效途径以减轻长序列展开过程中的内存负担，同时保证了RL训练的稳定性及最终模型的质量。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [40] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架LeMoF，用于选择性地整合每个模态内的层级引导表示，从而在异构临床环境中实现预测稳定性和区分能力之间的平衡性能。实验表明，LeMoF在不同编码器配置下均优于现有的多模态融合技术。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态临床预测方法依赖于静态模态整合方案和简单的融合策略，无法充分利用特定模态的表示。为了解决这个问题，作者提出了Level-guided Modal Fusion (LeMoF) 框架，旨在通过选择性集成各模态内不同层次的表示来改善这一状况。

Method: LeMoF框架通过对每个模态的不同编码层提取的表示（称为“层级”）进行有选择性的整合，并明确分离及学习全局模态级别的预测与层级特定的区分表示。

Result: 实验使用ICU数据对住院时长进行了预测，结果显示LeMoF在各种编码器配置下始终优于现有最先进的多模态融合技术。此外，还证实了层级整合是实现跨多种临床条件稳健预测性能的关键因素。

Conclusion: 通过引入LeMoF框架，研究成功展示了在保持预测稳定性的同时增强区分能力的可能性，为处理电子健康记录(EHR)和生物信号等异质数据提供了新思路。

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [41] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: 介绍了一种名为METAL的轻量级对齐方法，它仅使用英文文本学习少数线性层来将多语言文本嵌入映射到多模态空间。该方法在英语中表现良好，并且在未见过的语言上实现了强大的零样本迁移性能。此外，METAL还适用于音频-文本检索和跨语言文本-图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案过于依赖机器翻译，而多语言文本建模的进步却未能充分利用。因此，需要一种能够有效利用多语言资源的方法，以提高除英语外其他语言的多模态模型性能。

Method: 提出了一种名为METAL的轻量级对齐方法，通过仅使用英文文本来训练少量线性层，从而将多语言文本嵌入映射到一个多模态空间内。

Result: METAL在英语上的表现达到了基线水平（94.9%召回率@10），并且在XTD文本-图像检索任务中的11种语言（其中10种为未见语言）上平均实现了89.5%的召回率@10。此外，METAL还能泛化至音频-文本检索及跨语言文本-图像生成任务。

Conclusion: METAL是一种简单但有效的方案，能够在无需大量额外资源的情况下改善多语言环境下的多模态模型性能。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [42] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种树结构的多专家大语言模型框架Tree-Query，用于减少成对因果发现到一系列关于后门路径、（非）依赖性、潜在混淆和因果方向的查询，从而提供具有鲁棒性意识的信心分数的可解释判断。


<details>
  <summary>Details</summary>
Motivation: 传统的基于约束的方法（如PC、FCI）存在错误传播的问题，而最近基于大语言模型的因果预言机往往表现为不透明且缺乏信心的黑盒子。因此，需要一种能够提高因果发现准确性并给出可解释判断的新方法。

Method: 提出了Tree-Query框架，通过一系列简短的查询来降低成对因果发现的难度，这些查询涉及后门路径、（非）依赖性、潜在混淆变量以及因果方向，并据此生成带有鲁棒性评估的信心分数。

Result: 在从Mooij等人及UCI因果图派生的数据无关基准上，Tree-Query在结构度量方面优于直接使用大语言模型的基础方法；一个饮食-体重案例研究展示了其在筛选混淆因素及得出稳定高信心因果结论方面的应用。

Conclusion: Tree-Query为从大语言模型中获取数据无关的因果先验提供了原则性的途径，这可以补充下游的数据驱动因果发现工作。

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [43] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型微调过程中安全性和实用性之间的矛盾，并提出了一种新的方法SPF来解决这一问题。该方法通过移除与安全子空间冲突的梯度分量，在理论上保证了实用性的收敛并限制了安全性漂移，实验结果表明其在保持下游任务性能的同时恢复了几乎所有的预训练安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在针对具体任务进行微调时，可能会导致安全一致性显著下降，例如更容易受到越狱攻击的影响。尽管人们尝试在微调阶段加强防御措施，但仍然难以平衡安全性和实用性之间的关系：强调安全性会牺牲任务表现，而侧重于提升效率通常需要深度微调，这又不可避免地造成安全性的大幅下滑。

Method: 研究者们通过系统性实证分析揭示了安全一致性和实用性导向梯度之间的几何交互特性，包括发现安全梯度位于低秩子空间内、这些子空间经常呈负相关关系以及主要的安全方向可以通过单个样本有效估计等关键洞察。基于上述新颖见解，提出了名为安全保护微调（SPF）的新方法，该方法能够显式去除与低秩安全子空间相冲突的梯度成分。

Result: 理论证明显示SPF可以确保实用性收敛同时控制住安全性偏移；实际测试中，即使是在对抗性微调场景下，SPF也能够持续保持下游任务的表现，并恢复几乎全部预先训练好的安全一致性。此外，它还展示了对于深层微调和动态越狱攻击的强大抵抗力。

Conclusion: 本研究提供了关于如何始终如一地保持LLM微调期间的安全一致性的新机制理解和实用指导。

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [44] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: 提出了一种名为LOOKAT的方法，通过应用乘积量化和非对称距离计算来压缩KV缓存，实现了在保持高输出保真度的同时显著减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 当前的量化方法虽然可以压缩存储空间，但不能减少带宽使用，因为注意力计算需要先将INT4/INT8格式的键反量化为FP16格式才能进行。基于此问题，研究者们发现注意力评分与内积相似性搜索在数学上是等价的，并且可以利用向量数据库中的某些压缩技术来更好地压缩KV缓存。

Method: LOOKAT方法结合了乘积量化（product quantization）和非对称距离计算（asymmetric distance computation），通过对键向量分解成子空间、学习码本以及通过查找表计算注意力表格来实现。这种方法将注意力机制从内存限制转变为计算限制。

Result: 实验结果表明，在GPT-2模型上测试时，LOOKAT能够在95.7%的输出保真度下达到64倍的压缩率，在95.0%的保真度下达到32倍的压缩率。此外，该方法无需更改架构或重新训练模型即可维持大于0.95的等级相关性。理论分析也验证了随着子空间数量增加，等级相关性的下降趋势符合O(d_k/mK)规律。

Conclusion: LOOKAT提供了一种有效途径来大幅降低大型语言模型部署于边缘设备时所需的KV缓存大小，同时几乎不影响模型性能。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [45] [CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling](https://arxiv.org/abs/2601.10176)
*Mingyu Zhao,Haoran Bai,Yu Tian,Bing Zhu,Hengliang Luo*

Main category: cs.LG

TL;DR: 提出了一种新的统一框架CC-OR-Net，通过结构分解实现更稳健的排序和回归解耦，从而在客户终身价值(LTV)预测中达到了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LTV预测方法要么依赖于严格的统计假设，要么试图通过有序分桶来分离排名和回归，但它们往往通过基于损失的约束而不是内在架构设计来强制顺序性，无法平衡全局准确性和高价值用户的精确度。

Method: 提出了一个名为CC-OR-Net的新框架，它结合了三个专门组件：用于稳健排序的结构序数分解模块、用于细粒度回归的桶内残差模块以及针对顶级用户精度的目标高价值增强模块。

Result: 在超过3亿用户的真实数据集上评估，CC-OR-Net在所有关键业务指标上都实现了优于现有最先进方法的表现，为LTV预测提供了一个全面且具有商业价值的解决方案。

Conclusion: CC-OR-Net通过其独特的结构化方法，在处理零膨胀和长尾分布的数据时表现出色，特别是在保持对高价值用户精准预测的同时还能保证整体准确性方面。

Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.

</details>


### [46] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 本文提出了一种基于图正则化的主成分分析方法（GR-PCA），该方法能够处理非球形噪声情况下的高维数据，通过学习稀疏精度图并将载荷偏向于对应图拉普拉斯算子的低频傅里叶模式来纳入数据特征的依赖结构。


<details>
  <summary>Details</summary>
Motivation: 传统的主成分分析（PCA）在噪声为各向同性的情况下是最优的，但当变量间存在依赖关系且噪声不是独立同分布时，即噪声协方差不是球形时，PCA可能不是最优解。因此，引入了图正则化PCA（GR-PCA）来解决这个问题。

Method: 通过引入一个基于图的学习框架，其中包含学习稀疏精度图以及将主成分载荷偏向图拉普拉斯算子的低频部分。这种做法有助于抑制高频信号而保留与图结构一致的低频信号，从而得到与条件关系对齐的可解释主成分。

Result: GR-PCA在合成数据集上进行了评估，这些数据集覆盖了不同的图拓扑、信噪比和稀疏度水平。相比主流替代方案，它能够更好地集中方差在预期的支持上，产生更低图拉普拉斯能量的载荷，并保持样本外重建方面的竞争力。特别是当存在高频信号时，图拉普拉斯惩罚防止过拟合，虽然降低了重建准确性但提高了结构保真度。

Conclusion: 对于含有图相关高频信号的数据，GR-PCA相较于传统PCA具有明显优势；而对于几乎旋转不变的信号，则PCA仍然具有竞争力。该方法实现简单、模块化设计并且可扩展性强，提供了一条改进结构忠实度而不牺牲预测性能的实际途径。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [47] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为过程奖励学习（PRL）的新方法，旨在通过将强化学习目标分解为中间步骤，并为这些步骤分配严格的过程奖励来改进大型语言模型（LLMs）的推理能力。PRL不仅提高了LLMs平均推理性能，还通过提高pass@n指标扩展了推理边界。


<details>
  <summary>Details</summary>
Motivation: 当前改进大型语言模型（LLMs）推理能力的工作主要依赖于轨迹级别的结果奖励，缺乏对推理过程中细粒度的监督；同时现有训练框架虽然尝试结合过程信号优化LLMs，但往往需要额外繁琐步骤如蒙特卡洛树搜索(MCTS)、训练独立奖励模型等，降低了训练效率且设计背后直觉缺乏严格的理论支持。

Method: 提出了过程奖励学习（PRL），它基于熵正则化强化学习目标在中间步骤上的分解，并引入了可以相应分配给模型的严谨过程奖励。从理论上推导出PRL公式本质上等同于最大化奖励加上策略模型与参考模型之间的KL散度惩罚项。

Result: 实验结果显示，PRL不仅能提升LLMs在平均@n衡量下的推理能力表现，还通过改善pass@n指标扩大了推理范围。广泛的实验验证了PRL的有效性和可推广性。

Conclusion: PRL作为一种新的训练方法，通过提供更精细的过程监督信号，在提高大型语言模型推理能力和扩展其推理边界方面表现出色。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [48] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: 本文分析了差分私有随机梯度下降(DP-SGD)在$f$-差分隐私框架下的性能，并研究了单个epoch内进行$M$次梯度更新的洗牌采样。推导出可实现权衡曲线的一个显式次优上界，这导致了一个关于分离$κ$的几何下界。为了保证有意义的隐私，$κ$需要很小，但小的$κ$又会对高斯噪声乘数$σ$施加严格的下限，从而直接限制了可达到的实用性。研究表明，在标准最坏情况对抗模型下，DP-SGD不能同时实现强隐私和高实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管差分私有随机梯度下降（DP-SGD）是隐私训练的主要范例，但对于其在最坏情况对抗性隐私定义下的基本局限性仍缺乏深入理解。本研究旨在通过$f$-差分隐私框架来探索这些问题，特别是关注于当使用洗牌采样时，如何平衡隐私与实用性之间的关系。

Method: 采用$f$-差分隐私框架，通过对假设检验中的权衡曲线进行分析，研究者们得出了一个关于可实现权衡曲线的次优上界。基于此结果，他们进一步确定了机制权衡曲线与理想随机猜测线之间最大距离（即分离$κ$）的一个几何下界。此外，还证明了为了确保有意义的隐私水平，必须对高斯噪声乘数$σ$设定严格的下限，而这直接影响到了算法的实用性。

Result: 研究发现，对于洗牌DP-SGD而言，在标准最坏情况对抗模型下，若要保持较小的$κ$值，则必须满足$σ ≥ \frac{1}{\sqrt{2\ln M}}$或$κ ≥ \frac{1}{\sqrt{8}}(1-\frac{1}{\sqrt{4π\ln M}})$。这意味着无法同时获得强隐私保护和高性能。实验验证了这一理论边界所暗示的噪声水平会导致实际训练设置下的准确率显著下降。

Conclusion: 这项工作揭示了在标准最坏情况对抗假设下，DP-SGD面临着难以克服的瓶颈：即无法同时实现强大的隐私保护和高效率。即使随着梯度更新次数$M$趋于无穷大，所需噪声量减少的速度也非常缓慢；因此，在实践中，为了维持合理的隐私水平，往往不得不牺牲一定的模型性能。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [49] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: 本文通过分析梯度与Hessian矩阵主特征向量之间的角度来衡量尖锐度，提出了一种新的方法X-SAM，该方法通过对沿主导特征向量的正交分解来修正梯度，从而更直接有效地对Hessian的最大特征值进行正则化。实验结果验证了X-SAM在收敛性和泛化性能上的优势。


<details>
  <summary>Details</summary>
Motivation: Sharpness-Aware Minimization (SAM)虽然旨在通过最小化模型参数小邻域内的最坏情况扰动损失来提高泛化能力，但在实际训练过程中，其优化行为并不总是符合理论预期。为了解决这个问题，作者从谱和几何的角度出发，寻找一种能够更加有效执行尖锐度正则化的方法。

Method: 作者提出了一个称为X-SAM的新方法，该方法利用梯度与Hessian矩阵主特征向量之间的夹角作为尖锐度的度量，并通过沿着顶部特征向量的正交分解来调整梯度方向，以此达到更直接且高效地调节Hessian最大特征值的目的。

Result: 理论分析表明，当梯度与Hessian主特征向量间的角度小于等于90度时，SAM的尖锐度正则化效果可能会减弱；而X-SAM能够在这样的情况下仍然保持有效的正则化作用。此外，广泛的实验证明了X-SAM不仅具有良好的收敛性还表现出优越的泛化性能。

Conclusion: 通过引入X-SAM，本文成功改进了传统的SAM方法，在理论上证明了新方法的有效性并通过实验进一步确认了其实用价值。

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [50] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出了一种不需要运行到故障标签的涡轮风扇发动机无监督健康监测框架，通过回归归一化去除操作条件影响，并使用LSTM自编码器仅在每个轨迹的健康部分上进行训练，基于持续重建误差和自适应阈值实现实时警报，展示出高召回率和低误报率。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需运行到故障标签就能实现对涡轮风扇发动机健康状况监测的方法，以快速部署、适用于不同机队，并作为剩余使用寿命模型的补充预警层。

Method: 首先，采用基于回归的归一化方法去除NASA CMAPSS传感器流中的操作条件效应；接着，在每个轨迹的健康部分上训练一个长短期记忆（LSTM）自编码器；最后，利用自适应数据驱动阈值估计持续重构误差来触发实时警报。

Result: 基准测试结果表明该方法在多种运行模式下具有较高的召回率和较低的误报率，证明了其可以快速部署、扩展到不同的机队中，并且能够作为剩余使用寿命模型的一个互补性早期预警层次。

Conclusion: 本研究提出的无监督健康监测框架对于涡轮风扇发动机来说是一种有效的方法，能够在不依赖于运行至故障标签的情况下提供可靠的健康状态监控，同时表现出良好的泛化能力和实用性。

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [51] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: 本文研究了一个大型语言模型服务器处理N种不同类型任务查询的问题，通过分配固定数量的思考令牌来平衡准确性和延迟。在FIFO服务规则下，系统表现为M/G/1队列，并且建立了优化问题以最大化加权平均准确度目标同时考虑平均系统时间和令牌预算限制等条件。提出了严格的凹函数性质保证最优解的存在唯一性，以及一种具有全局步长界限的投影梯度方法确保收敛。最后，通过对连续解进行四舍五入获得整数值令牌分配方案，并评估了性能损失。


<details>
  <summary>Details</summary>
Motivation: 为了处理来自不同任务类型的异构查询流，同时优化这些查询的响应准确性和延迟，本文探讨了如何在一个单一的语言模型服务器中有效分配计算资源（即思考令牌）。

Method: 采用排队论中的M/G/1模型来描述系统的运行机制；构建一个约束下的优化问题框架，旨在最大化加权平均准确度与最小化平均系统时间之间的折衷；利用严格凹函数特性证明了最优解的存在和唯一性；提出了一种带全局步长界限的投影梯度算法用于求解该优化问题；最后，通过模拟实验对整数令牌分配策略进行了性能评估。

Result: 确定了优化问题的目标函数在稳定区域内是严格凹的，从而确保了解的唯一性；开发了一种可以保证在非收缩区域也能够收敛的投影梯度方法；通过将连续解四舍五入为整数来实现实际可行的令牌分配策略；仿真结果验证了所提方法的有效性。

Conclusion: 本研究表明，通过适当调整针对不同类型任务分配的思考令牌数量，可以在保持系统稳定性的同时显著提高整体性能。提出的优化框架不仅提供了理论基础，还给出了实用的解决方案来指导实际应用中的资源分配决策。

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [52] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出了一种轻量级且鲁棒的双重因果解缠框架（DualCD），以增强模型在领域增量场景下的鲁棒性，通过引入时间特征解缠模块和双重因果干预机制来捕捉类别因果特征并消除混淆特征的影响，从而提高时间序列分类任务中模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的研究在领域增量学习方面面临挑战，而准确的时间序列分类对于智能服务至关重要。

Method: 设计了一个名为DualCD的框架，该框架包括一个用于捕获类别因果特征与虚假特征的时间特征解缠模块，以及一个双重因果干预机制来排除类内和类间混淆特征的影响。

Result: 广泛的实验表明，DualCD能够有效提升模型在领域增量场景中的性能，并为此类研究提供了一个全面的基准。

Conclusion: DualCD框架为解决领域增量学习中的时间序列分类问题提供了有效的解决方案，并有助于推动相关领域的进一步研究。

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [53] [Meta Dynamic Graph for Traffic Flow Prediction](https://arxiv.org/abs/2601.10328)
*Yiqing Zou,Hanning Yuan,Qianyu Yang,Ziqiang Yuan,Shuliang Wang,Sijie Ruan*

Main category: cs.LG

TL;DR: 本文提出了一种新的交通预测框架MetaDG，通过动态图结构显式地建模时空动态特性，以解决现有方法在处理时空依赖性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的交通流预测方法在处理时空依赖关系时存在局限，主要体现在对动态特性的建模通常局限于空间拓扑（例如邻接矩阵的变化），并且异质性的建模常常被分离开来处理空间和时间维度。为了解决这些问题，提出了MetaDG框架。

Method: MetaDG框架利用节点表示的动态图结构来显式地建模时空动态特性，生成动态邻接矩阵和元参数，从而将动态建模扩展到超出拓扑范围，并且把时空异质性的捕捉统一到了一个维度。

Result: 通过对四个真实世界数据集进行广泛的实验验证了MetaDG的有效性。

Conclusion: MetaDG提供了一种有效的方法来改进交通流量预测中的时空依赖性建模问题，通过动态图结构与元参数的使用，使得模型能够更好地适应时空变化并提高预测准确性。

Abstract: Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.

</details>


### [54] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出了一个名为Strategy-aware Surprise (SuS)的新颖内在动机框架，该框架利用预测前后的不匹配作为强化学习中探索的新奇信号。通过结合策略稳定性（SS）和策略惊奇（SuS），以及在数学推理任务上的评估显示，与基线方法相比，SuS在Pass@1上提高了17.4%，在Pass@5上提高了26.4%，同时保持了更高的策略多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于好奇心的方法主要依赖于状态预测误差来驱动探索，而本文提出的方法旨在通过引入策略稳定性和策略惊奇两个互补组件来改进这一过程，从而提高探索效率及解决方案的多样性。

Method: 开发了一个名为Strategy-aware Surprise (SuS)的框架，它不仅考虑了行为策略的一致性（策略稳定性, SS），还捕捉到了相对于当前策略表示而言意想不到的结果（策略惊奇, SuS）。通过学习权重系数将这两个信号整合到奖励公式中。

Result: 在使用大型语言模型进行的数学推理任务测试中，SuS相较于基线方法在Pass@1指标上提升了17.4%，在Pass@5指标上提升了26.4%。此外，消融研究证实移除任一组件会导致至少10%的性能下降。

Conclusion: SuS提供了一种新的内在动机方法，能够有效提升强化学习环境中探索的质量和多样性，特别是对于需要复杂决策的任务。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [55] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: 本文提出了一种名为EvoMorph的多目标进化框架，用于为时间序列外在回归（TSER）应用生成生理上合理且多样化的反事实解释（CFE）。EvoMorph通过优化基于可解释信号描述符的形态感知目标，并应用转换来保持波形结构，从而解决了现有方法在处理连续生物医学时间序列时遇到的问题。该方法在三个PPG数据集上进行了评估，并且还作为不确定性量化的工具进行案例研究，将反事实敏感性与bootstrap-ensemble不确定性和数据密度度量相关联。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备对光体积描记法(PPG)等生理信号进行持续、大规模监测，为数据驱动的临床评估提供了新的机会。尽管时间序列外在回归(TSER)模型越来越多地利用PPG信号来估计心率等临床相关结果，但单点估计不足以支持临床推理和信任。为了使临床医生理解预测是否在生理学上合理的变体下稳定，以及生理信号的真实变化如何有意义地改变模型预测，需要引入反事实解释(CFE)来解答这些“如果”问题。然而，现有的时间序列CFE生成方法主要局限于分类，忽略了波形形态，通常产生生理上不合理的信号，限制了它们在连续生物医学时间序列中的适用性。

Method: 为了解决这些问题，研究者们提出了EvoMorphism，这是一种针对TSER应用设计的多目标进化框架，旨在生成生理上合理且多样的CFE。EvoMorphism通过定义于可解读信号描述符上的形态意识目标来进行优化，并运用变换技术以保持波形结构不变。

Result: EvoMorphism在三个PPG数据集（包括心率、呼吸频率和氧饱和度）上进行了评估，同时使用了一个最近不同邻居基线作为对比。此外，在一个案例研究中，研究人员还将EvoMorphism作为一种工具来量化不确定性，即通过关联反事实敏感性与bootstrap-ensemble不确定性及数据密集度指标来进行分析。

Conclusion: 总的来说，EvoMorphism能够为连续生物医学信号生成具有生理意识的反事实情况，并支持不确定性意识下的可解释性，这促进了临床时间序列应用中可信模型分析的发展。

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [56] [PLGC: Pseudo-Labeled Graph Condensation](https://arxiv.org/abs/2601.10358)
*Jay Nandy,Arnab Kumar Mondal,Anuj Rathore,Mahesh Chandran*

Main category: cs.LG

TL;DR: 提出了一种自监督框架Pseudo-Labeled Graph Condensation (PLGC)，它可以在没有真实标签的情况下，通过从节点嵌入构建潜在伪标签，并优化浓缩图以匹配原始图的结构和特征统计信息。该方法在标签噪声或分布偏移下表现出强大的鲁棒性，对于节点分类和链接预测任务都达到了与最先进的有监督浓缩方法相当的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的图凝聚方法依赖于干净且受监督的标签，这限制了当标签稀缺、嘈杂或不一致时的可靠性。因此，需要一种不需要真实标签的方法来解决这个问题。

Method: Pseudo-Labeled Graph Condensation (PLGC) 是一个自监督框架，它通过从节点嵌入中构造潜在伪标签并优化浓缩图以匹配原始图的结构和特征统计数据来工作。该方法的关键贡献包括：(1) 对为什么在标签噪声和分布偏移下有监督凝聚会失败进行诊断；(2) 一种无需标签的凝聚方法，联合学习潜在原型和节点分配；(3) 理论保证表明伪标签保留了原始图的潜在结构统计信息，并确保准确的嵌入对齐。

Result: 实验结果表明，在节点分类和链接预测任务上，PLGC 在干净数据集上达到了与最先进监督凝聚方法相媲美的性能，并且在标签噪声条件下显示出了显著的鲁棒性，通常大幅超越所有基线方法。

Conclusion: 研究结果突出了自监督图凝聚在噪声或弱标记环境中的实际和理论优势。

Abstract: Large graph datasets make training graph neural networks (GNNs) computationally costly. Graph condensation methods address this by generating small synthetic graphs that approximate the original data. However, existing approaches rely on clean, supervised labels, which limits their reliability when labels are scarce, noisy, or inconsistent. We propose Pseudo-Labeled Graph Condensation (PLGC), a self-supervised framework that constructs latent pseudo-labels from node embeddings and optimizes condensed graphs to match the original graph's structural and feature statistics -- without requiring ground-truth labels. PLGC offers three key contributions: (1) A diagnosis of why supervised condensation fails under label noise and distribution shift. (2) A label-free condensation method that jointly learns latent prototypes and node assignments. (3) Theoretical guarantees showing that pseudo-labels preserve latent structural statistics of the original graph and ensure accurate embedding alignment. Empirically, across node classification and link prediction tasks, PLGC achieves competitive performance with state-of-the-art supervised condensation methods on clean datasets and exhibits substantial robustness under label noise, often outperforming all baselines by a significant margin. Our findings highlight the practical and theoretical advantages of self-supervised graph condensation in noisy or weakly-labeled environments.

</details>


### [57] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 本文研究了多步前瞻信息的表格强化学习问题，提出了一种自适应批量策略（ABP），并设计了一种乐观后悔最小化算法来学习未知环境下的最优ABP。


<details>
  <summary>Details</summary>
Motivation: 在处理具有多步前瞻信息的强化学习问题时，尽管这种信息可以显著提高价值，但找到最优策略是NP难问题。现有的两种常见解决方法——固定批量策略和模型预测控制都有其局限性。

Method: 提出一种新的策略称为自适应批量策略（Adaptive Batching Policies, ABPs），并为这些策略推导出最优贝尔曼方程；设计了一个乐观后悔最小化算法以支持在与未知环境交互时学习到最优ABP。

Result: 提出的算法在遗憾边界上达到了最优阶数，仅可能受到前瞻范围$\ell$的影响，而这个值通常被视为一个小常数。

Conclusion: 通过引入自适应批量策略及其相关算法，该研究为有效利用前瞻信息提供了新途径，并且在理论上保证了性能。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [58] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow是一个利用流匹配来捕捉复杂行为流形的离线强化学习框架，通过在数据驱动的信任区域内学习轻量级的细化模块，避免了求解器微分和损失项平衡的需求，从而保持了流动迭代表达能力的同时实现了性能上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决生成策略优化中的计算成本问题以及传统的单步蒸馏方法牺牲迭代生成能力的问题，提出了一种新的离线强化学习框架。

Method: 采用流匹配技术，并在一个由数据定义的信任区域内学习一个轻量级的细化模块，这种方法避免了需要通过对ODE求解器进行反向传播来进行优化，同时不需要平衡不同损失项，保证了稳定改进与流动迭代表达能力的完全保留。

Result: 在具有挑战性的OGBench基准测试中，DeFlow表现出色，并且展示了从离线到在线适应的有效性。

Conclusion: DeFlow提供了一个有效的方法来解决离线强化学习中存在的计算效率低下及模型表达能力受限等问题，通过其独特设计能够在保持高效的同时达到更好的性能。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [59] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: 介绍了PROMA，一种用于大型语言模型微调的近端策略更新方法，通过在微批次聚合前投影出序列梯度分量来累积策略梯度，实现在不增加额外前向或后向传递的情况下更稳定的学习策略。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法如PPO和GRPO在进行近端更新时可能遇到的熵塌缩问题以及对参考策略或似然比裁剪的依赖性，提出了PROMA方法以实现更稳定的策略学习并更好地控制局部KL散度。

Method: PROMA通过在反向传播过程中逐层应用投影，在微批次（microbatches）之间累积策略梯度，同时去除序列级别的梯度成分，以此方式执行高效的实现而无需额外的正向或反向计算过程。

Result: 实验表明，与GRPO相比，PROMA能够更加严格地控制局部KL散度，从而促进更稳定的策略学习；此外，PROMA能够在不引起熵崩溃的情况下完成近端更新，并且不需要依赖于参考策略或似然比裁剪机制。

Conclusion: PROMA作为一种新的近端策略更新技术，对于大规模语言模型的微调提供了一种有效的方法，它不仅提高了策略学习的稳定性，还避免了其他方法中常见的熵塌缩等问题。

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [60] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 本研究探讨了使用GPT-2架构的Transformer模型来生成新的无线通信调制方案，通过与传统方法的关键性能指标比较，结果表明Transformer生成的调制方案在某些情况下可以达到甚至超过传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 认知无线电系统能够从机器学习技术的进步中显著受益，特别是在频谱效率、鲁棒性和安全性方面。通过创新的方法如使用Transformer模型可以进一步增强这些系统。

Method: 该研究采用GPT-2架构对现有调制公式的数据集进行训练，以生成新的调制方案，并将这些新方案与传统方法就信噪比（SNR）和功率谱密度（PSD）等关键性能指标进行了对比。

Result: 结果显示，基于Transformer生成的调制方案不仅达到了与传统方法相当的水平，在某些情况下还超过了传统方法的表现。

Conclusion: 先进的认知无线电系统可以通过实施Transformer模型来获得很大的好处，从而实现更加高效、鲁棒和安全的通信系统。

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [61] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: 本文提出了一种混合透明局部模型的方法，用于设计可解释（或透明）的模型。通过学习透明标记函数及其在输入空间中实现小风险的局部性，该方法适用于不同局部区域内适合使用简单透明函数建模的情况。文章为二元线性分类问题和线性回归问题建立了严格的PAC-B夷式风险边界，并通过合成数据集展示了算法的工作原理。真实数据集上的结果表明了该方法与其他现有方法相比具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在人类活动多个领域的主导地位日益增强，对这些模型透明度的需求也在不断增加。透明度有助于识别安全性和非歧视等因素。

Method: 提出了一个混合透明局部模型的方法，旨在当简单且透明的功能适合于某些局部/区域内的实例标签建模时应用，但这种功能可能随着从一个局部到另一个局部的变化而突然改变。因此，所提出的算法既要学习透明的标记函数，也要学习输入空间中的局部性，在该局部性中，标记函数在其分配的局部性中实现了较小的风险。为此引入了一个新的多预测器（及多局部性）损失函数。

Result: 对于二元线性分类问题和线性回归问题，均建立了严格的PAC-贝叶斯风险边界。通过合成数据集说明了学习算法的工作方式，并且真实数据集的结果突出了本方法相较于其他现有方法以及某些不透明模型的竞争优势。

Conclusion: 混合透明局部模型提供了一种有竞争力的解决方案，能够提高模型的透明度而不牺牲性能。此方法不仅理论上得到了支持，而且在实际应用中也表现出良好的效果。

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [62] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: 本文提出了一种过程引导的概念瓶颈模型（PG-CBM），通过遵循领域定义的因果机制来改进深度学习模型的可解释性和准确性，特别是在科学应用中。


<details>
  <summary>Details</summary>
Motivation: 标准的概念瓶颈模型(CBMs)忽视了特定领域的关联和因果机制，并且对完整概念标签的依赖限制了其在监督稀少但过程明确定义的科学领域中的适用性。

Method: 提出了过程引导的概念瓶颈模型(PG-CBM)，该模型作为CBMs的一种扩展，通过生物物理上有意义的中间概念来约束学习以遵循领域定义的因果机制。

Result: 以地球观测数据估算地上生物量密度为案例研究，表明相比于多个基准模型，PG-CBM减少了误差和偏差，同时利用了多源异构训练数据并产生了可解释的中间输出。

Conclusion: 除了提高准确性外，PG-CBM还增强了透明度、能够检测出虚假学习现象，并提供了科学见解，代表了向更值得信赖的人工智能系统迈进的一步。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [63] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: 本文综述了组合优化增强机器学习（COAML）的最新进展，提出了一个统一框架来描述其方法论构建模块，并基于不确定性和决策结构的形式开发了一种问题设置分类法。文章回顾了静态和动态问题的算法方法、跨领域的应用以及在经验成本最小化、模仿学习和强化学习方面的研究贡献，并指出了未来的研究前沿方向。


<details>
  <summary>Details</summary>
Motivation: 为了将预测模型与组合决策相结合，组合优化增强机器学习（COAML）作为一种强有力的方法出现。它通过在学习流程中嵌入组合优化oracle，使得能够构建既基于数据又保持可行性的策略，从而连接了机器学习、运筹学和随机优化的传统领域。

Method: 提出了一种COAML管道的统一框架，介绍了它们的方法论构成要素，并正式化了它们与经验成本最小化的联系。根据不确定性形式和决策结构发展出一种问题设置分类法。利用此分类法，对静态和动态问题的算法方法进行了回顾；调查了包括调度、车辆路径规划、随机编程及强化学习等不同领域的应用；并从经验成本最小化、模仿学习和强化学习的角度综合了方法论上的贡献。

Result: 该综述不仅为初学者提供了进入该领域的教程介绍，也为未来处于组合优化与机器学习交叉点的研究提供了路线图。此外，还确定了几大关键研究前沿。

Conclusion: 本论文作为一份全面的综述，旨在为组合优化增强机器学习这一新兴领域提供一个清晰的理解框架，同时指出未来研究的关键方向。

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [64] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的概率框架ProbFM，利用深度证据回归（DER）来提供具有明确认识-偶然不确定性分解的理论基础的不确定性量化。在加密货币收益预测上的评估表明，DER保持了有竞争力的预测准确性，同时提供了明确的认识-偶然不确定性分解。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型（TSFMs）在零样本金融预测中表现出色，但其在金融应用中的采用受到不确定性量化基本限制的影响：现有方法要么依赖于严格的分布假设，混淆了不同来源的不确定性，或者缺乏原则性的校准机制。尽管最近的一些TSFMs采用了混合模型、学生t-分布或一致性预测等复杂技术，它们未能解决提供理论支持的不确定性分解的核心挑战。

Method: 研究者们首次提出了一个名为ProbFM的新颖转换器基概率框架，该框架利用深度证据回归（DER）来提供带有显式认知-随机不确定性质解的原则性不确定性量化。与预设分布形式或需要基于采样的推理方法不同，ProbFM通过高阶证据学习来学习最优不确定性表示，同时保持单次传递计算效率。为了独立于架构复杂性严格评估核心DER不确定性量化方法，使用一致的LSTM架构对五种概率方法进行了广泛的控制比较研究：DER、高斯NLL、学生t-NLL、分位数损失和一致性预测。

Result: 在加密货币回报预测上的评估显示，与其他方法相比，DER不仅保持了竞争性的预测精度，而且能够提供清晰的认知-随机不确定性分解。

Conclusion: 本研究建立了一个可扩展的基础模型中原则性不确定性量化的框架，并为DER在金融应用中的有效性提供了实证证据。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [65] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出了一种基于摊销随机变分推理的数据驱动框架，用于学习能够跨参数空间和强迫条件泛化的连续时间随机降阶模型（ROMs），并展示了其在三个挑战性测试问题上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的降阶模型方法难以处理随机动力学，并且无法量化预测不确定性，这限制了它们在鲁棒决策制定中的应用。

Method: 该方法利用马尔可夫高斯过程的重参数化技巧来避免训练期间使用计算成本高昂的前向求解器，从而联合学习概率自编码器和控制潜在动力学的随机微分方程。此外，该方法还允许灵活地结合物理信息先验知识。

Result: 通过三个具有挑战性的测试问题，展示了新方法对于未见过的参数组合和强迫条件的良好泛化能力，以及相比于现有方法的显著效率提升。

Conclusion: 所提出的框架为复杂动态系统建模提供了一个高效且强大的工具，特别是在需要考虑不确定性和不同工作条件的情况下。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [66] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: 本文提出了一个名为CEPAM的新方法，该方法在联邦学习中同时实现了通信效率和隐私保护，并对其隐私保证、收敛性质以及实用性进行了理论分析和实验评估。


<details>
  <summary>Details</summary>
Motivation: 联邦学习允许多方在不共享其基础数据的情况下共同训练学习模型，为在数据治理限制下的隐私保护合作提供了实际途径。继续研究联邦学习对于解决其中的关键挑战至关重要，包括各方之间的通信效率和隐私保护问题。

Method: CEPAM利用了拒绝采样通用量化器（RSUQ），这是一种随机向量量化器，其量化误差等同于规定的噪声，可以通过调整来定制各方之间的隐私保护。

Result: 通过实验评估了CEPAM的实用性表现，包括与其他基线相比的收敛曲线，以及不同参与者之间的准确性-隐私权衡。

Conclusion: 本研究对CEPAM的隐私保障和收敛特性进行了理论分析，并通过实验证明了其相对于其他方法的有效性。

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [67] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 本文研究了通过迭代参数混合训练的半异步客户端-服务器感知器模型，考虑了联邦和分布式部署中的三个系统效应：陈旧更新、部分参与以及上下行链路的不完美通信。提出了一种称为“陈旧度桶聚合加填充”的服务器端聚合规则，并在一定条件下证明了有限时间内累积加权感知器错误数的期望边界。


<details>
  <summary>Details</summary>
Motivation: 作者旨在解决联邦学习和分布式部署中由于延迟模型交付、客户端计算应用延迟、间歇性客户端可用性和通信噪声导致的学习性能问题。

Method: 采用了一种名为“陈旧度桶聚合加填充”的新方法来处理不同年龄更新的聚合问题，这种方法能够在没有假设任何随机延迟或参与模式的情况下强制执行预定的陈旧度分布。

Result: 研究表明，在给定的服务器轮次内，对累积加权感知器错误数设定了一个有限时间内的预期上限；当不存在噪声时，满足轻微的新鲜参与条件即可获得明确的有限轮次稳定化界限。

Conclusion: 提出的陈旧度桶聚合策略能够有效管理具有不同延迟水平的更新，同时保持良好的学习性能，即使在存在通信噪声的情况下也能控制错误的增长。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [68] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 本文提出了一种新的扩散模型求解器，该求解器依赖于低度近似和配置方法之间的微妙相互作用。证明了其迭代复杂度在1/ε中是对数多项式的，这是第一个仅使用（近似的）对数据分布分数的访问就能保证高精度的基于扩散的采样器。此外，该复杂度界限不直接依赖于环境维度，而是通过目标分布支持的有效半径来影响求解器的复杂度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在从丰富的多模态分布中采样方面取得了显著的成功，但它们所依赖的微分方程无法以闭式形式解决，通常需要许多小的迭代才能产生高质量样本。先前的工作表明，扩散模型离散化方法的迭代复杂度随着环境维度和逆精度1/ε呈多项式增长。为了提高采样效率并减少迭代次数，作者们开发了一种新方法。

Method: 提出的新方法利用了低度近似与配置方法之间的一种微妙平衡。这种方法旨在减少达到特定精度所需的迭代次数，并且其性能分析显示，迭代复杂度在1/ε上是对数多项式的。

Result: 研究表明，新提出的求解器在1/ε上的迭代复杂度是对数多项式的，这意味着它能够更高效地生成高质量样本。此外，这种复杂度并不直接依赖于问题的维度，而是通过目标分布支持的有效半径来间接影响。

Conclusion: 本研究为基于扩散模型的采样提供了一个迭代复杂度较低的新方法，这对于需要高精度但同时又希望减少计算成本的应用来说是一个重要的进步。

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [69] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 本文研究了异构分布式线性可分计算问题，提出了一种通用计算方案和通用反向界，以描述在任意异构数据分配下任务函数的可计算维度与通信成本之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 针对现有研究通常假设每个工作者持有相同数量的数据集这一局限性，本研究探讨了更为普遍的情况，即跨工作者存在任意异构数据分配，并且目标是刻画在这种情况下任务函数的可计算维度与通信成本之间的基本权衡关系。

Method: 通过分析数据分配结构，为任意异构数据分配条件下的整数通信成本提出了一个通用计算方案和一个通用反向界。随后，将所提出的计算方案及反向界扩展到了分数通信成本的情形。

Result: 提出的通用计算方案和反向界在某些参数条件下能够达到一致。此外，这些方法也被成功地扩展应用到了分数通信成本的情况中。

Conclusion: 本文工作揭示了在面对更广泛的异构数据分配场景时，如何有效地平衡任务函数的可计算维度与所需通信成本之间关系的方法论。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [70] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: 本文提出了一种新的软件事务内存（STM）系统Multiverse，它结合了未版本化TM和多版本的优点，支持版本化和未版本化的事务并发执行。对于常见工作负载，Multiverse的性能与最先进的未版本化STM相当；对于长时间读取和频繁更新的工作负载，其性能显著优于现有STM系统。


<details>
  <summary>Details</summary>
Motivation: 现有的STM在处理长时间运行且访问大量经常更新地址的读操作时存在局限性。虽然多版本技术可以解决这个问题，但其成本较高，并且可能降低那些不需要版本控制的事务性能。因此，研究旨在开发一种既能保持未版本化事务高效运行，又能快速处理需要版本化事务的新STM。

Method: 设计并实现了名为Multiverse的新STM系统，该系统允许版本化和未版本化事务同时执行。通过优化架构，确保未版本化事务获得接近当前最佳未版本化STM的性能，同时也为需要长时间读取的操作提供了高效的版本化事务支持。

Result: 实验结果显示，对于没有长时间读取的一般工作负载，Multiverse达到了与其他先进STM相媲美甚至更优的性能水平。而在面对包含长时间读取和高频率更新的工作负载时，Multiverse的表现远超其他STM系统，在某些情况下吞吐量提高了几个数量级。

Conclusion: Multiverse作为一种新型STM，成功地将未版本化与多版本的优势结合起来，为不同类型的并发工作负载提供了灵活而高效的解决方案。特别是对于需要支持长时间读取同时又要保证高性能的应用场景，Multiverse展现出了巨大的潜力。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [71] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 本文介绍了两个工具Babel和ORION，它们分别解决了由于标识符模式和数据模型的差异导致的数据资源在实践中难以有效互操作的问题。通过这两个工具的应用，创建了一个完全可互操作的知识库库，可供下载使用。


<details>
  <summary>Details</summary>
Motivation: 尽管许多资源使用了符合FAIR原则且注释良好的词汇表创建，但这些资源作为一个整体生态系统在实践中往往无法有效地互操作。这主要是因为这些资源中使用的标识符模式和数据模型存在差异。

Method: 开发了两个工具：Babel 通过生成一组精选的标识符映射来解决多个标识符方案的问题，从而创建等效标识符的集合，并通过高性能API暴露出来；ORION 通过摄取知识库并将其转换为一个共同的、社区管理的数据模型来解决多个数据模型的问题。

Result: Babel 和 ORION 能够支持数据互操作，并且已经创建了一个可通过应用这两个工具而得来的完全可互操作的知识库库，该库可从 https://robokop.renci.org 下载和使用。

Conclusion: Babel 和 ORION 工具能够有效改善现有科学数据资源之间的互操作性问题，使得不同来源的数据可以在统一的框架下进行有效的交互与共享。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [72] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: 提出了一种名为SDP(Selective-Discovery-and-Prune)的方法，用于发现按冗余计数排名的前k个函数依赖关系（FDs），以解决在大数据集上计算所有有效FD的成本过高和结果过多的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的函数依赖关系发现算法通常会找到所有有效的依赖关系，但这样做会导致两方面的问题：一是对于大规模高维度数据，其计算成本过高；二是生成的结果集过大，难以从中识别出有用的依赖关系。

Method: 提出了SDP (Selective-Discovery-and-Prune) 方法来发现按照冗余计数排序的前k个函数依赖关系。该方法利用了冗余度上限来剪枝搜索空间，并通过三个优化措施进一步提高效率：依据分区基数对属性进行排序、使用分区基数矩阵中的成对统计信息来收紧界限以及采用全局调度器优先探索有希望的分支。

Result: 超过40个数据集上的实验表明，与穷举法相比，SDP不仅速度快得多，而且占用内存更少。

Conclusion: SDP方法提供了一种高效且资源节约的方式来发现最重要的函数依赖关系，特别适用于处理大规模和高维度的数据集。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [73] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 本文提出了一种新的方法，通过合并结构相似的语句或事务来提高应用程序端的事务处理性能。设计了名为TransactionMerger的中间件，用于收集和合并来自不同客户端的事务，并提供了一种静态分析工具以识别合并机会而不违反隔离性。实验表明，这种事务合并可以显著提高TPC-C和Spree应用的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 旨在通过在应用程序层面合并结构上相似的语句或事务，以提升事务处理的整体表现。

Method: 1) 利用特定SQL语义合并类似语句；2) 消除冗余读取；3) 通过对它们聚合效应的事先计算来跨事务合并竞争语句。开发了TransactionMerger中间件及配套的静态分析工具来实现上述目标。

Result: 对于TPC-C基准测试，吞吐量最高提升了2.65倍；而对于Spree（一个流行的现实世界应用），吞吐量则提高了3.52倍。

Conclusion: 通过合并结构相似的事务能够显著增强事务处理能力，在不违背数据库隔离性的前提下为实际应用场景带来了可观的性能增益。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [74] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 本文提出了一种将（基础）数学数据模型模式转换为关系模式及一系列非关系约束的伪代码算法，该算法被证明是快速、稳健、完整且最优的。算法应用于建模家谱树子宇宙的数学模式，并提供了执行某些非关系约束的SQL和VBA代码示例以及开发此类约束强制执行代码的指导方针。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过提供一种高效且可靠的算法来解决从数学数据模型到关系数据库结构转化的问题，同时确保能够处理相关的非关系约束。

Method: 采用伪代码形式设计了算法，用于将（基础）数学数据模型方案转化为关系型数据库模式及相关的一系列非关系约束。此外，还展示了如何使用SQL和VBA语言实现特定非关系约束的具体例子。

Result: 所提出的算法表现出色，在速度、稳定性、完整性以及优化程度上都达到了预期目标。并且成功地应用到了一个具体案例中，即对家谱树子宇宙进行建模。

Conclusion: 该研究不仅提出了一个有效的算法来促进不同类型数据模型之间的转换，而且还提供了实际操作层面的支持，如编写特定编程语言代码以实施非关系型约束等，从而为MatBase智能数据库管理系统原型的发展做出了贡献。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>
