<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 56]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression](https://arxiv.org/abs/2512.15721)
*Sveinung Myhre*

Main category: cs.LG

TL;DR: 提出了一种名为DiscoverDCP的数据驱动框架，该框架结合了符号回归和约束凸规划(DCP)规则集来进行系统识别。通过强制所有发现的候选模型表达式遵循DCCP组合规则，确保输出表达式天然具有全局凸性，避免了事后凸性验证的计算难题。


<details>
  <summary>Details</summary>
Motivation: 传统的固定参数凸表达式（例如二次函数）可能过于限制或不够准确，因此需要一种方法来发现更放松且准确的功能形式，并保证这些形式是凸的，从而适用于安全关键的控制与优化任务。

Method: 开发了DiscoverDCP框架，它将符号回归技术与DCP规则相结合，以生成同时满足DCP规则的候选模型表达式。这样做的目的是为了直接产生具有全局凸性的模型，而无需后续复杂的凸性验证步骤。

Result: DiscoverDCCP能够发现比传统固定参数凸表达式更加灵活和准确的凸替代模型。这些模型不仅易于解释而且可以验证，非常适合用于对安全性要求极高的控制系统及优化问题中。

Conclusion: 本研究介绍了一种新的数据驱动方法DiscoverDCP，旨在通过结合符号回归和DCP规则来自动生成可用于安全关键领域的凸模型。这种方法有助于提升模型准确性的同时保持其数学性质上的优越性。

Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.

</details>


### [2] [LLaDA2.0: Scaling Up Diffusion Language Models to 100B](https://arxiv.org/abs/2512.15745)
*Tiwei Bie,Maosong Cao,Kun Chen,Lun Du,Mingliang Gong,Zhuochen Gong,Yanmei Gu,Jiaqi Hu,Zenan Huang,Zhenzhong Lan,Chengxi Li,Chongxuan Li,Jianguo Li,Zehuan Li,Huabin Liu,Ling Liu,Guoshan Lu,Xiaocheng Lu,Yuxin Ma,Jianfeng Tan,Lanning Wei,Ji-Rong Wen,Yipeng Xing,Xiaolu Zhang,Junbo Zhao,Da Zheng,Jun Zhou,Junlin Zhou,Zhanchao Zhou,Liwang Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: 本文介绍了LLaDA2.0，一种通过自回归模型系统转换而来的离散扩散大规模语言模型（dLLM），参数规模达到100B。它采用新颖的3阶段块级WSD训练方案，支持知识继承、逐步适应和效率感知设计原则，无需从头开始昂贵的训练过程。结合后训练对齐技术SFT和DPO，开发了两个针对实际部署优化的指令调优MoE变体：LLaDA2.0-mini (16B) 和 LLaDA2.0-flash (100B)，并已开源。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过提出一种新的方法来避免从零开始训练大型语言模型的成本高昂问题，同时保持模型性能和效率的优势。

Method: 通过将预训练的自回归模型转化为离散扩散大规模语言模型(dLLM)，利用一个包含三个阶段（热身、稳定、衰减）的独特块级WSD训练计划。此外，还使用了后续的微调(SFT)与偏好偏移(DPO)技术以进一步优化模型。

Result: 成功创建了两种版本的LLaDA2.0模型 - LLaDA2.0-mini (16B) 和 LLaDA2.0-flash (100B)，它们在前沿规模上提供了卓越的表现和效率，并且已经公开源代码。

Conclusion: LLaDA2.0及其变种为大规模语言模型的实际部署提供了一个新的范式，通过创新的方法实现了高效能和高效率。

Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.

</details>


### [3] [A Unified Generative-Predictive Framework for Deterministic Inverse Design](https://arxiv.org/abs/2512.15746)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出了一种统一的生成-预测框架Janus，用于解决异质材料微观结构的逆向设计问题。Janus结合了深度编码器-解码器架构和可分离神经架构的预测KHRONOS头部，能够学习一个同时适合生成逆变和物理预测剪枝的潜在流形。实验表明，Janus在MNIST数据集上实现了高保真重建、准确分类以及所有十个目标类别的多样化生成逆变，并在热导率标记的异质微观结构逆向设计中取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 由于高度非线性的正向物理特性、精细分辨图像相关的高维设计空间以及多模态输入属性流等因素，异质材料微观结构的逆向设计是一个根本上不适定且计算成本高昂的问题。虽然现代生成模型能够很好地模拟这种复杂的正向行为，但大多数模型并不天然支持快速稳定的基于物理信息偏置的确定性逆变。

Method: 提出了一种名为Janus的统一生成-预测框架，该框架通过将深度编码器-解码器架构与可分离神经架构的预测KHRONOS头相结合来解决上述问题。Janus学习到一个潜在流形，在拓扑学意义上，该流形对于生成逆变来说是等距的，而对于物理预测来说又是被修剪过的；联合目标诱导了潜在空间中的解缠。

Result: Janus首先在MNIST数据集上得到验证，显示了对所有十个目标类别具有高保真度的重建、精确分类及多样化的生成逆变能力。随后应用于带有热导率标签的异质微观结构逆向设计，实现了前向预测精度R^2=0.98（相对误差为2%）和低于5%的像素级重建误差。逆解满足目标属性至1%相对误差以内。

Conclusion: 通过在一个单一潜在空间内统一预测与生成，Janus能够在较低的计算成本下实现实时的、基于物理信息的逆向微观结构生成，这通常与经典优化方法相关联。

Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.

</details>


### [4] [D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models](https://arxiv.org/abs/2512.15747)
*Javon Hickmon*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的零样本方法——多样化人口数据生成（D3G），旨在提高预训练多模态模型在图像分类任务中的准确性，同时减少人口统计学偏差。通过使用CLIP作为基础多模态模型和Stable Diffusion XL作为生成模型，在推理时提供多样化的人口统计数据来改善性能，并探讨了不同人口特征对准确率的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管像CLIP这样的多模态模型已经在图像分类任务中取得了良好的表现，但该领域仍然面临挑战，特别是对于低容量模型来说容易出现拟合不足的问题。此外，确保高质量的数据集含有丰富的跨模态表示是困难的。当数据集没有平衡代表各类别时，预测会偏向于代表性更强的类别而忽视其他类别。这些问题可能导致零样本图像分类中产生有害偏见。

Method: 提出了Diverse Demographic Data Generation (D3G) 方法，这是一种不需要额外训练步骤的、面向零样本场景的技术，用于增强分类精度并减少预训练多模态模型中的人口统计学偏见。此方法结合了CLIP作为基本的多模态处理工具以及Stable Diffusion XL用作生成更多样化数据的手段。

Result: 研究表明，在推理阶段引入多样化的群体数据可以有效提升模型的表现力，并且还分析了单个人口属性如何影响最终的准确性指标。

Conclusion: 通过D3G方法可以在不进行额外训练的情况下，提高预训练多模态模型在零样本图像分类任务上的性能，并有助于减轻模型中存在的人口统计学偏见问题。

Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.

</details>


### [5] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: 提出了一种新的方法Abacus，用于预测用户事件的经验频率分布，并结合了序列学习目标的混合目标函数，在两个真实数据集上实验表明，该预训练方法优于现有方法，能够加速下游任务收敛，同时混合方法相比基线提高了高达6.1%的AUC值。


<details>
  <summary>Details</summary>
Motivation: 在展示广告系统中，建模用户购买行为是一项关键挑战，对于实时竞价至关重要。困难在于正向用户事件的稀疏性和用户动作的随机性，导致严重的类别不平衡和不规则的事件时间。通常的预测系统依赖于手工制作的“计数器”特征，忽略了用户意图随时间变化的细微差别。而当前的序列模型提取直接的序列信号，却忽略了有用的事件计数统计信息。

Method: 引入了Abacus，这是一种预测用户事件经验频率分布的新方法。此外，提出了一个将Abacus与序列学习目标统一起来的混合目标函数，它结合了聚合统计数据的稳定性与序列建模的敏感性。

Result: 在两个真实世界的数据集上的实验表明，Abacus预训练方法比现有方法更能加快下游任务的收敛速度；而采用混合方法相较于基准方法能提高至多6.1%的AUC表现。

Conclusion: 通过结合Abacus自监督预训练策略与深度序列模型，可以在保持对用户行为序列敏感度的同时，利用用户事件频率分布的信息优势，从而有效提升显示广告系统的性能。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [6] [Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?](https://arxiv.org/abs/2512.15748)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.LG

TL;DR: 研究探讨了大型多模态模型（LMMs）在视觉物种识别（VSR）任务中的表现，发现尽管LMMs在一般识别任务上表现出色，但在VSR任务中却不如通过少量样本学习（FSL）训练的专家模型。不过，研究还发现LMMs能够有效纠正专家模型的错误预测，并基于此提出了一种名为后处理校正（POC）的方法，该方法可以显著提高FSL方法的准确性，无需额外训练或手动干预。


<details>
  <summary>Details</summary>
Motivation: 这项研究旨在探索大型多模态模型是否能在高度专门化的视觉物种识别任务中胜过少量样本学习训练出的专家模型。由于物种级别的标注需要专业知识，这使得获取大量带标签的数据变得不切实际，因此使用少量样本来训练模型成为一种现实的选择。

Method: 研究者首先评估了LMMs在VSR任务上的性能，然后发现了LMMs能够在后处理阶段修正由FSL专家模型做出的错误预测。基于这一观察，他们开发了一个名为Post-hoc Correction (POC)的方法，该方法利用增强提示来促使LMM重新排序专家模型的前几项预测结果。

Result: 实验表明，在五个具有挑战性的VSR基准测试中，POC方法比先前的FSL方法提高了6.4%的准确率，且无需额外训练、验证或人工介入。此外，POC还能与不同的预训练基础模型和LMM兼容，作为一个即插即用模块显著提升现有FSL方法的效果。

Conclusion: 虽然LMMs在直接应用于VSR任务时表现不佳，但它们可以通过后处理校正机制有效地改善FSL专家模型的结果。提出的POC方法为改进VSR任务提供了一条新的途径，同时保持了实施简便性。

Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.

</details>


### [7] [A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel](https://arxiv.org/abs/2512.15749)
*Abiel Kim*

Main category: cs.LG

TL;DR: The paper explores the unique characteristics of ReLU MLPs under the NTK regime, especially focusing on the extrapolation behavior near the origin. It uncovers that quadratic extrapolation occurs for points close to the origin, which contrasts with the well-known linear extrapolation for out-of-distribution points far from the origin.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that while there is extensive analysis on how ReLU MLPs tend to extrapolate linearly for out-of-distribution points, the specific case of extrapolation near the origin within the NTK framework has not been thoroughly investigated. The non-translational invariance of the infinite-dimensional feature map induced by the neural tangent kernel makes this a particularly interesting and unexplored area, leading to the discovery of distinct quadratic extrapolation near the origin.

Method: The method involves analyzing the behavior of ReLU MLPs under the NTK regime, specifically looking at the differences between extrapolating very far from the origin (where linearity is expected) versus very close to the origin (where new behaviors are discovered). This includes leveraging the properties of the neural tangent kernel and its associated feature map, as well as exploring the implications of rotational invariance in this context.

Result: The key result is the identification of quadratic extrapolation when evaluating points close to the origin, contrasting with the typical linear extrapolation observed for out-of-distribution points far from the origin. This finding highlights the importance of considering the location of evaluation points relative to the training data distribution when predicting model behavior.

Conclusion: The conclusion emphasizes the significance of understanding the nuanced behavior of ReLU MLPs under the NTK regime, particularly regarding the difference in extrapolation patterns near the origin compared to those far away. This work contributes to a more comprehensive view of how these models perform outside their training data range, pointing towards potential improvements or adjustments in model design and application.

Abstract: It has been demonstrated both theoretically and empirically that the ReLU MLP tends to extrapolate linearly for an out-of-distribution evaluation point. The machine learning literature provides ample analysis with respect to the mechanisms to which linearity is induced. However, the analysis of extrapolation at the origin under the NTK regime remains a more unexplored special case. In particular, the infinite-dimensional feature map induced by the neural tangent kernel is not translationally invariant. This means that the study of an out-of-distribution evaluation point very far from the origin is not equivalent to the evaluation of a point very near the origin. And since the feature map is rotation invariant, these two special cases may represent the most canonically extreme bounds of ReLU NTK extrapolation. Ultimately, it is this loose recognition of the two special cases of extrapolation that motivate the discovery of quadratic extrapolation for an evaluation close to the origin.

</details>


### [8] [Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference](https://arxiv.org/abs/2512.16391)
*Dhruv Deshmukh,Saurabh Goyal,Nipun Kwatra,Ramachandran Ramjee*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的稀疏注意力方法Kascade，通过在选定的锚层中计算Top-k索引并在中间重用层中重复使用这些索引，以提高长上下文语言模型推理的速度。实验表明，该方法在保持高精度的同时，显著提升了解码和预填充注意力阶段的处理速度。


<details>
  <summary>Details</summary>
Motivation: 随着基于推理模型和RAG（检索增强生成）的长上下文语言模型推理变得越来越流行，注意力机制成为了主要的延迟来源。因此，需要一种能够减少这种延迟的方法，同时保持模型的准确性。

Method: Kascade是一种无需额外训练的稀疏注意力方法，它利用了已知观察结果如：1) softmax后的注意力本质上是稀疏的；2) 高权重键的身份在相邻层之间是稳定的。具体来说，Kascade会在一组精心挑选出来的“锚层”里准确地计算出Top-k索引，并把这些索引用于中间层，即所谓的“重用层”。锚层的选择是通过一个动态规划目标来实现的，旨在最大化开发集上的跨层相似性。此外，Kascade还考虑到了高效实现的要求，比如块级操作，并且整个过程对于预填充和解码注意力都是适用的。值得注意的是，Kascade中的Top-k选择与重用是针对每个头独立进行的。

Result: Kascade相较于FlashAttention-3基线，在H100 GPU上实现了高达4.1倍的解码注意力加速和2.2倍的预填充注意力加速，同时在LongBench和AIME-24等长上下文基准测试中几乎达到了密集注意力的准确性水平。

Conclusion: Kascade提供了一个有效的方法来加快长上下文LLM推理期间的注意力计算，同时保持了模型的准确性。这种方法不仅限于特定架构，而且易于部署到不同模型上。

Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.

</details>


### [9] [ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning](https://arxiv.org/abs/2512.15756)
*Yoonpyo Lee*

Main category: cs.LG

TL;DR: 本文介绍了一种名为ReactorFold的生成框架，它将核反应堆燃料组件设计重新定义为语言模型的序列建模问题。通过使用蒙特卡洛数据、参数高效的微调以及直接偏好优化（DPO），该模型能够学习压水堆组件的潜在结构，并在一次前向传递中生成候选布局。值得注意的是，尽管仅接受固定数量钆可燃吸收棒配置的训练，但该模型能自主调整钆的数量以满足严格的功率峰值限制，并发现高性能的非对称配置，挑战了传统的对称加载启发式方法。


<details>
  <summary>Details</summary>
Motivation: 传统的方法在预先设定的设计空间内搜索新型核反应堆核心设计，这限制了它们发现全新设计拓扑的能力。因此，研究者们开发了ReactorFold，旨在克服这一局限性，允许探索更广泛且不受人为约束的设计方案。

Method: 采用一种基于语言模型的生成方法，结合蒙特卡洛模拟产生的数据进行训练；利用参数高效微调技术来降低计算成本；并通过直接偏好优化（DPO）确保生成的设计符合特定的安全与性能要求。

Result: 实验结果表明，ReactorFold不仅能够生成满足严格功率峰值限制的设计，还能自适应地改变关键材料（如钆）的数量，从而产生优于传统方法的新颖设计方案。此外，它还能够识别出具有不对称布局特征的高效率设计，这些设计是传统搜索算法无法触及的。

Conclusion: 研究表明，像ReactorFold这样的语言模型可以通过学习物理因果关系超越人类设定的设计界限，为核反应堆设计提供了一条新路径。

Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.

</details>


### [10] [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)
*A. Quadir,M. Sajid,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 本文提出了一种新的多视图孪生限制核机器(TMvRKM)模型，该模型结合了核机器的优势和多视图框架，解决了传统基于核方法的计算和泛化挑战。通过正则化最小二乘法方法有效确定最优分离超平面，并采用早融合和晚融合策略来平衡多个视角之间的错误。实验结果表明，TMvRKM在UCI、KEEL和AwA基准数据集上表现出色，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 多视图学习（MVL）旨在利用来自多个视角的信息以提高泛化性能。尽管现有的多视图支持向量机(MvSVM)方法取得了一些成功，但在高维空间中使用核技巧捕捉决策边界时仍面临挑战，且对视图间不一致性敏感。为解决这些问题并进一步提升模型效率与表现，提出了新的模型。

Method: 提出了多视图孪生限制核机器(TMvRKM)，这是一种新型模型，它将核机器的优势与多视图框架相结合，旨在克服传统基于核的方法所面临的计算及泛化问题。TMvRKM通过正则化最小二乘法而不是求解大规模二次规划问题来高效地确定最佳分割超平面；此外，其原始目标函数包含一个耦合项，用以有效平衡不同视图间的误差。

Result: TMvRKM在UCI、KEEL和AwA等基准数据集上的测试结果表明，无论是从实验分析还是统计角度来看，该模型均展现出卓越的泛化性能，在所有场景下都超越了基线模型。

Conclusion: 新提出的TMvRKM模型不仅提高了多视图学习任务中的计算效率，还显著增强了分类性能。通过整合早期和晚期融合策略，该模型能够更好地利用所有视图信息进行训练，同时保持对单个视图特有变化的适应性。

Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.

</details>


### [11] [Yantra AI -- An intelligence platform which interacts with manufacturing operations](https://arxiv.org/abs/2512.15758)
*Varshini Krishnamurthy*

Main category: cs.LG

TL;DR: 本文开发并测试了一个智能生产系统，用于解决XRIT公司的能源管理、预测性维护和AI决策支持问题。通过集成机器学习模型及使用Streamlit实现数据可视化，并引入基于GPT-4的虚拟助手来提升工作效率和决策质量。


<details>
  <summary>Details</summary>
Motivation: 随着工业4.0的发展，智能生产的模式正在经历快速变化，包括实时追踪、机器学习以及人工智能系统的应用成为提高运营效率的关键因素。本研究旨在为XRIT公司构建一个能够处理诸如能源管理、预测维护和AI驱动决策支持等关键挑战的智能化生产体系。

Method: 采用随机森林分类器进行主动维护与孤立森林算法识别异常值等机器学习技术构建智能生产系统；利用Streamlit工具提供交互式仪表板实现数据可视化；并通过整合基于GPT-4的人工智能虚拟助手来增强员工获取即时信息的能力。

Result: 实验结果表明该系统在提高工作效率、优化能源管理和改善维修计划方面表现优异。此外，还展示了如何通过AI助手解答复杂问题从而辅助做出更佳的操作决策。

Conclusion: 所提出的智能生产系统有效解决了XRIT公司在能源管理、预测性维护及决策支持上的需求，显著提升了整体运营效率。未来的工作将集中在进一步优化系统以适应实际生产环境中的实时数据融合及其他改进措施上。

Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.

</details>


### [12] [Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning](https://arxiv.org/abs/2512.15759)
*Jahidul Arafat*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习框架SCFA，通过引入领域知识约束来优化非IID数据条件下的模型训练。理论和实验结果表明，该方法可以加快收敛速度、减少模型差异并提高隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 在非IID数据条件下，联邦学习的收敛速度较慢。现有解决方案通常对所有客户端更新一视同仁，忽略了语义的有效性。

Method: 提出了语义约束的联邦聚合（SCFA）框架，将领域知识约束纳入分布式优化，并证明了其收敛率。

Result: 与标准联邦学习相比，SCFA能够以2.7倍的优势保持实用性；在博世生产数据集上的实验显示，SCFA使收敛速度快了22%，模型分歧减少了41.3%。

Conclusion: SCFA为基于约束的联邦学习提供了首个收敛理论，并且在实际应用中表现出了良好的性能改进。

Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.

</details>


### [13] [Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps](https://arxiv.org/abs/2512.15761)
*Christopher Blum,Michael Neidlin*

Main category: cs.LG

TL;DR: 本研究提出了一种基于计算流体动力学(CFD)流特征的可解释机器学习框架，用于空间血栓评估。通过结合逻辑回归模型和结构化的特征选择流程，该框架能够识别与血栓风险增加相关的特定流特征，并在应用于离心泵时成功预测了可能形成血栓的区域。这一方法不仅保持了计算效率还提供了机制透明性，有助于快速筛查血栓生成情况而无需进行重复或成本高昂的模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的计算模型难以将复杂的流动条件转化为可靠且易于理解的血栓风险预测，这反映了我们对于哪些具体流动特性会促进血栓形成和发展缺乏全面了解。

Method: 引入了一个基于CFD衍生流动特性的空间血栓评估可解释机器学习框架。使用逻辑回归（LR）模型加上一个结构化特征选择过程来推导出一组紧凑且物理上可解释的特征集，包括非线性特征组合。

Result: 该模型能够再现标记的风险分布，并确定与血栓风险增加相关的一系列流动特性。当应用于离心泵时，即使仅在一个轴向泵操作点上训练，也能预测合理的易发血栓区域。

Conclusion: 可解释的机器学习可以将局部流动特性与血栓风险联系起来，同时保持计算效率和机制透明度。低成本的计算使得无需重复或昂贵的模拟即可快速筛选出血栓生成情况。所提出的框架为将可解释机器学习整合进以CFD驱动的血栓分析和设备设计工作流程提供了方法基础。

Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.

</details>


### [14] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: 本文提出了一种新的跨样本增强测试时适应框架CSA-TTA，用于提高术中低血压预测的准确性。通过构建跨样本库和引入粗到细检索策略，结合自监督掩码重建和回顾性序列预测信号训练模型，提高了模型对快速和细微术中动态的适应能力。在VitalDB数据集和真实世界医院数据集上的实验表明，该方法在微调和零样本场景下均能显著提高召回率和F1分数，展示了强大的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 术中低血压（IOH）是一个重要的手术风险因素，但由于患者个体差异大，准确预测仍然具有挑战性。现有的测试时适应方法因IOH事件罕见而难以提供可靠的测试时训练。为了解决这个问题，研究者们提出了一个新框架来改善个性化预测的效果。

Method: 研究人员提出了CSA-TTA框架，首先通过将历史数据分为低血压与非低血压样本建立跨样本库；接着采用K-Shape聚类识别代表性集群中心，并基于当前患者信号检索最相似的前K个样本；同时，在训练过程中结合了自监督掩码重建和回顾性序列预测信号以加强模型对快速及微妙变化的适应能力。

Result: CSA-TTA被应用于VitalDB数据集以及实际医院数据集中，并与先进的时间序列预测模型如TimesFM和UniTS进行了集成测试。结果表明，无论是在微调还是零样本情况下，CSA-TTA都能持续提升表现，例如在VitalDB上，微调场景下的召回率和F1分数分别提升了1.33%和1.13%，而在零样本场景下则分别提升了7.46%和5.07%。

Conclusion: 提出的CSA-TTA框架有效地解决了由于低血压事件稀少导致的测试时训练不可靠问题，通过跨样本增广方式增强了模型对于个体差异的适应能力，从而提高了术中低血压预测的准确性。此外，该方法展现出了良好的鲁棒性和泛化性能。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [15] [Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic](https://arxiv.org/abs/2512.15765)
*Mélissa Tamine,Otmane Sakhi,Benjamin Heymann*

Main category: cs.LG

TL;DR: 本文探讨了如何通过直接偏好优化（DPO）来简化大型语言模型（LLMs）训练中的数据估值问题，特别是Shapley值的计算。这使得数据所有者可以更好地做出关于数据管理和协作的决策。


<details>
  <summary>Details</summary>
Motivation: 随着数据成为训练大型语言模型的关键资产之一，数据所有者面临着如何高效地进行数据整理和投资、以及如何与其他数据所有者合作以共同训练更优模型同时公平分配收益的问题。

Method: 采用直接偏好优化（DPO）的方法，并利用其特定的数学结构来实现可扩展的Shapley值计算，从而克服传统方法中因需多次重新训练模型而导致的高昂计算成本。

Result: 展示了对于使用DPO训练的LLMs而言，其特殊的数学形式显著简化了Shapley值的计算过程，为数据估值与大型语言模型之间的交叉应用打开了大门。

Conclusion: 本研究提出的方法为解决数据估值问题提供了一种新的途径，特别是针对大型语言模型的数据集评估方面，有助于促进更加有效的数据管理和多方协作。

Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.

</details>


### [16] [TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration](https://arxiv.org/abs/2512.15773)
*Ye Li,Jiahe Feng,Yuan Meng,Kangye Ji,Chen Tang,Xinwan Wen,Shutao Xia,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 本文提出了TS-DP框架，通过模仿基础模型的Transformer-based drafter和基于RL的调度器来实现对Diffusion Policy的时间自适应推测解码，从而在不降低性能的情况下大幅提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略（DP）虽然擅长于具身控制任务，但因需要多次迭代去噪步骤而面临高推理延迟和计算成本问题。静态或有损加速方法如量化不能很好地处理动态变化的任务难度，而推测解码提供了一种无损且可适应的方法，但其在DP中的应用尚未得到充分探索。

Method: 提出了一种名为时间感知强化学习基础上的推测扩散策略（TS-DP），该方法首先利用一个基于Transformer的起草者来模仿基线模型的行为，并替换掉其中耗时的去噪调用；其次，通过一个基于强化学习的调度器进一步根据任务难度的变化调整推测参数，在保持准确度的同时提高效率。

Result: 实验表明，TS-DP能够达到最高4.17倍的速度提升，超过94%的草稿被接受，实现了25Hz的推理频率，使得基于扩散的方法可以在不牺牲表现的前提下实现实时控制。

Conclusion: 通过引入TS-DP框架，研究人员成功地为DP提供了具有时间适应性的推测解码能力，这不仅极大地提高了推理速度，而且保证了在多样化具身环境下的高效与准确性。

Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.

</details>


### [17] [Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence](https://arxiv.org/abs/2512.15780)
*Samruddhi Baviskar*

Main category: cs.LG

TL;DR: 该论文评估了表格机器学习模型在金融决策中的对抗鲁棒性，特别是在信用评分和欺诈检测数据上应用基于梯度的攻击，并测量了对歧视、校准以及金融风险指标的影响。结果显示，在小扰动下性能显著下降，但通过对抗训练可以部分恢复。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索金融领域中使用的表格机器学习模型面对对抗性攻击时的表现如何，尤其是在信用评分与欺诈检测这类关键应用场景下的稳定性和可靠性问题。

Method: 采用基于梯度的攻击方法对用于信用评分及欺诈检测的表格数据进行处理，并分析这些攻击对模型的歧视程度、校准效果以及相关金融风险衡量标准的具体影响。

Result: 研究发现，即使是在很小的数据扰动情况下，模型的性能也会出现明显下降；不过，通过实施对抗训练能够在一定程度上缓解这种负面影响。

Conclusion: 结论指出，尽管表格机器学习模型在金融决策中展现出了一定的实用性，但它们对于对抗性攻击仍然十分脆弱。因此，为了提高模型的安全性和可靠性，建议采取包括但不限于对抗训练在内的多种防御措施。

Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.

</details>


### [18] [Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection](https://arxiv.org/abs/2512.15900)
*Avais Jan,Prakash Chourasia,Sarwan Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 本研究评估了九种不同的核函数对分子序列进行t-SNE降维的效果，通过多种嵌入方法和生物数据集的实验表明余弦相似性核在保持成对距离、运行效率以及下游分析任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统的t-SNE采用高斯核计算成对相似度，但其缺乏数据依赖性和较高的计算开销限制了它对于分类生物序列的可扩展性和有效性。尽管提出了隔离核作为替代方案，但它可能不是捕捉序列相似性的最优选择。因此，需要探索更有效的核函数来提高t-SNE在处理大规模生物序列时的表现。

Method: 研究采用了九种不同的核函数应用于t-SNE，并使用三种嵌入方法（One-Hot编码、Spike2Vec及最小化器）对分子序列进行了测试。通过主观可视化与客观指标（如邻域保留分数）相结合的方式，对比了不同核函数下t-SNE的表现。此外，还通过对六个多样化的生物数据集执行广泛的分类和聚类实验，利用多种机器学习算法及评价指标进一步验证了研究发现。

Result: 结果表明，在多数情况下，余弦相似性核比其他核函数（包括高斯核和隔离核）具有更好的性能，不仅在保持低维度空间内成对距离方面表现出色，而且在运行时间效率上也更优。此外，核函数的选择显著影响着可视化质量及后续分析任务的结果，其中余弦相似性核提供了跨不同类型数据和嵌入策略中最稳健的表现。

Conclusion: 本研究表明，余弦相似性核在t-SNE应用于生物序列分析时，无论是从视觉呈现还是从实际应用角度考虑，都优于其他几种常见的核函数。这一发现为选择合适的核函数以优化大规模生物序列数据分析提供了重要参考。

Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.

</details>


### [19] [Introduction to Symbolic Regression in the Physical Sciences](https://arxiv.org/abs/2512.15920)
*Deaglan J. Bartlett,Harry Desmond,Pedro G. Ferreira,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 这篇论文介绍了2025年皇家学会讨论会议背景下推出的《物理科学中的符号回归》特刊。文章概述了符号回归的概念基础，与传统回归方法进行了对比，并探讨了其在物理科学中的主要应用案例及面临的挑战。此外，还强调了结合对称性约束、渐近行为等理论信息的新兴方向。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于介绍和推广符号回归（SR）作为一种强大的方法来从数据中揭示可解释的数学关系，旨在展示符号回归在科学研究发现和高效经验建模方面的潜力及其在物理科学领域日益增长的重要性。

Method: 通过回顾符号回归的基本概念与其相对于传统回归方法的特点，以及探讨它在物理科学中的各种应用场景，包括自动方程发现、复杂现象建模及昂贵计算模拟的紧凑仿真器构建等。

Result: 综述了符号回归的方法论考量点，如搜索空间设计、操作符选择、复杂度控制、特征选择及与现代AI方法的整合，并指出了当前面临的主要挑战，比如可扩展性问题、噪声鲁棒性、过拟合风险及计算复杂度。

Conclusion: 该特刊的文章共同展示了符号回归技术的发展加速及其在物理科学领域不断扩大的相关性，特别提到了融合对称性约束、渐近行为和其他理论信息作为未来发展的关键方向。

Abstract: Symbolic regression (SR) has emerged as a powerful method for uncovering interpretable mathematical relationships from data, offering a novel route to both scientific discovery and efficient empirical modelling. This article introduces the Special Issue on Symbolic Regression for the Physical Sciences, motivated by the Royal Society discussion meeting held in April 2025. The contributions collected here span applications from automated equation discovery and emergent-phenomena modelling to the construction of compact emulators for computationally expensive simulations.
  The introductory review outlines the conceptual foundations of SR, contrasts it with conventional regression approaches, and surveys its main use cases in the physical sciences, including the derivation of effective theories, empirical functional forms and surrogate models. We summarise methodological considerations such as search-space design, operator selection, complexity control, feature selection, and integration with modern AI approaches. We also highlight ongoing challenges, including scalability, robustness to noise, overfitting and computational complexity. Finally we emphasise emerging directions, particularly the incorporation of symmetry constraints, asymptotic behaviour and other theoretical information. Taken together, the papers in this Special Issue illustrate the accelerating progress of SR and its growing relevance across the physical sciences.

</details>


### [20] [A Unification of Discrete, Gaussian, and Simplicial Diffusion](https://arxiv.org/abs/2512.15923)
*Nuria Alina Chandra,Yucen Lily Li,Alan N. Amin,Alex Ali,Joshua Rollins,Sebastian W. Ober,Aniruddh Raghu,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本研究提出了一种统一的理论框架，将离散空间扩散、欧几里得空间中的高斯扩散和单形上的扩散这三种方法视为同一基础过程的不同参数化形式：Wright-Fisher群体遗传学模型。通过这种理论，研究者能够利用数学遗传学文献来实现稳定的单形扩散，并且训练了一个可以在测试时在任意三个领域执行扩散任务的单一模型，实验表明该模型在条件DNA生成方面比之前的模型更稳定且表现更好。


<details>
  <summary>Details</summary>
Motivation: 为了使用扩散模型对诸如DNA、蛋白质和语言等离散序列进行建模，实践者必须从三种主要方法中选择一种。然而，这些模型具有不同的算法、理论结构和权衡点。理想情况下，我们希望将每个模型看作是相同基础框架下的实例，从而允许实践者为下游应用切换模型。但先前的理论仅考虑了特殊情况下的联系。

Method: 研究人员开发了一个理论，将所有三种离散扩散方法统一为同一个基础过程的不同参数化版本，即Wright-Fisher人口遗传学模型。特别地，他们发现了单形和高斯扩散作为两种大规模人群限制。该理论正式连接了这些模型的可能性和超参数，并利用数十年的数学遗传学文献解锁了稳定的单形扩散。

Result: 研究表明，Wright-Fisher单形扩散更加稳定，在条件性DNA生成上优于以往的单形扩散模型。此外，还展示了可以同时在多个领域训练模型，这样的模型与只在一个特定领域训练的模型相比具有竞争力。

Conclusion: 这项工作提供了一个统一不同扩散方法的框架，使得实践者能够在不牺牲模型性能的情况下灵活地跨领域应用扩散技术。

Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.

</details>


### [21] [BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research](https://arxiv.org/abs/2512.15931)
*Tiancheng Gao,Scott C. Lowe,Brendan Furneaux,Angel X Chang,Graham W. Taylor*

Main category: cs.LG

TL;DR: 研究介绍了一种名为BarcodeMamba+的新模型，专为解决真菌条形码分类中的挑战而设计。通过预训练和微调的方式，并结合一系列增强技术，如层次标签平滑、加权损失函数等，该模型在稀疏数据环境中表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的DNA条形码分类对于全球生物多样性监测至关重要，但真菌由于标签稀疏和长尾分布特性给这项任务带来了巨大挑战。传统监督学习方法难以泛化到未见物种且不易捕捉数据的层级性质。

Method: 开发了基于强大且高效的状态空间模型架构的BarcodeMamba+模型。采用预训练与微调相结合的方法，利用部分标记的数据，并在微调过程中引入包括层次标签平滑、加权损失函数及MycoAI的多头输出层在内的多种改进措施来应对真菌分类难题。

Result: 实验表明，所提出的每项组件都显著提高了性能；在具有明显不同分类分布偏移的真实世界真菌分类基准上，最终模型在所有分类水平上均超越了各种现有方法的表现。

Conclusion: 本研究为基于基因组学的生物多样性研究提供了一个强有力的新工具，并为这一充满挑战领域内的有效可扩展训练范式奠定了基础。

Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.

</details>


### [22] [In-Context Semi-Supervised Learning](https://arxiv.org/abs/2512.15934)
*Jiashuo Fan,Paul Rosu,Aaron T. Wang,Michael Li,Lawrence Carin,Xiang Cheng*

Main category: cs.LG

TL;DR: 本文研究了在上下文半监督学习（IC-SSL）中，当标签稀少或缺失时，Transformer模型如何利用未标记的数据来学习稳健的、依赖于上下文的表示，并提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有理论主要关注有明确标签对的监督环境下的情境学习能力，但在实践中观察到即使标签稀缺或不存在，Transformer模型仍表现出色。这表明未标记的情境演示中存在关键结构。因此，引入并研究了情境半监督学习（IC-SSL），旨在探索Transformer如何利用未标记数据提升低标签环境下表现。

Method: 通过引入情境半监督学习框架，在少量标记样本与大量未标记点共存的情况下，研究了Transformer模型如何从这些未标记上下文中学习到稳健且依赖于特定情境的表示方法。

Result: 研究表明，通过利用未标记的数据作为上下文信息，Transformer能够学习到有助于准确预测和显著改善低标签设置下性能的表示。

Conclusion: 本研究为理解Transformer如何在情境学习框架内利用未标记上下文进行表征学习提供了基础性见解，展示了即使是在标签有限的情况下，也能通过充分利用未标记数据来增强模型的表现。

Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.

</details>


### [23] [Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling](https://arxiv.org/abs/2512.15956)
*John Hateley,Sriram Narasimhan,Omid Abari*

Main category: cs.LG

TL;DR: 本研究提出了一种新颖、成本效益高且可扩展的方法，使用商用RFID在森林环境中跟踪大量资产，特别针对野火响应应用。通过采用高斯过程建模环境中的RF信号响应特征，并利用新的加权对数似然方法将未知环境与先前建模的环境匹配，实现了与GPS相媲美的定位精度，但成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有的商用RFID系统在森林环境中因信号衰减、多路径效应和环境变化等因素导致标签定位效果差。而当前通过指纹识别解决此问题的方法依赖于预先在已知位置部署标签，本研究旨在解决当无法在已知位置贴标时仍能实现精确标签定位的问题。

Method: 提出了基于RF信号响应特征建模不同环境的高斯过程方法，不借助额外传感器如GPS或摄像头的帮助；开发了一种新的加权对数似然方法来关联未知环境与模型字典中最接近的环境。

Result: 实验结果表明，可以达到与GPS相当的定位精度，但仅需被动式商用RFID即可实现，允许同时追踪移动读取器附近的数十个野火相关资产，无需提前标记已知位置，而且成本远低于GPS。

Conclusion: 这项工作展示了一种可行的解决方案，能够以较低的成本为森林环境中的大量资产提供精准定位服务，尤其适用于野火响应场景。

Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.

</details>


### [24] [Provably Extracting the Features from a General Superposition](https://arxiv.org/abs/2512.15987)
*Allen Liu*

Main category: cs.LG

TL;DR: 本文提出了一种有效的查询算法，能够从对函数f的噪声预言机访问中识别出所有非退化响应的特征方向，并重建函数f。该算法在比以往研究更广泛的设定下工作，允许任意叠加和一般响应函数，通过迭代地细化搜索空间以定位隐藏的方向v_i。


<details>
  <summary>Details</summary>
Motivation: 研究者们普遍认为复杂的机器学习模型通常是通过线性表示来编码特征的，但这些特征处于叠加状态，使得它们难以恢复。本文旨在探索一种基本设置，即从黑盒查询访问中学习叠加中的特征，特别是当特征数量大于基础维度（即n>d）时，这种情况下对于典型的算法方法来说尤其具有挑战性。

Method: 本文的主要贡献是一种新的有效查询算法，它可以从对给定函数f的噪声预言机访问中识别出所有非退化响应的特征方向并重建函数f。此算法引入了一种在傅里叶空间中搜索的方法，通过迭代地细化搜索空间来定位隐藏的方向v_i。

Result: 实验结果表明，即使在存在噪声的情况下，所提出的算法也能够成功地识别出所有的特征方向，并且准确地重建原始函数f。这证明了该算法不仅在理论上可行，在实践中也同样有效。

Conclusion: 本文提供了一个新颖且高效的解决方案来处理特征叠加问题，特别是在过完备状态下。该算法能够在更通用的情况下工作，为解决复杂机器学习模型中的特征提取问题开辟了新途径。

Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,σ_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $σ_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $σ_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.

</details>


### [25] [Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives](https://arxiv.org/abs/2512.15997)
*Robert Stephany,William Michael Anderson,Youngsoo Choi*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法来改进减少阶模型(ROMs)在长时间范围内解决参数化偏微分方程(PDEs)的能力，通过引入一个灵活的高阶但成本低廉的有限差分方案以及一种Rollout损失函数，该方法在二维Burgers方程上得到了验证。


<details>
  <summary>Details</summary>
Motivation: 虽然现代减少阶模型能够处理参数化的偏微分方程组，但它们在长时间范围内的预测能力会下降。为了解决这个问题，文章旨在提出新方法以增强这些模型在任意时间跨度上的准确性。

Method: 1. 引入了一个既灵活又具有高阶特性同时成本较低的有限差分方案。
2. 提出了一种称为Rollout loss的新损失函数，专门用于训练减少阶模型，使其能够在任意的时间范围内做出准确预测。

Result: 所提出的方法被应用于二维Burgers方程，并展示了其有效性。

Conclusion: 通过采用新型有限差分方案与Rollout损失相结合的方式，可以有效提升减少阶模型对于复杂偏微分方程的求解效率及长期预测精度。

Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.

</details>


### [26] [Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results](https://arxiv.org/abs/2512.16013)
*Ruolei Zeng,Arun Sharma,Shuai An,Mingzhou Yang,Shengya Zhang,Licheng Liu,David Mulla,Shashi Shekhar*

Main category: cs.LG

TL;DR: 该论文提出了一种基于微调的站点校准-知识引导机器学习框架FTBSC-KGML，旨在提高农业生态系统碳循环量化在决策相关尺度上的准确性和成本效益。通过结合迁移学习和空间异质性，FTBSC-KGML能够更有效地估计土地排放，并在不同地点表现出更好的局部准确性与解释力一致性。


<details>
  <summary>Details</summary>
Motivation: 准确且经济高效地量化决策相关尺度下的农业生态系统碳循环对于气候缓解和可持续农业至关重要。然而，现有的方法未能充分利用迁移学习和输入中的空间异质性，限制了它们在变化显著区域的应用。

Method: 提出了FTBSC-KGML框架，该框架结合了预训练、微调过程以及特定于站点的参数调整，增强了KGML-ag模型的能力。利用来自多个中西部站点的遥感总初级生产力（GPP）、气候和土壤协变量数据，FTBSC-KGML能够在考虑迁移学习和空间异质性的同时估算土地排放。

Result: 实验表明，与纯全局模型相比，FTBSC-KGML不仅实现了更低的验证误差，而且在解释力方面表现得更加一致，更好地捕捉到了各州之间的空间变异性。

Conclusion: FTBSC-KGML框架通过引入对空间异质性的意识和迁移学习机制，在有限的数据下提高了局部准确性，同时保持了解释性，从而为农业生态系统碳循环的量化提供了新的解决方案。

Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.

</details>


### [27] [CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting](https://arxiv.org/abs/2512.16046)
*Shu Wan,Reepal Shah,John Sabo,Huan Liu,K. Selçuk Candan*

Main category: cs.LG

TL;DR: 本文提出CauStream，一种用于因果时空径流预测的统一框架。该模型在多个美国主要河流流域上表现优于现有最先进方法，并且能够学习到与领域知识一致的因果图，为理解流域动态提供了可解释性见解。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然在预测性能上表现出色，但往往忽略了潜在的物理过程，限制了解释性和泛化能力。最近的因果学习方法通过整合领域知识解决了这些问题，但它们通常依赖于固定的因果图，无法适应数据变化。

Method: 提出了CauStream，这是一个用于因果时空径流预测的统一框架。它同时学习（i）气象强迫之间的径流因果图和（ii）捕捉站点间动态依赖关系的路由图。此外，在非参数设置下建立了这些因果结构的可识别性条件。

Result: CauStream在三个主要美国河流流域以及三个预测时间范围内进行了评估。模型始终优于先前最先进的方法，特别是在更长的预测窗口中表现更好，显示出对未见条件更强的泛化能力。除了预测外，CauStream还学会了捕捉水文因素和站点之间关系的因果图。

Conclusion: CauStream为因果时空建模提供了一个有原则的基础，具有扩展到广泛科学和环境应用中的潜力。

Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.

</details>


### [28] [In-Context Multi-Operator Learning with DeepOSets](https://arxiv.org/abs/2512.16074)
*Shao-Ting Chiu,Aditya Nambiar,Ali Syed,Jonathan W. Siegel,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: 本研究展示了DeepOSets架构，通过适当修改后，能够在无需权重更新的情况下，从用户提示中给出的参数和解的示例对中恢复未在训练期间见过的新PDE（偏微分方程）的解算子。此外，还证明了DeepOSets是连续算子类上的通用一致逼近器，这是科学机器学习文献中的首个此类结果。实验表明该模型能够利用上下文示例准确预测训练过程中未曾见过的PDE的参数查询对应的解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索非自回归、非基于注意力机制的神经架构DeepOSets是否能在没有进一步权重更新的情况下展现出上下文学习的能力，并且能否成为一类连续算子的通用一致逼近器。

Method: 采用DeepOSets架构，结合了通过DeepSets架构进行集合学习以及通过DeepONets进行算子学习的方法。通过对DeepOSets进行适当的调整，使其成为一个多算子上下文学习者。

Result: 研究表明，经过适当修改后的DeepOSets架构能够作为多算子上下文学习者，从给定的例子中恢复出新的PDE解算子而不需要权重更新；并且DeepOSets被证实为连续算子类上的一个通用一致逼近器。实验部分通过泊松方程和反应扩散方程的正向与逆向边值问题验证了模型的有效性。

Conclusion: 结论指出，DeepOSets架构不仅可以在监督学习问题中展示出上下文学习能力，而且对于新出现的PDE也能有效地利用上下文信息来预测其解决方案，无需额外的权重更新。此外，它作为一个连续算子类的通用一致逼近器，代表了科学机器学习领域的一个重要进展。

Abstract: In-context Learning (ICL) is the remarkable capability displayed by some machine learning models to learn from examples in a prompt, without any further weight updates. ICL had originally been thought to emerge from the self-attention mechanism in autoregressive transformer architectures. DeepOSets is a non-autoregressive, non-attention based neural architecture that combines set learning via the DeepSets architecture with operator learning via Deep Operator Networks (DeepONets). In a previous study, DeepOSets was shown to display ICL capabilities in supervised learning problems. In this paper, we show that the DeepOSets architecture, with the appropriate modifications, is a multi-operator in-context learner that can recover the solution operator of a new PDE, not seen during training, from example pairs of parameter and solution placed in a user prompt, without any weight updates. Furthermore, we show that DeepOSets is a universal uniform approximator over a class of continuous operators, which we believe is the first result of its kind in the literature of scientific machine learning. This means that a single DeepOSets architecture exists that approximates in-context any continuous operator in the class to any fixed desired degree accuracy, given an appropriate number of examples in the prompt. Experiments with Poisson and reaction-diffusion forward and inverse boundary-value problems demonstrate the ability of the proposed model to use in-context examples to predict accurately the solutions corresponding to parameter queries for PDEs not seen during training.

</details>


### [29] [Privacy Blur: Quantifying Privacy and Utility for Image Data Release](https://arxiv.org/abs/2512.16086)
*Saeed Mahloujifar,Narine Kokhlikyan,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 研究了图像数据中隐私信息的模糊处理方法，发现高斯模糊在实际低精度实现中容易受到逆转攻击，而像素化和加噪像素化在适当粒度下可以同时提供隐私保护和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像数据往往包含私人信息，如人脸和车牌号，负责的数据发布必须确保这些信息被隐藏。然而，发布的数据也应保持其对于模型训练的有用性。标准的图像隐私信息模糊处理方法是高斯模糊。本研究旨在探讨不同模糊算法之间的隐私-效用权衡。

Method: 本文评估了四种模糊算法：高斯模糊、像素化、像素化加上噪声添加（DP-Pix）以及裁剪。通过逆转攻击和区分攻击来评价隐私性，同时通过学习表示的质量来衡量实用性，当模型在经过模糊处理的人脸数据上进行训练时。

Result: 结果表明，在实际低精度实现中，高斯模糊是最不私密的方法——容易受到逆转攻击的影响。相比之下，当使用适当的粒度级别时，像素化及像素化加上噪声添加为多种计算机视觉任务提供了隐私性和实用性。

Conclusion: 推荐使用像素化以及像素化加噪声作为更优的隐私保护方法，并且提出了一个名为Privacy Blur的软件包来实施建议的方法及其参数。

Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.

</details>


### [30] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: 本文介绍了一个名为AIMM的AI驱动框架，该框架结合Reddit活动、机器人和协调指标以及OHLCV市场特征，为每个股票代码生成每日AIMM操纵风险评分。通过构建AIMM-GT数据集、实现前向行走评估及前瞻性预测记录，并分析了预警时间，证明了AIMM对GME事件具有初步的区分能力和早期警告功能。


<details>
  <summary>Details</summary>
Motivation: 随着市场操纵越来越多地源自社交媒体上的协同活动而非孤立交易，零售投资者、监管机构和经纪公司需要一种工具来将在线叙述与协调模式联系起来以监测市场行为。

Method: 开发了一种AI驱动的框架AIMM，它融合了来自Reddit的活动数据、机器人和协调性指标以及OHLCV市场特征，生成每日AIMM操纵风险分数。使用了原生支持parquet格式的数据管道以及Streamlit仪表盘供分析师探索可疑窗口、检查相关帖子和价格变动，并随时间记录模型输出。

Result: 创建了包含33个标签日（涵盖8只股票）的AIMM Ground Truth数据集；实现了面向未来的评估和预测记录机制；分析显示AIMM在2021年1月GME挤压高峰前22天就发出了警示信号。

Conclusion: 尽管当前标记的数据集规模较小，但结果表明AIMM对于GME事件显示出初步的辨别能力并能够提供早期警告。研究团队发布了代码、数据集结构和仪表盘设计方案，以促进关于社交媒体驱动的市场监管的研究。

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [31] [BUILD with Precision: Bottom-Up Inference of Linear DAGs](https://arxiv.org/abs/2512.16111)
*Hamed Ajorlou,Samuel Rey,Gonzalo Mateos,Geert Leus,Antonio G. Marques*

Main category: cs.LG

TL;DR: 本文提出了一种名为BUILD的算法，用于从观察数据中学习有向无环图（DAGs）的结构。在满足线性高斯结构方程模型（SEM）且噪声方差相等的条件下，该算法能够通过识别叶节点及其父节点，并逐步修剪这些叶子节点来重构DAG。为了提高鲁棒性，算法还采用了定期重新估计精度矩阵的策略。实验结果表明，BUILD算法与最先进算法相比具有竞争力，并能显式地控制复杂度。


<details>
  <summary>Details</summary>
Motivation: 从观察数据中学习有向无环图（DAGs）的结构是因果发现、统计信号处理和机器学习中的核心问题。当处于线性高斯结构方程模型（SEM）且噪声方差相同的情况下，这一问题是可识别的。基于此背景，研究者旨在开发一种新的方法来更有效地恢复DAG结构。

Method: 研究团队提出了BUILD算法，这是一种确定性的分步算法，它首先识别出叶节点及其父母节点，然后通过移除相关的边来修剪这些叶子节点，从而继续进行下一步直到完全重构出DAG。面对实际应用中因有限数据而需估计精度矩阵以及可能遇到的条件数不佳问题，该方法采取了定期重新估计精度矩阵的策略以增强鲁棒性。

Result: 通过在具有挑战性的合成基准测试上的可重复实验结果证明，BUILD算法不仅能够与当前最先进的DAG学习算法相媲美，而且还能提供对计算复杂度更加明确的掌控方式。

Conclusion: BUILD算法为从观察数据中学习DAG结构提供了一个有效的方法，在保证性能的同时提供了良好的复杂度控制能力，显示出其作为现有技术补充或替代方案的巨大潜力。

Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.

</details>


### [32] [Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure](https://arxiv.org/abs/2512.16126)
*Lulu Xue,Shengshan Hu,Linqiang Qian,Peijin Guo,Yechao Zhang,Minghui Li,Yanjun Zhang,Dayong Ye,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 本文探讨了机器遗忘技术对保留数据的隐私风险，首次揭示了在双视角设置下机器遗忘引入的脆弱性。提出了DVIA（双视角推理攻击），通过黑盒查询两种模型来提取保留数据的成员信息。实验表明DVIA的有效性，并突出了双视角设置下的内在隐私风险。


<details>
  <summary>Details</summary>
Motivation: 先前的工作主要集中在被遗忘的数据隐私上，而忽视了保留数据所面临的隐私风险。为了填补这一空白，作者们关注于保留数据的隐私风险，在双视角设置下首次揭露了机器遗忘所带来的新的脆弱性。

Method: 从信息论的角度出发，引入了“隐私知识增益”的概念，并证明了双视角设置允许对手获取比单独查询任一模型更多的信息，从而放大了隐私泄露。为有效展示这种威胁，提出了DVIA——一种双视角推理攻击方法，该方法利用对原始模型和遗忘后模型的黑盒查询来抽取关于保留数据的成员信息。DVIA不需要训练攻击模型，而是采用轻量级似然比推理模块进行高效推理。

Result: 跨不同数据集和模型架构的实验验证了DVIA的有效性，强调了双视角设置中固有的隐私风险。

Conclusion: 本研究表明，在双视角环境下，机器遗忘不仅未能完全保障个人数据权利，反而可能加剧隐私泄露问题。通过提出DVIA，进一步证实了即使是在不直接访问敏感数据的情况下，通过对原始与修改后的模型进行对比分析也能够有效地推断出用户的私人信息。

Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.

</details>


### [33] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: 介绍了INTELLECT-3，一个拥有106B参数（激活状态为12B）的专家混合模型，通过大规模强化学习训练而成。该模型在数学、代码、科学和推理基准测试中表现出色，并开源了包括RL框架在内的全部基础设施栈。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在多个领域如数学、代码、科学和推理等基准测试中达到顶尖表现的AI模型，并且能够通过开源其基础设施栈来促进社区的发展和技术进步。

Method: 使用大型强化学习（RL）架构堆栈训练了一个名为INTELLECT-3的106B参数Mixture-of-Experts模型，其中实际激活参数量为12B。引入了prime-rl这一开放框架用于大规模异步强化学习任务，支持从单节点到数千个GPU的无缝扩展，并特别针对多轮交互和工具使用进行了优化。

Result: INTELLECT-3在多个关键领域的基准测试中取得了领先于许多更大规模前沿模型的成绩。同时，成功地将整个创建过程中的基础设施栈开源给社区，促进了更广泛的技术交流与合作。

Conclusion: 本研究不仅展示了通过高效的大规模强化学习方法可以训练出高性能的小型化模型，同时也强调了开放共享技术资源对于推动整个行业向前发展的重要性。

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [34] [Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions](https://arxiv.org/abs/2512.16200)
*Haishan Ye*

Main category: cs.LG

TL;DR: 本文分析了一种基于排名的零阶优化算法，首次给出了非渐近性的明确查询复杂度。对于强凸函数，达到ε-次优解的复杂度为$\widetilde{\mathcal O}\left(\frac{dL}{\mu}\log\frac{dL}{\mu\delta}\log\frac{1}{\varepsilon}\right)$；对于光滑非凸目标，则为$\mathcal O\left(\frac{dL}{\varepsilon}\log\frac{1}{\varepsilon}\right)$。


<details>
  <summary>Details</summary>
Motivation: 尽管基于排名的零阶（ZO）优化方法在许多成功的算法中得到广泛应用，如CMA-ES、自然进化策略和基于排名的遗传算法，但其理论理解仍然有限。现有分析仅提供渐近性见解，并未给出选择前k方向的具体收敛率。

Method: 研究提出并分析了一个简单的基于排名的ZO算法，采用新颖的方法避免了传统漂移和信息几何技术，从而得出了首个具体的、非渐近性的查询复杂度。

Result: 对于一个d维问题，如果函数是L-平滑且μ-强凸的，该算法达到ε-次优解的查询复杂度为$\widetilde{\mathcal O}\left(\frac{dL}{\mu}\log\frac{dL}{\mu\delta}\log\frac{1}{\varepsilon}\right)$；而对于光滑非凸目标，达到相应精度的查询复杂度则为$\mathcal O\left(\frac{dL}{\varepsilon}\log\frac{1}{\varepsilon}\right)$。这些结果以至少$1-\delta$的概率成立，其中$0<\delta<1$。

Conclusion: 本研究通过提供具体而非渐近性的查询复杂度，填补了基于排名的ZO方法理论分析的空白，揭示了为何基于排名的启发式方法能够实现高效的ZO优化。

Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $μ$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-δ$ with $0<δ<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.

</details>


### [35] [Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models](https://arxiv.org/abs/2512.16244)
*Xueqi Ma,Xingjun Ma,Sarah Monazam Erfani,Danilo Mandic,James Bailey*

Main category: cs.LG

TL;DR: 提出了一种从粗到细的开放集分类(CFC)框架，该框架利用大型语言模型(LLM)对图数据进行处理。CFC通过LLM提示进行OOD检测和异常标签生成，并使用GNN进行更精细的分类，从而提高OOD检测和ID分类性能。实验结果表明，CFC在图和文本领域上的OOD检测比现有方法提高了10%，在图数据集上的OOD分类准确率高达70%。


<details>
  <summary>Details</summary>
Motivation: 现有的开放集分类方法通常将所有OOD样本视为单一类别处理，但在实际应用中，特别是像欺诈检测和医疗诊断这样的高风险场景下，需要对OOD样本有更深入的理解，包括它们可能的标签。因此提出了一个关键问题：是否可以在没有真实标签信息的情况下扩展OOD检测至OOD分类？

Method: Coarse-to-Fine open-set Classification (CFC) 框架，它利用大型语言模型（LLMs）来处理图数据集。CFC 包括三个主要组成部分：使用 LLM 提示进行 OOD 检测和离群值标签生成的粗略分类器；基于 GNN 的精细分类器，该分类器用粗略分类器识别出的 OOD 样本训练以增强 OOD 检测与 ID 分类能力；以及通过 LLM 提示和后处理 OOD 标签实现的改进 OOD 分类。

Result: 实验结果显示，在图和文本领域上，CFC相较于最先进的方法提高了10%的OOD检测效果，并且在图数据集上达到了最高70%的OOD分类准确率。

Conclusion: CFC框架不仅能够有效提升OOD检测的性能，而且还能在没有真实标签信息的前提下实现相对准确的OOD分类，为开放世界场景下的GNN部署提供了新的思路和技术支持。

Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.

</details>


### [36] [Sharpness-aware Federated Graph Learning](https://arxiv.org/abs/2512.16247)
*Ruiyu Li,Peige Zhao,Guangxia Li,Pengcheng Wu,Xingyu Gao,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦图学习算法SEAL，通过同时最小化损失函数及其尖锐度，并引入基于局部表示相关矩阵的正则项来缓解维度坍塌问题，从而提高了GNN模型在异构数据上的泛化能力和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦图学习（FGL）系统面临的主要挑战之一是客户端之间本地训练数据分布的差异性，即数据异质性问题。现有解决方案通常导致局部模型陷入尖锐的极小值点，削弱了它们对未知分布图数据的泛化能力；同时，学习到的局部图数据表示中普遍存在的维度坍塌现象也对GNN模型的分类能力产生了负面影响。

Method: 为了解决上述问题，研究者们提出了一个新颖的优化目标——考虑局部GNN模型尖锐度（即损失面曲率）的新目标。通过同时最小化损失函数和尖锐度，寻找位于平坦区域且具有均匀低损失值的模型参数，以此提高模型对异构数据的泛化性能。此外，还引入了一个基于局部表示相关矩阵的正则化项，旨在放松由个体局部图样本生成的表示之间的相关性，从而减轻学习模型中的维度坍塌问题。

Result: 实验研究表明，在多个图分类基准测试中，提出的SEAL算法相较于最新的FGL基线方法能够持续提供更高的分类准确性和更好的泛化能力，特别是在参与者数量增加时表现更佳。

Conclusion: SEAL算法有效地解决了联邦图学习中存在的数据异质性和维度坍塌问题，显著提升了GNN模型在处理大型现实世界图数据时的表现。

Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.

</details>


### [37] [Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data](https://arxiv.org/abs/2512.16277)
*Jialiang Wang,Xueyan Bao,Hao Wu*

Main category: cs.LG

TL;DR: 提出了Sharpness-aware SLF (SSLF)模型，通过Hessian-vector产品获取二阶信息，并在曲率中注入sharpness项，实验表明该模型优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 为了解决Second-order Latent Factor (SLF) 模型优化困难的问题，特别是在处理高维不完整数据时遇到的双线性和非凸性质挑战。

Method: 提出了一种新的Sharpness-aware SLF (SSLF) 模型，该模型利用Hessian-vector乘积来获取二阶信息，并通过设计的Hessian-vector乘积将一个sharpness项注入到曲率（Hessian）中。

Result: 在多个工业数据集上的实验表明，所提出的SSLF模型始终优于最先进的基线方法。

Conclusion: Sharpness-aware SLF (SSLF)模型成功解决了SLF模型中的优化难题，通过引入sharpness感知机制改进了从高维不完整数据中学习节点间交互模式的效果。

Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.

</details>


### [38] [Feature-Selective Representation Misdirection for Machine Unlearning](https://arxiv.org/abs/2512.16297)
*Taozhao Chen,Linghan Huang,Kim-Kwang Raymond Choo,Huaming Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的有原则的激活编辑框架——选择性表示误导以实现遗忘（SRMU），用于处理在高度纠缠分布场景下的模型遗忘问题。实验结果表明，SRMU 在最小化效用损失的同时提供了最先进的遗忘性能，并且在现有基线方法失效的情况下仍保持有效。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地应用于安全关键和受监管领域，保留敏感或禁止的知识引入了从隐私泄露到潜在滥用等不断上升的风险。虽然机器遗忘可以帮助确保部署的模型符合不断发展的法律、安全和治理要求，但当前的技术假设忘记与保留数据集之间有清晰的界限，这在实际操作中是具有挑战性的。因此，需要一种新方法来解决这个问题。

Method: 提出了选择性表示误导以实现遗忘（SRMU）框架，该框架通过特征感知和方向控制的扰动来强制执行。与无差别地扰动模型权重不同，SRMU 使用了一个带有激活重要性图的结构化误导向量，旨在允许SRMU 选择性地抑制有害表示同时保持良性表示的实用性。

Result: 实验是在广泛使用的WMDP基准上进行的，覆盖了低至高纠缠配置。实证结果显示，SRMU 在最小化效用损失的同时提供了最前沿的遗忘表现，并且即使在存在20-30%重叠的情况下也能保持有效性，而此时现有的基线方法已经失效。

Conclusion: SRMU为基于LLM的应用程序中的安全性驱动模型治理、隐私合规性和受控知识移除提供了一个强大的基础。

Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.

</details>


### [39] [Multivariate Uncertainty Quantification with Tomographic Quantile Forests](https://arxiv.org/abs/2512.16383)
*Takuya Kanazawa*

Main category: cs.LG

TL;DR: 提出了一种新的非参数、不确定性感知的树型回归模型——Tomographic Quantile Forests (TQF)，用于多变量目标。TQF能够学习条件分位数作为输入和单位方向的函数，并通过最小化切片Wasserstein距离来重构多元条件分布，具有单模型覆盖所有方向且不对凸性加以限制的优点。


<details>
  <summary>Details</summary>
Motivation: 为了安全和可信地部署现实世界中的AI，量化预测不确定性是必要的。然而，对于多变量目标而言，完全非参数估计条件分布仍然是一个挑战。

Method: 提出了Tomographic Quantile Forests (TQF)模型，它是一种非参数、能感知不确定性的基于树的回归方法，适用于多变量输出。TQF模型通过学习在给定输入x及单位方向n下的条件分位数$\mathbf{n}^{\top}\mathbf{y}$来进行工作。在推理时，该模型通过对多个方向上的分位数进行聚合，并通过最小化切片Wasserstein距离来重建多元条件分布。

Result: TQF能够在合成数据集与真实世界数据集中表现出色，相比于传统的方向-分位数方法只能生成凸形分位区域并且需要为不同方向训练不同的模型，TQF仅用单一模型即可覆盖所有方向，同时不施加任何凸性约束。

Conclusion: TQF提供了一种新颖的方法来处理多变量目标的预测不确定性问题，相较于现有技术，它不需要对每个方向单独训练模型，也不强制要求结果为凸形状，这使得其在实际应用中更加灵活有效。

Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.

</details>


### [40] [Emergent Bias and Fairness in Multi-Agent Decision Systems](https://arxiv.org/abs/2512.16433)
*Maeve Madigan,Parameswaran Kamalaruban,Glenn Moynihan,Tom Kempton,David Sutton,Stuart Burrell*

Main category: cs.LG

TL;DR: 本文探讨了多智能体系统在金融领域中的公平性评估问题，通过大规模模拟研究不同配置下的多智能体系统，揭示了这些系统中出现的集体偏见模式，并强调需要将多智能体决策系统作为一个整体来评估其公平性风险。


<details>
  <summary>Details</summary>
Motivation: 针对多智能体系统在诸如消费者金融等高风险领域的应用时面临的偏见风险难以评估的问题，提出开发适用于多智能体预测系统的公平性评估方法论的需求，以确保这类系统部署的安全性。

Method: 采用大规模仿真技术，在多种多智能体配置下（包括不同的通信与协作机制）检查公平性指标，以此探索金融决策制定过程中出现的偏见模式。

Result: 研究表明，多智能体系统在金融决策中可能表现出真正的集体行为特征，这种偏见无法简单地归因于单个智能体组件；这表明公平性风险是金融多智能体系统模型风险的重要组成部分，对信贷评分和收入估计等任务有实际影响。

Conclusion: 为了准确衡量多智能体预测系统的公平性特点及其潜在风险，应当将其视为一个整体来进行评估，而非仅仅分析各个组成部件。

Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.

</details>


### [41] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 本文将LDA模型中主题数量T的选择视为一个黑盒优化问题，通过比较四种优化方法（GA、ES、PABBO和SABBO）发现，虽然最终这些方法能达到相似的困惑度水平，但学习型优化器如PABBO和SABBO在样本效率和时间效率上显著优于传统的GA和ES。


<details>
  <summary>Details</summary>
Motivation: 选择合适的主题数量T对于LDA模型的统计拟合性和可解释性至关重要。本研究旨在通过将此过程构建为一个离散的黑盒优化问题来探索更有效的解决方案。

Method: 研究采用了遗传算法(GA)、进化策略(ES)以及两种学习型、摊销方法——偏好摊销黑盒优化(PABBO)与锐度感知黑盒优化(SABBO)，并通过固定评估预算下训练LDA模型并测量验证困惑度的方式进行比较。

Result: 实验结果表明，尽管GA、ES、PABBO及SABBO最终都能达到相似的最佳困惑度范围，但PABBO与SABBO在样本量需求及时效性方面远胜于GA和ES。特别地，SABBO几乎仅需一次评估即可确定接近最优的主题数量；而PABBO也能在几次评估后找到有竞争力的配置。相比之下，GA和ES则需要近乎整个预算才能接近同一水平。

Conclusion: 该研究表明，在选择LDA中的最佳主题数时，采用学习型摊销方法如PABBO和SABBO能够显著提高效率。这为未来相关领域的研究提供了新的思路。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [42] [Persistent Multiscale Density-based Clustering](https://arxiv.org/abs/2512.16558)
*Daniël Bot,Leland McInnes,Jan Aerts*

Main category: cs.LG

TL;DR: 提出了一种新的基于密度的聚类算法PLSCAN，该算法能够有效识别HDBSCAN*产生稳定簇的最小簇大小。PLSCAN在实际应用中表现优于HDBSCAN*，特别是在对互达邻居数量变化的敏感度更低，并且在低维数据集上具有与k-Means相竞争的运行时间。


<details>
  <summary>Details</summary>
Motivation: 在探索性数据分析（EDA）中，基于密度的聚类算法特别适用，因为它们仅假设存在某种密度即可描述高密度区域。然而，实际应用这些算法时需要选择合适的超参数，这在没有事先了解数据分布的情况下是困难的。比如DBSCAN需要选择一个密度阈值，而HDBSCAN*依赖于一个最小簇大小参数。

Method: 提出了持久叶子空间聚类应用于噪声（PLSCAN），这是一种新型的基于密度的聚类算法，能够高效地识别出所有HDBSCAN*产生稳定(叶子)簇所需的最小簇大小。PLSCAN运用了尺度空间聚类原则，在一种新的度量空间下等同于持久同调。

Result: PLSCAN与HDBSCAN*在多个真实世界的数据集上的性能进行了对比，表明它达到了更高的平均ARI（Adjusted Rand Index），并且对于互达邻居数量的变化不太敏感。此外，还将PLSCAN的计算成本与k-Means进行了比较，证明了它在低维数据集上的运行时间具有竞争力。在更高维度的数据集上，其运行时间的增长趋势更接近HDBSCAN*。

Conclusion: 通过引入PLSCAN，研究者为EDA提供了一种改进的方法来执行基于密度的聚类分析，该方法不仅提高了聚类结果的质量，还降低了对特定超参数设置的依赖性和敏感性。

Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.

</details>


### [43] [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)
*Barna Pásztor,Thomas Kleine Buening,Andreas Krause*

Main category: cs.LG

TL;DR: 提出了Stackelberg从人类反馈中学习（SLHF）的新框架，该框架通过两个策略之间的顺序博弈来解决偏好优化问题。与基于人类反馈的强化学习(RLHF)和纳什均衡学习(NLHF)相比，SLHF利用了顺序游戏的不对称性来捕捉更丰富的偏好结构，并且在实验中展示了跨不同偏好数据集的强大一致性以及模型家族间的推理时间改进无需进一步微调。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有方法如RLHF和NLHF在处理复杂偏好结构时的局限性，提出了一种新的偏好优化框架——SLHF，它能够更好地捕捉人类偏好的细微差别并通过顺序决策过程提高模型对人类偏好的适应能力。

Method: SLHF将偏好优化分解为跟随者的学习精炼问题和领导者对抗对手的优化问题，利用序列博弈的形式来体现这种优化过程。此外，SLHF允许在推理阶段进行精炼，即跟随者可以学习如何改进领导者的行动，并通过迭代抽样来利用这些改进。

Result: 实验结果表明，SLHF不仅能够在多种偏好数据集上实现强大的一致性，而且还能在参数规模从0.5亿到80亿之间良好扩展，并且所获得的推理时间改进可以在不同的模型族之间迁移而无需额外的微调。

Conclusion: SLHF作为一种新颖的方法，在处理复杂的偏好优化任务方面展现了显著优势，特别是在保持一致性、减少数据敏感性和增强对非传递性偏好的鲁棒性等方面。

Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

</details>


### [44] [Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario](https://arxiv.org/abs/2512.16648)
*Liu Yang,Qiang Li,Luxiong Wen,Jian Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为MS-SHOT的新方法，解决了无源数据跨接收器RFFI适应问题。该方法通过动量中心引导的软伪标签和强制全局结构约束来提高目标域性能，特别是在处理标签偏移或未知非均匀类分布时表现出色。实验表明，MS-SHOT在准确性和鲁棒性方面均优于现有方法，为RFFI中的无源数据跨接收器适应提供了一个实用且可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算的迅速普及，射频指纹识别（RFFI）对于安全设备认证变得越来越重要。然而，基于深度学习的RFFI模型在实际部署中面临一个关键挑战：当应用于具有不同硬件特性的接收器时，由于接收器变化引起的分布偏移，其性能通常会显著下降。为了解决这个问题，研究了无源数据跨接收器RFFI(SCRFFI)问题，旨在使预先训练好的模型能够适应来自目标接收器的未标记信号，而无需访问任何源域数据。

Method: 首先构建了一个新颖的基于约束伪标签的SCRFFI适应框架，并对其泛化性能进行了理论分析。分析揭示了一个关键见解：目标域性能高度依赖于适应过程中生成的伪标签质量。受此启发，提出了Momentum Soft伪标签Source Hypothesis Transfer (MS-SHOT)，一种新的SCRFFI方法，该方法结合了动量中心引导的软伪标签生成以及执行全局结构约束以鼓励自信且多样化的预测。

Result: 广泛的实验证明，在真实世界的数据集上，MS-SHOT在准确性和鲁棒性方面始终优于现有的方法。尤其值得注意的是，MS-SHOT有效解决了涉及标签偏移或目标域内未知、非均匀类别分布的情况——这是先前方法的一个重大局限性。

Conclusion: MS-SHOT提供了一种实用且可扩展的方法，用于解决RFFI领域中的无源数据跨接收器适应问题，特别是当面对标签偏移或未知非均匀类别分布时显示出了卓越的表现。

Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.

</details>


### [45] [Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification](https://arxiv.org/abs/2512.16687)
*Natnael Tilahun Sinshaw,Mengmei He,Tadesse K. Bahiru,Sudhir Kumar Mohapatra*

Main category: cs.LG

TL;DR: 本研究对比分析了几种广泛使用的机器学习算法和支持神经符号AI（NeSy）在文本分类问题上的表现，探讨了不同文本表示和特征提取技术的影响。实验结果表明，在有限的数据集上，NeSy方法的表现与强大的多层感知器（MLP）相当。


<details>
  <summary>Details</summary>
Motivation: 鉴于文本分类问题在市场分析、客户推荐等领域的广泛应用，本研究旨在通过对比分析多种机器学习算法及NeSy方法，探索最有效的解决方案。同时，也考察了不同的文本表示形式和特征提取技术对模型性能的影响。

Method: 研究采用了支持向量机(SVM)、朴素贝叶斯(NB)、逻辑回归(LR)、AdaBoost、XGBoost以及SVM的一个变体(SVM_R)等多种经典机器学习算法，并引入了神经符号AI(NeSy)进行对比。此外，还使用了TF-IDF、通用句子编码器(USE)、RoBERTa等文本表示方法，结合卡方检验、互信息、主成分分析等特征选择技术来评估模型效果。

Result: 实验结果显示，在处理有限数据集时，NeSy方法能够达到与多层感知器(MLP)相近的性能水平。这表明即使在数据不足的情况下，NeSy也可能提供一种有效的替代方案。

Conclusion: 研究表明，虽然传统机器学习方法仍然有效，但基于NeSy的方法即使在小样本条件下也能表现出色。未来的研究将着眼于扩展知识库、嵌入类型范围以及超参数配置，以进一步验证NeSy方法的有效性。

Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.

</details>


### [46] [CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies](https://arxiv.org/abs/2512.16700)
*John M. Statheros,Hairong Wang,Richard Klein*

Main category: cs.LG

TL;DR: 本文介绍了一种基于视觉变换器的模型CLARiTy，用于胸部X光片的多标签分类和弱监督定位。该模型通过使用类特定标记生成辨别注意力图，并利用SegmentCAM模块进行前景分割和背景抑制，从而在14种病理分类上达到了竞争性的性能，在8种病理定位上取得了最先进的弱监督定位性能。


<details>
  <summary>Details</summary>
Motivation: 胸部X光片（CXRs）的解读面临显著挑战，尤其是在实现准确的多标签病理分类和空间定位方面。这些任务需要不同层次的注释粒度，但通常受到区域级（密集）注释稀缺的限制。

Method: 提出了CLARiTy（Class Localizing and Attention Refining Image Transformer），一种基于视觉变换器的模型，用于胸部病灶的联合多标签分类与弱监督定位。该模型采用多个类特定标记来生成区分性注意力图，并使用了具有明确解剖学先验知识的SegmentCAM模块以进行前景分割和背景抑制。训练时仅使用来自NIH ChestX-ray14数据集的图像级别标签，并从ConvNeXtV2教师模型中提取信息以提高效率。

Result: CLARiTy-S-16-512配置在14种病理性状分类上表现出色，并且对于8种病理性状的弱监督定位表现领先于现有方法，提高了50.7%。特别是对小病灶如结节和肿块的识别有显著提升。其低分辨率变体CLARiTy-S-16-224在保持高效的同时明显超越了基线水平，显示出在资源有限环境中的应用潜力。

Conclusion: CLARiTy通过利用ViT自注意力机制获取全局上下文及类别特定定位，并结合卷积背景抑制技术产生精确、噪声减少的热图，推动了CNN-ViT混合模型的发展。

Abstract: The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.

</details>


### [47] [Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library](https://arxiv.org/abs/2512.16715)
*Oliver Stritzel,Nick Hühnerbein,Simon Rauch,Itzel Zarate,Lukas Fleischmann,Moike Buck,Attila Lischka,Christian Frey*

Main category: cs.LG

TL;DR: 本文提出了一种名为SPICE的Python框架，该框架重新实现了三种流行的基于深度学习的预测过程挖掘(PPM)方法，并设计了一个具有严格配置性的通用基础框架，以实现对过去和未来建模方法的可重复性和稳健比较。


<details>
  <summary>Details</summary>
Motivation: 当前许多预测过程挖掘（PPM）方法缺乏可再现性、决策透明度以及将新数据集纳入基准测试的能力，这使得不同实现之间的比较变得非常困难。

Method: 开发了名为SPICE的Python框架，在PyTorch中重新实现了三种流行的基于深度学习的PPM方法，并构建了一个高度可配置的基础框架来支持现有及未来模型间的公平对比。

Result: 通过与原始报告指标及在11个数据集上进行公平评估得到的结果对比表明，SPICE能够提供更加一致且可靠的性能评价标准。

Conclusion: SPICE作为一个开源工具，为研究者们提供了一个强大的平台来进行PPM领域内各种模型的有效比较，从而促进了该领域的进一步发展。

Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.

</details>


### [48] [Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation](https://arxiv.org/abs/2512.16718)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文提出了一种基于多调和样条包构建的级联架构，旨在解决机器学习回归问题中的可扩展性挑战，并且对于内在维度未知的问题具有理论上的合理性。还介绍了用于前向计算和端到端微分的有效矩阵过程。


<details>
  <summary>Details</summary>
Motivation: 直接应用先前提出的基于随机函数理论框架下的解决方案存在计算成本高（O(N^3)）及当输入空间维度过高时原理论假设失效的问题。因此，需要一种新的方法来同时提高算法的可扩展性和处理高维输入的能力。

Method: 通过构建由多调和样条组成的级联架构，这种方法不仅解决了规模性问题，而且对那些内在维度未知的问题提供了理论支持。此外，文中还提出了有效的矩阵操作方法，以实现级联结构内的前向计算以及端到端的区分。

Result: 所提出的级联架构能够有效减少计算复杂度，使得该方法在面对高维数据时更加实用。同时，通过引入高效的矩阵运算技术，进一步提高了模型训练与预测过程中的效率。

Conclusion: 这项研究为解决高维空间中机器学习回归问题提供了一个新的视角，即利用多调和样条构建级联架构的方法，在保持理论正确性的同时实现了良好的可扩展性。

Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.

</details>


### [49] [Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis](https://arxiv.org/abs/2512.16742)
*Wisnu Uriawan,Muhamad Veva Ramadhan,Firman Adi Nugraha,Hasbi Nur Wahid,M Dantha Arianvasya,Muhammad Zaki Alghifari*

Main category: cs.LG

TL;DR: 本研究针对印度尼西亚朝圣服务数字化过程中出现的假冒移动应用欺诈问题，通过实现和评估机器学习算法来自动验证应用程序的真实性。实验中使用了SVM、随机森林和朴素贝叶斯三种分类器，其中SVM表现最佳，准确率达到92.3%。


<details>
  <summary>Details</summary>
Motivation: 随着印度尼西亚朝圣（Hajj和Umrah）服务快速数字化，虽然极大地方便了朝圣者，但也为通过假冒移动应用进行数字欺诈提供了机会。这些欺诈性应用不仅导致经济损失，还严重威胁个人隐私安全，因为它们会收集敏感个人信息。

Method: 研究采用了一个综合数据集，包括在宗教事务部注册的官方应用以及在应用商店中流通的非官方应用，并比较了支持向量机(SVM)、随机森林(RF)和朴素贝叶斯(NB)这三种稳健分类器的表现。此外，研究还利用了一种结合文本分析(TF-IDF)与元数据分析的方法来进行特征提取。

Result: 实验结果显示，SVM算法表现最好，其准确率为92.3%，精确度为91.5%，F1分数为92.0%。进一步的特征分析表明，与合法性和高风险权限相关的特定关键词是最具区分性的特征。

Conclusion: 提出的系统作为一种主动且可扩展的解决方案，旨在提高宗教旅游部门中的数字信任水平，并可能作为国家验证系统的原型。

Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.

</details>


### [50] [NRGPT: An Energy-based Alternative for GPT](https://arxiv.org/abs/2512.16762)
*Nima Dehmamy,Benjamin Hoover,Bishwajit Saha,Leo Kozachkov,Jean-Jacques Slotine,Dmitry Krotov*

Main category: cs.LG

TL;DR: 本文提出了一种将GPT架构与基于能量的建模(EBM)框架统一起来的方法，通过在能量景观上探索标记来实现推理步骤。尽管这种探索在某些情况下会变成梯度下降但不一定会产生最佳性能模型，该方法在简单语言、代数ListOPS任务和更复杂的OpenWebText语言建模等场景中表现良好，并且可能更能抵抗过拟合。


<details>
  <summary>Details</summary>
Motivation: 作者旨在结合GPT架构与基于能量的建模（EBM）范式，以期在保持GPT优势的同时引入EBM的独特视角，即把推理视为在能量景观上的动态过程。

Method: 提出了eNeRgy-GPT (NRGPT) 模型，它对GPT设置进行了最小修改以便与EBM框架统一。此模型中的推理步骤被设想为在能量景观上探索标记的过程。

Result: 研究表明，在特定条件下，这种探索可以转化为梯度下降，而且NRGPT在简单的语言数据集（如莎士比亚数据集）、代数ListOPS任务以及更复杂的环境例如OpenWebText语言建模中都表现出色。此外，观察到这些模型可能更能抵抗长时间训练期间的过拟合现象。

Conclusion: 本研究成功地将GPT架构与EBM框架结合起来，形成了新的NRGPT模型，该模型不仅能够有效处理多种类型的语言任务，还显示出较强的抗过拟合能力，为未来的研究提供了新的方向。

Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.

</details>


### [51] [Pattern recognition in complex systems via vector-field representations of spatio-temporal data](https://arxiv.org/abs/2512.16763)
*Ingrid Amaranta Membrillo Solis,Maria van Rossem,Tristan Madeleine,Tetiana Orlova,Nina Podoliak,Giampaolo D'Alessandro,Jacek Brodzki,Malgosia Kaczmarek*

Main category: cs.LG

TL;DR: 本文提出了一种基于离散测度空间上向量场理论的几何框架，用于分析复杂系统的时空数据。通过引入适合数据分析和机器学习应用的两参数度量族，该框架能够处理时间依赖图像、图像梯度以及定义在图和单纯复形上的实值或向量值函数。经验证，所提方法结合多维缩放技术，在降维、模式分解、相空间重构及吸引子特征描述等方面表现出色，为理解特别是传统建模不切实际但实验数据丰富的复杂动力系统提供了一个强有力的途径。


<details>
  <summary>Details</summary>
Motivation: 由于复杂系统展现出高维度非线性动态特性，给模型建立、分类与预测带来极大挑战。尽管信息技术的进步使得数据驱动的研究方法成为可能，但时空数据的庞大体积和复杂性往往限制了传统方法如降维、相空间重建及吸引子特征化的效果。因此，需要一种新的框架来更有效地分析这些数据。

Method: 文章介绍了一种基于离散测度空间上的向量场理论的新几何框架，设计了一套适用于数据分析和机器学习应用的两参数度量家族。该框架能够支持对时间相关图像、图像梯度及定义于图表和单纯复形上的实值或向量值函数的操作。

Result: 通过生物和物理系统在平坦与弯曲域上的数值模拟数据验证表明，所提出的度量标准与多维尺度相结合，在解决关键分析难题方面非常有效，包括实现降维、模式分解、相空间重建以及吸引子特征化。

Conclusion: 研究结果表明，所提出的几何框架及其度量标准为理解和分析复杂动态系统提供了有力工具，特别是在那些难以用传统建模方法处理但拥有大量实验数据的情境下。

Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.

</details>


### [52] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 本文提出了一种名为知识转换(KT)的方法，该方法结合了知识蒸馏、主动学习和因果推理，用于在线边缘机器学习中为未来未见的数据点确定标签。实验结果表明，在给定一个稳定的教师模型时，学生模型能够达到预期的最大性能。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘机器学习方法通常假设模型是在中心训练后部署的，这使得它们在面对未见数据时不够有效。为了应对这一挑战，在线边缘机器学习允许直接在边缘设备上训练并持续更新模型。本文旨在解决在线边缘机器学习中的一个关键问题：如何为真正未来的、未见的数据点确定标签。

Method: 提出了一种名为知识转换（KT）的混合方法，它结合了知识蒸馏、主动学习以及因果推理技术。简而言之，KT通过将来自教师模型的知识转化来生成伪标签，扮演着主动学习中oracle的角色，以此来训练学生模型。

Result: 通过模拟实验验证了该方法的有效性，实验设置了两种情况：(1)使用一个不太稳定的教师模型；(2)使用相对更稳定的教师模型。结果显示，当提供一个稳定的教师模型时，学生模型最终能够达到其预期的最大表现。

Conclusion: 知识转换（KT）方法对于满足以下条件的情景特别有益：(1) 教师的任务是通用的，意味着现有的预训练模型可能足够完成其任务，因此不需要从头开始训练教师模型；和/或 (2) 获取学生任务所需的标签既困难又昂贵。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


### [53] [Sequencing to Mitigate Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2512.16871)
*Hesham G. Moussa,Aroosa Hameed,Arashmid Akhavain*

Main category: cs.LG

TL;DR: 本文提出了一种通过优化任务顺序来缓解持续学习中灾难性遗忘问题的方法，该方法借鉴了神经架构搜索中的零样本评分算法，并且实验结果显示这种方法可以显著减少遗忘现象，同时与传统持续学习策略结合时能够提高性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了应对现实世界的动态变化，智能系统需要在其生命周期内不断地获取、更新和利用知识。持续学习为此提供了一个基础，但其面临的主要挑战是灾难性遗忘，即在学习新任务时会导致先前学过的任务性能急剧下降。

Method: 本研究从一个新的角度出发，考虑了任务呈现给模型时的最佳顺序安排。研究了任务排序在减轻灾难性遗忘方面的作用，并提出了确定最佳任务顺序的方法。所提方法利用了受神经架构搜索启发的零样本评分算法。

Result: 结果表明，智能的任务排序可以大幅度减少灾难性遗忘。而且，当与传统的持续学习策略相结合时，排序提供了增强的性能和对遗忘的抵抗力。此外，所提出的方法还可以应用于其他领域，如课程学习。

Conclusion: 通过优化任务的学习顺序，可以有效缓解持续学习中的灾难性遗忘问题，为开发更加自适应的AI系统奠定了基础。

Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.

</details>


### [54] [Impacts of Racial Bias in Historical Training Data for News AI](https://arxiv.org/abs/2512.16901)
*Rahul Bhargava,Malene Hornstrup Jespersen,Emily Boardman Ndulue,Vivica Dsouza*

Main category: cs.LG

TL;DR: 该论文探讨了基于《纽约时报》注释语料库训练的多标签分类器中出现的“blacks”主题标签问题，通过可解释的人工智能方法分析发现，该标签部分作为跨少数群体的“种族主义检测器”，但在现代实例如反亚裔仇恨故事和黑人命也是命运动报道上表现不佳。研究揭示了新闻编辑室在采用AI工作流程工具时如何减少历史偏见再现的风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术迅速应用于涉及大量文本数据的企业和研究领域，这些基于现有数据训练的模型可能编码了几十年前的态度和刻板印象。本研究旨在调查一个基于广泛使用的《纽约时报》注释语料库训练出的多标签分类器中的“blacks”主题标签问题，并探究其对模型使用的影响。

Method: 通过定量与定性手段结合可解释的人工智能方法，研究者们探索了“blacks”标签在训练语料库中的使用情况、它在训练后的分类器中可能编码的概念及其对模型应用的具体影响。

Result: 研究发现，“blacks”标签在一定程度上充当了针对某些边缘化群体的一般‘种族主义探测器’角色，但面对像新冠疫情期间针对亚裔的仇恨事件以及关于‘黑人的命也是命’运动报道等现代例子时，其表现未能达到预期。

Conclusion: 这项案例研究表明，在新闻编辑室环境中类似应用可能导致意外输出，进而影响大型语言模型的各种潜在用途，比如故事发现、受众定位、摘要生成等。这暴露了一个根本性的矛盾：新闻编辑室如何在采纳AI赋能的工作流工具的同时降低重现历史偏见的风险。

Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.

</details>


### [55] [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)
*Andrew Wagenmaker,Perry Dong,Raymond Tsao,Chelsea Finn,Sergey Levine*

Main category: cs.LG

TL;DR: 本文探讨了预训练策略对微调性能的影响，并提出了一种新的方法——后验行为克隆（PostBC），该方法在确保覆盖演示者动作的同时，能够提供比标准行为克隆更有效的初始化，从而提高强化学习微调的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解预训练策略如何影响微调表现，并探索怎样预训练策略可以使其成为有效微调的良好起点。当前，虽然很多工作集中在开发更高效的微调算法上，但对于保证预训练策略作为良好起始点的研究相对较少。

Method: 首先从理论上证明了标准行为克隆可能无法充分覆盖演示者的行动空间，这不利于后续的RL微调。为解决这一问题，作者提出通过训练一个模型来模拟给定演示数据集下演示者行为的后验分布，而非直接拟合观察到的演示。这种新方法被称为后验行为克隆(PostBC)。

Result: 实验结果表明，在机器人控制领域中使用现代生成模型实现PostBC不仅可行，而且相较于传统的行为克隆，在现实世界中的机器人操控任务以及实际机器人控制基准测试中都展现出了显著更好的RL微调性能。

Conclusion: 通过采用后验行为克隆方法进行预训练，可以得到更适合于后续强化学习微调的初始策略，同时保持不低于传统行为克隆方法的预训练性能。

Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

</details>


### [56] [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912)
*Peter Chen,Xiaopeng Li,Ziniu Li,Wotao Yin,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: 本文探讨了具有可验证奖励的强化学习（RLVR）中探索-利用权衡对大型语言模型（LLMs）推理能力的影响。研究发现，虚假奖励通过减少策略熵来提高输出的信心和确定性，而仅仅最小化熵不足以改善性能。提出了一种奖励错位模型来解释为什么虚假奖励可以在污染环境中提升性能。这些发现为更有效的RLVR训练提供了原则。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解为何抑制探索和利用反而能提高大型语言模型的数学推理能力，并澄清虚假奖励背后的工作机制以及它们如何与剪辑偏差及模型污染相互作用来提升性能。

Method: 本研究关注两个核心问题：(i) 策略熵与表现之间的关系；(ii) 虚假奖励是否通过剪辑偏差和模型污染之间的相互作用带来收益。通过对这两个方面的深入分析，提出了一个奖励错位模型以解释观察到的现象。

Result: 结果显示，在虚假奖励下，剪辑偏差减少了策略熵，促使模型生成更加自信且确定性的输出；单独依赖于熵最小化并不能有效改善性能。此外，研究还揭示了虚假奖励即使在非污染情况下也能增强性能的原因。

Conclusion: 研究阐明了虚假奖励在RLVR中的积极作用机制，并为未来设计更高效的RLVR训练方案提供了指导原则。

Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [57] [DP-Bench: A Benchmark for Evaluating Data Product Creation Systems](https://arxiv.org/abs/2512.15798)
*Faisal Chowdhury,Sola Shirai,Sarthak Dash,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.DB

TL;DR: 本文介绍了一个名为DP-Bench的基准测试，用于评估自动生成数据产品的性能，并提出了一些基于大语言模型的方法作为基线。


<details>
  <summary>Details</summary>
Motivation: 针对目前缺乏对自动生成数据产品进行评估的基准这一问题，研究者们开发了首个专门为此任务设计的基准测试工具DP-Bench。

Method: 通过利用现有的ELT（提取-加载-转换）和Text-to-SQL基准测试资源创建了DP-Bench，并提出了几种基于大型语言模型的方法来自动产生数据产品。

Result: 成功构建了一个新的基准DP-Bench以及一些基于大型语言模型的自动生成数据产品的基线方法。

Conclusion: DP-Bench为自动生成数据产品领域提供了一个重要的评价标准，同时所提出的基于大型语言模型的方法也为未来的研究提供了有价值的参考。

Abstract: A data product is created with the intention of solving a specific problem, addressing a specific business usecase or meeting a particular need, going beyond just serving data as a raw asset. Data products enable end users to gain greater insights about their data. Since it was first introduced over a decade ago, there has been considerable work, especially in industry, to create data products manually or semi-automatically. However, there exists hardly any benchmark to evaluate automatic data product creation. In this work, we present a benchmark, first of its kind, for this task. We call it DP-Bench. We describe how this benchmark was created by taking advantage of existing work in ELT (Extract-Load-Transform) and Text-to-SQL benchmarks. We also propose a number of LLM based approaches that can be considered as baselines for generating data products automatically. We make the DP-Bench and supplementary materials available in https://huggingface.co/datasets/ibm-research/dp-bench .

</details>


### [58] [Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive](https://arxiv.org/abs/2512.15815)
*Valeria Granata,Francois Liot,Xing Wang,Steen Lysgaard,Ivano E. Castelli,Tejs Vegge,Nicola Marzari,Giovanni Pizzi*

Main category: cs.DB

TL;DR: 本文介绍了BIG-MAP Archive，一个基于云的、学科性的私有存储库，旨在解决大型联盟中的数据共享问题。它通过提供可扩展架构、用户友好界面和强大的安全及访问控制来促进合作与确保敏感数据的安全访问。该档案库支持灵活的权限访问，并易于为其他联盟重新部署。


<details>
  <summary>Details</summary>
Motivation: 在大型联盟中实现数据共享面临着组织和技术上的挑战。为了促进合作、方便发现交流以及确保对敏感数据的安全访问，需要一个通用平台来解决这些问题。

Method: 创建了BIG-MAP Archive，这是一个基于InvenioRDM构建的云端私有存储库，专为满足联盟特定需求而设计。它提供了细粒度权限管理的数据和元数据上传功能，允许项目成员或整个倡议的访问。此外，还实现了正式化的上传流程以确保数据格式正确且准备好发布到开放存储库。

Result: BIG-MAP Archive成功地解决了大型联盟内部的数据共享难题，不仅保障了数据保密性也支持基于权限的灵活访问方式。而且，此解决方案还可以轻松地应用于其他类似联盟如MaterialsCommons4.eu 和 RAISE等。

Conclusion: BIG-MAP Archive为大型联盟内安全可控的数据共享提供了一个有效的解决方案，同时其灵活性和安全性使其能够适应不同场景下的需求。

Abstract: Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).

</details>


### [59] [Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers](https://arxiv.org/abs/2512.16083)
*Thanh Dat Hoang,Thanh Tam Nguyen,Thanh Trung Huynh,Hongzhi Yin,Quoc Viet Hung Nguyen*

Main category: cs.DB

TL;DR: 本文提出了一种新的框架	oolname，用于优化Text2SQL系统在处理大规模数据库时的表现。该框架通过使用查询感知的LLM编码器、轻量级图转换器以及Steiner-tree启发式方法来筛选和压缩模式，从而提高了召回率和精确度，同时保持了亚秒级的中位延迟，并能够扩展到包含超过23,000列的模式。


<details>
  <summary>Details</summary>
Motivation: 现有的Text2SQL系统在处理小型数据库时表现良好，但在面对现实世界中庞大且复杂的模式（如Spider 2.0基准测试所展示的）时失效。当前解决此问题的方法要么依赖于成本高昂的多步骤提示流程，要么通过独立地根据用户问题对列进行排名而忽略了列之间的结构关系。

Method: 	oolname是一种开源的、高效的模式过滤框架，它通过以下方式压缩Text2SQL提示：(i) 使用增强有值和元数据的查询感知LLM编码器对列进行排名；(ii) 通过功能依赖上的轻量级图转换器重新排列相互连接的列；(iii) 利用Steiner树启发式选择一个保持连通性的子模式。

Result: 实验表明，	oolname在实际数据集上实现了接近完美的召回率，并且比CodeS、SchemaExP、Qwen重排序器以及嵌入检索器具有更高的精度。此外，它还保持了亚秒级别的中位延迟，并能扩展至包含超过23,000列的模式。

Conclusion: 本研究提出的	oolname为Text2SQL系统提供了有效的解决方案，以应对大规模数据库带来的挑战，不仅改善了性能指标，而且维持了良好的计算效率。

Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.

</details>


### [60] [ModelTables: A Corpus of Tables about Models](https://arxiv.org/abs/2512.16106)
*Zhengyuan Dong,Victor Zhong,Renée J. Miller*

Main category: cs.DB

TL;DR: 提出了ModelTables，一个模型湖中表格的基准，捕捉了性能和配置表的结构化语义。该数据集由Hugging Face模型卡、GitHub README和参考论文构建而成，覆盖超过60,000个模型和90,000张表格，并通过三种互补信号来评估模型和表格的相关性。研究展示了基于此基准的一个广泛实证用例——表格搜索，并比较了几种不同的检索方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前对于AI模型的结构化描述缺乏一个大规模的基准，特别是针对模型性能和配置信息的表格形式的数据。此外，仅依赖文本检索会忽略这些表格所包含的重要结构化语义。因此，作者旨在创建一个能够更好地支持模型湖中表格搜索及语义检索的数据集。

Method: 从Hugging Face模型卡、GitHub README以及引用的论文中构建了一个名为ModelTables的数据集。为了评估模型与表格间的关联性，采用了三种互补的信息源：论文引用链接、明确的模型卡片链接与继承关系、共享训练数据集。利用这个基准，作者还探索了多种数据湖搜索操作符（如可联合、可连接、关键词搜索）和信息检索基线方法（包括密集型、稀疏型和混合检索）在表格搜索中的表现。

Result: 实验结果显示，基于联合的语义表格检索整体上达到了54.8%的P@1精度（在引用、继承和共享数据集信号上分别为54.6%、31.3%、30.6%）。而基于表格的密集检索则达到66.5% P@1精度，元数据混合检索实现了54.1% P@1精度。这些结果表明存在改进表格搜索方法的空间。

Conclusion: 通过发布ModelTables及其创建协议，提供了首个大规模描述AI模型的结构化数据基准。这一工作不仅为模型湖中的表格发现提供了直观的理解和证据，也为开发更精确的语义检索、结构化比较以及有原则地组织结构化模型知识指明了方向。

Abstract: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.

</details>


### [61] [Subset Sampling over Joins](https://arxiv.org/abs/2512.16321)
*Aryan Esmailpour,Xiao Hu,Jinchao Huang,Stavros Sintos*

Main category: cs.DB

TL;DR: 本文提出了针对连接结果的子集采样的首个高效算法，包括静态索引、一次性算法和动态索引，这些技术在输入大小和预期样本大小方面实现了接近最优的时间和空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统的从显式列表中进行子集采样已经被充分理解，但现代应用（如关系数据上的机器学习）往往需要从通过关系连接定义的集合中采样。由于连接结果可能远大于输入数据，直接实现所有连接结果来进行子集采样是计算上不可行的。

Method: 本文介绍了三种处理连接上子集采样的方法：一种用于生成多个独立子集样本的静态索引；一种用于生成单个子集样本的一次性算法；以及一种支持元组插入同时维护一次性样本或生成多个独立样本的动态索引。

Result: 所提出的技术在处理与输入大小和期望样本大小相关的任务时，在时间和空间复杂度方面表现出了接近最优的性能。

Conclusion: 本文开发了适用于非循环连接的子集采样问题的首批高效算法，并为未来研究提供了坚实的基础。

Abstract: Subset sampling (also known as Poisson sampling), where the decision to include any specific element in the sample is made independently of all others, is a fundamental primitive in data analytics, enabling efficient approximation by processing representative subsets rather than massive datasets. While sampling from explicit lists is well-understood, modern applications -- such as machine learning over relational data -- often require sampling from a set defined implicitly by a relational join. In this paper, we study the problem of \emph{subset sampling over joins}: drawing a random subset from the join results, where each join result is included independently with some probability. We address the general setting where the probability is derived from input tuple weights via decomposable functions (e.g., product, sum, min, max). Since the join size can be exponentially larger than the input, the naive approach of materializing all join results to perform subset sampling is computationally infeasible. We propose the first efficient algorithms for subset sampling over acyclic joins: (1) a \emph{static index} for generating multiple (independent) subset samples over joins; (2) a \emph{one-shot} algorithm for generating a single subset sample over joins; (3) a \emph{dynamic index} that can support tuple insertions, while maintaining a one-shot sample or generating multiple (independent) samples. Our techniques achieve near-optimal time and space complexity with respect to the input size and the expected sample size.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [62] [XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies](https://arxiv.org/abs/2512.15804)
*Balreet Grewal,James Graham,Jeff Muizelaar,Jan Honza Odvarko,Suhaib Mujahid,Marco Castelluccio,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 研究介绍了一种名为XBIDetective的工具，该工具利用视觉语言模型（VLMs）自动检测跨浏览器不一致性（XBIs）。通过在Mozilla Firefox和Google Chrome中抓取网站截图，并使用VLM进行分析，该工具能够以79%的准确率识别跨浏览器差异，并且对于动态元素和广告的检测分别达到了84%和85%的准确率。此外还讨论了从工业实践中获得的经验教训以及XBIDetective的一些潜在实用案例。


<details>
  <summary>Details</summary>
Motivation: 浏览器渲染错误很难被开发者发现，因为这些错误可能只会在极少数特定条件下出现。而跨浏览器不一致性可以作为检测这类渲染问题的一个有用指引。现有的基于视觉和DOM的分析技术在处理动态和交互式元素时往往遇到困难。

Method: 开发了XBIDetective工具，它能够在Mozilla Firefox和Google Chrome中自动捕获网站截图，并使用视觉语言模型(VLM)来分析这些图片以识别出跨浏览器不一致性(XBIs)。评估了现成VLM与微调后VLM的效果，通过对1,052个网站的数据集进行测试。

Result: XBIDetective工具在使用微调后的VLM时，能够以79%的准确率识别跨浏览器差异；对动态元素的检测准确率为84%，对广告的检测准确率为85%。

Conclusion: XBIDetective提供了一种有效的方法来自动检测跨浏览器不一致性，特别是对于那些难以通过传统方法捕捉到的问题。此工具展示了在自动化回归测试、大规模网站监控及快速分类XBI错误报告等方面的潜在应用价值。

Abstract: Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.

</details>


### [63] [OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering](https://arxiv.org/abs/2512.15979)
*Mia Mohammad Imran,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型(LLM)的注释框架(OLAF)，旨在提高软件工程研究中使用LLM进行自动化或辅助注释任务时的透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被用于经验软件工程中的自动化或辅助注释任务，这些注释的可靠性和可重复性尚未得到充分探索。现有研究往往缺乏对可靠性、校准和漂移的标准测量，并且经常忽略关键配置细节。

Method: 文章提出了一个概念框架——基于大语言模型的注释操作化框架(OLAF)，该框架围绕着几个核心概念组织：可靠性、校准、漂移、共识、聚合和透明度。

Result: 通过提出OLAF框架，作者们期望能够激发方法论上的讨论以及未来实证工作，以促进软件工程研究领域内基于大语言模型的注释更加透明和可复现。

Conclusion: 本文认为应该将基于大语言模型的注释视为一种度量过程而非纯粹的自动化活动，并希望通过提出OLAF框架来推动这一领域的进一步发展。

Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.

</details>


### [64] [Embedding Software Intent: Lightweight Java Module Recovery](https://arxiv.org/abs/2512.15980)
*Yirui He,Yuqi Huai,Xingyu Chen,Joshua Garcia*

Main category: cs.SE

TL;DR: 本文提出了一种名为ClassLAR的新方法，该方法通过完全限定的类名从单体Java系统中恢复Java模块，并在20个流行的Java项目上进行了评估，显示出比现有技术更好的架构级相似性度量和更快的执行时间。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统的规模不断扩大，仅依靠代码层面的抽象变得不切实际。虽然架构抽象提供了一种管理这些系统的方法，但保持它们与实际代码的一致性一直是个问题。Java 9引入的Java平台模块系统（JPMS）解决了这个问题，但是现有的架构恢复技术对于将现有的单体项目模块化为JPMS模块来说效果不佳。

Method: 提出了ClassLAR（基于类和语言模型的架构恢复），这是一种新颖、轻量且高效的方法，它使用完全限定的类名从单体Java系统中恢复Java模块。ClassLAR利用语言模型从包名和类名中提取语义信息，捕捉结构和功能意图。

Result: 在对20个流行Java项目的评估中，ClassLAR在架构级别的相似性度量方面优于所有最先进的技术，同时实现了3.99到10.50倍的更快执行时间。

Conclusion: ClassLAR方法为解决从大型Java系统中恢复模块的问题提供了有效途径，显著提高了恢复效率和准确性。

Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.

</details>


### [65] [LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)](https://arxiv.org/abs/2512.16070)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本研究探讨了大型语言模型（LLMs）在多目标性能建模中的采样效果，通过设计并实现基于反馈的框架LLM4Perf，在四个高度可配置的真实系统中进行评估。结果表明，LLM引导的方法在大多数情况下优于传统基线，并且在约68.8%的评估场景中达到最佳性能。此外，研究还展示了LLM4Perf内组件及超参数的选择如何影响其有效性。


<details>
  <summary>Details</summary>
Motivation: 软件系统的性能严重依赖于复杂的配置选项，而建立准确的性能模型需要有效的采样策略。然而，现有的方法往往难以处理多目标优化问题，也无法利用文档中的语义信息。鉴于大型语言模型（LLMs）近期的成功，研究者提出一个核心问题：LLMs能否作为多目标性能建模的有效采样器？

Method: 提出了一个名为LLM4Perf的基于反馈的框架，该框架被用来系统地评估跨四个真实世界、高度可配置系统中的LLM引导采样过程。

Result: 实验结果显示，LLM引导的方法在大部分情况下优于传统基线，具体而言，在总共112个评估场景中有77个达到了最佳性能。此外，对于448种情况中的410种，这种剪枝方法也提高了基线方法的表现。

Conclusion: 论文提供了强有力的证据支持LLMs在性能工程中的有效性，并对推动其成功的机制提供了具体的见解。

Abstract: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

</details>


### [66] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 本文对2015年至2025年间发布的42项关于Apache Kafka的研究进行了结构化综合分析，归纳出九种常见的设计模式，并指出当前研究中存在的配置公开、评估严格性和可重复性方面的问题。通过提供统一的分类法、模式基准矩阵以及实用决策启发式方法，为架构师和研究人员提供了指导。


<details>
  <summary>Details</summary>
Motivation: 尽管Apache Kafka已经成熟并得到广泛应用，但关于可重用架构设计模式和可重复基准测试方法的研究在学术和工业出版物中仍然分散。本研究旨在填补这一空白，通过系统地整理现有文献来识别常见的Kafka设计模式，并解决当前研究中存在的一致性和可重复性问题。

Method: 通过对2015年至2025年期间发表的42篇同行评审论文进行结构化综合分析。该分析涵盖了九种反复出现的Kafka设计模式及其共同使用趋势、特定领域部署情况，以及采用标准套件（如TPCx Kafka和Yahoo Streaming Benchmark）与自定义工作负载进行的经验性基准测试实践。

Result: 确定了九种常见的Kafka设计模式；揭示了配置披露、评估严格性和再现性方面的重大不一致，这些问题限制了跨研究比较和实际复制能力。提出了统一的分类体系、模式基准矩阵及实用决策指南。

Conclusion: 这项工作为设计可重复性高、性能优越且容错性强的基于Kafka的事件流处理系统提供了有价值的见解和实用建议。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [67] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LaaJ）在COBOL代码生成中的应用限制，并开发了一个轻量级分析检查工具来辅助LaaJ提高错误检测率。结合使用时，能够显著提升评估准确性，达到最高94%的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型作为判断者（LaaJ）越来越多地被部署到代码生成流程中，它们在处理特定领域问题时的局限性引发了对其执行关键评估任务可靠性的担忧。为了更深入理解这些局限性，本研究通过一个具体的工业案例——即通过COBOL代码生成实现遗留代码现代化——来考察LaaJ的行为表现。

Method: 首先分析由LaaJ评判的生成COBOL程序及其相关评价，利用专家知识构建初步分类体系；基于此开发出一个轻量级分析检查工具，用于标记实践中观察到的30多个领域特有问题。该工具的输出作为分析提示，动态注入到评判提示中以促使LaaJ重新审视可能忽视的部分。

Result: 实验表明，单独使用LaaJ仅能检测约45%存在的错误；而单独使用分析检查工具则缺乏解释深度。但当两者结合时，在最佳配置下可以达到高达94%的覆盖率，并产生质量更高、更加准确的解释。

Conclusion: 分析-LLM混合方法能够在实际部署的管道中大幅提高评估可靠性。此外，研究团队还公开了所使用的数据集和所有提示。

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [68] [Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation](https://arxiv.org/abs/2512.16335)
*Yibiao Yang,Qingyang Li,Maolin Sun,Jiangchang Wu,Yuming Zhou*

Main category: cs.SE

TL;DR: 本文对比了基于BIC的策略（Basic）与基于SBFL的技术在编译器故障定位中的效果，发现Basic方法在很多情况下表现优于或等同于最先进的基于SBFL的方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对编译器错误隔离提出了许多复杂的基于频谱的故障定位(SBFL)技术，但其有效性尚未与实践中广泛采用的基于bug诱导提交(BIC)策略进行比较评估。

Method: 本研究通过识别最近的好版本和最早的坏版本，并使用二分查找法确定导致bug的提交，将该提交中修改的所有文件标记为潜在的错误文件。接着，利用包含60个GCC bug和60个LLVM bug的基准测试集对Basic方法与基于SBFL的技术进行了严格比较。

Result: 分析显示，Basic方法在关键的Top-1和Top-5排名指标上表现出色，很多时候甚至优于最先进的基于SBFL的方法。

Conclusion: 研究表明，在实际编译器调试场景中，基于SBFL的技术的有效性方面提供了新的见解，并建议未来的研究在开发和评估新的编译器故障隔离方法时采用Basic作为基线。

Abstract: Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.

</details>


### [69] [An Empirical Study of the Realism of Mutants in Deep Learning](https://arxiv.org/abs/2512.16741)
*Zaheed Ahmed,Philip Makedonski,Jens Grabowski*

Main category: cs.SE

TL;DR: 本研究首次实证比较了深度学习中预训练和后训练变异方法在现实性方面的差异，发现预训练变异体与真实故障的耦合强度更高、行为相似度更大，但其计算成本也更高。


<details>
  <summary>Details</summary>
Motivation: 突变分析是通过向程序注入人工故障来评估传统软件开发范式下测试质量的一种成熟技术。它在深度学习（DL）中的应用已经扩展到支持故障定位、修复、数据生成以及模型鲁棒性评估等任务。然而，关于突变体是否像在传统软件系统中一样能够很好地模拟真实故障这一假设，在DL领域尚未得到充分验证。

Method: 本研究提出了一个统计框架，用于量化预训练和后训练两种DL变异方法与真实故障之间的耦合强度及行为相似度，并使用公开可用的错误数据集CleanML、DeepFD、DeepLocalize和defect4ML进行实验。

Result: 结果表明，与后训练变异相比，预训练变异展现出更强的耦合性和更高的行为相似度，意味着更好的现实性；但是，预训练变异显著的计算成本也指出了需要开发更有效的后训练操作符以匹配或超越预训练变异所展示出的真实性的需求。

Conclusion: 预训练变异体比后训练变异体具有更强的现实性，但由于其较高的计算成本，未来的研究应该聚焦于开发更加高效且同样真实的后训练变异方法。

Abstract: Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.
  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.
  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.

</details>


### [70] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 本研究首次从概念层面探讨了大型语言模型在处理软件工程任务时如何依赖和内部化代码注释。通过激活或停用这些注释概念，观察到对不同任务性能的影响显著且具有模型特异性。此外，实验表明代码总结任务最能激活注释概念的使用，而代码补全则对此最为不敏感。


<details>
  <summary>Details</summary>
Motivation: 尽管代码注释是非功能性元素，但大型语言模型（LLM）经常依赖它们来执行软件工程任务。然而，这种依赖性具体存在于模型的哪个部分以及它如何影响性能仍不清楚。

Method: 采用概念激活向量(CAV)方法分析了三个软件工程任务：代码补全、翻译和细化，并通过系统地激活和停用嵌入空间中的这些概念来观察性能变化。

Result: 发现大型语言模型不仅将注释内化为独立的潜在概念，还能区分Javadoc、行内及多行注释等子类型；并且，当激活或停用这些概念时，对于不同任务的性能影响范围可以从-90%到+67%。

Conclusion: 研究表明，通过理解和操纵内部概念表示而非仅仅依赖表面层次输入，可以为构建更有效的软件工程工具和模型开辟新方向。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [71] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文提出了一种新的测试框架CAFFE，用于评估大型语言模型（LLMs）的反事实公平性。该框架通过明确定义组件来正式化公平性测试案例，并能自动生成目标测试数据及使用语义相似度指标评估模型响应。实验表明，与现有的变体测试方法相比，CAFFE能够实现更广泛的偏见覆盖和更可靠地检测不公平行为。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现代软件系统中扮演越来越重要的角色，对于这些模型公平性的担忧也日益增加。先前的工作已经提出了变形测试来检测公平性问题，但需要一种更加结构化且意图感知的方法来进行反事实公平性的测试。

Method: 本文介绍了一种名为CAFFE的新视角，它为测试大型语言模型中的反事实公平性提供了一个结构化和意图感知的框架。CAFFE通过明确定义包括提示意图、对话上下文、输入变量、预期公平阈值以及测试环境配置在内的组件来正式化LLM-公平性测试案例；辅助测试者自动生成有针对性的测试数据；并利用语义相似度指标来评估模型响应。

Result: 实验在三个不同架构家族的大型语言模型上进行，结果表明CAFFE比现有变形测试方法能够达到更广的偏见覆盖范围，并且更可靠地检测到不公平的行为。

Conclusion: CAFFE作为一个旨在提高大型语言模型公平性测试效率和准确性的新框架，显示出其相对于传统变形测试方法的优势。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [72] [On Recommending Category: A Cascading Approach](https://arxiv.org/abs/2512.16033)
*Qihao Wang,Pritom Saha Akash,Varvara Kollia,Kevin Chen-Chuan Chang,Biwei Jiang,Vadim Von Brzeski*

Main category: cs.IR

TL;DR: 本文提出了一种级联类别推荐模型(CCRec)，该模型使用变分自编码器(VAE)来编码项目级信息以执行类别级推荐，实验表明该模型在处理类别级推荐方面优于专为项目级推荐设计的方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要集中在推荐一组项目上，但在线电子商务平台已经开始关注探索用户在类别层面的潜在兴趣。类别级别的推荐允许电子商务平台通过扩大用户的兴趣到不同类型的项目来促进用户的参与。此外，当对那些信息和过去互动很少的用户进行项目级别推荐变得极其困难时，它补充了项目级别的推荐。

Method: 提出了一个名为CCRec的级联类别推荐模型，该模型利用变分自编码器（VAE）来将项目级信息编码用于类别级推荐。

Result: 实验结果表明，所提出的CCRec模型相比专门针对项目级推荐设计的方法，在处理类别级推荐时具有优势。

Conclusion: 通过引入一种新的级联类别推荐方法，能够更好地满足电子商务平台上用户对于不同类别商品的兴趣拓展需求，并且在面对信息稀疏用户时提供了比传统项目级推荐更有效的解决方案。

Abstract: Recommendation plays a key role in e-commerce, enhancing user experience and boosting commercial success. Existing works mainly focus on recommending a set of items, but online e-commerce platforms have recently begun to pay attention to exploring users' potential interests at the category level. Category-level recommendation allows e-commerce platforms to promote users' engagements by expanding their interests to different types of items. In addition, it complements item-level recommendations when the latter becomes extremely challenging for users with little-known information and past interactions. Furthermore, it facilitates item-level recommendations in existing works. The predicted category, which is called intention in those works, aids the exploration of item-level preference. However, such category-level preference prediction has mostly been accomplished through applying item-level models. Some key differences between item-level recommendations and category-level recommendations are ignored in such a simplistic adaptation. In this paper, we propose a cascading category recommender (CCRec) model with a variational autoencoder (VAE) to encode item-level information to perform category-level recommendations. Experiments show the advantages of this model over methods designed for item-level recommendations.

</details>


### [73] [Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach](https://arxiv.org/abs/2512.16425)
*Allard Oelen,Mohamad Yaser Jaradeh,Sören Auer*

Main category: cs.IR

TL;DR: 介绍了ASK，一个利用向量搜索、大型语言模型和知识图谱来帮助研究人员寻找相关学术文献的AI驱动系统。该系统允许用户以自然语言输入研究问题，并能自动提取关键信息生成答案。评估表明系统易于使用且用户满意度高。


<details>
  <summary>Details</summary>
Motivation: 随着已发表学术文献数量的不断增加，找到相关文献变得越来越困难。随着生成式人工智能特别是大型语言模型的兴起，为发现和探索文献提供了新的可能性。

Method: ASK采用了神经-符号方法，结合了向量搜索技术、大型语言模型（LLMs）以及知识图谱来主动支持研究人员查找相关的学术文献。此外，通过检索增强生成(RAG)的方法，ASK能够根据用户用自然语言提出的研究问题自动提取关键信息并生成答案。

Result: 对ASK系统的可用性和有用性进行了评价，结果表明该系统用户友好，使用者在使用过程中总体上感到满意。

Conclusion: ASK作为一个AI驱动的学术文献搜索与探索系统，在帮助研究人员更有效地找到相关文献方面展现出了良好的潜力和实用性。

Abstract: As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.

</details>


### [74] [InfoDCL: Informative Noise Enhanced Diffusion Based Contrastive Learning](https://arxiv.org/abs/2512.16576)
*Xufeng Liang,Zhida Qin,Chong Zhang,Tianyu Huang,Gangyi Ding*

Main category: cs.IR

TL;DR: 提出了一种基于扩散的对比学习框架InfoDCL，用于推荐系统中更有效地捕捉用户偏好。通过结合辅助语义信息生成信号，并采用协同训练目标策略来优化生成和偏好学习之间的相互影响。此外，在推理阶段使用多层GCN以整合高阶共现信息的同时保持训练效率。实验表明，该方法在五个真实数据集上显著优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在构建稀疏视图时通常随机扰动原始交互图，导致无法充分捕捉到真实的用户偏好信息。鉴于推荐数据本身就很稀疏，这种方法只能获取有限的语义信息。为了解决这个问题，本文提出了一个新的解决方案。

Method: 提出的方法名为InfoDCL，它利用单步扩散过程将噪声与辅助语义信息相结合，生成反映真实用户偏好的对比视图。同时，通过分析生成与偏好学习间的相互作用，设计了一个协作式训练目标策略，旨在将两者间潜在的干扰转化为合作。此外，仅在推断阶段应用多层GCN，以确保在包含更高阶共现信息的同时不牺牲训练效率。

Result: 广泛的实验证明了InfoDCL在五个实际数据集上的表现明显优于当前最先进的方法。

Conclusion: InfoDCL提供了一种有效提升推荐性能的方案，并为在对比学习框架中应用扩散方法开辟了新途径。

Abstract: Contrastive learning has demonstrated promising potential in recommender systems. Existing methods typically construct sparser views by randomly perturbing the original interaction graph, as they have no idea about the authentic user preferences. Owing to the sparse nature of recommendation data, this paradigm can only capture insufficient semantic information. To address the issue, we propose InfoDCL, a novel diffusion-based contrastive learning framework for recommendation. Rather than injecting randomly sampled Gaussian noise, we employ a single-step diffusion process that integrates noise with auxiliary semantic information to generate signals and feed them to the standard diffusion process to generate authentic user preferences as contrastive views. Besides, based on a comprehensive analysis of the mutual influence between generation and preference learning in InfoDCL, we build a collaborative training objective strategy to transform the interference between them into mutual collaboration. Additionally, we employ multiple GCN layers only during inference stage to incorporate higher-order co-occurrence information while maintaining training efficiency. Extensive experiments on five real-world datasets demonstrate that InfoDCL significantly outperforms state-of-the-art methods. Our InfoDCL offers an effective solution for enhancing recommendation performance and suggests a novel paradigm for applying diffusion method in contrastive learning frameworks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [75] [A Tri-Dynamic Preprocessing Framework for UGC Video Compression](https://arxiv.org/abs/2512.16101)
*Fei Zhao,Mengxi Guo,Shijie Zhao,Junlin Li,Li Zhang,Xiaodong Xie*

Main category: cs.MM

TL;DR: 提出了一种三动态预处理框架来优化用户生成内容(UGC)视频的编码，通过自适应因子、量化级别和lambda折衷调整，实验结果表明该方法在大规模测试集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 用户生成的内容(UGC)已经成为互联网流量的主要力量，但UGC视频相比于传统的编码测试视频具有更高的变异性及多样性特征，这对基于数据驱动的机器学习算法提出了挑战。

Method: 提出了一种三动态预处理框架，包括使用自适应因子调节预处理强度、采用自适应量化级别微调编解码器模拟器以及利用自适应lambda折衷来调整率失真损失函数。

Result: 在大规模测试集上的实验结果显示，所提出的方法取得了优异的表现。

Conclusion: 该研究针对UGC视频特点设计的三动态预处理框架有效提升了编码优化的效果，在实际应用中具有很好的潜力。

Abstract: In recent years, user generated content (UGC) has become the dominant force in internet traffic. However, UGC videos exhibit a higher degree of variability and diverse characteristics compared to traditional encoding test videos. This variance challenges the effectiveness of data-driven machine learning algorithms for optimizing encoding in the broader context of UGC scenarios. To address this issue, we propose a Tri-Dynamic Preprocessing framework for UGC. Firstly, we employ an adaptive factor to regulate preprocessing intensity. Secondly, an adaptive quantization level is employed to fine-tune the codec simulator. Thirdly, we utilize an adaptive lambda tradeoff to adjust the rate-distortion loss function. Experimental results on large-scale test sets demonstrate that our method attains exceptional performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [76] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: 本文介绍了一种名为LOG.io的解决方案，旨在为分布式数据管道提供正确的回滚恢复和细粒度的数据血统捕获。它专为无服务器可扩展架构设计，使用基于日志的回滚恢复协议，并支持通用编程模型。实验表明，在某些条件下，LOG.io在正常处理和恢复期间的表现优于或等同于异步屏障快照协议(ABS)。此外，通过数据并行化可以显著减少LOG.io的开销，而ABS则无法从中受益。


<details>
  <summary>Details</summary>
Motivation: 针对现有技术（如ABS协议）在分布式数据管道中遇到的限制，特别是在处理非确定性操作、与外部系统交互及自定义代码时的问题，以及在存在延迟操作员情况下进行高效恢复的需求，提出了一种新的解决方案LOG.io。

Method: LOG.io采用基于日志的回滚恢复协议，适用于无服务器可扩展架构，支持包括非确定性操作在内的通用编程模型。该方法允许失败的操作独立恢复而不影响其他活跃操作，促进操作员在流水线执行期间动态扩展。

Result: 性能评估显示，在数据管道中有延迟操作员且事件吞吐量适中的情况下，LOG.io在正常处理时表现与ABS相当，在恢复阶段优于ABS；而在其他情况下，ABS表现更佳。但通过数据并行化可以大幅降低LOG.io的额外开销。此外，以事件级精度捕获任意两个操作员间数据血统的开销非常小，在所有实验中均低于1.5%。

Conclusion: LOG.io作为一款专为无服务器环境设计的解决方案，在特定场景下提供了比ABS更好的恢复性能和灵活性，同时保持了较低的数据血统捕获开销。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [77] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: 为了解决大型语言模型（LLM）性能中的PCIe带宽限制问题，本文提出了多路径内存访问（MMA）方案，这是首个能够实现GPU与主机内存之间高效多路径数据传输的方法。通过动态库注入方式部署，无需修改代码即可让LLM应用程序受益。实验显示，MMA相比单路径带宽提升了4.62倍，达到了245 GB/s的峰值带宽，并且在端到端评估中显著降低了LLM服务的时间至首个令牌（TTFT）以及vLLM睡眠模式下的模型切换延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如前缀缓存获取和模型切换）对数据传输速率的需求不断增加，现有PCIe带宽成为了制约其性能的关键瓶颈。尽管在同一服务器内部实现GPU与主机内存之间的多路径数据传输理论上可行，但目前异构协议（如PCIe和NVLink）将实际带宽限制于单一PCIe链路水平，导致服务器内部带宽未能充分利用。

Method: 提出了一种名为多路径内存访问（MMA）的新方案，旨在首次实现GPU与主机内存间的数据高效多路径传输。该方案可通过动态库注入无缝部署，使LLM应用无需任何代码改动即可享受MMA带来的好处。

Result: 实验结果表明，在测试环境中，MMA能够显著提高GPU与内存之间的数据传输带宽，达到245 GB/s的最大值，这比原生单路径带宽提高了4.62倍。此外，端到端评价显示，对于LLM服务而言，MMA可以将时间至首个令牌(TTFT)减少1.14倍至2.38倍；同时，在vLLM的睡眠模式下，模型切换延迟也减少了1.12倍至2.48倍。

Conclusion: MMA方案有效解决了由于PCIe带宽限制而产生的大语言模型性能瓶颈问题，不仅大幅提升了数据传输效率，还显著改善了LLM服务响应速度及模型切换效率。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [78] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 研究了无服务器计算中的冷启动问题，将其视为开发者可见的设计问题。通过分析开源无服务器系统中的81份问题报告，提出了初始化反模式、修复策略和诊断挑战的分类体系，并基于这些见解开发了SCABENCH基准测试工具和INITSCOPE分析框架。实验结果显示INITSCOPE在定位准确性和减少诊断工作量方面优于现有工具，同时提高了开发者任务完成的速度与准确性。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算简化了部署和扩展过程，但冷启动延迟仍然是一个主要性能瓶颈。本研究旨在将冷启动问题视为一个对开发者可见的设计问题来解决，而非仅仅作为一个黑盒优化问题对待。

Method: 通过对来自开源无服务器系统的81份经过审核的问题报告进行分析，研究人员得出了关于设计、打包以及运行时层面的初始化反模式、修复策略及诊断挑战的分类体系。基于这些发现，他们引入了一个可复现的基准测试（SCABENCH）以及一个轻量级分析框架（INITSCOPE），该框架能够链接加载的代码与其执行情况。

Result: 使用SCABENCH作为测试平台，INITSCOPE相比先前工具在提高定位准确性上达到了最高40%的提升，并且减少了64%的诊断工作量。此外，在开发者研究中也显示了更高的任务完成准确率以及更快的问题诊断速度。

Conclusion: 该研究表明，通过采取基于证据的方法并关注性能，可以有效缓解无服务器架构下的冷启动问题。这不仅为开发者提供了更清晰的设计指导原则，也为未来的研究和改进提供了一个公开可用的研究成果。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [79] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 本文提出了一种在线调度框架，该框架集成了条件负载均衡、动态分区和作业迁移，以解决NVIDIA的多实例GPU（MIG）技术带来的资源争用和GPU碎片化问题。实验结果表明，所提方法显著提高了系统效率，当所有技术都应用时，最大可将makespan提高35%。


<details>
  <summary>Details</summary>
Motivation: 随着现代GPU工作负载越来越多地需要有效的资源共享，因为许多任务不需要GPU的全部容量，而NVIDIA的多实例GPU（MIG）通过启用硬件级GPU分区提供了强大的资源隔离。但是，有效利用MIG引入了新的挑战：共享组件如PCIe带宽导致的资源竞争以及由于MIG配置数量有限而引起的GPU碎片化问题。

Method: 为了解决上述问题，作者们提出了一种包含有条件负载均衡、动态分区和作业迁移在内的在线调度框架。这种方法能够动态调整作业放置来最小化竞争，并重新组织GPU分配以对抗内部和外部碎片化。

Result: 实验结果显示，采用这种新方法可以显著提升系统效率，当同时应用所有提到的技术时，makespan最多可以提高35%。

Conclusion: 研究提出的在线调度框架有效地解决了使用MIG技术时遇到的资源竞争和GPU碎片化问题，从而大幅提升了系统的整体效率。

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [80] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 提出了一种名为Staggered Batch Scheduling (SBS)的机制，通过缓冲请求形成最优执行批次，以解决DP+EP架构下的内部同步成本高问题。该方法减少了Time-to-First-Token (TTFT)，并结合Load-Aware Global Allocation策略提高了吞吐量，在实际部署中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）服务向复杂、分布式架构发展，特别是采用P/D分离的大规模DP+EP模式时，出现了独特的调度挑战。传统调度器将实例视为黑盒处理的方式不再适用，因为DP+EP架构内部存在较高的同步开销。直接请求分发会导致严重的引擎内排队和并行化空泡现象，从而降低首次令牌时间（TTFT）。

Method: 研究者提出了Staggered Batch Scheduling (SBS)机制来应对上述问题。该机制通过故意缓存请求以形成最佳执行批次，实现时间上的解耦，消除了内部排队空泡同时不牺牲吞吐量。此外，利用由缓存创建的调度窗口，引入了针对Prefill和Decode阶段计算负载均衡的全局分配策略。

Result: 在生产环境中的H800集群上部署该系统服务于Deepseek-V3时，相较于最先进的即时调度基线，新方法能够减少30%-40%的TTFT，并提高15%-20%的吞吐量。

Conclusion: 通过实施Staggered Batch Scheduling与Load-Aware Global Allocation策略，可以有效缓解DP+EP架构下因内部高同步需求导致的服务效率低下问题，显著改善了TTFT及整体吞吐性能。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [81] [FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store](https://arxiv.org/abs/2512.16148)
*Zhisheng Hu,Jiacheng Shen,Ming-Chang Yang*

Main category: cs.DC

TL;DR: FlexKV, a memory-disaggregated key-value (KV) store with index proxying, tackles the poor performance of existing approaches in disaggregated memory (DM) architecture by dynamically offloading the index to compute nodes, thus improving throughput and reducing latency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the issues of overdependence on one-sided atomic operations in index processing and constrained efficiency in compute-side caches that lead to poor performance in existing memory-disaggregated key-value stores within the disaggregated memory (DM) data center architecture. The goal is to improve resource utilization and overall system performance.

Method: The proposed method, FlexKV, involves dynamically offloading the index to compute nodes for accelerated index processing and maintaining high-performance compute-side caches. To enable efficient index proxying, FlexKV introduces a rank-aware hotness detection algorithm, a two-level CN memory optimization scheme, and an RPC-aggregated cache management mechanism.

Result: Experimental results demonstrate that FlexKV significantly enhances performance, achieving up to 2.94 times higher throughput and reducing latency by up to 85.2% when compared to state-of-the-art memory-disaggregated KV stores.

Conclusion: In conclusion, FlexKV successfully addresses the limitations of current memory-disaggregated key-value stores in disaggregated memory architectures by introducing innovative solutions such as dynamic index offloading, load balancing, memory optimization, and efficient cache management, thereby leading to substantial improvements in both throughput and latency.

Abstract: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.

</details>


### [82] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 本文介绍了一个支持科学工作负载中的人工智能的联邦计算平台，提供了一致且透明的访问服务，并覆盖了从模型开发、训练到部署的整个机器学习生命周期。


<details>
  <summary>Details</summary>
Motivation: 为了在科学工作负载中支持人工智能的发展，需要一个能够提供一致和透明访问多个物理分布的电子基础设施的联邦计算平台。

Method: 通过创建一个综合的服务目录，该平台提供了集成的用户体验，涵盖了完整的机器学习生命周期，包括模型开发（具有专门的交互式开发环境）、培训（配备GPU资源、注释工具、实验跟踪和联邦学习支持）以及部署（涵盖云连续体中的各种部署选项）。此外，该平台还提供了AI模型的可追溯性和可重复性工具，与不同的人工智能模型供应商、数据集和存储资源集成，允许用户与更广泛的机器学习生态系统互动。

Result: 该平台不仅为用户提供了一个全面支持机器学习全生命周期的解决方案，而且通过其易定制性降低了外部社区采用的障碍。

Conclusion: 这个专为支持科学工作负载而设计的联邦计算平台，通过提供一致性、透明度以及与广泛机器学习生态系统的集成，成功地满足了科学研究者对于高效开展人工智能研究的需求。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [83] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 本文提出了一种新的CPU-GPU协作推理框架，通过在GPU上实现专家缓存机制来减少数据传输需求，并通过利用CPU多线程优化处理缓存未命中情况，从而提高基于消费者级硬件的单请求推理场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管混合专家（MoE）模型通过选择性地激活参数子集提供了计算效率的解决方案，但最先进的MoE模型仍然需要超出典型消费级GPU容量的大量内存。传统的将模型权重在CPU和GPU之间转移的方法引入了延迟，限制了推理性能。

Method: 提出了一个新颖的CPU-GPU协作推理框架，该框架包括在GPU上的专家缓存机制以减少数据传输要求，并通过缓存命中加快推理速度。对于缓存未命中的情况，计算被卸载到CPU进行高效处理，这得益于CPU多线程优化的优势。

Result: 评估结果表明，所提出的框架提高了性能，并强调了CPU-GPU协作在最大化消费者级系统硬件利用率方面的潜力，特别是在单请求推理场景下。

Conclusion: 通过引入一种包含GPU上专家缓存机制以及有效利用CPU多线程优势的新颖CPU-GPU协作推理方法，能够在保持高性能的同时显著降低对昂贵硬件的需求，为在消费级设备上部署大型语言模型提供了解决方案。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>
