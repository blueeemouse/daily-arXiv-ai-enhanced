<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 108]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Survey of Code Review Benchmarks and Evaluation Practices in Pre-LLM and LLM Era](https://arxiv.org/abs/2602.13377)
*Taufiqul Islam Khan,Shaowei Wang,Haoxiang Zhang,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 本研究对2015年至2025年间代码审查基准进行了全面调查，涵盖大语言模型（LLM）前后时期。通过分析99篇研究论文，提出了一个多层次分类法，组织代码审查研究为五个领域和18个细粒度任务，并指出了当前基准的局限性及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）快速发展，自动化支持代码审查成为研究热点。然而，由于缺乏对现有基准和评估实践的系统理解，该领域的进展受到阻碍。现有的代码审查数据集分散、设计差异大，难以提供关于实际评估审查能力的深入见解。

Method: 研究者们分析了从2015年到2025年间发布的99篇研究论文（包括58篇前LLM时代和41篇LLM时代的论文），提取了关键元数据如数据集、评估指标、数据来源以及目标任务等信息。基于这些分析，他们提出了一种多级分类法来组织代码审查研究。

Result: 研究表明，在代码审查领域正朝着端到端生成式同行评审转变，同时增加了多语言覆盖范围，而独立的变化理解任务有所减少。此外，还识别出现有基准的一些局限性，并对未来方向提出了建议，比如更广泛的任务覆盖、动态运行时评估以及由分类法指导的细粒度评估。

Conclusion: 这项调查为开发更加现实且全面的大语言模型基础代码审查基准提供了结构化基础。

Abstract: Code review is a critical practice in modern software engineering, helping developers detect defects early, improve code quality, and facilitate knowledge sharing. With the rapid advancement of large language models (LLMs), a growing body of work has explored automated support for code review. However, progress in this area is hindered by the lack of a systematic understanding of existing benchmarks and evaluation practices. Current code review datasets are scattered, vary widely in design, and provide limited insight into what review capabilities are actually being assessed. In this paper, we present a comprehensive survey of code review benchmarks spanning both the Pre-LLM and LLM eras (2015--2025). We analyze 99 research papers (58 Pre-LLM era and 41 LLM era) and extract key metadata, including datasets, evaluation metrics, data sources, and target tasks. Based on this analysis, we propose a multi-level taxonomy that organizes code review research into five domains and 18 fine-grained tasks. Our study reveals a clear shift toward end-to-end generative peer review, increasing multilingual coverage, and a decline in standalone change understanding tasks. We further identify limitations of current benchmarks and outline future directions, including broader task coverage, dynamic runtime evaluation, and taxonomy-guided fine-grained assessment. This survey provides a structured foundation for developing more realistic and comprehensive benchmarks for LLM-based code review.

</details>


### [2] [InEx-Bug: A Human Annotated Dataset of Intrinsic and Extrinsic Bugs in the NPM Ecosystem](https://arxiv.org/abs/2602.13400)
*Tanner Wright,Adams Chen,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 本文介绍了一个名为InEx-Bug的数据集，该数据集对来自103个NPM仓库的377个GitHub问题进行了手动标注，将问题分类为内在缺陷、外在缺陷（依赖/环境问题）、非缺陷或未知。数据分析显示，内在缺陷比外在缺陷解决得更快、更频繁地被关闭且需要代码更改的情况更多；而外在缺陷则表现出更高的重开率和延迟复发的特点。


<details>
  <summary>Details</summary>
Motivation: 理解软件缺陷的原因对于可靠的软件维护和生态系统稳定性至关重要。然而，现有的bug数据集并不区分项目内部产生的问题与由外部依赖或环境因素引起的问题。为了填补这一空白，作者们创建了InEx-Bug数据集。

Method: 研究者从103个NPM存储库中选择了377个GitHub问题，并手动给这些问题打上标签，将其分为内在缺陷、外在缺陷、非缺陷或未知四类。此外，还收集了丰富的时序性和行为元数据，如维护者的参与度、代码变更情况以及重新打开模式等。

Result: 分析结果显示，内在缺陷相比外在缺陷而言，解决速度更快（中位数8.9天对比10.2天），关闭概率更高（92%对比78%），并且需要进行代码修改的比例也更大（57%对比28%）。相反地，外在缺陷具有较高的重开率（12%对比4%）以及较晚的复发时间（中位数157天对比87天）。

Conclusion: InEx-Bug数据集不仅为研究NPM生态系统中的内在与外在缺陷提供了基础，而且通过比较这两类缺陷之间的差异，有助于开发者更好地理解和处理不同类型的软件问题。

Abstract: Understanding the causes of software defects is essential for reliable software maintenance and ecosystem stability. However, existing bug datasets do not distinguish between issues originating within a project from those caused by external dependencies or environmental factors. In this paper we present InEx-Bug, a manually annotated dataset of 377 GitHub issues from 103 NPM repositories, categorizing issues as Intrinsic (internal defect), Extrinsic (dependency/environment issue), Not-a-Bug, or Unknown. Beyond labels, the dataset includes rich temporal and behavioral metadata such as maintainer participation, code changes, and reopening patterns. Analyses show Intrinsic bugs resolve faster (median 8.9 vs 10.2 days), are close more often (92% vs 78%), and require code changes more frequently (57% vs 28%) compared to Extrinsic bugs. While Extrinsic bugs exhibit higher reopen rates (12% vs 4%) and delayed recurrence (median 157 vs 87 days). The dataset provides a foundation for further studying Intrinsic and Extrinsic defects in the NPM ecosystem.

</details>


### [3] [Execution-State-Aware LLM Reasoning for Automated Proof-of-Vulnerability Generation](https://arxiv.org/abs/2602.13574)
*Haoyu Li,Xijia Che,Yanhao Wang,Xiaojing Liao,Luyi Xing*

Main category: cs.SE

TL;DR: DrillAgent, a framework that combines LLM-based semantic reasoning with feedback from actual program execution to generate Proof-of-Vulnerabilities (PoVs), outperforms other LLM-based methods on a benchmark of real-world C/C++ vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing automated exploit generation methods, which struggle with satisfying complex semantic constraints, and to improve the precision of PoV generation by integrating concrete execution behavior with LLMs' reasoning capabilities.

Method: DrillAgent uses an iterative process of hypothesis-verification-refinement, where it formulates inputs based on code analysis, observes how the program executes, and then refines its approach by converting low-level execution details into higher-level constraints. This method creates a closed-loop system that improves the accuracy of the generated PoVs over time.

Result: Evaluation on SEC-bench shows that DrillAgent can solve up to 52.8% more CVE tasks than the best-performing baseline, demonstrating the effectiveness of incorporating execution-state-aware reasoning in the PoV generation process.

Conclusion: The paper concludes that the integration of LLMs with dynamic execution feedback through DrillAgent significantly enhances the ability to generate accurate PoVs, making it a superior solution for software security validation compared to purely static or unguided LLM approaches.

Abstract: Proof-of-Vulnerability (PoV) generation is a critical task in software security, serving as a cornerstone for vulnerability validation, false positive reduction, and patch verification. While directed fuzzing effectively drives path exploration, satisfying complex semantic constraints remains a persistent bottleneck in automated exploit generation. Large Language Models (LLMs) offer a promising alternative with their semantic reasoning capabilities; however, existing LLM-based approaches lack sufficient grounding in concrete execution behavior, limiting their ability to generate precise PoVs.
  In this paper, we present DrillAgent, an agentic framework that reformulates PoV generation as an iterative hypothesis-verification-refinement process. To bridge the gap between static reasoning and dynamic execution, DrillAgent synergizes LLM-based semantic inference with feedback from concrete program states. The agent analyzes the target code to hypothesize inputs, observes execution behavior, and employs a novel mechanism to translate low-level execution traces into source-level constraints. This closed-loop design enables the agent to incrementally align its input generation with the precise requirements of the vulnerability. We evaluate DrillAgent on SEC-bench, a large-scale benchmark of real-world C/C++ vulnerabilities. Experimental results show that DrillAgent substantially outperforms state-of-the-art LLM agent baselines under fixed budget constraints, solving up to 52.8% more CVE tasks than the best-performing baseline. These results highlight the necessity of execution-state-aware reasoning for reliable PoV generation in complex software systems.

</details>


### [4] [From What to How: Bridging User Requirements with Software Development Using Large Language Models](https://arxiv.org/abs/2602.13611)
*Xiao He,Ru Chen,Jialun Cao*

Main category: cs.SE

TL;DR: This paper introduces DesBench, a benchmark for assessing LLMs' performance on software design tasks, revealing significant challenges in code generation and OOP, yet promising results in creating acceptance tests.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the gap in evaluating large language models (LLMs) on their capability to handle software design, which is as critical as implementation but often overlooked in existing benchmarks. The aim is to investigate if LLMs can manage software design and follow specific designs when writing code.

Method: The paper proposes DesBench, a benchmark designed to evaluate LLMs on three tasks related to software design: design-aware code generation, object-oriented modeling, and the design of acceptance test cases. The evaluation was conducted on 30 manually crafted Java projects, using seven state-of-the-art LLMs.

Result: The results show that LLMs struggle with generating correct implementations without detailed designs, face challenges in defining operations and inter-class relationships in object-oriented modeling, but can produce acceptance test cases with code coverage quality similar to human-written ones.

Conclusion: The research highlights the current limitations of LLMs in handling software design, particularly in code generation and object-oriented modeling, while showing that LLMs can generate acceptance test cases with comparable quality to those written by humans. It calls for further investigation into new design methodologies and languages suitable for LLM-based development.

Abstract: Recently, large language models (LLMs) are extensively utilized to enhance development efficiency, leading to numerous benchmarks for evaluating their performance. However, these benchmarks predominantly focus on implementation, overlooking the equally critical aspect of software design. This gap raises two pivotal questions: (1) Can LLMs handle software design? (2) Can LLMs write code following the specific designs? To investigate these questions, this paper proposes DesBench, a design-aware benchmark for evaluating LLMs on three software design-related tasks: design-aware code generation, object-oriented modeling, and the design of acceptance test cases. DesBench comprises 30 manually crafted Java projects that include requirement documents, design models, implementations, and acceptance tests, amounting to a total of 30 design models, 194 Java classes, and 737 test cases. We evaluated seven state-of-the-art LLMs, including three DeepSeek R1, two Qwen2.5, and two GPT models, using DesBench. The results reveal that LLMs remain significantly challenged by the intricacies of software design: (1) For code generation, LLMs struggle to produce correct implementations when provided with only high-level or no designs. (2) In object-oriented modeling, while LLMs can accurately identify objects and classes, they face challenges in defining operations and inter-class relationships. (3) Acceptance test cases generated by LLMs from functional requirements achieve code coverage quality comparable to those written by humans. Our research highlights the current limitations of LLMs in managing software design and calls for further investigation into new design methodologies and languages suitable for LLM-based development.

</details>


### [5] [VeriSBOM: Secure and Verifiable SBOM Sharing Via Zero-Knowledge Proofs](https://arxiv.org/abs/2602.13682)
*Gianpietro Castiglione,Shahriar Ebrahimi,Narges Khakpour*

Main category: cs.SE

TL;DR: VeriSBOM框架利用零知识证明技术，为软件物料清单(SBOM)提供了可选择性披露和加密验证的能力，允许第三方独立验证软件依赖的安全性和合规性，同时保持了信息的机密性。


<details>
  <summary>Details</summary>
Motivation: 由于SBOM中可能包含敏感信息，如专有依赖或未修补漏洞等，直接提供完整的SBOM给外部可能导致知识产权泄露等问题。因此需要一种既能保护敏感信息又能保证SBOM准确性的方法。

Method: 提出了VeriSBOM框架，该框架使用零知识证明来实现SBOM的选择性披露与验证。通过结合可扩展向量承诺方案及基于折叠的证明聚合技术生成简洁的零知识证明，以验证安全性和合规属性的同时保持机密性。

Result: 实现了VeriSBOM，并对其安全性进行了分析，在实际软件包注册表上评估了性能。结果表明，该方法能够支持可扩展、隐私保护以及可验证的SBOM共享与验证。

Conclusion: VeriSBOM提供了一种新的解决方案，能够在不完全公开敏感信息的前提下，让第三方对软件物料清单中的关键声明进行验证，从而增强软件供应链透明度的同时保护了企业的商业和技术秘密。

Abstract: A Software Bill of Materials (SBOM) is a key component for the transparency of software supply chain; it is a structured inventory of the components, dependencies, and associated metadata of a software artifact. However, an SBOM often contain sensitive information that organizations are unwilling to disclose in full to anyone, for two main concerns: technological risks deriving from exposing proprietary dependencies or unpatched vulnerabilities, and business risks, deriving from exposing architectural strategies. Therefore, delivering a plaintext SBOM may result in the disruption of the intellectual property of a company. To address this, we present VeriSBOM, a trustless, selectively disclosed SBOM framework that provides cryptographic verifiability of SBOMs using zero-knowledge proofs. Within VeriSBOM, third parties can validate specific statements about a delivered software. Respectively, VeriSBOM allows independent third parties to verify if a software contains authentic dependencies distributed by official package managers and that the same dependencies satisfy rigorous policy constraints such as the absence of vulnerable dependencies or the adherence with specific licenses models. VeriSBOM leverages a scalable vector commitment scheme together with folding-based proof aggregation to produce succinct zero-knowledge proofs that attest to security and compliance properties while preserving confidentiality. Crucially, the verification process requires no trust in the SBOM publisher beyond the soundness of the underlying primitives, and third parties can independently check proofs against the public cryptographic commitments. We implement VeriSBOM, analyze its security, and evaluate its performance on real-world package registries. The results show that our method enables scalable, privacy-preserving, and verifiable SBOM sharing and validation.

</details>


### [6] [Impacts of Generative AI on Agile Teams' Productivity: A Multi-Case Longitudinal Study](https://arxiv.org/abs/2602.13766)
*Rafael Tomaz,Paloma Guenes,Allysson Allex Araújo,Maria Teresa Baldassarre,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究通过为期13个月的多案例纵向研究，分析了生成型人工智能（GenAI）工具对敏捷软件团队生产力的影响。结果表明，GenAI工具可以显著提高团队表现和幸福感，尤其是在绩效和感知效率方面，而开发人员的活动量保持不变。这强调了使用多维度框架如SPACE来全面评估GenAI影响的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成型人工智能（GenAI）工具在软件工程领域带来了明显的变化，但大多数研究都是短期且针对个体实验的。对于这些工具在工业敏捷环境下的长期、团队级生产率影响尚未得到充分描述。

Method: 本研究采用了一种多案例纵向研究方法，涉及一家大型技术咨询公司的3个敏捷团队，在大约13个月内进行。研究者收集并比较了历史冲刺（采用前）与研究冲刺（采用后）期间来自Jira、SonarQube、Git的定量遥测数据以及定性调查数据。

Result: 研究表明，GenAI工具能够显著改善团队的表现和幸福感，特别是在绩效和感知效率方面有显著提升，同时开发者的活动量保持稳定。这意味着GenAI增加了开发工作价值密度而非其数量。

Conclusion: GenAI工具不仅提高了团队的工作效率和满意度，还特别体现在提升了工作的价值密度而不是增加工作量上。这证明了像SPACE这样的多维度框架对于捕捉到GenAI实际影响的重要性，而这种影响是仅通过测量活动量无法发现的。

Abstract: Context: Generative Artificial Intelligence (GenAI) tools, such as GitHub Copilot and GPT tools, represent a paradigm shift in software engineering. While their impact is clear, most studies are short-term, focused on individual experiments. The sustained, team-level effects on productivity within industrial agile environments remain largely uncharacterized. Goal: This study aims to provide a longitudinal evaluation of GenAI's impact on agile software teams. We characterize its effect on developers' productivity by applying the multi-dimensional SPACE framework. Method: We conducted a multi-case longitudinal study involving 3 agile teams at a large technology consulting firm for around 13 months. We collected and compared quantitative telemetry (Jira, SonarQube, Git) and qualitative survey data from historical (pre-adoption) and research (post-adoption) sprints. Conclusion: GenAI tools can significantly improve team performance and well-being. Our key finding is a sharp increase in Performance and perceived Efficiency concurrent with flat developer Activity. This suggests GenAI increases the value density of development work, not its volume. This finding validates the necessity of multi-dimensional frameworks like SPACE to capture the true, nuanced impact of GenAI in situ, which would be invisible to studies measuring Activity alone.

</details>


### [7] [Impostor Phenomenon as Human Debt: A Challenge to the Future of Software Engineering](https://arxiv.org/abs/2602.13767)
*Paloma Guenes,Rafael Tomaz,Maria Teresa Baldassarre,Alexander Serebrenik*

Main category: cs.SE

TL;DR: 该论文提出将冒名顶替现象（IP）视为一种人力债务，并探讨了其与ICSE2026关于软件工程未来预调查结果的关系。作者认为，如同技术债务因短期目标优先于长期结构完整性而产生一样，人力债务则是由于社会技术生态系统中缺乏心理安全感和包容性支持而积累的。这种债务对代表性不足的工程师和研究人员的影响更大。为解决这一问题，提出了文化重构、透明度以及通过同盟关系进行主动维护的方法，呼吁领导者和机构解决加剧这些感受的环境因素，确保所有专业人员的可持续生态系统。


<details>
  <summary>Details</summary>
Motivation: 作者注意到软件工程领域内存在冒名顶替现象，但通常仅从个人内部视角来理解。为了更全面地看待这个问题，他们引入了‘人力债务’的概念，旨在强调组织层面对于缓解此类问题的重要性，特别是针对那些在传统层级结构和学术环境中面临更多挑战的少数群体成员。

Method: 本文采用概念分析法，通过对比技术债务来定义人力债务的概念，并基于ICSE2026预调查的结果讨论了软件工程行业未来的趋势如何影响人力债务的问题。此外，还提出了包括文化重构在内的几种策略来减少或管理人力债务。

Result: 研究发现，将冒名顶替现象视为一种人力债务可以帮助我们更好地理解它给软件工程社区带来的长期负面影响。同时，通过实施文化重构等措施可以有效地减轻这种债务，从而促进更加健康和包容的工作环境。

Conclusion: 为了构建一个支持所有软件工程师成长并感到被接纳的可持续生态系统，必须正视并处理好人力债务问题。这不仅需要改变现有的组织文化，还需要领导者和机构采取具体行动来创造一个更加开放和支持性的环境。

Abstract: The Impostor Phenomenon (IP) impacts a significant portion of the Software Engineering workforce, yet it is often viewed primarily through an internal individual lens. In this position paper, we propose framing the prevalence of IP as a form of Human Debt and discuss the relation with the ICSE2026 Pre Survey on the Future of Software Engineering results. Similar to technical debt, which arises when short-term goals are prioritized over long-term structural integrity, Human Debt accumulates due to gaps in psychological safety and inclusive support within socio-technical ecosystems. We observe that this debt is not distributed equally, it weighs heavier on underrepresented engineers and researchers, who face compounded challenges within traditional hierarchical structures and academic environments. We propose cultural refactoring, transparency and active maintenance through allyship, suggesting that leaders and institutions must address the environmental factors that exacerbate these feelings, ensuring a sustainable ecosystem for all professionals.

</details>


### [8] [A Quasi-Experimental Evaluation of Coaching to Mitigate the Impostor Phenomenon in Early-Career Software Engineers](https://arxiv.org/abs/2602.13774)
*Paloma Guenes,Joan Leite,Rafael Tomaz,Allysson Allex Araujo,Jean Natividade,Maria Teresa Baldassarre,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该研究通过准实验设计，对早期职业软件工程师进行了结构化团体辅导干预，以减少冒名顶替现象（IP）的感受。尽管辅导后CIPS得分有轻微下降，但对照组在观察阶段也有改善，表明环境和时间因素可能比正式干预影响更大。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域中，由于对专业知识和创新的高期望，冒名顶替现象普遍存在。尽管提出了一些如辅导等干预措施来缓解这一问题，但在软件工程领域的实证证据仍较少被探索。

Method: 本研究采用准实验设计，选取了20名参与者并分配到两个项目团队中，并使用了等待名单控制设计，辅以非参与式观察。治疗组接受了三次辅导干预，而对照组则在观察期后接受干预。利用Clance冒名顶替现象量表（CIPS）评估IP，并结合WHO-5幸福感、SWLS生活满意度及PANAS情绪评价指标进行衡量。

Result: 结果显示，辅导导致CIPS分数略有下降；同时，在观察阶段内，对照组也有所改进，这表明背景与时间性因素可能比正式干预具有更强的影响。

Conclusion: 这些结果提示，虽然辅导可以帮助反思和增强关于IP的认识，但是团队合作和项目工作的其他情境方面也可能促进了这种变化。这项研究为理解结构化的IP干预如何在软件工程环境中发挥作用提供了新的实证步骤。

Abstract: Context: The Impostor Phenomenon (IP), the persistent belief of being a fraud despite evident competence, is common in Software Engineering (SE), where high expectations for expertise and innovation prevail. Although coaching and similar interventions are proposed to mitigate IP, empirical evidence in SE remains underexplored.
  Objective: This study examines the impact of a structured group coaching intervention on reducing IP feelings among early-career software engineers.
  Method: We conducted a quasi-experiment with 20 participants distributed across two project teams using a wait-list control design, complemented by non-participant observation. The treatment group received a three-session coaching intervention, while the control group received it after an observation phase. IP was assessed using the Clance Impostor Phenomenon Scale (CIPS), alongside evaluated measures of well-being (WHO-5), life satisfaction (SWLS), and affect (PANAS).
  Results: The coaching resulted in modest reductions in CIPS scores, whereas the control group also improved during the observation phase, suggesting that contextual and temporal factors may have exerted a stronger influence than the formal intervention.
  Conclusion: These results suggest that coaching may support reflection and awareness related to IP, yet other contextual aspects of team collaboration and project work might also contribute to these changes. This study offers a novel empirical step toward understanding how structured IP interventions operate within SE environments.

</details>


### [9] [CodeGlance: Understanding Code Reasoning Challenges in LLMs through Multi-Dimensional Feature Analysis](https://arxiv.org/abs/2602.13962)
*Yunkun Wang,Xuanhe Zhang,Junxiao Han,Chen Zhi,Shuiguang Deng*

Main category: cs.SE

TL;DR: 本研究介绍了CodeGlance，一个旨在探索代码推理挑战的多维度基准测试工具，涵盖了内在逻辑推理、API交互推理和未见过的功能推理三个现实场景。通过评估7种最先进的大语言模型（LLM），研究发现对于较小的模型而言，处理未知功能尤其具有挑战性，并且识别出执行跟踪长度、API调用次数和控制流复杂度等关键代码复杂性特征对不同场景下的代码推理难度有显著影响。此外，研究还探讨了几种常见的增强策略在提高推理性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的代码推理研究主要集中在孤立的代码片段上，忽略了涉及外部API交互和不熟悉函数的真实世界场景的复杂性。这种差距限制了我们对在多样化的编程上下文中真正使代码推理对大型语言模型（LLMs）构成挑战的因素的理解。

Method: 提出了CodeGlance，一种多维基准测试方法，用于调查三种现实情况下的代码推理挑战：内在逻辑推理、API交互推理以及未见函数推理。通过对7个最先进的大型语言模型进行系统评估，来揭示各种规模模型在这些情境下面临的具体困难。

Result: 研究结果显示，对于小型模型来说，未见函数推理构成了特别大的挑战；例如，Qwen2.5-3b模型在未见函数上的准确率仅为6.0%，而在熟悉的API上则为37.5%。同时，研究确定了一些关键的代码复杂性特征——如执行轨迹长度、API调用次数及控制流程复杂性——这些都极大影响着跨场景的代码推理难度。此外，对于常用增强策略（比如思维链、文档检索和代码搜索）如何提升推理表现也进行了探讨，发现它们的有效性很大程度上取决于挑战是源自逻辑复杂性还是知识缺口。

Conclusion: 这项工作提供了可操作的指导方针，以开发更强大的代码推理系统，并将基于LLM的编程助手部署到实际软件开发中去。

Abstract: In modern software development, developers frequently need to understand code behavior at a glance -- whether reviewing pull requests, debugging issues, or navigating unfamiliar codebases. This ability to reason about dynamic program behavior is fundamental to effective software engineering and increasingly supported by Large Language Models (LLMs). However, existing studies on code reasoning focus primarily on isolated code snippets, overlooking the complexity of real-world scenarios involving external API interactions and unfamiliar functions. This gap hinders our understanding of what truly makes code reasoning challenging for LLMs across diverse programming contexts.
  We present CodeGlance, a multi-dimensional benchmark investigating code reasoning challenges across three realistic scenarios: intrinsic logic reasoning, API interaction reasoning, and unseen function reasoning. Through systematic evaluation of 7 state-of-the-art LLMs, we reveal that unseen function reasoning poses significant challenges especially for smaller models, with Qwen2.5-3b achieving only 6.0\% accuracy on unseen functions compared to 37.5\% on familiar APIs. We identify critical code complexity features -- including execution trace length, API invocation count, and control flow complexity -- that significantly impact code reasoning difficulty across scenarios. We further investigate how common augmentation strategies, including CoT, document retrieval, and code search, can improve reasoning performance, finding that their effectiveness varies substantially depending on whether challenges stem from logical complexity or knowledge gaps. These findings provide actionable guidance for developing more capable code reasoning systems and deploying LLM-based programming assistants in real-world software development.

</details>


### [10] [Every Maintenance Has Its Exemplar: The Future of Software Maintenance through Migration](https://arxiv.org/abs/2602.14046)
*Zirui Chen,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 本文提出了基于迁移方法的软件维护系统研究议程，定义了迁移维护生命周期的四个关键阶段，并分析了每个阶段可能出现的挑战，旨在推动自动化软件维护的发展。


<details>
  <summary>Details</summary>
Motivation: 当前软件维护工作耗时、费力且易出错，需要通过自动化来提高效率。从其他软件系统的维护活动中学习，特别是采用迁移的方法（如API演变适应、软件测试和补丁迁移）展现出了在提高维护任务效率方面的巨大潜力。

Method: 本文提出了一套基于迁移的软件维护方法的研究框架，该框架包括识别可迁移的维护任务、为目标项目选择合适的迁移源、跨系统匹配相关数据并调整至目标上下文以及验证迁移正确性这四个关键步骤。同时，文中还探讨了各阶段可能遇到的挑战。

Result: 确立了基于迁移的软件维护生命周期模型及其四阶段过程，并对实现过程中面临的挑战进行了初步分析。

Conclusion: 通过提出首个基于迁移方法的软件维护系统研究议程，本文希望激励社区更深入地探索这一领域，并解决推进自动化软件维护所需克服的关键难题。

Abstract: Maintenance is a critical stage in the software lifecycle, ensuring that post-release systems remain reliable, efficient, and adaptable. However, manual software maintenance is labor-intensive, time-consuming, and error-prone, which highlights the urgent need for automation. Learning from maintenance activities conducted on other software systems offers an effective way to improve efficiency. In particular, recent research has demonstrated that migration-based approaches transfer knowledge, artifacts, or solutions from one system to another and show strong potential in tasks such as API evolution adaptation, software testing, and migrating patches for fault correction. This makes migration-based maintenance a valuable research direction for advancing automated maintenance.
  This paper takes a step further by presenting the first systematic research agenda on migration-based approaches to software maintenance. We characterize the migration-based maintenance lifecycle through four key stages: \ding{182} identifying a maintenance task that can be addressed through migration, \ding{183} selecting suitable migration sources for the target project,\ding{184} matching relevant data across systems and adapting the migrated data to the target context, and \ding{185} validating the correctness of the migration. We also analyze the challenges that may arise at each stage. Our goal is to encourage the community to explore migration-based approaches more thoroughly and to tackle the key challenges that must be solved to advance automated software maintenance.

</details>


### [11] [An Empirical Study of the Evolution of GitHub Actions Workflows](https://arxiv.org/abs/2602.14572)
*Pooya Rostami Mazrae,Alexandre Decan,Tom Mens,Mairieli Wessel*

Main category: cs.SE

TL;DR: 本研究通过混合方法分析了GitHub Actions工作流随时间的变化，发现仓库中位数包含三个工作流文件，每周有7.3%的工作流文件发生变化。大多数更改涉及任务配置和任务说明，未发现大型语言模型编码工具或其他主要技术变化对工作流创建和维护频率的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解CI/CD实践（特别是GitHub Actions）在协作软件开发中的作用及其随时间演变的方式。

Method: 采用混合方法进行研究，首先通过初步定性分析439个修改过的工作流文件来识别七种类型的概念变更；然后，在从2019年11月至2025年8月的时间段内对超过49,000个GitHub仓库执行定量分析，这些仓库总计拥有超过267,000个工作流变更历史记录和超过340万个工作流文件版本。

Result: 发现仓库通常包含中位数为三个的工作流文件，并且所有工作流文件中有7.3%每周都会被更改。大约四分之三的更改仅包含单一变更，其中大部分与工作流作业中的任务配置和任务说明有关。没有明确证据表明LLM编码工具或其它重大技术变革对工作流创建及维护频率产生了影响。

Conclusion: 需要改进工具以支持更细粒度的维护任务，例如更广泛地采用依赖管理以及基于AI的支持来确保并维持工作流的安全性和质量。

Abstract: CI/CD practices play a significant role during collaborative software development by automating time-consuming and repetitive tasks such as testing, building, quality checking, dependency and security management. GitHub Actions, the CI/CD tool integrated into GitHub, allows repository maintainers to automate development workflows. We conducted a mixed methods analysis of GitHub Actions workflow changes over time. Through a preliminary qualitative analysis of 439 modified workflow files we identified seven types of conceptual changes to workflows. Next, we performed a quantitative analysis over 49K+ GitHub repositories totaling 267K+ workflow change histories and 3.4M+ workflow file versions from November 2019 to August 2025. This analysis revealed that repositories contain a median of three workflow files, and 7.3% of all workflow files are being changed every week. The changes made to workflows tend to be small, with about three-quarters containing only a single change. The large majority of the observed changes have to do with task configuration and task specification in workflow jobs. We did not find any conclusive evidence of the effect of LLM coding tools or other major technological changes on workflow creation and workflow maintenance frequency. Our findings highlight the need for improved tooling to support fine-grained maintenance tasks, such as a broader adoption of dependency management and AI-based support for ensuring and sustaining workflow security and quality.

</details>


### [12] [Automated Classification of Source Code Changes Based on Metrics Clustering in the Software Development Process](https://arxiv.org/abs/2602.14591)
*Evgenii Kniazev*

Main category: cs.SE

TL;DR: 本文提出了一种基于变更度量聚类的源代码变更自动分类方法，通过使用k-means算法和余弦相似性度量来实现自动分发步骤，并由专家将聚类映射到预定义的变更类别。该方法在五个软件系统上进行了验证，包括两个开源项目（Subversion 和 NHibernate），并展示了良好的分类纯度和熵值。


<details>
  <summary>Details</summary>
Motivation: 为了减少代码变更审查所需的时间，提出了一个自动化的方法来对软件开发过程中的源代码变更进行分类。

Method: 该方法分为两步：首先计算每个代码变更的度量向量并进行聚类；然后由专家将得到的聚类映射到预定义的变更类别。采用了k-means算法以及度量向量之间的余弦相似性度量来进行聚类，并使用了涵盖代码行数、圈复杂度、文件数量、接口变更和结构变更等11个源代码度量标准。

Result: 该方法被应用于五个软件系统，其中包括两个开源项目(Subversion和NHibernate)。结果显示，在显著性水平为0.05的情况下，分类纯度P_C = 0.75 +/- 0.05，熵E_C = 0.37 +/- 0.06。

Conclusion: 提出的基于聚类的源代码变更自动分类方法能够有效地减少代码变更审查时间，同时保持较高的分类准确性和一致性。

Abstract: This paper presents an automated method for classifying source code changes during the software development process based on clustering of change metrics. The method consists of two steps: clustering of metric vectors computed for each code change, followed by expert mapping of the resulting clusters to predefined change classes. The distribution of changes into clusters is performed automatically, while the mapping of clusters to classes is carried out by an expert. Automation of the distribution step substantially reduces the time required for code change review. The k-means algorithm with a cosine similarity measure between metric vectors is used for clustering. Eleven source code metrics are employed, covering lines of code, cyclomatic complexity, file counts, interface changes, and structural changes. The method was validated on five software systems, including two open-source projects (Subversion and NHibernate), and demonstrated classification purity of P_C = 0.75 +/- 0.05 and entropy of E_C = 0.37 +/- 0.06 at a significance level of 0.05.

</details>


### [13] [Consistent or Sensitive? Automated Code Revision Tools Against Semantics-Preserving Perturbations](https://arxiv.org/abs/2602.14595)
*Shirin Pirouzkhah,Souhaila Serbout,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 本研究探讨了自动化代码修订（ACR）工具在处理语义等价但形式略有不同的代码变体时的一致性问题。通过设计九种语义保留扰动并应用于从真实GitHub项目中抽取的2032个Java方法，生成超过1万种变化版本来测试五种先进的基于转换器的ACR工具。结果表明，当面对语义相同但经过改动的代码时，这些工具正确生成修订的能力最多可下降45.3%。尝试了几种减轻该问题的方法，但收效甚微，指出了解决此问题仍需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决自动化代码修订(ACR)工具在现实应用中面临的一个关键挑战：即尽管这类工具在历史数据上表现良好，但由于其内在的概率性质导致它们在遇到表达相同问题的不同代码变体时可能会产生不一致的结果。这种不一致性会使得即使是语义相同的代码变体也有可能收到差异化的修订建议，从而影响了ACR工具的实际使用价值。

Method: 研究者首先定义了九种类别的语义保持扰动(SPP)，并将这些扰动应用于来自实际GitHub项目的2032个Java方法上，由此生成了超过1万个用于评估的变动版本。接着利用这批变动后的代码样例来测试五个最先进基于转换器架构的ACR工具对于语义相同但形式有异的代码变体进行一致处理的能力。

Result: 实验结果显示，在处理经过语义保持变换后得到的代码变体时，所考察的ACR工具能够准确生成修订意见的比例最高可降低45.3%。此外还观察到，扰动越接近于目标区域，则ACR工具更可能无法给出正确的修订方案。

Conclusion: 当前主流的ACR工具在应对语义等同但表面形态有所差异的代码变体时存在显著的一致性问题，这限制了它们的应用范围和可靠性。虽然研究中尝试了一些旨在改善输入表示以提高一致性的策略，但效果有限，因此如何有效提升ACR工具在这方面的能力依然是一个待解难题。

Abstract: Automated Code Revision (ACR) tools aim to reduce manual effort by automatically generating code revisions based on reviewer feedback. While ACR tools have shown promising performance on historical data, their real-world utility depends on their ability to handle similar code variants expressing the same issue - a property we define as consistency. However, the probabilistic nature of ACR tools often compromises consistency, which may lead to divergent revisions even for semantically equivalent code variants. In this paper, we investigate the extent to which ACR tools maintain consistency when presented with semantically equivalent code variants. To do so, we first designed nine types of semantics-preserving perturbations (SPP) and applied them to 2032 Java methods from real-world GitHub projects, generating over 10K perturbed variants for evaluation. Then we used these perturbations to evaluate the consistency of five state-of-the-art transformer-based ACR tools. We found that the ACR tools' ability to generate correct revisions can drop by up to 45.3%, when presented with semantically equivalent code. The closer the perturbation is to this targeted region, the more likely an ACR tool is to fail to generate the correct revision. We explored potential mitigation strategies that modify the input representation, but found that these attention-guiding heuristics yielded only marginal improvements, thus leaving the solution to this problem as an open research question.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [14] [LoPace: A Lossless Optimized Prompt Accurate Compression Engine for Large Language Model Applications](https://arxiv.org/abs/2602.13266)
*Aman Ulla*

Main category: cs.DB

TL;DR: 本研究提出了一种名为LoPace的创新压缩框架，专门用于大规模语言模型应用中的提示存储。通过结合Zstandard压缩、字节对编码标记化和二进制打包以及这两种方法的混合方式，LoPace能够在保持无损重建的前提下，平均节省72.2%的空间。在386个不同类型的提示上进行测试后，发现混合方法始终优于单独使用任一技术，并且具有良好的扩展性和较小的内存占用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型改变了自然语言处理的方式，但在生产环境中高效地存储和管理提示仍然是一个难题。

Method: 提出了LoPace（Lossless Optimized Prompt Accurate Compression Engine），一种专为LLM应用设计的新颖压缩框架。该框架采用了三种不同的数据压缩方法：基于Zstandard的压缩、字节对编码（BPE）与二进制打包相结合的标记化方法，以及这两种技术的混合方法。

Result: 通过对386个不同类型的提示进行测试，包括代码片段、Markdown文档和结构化内容，证明了LoPace能够平均节省72.2%的空间，同时仍能实现100%无损重构。混合方法总是优于单独的技术，其平均压缩比达到4.89倍（范围：1.22-19.09倍），速度为3.3至10.7 MB/秒。

Conclusion: 研究结果表明，LoPace具备生产就绪性，拥有较小的内存占用量（平均0.35MB），并且对于大型数据库和实时LLM应用程序表现出极佳的可扩展性。

Abstract: Large Language Models (LLMs) have changed the way natural language processing works, but it is still hard to store and manage prompts efficiently in production environments. This paper presents LoPace (Lossless Optimized Prompt Accurate Compression Engine), a novel compression framework designed specifically for prompt storage in LLM applications. LoPace uses three different ways to compress data: Zstandard-based compression, Byte-Pair Encoding (BPE) tokenization with binary packing, and a hybrid method that combines the two. We show that LoPace saves an average of 72.2\% of space while still allowing for 100\% lossless reconstruction by testing it on 386 different prompts, such as code snippets, markdown documentation, and structured content. The hybrid method always works better than each technique on its own. It gets mean compression ratios of 4.89x (range: 1.22--19.09x) and speeds of 3.3--10.7 MB/s. Our findings show that LoPace is ready for production, with a small memory footprint (0.35 MB on average) and great scalability for big databases and real-time LLM apps.

</details>


### [15] [MergePipe: A Budget-Aware Parameter Management System for Scalable LLM Merging](https://arxiv.org/abs/2602.13273)
*Yuanyi Wang,Yanggan Gu,Zihao Wang,Kunxi Li,Yifan Yang,Zhaoyi Yan,Congkai Xie,Jianmin Wu,Hongxia Yang*

Main category: cs.DB

TL;DR: 本研究提出了MergePipe，一种用于大规模语言模型合并的参数管理系统，它通过引入对模型参数、合并计划和执行谱系的数据管理和执行问题视角来提高可扩展性。MergePipe采用成本感知规划器，并通过流式执行引擎在事务保证下实现模型合并，从而显著减少I/O操作并加快处理速度。


<details>
  <summary>Details</summary>
Motivation: 随着特定任务或领域专家模型数量的增长，现有模型合并实施将模型参数视为无结构文件，采取无状态的一次性方式执行合并，这导致了过多的磁盘I/O、冗余参数扫描以及较差的可扩展性。为了解决这些问题，提出了MergePipe系统。

Method: MergePipe通过引入一个基于目录驱动的抽象层覆盖模型参数、合并计划及执行历史，核心部分采用了成本感知规划器，该规划器显式地建模专家参数I/O并且强制执行用户指定的I/O预算限制。随后，利用一个流式执行引擎，在具有事务保证的情况下实现合并后的模型。

Result: 实验表明，与最先进的大型语言模型合并管道相比，MergePipe能够将总的I/O降低高达一个数量级，并提供高达11倍的整体端到端加速（最多减少90%的实际运行时间）。

Conclusion: 通过将大型语言模型合并视为数据管理和执行的问题，MergePipe成功减少了I/O操作并提高了合并过程中的可扩展性和效率，证明了其在应对日益增长的专家模型集成需求方面的有效性。

Abstract: Large language model (LLM) merging has become a key technique in modern LLM development pipelines, enabling the integration of multiple task- or domain-specific expert models without retraining. However, as the number of experts grows, existing merging implementations treat model parameters as unstructured files and execute merges in a stateless, one-shot manner, leading to excessive disk I/O, redundant parameter scans, and poor scalability.
  In this paper, we present \textbf{MergePipe}, a parameter management system for scalable LLM merging. MergePipe is the first system that treats LLM merging as a data management and execution problem, and introduces a catalog-driven abstraction over model parameters, merge plans, and execution lineage. At its core, MergePipe employs a cost-aware planner that explicitly models expert parameter I/O and enforces user-specified I/O budgets, followed by a streaming execution engine that materializes merged models under transactional guarantees. Our key insight is that while base model reads and output writes are unavoidable, expert parameter reads dominate merge cost and constitute the primary optimization target. By making expert access budget-aware throughout planning and execution, MergePipe mitigates the $O(K)$ I/O growth of naive pipelines and achieves predictable scaling behavior. Experiments show that MergePipe reduces total I/O by up to an order of magnitude and delivers up to $11\times$ end-to-end speedups (up to 90\% wall-time reduction) over state-of-the-art LLM merging pipelines.

</details>


### [16] [Towards a Hybrid Quantum-Classical Computing Framework for Database Optimization Problems in Real Time Setup](https://arxiv.org/abs/2602.14263)
*Hanwen Liu,Ibrahim Sabek*

Main category: cs.DB

TL;DR: 本文提出了一种实时量子增强数据库系统，旨在解决数据库优化问题。通过开发两种互补的可扩展性策略来应对大规模挑战，并与数据库查询优化器集成作为初步原型，实现实验结果显示相比经典查询优化器最高可达14倍的改进，并且在效率和解决方案质量上优于黑盒量子求解器。


<details>
  <summary>Details</summary>
Motivation: 先前的工作通常直接将问题提交给黑盒量子或受量子启发的求解器，期望直接获得良好的最终解决方案。由于这些求解器的黑盒性质，用户无法对求解过程进行细粒度控制以平衡准确性和效率，这限制了在大多数数据库问题出现的实时场景中的灵活性。此外，这种方法处理大规模数据库优化问题的能力有限。

Method: 作者提出了一个实时量子增强数据库系统的愿景，该系统能够为数据库优化问题提供透明解决方案。他们开发了两种互补的可扩展性策略来解决超过硬件限制的大规模挑战、过度复杂性和超尺寸问题。此外，他们还将方法与数据库查询优化器进行了集成，创建了一个初步原型。

Result: 实验评估显示，在实际工作负载下，所提方法比经典查询优化器实现了高达14倍的性能提升。同时，在效率和解决方案质量方面也超过了黑盒量子求解器的表现。

Conclusion: 研究结果表明，所提出的实时量子增强数据库系统能够有效解决数据库优化问题，并在效率和解决方案质量上提供了显著的优势。

Abstract: Quantum computing has shown promise for solving complex optimization problems in databases, such as join ordering and index selection. Prior work often submits formulated problems directly to black-box quantum or quantum-inspired solvers with the expectation of directly obtaining a good final solution. Due to the black-box nature of these solvers, users cannot perform fine-grained control over the solving procedure to balance the accuracy and efficiency, which in turn limits flexibility in real-time settings where most database problems arise. Moreover, it leads to limited potential for handling large-scale database optimization problems. In this paper, we propose a vision for the first real-time quantum-augmented database system, enabling transparent solutions for database optimization problems. We develop two complementary scalability strategies to address large-scale challenges, overcomplexity, and oversizing that exceed hardware limits. We integrate our approach with a database query optimizer as a preliminary prototype, evaluating on real-world workload, achieving up to 14x improvement over the classical query optimizer. We also achieve both better efficiency and solution quality than a black-box quantum solver.

</details>


### [17] [Qute: Towards Quantum-Native Database](https://arxiv.org/abs/2602.14699)
*Muzhi Chen,Xuanhe Zhou,Wei Zhou,Bangrui Xu,Surui Tang,Guoliang Li,Bingsheng He,Yeye He,Yitong Song,Fan Wu*

Main category: cs.DB

TL;DR: 本论文提出了一种名为Qute的量子数据库，它将量子计算视为主要执行选项。通过编译扩展形式的SQL为高效的量子电路、采用混合优化器动态选择执行计划、引入选择性量子索引以及设计保真度存储方案来克服当前量子比特限制，并展示了在实际量子处理器上的性能优于经典基线。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决现有方法要么在经典机器上运行量子算法，要么调整已有数据库以适应量子模拟的问题，提出了一个将量子计算作为首选执行方式的量子数据库解决方案。

Method: 通过将一种扩展形式的SQL编译成门效率高的量子电路；使用混合优化器根据情况动态地在量子和经典执行计划之间进行选择；引入了选择性量子索引技术；设计了保持保真度的存储方案以缓解当前量子比特数量有限带来的挑战。

Result: 在真实量子处理器（origin_wukong）上部署Qute后，其表现超过了大规模的经典基线系统。此外，还公开了一个开源原型。

Conclusion: 这项工作展示了量子数据库作为一种新型数据处理方式的巨大潜力，并为向量子原生数据库过渡提供了三阶段演进路线图。

Abstract: This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.

</details>


### [18] [A Unified Mathematical Framework for Distributed Data Fabrics: Categorical Hypergraph Models](https://arxiv.org/abs/2602.14708)
*T. Shaska,I. Kotsireas*

Main category: cs.DB

TL;DR: 本文提出了一种基于超图结构的数学框架，用于统一分布式系统中的异构数据管理。通过将数据集、元数据、转换、策略和分析建模为一个分布式的系统，并利用范畴论方法支持数据集成和联邦学习等操作。此外，还探讨了该框架在保证一致性、完整性和因果关系的同时处理NP难问题如模式匹配和动态分区的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式数据结构缺乏严格的数学基础，经常依赖于临时架构，在一致性、系谱和扩展性方面存在问题。因此，需要一种新的数学框架来解决这些问题。

Method: 作者提出了一个基于超图结构\( \mathcal{F} = (D, M, G, T, P, A) \)的数据结构数学框架，其中包含了数据集、元数据、转换、策略和分析等元素。采用范畴论的方式，将数据集视为对象而转换作为态射，以支持数据集成与联邦学习等功能。进一步地，将此超图嵌入到模块化张量类别中，通过编织幺半群结构捕捉关系对称性，并借助Hurwitz空间的几何类比增强代数建模能力。

Result: 研究证明了诸如模式匹配和动态分区等关键任务属于NP难问题，并提出了光谱方法和基于对称性的校准作为可扩展解决方案。该框架确保在CAP和CAL定理下的一致性、完整性和因果关系，同时利用稀疏关联矩阵和编织动作实现容错操作。

Conclusion: 所提出的数学框架为分布式系统的异构数据管理提供了一个统一且强大的工具，能够有效应对一致性、扩展性挑战以及复杂的计算任务。

Abstract: Current distributed data fabrics lack a rigorous mathematical foundation, often relying on ad-hoc architectures that struggle with consistency, lineage, and scale. We propose a mathematical framework for data fabrics, unifying heterogeneous data management in distributed systems through a hypergraph-based structure \( \mathcal{F} = (D, M, G, T, P, A) \). Datasets, metadata, transformations, policies, and analytics are modeled over a distributed system \( Σ= (N, C) \), with multi-way relationships encoded in a hypergraph \( G = (V, E) \). A categorical approach, with datasets as objects and transformations as morphisms, supports operations like data integration and federated learning. The hypergraph is embedded into a modular tensor category, capturing relational symmetries via braided monoidal structures, with geometric analogies to Hurwitz spaces enriching the algebraic modeling. We prove the NP-hardness of critical tasks, such as schema matching and dynamic partitioning, and propose spectral methods and symmetry-based alignments for scalable solutions. The framework ensures consistency, completeness, and causality under CAP and CAL theorems, leveraging sparse incidence matrices and braiding actions for fault-tolerant operations.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [19] [SRA: Semantic Relation-Aware Flowchart Question Answering](https://arxiv.org/abs/2602.13771)
*Xinyu Li,Bowei Zou,Yuchong Chen,Yifan Fan,Yu Hong*

Main category: cs.MM

TL;DR: 提出了一种新的语义关系感知(SRA)流程图问答方法，该方法利用大型语言模型(LLM)来检测节点间的语义关系，并根据问题意图控制推理过程的深度及选择合适的中间语言。实验表明，SRA方法在提升不同中间语言如Graphviz、Mermaid和Plantuml时取得了广泛改进。


<details>
  <summary>Details</summary>
Motivation: 现有的流程图转换为中间语言的方法虽然能够有效连接问题与流程图之间的模态差异并揭示节点间的链接关系，但仍然缺乏对复杂语义/逻辑关系（如条件和因果关系）的处理能力，这限制了对复杂问题进行深入推理的能力。

Method: 提出了一个名为SRA的新方法，通过使用大型语言模型识别节点间的话语语义关系，将基于链接的中间语言升级为基于语义关系的中间语言。同时实施了一个可控制的中间语言推理过程，在此过程中分析问题意图以确定推理深度（浅层或深层推理）以及相匹配的中间语言。

Result: 在FlowVQA基准数据集上的测试结果显示，当升级不同的中间语言（如Graphviz、Mermaid和Plantuml）时，SRA方法表现出普遍性的改进效果。

Conclusion: 本研究提出的SRA FlowchartQA方法有效地解决了现有流程图问答技术中忽视复杂语义/逻辑关系的问题，通过引入更深层次的语义理解机制，提高了对于复杂问题解答的准确性。

Abstract: Flowchart Question Answering (FlowchartQA) is a multi-modal task that automatically answers questions conditioned on graphic flowcharts. Current studies convert flowcharts into interlanguages (e.g., Graphviz) for Question Answering (QA), which effectively bridge modal gaps between questions and flowcharts. More importantly, they reveal the link relations between nodes in the flowchart, facilitating a shallow relation reasoning during tracing answers. However, the existing interlanguages still lose sight of intricate semantic/logic relationships such as Conditional and Causal relations. This hinders the deep reasoning for complex questions. To address the issue, we propose a novel Semantic Relation-Aware (SRA) FlowchartQA approach. It leverages Large Language Model (LLM) to detect the discourse semantic relations between nodes, by which a link-based interlanguage is upgraded to the semantic relation based interlanguage. In addition, we conduct an interlanguage-controllable reasoning process. In this process, the question intention is analyzed with the aim to determine the depth of reasoning (Shallow or Deep reasoning), as well as the well-matched interlanguage. We experiment on the benchmark dataset FlowVQA. The test results show that SRA yields widespread improvements when upgrading different interlanguages like Graphviz, Mermaid and Plantuml

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies](https://arxiv.org/abs/2602.14107)
*Yuze Liu,Shibo Chu,Tiehua Zhang,Hao Zhou,Zhishu Shen,Jinze Wang,Jianzhong Qi,Feng Xia*

Main category: cs.DC

TL;DR: 本文提出了一种名为ML-ECS的协作多模态学习框架，旨在解决边缘云协同环境下的模态异质性和模型结构异质性问题。通过引入跨模态对比学习(CCL)、自适应多模态调整(AMT)、模态感知模型聚合(MMA)以及SLM增强CCL(SE-CCL)，该框架在不同模态可用性条件下实现了对最先进基线的持续超越，并且以极高的通信效率运行。


<details>
  <summary>Details</summary>
Motivation: 针对现实世界边缘环境中因模态组合及特定编码器/融合模块差异导致的合作多模态学习挑战，提出了一个能够处理模态异质性和模型结构异质性的新框架。

Method: 开发了ML-ECS框架，包括四个关键组件：跨模态对比学习用于统一潜在空间中的模态表示；自适应多模态调优来保持本地数据集特有的知识；模态意识模型聚合以稳健地汇总信息并减少由于缺失模态引起的噪声；以及SLM增强的CCL促进云端与边缘间双向知识转移。

Result: 实验结果表明，在各种多模态任务上，即使面对变化的模态可用性条件，ML-ECS也能够优于当前最先进的基准，Rouge-LSum指标提升了5.44%到12.08%，同时提高了客户端和服务器端的表现。此外，通过仅交流低秩LoRA参数和融合表示，ML-ECS达成了仅需总参数体积0.65%的高效通讯。

Conclusion: ML-ECS为边缘云协同场景下隐私保护型基础模型部署提供了一个有效解决方案，它不仅解决了模态和模型结构异质性带来的挑战，还在性能和通信效率方面表现出色。

Abstract: Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.

</details>


### [21] [Floe: Federated Specialization for Real-Time LLM-SLM Inference](https://arxiv.org/abs/2602.14302)
*Chunlin Tian,Kahou Tam,Yebo Wu,Shuaihang Zhong,Li Li,Nicholas D. Lane,Chengzhong Xu*

Main category: cs.DC

TL;DR: Floe, a hybrid federated learning framework, integrates cloud-based LLMs with on-edge SLMs to achieve low-latency and privacy-preserving inference in real-time systems, enhancing both performance and personalization.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges of deploying large language models (LLMs) in real-time systems, particularly focusing on reducing computational demands, preserving user privacy, and improving the efficiency of model deployment across various hardware environments.

Method: The method proposed in this paper, Floe, involves a combination of a cloud-based black-box LLM for general knowledge and lightweight small language models (SLMs) deployed on edge devices. It utilizes an on-device fine-tuning process and a heterogeneity-aware LoRA adaptation strategy to facilitate efficient deployment. Additionally, a logit-level fusion mechanism ensures smooth coordination between the cloud and edge models.

Result: Experimental results show that Floe not only enhances user privacy and personalization but also significantly improves model performance and reduces inference latency on edge devices, outperforming baseline methods under real-time constraints.

Conclusion: In conclusion, Floe represents a promising solution for deploying LLMs in real-time, resource-constrained, and latency-sensitive environments, offering a balanced approach to privacy, performance, and personalization.

Abstract: Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.

</details>


### [22] [Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement](https://arxiv.org/abs/2602.14704)
*Zong Yu Lee,Xueyan Tang*

Main category: cs.DC

TL;DR: 本文评估了在非预见性、预见性和学习增强在线设置下的最新MinUsageTime DVBP算法，用于虚拟机放置问题，并通过微软Azure的真实数据集进行了实验，讨论了实验结果的见解。


<details>
  <summary>Details</summary>
Motivation: 为了提高数据中心物理机器资源的利用效率，解决云环境中虚拟机放置这一关键挑战。

Method: 将虚拟机放置定义为一个旨在最小化物理机器总使用时间的MinUsageTime DVBP问题，并在不同场景下（未知、已知和预测虚拟机生命周期）评估现有及新开发或改进的算法。

Result: 通过真实世界的数据集验证了算法的有效性，并从实验结果中得出了关于算法结构和有效设计元素的洞见。

Conclusion: 对于虚拟机放置问题，不同的MinUsageTime DVBP算法各有优势；基于实验分析，提出了一些实际应用中表现良好的设计建议。

Abstract: Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [23] [Unleash the Potential of Long Semantic IDs for Generative Recommendation](https://arxiv.org/abs/2602.13573)
*Ming Xia,Zhiqin Zhou,Guoxin Ma,Dongmin Huang*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架ACERec，通过引入注意力令牌合并器和意图令牌解决了语义ID生成推荐中表达性和计算效率之间的权衡问题。实验表明，该方法在六个真实数据集上显著优于现有技术，特别是在NDCG@10指标上平均提高了14.40%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于残差量化的方法为了便于处理限制了语义ID的长度，而基于优化乘积量化的技术则通过简单的刚性聚合压缩长语义ID，导致细粒度语义信息丢失。为了解决这一矛盾，即如何在保持高效序列建模的同时不损失丰富的语义表达能力，提出了新框架ACERec。

Method: ACERec框架采用注意力令牌合并器将长且具有表现力的语义令牌提炼成紧凑的潜在表示，并引入专门的意图令牌作为动态预测锚点。此外，通过双粒度目标引导学习过程，旨在协调细粒度令牌预测与全局项目级语义对齐。

Result: 在六个实际基准测试中的广泛实验证明，ACERec持续优于最先进的基线，在NDCG@10指标上平均提升达14.40%，成功地调和了语义表达能力和计算效率之间的关系。

Conclusion: 研究结果表明，通过ACERec框架可以在保持较高语义表达能力的同时提高计算效率，从而有效解决当前基于语义ID的生成推荐中存在的难题。

Abstract: Semantic ID-based generative recommendation represents items as sequences of discrete tokens, but it inherently faces a trade-off between representational expressiveness and computational efficiency. Residual Quantization (RQ)-based approaches restrict semantic IDs to be short to enable tractable sequential modeling, while Optimized Product Quantization (OPQ)-based methods compress long semantic IDs through naive rigid aggregation, inevitably discarding fine-grained semantic information. To resolve this dilemma, we propose ACERec, a novel framework that decouples the granularity gap between fine-grained tokenization and efficient sequential modeling. It employs an Attentive Token Merger to distill long expressive semantic tokens into compact latents and introduces a dedicated Intent Token serving as a dynamic prediction anchor. To capture cohesive user intents, we guide the learning process via a dual-granularity objective, harmonizing fine-grained token prediction with global item-level semantic alignment. Extensive experiments on six real-world benchmarks demonstrate that ACERec consistently outperforms state-of-the-art baselines, achieving an average improvement of 14.40\% in NDCG@10, effectively reconciling semantic expressiveness and computational efficiency.

</details>


### [24] [Climber-Pilot: A Non-Myopic Generative Recommendation Model Towards Better Instruction-Following](https://arxiv.org/abs/2602.13581)
*Da Guo,Shijia Wang,Qiang Xiao,Yintao Ren,Weisheng Li,Songpei Xu,Ming Yue,Bin Huang,Guanlin Wu,Chuanjiang Luo*

Main category: cs.IR

TL;DR: 本文提出了一种统一的生成式检索框架Climber-Pilot，旨在解决生成式推荐系统中的短视问题和指令跟随挑战。通过引入时间感知多项目预测(TAMIP)训练范式以及条件引导稀疏注意力(CGSA)机制，该方法在保持单步推理效率的同时提高了长期多项目的预测准确性，并直接将业务约束融入生成过程。网易云音乐平台上的离线实验和在线A/B测试表明，Climber-Pilot相比现有最先进基线，在核心业务指标上提升了4.24%。


<details>
  <summary>Details</summary>
Motivation: 生成式检索为推荐系统提供了优于传统双塔架构的序列建模能力，但在大规模工业场景中，这类模型往往因单步推理及严格的延迟限制而表现出固有的短视性，即倾向于将多样化的用户意图简化为局部最优预测，无法捕捉到长期和多项目的消费模式。此外，现实世界的检索系统还需遵循如类别级控制等明确检索指令，但将这种指令跟随行为整合进生成式检索中仍面临挑战，因为现有的条件作用或事后过滤方法往往会损害相关性或效率。

Method: 1. 提出了一种名为时间感知多项目预测（TAMIP）的新颖训练范式，旨在缓解生成式检索中的固有短视现象。通过时间感知遮蔽技术将长期、多项目的预见性提炼至模型参数中，TAMIP在保持高效单步推理的同时减少了局部最优预测的问题。
2. 为了支持灵活的指令跟随检索，开发了条件引导稀疏注意力（CGSA）机制，它利用稀疏注意力直接在生成过程中加入业务约束，无需增加额外的推理步骤。

Result: 广泛的离线实验与网易云音乐平台上的在线A/B测试显示，Climber-Pilot显著优于最先进的基准模型，实现了核心业务指标4.24%的增长。

Conclusion: Climber-Pilot作为一个综合性的生成式检索框架，成功解决了生成式推荐系统中存在的短视问题以及对复杂业务规则的支持难题，证明了其在实际应用中的有效性。

Abstract: Generative retrieval has emerged as a promising paradigm in recommender systems, offering superior sequence modeling capabilities over traditional dual-tower architectures. However, in large-scale industrial scenarios, such models often suffer from inherent myopia: due to single-step inference and strict latency constraints, they tend to collapse diverse user intents into locally optimal predictions, failing to capture long-horizon and multi-item consumption patterns. Moreover, real-world retrieval systems must follow explicit retrieval instructions, such as category-level control and policy constraints. Incorporating such instruction-following behavior into generative retrieval remains challenging, as existing conditioning or post-hoc filtering approaches often compromise relevance or efficiency. In this work, we present Climber-Pilot, a unified generative retrieval framework to address both limitations. First, we introduce Time-Aware Multi-Item Prediction (TAMIP), a novel training paradigm designed to mitigate inherent myopia in generative retrieval. By distilling long-horizon, multi-item foresight into model parameters through time-aware masking, TAMIP alleviates locally optimal predictions while preserving efficient single-step inference. Second, to support flexible instruction-following retrieval, we propose Condition-Guided Sparse Attention (CGSA), which incorporates business constraints directly into the generative process via sparse attention, without introducing additional inference steps. Extensive offline experiments and online A/B testing at NetEase Cloud Music, one of the largest music streaming platforms, demonstrate that Climber-Pilot significantly outperforms state-of-the-art baselines, achieving a 4.24\% lift of the core business metric.

</details>


### [25] [GEMs: Breaking the Long-Sequence Barrier in Generative Recommendation with a Multi-Stream Decoder](https://arxiv.org/abs/2602.13631)
*Yu Zhou,Chengcheng Guo,Kuo Cai,Ji Liu,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: GEMs, a multi-stream decoder framework for generative recommendations, effectively handles long user behavior sequences by dividing them into Recent, Mid-term, and Lifecycle streams, each with its own processing method, to improve recommendation accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Generative recommendations (GR) struggle with the high computational cost of processing very long user behavior sequences, which limits the sequence length and hinders the model's ability to capture users' lifelong interests. Additionally, attention mechanisms tend to have a 'recency bias', making it difficult to learn from long-term history.

Method: GEMs (Generative rEcommendation with a Multi-stream decoder) is introduced as a new framework that overcomes the challenge of long sequences by splitting user behaviors into three temporal streams: Recent, Mid-term, and Lifecycle. Each stream uses a different inference scheme: a real-time extractor for recent dynamics, a lightweight indexer for mid-term cross-attention, and a two-stage offline-online module for lifelong modeling. The streams are combined using a parameter-free fusion strategy to create a comprehensive interest representation.

Result: Experiments on large-scale industrial datasets show that GEMs surpasses existing methods in terms of recommendation accuracy. It has also been successfully implemented in an industrial setting, demonstrating exceptional inference efficiency while handling user sequences exceeding 100,000 interactions.

Conclusion: GEMs presents a novel approach to generative recommendations by efficiently managing extremely long user behavior sequences through a multi-stream perspective, thereby enhancing both the accuracy and efficiency of recommendations.

Abstract: While generative recommendations (GR) possess strong sequential reasoning capabilities, they face significant challenges when processing extremely long user behavior sequences: the high computational cost forces practical sequence lengths to be limited, preventing models from capturing users' lifelong interests; meanwhile, the inherent "recency bias" of attention mechanisms further weakens learning from long-term history. To overcome this bottleneck, we propose GEMs (Generative rEcommendation with a Multi-stream decoder), a novel and unified framework designed to break the long-sequence barrier by capturing users' lifelong interaction sequences through a multi-stream perspective. Specifically, GEMs partitions user behaviors into three temporal streams$\unicode{x2014}$Recent, Mid-term, and Lifecycle$\unicode{x2014}$and employs tailored inference schemes for each: a one-stage real-time extractor for immediate dynamics, a lightweight indexer for cross attention to balance accuracy and cost for mid-term sequences, and a two-stage offline-online compression module for lifelong modeling. These streams are integrated via a parameter-free fusion strategy to enable holistic interest representation. Extensive experiments on large-scale industrial datasets demonstrate that GEMs significantly outperforms state-of-the-art methods in recommendation accuracy. Notably, GEMs is the first lifelong GR framework successfully deployed in a high-concurrency industrial environment, achieving superior inference efficiency while processing user sequences of over 100,000 interactions.

</details>


### [26] [PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers](https://arxiv.org/abs/2602.13647)
*Rui Yu,Tianyi Wang,Ruixia Liu,Yinglong Wang*

Main category: cs.IR

TL;DR: 本文提出了一种名为PT-RAG的新框架，该框架通过保留学术论文的原生层次结构来提高检索增强生成（RAG）在长篇学术文章问答中的性能。它首先构建了一个忠实于结构的PaperTree索引，然后设计了一种路径引导的检索机制，以在固定令牌预算下选择相关度高的从根到叶的路径，从而产生紧凑、连贯且熵低的检索上下文。实验结果表明，PT-RAG相比现有方法能显著减少上下文碎片化，并更准确地将令牌分配给证据区域，最终提高了答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成(RAG)方法在处理长篇学术文章时，通常会破坏原文档的层次结构，导致检索空间变得无序，进而产生片段化的上下文，不恰当地将有限的令牌资源分配给了非证据部分，增加了下游语言模型的推理负担。为了解决这些问题，提出了新的PT-RAG框架。

Method: PT-RAG框架首先继承了原始文档的层级结构，创建了一个保持结构保真度的PaperTree索引，以此作为低熵检索先验。随后，该框架引入了一种路径引导的检索机制，根据查询语义匹配相关章节，并在固定令牌预算内挑选高相关性的从根至叶路径，从而生成紧凑、一致且熵值较低的检索内容。

Result: 通过对三个学术问答基准数据集进行评估，与强基线相比，PT-RAG实现了更低的章节熵和证据对齐交叉熵，表明其能够有效降低上下文碎片化程度，并更加精准地向证据区域分配令牌资源。这些结构性的优势直接转化为更高的回答质量。

Conclusion: PT-RAG通过利用学术论文固有的层次结构作为低熵检索基础，在避免预处理过程中熵增加的同时，为后续检索提供了自然的低熵结构基础。实验证明，这种方法不仅减少了检索内容的碎片化，还提高了证据区域令牌分配的准确性，从而提升了整体的回答质量。

Abstract: Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.

</details>


### [27] [Pailitao-VL: Unified Embedding and Reranker for Real-Time Multi-Modal Industrial Search](https://arxiv.org/abs/2602.13704)
*Lei Chen,Chen Ju,Xu Chen,Zhicheng Wang,Yuheng Jiao,Hongfeng Zhan,Zhaoyang Li,Shihao Xu,Zhixiang Zhao,Tong Jia,Jinsong Lan,Xiaoyong Zhu,Bo Zheng*

Main category: cs.IR

TL;DR: 本文介绍了Pailitao-VL，一个针对高精度实时工业搜索设计的多模态检索系统。通过解决现有最佳解决方案中的三个关键挑战，并引入两种基础性范式转变，该系统实现了先进的性能并在阿里巴巴电商平台的实际测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 当前最佳解决方案存在检索粒度不足、易受环境噪声影响以及效率与性能之间的差距等问题。为了解决这些挑战，研究团队开发了Pailitao-VL系统，旨在提高检索的准确性和效率。

Method: 1. 从传统的对比学习转向绝对ID识别任务，通过将实例锚定到由数十亿语义原型定义的全局一致潜在空间来克服现有嵌入解决方案中的随机性和粒度瓶颈。
2. 将生成式重排序器从孤立的点评估进化至比较校准列表策略，结合块级比较推理与校准后的绝对相关性评分以实现细致的区别分辨率同时避免传统重排序方法带来的延迟问题。

Result: 广泛的离线基准测试和在线A/B测试表明，Pailitao-VL在阿里巴巴电商平台上达到了最先进的性能水平，并带来了重要的商业影响。

Conclusion: 本研究表明，通过采用先进的MLLM（多模态语言模型）基础架构，即使是在要求苛刻的大规模生产环境中也能够部署稳健且可扩展的检索系统。

Abstract: In this work, we presented Pailitao-VL, a comprehensive multi-modal retrieval system engineered for high-precision, real-time industrial search. We here address three critical challenges in the current SOTA solution: insufficient retrieval granularity, vulnerability to environmental noise, and prohibitive efficiency-performance gap. Our primary contribution lies in two fundamental paradigm shifts. First, we transitioned the embedding paradigm from traditional contrastive learning to an absolute ID-recognition task. Through anchoring instances to a globally consistent latent space defined by billions of semantic prototypes, we successfully overcome the stochasticity and granularity bottlenecks inherent in existing embedding solutions. Second, we evolved the generative reranker from isolated pointwise evaluation to the compare-and-calibrate listwise policy. By synergizing chunk-based comparative reasoning with calibrated absolute relevance scoring, the system achieves nuanced discriminative resolution while circumventing the prohibitive latency typically associated with conventional reranking methods. Extensive offline benchmarks and online A/B tests on Alibaba e-commerce platform confirm that Pailitao-VL achieves state-of-the-art performance and delivers substantial business impact. This work demonstrates a robust and scalable path for deploying advanced MLLM-based retrieval architectures in demanding, large-scale production environments.

</details>


### [28] [DMESR: Dual-view MLLM-based Enhancing Framework for Multimodal Sequential Recommendation](https://arxiv.org/abs/2602.13715)
*Mingyao Huang,Qidong Liu,Wenxuan Yang,Moranxin Wang,Yuqi Sun,Haiping Zhu,Feng Tian,Yan Chen*

Main category: cs.IR

TL;DR: 提出了一种基于双视角MLLM的增强框架（DMESR），旨在解决现有MLLM增强推荐方法中存在的跨模态语义表示不一致和细粒度语义信息丢失的问题，通过对比学习机制对齐跨模态语义，并引入交叉注意力融合模块结合粗略与细粒度语义知识，实验表明该方法有效且具有良好的泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于多模态大语言模型(MLLM)的推荐方法在利用多模态理解能力来丰富项目语义表示时遇到两大问题：一是难以有效地对齐多模态表示，导致跨模态语义信息利用不足；二是过度依赖MLLM生成的内容而忽视了项目原始文本数据中包含的细粒度语义线索。为了解决这些问题，提出了一个名为DMESR的新框架。

Method: DMESR框架采用了对比学习机制以改善由MLLM生成的跨模态语义表示之间的对齐情况，并设计了一个交叉注意力融合模块，该模块能够将从MLLM获取的粗略语义知识与来自项目原始文本的细粒度语义相结合。之后，这两种融合后的表征可以平滑地集成到下游序列推荐模型中去。

Result: 在三个真实世界的数据集以及三种流行的序列推荐架构上进行了广泛的实验验证，结果表明所提出的DMESR方法不仅在效果上优于其他方案，而且展现出很强的通用性。

Conclusion: 研究证明了通过采用对比学习机制来调整跨模态语义表示并结合交叉注意力融合模块整合不同层次语义信息的方法，可以显著提高基于MLLM的序列推荐系统的性能。

Abstract: Sequential Recommender Systems (SRS) aim to predict users' next interaction based on their historical behaviors, while still facing the challenge of data sparsity. With the rapid advancement of Multimodal Large Language Models (MLLMs), leveraging their multimodal understanding capabilities to enrich item semantic representation has emerged as an effective enhancement strategy for SRS. However, existing MLLM-enhanced recommendation methods still suffer from two key limitations. First, they struggle to effectively align multimodal representations, leading to suboptimal utilization of semantic information across modalities. Second, they often overly rely on MLLM-generated content while overlooking the fine-grained semantic cues contained in the original textual data of items. To address these issues, we propose a Dual-view MLLM-based Enhancing framework for multimodal Sequential Recommendation (DMESR). For the misalignment issue, we employ a contrastive learning mechanism to align the cross-modal semantic representations generated by MLLMs. For the loss of fine-grained semantics, we introduce a cross-attention fusion module that integrates the coarse-grained semantic knowledge obtained from MLLMs with the fine-grained original textual semantics. Finally, these two fused representations can be seamlessly integrated into the downstream sequential recommendation models. Extensive experiments conducted on three real-world datasets and three popular sequential recommendation architectures demonstrate the superior effectiveness and generalizability of our proposed approach.

</details>


### [29] [MixFormer: Co-Scaling Up Dense and Sequence in Industrial Recommenders](https://arxiv.org/abs/2602.14110)
*Xu Huang,Hao Zhang,Zhifang Fan,Yunwen Huang,Zhuoxing Wei,Zheng Chai,Jinan Ni,Yuchao Zheng,Qiwei Chen*

Main category: cs.IR

TL;DR: 提出了一种名为MixFormer的统一Transformer架构，旨在为推荐系统同时建模序列行为和特征交互，通过集成设计提高了模型容量的有效分配，并且在大规模工业数据集上展现了优越的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的推荐系统模型在结构上是碎片化的，将序列建模和特征交互作为独立模块实现，这导致了在有限计算预算下模型容量必须在密集特征交互与序列建模之间进行次优分配的问题。

Method: 提出了MixFormer，一种专门为推荐系统定制的统一Transformer风格架构，它在一个单一框架内共同建模序列行为和特征交互。此外，还引入了一种用户-项目解耦策略来优化效率，减少冗余计算和推理延迟。

Result: 广泛的实验表明，MixFormer在大规模工业数据集上始终表现出更高的准确性和效率。在线A/B测试也显示，在两个生产推荐系统（抖音和抖音轻量版）中，用户参与度指标有所提高。

Conclusion: MixFormer通过其统一的设计有效地解决了现有推荐系统中的共扩展难题，不仅增强了模型的表现力，同时也保持了工业实用性。

Abstract: As industrial recommender systems enter a scaling-driven regime, Transformer architectures have become increasingly attractive for scaling models towards larger capacity and longer sequence. However, existing Transformer-based recommendation models remain structurally fragmented, where sequence modeling and feature interaction are implemented as separate modules with independent parameterization. Such designs introduce a fundamental co-scaling challenge, as model capacity must be suboptimally allocated between dense feature interaction and sequence modeling under a limited computational budget. In this work, we propose MixFormer, a unified Transformer-style architecture tailored for recommender systems, which jointly models sequential behaviors and feature interactions within a single backbone. Through a unified parameterization, MixFormer enables effective co-scaling across both dense capacity and sequence length, mitigating the trade-off observed in decoupled designs. Moreover, the integrated architecture facilitates deep interaction between sequential and non-sequential representations, allowing high-order feature semantics to directly inform sequence aggregation and enhancing overall expressiveness. To ensure industrial practicality, we further introduce a user-item decoupling strategy for efficiency optimizations that significantly reduce redundant computation and inference latency. Extensive experiments on large-scale industrial datasets demonstrate that MixFormer consistently exhibits superior accuracy and efficiency. Furthermore, large-scale online A/B tests on two production recommender systems, Douyin and Douyin Lite, show consistent improvements in user engagement metrics, including active days and in-app usage duration.

</details>


### [30] [High Precision Audience Expansion via Extreme Classification in a Two-Sided Marketplace](https://arxiv.org/abs/2602.14358)
*Dillon Davis,Huiji Gao,Thomas Legrand,Juan Manuel Caicedo Carvajal,Malay Haldar,Kedar Bellare,Moutupsi Paul,Soumyadip Banerjee,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 本文介绍了Airbnb如何通过将世界划分为2500万个均匀单元格，并从中检索最可预订的高精度矩形地图单元，来重构其搜索系统以提高位置检索效率。


<details>
  <summary>Details</summary>
Motivation: Airbnb需要平衡全球范围内多样化的房源供应与客人的不同需求（如地点、设施、风格和价格预期）。为了满足这些期望，关键在于一个高效的检索阶段，即在应用资源密集型排序模型之前，仅展示客人可能实际预订的房源。与许多推荐引擎不同，该系统面临一个独特的挑战——位置检索，它位于排序之前，决定了查询哪些地理区域以过滤出候选房源集合。

Method: 文章提出了一种新的方法，即将整个世界划分为2500万个一致大小的单元格，并从这些单元格中识别出最有可能被预订且具有高精度的位置矩形区域作为检索范围。这取代了原有的基于深度贝叶斯强盗算法预测矩形检索边界的方法。

Result: 通过采用这种方法，Airbnb能够显著提升位置检索的质量，从而为用户提供更相关的结果。

Conclusion: 通过对搜索架构进行重新设计，专注于从高度可预订的精确地理单元中检索信息，Airbnb能够更好地应对位置检索带来的独特挑战，最终改善用户体验。

Abstract: Airbnb search must balance a worldwide, highly varied supply of homes with guests whose location, amenity, style, and price expectations differ widely. Meeting those expectations hinges on an efficient retrieval stage that surfaces only the listings a guest might realistically book, before resource intensive ranking models are applied to determine the best results. Unlike many recommendation engines, our system faces a distinctive challenge, location retrieval, that sits upstream of ranking and determines which geographic areas are queried in order to filter inventory to a candidate set. The preexisting approach employs a deep bayesian bandit based system to predict a rectangular retrieval bounds area that can be used for filtering. The purpose of this paper is to demonstrate the methodology, challenges, and impact of rearchitecting search to retrieve from the subset of most bookable high precision rectangular map cells defined by dividing the world into 25M uniform cells.

</details>


### [31] [Behavioral Feature Boosting via Substitute Relationships for E-commerce Search](https://arxiv.org/abs/2602.14502)
*Chaosheng Dong,Michinari Momma,Yijia Wang,Yan Gao,Yi Sun*

Main category: cs.IR

TL;DR: 提出了一种基于替代关系的产品行为特征增强方法（BFS），通过聚合相似产品的行为信号来缓解新产品的冷启动问题，提高了电商平台上新产品的搜索相关性和发现率。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台上的新产品由于互动数据有限而面临冷启动问题，这会降低它们的搜索可见性并影响相关性排名。为了解决这个问题，提出了一个简单但有效的方法。

Method: 利用产品间的替代关系（即满足相似用户需求的产品）并通过聚合这些替代品的行为信号（如点击、加入购物车、购买和评分）来给新产品提供一个温暖的开始。将这些丰富后的信号整合进排名模型中，以减轻冷启动效应，并提高相关性和竞争力。

Result: 在线上和线下实验中表明，BFS显著提升了冷启动产品的搜索相关性和产品发现能力。此外，BFS是可扩展且实用的，在改善用户体验的同时增加了新发布商品在电商搜索中的曝光度。

Conclusion: BFS增强型排名模型已经投入生产使用，自2025年起服务于客户，证明了其在实际应用中的价值。

Abstract: On E-commerce platforms, new products often suffer from the cold-start problem: limited interaction data reduces their search visibility and hurts relevance ranking. To address this, we propose a simple yet effective behavior feature boosting method that leverages substitute relationships among products (BFS). BFS identifies substitutes-products that satisfy similar user needs-and aggregates their behavioral signals (e.g., clicks, add-to-carts, purchases, and ratings) to provide a warm start for new items. Incorporating these enriched signals into ranking models mitigates cold-start effects and improves relevance and competitiveness. Experiments on a large E-commerce platform, both offline and online, show that BFS significantly improves search relevance and product discovery for cold-start products. BFS is scalable and practical, improving user experience while increasing exposure for newly launched items in E-commerce search. The BFS-enhanced ranking model has been launched in production and has served customers since 2025.

</details>


### [32] [Adaptive Autoguidance for Item-Side Fairness in Diffusion Recommender Systems](https://arxiv.org/abs/2602.14706)
*Zihan Li,Gustavo Escobedo,Marta Moscati,Oleg Lesota,Markus Schedl*

Main category: cs.IR

TL;DR: 本文提出了一种新的扩散推荐系统A2G-DiffRec，它通过自适应自我引导来减少流行度偏差，并在多个数据集上显示出在提高项目公平性的同时仅轻微牺牲了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散推荐系统虽然能够实现强大的推荐准确性，但往往存在流行度偏差问题，导致不同项目的曝光率不均等。为了解决这一问题，作者提出了A2G-DiffRec方法。

Method: A2G-DiffRec利用一个训练程度较低的自身版本作为指导模型，通过学习如何自适应地调整主模型和弱模型输出之间的权重，同时由促进不同流行度级别项目间平衡曝光的流行度正则化监督整个过程。

Result: 实验结果表明，在MovieLens-1M、Foursquare-Tokyo以及Music4All-Onion数据集上，与现有引导式扩散推荐器及其他非扩散基线相比，A2G-DiffRec能够在几乎不影响准确性的前提下有效提升对物品端的公平性。

Conclusion: A2G-DiffRec为解决扩散推荐系统中的流行度偏差问题提供了一个有效的解决方案，通过引入自适应自动引导机制显著提高了推荐过程中项目展示的公平性。

Abstract: Diffusion recommender systems achieve strong recommendation accuracy but often suffer from popularity bias, resulting in unequal item exposure. To address this shortcoming, we introduce A2G-DiffRec, a diffusion recommender that incorporates adaptive autoguidance, where the main model is guided by a less-trained version of itself. Instead of using a fixed guidance weight, A2G-DiffRec learns to adaptively weigh the outputs of the main and weak models during training, supervised by a popularity regularization that promotes balanced exposure across items with different popularity levels. Experimental results on the MovieLens-1M, Foursquare-Tokyo, and Music4All-Onion datasets show that A2G-DiffRec is effective in enhancing item-side fairness at a marginal cost of accuracy reduction compared to existing guided diffusion recommenders and other non-diffusion baselines.

</details>


### [33] [Orcheo: A Modular Full-Stack Platform for Conversational Search](https://arxiv.org/abs/2602.14710)
*Shaojie Jiang,Svitlana Vakulenko,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文介绍了一个名为Orcheo的开源平台，旨在为对话搜索研究提供一个统一框架，以解决社区贡献分享不足和端到端原型部署困难的问题。该平台具有模块化架构、生产就绪基础设施以及丰富的启动套件资源等优势。


<details>
  <summary>Details</summary>
Motivation: 对话搜索（CS）需要复杂的软件工程管道来整合查询重构、排名和响应生成等功能。当前CS研究人员面临两大障碍：缺乏与社区高效共享研究成果的统一框架，以及难以部署进行用户评估所需的端到端原型。

Method: 提出了Orcheo，一个专为解决上述问题而设计的开源平台。它通过单文件节点模块促进组件重用，提供了生产级基础设施支持从原型到系统的过渡，并配备了50多个现成组件用于快速构建完整的CS流程。

Result: Orcheo展示了其在模块性和易用性方面的优势，通过案例研究验证了其实用价值。

Conclusion: Orcheo作为一个开放源代码平台，在MIT许可证下发布，有助于促进对话搜索领域内的资源共享与研究进展。

Abstract: Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.

</details>


### [34] [Intent-Driven Dynamic Chunking: Segmenting Documents to Reflect Predicted Information Needs](https://arxiv.org/abs/2602.14784)
*Christos Koutsiaris*

Main category: cs.IR

TL;DR: 本文提出了一种新的文档分割方法，即意图驱动的动态分块（IDC），该方法利用预测的用户查询来指导文档分割。实验结果表明，IDC在六个不同的问答数据集上优于传统分割策略，提高了检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统的文档分割方法忽略了用户意图，导致信息片段可能切断答案或包含无关噪声。为了改善这一状况，并提高检索系统的相关性搜索能力，提出了意图驱动的动态分块方法。

Method: 通过使用大型语言模型生成文档可能的用户意图，然后采用动态规划算法找到全局最优的分块边界，实现了意图驱动的动态分块（IDC）方法。这是首次将动态规划应用于考虑到意图的文档分割中，避免了贪婪算法的局限性。

Result: IDC在五个数据集中优于传统分块策略，将top-1检索准确度提高了5%到67%，而在第六个数据集中与最佳基线持平。此外，相比基线方法，IDC生成的分块数量减少了40-60%，同时达到了93-100%的答案覆盖率。

Conclusion: 通过对文档结构进行调整以匹配预期的信息需求，可以显著提升检索性能，特别是对于长篇和异质性的文档。

Abstract: Breaking long documents into smaller segments is a fundamental challenge in information retrieval. Whether for search engines, question-answering systems, or retrieval-augmented generation (RAG), effective segmentation determines how well systems can locate and return relevant information. However, traditional methods, such as fixed-length or coherence-based segmentation, ignore user intent, leading to chunks that split answers or contain irrelevant noise. We introduce Intent-Driven Dynamic Chunking (IDC), a novel approach that uses predicted user queries to guide document segmentation. IDC leverages a Large Language Model to generate likely user intents for a document and then employs a dynamic programming algorithm to find the globally optimal chunk boundaries. This represents a novel application of DP to intent-aware segmentation that avoids greedy pitfalls. We evaluated IDC on six diverse question-answering datasets, including news articles, Wikipedia, academic papers, and technical documentation. IDC outperformed traditional chunking strategies on five datasets, improving top-1 retrieval accuracy by 5% to 67%, and matched the best baseline on the sixth. Additionally, IDC produced 40-60% fewer chunks than baseline methods while achieving 93-100% answer coverage. These results demonstrate that aligning document structure with anticipated information needs significantly boosts retrieval performance, particularly for long and heterogeneous documents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents](https://arxiv.org/abs/2602.13345)
*Ethan Seefried,Ran Eldegaway,Sanjay Das,Nathaniel Blanchard,Tirthankar Ghosal*

Main category: cs.LG

TL;DR: Blueprint, a layout-aware multimodal retrieval system, is introduced for improving the search and retrieval of engineering drawings and technical records in legacy archives. It enhances metadata generation and retrieval accuracy, achieving significant improvements over existing methods as evaluated on a 5k-file benchmark.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of retrieving decades-old engineering drawings and technical records from legacy archives, which suffer from inconsistent or missing metadata, making the retrieval process difficult and often manual.

Method: The method involves developing Blueprint, a system that detects canonical drawing regions, applies region-restricted OCR based on Vision-Language Models (VLM), normalizes identifiers, and fuses lexical and dense retrieval approaches with a lightweight reranker at the region level. The system was deployed on approximately 770,000 unlabeled files to automatically generate structured metadata suitable for cross-facility searches.

Result: Blueprint was evaluated using a 5,000-file benchmark with 350 expert-curated queries and achieved a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the best vision-language baseline. The system outperformed across various intents, including visual, textual, and multimodal. Oracle ablations also indicated potential for further enhancement under ideal conditions of perfect region detection and OCR.

Conclusion: The paper concludes by releasing all related queries, runs, annotations, and code to support reproducible evaluation on legacy engineering archives, highlighting Blueprint's effectiveness in improving the retrieval of technical documents through advanced multimodal techniques.

Abstract: Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.
  We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.

</details>


### [36] [The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric](https://arxiv.org/abs/2602.13359)
*Hannes Kath,Thiago S. Gouvêa,Daniel Sonntag*

Main category: cs.LG

TL;DR: 本文回顾了过去八年的主动学习评价文献，正式引入了一个名为加速因子的性能指标，该指标能够量化查询方法在多轮迭代中的表现。通过四个不同领域的数据集和七种类型的查询方法进行实证评估后发现，加速因子不仅能准确反映所需样本比例，还显示出跨迭代的优越稳定性。


<details>
  <summary>Details</summary>
Motivation: 主动学习（AL）旨在通过使用查询方法来选择最具信息量的样本来提高性能与标注比率，但目前缺乏适当的性能度量标准来评估这一迭代过程。

Method: 本文提出了一个名为加速因子的新性能度量，并通过涵盖不同领域内的四个数据集以及七种类型查询方法进行了实验验证。

Result: 实验结果证实了加速因子背后的假设，展示了它在捕捉所描述样本比例方面的准确性，并揭示了其在迭代过程中更优的稳定性。

Conclusion: 加速因子作为主动学习中的一种新型性能度量被提出并验证，为评估查询方法提供了一种有效手段。

Abstract: Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.

</details>


### [37] [Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2602.13413)
*Yuchen Fang,James Demmel,Javad Lavaei*

Main category: cs.LG

TL;DR: 本文研究了在重尾噪声环境下，随机预处理的随机梯度下降（SPSGD）及其加速变体的最坏情况复杂性理论，并证明了归一化相比裁剪在保证收敛性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 探索在重尾噪声条件下，不同稳定训练方法如归一化和裁剪对SPSGD算法及其加速版本性能的影响，特别是它们在最坏情况下表现的区别。

Method: 通过假设随机梯度噪声具有有限的p-阶矩（p ∈ (1,2]），并在T次迭代后测量收敛性来分析。引入了一种新的向量值Burkholder型不等式以支持分析。

Result: 归一化确保当问题参数已知时以\(\mathcal{O}(T^{-\frac{p-1}{3p-2}})\)的速度收敛到一阶驻点；当未知时，则以\(\mathcal{O}(T^{-\frac{p-1}{2p}})\)的速度收敛。相比之下，证明了裁剪可能因统计依赖关系而无法在最坏的情况下保证收敛。

Conclusion: 研究表明，在大规模模型训练中，归一化相对于裁剪表现出更好的收敛性保证，为实践中更倾向于使用归一化提供了理论依据。

Abstract: We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\mathcal{O}(T^{-\frac{p-1}{3p-2}})$ when problem parameters are known, and $\mathcal{O}(T^{-\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.

</details>


### [38] [High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator](https://arxiv.org/abs/2602.13416)
*Haiwen Guan,Moein Darman,Dibyajyoti Chakraborty,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: 本文介绍了一种基于深度学习的降尺度框架，该框架利用概率扩散生成模型将低分辨率的气候模拟器LUCIE的结果降尺度到25公里的高分辨率，并在多个评估指标上表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有数据驱动模型在长期稳定性、气候漂移以及计算成本方面的问题，同时提高LUCIE气候模拟器的空间分辨率以支持更详细的区域影响评估。

Method: 开发了一种基于深度学习的降尺度方法，使用概率扩散生成模型结合条件和后验采样框架，将LUCIE输出从约300公里的分辨率降尺度至25公里。训练过程基于ERA5再分析数据集中的大约14,000个时间步长（覆盖2000-2009年），并在2010年至2020年的LUCIE预测上进行评估。

Result: 提出的降尺度方法能够在保持LUCIE提供的粗略动力学特性的同时，在约28公里的分辨率下生成精细尺度的气候统计信息。通过多种评估指标验证了其有效性，包括纬度平均RMSE、功率谱、概率密度函数及经向风的第一经验正交函数等。

Conclusion: 这项研究表明，通过引入先进的降尺度技术可以有效提升轻量级气候模拟器LUCIE的空间分辨率，从而增强其对于局部气候变化研究的应用价值。

Abstract: The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.

</details>


### [39] [Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity](https://arxiv.org/abs/2602.13486)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为raFLoRA的方法，通过分解局部更新为秩分区并根据每个分区的有效客户端贡献进行加权聚合来解决异构FedLoRA中的秩塌陷问题。实验表明，该方法能够防止秩塌陷、提高模型性能同时保持通信效率。


<details>
  <summary>Details</summary>
Motivation: 在实际联邦学习场景中，由于客户端系统资源和数据分布的异质性，促使了跨客户端的LoRA等级异质化需求。研究者们发现了一个先前被忽视的现象——即当全局更新的能量集中在最小共享等级上时，会导致性能不佳并对等级配置高度敏感的问题，称为‘秩塌陷’。

Method: 受此启发，本文提出了raFLoRA，这是一种基于等级划分的聚合方法，它将本地更新分解为不同的等级分区，并依据各分区的有效客户端贡献度进行加权聚合。

Result: 广泛的实验证明，raFLoRA能够在分类与推理任务中有效避免秩塌陷现象，相比最先进的FedLoRA基线提升了模型表现并且维持了良好的通信效率。

Conclusion: raFLoRA通过针对性地处理异构FedLoRA环境下的秩塌陷挑战，提供了一种新的解决方案，不仅提高了模型性能还确保了通信效率，为联邦学习领域内的低秩适应技术开辟了新路径。

Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

</details>


### [40] [Text Has Curvature](https://arxiv.org/abs/2602.13418)
*Karish Grover,Hanqing Zeng,Yinglong Xia,Christos Faloutsos,Geoffrey J. Gordon*

Main category: cs.LG

TL;DR: 该论文提出了一种文本固有的曲率概念，并通过引入名为Texture的文本原生、词级离散曲率信号来检测、定义和应用这种曲率。研究结果表明，自然语料中的语义推理是非平坦的，即语言具有内在的曲率。Texture通过协调一个被遮蔽词左右上下文的信念来定义曲率场，并在两个代表性任务上展示了其效用，包括通过曲率引导压缩改善长上下文推理以及通过曲率引导路由增强检索生成。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地采用弯曲几何学（如双曲空间用于层次结构，混合曲率流形用于组合结构）进行建模，但关于文本本身是否具有曲率这一基础科学问题仍未解决。本文旨在探讨文本中是否存在一种与所选嵌入空间无关而属于语言本身的曲率含义。

Method: 提出了名为Texture的文字级离散曲率信号，并通过薛定谔桥调和了围绕被掩码单词的左右上下文信念，从而定义出一个曲率场，其中正值表示上下文聚焦意义的地方，负值则代表上下文分散成竞争延续之处。

Result: 研究表明自然语料中的语义推理是非平直的，证明了语言确实拥有内在曲率。此外，还展示了如何利用Texture作为一般测量和控制手段，在不依赖于几何训练的情况下实现几何处理；具体应用实例包括通过曲率引导的数据压缩改善长距离上下文推理及通过曲率指导的路径选择增强检索辅助生成。

Conclusion: 这项工作确立了一个文本原生的曲率范式，使得曲率变得可测量且实际有用。

Abstract: Does text have an intrinsic curvature? Language is increasingly modeled in curved geometries - hyperbolic spaces for hierarchy, mixed-curvature manifolds for compositional structure - yet a basic scientific question remains unresolved: what does curvature mean for text itself, in a way that is native to language rather than an artifact of the embedding space we choose? We argue that text does indeed have curvature, and show how to detect it, define it, and use it. To this end, we propose Texture, a text-native, word-level discrete curvature signal, and make three contributions. (a) Existence: We provide empirical and theoretical certificates that semantic inference in natural corpora is non-flat, i.e. language has inherent curvature. (b) Definition: We define Texture by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge, yielding a curvature field that is positive where context focuses meaning and negative where it fans out into competing continuations. (c) Utility: Texture is actionable: it serves as a general-purpose measurement and control primitive enabling geometry without geometric training; we instantiate it on two representative tasks, improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing. Together, our results establish a text-native curvature paradigm, making curvature measurable and practically useful.

</details>


### [41] [Comparing Classifiers: A Case Study Using PyCM](https://arxiv.org/abs/2602.13482)
*Sadra Sabouri,Alireza Zolanvari,Sepand Haghighi*

Main category: cs.LG

TL;DR: 本文介绍了PyCM库在多分类器深度评估中的应用，通过两个案例展示了不同评估指标如何影响模型效能的解读，并强调了多维度评估框架对于揭示模型性能细微差异的重要性。


<details>
  <summary>Details</summary>
Motivation: 选择最优分类模型需要对模型性能有全面而深入的理解。标准评估指标可能无法捕捉到模型性能上的细微差别，因此有必要采用一种多维度的评估方法来更准确地衡量模型表现。

Method: 文章采用了PyCM库作为工具，通过分析两个不同的案例场景，展示了如何利用多种评价指标来进行更为细致和全面的模型评估。

Result: 研究发现，选用不同的评估指标会从根本上改变对一个模型有效性的理解；使用多维度评估框架能够揭示出模型间虽小但重要的性能差异。

Conclusion: 为了更好地理解和比较分类模型的性能，应该采用像PyCM这样的工具提供的多维度评估方法，以确保不会忽略任何关键的性能指标。

Abstract: Selecting an optimal classification model requires a robust and comprehensive understanding of the performance of the model. This paper provides a tutorial on the PyCM library, demonstrating its utility in conducting deep-dive evaluations of multi-class classifiers. By examining two different case scenarios, we illustrate how the choice of evaluation metrics can fundamentally shift the interpretation of a model's efficacy. Our findings emphasize that a multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance. However, standard metrics may miss these subtle performance trade-offs.

</details>


### [42] [Finding Highly Interpretable Prompt-Specific Circuits in Language Models](https://arxiv.org/abs/2602.13483)
*Gabriel Franco,Lucas M. Tassis,Azalea Rohr,Mark Crovella*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的方法ACC++，用于从单次前向传递中提取更清晰、更低维度的注意力头内的因果信号。通过这种方法，研究者发现即使在固定任务内，不同提示也会诱导出不同的机制。这表明电路是与提示相关的，并且可以通过分析提示家族来更好地理解模型内部的工作机制。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数工作在任务层面识别电路时，假设每个任务有一个稳定的机制，但这种假设可能掩盖了重要结构：即使在同一任务内，电路也是特定于提示的。

Method: 基于注意力因果通信（ACC），引入了改进版本ACC++，该方法能从单次前向传递中提取更干净、更低维的因果信号。将ACC++应用于GPT-2、Pythia和Gemma 2中的间接对象识别（IOI）任务，揭示了不同提示模板会系统性地诱导出不同的机制。

Result: 对于任何模型来说，在间接对象识别任务中并不存在单一的电路；不同提示模板会导致显著不同的机制。尽管存在这种变化，提示仍然可以聚类成具有相似电路的提示家族。

Conclusion: 通过将分析单元从任务转移到提示上，研究结果重新定义了电路作为有意义的研究对象，使得在存在提示特定机制的情况下能够进行可扩展的电路描述。

Abstract: Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.

</details>


### [43] [TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers](https://arxiv.org/abs/2602.13498)
*Peng Cheng,Jiucheng Zang,Qingnan Li,Liheng Ma,Yufei Cui,Yingxue Zhang,Boxing Chen,Ming Jian,Wen Tong*

Main category: cs.LG

TL;DR: 本文提出了一种新的优化器TrasMuon，它通过全局RMS校准和基于能量的信任区域裁剪来稳定更新幅度，同时保持了Muon的近等距几何特性。实验表明，TrasMuon在视觉和语言模型上比基线方法收敛更快，并且在没有预热阶段的情况下也表现出更好的稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的Muon风格优化器虽然能够提供优于Adam系列方法的表现，但其正交化过程会丢弃幅度信息，导致训练对步长超参数敏感并且容易受到高能突发的影响。

Method: 提出了TrasMuon优化器，该方法保留了Muon的近等距几何特征，同时通过两种方式来稳定更新幅度：(i) 全局RMS校准；(ii) 基于能量的信任区域裁剪。这种方法定义了一个基于相对能量比率的信任区域，以限制更新到一个稳定的范围内。

Result: 实验证明，在视觉和语言模型上的测试中，TrasMuon相较于其他基准方法具有更快的收敛速度。此外，即使不经过预热阶段，TrasMuon依然展示出优秀的稳定性和鲁棒性。

Conclusion: 通过引入TrasMuon这一新型优化器，研究者们不仅解决了Muon风格优化器中存在的稳定性问题，还进一步提高了优化效率。这表明TrasMuon是一个有潜力替代现有优化算法的新选择。

Abstract: Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\textbf{T}rust \textbf{R}egion \textbf{A}daptive \textbf{S}caling \textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.

</details>


### [44] [$γ$-weakly $θ$-up-concavity: Linearizable Non-Convex Optimization with Applications to DR-Submodular and OSS Functions](https://arxiv.org/abs/2602.13506)
*Mohammad Pedramfar,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文介绍了一种新的条件γ-弱θ-上凹性，用于描述一类单调非凸函数。该条件统一并推广了DR-次模函数和单边平滑（OSS）函数，并证明了这些函数是可线性化的。通过将问题简化为线性优化，文章为离线优化以及在线设置中的静态和动态遗憾界提供了统一的近似保证。此外，对于DR-次模最大化恢复了最优近似系数，并显著提高了OSS优化在拟阵约束下的现有近似系数。


<details>
  <summary>Details</summary>
Motivation: 优化单调非凸函数是机器学习和组合优化领域的一个基本挑战。本文旨在引入一种新的条件来更好地刻画这类函数，并提供一种更强大的统一框架，以解决相关问题。

Method: 作者提出了γ-弱θ-上凹性的概念，这是一种新颖的一阶条件，它不仅统一而且严格地推广了DR-次模函数与OSS函数的概念。研究中展示了满足这种条件的函数可以被线性化处理，即对于任意可行点能够构造一个线性替代函数，其增益可以逼近原始非线性目标函数。

Result: 研究表明，γ-弱θ-上凹函数确实具有线性化性质，这使得许多问题可以通过转化为线性优化问题而得到解决。特别地，在DR-次模最大化方面达到了最优近似系数；对于有拟阵约束的OSS优化问题，相比之前的方法也有了显著改进。

Conclusion: 本研究提出的新框架为解决一系列优化问题提供了强有力的支持，包括但不限于离线环境下的优化任务及在线场景中的静态与动态遗憾界限估计。同时，该方法还对特定类型的优化问题如DR-次模最大化和OSS优化给出了改进的近似解质量。

Abstract: Optimizing monotone non-convex functions is a fundamental challenge across machine learning and combinatorial optimization. We introduce and study $γ$-weakly $θ$-up-concavity, a novel first-order condition that characterizes a broad class of such functions. This condition provides a powerful unifying framework, strictly generalizing both DR-submodular functions and One-Sided Smooth (OSS) functions. Our central theoretical contribution demonstrates that $γ$-weakly $θ$-up-concave functions are upper-linearizable: for any feasible point, we can construct a linear surrogate whose gains provably approximate the original non-linear objective. This approximation holds up to a constant factor, namely the approximation coefficient, dependent solely on $γ$, $θ$, and the geometry of the feasible set. This linearizability yields immediate and unified approximation guarantees for a wide range of problems. Specifically, we obtain unified approximation guarantees for offline optimization as well as static and dynamic regret bounds in online settings via standard reductions to linear optimization. Moreover, our framework recovers the optimal approximation coefficient for DR-submodular maximization and significantly improves existing approximation coefficients for OSS optimization, particularly over matroid constraints.

</details>


### [45] [Singular Vectors of Attention Heads Align with Features](https://arxiv.org/abs/2602.13524)
*Gabriel Franco,Carson Loughridge,Mark Crovella*

Main category: cs.LG

TL;DR: 本文探讨了在语言模型中特征表示与奇异向量对齐的现象，通过实证和理论分析证明了这种对齐是可靠的，并提出了一种基于稀疏注意力分解的方法来识别真实模型中的这种对齐。


<details>
  <summary>Details</summary>
Motivation: 先前的研究假设可以从注意力矩阵的奇异向量推断出特征表示，但缺乏充分的理由支持这一假设。本文旨在探索为何以及何时奇异向量会与特征保持一致。

Method: 首先，在一个可以直接观察到特征的模型中展示了奇异向量与特征之间的稳健对齐；接着，从理论上说明了在多种条件下预期会发生这样的对齐；最后，针对实际模型中无法直接观测到特征表示的问题，提出了稀疏注意力分解作为检测对齐的一种可测试预测方法。

Result: 研究发现奇异向量确实能在特定条件下与特征表示对齐，并且通过稀疏注意力分解方法在实际模型中找到了支持这一结论的证据。

Conclusion: 奇异向量与特征表示的对齐可以成为语言模型中特征识别的一个合理且有理论依据的基础。

Abstract: Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models. Together these results suggest that alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models.

</details>


### [46] [QuaRK: A Quantum Reservoir Kernel for Time Series Learning](https://arxiv.org/abs/2602.13531)
*Abdallah Aaraba,Soumaya Cherkaoui,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: 本文提出了QuaRK框架，它结合了实际硬件的量子水库特征化器和基于核的经典读出方案，以处理时间序列学习问题。通过经典阴影断层扫描有效测量k局部可观测值，生成紧凑的特征向量，并利用经典的基于核的读出学习目标映射。此外，还为依赖性时间数据提供了学习理论泛化保证，从而为构建可靠的时间学习者提供原则性指导。


<details>
  <summary>Details</summary>
Motivation: 尽管量子水库计算为通过丰富的量子动力学建模序列数据提供了很有前途的方法，但文献中关于高效且可实现的量子水库架构以及模型学习保证的研究仍然很少。为了填补这一空白，作者们开发了QuaRK框架。

Method: QuaRK框架首先使用一个能够反映真实硬件特性的量子水库来处理输入的时间序列数据，然后通过经典阴影断层技术从这些数据中提取k局部可观测值形成特征向量。接着，采用基于核方法的经典读出机制对所得到的特征进行学习，此过程中引入了显式正则化与快速优化策略。

Result: 实验结果验证了QuaRK的有效性，并在合成β混合时间序列任务上展示了预测的插值和泛化行为。

Conclusion: QuaRK不仅保持了核方法在建模非线性时间函数方面的灵活性，而且对于高维数据也是可扩展的。此外，通过将设计和资源选择与有限样本性能联系起来，本研究为构建可靠的时间序列学习系统提供了有原则性的指导。

Abstract: Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.

</details>


### [47] [Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction](https://arxiv.org/abs/2602.13532)
*Nobutaka Ono*

Main category: cs.LG

TL;DR: 本文提出了一种快速的元素选择算法，用于无需乘法操作的降维处理。通过仅选取输入中的一部分元素来生成降维向量。对于所选元素子集的质量评估，采用了线性回归预测目标向量时的最小均方误差标准。当没有明确的目标时，则使用输入本身作为目标，形成基于重建的标准。为了解决由此产生的组合优化问题，并避免穷举搜索，作者利用矩阵求逆引理导出了一个有效公式，用来计算交换已选和未选元素导致的目标函数变化，并执行基于交换的局部搜索直到无法进一步改进。MNIST手写数字图像上的实验表明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了减少不必要的模型参数、缓解过拟合以及加速训练和推理过程，降维技术是必不可少的。然而，常用的主成分分析（PCA）依赖于矩阵乘法，在资源受限系统上，乘法运算本身可能成为瓶颈。因此，需要一种不依赖于乘法操作的降维方法。

Method: 本文提出的元素选择算法通过直接从输入中挑选部分元素来实现降维。评价候选子集质量的方法是根据选定元素预测目标向量时的最小均方误差进行线性回归分析。若缺乏明确的目标向量，则可采用输入自身作为目标，从而形成以重建为基础的选择标准。针对此组合优化问题，作者借助矩阵求逆引理开发了一个能够高效计算因替换特定元素而导致目标函数值变化的公式，并据此实施基于交换的局部搜索策略。

Result: 实验结果表明，在处理MNIST手写数字图像数据集时，所提出的方法能够有效地完成降维任务。

Conclusion: 本文介绍了一种新颖且高效的非乘法型降维方法——元素选择算法。该方法不仅减少了计算成本，还能够在保持性能的同时实现对大规模数据集的有效降维。

Abstract: In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.

</details>


### [48] [Interpretable clustering via optimal multiway-split decision trees](https://arxiv.org/abs/2602.13586)
*Hayato Suzuki,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 提出了一种基于最优多路分割决策树的可解释聚类方法，通过将问题重新表述为0-1整数线性优化问题来提高计算效率和解决方案质量。该方法结合了一维K-means算法用于连续变量离散化，并在真实数据集上展示了优于基线方法的聚类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类方法通常通过解决混合整数非线性优化问题来构建二叉决策树，这导致了较高的计算成本以及次优解。此外，二叉决策树往往过深，不易于解释。

Method: 本研究提出了一种新的聚类方法，该方法基于最优多路分割决策树，并将其建模为一个0-1整数线性优化问题。此外，还集成了一维K-means算法以实现对连续变量的有效离散化。

Result: 在公开的真实世界数据集上的广泛实验表明，所提方法在聚类准确性与可解释性方面优于基准方法，同时生成的决策树具有简洁的决策规则，在多种评估指标下保持竞争力。

Conclusion: 新提出的基于最优多路分割决策树的聚类方法不仅提高了聚类结果的准确性和可解释性，而且通过采用更有效的优化形式降低了计算复杂度。

Abstract: Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.

</details>


### [49] [Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?](https://arxiv.org/abs/2602.13626)
*Mingqiao Zhang,Qiyao Peng,Yumeng Wang,Chunyuan Liu,Hongtao Liu*

Main category: cs.LG

TL;DR: 本文探讨了基于大型语言模型（LLMs）的推荐系统中基准数据泄露的问题，通过实验模拟不同数据泄露情景，揭示了域相关和域无关的数据泄露对模型性能的不同影响。研究强调了数据泄露作为评估LLM推荐系统性能时一个未被充分考虑的关键因素的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地融入推荐系统，对于评估可靠性的挑战也随之增加。本文旨在识别并研究一个先前被忽视的问题：基于LLM的推荐中存在的基准数据泄露问题。当LLM在预训练或微调过程中接触到并可能记住了基准数据集时，会导致性能指标虚高，不能准确反映模型的真实表现。

Method: 通过进行基础模型的持续预训练来模拟不同的数据泄露场景，使用了包括来自领域内外用户-项目交互的战略性混合语料库。

Result: 实验显示，数据泄露具有双重效应：当泄露的数据与领域相关时，它会带来显著但虚假的性能提升；而领域无关的数据泄露通常会降低推荐准确性。这表明数据泄露以一种复杂且条件性的方式影响着推荐结果。

Conclusion: 数据泄露是基于LLM的推荐系统中一个关键但之前未被充分考虑的因素，它可能会影响模型的真实性能。

Abstract: The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.

</details>


### [50] [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519)
*Chaosheng Dong,Peiyao Xiao,Yijia Wang,Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文介绍了DeepMTL2R，一个开源的深度学习框架，用于同时优化多个相关性标准的学习排序任务。该框架集成了多种多任务学习算法，并支持多目标优化以识别帕累托最优排名模型。通过在公开数据集上的实验，展示了其有效性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 针对需要同时优化多个相关性标准的排序任务，提出了一种新的深度学习框架DeepMTL2R来整合异构的相关信号到统一的、上下文感知的模型中，旨在解决多样化甚至可能存在冲突的目标之间的有效学习问题。

Method: DeepMTL2R利用了Transformer架构中的自注意力机制来整合不同的相关性信号，形成一个能够捕捉项目和标签间复杂依赖关系及长距离交互作用的统一模型。此外，该框架还包含了21种最先进的多任务学习算法，并支持多目标优化策略，以确定帕累托最优的排名模型。

Result: 通过在一个公开的数据集上进行测试，DeepMTL2R展示出了其竞争性的性能，并且可视化了不同目标之间的权衡情况。

Conclusion: DeepMTL2R作为一个可扩展且表达力强的解决方案，为现代排序系统提供了有效的支持，并促进了不同多任务学习策略之间的可控比较。

Abstract: This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.

</details>


### [51] [Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series](https://arxiv.org/abs/2602.13649)
*Li Zhang,Nital Patel,Xiuqi Li,Jessica Lin*

Main category: cs.LG

TL;DR: 本文提出了联合时间序列链（Joint Time Series Chain）的新定义，旨在发现中断的时间序列或两个相关时间序列之间的意外演变趋势，并通过实证评估证明了该方法在定位不寻常演变模式方面优于现有的时间序列链工作。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列链的定义仅考虑在单一时间序列中寻找链条，这可能会错过中断时间序列中的意外演变模式或两个相关时间序列间的关联。为了解决这一局限性，研究者引入了新的“联合时间序列链”定义，专门用于发现中断时间序列或两个相关时间序列间意想不到的演变趋势。

Method: 提出了一个新的定义——联合时间序列链，特别设计用来发现在中断时间序列或两个相关时间序列之间意外的演变趋势；进一步提出了一种有效的排序标准来识别最佳链条。

Result: 通过广泛的实证评价展示了所提方法在定位不寻常演变模式上优于现有时间序列链工作，并通过Intel的实际制造应用案例展示了工作的实用性。

Conclusion: 联合时间序列链提供了一种有效的方法来揭示跨中断或相关时间序列的潜在异常演变趋势，对实际应用场景具有重要意义。

Abstract: Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .

</details>


### [52] [Alignment Adapter to Improve the Performance of Compressed Deep Learning Models](https://arxiv.org/abs/2602.14635)
*Rohit Raj Rai,Abhishek Dhaka,Amit Awekar*

Main category: cs.LG

TL;DR: 提出了一种名为Alignment Adapter (AlAd)的轻量级适配器，用于对齐压缩模型与原始大模型之间的token级别嵌入，以提高资源受限环境下压缩深度学习模型的表现。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境中部署压缩后的深度学习模型时，其性能往往不如大规模模型。为了缩小这一差距，提出了一个基于滑动窗口的轻量级适配器——Alignment Adapter(AlAd)，旨在保持局部上下文语义的同时灵活地跨越不同的维度或架构进行对齐，并且完全独立于底层压缩方法。

Method: 本研究介绍的方法是使用名为Alignment Adapter（AlAd）的轻量级、基于滑动窗口的适配器来调整压缩模型中的token级别嵌入与原大型模型相匹配。AlAd的设计允许它作为即插即用模块安装到冻结的压缩模型上，或者通过联合微调AlAd和压缩模型以进一步提高性能。

Result: 通过对BERT家族模型在三个词元级别的自然语言处理任务上的实验表明，AlAd能够在仅增加少量大小和延迟的情况下显著提升压缩模型的表现。

Conclusion: Alignment Adapter为改善压缩深度学习模型的性能提供了一个有效的解决方案，在几乎不增加额外开销的前提下，能够显著提高这些模型在多种NLP任务上的表现。

Abstract: Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.

</details>


### [53] [Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation](https://arxiv.org/abs/2602.13651)
*Stefan Behfar,Richard Mortier*

Main category: cs.LG

TL;DR: 该论文提出了一种名为累积效用平等的新公平原则，旨在确保联邦学习中的客户端根据其参与机会获得可比较的长期收益。通过引入可用性标准化累积效用的概念，研究者们能够区分不可避免的物理限制和可以避免的算法偏见，并在时间分布不均、非独立同分布的联邦基准测试上展示了显著改善长期代表性的同时保持了几乎完美的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习（FL）系统中，客户端的参与是间歇性的、异质的，并且往往与数据特性或资源限制相关联。现有的FL公平性方法主要集中在基于参与的情况下平衡损失或准确率，这隐含地假设所有客户端都有相似的机会随时间贡献。然而，当参与本身不均匀时，这些目标可能会导致间歇性可用客户端的系统性代表性不足，即使每轮的表现看起来是公平的。

Method: 提出了累积效用平等这一新的公平原则，旨在评估客户端是否根据每次参与机会获得了可比较的长期效益，而不仅仅是针对每个训练轮次。为了实现这个概念，作者引入了可用性标准化累积效用，以此来区分不可抗力因素带来的物理限制与因调度和聚合引起的可规避算法偏见。

Result: 实验结果表明，在时间分布不均、非独立同分布的数据集上，所提方法能够显著提高长期代表性的同时，还能维持接近完美的性能。

Conclusion: 本研究提出的累积效用平等原则及其对应的度量方法——可用性标准化累积效用，为解决联邦学习场景下由于客户端参与不均衡导致的公平性问题提供了新思路。它不仅有助于识别并减少算法层面的偏见，同时也保证了模型的整体性能不受影响。

Abstract: In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.

</details>


### [54] [Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation](https://arxiv.org/abs/2602.14914)
*Olivier Jeunen,Shashank Gupta*

Main category: cs.LG

TL;DR: 本研究证明了使用最优加性基线的$β^\star$-IPS估计量在均方误差上渐近优于自归一化逆倾向评分（SNIPS），并建议在排名和推荐系统中从自归一化转向最优基线校正。


<details>
  <summary>Details</summary>
Motivation: 虽然自归一化逆倾向评分(SNIPS)是减少OPE方差的标准工具，但最近的研究表明加性控制变量（基线校正）可能提供更好的性能。然而，在评估方面还缺乏理论保证。

Method: 通过分析分解方差差距，证明了SNIPS等同于使用特定但通常次优的加性基线，并且$β^\star$-IPS估计量在均方误差上渐近优于SNIPS。

Result: $β^\star$-IPS估计量在均方误差上表现出比SNIPS更优的性能。此外，研究揭示了SNIPS实际上相当于采用了一种特定但非最优的加性基线方法。

Conclusion: 研究表明应当从使用自归一化转向采用最优基线校正的方法，以提高排名和推荐系统的离线策略评估效果。

Abstract: Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.

</details>


### [55] [Zero-Order Optimization for LLM Fine-Tuning via Learnable Direction Sampling](https://arxiv.org/abs/2602.13659)
*Valery Parfenov,Grigoriy Evseev,Andrey Veprikov,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 本文提出了一种策略驱动的零阶(ZO)框架，通过学习扰动方向的采样分布来减少方向估计的方差，从而在大规模预训练语言模型微调中实现更有效的内存使用。实验结果表明，该方法相比标准ZO基线在性能上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着大型预训练语言模型（LLMs）微调对内存需求的不断增加，尤其是在资源受限环境下部署时面临挑战。传统的零阶方法虽然可以节省大量内存，但由于高方差和对参数维度的不良依赖性限制了其应用范围。为了解决这些问题，作者提出了一个基于策略的零阶框架。

Method: 提出的方法将扰动方向上的采样分布视为可学习的策略，并通过更新这个策略来降低方向估计的方差。开发了一个实际算法来实现这一想法，并且进行了理论分析，证明了学习到的采样分布能够提高梯度信息的质量并放松收敛界面对参数维度d的显式依赖。

Result: 实验验证了该方法在具有挑战性的LLM微调基准测试中的表现，相比于标准ZO基线显示出了显著改进的性能。

Conclusion: 自适应方向采样是使大规模ZO微调成为可能的一个有希望的方向。

Abstract: Fine-tuning large pretrained language models (LLMs) is a cornerstone of modern NLP, yet its growing memory demands (driven by backpropagation and large optimizer States) limit deployment in resource-constrained settings. Zero-order (ZO) methods bypass backpropagation by estimating directional derivatives from forward evaluations, offering substantial memory savings. However, classical ZO estimators suffer from high variance and an adverse dependence on the parameter dimensionality $d$, which has constrained their use to low-dimensional problems. In this work, we propose a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy and updates it to reduce the variance of directional estimates. We develop a practical algorithm implementing this idea and provide a theoretical analysis, showing that learned sampling distributions improve the quality of gradient information and relax the explicit dependence on $d$ in convergence bounds. Empirically, we validate the approach on challenging LLM fine-tuning benchmarks, demonstrating substantially improved performance compared to standard ZO baselines. Our results suggest that adaptive direction sampling is a promising route to make ZO fine-tuning viable at scale. The source code is available at https://github.com/brain-lab-research/zo_ldsd

</details>


### [56] [Optimized Certainty Equivalent Risk-Controlling Prediction Sets](https://arxiv.org/abs/2602.13660)
*Jiayi Huang,Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架OCE-RCPS，它提供了对一般优化确定性等价风险度量（包括条件价值风险和熵风险）的高概率保证。通过实验表明，OCE-RCPS在各种风险度量和可靠性配置下都能一致地达到目标满意度，而传统的OCE-CRC方法则无法提供这样的概率保证。


<details>
  <summary>Details</summary>
Motivation: 在诸如医学图像分割等安全关键应用中，预测系统必须提供超出传统预期损失控制之外的可靠性保证。虽然风险控制预测集(RCPS)为预期风险提供了概率保证，但它们未能捕捉到尾部行为和最坏情况，这些在高风险环境中至关重要。

Method: 本文介绍了一种名为优化确定性等价RCPS (OCE-RCPS)的新框架，该框架对通用优化确定性等价(OCE)风险度量（如条件价值风险(CVaR)和熵风险）提供高概率保证。OCE-RCPS利用上置信界来识别满足用户指定风险容忍水平的预测集参数，并具有可证明的可靠性。

Result: 研究表明，对于未覆盖损失和假阴性率等损失函数，OCE-RCPS能够满足所需的概率约束。实验还展示了，在不同风险度量和可靠性设置下，OCE-RCPS能够持续达到目标满意率；相比之下，OCE-CRC未能提供概率上的保证。

Conclusion: OCE-RCPS作为一种新方法，能有效地为安全关键领域的预测问题提供更强的风险管理能力，特别是在需要考虑极端事件影响的情况下。

Abstract: In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.

</details>


### [57] [ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer](https://arxiv.org/abs/2602.13666)
*Edward Chen,Natalie Dullerud,Pang Wei Koh,Thomas Niedermayr,Elizabeth Kidd,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: 本文介绍了一种名为ALMo的决策支持系统，用于高剂量率（HDR）宫颈癌近距离放射治疗中，通过直观的目标和限制值帮助临床医生优化治疗计划。ALMo在回顾性评估中表现出色，不仅提高了治疗计划的质量，还显著减少了规划时间。


<details>
  <summary>Details</summary>
Motivation: 在复杂的临床决策过程中，尤其是在高剂量率（HDR）宫颈癌近距离放射治疗中，临床医生需要同时考虑多种竞争指标，包括理想目标和严格限制条件。这要求他们处理高维度权衡问题，以推断出最佳患者特定策略，但这一过程认知负担重且容易产生变异性。为解决这一难题，提出了ALMo系统。

Method: ALMo是一种交互式决策支持系统，采用新颖的优化框架减少手动输入需求，并允许通过直接操作直观的目标与限制值来控制毒性风险。该系统旨在简化多标准临床决策中的互动流程。

Result: 通过对25个临床案例的回顾性评价显示，使用ALMo生成的治疗方案质量始终等于或优于手工规划的结果，其中65%的案例显示出剂量学上的改进。此外，该系统将平均规划时间缩短至约17分钟，而传统方法通常需要30-60分钟。

Conclusion: ALMo不仅证明了其在提高HDR宫颈癌近距离放射治疗效率和质量方面的有效性，也展示了其作为通用框架，在其他多标准临床决策场景中应用的潜力。

Abstract: In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.

</details>


### [58] [Advancing Analytic Class-Incremental Learning through Vision-Language Calibration](https://arxiv.org/abs/2602.13670)
*Binyu Zhao,Wei Zhang,Xingrui Yu,Zhaonian Zou,Ivor Tsang*

Main category: cs.LG

TL;DR: 本文提出了一种名为VILA的双分支框架，通过两层视觉-语言校准策略推进了基于预训练模型的类增量学习（CIL），在保持分析学习高效性的同时克服了其脆弱性。实验表明VILA在多个基准测试中表现出色，特别是在细粒度和长序列场景下。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练模型的类增量学习方法面临效率与长期稳定性之间的权衡问题，尤其是累积误差和特征不兼容性导致效能下降。文章旨在解决这些问题，特别是由表示刚性引起的瓶颈。

Method: 提出了VILA框架，采用双分支设计结合可塑的任务适应特征与固定的通用语义锚点，在特征层面通过几何校准融合两者，并在决策层面利用跨模态先验纠正预测偏差。

Result: VILA在八个基准数据集上的广泛实验显示，该方法能够一致地提供更好的性能，尤其是在处理细粒度和长序列任务时表现尤为突出。

Conclusion: VILA框架成功地将高精度预测与分析学习的简洁性结合起来，为基于预训练模型的类增量学习提供了一个有效解决方案。

Abstract: Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA

</details>


### [59] [On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling](https://arxiv.org/abs/2602.13684)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文研究了相关聚类中的稀疏化-近似权衡问题，发现伪度量加权实例和一般加权实例之间存在结构上的二分法。作者证明了聚类分歧类的VC维度为n-1，提出了一个稀疏化的LP-PIVOT变体，并证明了在观察到大约n^(3/2)条边时，该算法可以达到稳健的10/3近似比。同时，也证明了没有伪度量结构的情况下，任何只观察到o(n)条均匀随机边的算法都会导致无界的近似比。


<details>
  <summary>Details</summary>
Motivation: 由于基于线性规划（LP）的相关聚类（CC）方法需要Θ(n^3)个三角不等式约束条件，在大规模数据集上变得非常困难。因此，本文旨在探讨在保持LP近似保证的同时，需要多少边信息。

Method: 通过理论分析，作者们首先确定了聚类分歧类的VC维度，然后提出了一种稀疏化的LP-PIVOT算法变体，这种算法能够通过三角不等式来推断缺失的LP边缘概率。此外，还使用Yao极小极大原理来证明对于非伪度量结构的数据，如果观察到的边数量少于一定阈值，则任何算法都将面临无界近似比的问题。

Result: 研究表明，当观察到约n^(3/2)条边时，稀疏化的LP-PIVOT算法可以实现一个稳健的10/3近似比（外加一个由可计算统计量控制的附加项）。此外，对于非伪度量结构的数据，如果观察到的边数少于o(n)，则任何算法都无法得到有界近似比。

Conclusion: 论文得出了关于相关聚类中稀疏化与近似精度之间关系的重要结论，指出伪度量结构不仅影响问题的可处理性，而且影响其对不完全信息的鲁棒性。

Abstract: Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $Θ(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\varepsilon$-coresets of optimal size $\tilde{O}(n/\varepsilon^2)$; that at most $\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\overlineΓ_w$) once $\tildeΘ(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.

</details>


### [60] [Attention Head Entropy of LLMs Predicts Answer Correctness](https://arxiv.org/abs/2602.13699)
*Sophie Ostmeier,Brian Axelrod,Maya Varma,Asad Aali,Yabin Zhang,Magdalini Paschali,Sanmi Koyejo,Curtis Langlotz,Akshay Chaudhari*

Main category: cs.LG

TL;DR: 本文提出了一种名为Head Entropy的新方法，通过测量注意力权重分布的熵来预测大型语言模型答案的正确性。该方法在不同领域和数据集上表现出色，甚至在答案生成之前仅基于问题/上下文的注意力模式就能提供预测信号。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）常常产生看似合理但实际上错误的答案，这在医疗等关键安全领域构成了风险。现有的评估手段要么成本高昂（如人工评估），要么存在引入隐藏错误的风险（如使用LLM作为评判者）。虽然最近的研究提出了利用模型内部信息检测上下文幻觉的方法，但这些方法能否有效预测答案的正确性及其跨领域的泛化能力仍不清楚。

Method: 研究者们开发了Head Entropy方法，它通过分析注意力机制中每个头的2-Renyi熵来度量注意力质量分布，并采用稀疏逻辑回归技术来预测答案的准确性。此外，还探讨了仅依靠问题或上下文阶段的注意力模式是否能携带足够的预测信息。

Result: 实验表明，Head Entropy不仅在分布内达到了与基线相当或更好的性能，在跨领域测试时其表现也显著优于现有方法，平均提高了8.5%的AUROC值。特别地，当只考虑问题或背景信息时，该方法相较于最接近的竞争对手提升了约17.7%的AUROC。

Conclusion: Head Entropy为评估大型语言模型输出质量提供了新的视角，尤其是在跨领域应用方面展现出优越的泛化能力。这种方法有助于提高对LLMs生成内容可靠性的信心，特别是在需要高精度回答的情境下。

Abstract: Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

</details>


### [61] [Optimal Regret for Policy Optimization in Contextual Bandits](https://arxiv.org/abs/2602.13700)
*Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文提出了一个策略优化技术，并首次证明了其在随机上下文多臂老虎机问题中可以达到最优的遗憾界。同时，通过实验证明了该算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了弥合理论与实践之间的差距，证明广泛使用的策略优化方法在处理上下文多臂老虎机问题时能够达到严格证明的最优遗憾界。

Method: 采用了一种新的策略优化技术，并结合一般离线函数逼近来解决随机上下文多臂老虎机问题。

Result: 所提出的算法不仅高效，而且达到了$\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$的最优遗憾界。实验结果进一步支持了理论发现。

Conclusion: 研究成功地为策略优化方法在解决随机上下文多臂老虎机问题时提供了首个高概率最优遗憾界限，从而证实了这类方法在理论上的最优性和实际应用中的有效性。

Abstract: We present the first high-probability optimal regret bound for a policy optimization technique applied to the problem of stochastic contextual multi-armed bandit (CMAB) with general offline function approximation. Our algorithm is both efficient and achieves an optimal regret bound of $\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$, where $K$ is the number of rounds, $\mathcal{A}$ is the set of arms, and $\mathcal{F}$ is the function class used to approximate the losses. Our results bridge the gap between theory and practice, demonstrating that the widely used policy optimization methods for the contextual bandit problem can achieve a rigorously-proved optimal regret bound. We support our theoretical results with an empirical evaluation of our algorithm.

</details>


### [62] [Near-Optimal Regret for Policy Optimization in Contextual MDPs with General Offline Function Approximation](https://arxiv.org/abs/2602.13706)
*Orin Levy,Aviv Rosenberg,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文提出了首个用于随机情境马尔可夫决策过程（CMDPs）的策略优化算法OPO-CMDP，在一般离线函数逼近下实现了高概率遗憾界，直接改进了现有最先进成果，并展示了乐观策略优化为解决CMDPs提供了一条自然、计算上优越且理论接近最优的路径。


<details>
  <summary>Details</summary>
Motivation: 针对随机情境马尔可夫决策过程（CMDPs），当前方法在离线函数逼近下的性能仍有提升空间。本文旨在开发一种新的策略优化算法，以提高对状态和动作空间大小的依赖性，并改善现有技术的表现。

Method: 提出了一种名为OPO-CMDP的新策略优化算法，该算法适用于使用一般离线函数逼近的随机情境马尔可夫决策过程中。通过该方法，研究者们能够实现一个关于状态数|S|和动作数|A|具有最优依赖性的遗憾界限。

Result: OPO-CMDP算法达到了一个高概率遗憾界，形式为$\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)})$。这一结果直接提升了现有技术水平，并且对于处理CMDPs问题提供了理论上的近似最优解。

Conclusion: 本研究表明，乐观策略优化为解决CMDPs提供了一个自然、计算效率高且理论上接近最优的方法。所提出的OPO-CMDP算法不仅在理论上有所突破，同时也指出了未来研究的一个有潜力的方向。

Abstract: We introduce \texttt{OPO-CMDP}, the first policy optimization algorithm for stochastic Contextual Markov Decision Process (CMDPs) under general offline function approximation. Our approach achieves a high probability regret bound of $\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)}),$ where $S$ and $A$ denote the state and action spaces, $H$ the horizon length, $T$ the number of episodes, and $\mathcal{F}, \mathcal{P}$ the finite function classes used to approximate the losses and dynamics, respectively. This is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving the current state-of-the-art (Qian, Hu, and Simchi-Levi, 2024). These results demonstrate that optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal path for solving CMDPs.

</details>


### [63] [HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.13710)
*Xin Yan,Zhenglin Wan,Feiyang Ye,Xingrui Yu,Hangyu Du,Yang You,Ivor Tsang*

Main category: cs.LG

TL;DR: 提出了一种名为HBVLA的二值化框架，旨在解决视觉-语言-动作（VLA）模型在资源受限机器人和边缘平台上的部署问题。通过使用策略感知增强Hessian来识别对动作生成至关重要的权重，并采用稀疏正交变换和哈达玛域内组级1比特量化处理非显著权重，以实现高效且准确的模型压缩。实验结果表明，在LIBERO和SimplerEnv等VLAs上，该方法能够保持接近全精度性能，同时显著优于现有二值化方法，证明了其在硬件限制条件下的可靠部署能力。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型虽然能够实现基于指令的实体控制，但由于计算量大、内存占用高，导致难以直接应用于资源受限的机器人及边缘设备中。尽管将权重减少到1位精度可以大幅提高效率，但现有方法无法有效缩小二值化与全精度权重之间的分布差距，这会导致长时间闭环执行过程中量化误差累积，严重降低动作质量。

Method: 为了解决上述问题，研究者提出了HBVLA，一种专为VLA设计的二值化框架。首先利用策略感知增强Hessian矩阵来识别对于动作生成真正关键的权重；接着，对非关键权重应用稀疏正交转换，诱导出低熵中间状态；最后，在哈达玛域内对重要和非重要权重进行组级1位量化。

Result: 在LIBERO数据集上，经过量化的OpenVLA-OFT保留了92.2%的全精度表现；而在SimplerEnv环境中，量化后的CogAct则保持了93.6%的表现，明显优于当前最先进的一系列二值化技术。此外，实际测试也显示HBVLA相比全精度模型仅有轻微的成功率下降，展示了其在严格硬件约束下良好的可部署性。

Conclusion: 本研究为超低位量化提供了实用基础，使得VLA模型能够在硬件资源极其有限的情况下更加可靠地被部署至各类机器人平台上。

Abstract: Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.

</details>


### [64] [Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition](https://arxiv.org/abs/2602.13759)
*ZhiMing Li,JiaHe Feng*

Main category: cs.LG

TL;DR: 本文提出了一种离散的双括号流方法，用于在矩阵-向量乘积（MVP）oracle下的无矩阵特征分解。所提方法对各向同性位移具有路径不变性，并且能够根据迹为零的协方差$C_e$来确定最大稳定步长。通过严格的鞍点几何和输入到状态稳定性分析，证明了全局收敛性，并提供了样本复杂度和有限时间收敛性的高概率保证。


<details>
  <summary>Details</summary>
Motivation: 标准的随机逼近方法要么使用固定步长，这将稳定性与$\|C_k\|_2$联系起来，要么以导致更新消失的方式调整步长。为了解决这些问题，本文旨在开发一种新的方法，该方法能够在不依赖于$\sigma_k^2 I$的情况下实现更有效的特征分解。

Method: 本文介绍了一种离散的双括号流动方法，其生成器对于各向同性移动是不变的，在离散时间级别上对于$\sigma_k^2 I$也表现出路径不变性。基于此轨迹及一个最大稳定步长$\eta_{max} \propto 1/\|C_e\|_2^2$，仅取决于迹为零的协方差$C_e$。

Result: 通过严格的鞍点几何学以及针对对角化目标的输入到状态稳定性分析，确立了全局收敛性。样本复杂度表现为$O(\|C_e\|_2^2 / (\Delta^2 \varepsilon))$当存在迹为零扰动时。此外，还给出了退化块的显式表征，从而加速了鞍点逃逸率至$O(\log(1/\zeta))$并确保了高概率下的有限时间收敛性。

Conclusion: 所提出的离散双括号流方法有效地解决了在矩阵-向量乘积oracle下进行无矩阵特征分解的问题，提供了一种新的途径来处理协方差算子中的各向同性偏移问题，同时保持了良好的收敛性和效率。

Abstract: We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + σ_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\|C_k\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $σ_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $η_{max} \propto 1/\|C_e\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\|C_e\|_2^2 / (Δ^2 ε))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\log(1/ζ))$ saddle-escape rate and a high-probability finite-time convergence guarantee.

</details>


### [65] [On Representation Redundancy in Large-Scale Instruction Tuning Data Selection](https://arxiv.org/abs/2602.13773)
*Youwei Shu,Shaomian Zheng,Dingnan Jin,Wenjie Qu,Ziyao Guo,Qing Cui,Jun Zhou,Jiaheng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为压缩表示数据选择（CRDS）的新框架，通过减少语义嵌入的冗余来提高指令调优的数据质量。实验结果表明，该方法在四个数据集上仅使用3.5%的数据就超过了全数据基线平均0.71%的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明，相比更大但噪音或低质量的数据集，较小的高质量数据集可以训练出表现更好的模型，但在工业规模上的指令调优数据选择方法仍缺乏系统性研究。此外，现有最先进LLM编码器生成的语义嵌入存在高度冗余问题。

Method: 提出了两种变体的压缩表示数据选择（CRDS）框架：CRDS-R利用Rademacher随机投影结合Transformer隐藏层表示；CRDS-W则采用白化降维技术以改善表示质量。

Result: 实验结果显示，两种CRDS变体均显著提升了数据质量，并且在基于表示的选择方法中始终优于当前最先进的方法。特别是CRDS-W，在仅使用3.5%的数据情况下，其性能比使用全部数据的基线平均高出0.71%。

Conclusion: 通过引入压缩表示数据选择（CRDS）框架，有效减少了语义嵌入中的冗余性，从而为指令调优提供了更高质量的数据选择方案。

Abstract: Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.

</details>


### [66] [MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2602.13783)
*Xiaoyun Yu,Li fan,Xiangfei Qiu,Nanqing Dong,Yonggui Huang,Honggang Qi,Geguang Pu,Wanli Ouyang,Xi Chen,Jilin Hu*

Main category: cs.LG

TL;DR: 提出了一种名为Memory for Time Series (MEMTS)的新方法，通过知识持久化模块(KPM)内部化领域特定的时间动态，无需检索即可实现时间序列预测中的领域适应。该方法在保持高效推理的同时，有效缓解了对通用时间模式的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列基础模型（TSFMs）虽然在广义预测中表现出色，但在具有时间分布变化和领域特定周期结构的实际应用场景中性能显著下降。当前解决方案如领域自适应预训练(DAPT)会破坏已学习到的全局时间模式，而检索增强生成(RAG)则引入大量检索开销，导致效率瓶颈。

Method: 提出了一种轻量级且即插即用的方法——Memory for Time系列(MEMTS)，它通过知识持久化模块(KPM)将领域特定的时间动态，例如重复出现的季节性模式和趋势内化为一组紧凑的学习型潜在原型。这样做的目的是将碎片化的历史观察转换成连续的、参数化的知识表示。

Result: MEMTS在多个数据集上的广泛实验表明其达到了最先进(SOTA)的表现。

Conclusion: MEMTS提供了一种有效的解决方案，能够在不改变冻结的TSFM主干架构的情况下，以常数时间推断和几乎零延迟达到准确的领域适应，并有效地减轻了对一般时间模式的灾难性遗忘。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.

</details>


### [67] [AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning](https://arxiv.org/abs/2602.13807)
*Xiaoyu Tao,Yuchong Wu,Mingyue Cheng,Ze Guo,Tian Gao*

Main category: cs.LG

TL;DR: AnomaMind, a new framework for time series anomaly detection, treats the process as a sequential decision-making task. It uses a coarse-to-fine approach to identify anomalies, adaptive feature preparation, and self-reflection to refine its decisions. The system is powered by a hybrid inference mechanism that combines general-purpose models and reinforcement learning to optimize performance, showing consistent improvements in diverse settings.


<details>
  <summary>Details</summary>
Motivation: The motivation behind AnomaMind is to address the limitations of current anomaly detection methods, which often treat the task as a fixed-feature, discriminative prediction problem. This can lead to poor performance when dealing with anomalies that are context-dependent or exhibit diverse patterns. AnomaMind aims to introduce an evidence-driven, diagnostic process that includes adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference.

Method: AnomaMind employs a structured workflow for anomaly detection, featuring a coarse-to-fine localization of anomalous intervals, multi-turn tool interactions for adaptive feature preparation, and self-reflective refinement of anomaly decisions. A key component is the hybrid inference mechanism, where general-purpose models handle tool interaction and self-reflection, while anomaly detection decisions are optimized through reinforcement learning based on verifiable feedback at the workflow level.

Result: Experiments conducted across various scenarios have shown that AnomaMind consistently outperforms existing methods in terms of anomaly detection accuracy. The framework's ability to adapt to different contexts and refine its decisions through an iterative process contributes to its superior performance.

Conclusion: The AnomaMind framework represents a significant advancement in time series anomaly detection, offering a more flexible and context-sensitive approach. By treating anomaly detection as a sequential decision-making process, it effectively addresses the challenges posed by context-dependent and diverse anomalies, leading to improved detection performance.

Abstract: Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.

</details>


### [68] [Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation](https://arxiv.org/abs/2602.13810)
*Guojian Zhan,Letian Tao,Pengcheng Wang,Yixiao Wang,Yiheng Li,Yuxin Chen,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出了平均速度策略（MVP），一种新的生成策略函数，通过建模平均速度场实现最快的一步动作生成，并引入瞬时速度约束以保证高表达性。实验表明，该方法在多个机器人操作任务中取得了最先进的成功率，并显著提高了训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 流式策略虽然在处理复杂动作分布方面表现出色，但其表现力与计算负担之间存在权衡，通常由流步骤的数量控制。为了克服这一局限，本文提出了一种旨在提高表达力同时减少计算量的新方法。

Method: 提出了一种名为平均速度策略（MVP）的新生成策略函数，它通过建模平均速度场来加速动作生成过程。为了确保模型具有高度的表现力，在训练过程中对平均速度场施加了瞬时速度约束（IVC）。

Result: MVP在Robomimic和OGBench中的多个挑战性机器人操作任务上达到了最先进水平的成功率。此外，相较于现有的基于流的策略基线，MVP在训练和推理速度上都有显著提升。

Conclusion: 本文介绍的方法不仅增强了策略的表现力，还有效降低了计算成本，为强化学习领域提供了更高效、更具表现力的策略选择方案。

Abstract: Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

</details>


### [69] [Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference](https://arxiv.org/abs/2602.13813)
*Jorge Carrasco-Pollo,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 本文提出了Pawsterior，一种变分流匹配框架，旨在改进和扩展基于模拟的推理（SBI）。通过引入端点诱导仿射几何限制原则，直接将域几何纳入推理过程，并能够处理包含离散潜在结构的SBI任务，从而解决了标准流匹配方法在受限空间应用上的不足。


<details>
  <summary>Details</summary>
Motivation: 很多基于模拟的推理问题涉及到受结构化领域约束的后验分布，比如有界物理参数或混合离散-连续变量，但是一般的流匹配方法通常是在无约束空间中操作的。这种不匹配导致了学习效率低下并且难以遵守物理约束条件。

Method: 首先，作者推广了CatFlow的几何归纳偏置，正式提出了一种名为端点诱导仿射几何限制的原则，该原则通过一个双边变分模型直接将域几何整合到推理过程中。其次，提出的变分参数化使得Pawsterior能够处理那些本质上与传统流匹配方法不兼容、涉及离散潜在结构的SBI任务。

Result: 此方法不仅提高了抽样过程中的数值稳定性，还带来了更好的后验保真度，这在标准SBI基准测试上得到了验证。此外，它允许对以前无法使用常规流匹配方法解决的一类更广泛的结构化SBI问题进行探索。

Conclusion: 通过同时应对几何约束和离散潜在结构，Pawsterior扩展了流匹配的应用范围至先前不可达的一系列结构化SBI问题。

Abstract: We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.

</details>


### [70] [Testing For Distribution Shifts with Conditional Conformal Test Martingales](https://arxiv.org/abs/2602.13848)
*Shalev Shaer,Yarin Bar,Drew Prinster,Yaniv Romano*

Main category: cs.LG

TL;DR: 提出了一种新的顺序测试方法，通过固定参考数据集来检测分布变化，避免了现有方法中的测试时污染问题。该方法提供了任意时刻有效的I型错误控制，并保证了渐近功率为一和预期检测延迟有界。实验表明，这种方法比标准的CTMs更快地检测到变化。


<details>
  <summary>Details</summary>
Motivation: 现有的CTM检测器在每次接收新样本时都会不断扩充参考集，以此来评估新样本相对于过去观察结果的异常程度。尽管这种设计提供了任意时刻有效的I型错误控制，但存在测试时污染的问题：一旦发生变动后，变动后的观测值会进入参考集，从而稀释了分布变动的证据，增加了检测延迟并降低了功效。

Method: 本研究提出的方法通过将每个新样本与固定的零假设参考数据集进行比较，从设计上避免了污染问题。主要技术贡献在于构建了一个稳健的鞅，在考虑到由有限参考集引起的参考分布估计误差的情况下，该鞅在给定零假设参考数据条件下依然有效。

Result: 该方法不仅提供了任意时刻有效的I型错误控制，还保证了渐近功率达到1以及预期检测延迟有界限。实验证明，此方法比传统的CTMs能够更快速地检测到分布的变化。

Conclusion: 这项工作介绍了一种新颖的、基于固定参考数据集的顺序测试策略，用于检测任意分布变化。它解决了现有方法中存在的测试时污染问题，并且在理论上和实践上都证明了其有效性。

Abstract: We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.
  In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.

</details>


### [71] [sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals](https://arxiv.org/abs/2602.13857)
*Weixuan Yuan,Zengrui Jin,Yichen Wang,Donglin Xie,Ziyi Ye,Chao Zhang,Xuesong Chen*

Main category: cs.LG

TL;DR: 本文介绍了一种名为sleep2vec的基础模型，它能够处理多样且不完整的夜间生物信号，并通过跨模态对齐学习共享表示。该模型在睡眠分期和临床结果评估方面表现出色，对于不同模量子集及传感器丢失情况具有鲁棒性。此外，研究还首次描述了关于夜间生物信号的扩展法则。


<details>
  <summary>Details</summary>
Motivation: 传统的睡眠分期到临床诊断依赖于多种设备捕获夜间生物信号，但设备间的异质性和频繁的传感器故障给这些多模态信号的统一建模带来了挑战。

Method: 提出了一种名为sleep2vec的基础模型，该模型利用跨模态对齐从42,249个过夜记录中对比预训练，覆盖九种模态，并采用Demography、Age、Site & History-aware InfoNCE目标函数，结合生理和采集元数据（如年龄、性别、记录地点）来动态加权负样本并减轻特定队列的捷径问题。

Result: 在下游睡眠分期和临床结果评估任务上，sleep2vec持续优于强基准，并且对任何可用模态子集和传感器丢失保持鲁棒。此外，本研究还首次表征了关于夜间生物信号相对于模态多样性和模型容量的扩展法则。

Conclusion: 统一的跨模态对齐加上有原则的扩展方法支持了现实世界中夜间生物信号的有效标签使用和通用建模。

Abstract: Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.

</details>


### [72] [Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning](https://arxiv.org/abs/2602.13934)
*Zhimin Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息结构的五级可学习性层次体系，解释了为什么代码生成比强化学习更可靠地进步，并指出机器学习进展的上限更多取决于任务是否可学习，而非模型大小。


<details>
  <summary>Details</summary>
Motivation: 作者注意到代码生成比强化学习更加可靠地取得了进展，主要原因是代码具有使得其易于被学习的信息结构。这启发了作者探索不同任务的可学习性与其信息结构之间的关系。

Method: 文章通过建立表达性、可计算性和可学习性三种计算问题属性之间的正式区别，提出了一个五级的学习能力层次体系，并给出统一模板来明确这些结构性差异。

Result: 研究表明，监督学习在代码上的可扩展性是可预测的，而强化学习则不然；同时指出了仅靠扩大规模就能解决剩余ML挑战这一常见假设需要重新审视。

Conclusion: 任务的可学习性对于机器学习的进步至关重要，且与任务本身的信息结构紧密相关。相比于单纯增加模型大小，找到使任务变得可学习的方法更为关键。

Abstract: Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.

</details>


### [73] [An Adaptive Model Selection Framework for Demand Forecasting under Horizon-Induced Degradation to Support Business Strategy and Operations](https://arxiv.org/abs/2602.13939)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: 本文提出了一种名为AHSIV的模型选择框架，旨在解决由于预测范围引起的排名不稳定性问题。该方法在Walmart、M3、M4和M5数据集上进行了测试，并证明了其在提高特定时间范围内的最佳模型选择频率方面优于最强单一指标基线。


<details>
  <summary>Details</summary>
Motivation: 商业环境中的需求间歇性和高变异性对模型选择提出了挑战，没有一种预测模型能够普遍适用，且不同错误度量标准、需求状态和预测范围下模型排名会有所不同，这导致了多SKU决策背景下的不确定性。

Method: 提出了AHSIV（针对间歇性和变异性的自适应混合选择器），这是一种考虑了预测范围并根据需求状态调整的选择框架。它结合了通过“基于预测范围的度量退化”过程调整的缩放误差度量和绝对误差度量、结构需求分类、多目标帕累托优势以及层次偏差细化于一体化的决策架构中。

Result: 实验结果表明，在多种训练-测试划分方案及十二步预测范围内，AHSIV在总体性能上与最强单一指标基线达到统计等效性的同时，还提高了特定时间范围内的最优模型选择频率。

Conclusion: 研究发现指出，在异质需求环境下，模型选择不能被视为静态排名问题；而一致地考虑预测范围、结构适应性强的机制为多SKU预测提供了一个原则性的、操作连贯的解决方案。

Abstract: Business environments characterized by structural demand intermittency, high variability, and multi-step planning horizons require robust and reproducible model selection mechanisms. Empirical evidence shows that no forecasting model is universally dominant and that relative rankings vary across error metrics, demand regimes, and forecast horizons, generating ambiguity in multi-SKU decision contexts. This study proposes AHSIV (Adaptive Hybrid Selector for Intermittency and Variability), a horizon-aware and regime-conditioned model selection framework designed to address horizon-induced ranking instability. The proposed approach integrates scaled and absolute error metrics adjusted through a Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture. The empirical evaluation is conducted on the Walmart, M3, M4, and M5 datasets under multiple train-test partition schemes and twelve-step forecasting horizons. Results indicate that AHSIV achieves statistical equivalence with the strongest monometric baseline in terms of aggregated performance while increasing the frequency of horizon-specific best-model selection. The findings demonstrate that model selection in heterogeneous demand environments cannot be treated as a static ranking problem, and that horizon-consistent, structurally adaptive mechanisms provide a principled, operationally coherent solution for multi-SKU forecasting.

</details>


### [74] [You Can Learn Tokenization End-to-End with Reinforcement Learning](https://arxiv.org/abs/2602.13940)
*Sam Dauncey,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本研究提出了一种使用得分函数估计来学习LLM中分词边界的新方法，该方法通过直接优化离散分词边界的绘制以最小化损失，提供了更严格的理论保证。结合强化学习中的时间折扣技术，可以有效降低得分函数的方差，使其在实践中更加可行。实验结果表明，在1亿参数规模下，新方法在定性和定量上均优于之前提出的直通估计法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）架构趋向于端到端的设计，但分词化作为硬编码压缩步骤仍然存在于训练流程中。已有研究表明，通过将此压缩步骤引入LLMs架构内并采用启发式方法确定分词边界可以获得良好的效果；此外，尝试利用直通估计法学习这些边界，将确定离散分词边界的问题视为连续问题处理。然而，本研究旨在探索一种新的方式——即利用得分函数估计来学习这些分词边界，从而直接解决离散化问题，并减少因处理连续问题而产生的偏差。

Method: 研究人员采用了得分函数估计的方法来学习LLM中的分词边界，这种方法相比之前的直通估计法能够提供更严格的理论保障，因为它直接针对离散分词边界进行优化以达到最小化损失的目标。同时，为了控制得分函数估计过程中遇到的高方差问题，研究团队引入了来自强化学习领域的时间折扣等技术手段。

Result: 研究结果表明，所提出的方法不仅在理论上具有优势，而且实际应用中也表现出色：在1亿参数级别的模型上，无论是从定性还是定量的角度来看，该方法都超过了先前基于直通估计的技术表现。

Conclusion: 本论文介绍了一种新颖且有效的策略用于学习LLM内部的分词边界，相较于现有的解决方案，它通过直接优化离散决策过程展现了更好的性能。这一发现为未来改进LLM训练流程提供了新的视角。

Abstract: Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs' architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.

</details>


### [75] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习训练范式——体验强化学习(ERL)，通过在强化学习过程中嵌入明确的经验-反思-巩固循环，将反馈转化为结构化的行为修正，从而提高了探索效率和优化稳定性。实验表明，在稀疏奖励控制环境和能动推理基准测试中，ERL相比强大的RL基线方法显著提升了学习效率与最终表现。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法在处理语言模型从环境中学习时遇到的稀疏且延迟反馈问题上存在挑战，因为模型需要隐式地推断观察到的失败如何转化为未来迭代中的行为改变。为了解决这个问题，提出了体验强化学习(ERL)来更有效地利用反馈进行学习。

Method: 研究者们开发了Experiential Reinforcement Learning (ERL)，一种将经验-反思-巩固循环直接嵌入到强化学习流程中的新训练模式。对于给定任务，该模型首先尝试生成一个初步方案，然后基于所接收到的环境反馈产出反思，并据此指导第二次更为精细的尝试；成功的尝试会被强化并内化至基础策略之中。

Result: 实验结果表明，在面对稀疏奖励控制环境及涉及工具使用的推理任务时，相较于其他先进的强化学习基线技术，ERL能够分别提升高达81%和11%的学习效率与最终性能。

Conclusion: 整合显式的自我反思机制于策略训练中提供了一种实用的方法，可以有效转化外部反馈为持久的行为改进，从而增强模型的表现。

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [76] [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)
*Yuhang Li,Reena Elangovan,Xin Dong,Priyadarshini Panda,Brucek Khailany*

Main category: cs.LG

TL;DR: 本文提出了一种名为Quantized Reinforcement Learning (QuRL)的方法，通过使用量化执行者来加速基于强化学习训练大语言模型中的rollout过程。该方法解决了长期训练崩溃和权重更新问题，并在实验中实现了20%到80%的训练加速。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的自回归解码特性，在基于强化学习的训练过程中，rollout过程成为了效率瓶颈，占总训练时间的70%。为了解决这个问题，提出了Quantized Reinforcement Learning (QuRL)方法，旨在通过采用量化执行者来加快rollout速度。

Method: 提出了两种技术解决QuRL中的两个挑战：一是Adaptive Clipping Range (ACR)，它根据全精度执行者与量化执行者之间的策略比率动态调整剪切比率，这对于缓解长期训练崩溃至关重要；二是不变缩放技术，用以减少量化噪声并增加权重更新，从而应对权重更新问题。

Result: 通过在DeepScaleR和DAPO上进行INT8和FP8量化实验评估了所提方法，结果显示在训练过程中实现了20%到80%的rollout加速。

Conclusion: QuRL通过引入量化执行者有效提高了基于强化学习的大语言模型训练效率，同时提出的ACR和不变缩放技术成功解决了伴随而来的训练稳定性及权重更新难题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.

</details>


### [77] [Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds](https://arxiv.org/abs/2602.13960)
*Zedong Wang,Yuyang Wang,Ijay Narang,Felix Wang,Yuzhou Wang,Siva Theja Maguluri*

Main category: cs.LG

TL;DR: 本文提供了固定步长条件下，随机逼近算法稳态与适当高斯分布之间Wasserstein距离的具体非渐近误差界，并进一步导出了非均匀Berry-Esseen型尾部概率界。此外，还对一般凸目标的SGD进行了分析，确定了正确的尺度下非高斯（Gibbs）极限律，并给出了相应的预极限Wasserstein误差界。


<details>
  <summary>Details</summary>
Motivation: 尽管已知随着步长$\alpha \downarrow 0$，中心化和缩放后的稳态会弱收敛到一个高斯随机向量，但对于固定的$\alpha$，这种弱收敛并不能为用高斯极限来近似稳态提供可用的误差界。因此，本研究旨在提供固定步长条件下的明确、非渐近误差界。

Method: 首先证明了一般性的定理，用于在满足漂移正则性和噪声矩条件的情况下，限制中心化-缩放稳态与适当的高斯分布之间的Wasserstein距离。接着，在三个代表性SA设置中实例化这些定理：(1) 平滑强凸目标函数的随机梯度下降(SGD)，(2) 线性SA，以及(3) 收缩非线性SA。最后，对于超出强凸性的一般凸目标的SGD，研究了其在正确缩放下所呈现的非高斯（Gibbs）极限律。

Result: 得到了维数和步长依赖的显式边界，在小$\alpha$情况下Wasserstein距离的阶为$\alpha^{1/2}\log(1/\alpha)$。基于Wasserstein近似误差，进一步推导出比较稳态尾概率与高斯尾的非均匀Berry-Esseen型尾部概率界。此外，针对一般凸目标的SGD，识别出正确的尺度下的一种非高斯（Gibbs）极限律，并给出相应的预极限Wasserstein误差界。

Conclusion: 这项工作为固定步长条件下随机逼近方法的稳态行为提供了新的见解，特别是通过提供非渐近误差界及非均匀尾部概率界，增强了我们对这类算法的理解。此外，对于更广泛的目标函数类别，即一般凸情况，也进行了探索并发现了一种新的非高斯极限行为。

Abstract: Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $α\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $α$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $α$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $α^{1/2}\log(1/α)$ for small $α$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $α$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.

</details>


### [78] [KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra](https://arxiv.org/abs/2602.14011)
*Liangyu Su,Jun Shu,Rui Liu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为KoopGen的新框架，它基于生成器的神经Koopman方法，通过结构化的、依赖于状态的Koopman生成器表示来建模动力学系统。该方法在保持精确算子理论约束的同时分离了保守传输与不可逆耗散，从而提高了预测精度和稳定性，并有助于理解连续谱动力学中的哪些成分是可以解释并可学习的。


<details>
  <summary>Details</summary>
Motivation: 对于高维且时空混沌的动力系统进行表示和预测仍然是动力系统和机器学习领域的一个基本挑战。虽然数据驱动模型可以实现准确的短期预测，但它们通常缺乏稳定性、可解释性和在宽带或连续光谱主导情况下的可扩展性。基于Koopman的方法为非线性动态提供了一个有原则的线性视角，但现有方法依赖于有限维假设或显式的光谱参数化，在高维环境中性能下降。

Method: 提出了KoopGen，一种基于生成器的神经Koopman框架，通过构建一个依赖于状态的Koopman生成器结构化表示来模拟动力学行为。此方法利用内在的笛卡尔分解将斜自伴随（skew-adjoint）与自伴随（self-adjoint）部分分开，同时在学习过程中施加精确的操作数理论限制。

Result: 从非线性振荡器到高维混沌及时空动力学的各种系统中，KoopGen提高了预测准确性与稳定性，并阐明了连续谱动力学中的哪些组成部分能够被赋予可解释性和可学习性的表征。

Conclusion: KoopGen作为一种创新的框架，在处理复杂高维动力学问题时展现出优越性能，不仅增强了预测能力，还促进了对连续谱动力学特征的理解。

Abstract: Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

</details>


### [79] [EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models](https://arxiv.org/abs/2602.14024)
*Xinxing Zhou,Qingren Yao,Yiji Zhao,Chenghao Liu,Flora Salim,Xiaojie Yuan,Yanlong Wen,Ming Jin*

Main category: cs.LG

TL;DR: 本文提出了一种新的基础模型家族EIDOS，通过将预训练从未来值预测转变为潜在空间的预测学习来优化时间序列模型。该方法鼓励产生结构化和时间上连贯的潜在状态，并通过联合目标函数结合了潜在空间对齐、观察性接地以及直接预测监督。实验结果表明，EIDOS在GIFT-Eval基准测试中表现优异，减少了表示空间中的结构碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 当前的时间序列基础模型通常直接通过预测未来的观测值来进行预训练，这种方法往往导致捕获表面噪声而非连贯可预测的时间动态的弱结构潜在表示。为了解决这一问题，研究者们开发了EIDOS模型。

Method: EIDOS利用因果Transformer来预测潜在表示的发展，并设计了一个轻量级聚合分支来构建目标表示，以确保潜在空间学习的目标稳定性。整个模型通过一个综合了潜在空间对齐、与输入信号相连接的观察性接地以及直接预测监督的联合目标函数进行优化。

Result: 在GIFT-Eval基准测试中，EIDOS显著减轻了表示空间内的结构性碎片化问题，并取得了最先进的性能。

Conclusion: 研究表明，约束模型学习可预测的潜在动力学是迈向更稳健可靠的时间序列基础模型的一个有原则步骤。

Abstract: Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

</details>


### [80] [Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers](https://arxiv.org/abs/2602.14050)
*Atsushi Shimizu,Shohei Taniguchi,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 本文提出了一种新的位置编码策略——随机浮点采样（RFS），该方法通过使用随机采样的连续值而非预定义的离散集来选择位置索引，从而提高语言模型在未见过长度输入上的泛化能力。实验表明，RFS不仅在长度泛化任务上表现优异，而且在零样本常识推理基准测试中也表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对语言模型难以处理比预训练时更长输入的问题，即长度泛化问题，研究者们提出了随机浮点采样（RFS）这一简单但强大的位置编码方案，旨在解决当面对未曾见过的数据长度时出现的分布外(OOD)问题。

Method: RFS通过采用随机生成的连续数值代替从固定离散集合选取的位置索引来作为位置编码的基础，以此方式让模型在训练过程中接触到多样化的索引，进而避免了因长度超出训练数据范围而导致性能下降的情况。此外，RFS的优势可以很容易地与绝对正弦编码、RoPE和ALiBi等现有位置编码技术结合。

Result: 实验证明，相比传统的方法，RFS在处理未知长度输入的任务上展现出了更好的长度泛化能力，并且还在零样本常识推理任务中取得了更优的表现。

Conclusion: 随机浮点采样（RFS）作为一种新颖的位置编码策略，在提升语言模型对不同长度输入的适应性方面展现出巨大潜力，特别是在长度泛化及零样本学习场景下具有显著优势。

Abstract: Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.

</details>


### [81] [Policy Gradient with Adaptive Entropy Annealing for Continual Fine-Tuning](https://arxiv.org/abs/2602.14078)
*Yaqian Zhang,Bernhard Pfahringer,Eibe Frank,Albert Bifet*

Main category: cs.LG

TL;DR: 本文提出了一种新的训练策略aEPG，通过从探索性学习（类似CE）过渡到开发性学习（类似EPG），以提高预训练视觉模型在类增量设置下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型预训练视觉模型取得了成功，但在类增量设置下适应新任务时仍然容易遭受灾难性遗忘。当前参数高效微调方法大多依赖于交叉熵损失来学习新数据，而本文旨在直接最小化分类错误率，并提出一种新的训练策略以改善模型的适应性能。

Method: 通过将分类问题构建成一步马尔可夫决策过程，作者们导出了一个预期策略梯度(EPG)方法，该方法能够直接以低方差梯度估计减少错分误差。基于对CE和EPG之间关系的理解，提出了自适应熵退火(aEPG)训练策略，它能够在探索性与开发性学习模式间平滑过渡。

Result: aEPG 基于的方法在不同的基准测试和各种PEFT模块上都优于基于CE的方法。此外，研究还评估了多种熵正则化方法，证明输出预测分布的熵越低，预训练视觉模型的适应性越好。

Conclusion: 本研究表明，通过采用aEPG这样直接针对0-1损失优化的方法，可以有效提升预训练视觉模型在面对新任务时的表现，尤其是在类增量学习场景中。

Abstract: Despite their success, large pretrained vision models remain vulnerable to catastrophic forgetting when adapted to new tasks in class-incremental settings. Parameter-efficient fine-tuning (PEFT) alleviates this by restricting trainable parameters, yet most approaches still rely on cross-entropy (CE) loss, a surrogate for the 0-1 loss, to learn from new data. We revisit this choice and revive the true objective (0-1 loss) through a reinforcement learning perspective. By formulating classification as a one-step Markov Decision Process, we derive an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with a low-variance gradient estimation. Our analysis shows that CE can be interpreted as EPG with an additional sample-weighting mechanism: CE encourages exploration by emphasizing low-confidence samples, while EPG prioritizes high-confidence ones. Building on this insight, we propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning. aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.

</details>


### [82] [Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing](https://arxiv.org/abs/2602.14086)
*Jae-Hwan Choi,Jiwoo Yoon,Dohyun Kwon,Jaewoong Choi*

Main category: cs.LG

TL;DR: 本文研究了无限维希尔伯特空间中的神经最优传输问题，并通过高斯平滑策略解决了非正则设置下产生的虚假解问题，证明了在正则源测度条件下，该方法能够得到唯一的Monge映射。实验结果表明该方法优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 在非正则设置下，半对偶神经最优传输（Semi-dual Neural OT）常常产生无法准确捕捉目标分布的虚假解。为了解决这一问题，本文旨在通过分析和改进现有框架来提高算法的准确性。

Method: 文章采用基于布朗运动的高斯平滑策略扩展了半对偶框架，并且在正则测度框架内对该问题进行了理论分析。主要贡献在于证明了当源测度满足一定条件时，该公式是适定的并且可以恢复唯一的Monge映射。此外，还提供了关于平滑后测度正则性的精确刻画。

Result: 实验证明，对于合成函数数据集和时间序列数据集，所提出的方法能够有效抑制虚假解的生成，并且表现优于现有的基线方法。

Conclusion: 本研究表明，通过引入适当的平滑技术，可以在无限维空间中解决神经最优传输问题中存在的虚假解问题，从而更准确地逼近目标分布。

Abstract: We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.

</details>


### [83] [A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers](https://arxiv.org/abs/2602.14154)
*Yuxuan Linghu,Zhiyuan Liu,Qi Deng*

Main category: cs.LG

TL;DR: 提出了dXPP，一种基于惩罚的区分框架，它将二次规划(QP)求解与区分过程解耦，从而绕过了显式KKT区分中的困难，提高了计算效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法通过Karush-Kuhn-Tucker(KKT)系统来实现区分，但这种方法在大规模问题上的计算成本高且数值稳定性差。为了克服这些局限性，研究者提出了一种新的方法。

Method: dXPP框架首先在求解步骤（前向传递）中保持求解器无关性，允许使用任何黑盒QP求解器；接着，在区分步骤（反向传递）时，将解决方案映射到一个平滑近似的惩罚问题上，并对其进行隐式区分，仅需解决规模更小的线性系统。

Result: dXPP在多种任务上进行了评估，包括随机生成的QP、大规模稀疏投影问题以及一个多时期的投资组合优化实际案例。实验结果表明，dXPP不仅能够与基于KKT的区分方法相媲美，而且在处理大规模问题时实现了显著的速度提升。

Conclusion: dXPP提供了一个有效的方法来提高二次规划求解过程中区分操作的效率和鲁棒性，特别是在面对大规模问题时表现尤为突出。

Abstract: Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.

</details>


### [84] [Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling](https://arxiv.org/abs/2602.14169)
*Yiran Guo,Zhongjian Qiao,Yingqi Xie,Jie Liu,Dan Ye,Ruiqing Zhang,Shuang Qiu,Lijie Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Deep Dense Exploration (DDE) 的策略，旨在通过聚焦于深层可恢复状态（即pivots）来改善大型语言模型强化学习中的探索效率。DEEP-GRPO作为DDE的一个实例，结合了数据驱动的效用函数、局部密集重采样以及双流优化目标等创新点，实验显示其在数学推理基准上优于GRPO、基于树的方法及其他强基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强化学习中对于大型语言模型的有效探索存在局限性：GRPO仅从根部采样导致高概率轨迹饱和且深层次易错状态未充分探索；基于树的方法盲目分散预算至琐碎或不可恢复状态，造成样本稀释问题，无法揭示罕见正确的后缀并稳定局部基线。

Method: 提出了Deep Dense Exploration (DDE) 策略，并通过DEEP-GRPO具体实现，该方法包含三个主要创新点：1. 轻量级的数据驱动效用函数自动平衡可恢复性和深度偏差以识别关键状态；2. 在每个关键状态下进行局部密集重采样以增加发现正确后续轨迹的概率；3. 采用双流优化目标分离全局策略学习与局部修正更新。

Result: 在数学推理基准测试中，所提出的方法持续优于GRPO、基于树的方法以及其他强大的基线方法。

Conclusion: 本研究介绍的Deep Dense Exploration (DDE) 方法及其DEEP-GRPO实现有效地解决了当前大型语言模型强化学习中存在的探索挑战，通过集中探索重要状态提高了性能。

Abstract: Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.

</details>


### [85] [TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models](https://arxiv.org/abs/2602.14200)
*Nicolas Zumarraga,Thomas Kaar,Ning Wang,Maxwell A. Xu,Max Rosenblattl,Markus Kreft,Kevin O'Sullivan,Paul Schmiedmayer,Patrick Langer,Robert Jakob*

Main category: cs.LG

TL;DR: 本研究引入了TS-Haystack，一个长上下文时间检索基准测试，旨在解决现有时间序列语言模型在处理长时序数据时面临的挑战。实验表明，现有的时间序列编码器在增加上下文长度时会忽略时间粒度，这对分类任务有帮助但损害了局部事件的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列语言模型（TSLMs）在处理真实世界中可能包含数百万个数据点的长时间序列传感器流时存在局限性，尤其是在需要精确时间定位和严格计算限制的情况下。当前的基准测试未能涵盖这一场景。

Method: 创建了一个名为TS-Haystack的新基准测试，该基准涵盖了四大类十种任务类型，并通过将短活动片段嵌入到更长的时间加速度计记录中来实现系统性评估。此外，还探讨了不同模型和编码策略下分类与检索行为之间的差异。

Result: 研究发现，随着上下文长度的增加，学习到的潜在压缩虽然能够保持甚至提高分类准确性（压缩比高达176倍），但对于局部事件的检索性能却有所下降。这表明，在不牺牲时间精度的前提下，需要开发能够分离序列长度与计算复杂性的架构设计。

Conclusion: 为了解决长上下文检索问题，需要新的架构设计方法，以确保在扩展上下文长度的同时保持时间信息的准确性。

Abstract: Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.

</details>


### [86] [Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws](https://arxiv.org/abs/2602.14208)
*Jinbo Wang,Binghui Li,Zhanpeng Zhou,Mingze Wang,Yuxuan Sun,Jiaqi Zhang,Xunliang Cai,Lei Wu*

Main category: cs.LG

TL;DR: 本研究利用功能缩放定律(FSL)框架分析了批量大小调度(BSS)在大规模深度学习训练中的作用，揭示了针对不同难度任务的最佳BSS策略，并通过大型语言模型预训练实验证明了后期切换到大批次策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管批量大小调度对深度学习训练的优化动态和计算效率至关重要，但其理论基础尚不明确。

Method: 采用功能缩放定律（FSL）框架来分析批量大小调度问题，通过区分任务难易度提出相应的最佳调度策略，并基于快速追赶效应解释了为什么可以在训练后期安全地转向大批次处理而不牺牲性能。

Result: 研究表明对于简单任务，整个训练过程中持续增加批量大小为最优；而对于困难任务，则应在大部分训练时间内保持较小批量，在后期才转换至大批量。此外，通过广泛的大型语言模型预训练实验验证了理论预测的准确性。

Conclusion: 使用FSL框架分析BSS提供了一种有原则的方法，能够帮助理解并设计出更有效的批量大小调度方案，特别是在节省数据消耗的同时不牺牲性能方面展现出巨大潜力。

Abstract: Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.

</details>


### [87] [MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM](https://arxiv.org/abs/2602.14209)
*Omin Kwon,Yeonjae Kim,Doyeon Kim,Minseo Kim,Yeonhong Park,Jae W. Lee*

Main category: cs.LG

TL;DR: 本文提出MAGE，一种针对块扩散语言模型的稀疏注意力机制，通过在首次[MASK]去噪步骤中准确预测重要KV条目和预算需求，实现几乎无损的准确性同时显著减少KV预算并提高处理速度。此外，还提供了一种低成本微调策略以增强[MASK]引导模式的有效性。


<details>
  <summary>Details</summary>
Motivation: 块扩散语言模型虽然前景广阔，但在长上下文场景下由于KV缓存机制导致内存访问成为主要瓶颈。现有的动态稀疏注意力方法并不适用于块扩散模型，因此需要新的解决方案来优化这一问题。

Method: MAGE利用块扩散模型的独特性质，在第一次全[MASK]去噪步骤时就能可靠地预测出重要的KV条目及预算需求，从而只需进行一次精确的注意力传递，并且可以在无需额外训练的情况下重用这些信息来进行稀疏去噪。此外，还引入了一个轻量级的微调策略，进一步强化了[MASK]导向的模式。

Result: 实验表明，MAGE能够在保持接近无损精度的同时大幅度降低KV预算的需求，并且相比自回归稀疏注意力基线方案提供了高达3-4倍的整体加速。对于1.5B和7B规模的模型，仅需几小时的单个NVIDIA H100 GPU训练即可完成微调。

Conclusion: MAGE为块扩散语言模型提供了一种高效的稀疏注意力方法，不仅解决了KV缓存带来的性能瓶颈问题，还通过简单有效的微调策略增强了模型的表现力，展示了其在长文本生成任务中的巨大潜力。

Abstract: Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.

</details>


### [88] [Robust multi-task boosting using clustering and local ensembling](https://arxiv.org/abs/2602.14231)
*Seyedsaman Emami,Daniel Hernández-Lobato,Gonzalo Martínez-Muñoz*

Main category: cs.LG

TL;DR: 本文提出了一种鲁棒的多任务学习框架RMB-CLE，它通过基于错误的任务聚类和局部集成来避免负迁移现象。实验表明该方法在合成数据和真实世界基准测试中均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统的多任务学习方法在面对不相关或噪声任务时可能会遭受负迁移问题，影响模型性能。因此，需要一种能够有效识别并处理任务间关系的方法，以提高学习效果。

Method: 作者提出了RMB-CLE框架，该框架结合了基于误差的任务聚类与局部集成策略。通过对跨任务错误进行分析，直接从这些错误中推导出任务间的相似性，并利用凝聚式聚类自适应地将任务分组。每个集群内则通过局部集成机制实现知识共享的同时保持特定于任务的模式。

Result: 实验结果显示，在合成数据上RMB-CLE能够恢复真实的任务簇结构；而在多种实际及合成基准测试中，相比于其他多任务、单任务以及基于池化的集成方法，RMB-CLE表现出一致性的优越性能。

Conclusion: RMB-CLE不仅仅是一种简单的聚类与增强技术的组合，而是一个通用且可扩展的框架，为实现鲁棒的多任务学习提供了新的基础。

Abstract: Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.

</details>


### [89] [Evaluating LLMs in Finance Requires Explicit Bias Consideration](https://arxiv.org/abs/2602.14233)
*Yaxuan Kong,Hoyoung Lee,Yoontae Hwang,Alejandro Lopez-Lira,Bradford Levy,Dhagash Mehta,Qingsong Wen,Chanyeol Choi,Yongjae Lee,Stefan Zohren*

Main category: cs.LG

TL;DR: 本文探讨了金融领域大型语言模型（LLMs）应用中常见的五种偏差，包括前瞻偏差、幸存者偏差、叙述偏差、目标偏差和成本偏差，并指出这些偏差会以不同方式影响金融任务。通过回顾164篇相关论文，作者发现没有一种偏差在超过28%的研究中被讨论到。为此，文章提出了一种结构性有效性框架及评估清单，旨在为偏差诊断与未来系统设计提供最低要求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地融入金融工作流程，针对这些模型的评估实践却未能跟上步伐。金融特定偏差可能会夸大性能、污染回溯测试并导致报告结果对实际部署无用。

Method: 通过对2023年至2025年间发表的164篇论文进行回顾分析，识别出金融LLM应用中存在的五种常见偏差类型。基于此分析，提出了一个结构性有效性框架以及包含最小要求的评估清单，用于偏差诊断和指导未来系统的设计。

Result: 研究发现，在所考察的文献中没有任何一种偏差被超过28%的研究提及或讨论。这表明当前对于金融LLM系统中的偏差问题缺乏足够的关注。

Conclusion: 金融LLM系统中的偏差需要得到明确的关注，在任何结果被用来支持部署声明之前，必须确保结构上的有效性。为此，提出了一个新的框架和检查表来帮助识别偏差并指导未来的研究设计。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.

</details>


### [90] [Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting](https://arxiv.org/abs/2602.14267)
*Manal Rahal,Bestoun S. Ahmed,Roger Renström,Robert Stener*

Main category: cs.LG

TL;DR: 本文提出了一种基于迁移学习的框架DELTAiF，用于准确预测家庭热水消耗。该方法通过从代表性家庭学到的知识来调整其他家庭的模型，无需为每个热泵安装单独训练机器学习模型，从而大大减少了训练时间（约67%），同时保持了高预测精度（0.874至0.991之间）和较低的平均绝对百分比误差值（0.001至0.017）。


<details>
  <summary>Details</summary>
Motivation: 随着住宅热泵安装数量的迅速增加，优化家庭热水生产变得至关重要，但面临技术和可扩展性的挑战。为了适应实际的家庭需求，需要准确预测热水需求以确保舒适度并减少能源浪费。然而，传统的针对每个家庭单独训练机器学习模型的方法在大规模应用时计算成本高昂，特别是在云连接的热泵部署中。

Method: 本研究引入了DELTAiF，这是一种基于迁移学习（TL）的框架，能够提供可扩展且准确的家庭热水消耗预测。DELTAiF通过预测大型热水使用事件（如淋浴）来实现家庭层面的自适应且可扩展的热水生产。它利用从一个代表性家庭学到的知识，并将其调整应用于其他家庭，避免了为每个热泵安装单独训练机器学习模型的需求。

Result: 这种方法将整体训练时间减少了大约67%，同时保持了介于0.874到0.991之间的高预测准确性值以及0.001到0.017之间的平均绝对百分比误差值。结果表明，当源家庭表现出规律性消费模式时，迁移学习特别有效，使得大规模热水需求预测成为可能。

Conclusion: DELTAiF框架证明了迁移学习在提高家庭热水消耗预测效率与准确性方面的潜力，特别是对于具有规律性消费模式的家庭而言，这为实现高效、节能的家庭热水供应提供了新的解决方案。

Abstract: With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.
  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.

</details>


### [91] [Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization](https://arxiv.org/abs/2602.14272)
*Yilun Kuang,Yash Dagade,Deep Chakraborty,Erik Learned-Miller,Randall Balestriero,Tim G. J. Rudner,Yann LeCun*

Main category: cs.LG

TL;DR: 本文提出了一种新的自监督学习方法Radial-VCReg，通过引入径向高斯化损失来优化特征的范数分布，使其更接近卡方分布，从而比现有方法VCReg更能有效地将各种分布转换为正态分布，并在合成数据集和真实世界数据集中表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的自监督学习方法如VCReg通过规范一阶和二阶特征统计量来解决维度灾难问题，但无法完全达到最大熵状态。为了克服这一限制，作者们希望开发一种新方法能够减少高阶依赖性并促进更加多样化且信息丰富的表示。

Method: 提出了Radial-VCReg，在VCReg的基础上增加了一个径向高斯化损失项，该损失项促使特征范数与卡方分布对齐——这是高维高斯分布的一个定义特性。

Result: 实验结果表明，无论是在合成数据集还是真实世界的数据集上，Radial-VCReg都能够持续地通过减少高阶依赖性和促进更具多样性和信息性的表示来提高性能。

Conclusion: Radial-VCReg作为一种改进了的信息最大化方法，能够更好地将不同类型的分布推向正态分布，从而产生更优的表现。

Abstract: Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.

</details>


### [92] [Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data](https://arxiv.org/abs/2602.14274)
*Boning Zhou,Ziyu Wang,Han Hong,Haoqi Hu*

Main category: cs.LG

TL;DR: 本文提出了一种利用基于Transformer的语言模型从非结构化文本中进行因果推理的框架，并通过与结构化数据得到的因果估计结果比较，验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，用于因果推断的结构化数据可能不完整或不可用，因此需要一种能够使用非结构化文本执行因果推断的方法。

Method: 开发了一个框架，该框架利用基于Transformer的语言模型来处理非结构化文本，以执行因果推断任务。

Result: 研究发现表明，从非结构化文本中得出的因果估计与从结构化数据中获得的结果一致，证明了非结构化文本在因果推断中的潜力。

Conclusion: 本研究扩展了因果推断方法的应用范围，使其能够在仅有文本数据可用的情况下支持数据驱动的商业决策制定。

Abstract: Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.

</details>


### [93] [Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems](https://arxiv.org/abs/2602.14275)
*Lamine Rihani*

Main category: cs.LG

TL;DR: 本文提出了一种称为反向n元输出测试的新方法，通过直接在特定领域的输出等价类上构建覆盖数组来解决AI/ML系统和量子计算软件的测试挑战。这种方法能够合成输入特征配置或量子电路参数，从而从不透明模型中引出目标行为特征。该框架提供了跨两个领域的协同优势，包括提高故障检测率、增强测试套件效率以及结构化的MLOps/量子验证流程。


<details>
  <summary>Details</summary>
Motivation: 针对AI/ML系统及新兴量子计算软件中存在的前所未有的测试难题，比如高维度/连续输入空间、概率性/非确定性输出分布等，研究者们提出了新的测试方法论。

Method: 引入了反向n元输出测试，这是一种基于数学原理的方法转换，它直接在领域特定的输出等价类之上构建覆盖数组，并通过无梯度元启发式优化解决计算上具有挑战性的黑盒逆映射问题，以合成能够从不透明模型中引发目标行为签名的输入特征配置或量子电路参数。

Result: 该框架为机器学习校准/边界失败和量子错误综合症提供了显著改进的故障检测率；增强了测试套件的效率；并且提供了带有自动分区发现功能的结构化MLOps/量子验证管道。

Conclusion: 提出的反向n元输出测试方法为应对AI/ML系统和量子计算软件的独特测试挑战提供了一个有效的解决方案，同时也在提高测试覆盖率、故障检测效率等方面展示了其优越性。

Abstract: Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.

</details>


### [94] [DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices](https://arxiv.org/abs/2602.14301)
*Songyuan Li,Jia Hu,Ahmed M. Abdelmoniem,Geyong Min,Haojun Huang,Jiwei Huang*

Main category: cs.LG

TL;DR: 提出了DeepFusion，一种新的联邦学习框架，通过联邦知识蒸馏融合设备上的异构大语言模型知识到全局Mixture-of-Experts (MoE) 模型中。该方法包括一个名为View-Aligned Attention (VAA) 的模块来解决因模型架构差异导致的视角不匹配问题，并在实际测试中展示了接近集中式训练的表现同时降低了通信成本和提高了token困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Mixture-of-Experts (MoE) 的大型语言模型需要大量的多样化训练数据，而传统的联邦学习方法要求每个设备都托管本地MoE模型，这对于资源受限的设备来说是不可行的。因此，研究者们开发了DeepFusion，旨在让不同设备根据自身条件配置并训练模型，然后通过联邦知识蒸馏整合这些知识以形成一个知识丰富的全局MoE模型。

Method: DeepFusion允许每台设备独立地设置和训练适合自己需求及硬件限制的大语言模型。此外，引入了一种新颖的View-Aligned Attention (VAA) 模块，它能够整合来自全局MoE模型的多阶段特征表示，从而创建出与设备上模型预测视角相一致的观点，有效促进了跨架构的知识迁移。

Result: 实验结果表明，使用行业级MoE模型（如Qwen-MoE 和 DeepSeek-MoE）以及真实世界数据集（例如医疗和金融领域），DeepFusion 能够达到接近于中心化MoE训练的表现。相比于其他重要的联邦MoE基线方法，DeepFusion 最多可以减少71%的通信开销，并将token困惑度提高最多5.28%。

Conclusion: DeepFusion 提供了一种创新且高效的方案来促进在资源受限设备间进行大规模语言模型的联合训练，不仅显著减少了通信成本，还提升了模型性能。

Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.

</details>


### [95] [In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes](https://arxiv.org/abs/2602.14318)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 本研究批判性地审视了Transformer模型的可信度问题，通过全面回顾其可解释性、鲁棒性、公平性和隐私性来评估这些模型在高风险应用中的可靠性，并识别出跨领域的结构脆弱性、特定领域风险以及开放的研究挑战。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer架构在诸如计算机视觉、自然语言处理、医疗保健等关键应用中的广泛应用，对其可信度进行深入和严格理解变得至关重要。

Method: 本文通过综合分析Transformer模型在自然语言处理、计算机视觉及科学与工程领域（如机器人学、医学、地球科学等）中安全关键应用的信任度，评价其可解释性、对对抗攻击的鲁棒性、公平性和隐私性等方面的表现。

Result: 研究发现了一些跨领域的结构脆弱性、特定于领域的风险以及限制Transformer可靠部署的开放研究挑战。

Conclusion: 尽管Transformer架构在多个领域取得了显著成就，但要实现其在安全关键应用中的完全信任仍面临诸多挑战，需要进一步研究以解决已识别的问题。

Abstract: Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.

</details>


### [96] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 本文提出了一种名为AERO的新方法，通过自适应rollout策略、选择性拒绝和贝叶斯后验来提高GRPO在大语言模型微调中的计算效率。实验结果表明，在不牺牲性能的情况下，AERO能够显著减少训练所需的计算资源和时间。


<details>
  <summary>Details</summary>
Motivation: 在使用GRPO对大型语言模型进行基于强化学习的微调时，当一组内的所有rollouts都具有相同的结果时（全正确或全错误），会导致组标准化优势为零，从而没有梯度信号产生，浪费了微调计算资源。为了克服这个问题，作者提出了AERO方法，旨在提高计算效率的同时保持或改进模型性能。

Method: AERO通过采用自适应rollout策略、实施选择性拒绝以策略性地剪枝rollouts，并维持一个贝叶斯后验分布来避免出现零优势区域。这种方法不仅提高了对于给定任务的学习效率，还减少了不必要的计算开销。

Result: 在三种不同配置的模型上测试显示，AERO能够在不牺牲性能的前提下，将总的训练计算量减少约48%，同时平均每个步骤的墙上时间缩短大约45%。此外，AERO在Pass@8和Avg@8指标上达到了与GRPO相当甚至更好的效果。

Conclusion: AERO提供了一种实用、可扩展且高效的策略，用于基于强化学习的大规模语言模型调整，它能在大幅降低计算需求的同时保持或提升模型表现。

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [97] [WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control](https://arxiv.org/abs/2602.14351)
*Mehran Aghabozorgi,Alireza Moazeni,Yanshu Zhang,Ke Li*

Main category: cs.LG

TL;DR: WIMLE, a model-based reinforcement learning method, improves sample efficiency and performance across 40 continuous-control tasks by using multi-modal world models and uncertainty-aware weighting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the common issues in model-based reinforcement learning, such as compounding model error, unimodal world models, and overconfident predictions, which lead to underperformance in practice.

Method: WIMLE extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework, enabling the learning of stochastic, multi-modal world models. It uses ensembles and latent sampling for estimating predictive uncertainty and weights each synthetic transition by its predicted confidence during training.

Result: WIMLE demonstrates superior sample efficiency and competitive or better asymptotic performance compared to strong model-free and model-based baselines across 40 continuous-control tasks. It notably improves sample efficiency on the Humanoid-run task and solves more tasks on HumanoidBench than other leading methods.

Conclusion: The results indicate that incorporating IMLE-based multi-modality and uncertainty-aware weighting in WIMLE can lead to stable and efficient model-based reinforcement learning, outperforming existing methods in several challenging scenarios.

Abstract: Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.

</details>


### [98] [A Study on Multi-Class Online Fuzzy Classifiers for Dynamic Environments](https://arxiv.org/abs/2602.14375)
*Kensuke Ajimoto,Yuma Yamamoto,Yoshifumi Kusunoki,Tomoharu Nakashima*

Main category: cs.LG

TL;DR: 本文提出了一种用于动态环境的多类在线模糊分类器，并通过合成动态数据和基准数据集上的数值实验评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 传统在线模糊分类器仅考虑两类问题，而本文旨在将其扩展到多类问题上。

Method: 本文提出的方法是基于一种多类在线模糊分类器，该分类器由一系列模糊if-then规则组成，其中前件模糊集由用户预先确定，而后件实值则通过从训练数据中学习得到。

Result: 通过对合成动态数据及若干基准数据集进行数值实验，验证了所提出的多类在线模糊分类器的有效性。

Conclusion: 本研究表明，所提出的多类在线模糊分类器能够有效地处理动态环境中的多类分类问题。

Abstract: This paper proposes a multi-class online fuzzy classifier for dynamic environments. A fuzzy classifier comprises a set of fuzzy if-then rules where human users determine the antecedent fuzzy sets beforehand. In contrast, the consequent real values are determined by learning from training data. In an online framework, not all training dataset patterns are available beforehand. Instead, only a few patterns are available at a time step, and the subsequent patterns become available at the following time steps. The conventional online fuzzy classifier considered only two-class problems. This paper investigates the extension to the conventional fuzzy classifiers for multi-class problems. We evaluate the performance of the multi-class online fuzzy classifiers through numerical experiments on synthetic dynamic data and also several benchmark datasets.

</details>


### [99] [The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization](https://arxiv.org/abs/2602.14423)
*Abdelali Bouyahia,Frédéric LeBlanc,Mario Marchand*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的框架，系统地解释了数据增强对泛化和不变性学习的影响。通过将增强分布建模为原始数据分布与变换分布的组合，作者推导出一个新的泛化界，该界分解为三个可解释的项：1) 原始数据与增强数据之间的分布差异；2) 测量算法对训练数据依赖性的稳定性项；3) 捕捉增强变异性影响的敏感性项。此外，引入了群直径的概念来连接界限与增强群的几何结构。


<details>
  <summary>Details</summary>
Motivation: 尽管数据增强技术被广泛应用于提高现代机器学习中的泛化能力，但其理论作用仍不完全清楚。为了更好地理解数据增强如何影响模型的泛化能力和不变性学习，本文试图从信息论的角度出发，提供一个系统的分析框架。

Method: 本研究基于互信息边界建立了一个新的理论框架，该框架通过将增强过程视为原始数据与一系列变换相结合的结果，从而自然地引入了轨道平均损失函数的概念。在对损失函数及增强过程做出次高斯假设的基础上，作者推导出了新的泛化边界。同时定义了‘群直径’这一概念，用以量化增强操作在输入空间中所能造成的最大扰动，并指出群直径对于控制上述三方面因素的重要性。

Result: 结果表明，新提出的泛化边界能够有效地分解并解释数据增强对泛化性能的影响。通过数值实验验证了所提理论边界的有效性，证明它可以可靠地追踪并预测真实泛化差距的行为。

Conclusion: 这项工作不仅加深了我们对数据增强背后机制的理解，还为设计更有效的增强策略提供了理论指导。特别是，通过‘群直径’这一概念揭示了增强强度与保持数据忠实度之间存在的内在权衡。

Abstract: Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.

</details>


### [100] [A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking](https://arxiv.org/abs/2602.14430)
*Prithwijit Chowdhury,Ahmad Mustafa,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.LG

TL;DR: 本文提出了一种统一框架，用于生成反事实并量化必要性和充分性，以评估LIME和SHAP在高维结构化油气勘探风险数据上所提供解释的稳健性。


<details>
  <summary>Details</summary>
Motivation: 由于基于机器学习的分类器在决策过程中的透明度不足，导致了可解释AI（XAI）方法如LIME和SHAP的发展。然而，这些方法对同一情景给出的解释可能不同，特别是对于复杂的数据集。因此，使用理论支持的因果概念来增强解释策略的可信度显得尤为重要。

Method: 提出一个统一框架来生成反事实，并且量化特征的重要性与相关性，然后利用该框架评估LIME和SHAP在处理高维度结构化油气勘探风险数据时所提供的解释的稳健性。

Result: 通过稳健性测试，深入理解模型处理错误数据的能力以及哪种XAI模块最适合我们的数据集进行油气指示。

Conclusion: 采用基于因果关系的概念来加强解释策略可以提高其可靠性及鲁棒性，从而为复杂的油气勘探风险评估提供更加值得信赖的结果。

Abstract: In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of "importance" and "relevance" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.

</details>


### [101] [S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations](https://arxiv.org/abs/2602.14432)
*Arnav Chavan,Nahush Lele,Udbhav Bamba,Sankalp Dayal,Aditi Raghunathan,Deepak Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种名为选择性谱衰减（$S^2D$）的方法，通过在微调过程中有针对性地正则化与最大奇异值相对应的权重分量来减少激活异常值，从而提高了模型量化后的准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型中的激活异常值给模型量化带来了挑战，导致量化过程中出现严重的精度下降问题。研究发现，随着预训练规模的增加（如从CLIP到更广泛训练的SigLIP和SigLIP2），异常值问题变得更加严重。为了解决这个问题，并且保持模型部署效率，提出了新的方法。

Method: 通过理论分析及实证研究，作者发现了激活异常值与权重矩阵的最大奇异值之间存在直接联系。基于这一发现，提出了选择性谱衰减($S^2D$)方法，在微调时仅对与最大奇异值相关的权重部分进行精确正则化处理。

Result: $S^2D$能够显著降低激活异常值，并生成易于量化的良好条件表示。实验表明，使用$S^2D$训练的模型在W4A4量化下ImageNet上的PTQ准确率提高了7%，结合QAT时可获得额外4%的增益。这些改进也适用于各种下游任务和视觉-语言模型。

Conclusion: 通过采用$S^2D$技术，可以在不牺牲部署效率的情况下扩展更大规模且经过严格训练的模型。此方法有效解决了因激活异常值导致的量化难题，促进了更高效、更高性能模型的发展。

Abstract: Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.

</details>


### [102] [Selective Synchronization Attention](https://arxiv.org/abs/2602.14445)
*Hasi Hays*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制——选择性同步注意力（SSA），它基于Kuramoto模型的耦合振荡器的稳态解来替代标准的点积自注意力。每个token被表示为一个具有可学习自然频率和相位的振子，其间的同步强度作为注意力权重。该方法提供了三个关键优势：自然稀疏性、统一的位置-语义编码以及单次闭式计算。


<details>
  <summary>Details</summary>
Motivation: 变压器架构已成为现代深度学习的基础，但其核心自注意力机制存在二次计算复杂度的问题，并且缺乏生物神经计算的支持。为了克服这些问题，作者提出了选择性同步注意力(SSA)这一新机制。

Method: 在SSA中，每个标记被视为具有可学习自然频率和相位的振荡器；标记对之间的同步强度通过频率依赖耦合和相位锁定条件确定，用作注意力权重。该公式提供了三个主要优点：(i) 自然稀疏性，(ii) 统一的位置-语义编码，(iii) 单程闭式计算。

Result: 通过分析同步矩阵显示，在初始化时甚至显示出非均匀的、头部多样性的耦合模式，这表明比随机初始化变压器产生的近似均匀注意力更强的架构归纳偏差。

Conclusion: 选择性同步注意力机制不仅解决了传统自注意力机制中存在的问题，还引入了更接近生物神经计算的概念。这种机制在保持高效的同时，也增强了模型的表达能力。

Abstract: The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.

</details>


### [103] [WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity](https://arxiv.org/abs/2602.14452)
*Lei Chen,Yuan Meng,Xiaoyu Zhan,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的权重感知混合粒度激活稀疏方法（WiSparse），通过结合激活和权重信息自适应分配稀疏性，以提高大型语言模型推理效率。在50%的稀疏度下，该方法能保持Llama3.1密集性能的97%，比最强基线高出2.23个百分点，并实现了端到端推理速度21.4%的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的无训练激活稀疏方法仅依赖于激活信息和统一的稀疏比率，忽略了权重的重要性以及模型块间敏感度变化的影响，导致性能不佳。研究者观察到了现代大型语言模型中的两个关键现象：不重要的激活可能与非常重要的权重对齐；稀疏敏感度在模型块之间非单调变化。

Method: 提出了Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse)，该方法利用激活和权重信息进行自适应稀疏性分配。具体来说，引入了权重感知机制，将激活幅度与预计算的权重范数相结合，准确识别显著通道。此外还设计了一个混合粒度分配方案：通过进化搜索将全局预算分配给各个区块以保护敏感区域，然后在区块内部细化以最小化重构误差。

Result: 实验表明，在50%的稀疏度下，WiSparse能够保留Llama3.1密集性能的97%，比最强基准高出2.23个百分点，同时实现了21.4%的端到端推理加速。

Conclusion: 本研究推进了无训练方法在高效LLM推理方面的极限，展示了在没有额外训练的情况下可以实现的速度提升的新边界。

Abstract: Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.

</details>


### [104] [Traceable Latent Variable Discovery Based on Multi-Agent Collaboration](https://arxiv.org/abs/2602.14456)
*Huaming Du,Tao Hu,Yijie Huang,Yu Zhao,Guisong Liu,Tao Gu,Gang Kou,Carl Yang*

Main category: cs.LG

TL;DR: 提出了一种新的因果建模框架TLVD，该框架结合了大型语言模型（LLMs）基于元数据的推理能力和传统因果发现算法（TCDA）的数据驱动建模能力，以推断潜在变量及其语义。通过在多个真实世界数据集上的实验验证了其有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中揭示潜在因果机制对于科学和技术进步至关重要。然而，高质量数据的缺乏以及传统因果发现算法对无潜在混淆因素假设的依赖性，以及它们倾向于忽视潜在变量的精确语义，一直是更广泛应用因果发现的主要障碍。

Method: 首先采用数据驱动的方法构建一个包含潜在变量的因果图。然后，利用多LLM协作进行潜在变量推断，将这一过程建模为信息不完全的游戏，并寻求贝叶斯纳什均衡（BNE）来推断可能的具体潜在变量。最后，为了验证跨多个基于Web的真实数据源中的推断潜在变量，使用LLM进行证据探索以确保可追溯性。

Result: 在医院提供的三个去标识化真实患者数据集和两个基准数据集上全面评估了TLVD。广泛的实验结果证实了TLVD的有效性和可靠性，在五个数据集中Acc平均提高了32.67%，CAcc提高了62.21%，ECit提高了26.72%。

Conclusion: 提出的TLVD框架成功地结合了LLMs与TCDA的优点，为推断潜在变量及其语义提供了一个有效解决方案，并且在多个实际数据集上展示了显著性能提升。

Abstract: Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.

</details>


### [105] [Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment](https://arxiv.org/abs/2602.14462)
*Hong Li,Zhen Zhou,Honggang Zhang,Yuping Luo,Xinyue Wang,Han Gong,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级、模型无关的诊断框架，用于量化数据并行训练中由于不同工作节点优化动态不一致导致的隐藏不稳定性。通过引入损失离散度、梯度范数离散度和梯度方向一致性三个指标，在不影响模型架构或同步机制的情况下提供了对潜在问题的可操作性洞察。实验表明这些指标能够有效揭示大规模细调过程中未被注意到的不稳定模式。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型的数据并行全参数微调过程中，虽然参数同步确保了每次迭代后模型权重数值上的等价性，但这并不意味着在梯度聚合前各工作节点级别的优化动态是一致的。这种跨工作节点的损失与梯度差异在传统聚合监控信号下可能保持隐形状态。

Method: 提出了一种轻量级且与模型无关的诊断框架来量化基于标准流程中可用训练信号的工作节点级别的一致性。该框架引入了三个互补指标：损失离散度、梯度-范数离散度以及通过计算跨工作节点余弦相似度衡量的梯度方向一致性。这些指标几乎不会带来额外开销，并且不需要对模型架构、同步机制或优化算法进行任何修改。

Result: 通过对1B参数的openPangu-Embedded-1B-V1.1模型在tatsu-lab/alpaca数据集上使用8-NPU DP设置进行全面微调，并在控制交叉排名随机性的条件下进行了验证。实验结果表明，随着数据洗牌和随机种子逐步不同步化，损失/梯度分散度显著增加，方向一致性降低，尽管全局平均损失曲线看起来平滑。

Conclusion: 所提出的指示器为大规模DP微调中的隐藏不稳定模式提供了可操作的可见性，使得更可靠的问题诊断和配置评估成为可能。

Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \texttt{openPangu-Embedded-1B-V1.1} model on the \texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.

</details>


### [106] [LACONIC: Length-Aware Constrained Reinforcement Learning for LLM](https://arxiv.org/abs/2602.14468)
*Chang Liu,Yiran Zhao,Lawrence Liu,Yaoqi Ye,Csaba Szepesvári,Lin F. Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为LACONIC的强化学习方法，该方法在训练过程中执行目标令牌预算，以解决大型语言模型通过奖励驱动训练时产生的过长响应问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然提升了大型语言模型的能力，但可能导致生成的响应过长，从而增加推理延迟和计算开销。现有长度控制方法通常依赖于固定的启发式奖励调整，这可能与任务目标不一致且需要精细调优。

Method: LACONIC通过结合任务奖励和基于长度的成本来更新策略模型，并在整个训练过程中自适应地调整成本规模，以平衡简洁性和任务表现。

Result: LACONIC在保持或提高pass@1的同时将输出长度减少了超过50%，并且在通用知识和多语言基准测试中，使用44%更少的令牌维持了域外性能。此外，它能够无缝集成到标准RL调优流程中，无需更改推理过程且部署开销极小。

Conclusion: LACONIC提供了一种有效的方法来控制大型语言模型生成内容的长度，同时保证了良好的任务完成度。

Abstract: Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.

</details>


### [107] [One Good Source is All You Need: Near-Optimal Regret for Bandits under Heterogeneous Noise](https://arxiv.org/abs/2602.14474)
*Aadirupa Saha,Amith Bhat,Haipeng Luo*

Main category: cs.LG

TL;DR: 本文提出了一种名为SOAR的新算法，用于解决具有M个异构数据源的K臂多臂老虎机问题。该算法能够快速剔除高方差的数据源，并通过平衡的min-max LCB-UCB方法同时识别最优臂和最优（最小方差）数据源。理论分析表明，即使在不知道哪个数据源方差最小的情况下，SOAR也能达到与单个最佳方差源的标准多臂老虎机问题相匹配的最佳实例依赖遗憾界，仅需额外承担一小部分成本以识别最优数据源。实验结果证明了SOAR相比基线方法在合成问题实例及真实世界MovieLens 25M数据集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究目标是在具有多个未知且不同噪声方差的数据源条件下，如何自适应地选择查询哪一个数据源，同时最小化多臂老虎机问题中的遗憾。此设置增加了标准多臂老虎机问题的复杂性，因为需要考虑数据源的质量对学习过程的影响。

Method: 提出了Source-Optimistic Adaptive Regret minimization (SOAR)算法，该算法首先利用尖锐的方差集中界快速排除高方差来源，然后采用一种‘平衡min-max LCB-UCB方法’来同时处理寻找最佳臂和最优（最低方差）数据源的任务。

Result: 理论分析显示，SOAR达到了一个实例相关的遗憾上界，它几乎等同于使用单一最佳方差源时的标准多臂老虎机问题所能达到的最佳界限，但仅需增加少量不可避免的成本用来识别最优数据源。实验方面，在多个合成问题实例以及实际的MovieLens 25M数据集上验证了SOAR相较于基准方法展现出更优的表现。

Conclusion: 通过提出并分析SOAR算法，本研究为解决含有多变数据源质量的多臂老虎机问题提供了一个有效的方法。所提出的算法不仅理论上优于一些已有的基准策略，而且在实际应用中也展示了其优越性。

Abstract: We study $K$-armed Multiarmed Bandit (MAB) problem with $M$ heterogeneous data sources, each exhibiting unknown and distinct noise variances $\{σ_j^2\}_{j=1}^M$. The learner's objective is standard MAB regret minimization, with the additional complexity of adaptively selecting which data source to query from at each round. We propose Source-Optimistic Adaptive Regret minimization (SOAR), a novel algorithm that quickly prunes high-variance sources using sharp variance-concentration bounds, followed by a `balanced min-max LCB-UCB approach' that seamlessly integrates the parallel tasks of identifying the best arm and the optimal (minimum-variance) data source. Our analysis shows SOAR achieves an instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, up to preprocessing costs depending only on problem parameters, where ${σ^*}^2 := \min_j σ_j^2$ is the minimum source variance and $Δ_i$ denotes the suboptimality gap of the $i$-th arm. This result is both surprising as despite lacking prior knowledge of the minimum-variance source among $M$ alternatives, SOAR attains the optimal instance-dependent regret of standard single-source MAB with variance ${σ^*}^2$, while incurring only an small (and unavoidable) additive cost of $\tilde O(\sqrt{K \sum_{j=1}^M σ_j^2})$ towards the optimal (minimum variance) source identification. Our theoretical bounds represent a significant improvement over some proposed baselines, e.g. Uniform UCB or Explore-then-Commit UCB, which could potentially suffer regret scaling with $σ_{\max}^2$ in place of ${σ^*}^2$-a gap that can be arbitrarily large when $σ_{\max} \gg σ^*$. Experiments on multiple synthetic problem instances and the real-world MovieLens\;25M dataset, demonstrating the superior performance of SOAR over the baselines.

</details>


### [108] [Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts](https://arxiv.org/abs/2602.14490)
*Buze Zhang,Jinkai Tao,Zilang Zeng,Neil He,Ali Maatouk,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 本文提出了一种名为Mixture of Space (MoS)的统一框架，该框架利用多种几何空间同时学习更丰富、具有曲率感知能力的表示。基于此方案，我们开发了MoSLoRA，它通过异构几何专家扩展了低秩适应（LoRA），使模型能够根据输入上下文动态选择或结合适当的几何空间。此外，还设计了一个轻量级路由机制以减少频繁切换流形带来的计算开销，并提供了关于曲率优化如何影响训练稳定性和模型性能的经验见解。实验表明，MoSLoRA在多个基准测试中始终优于强大的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调(PEFT)方法主要在欧几里得空间内操作，这限制了它们捕捉语言数据中复杂几何结构的能力。尽管其他类型的几何空间如双曲几何和球面流形为处理特定类型的数据提供了理论上的优势，但将表示强制归入单一类型的空间最终会限制表达力。因此，需要一种能够同时利用多种几何空间的方法来增强模型的表现力。

Method: 提出了Mixture of Space (MoS)，一个可以同时利用多种几何空间来学习更丰富的、具有曲率感知能力表示的统一框架。基于MoS，开发了MoSLoRA技术，它通过引入异构几何专家扩展了传统的Low-Rank Adaptation (LoRA)方法，允许模型依据输入内容动态地选取或组合合适的几何空间。此外，为了缓解由于频繁切换不同几何空间所带来的额外计算负担，设计并实现了一种轻量级路由机制。

Result: 通过跨多个基准测试的实验证明，MoSLoRA相较于强基线方法展现出显著优越性，在MATH500上最高提升了5.6%，而在MAWPS上则达到了15.9%的提升。此外，研究还揭示了曲率优化对于训练过程稳定性及整体模型性能的影响。

Conclusion: Mixture of Space (MoS)及其应用MoSLoRA提供了一种有效的方法来克服现有PEFT技术仅依赖于单一几何空间进行下游任务适应时所面临的局限性。通过灵活地整合不同几何空间的优势，MoSLoRA不仅增强了模型对复杂数据结构的理解能力，而且在实际应用场景中也表现出色。

Abstract: Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.

</details>


### [109] [Divine Benevolence is an $x^2$: GLUs scale asymptotically faster than MLPs](https://arxiv.org/abs/2602.14495)
*Alejandro Francisco Queiruga*

Main category: cs.LG

TL;DR: 本文通过数值分析方法揭示了GLU变体在前沿大语言模型中占主导地位的原因，即它们具有x^2的特性，这使得其在函数重构问题上的缩放斜率L(P)为P^-3，优于MLP的P^-2。基于这一发现，作者提出了一种新的“门控二次单元”，其L(P)斜率比GLU和MLP更陡峭，从而为大型模型的设计提供了从基本原理出发的新方向。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解释为什么GLU变体及其外积架构会在最新的大语言模型及排名模型中变得如此流行，并试图超越纯经验性观察，通过数值分析理论来理解这些模型设计背后的深层次原因。

Method: 研究者们运用了数值分析的方法论，特别是针对函数逼近理论，来探索不同模型架构（如GLU与MLP）对于特定任务（例如1D函数近似）的表现差异。他们还提出了一个新的架构——门控二次单元，并通过参数构建和实证检验比较了不同类型模型在处理函数重建问题时的L(P)缩放斜率。

Result: 结果显示，在函数重构问题上，GLU相较于传统的MLP能够实现更快的渐近缩放速度（L(P)∝P^(-3)对比于MLP的P^(-2)）。此外，新提出的Gated Quadratic Unit展现出了比GLU更为优越的性能指标。

Conclusion: 结论指出，基于数值分析理论，可以为未来的大规模模型设计提供指导，以达到更好的扩展性能。这不仅解释了现有模型选择背后的原因，同时也为开发更加高效的新架构开辟了道路。

Abstract: Scaling laws can be understood from ground-up numerical analysis, where traditional function approximation theory can explain shifts in model architecture choices. GLU variants now dominate frontier LLMs and similar outer-product architectures are prevalent in ranking models. The success of these architectures has mostly been left as an empirical discovery. In this paper, we apply the tools of numerical analysis to expose a key factor: these models have an $x^2$ which enables \emph{asymptotically} faster scaling than MLPs. GLUs have piecewise quadratic functional forms that are sufficient to exhibit quadratic order of approximation. Our key contribution is to demonstrate that the $L(P)$ scaling slope is $L(P)\propto P^{-3}$ for GLUs but only $L(P)=P^{-2}$ for MLPs on function reconstruction problems. We provide a parameter construction and empirical verification of these slopes for 1D function approximation. From the first principles we discover, we make one stride and propose the ``Gated Quadratic Unit'' which has an even steeper $L(P)$ slope than the GLU and MLP. This opens the possibility of architecture design from first principles numerical theory to unlock superior scaling in large models. Replication code is available at https://github.com/afqueiruga/divine_scaling.

</details>


### [110] [Covariance-Aware Transformers for Quadratic Programming and Decision Making](https://arxiv.org/abs/2602.14506)
*Kutay Tire,Yufan Zhang,Ege Onur Taga,Samet Oymak*

Main category: cs.LG

TL;DR: 本文探讨了使用变换器解决二次规划问题的方法，并展示了这种方法如何通过利用协方差矩阵来改进决策问题。研究发现，线性注意力机制可以解决无约束的二次规划问题，而结合多层感知器（MLP）的变换器块能够处理带有ℓ1惩罚或约束的二次规划问题。基于这些理论，作者提出了一种名为Time2Decide的新方法，该方法通过明确地将变量间的协方差矩阵输入到时间序列基础模型中，从而增强其性能。实验结果表明，对于经典的投资组合优化问题，Time2Decide在适用场景下不仅优于基础的时间序列模型，也超越了传统的“先预测后优化”流程。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索变换器（transformers）解决二次规划问题的能力以及这种能力如何有助于涉及协方差矩阵的决策问题。

Method: 首先证明了线性注意机制可以通过对矩阵变量按行进行分词并模仿梯度下降迭代来解决无约束的二次规划(QP)问题。此外，通过集成多层感知器(MLP)，一个变换器块可以解决(i) ℓ1-惩罚的QP问题，通过模仿迭代软阈值化；(ii)当配备额外反馈回路时，还能解决ℓ1-约束的QP问题。基于此，提出了Time2Decdie方法，它通过显式地向时间序列基础模型提供变量之间的协方差矩阵来增强模型。

Result: 实验结果显示，在经典的具有ℓ1约束QP形式的投资组合优化问题上，Time2Decide普遍优于基础的时间序列模型。更重要的是，在适当的情况下，Time2Decide的表现甚至超过了传统的'先预测后优化(PtO)'过程。

Conclusion: 结论是，变换器通过显式利用二阶统计信息能够在一次前向传递中有效解决复杂的决策问题如投资组合构建等。

Abstract: We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\frac{1}{2}x^\top Ax+b^\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical "Predict-then-Optimize (PtO)" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.

</details>


### [111] [Truly Adapting to Adversarial Constraints in Constrained MABs](https://arxiv.org/abs/2602.14543)
*Francesco Emanuele Stradi,Kalana Kalupahana,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 本文研究了在非平稳环境下，多臂老虎机问题的约束变种，提出了一种新的算法，在随机约束和任意变化损失的情况下，达到了最优的遗憾率和正向约束违反率。


<details>
  <summary>Details</summary>
Motivation: 研究者们希望解决在非平稳环境下的多臂老虎机问题中，同时最小化总损失和控制多个未知约束违反的问题。

Method: 提出了针对全反馈情况下的算法，并证明其可以达到$\widetilde{\mathcal{O}}(\sqrt{T}+C)$的遗憾率和同样水平的正向约束违反；然后展示了如何在只有损失的强盗反馈可用时扩展这些保证；最后，当约束也仅有强盗反馈可用时，设计了一个能达到$\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$遗憾率和$\widetilde{\mathcal{O}}(\sqrt{T}+C)$正向违反率的算法。

Result: 所提出的算法在随机约束而损失可能任意变化的情形下，能够实现最优级别的遗憾率与正向约束违反率，且在约束逐渐变得更具对抗性时，提供平滑退化的保证。

Conclusion: 本研究为处理既包含随机因素又包含对抗性质的复杂环境中的多臂老虎机问题提供了有效的解决方案，特别是对于那些需要考虑额外约束条件的情况。

Abstract: We study the constrained variant of the \emph{multi-armed bandit} (MAB) problem, in which the learner aims not only at minimizing the total loss incurred during the learning dynamic, but also at controlling the violation of multiple \emph{unknown} constraints, under both \emph{full} and \emph{bandit feedback}. We consider a non-stationary environment that subsumes both stochastic and adversarial models and where, at each round, both losses and constraints are drawn from distributions that may change arbitrarily over time. In such a setting, it is provably not possible to guarantee both sublinear regret and sublinear violation. Accordingly, prior work has mainly focused either on settings with stochastic constraints or on relaxing the benchmark with fully adversarial constraints (\emph{e.g.}, via competitive ratios with respect to the optimum). We provide the first algorithms that achieve optimal rates of regret and \emph{positive} constraint violation when the constraints are stochastic while the losses may vary arbitrarily, and that simultaneously yield guarantees that degrade smoothly with the degree of adversariality of the constraints. Specifically, under \emph{full feedback} we propose an algorithm attaining $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ regret and $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation, where $C$ quantifies the amount of non-stationarity in the constraints. We then show how to extend these guarantees when only bandit feedback is available for the losses. Finally, when \emph{bandit feedback} is available for the constraints, we design an algorithm achieving $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation and $\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$ regret.

</details>


### [112] [Governing AI Forgetting: Auditing for Machine Unlearning Compliance](https://arxiv.org/abs/2602.14553)
*Qinqi Lin,Ningning Ding,Lingjie Duan,Jianwei Huang*

Main category: cs.LG

TL;DR: 本文提出了一种新的经济框架来审计机器遗忘(MU)的合规性，通过将认证遗忘理论与监管执行相结合，解决了MU技术可行性和法规实施之间的根本差距。文章采用假设检验的方法来解释认证遗忘，以确定审计员的检测能力，并提出了一个博弈论模型来捕捉审计员和运营商之间的策略互动。研究揭示了随着删除请求的增加，审计员可以最优地减少检查强度，因为运营商较弱的数据遗忘使得非合规行为更容易被发现。此外，还证明了虽然未公开审计为审计员提供了信息优势，但它反而降低了相对于公开审计的监管成本效益。


<details>
  <summary>Details</summary>
Motivation: 尽管有法律规定了‘被遗忘权’，但AI操作者经常未能遵守数据删除请求。而机器遗忘（MU）提供了一种技术解决方案，能够从训练好的模型中移除个人数据的影响。然而，在MU的技术可行性和法规实施之间存在着根本性的差距，这给确保合规带来了挑战。

Method: 首先利用一种假设检验的方式解读认证遗忘，从而界定出审计人员的检测能力；接着提出一个博弈论模型来描述审计方与运营方之间的策略互动。面对MU特有的非线性问题，作者通过将其转化为易于处理的一元辅助问题，成功地解耦系统并建立了均衡的存在性、唯一性和结构性质，而无需依赖显式解。

Result: 分析表明，随着删除请求的增多，审计员实际上可以最优化地降低检查频率，因为运营商较弱的遗忘过程使得违规行为更加容易被察觉。此外，研究还指出，虽然保密审计为审计方提供了信息上的优势，但从监管成本效益角度来看，它却意外地不如公开审计有效。

Conclusion: 该论文首次提出了一个用于审核机器遗忘合规性的经济框架，通过整合认证遗忘理论与监管执行，解决了MU技术实现与法规执行间存在的根本差异。研究不仅揭示了随删除请求增加时审计策略的反直觉变化，也探讨了不同审计方式对监管效果的影响。

Abstract: Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.

</details>


### [113] [Replicable Constrained Bandits](https://arxiv.org/abs/2602.14580)
*Matteo Bollini,Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.LG

TL;DR: 本文研究了在有约束的多臂老虎机问题中实现算法可复制性的可能性，并设计了既能保证收益最大化又能满足多种约束条件的可复制算法。此外，还首次为无约束的多臂老虎机开发了一个基于乐观面对不确定原则的可复制UCB类算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器学习领域实验可重复性的需求，特别是在有约束条件下多臂老虎机（MAB）问题中的算法可复制性。

Method: 通过设计特定的可复制在线学习算法，在保持高概率下对同一环境做出相同决策的同时，确保这些算法能够在受限MAB问题中达到与非可复制算法相匹配的遗憾和约束违反度。另外，开发了一种新的适用于无约束MAB问题的可复制UCB类算法。

Result: 证明了在受限MAB环境中实现算法可复制性是可行的；所设计的可复制算法其表现（就遗憾和约束违反而言）与非可复制算法相当；首次提出了一个基于乐观面对不确定性原则的可复制UCB类算法。

Conclusion: 本研究表明，即使是在有额外约束的情况下，也能够设计出既可复制又性能良好的在线学习算法，这为未来的研究开辟了新的方向。

Abstract: Algorithmic \emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.

</details>


### [114] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: 提出了一种新的解耦连续时间的actor-critic算法，通过交替更新解决了现有方法中V函数和q优势率函数难以训练的问题。该方法在理论上证明了收敛性，并在实际应用中超过了先前连续时间和领先离散时间基线的表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多控制问题都是在连续时间内以非均匀、事件驱动的方式决策的。标准的基于固定步长贝尔曼更新的离强化学习在这种情况下表现不佳：当时间间隔缩小，Q函数会退化为价值函数V，导致无法区分动作的好坏。

Method: 提出了一种新的解耦连续时间的actor-critic算法，其中q从V的价值扩散生成器中学习，而V则通过一个即使在极小时间步长下也保持信息量的哈密顿值流来更新。这种方法避免了将V和q纠缠在一个复杂的大优化问题中，从而简化了训练过程。

Result: 新方法在理论分析上通过新颖的概率论点证明了收敛性，克服了基于生成器的哈密顿缺乏超范数下的贝尔曼式收缩带来的挑战。实验结果表明，该方法在具有挑战性的连续控制基准测试以及真实世界的交易任务上均优于先前的连续时间及领先的离散时间基线方法，实现了单季度21%的收益——几乎是次优方法的两倍。

Conclusion: 本文提出的解耦连续时间actor-critic算法不仅解决了连续时间环境下传统强化学习方法遇到的问题，而且在理论与实践两方面都展示了其优越性。

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


### [115] [OPBench: A Graph Benchmark to Combat the Opioid Crisis](https://arxiv.org/abs/2602.14602)
*Tianyi Ma,Yiyang Li,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 介绍了OPBench，这是首个针对阿片类危机的全面基准测试，包括五个数据集跨越三个关键应用领域，并且提供了一个统一的评估框架来系统地比较图学习方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决缺乏一个全面基准来系统性评估真实世界阿片类危机场景下图学习方法的问题，旨在通过引入OPBench来填补这一空白。

Method: 创建了OPBench，它包含五种数据集，涵盖了从医疗索赔中检测过量用药、从数字平台检测非法药物贩运以及从饮食模式预测药物滥用等关键应用场景。同时，与领域专家和权威机构合作整理并注释数据集，建立了一个包含标准化协议、预定义数据分割和可重复基线的统一评估框架。

Result: 通过广泛的实验分析了现有图学习方法的优势与局限性，为未来对抗阿片类危机的研究提供了有价值的见解。

Conclusion: OPBench作为首个专注于阿片类危机的综合性基准测试，不仅促进了不同图学习方法之间的公平比较，还为该领域的进一步研究指明了方向。

Abstract: The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.

</details>


### [116] [Concepts' Information Bottleneck Models](https://arxiv.org/abs/2602.14626)
*Karim Galliamov,Syed M Ahsan Kazmi,Adil Khan,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 本文提出了一种显式的Information Bottleneck正则化方法，用于概念瓶颈模型（CBMs）的概念层，以减少输入与概念之间的信息量I(X;C)，同时保持任务相关信息I(C;Y)。该方法在不改变模型架构或增加额外监督的情况下，提高了预测性能和概念层面干预的可靠性。


<details>
  <summary>Details</summary>
Motivation: 概念瓶颈模型（CBMs）旨在通过人类可理解的概念层提供可解释的预测，但往往面临准确性降低以及概念泄露的问题，这损害了模型的忠实性。为了克服这些问题，并提高模型的预测性能及概念级干预的可靠性，提出了新的解决方案。

Method: 引入一种作用于概念层的明确信息瓶颈正则化器，它惩罚I(X;C)的同时保留I(C;Y)中的任务相关资讯，促进最小充分的概念表示。还开发了两种实用变体（一个变分目标和基于熵的替代方案），并将它们无缝集成到标准CBM训练过程中，无需更改架构或添加更多监管。

Result: 经过六个CBM家族和三个基准测试评估后，IB正则化模型始终优于其原始版本。信息平面分析进一步证实了所期望的行为。这些结果表明，强制执行最小充分概念瓶颈不仅提升了预测表现，也增强了概念层面干预的可靠性。

Conclusion: 提出的正则化方法为创建更忠实且易于干预的CBMs提供了一条理论基础、架构无关的道路，解决了之前因训练协议不同而产生的评估不一致问题，并展示了跨模型家族和数据集的一致性改进。

Abstract: Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.

</details>


### [117] [An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale](https://arxiv.org/abs/2602.14656)
*Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文提出了一种名为POGO的新算法，该算法在保持正交性约束的同时提高了计算效率，尤其适用于大规模机器学习问题。与现有优化器相比，POGO不仅速度快、GPU友好，并且能够处理数千个正交矩阵的优化问题，极大缩短了计算时间。


<details>
  <summary>Details</summary>
Motivation: 当前处理正交性约束的优化器要么计算成本高，不适用于有数百或数千个约束的问题；要么像Landing算法那样暂时放松正交性要求。为了解决这些问题并充分利用正交性约束的优势，特别是在大规模机器学习场景中，研究者们开发了改进版的方法。

Method: 通过改进Landing算法背后的思想，研究人员提出了POGO算法。该算法允许使用现代自适应优化器同时确保有效满足正交性约束。POGO仅由5个矩阵乘法组成，快速且对GPU友好，在实践中始终维持着正交性。

Result: 实验结果表明，在多个具有挑战性的基准测试上，POGO显著优于最近提出的其他优化器。它能够在几分钟内完成对包含数千个正交矩阵的问题进行优化，而其他方法则需要数小时。

Conclusion: POGO算法代表了利用正交性约束进行大规模机器学习的一个重要里程碑。其高效性和易用性（包括公开提供的PyTorch实现）使得更多复杂模型和应用成为可能。

Abstract: Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.

</details>


### [118] [Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error](https://arxiv.org/abs/2602.14682)
*Farzan Farnia,Mohammad Jalali,Azim Ospanov*

Main category: cs.LG

TL;DR: 本研究通过使用Vendi和RKE两种基于熵的多样性评分方法，比较了先进生成模型产生的样本与目标数据分布中抽取的测试样本之间的多样性，发现生成样本的多样性显著低于测试数据，表明现代生成模型存在系统性的多样性偏差。进一步分析指出这种偏差源于有限样本估计的固有缺陷，并提出基于Vendi和RKE的策略来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 虽然深度生成模型在生成高质量样本方面取得了巨大成功，但关于这些模型是否能够准确反映底层数据分布的多样性的问题尚未得到充分研究。

Method: 利用最近提出的无需参考的基于熵的多样性评分指标（Vendi和RKE），直接对比最先进的生成模型所生成样本与从目标数据分布中抽取的测试样本之间的多样性差异。

Result: 研究发现在多个基准数据集上，测试数据的Vendi和RKE多样性得分始终明显高于生成样本，揭示了现代生成模型中存在的系统性向下多样性偏差；进一步分析显示，基于熵的多样性分数期望值随样本大小增加而增大，暗示着从有限训练集中估算的多样性可能低估了真实分布的多样性。

Conclusion: 为解决生成模型中的多样性偏差问题，文章提出了基于Vendi和RKE的多样性意识正则化和引导策略作为潜在解决方案，并提供了实验证据支持其有效性。

Abstract: Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.

</details>


### [119] [SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data](https://arxiv.org/abs/2602.14687)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 本文介绍了一种新的基准工具SynthSAEBench，用于生成具有现实特征特性的大规模合成数据，并提供了一个标准化的基准模型SynthSAEBench-16k，以促进稀疏自动编码器架构的直接比较。该工具再现了先前观察到的现象，并识别出一种新型失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏自动编码器（SAE）在大型语言模型（LLM）上的基准测试往往过于嘈杂，无法区分架构改进，而当前的合成数据实验规模太小且不切实际，难以提供有意义的比较。因此需要一个能够精确验证架构创新的基准。

Method: 开发了名为SynthSAEBench的工具包，可以生成具有相关性、层次结构和叠加等现实特征的大规模合成数据，并提出了一个标准化的基准模型SynthSAEBench-16k。

Result: 通过使用SynthSAEBench，研究者们能够复现之前在LLM SAE上观察到的一些现象，包括重建与潜在质量指标之间的脱节、较差的SAE探测结果以及由L0调节的精度-召回率权衡。此外，还发现了一种新的失效模式：匹配追踪SAE利用叠加噪声来改善重建效果，而不需要学习真实特征。

Conclusion: SynthSAEBench通过提供真实特征和受控消融补充了LLM基准，使研究人员能够在扩展至LLM之前精确诊断SAE的故障模式并验证架构改进。

Abstract: Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

</details>


### [120] [A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)](https://arxiv.org/abs/2602.14696)
*Nihal V. Nayak,Paula Rodriguez-Diaz,Neha Hulkund,Sara Beery,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 本文通过解构和系统分析数据表示和选择算法这两种核心要素，为大型语言模型的指令微调中针对特定任务的指令选择提供了清晰度。研究发现基于梯度的数据表示在低预算下与贪心轮转选择算法结合时表现最佳，并且统一了几种现有的选择算法作为选定子集与查询集之间的近似距离最小化形式。


<details>
  <summary>Details</summary>
Motivation: 当前关于针对性指令选择的文献分散且不透明，缺乏一致的方法来指导实际操作者如何为其目标任务选择指令。

Method: 作者提出了一种框架来解耦并系统地分析两个核心组成部分：数据表示和选择算法，使得能够在不同模型、任务和预算条件下进行控制比较。

Result: 研究表明只有基于梯度的数据表示能够选择出与查询相似度始终预测性能良好的子集；当预算较低时，基于梯度的表现加上贪心轮转选择算法平均表现最好，但随着预算增加这种优势减少。

Conclusion: 这项工作为LLM微调中的更原则性数据选择奠定了基础，并提供了一些关键见解。

Abstract: Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.

</details>


### [121] [D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation](https://arxiv.org/abs/2602.14728)
*Nozomu Fujisawa,Masaaki Kondo*

Main category: cs.LG

TL;DR: 该论文提出了D2-LoRA方法，通过使用较少的训练样本和两个epoch，在保持推理时数值等价性的同时，实现了跨八个问答和阅读理解基准测试的76.4%平均准确率。与现有方法相比，D2-LoRA在参数效率、准确性及生成任务上均表现出色，并且提供了一种几何分析来解释其设计如何稳定训练过程。


<details>
  <summary>Details</summary>
Motivation: 研究者们旨在探索在实际数据和计算资源限制下，高效的参数微调设计方案。目的是开发一种新的方法，能够在有限的训练样本条件下提高模型性能，同时保持低延迟和数值上的稳定性。

Method: D2-LoRA结合了带符号的低秩残差更新（包括加法和减法部分）以及训练时的列向投影以保持每列接近原始范数。训练后，适配器合并成单个权重矩阵，从而实现零推理延迟。此外，还提供了几何分析来说明投影如何稳定训练过程，并通过消融研究分离每个设计组件的贡献。

Result: D2-LoRA在八个问答和阅读理解基准上达到了76.4%的平均准确率；相比于LoRA，在相同参数量的情况下提高了1.6个百分点的准确率；对于生成任务也有所改进（ROUGE-L增加了1.2，胜率提高了1.1%）。此外，该方法显示出36%更低的训练波动性和约1.91倍的评估吞吐量恢复。

Conclusion: D2-LoRA作为一种有效的参数微调方法，在减少训练样本需求的同时，能够显著提升模型在多种自然语言处理任务上的表现。它不仅优于或匹配了现有技术如LoRA和DoRA的表现，而且在训练稳定性和推理速度方面也具有优势。

Abstract: We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.

</details>


### [122] [Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training](https://arxiv.org/abs/2602.14759)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.LG

TL;DR: 本文提出了一种在预训练语言模型中通过重复应用选定的块范围来进行推理时内部循环的方法，以延长细化过程。实验表明，这种方法可以带来适度但一致的准确性改进，并且使得潜在轨迹更加稳定、语义细化持续进行。


<details>
  <summary>Details</summary>
Motivation: 作者受到先前研究的启发，认为Transformer中的某些层起到了细化作用，并且内部空间在各层之间是共享的。基于此观点，他们提出了一种新的方法——推理时内部循环，来探索是否可以通过简单地在测试阶段循环计算来获得额外的细化效果。

Method: 该文提出的方法是在已有的预训练语言模型上实施推理时内部循环，即选择性地重复应用模型中的某个块范围，以进一步细化模型的表示。

Result: 实验结果表明，在多个基准测试中，所提出的方法能够带来适度而稳定的准确率提升；同时，对由此产生的潜在轨迹分析显示了更稳定的状态演变和持续的语义细化。

Conclusion: 这项研究表明，通过对冻结的预训练模型在测试时执行简单的循环计算，确实可以获得额外的细化效果，从而提高模型性能。

Abstract: Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.

</details>


### [123] [Universal Algorithm-Implicit Learning](https://arxiv.org/abs/2602.14761)
*Stefano Woerner,Seong Joon Oh,Christian F. Baumgartner*

Main category: cs.LG

TL;DR: 本文提出了一种新的元学习理论框架，定义了实用的普遍性和算法显式与算法隐式学习之间的区别，并基于此框架开发了TAIL，这是一种基于Transformer的元学习器，能够在不同领域、模态和标签配置的任务中工作。TAIL在标准的小样本基准测试中达到了最先进的性能，同时还能泛化到未见过的领域和模态上，处理训练过程中未曾遇到过的更多类别的任务，并且相比之前的Transformer方法大大节省了计算资源。


<details>
  <summary>Details</summary>
Motivation: 当前的元学习方法局限于具有固定特征和标签空间的狭窄任务分布，限制了它们的应用范围。此外，现有文献中关于'通用'和'多用途'等关键词使用不一致，缺乏精确定义，影响了研究间的可比性。

Method: 提出了一个元学习理论框架，明确界定了实用普遍性的概念，并引入了算法显式与算法隐式学习的区别；基于该框架设计了TAIL，一种利用随机投影进行跨模态特征编码、通过随机注入标签嵌入来扩展至更大标签空间以及采用高效内联查询处理技术的基于Transformer的元学习器。

Result: TAIL在标准小样本基准测试中取得了最先进表现，同时能够适应未见领域和模态的任务，解决仅用图像训练后出现的文本分类问题，处理比训练时所见类别数多达20倍的任务，并相较于先前的Transformer方法实现了数量级上的计算成本节约。

Conclusion: 通过引入一个新的元学习框架及其实现——TAIL，证明了可以创建出更加通用且高效的元学习模型，这些模型不仅限于特定类型的任务或数据集，而且能在更广泛的场景下展现出色的表现。

Abstract: Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like "universal" and "general-purpose" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.

</details>


### [124] [On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials](https://arxiv.org/abs/2602.14789)
*Rotem Mulayoff,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 本文研究了非线性项对梯度下降法（GD）和随机梯度下降法（SGD）在最小值附近稳定振荡的影响，提出了一个精确的多变量设置下稳定振荡的判据，并证明了如果所有批次都是线性稳定的，则SGD的非线性动力学在期望上也是稳定的。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的工作通常依赖于线性化来确定稳定性，但尚不清楚线性化的动态是否能忠实反映全部非线性行为。最近的研究表明，即使步长衰减后仍会收敛，梯度下降法（GD）也可能在线性不稳定的最小值附近稳定振荡，这表明线性分析可能具有误导性。

Method: 通过对非线性项效应的具体研究，推导出GD在多变量设置下接近最小值时稳定振荡的确切标准。将分析扩展至SGD，展示即使单个批次不稳定，非线性动力学也可能会在预期中发散。

Result: 得出了一个取决于高阶导数、泛化现有结果的稳定振荡条件。发现稳定性可以由单个不稳定振荡的批次决定，而不是如线性分析所建议的平均效果。最后，证明了如果所有批次都是线性稳定的，则SGD的非线性动力学在期望上是稳定的。

Conclusion: 非线性项对于理解优化算法在训练过程中的动态稳定性至关重要。与仅基于线性分析的结论相比，考虑非线性效应能够提供关于GD和SGD行为更加准确的见解。

Abstract: The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.

</details>


### [125] [Extending Multi-Source Bayesian Optimization With Causality Principles](https://arxiv.org/abs/2602.14791)
*Luuk Jacobs,Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: 本文提出了一种多源因果贝叶斯优化（MSCBO）算法，该算法结合了多源贝叶斯优化（MSBO）和因果贝叶斯优化（CBO）的优点，旨在提高高维问题中的优化效率并减少计算复杂性。通过理论基础与实证分析，展示了MSCBO相较于其基础方法在合成数据集和真实世界数据集上的优越性能，特别是在噪声水平不同的情况下。研究结果表明，将MSBO与CBO的因果原则相结合有助于降低维度、减少运营成本，并最终提升收敛速度、性能及可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统多源贝叶斯优化（MSBO）假设目标函数的输入变量是独立同分布的，这限制了它在存在因果信息且可以执行干预措施场景下的有效性，如临床试验或政策制定。为了解决这一局限性，文章提出结合单源领域内已有的因果贝叶斯优化（CBO）方法的优势，来增强多源环境下的优化效果。

Method: 作者提出了一个多源因果贝叶斯优化（MSCBO）框架，该框架综合了多源贝叶斯优化与因果贝叶斯优化两者的优点。论文首先介绍了这两种技术背后的理论基础，然后详细阐述了如何通过它们之间的协同作用开发出MSCBO算法。

Result: 通过对合成数据集以及来自现实世界的多个数据集进行实验对比，证明了所提出的MSCBO算法不仅能够有效处理不同噪声水平的情况，而且在优化效率、收敛速度等方面优于传统的MSBO和CBO方法。

Conclusion: 研究得出结论，将多源贝叶斯优化与因果贝叶斯优化的原则结合起来，可以实现维度降低、操作成本减少的效果，同时提高了算法的收敛速率、整体表现力及可扩展能力。

Abstract: Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.

</details>


### [126] [Learning State-Tracking from Code Using Linear RNNs](https://arxiv.org/abs/2602.14814)
*Julien Siems,Riccardo Grazzi,Kirill Kalinin,Hitesh Ballani,Babak Rahmani*

Main category: cs.LG

TL;DR: 研究将置换组合转换为代码，通过REPL跟踪交错状态揭示和变量变换，发现线性RNN在该设置下表现优秀而Transformers失败。进一步探讨了在代码中跟踪状态的困难性，并证明在线性有限状态自动机的状态跟踪中，非线性RNN可能优于线性RNN。


<details>
  <summary>Details</summary>
Motivation: 由于序列模型架构如Transformers和RNNs（线性和非线性）在处理状态跟踪任务时存在局限性，特别是对于置换组合这类序列到序列的任务，与训练语言模型常用的下一个token预测设置不兼容。因此本研究旨在通过将置换组合转化为代码来解决这一问题，从而更好地理解这些模型的能力边界。

Method: 将置换组合转换成代码，利用REPL跟踪结合状态显示和变量变换。对能够进行状态跟踪的线性RNN以及Transformers进行了比较，并且针对在代码中难以跟踪状态的问题，将其定义为追踪一个具有确定性状态揭示的概率有限状态自动机的状态。

Result: 结果显示，在给定的新设置下，线性RNN表现出色，而Transformers则未能成功完成任务。此外，当考虑状态不是完全可观察的情况下，非线性RNN在状态跟踪方面可能比线性RNN更胜一筹。

Conclusion: 通过将置换组合任务转化为代码形式，可以有效区分不同类型的序列模型在此类任务上的性能差异。同时指出，在某些特定条件下（如状态部分可观测），非线性RNN可能更适合于状态跟踪任务。

Abstract: Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.

</details>


### [127] [A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers](https://arxiv.org/abs/2602.14855)
*Ryan DeWolfe,Paweł Prałat,François Théberge*

Main category: cs.LG

TL;DR: 定义了一种实用的相似性度量方法，用于比较含有重叠和异常值的聚类结果，并证明了该方法具有若干理想属性且不受其他常见聚类比较度量偏差的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类算法外在评价方法尚无法有效处理含有异常值或重叠簇的聚类情形。

Method: 提出了一种新的相似性度量标准来对比包含重叠区域和异常点的聚类结果。

Result: 新提出的相似性度量标准展现出了多个理想的特性，并且实验证明它不会受到其他一些常见的聚类比较指标所存在的偏见影响。

Conclusion: 本研究为评估带有重叠和异常值情况下的聚类提供了一个有效的工具。

Abstract: Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.

</details>


### [128] [Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning](https://arxiv.org/abs/2602.14868)
*Ilia Mahrooghi,Aryo Lotfi,Emmanuel Abbe*

Main category: cs.LG

TL;DR: 提出了一种名为Goldilocks的新颖数据采样策略，通过教师模型选择适合学生模型难度的问题来提高强化学习过程中大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习是一种强大的范式，可以解锁大型语言模型中的推理能力。但是依赖稀疏奖励会让这个过程样本效率极低，因为模型必须在几乎没有反馈的情况下探索巨大的搜索空间。传统的课程学习试图通过基于复杂性对数据进行排序来缓解这个问题，但特定模型的最佳排序通常是不清楚的。

Method: 提出了Goldilocks，这是一种新的由教师驱动的数据采样策略，旨在预测每个问题对学生模型的难度。教师模型会选择对学生来说既不太难也不太简单的合适难度问题（即Goldilocks原则），同时使用GRPO训练学生。根据学生在已见样本上的表现，教师不断适应学生不断发展的能力。

Result: 在OpenMathReasoning数据集上，与使用标准GRPO训练的模型相比，在相同的计算预算下，采用Goldilocks数据采样方法能够提高模型的表现。

Conclusion: Goldilocks数据采样策略提供了一种有效的方法来增强基于强化学习的大规模语言模型训练，特别是当面临稀疏奖励挑战时。

Abstract: Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

</details>


### [129] [On the Learning Dynamics of RLVR at the Edge of Competence](https://arxiv.org/abs/2602.14872)
*Yu Huang,Zixin Wen,Yuejie Chi,Yuting Wei,Aarti Singh,Yingbin Liang,Yuxin Chen*

Main category: cs.LG

TL;DR: 本文开发了一种理论，解释了基于最终结果的奖励如何帮助克服长期推理障碍，并发现任务难度谱的平滑性对RLVR的有效性至关重要。当数据中存在难度突变时，学习过程会出现长时间停滞；而难度平滑过渡则会产生接力效应，促进模型能力稳步提升。此外，研究还提出适当设计的数据组合可以带来可扩展的性能增益。


<details>
  <summary>Details</summary>
Motivation: 理解仅基于最终结果的奖励如何帮助强化学习（特别是对于大型推理模型）克服长期推理障碍。

Method: 通过开发一个关于变压器在组合推理任务上进行强化学习训练动态的理论，采用并调整有限群傅里叶分析工具来适应当前的研究环境。

Result: 揭示了任务难度谱的平滑性是影响RLVR有效性的一个重要因素；验证了当难度变化连续时，会形成一种接力效应，使模型能够持续进步；同时也指出，合理设计的数据混合策略有助于实现性能的显著提高。

Conclusion: 适当的难度谱设计和数据混合策略可以有效促进强化学习过程中模型能力的发展，特别是在处理复杂推理任务时。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.

</details>


### [130] [Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems](https://arxiv.org/abs/2602.14901)
*Pramit Saha,Joshua Strong,Mohammad Alsharid,Divyanshu Mishra,J. Alison Noble*

Main category: cs.LG

TL;DR: 本文介绍了一种名为ToolSelect的方法，用于在医疗保健系统中根据具体任务从多个专业模型中选择最合适的模型。通过最小化采样专业工具候选者的群体风险，并使用任务条件选择损失的一致替代物来实现自适应学习。ToolSelect基于注意力神经过程，依据查询和每个模型的行为总结进行选择。为了测试这一方法，作者首次引入了一个包含多种任务专用模型的胸部X光环境（ToolSelectBench），结果显示ToolSelect在四个不同任务家族上均优于10种现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健系统中，针对特定任务往往不存在单一最优模型，而是需要依赖于多个竞争性的专业模型，这些模型在不同的数据样本上表现优异。因此，对于任何给定的查询，系统必须能够从一个异质的专业工具池中可靠地选择出正确的模型。此外，当前缺少一个标准测试平台来评估这种选择机制的有效性。

Method: 提出了ToolSelect，一种能够自适应学习模型选择策略的方法，旨在通过最小化基于采样的专业工具候选者集合上的总体风险来优化选择过程。该方法采用了基于注意力机制的神经过程作为选择器，该选择器根据查询以及各个模型的行为摘要来进行决策。同时，为了解决缺乏标准测试环境的问题，研究人员创建了ToolSelectBench，这是一个配备了多样化任务专用模型集的胸部X射线环境。

Result: 实验结果表明，在四个不同的任务系列（疾病检测、报告生成、视觉定位与VQA）上，ToolSelect相对于其他十种最先进的方法展现出了更优的表现。

Conclusion: ToolSelect提供了一种有效的方法来解决医疗领域内根据具体情况选择合适专业模型的问题，并且在新建立的基准测试环境中证明了其优越性。

Abstract: Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.

</details>


### [131] [Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift](https://arxiv.org/abs/2602.14913)
*Farbod Siahkali,Ashwin Verma,Vijay Gupta*

Main category: cs.LG

TL;DR: 本文分析了伪校准作为工具在有界标签条件协变量偏移模型下对抗性能损失的使用，并提出了一种源调优伪校准算法，该算法根据分类器的不确定性在线性硬伪标签和随机标签之间进行插值。实验表明，提出的边界定性地跟踪伪校准行为，且源调优方案减轻了分布偏移下的覆盖退化，同时保持了非平凡的预测集大小。


<details>
  <summary>Details</summary>
Motivation: 共形预测（CP）在交换性假设下提供无分布边缘覆盖保证，但若数据分布发生偏移，这些保证可能失效。因此，研究者们探索了如何利用伪校准来解决在数据分布发生变化时的性能下降问题。

Method: 本文采用领域适应工具推导出目标覆盖率的下界，该下界取决于分类器的源域损失及偏移的Wasserstein度量。基于此结果，提出了设计伪校准集合的方法，通过增加松弛参数来提高共形阈值，以确保目标覆盖率高于预设水平。此外，还提出了一种依据分类器不确定性在硬伪标签与随机标签间进行插值的源调优伪校准算法。

Result: 数值实验显示，所提出的边界能够定性地追踪伪校准的行为；源调优方案有助于缓解因分布偏移导致的覆盖度降低问题，同时还能维持具有实际意义的预测集规模。

Conclusion: 本研究表明，通过调整伪校准过程中的阈值，可以有效对抗由于数据分布变化引起的共形预测性能下降问题。此外，源调优伪校准算法为处理这种情况提供了一个有效的解决方案。

Abstract: Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.

</details>


### [132] [Variance-Reduced $(\varepsilon,δ)-$Unlearning using Forget Set Gradients](https://arxiv.org/abs/2602.14938)
*Martin Van Waerebeke,Marco Lorenzi,Kevin Scaman,El Mahdi El Mhamdi,Giovanni Neglia*

Main category: cs.LG

TL;DR: 提出了一种新的算法VRU，该算法直接利用遗忘集梯度进行更新，并保证了(ε,δ)-遗忘。实验表明，VRU在收敛速度上优于现有的一阶(ε,δ)-遗忘方法以及明确利用遗忘集的经验基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的针对强凸目标函数的一阶方法能够实现(ε,δ)-遗忘，但仅使用遗忘集来校准注入噪声，从未作为直接优化信号。相比之下，虽然一些有效的经验启发式方法会利用遗忘样本（例如通过梯度上升），但它们缺乏正式的遗忘保证。为了解决这一差距，本文提出了方差减少遗忘(VRU)算法。

Method: VRU算法是一种一阶算法，它首次直接将遗忘集梯度纳入其更新规则中，并且能够满足(ε,δ)-遗忘要求。研究者们还建立了VRU的收敛性，并证明了加入遗忘集后能获得严格改进的速度，即相较于现有的一阶(ε,δ)-遗忘方法而言，错误依赖性更好。此外，在低误差情况下，理论上VRU渐近地优于所有忽略遗忘集的一阶方法。

Result: 理论分析与实验结果均显示，VRU在收敛性能上超越了当前最先进的认证遗忘方法以及那些明确利用遗忘集的经验基准方法。

Conclusion: VRU算法不仅直接利用遗忘集信息以提高模型遗忘效率，同时提供了严格的(ε,δ)-遗忘保证，从而在理论上和实践上都表现出了优越性。

Abstract: In machine unlearning, $(\varepsilon,δ)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\varepsilon,δ)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\varepsilon,δ)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\varepsilon,δ)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.

</details>


### [133] [Locally Adaptive Multi-Objective Learning](https://arxiv.org/abs/2602.14952)
*Jivat Neet Kaur,Isaac Gibbs,Michael I. Jordan*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过将多目标学习方法的一部分替换为自适应在线算法来实现局部适应性，从而在能源预测和算法公平性的数据集上改善了现有方法，并在子组间实现了无偏预测，同时在分布变化下保持了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的解决多目标同时满足的预测器学习问题的方法，在整个时间范围内以最坏情况最小化一组目标，实际上可能无法很好地适应分布的变化。尽管之前的工作试图通过引入额外的目标来针对连续子区间提供局部保证来缓解这一问题，但这些提案的经验评估却很少。

Method: 作者提出了一种替代程序，该程序通过用自适应在线算法替换多目标学习方法中的一个部分来达到局部适应性。

Result: 实证评估表明，所提出的方法在能源预测和算法公平性的数据集上优于现有方法，并且能够在子组中实现无偏预测，同时在分布变化时仍保持鲁棒性。

Conclusion: 本文提出的具有局部适应性的新方法能够更好地应对数据分布随时间任意变化的情况，并在多个实际应用领域中表现出色。

Abstract: We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.

</details>


### [134] [Use What You Know: Causal Foundation Models with Partial Graphs](https://arxiv.org/abs/2602.14972)
*Arik Reuter,Anish Dhir,Cristiana Diaconu,Jake Robertson,Ole Ossen,Frank Hutter,Adrian Weller,Mark van der Wilk,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过在因果基础模型(CFMs)中加入因果信息（如因果图或祖先信息）来提高其预测性能。实验表明，通过在注意力机制中注入可学习的偏差是最有效的方法，可以使通用CFM匹配专门针对特定因果结构训练的模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的因果量估计依赖于根据特定假设定制的估计器。最近提出的因果基础模型(CFMs)旨在通过在一个步骤内完成因果发现和推理来提供一种更统一的方法。然而，当前这些模型不允许整合领域知识，这可能导致次优预测。本研究旨在解决这一问题。

Method: 研究者们引入了将因果信息（例如因果图或祖先信息）条件化到CFMs中的方法，并系统地评估了不同的条件策略，发现向注意力机制中注入可学习偏置是在利用完整及部分因果信息时最有效的方法。

Result: 实验证明，这种条件设置允许通用CFM达到与为特定因果结构训练的专业模型相当的表现。

Conclusion: 该方法解决了迈向一体化因果基础模型道路上的一个核心障碍：能够在数据驱动方式下回答因果查询的同时有效地利用任何程度的领域专长。

Abstract: Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.

</details>


### [135] [MacroGuide: Topological Guidance for Macrocycle Generation](https://arxiv.org/abs/2602.14977)
*Alicja Maksymiuk,Alexandre Duplessis,Michael Bronstein,Alexander Tong,Fernanda Duarte,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 本文提出了一种名为MacroGuide的新方法，利用持久同调性来指导预训练分子生成模型在无条件和有条件（蛋白质口袋）环境下生成大环化合物。实验表明，这种方法可以显著提高大环化合物的生成率，并且在化学有效性、多样性以及PoseBusters检查等关键质量指标上达到了或超过了现有最先进水平。


<details>
  <summary>Details</summary>
Motivation: 大环化合物由于其对难以靶向的目标具有增强的选择性和结合亲和力，因此作为小分子药物的一种有希望的替代品而受到关注。然而，由于公共数据集中此类化合物数量稀少及标准深度生成模型中实施拓扑约束所面临的挑战，它们在生成建模领域仍处于未充分探索状态。

Method: MacroGuide: 利用持久同调性的拓扑引导机制，用于指导预训练分子生成模型朝向大环化合物的生成方向采样。通过在每个去噪步骤中从原子位置构建Vietoris-Rips复形并优化持久同调特征来促进环结构的形成。

Result: 应用MacroGuide到预训练扩散模型后，大环化合物的生成率从1%提升到了99%，同时在化学有效性、多样性和PoseBusters检查等方面达到或超过了最先进的性能表现。

Conclusion: MacroGuide提供了一种有效的方法来克服当前分子生成模型中存在的限制，特别是针对大环化合物的生成问题。它不仅提高了大环化合物的生成效率，而且确保了这些化合物的质量符合重要评估标准。

Abstract: Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.

</details>


### [136] [Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations](https://arxiv.org/abs/2602.14983)
*Carolin Cissee,Raneen Younis,Zahra Ahmadi*

Main category: cs.LG

TL;DR: 本文提出了一种名为COrAL的新框架，该框架能够显式且同时保留多模态表示中的冗余、独特和协同信息。通过采用具有正交约束的双路径架构来解缠共享和模特性征，并引入非对称遮罩以促进模态间依赖性的建模。实验结果表明，COrAL在多种数据集上表现出色，且运行间的性能方差低，证明了明确建模多模态信息全谱可以产生更稳定、可靠和全面的嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督多模态对比学习方法主要捕捉跨模态的冗余信号，而忽略了特定于单个模态或通过模态交互产生的信息。即使最近的方法试图扩展这一视角，但它们要么未能显式地建模协同交互，要么以纠缠的方式学习不同的信息成分，导致表征不完整及潜在的信息泄露。

Method: COrAL利用一种双路径架构结合正交性限制来区分共享特征与模态特有特征，确保信息成分的清晰分离。此外，通过引入带有互补视图模式的非对称遮罩技术，强制模型推断跨模态依赖关系而非仅依赖冗余线索。

Result: 广泛的合成基准测试和多样化的MultiBench数据集上的实验表明，COrAL不仅与最先进方法相比表现一致甚至更优，而且在多次运行中展现出较低的表现方差。

Conclusion: 明确建模包括冗余、独特以及协同在内的全部多模态信息，有助于生成更加稳定、可靠和综合的嵌入向量。

Abstract: Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.

</details>


### [137] [Spectral Convolution on Orbifolds for Geometric Deep Learning](https://arxiv.org/abs/2602.14997)
*Tim Mangliers,Bernhard Mössner,Benjamin Himpel*

Main category: cs.LG

TL;DR: 本文介绍了在轨道几何结构数据上应用谱卷积的概念，为几何深度学习提供了一个新的构建模块，并通过音乐理论的例子进行了说明。


<details>
  <summary>Details</summary>
Motivation: 由于来自应用场景的数据需求，需要识别更多的拓扑和几何结构，以便这些用例可以被机器学习所利用。

Method: 本文引入了在轨道（orbifolds）上进行谱卷积的概念。

Result: 提出了一个基于谱卷积的新的构建块，它使得在具有轨道结构的数据上进行学习成为可能。

Conclusion: 通过将谱卷积扩展到轨道上，研究者们能够为几何深度学习开拓新的领域，特别是对于那些拥有更复杂几何结构的数据集。

Abstract: Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.

</details>


### [138] [Boundary Point Jailbreaking of Black-Box LLMs](https://arxiv.org/abs/2602.15001)
*Xander Davies,Giorgi Giglemiani,Edmund Lau,Eric Winsor,Geoffrey Irving,Yarin Gal*

Main category: cs.LG

TL;DR: 本文介绍了一种新的自动越狱攻击方法BPJ，该方法完全基于黑盒环境，仅依赖于每次查询时分类器是否标记交互的单一信息位。BPJ通过将目标有害字符串转换为一系列中间攻击目标，并主动选择能够最好地检测攻击强度微小变化的评估点（'边界点'），来优化攻击。这是首个成功开发出对宪法分类器具有普遍性的越狱攻击的全自动算法，也是首个在不依赖人工攻击种子的情况下成功对抗GPT-5输入分类器的自动化攻击算法。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的防御机制已经能够抵御数千小时的人类红队测试，但研究者们仍然需要探索新型的、更有效的攻击方式来测试和改进这些防护措施。本文旨在介绍一种名为BPJ的新类别自动化越狱攻击技术，它能够在不依赖白盒或灰盒假设的前提下绕过最强的工业级保护措施。

Method: BPJ采用了一种全新的策略，即将目标有害字符串转化为一系列中间攻击目标，并通过主动选择那些最能体现攻击强度细微变化的‘边界点’来进行优化。这种方法只需要知道分类器是否对某个交互进行了标记这一单比特信息即可工作，从而实现了真正的黑盒攻击。

Result: 实验结果表明，BPJ是第一种能在没有人类提供初始攻击样本的情况下成功生成通用越狱攻击的技术，同时也能有效对抗GPT-5等先进模型中的输入分类器。此外，BPJ在单独交互中难以被防御，但在优化过程中会产生大量标记，提示有效的防御可能需要结合批量级别的监控手段。

Conclusion: 本研究表明，即使是最先进的防御系统也可能存在漏洞，特别是在面对像BPJ这样创新且高度自动化的攻击方法时。为了更好地保护AI系统免受恶意利用，未来的研究应该考虑如何加强针对此类攻击的防御策略，包括但不限于引入更高级别的监测机制。

Abstract: Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as "jailbreaks". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength ("boundary points"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.

</details>


### [139] [PDE foundation models are skillful AI weather emulators for the Martian atmosphere](https://arxiv.org/abs/2602.15004)
*Johannes Schmude,Sujit Roy,Liping Wang,Theodore van Kessel,Levente Klein,Marcus Freitag,Eloisa Bentivegna,Robert Manson-Sawko,Bjorn Lutjens,Manil Maskey,Campbell Watson,Rahul Ramachandran,Juan Bernabe-Moreno*

Main category: cs.LG

TL;DR: 研究展示了通过在大量偏微分方程数值解上预训练的AI基础模型，可以被调整和微调以创建适用于火星大气预测的高效天气模拟器。通过将Poseidon PDE基础模型从二维扩展到三维，并保持预训练信息，同时使用约34GB的四年火星数据进行训练，在有限计算预算下实现了对保留年份预测性能提高34.4%的结果。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用预训练于多种偏微分方程解决方案之上的AI基础模型来解决现实世界中缺乏足够训练数据或合适计算预算的问题，特别是针对火星大气的天气预测问题。

Method: 基于Poseidon PDE基础模型开发了一种方法，能够将该模型从二维扩展至三维，并且在此过程中保留原有的预训练信息。使用了大约34GB、覆盖四个火星年的数据集来进行模型训练，同时分析了模型面对稀疏初始条件时的表现。

Result: 通过结合预训练与模型扩展技术，对于一个预留出来的测试年份的数据，模型的性能提高了34.4%。这表明PDE-FMs不仅能够近似解决其他PDEs，而且还可以为那些具有复杂相互作用但缺乏足够训练数据或适当计算资源的实际问题提供支持。

Conclusion: 研究表明，通过预先训练于多样化的偏微分方程解决方案之上并随后进行适当的调整与优化，AI基础模型可以在解决实际问题（如火星大气的天气预报）方面表现出色，即使是在面临数据稀缺或计算资源有限的情况下也不例外。

Abstract: We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.

</details>


### [140] [Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees](https://arxiv.org/abs/2602.15008)
*Daniil Dmitriev,Zhihan Huang,Yuting Wei*

Main category: cs.LG

TL;DR: 本文研究了基于连续时间马尔可夫链(CTMC)公式下的分数基离散扩散模型的采样效率，特别是对于$\tau$-leaping采样器。在均匀和遮罩噪声过程中，文章建立了达到KL散度$\varepsilon$精度的明确收敛保证。对于均匀离散扩散，$\tau$-leaping算法实现了迭代复杂度为$\tilde O(d/\varepsilon)$，消除了对词汇量大小$S$的线性依赖，并将现有界限提高了$d$倍；同时证明了一般情况下维度$d$的线性依赖是不可避免的。对于遮罩离散扩散，通过引入修改后的$\tau$-leaping采样器，其收敛率由一个称为有效总相关的信息论量控制，该量被限制在$d \log S$内，但对于结构化数据可以是次线性甚至常数，从而使得采样器能够自适应于低维结构而无需先验知识或算法修改，在各种实际示例中表现出次线性的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在离散空间上的扩散模型取得了显著的经验成功，但它们的理论基础仍然不完整。本文旨在填补这一空白，通过对基于连续时间马尔可夫链（CTMC）框架下的分数基离散扩散模型进行深入研究，特别关注$\tau$-leaping采样器的采样效率。

Method: 采用连续时间马尔可夫链（CTMC）框架来分析分数基离散扩散模型，特别是针对$\tau$-leaping采样方法。研究了两种类型的噪声过程：均匀噪声和遮罩噪声，并为每种情况提供了达到特定KL散度准确度所需的迭代次数的理论保证。

Result: 对于均匀离散扩散，证明了$\tau$-leaping算法的迭代复杂度为$\tilde O(d/\varepsilon)$，其中$d$为目标分布的空间维度，这极大地减少了对词汇量大小$S$的依赖。此外，还展示了在一般情况下，维度$d$上的线性依赖无法避免。对于遮罩离散扩散，提出了一种改进的$\tau$-leaping采样器，其性能由一个与数据结构紧密相关的有效总相关量决定，这使得采样器能够自动适应具有低维结构的数据集，例如隐藏马尔科夫模型、图像数据以及随机图等。

Conclusion: 本文的工作加深了我们对分数基离散扩散模型的理解，特别是在提高采样效率方面。通过引入新的理论分析工具，不仅揭示了现有方法的局限性，也为设计更加高效且适应性强的新一代采样器奠定了基础。

Abstract: Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $τ$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $τ$-leaping algorithm achieves an iteration complexity of order $\tilde O(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $τ$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.

</details>


### [141] [Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation](https://arxiv.org/abs/2602.15022)
*Cai Zhou,Zijie Chen,Zian Li,Jike Wang,Kaiyi Jiang,Pan Li,Rose Yu,Muhan Zhang,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 该论文提出了一种新的处理化学和科学中生成任务的方法，通过将每个样本映射到一个具有规范姿态或顺序的轨道代表上，然后在规范切片上训练无约束（非等变）扩散或流模型，并最终通过在生成时采样随机对称变换来恢复不变分布。这种方法被证明比传统的等变方法更有效，尤其是在3D分子生成任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 作者们挑战了传统上通过架构约束（如等变去噪器和不变先验）来强制执行不变性和等变性的策略，提出了基于规范化视角的新方法，旨在提高模型的表达能力以及加速训练过程。

Method: 首先将每个样本映射到一个具有规范姿态或顺序的轨道代表上；接着，在这个规范切片上训练一个无约束（非等变）扩散或流模型；最后，通过在生成时刻采样一个随机对称变换来恢复不变分布。研究还探讨了几何谱基规范化与轻微位置编码的应用，以及如何通过优化传输进一步提高训练效率。

Result: 所提出的规范扩散方法显著优于等变基线模型，在3D分子生成任务中的表现尤为出色。此外，借助名为Canon的新架构，CanonFlow在GEOM-DRUG数据集上取得了最先进的性能，即使是在少步生成的情况下优势依然明显。

Conclusion: 本研究表明，采用规范化视角而非传统等变约束来处理具有群对称性不变性质的生成任务是可行且有效的。它不仅提高了模型的表达能力，还通过减少由群混合引起的扩散分数复杂度及降低流匹配中的条件方差而加速了训练过程。

Abstract: Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.

</details>


### [142] [Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization](https://arxiv.org/abs/2602.15028)
*Shangding Gu*

Main category: cs.LG

TL;DR: 本研究通过引入大规模基准PAPerBench，探索了上下文长度对大型语言模型（LLMs）个性化质量和隐私保护的影响。随着上下文长度增加，个人化和隐私表现均出现下降。此外，还提供了一个理论分析来解释这种现象，并指出当前模型在长上下文处理上的普遍局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于尽管大型语言模型越来越多地应用于重视隐私和个人化的场景中，但关于上下文长度如何影响这些方面的理解仍相对有限。

Method: 研究人员创建了一个名为PAPerBench的大规模基准测试，该测试包含大约29,000个实例，其上下文长度从1K到256K个令牌不等，总共生成了377,000个评估问题。利用此基准，他们对不同情景下的个性化性能和隐私风险进行了联合评估。

Result: 实验结果表明，在最先进的LLM上，随着上下文长度的增加，个性化效果和隐私保护都会恶化。此外，还提出了一种基于注意力稀释的理论分析，为这一观察提供了支持。

Conclusion: 综合实证与理论发现揭示了现有模型的一个普遍扩展差距——即较长的上下文反而导致较少的关注度。这项工作不仅指出了当前技术的限制，也为未来可扩展隐私和个人化研究奠定了基础。

Abstract: Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench

</details>
