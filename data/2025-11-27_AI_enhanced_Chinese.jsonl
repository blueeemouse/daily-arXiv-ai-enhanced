{"id": "2511.20732", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20732", "abs": "https://arxiv.org/abs/2511.20732", "authors": ["Ziyuan Gao", "Philippe Morel"], "title": "Prompt-Aware Adaptive Elastic Weight Consolidation for Continual Learning in Medical Vision-Language Models", "comment": "Accepted by 32nd International Conference on MultiMedia Modeling (MMM 2026)", "summary": "Medical AI systems face catastrophic forgetting when deployed in clinical settings, where models must learn new imaging protocols while retaining prior diagnostic capabilities. This challenge is particularly acute for medical vision-language models that must preserve complex cross-modal alignments between medical images and clinical terminology across diverse imaging modalities. We introduce Prompt- Aware Adaptive Elastic Weight Consolidation (PA-EWC), a novel continual learning approach that addresses catastrophic forgetting through prompt-guided parameter specialization. Our method systematically categorizes model parameters based on their functional roles in processing visual-descriptive, spatial-guided, and medical-semantic information, enabling targeted protection of critical knowledge while allowing adaptation to new clinical requirements. PA-EWC incorporates adaptive Fisher Information computation with gradient stability analysis and develops weighted complexity metrics based on medical terminology density. We evaluate our approach across five medical imaging datasets (Kvasir-SEG, ISIC 2018, CheXlocalize, BUSI, CAMUS) representing diverse modalities including endoscopy, dermoscopy, radiography, and ultrasound. Experimental results demonstrate that PA-EWC reduces catastrophic forgetting by up to 17.58% compared to baseline methods, with performance improvements of 4.30% on chest X-ray pathology localization and 6.06% on polyp segmentation.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21146", "categories": ["cs.MM", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.21146", "abs": "https://arxiv.org/abs/2511.21146", "authors": ["Xinyue Guo", "Xiaoran Yang", "Lipan Zhang", "Jianxuan Yang", "Zhao Wang", "Jian Luan"], "title": "AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control", "comment": null, "summary": "Sound effect editing-modifying audio by adding, removing, or replacing elements-remains constrained by existing approaches that rely solely on low-level signal processing or coarse text prompts, often resulting in limited flexibility and suboptimal audio quality. To address this, we propose AV-Edit, a generative sound effect editing framework that enables fine-grained editing of existing audio tracks in videos by jointly leveraging visual, audio, and text semantics. Specifically, the proposed method employs a specially designed contrastive audio-visual masking autoencoder (CAV-MAE-Edit) for multimodal pre-training, learning aligned cross-modal representations. These representations are then used to train an editorial Multimodal Diffusion Transformer (MM-DiT) capable of removing visually irrelevant sounds and generating missing audio elements consistent with video content through a correlation-based feature gating training strategy. Furthermore, we construct a dedicated video-based sound editing dataset as an evaluation benchmark. Experiments demonstrate that the proposed AV-Edit generates high-quality audio with precise modifications based on visual content, achieving state-of-the-art performance in the field of sound effect editing and exhibiting strong competitiveness in the domain of audio generation.", "AI": {"tldr": "AV-Edit\uff1a\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u8bed\u4e49\u7684\u751f\u6210\u5f0f\u97f3\u6548\u7f16\u8f91\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u89c6\u9891\u4e2d\u7684\u73b0\u6709\u97f3\u9891\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7f16\u8f91", "motivation": "\u73b0\u6709\u7684\u97f3\u6548\u7f16\u8f91\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4f4e\u7ea7\u4fe1\u53f7\u5904\u7406\u6216\u7c97\u7565\u6587\u672c\u63d0\u793a\uff0c\u5bfc\u81f4\u7075\u6d3b\u6027\u6709\u9650\u3001\u97f3\u9891\u8d28\u91cf\u6b20\u4f73", "method": "\u4f7f\u7528\u5bf9\u6bd4\u89c6\u542c\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08CAV-MAE-Edit\uff09\u8fdb\u884c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u8bad\u7ec3\u57fa\u4e8e\u76f8\u5173\u7279\u5f81\u95e8\u63a7\u8bad\u7ec3\u7b56\u7565\u7684\u591a\u6a21\u6001\u6269\u6563\u53d8\u538b\u5668\uff08MM-DiT\uff09", "result": "\u5b9e\u9a8c\u8868\u660eAV-Edit\u80fd\u591f\u57fa\u4e8e\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u5177\u6709\u7cbe\u786e\u4fee\u6539\u7684\u9ad8\u8d28\u91cf\u97f3\u9891", "conclusion": "AV-Edit\u5728\u97f3\u6548\u7f16\u8f91\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u97f3\u9891\u751f\u6210\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7ade\u4e89\u529b"}}
{"id": "2511.21244", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.21244", "abs": "https://arxiv.org/abs/2511.21244", "authors": ["Ziheng Guo", "Tianxiang Wei", "Zeyu Li", "Lianghao Zhang", "Sisi Li", "Jiawan Zhang"], "title": "PixelatedScatter: Arbitrary-level Visual Abstraction for Large-scale Multiclass Scatterplots", "comment": null, "summary": "Overdraw is inevitable in large-scale scatterplots. Current scatterplot abstraction methods lose features in medium-to-low density regions. We propose a visual abstraction method designed to provide better feature preservation across arbitrary abstraction levels for large-scale scatterplots, particularly in medium-to-low density regions. The method consists of three closely interconnected steps: first, we partition the scatterplot into iso-density regions and equalize visual density; then, we allocate pixels for different classes within each region; finally, we reconstruct the data distribution based on pixels. User studies, quantitative and qualitative evaluations demonstrate that, compared to previous methods, our approach better preserves features and exhibits a special advantage when handling ultra-high dynamic range data distributions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6563\u5e03\u56fe\u53ef\u89c6\u5316\u62bd\u8c61\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u57df\u7684\u7279\u5f81\u4fdd\u7559\uff0c\u901a\u8fc7\u4e09\u6b65\u6d41\u7a0b\u5b9e\u73b0\u66f4\u597d\u7684\u7279\u5f81\u4fdd\u5b58\u6548\u679c", "motivation": "\u5927\u89c4\u6a21\u6563\u5e03\u56fe\u4e2d\u8fc7\u5ea6\u7ed8\u5236\u4e0d\u53ef\u907f\u514d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u57df\u5bb9\u6613\u4e22\u5931\u91cd\u8981\u7279\u5f81", "method": "1. \u5c06\u6563\u5e03\u56fe\u5212\u5206\u4e3a\u7b49\u5bc6\u5ea6\u533a\u57df\u5e76\u5747\u8861\u89c6\u89c9\u5bc6\u5ea6 2. \u4e3a\u6bcf\u4e2a\u533a\u57df\u5185\u4e0d\u540c\u7c7b\u522b\u5206\u914d\u50cf\u7d20 3. \u57fa\u4e8e\u50cf\u7d20\u91cd\u5efa\u6570\u636e\u5206\u5e03", "result": "\u7528\u6237\u7814\u7a76\u3001\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\uff0c\u672c\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u7279\u5f81\uff0c\u5728\u5904\u7406\u8d85\u9ad8\u52a8\u6001\u8303\u56f4\u6570\u636e\u5206\u5e03\u65f6\u5177\u6709\u7279\u6b8a\u4f18\u52bf", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u6563\u5e03\u56fe\u63d0\u4f9b\u4e86\u8de8\u4efb\u610f\u62bd\u8c61\u7ea7\u522b\u7684\u66f4\u597d\u7279\u5f81\u4fdd\u6301\uff0c\u7279\u522b\u662f\u5728\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u57df\u8868\u73b0\u51fa\u8272"}}
{"id": "2511.20867", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20867", "abs": "https://arxiv.org/abs/2511.20867", "authors": ["Puneet S. Bagga", "Vivek F. Farias", "Tamar Korkotashvili", "Tianyi Peng", "Yuhang Wu"], "title": "E-GEO: A Testbed for Generative Engine Optimization in E-Commerce", "comment": null, "summary": "With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a \"universally effective\" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.20904", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20904", "abs": "https://arxiv.org/abs/2511.20904", "authors": ["Mengliang ZHang"], "title": "Generating Querying Code from Text for Multi-Modal Electronic Health Record", "comment": null, "summary": "Electronic health records (EHR) contain extensive structured and unstructured data, including tabular information and free-text clinical notes. Querying relevant patient information often requires complex database operations, increasing the workload for clinicians. However, complex table relationships and professional terminology in EHRs limit the query accuracy. In this work, we construct a publicly available dataset, TQGen, that integrates both \\textbf{T}ables and clinical \\textbf{T}ext for natural language-to-query \\textbf{Gen}eration. To address the challenges posed by complex medical terminology and diverse types of questions in EHRs, we propose TQGen-EHRQuery, a framework comprising a medical knowledge module and a questions template matching module. For processing medical text, we introduced the concept of a toolset, which encapsulates the text processing module as a callable tool, thereby improving processing efficiency and flexibility. We conducted extensive experiments to assess the effectiveness of our dataset and workflow, demonstrating their potential to enhance information querying in EHR systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TQGen\u6570\u636e\u96c6\u548cTQGen-EHRQuery\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u67e5\u8be2\u6765\u6539\u8fdb\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\u7684\u4fe1\u606f\u68c0\u7d22\u6548\u7387\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5305\u542b\u5927\u91cf\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4f46\u590d\u6742\u7684\u6570\u636e\u8868\u5173\u7cfb\u548c\u4e13\u4e1a\u672f\u8bed\u9650\u5236\u4e86\u67e5\u8be2\u51c6\u786e\u6027\uff0c\u589e\u52a0\u4e86\u4e34\u5e8a\u533b\u751f\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002", "method": "\u6784\u5efaTQGen\u6570\u636e\u96c6\uff0c\u96c6\u6210\u8868\u683c\u548c\u4e34\u5e8a\u6587\u672c\uff1b\u63d0\u51faTQGen-EHRQuery\u6846\u67b6\uff0c\u5305\u542b\u533b\u5b66\u77e5\u8bc6\u6a21\u5757\u548c\u95ee\u9898\u6a21\u677f\u5339\u914d\u6a21\u5757\uff1b\u5f15\u5165\u5de5\u5177\u96c6\u6982\u5ff5\u5c01\u88c5\u6587\u672c\u5904\u7406\u6a21\u5757\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u548c\u5de5\u4f5c\u6d41\u7a0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u63d0\u5347EHR\u7cfb\u7edf\u4fe1\u606f\u67e5\u8be2\u80fd\u529b\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.21121", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21121", "abs": "https://arxiv.org/abs/2511.21121", "authors": ["Anup Roy", "Rishabh Gyanendra Upadhyay", "Animesh Rameshbhai Panara", "Robin Mills"], "title": "Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval", "comment": null, "summary": "Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.\n  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.\n  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21389", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21389", "abs": "https://arxiv.org/abs/2511.21389", "authors": ["Guoxiao Zhang", "Ao Li", "Tan Qu", "Qianlong Xie", "Xingxing Wang"], "title": "FITRep: Attention-Guided Item Representation via MLLMs", "comment": null, "summary": "Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact.", "AI": {"tldr": "FITRep\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u767d\u7bb1\u5546\u54c1\u8868\u5f81\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5d4c\u5165\u4e2d\u7684\u5c40\u90e8\u7ed3\u6784\u584c\u9677\u95ee\u9898\uff0c\u5728\u7f8e\u56e2\u5e7f\u544a\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86CTR\u548cCPM\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u591a\u6a21\u6001\u8868\u5f81\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5ffd\u7565\u4e86\u5546\u54c1\u5143\u7d20\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\uff08\u5982\u4e3b\u6b21\u5143\u7d20\uff09\uff0c\u5bfc\u81f4\u5c40\u90e8\u7ed3\u6784\u584c\u9677\u95ee\u9898\uff0c\u5f71\u54cd\u5546\u54c1\u53bb\u91cd\u6548\u679c\u3002", "method": "\u63d0\u51faFITRep\u6846\u67b6\uff1a1) CHIE\u6a21\u5757\u7528MLLMs\u63d0\u53d6\u5c42\u6b21\u5316\u8bed\u4e49\u6982\u5ff5\uff1b2) SPDR\u6a21\u5757\u57fa\u4e8eUMAP\u8fdb\u884c\u7ed3\u6784\u4fdd\u6301\u7684\u964d\u7ef4\uff1b3) FBC\u6a21\u5757\u57fa\u4e8eFAISS\u5b9e\u73b0\u9ad8\u6548\u805a\u7c7b\u3002", "result": "\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cFITRep\u5728\u7f8e\u56e2\u5e7f\u544a\u7cfb\u7edf\u4e0a\u5e26\u6765CTR\u63d0\u53473.60%\uff0cCPM\u63d0\u53474.25%\u3002", "conclusion": "FITRep\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ed3\u6784\u611f\u77e5\u7684\u8868\u5f81\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5546\u54c1\u53bb\u91cd\u4e2d\u7684\u7ed3\u6784\u584c\u9677\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5546\u4e1a\u4ef7\u503c\u3002"}}
{"id": "2511.21394", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21394", "abs": "https://arxiv.org/abs/2511.21394", "authors": ["Guoxiao Zhang", "Tan Qu", "Ao Li", "DongLin Ni", "Qianlong Xie", "Xingxing Wang"], "title": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction", "comment": null, "summary": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.", "AI": {"tldr": "RIA\u6846\u67b6\uff1a\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u6392\u5e8f-\u91cd\u6392\u5e8f\u67b6\u6784\uff0c\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u8350\u7cfb\u7edf\u4f18\u5316", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6392\u5e8f\u548c\u91cd\u6392\u5e8f\u89e3\u8026\uff0c\u5bfc\u81f4\u7ec4\u5408\u7a00\u758f\u6027\u548c\u8868\u793a\u80fd\u529b\u53d7\u9650", "method": "\u63d0\u51faRIA\u6846\u67b6\uff0c\u5305\u542bUCDT\u3001CUHT\u3001LMH\u548cEC\u56db\u4e2a\u6a21\u5757\uff0c\u5b9e\u73b0\u6392\u5e8f\u548c\u91cd\u6392\u5e8f\u7684\u6df1\u5ea6\u878d\u5408", "result": "\u5728\u516c\u5f00\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\u6a21\u578b\uff0cAUC\u548cLogLoss\u663e\u8457\u63d0\u5347", "conclusion": "RIA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6392\u5e8f-\u91cd\u6392\u5e8f\u89e3\u8026\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e26\u6765\u663e\u8457\u7684CTR\u548cCPM\u63d0\u5347"}}
