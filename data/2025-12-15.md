<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 32]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis](https://arxiv.org/abs/2512.10973)
*Jiang Liu,Yujie Li,Chan Zhou,Yihao Xie,Qilong Sun,Xin Shu,Peiwei Li,Chunyong Yang,Yiziting Zhu,Jiaqi Zhu,Yuwen Chen,Bo An,Hao Wu,Bin Yi*

Main category: cs.LG

TL;DR: 本研究提出了一种基于强化学习(RL)的新框架，用于优化手术脓毒症患者的个性化肝素治疗。通过将离散的SOFA评分转换为连续的cxSOFA评分，并引入了治疗效果比较矩阵(TECM)，该方法能够在更细致的状态和奖励函数基础上评估治疗策略。在使用MIMIC-IV v1.0和eICU v2.0数据库的数据进行模型训练与评估后发现，cxSOFA-CQL模型表现最佳，能够显著降低死亡率并缩短平均住院时间。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是一种由严重感染引起的威胁生命的状况，导致急性器官功能障碍。本研究旨在开发一种数据驱动的度量标准和连续奖励函数，以优化接受手术脓毒症患者个体化肝素治疗的效果。

Method: 采用MIMIC-IV v1.0和eICU v2.0数据库中的数据进行模型开发与评估。首先，将离散的SOFA评分转换为连续的cxSOFA评分；其次，基于cxSOFA评分逐步定义“好”或“差”的治疗策略；最后，提出类似于分类任务中混淆矩阵的治疗效果比较矩阵(TECM)，来评价不同的治疗策略。研究中应用了多种RL算法（Q-Learning、DQN、DDQN、BCQ及CQL）以优化治疗方案，并全面评估了所提框架的表现。

Result: 在所有AI衍生策略中，cxSOFA-CQL模型表现最好，能将死亡率从1.83%降至0.74%，同时将平均住院时间从11.11天减少到9.42天。TECM显示了跨模型的一致性结果，突出了该方法的鲁棒性。

Conclusion: 提出的RL框架能够实现可解释且稳健的手术脓毒症肝素治疗优化。连续cxSOFA评分以及基于TECM的评估提供了更加细致的治疗效果评估手段，显示出提高临床结果及决策支持可靠性的潜力。

Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.

</details>


### [2] [MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax](https://arxiv.org/abs/2512.10991)
*Zhanpeng Chen,Weihao Gao,Shunyu Wang,Yanan Zhu,Hong Meng,Yuexian Zou*

Main category: cs.LG

TL;DR: MolSculpt is a new framework that bridges the gap between 1D syntactic generation and 3D geometric realization of molecules, by integrating 1D latent chemical knowledge with 3D molecular diffusion models to generate precise 3D molecular geometries, achieving state-of-the-art performance in 3D molecule generation.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the disconnect between 1D syntactic generation and 3D geometric realization in the context of generating 3D molecular geometries. Prior methods using 1D representations like SELFIES do not fully utilize the embedded chemical knowledge, leading to less accurate 3D structures. MolSculpt aims to integrate 1D chemical information more effectively into the 3D generation process.

Method: MolSculpt employs a frozen 1D molecular foundation model and a 3D molecular diffusion model. It uses learnable queries to extract chemical knowledge from the 1D model, which is then projected into the conditioning space of the 3D diffusion model, guiding the 3D geometry generation through an end-to-end optimization process.

Result: Experiments show that MolSculpt outperforms existing methods in de novo 3D molecule generation and conditional 3D molecule generation, demonstrating superior 3D fidelity and stability on the GEOM-DRUGS and QM9 datasets.

Conclusion: MolSculpt successfully bridges the gap between 1D and 3D molecular representations, providing a more effective way to generate accurate 3D molecular geometries, which is essential for drug discovery and material science.

Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.

</details>


### [3] [Memoryless Policy Iteration for Episodic POMDPs](https://arxiv.org/abs/2512.11082)
*Roy van Zuijlen,Duarte Antunes*

Main category: cs.LG

TL;DR: 本文提出了一类新的单调改进的策略迭代算法，用于解决部分可观测马尔可夫决策过程（POMDPs）问题。该算法在单阶段输出基础上进行策略改进和评估，并且确定了能最大化计算效率指标的最佳模式。此外，还开发了一个无模型变体直接从数据中估计值并学习无记忆策略。此方法在基于模型和无模型设置下均比现有算法有显著的计算速度优势。


<details>
  <summary>Details</summary>
Motivation: 为了解决POMDPs问题，提供一种实用的方法来替代传统的高维信念空间操作，作者们试图将经典方法如策略迭代应用于直接在输出空间工作的无记忆或有限记忆策略上。然而，由于输出过程是非马尔科夫性质的，这使得跨阶段的策略改进步骤相互依赖，因此这样的扩展变得困难。

Method: 作者们引入了一种新的、能够单调改进的策略迭代算法族，这些算法交替执行基于单阶段输出的策略改进与按照预定周期模式进行的策略评估。通过这种方法，可以识别出最大化自然计算效率指数的最佳模式以及具有最小周期的最简单模式。同时，基于这种结构，进一步开发了可以直接从数据估计价值并学习无记忆策略的无模型版本。

Result: 新提出的算法在多个POMDP示例中表现出色，无论是在基于模型还是无模型环境下，都相对于策略梯度基线和其他最近的专业算法实现了显著的计算加速。

Conclusion: 这项工作为处理POMDPs提供了一个有效的途径，通过利用单调改进的策略迭代算法及相应的无模型变体，在保持性能的同时大幅提高了计算效率。

Abstract: Memoryless and finite-memory policies offer a practical alternative for solving partially observable Markov decision processes (POMDPs), as they operate directly in the output space rather than in the high-dimensional belief space. However, extending classical methods such as policy iteration to this setting remains difficult; the output process is non-Markovian, making policy-improvement steps interdependent across stages. We introduce a new family of monotonically improving policy-iteration algorithms that alternate between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. We show that this family admits optimal patterns that maximize a natural computational-efficiency index, and we identify the simplest pattern with minimal period. Building on this structure, we further develop a model-free variant that estimates values from data and learns memoryless policies directly. Across several POMDPs examples, our method achieves significant computational speedups over policy-gradient baselines and recent specialized algorithms in both model-based and model-free settings.

</details>


### [4] [Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning](https://arxiv.org/abs/2512.11095)
*Sana Rahmani,Javad Hashemi,Ali Etemad*

Main category: cs.LG

TL;DR: 本文首次系统研究了用于心电图诊断的部分标签学习(PLL)方法，通过调整九种PLL算法以适应多标签ECCG诊断，并在多种临床驱动的模糊生成策略下进行评估。实验表明，PLL方法对不同类型和程度的模糊性的鲁棒性存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 由于实际应用中ECG诊断存在着由重叠病症和诊断分歧导致的标签模糊问题，但现有ECG模型训练基于清晰无歧义标注的假设，限制了其在真实世界条件下的发展与有意义的评价。而部分标签学习（PLL）框架虽然设计来处理模糊标签，但在医疗时间序列领域尤其是ECG中的有效性仍待探索。

Method: 本研究选择了9种PLL算法并将其应用于多标签ECG诊断；采用了一系列基于临床动机的模糊性生成策略，包括非结构化（如随机）和结构性模糊性（如心脏病专家推导的相似性、治疗关系及诊断分类学）。

Result: 实验结果展示了不同PLL方法对于不同类型和不同程度模糊性的鲁棒性存在很大差异。

Conclusion: 通过对当前PLL方法在临床环境中关键局限性的识别，为开发稳健且符合临床需求的模糊感知学习框架指明了未来方向。

Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.

</details>


### [5] [Limits and Gains of Test-Time Scaling in Vision-Language Reasoning](https://arxiv.org/abs/2512.11109)
*Mohammadjavad Ahmadpour,Amirmahdi Meighani,Payam Taebi,Omid Ghahroodi,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 研究了不同视觉-语言模型在应用测试时推理方法的效果，发现封闭源模型从结构化推理和迭代自我完善中受益，而开源模型则表现出不一致的行为。此外，测试时缩放的有效性取决于数据集，对多步骤推理任务有明显改进，但在以感知为重点的基准测试中仅提供有限增益。


<details>
  <summary>Details</summary>
Motivation: 探索测试时缩放（TTS）在视觉-语言模型中的应用潜力及其对于不同类型模型和任务的影响。

Method: 通过系统性的实证研究，将几种推理时间方法应用于开源与封闭源视觉-语言模型，并在不同基准上评估其性能。

Result: 封闭源模型能够持续地从结构化推理及迭代自优化中获益；相比之下，开源视觉-语言模型表现出了不一致的行为：外部验证提供了最可靠的性能提升，而迭代优化往往导致性能下降。测试时扩展的有效性还显示出对数据集的依赖性，在多步推理任务上带来显著改进，但对于侧重于感知能力的基准测试，则仅提供有限的帮助。

Conclusion: 测试时缩放并非通用解决方案，需要根据模型能力和任务特性进行定制。这为未来关于自适应TTS策略以及多模态奖励模型的研究指明了方向。

Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.

</details>


### [6] [Fairness-Regularized Online Optimization with Switching Costs](https://arxiv.org/abs/2512.11131)
*Pengfei Li,Yuelin Han,Adam Wierman,Shaolei Ren*

Main category: cs.LG

TL;DR: 本文提出了一种新的公平正则化平滑在线凸优化算法FairOBD，解决了在存在切换成本的情况下最小化命中成本、切换成本和公平成本之间的矛盾。通过引入辅助变量将长期公平成本分解为一系列在线成本，并利用该变量对在线行动进行规范化以促进公平结果。实验表明，FairOBD能有效减少总公平正则化成本并更好地促进公平结果。


<details>
  <summary>Details</summary>
Motivation: 在线优化问题中，公平性和动作平滑性是两个关键考虑因素，但它们尚未被同时解决。本文旨在研究一种新的具有挑战性的设置，即带有切换成本的公平正则化平滑在线凸优化。

Method: 提出了FairOBD（公平正则化的在线平衡下降）算法，该算法通过引入一个辅助变量来分解长期公平成本为一系列在线成本，并使用这个辅助变量来规范在线行为以达到公平的结果。此外，还提出了一种新方法来考虑切换成本，并证明了当T趋向于无穷大时，FairOBD相对于一个新颖的基准——参数化约束下的最优离线算法——提供了最坏情况渐近竞争比。

Result: 理论分析表明，在没有切换成本的情况下，任何在线算法都不可能随着问题周期长度T的增加而实现次线性遗憾或有限的竞争比。但是，通过引入FairOBD算法，可以在考虑到切换成本的同时，有效地减少总的公平正则化成本，并且与现有基线解决方案相比，能够更好地促进公平结果。

Conclusion: 实验结果表明，FairOBD能够在动态计算资源调配场景下有效降低公平正则化总成本，并且相较于已有方案能够更好地促进公平结果。

Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.

</details>


### [7] [The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions](https://arxiv.org/abs/2512.11138)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 本文提出了一种新的可微谱方法——Vekua层，用于解决隐式神经表示(INRs)中存在的谱偏置和非凸优化的计算成本问题。通过将假设空间限制在控制微分算子的核内，并利用调和和Fourier-Bessel基底，Vekua层能够把学习任务转化为一个严格的凸最小二乘问题，从而通过线性投影解决。实验结果表明，在处理精确重建任务时，该方法能达到机器精度；面对不相干传感器噪声时展现出更好的稳定性；此外，它还支持从部分边界数据进行全局场的“全息”外推。


<details>
  <summary>Details</summary>
Motivation: 针对隐式神经表示（INR）在参数化物理场时存在的谱偏置问题及非凸优化所带来的高昂计算成本，研究者们寻求一种更高效且稳定的解决方案。

Method: 提出了基于广义解析函数理论的Vekua层（VL），这是一种可微谱方法。通过将假设空间限定于管理微分算子的核内，并特别使用了调和与Fourier-Bessel基，VL能够将学习过程从迭代梯度下降转换为严格凸的最小二乘问题，进而通过线性投影求解。

Result: 实验结果显示，对于精确重建任务，VL能够达到接近机器精度的表现（MSE约等于10^-33）。同时，在存在不相干传感器噪声的情况下，VL同样表现出优越的稳定性（MSE约为0.03）。此外，VL还能实现从部分边界数据出发对全球场进行‘全息’外推的能力，这是标准坐标基近似方法所不具备的特点。

Conclusion: Vekua层作为一种新颖的方法论，不仅有效解决了INRs中的谱偏置问题及非凸优化挑战，而且在精确重建、抗噪稳定性和基于部分信息的全局场外推方面展示了显著优势，为物理信息编码提供了新的视角。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.

</details>


### [8] [Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles](https://arxiv.org/abs/2512.11145)
*Lennard Manuel,Hamid Gadirov,Steffen Frey*

Main category: cs.LG

TL;DR: 本文提出了一种增强的自编码器框架，该框架结合了基于软轮廓分数的聚类损失和对比损失，以提高科学集合数据集的可视化和可解释性。通过使用EfficientNetV2为未标记的数据生成伪标签，并联合优化重建、聚类和对比目标，使得相似的数据点在潜在空间中聚集而不同的集群被分开。实验结果表明，加入聚类或对比损失的模型在两个科学集合数据集上稍微优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 分析和可视化高维度和复杂性的科学集合数据集面临重大挑战。尽管降维技术和自编码器是提取特征的强大工具，但它们处理如此高维的数据时往往遇到困难。因此，需要一种改进的方法来有效处理这些数据并提高其可视化与可解释性。

Method: 提出了一种新的自编码器框架，该框架加入了基于软轮廓分数的聚类损失以及对比损失。首先利用EfficientNetV2为无标签的科学集合数据集部分生成伪标签；然后共同优化重建、聚类和对比目标；最后采用UMAP技术将得到的潜在表示转换成2D投影图，并用轮廓分数进行评估。

Result: 实验对多种类型的自编码器进行了评估比较，特别是在从马尔科夫链蒙特卡洛导出的土壤通道结构和液滴撞击薄膜动力学这两个科学集合数据集上的表现。结果显示，包含聚类或对比损失的模型相比基准方法有轻微的优势。

Conclusion: 本研究提出的结合了聚类和对比损失的增强型自编码器框架，在处理高维复杂的科学集合数据集时能够提供更好的可视化效果和更高的可解释性。

Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.

</details>


### [9] [Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities](https://arxiv.org/abs/2512.11178)
*Takuya Kurihana,Xiaojian Zhang,Wing Yee Au,Hon Yung Wong*

Main category: cs.LG

TL;DR: 该研究提出了一种异构数据管道，能够跨领域融合时间变化、空间变化以及时空序列数据集，以解决多领域和地区的复杂城市问题。通过整合来自50多个数据源的信息，特别是将空间变化数据集中的同质性融入图学习中，该方法在不同地区或领域应用时表现出强大的预测性能和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代城市越来越依赖数据驱动的洞察力来支持决策，但城市层面的数据往往格式各异，由具有不同目标和标准的地方机构独立收集。尽管国家级数据集数量众多、范围广泛且易于使用，但仍显示出显著的异质性和多模态性。

Method: 本研究设计了一个异构数据管道，可以处理随时间和空间变化的数据集，并通过数据学习模块将空间变化数据集中的相似性整合到图学习中，从而将各种地方的信息嵌入模型里。

Result: 通过五个真实世界的观察案例，使用了从多个城市收集的各种公开可访问的数据集（如共享乘车、交通事故和犯罪报告）验证了框架的通用性和灵活性。结果显示，所提出的框架不仅展示了强大的预测性能，而且在转移到新的地区或领域时几乎不需要重新配置。

Conclusion: 这项研究推进了以可扩展方式构建基于数据的城市系统的目标，解决了智慧城市分析中最紧迫的挑战之一。

Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.

</details>


### [10] [On the failure of ReLU activation for physics-informed machine learning](https://arxiv.org/abs/2512.11184)
*Conor Rowan*

Main category: cs.LG

TL;DR: 该研究探讨了ReLU激活函数在物理信息机器学习问题中的表现不佳的原因，指出即使是在只涉及一阶导数的变分问题中，ReLU也因自动微分无法正确处理不连续场的导数而导致性能差。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，在基于物理信息的机器学习任务中，与其他激活函数（如Sigmoid、双曲正切和Swish）相比，ReLU的表现较差。本研究旨在诊断并解释ReLU在这些问题上表现不佳的原因。

Method: 通过分析发现，即使对于只包含一阶导数的问题，ReLU由于其分段线性形式也无法很好地工作。研究进一步指出，训练过程中自动微分工具（以PyTorch为例）对不连续场的二阶导数处理不当是导致ReLU性能不佳的关键原因。

Result: 研究结果表明，ReLU在物理信息机器学习问题中的失败不仅限于它已知的不能应用于二阶微分方程的情况，还扩展到了仅含有一阶导数的问题。这是因为自动微分工具未能准确地表征不连续场的导数，从而误导了物理信息损失函数的梯度计算。

Conclusion: 本研究表明，ReLU激活函数不适合用于物理信息机器学习任务，特别是当涉及到需要精确导数计算的情形时。此外，研究强调了选择合适激活函数的重要性以及当前自动微分技术在处理某些类型函数时存在的局限性。

Abstract: Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.

</details>


### [11] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 本文提出了一种梯度投影框架，用于在大规模文本到图像的扩散模型中系统地防止禁止概念级特征的内化。该方法通过在反向传播过程中识别并剔除与禁止属性对齐的训练信号来实现这一点，从而大大减少了记忆效应，同时保持了生成质量和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 大型文本到图像扩散模型中的记忆现象带来了显著的安全和知识产权风险，包括敌对属性提取以及未经授权复制敏感或专有特征的风险。传统的去记忆化技术如正则化和数据过滤虽然可以限制过拟合特定训练样本的问题，但未能有效阻止模型内部吸收被禁止的概念级特征。简单地丢弃所有包含敏感特征的图像会导致宝贵的训练数据浪费，因此需要一种能够在概念级别上进行选择性遗忘的方法。

Method: 为了解决这个问题，研究者们引入了一个梯度投影框架，旨在强制执行严格的概念级特征排除要求。该防御机制在反向传播期间运作，通过系统地识别并与禁止属性嵌入对齐的训练信号进行切除。具体来说，他们将每个梯度更新投射到敏感特征嵌入空间的正交补集上，从而消除其对模型权重的影响。

Result: 广泛实验表明，该框架显著降低了记忆现象的发生，同时严格保持了生成质量与语义准确性。此外，这种方法能够无缝集成到标准扩散模型训练流程中，并且与现有防御措施互补。

Conclusion: 通过将记忆控制重新定义为选择性学习，本方法为知识产权安全和隐私保护的生成式AI设立了一个新的范例。

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [12] [Fast EXP3 Algorithms](https://arxiv.org/abs/2512.11201)
*Ryoma Sato,Shinji Ito*

Main category: cs.LG

TL;DR: 本文指出EXP3算法可以在每轮中以常数时间实现，提出了更实用的算法，并分析了这些算法在遗憾界和时间复杂度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进现有的EXP3算法，使其更加高效且实用，同时探讨不同算法在性能上的折衷。

Method: 通过对EXP3算法进行优化，提出新的算法实现，并对提出的算法与原有方法在遗憾界及计算效率上进行了对比分析。

Result: 结果表明，通过所提方法可以使得EXP3算法在保持理论性能的同时，达到更高的实际运行效率。

Conclusion: 结论是新提出的算法不仅能够有效降低时间复杂度，而且还能维持良好的遗憾界表现，为实际应用提供了更多选择。

Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.

</details>


### [13] [Latent Variable Causal Discovery under Selection Bias](https://arxiv.org/abs/2512.11219)
*Haoyue Dai,Yiwen Qiu,Ignavier Ng,Xinshuai Dong,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 本文探讨了在存在选择偏差的情况下，如何通过秩约束来识别潜在变量的因果关系，提出了一种即使在选择偏差下也能有效识别经典单因子模型的方法。


<details>
  <summary>Details</summary>
Motivation: 解决潜变量因果发现中的选择偏差问题非常重要但研究不足，主要原因是缺乏合适的统计工具。尽管已有处理潜变量的各种工具，但它们尚未适应于选择偏差的情形。

Method: 本文通过研究秩约束来尝试解决这一问题，这些约束是条件独立性约束的一种泛化，并利用线性高斯模型中协方差子矩阵的秩。作者提供了一个关于这种秩约束的图论表征。

Result: 研究表明，尽管选择过程可能显著复杂化联合分布，但在偏差协方差矩阵中的秩仍然保留了关于因果结构和选择机制有意义的信息。使用所提出的秩约束方法，经典的单因子模型能够在选择偏差条件下被识别。模拟实验和实际案例验证了该方法的有效性。

Conclusion: 通过引入秩约束作为分析手段，为处理包含选择偏差的数据集中的潜变量因果发现提供了新的视角和解决方案。

Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.

</details>


### [14] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出了一种名为ASR-KF-EGR的无训练推理时间框架，通过可逆软冻结机制暂时暂停低重要性令牌的KV更新，从而在保持生成质量的同时显著减少活动KV缓存大小。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型在内存受限环境下的部署效率，同时保持高质量的生成结果。

Method: 引入一种可逆软冻结机制，对滑动注意力窗口内识别出的低重要性令牌临时暂停KV更新，并且将所有令牌保存在GPU外存储中，需要时恢复。此外还扩展了亚线性冻结调度机制，以防止过度激进的压缩。

Result: 在LLaMA-3 8B上的初步实验表明，该方法可以减少55-67%的活动KV缓存大小，同时维持生成质量和通过细针测试。

Conclusion: ASR-KF-EGR提供了一个实用的解决方案，用于在内存受限的情况下高效部署长上下文的大规模语言模型，无需微调且与架构无关。

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [15] [Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language](https://arxiv.org/abs/2512.11251)
*Yunkai Zhang,Yawen Zhang,Ming Zheng,Kezhen Chen,Chongyang Gao,Ruian Ge,Siyuan Teng,Amine Jelloul,Jinmeng Rao,Xiaoyuan Guo,Chiang-Wei Fang,Zeyu Zheng,Jie Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Insight Miner的大规模多模态模型，用于生成高质量、全面的时间序列描述，并引入了首个通用领域的时间序列与语言对齐数据集TS-Insights。通过在TS-Insights上的指令调优后，Insight Miner在生成时间序列描述和见解方面优于当前最先进的多模态模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据分析对于许多科学和工业领域至关重要，但从中挖掘洞察通常需要深厚的专业知识，这既耗时又费力。为了克服这一挑战并提高效率，提出了一个新的解决方案。

Method: 开发了名为Insight Miner的大型多模态模型，并创建了一个名为TS-Insights的数据集，该数据集包含从20个预测数据集中抽取的10万个时间序列窗口。使用一种新颖的工作流程，结合统计工具提取特征并通过GPT-4合成连贯的趋势描述来构建这个数据集。

Result: 经过TS-Insights上的指令调优后，Insight Miner在生成时间序列描述和见解方面表现优于现有的最先进多模态模型，如LLaVA和GPT-4。

Conclusion: 研究表明，利用大规模多模态模型进行时间序列分析是一个有前景的方向，为让大语言模型能够将时间序列作为原生输入模态解读奠定了基础步骤。

Abstract: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \textbf{TS-Insights}\footnote{Available at \href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.

</details>


### [16] [QGEC : Quantum Golay Code Error Correction](https://arxiv.org/abs/2512.11307)
*Hideo Mukai,Hoshitaro Ohnishi*

Main category: cs.LG

TL;DR: 本研究提出了一种基于经典信息理论中高效编码方法Golay码的量子纠错方法（QGEC），并通过Transformer进行解码计算。研究发现，不同噪声模型和生成多项式的权重对解码器准确性的影响存在差异，并且与需要50个数据量子比特、具有5的距离编码的toric码相比，需要23个数据量子比特、具有7的距离编码的Golay码实现了更高的解码精度，这表明使用Transformer实现量子纠错可能使Golay码更有效地实现容错量子计算。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在特定问题上相较于经典计算机能够显著减少计算负担，但量子比特容易受到外部噪声的影响。量子纠错（QEC）对于处理这种脆弱性至关重要。这项研究旨在通过引入一种新的量子纠错方法——量子Golay码纠错（QGEC），来提高量子纠错效率。

Method: 研究者提出了利用Golay码的量子纠错方法（QGEC），并采用Transformer模型来进行解码运算。他们评估了解码器在由具有三种不同权重集合的生成多项式定义的编码空间中的准确性，并考虑了三种不同的噪声模型，这些模型对应于位翻转错误和相位翻转错误的不同相关性。此外，在遵循离散均匀分布的噪声模型下，比较了分别针对Golay码和toric码训练的相同架构的Transformer解码器的解码性能。

Result: 结果表明，噪声模型之间的相关性越小，解码器的准确性越高；而生成多项式的权重对解码器准确性影响不大。更重要的是，尽管Golay码仅需23个数据量子比特且代码距离为7，它却比需要50个数据量子比特且代码距离为5的toric码展现出更高的解码准确性。

Conclusion: 研究表明，使用Transformer实现的基于Golay码的量子纠错方法有可能以更高的效率支持容错量子计算。Golay码凭借其较少的数据量子比特需求以及较长的代码距离，在某些情况下提供了优于toric码的解码性能。

Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.

</details>


### [17] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 本文提出了一种基于对称性感知的引导框架，通过理论分析和实验证明了利用对称性在引导过程中可以显著提高样本效率、防止价值分歧，并且即使在极其有限的演示数据下也能实现强大的策略改进。


<details>
  <summary>Details</summary>
Motivation: 虽然等变扩散策略（EDPs）结合了扩散模型的生成表达力以及几何对称性带来的强大泛化能力和样本效率，但直接使用标准（非等变）强化学习来微调这些策略可能会忽略EDPs旨在利用的对称性，导致样本效率低下和不稳定。因此，研究者希望开发一种能有效利用这些对称性的方法。

Method: 研究者首先从理论上证明了EDP的扩散过程具有等变性质，这为引入一个适合于等变扩散控制的群不变潜噪声马尔可夫决策过程奠定了基础。接着，他们提出了一个原则性的对称性意识引导框架，并通过一系列实验比较了标准、等变及近似等变强化学习策略在不同程度对称性任务上的表现。

Result: 实验结果表明，在引导过程中利用对称性能显著提高样本效率、防止价值分歧，并且即使是在非常有限的演示数据情况下也能达到显著的策略提升。此外，研究还探讨了严格等变性在对称性破坏情况下的实际界限。

Conclusion: 通过将对称性纳入到EDP的控制过程中，不仅可以提高样本效率还能避免价值发散问题，从而使得即便在极少量示例训练的情况下也能够实现良好的策略增强。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [18] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架——Null-Space约束策略优化（NSPO），用于在保持大语言模型核心能力的同时确保其行为符合人类价值观、社会规范和伦理原则。实验证明，NSPO能够大幅度超越现有方法，在不牺牲通用任务准确性的情况下达到最先进的安全性能，并且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地应用于实际场景中，确保它们的行为与人类的价值观、社会规范和道德原则相一致变得至关重要。然而，基于强化学习的安全对齐往往会导致已学习的一般能力被遗忘，即所谓的对齐税。为了解决这个问题，作者提出了一个新的RL框架来解决LLM的安全对齐问题，同时保持其核心能力。

Method: 引入了Null-Space约束的策略优化（NSPO）这一新方法，通过将安全策略梯度几何投影到一般任务的零空间中来减少安全对齐税。此外，从理论上证明了NSPO能够保留模型原有的核心功能，同时仍能保证有效的安全对齐方向。

Result: 广泛的实验表明，NSPO比现有方法有了显著改进，能够在包括数学、代码以及指令跟随任务在内的通用任务上实现最先进级别的安全表现而不损害准确度。特别值得注意的是，NSPO表现出很高的数据效率，仅需使用PKU-SafeRLHF公开的人类标注安全数据中的40%即可获得良好的安全性能。

Conclusion: NSPO作为一种新颖的RL框架，在保持大语言模型原有核心能力的前提下有效地解决了安全对齐问题，并且具有较高的数据效率优势。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>


### [19] [Sliced ReLU attention: Quasi-linear contextual expressivity via sorting](https://arxiv.org/abs/2512.11411)
*Siwan Boufadène,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制——切片ReLU注意力，它通过排序操作实现准线性复杂度，并且保留了强大的理论表达能力。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有基于softmax和ReLU的注意力机制在处理非常长的上下文时的计算效率问题，同时保持良好的序列到序列解缠任务能力和上下文通用逼近性质。

Method: 引入了切片ReLU注意力机制，该机制通过对键-查询差异的一维投影进行操作，并利用排序来达到准线性的时间复杂度O(n log(n))。

Result: 展示了切片ReLU注意力不仅在计算上有优势，还能够执行非平凡的序列到序列解缠任务，并满足上下文通用逼近性质。此外，小规模实验表明了这种核函数具有实际应用潜力。

Conclusion: 切片ReLU注意力作为一种新颖的方法，在提高处理长序列效率的同时，保持了与softmax注意力相似的表达力，为未来研究提供了新方向。

Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

</details>


### [20] [Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces](https://arxiv.org/abs/2512.11448)
*Arghya Pratihar,Arnab Seal,Swagatam Das,Inesh Chattopadhyay*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法HypeGBMS，将Gaussian Blurring Mean Shift (GBMS)扩展到双曲空间中，以更好地处理具有层次或树状结构的数据集。通过使用双曲距离和Möbius加权平均值，HypeGBMS能够保持与空间几何一致的更新，并且在分层数据集中展示了改进的聚类质量。实验结果表明，在非欧几里得设置下，HypeGBMS显著优于传统的均值漂移聚类方法。


<details>
  <summary>Details</summary>
Motivation: 尽管高斯模糊均值偏移（GBMS）在识别欧几里得空间中的任意形状聚类方面已被证明是有效的，但在处理具有层次或树状结构的数据集时却遇到了困难。因此，需要一种新方法来克服这一限制。

Method: 引入了HypeGBMS，这是一种将GBMS扩展到双曲空间的新方法。该方法用双曲距离替换欧几里得计算，并采用Möbius加权平均值以确保所有更新都与空间几何保持一致。

Result: 提供了关于收敛性和计算复杂性的理论见解，并通过实验证明了HypeGBMS在层次数据集上提高了聚类质量。此外，在11个真实世界数据集上的广泛评估显示，HypeGBMS在非欧几里得环境中显著优于传统均值偏移聚类方法。

Conclusion: 这项工作结合了经典的均值偏移聚类和双曲表示学习，为弯曲空间中的基于密度的聚类提供了一种有原则的方法。HypeGBMS不仅保留了GBMS的密度寻求行为，而且有效地捕捉到了潜在的层次结构，显示出其在处理非欧几里得数据时的强大性和有效性。

Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.

</details>


### [21] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 本文提出了一种可塑性上限框架，用于优化监督微调（SFT）和强化学习（RL）的结合使用，特别是专家轨迹的有效利用。通过广泛的基准测试，确定了顺序SFT-然后-RL管道为最佳标准，并提供了具体的扩展指导原则，包括转换到RL的最佳时机、数据规模的重要性以及如何选择最大化最终性能上限的专家轨迹。


<details>
  <summary>Details</summary>
Motivation: 尽管有效的后训练结合了监督微调（SFT）和强化学习（RL），但如何最有效地利用专家轨迹仍未得到解决。

Method: 提出了可塑性上限框架来从理论上支持这一领域，将表现分解为基础SFT表现和随后的RL可塑性。通过广泛的基准测试确立了SFT-然后-RL流水线作为优越的标准。

Result: 发现表明，在SFT稳定或轻微过拟合子阶段过渡到RL可以最大化最终上限；在SFT-然后-RL扩展上下文中“少即是多”并不成立，数据规模决定了主要的后训练潜力，而轨迹难度则作为性能乘数；最小SFT验证损失是选取能够最大化最终性能上限的专家轨迹的一个稳健指标。

Conclusion: 研究结果为最大化从专家轨迹中提取的价值提供了可操作的指导方针。

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [22] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: 本文提出了一种名为Co-TSFA的正则化框架，用于时间序列预测中区分应忽略的异常和需响应的异常。通过生成仅输入和输入-输出增强来模拟与预测无关和相关的异常，并引入潜在输出对齐损失将表示变化与预测变化联系起来。实验表明，该方法在异常条件下提高了性能，同时保持了正常数据上的准确性。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的应用场景中，一些异常事件具有持久的影响并对预测产生影响，而另一些则是短暂的、应该被忽略。标准的预测模型无法很好地区分这些情况，往往要么对噪音过度反应，要么忽略了持续的变化。因此，需要一种新的方法能够智能地判断哪些异常需要关注，哪些可以忽略。

Method: 提出了Co-TSFA（带异常对比的时间序列预测）框架，它通过创建仅基于输入和基于输入-输出的数据增强版本来分别模拟与预测不相关和相关的异常情况。此外，引入了一个潜在输出对齐损失函数，确保当输入发生改变时预测结果也相应调整。这样的设计有助于模型对于不重要的扰动保持不变性，同时对有意义的分布变化保持敏感。

Result: 通过交通流量、电力需求基准测试以及一个真实世界的现金需求数据集上进行的实验显示，Co-TSFA不仅能够在存在异常的情况下提高预测表现，而且还能保证在常规数据上的准确度不受影响。

Conclusion: Co-TSFA为处理含有异常值的时间序列预测问题提供了一个有效的解决方案，通过学习如何区分并适当地应对不同类型的异常，从而提高了预测模型的整体鲁棒性和准确性。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


### [23] [xGR: Efficient Generative Recommendation Serving at Scale](https://arxiv.org/abs/2512.11529)
*Qingxiao Sun,Tongxuan Liu,Shen Zhang,Siyu Wu,Peijun Yang,Haotian Liang,Menxin Li,Xiaolong Ma,Zhiwei Liang,Ziyi Ren,Minchao Zhang,Xinyu Liu,Ke Zhang,Depei Qian,Hailong Yang*

Main category: cs.LG

TL;DR: 提出了一种名为xGR的面向生成式推荐系统的服务系统，通过统一预填充和解码阶段处理、早期排序终止与基于掩码的项目过滤以及重构整体流水线来利用多层次重叠和多流并行性，实现实时高并发场景下的低延迟要求。实验表明，在严格的延迟限制下，xGR相比最先进基线至少提高了3.49倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式推荐（GR）集成了大型语言模型以增强对长用户-项目序列的理解，但其工作负载与LLM服务有显著不同。GR通常处理较长的提示信息而产生短且固定长度的输出，但由于较大的波束宽度导致每次解码阶段的计算成本特别高。此外，波束搜索涉及庞大的项目空间，使得排序开销变得特别耗时。因此，需要一种新的解决方案来满足实时高并发场景下的严格低延迟要求。

Method: xGR采用了三种主要策略：1) 通过分阶段计算和分离的KV缓存来统一预填充和解码阶段的处理；2) 实现早期排序终止和基于掩码的项目过滤，并重复使用数据结构；3) 重构整个流水线以利用多层次重叠和多流并行性。

Result: 实验结果表明，在实际推荐服务数据集上，当面临严格延迟约束时，xGR能够达到至少比当前最先进的基线高出3.49倍的吞吐量。

Conclusion: xGR作为一种专为生成式推荐设计的服务系统，成功地解决了现有方法在高并发场景下面临的性能瓶颈问题，实现了显著的吞吐量提升同时保持了低延迟特性。

Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

</details>


### [24] [Parametric Numerical Integration with (Differential) Machine Learning](https://arxiv.org/abs/2512.11530)
*Álvaro Leitao,Jonatan Ráfales*

Main category: cs.LG

TL;DR: 本文介绍了一种基于机器/深度学习的方法来解决参数积分问题，特别是通过在训练过程中结合导数信息的差异学习框架，在统计泛函、Chebyshev展开近似函数和直接由微分方程产生的积分等多种代表性问题上优于传统架构。


<details>
  <summary>Details</summary>
Motivation: 为了解决参数积分问题，并提高此类问题求解过程中的精度、可扩展性和样本效率。

Method: 采用了一种包含导数信息的差异学习框架，应用于统计泛函（如矩和累积分布函数）、通过Chebyshev展开逼近函数以及直接来源于微分方程的积分等领域。

Result: 与标准架构相比，基于差异机器学习的方法在所有案例中均表现出更低的均方误差、更好的可扩展性及改进的样本效率。

Conclusion: 该研究表明，利用包含导数信息的学习方法可以有效提升对参数积分问题的解决能力。

Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.

</details>


### [25] [Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction](https://arxiv.org/abs/2512.11547)
*Janaina Mourão-Miranda,Zakria Hussain,Konstantinos Tsirlis,Christophe Phillips,John Shawe-Taylor*

Main category: cs.LG

TL;DR: 本文提出了一种新的弹性网正则化多核学习(ENMKL)方法，该方法提供了核权重的简单解析更新。通过SVM和KRR两种算法实现，并在三种神经影像应用中进行了评估，结果显示ENMKL在所有任务中匹配或优于$l1$-范数MKL，且生成更稀疏、更具解释性的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的弹性网正则化多核学习(ENMKL)方法通常采用两阶段过程来更新核权重，这可能不够高效。为了提高效率并保持模型可解释性同时选择相关联的核，作者提出了一个替代的ENMKL公式，它能够提供核权重的直接解析解。

Method: 提出了一种新的ENMKL公式，允许对核权重进行简单的分析更新。基于此框架，为支持向量机(SVM)和核岭回归(KRR)开发了明确的算法，并将其整合到开源工具箱PRoNTo中。

Result: 新提出的ENMKL算法在三个神经成像应用中与$l1$-范数MKL以及使用未加权核和训练的SVM（或KRR）相比，在大多数情况下表现相同或更好。特别是在生成更稀疏、更易解释的模型方面，ENMKL展现出了优势。

Conclusion: 所提出的ENMKL方法不仅提高了计算效率，而且在多个神经影像学案例研究中证明了其有效性，特别是当需要模型解释性和处理相关核时。

Abstract: Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the "elastic-net" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.

</details>


### [26] [Fully Inductive Node Representation Learning via Graph View Transformation](https://arxiv.org/abs/2512.11561)
*Dooho Lee,Myeong Kong,Minho Jeong,Jaemin Yoo*

Main category: cs.LG

TL;DR: 本文介绍了一种新的表示轴——视图空间，以及一种基于此的节点和特征置换等变映射方法GVT，进而提出了一个全归纳模型Recurrent GVT用于节点表征学习。该模型在OGBN-Arxiv上预训练，并在27个节点分类基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对于图结构数据而言，实现跨数据集、完全归纳推理是困难的，因为特征空间在维度和语义上差异很大。任何特征空间的变化都可能违反对未见数据集的归纳适用性，这严格限制了图模型的设计空间。

Method: 引入了视图空间的概念，作为自然编码任意图的一种统一方式；提出了图视图转换（GVT），这是一种在视图空间中的节点和特征置换等变映射；基于GVT构建了Recurrent GVT，一个针对节点表征学习的全归纳模型。

Result: 当在OGBN-Arxiv上预训练并在27个节点分类基准上评估时，Recurrent GVT比先前的全归纳图模型GraphAny高出8.93%，并且至少比12个单独调优的GNN高出3.30%。

Conclusion: 研究结果确立了视图空间作为全归纳节点表征学习的有效基础。

Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

</details>


### [27] [Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584)
*Stefan Tabakov,Asen Popov,Dimitar Dimitrov,S. Ensiye Kiyamousavi,Vladimir Hristov,Boris Kraychev*

Main category: cs.LG

TL;DR: 本文提出了一种名为原子动作切片（AAS）的方法，旨在改善视觉-语言-动作模型在新技能或对象组合任务中的泛化能力。通过将长序列演示分解为短且类型化的原子动作，使得规划器更容易使用和策略学习更加高效。实验表明，在LIBERO数据集上微调CLIP-RT+模型后，任务成功率有所提高，并公开发布了GATE-VLAP数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在需要新的技能或对象组合的任务中表现不佳。为了解决这个问题，作者引入了原子动作切片方法，它能够将长时间范围内的演示分解成更易于处理的短小片段，从而促进规划与学习过程。

Method: 采用了一种称为原子动作切片(AAS)的技术来分割长时序示例视频，将其转化为一系列带有类型、时间跨度及置信度标签的小型原子行为单元。基于LIBERO演示视频，创建了一个包含2,124个经过验证的数据片段的新数据集。此外，还评估了不同规模模型对于多对象任务处理能力的影响，并通过对CLIP-RT+模型进行微调以测试其性能提升效果。

Result: 结果显示，使用Gemini 2.5 Pro作为更强的分段工具时，生成的动作序列与规划定义的计划高度匹配，并且即使在关键帧存在抖动的情况下也能保持良好性能；而较小规模的模型在处理涉及多个对象的任务时表现较差。在LIBERO-Goal和LIBERO-Long两个基准上，对CLIP-RT+进行微调后，任务成功率分别从94.2%提升至95.3%以及从83.8%增长到了88.8%。

Conclusion: 研究证明了原子动作切片方法的有效性，不仅提高了视觉-语言-动作模型解决复杂任务的能力，同时也为未来相关领域的发展提供了有价值的数据资源。

Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)

</details>


### [28] [A Fast Interpretable Fuzzy Tree Learner](https://arxiv.org/abs/2512.11616)
*Javier Fumanal-Idocin,Raquel Fernandez-Peralta,Javier Andreu-Perez*

Main category: cs.LG

TL;DR: 本文提出了一种从清晰规则到模糊树的经典基于树的分裂算法的适应方法，结合了贪婪算法的计算效率和模糊逻辑的可解释性优势，从而在保持竞争力的预测性能的同时，实现了可解释的语言分区，并显著提高了运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有的许多模糊规则挖掘算法无法保证有意义的语言分区和较小的规则库大小，而进化方法虽然可以生成高质量模型但计算成本过高，基于神经的方法如ANFIS则难以保留语言解释。

Method: 作者提出了一种将经典基于树的分裂算法从清晰规则转换为模糊树的方法，旨在结合贪婪算法的计算效率与模糊逻辑的可解释性优点。

Result: 实验表明，该方法在表格分类基准测试中达到了与最先进的模糊分类器相当的准确性，同时具有显著更低的计算成本，并且能够产生具有约束复杂度的更易解释的规则库。

Conclusion: 所提出的方法通过改进运行时间和维持良好预测表现的同时，解决了现有模糊系统中的可解释性和效率问题，为模糊规则系统的开发提供了新的视角。

Abstract: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public

</details>


### [29] [Bridging Streaming Continual Learning via In-Context Large Tabular Models](https://arxiv.org/abs/2512.11668)
*Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros*

Main category: cs.LG

TL;DR: 本文提出使用大型上下文表格模型（LTMs）作为流式持续学习（SCL）的自然桥梁，通过即时总结无界数据流为紧凑概要来同时解决流学习中对高效处理的需求和持续学习中对抗遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的研究社区分别处理连续学习（CL）与流学习（SL）中的挑战，但没有一个明确的算法可以同时解决这两个领域的问题。本文旨在通过使用大型上下文表格模型（LTMs）提供一种解决方案，它能够结合两个领域的优点，即长期保留知识的同时快速适应高频率的数据流。

Method: 提出了基于大型上下文表格模型（LTMs）构建流式持续学习系统的方法，并围绕两个核心原则组织：(1) 分布匹配，平衡可塑性和稳定性；(2) 分布压缩，通过多样化和检索机制控制内存大小。

Result: 展示了如何利用分而治之策略管理可塑性（在当前分布上表现良好）与稳定性（保留过去的知识）之间的张力，同时也实施了一个最小复杂度约束以促进多样化（避免存储内容冗余）和检索（必要时重新优先考虑过去信息）。

Conclusion: 大型上下文表格模型（LTMs）为实现流式持续学习提供了一种有效方法，通过即时生成紧凑的数据流摘要，既符合流学习的目标也满足了持续学习的要求。

Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

</details>


### [30] [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760)
*Aditya Tripathi,Karan Sharma,Rahul Mishra,Tapas Kumar Maiti*

Main category: cs.LG

TL;DR: 本文提出了一种名为SpectralKrum的新防御方法，该方法结合了频谱子空间估计和基于几何邻域的选择来应对联邦学习中非IID数据分布下的拜占庭客户端攻击。实验表明，在某些类型的攻击下SpectralKrum表现良好，但在标签翻转和最小-最大攻击下优势有限。


<details>
  <summary>Details</summary>
Motivation: 联邦学习架构允许客户端保留本地数据的同时参与模型训练，但这也暴露了一个基本的漏洞：拜占庭客户端可以注入任意损坏的更新，从而降低或颠覆全局模型的质量。尽管已有的鲁棒聚合方法在理想假设下提供了理论保证，当客户端数据分布不均匀（非IID）且对手能够观察或近似防御机制时，这些方法的有效性会显著下降。

Method: SpectralKrum是一种新的防御手段，它融合了频谱子空间估计与基于几何邻居选择的方法。该方法的核心见解是，尽管每个客户端存在异质性，良性优化轨迹仍集中在可以通过历史聚合估计出的低维流形附近。SpectralKrum将传入的更新投影到这个学习到的子空间中，并在此压缩坐标上应用Krum选择法，同时过滤掉那些正交残差能量超过数据驱动阈值的候选者。

Result: 通过在CIFAR-10数据集上进行的超过56,000轮训练测试显示，面对方向性和子空间感知攻击（如自适应转向、缓冲漂移）时，SpectralKrum具有竞争力；但对于标签翻转和最小-最大攻击，由于恶意更新与良性更新在频谱上难以区分，SpectralKrum的优势并不明显。

Conclusion: SpectralKrum为联邦学习中的鲁棒性提供了一种新思路，尤其是在处理非IID数据分布及特定类型攻击方面表现出色。然而，对于某些特定形式的攻击，其效果受限，提示未来研究需要进一步探索如何增强此类条件下的防御能力。

Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.

</details>


### [31] [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784)
*Etienne Boursier,Claire Boyer*

Main category: cs.LG

TL;DR: 本文开发了一个统一的基于度量的框架，用于研究单层softmax注意力机制在有限和无限提示下的行为。通过分析发现，当提示长度足够大时，softmax注意力机制可以继承线性注意力机制的分析结构，从而为研究大型提示机制中的softmax注意力层的训练动态和统计行为提供了原则性和广泛应用的工具包。


<details>
  <summary>Details</summary>
Motivation: 由于softmax注意力机制是transformer架构的核心组成部分，但其非线性结构给理论分析带来了重大挑战。因此，作者希望通过开发一个统一的、基于度量的框架来解决这个问题，以便更好地理解单层softmax注意力在不同条件下的表现。

Method: 文章首先利用独立同分布高斯输入条件下softmax操作符在无限提示限制下收敛到对底层输入令牌度量起作用的线性操作符的事实。基于此洞见，建立了关于softmax注意力输出和梯度的非渐近集中界限，并证明了这种集中性在整个训练轨迹中保持稳定。此外，在上下文线性回归情况下，使用可处理的无限提示动力学来分析有限提示长度下的训练。

Result: 研究结果表明，当提示长度足够长时，针对线性注意力优化分析可以直接转移到softmax注意力上，意味着大型提示softmax注意力继承了其线性对应物的分析结构。这为研究大型提示制度中softmax注意力层的训练动态和统计行为提供了一个有原则且广泛适用的工具箱。

Conclusion: 该研究成功地构建了一个适用于研究在有限与无限提示情形下单层softmax注意力机制的框架，揭示了随着提示长度增加，softmax注意力趋向于线性化的特性，进而使得原本为线性注意力设计的许多分析方法能够直接应用于大规模提示情境下的softmax注意力模型。

Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

</details>


### [32] [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793)
*Ahmad Shamail,Claire McWhite*

Main category: cs.LG

TL;DR: 本文提出了一种简单的几何方法，通过随机顺序添加元素并绘制其贡献来发现系统组件之间的相互作用和冗余。L-分数作为从-1（完全协同）到0（独立）再到+1（完全冗余）的连续度量，帮助区分相互作用、独立性和冗余性。该方法适用于任何可以逐步评估性能且不重复元素序列的领域。


<details>
  <summary>Details</summary>
Motivation: 为了分析系统内各组成部分之间复杂的相互作用，如某些特征或行为效果相互放大、提供冗余信息或独立贡献，本文提出一种简单直观的方法，以识别这些交互模式，并量化每个元素的贡献如何依赖于之前添加的元素。

Method: 论文中介绍了一种基于几何学的方法，通过在多次试验中随机顺序地加入元素并绘出它们的贡献。根据所形成的特定L形图案，可以直接反映互动结构。通过L-分数这一指标，可以对从完美协同到独立再到完美冗余的情况进行连续度量。此外，该方法还能够自然地揭示三者或更多元素间的高阶交互作用。

Result: 研究结果显示，当元素对的贡献被可视化为二维点云时，冗余对形成L形图案，其中只有首先添加的元素有贡献；而协同作用对则形成另一种L形图案，仅当元素共同出现时才有贡献。独立元素则表现出与顺序无关的分布。L-分数不仅能够度量成对关系，还可以通过一致的跨对关系自然地揭示三个或更多元素间的高阶交互作用。

Conclusion: 本文提出的基于几何的方法为探索任意领域内的元素间交互结构提供了一个统一的框架，该方法不受具体度量标准限制，适用于所有可以按非重复元素序列逐步评估表现的场景。

Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [33] [Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis](https://arxiv.org/abs/2512.10963)
*Zheqi Hu,Xuanjing Chen,Jinlin Hu*

Main category: cs.IR

TL;DR: 本研究提出了一种基于BERT的跨模态变压器与基于注意力的融合的多模态情绪和意图识别模型（MMEI），并将其集成到云原生个性化AIGC推荐框架中。通过处理视觉、听觉和文本模态，该模型能够学习情绪-意图表示，并据此驱动个性化内容推荐。实验结果表明，与最佳融合变换器基线相比，MMEI模型在F1分数上提高了4.3%，交叉熵损失降低了12.3%；用户级别的在线评估显示，情感驱动的推荐增加了15.2%的参与时间和11.8%的满意度评分。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容(AIGC)在音乐、视频、文学等领域的快速增长，对具有情感意识的推荐系统的需求变得越来越重要。传统推荐系统主要依赖于用户行为数据如点击、浏览或评分，而忽略了用户在与内容互动过程中的实时情绪和意图状态。为了解决这一局限性，本研究旨在开发一种能够理解用户情绪和意图的新方法，以提高推荐系统的个性化程度及用户体验。

Method: 本研究提出了一种名为MMEI的多模态情绪与意图识别模型，该模型基于BERT的跨模态变压器并通过基于注意力机制进行融合。它能同时处理三种不同形式的数据：视觉（面部表情）、听觉（语音语调）以及文本（评论或话语）。首先使用预训练编码器ViT、Wav2Vec2和BERT分别处理这三类信息，然后通过一个基于注意力机制的融合模块来学习情绪-意图表示。这些嵌入随后被用于通过上下文匹配层来进行个性化的推荐。

Result: 实验结果显示，在基准情绪数据集(AIGC-INT, MELD, CMU-MOSEI)和AIGC交互数据集上的测试中，所提出的MMEI模型相较于最好的基于融合的Transformer基线，在F1分数上提升了4.3%，并且减少了12.3%的交叉熵损失。此外，用户层面的在线评价也揭示出，情绪驱动型推荐使用户的参与时间增加了15.2%，满意度得分提高了11.8%。

Conclusion: 这项工作展示了跨模态情绪智能对于下一代AIGC生态系统的重要性，能够实现适应性强、富有同情心且情境感知的推荐体验。研究表明，通过结合多种感官输入理解用户的情绪和意图，可以显著改善AI生成内容与用户情感状态之间的匹配度，从而提升整体用户体验。

Abstract: With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.

</details>


### [34] [FAIR: Focused Attention Is All You Need for Generative Recommendation](https://arxiv.org/abs/2512.11254)
*Longtao Xiao,Haolin Zhang,Guohao Cai,Jieming Zhu,Yifan Wang,Heng Chang,Zhenhua Dong,Xiu Li,Ruixuan Li*

Main category: cs.IR

TL;DR: 提出了一种名为FAIR的生成推荐框架，通过集中注意力机制、抗噪目标和互信息最大化目标来提高用户行为序列建模的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的生成式推荐系统在对用户行为进行建模时，需要将项目离散化成多码表示，这增加了原始项目序列的长度，并且使得模型难以区分重要与不重要的上下文信息。

Method: (1) 在标准Transformer中集成了一种集中注意力机制，学习两组独立的Q和K注意力权重，并计算其差值作为最终注意力分数；(2) 引入了抗噪目标函数，旨在随机扰动下保持稳定的注意力模式；(3) 采用互信息最大化目标，指导模型识别对于下一步预测最具信息量的上下文。

Result: FAIR框架的有效性在四个公开基准测试上得到了验证，显示出了优于现有方法的表现。

Conclusion: FAIR作为一种新的生成推荐方法，通过改进注意力机制以及引入额外的目标函数，成功解决了传统Transformer模型在处理长序列及噪声数据时所面临的挑战，提高了推荐系统的性能。

Abstract: Recently, transformer-based generative recommendation has garnered significant attention for user behavior modeling. However, it often requires discretizing items into multi-code representations (e.g., typically four code tokens or more), which sharply increases the length of the original item sequence. This expansion poses challenges to transformer-based models for modeling user behavior sequences with inherent noises, since they tend to overallocate attention to irrelevant or noisy context. To mitigate this issue, we propose FAIR, the first generative recommendation framework with focused attention, which enhances attention scores to relevant context while suppressing those to irrelevant ones. Specifically, we propose (1) a focused attention mechanism integrated into the standard Transformer, which learns two separate sets of Q and K attention weights and computes their difference as the final attention scores to eliminate attention noise while focusing on relevant contexts; (2) a noise-robustness objective, which encourages the model to maintain stable attention patterns under stochastic perturbations, preventing undesirable shifts toward irrelevant context due to noise; and (3) a mutual information maximization objective, which guides the model to identify contexts that are most informative for next-item prediction. We validate the effectiveness of FAIR on four public benchmarks, demonstrating its superior performance compared to existing methods.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [Q-BAR: Blogger Anomaly Recognition via Quantum-enhanced Manifold Learning](https://arxiv.org/abs/2512.11071)
*Maida Wang*

Main category: cs.MM

TL;DR: 提出了一种名为Q-BAR的量子-经典混合框架，用于在数据稀缺情况下检测在线媒体中创作者内容的语义变异。该方法利用变分量子电路的高表达能力和参数效率，在低数据量下也能实现对语义异常的有效检测，并且相较于传统方法使用更少的可训练参数即达到了较好的检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着推荐驱动的在线媒体的发展，创作者面临的一个问题是语义突变，恶意编辑者通过保留视觉保真度同时改变原意来篡改内容。由于每位创作者可用的代表性样本数量极少（通常少于50个），为每个创作者单独训练鲁棒的检测模型变得非常具有挑战性。

Method: 本文提出了一个名为量子增强博客异常识别(Q-BAR)的量子-经典混合框架，该框架利用变分量子电路的高度表达力和参数效率来检测低数据环境下的语义异常。与传统的深度异常检测器相比，这种方法采用参数高效的量子异常检测策略，将多模态特征映射到希尔伯特空间超球体中。

Result: 在一个包含100位创作者的精选数据集上进行测试时，本研究提出的量子增强方法仅使用数百个量子参数就实现了稳健的检测性能，相比于经典基线方法显著减少了可训练参数的数量。这表明了量子机器学习技术在个性化媒体取证方面的潜力。

Conclusion: Q-BAR框架证明了即使是在数据极其有限的情况下，也能够有效地检测出针对特定创作者的语义异常情况。通过结合量子计算的优势，该方法不仅提高了检测准确性，还有效避免了过拟合问题，展示了量子机器学习应用于个性化媒体内容保护的新方向。

Abstract: In recommendation-driven online media, creators increasingly suffer from semantic mutation, where malicious secondary edits preserve visual fidelity while altering the intended meaning. Detecting these mutations requires modeling a creator's unique semantic manifold. However, training robust detector models for individual creators is challenged by data scarcity, as a distinct blogger may typically have fewer than 50 representative samples available for training. We propose quantum-enhanced blogger anomaly recognition (Q-BAR), a hybrid quantum-classical framework that leverages the high expressivity and parameter efficiency of variational quantum circuits to detect semantic anomalies in low-data regimes. Unlike classical deep anomaly detectors that often struggle to generalize from sparse data, our method employs a parameter-efficient quantum anomaly detection strategy to map multimodal features into a Hilbert space hypersphere. On a curated dataset of 100 creators, our quantum-enhanced approach achieves robust detection performance with significantly fewer trainable parameters compared to classical baselines. By utilizing only hundreds of quantum parameters, the model effectively mitigates overfitting, demonstrating the potential of quantum machine learning for personalized media forensics.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [36] [KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration](https://arxiv.org/abs/2512.11067)
*Guorui Xiao,Enhao Zhang,Nicole Sullivan,Will Hansen,Magdalena Balazinska*

Main category: cs.DB

TL;DR: KathDB 是一个结合了关系语义和基础模型对多模态数据处理能力的新系统，它在查询解析、执行及结果解释过程中包含了人机交互渠道，使用户能够跨数据模态迭代地获得可解释的答案。


<details>
  <summary>Details</summary>
Motivation: 传统数据库管理系统（DBMSs）虽然提供了强大的语义保证和高级查询优化，但编写复杂的SQL语句既困难又仅限于结构化表格。现有的多模态系统要么让用户手动使用（甚至创建）机器学习UDF，要么完全依赖黑盒LLM进行处理，牺牲了易用性或可解释性。为了解决这些问题，提出了KathDB系统。

Method: 通过整合关系型数据库的语义特性与基础模型对于文本、图像乃至视频等多模态数据的推理能力，同时引入了在查询过程中的用户与AI互动机制，使得KathDB能够在保持系统易用性的同时增强输出结果的可解释性。

Result: KathDB允许用户跨越不同类型的数据模式逐步获取到易于理解的结果，并且在整个查询流程中——从解析到执行再到最终解释——都提供了人机协作的可能性。

Conclusion: KathDB代表了一种新型数据库系统的尝试，它不仅支持多模态数据处理还增强了用户体验，特别是通过引入人机交互来提高结果的可解释性和系统的整体可用性。

Abstract: Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.

</details>


### [37] [Acyclic Conjunctive Regular Path Queries are no Harder than Corresponding Conjunctive Queries](https://arxiv.org/abs/2512.11129)
*Mahmoud Abo Khamis,Alexandru-Mihai Hurjui,Ahmet Kara,Dan Olteanu,Dan Suciu*

Main category: cs.DB

TL;DR: 本文提出了一种针对无环连结正则路径查询（CRPQ）的输出敏感算法，其复杂度取决于输入大小、输出大小以及查询的一个著名参数“自由连通分数超树宽”。该算法不仅改进了最近提出的针对无环CRPQ的输出敏感算法的复杂度，并且对于给定的无环CRPQ Q而言，其复杂度与对应的连接查询(CQ)的最佳已知输出敏感复杂度相匹配。这意味着除非在非递归CQs的输出敏感评估方面取得进展，否则很难进一步优化无环CRPQs的处理效率。结果表明，至少从输出敏感分析的角度来看，无环CRPQs中的递归方面并没有增加额外的复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进现有针对无环连结正则路径查询（CRPQ）的输出敏感算法的计算效率。通过引入新的算法来减少处理这类查询时的时间复杂度，特别是当涉及到大数据集或产生大量输出的情况时。此外，探索了CRPQs与等效的递归Datalog程序之间的关系，试图揭示它们在复杂性方面的差异。

Method: 提出了一种新的输出敏感算法用于评估无环CRPQs，该算法的复杂度基于输入大小、输出大小及一个称为“自由连通分数超树宽”的查询参数进行衡量。此方法旨在提供一种更有效的解决方案，以应对之前算法在处理特定类型查询时遇到的性能瓶颈。

Result: 新算法成功地提高了无环CRPQs的处理效率，其表现优于现有方法。更重要的是，对于任何给定的无环CRPQ Q，本算法实现了与相应CQ（即结构相同但每个RPQ被替换为二元原子或两个二元原子的连接）最佳已知输出敏感复杂度相匹配的效果。这表明，在不改进非递归CQs的输出敏感评估技术前提下，难以进一步降低无环CRPQs的计算复杂度。

Conclusion: 这项工作证明了即使面对包含递归特性的查询如CRPQs，也有可能设计出具有竞争力的输出敏感算法，其性能可与非递归形式相当。这为进一步研究如何高效处理更加复杂的数据库查询提供了新的视角和可能性。

Abstract: We present an output-sensitive algorithm for evaluating an acyclic Conjunctive Regular Path Query (CRPQ). Its complexity is written in terms of the input size, the output size, and a well-known parameter of the query that is called the "free-connex fractional hypertree width". Our algorithm improves upon the complexity of the recently introduced output-sensitive algorithm for acyclic CRPQs. More notably, the complexity of our algorithm for a given acyclic CRPQ Q matches the best known output-sensitive complexity for the "corresponding" conjunctive query (CQ), that is the CQ that has the same structure as the CRPQ Q except that each RPQ is replaced with a binary atom (or a join of two binary atoms). This implies that it is not possible to improve upon our complexity for acyclic CRPQs without improving the state-of-the-art on output-sensitive evaluation for acyclic CQs. Our result is surprising because RPQs, and by extension CRPQs, are equivalent to recursive Datalog programs, which are generally poorly understood from a complexity standpoint. Yet, our result implies that the recursion aspect of acyclic CRPQs does not add any extra complexity on top of the corresponding (non-recursive) CQs, at least as far as output-sensitive analysis is concerned.

</details>


### [38] [Benchmarking RL-Enhanced Spatial Indices Against Traditional, Advanced, and Learned Counterparts](https://arxiv.org/abs/2512.11161)
*Guanli Liu,Renata Borovica-Gajic,Hai Lan,Zhifeng Bao*

Main category: cs.DB

TL;DR: 该研究首次为强化学习增强的空间索引（RLESIs）提供了模块化和可扩展的基准测试框架，通过对比传统、高级及学习型空间索引发现，在查询效率和构建成本上RLESIs表现不佳，尽管它们在架构兼容性上有潜力，但高昂的调优成本和有限的泛化能力限制了其实用性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一实现和全面评估，特别是基于磁盘设置的情况下，强化学习增强的空间索引（RLESIs）的实际好处尚不清楚。

Method: 基于现有的空间索引库开发了一个模块化且可扩展的基准测试框架，该框架将索引训练与构建分离，支持参数调整，并允许与传统、先进以及学习型空间索引进行一致比较。对12种代表性空间索引进行了跨六个数据集和多样化工作负载（包括点查询、范围查询、kNN、空间连接及混合读写查询）的评估。

Result: 虽然经过调优后RLESIs可以降低查询延迟，但在查询效率和索引构建成本方面始终不如学习型空间索引和高级变体。

Conclusion: 尽管RLESIs展示了良好的架构兼容性前景，但其高昂的调优成本和有限的泛化能力阻碍了实际应用。

Abstract: Reinforcement learning has recently been used to enhance index structures, giving rise to reinforcement learning-enhanced spatial indices (RLESIs) that aim to improve query efficiency during index construction. However, their practical benefits remain unclear due to the lack of unified implementations and comprehensive evaluations, especially in disk-based settings.
  We present the first modular and extensible benchmark for RLESIs. Built on top of an existing spatial index library, our framework decouples index training from building, supports parameter tuning, and enables consistent comparison with traditional, advanced, and learned spatial indices.
  We evaluate 12 representative spatial indices across six datasets and diverse workloads, including point, range, kNN, spatial join, and mixed read/write queries. Using latency, I/O, and index statistics as metrics, we find that while RLESIs can reduce query latency with tuning, they consistently underperform learned spatial indices and advanced variants in both query efficiency and index build cost. These findings highlight that although RLESIs offer promising architectural compatibility, their high tuning costs and limited generalization hinder practical adoption.

</details>


### [39] [A Cross-Chain Event-Driven Data Infrastructure for Aave Protocol Analytics and Applications](https://arxiv.org/abs/2512.11363)
*Junyi Fan,Li Sun*

Main category: cs.DB

TL;DR: 本文构建了首个全面的Aave V3跨链事件级数据基础设施，覆盖六大EVM兼容链从部署至2025年10月的数据，收集并解码了八种核心事件类型，生成超过5000万条结构化记录。通过开源Python流水线确保了数据处理的严格时序性和可再现性，为去中心化借贷市场的资金流动、利率动态等提供基础研究资源。


<details>
  <summary>Details</summary>
Motivation: 尽管去中心化借贷协议如Aave V3管理着超过100亿美元的价值总额，但针对其的研究受到缺乏标准化跨链事件级别数据集的限制。为了填补这一空白，促进对去中心化借贷市场及系统性风险的理解与研究。

Method: 开发了一个开放源代码的Python流水线，用于收集和完全解码来自六个主要EVM兼容链（以太坊、Arbitrum、Optimism、Polygon、Avalanche和Base）上的八种核心事件类型，并加入区块元数据和美元估值信息。此过程采用了动态批处理大小调整和自动分片技术来保证数据处理的高效性和准确性。

Result: 成功创建了一个包含超过5000万个结构化记录的数据集，这些记录按照严格的时序顺序排列且具有完全可复现性。该公开可用的数据集支持对资本流向、利率变动、清算连锁反应以及跨链用户行为进行细致分析。

Conclusion: 本研究提供的数据基础设施为未来关于去中心化借贷市场及其潜在系统性风险的研究奠定了坚实的基础，有助于深化我们对于此类新兴金融体系运作机制的认识。

Abstract: Decentralized lending protocols, exemplified by Aave V3, have transformed financial intermediation by enabling permissionless, multi-chain borrowing and lending without intermediaries. Despite managing over $10 billion in total value locked, empirical research remains severely constrained by the lack of standardized, cross-chain event-level datasets.
  This paper introduces the first comprehensive, event-driven data infrastructure for Aave V3 spanning six major EVM-compatible chains (Ethereum, Arbitrum, Optimism, Polygon, Avalanche, and Base) from respective deployment blocks through October 2025. We collect and fully decode eight core event types -- Supply, Borrow, Withdraw, Repay, LiquidationCall, FlashLoan, ReserveDataUpdated, and MintedToTreasury -- producing over 50 million structured records enriched with block metadata and USD valuations.
  Using an open-source Python pipeline with dynamic batch sizing and automatic sharding (each file less than or equal to 1 million rows), we ensure strict chronological ordering and full reproducibility. The resulting publicly available dataset enables granular analysis of capital flows, interest rate dynamics, liquidation cascades, and cross-chain user behavior, providing a foundational resource for future studies on decentralized lending markets and systemic risk.

</details>


### [40] [Bridging Textual Data and Conceptual Models: A Model-Agnostic Structuring Approach](https://arxiv.org/abs/2512.11403)
*Jacques Chabin,Mirian Halfeld Ferrari,Nicolas Hiot*

Main category: cs.DB

TL;DR: 本文提出了一种将文本数据自动结构化为模型无关模式的方法，能够与任何数据库模型对齐。通过迭代树重写和语法提取生成模式及其实例，并以临床医学案例作为概念验证。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是开发一种可以将文本数据自动转换为模型无关模式的技术，以便能与任意数据库模型进行对齐。

Method: 该方法首先将文本数据表示为语义丰富的语法树，然后通过迭代树重写和基于元语法\metaG指导下的语法提取来细化这些树。

Result: 结果表明，该方法可以通过临床医学案例作为概念验证来展示其适用性。

Conclusion: 本文介绍了一种新的、自动化的技术来处理文本数据并将其转化为通用的数据模式，这项技术在医疗领域的应用证明了其有效性。

Abstract: We introduce an automated method for structuring textual data into a model-agnostic schema, enabling alignment with any database model. It generates both a schema and its instance. Initially, textual data is represented as semantically enriched syntax trees, which are then refined through iterative tree rewriting and grammar extraction, guided by the attribute grammar meta-model \metaG. The applicability of this approach is demonstrated using clinical medical cases as a proof of concept.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 本文通过比较自动和手动创建的测试用例在SBFL得分及代码覆盖率上的表现，探讨了两者在故障定位支持方面的优劣。研究发现自动生成的测试虽然在分支覆盖率上更优，但在深度嵌套结构代码中的SBFL得分较低。


<details>
  <summary>Details</summary>
Motivation: 软件开发中手动创建测试用例耗时较多，因此需要更高效的测试方法来减轻开发者负担。尽管已有多种自动化测试生成工具被开发出来，并且也有研究评估了这些工具产生的测试的有效性，但大多数研究主要集中在覆盖度量上，很少有研究考察这些测试对故障定位的支持程度，特别是通过变异测试引入的人工故障。

Method: 本研究采用了一种新的评价指标——SBFL得分，该指标可以显示使用SBFL技术进行故障定位的准确性。通过对比自动生成的测试与手动创建的测试之间的SBFL得分和代码覆盖率，旨在揭示这两种测试方法各自的优缺点。

Result: 实验结果表明，自动生成的测试比手动创建的测试获得了更高的分支覆盖率，但对于具有深层嵌套结构的代码而言，它们的SBFL得分较低。

Conclusion: 研究表明，自动生成的测试用例在提高代码覆盖率方面优于手动创建的测试，但在处理复杂代码结构时，对于准确地定位错误的能力则不如后者。这为如何有效地结合两种测试方法提供了指导。

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [42] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 本研究探索了19种小型量化LLM（参数少于200亿）在C代码到Java代码自动翻译任务中的表现。通过结合使用抽象语法树(AST)和基于规则的提示策略，研究发现模型间存在明显的性能分层：大部分模型完全无法生成可运行的Java代码；少数模型虽然能生成可运行代码但存在严重的语义错误；只有三种模型表现良好，但也难以处理复杂的C语言概念。


<details>
  <summary>Details</summary>
Motivation: C代码到Java代码的自动化转换是一项充满挑战的任务，主要由于编程范式、内存模型及数据类型之间的根本差异。这项研究旨在评估小型量化语言模型在此转换任务中的有效性和局限性。

Method: 采用一种新颖的混合流程，利用抽象语法树进行语义分解，并实施高度约束的基于规则的提示策略来测试19种小型量化LLM的表现。

Result: 研究结果显示，模型性能明显分为三个层次：大多数模型（第三级）无法生成任何可运行的基础Java代码；中等层级（第二级）的几个模型能够生成可运行代码，但存在严重语义错误；仅有三款模型（第一级）表现较为理想，通过了超过50%的测试用例，但对于复杂的C语言特性仍面临困难。

Conclusion: 尽管某些高级量化模型在C至Java代码转换任务上表现出了一定的能力，但所有测试模型都显示出对于复杂C语言概念处理上的局限性，这表明当前量化模型的理解能力存在着硬性上限。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [43] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 本研究首次全面评估了差分隐私（DP）在代码大模型（CodeLLMs）中的有效性。研究表明，DP能够显著减少CodeLLMs的记忆行为，同时保持甚至提升其代码生成能力，并且对训练效率和能耗影响不大，为保护隐私的CodeLLMs训练提供了一个实用的选择。


<details>
  <summary>Details</summary>
Motivation: 尽管CodeLLMs在生成代码片段、文档和测试用例方面展示了非凡的能力，但它们可能会无意中记忆并再现训练数据中的片段，这带来了隐私泄露和知识产权侵犯的风险。这些风险限制了CodeLLMs在敏感领域的应用以及只能使用公开的数据源进行训练。为了减轻记忆风险而不牺牲任务性能，研究引入了差分隐私技术到CodeLLMs中。

Method: 首先识别并理解CodeLLMs在微调过程中产生记忆行为的根本原因。接着，通过实证评估差分隐私对于缓解记忆现象的影响，同时保持代码生成能力的效果。

Result: 研究发现表明，差分隐私能大幅降低各种类型代码片段的记忆问题，尤其是那些最容易被记住的片段类型也得到了最有效的缓解。此外，观察到差分隐私虽然略微增加了困惑度，但保持甚至提高了CodeLLMs的代码生成能力。最后，分析了差分隐私对训练效率及能源消耗的影响，结果显示其对训练时间和能量使用没有显著影响。

Conclusion: 差分隐私是一种有效的方法来减轻CodeLLMs的记忆风险，同时几乎不损害模型的功能性。它使得在实践中采用差分隐私成为可能，而不会显著地牺牲模型的实用性，因此为希望保护隐私的同时开发高效CodeLLMs的研究者们提供了新的方向。

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [44] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 本文提出了一种任务调度算法，该算法通过明确提高CPU、磁盘和I/O等资源的利用率来增加活跃资源的使用率，从而实现更高的能源效率。仿真结果表明，与现有的MaxUtil算法相比，该算法能够消耗更少的能量。


<details>
  <summary>Details</summary>
Motivation: 随着云计算服务应用的增多，其所消耗的能量也显著增加。当前大部分能源有限且对环境有温室效应。因此，迫切需要减少云服务提供商所消耗的能量，并且研究社区面临开发节能算法的挑战。

Method: 提出了一个基于适应度值的任务调度算法，该适应度值是CPU、磁盘和I/O利用率以及任务处理时间的函数。

Result: 通过合成数据集对提出的算法及现有MaxUtil算法进行了广泛的仿真测试。结果显示，所提出的算法比MaxUtil算法更加节能，消耗的能量更少。

Conclusion: 新提出的任务调度算法在提高资源利用效率的同时，也实现了更好的能源效率，为云服务提供了一个更为节能的选择。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [45] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX是一个用于为新兴加速平台生成功能正确的Triton PyTorch ATen内核的AI系统，它结合了开源大型语言模型、自定义检查器、即时编译和基于PyTorch OpInfo的测试框架，并且成功地为481个独特的ATen操作符生成了通过所有对应PyTorch OpInfo测试的内核和封装器。


<details>
  <summary>Details</summary>
Motivation: 开发TritorX的动机是提供一个能够大规模生成适用于新加速平台的功能正确Triton PyTorch ATen内核的AI系统。与之前专注于有限高使用率内核性能的方法不同，TritorX旨在覆盖整个操作集，强调正确性和普遍性，包括各种数据类型、形状和参数模式。

Method: TritorX整合了开源的大规模语言模型、定制化的代码检查工具（linter）、即时编译技术以及基于PyTorch OpInfo的测试框架来实现其目标。该系统不仅支持真实Meta Training and Inference Accelerator (MTIA)硬件，还能够在下一代设备的硬件仿真环境中运行。

Result: 实验结果表明，TritorX成功为481种不同的ATen操作符生成了内核及封装程序，这些生成的内容全部通过了相应的超过20,000项PyTorch OpInfo测试。

Conclusion: TritorX展示出了一条可行之路，即在一夜之间为新的加速平台生成完整的PyTorch ATen后端，这极大地促进了软件对新硬件的支持速度。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [46] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文全面分析了实时迁移技术，重点关注容器和虚拟机迁移技术的现状及其实用性之间的差异，并探讨了迁移目标与操作限制对现有技术的影响。通过概述当前的技术挑战并为未来的研究和发展方向提供指导，旨在为爱好者提供有价值的资源，并促进实时迁移技术及其在不同计算环境中的实际应用的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的关于实时迁移技术的综述往往忽略了在实际应用场景中至关重要的技术和实践挑战。本研究旨在填补这一空白，通过综合分析来解决这些问题。

Method: 通过对现有文献的回顾整合，以及对实时迁移技术在多个维度上的深入分析，包括迁移技术、迁移单元和基础设施特征等方面进行考察。

Result: 揭示了尽管努力使实时迁移变得更为普及，但其依赖于多种系统因素的特点仍给实施带来了一定难度。在某些情况下，复杂性和资源需求可能超过带来的好处，使得实施难以证明其合理性。此外，还发现了基于容器和虚拟机的迁移技术之间存在采用率上的差距。

Conclusion: 该工作不仅为热衷于实时迁移技术的人士提供了宝贵的参考资料，同时也为推动相关技术的进步及其在多样化计算环境下的实际部署做出了贡献。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [47] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 本研究针对GPU集群在处理异构工作负载时存在的利用率低的问题，提出并评估了三种动态调度器：混合优先级（HPS）、预测回填（PBS）和智能批处理（SBS）。这些调度器旨在提高多租户GPU集群中的利用率、公平性和整体吞吐量。与静态基线相比，动态调度器显著提高了性能，在利用率、吞吐量和减少饥饿方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 实际部署中GPU集群的平均利用率接近50%，这主要是由于碎片化、异构工作负载以及静态调度策略的局限性造成的。为了改善这一情况，研究提出了几种专门设计的动态调度器来提升效率。

Method: 研究者设计了三个专业化的动态调度器：Hybrid Priority (HPS)、Predictive Backfill (PBS) 和 Smart Batch (SBS)，并通过模拟1,000个AI作业在一个拥有64个GPU、8节点的集群上进行测试，该集群包含了训练、推理和研究工作的现实混合。

Result: HPS达到了最高的利用率(78.2%)和吞吐量(每小时25.8个工作)，同时在动态方法中具有最低的公平性差异(457)，将饥饿现象减少到只有12个作业。PBS改善了碎片处理问题，达到76.1%的利用率；而SBS对于结构相似的工作提高了效率，达到74.6%的利用率。

Conclusion: 实验结果表明，目标明确且透明的调度策略能够显著提高异构AI集群中GPU的效率，并为未来的生产调度框架提供了实用基础。

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [48] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 本文比较了集中式分层联邦学习（HFL）与去中心化聚合联邦学习（AFL）和去中心化持续联邦学习（CFL）架构，通过Fashion MNIST和MNIST数据集的评估表明，在精度、召回率、F1分数以及平衡准确度方面，AFL和CFL优于HFL。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习领域的快速发展，特别是去中心化方法的进步，研究者们希望找到比传统的集中式方法更优的学习架构。集中式分层联邦学习面临着通信瓶颈和隐私问题，而AFL和CFL通过将计算和聚合过程分散到多个设备上提供了一种有前景的替代方案。

Method: 本研究通过对Fashion MNIST和MNIST数据集进行实验，对比分析了HFL、AFL和CFL这三种不同的联邦学习架构在性能上的差异。

Result: 实验结果显示，去中心化的AFL和CFL方法在精度、召回率、F1分数及平衡准确度等多个指标上均优于集中式的HFL方法。

Conclusion: 研究表明，去中心化聚合机制对于实现跨分布式设备的合作模型训练非常重要，并且去中心化方法如AFL和CFL能够提供更好的性能。这项比较研究为联邦学习领域向去中心化方法转变提供了有价值的见解。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [49] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 本文提出了三种互补的方法来实现GPU原生编译，以消除AI代码生成系统中的CPU-GPU数据传输瓶颈。方法包括：适应GPU执行的并行传统编译、使用序列到序列翻译和概率验证的神经编译以及结合两种策略的混合架构。通过理论分析展示了这些方法在减少延迟和能耗方面的潜力，其中传统GPU编译可提供2-5倍改进，神经编译可达10-100倍加速，而混合方法则提供了具有正确性保证的实际部署路径。


<details>
  <summary>Details</summary>
Motivation: 当前的人工智能代码生成系统由于在编译、执行和测试阶段CPU与GPU之间的数据传输而面临着显著的延迟瓶颈。

Method: 建立了三种互补方法的理论基础来实现GPU原生编译，从而消除这些数据传输：（1）适用于GPU执行的并行传统编译；（2）利用学习的序列到序列转换加上概率验证的神经编译；（3）结合这两种策略的混合架构。

Result: 推导出的延迟和能量边界表明，在代码迭代周期中潜在的速度提升为10至100倍。分析显示，传统的GPU编译通过消除数据传输可提供2至5倍的改进，神经编译通过大规模并行处理可达到10至100倍的速度提升，而混合方法则提供了具有正确性保障的实际部署途径。

Conclusion: 正式化了允许用平行探索换取编译准确性的一个概率验证框架，并讨论了这对自我改进的人工智能系统及未来模拟计算基底的影响。

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [50] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux, a cluster scheduling framework, improves the efficiency of reinforcement learning (RL) post-training by reclaiming idle time through cross-cluster orchestration and co-execution group abstraction, leading to 1.84x cost efficiency over standard methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency caused by strict synchronization requirements in on-policy reinforcement learning algorithms, which leads to idling of one cluster while the other is active, thus reducing hardware utilization and increasing costs.

Method: RollMux introduces a co-execution group abstraction that divides the cluster into isolated domains, enabling a two-tier scheduling system. It uses an inter-group scheduler for job placement with conservative stochastic planning and an intra-group scheduler for optimal round-robin scheduling. The method also ensures model states are cached for quick context switching.

Result: RollMux was tested on a large-scale setup with 328 H20 and 328 H800 GPUs, showing a 1.84 times improvement in cost efficiency compared to traditional disaggregation approaches and a 1.38 times improvement over advanced co-located baselines, while maintaining full service level objective (SLO) attainment.

Conclusion: Through innovative scheduling and resource management, RollMux significantly enhances the cost efficiency and performance of reinforcement learning systems, especially in scenarios where rollout and training phases are physically separated.

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [51] [FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634)
*Elia Palme,Juan Pablo Dorsch,Ali Khosravi,Giovanni Pizzi,Francesco Pagnamenta,Andrea Ceriani,Eirini Koutsaniti,Rafael Sarmiento,Ivano Bonesana,Alejandro Dabin*

Main category: cs.DC

TL;DR: FirecREST v2, an upgraded open-source RESTful API for HPC resources, offers a 100x performance boost over its predecessor. The paper covers the redesign process, focusing on enhanced security and high throughput, and presents systematic performance testing results along with key design changes. It also discusses future improvement opportunities.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to present the significant advancements made in FirecREST v2, particularly in terms of performance, security, and throughput, as well as to share the lessons learned from the ground-up redesign process. The authors aim to provide valuable insights into overcoming common challenges faced by proxy-based APIs with heavy I/O operations.

Method: The method involves a detailed exploration of the redesign process of FirecREST, including the implementation of enhanced security measures and high throughput capabilities. The authors also conducted systematic performance testing to identify and address bottlenecks, and documented the key architectural and design modifications that led to the notable performance improvements.

Result: The result is a comprehensive overview of how the new version of FirecREST achieves a 100x performance improvement. This includes specific details on the design and architectural changes that were critical to these gains, supported by independent peer validation. Additionally, the paper outlines potential areas for further enhancements.

Conclusion: In conclusion, the paper successfully demonstrates the substantial performance, security, and throughput upgrades in FirecREST v2, providing a blueprint for similar projects. By sharing their experiences and findings, the authors not only highlight the current achievements but also set the stage for ongoing development and optimization within the HPC community.

Abstract: Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.
  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.

</details>


### [52] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO, a new video analytics framework, optimizes resource usage for continuous learning by grouping cameras with similar data drift and sharing models among them, thus reducing compute and communication costs while improving retraining accuracy or supporting more cameras.


<details>
  <summary>Details</summary>
Motivation: 现有的为每个摄像头单独重新训练模型以应对数据漂移的方法存在较高的计算和通信成本，这使得该方法难以扩展。

Method: ECCO框架通过轻量级分组算法动态形成并更新摄像头组、动态分配GPU资源的GPU分配器以及根据分配的GPU资源配置帧采样和带宽共享的传输控制器来减少计算和通信成本。

Result: 与领先的基线相比，ECCO在使用相同的计算和通信资源的情况下提高了6.7%-18.1%的再训练准确性，或者在保持相同准确性的同时支持多达3.3倍的并发摄像头数量。

Conclusion: ECCO提供了一种资源高效的方式来进行连续学习，它能够显著降低视频分析中的计算和通信成本，同时提高模型再训练的准确性或支持更多的摄像头。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>
