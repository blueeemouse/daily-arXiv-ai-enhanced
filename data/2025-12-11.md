<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.LG](#cs.LG) [Total: 43]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Meta Lattice: Model Space Redesign for Cost-Effective Industry-Scale Ads Recommendations](https://arxiv.org/abs/2512.09200)
*Liang Luo,Yuxin Chen,Zhengyu Zhang,Mengyue Hang,Andrew Gu,Buyun Zhang,Boyang Liu,Chen Chen,Chengze Fan,Dong Liang,Fan Yang,Feifan Gu,Huayu Li,Jade Nie,Jiayi Xu,Jiyan Yang,Jongsoo Park,Laming Chen,Longhao Jin,Qianru Li,Qin Huang,Shali Jiang,Shiwen Shen,Shuaiwen Wang,Sihan Zeng,Siyang Yuan,Tongyi Tang,Weilin Zhang,Wenjun Wang,Xi Liu,Xiaohan Wei,Xiaozhen Xia,Yuchen Hao,Yunlong He,Yasmine Badr,Zeliang Chen,Maxim Naumov,Yantao Yao,Wenlin Chen,Santanu Kolay,GP Musumeci,Ellie Dingqiao Wen*

Main category: cs.IR

TL;DR: 提出了Lattice框架，通过模型空间的重新设计解决了多领域、多目标学习中的数据碎片化和基础设施成本上升问题，实现了质量和成本效率的重大改进。


<details>
  <summary>Details</summary>
Motivation: 面对产品、界面、政策和法规快速变化带来的挑战，特别是在行业规模上部署最先进的推荐模型时遇到的数据碎片化和基础设施成本增加的问题，需要一种新的方法来提高推荐系统的性能并降低成本。

Method: 提出了一种名为Lattice的推荐框架，该框架以模型空间重新设计为核心，超越了单纯模型与学习目标上的多领域多目标(MDMO)学习。它综合运用跨领域知识共享、数据整合、模型统一、提炼以及系统优化等手段应对上述挑战。

Result: 在Meta的应用表明，Lattice带来了10%的收入驱动顶级指标增长、用户满意度提升11.5%、转化率提高了6%，同时节省了20%的容量。

Conclusion: Lattice作为一个创新性的推荐框架，在解决行业规模下部署先进推荐模型所面临的数据碎片化和高昂基础设施成本方面表现出了显著的优势，不仅提高了服务质量还降低了运营成本。

Abstract: The rapidly evolving landscape of products, surfaces, policies, and regulations poses significant challenges for deploying state-of-the-art recommendation models at industry scale, primarily due to data fragmentation across domains and escalating infrastructure costs that hinder sustained quality improvements.
  To address this challenge, we propose Lattice, a recommendation framework centered around model space redesign that extends Multi-Domain, Multi-Objective (MDMO) learning beyond models and learning objectives. Lattice addresses these challenges through a comprehensive model space redesign that combines cross-domain knowledge sharing, data consolidation, model unification, distillation, and system optimizations to achieve significant improvements in both quality and cost-efficiency.
  Our deployment of Lattice at Meta has resulted in 10% revenue-driving top-line metrics gain, 11.5% user satisfaction improvement, 6% boost in conversion rate, with 20% capacity saving.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [2] [CUBE: A Cardinality Estimator Based on Neural CDF](https://arxiv.org/abs/2512.09622)
*Xiao Yan,Tiezheng Nie,Boyang Fang,Derong Shen,Kou Yue,Yu Ge*

Main category: cs.DB

TL;DR: 本文提出了一种基于累积分布函数(CDF)的新方法来进行基数估计，该方法无需采样或积分计算即可确保准确且可预测的估计结果，并通过合并计算加速推理过程，实现快速且几乎恒定的推理速度，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于数据驱动的方法虽然提高了估计准确性，但无法同时保证低推理延迟和可扩展性，随着数据维度的增长，优化时间甚至可能超过实际查询执行时间。此外，采用概率模型进行推理时，由于采样或积分过程会导致不可预测的估计结果，影响了稳定性，进而导致查询执行性能不稳定，增加了数据库调优的难度。

Method: 提出了一种基于累积分布函数(CDF)的基数估计新方法，这种方法不需要依赖于采样或积分来计算范围查询基数，从而确保了估计结果既准确又可预测。为了进一步提高效率，研究还引入了通过合并计算来加速推理过程的技术，即使在数据维度增加的情况下也能维持较高的估计精度与接近常数级别的快速推理速度。

Result: 实验结果显示，所提出的方法比当前最先进的数据驱动型基数估计器快10倍以上，在提高估计准确性的同时显著提升了推理速度，展现出优秀的维度扩展能力，非常适合实际数据库应用环境。

Conclusion: 本研究开发的一种基于CDF的基数估计方法不仅解决了现有技术中存在的估计准确性与推理延迟之间的矛盾问题，而且通过创新性的计算合并策略实现了对大规模、高维数据集的有效处理，为数据库系统的性能优化提供了新的解决方案。

Abstract: Modern database optimizer relies on cardinality estimator, whose accuracy directly affects the optimizer's ability to choose an optimal execution plan. Recent work on data-driven methods has leveraged probabilistic models to achieve higher estimation accuracy, but these approaches cannot guarantee low inference latency at the same time and neglect scalability. As data dimensionality grows, optimization time can even exceed actual query execution time. Furthermore, inference with probabilistic models by sampling or integration procedures unpredictable estimation result and violate stability, which brings unstable performance with query execution and make database tuning hard for database users. In this paper, we propose a novel approach to cardinality estimation based on cumulative distribution function(CDF), which calculates range query cardinality without sampling or integration, ensuring accurate and predictable estimation results. With inference acceleration by merging calculations, we can achieve fast and nearly constant inference speed while maintaining high accuracy, even as dimensionality increases, which is over 10x faster than current state-of-the-art data-driven cardinality estimator. This demonstrates its excellent dimensional scalability, making it well-suited for real-world database applications.

</details>


### [3] [Exqutor: Extended Query Optimizer for Vector-augmented Analytical Queries](https://arxiv.org/abs/2512.09695)
*Hyunjoon Kim,Chaerim Lim,Hyeonjun An,Rathijit Sen,Kwanghyun Park*

Main category: cs.DB

TL;DR: 本文提出了一种扩展的查询优化器Exqutor，专为向量增强的分析查询设计，通过精确基数估计技术或基于采样的方法来提高向量搜索组件的估计准确性，从而优化查询计划。实验显示，在pgvector、VBASE和DuckDB中集成该框架后，对于向量增强型分析查询的性能提高了多达四个数量级。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成（RAG）领域的发展，特别是表增强生成对结构化数据的整合，结合表格与向量搜索的工作负载变得越来越普遍。然而，由于向量搜索组件的基数估计不准确，导致难以高效执行这类查询并产生次优的查询计划。

Method: 提出了Exqutor，一个针对向量增强分析查询设计的扩展查询优化器。当存在向量索引时（例如HNSW、IVF），Exqutor利用精确基数查询优化技术提高估计精度；在缺乏此类索引的情况下，则采用基于采样的方法，并自适应调整样本大小以平衡估计精度与抽样开销。

Result: 通过将提出的框架集成到pgvector、VBASE及DuckDB中，对于向量增强型分析查询，观察到了高达四个数量级的性能提升。

Conclusion: Exqutor有效地解决了向量搜索组件因基数估计不准而导致的问题，无论是通过使用精确基数估计还是动态调整样本大小的策略，都显著提升了向量增强型分析查询的整体性能。

Abstract: Vector similarity search is becoming increasingly important for data science pipelines, particularly in Retrieval-Augmented Generation (RAG), where it enhances large language model inference by enabling efficient retrieval of relevant external knowledge. As RAG expands with table-augmented generation to incorporate structured data, workloads integrating table and vector search are becoming more prevalent. However, efficiently executing such queries remains challenging due to inaccurate cardinality estimation for vector search components, leading to suboptimal query plans. In this paper, we propose Exqutor, an extended query optimizer for vector-augmented analytical queries. Exqutor is a pluggable cardinality estimation framework designed to address this issue, leveraging exact cardinality query optimization techniques to enhance estimation accuracy when vector indexes (e.g., HNSW, IVF) are available. In scenarios lacking these indexes, we employ a sampling-based approach with adaptive sampling size adjustment, dynamically tuning the sample size to balance estimation accuracy and sampling overhead. This allows Exqutor to efficiently approximate vector search cardinalities while minimizing computational costs. We integrate our framework into pgvector, VBASE, and DuckDB, demonstrating performance improvements of up to four orders of magnitude on vector-augmented analytical queries.

</details>


### [4] [Baseline: Operation-Based Evolution and Versioning of Data](https://arxiv.org/abs/2512.09762)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.DB

TL;DR: Baseline是一个支持多维度变化的丰富结构化数据平台，基于操作差异技术来管理包括重构和模式更改在内的高级操作。它提供了一种基于操作的数据版本控制方法，即使在发生结构转换的情况下也能进行细粒度的差异比较和合并。此外，该平台还推测性地执行分支上的未来时间线以实现查询操作化，并且能够自动适应模式变更。


<details>
  <summary>Details</summary>
Motivation: 传统的版本控制系统难以处理数据结构中的细粒度变化，特别是在经历如模式更改这样的结构性转换时。Baseline旨在通过引入一种新的数据管理方式——操作差异，来解决这一问题，从而简化用户对版本控制的理解与使用。

Method: Baseline采用了名为'操作差异'的新技术，允许系统按照包含重构和模式更改等在内的高层级操作来管理数据。这种方法被用来构建一个基于操作形式的数据结构版本控制系统。另外，研究者还提出了将查询转化为一系列模式和数据操作序列的概念，并在一个包含选择和连接的查询语言片段上发展了这个想法。

Result: Baseline成功实现了对于编程语言和关系数据库中使用的数据结构的版本控制，并且即使存在模式更改也可以进行细粒度的差异比较和合并。此外，通过将查询操作化为特定的未来时间线并作为当前状态的一个分支执行，可以使得查询能够免费适应模式变更。

Conclusion: Baseline项目开发了解决最近一篇论文中所确定的八个模式演变挑战问题中的四个。这表明了操作差异技术在应对复杂数据结构变化方面具有潜力。

Abstract: Baseline is a platform for richly structured data supporting change in multiple dimensions: mutation over time, collaboration across space, and evolution through design changes. It is built upon Operational Differencing, a new technique for managing data in terms of high-level operations that include refactorings and schema changes. We use operational differencing to construct an operation-based form of version control on data structures used in programming languages and relational databases.
  This approach to data version control does fine-grained diffing and merging despite intervening structural transformations like schema changes. It offers users a simplified conceptual model of version control for ad hoc usage: There is no repo; Branching is just copying. The informaton maintained in a repo can be synthesized more precisely from the append-only histories of branches. Branches can be flexibly shared as is commonly done with document files, except with the added benefit of diffing and merging.
  We conjecture that queries can be operationalized into a sequence of schema and data operations. We develop that idea on a query language fragment containing selects and joins.
  Operationalized queries are represented as a future timeline that is speculatively executed as a branch off of the present state, returning a value from its hypothetical future. Operationalized queries get rewritten to accommodate schema change "for free" by the machinery of operational differencing.
  Altogether we develop solutions to four of the eight challenge problems of schema evolution identified in a recent paper.

</details>


### [5] [Fast Factorized Learning: Powered by In-Memory Database Systems](https://arxiv.org/abs/2512.09836)
*Bernhard Stöckl,Maximilian E. Schüle*

Main category: cs.DB

TL;DR: 本研究实现了基于内存数据库系统中的因子化学习技术，并通过开源实现展示了在内存数据库系统上进行线性回归学习时，相比非因子化学习性能提升70%，相比基于磁盘的数据库系统性能提升了100倍。


<details>
  <summary>Details</summary>
Motivation: 由于先前工作缺乏公开代码，导致无法在内存数据库系统中重现使用共因子计算的实验。本研究旨在描述如何在内存数据库环境中利用共因子进行数据库内因子化学习的实现。

Method: 该研究开发了一个开源实现，用于基于因子化连接（factorized joins）在PostgreSQL（作为基于磁盘的数据库系统）和HyPer（作为内存引擎）上学习线性回归。

Result: 评估表明，在内存数据库系统上采用因子化学习比非因子化学习提高了70%的性能，而与基于磁盘的数据库系统相比，性能提高了100倍。

Conclusion: 现代数据库引擎能够通过在数据提取之前预先计算聚合来加速训练，从而为机器学习流程做出贡献。

Abstract: Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning](https://arxiv.org/abs/2512.09006)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLMs）在代码漏洞检测（CVD）任务中的性能，并提出了一种称为双重微调的新方法。尽管提示工程效果不佳，但检索增强生成(RAG)作为示例选择技术表现良好。研究表明，微调对于解决CVD任务至关重要，而Llama模型展示了在此领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发周期加速导致的软件生产显著增加，近年来软件漏洞数量也在不断上升。因此，自动化源代码漏洞检测过程变得非常必要。本研究旨在探索被认为迄今为止最有效的AI模型之一——大型语言模型（LLMs）在CVD任务中的表现，并通过不同先进技术来提高其有效性。

Method: 研究采用了最新的开源Llama-3.1 8B模型以及从BigVul和PrimeVul数据集中提取的源代码样本。此外，还探索了多种微调与提示工程设置，包括提出的一种名为双重微调的新方法，同时测试了较少被研究的测试时微调方法。

Result: 研究发现，微调对于完成CVD任务非常重要，其中双重微调表现出色。虽然提示工程未能有效提升性能，但检索增强生成(RAG)作为一种示例选择技术却相对表现较好。Llama模型也显示出在CVD领域中的巨大潜力。

Conclusion: 该研究表明，为了成功应用于代码漏洞检测任务，对大型语言模型进行适当微调是关键。尽管某些研究问题已得到解答，但仍有许多待进一步探索之处，为未来工作提供了广泛视角。

Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.

</details>


### [7] [TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization](https://arxiv.org/abs/2512.09196)
*Haonan Li,Keyu Man,Partha Kanuparthy,Hanning Chen,Wei Sun,Sreen Tallam,Chenguang Zhu,Kevin Zhu,Zhiyun Qian*

Main category: cs.SE

TL;DR: 本文介绍了一种名为TritonForge的基于性能分析指导的框架，用于自动优化Triton GPU内核。通过集成内核分析、运行时性能分析以及迭代代码转换，该框架能够简化优化流程。利用大型语言模型进行代码推理和转换，TritonForge在不同的内核类型和GPU架构上实现了相较于基础实现最高达5倍的性能提升，并且平均有1.76倍的成功案例。


<details>
  <summary>Details</summary>
Motivation: 尽管Triton这种针对GPU编程的领域特定语言能够让开发者以简洁的代码写出高效的内核程序，但要达到专家级的性能表现仍需要对GPU架构及底层性能权衡有深刻理解。为了降低这一过程中的劳动强度和技术门槛，提出了TritonForge框架。

Method: TritonForge结合了内核分析、运行时性能分析与迭代式的代码转换来自动化地优化Triton内核。它根据性能分析结果提供数据驱动反馈，识别出性能瓶颈，提出针对性的代码修改建议，并自动评估这些改动的影响。此外，该系统还利用大型语言模型（LLMs）来帮助完成代码理解和转换任务。

Result: 实验表明，在多种类型的内核和不同的GPU架构下，TritonForge相对于基准实现能够达到高达5倍的性能提升；同时，在大多数情况下（平均而言为1.76倍），优化尝试是成功的。

Conclusion: TritonForge为自动化的GPU性能优化提供了坚实的基础，展示了即使对于复杂的GPU内核优化问题，也能通过自动化工具显著提高效率而不牺牲性能。

Abstract: High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.

</details>


### [8] [Bug Priority Change Prediction: An Exploratory Study on Apache Software](https://arxiv.org/abs/2512.09216)
*Guangzong Cai,Zengyang Li,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 提出了一种基于bug修复演变特征和类别不平衡处理策略的两阶段bug报告优先级变更预测方法，旨在提高预测模型性能。实验结果显示，在bug报告阶段构建的预测模型F1分数达到了0.798，在bug修复阶段构建的模型F1加权和F1宏观分别为0.712和0.613。此外，还探讨了预测模型跨项目适用性及其在不同优先级水平下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前手动评估bug优先级变更过程繁琐且依赖于开发人员和项目经理的主观判断，容易导致错误的优先级调整，从而妨碍及时修复bug。鉴于对bug优先级变更预测的研究不足，作者们提出了一个新的解决方案来改善这一现状。

Method: 研究者们提出了一种新的两阶段bug报告优先级变更预测方法，该方法基于bug修复演变特征及类别不平衡处理策略。首先，将bug生命周期划分为bug报告与bug修复两个阶段，并为每个阶段构建了优先级变更预测模型。

Result: 通过使用来自32个非平凡Apache项目的bug数据集进行实验评估表明，所提出的bug修复演变特征及采用的类别不平衡处理策略能够有效提升预测模型的表现。具体来说，针对bug报告阶段的预测模型F1分数达到了0.798；而针对bug修复阶段的预测模型F1加权平均值和F1宏平均值分别为0.712和0.613。

Conclusion: 研究表明，新提出的基于bug修复演变特征的方法结合有效的类别不平衡处理策略可以在很大程度上提高bug优先级变更预测准确性。尽管不同项目之间模型表现存在较大差异，但整体而言效果良好；同时，在不同优先级水平下也保持了较高的预测性能。

Abstract: Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.

</details>


### [9] [Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values](https://arxiv.org/abs/2512.09562)
*Radoslaw Klimek,Jakub Blazowski*

Main category: cs.SE

TL;DR: 本文提出了一种将挖掘出的工作流程转换为逻辑规范并使用自动定理证明器分析其属性的方法，并采用合作博弈论中的Shapley值来量化工作流元素的贡献。实验表明该方法能够识别关键节点、揭示冗余以及暴露有害结构，为可解释的工作流分析指出了一个新方向。


<details>
  <summary>Details</summary>
Motivation: 当前工作流挖掘虽然能够从事件日志中发现层次化的流程树模型，但这些模型为何满足或违反某些逻辑属性，以及各个组成元素如何影响整体行为尚不清楚。为了增强工作流模型的可解释性，并支持软件工程实践中如合规检查、流程优化等需求，需要开发新的分析方法。

Method: 首先将通过工作流挖掘得到的流程模型转化为形式化的逻辑规范，然后利用自动定理证明工具对转换后的模型进行包括可满足性、活性及安全性在内的多种性质验证。基于此，进一步引入来自合作博弈理论的Shapley值概念，以衡量每个工作流组件对于最终结果的影响程度。

Result: 在基准数据集上的实验证明了所提方法的有效性，能够成功地识别出对整个系统至关重要的节点，同时也能有效地揭露设计中存在的冗余部分和潜在问题区域。

Conclusion: 结合逻辑分析与博弈论技术的工作流解析途径不仅提高了现有工作流模型的透明度，还为未来的流程挖掘工具开发提供了新的思路，具有直接应用于软件工程实践中的潜力。

Abstract: Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.

</details>


### [10] [LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection](https://arxiv.org/abs/2512.09627)
*Jingwei Ye,Zhi Wang,Chenbin Su,Jieshuai Yang,Jiayi Ding,Chunbo Liu,Ge Chu*

Main category: cs.SE

TL;DR: 提出了一种名为LogICL的框架，该框架将大型语言模型（LLM）的推理能力提炼到一个轻量级编码器中，用于跨领域的日志异常检测。通过构建衡量演示实用性的delta矩阵，并采用多目标损失函数优化编码器，LogICL在少样本和零样本跨领域基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的模型需要大量的资源和标注数据，这在目标领域中日志稀缺时会加剧冷启动问题。而现有的跨域方法虽然利用了源日志，但由于依赖于表面词汇相似性，在结构差异下无法捕捉潜在语义等价性，导致泛化能力不足。

Method: 提出了LogICL框架，它通过最大边际相关性选择演示并构建一个delta矩阵来度量这些演示相对于零样本推理的实用性。编码器通过包含ICL引导项、最大均值差异以及监督对比损失的多目标损失函数进行优化。在推理阶段，经过优化的编码器使用语义相似性和delta分数检索具有推理意识的演示，使冻结的LLM能够以思维链的方式进行上下文学习，从而实现准确且可解释的检测。

Result: 实验结果表明，在少样本和零样本跨域基准上，LogICL达到了当前最佳的表现。进一步通过可视化和案例研究分析证实，LogICL不仅超越了表面词汇相似性，还有效地捕捉到了潜在的语义等价性，支持快速部署。

Conclusion: LogICL为解决跨领域日志异常检测中的挑战提供了一个有效的方法，尤其适用于那些由于数据稀缺而导致冷启动问题的情景。

Abstract: Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.

</details>


### [11] [Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis](https://arxiv.org/abs/2512.09679)
*Naizhu Jin,Zhong Li,Guang Yang,Tian Zhang,Qingkai Zeng*

Main category: cs.SE

TL;DR: 该研究通过实证和信息论的方法，系统地研究了思维链（CoT）提示在神经代码生成中的有效性。评估了五种范式跨六个Python基准、一个包含12种编程语言的多语言基准以及从7B到480B参数的六个模型的表现。结果显示，外部引导的CoT始终优于直接生成，结构化方法平均提高了5-12%的一次通过率，并且使用更少的token。CoT的好处取决于语言类型系统和模型容量；高质量的结构化CoT可以显著提高准确性，而简单的零样本CoT甚至可能降低性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在代码生成方面表现出色，但对于思维链（CoT）提示如何帮助这一过程的具体机制仍不清楚。为了填补这一知识空白，本研究旨在探索不同条件下CoT的有效性及其对代码生成质量的影响。

Method: 采用了一个综合性的实验与信息理论分析框架来研究CoT在神经代码生成中的效果。该研究覆盖了五个不同的范式、多个编程语言基准测试集及一系列具有不同参数规模的语言模型。通过条件互信息$I(Y;C|X)$作为概念视角，对比了不同策略下模型表现的变化。

Result: 研究发现，相较于直接生成代码的方式，受到外部指导的CoT方法能够带来更好的结果。特别地，结构化的CoT方法不仅提高了大约5至12个百分点的一次通过率，而且在实现这一点时所需使用的token数量明显少于反思性推理方法。此外，研究表明CoT带来的好处受制于程序语言的类型系统特性和模型自身的容量限制。

Conclusion: 选择合适的CoT策略对于提高代码生成任务的成功率至关重要。根据模型能力、语言特征以及任务复杂度挑选最佳方案是关键。高质量的结构化CoT能够显著提升准确率，而低质量或过于简单的CoT则可能反而损害性能。

Abstract: Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.

</details>


### [12] [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](https://arxiv.org/abs/2512.09775)
*Vladimir Balditsyn,Philippe Lalanda,German Vega,Stéphanie Chollet*

Main category: cs.SE

TL;DR: 本文提出了一种量化基于机器学习系统的不确定性的方法，通过在运行时评估模型预测的相关性，并在人类活动识别（HAR）领域进行了应用与验证。


<details>
  <summary>Details</summary>
Motivation: 随着普适计算和机器学习的融合，产生了许多影响经济和社会活动各个方面的服务。然而，人工智能技术的使用排除了一些标准软件开发实践，如严格的测试以确保消除所有错误并遵循明确定义的规范。由于机器学习模型是通过大量高维示例训练而不是手动编码的，因此其操作范围边界不确定，也无法保证绝对无误的表现。

Method: 本文提出的方法包括适应并联合利用一组选定的技术来评估运行时模型预测的相关性。这些提案被应用于高度异质且不断变化的人类活动识别（HAR）领域。

Result: 实验结果表明了所提方法的有效性，并详细讨论了对领域专家提供的帮助。

Conclusion: 本研究为量化基于ML系统的不确定性提供了一个新的视角，特别是在动态和复杂的应用场景下。

Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization](https://arxiv.org/abs/2512.08950)
*Aseel Rawashdeh*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯扩展的Act-Then-Measure（ATM）算法，该算法通过使用卡尔曼滤波式的贝叶斯更新来替代标准的Q学习，以保持对Q值的不确定性感知估计，并实现更稳定和样本效率更高的学习。在小规模环境和临床动机测试平台上的评估表明，在数据稀疏且噪声环境下贝叶斯ATM能够提供更低的方差和更稳定的策略行为；但在复杂的真实世界mHealth环境中，所有ATM变体表现不佳，这表明需要新的强化学习算法来处理因果结构、连续状态以及观察成本约束下的延迟反馈问题。


<details>
  <summary>Details</summary>
Motivation: 移动健康（mHealth）干预中的强化学习需要平衡干预效果与用户负担，特别是在状态测量（例如用户调查或反馈）既昂贵又必不可少的情况下。现有的Act-Then-Measure (ATM)启发式方法虽然解决了这一挑战，但其基于时间差分的Q学习方法在稀疏和嘈杂环境中容易不稳定。因此，研究者旨在开发一种更加稳定且样本高效的解决方案。

Method: 研究者提出了一个贝叶斯版本的ATM算法，它利用类似于卡尔曼滤波器的贝叶斯更新代替了传统的Q学习方法。这种方法允许在保持对Q值不确定性意识的同时，进行更稳定且样本效率更高的学习过程。

Result: 在小型表格环境及临床相关的测试平台上，贝叶斯ATM实现了与传统方法相当甚至更好的标量化回报，同时展现出更低的方差和更稳定的政策行为。然而，在更大、更复杂的mHealth场景中，无论是标准还是贝叶斯版本的ATM表现都不理想。

Conclusion: 研究结果强调了在低数据条件下采用不确定性感知方法的价值，同时也指出了开发新型RL算法的需求，这些新算法应能明确建模因果结构、连续状态以及在观测成本限制下的延迟反馈等问题。

Abstract: Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.

</details>


### [14] [An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings](https://arxiv.org/abs/2512.08954)
*Yuhao Xu,Jiaying Lu,Sirui Ding,Defu Cao,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: 本研究旨在探讨基础模型在心电图分析中的有效性。通过对比通用时间序列/心电图基础模型与时间序列深度学习模型，实验结果显示前者达到了80%的最佳性能率，表明其在心电图分析中是有效的。此外，研究还提供了深入的分析和见解，并公开了数据和代码以供参考。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）作为一种非侵入式收集心脏活动的方法，在诊断心脏状况方面被广泛应用。然而，分析ECG通常需要领域专业知识，这对将人工智能应用于医疗保健构成了障碍。尽管自监督学习和基础模型的进步使得AI系统能够在不完全依赖人类专业知识的情况下获取并利用领域知识，但目前缺乏对这些基础模型在ECCG表现上的全面分析。

Method: 为了回答“基础模型是否对ECG分析有用？”这一研究问题，本研究评估了语言/通用时间序列/ECG基础模型，并将其与时间序列深度学习模型进行了比较。

Result: 实验结果表明，通用时间序列/ECG基础模型达到了80%的最佳性能率，显示出它们在ECG分析中的有效性。

Conclusion: 这项研究表明了基础模型在推进生理波形分析方面的局限性和潜力。

Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.

</details>


### [15] [DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability](https://arxiv.org/abs/2512.08956)
*Kumarjit Pathak,Karthik K,Sachin Madan,Jitin Kapila*

Main category: cs.LG

TL;DR: 提出了一种新的KNN变体DW-KNN，它结合了指数距离和邻居有效性，从而在异构特征空间中提高了预测的可靠性。通过9个数据集上的综合评估显示，DW-KNN平均准确率达到0.8988，并且具有最低的交叉验证方差，表明其预测稳定性高。


<details>
  <summary>Details</summary>
Motivation: 传统的K-最近邻(KNN)分类器及其变体假设所有'k'个邻居都是同等可靠的，这在面对异构特征空间时成为一个限制，影响了对观察值真实水平的可靠预测。

Method: 提出了DW-KNN（双重加权KNN），一种将指数距离与邻居有效性相结合的方法，旨在提高实例级别的可解释性、抑制噪声或错误标记样本的影响以及减少超参数敏感性。

Result: 通过对9个数据集进行的全面评估表明，DW-KNN平均达到了0.8988的准确率，在六种方法中排名第二，仅比表现最好的集成KNN低0.2%。此外，它表现出最低的交叉验证方差(0.0156)，表明预测稳定性良好。统计显著性测试确认了相较于紧密度加权KNN(+4.09%)和核加权KNN(+1.13%)有明显改进($p<0.001$)。

Conclusion: DW-KNN为复杂自适应方案提供了一个简单而有效的替代选择，特别是在需要可解释预测结果的高风险应用场景中显得尤为有价值。

Abstract: K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.

</details>


### [16] [LUMOS: Large User MOdels for User Behavior Prediction](https://arxiv.org/abs/2512.08957)
*Dhruv Nigam*

Main category: cs.LG

TL;DR: 本文介绍了一种基于Transformer架构的LUMOS模型，该模型能够通过原始用户活动数据联合学习多种任务，无需特定任务模型和手动特征工程。实验表明，LUMOS在多个任务上优于传统方法，并且在线A/B测试显示DAU增长了3.15%。


<details>
  <summary>Details</summary>
Motivation: 针对在线B2C平台面临的大规模用户行为预测挑战，现有的解决方案依赖于特定任务模型和领域特定的特征工程，这不仅耗时而且计算成本高，缺乏可扩展性。

Method: 提出了一种名为LUMOS（Large User MOdel Series）的新型Transformer架构，它通过仅使用原始用户活动数据来联合学习多项任务，消除了对特定任务模型的需求以及手动特征工程。LUMOS引入了一个新的交叉注意力机制，使预测能够基于未来已知事件进行条件判断；同时采用多模态分词技术，将用户交易、事件背景与静态用户人口统计属性相结合以生成丰富的表示形式。

Result: 通过对覆盖2.75亿用户的生产数据集进行广泛实验，证明LUMOS在五个具有既定基准的任务上平均提高了0.025的ROC-AUC值（二分类任务）和减少了4.6%的MAPE（回归任务）。在线A/B测试进一步验证了这些改进转化为实际业务影响，每日活跃用户数增加了3.15%。

Conclusion: LUMOS提供了一种更高效、更具扩展性的用户行为预测方案，能够有效提升在线B2C平台的运营效率及用户体验。

Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.

</details>


### [17] [EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications](https://arxiv.org/abs/2512.08959)
*Ard Kastrati,Josua Bürki,Jonas Lauer,Cheng Xuan,Raffaele Iaquinto,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文介绍了一个统一的基准测试框架，专注于评估基于EEG的基础模型在临床应用中的表现。该基准涵盖了11个定义明确的诊断任务和14个公开可用的EEG数据集，并显示虽然基础模型在某些情况下表现出色，但更简单的模型在临床分布变化下仍保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 为了更好地评估基于EEG的基础模型在临床应用中的效果，特别是面对不同的疾病如癫痫、精神分裂症等时的表现情况。

Method: 构建了一个包含11项诊断任务和14个EEG数据集的评测框架，采用最少预处理和标准化评价协议来比较经典基线方法与现代基础模型之间的性能差异。

Result: 结果显示，在特定条件下基础模型确实能够展现出很好的性能；然而，当遇到临床环境中数据分布发生变化时，一些较为简单的模型依旧展现出了较强的竞争力。

Conclusion: 通过建立这样一个综合性的评测平台，不仅有助于提高研究工作的可重复性，同时也促进了不同类型模型之间更为直观有效的对比分析。

Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.

</details>


### [18] [Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces](https://arxiv.org/abs/2512.08960)
*Yueer Zhou,Yichen Wu,Ying Wei*

Main category: cs.LG

TL;DR: PS-LoRA通过在优化子空间内对齐更新来解决连续学习中的灾难性遗忘问题，采用双重正则化目标和基于幅度的合并策略，以保持学习表示的稳定性并有效适应新领域。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩适应方法（LoRA）虽然能够实现高效的持续学习，但容易因任务间破坏性干扰导致灾难性遗忘。本文旨在通过提出一种新的框架PS-LoRA来解决这一问题，该框架可以减少新旧任务梯度之间的冲突，从而改善模型对于过去任务的记忆能力。

Method: PS-LoRA引入了一个双正则化目标函数，一方面惩罚与历史权重轨迹相冲突的方向性更新，另一方面限制更新幅度的变化，确保与先前知识的一致性。此外，还开发了一种基于参数幅度的融合技术，能够在不需要重新训练的情况下将连续的学习适配器整合成一个更加鲁棒的表达形式。

Result: 实验结果表明，在自然语言处理(NLP)和视觉基准测试中，PS-LoRA相比当前最先进的方法能够更好地维持已学习表征的稳定性，并且能够高效地调整至新的任务或领域。

Conclusion: 研究证明了PS-LoRA作为一种有效的持续学习方法，能够显著减轻灾难性遗忘现象，同时促进模型快速适应新环境，为未来的研究提供了有价值的见解和技术支持。

Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.

</details>


### [19] [Financial Instruction Following Evaluation (FIFE)](https://arxiv.org/abs/2512.08965)
*Glenn Matlin,Siddharth,Anirudh JM,Aditya Shukla,Yahya Hassan,Sudheer Chava*

Main category: cs.LG

TL;DR: 本文介绍了一种新的高难度基准FIFE，用于评估语言模型在金融分析任务中的指令执行能力。通过88个手工编写的提示和一个可链接、可验证的约束验证系统，对53个不同类型的模型进行了零样本设置下的评估。结果显示，开放权重模型的表现优于专有系统，但即使是表现最好的模型也未能完全符合FIFE的复杂要求。


<details>
  <summary>Details</summary>
Motivation: 由于语言模型在处理复杂且相互依赖的指示时存在困难，特别是在像金融这样需要高度精确性的领域。为了改善这一状况并促进相关研究，作者们提出了FIFE作为一个专门针对金融分析任务设计的新基准。

Method: FIFE由88个手写提示组成，并使用了具有可链接及可验证约束条件的验证体系来提供细致的奖励信号。研究中选取了53个模型（包括专属、开放权重与开源类型），并在零样本环境下对其性能进行了评测。

Result: 主要发现表明，在严格/宽松标准下，顶级开放权重模型的表现超过了领先的专有系统，而最佳开源模型则明显落后。不过，即便是表现最优秀的模型也难以完全满足FIFE设定的高标准要求。

Conclusion: 尽管某些模型在新提出的FIFE基准测试中表现出色，但仍存在改进空间。为此，研究团队公开了他们的数据集和代码，以支持金融领域内强化学习的研究进步。

Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.

</details>


### [20] [CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing](https://arxiv.org/abs/2512.08967)
*Zixia Wang,Gaojie Jin,Jia Hu,Ronghui Mu*

Main category: cs.LG

TL;DR: 提出了一种名为CluCERT的新框架，通过聚类引导的去噪平滑来认证大型语言模型（LLMs）对对抗性提示的鲁棒性。该方法通过引入语义聚类过滤器减少噪声样本并保留有意义的扰动，同时通过精炼模块和快速同义词替换策略提高了计算效率。实验结果显示，在鲁棒性界限和计算效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的针对LLMs对抗性攻击的防御方法存在鲁棒性边界宽松以及计算成本高的问题。为了解决这些问题，并提高LLMs在面对意义保持变化如同义词替换时的鲁棒性，提出了新的解决方案。

Method: 开发了CluCERT框架，采用聚类引导的去噪平滑技术。关键组成部分包括一个用于减少噪声样本同时保留有效扰动的语义聚类过滤器、一个提取核心语义以提高计算效率的精炼模块，以及加速去噪过程的快速同义词替换策略。

Result: 实验表明，与当前最先进的认证方法相比，所提出的方法不仅在多个下游任务和越狱防御场景中提供了更严格的鲁棒性保证，还在计算效率上取得了显著提升。

Conclusion: CluCERT框架成功地解决了现有方法在鲁棒性认证上的局限性，通过引入创新的技术手段实现了更紧致的鲁棒性界限及更高的计算效率。

Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.

</details>


### [21] [StructuredDNA: A Bio-Physical Framework for Energy-Aware Transformer Routing](https://arxiv.org/abs/2512.08968)
*Mustapha Hamdi*

Main category: cs.LG

TL;DR: 本文提出了一种名为StructuredDNA的稀疏架构框架，用于模块化、节能的Transformer路由。通过引入基于生物物理和能量导向的路由层来替代密集的专家混合路由机制，实现了显著的能量利用密度减少，并在多个基准测试中展示了其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大规模计算模型的快速发展，能源和计算成本急剧增加。受生物系统中结构与功能从低能耗配置中产生的启发，研究者们旨在开发一种更节能且具有模块化的计算架构。

Method: StructuredDNA框架采用基于语义能量最小化的生物物理能量导向路由层代替了传统的密集型专家混合路由机制。输入数据被动态地组织成语义密码子，并通过最小化结合全局能量函数（该函数综合考虑了内聚性、不确定性和计算成本）来选择单一专家进行处理。

Result: 在BioASQ(K=50)上，StructuredDNA达成了97.7%的能量利用密度(EUD)降低以及0.998的语义稳定性指数(SSI)。此外，在WikiText-103上的实验表明，通过调整专家粒度(K=2048)，该架构能够在保持超过99%能效的同时适用于开放领域。

Conclusion: StructuredDNA为未来的稀疏计算框架提供了一个强大且不受领域限制的范例，它不仅将生物物理学原理与Transformer架构中的稀疏专家路由联系起来，还指出了未来朝向更加节能、模块化及可扩展计算系统的方向。

Abstract: The rapid scaling of large computational models has led to a critical increase in energy and compute costs. Inspired by biological systems where structure and function emerge from low-energy configurations, we introduce StructuredDNA, a sparse architecture framework for modular, energy-aware Transformer routing. StructuredDNA replaces dense Mixture-of-Experts routing with a bio-physical, energy-guided routing layer based on semantic energy minimization. Inputs are dynamically grouped into semantic codons, and routing selects a single expert by minimizing a global energy functional that combines cohesion, uncertainty, and computational cost.
  We validate StructuredDNA on both specialized (BioASQ) and open-domain benchmarks (WikiText-103). On BioASQ (K = 50), we achieve a 97.7% reduction in Energy Utilization Density (EUD) and a Semantic Stability Index (SSI) of 0.998. We further demonstrate a Semantic Scaling Law on WikiText-103, showing that the architecture generalizes to open domains by scaling expert granularity (K = 2048) while maintaining more than 99% energy efficiency. StructuredDNA thus establishes a robust, domain-agnostic paradigm for future sparse computational frameworks.
  StructuredDNA provides an explicit link between bio-physical principles and sparse expert routing in Transformer architectures, and points toward future energy-aware, modular, and scalable computational systems. We discuss limitations of this proof-of-concept study and outline directions for scaling the approach to larger models, datasets, and hardware platforms. The StructuredDNA implementation is available at https://github.com/InnoDeep-repos/StructuredDNA .

</details>


### [22] [Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs](https://arxiv.org/abs/2512.08976)
*Isha Chaturvedi,Anjana Nair,Yushen Li,Adhitya Rajendra Kumar,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的诊断方法——对比区域遮罩（CRM），用于揭示多模态大型语言模型在链式思维推理每一步中对特定视觉区域的依赖程度。通过系统地遮挡注释区域，并将由此产生的推理痕迹与未遮挡基线进行对比，CRM能够提供因果、步骤级别的归因。该方法强调了不仅需要评估答案的正确性，还需要评估推理的忠实度，从而推动了对于衡量性能以及推理稳健性和保真度的多模态评估框架的需求。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常只限于最终答案或注意力图谱的分析，而无法提供在链式思维推理过程中每个步骤上多模态大语言模型对特定视觉区域依赖性的因果、步骤级别归因。因此，研究者们开发了对比区域遮罩（CRM）这一新工具，旨在更深入地理解这些模型如何基于视觉输入进行推理，并识别不同模型在证据缺失时的表现差异。

Method: 提出了对比区域遮罩（CRM）技术，通过对已标注区域进行系统性遮挡，并比较遮挡后得到的推理路径与原始未遮挡情况下的基准，以此来揭示多模态大语言模型在链式思维推理过程中的具体依赖关系。

Result: 应用到如VisArgs等数据集上时，CRM能够展示出不同的失败模式：一些模型即使在缺乏证据的情况下也能保持其推理结构，但可能会产生幻觉；另一些则紧密依赖视觉线索，但在受到扰动时容易崩溃。这表明，仅仅关注答案的准确性是不够的，还需要考虑推理过程中的忠实度。

Conclusion: 通过引入CRM作为诊断工具，研究转变了视觉基准测试的重点，从单纯追求答案正确转向更加重视推理过程的质量。这突出了构建新的多模态评价体系的重要性，不仅要考察模型性能，还要综合考量其推理过程中的稳健性和真实性。

Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.

</details>


### [23] [Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques](https://arxiv.org/abs/2512.09054)
*Alon Arad,Saharon Rosset*

Main category: cs.LG

TL;DR: 本文提出了一种新的多类校准方法，通过直接将归一化纳入优化过程（NA-FIR）或建模为累积双变量保序回归问题（SCIR），旨在改善多分类任务中的概率预测准确性和可靠性。实验表明该方法在多种文本和图像分类数据集上均能持续提高负对数似然(NLL)和预期校准误差(ECE)指标。


<details>
  <summary>Details</summary>
Motivation: 针对多类监督学习任务中需要准确可靠的概率预测这一需求，现有的基于单对多策略的保序回归方法在校准效果上不如参数化方法，限制了其实际应用。

Method: 提出了两种考虑归一化的新型保序回归技术：一种是直接将归一化融入到优化过程中（称为NA-FIR）；另一种是将问题建模为累积双变量保序回归（称为SCIR）。这两种方法都是基于自然直观的假设而设计的，能够更好地满足实践者的需求。

Result: 通过对不同类型的数据集及模型架构进行实证评估发现，所提方法在NLL和ECE等关键评价指标上表现出色，优于现有方法。

Conclusion: 本研究提出的两种新方法能够在保持非参数性质的同时有效提升多分类任务中的概率预测质量，为解决多类校准问题提供了新思路。

Abstract: Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration produced suboptimal results when compared to parametric methods, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multiclass calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (NA-FIR) or modeling the problem as a cumulative bivariate isotonic regression (SCIR). Empirical evaluation on a variety of text and image classification datasets across different model architectures reveals that our approach consistently improves negative log-likelihood (NLL) and expected calibration error (ECE) metrics.

</details>


### [24] [A Diffusion-Based Framework for High-Resolution Precipitation Forecasting over CONUS](https://arxiv.org/abs/2512.09059)
*Marina Vicens-Miquel,Amy McGovern,Aaron J. Hill,Efi Foufoula-Georgiou,Clement Guilloteau,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 该研究提出了一种基于扩散的深度学习框架，通过比较仅使用过去观测数据、仅使用数值天气预报以及结合两者的混合模型三种残差预测策略，在美国大陆范围内对降水预报进行了改进。所有方法在不同预报时间上均优于HRRR基准模型，特别是在较长预报时间内表现更佳，有助于提高极端降雨情况下的应急准备能力。


<details>
  <summary>Details</summary>
Motivation: 准确的降水预报对于水文气象风险管理至关重要，特别是对于可能导致山洪暴发和基础设施损坏的极端降雨事件。本研究旨在通过引入新的深度学习框架来改善降水预报的质量，从而帮助更好地管理这些风险。

Method: 1. 介绍了三种不同的残差预测策略：完全数据驱动（基于MRMS系统的历史观测）、修正型（基于HRRR系统的预报）及混合型（结合MRMS与选定的HRRR预报变量）。2. 在统一设置下评估了这三种方法的表现，以了解每种数据源如何贡献于预测技能。3. 预报覆盖整个美国大陆，并采用自回归滚动方式从1小时扩展到12小时。4. 对性能进行评估时采用了涵盖全国范围和地区特定指标的方法，包括总体性能及在极端降雨阈值上的表现。5. 为确保可靠性，还集成了针对残差学习设置调整后的不确定性量化技术。

Result: 结果表明，在所有预报时段内，所提出的深度学习框架在像素级和空间统计度量方面始终优于HRRR基线模型。混合模型在最短预报时间内的表现最佳；而基于HRRR校正的模型则在较长预报时间内表现出色，能够保持高达12小时的良好技能水平。

Conclusion: 这项工作通过增强预测技能、可靠性和跨区域适用性推进了基于深度学习的降水预报领域的发展。特别地，它在延长预报时间方面的改进对于紧急情况下的决策制定尤为重要。

Abstract: Accurate precipitation forecasting is essential for hydrometeorological risk management, especially for anticipating extreme rainfall that can lead to flash flooding and infrastructure damage. This study introduces a diffusion-based deep learning (DL) framework that systematically compares three residual prediction strategies differing only in their input sources: (1) a fully data-driven model using only past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model using only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model integrating both MRMS and selected HRRR forecast variables. By evaluating these approaches under a unified setup, we provide a clearer understanding of how each data source contributes to predictive skill over the Continental United States (CONUS). Forecasts are produced at 1-km spatial resolution, beginning with direct 1-hour predictions and extending to 12 hours using autoregressive rollouts. Performance is evaluated using both CONUS-wide and region-specific metrics that assess overall performance and skill at extreme rainfall thresholds. Across all lead times, our DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics. The hybrid model performs best at the shortest lead time, while the HRRR-corrective model outperforms others at longer lead times, maintaining high skill through 12 hours. To assess reliability, we incorporate calibrated uncertainty quantification tailored to the residual learning setup. These gains, particularly at longer lead times, are critical for emergency preparedness, where modest increases in forecast horizon can improve decision-making. This work advances DL-based precipitation forecasting by enhancing predictive skill, reliability, and applicability across regions.

</details>


### [25] [Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction](https://arxiv.org/abs/2512.09074)
*Shangqing Xu,Zhiyuan Zhao,Megha Sharma,José María Martín-Olalla,Alexander Rodríguez,Gregory A. Wellenius,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 提出了DeepTherm系统，这是一个无需热相关死亡历史即可预测致命热浪的模块化预警系统。该系统通过深度学习技术将无热浪条件下的基准死亡率与其他原因导致的死亡率分开来处理。在西班牙的实际数据测试中显示出一致、稳健和准确的表现，并允许在漏报和误报之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: 由于定义和估计与热相关的死亡率存在困难，因此尽管可以预测热浪的发生并归因于历史死亡率，但预测即将到来的致命热浪仍然是一个挑战。此外，建立早期预警系统还面临数据可用性、时空鲁棒性和决策成本等额外要求。

Method: 开发了名为DeepTherm的模块化预警系统，利用深度学习的灵活性，采用双预测管道，一方面预测没有热浪及其他异常事件情况下的基准死亡率，另一方面则从所有原因造成的死亡率中分离出来。

Result: 通过在西班牙不同地区、时期及人群组的数据上评估DeepTherm，结果表明其表现一致、稳健且准确，同时能够在错过警报和错误警报之间找到平衡点。

Conclusion: DeepTherm为解决城市严重热浪对公众健康的威胁提供了一个有效的解决方案，它不需要依赖热相关死亡的历史数据，就能准确地预测致命热浪的到来。

Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.

</details>


### [26] [Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting](https://arxiv.org/abs/2512.09076)
*Moazzam Umer Gondal,Hamad ul Qudous,Asma Ahmad Farhan*

Main category: cs.LG

TL;DR: 本研究对比了轻量级加法模型（如Facebook Prophet和NeuralProphet）与复杂深度学习方法在预测北京PM2.5和PM10污染水平上的表现。结果表明，Facebook Prophet在准确性上优于其他模型，包括传统统计模型SARIMAX以及基于机器学习的方法LSTM和LightGBM，证明了解释性强的加法模型能够有效平衡预测精度、透明度及部署简易性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和混合管道技术在近期研究中占据主导地位，但它们的复杂性和较低的可解释性限制了实际应用。该研究旨在探索像Facebook Prophet (FBP) 和 NeuralProphet (NP) 这样的轻量级加法模型是否能为中国的北京市提供具有竞争力的颗粒物(PM2.5, PM10)预测。

Method: 研究使用了多年污染物和气象数据，采用了系统性的特征选择方法（相关性、互信息、mRMR）、防止泄漏的数据缩放处理以及按时间顺序划分的数据集。两种模型均使用污染物及其前体作为回归变量进行训练，其中NeuralProphet还利用了滞后依赖性。同时，研究还实现了两个机器学习基线模型(LSTM, LightGBM)和一个传统的统计模型(SARIMAX)作为对比。

Result: 评估结果显示，在7天保留测试中，Facebook Prophet在所有指标(MAE, RMSE, R^2)上持续优于NeuralProphet、SARIMAX以及其他基于学习的基准模型，对于两种污染物的测试R^2值均超过0.94。

Conclusion: 研究表明，易于解释的加法模型（如Facebook Prophet）在预测精度上不仅能够与传统方法相媲美，甚至超过了复杂的深度学习解决方案，同时保持了较高的透明度和部署简便性。

Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.

</details>


### [27] [Learning Unmasking Policies for Diffusion Language Models](https://arxiv.org/abs/2512.09106)
*Metod Jazbec,Theo X. Olausson,Louis Béthune,Pierre Ablin,Michael Kirchhof,Joao Monterio,Victor Turrisi,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 本文提出了一种使用强化学习训练采样程序的方法，以提高掩码离散扩散模型的生成质量和效率。实验表明，该方法在半自回归生成中与最先进的启发式策略性能相当，在全扩散设置下优于它们，并且这些策略可以泛化到新的基础dLLM和更长的序列长度。然而，当应用于领域外数据时，性能有所下降，调整精度-效率之间的平衡也具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码离散扩散模型通过逐步替换填充有特殊掩码标记的缓冲区来生成文本，但如何选择每一步要替换哪些标记是关键。虽然基于置信度阈值等启发式策略提高了生成质量和吞吐量，但这些方法需要手动调优且对于较大缓冲区表现不佳。

Method: 将掩码扩散采样过程形式化为马尔可夫决策过程，其中dLLM充当环境；提出一种基于单层transformer的轻量级策略架构，将dLLM标记置信度映射到解码决策上。

Result: 实验结果表明，所提出的训练策略在结合半自回归生成时达到了最先进启发式的性能水平，在全扩散设定下超越了后者。此外，这些策略能够适应新的底层dLLM及更长的序列长度，但在处理域外数据时表现欠佳，调整准确率与效率间权衡也有一定难度。

Conclusion: 通过采用强化学习训练采样策略，可以有效提升掩码离散扩散模型的文本生成质量与效率，尽管存在对域外数据适应性差以及精确调节性能-效率权衡不易的问题。

Abstract: Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.

</details>


### [28] [Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach](https://arxiv.org/abs/2512.09198)
*Phevos Paschalidis,Vasiliki Stoumpou,Lisa Everest,Yu Ma,Talhat Azemi,Jawad Haider,Steven Zweibel,Eleftherios M. Protopapas,Jeff Mather,Maciej Tysarowski,George E. Sarris,Robert C. Hagberg,Howard L. Haronian,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 研究人员开发了一种数据驱动的临床支持工具，用于为进行经导管主动脉瓣置换术（TAVR）的患者选择最佳的人工心脏瓣膜类型，以降低术后永久起搏器植入的风险。通过整合美国和希腊患者的多源数据，并采用叶片级分析方法，该模型在内部美国人群和外部希腊验证队列中分别降低了26%和16%的PPI率。


<details>
  <summary>Details</summary>
Motivation: 鉴于TAVR是治疗严重主动脉狭窄的有效手段之一，但关于哪种瓣膜最适合使用的指南仍存在争议，研究旨在通过创建一个数据驱动的临床决策支持工具来识别能够最小化永久起搏器植入风险的最佳瓣膜类型。

Method: 研究者构建了一个结合了美国与希腊患者群体的新数据集，整合了包括患者人口统计学信息、计算机断层扫描及超声心动图在内的三个不同数据源，并且考虑到各国记录系统之间的差异进行了协调。此外，引入了叶片级别的分析方法以利用群体异质性并避免与不确定的反事实风险估计相比较。

Result: 最终形成的处方模型显示，在研究者的内部美国人群中，与当前护理标准相比，PPI率减少了26%，而在外部希腊验证组中则减少了16%。

Conclusion: 据研究者所知，这项工作代表了首个统一的、个性化的TAVR中THV选择策略，它能够显著降低接受TAVR手术后需要永久起搏器植入的风险。

Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.

</details>


### [29] [LLMs for Analog Circuit Design Continuum (ACDC)](https://arxiv.org/abs/2512.09199)
*Yasaman Esfandiari,Jocelyn Rego,Austin Meyer,Jonathan Gallagher,Mia Levy*

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型（LLMs）在模拟电路设计这一需要领域特定推理的任务中的适用性和一致性问题，研究了不同数据表示对模型行为的影响，并比较了小模型与大基础模型在不同训练条件下的表现。结果揭示了LLMs在可靠性方面面临的挑战，包括对数据格式敏感、生成设计不稳定以及泛化能力有限等，为构建可靠且可部署的基础模型提供了见解。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型和变压器架构在自然语言任务中表现出色，但其在实际工程领域的可靠性和鲁棒性仍待探索，特别是在人类参与的设计流程中。本文旨在评估这些模型在模拟电路设计中的应用潜力及局限性，以增强人类在复杂工程任务中的能力。

Method: 通过研究不同的数据表示方法如何影响模型的行为，并对比小型模型（如T5, GPT-2）与大型基础模型（如Mistral-7B, GPT-oss-20B）在多种训练条件下的性能差异来完成分析。

Result: 研究表明，LLMs面临的关键可靠性挑战包括对数据格式的敏感性、生成设计时的不稳定性以及对于未见过的电路配置的泛化能力有限等问题。

Conclusion: 本研究为理解LLMs作为工具增强人类在复杂工程任务中能力的潜力与限制提供了初步证据，同时也指出了开发适用于结构化现实世界应用的可靠且可部署的基础模型所需考虑的关键因素。

Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.

</details>


### [30] [Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation](https://arxiv.org/abs/2512.09267)
*Ce Wang,Weihang Dai,Hanru Bai,Xiaomeng Li*

Main category: cs.LG

TL;DR: 本研究通过结合标记和未标记样本，利用谱序列算法恢复未标记样本的准确顺序排名，并通过对比学习来改进半监督回归方法中的特征表示能力。实验表明该方法优于现有的半监督深度回归方法。


<details>
  <summary>Details</summary>
Motivation: 对比学习方法在提高回归模型的表征能力方面有效，但高度依赖于标签信息来正确恢复特征间的顺序关系，这限制了其在半监督回归中的应用。为了减少对昂贵标注的依赖，研究者希望扩展对比回归方法，使之能够使用未标记数据。

Method: 研究者们构建了一个包含小批量内标记与未标记样本的特征相似性矩阵，以反映样本间的关系；采用谱序列算法从这个矩阵中恢复未标记样本的顺序排名；通过动态规划算法选取稳健特征用于矩阵构造；利用恢复的顺序关系对未标记样本进行对比学习；同时，这些顺序排名还被用来监督未标记样本的预测，作为额外的训练信号。

Result: 理论分析和多种数据集上的实验证明了提出的方法能够在半监督设置下超越现有最先进的半监督深度回归方法。

Conclusion: 本文介绍了一种新的半监督回归方法，它减少了对标签信息的依赖，允许更多数据参与特征表示学习，从而提高了结果的鲁棒性。

Abstract: Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.

</details>


### [31] [Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design](https://arxiv.org/abs/2512.09329)
*Amin Tavakoli,Raswanth Murugan,Ozan Gokdemir,Arvind Ramanathan,Frances Arnold,Anima Anandkumar*

Main category: cs.LG

TL;DR: 本文提出了一种简单且通用的蛋白质语言模型（PLM）监督微调（SFT）方法，该方法利用PLM本身结合轻量级策划流程和领域特定过滤器来构建高质量训练数据，从而提高生成蛋白质序列的保真度、可靠性和新颖性。通过将此方法应用于色氨酸合成酶家族，证明了其在生成更稳定和功能性更强的酶方面以及探索超越自然变异的蛋白质序列空间方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对蛋白质序列建模和蛋白质语言模型（PLMs）的监督微调（SFT）应用缺乏系统性的问题，尤其是考虑到获取高质量标注数据对于蛋白质来说比自然语言要困难得多这一点。

Method: 开发了一个简单而通用的快速监督微调（SFT）方案，专门用于提升PLMs的表现力。该方法不依赖于昂贵预编译实验数据集，而是利用PLM自身，并结合轻量级策展流程及领域特异性过滤器来创建高质量训练资料。这些过滤器可以独立地优化PLM输出并挑选出适合体外评估的候选者；当与SFT结合时，能够使PLMs产生更加稳定且功能性的酶类，同时扩展对天然变体之外蛋白质序列空间的探索。

Result: 通过对色氨酸合成酶家族的应用展示，监督微调后的模型不仅能生成更为新颖的序列，而且在目标设计约束条件和新兴蛋白质属性测量方面也表现出了改进的特点。

Conclusion: 本研究提供了一种有效的策略，用于改善蛋白质语言模型生成序列的质量，特别是在稳定性、功能性和新颖性方面。此外，这种方法还促进了对超出自然界已知范围的新颖蛋白质序列的研究。

Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.

</details>


### [32] [Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality](https://arxiv.org/abs/2512.09355)
*Junru Zhou,Yicheng Wang,Pan Li*

Main category: cs.LG

TL;DR: 本研究探讨了子图GNN作为混合整数线性规划(MILP)中"学习分支"方法的理论中间地带。虽然理论上节点锚定子图GNN能够以低于3-WL的表达力近似强分支得分，但实证评估表明其计算成本高、内存瓶颈明显，导致解决时间比MPNNs和启发式方法慢。


<details>
  <summary>Details</summary>
Motivation: 尽管标准的消息传递GNN（MPNN）在处理MILP时效率高，但在表达能力上存在局限；而高阶GNN虽然表达能力强但计算成本过高。因此，研究旨在探索子图GNN是否能作为一种理论与实践上的折衷方案来优化MILP中的分支决策过程。

Method: 通过证明节点锚定子图GNN即使具有低于3-WL的表达力也足以近似强分支得分，并且对四个基准数据集进行了广泛的实证评估，以比较不同GNN模型在实际应用中的性能差异。

Result: 尽管从理论上讲，节点锚定子图GNN可以提供更好的分支决策，但其实现过程中面临着显著的内存瓶颈问题以及较慢的求解速度，这主要是由于它们相较于MPNNs和启发式方法有着更高的复杂度开销所致。

Conclusion: 对于MILP中的分支决策来说，目前高度表达性的GNN所带来的计算成本超过了其在决策质量上的提升，未来的研究需要集中在保持效率的同时增强模型的表达能力。

Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.

</details>


### [33] [KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction](https://arxiv.org/abs/2512.09365)
*Jiayu Qin,Zhengquan Luo,Guy Tadmor,Changyou Chen,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 本研究提出了一种新的框架，通过整合多种生物数据集并使用最优传输方法生成高质量伪标签来解决分子-蛋白质相互作用预测中的数据稀缺性和上下文信息不足问题，从而提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的分子-蛋白质相互作用（MPIs）模型面临两大挑战：标记的分子-蛋白质对稀缺以及大多数方法仅依赖于分子和蛋白质特征而忽略了更广泛的生物学背景信息，如基因、代谢途径等。为了解决这些问题，作者开发了一个新框架。

Method: 该框架首先聚集多样化的生物数据集，包括分子、蛋白质、基因及通路级别的相互作用；然后采用基于最优传输的方法为未标记的分子-蛋白质对生成高质量伪标签，利用已知相互作用的底层分布指导标签分配。

Result: 在多个MPI数据集上的评估表明，与最先进的方法相比，该框架在预测准确性和处理未见交互方面表现出显著改进。

Conclusion: 除了改善MPI预测外，这种方法还为利用多样化的生物数据源解决传统上受限于单一或双模态学习的问题提供了新的范式，为计算生物学和药物发现领域的未来进步铺平了道路。

Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.

</details>


### [34] [CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning](https://arxiv.org/abs/2512.09368)
*Mingyuan Li,Chunyu Liu,Zhuojun Li,Xiao Liu,Guangsheng Yu,Bo Du,Jun Shen,Qiang Wu*

Main category: cs.LG

TL;DR: 本研究提出了一种新的框架CFLight，该框架基于反事实学习来提高交通信号控制中的安全性。通过在发生不安全事件时回溯执行替代行动，并预测这些行动的结果，CFLight能够显著减少交叉口的碰撞事故并改善整体交通性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习方法在优化交通信号控制方面越来越受欢迎，但它们往往更注重驾驶效率而非安全性，且缺乏解释性。因此，需要一种能平衡效率与安全的方法，并且这种方法应当具有更高的可解释性。

Method: 提出了一种结合了反事实学习的新结构因果模型，用以预测不同行动后的结果。此外，还设计了一个新的反事实模块，该模块与额外的'X'模块集成，旨在促进更安全的强化学习实践。

Result: 实验表明，CFLight相比传统强化学习方法和最新的安全强化学习模型，在减少交通事故同时提升整体交通表现方面表现优异。

Conclusion: CFLight为强化学习提供了一个通用且安全的框架，不仅适用于交通信号控制领域，也为其他领域的应用开辟了可能性。

Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.

</details>


### [35] [Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs](https://arxiv.org/abs/2512.09369)
*Yezi Liu,William Youngwoo Chung,Hanning Chen,Calvin Yeung,Mohsen Imani*

Main category: cs.LG

TL;DR: PathHD, a lightweight KG reasoning framework, uses hyperdimensional computing and a single LLM call per query to improve efficiency, reduce costs, and maintain or enhance accuracy compared to neural baselines, while providing interpretable path-grounded rationales.


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的推理方法依赖于重型神经编码器或多次调用大型语言模型来对候选路径进行评分，导致延迟高、GPU成本大，并且决策过程不透明。这阻碍了这些方法的大规模可靠部署。

Method: 提出了PathHD框架，该框架使用超维度计算（HDC）替代神经路径评分，并且每个查询仅需要一次LLM调用。它将关系路径编码为块对角GHRR超向量，通过分块余弦相似度和Top-K剪枝来排名候选，最后通过单次LLM裁决给出最终答案及其支持路径。

Result: 在WebQSP、CWQ及GrailQA数据集上，PathHD以每次查询仅需一次LLM调用的情况下达到了与强神经基线相当甚至更好的Hits@1性能；减少了40-60%的端到端延迟以及3-5倍的GPU内存消耗；并且提供了忠实于路径的支持理由，增强了错误诊断能力和可控性。

Conclusion: 精心设计的HDC表示法为高效的知识图谱-大型语言模型推理提供了一个实用的基础，展示了在准确性、效率和可解释性之间的有利权衡。

Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

</details>


### [36] [Rates and architectures for learning geometrically non-trivial operators](https://arxiv.org/abs/2512.09376)
*T. Mitchell Roddenberry,Leo Tzou,Ivan Dokmanić,Maarten V. de Hoop,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 该论文扩展了学习理论，以包括双纤维化变换，并证明了这类算子不受维度诅咒的影响。同时，研究了一种基于水平集方法的交叉注意力架构，这种架构能够从很少的训练样本中稳定地学习双纤维化变换。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习方法已经被证明可以从非常少的训练样本中恢复高维空间之间的算子（例如PDEs的解映射），但这些结果主要针对具有简单几何形状的椭圆型算子。对于涉及奇点传播的问题（如波动、对流和流体动力学）的应用情况，其有效性仍需进一步探索。

Method: 通过将学习理论扩展到包含双纤维化变换（一种几何积分算子，包括广义Radon变换和测地线射线变换）来解决上述问题。此外，还研究了一种受到水平集方法启发的类似交叉注意力机制的架构，用以明确编码这些变换的几何特性。

Result: 证明了所考虑的双纤维化变换类别的算子不遭受维度诅咒：误差下降速度超过任何固定次幂的倒数训练样本数量。并且，提出的架构能够在极少量训练样本的情况下实现双纤维化变换的学习，展现出通用性、稳定性和高效性。

Conclusion: 本研究为科学机器学习中的算子学习提供了新的见解，特别是对于那些涉及到未知方式下奇点传播的问题。提出的方法不仅在理论上证明了其有效性，而且通过特定架构的设计展示了实际应用潜力。

Abstract: Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.

</details>


### [37] [Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM](https://arxiv.org/abs/2512.09378)
*Xun Li,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出了一种基于轻量级去噪扩散概率模型（LDPM）的联邦蒸馏辅助车辆边缘缓存方案，有效降低了通信开销，提高了缓存命中率，并对车辆速度变化具有良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了在不暴露用户隐私的情况下准确预测车辆用户感兴趣的内容，同时减少联邦学习中的通信开销以及解决车辆可能在训练完成前离开路边单元覆盖区域导致的问题。

Method: 采用一种结合了联邦蒸馏与轻量级去噪扩散概率模型(LDPM)的新方法来实现车辆边缘缓存，旨在保护隐私的同时提高系统性能。

Result: 仿真结果表明，所提出的方案能够显著降低通信开销、提升缓存命中率，并且对于不同车辆速度表现出良好的适应能力。

Conclusion: 该研究为车辆边缘缓存提供了一种有效的解决方案，不仅有助于保护用户隐私，还通过减少通信开销和增强系统鲁棒性提升了整体服务质量。

Abstract: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.

</details>


### [38] [Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting](https://arxiv.org/abs/2512.09398)
*Hongjun Wang,Jiawei Yong,Jiawei Wang,Shintaro Fukushima,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出了ConFormer，一种结合图传播与引导归一化层的新框架，用于交通预测。通过整合交通事故和规定数据，该模型在预测性能和效率上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的交通预测模型往往忽略了外部因素（如交通事故和规定）的影响，因为这些信息通常没有被很好地整合到数据中。这限制了预测的准确性。

Method: 开发了两个新的交通数据集，包括东京和加利福尼亚的数据，并且包含了交通事故和规定的信息。基于此，提出了名为ConFormer（条件变换器）的新框架，它结合了图传播技术与一个能够根据历史模式动态调整空间和时间节点间关系的引导归一化层。

Result: ConFormer在多个指标上持续优于主流时空基线模型，并且相比最先进的STAEFormer，在预测性能和效率方面都有所提升，同时保持了较低的计算成本和参数需求。

Conclusion: 研究表明，通过更好地整合外部因素信息并采用先进的模型设计，可以显著提高交通预测的准确性和效率。ConFormer为未来的研究提供了新的方向。

Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.

</details>


### [39] [Cauchy-Schwarz Fairness Regularizer](https://arxiv.org/abs/2512.09467)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文提出了一种新的Cauchy-Schwarz公平正则化器，用于减少模型预测和敏感属性之间的依赖性。该正则化器基于Cauchy-Schwarz散度，在多个数据集上的一系列实验表明，它在保持竞争力的准确性的同时持续改进了人口统计学平等性和机会均等指标，并且在不同的超参数设置下实现了更稳定的效用-公平性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的公平正则化方法由于使用了异质的距离测量和设计选择，导致其行为难以理解和性能不稳定。这引发了关于什么性质构成一个好的公平正则化器的基本问题。

Method: 研究者首先将现有的处理中方法分为三类：（i）匹配敏感群体间的预测统计量，（ii）对齐潜在表示，以及（iii）直接最小化预测与敏感属性之间的依赖性。受这些特性启发，提出了一个基于经验Cauchy-Schwarz散度来惩罚条件于敏感群体预测分布差异的Cauchy-Schwarz公平正则化器。

Result: 广泛的实验表明，提出的CS正则化器不仅在人口统计学平等性和机会均等度量上一致地优于现有方法，同时保持了有竞争力的准确性，并且相比于先前的正则化器，在不同超参数设定下提供了更加稳定的效用-公平性折衷。

Conclusion: 通过引入Cauchy-Schwarz公平正则化器，论文为机器学习中的组公平性提供了一个新工具，该工具能够有效提高公平性指标表现，同时维持良好的模型准确率。

Abstract: Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.

</details>


### [40] [Representation Invariance and Allocation: When Subgroup Balance Matters](https://arxiv.org/abs/2512.09496)
*Anissa Alloula,Charles Jones,Zuzanna Wakefield-Skorniewska,Francesco Quinzan,Bartłomiej Papież*

Main category: cs.LG

TL;DR: 研究发现，训练数据中不同群体的不平衡分布有时反而能提高模型在子群体上的表现。提出了一种假设，认为微调模型对子群体表示的依赖程度取决于预训练模型潜在空间中子群体之间的分离程度，并通过理论和实证分析验证了这一假设。


<details>
  <summary>Details</summary>
Motivation: 探索在训练数据中不同人口群体代表性不均等的情况下，如何影响模型在各个子群体上的泛化能力。挑战传统观点，即平衡子群体代表性总是优化性能。

Method: 通过系统地改变四个视觉和语言模型的训练数据组成来研究子群体分配，提出了潜分离假设，该假设说明了微调模型对子群体表示的依赖性与预训练模型潜在空间中子群体之间分离程度的关系，并对此进行了形式化、理论分析及实证验证。

Result: 发现子群体的表现对于数据平衡的敏感度存在差异，在某些情况下，不平衡的数据分布实际上可以改善子群体的表现；提出了潜分离假设并得到了验证。

Conclusion: 定量分析潜在子群体分离可以帮助指导基础模型微调过程中的数据收集和平衡决策。

Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.

</details>


### [41] [Contextual Dynamic Pricing with Heterogeneous Buyers](https://arxiv.org/abs/2512.09513)
*Thodoris Lykouris,Sloan Nietert,Princewill Okoroafor,Chara Podimata,Julian Zimmert*

Main category: cs.LG

TL;DR: 本文研究了具有异质买家群体的上下文动态定价问题，提出了一种基于乐观后验抽样的上下文定价算法，并为非上下文定价情况提出了一个方差感知缩放算法，实现了对K_★的最佳依赖。


<details>
  <summary>Details</summary>
Motivation: 与假设买方类型同质的先前工作不同，本文考虑了买方估值类型从具有有限支撑大小K_★的未知分布中抽取的情况。

Method: 开发了一种基于乐观后验抽样的上下文定价算法；对于非上下文定价情况，提出了一种方差感知缩放算法。

Result: 提出的上下文定价算法的遗憾被证明在d和T上是紧致的（除了对数项），并且方差感知缩放算法对于K_★达到了最优依赖。

Conclusion: 本文通过引入新的方法来处理异质买家群体的上下文动态定价问题，提供了一种有效的方法来减少遗憾，并且在特定情况下能够优化对买家类型数量的依赖。

Abstract: We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\widetilde{O}(K_{\star}\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\star}$.

</details>


### [42] [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](https://arxiv.org/abs/2512.09517)
*Nabil Anan Orka,Ehtashamul Haque,Maftahul Jannat,Md Abdul Awal,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: 研究提出了一种端到端全量子卷积模型QuanvNeXt，用于基于EEG的抑郁症诊断。通过引入新颖的交叉残差块，该模型减少了特征同质性并加强了跨特征关系，同时保持了参数效率。在两个开源数据集上的评估显示，QuanvNeXt达到了93.1%的平均准确率和97.2%的AUC-ROC，优于现有技术基准。此外，不确定性分析表明其预测结果校准良好，即使在高扰动水平下ECE分数也保持在低至中等范围。可解释AI分析进一步证实了QuanvNeXt能够有效识别区分健康对照组与重度抑郁障碍患者的时空模式。


<details>
  <summary>Details</summary>
Motivation: 为了提高基于EEG的抑郁症诊断准确性，并解决现有方法中存在的特征同质性问题及跨特征关系不足的问题，提出了新的模型QuanvNeXt。

Method: 设计了一个名为QuanvNeXt的全量子卷积架构，其中包括一个创新的交叉残差块，旨在减少特征同质性、增强跨特征间联系且不牺牲参数效率。

Result: QuanvNeXt在两个公开数据集上分别实现了93.1%的平均准确率和97.2%的AUC-ROC，表现优于如InceptionTime等当前最优基线方法。此外，在不同噪声水平下的不确定性分析证明了其预测结果的良好校准性；而事后可解释AI分析则验证了该模型能够有效地学习区分正常人与抑郁症患者的重要时空特征。

Conclusion: QuanvNeXt为基于EEG的抑郁症诊断提供了一种高效可靠的新途径，其不仅提高了分类性能，还增强了模型输出的可解释性和稳定性。

Abstract: This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.

</details>


### [43] [Latent-Autoregressive GP-VAE Language Model](https://arxiv.org/abs/2512.09535)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 研究了一种基于高斯过程（GP）的完全潜在自回归方案，并将其集成到变分自动编码器（VAE）中，以支持语言模型中的部分时间结构。


<details>
  <summary>Details</summary>
Motivation: 探索将序列动态从观察空间转移到连续潜在空间的可能性，同时保持语言生成通过非自回归解码器并行进行，旨在利用潜在空间的概率几何来支撑语言模型的部分时间结构，而非依赖于显式的神经操作。

Method: 提出了一个完整的方法论框架，包括因果GP先验、结构化的摊销后验以及基于正则化ELBO的训练协议。

Result: 在特意限制的概念验证框架内进行的经验评估表明，该模型可以稳定训练，并且序列和平行采样变体表现出一致的行为。

Conclusion: 结果表明，语言模型中的部分时间结构可以通过潜在空间的概率几何来支撑，而不是仅仅依靠显式的神经操作。

Abstract: We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.

</details>


### [44] [Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models](https://arxiv.org/abs/2512.09591)
*Magnus Ruud Kjaer,Rahul Thapa,Gauri Ganjoo,Hyatt Moore,Poul Joergen Jennum,Brandon M. Westover,James Zou,Emmanuel Mignot,Bryan He,Andreas Brink-Kjaer*

Main category: cs.LG

TL;DR: The paper introduces Stanford Sleep Bench, a large-scale polysomnography (PSG) dataset for sleep analysis, and evaluates self-supervised representation learning (SSRL) pre-training methods across various sleep-related tasks. It highlights the superior performance of contrastive learning in mortality and disease prediction, while also providing resources to support reproducibility and further research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations in sleep foundation models, specifically the lack of a shared dataset with diverse tasks and the absence of systematic evaluation of SSRL approaches across sleep-related tasks, by introducing a comprehensive PSG dataset and evaluating SSRL pre-training methods on it.

Method: The authors created Stanford Sleep Bench, a large-scale PSG dataset, and systematically evaluated several SSRL pre-training methods on this dataset across four different tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction.

Result: Results indicate that multiple pretraining methods perform comparably well for sleep staging, apnea diagnosis, and age estimation. Contrastively, for mortality and disease prediction, contrastive learning outperforms other methods and converges faster during pretraining.

Conclusion: Stanford Sleep Bench addresses key gaps in the field of sleep analysis, demonstrating that contrastive learning is particularly effective for predicting mortality and diseases. The release of the dataset and associated materials will help advance and standardize sleep research.

Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.

</details>


### [45] [Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks](https://arxiv.org/abs/2512.09621)
*Jingbo Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen*

Main category: cs.LG

TL;DR: 提出了一个三边合作语义通信框架，用于车辆用户在高速公路场景下通过V2I和V2V进行语义任务卸载，并使用MAPPO-PDN和线性规划解决优化问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决互联网车辆中高效边缘任务处理的问题，在高速公路场景下，提出了一种新的三边合作语义通信（TCSC）框架，允许车辆用户通过车对基础设施（V2I）和车对车（V2V）通信执行语义任务卸载。

Method: 该研究首先构建了一个混合整数非线性规划（MINLP）问题，然后将其分解为两个子问题。对于语义符号数量的优化问题，创新地提出了一种基于参数分布噪声的多智能体近端策略优化方法（MAPPO-PDN）。而卸载比例问题则采用线性规划（LP）来求解。

Result: 仿真结果显示，所提出的方案性能优于其他算法。

Conclusion: 通过结合语义通信与车联网边缘计算，以及提出的新框架TCSC，能够有效提高互联网车辆中的任务处理效率。

Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.

</details>


### [46] [Membership and Dataset Inference Attacks on Large Audio Generative Models](https://arxiv.org/abs/2512.09654)
*Jakub Proboszcz,Paweł Kochanski,Karol Korszun,Donato Crisostomi,Giorgio Strano,Emanuele Rodolà,Kamil Deja,Jan Dubinski*

Main category: cs.LG

TL;DR: 本研究探讨了通过成员推断攻击（MIA）和数据集推断（DI）来验证音频样本是否被用于训练生成式音频模型的可行性。实验结果表明，对于大型多样化数据集训练的模型，单一样本成员推断效果有限；而基于多个样本的数据集推断则更为有效，为版权保护提供了一个有前景的方向。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散和自回归架构的生成式音频模型在质量和表现力上的快速发展，这些模型往往是在大量艺术与商业作品上训练而成的，这引发了紧迫的版权问题。研究旨在探索能否可靠地验证艺术家的作品是否被纳入了训练数据集中，从而让版权所有者能够保护其内容。

Method: 采用成员推断攻击（MIA）对开源生成式音频模型进行分析，试图确定特定音频样本是否属于训练集的一部分。鉴于单一样本成员信号较弱的问题，进一步发展了数据集推断（DI）方法，该方法能够在多个样本上聚合不同的成员证据。

Result: 实证结果显示，针对大型且多样化的训练数据集，单独使用成员推断攻击的效果有限；然而，当考虑到艺术家或媒体所有者通常持有作品集合而非孤立样本时，数据集推断展现出了在音频领域内成功应用的可能性。

Conclusion: 数据集推断为评估艺术家作品是否贡献于模型训练提供了一种更加实用的方法，在大型音频生成模型时代为版权保护和数据集责任归属指明了一个充满希望的方向。

Abstract: Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.

</details>


### [47] [A data-driven approach to linking design features with manufacturing process data for sustainable product development](https://arxiv.org/abs/2512.09690)
*Jiahang Li,Lucas Cazzonelli,Jacqueline Höllig,Markus Doellken,Sven Matthiesen*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的方法，用于映射和分析设计特征与制造过程数据之间的关系，通过开发一个机器学习模型来自动提供建议以改进设计，并将制造过程数据与可持续性指标结合，为可持续产品开发开辟了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 随着工业物联网（IIoT）技术的日益普及，制造业能够实现过程数据的自动化实时收集，这为数据驱动的产品开发带来了新的机会。然而，当前的数据驱动方法大多局限于特定领域如设计或制造，对于整合设计特性与制造过程数据的研究较少。鉴于设计决策对制造结果有重大影响，缺乏这种整合限制了数据驱动产品设计改进的潜力。

Method: 本研究开发了一个全面的系统架构，确保持续的数据收集与整合。基于设计特性和制造过程数据之间的联系，构建了一个机器学习模型，该模型可以提供自动化的设计改进建议。

Result: 通过整合设计特征与制造过程数据，成功建立了一个有助于自动生成设计改进建议的机器学习模型。此外，这种方法还通过结合制造过程数据与可持续性指标，为可持续产品开发提供了新的方向。

Conclusion: 该论文展示了一种有效的方法，利用数据驱动的方式探索设计特征与制造过程数据间的关系，不仅促进了更高效的产品设计流程，同时也为推动可持续发展提供了新思路。

Abstract: The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.

</details>


### [48] [Mixture of Lookup Key-Value Experts](https://arxiv.org/abs/2512.09723)
*Zongcheng Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型MoLKV，通过引入基于查询-键值对的上下文感知机制来改进MoLE架构，从而在资源受限设备上实现更好的性能。实验表明，MoLKV在小规模评估中显著降低了验证损失。


<details>
  <summary>Details</summary>
Motivation: 尽管MoLE架构适合资源受限的终端用户设备，但由于其专家选择机制仅依赖于输入ID而不考虑上下文信息，这可能限制了模型的表现。为了克服这一局限性，研究者们提出了MoLKV模型，它能够根据当前序列中的上下文信息动态地选择专家。

Method: MoLKV模型将每个专家设计为键值对的形式。对于给定输入，由输入生成的查询与当前序列中缓存的键值对专家进行交互，产生一个基于上下文的专家输出。这种机制允许模型在做出预测时考虑到更丰富的上下文信息。

Result: 实验结果显示，在小规模测试中，相比原有的MoLE架构，MoLKV模型达到了明显更低的验证损失。

Conclusion: 通过引入基于查询-键值对的上下文感知机制，MoLKV成功地提高了原始MoLE架构在资源受限环境下的表现。

Abstract: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

</details>


### [49] [Circuits, Features, and Heuristics in Molecular Transformers](https://arxiv.org/abs/2512.09757)
*Kristof Varadi,Mark Marosi,Peter Antal*

Main category: cs.LG

TL;DR: 本文通过分析在类药物小分子上训练的自回归transformer，揭示了其在多个抽象层次上捕捉分子表示规则的能力背后的计算结构，并使用稀疏自动编码器提取与化学相关的激活模式特征字典。


<details>
  <summary>Details</summary>
Motivation: 尽管transformer能够生成有效且多样的化学结构，但对其如何捕捉分子表示规则的机制知之甚少。研究旨在探索这些模型内部的计算机制，以更好地理解它们是如何学会遵守化学有效性约束的。

Method: 采用自回归transformer对类药物小分子进行训练，并通过稀疏自动编码器（SAE）来提取与化学相关激活模式的特征字典。接着，在下游任务中验证了所发现机制的实际应用价值。

Result: 发现了与低级句法解析和更抽象的化学有效性约束相一致的计算模式；基于稀疏自动编码器提取到的特征有助于提高在多种实际场景下的预测表现。

Conclusion: 本研究表明，通过对自回归transformer进行机制性分析，可以揭示其捕捉分子表示规则能力背后隐藏的计算结构，而且这种洞察对于提升模型在实际化学问题中的性能具有重要意义。

Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.

</details>


### [50] [Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering](https://arxiv.org/abs/2512.09810)
*Adithya K Moorthy,V Vijaya Saradhi,Bhanu Prasad*

Main category: cs.LG

TL;DR: 该研究提出了一种新的方法来构建公平的k-最近邻(kNN)图和公平的ε-邻域图，通过在图构建过程中主动实施人口统计学平价，从而改善了谱聚类的公平性。实验结果表明，这种方法在合成数据集、真实表格数据集和图像数据集上都优于当前基准。


<details>
  <summary>Details</summary>
Motivation: 传统图聚类方法往往由于不公平的图构造而造成某些群体被低估，这导致了偏见的持续存在。本研究旨在通过在图形成过程中积极执行公平性约束，解决预处理阶段对于实现公平谱聚类的关键空白。

Method: 研究者们开发了新的方法来创建公平的k-最近邻(kNN)图和公平的ε-邻域图，在邻居选择步骤的最早阶段就加入了公平性约束条件，确保敏感特征在局部图结构中的比例代表同时保持几何一致性。

Result: 广泛的实验显示，所提出的公平图构造方法在三个合成数据集、七个实际表格数据集以及三个实际图像数据集上的图聚类任务中超越了现有基线。

Conclusion: 这项研究表明，在图构建过程中加入拓扑公平性能够自然地促进更公平的谱聚类结果，而无需对聚类算法本身进行修改。

Abstract: Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.

</details>


### [51] [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](https://arxiv.org/abs/2512.09850)
*Simone Cuonzo,Nina Deliu*

Main category: cs.LG

TL;DR: 本文提出了Conformal Bandits框架，将Conformal Prediction（CP）引入到经典的多臂老虎机问题中，旨在在有限时间内提供预测覆盖率的统计保证的同时最小化遗憾。通过模拟研究和投资组合分配应用，展示了该框架在小差距场景下的实践优势以及在实现名义覆盖率方面的附加价值，并且通过结合隐马尔可夫模型来捕捉金融市场中的状态转换行为，进一步提高了风险调整后的回报效率。


<details>
  <summary>Details</summary>
Motivation: 传统的多臂老虎机策略如汤普森采样和上置信界(UCB)通常依赖于分布假设或渐近保证；此外，它们主要关注遗憾，而忽视了其统计特性。为了填补这一空白，作者提出了一种新的框架，旨在将决策过程中最小化遗憾与统计保证结合起来。

Method: 作者通过引入Conformal Prediction (CP)技术到多臂老虎机问题中创建了一个名为Conformal Bandits的新框架。该方法不仅考虑到了遗憾最小化的问题，还提供了形式上的统计覆盖保证。同时，在特定的应用案例中，通过整合隐藏马尔科夫模型来更好地捕捉金融市场的制度转换行为，从而改善探索-利用平衡。

Result: 研究表明，相较于传统策略，Conformal Bandits在处理小差距情境时展现出更好的性能，特别是在达到名义覆盖率方面。当应用于投资组合配置时，能够获得更高的风险调整后收益效率。

Conclusion: Conformal Bandits为解决序列决策问题提供了一种新颖的方法，它成功地将遗憾最小化与统计保障相结合，尤其适用于奖励差异较小的情况。此外，结合隐藏马尔可夫模型可以更有效地应对金融市场中的挑战，提高整体表现。

Abstract: We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.
  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.

</details>


### [52] [HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression](https://arxiv.org/abs/2512.09886)
*Gustavo Coelho Haase,Paulo Henrique Dourado da Silva*

Main category: cs.LG

TL;DR: 提出了一种名为HPM-KD的新框架，通过集成六个协同组件解决了知识蒸馏中的几个关键问题，如超参数敏感性、容量差距、多教师协调不佳和计算资源利用效率低下。实验表明该方法在保持高精度的同时实现了显著的模型压缩，并减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 为了解决知识蒸馏技术中存在的几个主要问题：对超参数敏感需要大量手动调整、从大型教师模型到小型学生模型的知识转移存在能力差距、多教师场景下协调不佳以及计算资源使用效率低。

Method: 提出了HPM-KD框架，集成了六个相互补充的部分：自适应配置管理器、渐进式蒸馏链、注意力加权多教师合奏、元学习温度调度器、并行处理管道和共享优化内存。

Result: 实验结果表明，HPM-KD能够在保持85%准确度的同时实现10倍至15倍的模型压缩；消除了手动调参的需求；并通过并行化减少了30-40%的训练时间。消融研究证实了每个组成部分的独立贡献（0.10-0.98个百分点）。

Conclusion: HPM-KD框架有效解决了现有知识蒸馏方法面临的问题，不仅提高了模型压缩率还减少了训练时间和人工干预，同时保持了良好的性能。

Abstract: Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.

</details>


### [53] [Provably Learning from Modern Language Models via Low Logit Rank](https://arxiv.org/abs/2512.09892)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 本文基于现代语言模型在经验上表现出的低logit秩特性，提出了一种通过查询学习模型来高效学习近似低logit秩模型的算法，并提供了对现代语言模型具有实际意义的学习保证。


<details>
  <summary>Details</summary>
Motivation: 研究者们观察到，尽管现代语言模型及其内部机制非常复杂，但它们似乎都拥有一个共同的特点：大约的低logit秩。这意味着这些模型输出的概率分布可以用一个低秩矩阵很好地近似。这一发现为理解及改进语言模型提供了一个新的视角。

Method: 采用了一种基于logit查询的查询学习模型方法，该方法能够反映常见API访问模式，从而开发出一种有效学习任何近似低logit秩模型的算法。

Result: 成功设计并证明了一种针对近似低logit秩模型的有效学习算法，能够在查询条件下学习这样的模型。

Conclusion: 首次为捕捉到了现代语言模型特征的一种生成模型提供了端到端的学习保障，强调了所提出的结构假设与现实中观察到的语言模型行为紧密相关。

Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.

</details>


### [54] [FALCON: Few-step Accurate Likelihoods for Continuous Flows](https://arxiv.org/abs/2512.09914)
*Danyal Rehman,Tara Akhound-Sadegh,Artem Gazizov,Yoshua Bengio,Alexander Tong*

Main category: cs.LG

TL;DR: 提出了一种名为FALCON的新方法，通过引入混合训练目标来鼓励可逆性，实现了少步准确的似然估计，从而在分子玻尔兹曼采样中优于现有的正态流模型，并且速度提高了两个数量级。


<details>
  <summary>Details</summary>
Motivation: 当前用于解决统计物理中热力学平衡状态下分子状态可扩展采样的Boltzmann生成器主要使用连续归一化流（CNFs），但其似然计算成本极高，限制了实际应用。为了解决这一问题，本文提出了FALCON方法。

Method: FALCON通过引入一种新的混合训练目标来提高模型的可逆性，使得能够在保证重要性采样应用所需的足够精确度的前提下，仅需少量步骤即可完成采样。

Result: 实验表明，FALxon不仅在分子玻尔兹曼采样任务上超越了最新的正态流模型，而且相较于表现相当的CNF模型速度快了两个数量级。

Conclusion: 该研究成功地开发出了一种更为高效且快速的分子状态采样方法FALCON，它极大地降低了似然估计所需的时间成本，同时保持了良好的性能，为相关领域的进一步研究提供了有力支持。

Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.

</details>


### [55] [Closing the Train-Test Gap in World Models for Gradient-Based Planning](https://arxiv.org/abs/2512.09929)
*Arjun Parthasarathy,Nimit Kalra,Rohun Agrawal,Yann LeCun,Oumayma Bounou,Pavel Izmailov,Micah Goldblum*

Main category: cs.LG

TL;DR: 本文提出了一种改进的世界模型训练方法，通过在训练时合成数据来缩小训练-测试差距，使得基于梯度的规划能够更高效。该方法在对象操作和导航任务中表现出色，性能优于或等同于传统的无梯度交叉熵方法(CEM)，且仅需后者10%的时间预算。


<details>
  <summary>Details</summary>
Motivation: 作者观察到，虽然世界模型是根据下一个状态预测目标进行训练的，但在测试时它实际上被用来估计一系列动作。这导致了训练与测试之间的差距。为了解决这个问题，并提高基于梯度规划的表现，提出了新的训练方法。

Method: 本文引入了训练时的数据合成技术，旨在改善现有世界模型下基于梯度规划的效果。这些技术设计用于减少训练过程中与实际使用场景之间的差异。

Result: 实验结果表明，在多个物体操控及导航任务上，所提出的方法能够在只有传统无梯度交叉熵方法所需时间的十分之一的情况下，达到相同甚至更好的效果。

Conclusion: 通过优化世界模型训练流程中的数据生成策略，可以显著提升基于梯度规划算法的效率与表现，从而为解决复杂规划问题提供了一条新路径。

Abstract: World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [56] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: 本文提出了一种新的标记路由算法METRO，用于在内存受限条件下优化专家并行MoE服务性能。通过平衡每个GPU激活的专家数量而非处理的标记数量，METRO能够在保持高质量路由的同时减少解码延迟和提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的专家并行方法试图通过平衡每个GPU处理的标记数量来解决负载不平衡问题，但在内存受限情况下，这种方法反而会增加被激活的专家数量，从而加剧内存压力。

Method: 提出了最小专家标记路由（METRO），这是一种针对内存受限情况下的高性能专家并行MoE服务设计的新颖标记路由算法。METRO通过优化算法效率以及利用GPU的并行处理能力实现了接近最优的路由质量，并且采用了新型allGather方案以最小开销收集全局top-k知识。

Result: 实验评估表明，在真实系统(vLLM跨8个A100 GPU)和专有模拟器(8-16 B200 GPU)上，与EPLB相比，METRO能够将Qwen3和DeepSeek-V3服务中的解码延迟降低11%-22%，总吞吐量提高3%-21%；并且在固定解码SLO下，METRO比EPLB提高了多达4.11倍的解码吞吐量。

Conclusion: METRO算法通过有效地管理内存使用并优化了专家并行模型的服务性能，在内存成为瓶颈的情况下显著优于传统方法。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [57] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 提出了一种新的分布式、分层卸载框架，用于视觉转换器（ViTs），通过设计解决隐私挑战。该框架使用本地可信边缘设备作为编排器，将用户的视觉数据分割成更小的部分，并分发到多个独立的云服务器上，从而防止任何单一外部服务器拥有完整的图像，增强内容隐私性。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉智能工具虽然提供了便利，但对计算能力要求高，超出了资源受限的移动和可穿戴设备的能力。尽管将视觉数据卸载到云端是一种常见解决方案，但这在传输和服务器端计算过程中引入了显著的隐私漏洞。因此，需要一种能够保护用户隐私的新方法。

Method: 设计了一个新的分布式、分层卸载框架，专门针对视觉转换器（ViTs）。该方案利用像智能手机或NVIDIA Jetson这样的本地可信边缘设备作为边缘编排器，负责将用户的视觉数据划分为较小部分并分散至多台独立云服务器上处理。通过这种方式，确保没有单个外部服务器能够获得完整的图像信息，从而避免了全面的数据重建可能性。最终的数据合并与聚合计算仅发生在用户的可信边缘设备上。

Result: 本研究以Segment Anything Model (SAM)为例进行了实际案例研究，结果显示所提出的方法相比传统的基于云的方法大大提高了内容隐私性。评估表明，在保持接近基线的分割性能的同时，显著降低了内容重构和用户数据暴露的风险。

Conclusion: 提出的框架为边缘-云计算连续体中的视觉任务提供了一个可扩展且能保护隐私的解决方案，有效解决了现有技术中存在的隐私问题，同时保持了良好的性能表现。

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [58] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: BatANN, a distributed, disk-based approximate nearest neighbor (ANN) system, efficiently handles large-scale vector search, achieving near-linear throughput scaling and maintaining low latency on 100M- and 1B-point datasets.


<details>
  <summary>Details</summary>
Motivation: 随着数据集扩展到数十亿个向量，单服务器解决方案变得不再适用。为了应对未来更大规模的数据集，需要开发一种既能保持搜索效率又能实现近线性吞吐量扩展的分布式磁盘基础向量搜索系统。

Method: BatANN通过在访问存储于另一台机器上的邻域时，将查询的完整状态发送至该机器以继续执行，从而提高局部性。该方法确保了即使在分布式环境中也能维持单一全局图的对数搜索效率。

Result: 在使用10台服务器处理1亿和10亿点的数据集时，BatANN相比散射-聚集基线分别达到了6.21-6.49倍和2.5-5.10倍的吞吐量提升，并且平均延迟保持在6毫秒以下。此外，这些成果是在标准TCP上取得的。

Conclusion: BatANN是首个基于单一全局图运作的开源分布式磁盘基础向量搜索系统，它为大规模数据集提供了高效、可扩展的搜索解决方案。

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [59] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: 本文提出了一种名为WarmServe的多语言模型服务系统，通过预见未来工作负载来预热GPU，从而提高推理性能。该系统能有效减少首次出词时间（TTFT），相比现有基于自动扩展的服务系统，TTFT提高了最多50.8倍，并且能够处理比GPU共享系统多2.5倍的请求量。


<details>
  <summary>Details</summary>
Motivation: 当前多LLM服务系统为了优化GPU利用率而牺牲了推理性能，特别是在首次出词时间(TTFT)方面表现不佳。作者发现这种折衷的根本原因在于这些系统对将来工作负载特征缺乏了解。相反地，最近对于真实世界追踪数据的研究表明，LLM服务工作负载具有很高的周期性和长期可预测性。

Method: 提出了通用GPU工作者的概念，允许基于对未来工作负载的认知进行一对多的GPU预热。基于此概念设计并构建了WarmServe系统，它通过采用感知驱逐的模型放置策略来缓解集群范围内的预热干扰；通过主动预热提前准备通用GPU工作者；以及通过零开销内存切换机制管理GPU内存。

Result: 在实际数据集上的评估显示，与最先进的基于自动缩放的系统相比，WarmServe将TTFT提高了多达50.8倍，同时能够比GPU共享系统多服务高达2.5倍的请求数量。

Conclusion: WarmServe证明了通过利用工作负载的可预测性来进行有效的资源管理和预热可以显著改善多LLM服务系统的性能，特别是在降低TTFT和增加服务能力方面。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [60] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: 提出了一种基于帕累托的混合鲸鱼-海鸥优化算法（PHWSOA），用于解决云计算中的任务调度问题，同时优化了完成时间、虚拟机负载均衡和经济成本三个关键目标。实验结果表明，与WOA、GA、PEWOA和GCWOA等基线方法相比，该算法在减少完成时间、改善负载均衡及节省成本方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的任务调度解决方案主要针对单一或有限指标进行优化，忽视了多目标综合优化的需求。为了解决这一问题，本文提出了一个能够同时优化多个关键目标的新算法。

Method: 通过结合鲸鱼优化算法（WOA）和海鸥优化算法（SOA）的优点，并利用帕累托优势原则，开发出一种新的Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA)。此外，还引入了Halton序列初始化、帕累托引导变异机制以及并行处理来加速收敛速度，并集成了动态虚拟机负载重新分配机制以改进执行过程中的负载平衡。

Result: 在CloudSim模拟器上使用NASA-iPSC和HPC2N的真实工作负载跟踪数据进行的广泛实验表明，PHWSOA在减少makespan、提高VM负载平衡和降低成本方面实现了显著提升，具体表现为：makespan减少了72.1%，VM负载平衡提高了36.8%，成本节约了23.5%。这些成果明显优于包括WOA、GA、PEWOA和GCWOA在内的基准方法。

Conclusion: PHWSOA在云计算环境中展现出了强大的潜力，能够有效管理资源并实现多目标优化，为实际应用提供了高效的任务调度方案。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [61] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: 介绍了一种用于粒子图像测速法的合成图像生成器SynthPix，该软件包在性能和并行处理方面表现优异，特别适用于强化学习方法的数据训练和快速流估计方法的开发。


<details>
  <summary>Details</summary>
Motivation: 为了支持数据需求量大的强化学习方法进行流估计，并减少最近带有实时PIV反馈的主动流体控制研究中快速流估计方法开发过程中的迭代时间。

Method: 使用JAX实现了一个名为SynthPix的合成图像生成器，它专注于加速器上的性能和并行性，并且与现有工具相比，在每秒生成图像对的数量上实现了几倍的吞吐量提升。

Result: SynthPix能够以比现有工具高几个数量级的速度生成图像对，从而更好地服务于流体动力学社区的数据密集型应用。

Conclusion: SynthPix软件包为流体动力学领域提供了一个高效、高性能的解决方案，尤其适合于需要大量合成图像来训练模型或加快研发周期的应用场景。

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [62] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: 研究揭示了GPU深度学习训练中straggller的普遍性及其成因，提出了一种新的系统STAR来减少Time-To-Accuracy(TTA)，并通过资源重新分配和避免CPU与带宽过载来预防straggler。


<details>
  <summary>Details</summary>
Motivation: 当前对GPU深度学习训练场景下stragglers的普遍性、成因及影响以及现有缓解方法的有效性尚缺乏充分理解。

Method: 通过全面实验发现stragglers的主要原因是CPU和带宽使用不平衡，并且现有的从同步随机梯度下降(SSGD)切换到异步SGD(ASGD)的方法可能不会改善TTA甚至产生更多stragglers。为解决这些问题，提出了Straggler Tolerant And Resilient DL训练系统(STAR)，该系统包括新的同步模式、选择最佳同步模式的启发式和机器学习方法，并能重分配资源以支持所选模式同时最小化对共置作业的影响。此外，它还通过避免在分配PSs时过度加载CPU和带宽资源以及在梯度传输过程中主动防止stragglers的出现。

Result: 基于AWS的追踪驱动评估显示，相比于最先进系统，在PS架构下STAR生成的TTA降低了48-84%，在all-reduce架构下降低了51-70%，同时保持了SSGD的收敛精度。

Conclusion: STAR系统有效减少了深度学习训练中的TTA，同时保持了模型准确性，表明其在处理stragglers问题上具有明显优势。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [63] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 本文介绍了一种既能引入无锁特性又能保证可恢复性的转换方法，该方法从基于锁的实现开始，为锁获取和释放操作提供了可恢复且无锁的替代方案。


<details>
  <summary>Details</summary>
Motivation: 为了提高并发程序的性能和可靠性，文章旨在开发一种能够同时提供无锁特性和可恢复性的技术。

Method: 通过设计一种新的转换机制，将原有的基于锁的操作替换为支持嵌套锁、具有可恢复性且保持原有正确性的无锁版本。

Result: 成功地提出并验证了这种新型转换方法的有效性，它可以在不牺牲程序正确性的前提下增强其并发控制能力和故障恢复能力。

Conclusion: 本研究为构建更高效及鲁棒性强的并发系统开辟了新途径，特别是对于需要高度可靠性和高性能的应用场景而言。

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>
