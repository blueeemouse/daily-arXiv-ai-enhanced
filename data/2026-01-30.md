<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 130]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization](https://arxiv.org/abs/2601.20868)
*Rongzheng Wang,Yihong Huang,Muquan Li,Jiakai Li,Di Liang,Bob Simons,Pei Ke,Shuang Liang,Ke Qin*

Main category: cs.LG

TL;DR: 提出了Dynamics-Aware Solver Heuristics (DASH)框架，旨在通过共优化求解器搜索机制和运行时调度来提高组合优化问题的解决效率，并利用Profiled Library Retrieval (PLR)技术减少重新适应的成本。实验表明，DASH在四个组合优化问题上提高了超过3倍的运行时间效率，同时在不同规模的问题上超越了现有最先进方法的解决方案质量，同时减少了90%以上的LLM适应成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型（LLMs）驱动启发式设计（LHD）的方法存在两个主要限制：仅依赖最终结果的质量评价而忽略了收敛过程及运行效率；以及当面临分布变化时需要高昂的成本去重新调整以生成适用于新实例组的特定求解器。为了解决这些问题，研究者们提出了新的框架。

Method: 提出了一种名为Dynamics-Aware Solver Heuristics (DASH)的新框架，该框架通过一种能够意识到收敛性的度量方式共同优化求解器搜索机制与运行时调度策略，从而找出高效且高性能的求解器。此外，为了降低重适应的成本，DASH引入了Profiled Library Retrieval (PLR)机制，在进化过程中有效归档特化求解器，以便于针对异构分布情况下的低成本预热启动。

Result: 在四个不同的组合优化问题上的实验显示，DASH不仅将运行时间效率提升了超过3倍，而且在各种问题规模下都超过了当前最先进的基线方法的解质量。更重要的是，借助基于概要的预热启动能力，DASH能够在保持高精度的同时，将LLM适应成本降低了90%以上。

Conclusion: DASH框架通过改进求解器的设计流程并结合高效的求解器归档与检索技术，显著增强了处理组合优化问题的能力，特别是在应对分布变化时表现出了更低的成本和更高的性能。

Abstract: Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.

</details>


### [2] [Finetune-Informed Pretraining Boosts Downstream Performance](https://arxiv.org/abs/2601.20884)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.LG

TL;DR: 提出了一种名为Finetune-Informed Pretraining (FIP)的方法，该方法通过提高目标模态的掩码难度、增强损失权重和增加解码器容量来优化特定模态的表示学习，从而在下游微调时提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态预训练在构建通用表示方面是有效的，但在实际应用中，通常只有单一模态在下游微调时被大量使用。标准预训练策略对所有模态一视同仁，这可能导致真正重要的模态的表示不够优化。

Method: Finetune-Informed Pretraining (FIP)是一种与模型无关的方法，它通过对指定的目标模态施加更高的掩码难度、更强的损失权重以及扩增解码器的能力来进行表示学习的偏向调整，同时不改变共享编码器或需要额外监督。

Result: 当应用于无线信号星座图上的掩码建模时，FIP能够在不需要额外数据或计算资源的情况下一致地提高下游微调后的表现。

Conclusion: FIP易于实现，与架构兼容，并且广泛适用于多种多模态掩码建模流程。

Abstract: Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.

</details>


### [3] [A generative machine learning model for designing metal hydrides applied to hydrogen storage](https://arxiv.org/abs/2601.20892)
*Xiyuan Liu,Christian Hacker,Shengnian Wang,Yuhua Duan*

Main category: cs.LG

TL;DR: 本研究提出了一种结合因果发现和轻量级生成机器学习模型的框架，用于生成新型金属氢化物候选材料。通过对1000个生成的候选材料进行排名和筛选后，确定了6种未报道过的化学式和晶体结构，其中4种通过密度泛函理论模拟得到了验证，显示出未来实验研究的强大潜力。该框架为扩展氢存储数据集和加速材料发现提供了一种可扩展且高效的方法。


<details>
  <summary>Details</summary>
Motivation: 开发新的金属氢化物是实现碳中和能源系统中有效储氢的关键步骤。然而，现有的材料数据库（如Materials Project）中包含良好表征的氢化物数量有限，这限制了最优候选材料的发现。

Method: 本工作介绍了一个将因果发现与轻量级生成机器学习模型相结合的框架，以生成可能不存在于当前数据库中的新型金属氢化物候选材料。使用一个由450个样本组成的数据库（270个训练样本、90个验证样本以及90个测试样本），模型生成了1,000个候选材料。

Result: 在对这1,000个候选材料进行排序和过滤之后，确定了六个之前未曾报告过的化学式及其晶体结构。其中四个通过密度泛函理论(DFT)模拟得到了验证，并显示出了强大的未来实验研究潜力。

Conclusion: 总体而言，所提出的框架提供了一种可扩展且时间效率高的方法来扩充氢存储数据集并加快材料发现过程。

Abstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.

</details>


### [4] [Is Parameter Isolation Better for Prompt-Based Continual Learning?](https://arxiv.org/abs/2601.20894)
*Jiangyang Li,Chenhao Ding,Songlin Dong,Qiang Wang,Jianchao Zhao,Yuhang He,Yihong Gong*

Main category: cs.LG

TL;DR: 提出了一种基于提示共享的持续学习框架，通过全局提示池和任务感知门控路由机制实现动态解耦和协作优化，并引入历史感知调节器来防止常用提示因过度更新而被遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的持续学习方法通常为每个任务分配固定的提示集，导致跨任务间知识隔离及参数利用不充分。为了改善这一情况，满足持续学习的实际需求，作者提出了新的解决方案。

Method: 构建了一个全局提示池，并采用任务感知的门控路由机制，稀疏激活部分提示以达成任务特定特征表示的动态解耦与协作优化；同时加入历史感知调节器，利用累积的提示激活统计数据保护频繁使用的提示免受过多更新的影响。

Result: 广泛的分析和实验证明了所提方法在有效性和效率方面均优于现有静态分配策略。

Conclusion: 该研究提供了一种改进的持续学习方案，能够更有效地利用参数并减少知识遗忘，代表了处理持续学习挑战的一个重要进展。

Abstract: Prompt-based continual learning methods effectively mitigate catastrophic forgetting. However, most existing methods assign a fixed set of prompts to each task, completely isolating knowledge across tasks and resulting in suboptimal parameter utilization. To address this, we consider the practical needs of continual learning and propose a prompt-sharing framework. This framework constructs a global prompt pool and introduces a task-aware gated routing mechanism that sparsely activates a subset of prompts to achieve dynamic decoupling and collaborative optimization of task-specific feature representations. Furthermore, we introduce a history-aware modulator that leverages cumulative prompt activation statistics to protect frequently used prompts from excessive updates, thereby mitigating inefficient parameter usage and knowledge forgetting. Extensive analysis and empirical results demonstrate that our approach consistently outperforms existing static allocation strategies in effectiveness and efficiency.

</details>


### [5] [TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins](https://arxiv.org/abs/2601.20906)
*Nikita Makarov,Maria Bordukova,Lena Voith von Voithenberg,Estrella Pivel-Villanueva,Sabrina Mielke,Jonathan Wickes,Hanchen Wang,Mingyu Derek Ma,Keunwoo Choi,Kyunghyun Cho,Stephen Ra,Raul Rodriguez-Esteban,Fabian Schmich,Michael Menden*

Main category: cs.LG

TL;DR: 本文介绍了一种名为TwinWeaver的开源框架，能够将患者的纵向历史序列化为文本，从而支持使用大型语言模型进行统一事件预测和预测。基于此框架构建的Genie Digital Twin (GDT)在多种癌症类型上实现了更准确的临床事件预测和风险分层，同时展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 精准肿瘤学需要预测临床事件及轨迹，但稀疏多模态临床时间序列建模仍是一个关键挑战。

Method: 开发了TwinWeaver框架，该框架可以将患者的长期病史序列化成文本格式，并利用大型语言模型来进行统一事件预测和趋势预测。基于此框架，在93,054名患者、20种不同类型癌症的数据集上建立了Genie Digital Twin (GDT)系统。

Result: GDT显著降低了预测误差，平均绝对缩放误差（MASE）中位数达到0.87；提高了风险分层准确性，生存、进展和治疗转换任务上的平均一致性指数(C-index)为0.703；并且在处理未见过的临床试验数据时表现优异。此外，TwinWeaver还提供了一个可解释性的临床推理扩展功能。

Conclusion: TwinWeaver框架及其所支撑的Genie Digital Twin (GDT)系统在预测精度、风险分层以及对新数据集的适应性方面都表现出色，为纵向临床建模提供了可扩展且透明的基础。

Abstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.

</details>


### [6] [Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges](https://arxiv.org/abs/2601.20913)
*Chen Feng,Minghe Shen,Ananth Balashankar,Carsten Gerner-Beuerle,Miguel R. D. Rodrigues*

Main category: cs.LG

TL;DR: 本文提出了一种"有噪声但有效"的假设检验框架，通过利用少量人工标注的校准集来估计评审者的真正例率和假正例率，从而对大型语言模型进行可靠的认证。该方法提供了有限样本类型I错误控制的理论保证，并且在Jigsaw评论、仇恨言论和SafeRLHF上的实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 对于大型语言模型（LLM）来说，验证其失败率低于安全阈值是至关重要的，但也充满挑战。尽管“LLM作为裁判”提供了一定的可扩展性，但裁判的不完美、噪音以及偏见可能会使统计保障失效。

Method: 引入了一种'有噪声但有效'的假设检验框架，使用一小部分由人类标记的数据集来估计评判者的真实阳性率（TPR）和假阳性率（FPR），然后将一个经过方差修正的关键阈值应用于一个大规模由评判者标记的数据集上。

Result: 该研究不仅从理论上证明了在何种条件下有噪声测试比直接评估具有更高的统计功效，还通过实验证明了这一点。此外，揭示了实际方法与理论最优之间存在显著性能差距，量化了参数估计的成本。

Conclusion: 这些结果加深了对使用LLM作为评判者的统计评估的理解，并强调了不同推断工具之间的权衡。

Abstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While "LLM-as-a-Judge" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a "Noisy but Valid" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical "Oracle" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.

</details>


### [7] [DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training](https://arxiv.org/abs/2601.21824)
*Xinwei Qiang,Hongmin Chen,Shixuan Sun,Jingwen Leng,Xin Liu,Minyi Guo*

Main category: cs.LG

TL;DR: 研究人员通过将确定性注意力的反向传播视为有向无环图(DAG)上的调度问题，提出了DASH（用于高吞吐量的确定性注意力调度），该方法包含两种互补的调度策略：降序Q-块迭代和移位调度。实验表明，DASH可以提高注意力机制反向传播的吞吐量，最高可达基线的1.28倍，从而提升了可重复的大规模语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型训练中，确定性对于重现性至关重要，但通常会带来显著的性能成本。现有注意力实现如FlashAttention-3中，确定性的反向传递相较于非确定性版本可能导致高达37.9%的吞吐量减少。这主要是由于为了保证数值一致性，梯度累积操作必须序列化执行，导致计算资源利用率低下。

Method: 研究者将确定性注意力的反向传递建模为一个有向无环图（DAG）上的调度问题，并推导出能够最小化关键路径长度的调度方案。基于此，他们提出了DASH（Deterministic Attention Scheduling for High-Throughput），包括两种互补的调度策略：(i) 降序Q-块迭代，一种反转查询块遍历方式，旨在减少因果注意力中的流水线停顿；(ii) 移位调度，在DAG模型内理论最优的一种调度方案，能同时减少全掩码与因果掩码下的流水线停顿。

Result: 在NVIDIA H800 GPU上的实验证明，DASH有效缩小了确定性注意力与非确定性版本之间的性能差距。所提出的方法使得注意力机制反向传递的吞吐量相比基线提高了最多1.28倍，极大促进了可复现LLM训练的效率。

Conclusion: 通过引入DASH及其创新的调度策略，本研究成功缓解了确定性注意力在大规模语言模型训练过程中面临的性能瓶颈问题，为实现高效且可重复的训练提供了新途径。

Abstract: Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non-deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient-reduction phases, leading to significant hardware underutilization.
  To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH (Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q-Tile Iteration, a reversed query-block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks.
  Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28$\times$ compared to the baseline, significantly advancing the efficiency of reproducible LLM training.
  Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3.

</details>


### [8] [Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach](https://arxiv.org/abs/2601.20916)
*Anni Zhao,Ayca Ermis,Jeffrey Robert Vitt,Sergio Brasil,Wellingson Paiva,Magdalena Kasprowicz,Malgorzata Burzynska,Robert Hamilton,Runze Yan,Ofer Sadan,J. Claude Hemphill,Lieven Vandenberghe,Xiao Hu*

Main category: cs.LG

TL;DR: 本研究开发了一种结合系统识别和排序约束优化的机器学习算法，利用非侵入性信号来估计平均颅内压（ICP）。通过使用动脉血压（ABP）、脑血流速度（CBv）以及R波到R波间隔信号，该方法在综合数据库中模拟了ICP。结果表明，约31.88%的测试条目误差在2毫米汞柱以内，而34.07%的测试条目误差介于2至6毫米汞柱之间。这些发现为安全且广泛可及的ICP监测奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 准确无创地估计颅内压（ICP）是重症监护中的一个重大挑战。这项研究旨在通过开发一种新的机器学习算法来解决这个问题，该算法能够利用非侵入性的生理信号来提供精确的ICP值。

Method: 研究人员提出了一种机器学习框架，用于从任意非侵入性信号中获得准确的平均ICP值。他们采用了子空间系统识别算法来根据动脉血压（ABP）、脑血流速度（CBv）和R波至R波间隔信号在综合数据库中识别脑血流动力学模型以模拟ICP。接着，通过凸优化中的创新排序约束来学习描述非侵入性信号特征与估计误差之间关系的映射函数。

Result: 结果显示，在带有约束条件的非线性映射中，大约31.88%的测试数据估计误差在2毫米汞柱以内，另有34.07%的测试数据估计误差在2至6毫米汞柱之间。

Conclusion: 研究结果证明了所提出的非侵入性ICP估计方法的可行性，并为急性脑损伤及相关疾病患者的安全、广泛可及的ICP监测奠定了基础。然而，在临床应用之前还需要进一步验证和技术改进。

Abstract: Objective: Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. Methods: A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. Results: The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg to 6 mmHg from the nonlinear mapping with constraints. Conclusion: Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Significance: Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.

</details>


### [9] [Where Do the Joules Go? Diagnosing Inference Energy Consumption](https://arxiv.org/abs/2601.22076)
*Jae-Won Chung,Ruofan Wu,Jeff J. Ma,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 该论文对46个模型、7项任务和1858种不同配置在NVIDIA H100和B200 GPU上的推理时间和能耗进行了大规模测量研究，揭示了任务类型等因素可导致显著的能耗差异，并提出了一种框架来理解时间与能耗背后的机制。


<details>
  <summary>Details</summary>
Motivation: 随着能源成为机器学习计算中的关键资源，准确理解和诊断能耗差异的原因对于优化变得至关重要。为了实现这一目标，作者展开了这项研究。

Method: 进行了一项涉及46个模型、7项任务以及1858种不同设置的大规模测量研究，使用NVIDIA H100和B200 GPU作为硬件平台。基于观察结果，提出了一个框架来解释影响时间和能量消耗的根本机制。

Result: 实验发现显示，在不同的条件下能耗存在数量级的变化：例如，大语言模型的任务类型可能导致最高达25倍的能量差异；视频生成有时比图像生成耗能多出超过100倍；GPU利用率的不同也能造成3到5倍的能量差异。

Conclusion: 时间和能量消耗由内存使用率等潜在指标决定，而这些指标受到算法、软件及硬件层面多种因素的影响。提出的框架不仅有助于理解时间与能耗关系，还能够直接扩展应用于每瓦吞吐量这一关键指标上，这对于电力受限的数据中心尤为重要。

Abstract: Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.

</details>


### [10] [A Theory of Universal Agnostic Learning](https://arxiv.org/abs/2601.20961)
*Steve Hanneke,Shay Moran*

Main category: cs.LG

TL;DR: 该论文提供了一个在不可知情况下二分类最优通用率的完整理论，识别了所有概念类别的最优收敛速率可以归为四类：$e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$ 或者任意慢，并且确定了简单的组合结构来判断给定的概念类别属于哪一类别。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于扩展Bousquet等人(2021)关于可实现情况下的理论，通过移除分布上的可实现性假设，以提供一个更广泛适用的、针对不可知环境中的二分类问题的最优学习率的全面理解。

Method: 作者们开发了一套新的理论框架，用于分析在不可知环境下不同概念类别的最优学习速率，并通过识别影响这些速率的基本组合结构来进行分类。

Result: 研究表明对于任何概念类，其过度错误率的最佳通用收敛速度可以被归类为四种类型之一：$e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$ 或者是极其缓慢的。此外，还发现了能够决定特定概念类归属哪种类型的简单组合结构。

Conclusion: 结论是，通过这项工作我们获得了对二分类问题中哪些因素决定了学习算法性能极限的新见解，特别是在没有关于数据生成过程先验知识的情况下。

Abstract: We provide a complete theory of optimal universal rates for binary classification in the agnostic setting. This extends the realizable-case theory of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021) by removing the realizability assumption on the distribution. We identify a fundamental tetrachotomy of optimal rates: for every concept class, the optimal universal rate of convergence of the excess error rate is one of $e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$, or arbitrarily slow. We further identify simple combinatorial structures which determine which of these categories any given concept class falls into.

</details>


### [11] [Distributional Active Inference](https://arxiv.org/abs/2601.20985)
*Abdullah Akgül,Gulcin Baykal,Manuel Haußmann,Mustafa Mert Çelikok,Melih Kandemir*

Main category: cs.LG

TL;DR: 该论文提出了一种将主动推理无缝集成到分布强化学习框架中的正式抽象方法，使得在不依赖于转换动态建模的情况下也能获得性能优势。


<details>
  <summary>Details</summary>
Motivation: 针对复杂环境中的机器人系统最优控制所面临的感知状态信息的有效组织和远见行动规划的双重挑战，当前的强化学习框架仅解决了后者且样本效率低的问题。而主动推理作为解决这一双重问题的最新过程理论，在人工智能中的应用还局限于现有基于模型的方法的扩展。

Method: 通过构建一个涵盖基于模型、分布式以及无模型方法的强化学习算法的形式化抽象，研究者们成功地将主动推理整合到了分布式强化学习框架内。

Result: 提出的方法能够无需对转换动态进行建模就实现主动推理带来的性能提升。

Conclusion: 本研究为增强学习提供了一个新的视角，即通过形式化的抽象来结合主动推理与分布式强化学习的优势，从而提高了处理复杂任务时的学习效率。

Abstract: Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.

</details>


### [12] [Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings](https://arxiv.org/abs/2601.20987)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 研究引入了首个全球儿童发展预训练编码器，使用来自44个国家的357,700多名儿童的数据进行训练。该编码器在仅有50个样本的情况下表现优于从零开始的梯度提升方法，并且能够实现零样本部署到未见国家/地区，显示出高达0.84的AUC值。


<details>
  <summary>Details</summary>
Motivation: 每年有大量儿童经历可预防的发展迟缓，但机器学习在新国家的应用受到数据瓶颈限制：可靠的模型需要数千个样本，而新的项目启动时往往少于100个样本。本研究旨在通过引入预训练编码器来克服这一障碍，提高在资源受限环境下监测可持续发展目标4.2.1（SDG 4.2.1）的可行性。

Method: 研究人员开发了一个基于UNICEF调查数据训练而成的预训练编码器，该数据集涵盖了44个国家的357,709名儿童。通过对这个大型多样化数据集的学习，即使在只有少量本地训练样本（例如50个或500个）的情况下，也能有效应用于新地区的儿童发展评估。此外，还采用了迁移学习理论来解释为什么预训练模型能够很好地泛化到未曾见过的数据上。

Result: 实验结果显示，在仅有50个训练样本的情况下，预训练编码器达到了平均AUC为0.65的成绩（95%置信区间：0.56-0.72），相比从零开始训练的梯度提升模型提升了8-12%的表现。当样本数增加至500时，AUC进一步提高到了0.73。更重要的是，对于完全未见过的新国家/地区，该模型也展现出了高达0.84的AUC值。

Conclusion: 研究表明，通过利用跨多个国家收集的大规模数据进行预训练，可以显著改善在资源有限条件下对儿童发展状况进行准确预测的能力。这不仅有助于解决当前面临的数据稀缺问题，也为实现更广泛的全球儿童健康发展目标提供了强有力的支持。

Abstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.

</details>


### [13] [Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles](https://arxiv.org/abs/2601.20989)
*Lutz Oettershagen*

Main category: cs.LG

TL;DR: 本文提出了一种名为ACE的自适应认证算法，该算法在具有快速但噪声较大的弱预言机和稀缺但高保真的强预言机环境中，通过专注于关键边界项来减少对强预言机的查询次数，从而有效识别前k个条目。此外，还提出了一个完全自适应的两阶段方法ACE-W，进一步减少了强预言机的成本。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，准确评估项目价值可能非常昂贵，因此直接找到前k个最佳项目通常是不切实际的。为解决这一问题，研究者们探索了利用一个快速但存在噪音的弱预言机与一个虽然精确度高却资源有限的强预言机相结合的方法来降低成本。

Method: 1. 分析了一个简单的先筛选后验证基线（STC），并证明其在给定联合有效的弱置信区间条件下最多需要m(4ε_max)次强调用。
2. 提出了一种新的自适应认证算法ACE，它能够集中于处理关键边界项上的强查询，以实践中的方式减少了强调用次数。
3. 引入了ACE-W，这是一种完全自适应的两阶段方法，在运行ACE之前自适应地分配弱预算，进一步降低了强成本。

Result: 实验结果表明，ACE算法能够在保持理论上限O(m(4ε_max))的同时显著减少实践中对强预言机的实际调用次数。而ACE-W则通过更灵活地管理弱预言机资源进一步优化了整体性能。

Conclusion: 本研究为在受限于获取高质量数据的情况下如何高效地确定前k个项目提供了一种新颖且实用的方法。所提出的ACE及ACE-W算法不仅理论上达到了最优性，而且在实际应用中也表现出色。

Abstract: Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\varepsilon_{\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\varepsilon_{\max}$, where $m(\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $Ω(m(\varepsilon_{\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\varepsilon_{\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.

</details>


### [14] [Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research](https://arxiv.org/abs/2601.21008)
*Ruicheng Ao,David Simchi-Levi,Xinshang Wang*

Main category: cs.LG

TL;DR: 研究者们引入了两个新的基准测试，\ORDebug{} 和 \ORBias{}，来评估操作研究中模型的迭代自我修正能力和行为合理性。通过这些工具，研究发现特定领域的RLVR训练可以使一个80亿参数的模型在修复率、诊断准确性和解决速度上超越现有的前沿API。此外，在\ORBias{} 测试中，课程训练方法成功减少了从ID到OOD的数据偏移，并显著降低了系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM基准测试忽略了操作研究（OR）模型调试过程中的诊断循环，即分析不可简化不可行子系统（IIS）、识别约束冲突并逐步修复模型直到实现可行性。为了弥补这一空白，研究者提出了将求解器纳入评估循环的新基准。

Method: 研究者设计了两个新基准：\ORDebug{} 用于评估5,000多个问题上的迭代自我修正能力，覆盖9种错误类型；\ORBias{} 则通过2,000个报童问题实例（1,000个ID + 1,000个OOD）来衡量与封闭形式最优策略之间的系统性偏离。同时，采用领域特定的RLVR训练方法以提高模型性能。

Result: 实验结果表明，经过特定领域RLVR训练的80亿参数模型在恢复率、诊断准确度以及解决问题所需步骤数方面均优于现有前沿API。特别是，在\ORBias{} 基准测试中，采用课程训练法能有效减少从ID到OOD数据集间偏移，并大幅降低系统性偏差。

Conclusion: 通过引入考虑求解器反馈的过程级评估机制，能够针对地训练出更优的操作研究模型，这证明了即使是较小规模但经过专门优化的模型也能胜过单纯依靠规模扩张的方法。

Abstract: Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \textbf{solver in the evaluation loop}. \textbf{\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \IIS{} recomputation, providing deterministic, verifiable feedback. \textbf{\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\% vs 86.2\% recovery rate (+9.1\%), 62.4\% vs 47.8\% diagnostic accuracy (+14.6\%), and 2.25 vs 3.78 steps to resolution (1.7$\times$ faster). On \ORBias{}, curriculum training achieves the only negative ID$\rightarrow$OOD bias drift among models evaluated (-9.6\%), reducing systematic bias by 48\% (from 20.0\% to 10.4\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.

</details>


### [15] [Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference](https://arxiv.org/abs/2601.21012)
*Young Kyung Kim,Oded Schlesinger,Qiangqiang Wu,J. Matías Di Martino,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 本文提出了一种新的测试时适应方法OATTA，通过利用时间动态中的监督信号来改进预训练模型的预测。该方法在多个领域中均表现出色，能够显著提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时适应（TTA）方法通常忽视了数据流中的时间动态所提供的监督信号，这限制了它们处理分布偏移的能力。

Method: 提出了Order-Aware Test-Time Adaptation (OATTA) 方法，将测试时适应视为一个无梯度递归贝叶斯估计任务，并使用学习到的动态转移矩阵作为时间先验来精炼基础模型的预测。同时引入了一个似然比门控机制(LLR)，以确保在缺乏时间证据的情况下可以安全回退到基础预测器。

Result: OATTA被证明是一种轻量级、与模型无关的方法，在图像分类、可穿戴及生理信号分析、语言情感分析等多个领域内都优于现有基线方法，最大提升了6.35%的准确性。

Conclusion: 研究表明，建模时间动态为标准TTA方法提供了重要的补充信息，有助于更好地应对分布偏移问题。

Abstract: Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.

</details>


### [16] [SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model](https://arxiv.org/abs/2601.21031)
*Zongheng Guo,Tao Chen,Yang Jiao,Yi Pan,Xiao Hu,Manuela Ferrario*

Main category: cs.LG

TL;DR: 提出了一种针对PPG信号的新型基础模型SIGMA-PPG，该模型利用统计先验引导对抗性掩码机制和语义一致性约束来改善对噪声过拟合的问题，并在12个不同的下游任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的PPG信号基础模型受到信号内在冗余性和噪声的影响，标准的掩模建模容易产生平凡解，而对比方法缺乏形态学精度。

Method: 开发了SIGMA-PPG，一种具有先验指导对抗掩蔽机制的生成式基础模型，其中强化学习驱动的教师利用统计先验创建具有挑战性的学习路径以防止过度拟合到噪声；同时通过矢量量化引入语义一致性约束，保证生理相同的波形映射至共享索引，从而增强代码本语义密度并消除多余的特征结构。

Result: 经过超过120,000小时数据预训练后，SIGMA-PPG在12项不同下游任务上的平均性能优于五种最先进基线方法。

Conclusion: SIGMA-PPG提供了一种有效的方法来处理PPG信号中的冗余与噪声问题，展现出优越的性能。

Abstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.

</details>


### [17] [Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints](https://arxiv.org/abs/2601.21033)
*Omer Rochman-Sharabi,Gilles Louppe*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的约束采样框架，通过Predict-Project-Renoise (PPR)算法确保生成样本满足物理定律或观测一致性等硬性约束条件。实验表明，该方法显著减少了约束违规情况，并提高了样本的一致性和与真实约束分布的匹配度。


<details>
  <summary>Details</summary>
Motivation: 神经模拟器在科学应用中显示出潜力，但传统的模型无法保证物理准确性或满足约束条件。为了解决这个问题，研究引入了一种能够确保在生成时遵守如物理法则或观察一致性等硬性约束的采样框架。

Method: 提出了一个受限的前向过程，仅在满足约束条件的可行样本集上进行扩散，从而形成受限边缘分布。为了逆向这一过程，作者提出了一种迭代算法——预测-投影-再噪声化（PPR），它通过在去噪预测、投射到可行集合以及再次加噪之间交替操作来从受限边缘分布中采样。

Result: 在2D分布、偏微分方程和全球天气预报上的实验显示，与基线相比，PPR方法能将约束违规减少超过一个数量级，同时提高样本一致性和更好地匹配真实的受约束分布。

Conclusion: 通过引入约束采样框架及PPR算法，本研究有效解决了传统扩散模型应用于科学领域时面临的关键问题，即如何保证生成样本既符合给定物理规律又能满足特定约束条件，从而为相关领域的进一步研究提供了新的思路和技术手段。

Abstract: Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.

</details>


### [18] [Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning](https://arxiv.org/abs/2601.21037)
*Chengzu Li,Zanyi Wang,Jiaang Li,Yi Xu,Han Zhou,Huanyu Zhang,Ruichuan An,Dengyang Jiang,Zhaochong An,Ivan Vulić,Serge Belongie,Anna Korhonen*

Main category: cs.LG

TL;DR: 本文通过视频生成模型来解决视觉推理问题，特别是在迷宫导航和七巧板拼图任务中展示了零样本泛化、利用视觉上下文以及在测试时增加视频长度能够提高对复杂路径的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型虽然擅长文本推理，但在精细的空间理解和连续动作规划方面表现不佳，无法模拟复杂视觉推理所需的动态过程。

Method: 通过视频生成模型来进行视觉推理，并且评估了两种不同情境下的能力：迷宫导航（低视觉变化的序列离散规划）和七巧板拼图（高视觉变化的连续操作）。

Result: 实验揭示了三个关键见解：(1) 强大的零样本泛化能力；(2) 有效利用视觉背景作为显式控制；(3) 在顺序规划过程中观察到一种测试时缩放定律，即增加生成视频长度可以增强对空间和时间上复杂路径的零样本泛化能力。

Conclusion: 视频生成不仅是媒体工具，也是一种可扩展且通用的视觉推理范例。

Abstract: Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.

</details>


### [19] [Test-Time Adaptation for Unsupervised Combinatorial Optimization](https://arxiv.org/abs/2601.21048)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: 提出了TACO，一种模型无关的测试时自适应框架，用于统一和扩展现有的两种无监督神经组合优化（NCO）范式。TACO通过策略性地预热来部分放松已训练参数，同时保持归纳偏差，从而实现快速有效的无监督自适应。实验表明TACO在静态、分布偏移以及动态组合优化问题上都能有效提高解的质量，且额外计算成本几乎可以忽略不计。


<details>
  <summary>Details</summary>
Motivation: 当前无监督神经组合优化方法分为两类：一类是泛化能力强但实例自适应能力弱的模型；另一类是在测试时独立优化的实例特定模型，虽然灵活但容易陷入局部最优解，并且无法充分利用学习到的归纳结构。研究旨在探索如何结合这两类方法的优点，即利用泛化过程中学到的归纳偏置同时解锁所需的实例级灵活性。

Method: 提出了一种名为TACO的新框架，它通过策略性地进行预热处理来部分调整已训练好的参数，保留了归纳偏见的同时允许快速有效的无监督适应。该方法旨在解决现有泛化模型作为实例级优化起点不佳的问题，避免了单纯微调或从头开始优化带来的性能下降。

Result: TACO在最小顶点覆盖和最大团这两个经典组合优化问题上的实验显示，相较于直接微调泛化模型或是完全从零开始优化实例特定模型，TACO能够在保证解质量提升的同时几乎不增加额外的计算开销。此外，TACO还证明了其在面对静态、分布偏移及动态组合优化问题时的有效性和鲁棒性。

Conclusion: TACO成功地架起了泛化与实例特异性无监督神经组合优化之间的桥梁，为解决这一领域内的挑战提供了一个实用且高效的方法。

Abstract: Unsupervised neural combinatorial optimization (NCO) enables learning powerful solvers without access to ground-truth solutions. Existing approaches fall into two disjoint paradigms: models trained for generalization across instances, and instance-specific models optimized independently at test time. While the former are efficient during inference, they lack effective instance-wise adaptability; the latter are flexible but fail to exploit learned inductive structure and are prone to poor local optima. This motivates the central question of our work: how can we leverage the inductive bias learned through generalization while unlocking the flexibility required for effective instance-wise adaptation? We first identify a challenge in bridging these two paradigms: generalization-focused models often constitute poor warm starts for instance-wise optimization, potentially underperforming even randomly initialized models when fine-tuned at test time. To resolve this incompatibility, we propose TACO, a model-agnostic test-time adaptation framework that unifies and extends the two existing paradigms for unsupervised NCO. TACO applies strategic warm-starting to partially relax trained parameters while preserving inductive bias, enabling rapid and effective unsupervised adaptation. Crucially, compared to naively fine-tuning a trained generalizable model or optimizing an instance-specific model from scratch, TACO achieves better solution quality while incurring negligible additional computational cost. Experiments on canonical CO problems, Minimum Vertex Cover and Maximum Clique, demonstrate the effectiveness and robustness of TACO across static, distribution-shifted, and dynamic combinatorial optimization problems, establishing it as a practical bridge between generalizable and instance-specific unsupervised NCO.

</details>


### [20] [Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization](https://arxiv.org/abs/2601.21058)
*Seungki Hong,Kyeongwon Jeong,Taekwang Jang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Snowball的全连接Ising机器，它结合了双模式马尔可夫链蒙特卡洛自旋选择和异步自旋更新机制，以促进收敛并减少求解时间。原型机在AMD Alveo U250加速卡上实现了相对于最先进Ising机器8倍的求解时间减少。


<details>
  <summary>Details</summary>
Motivation: 为了实现Ising机器的实际部署，需要解决硬件拓扑、自旋选择与更新算法以及耦合系数精度可扩展性这三个挑战。这些问题包括受限拓扑下的次优映射需求、并行更新可能导致的振荡或停滞现象以及有限精度导致的可行映射缺失或解质量下降。

Method: 提出了Snowball，一种数字、可扩展、全连接的IsING机器，整合了双模式马尔科夫链蒙特卡罗自旋选择方法和异步自旋更新策略来促进收敛性和减少求解时间。此外，数字架构支持宽范围配置的耦合精度。

Result: 基于AMD Alveo U250加速卡的原型机，在相同的基准实例下，相比最先进的Ising机器实现了8倍的时间至解决方案减少。

Conclusion: 通过引入Snowball Ising机器，本研究展示了如何有效地降低解决组合优化问题所需的时间，同时保持甚至提高了解决方案的质量，为Ising机器技术向实际应用迈进提供了新的途径。

Abstract: Ising machines have emerged as accelerators for combinatorial optimization. To enable practical deployment, this work aims to reduce time-to-solution by addressing three challenges: (1) hardware topology, (2) spin selection and update algorithms, and (3) scalable coupling-coefficient precision. Restricted topologies require minor embedding; naive parallel updates can oscillate or stall; and limited precision can preclude feasible mappings or degrade solution quality.
  This work presents Snowball, a digital, scalable, all-to-all coupled Ising machine that integrates dual-mode Markov chain Monte Carlo spin selection with asynchronous spin updates to promote convergence and reduce time-to-solution. The digital architecture supports wide, configurable coupling precision, unlike many analog realizations at high bit widths. A prototype on an AMD Alveo U250 accelerator card achieves an 8$\times$ reduction in time-to-solution relative to a state-of-the-art Ising machine on the same benchmark instance.

</details>


### [21] [Human-LLM Collaborative Feature Engineering for Tabular Data](https://arxiv.org/abs/2601.21060)
*Zhuoyan Li,Aditya Bansal,Jinzhao Li,Shishuang He,Zhuoran Lu,Mutian Zhang,Qin Liu,Yiwei Yang,Swati Jain,Ming Yin,Yunyao Li*

Main category: cs.LG

TL;DR: 本文提出了一种人机协作的表格学习特征工程框架，通过解耦特征转换操作的提议和选择过程，并在过程中引入人类专家的偏好反馈来提高特征工程性能。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型（LLMs）进行自动化特征工程的方法通常将LLM视为黑盒优化器，这导致了对低收益操作的重复探索而缺乏有原则的优先级策略。为了解决这一问题，提出了一个新框架以更有效地指导特征转换操作的选择。

Method: 该方法首先将特征转换操作的提议和选择过程分离，其中LLM仅用于生成候选操作，而选择过程则基于对每个提议操作的效用和不确定性建模。此外，设计了一种机制，可以有选择地获取并整合人类专家对于哪些操作更为有前景的偏好反馈到选择过程中。

Result: 合成研究和真实用户研究表明，所提出的框架提高了多种表格数据集上的特征工程性能，并减少了用户在特征工程过程中的认知负担。

Conclusion: 通过结合人类专家的知识与机器学习模型的能力，新的框架能够更好地估计特征转换操作的效用，从而改善最终模型的表现。

Abstract: Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users' cognitive load during the feature engineering process.

</details>


### [22] [Out-of-Distribution Generalization in Graph Foundation Models](https://arxiv.org/abs/2601.21067)
*Haoyang Li,Haibo Chen,Xin Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 本文综述了图基础模型（GFMs）在处理分布外(OOD)泛化方面的最新进展，包括面临的挑战、现有方法的分类、OOD处理策略及预训练目标，并讨论了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 图形学习模型在超出其训练分布应用时通常会遇到泛化能力有限的问题。实际中，分布偏移可能源于图形结构、领域语义、可用模态或任务形式的变化。为解决这些挑战，提出了图基础模型(GFMs)，旨在通过跨多种图形和任务的大规模预训练来学习通用表示。

Method: 本研究首先讨论了图形学习中由分布偏移带来的主要挑战，并概述了一个统一的问题设置。接着根据现有方法是否设计为在固定任务规范下运行或支持跨异构任务公式化的泛化进行了分类，并总结了相应的OOD处理策略和预训练目标。

Result: 论文回顾了常见的评估协议，并探讨了未来研究的开放方向。据我们所知，这是第一篇关于GFMs中OOD泛化的调查报告。

Conclusion: 该文作为首份针对图基础模型中的分布外泛化问题的调研报告，提供了对于当前挑战、解决方案以及未来发展方向的全面审视。

Abstract: Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.

</details>


### [23] [Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed](https://arxiv.org/abs/2601.21094)
*Minjae Kwon,Josephine Lamp,Lu Feng*

Main category: cs.LG

TL;DR: 研究发现安全强化学习算法在训练时的安全保证可能无法转移到部署阶段，特别是在分布变化的情况下。通过使用糖尿病管理作为关键测试场景，提出了一种在测试时使用学习到的动力学模型过滤不安全动作的方法（shielding），以恢复不同算法和患者群体中的安全性。该方法提高了Time-in-Range指标，并降低了临床风险指数和血糖变异性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索安全强化学习算法是否能在训练条件与实际部署环境存在差异时仍能保持其安全性，特别是针对像糖尿病管理这样的对安全性要求极高的应用场景。

Method: 研究者们在一个统一的临床模拟器上对安全RL算法进行了基准测试，揭示了所谓的‘安全泛化差距’问题。为了解决这个问题，他们引入了测试时屏蔽（test-time shielding）技术，该技术利用学到的动力学模型来过滤掉不安全的动作。

Result: 结果显示，在多种安全RL算法、三种类型的糖尿病以及三个年龄组别中，屏蔽技术能够使Time-in-Range增加13-14%，同时降低临床风险指数和血糖波动性。

Conclusion: 本研究表明，尽管安全强化学习算法在训练过程中表现出良好的性能，但在面对分布转移时可能会出现安全问题。通过采用测试时屏蔽策略可以有效缓解这一问题，提高算法在实际应用中的安全性。此外，研究还提供了一个用于研究在安全关键控制领域下分布转移影响的平台。

Abstract: Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.

</details>


### [24] [SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning](https://arxiv.org/abs/2601.21649)
*Jinjun Peng,Magnus Saebo,Tianjun Zhong,Yi-Jie Cheng,Junfeng Yang,Baishakhi Ray,Simin Chen,Yangruibo Ding*

Main category: cs.LG

TL;DR: 本文提出了一种新的学习范式——以仓库为中心的学习（RCL），它通过专注于单个代码库的深度理解，而不是广泛的任务覆盖，来提高小型语言模型在处理复杂、不熟悉的代码库时的性能。基于这种新方法训练出的SWE-Spot-4B模型，在多个软件工程任务上超越了现有开放权重模型甚至某些高效商业模型的表现，并且显示出更高的训练样本效率和更低的推理成本。


<details>
  <summary>Details</summary>
Motivation: 目前的小型语言模型在处理复杂的、不熟悉的代码库时缺乏强大的泛化能力，而现有的以任务为中心的学习方式无法解决这一问题。为了解决这个问题并提升这些模型的能力，特别是在隐私敏感和资源受限环境中的应用，提出了新的学习范式RCL。

Method: 提出了一种称为Repository-Centric Learning (RCL)的新方法，该方法强调深入理解特定软件环境而非扩展到多种不同类型的任务。根据RCL理念设计了一个四单元的学习体验，将静态代码库转化为互动学习信号，用于训练专门针对特定仓库优化的小型专家模型SWE-Spot-4B。

Result: 实验结果显示，基于RCL训练得到的SWE-Spot-4B系列模型不仅打破了既定的规模趋势，在多个软件工程任务上优于更大规模的开放权重模型（如Meta的CWM, Qwen3-Coder-30B），而且在效率方面也超过了或匹配了专注效率的商用模型（例如GPT-4.1-mini, GPT-5-nano）。此外，RCL还表现出更高的训练样本效率和更低的推理成本。

Conclusion: 对于构建高效的智能系统而言，掌握特定代码库是与通用编码能力相辅相成的重要维度。RCL提供了一条有效路径，使得小型语言模型能够更好地适应复杂多变的工作环境。

Abstract: The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the "physics" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.

</details>


### [25] [TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning](https://arxiv.org/abs/2601.21135)
*Shicheng Fan,Kun Zhang,Lu Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为TRACE的框架，用于学习连续变化环境下的因果关系。实验表明，TRACE能够以高达0.99的相关性恢复机制轨迹，显著优于基于离散切换的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时间因果表示学习方法假设因果机制在不同领域之间瞬间切换，但现实世界中的系统往往表现出连续的机制过渡。例如，车辆在转弯时其动力学特性会逐渐变化，人的步态也会从行走平滑地过渡到跑步。为了更好地理解和建模这些连续过渡的过程，需要新的方法来捕捉这种渐变的行为。

Method: 本文提出了一个名为TRACE（Temporal Recovery of Atomic Causal Expertise）的Mixture-of-Experts框架。每个专家模型专注于学习训练过程中的一种原子机制，这使得即使是在测试阶段遇到从未见过的中间机制状态时，也能恢复出机制轨迹。通过将过渡机制建模为有限多个原子机制的凸组合，并由随时间变化的混合系数控制，从而实现了对潜在因果变量和连续混合轨迹的同时识别。

Result: 实验结果表明，在合成数据集和真实世界数据集上，TRACE都能够有效地恢复机制轨迹，最高可达0.99的相关性，这明显优于基于离散切换假设的方法。

Conclusion: 研究证明了对于展示连续机制转换的真实世界系统而言，TRACE提供了一种有效的方法来学习和恢复潜在的因果关系及机制轨迹。该方法不仅能够处理已知条件下的机制转换，还能泛化至训练期间未观察到的新情况。

Abstract: Temporal causal representation learning methods assume that causal mechanisms switch instantaneously between discrete domains, yet real-world systems often exhibit continuous mechanism transitions. For example, a vehicle's dynamics evolve gradually through a turning maneuver, and human gait shifts smoothly from walking to running. We formalize this setting by modeling transitional mechanisms as convex combinations of finitely many atomic mechanisms, governed by time-varying mixing coefficients. Our theoretical contributions establish that both the latent causal variables and the continuous mixing trajectory are jointly identifiable. We further propose TRACE, a Mixture-of-Experts framework where each expert learns one atomic mechanism during training, enabling recovery of mechanism trajectories at test time. This formulation generalizes to intermediate mechanism states never observed during training. Experiments on synthetic and real-world data demonstrate that TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines.

</details>


### [26] [Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2601.21147)
*Kevin Han,Haolin Cong,Bowen Deng,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本研究通过引入动态截断半径的方法，解决了机器学习原子间势（MLIPs）在分子动力学模拟中面临的内存消耗和推理时间两大瓶颈问题。该方法不仅保持了长时间尺度模拟的稳定性，还显著降低了内存使用量和加快了推理速度，并且对四种先进的MLIP模型进行了实现，结果显示内存消耗减少了2.26倍，推理时间加快了2.04倍。此外，误差分析表明新方法与固定截断半径版本相比准确度仅有微小下降。


<details>
  <summary>Details</summary>
Motivation: 当前，机器学习原子间势虽然在药物和材料发现等应用中的分子动力学模拟非常有用，但它们面临着两个主要瓶颈：推理时间和内存消耗，这限制了其达到现实模拟规模的能力。

Method: 提出了一种新的动态截断公式来替代传统的固定、常数值的截断半径，旨在特定数量的邻居原子上诱导稀疏性到基础原子图上，从而减少内存消耗并提高计算效率。

Result: 通过将动态截断应用于MACE、Nequip、Orbv3和TensorNet四种最先进的MLIPs上，实现了内存消耗减少2.26倍以及推理时间加快2.04倍的结果。同时，对于材料和分子数据集来说，动态截断模型相对于固定截断模型展现出极小的准确性损失。

Conclusion: 这项工作通过引入一种能够稳定执行长时间尺度分子动力学模拟的动态截断机制，有效解决了MLIPs面临的主要挑战，即内存消耗高和计算慢的问题。此外，所有模型实现及训练代码都将完全开源共享。

Abstract: Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.

</details>


### [27] [Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement](https://arxiv.org/abs/2601.21149)
*Maria Despoina Siampou,Shushman Choudhury,Shang-Ling Hsu,Neha Arora,Cyrus Shahabi*

Main category: cs.LG

TL;DR: 本文提出了一种名为ME-POIs的新框架，该框架通过将从语言模型中获得的POI嵌入与大规模人类移动数据相结合，来学习基于真实世界使用情况的、以POI为中心且不依赖于上下文的表示。实验结果表明，在五个新提出的地图丰富任务上，ME-POIs的表现优于仅基于文本或仅基于移动性的基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要关注从静态文本元数据得出的位置身份，或者学习与轨迹上下文相关的表示，这捕捉了移动规律而非地点的实际用途（即POI的功能）。作者认为POI功能是通用POI表示中缺失但至关重要的信号。

Method: 引入了Mobility-Embedded POIs (ME-POIs) 框架，该框架增强了从语言模型衍生出的POI嵌入，并结合大规模的人类移动数据，以学习基于真实世界使用的POI中心表示。对于长尾稀疏性问题，提出了一个新颖机制，能够从邻近频繁访问的POI传播时间访问模式到多个空间尺度。

Result: 在五个新提出用于测试POI身份和功能捕捉能力的地图丰富任务上，相比仅基于文本或仅基于移动性的基线方法，使用ME-POIs增强文本嵌入始终表现出色。值得注意的是，仅用移动数据训练的ME-POIs在某些任务上也能超越纯文本模型。

Conclusion: 研究证明了ME-POIs框架的有效性，特别是它能够通过整合移动数据显著提高POI表示的质量，强调了POI功能对于准确和泛化性强的POI表示的重要性。

Abstract: Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.

</details>


### [28] [A Federated Generalized Expectation-Maximization Algorithm for Mixture Models with an Unknown Number of Components](https://arxiv.org/abs/2601.21160)
*Michael Ibrahim,Nagi Gebraeel,Weijun Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedGEM的联邦广义期望最大化算法，用于解决当客户端本地数据中存在异质但可能重叠的聚类集合时未知总数K的联邦聚类问题。该算法通过在每个客户端上执行局部EM步骤，并围绕每个局部组件的最大值构建不确定性集来工作。中心服务器使用这些不确定性集来学习客户端之间的潜在聚类重叠，并通过闭式计算推断全局聚类数量。研究提供了概率收敛性保证，并通过数值实验表明FedGEM方法可以达到与集中式EM相当的性能，并且优于现有的联邦聚类方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决联邦环境下聚类总数未知以及各客户端拥有异构但可能重叠的局部数据集的问题。

Method: 提出了FedGEM算法，该算法让每个客户端进行局部的EM步骤并构建不确定性集，然后由中央服务器利用这些不确定性集学习客户间的集群重叠情况，并通过闭式计算推断出全局集群数量。

Result: 理论分析给出了在常见假设下的概率收敛保证；对于等向GMMs的具体设置，提供了一种可处理的低复杂度计算方式，并严格验证了算法收敛所需的假设。实证结果表明所提方法达到了与集中式EM相似的表现，并且优于其他已有的联邦聚类方法。

Conclusion: FedGEM是一种有效的联邦聚类方法，能够处理未知总聚类数及异构数据集的情况，其性能与集中式EM相当甚至更好。

Abstract: We study the problem of federated clustering when the total number of clusters $K$ across clients is unknown, and the clients have heterogeneous but potentially overlapping cluster sets in their local data. To that end, we develop FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. Our proposed algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. We perform a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, we study the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. We perform various numerical experiments, where we empirically demonstrate that our proposed method achieves comparable performance to centralized EM, and that it outperforms various existing federated clustering methods.

</details>


### [29] [Efficient Simple Regret Algorithms for Stochastic Contextual Bandits](https://arxiv.org/abs/2601.21167)
*Shuai Liu,Alireza Bakhtiari,Alex Ayoub,Botao Hao,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 本文研究了在简单遗憾目标下的随机上下文逻辑带问题，提出了首个实现简单遗憾$\tilde{\mathcal{O}}(d/\sqrt{T})$的算法，并且引入了针对简单遗憾设置的新版本汤普森采样。对于随机上下文线性带问题，这提供了首个针对随机算法的简单遗憾保证。这些理论成果通过一系列实验得到了验证。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决随机上下文逻辑带问题中缺乏简单遗憾保障的问题，特别是与已有的线性案例相比，逻辑设定下的相关结果尚未被发现。

Method: 作者基于上下文线性带问题和自协调分析的思想，设计了一种新的算法来达到简单遗憾$\tilde{\mathcal{O}}(d/\sqrt{T})$的目标。此外，还开发了一个专为简单遗憾场景定制的汤普森采样变体。

Result: 所提出的算法在动作集有限时完全可处理，并且对于随机上下文线性和逻辑带问题都实现了预期的简单遗憾界。随机算法比确定性算法运行成本更低。

Conclusion: 该研究不仅填补了逻辑带问题中简单遗憾保障的空白，而且通过新版本的汤普森采样方法，在保持高效的同时也达到了良好的性能表现。

Abstract: We study stochastic contextual logistic bandits under the simple regret objective. While simple regret guarantees have been established for the linear case, no such results were previously known for the logistic setting. Building on ideas from contextual linear bandits and self-concordant analysis, we propose the first algorithm that achieves simple regret $\tilde{\mathcal{O}}(d/\sqrt{T})$. Notably, the leading term of our regret bound is free of the constant $κ= \mathcal O(\exp(S))$, where $S$ is a bound on the magnitude of the unknown parameter vector. The algorithm is shown to be fully tractable when the action set is finite. We also introduce a new variant of Thompson Sampling tailored to the simple-regret setting. This yields the first simple regret guarantee for randomized algorithms in stochastic contextual linear bandits, with regret $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$. Extending this method to the logistic case, we obtain a similarly structured Thompson Sampling algorithm that achieves the same regret bound -- $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$ -- again with no dependence on $κ$ in the leading term. The randomized algorithms, as expected, are cheaper to run than their deterministic counterparts. Finally, we conducted a series of experiments to empirically validate these theoretical guarantees.

</details>


### [30] [The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset](https://arxiv.org/abs/2601.21170)
*Augusto Santos,Teresa Santos,Catarina Rodrigues,José M. F. Moura*

Main category: cs.LG

TL;DR: 提出了一种机器学习方法，通过学习来自协方差或精度矩阵的幂族估计器的最佳特征表示来早期检测复杂系统中的突发现象，如癫痫发作、客户流失等。该方法在癫痫检测和客户流失预测上展示了竞争力的结果，并且最佳协方差幂展现了良好的可识别性同时捕捉到结构特征，从而将预测性能与可解释的统计结构结合在一起。


<details>
  <summary>Details</summary>
Motivation: 针对复杂系统中由于未知且部分观测的数据生成过程而导致难以揭示和利用系统潜在因果结构的核心挑战，提出了一种能够早期检测突发现象（例如癫痫发作、突然的客户流失或疫情爆发）的方法。

Method: 该方法从一个单参数估计器家族——经验协方差或精度矩阵的幂——中学习一种最优特征表示，提供了一种调谐至驱动关键事件出现的底层结构的原则性方式。随后，一个监督学习模块对所学得的表示进行分类。

Result: 实证证明了该方法在癫痫检测和客户流失预测上的有效性，达到了竞争性的结果。此外，还发现最优协方差幂不仅具有良好的可识别性，而且能够捕捉到结构特征。

Conclusion: 这项研究提出的方法不仅在预测方面表现出色，而且通过展示出良好的可识别性和结构签名捕捉能力，为理解复杂的系统提供了更加可解释的统计结构。

Abstract: Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.

</details>


### [31] [AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection](https://arxiv.org/abs/2601.21171)
*Kamal Berahmand,Saman Forouzandeh,Mehrnoush Mohammadi,Parham Moradi,Mahdi Jalili*

Main category: cs.LG

TL;DR: 提出了一种名为AC2L-GAD的主动反事实对比学习框架，旨在解决图异常检测中的标签稀缺性和极端类别不平衡问题。通过结合信息论主动选择与反事实生成，该方法能够识别结构复杂的节点，并为这些节点生成保持异常特征的正向增强以及提供难例对比的正常负样本，同时将计算成本高的反事实生成限制在策略性选定的子集上。实验结果表明，AC2L-GAD相比现有技术在多个基准数据集上表现出色或更优，特别是在那些异常点显示出复杂属性-结构交互的数据集中。


<details>
  <summary>Details</summary>
Motivation: 图异常检测面临的主要挑战是标签稀缺和极端类别不平衡。虽然图对比学习提供了一个有前景的无监督解决方案，但当前的方法存在两个关键局限：随机增强破坏了正样本对之间的语义一致性，而简单的负采样则产生了平凡且不具信息量的对比。

Method: AC2L-GAD，一种主动反事实对比学习框架，通过原则性的反事实推理来应对上述局限。它结合了信息论主动选择与反事实生成技术，以识别出结构复杂的节点，并生成保留异常特性的正面增强样本及提供困难对比的普通负面样本，同时仅针对战略性挑选的子集执行耗时的反事实生成过程。

Result: 相较于全图反事实生成，本设计大约减少了65%的计算开销，同时保持了检测质量。在九个基准数据集上的实验显示，包括来自GADBench的真实世界金融交易图，AC2L-GAD相比最先进基线方法实现了竞争性或更优的表现，尤其在异常点展现复杂属性-结构相互作用的数据集中表现突出。

Conclusion: AC2L-GAD通过创新地采用主动选择加反事实生成的方法，在减少计算负担的同时有效提升了图异常检测任务中的性能，特别是在处理具有复杂属性-结构交互特征的异常点时展现出显著优势。

Abstract: Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.

</details>


### [32] [Breaking the Reasoning Horizon in Entity Alignment Foundation Models](https://arxiv.org/abs/2601.21174)
*Yuanning Cui,Zequn Sun,Wei Hu,Kexuan Xin,Zhangjie Fu*

Main category: cs.LG

TL;DR: 提出了一种基于并行编码策略的实体对齐基础模型，通过种子实体对作为局部锚点来引导信息流，并结合合并关系图和可学习交互模块以提高匹配精度。实验表明该框架不仅有效解决了长距离依赖问题，还展现了对未见知识图谱的强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的实体对齐（EA）模型缺乏迁移性，无法在不重新训练的情况下对未见过的知识图谱进行对齐。尽管使用图基础模型（GFMs）提供了解决方案，但直接将GFMs应用于EA的效果不佳，主要原因是存在“推理视野差距”：与GFMs中的链接预测不同，EA需要捕捉跨稀疏且异构的知识图谱结构的长距离依赖关系。

Method: 为了解决上述挑战，本文提出了一个由并行编码策略驱动的实体对齐基础模型。利用种子EA对作为局部锚点来指导信息流动，同时初始化和编码两个并行流。这促进了基于锚点的消息传递，并通过利用局部结构接近性而不是全局搜索显著缩短了推理轨迹。此外，引入了一个合并的关系图来建模全局依赖性以及一个可学习的交互模块用于精确匹配。

Result: 广泛的实验证明了所提框架的有效性，特别是在应对未见知识图谱时表现出强大的泛化能力。

Conclusion: 本研究开发了一种创新的方法来改进实体对齐任务，通过引入并行编码策略、局部锚点引导的信息流及专门设计用于处理长距离依赖性的组件，从而提高了模型在面对新KG时的表现。

Abstract: Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical "reasoning horizon gap": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.

</details>


### [33] [Flow Perturbation++: Multi-Step Unbiased Jacobian Estimation for High-Dimensional Boltzmann Sampling](https://arxiv.org/abs/2601.21177)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 本文提出了一种名为Flow Perturbation++的方法，该方法通过在概率流ODE的离散化过程中每一步进行无偏雅可比估计来减少方差，同时保持了Flow Perturbation的无偏性。实验表明，在1000维高斯混合模型和全原子Chignolin蛋白上，相比基于Hutchinson的方法和单步Flow Perturbation基线，新方法显著提高了平衡采样的性能。


<details>
  <summary>Details</summary>
Motivation: 连续归一化流（CNFs）在高维系统中进行无偏玻尔兹曼采样时面临计算成本高的问题，尤其是雅可比行列式评估需要D次反向传播。现有的随机雅可比估计器如Hutchinson迹估计器虽能降低计算量但会引入偏差；而最近提出的Flow Perturbation方法虽然无偏，但方差较高。因此，研究旨在开发一种既能减少方差又能保持无偏性的新型方法。

Method: 本文介绍了一种称为Flow Perturbation++的新方法，它通过将概率流ODE离散化，并在每个积分步骤执行无偏逐步雅可比估计来实现。这种多步构造保留了Flow Perturbation的无偏性，同时大幅度降低了估计量的方差。

Result: 实验结果表明，当集成到顺序蒙特卡洛框架中时，与基于Hutchinson的方法和单步Flow Perturbation基线相比，Flow Perturbation++在1000维高斯混合模型和全原子Chignolin蛋白上的平衡采样方面取得了显著改进。

Conclusion: 本研究表明，通过采用Flow Perturbation++方法可以有效地解决高维空间中连续归一化流用于无偏玻尔兹曼采样时面临的高计算成本及方差问题，从而为复杂系统的高效采样提供了新的解决方案。

Abstract: The scalability of continuous normalizing flows (CNFs) for unbiased Boltzmann sampling remains limited in high-dimensional systems due to the cost of Jacobian-determinant evaluation, which requires $D$ backpropagation passes through the flow layers. Existing stochastic Jacobian estimators such as the Hutchinson trace estimator reduce computation but introduce bias, while the recently proposed Flow Perturbation method is unbiased yet suffers from high variance. We present \textbf{Flow Perturbation++}, a variance-reduced extension of Flow Perturbation that discretizes the probability-flow ODE and performs unbiased stepwise Jacobian estimation at each integration step. This multi-step construction retains the unbiasedness of Flow Perturbation while achieves substantially lower estimator variance. Integrated into a Sequential Monte Carlo framework, Flow Perturbation++ achieves significantly improved equilibrium sampling on a 1000D Gaussian Mixture Model and the all-atom Chignolin protein compared with Hutchinson-based and single-step Flow Perturbation baselines.

</details>


### [34] [Rethinking Refinement: Correcting Generative Bias without Noise Injection](https://arxiv.org/abs/2601.21182)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 提出了一种双阶段流细化（BFR）框架，用于改进生成模型的样本质量，特别是在高维度情况下。该方法通过在潜在空间和数据空间中进行修正来实现无噪声注入或多重采样的偏置校正。实验显示，在保持样本多样性的同时，显著提高了图像生成的质量。


<details>
  <summary>Details</summary>
Motivation: 生成模型在高维度情况下常表现出系统性偏差，从而降低样本质量。为了解决这一问题，研究者们提出了新的后处理方法，旨在不引入额外噪声或复杂重采样过程的情况下，对已生成样本进行有效的偏置校正。

Method: 开发了基于流匹配的双阶段流细化（BFR）框架，包括两个主要策略：一是针对近似可逆生成器的潜在空间对齐；二是利用轻量级增强训练的数据空间细化。与需要扰乱采样动态的传统改进方法不同，BFR维持原始ODE轨迹不变，并对生成样本施加确定性的修正。

Result: 在MNIST、CIFAR-10及256x256分辨率下的FFHQ数据集上进行了实验验证。结果表明，BFR能够一致地提高生成图像的真实性和覆盖范围。特别是对于MNIST数据集，从基础样本FID为3.95开始，仅通过一次额外函数评估(1-NFE)，潜空间精细化达到了最先进的FID值1.46，同时保持了样本多样性。

Conclusion: 双阶段流细化(BFR)框架提供了一种有效的方法来纠正生成模型中的系统性偏差，而无需依赖于复杂的多步重采样技术或噪声注入。这不仅简化了修正流程，还显著提升了生成图像的质量和多样性。

Abstract: Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \textbf{state-of-the-art} FID of \textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.

</details>


### [35] [Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification](https://arxiv.org/abs/2601.21203)
*Weiguang Wang,Yong Liu,Yingjie Gao,Guangyuan Xu*

Main category: cs.LG

TL;DR: 提出了一种基于自训练范式的跨被试域适应方法，以提高SSVEP信号解码的准确性。该方法包括滤波器组欧氏对齐策略、跨被试自训练框架（含预训练对抗学习和双集成自训练两个阶段）以及时间-频率增强对比学习模块。在Benchmark和BETA数据集上的广泛实验表明，该方法在不同信号长度下均表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决由于个体间信号变化大及用户特定注释成本高导致的SSVEP信号识别性能受限问题。

Method: 采用滤波器组欧氏对齐策略挖掘SSVEP滤波器组中的频率信息；设计了包含预训练对抗学习（PTAL）与双集成自训练（DEST）两阶段的跨被试自训练（CSST）框架；引入时间-频率增强对比学习（TFA-CL）模块来加强特征辨别能力。

Result: 在Benchmark和BETA数据集上进行的大量实验证明，所提方法对于不同长度的信号都能达到最先进的表现水平。

Conclusion: 提出的方法有效地提升了跨被试条件下SSVEP信号解码准确性，并且在多个基准测试中展示了优越性。

Abstract: Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.

</details>


### [36] [A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models](https://arxiv.org/abs/2601.21207)
*Chuan-Shen Hu*

Main category: cs.LG

TL;DR: 本文提出了一种基于胞状层理论框架的方法，用于建模和分析图结构中节点特征和边权重的局部一致性和调和性，并通过拓扑数据分析的多尺度扩展来捕捉图模型中的层次特征交互。


<details>
  <summary>Details</summary>
Motivation: 几何与拓扑深度学习（GDL和TDL）在训练过程中特征的分布和扩散行为仍然是一个未充分探索的问题。

Method: 引入了一个胞状层理论框架来建模和分析图架构内节点特征和边权重的局部一致性和调和性；提出了受拓扑数据分析启发的多尺度扩展方法以捕捉图模型中的层次特征交互。

Result: 该框架提供了关于特征扩散和聚集的拓扑视角，并允许基于其基础的几何和拓扑结构以及定义在其上的学习信号对GDL和TDL架构进行联合表征，为未来的研究提供了见解。

Conclusion: 通过提出的胞状层理论框架和多尺度扩展方法，能够更好地理解GDL和TDL架构下的特征扩散过程，并为诸如节点分类、子结构检测及社区发现等传统任务提供新的研究视角。

Abstract: Combinatorial and topological structures, such as graphs, simplicial complexes, and cell complexes, form the foundation of geometric and topological deep learning (GDL and TDL) architectures. These models aggregate signals over such domains, integrate local features, and generate representations for diverse real-world applications. However, the distribution and diffusion behavior of GDL and TDL features during training remains an open and underexplored problem. Motivated by this gap, we introduce a cellular sheaf theoretic framework for modeling and analyzing the local consistency and harmonicity of node features and edge weights in graph-based architectures. By tracking local feature alignments and agreements through sheaf structures, the framework offers a topological perspective on feature diffusion and aggregation. Furthermore, a multiscale extension inspired by topological data analysis (TDA) is proposed to capture hierarchical feature interactions in graph models. This approach enables a joint characterization of GDL and TDL architectures based on their underlying geometric and topological structures and the learned signals defined on them, providing insights for future studies on conventional tasks such as node classification, substructure detection, and community detection.

</details>


### [37] [PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations](https://arxiv.org/abs/2601.21234)
*Kaiyuan Tan,Kendra Givens,Peilun Li,Thomas Beckers*

Main category: cs.LG

TL;DR: 本文提出了一种名为PHDME的端口哈密顿扩散框架，用于处理稀疏观测和不完全物理信息的情况。通过训练高斯过程分布的端口哈密顿系统来捕捉动力学的能量表示，并基于此生成物理一致的人工数据集以训练扩散模型。该方法在数据稀缺情况下表现出了更高的准确性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在数据稀少的情况下通常不可靠；而物理信息机器学习方法虽然可以改善这种状况，但往往需要明确的控制方程，这在复杂非线性动态中难以获得。因此，作者提出了一个不需要完整控制方程式知识的新框架PHDME，旨在解决稀疏观察和不完全物理条件下的问题。

Method: PHDME首先使用有限的观测值训练一个高斯过程分布式的端口-哈密顿系统（GP-dPHS），以捕捉动力学的能量基础表示。然后利用GP-dPHS生成物理上一致的人工数据集用于扩散训练，并通过结构化的物理残差损失来告知扩散模型。训练完成后，扩散模型可作为快速轨迹生成的摊销采样器和预测器。此外，还应用了分裂一致性校准方法为生成的预测提供不确定性说明。

Result: 实验表明，在PDE基准测试以及真实世界的弹簧系统案例中，所提出的方法相较于现有技术在数据稀缺条件下展现出了更高的精度与更好的物理一致性。

Conclusion: 通过结合端口-哈密顿结构先验与扩散模型的优势，PHDME能够有效地处理稀疏观测及部分未知物理规律情况下的动力系统预测任务，为在数据量有限场景下实现更准确且物理上合理的预测提供了新的途径。

Abstract: Diffusion models provide expressive priors for forecasting trajectories of dynamical systems, but are typically unreliable in the sparse data regime. Physics-informed machine learning (PIML) improves reliability in such settings; however, most methods require \emph{explicit governing equations} during training, which are often only partially known due to complex and nonlinear dynamics. We introduce \textbf{PHDME}, a port-Hamiltonian diffusion framework designed for \emph{sparse observations} and \emph{incomplete physics}. PHDME leverages port-Hamiltonian structural prior but does not require full knowledge of the closed-form governing equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent artificial dataset for diffusion training, and to inform the diffusion model with a structured physics residual loss. After training, the diffusion model acts as an amortized sampler and forecaster for fast trajectory generation. Finally, we apply split conformal calibration to provide uncertainty statements for the generated predictions. Experiments on PDE benchmarks and a real-world spring system show improved accuracy and physical consistency under data scarcity.

</details>


### [38] [Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification](https://arxiv.org/abs/2601.21244)
*Yiju Guo,Tianyi Hu,Zexu Sun,Yankai Lin*

Main category: cs.LG

TL;DR: 提出了一种名为LENS的框架，通过识别并移除造成干扰的提示词来提高强化学习中奖励验证的有效性和效率。实验表明，该方法相比GRPO在性能和收敛速度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前的RLVR方法受限于有限的rollout预算下探索效率低下，导致复杂任务中的采样成功率低和训练不稳定。研究发现许多探索失败并非由于问题难度大，而是由少数几个引入干扰的提示词造成的。

Method: 提出的Less Noise Sampling Framework (LENS)首先通过识别并移除这些干扰性提示词来进行净化处理，然后将从净化过程中获得的成功rollouts转移用于监督原噪声提示下的策略优化，使模型能够学会忽略现实世界嘈杂提示设置中的干扰。

Result: 实验结果显示，LENS比GRPO表现出更高的性能和更快的收敛速度，平均增益为3.88%，并且加速超过1.6倍。

Conclusion: 这项工作强调了剪枝干扰词对于提高rollout效率的关键作用，并为RLVR研究提供了新的视角。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.

</details>


### [39] [DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher](https://arxiv.org/abs/2601.21283)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 提出了一种名为DUET的新型基于蒸馏的知识遗忘方法，结合了调优和上下文两种现有方法的优点，通过模仿一个能有效拒绝生成不希望知识的同时保留通用领域知识的教师模型来实现。该方法在忘记效果和实用性保持方面表现出色，并且比现有的最先进方法更高效。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型（LLM）知识遗忘技术存在重要限制：基于调优的方法计算成本高且容易导致灾难性遗忘；而基于上下文的知识遗忘虽然对精确遗忘较为轻便，但易受提示移除或逆向工程攻击。为解决这些问题，提出了DUET方法。

Method: DUET是一种基于蒸馏的知识遗忘方法，它让学习者模型模仿由提示引导的教师模型的行为，后者能够有效地拒绝产生不需要的知识，同时保持一般领域的知识。

Result: 广泛的评估表明，DUET不仅在遗忘性能和效用保持方面优于现有基准，而且在数据效率上也远超当前最先进的遗忘方法。

Conclusion: DUET代表了一种有效的、数据高效的解决方案，用于从大规模语言模型中去除不需要的知识，同时保持其整体功能。

Abstract: LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.

</details>


### [40] [Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation](https://arxiv.org/abs/2601.21285)
*Ruifeng Zhang,Zexi Huang,Zikai Wang,Ke Sun,Bohang Zheng,Zhen Ouyang,Huimin Xie,Phil Shen,Junlin Zhang,Wentao Guo,Qinglei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Zenith的可扩展且高效的排名架构，它能够以最小的运行时开销学习复杂的特征交互，并在TikTok Live上部署后，在线CTR AUC和Logloss分别提高了+1.05%/-1.10%，同时在优质观看会话/用户数和优质观看时长/用户数方面也分别实现了+9.93%和+8.11%的增长。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的工作探索了多种模型架构来捕捉多粒度特征交互，但对于高效处理特征并扩大模型容量而不导致过高的推理延迟的关注相对较少。本文旨在通过提出一种新的架构解决这一问题。

Method: 提出了Zenith架构，该架构通过Token Fusion和Token Boost模块处理少数高维主要令牌，展示了相比其他最先进排名方法更优的扩展性法则。

Result: Zenith架构在TikTok Live上的A/B测试显示，在线CTR AUC和Logloss分别提升了+1.05%/-1.10%，同时在优质观看会话/用户数(+9.93%)和优质观看时长/用户数(+8.11%)上也取得了显著增长。

Conclusion: Zenith提供了一种有效的方法来提高推荐系统的性能，特别是在处理大规模数据集时保持较低的运行时开销方面表现出色。

Abstract: Accurately capturing feature interactions is essential in recommender systems, and recent trends show that scaling up model capacity could be a key driver for next-level predictive performance. While prior work has explored various model architectures to capture multi-granularity feature interactions, relatively little attention has been paid to efficient feature handling and scaling model capacity without incurring excessive inference latency. In this paper, we address this by presenting Zenith, a scalable and efficient ranking architecture that learns complex feature interactions with minimal runtime overhead. Zenith is designed to handle a few high-dimensional Prime Tokens with Token Fusion and Token Boost modules, which exhibits superior scaling laws compared to other state-of-the-art ranking methods, thanks to its improved token heterogeneity. Its real-world effectiveness is demonstrated by deploying the architecture to TikTok Live, a leading online livestreaming platform that attracts billions of users globally. Our A/B test shows that Zenith achieves +1.05%/-1.10% in online CTR AUC and Logloss, and realizes +9.93% gains in Quality Watch Session / User and +8.11% in Quality Watch Duration / User.

</details>


### [41] [TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification](https://arxiv.org/abs/2601.21289)
*Akash Pandey,Payal Mohapatra,Wei Chen,Qi Zhu,Sinan Keten*

Main category: cs.LG

TL;DR: 本文提出了一种新的深度学习框架TimeSliver，用于解释时间序列分类模型的决策。该框架通过同时利用原始时间序列数据及其符号抽象来构建保持原始时间结构的表示，并为每个时间点分配有意义的重要性分数。TimeSlilver在多种合成和真实世界多变量时间序列数据集上优于其他时间归因方法，且在26个UEA基准数据集上的预测性能接近最先进基线。


<details>
  <summary>Details</summary>
Motivation: 现有的基于梯度和特征归因的事后可解释方法存在参考状态敏感性问题，难以跨时间序列数据集泛化，因为它们独立处理时间点并忽略顺序依赖关系。尽管使用自注意力机制估计时间归因提供了一种可解释的时间序列分类视角，但最新发现表明这些注意力权重往往不能忠实衡量时间重要性。因此，需要一种新的方法来提高时间序列模型决策的透明度。

Method: 研究者们提出了一个名为TimeSliver的新颖可解释驱动的深度学习框架，它联合利用了原始时间序列数据及其符号抽象，构建了一个能够维持原有时间结构的表示。这种表示中的每个元素都线性编码了每个时间片段对最终预测的贡献，从而可以给每个时间点分配有意义的重要性得分。

Result: 对于时间序列分类任务，TimeSliver在7个不同的合成和真实世界的多变量时间序列数据集上比其他时间归因方法表现高出11%。此外，在26个UEA基准数据集中，TimeSliver的预测性能与最先进的基线相差不超过2%，这使得它成为一个强有力且可解释的一般时间序列分类框架。

Conclusion: TimeSliver作为一个强大的、可解释的时间序列分类框架，不仅提高了模型决策的透明度，还在多个基准测试中展示了优秀的性能。

Abstract: Identifying the extent to which every temporal segment influences a model's predictions is essential for explaining model decisions and increasing transparency. While post-hoc explainable methods based on gradients and feature-based attributions have been popular, they suffer from reference state sensitivity and struggle to generalize across time-series datasets, as they treat time points independently and ignore sequential dependencies. Another perspective on explainable time-series classification is through interpretable components of the model, for instance, leveraging self-attention mechanisms to estimate temporal attribution; however, recent findings indicate that these attention weights often fail to provide faithful measures of temporal importance. In this work, we advance this perspective and present a novel explainability-driven deep learning framework, TimeSliver, which jointly utilizes raw time-series data and its symbolic abstraction to construct a representation that maintains the original temporal structure. Each element in this representation linearly encodes the contribution of each temporal segment to the final prediction, allowing us to assign a meaningful importance score to every time point. For time-series classification, TimeSliver outperforms other temporal attribution methods by 11% on 7 distinct synthetic and real-world multivariate time-series datasets. TimeSliver also achieves predictive performance within 2% of state-of-the-art baselines across 26 UEA benchmark datasets, positioning it as a strong and explainable framework for general time-series classification.

</details>


### [42] [Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning](https://arxiv.org/abs/2601.21293)
*Changyu Li,Dingcheng Huang,Kexuan Yao,Xiaoya Ni,Lijuan Shen,Fei Luo*

Main category: cs.LG

TL;DR: 提出了一种物理引导的微型曼巴变压器（PG-TMT）用于旋转机械的可靠性中心预测，该模型结合了微瞬态捕捉、近线性长程动态建模和跨通道共振编码，并通过极端值理论确保决策可靠性，实现了在不同领域数据集上的高精度召回率AUC和较短的平均检测时间。


<details>
  <summary>Details</summary>
Motivation: 针对旋转机械的可靠性中心预测需要在非平稳工作条件下、跨越速度/负载/传感器域以及严重类别不平衡的情况下保持准确的早期预警信号，同时将误报率控制在一个较小且可预测的范围内。

Method: 提出了Physics-Guided Tiny-Mamba Transformer (PG-TMT)，一种专为在线状态监测设计的紧凑型三分支编码器。它包括一个用于捕捉微瞬变的深度可分离卷积基干、一个用于建模近线性长程动力学的Tiny-Mamba状态空间分支，以及一个轻量级局部Transformer来编码跨通道共振。此外，还引入了一个从时间到频谱的解析映射，将模型的注意力谱与经典的轴承故障阶次带相关联，从而得到一个量化物理合理性的带对齐评分。为了保证决策的可靠性，使用极值理论(EVT)对健康分数超标情况进行建模，以实现特定的目标误报强度(每小时事件数)；双阈值滞后加上最小保持时间进一步减少了抖动现象。

Result: 在CWRU, Paderborn, XJTU-SY及工业试点数据集上，采用无泄漏流协议并对漏检实施右删失处理后，PG-TMT达到了更高的精确度-召回率曲线下面积(AUC)、具有竞争力或更优的ROC AUC，以及在匹配误报强度下较短的平均检测时间，同时还表现出强大的跨域迁移能力。

Conclusion: 通过将符合物理特性的表示与EVT校准的决策规则相结合，PG-TMT提供了经过校准、可解释且准备就绪的早期预警，适用于以可靠性为中心的预后健康管理。

Abstract: Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.

</details>


### [43] [Missing-Data-Induced Phase Transitions in Spectral PLS for Multimodal Learning](https://arxiv.org/abs/2601.21294)
*Anders Gjølbye,Ida Kargaard,Emma Kargaard,Lars Kai Hansen*

Main category: cs.LG

TL;DR: 本文研究了在高维尖峰模型中，当数据存在独立条目级完全随机缺失时，偏最小二乘法(PLS-SVD)的表现。结果表明，经过适当归一化后，被遮掩的交叉协方差矩阵的行为类似于一个有效信号强度衰减了√ρ的尖峰矩形随机矩阵。这导致PLS-SVD表现出一种尖锐的BBP型相变：低于关键信噪比阈值时，主奇异向量渐近无信息；高于该阈值时，它们与潜在共享方向达到非平凡对齐，并且有封闭形式的渐近重叠公式。模拟和半合成多模态实验验证了预测的相图和不同长宽比、信号强度及缺失水平下的恢复曲线。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨当面对成对数据集中的条目级随机缺失情况时，偏最小二乘法（PLS）特别是基于奇异值分解的方法（PLS-SVD），如何有效地学习共享结构。由于现实世界中的多模态数据集经常存在两个视图内都有缺失值的情况，理解这种条件下算法性能的变化对于提高数据分析质量至关重要。

Method: 通过采用独立条目级完全随机缺失的遮掩方式，在比例化的高维尖峰模型背景下考察PLS-SVD方法。分析过程中特别关注了经过适当归一化处理后的被遮掩交叉协方差矩阵性质变化及其对PLS-SVD性能的影响。

Result: 研究表明，当信号强度超过某个临界信噪比阈值时，PLS-SVD能够实现与潜在共享方向的非平凡对准，并给出了闭式渐进重叠公式来描述这一过程。反之，则主奇异向量变得渐近无信息。此外，仿真和半合成多模态实验证实了理论预测的准确性。

Conclusion: 本研究表明，在存在数据缺失的情况下，PLS-SVD依然可以通过适当的条件识别出有用的共享结构信息。更重要的是，它揭示了一个清晰的相变现象，即只有当信号强度足够大时，才能保证算法输出的有效性。这些发现为理解和改进处理不完整多模态数据集提供了新的视角。

Abstract: Partial Least Squares (PLS) learns shared structure from paired data via the top singular vectors of the empirical cross-covariance (PLS-SVD), but multimodal datasets often have missing entries in both views. We study PLS-SVD under independent entry-wise missing-completely-at-random masking in a proportional high-dimensional spiked model. After appropriate normalization, the masked cross-covariance behaves like a spiked rectangular random matrix whose effective signal strength is attenuated by $\sqrtρ$, where $ρ$ is the joint entry retention probability. As a result, PLS-SVD exhibits a sharp BBP-type phase transition: below a critical signal-to-noise threshold the leading singular vectors are asymptotically uninformative, while above it they achieve nontrivial alignment with the latent shared directions, with closed-form asymptotic overlap formulas. Simulations and semi-synthetic multimodal experiments corroborate the predicted phase diagram and recovery curves across aspect ratios, signal strengths, and missingness levels.

</details>


### [44] [Grounding and Enhancing Informativeness and Utility in Dataset Distillation](https://arxiv.org/abs/2601.21296)
*Shaobo Wang,Yantai Yang,Guo Chen,Peiru Li,Kaixin Li,Yufa Zhou,Zhaorun Chen,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息性和实用性的数据集蒸馏框架InfoUtil，通过最大化样本的信息性并选择具有全局影响力的样本来优化合成数据集的质量。实验表明，在ImageNet-1K数据集上使用ResNet-18模型时，该方法相较于现有最先进技术性能提高了6.1%。


<details>
  <summary>Details</summary>
Motivation: 当前的数据集蒸馏方法往往依赖于启发式方法来平衡效率和质量，但对原始数据与合成数据之间基本关系的研究还不足。为了解决这个问题，并在坚实的理论框架内重新审视基于知识蒸馏的数据集蒸馏过程，提出了新的概念和方法。

Method: 提出了InfoUtil框架，该框架结合了两个关键组成部分：（1）利用Shapley值归因进行博弈论信息性最大化，以从样本中提取关键信息；（2）基于梯度范数选择具有全局影响力的样本，从而实现原则上的实用性最大化。

Result: 实验结果显示，在使用ResNet-18模型的ImageNet-1K数据集上，提出的方法比之前的最先进方法提高了6.1%的性能。

Conclusion: 通过引入信息性和实用性的概念以及开发InfoUtil框架，本文提供了一种更有效率且高质量的数据集蒸馏方法，能够在保持或提高模型性能的同时减少所需的数据量。

Abstract: Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.

</details>


### [45] [Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning with a New Contraction Principle](https://arxiv.org/abs/2601.21301)
*Zijun Chen,Zaiwei Chen,Nian Si,Shengbo Wang*

Main category: cs.LG

TL;DR: 本文研究了平均奖励马尔可夫决策过程中同步和异步Q学习的收敛速度问题，通过引入可达性假设和采用惰性动态采样方法，证明了一种简单变体的Q学习算法能够达到最优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 在平均奖励马尔可夫决策过程（MDP）中，缺乏自然的收缩性质给分析Q学习算法的收敛率带来了挑战。现有的一些非渐近结果要么依赖于强假设来强制半范数收缩，要么依靠折扣或阶段性MDP作为连续逼近手段，这通常需要未知参数或导致次优的样本复杂度。因此，本研究旨在寻找一种更有效的方法，在不施加过强假设的前提下，解决这一问题并提高样本效率。

Method: 作者提出了一种基于可达性假设的方法，并对原始MDP进行‘惰性’变换——即以一定概率保持系统当前状态不变。通过对这种变换后的MDP构造一个实例相关的半范数，证明了贝尔曼算子在这种半范数下成为一步收缩的。基于此发现，进一步分析了经过调整后的同步及异步Q学习算法的性能。

Result: 研究表明，在所提出的条件下，对于进行了惰性转换的MDP，无论是同步还是异步形式的简化版Q学习算法都能够保证接近最优的\(\widetilde{O}(\varepsilon^{-2})\)（忽略对数因子）样本复杂度。这意味着相比之前的工作，该方法能够在不需要额外参数知识的情况下，实现更高效的学习过程。

Conclusion: 本文为平均奖励MDPs下的Q学习提供了一种新的分析框架，表明通过适当设计系统动态（如引入惰性），可以显著改善学习算法的理论性能保证。特别是，它展示了如何利用特定结构（可达性和适当的半范数选择）来克服传统上难以处理的问题，从而获得更佳的样本效率。

Abstract: We present the convergence rates of synchronous and asynchronous Q-learning for average-reward Markov decision processes, where the absence of contraction poses a fundamental challenge. Existing non-asymptotic results overcome this challenge by either imposing strong assumptions to enforce seminorm contraction or relying on discounted or episodic Markov decision processes as successive approximations, which either require unknown parameters or result in suboptimal sample complexity. In this work, under a reachability assumption, we establish optimal $\widetilde{O}(\varepsilon^{-2})$ sample complexity guarantees (up to logarithmic factors) for a simple variant of synchronous and asynchronous Q-learning that samples from the lazified dynamics, where the system remains in the current state with some fixed probability. At the core of our analysis is the construction of an instance-dependent seminorm and showing that, after a lazy transformation of the Markov decision process, the Bellman operator becomes one-step contractive under this seminorm.

</details>


### [46] [The Surprising Difficulty of Search in Model-Based Reinforcement Learning](https://arxiv.org/abs/2601.21306)
*Wei-Di Chang,Mikael Henaff,Brandon Amos,Gregory Dudek,Scott Fujimoto*

Main category: cs.LG

TL;DR: 本文探讨了基于模型的强化学习中的搜索问题，发现即使在模型非常准确的情况下，搜索也可能损害性能。更重要的是缓解分布偏移而非单纯提高模型或价值函数的准确性。基于这一见解，研究者识别出了一些关键技术以实现有效的搜索，并在多个流行的基准领域中达到了最先进的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的观点认为，在基于模型的强化学习中，长期预测和累积误差是主要障碍。本文挑战了这种看法，指出搜索并不能简单地作为学习到的策略的替代品。

Method: 通过实验分析表明，即使模型非常准确，搜索也可能会损害性能。研究的重点转向了如何缓解因使用模型进行规划而引起的分布偏移问题上。

Result: 研究发现了缓解分布偏移比提高模型或价值函数准确性更加重要，并且据此确定了几种能够实现有效搜索的关键技术。

Conclusion: 该研究表明，在基于模型的强化学习里，有效地处理分布偏移对于改进算法性能至关重要；所提出的方法在多个标准测试环境中取得了领先的成绩。

Abstract: This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.

</details>


### [47] [Transferable Graph Condensation from the Causal Perspective](https://arxiv.org/abs/2601.21309)
*Huaming Du,Yijie Huang,Su Yao,Yiying Wang,Yueyang Zhou,Jingwen Yang,Jinshi Zhang,Han Ji,Yu Zhao,Guisong Liu,Hegui Zhang,Carl Yang,Gang Kou*

Main category: cs.LG

TL;DR: 提出了一种基于因果不变性和可迁移性的图数据集压缩方法TGGC，通过因果干预提取领域因果不变特征，并通过增强的压缩操作和频谱域对比学习保留原始图的因果信息。实验表明，该方法在跨任务和跨领域的复杂场景下比现有方法提高了13.41%的表现，并在单个数据集和任务场景中5个数据集上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 随着图数据集规模的增长，虽然提升了图表示学习方法的性能，但也带来了显著的训练挑战。现有的图数据集压缩技术虽然能将大规模数据集压缩成较小但信息丰富的数据集，保持相似的测试表现，但这些方法严格要求下游应用与原数据集和任务相匹配，在跨任务和跨领域场景中往往失败。为解决这些问题，提出了新的解决方案。

Method: 提出的方法TGGC首先利用因果干预从图的空间域中提取领域因果不变特征，以保留跨领域知识；接着执行增强的压缩操作，充分捕捉原始图的结构和特征信息；最后通过频谱域强化对比学习，将因果不变特征注入到压缩图中，确保压缩后的图保留了原始图的因果信息。

Result: 实验结果表明，TGGC在跨任务和跨领域复杂场景下的表现相较于现有方法最高提升了13.41%，并且在单一数据集和任务场景中的6个数据集里有5个达到了最先进的性能。

Conclusion: TGGC是一种有效的、可迁移的图数据集压缩方法，能够较好地应对跨任务及跨领域的挑战，同时在维持甚至提升性能方面表现出色。

Abstract: The increasing scale of graph datasets has significantly improved the performance of graph representation learning methods, but it has also introduced substantial training challenges. Graph dataset condensation techniques have emerged to compress large datasets into smaller yet information-rich datasets, while maintaining similar test performance. However, these methods strictly require downstream applications to match the original dataset and task, which often fails in cross-task and cross-domain scenarios. To address these challenges, we propose a novel causal-invariance-based and transferable graph dataset condensation method, named \textbf{TGCC}, providing effective and transferable condensed datasets. Specifically, to preserve domain-invariant knowledge, we first extract domain causal-invariant features from the spatial domain of the graph using causal interventions. Then, to fully capture the structural and feature information of the original graph, we perform enhanced condensation operations. Finally, through spectral-domain enhanced contrastive learning, we inject the causal-invariant features into the condensed graph, ensuring that the compressed graph retains the causal information of the original graph. Experimental results on five public datasets and our novel \textbf{FinReport} dataset demonstrate that TGCC achieves up to a 13.41\% improvement in cross-task and cross-domain complex scenarios compared to existing methods, and achieves state-of-the-art performance on 5 out of 6 datasets in the single dataset and task scenario.

</details>


### [48] [Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation](https://arxiv.org/abs/2601.21315)
*Seonghwi Kim,Sung Ho Jo,Wooseok Ha,Minwoo Chae*

Main category: cs.LG

TL;DR: 本文提出了一种新的分布鲁棒学习框架，用于解决无监督领域适应(UDA)问题，特别是在目标域数据有限或源域存在虚假相关性的情况下。该方法可以与现有的UDA方法无缝集成，并在各种分布偏移场景下的实验表明，当目标数据极其稀缺时，该方法始终优于强大的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督领域适应(UDA)方法在实践中经常遇到困难，特别是在目标域只有有限的未标记数据或源域被虚假相关性主导的情况下。为了克服这些挑战，研究者们提出了一个新的框架。

Method: 提出了一种新的分布鲁棒学习框架，能够同时建模协变量分布和条件标签分布中的不确定性。该框架受到多源领域适应设置的启发，但也可直接应用于单一源场景。此外，还开发了一个高效的学习算法，可与现有UDA方法无缝集成。

Result: 广泛的实验结果表明，在不同的分布偏移情况下，所提方法相比强基线表现更优，尤其是在目标数据非常稀缺的情况下。

Conclusion: 新提出的分布鲁棒学习框架为解决UDA问题提供了一个有效途径，特别是在处理有限的目标域数据或对抗源域内的虚假相关性方面展现出了显著优势。

Abstract: Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.

</details>


### [49] [Memorization Control in Diffusion Models from Denoising-centric Perspective](https://arxiv.org/abs/2601.21348)
*Thuy Phuong Vu,Mai Viet Hoang Do,Minhhuy Le,Dinh-Cuong Hoang,Phan Xuan Tan*

Main category: cs.LG

TL;DR: 本文从去噪角度研究了扩散模型中的记忆现象，并提出了一种新的时间步采样策略来控制学习发生的位置，从而直接调控记忆与泛化的权衡。实验表明该方法能够有效减少记忆现象并提高生成数据与训练数据的分布一致性。


<details>
  <summary>Details</summary>
Motivation: 控制扩散模型中的记忆现象对于需要生成数据紧密匹配训练分布的应用至关重要。现有方法主要集中在数据或模型层面的修改，而将扩散模型视为孤立的预测器。本文旨在从去噪中心的角度探讨记忆现象，指出均匀时间步采样导致的学习贡献不均等是引起记忆倾向的原因之一。

Method: 提出了一种新的时间步采样策略，通过调整置信区间宽度来明确控制沿去噪轨迹的学习位置。这种方法允许直接管理记忆-泛化之间的平衡。

Result: 在图像和一维信号生成任务上的实验表明，强调后期去噪步骤可以一致地减少记忆效应，并增强生成样本与训练数据之间的分布对齐度。

Conclusion: 本研究提供了一种新颖的方法来调节扩散模型内的记忆水平，通过改变学习过程中的时间步选择机制，证明了其在多种生成任务中改善模型性能的有效性。

Abstract: Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.

</details>


### [50] [L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.21349)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 提出了一种名为L2R的新路由框架，通过在低秩潜在路由空间中执行专家分配，并引入饱和内积评分(SIPS)来控制路由函数的Lipschitz行为，从而提高MoE模型中的路由区分度和专家特化稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前许多现代MoE系统仍然使用原始高维表示空间中的线性路由器，这会导致表示不匹配、角度集中以及尺度敏感评分问题，这些问题共同削弱了路由区分度及稳定的专家特化。

Method: 提出了Low-rank & Lipschitz-controlled Routing (L2R)，一种统一的路由框架，它重塑了路由空间和评分几何。L2R在一个共享的低秩潜在路由空间中执行专家分配，并且引入了Saturated Inner-Product Scoring (SIPS)来显式地控制路由函数的Lipschitz行为，从而产生更平滑和更稳定的路由几何。此外，L2R还结合了一个参数高效的多锚点路由机制以增强专家的表现力。

Result: 在大规模语言MoE模型和ImageNet上的视觉MoE设置上进行的广泛实验表明，L2R能够持续改进路由稳定性、专家特化程度以及整体模型性能。

Conclusion: 本研究提出的L2R方法为改善Mixture-of-Experts（MoE）模型中的路由问题提供了有效解决方案，通过优化路由空间与评分机制显著提升了模型的整体表现。

Abstract: Mixture-of-Experts (MoE) models scale neural networks by conditionally activating a small subset of experts, where the router plays a central role in determining expert specialization and overall model performance. However, many modern MoE systems still adopt linear routers in raw high-dimensional representation spaces, where representation mismatch, angular concentration, and scale-sensitive scoring can jointly undermine routing discriminability and stable expert specialization. In this work, we propose Low-rank \& Lipschitz-controlled Routing (L2R), a unified routing framework that reshapes both the routing space and scoring geometry. L2R performs expert assignment in a shared low-rank latent routing space and introduces Saturated Inner-Product Scoring (SIPS) to explicitly control the Lipschitz behavior of routing functions, yielding smoother and more stable routing geometry. In addition, L2R incorporates a parameter-efficient multi-anchor routing mechanism to enhance expert expressiveness. Extensive experiments on a large-scale language MoE model and a vision MoE setting on ImageNet demonstrate that L2R consistently improves routing stability, expert specialization, and overall model performance.

</details>


### [51] [Factored Causal Representation Learning for Robust Reward Modeling in RLHF](https://arxiv.org/abs/2601.21350)
*Yupei Yang,Lin Yang,Wanxi Deng,Lin Qu,Fan Feng,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: 提出了一种基于因果视角的分解表示学习框架，旨在通过将模型的上下文嵌入分解为因果因素和非因果因素来提高奖励模型的鲁棒性，并通过实验验证了该方法在减少奖励黑客行为方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 标准奖励模型容易受到与人类标签无因果关系的虚假特征的影响，导致高预测奖励并不意味着更好的行为表现。为了使大型语言模型更好地与人类偏好对齐，需要一种更加可靠的方法来构建奖励模型。

Method: 本文提出了一个分解表示学习框架，将模型的上下文嵌入分为足够用于奖励预测的因果因素以及捕捉与奖励无关属性（如长度或谄媚偏差）的非因果因素。奖励头仅依赖于因果组件。此外，引入了一个对抗头，训练它从非因果因素预测奖励，同时应用梯度反转以阻止它们编码与奖励相关的信息。

Result: 实验证明，在数学和对话任务上，所提出的方法可以学习到更稳健的奖励模型，并且相对于最先进的基线，始终提高了下游RLHF性能。进一步分析表明，该方法在减轻因长度或谄媚偏差引起的奖励黑客行为方面是有效的。

Conclusion: 采用因果视角设计的分解表示学习框架能够有效提升奖励模型的质量，使得通过人类反馈进行强化学习时，大语言模型能更好地符合人类偏好。

Abstract: A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model's contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.

</details>


### [52] [Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving](https://arxiv.org/abs/2601.21351)
*Chendong Song,Meixuan Wang,Hang Zhou,Hong Liang,Yuan Lyu,Zixi Chen,Yuwei Fan,Zijie Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种用于LLM解码的新兴架构——注意力-FFN拆分（AFD），并通过分析框架确定了最优的A/F比例，以最大化系统中每个实例的平均吞吐量。


<details>
  <summary>Details</summary>
Motivation: 注意力-FFN拆分（AFD）作为一种新兴的LLM解码架构，允许内存和计算资源独立扩展，但其性能对注意力/FFN配置比例非常敏感。不当的比例会导致步级阻塞和昂贵的设备空闲时间。因此，研究者开发了一个可操作的分析框架来解决这一问题。

Method: 研究者为$r$A-$1$F拓扑结构中的AFD捆绑尺寸开发了一个易于处理的分析框架。考虑到注意力侧的工作是非平稳的而FFN工作相对稳定，他们使用概率工作负载模型推导出闭式规则来找到最优的A/F比例，从而在整个系统中实现每实例平均吞吐量的最大化。

Result: 通过校准过的AFD模拟器验证理论表明，在不同工作负载下，理论上最优的A/F比与仿真结果中的最优值相差不超过10%，并且能够持续减少空闲时间。

Conclusion: 通过引入一个可操作的分析框架以及概率工作负载模型，研究者成功地找出了在给定条件下最大化系统吞吐量的同时最小化闲置时间的最佳A/F比例设定方法。

Abstract: Attention-FFN disaggregation (AFD) is an emerging architecture for LLM decoding that separates state-heavy, KV-cache-dominated Attention computation from stateless, compute-intensive FFN computation, connected by per-step communication. While AFD enables independent scaling of memory and compute resources, its performance is highly sensitive to the Attention/FFN provisioning ratio: mis-sizing induces step-level blocking and costly device idle time. We develop a tractable analytical framework for sizing AFD bundles in an $r$A-$1$F topology, where the key difficulty is that Attention-side work is nonstationary-token context grows and requests are continuously replenished with random lengths-while FFN work is stable given the aggregated batch. Using a probabilistic workload model, we derive closed-form rules for the optimal A/F ratio that maximize average throughput per instance across the system. A trace-calibrated AFD simulator validates the theory: across workloads, the theoretical optimal A/F ratio matches the simulation-optimal within 10%, and consistently reduces idle time.

</details>


### [53] [Expected Improvement via Gradient Norms](https://arxiv.org/abs/2601.21357)
*Joshua Hang Sai Ip,Georgios Makrygiorgos,Ali Mesbah*

Main category: cs.LG

TL;DR: 本文提出了一种新的获取函数EI-GN，该函数通过利用梯度信息来改进贝叶斯优化中的期望改进方法，从而在高表现区域和接近一阶平稳性的区域促进采样。实证评估表明，EI-GN在标准基准测试中相比基线方法表现出一致的改进，并且也适用于控制策略学习问题。


<details>
  <summary>Details</summary>
Motivation: 尽管期望改进（EI）作为贝叶斯优化中广泛使用的获取函数取得了经验上的成功，但它过于依赖于开发已知的信息，可能导致收敛到次优稳定点。因此，需要一种既能探索又能有效利用梯度信息的方法，以提高优化效率和结果质量。

Method: 作者提出了基于梯度范数的期望改进（EI-GN），这是一种新的获取函数，它将改进原则应用于一个考虑梯度的辅助目标上，以此鼓励在性能高且接近第一阶平稳性的区域内进行采样。此方法还依赖于观测到的梯度值来构建增强型替代模型，支持从函数评价中推断出梯度。此外，文章提供了EI-GN的一种可处理的闭式表达形式，使得该获取函数可以被高效地优化。

Result: 通过在标准贝叶斯优化基准上的实证评估显示，与传统基线相比，使用EI-GN方法能够带来持续性的改进。此外，EI-GN也被证实可用于控制策略学习任务。

Conclusion: 提出的EI-GN方法有效地结合了梯度信息和期望改进原则，在不牺牲探索性的同时提高了局部搜索的质量，从而为解决昂贵黑盒函数优化问题提供了一个强有力的新工具。

Abstract: Bayesian Optimization (BO) is a principled approach for optimizing expensive black-box functions, with Expected Improvement (EI) being one of the most widely used acquisition functions. Despite its empirical success, EI is known to be overly exploitative and can converge to suboptimal stationary points. We propose Expected Improvement via Gradient Norms (EI-GN), a novel acquisition function that applies the improvement principle to a gradient-aware auxiliary objective, thereby promoting sampling in regions that are both high-performing and approaching first-order stationarity. EI-GN relies on gradient observations used to learn gradient-enhanced surrogate models that enable principled gradient inference from function evaluations. We derive a tractable closed-form expression for EI-GN that allows efficient optimization and show that the proposed acquisition is consistent with the improvement-based acquisition framework. Empirical evaluations on standard BO benchmarks demonstrate that EI-GN yields consistent improvements against standard baselines. We further demonstrate applicability of EI-GN to control policy learning problems.

</details>


### [54] [Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.21418)
*Qian Wan,Ziao Xu,Luona Wei,Xiaoxuan Shen,Jianwen Sun*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的大规模推理模型（LRM）训练框架Difficulty-aware Policy Optimization (DiPO)，该框架能够使模型自发地评估任务复杂度，并据此调整推理资源的分配，从而有效减少冗余推理步骤而不影响性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模推理模型在处理简单任务时容易出现过度思考的问题，导致资源使用效率低下。尽管已有研究尝试通过提示设计或模型训练来缓解这个问题，但它们往往忽略了任务难度感知的重要性，使得模型难以有效地分配推理资源。

Method: 本文提出了Difficulty-aware Policy Optimization (DiPO)框架，利用强化学习技术鼓励模型自发地对任务复杂性进行建模，并将这种意识融入到后训练过程中以调节生成偏好。同时，介绍了一种基于模型自我推理的任务难度建模方法以及一种增强型奖励函数，后者能够在考虑推理表现和输出格式的同时，对于过长的推理施加惩罚。

Result: 实验结果显示，采用DiPO框架后，模型可以自发地调整推理开销，在不损失因思维压缩而带来的性能下降的前提下，显著减少了冗余令牌的数量。

Conclusion: 通过引入任务难度意识并结合强化学习机制，DiPO提供了一种有效的方法来优化大规模推理模型的推理过程，使其更加高效且适应性强。

Abstract: Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.

</details>


### [55] [ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation](https://arxiv.org/abs/2601.21420)
*Zihao Huang,Jundong Zhou,Xingwei Qu,Qiyang Min,Ge Zhang*

Main category: cs.LG

TL;DR: ConceptMoE, a new method for large language models, dynamically merges similar tokens into concepts, improving efficiency and performance. It outperforms standard MoE in various tasks, reduces computation, and speeds up both prefill and decoding processes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency of current large language models which apply uniform computation across all tokens, regardless of their predictability or need for deep reasoning. ConceptMoE aims to improve this by dynamically merging semantically similar tokens and optimizing the computational resources needed for different types of sequences.

Method: ConceptMoE uses a learnable chunk module to identify and merge semantically similar tokens into concept representations, compressing input sequences before they enter the main model. This approach allows for implicit token-level compute allocation. The method also leverages MoE (Mixture of Experts) architecture to enable controlled evaluation and maintain baseline FLOPs and parameters while isolating architectural benefits.

Result: ConceptMoE outperforms standard MoE on multiple benchmarks, including +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. It also reduces attention computation and KV cache usage, leading to significant speedups in prefill and decoding processes, especially for long sequences.

Conclusion: ConceptMoE demonstrates that adaptive concept-level processing can fundamentally enhance the effectiveness and efficiency of large language models, making it a promising direction for future research and practical applications.

Abstract: Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio $R$ before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to $R^2\times$ and KV cache by $R\times$. At $R=2$, empirical measurements show prefill speedups reaching 175\% and decoding speedups up to 117\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.

</details>


### [56] [From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning](https://arxiv.org/abs/2601.21436)
*Hang Ni,Weijia Zhang,Fei Wang,Zezhi Shao,Hao Liu*

Main category: cs.LG

TL;DR: 提出了一种名为MADI的多模态大语言模型，通过精细对齐和解缠互动来提高时间序列理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理时间序列时存在跨模态细粒度错位及共享与特定模态语义严重纠缠的问题，影响了局部解释与互补推理的有效性。

Method: MADI采用补丁级对齐、离散解缠互动以及关键令牌高亮三项技术，分别解决跨异构模态间的物理基础细粒度对应关系、分离模态共通语义为紧凑离散潜在变量并适应性地协同净化后的独特模态信息、强调对查询相关信号的信息性以实现鲁棒推理等问题。

Result: 实验结果表明，在合成和真实世界基准测试中，MADI相对于通用的大语言模型和专门针对时间序列的多模态大语言模型表现更优。

Conclusion: MADI通过引入精细对齐与解缠交互机制，显著提升了多模态大语言模型在时间序列理解和推理任务中的性能。

Abstract: Advances in multi-modal large language models (MLLMs) have inspired time series understanding and reasoning tasks, that enable natural language querying over time series, producing textual analyses of complex temporal dynamics. Recent attempts hybridize numerical time series with their visualized plots, facilitating precise value reasoning and visual structure comprehension for comprehensive time series understanding of MLLMs. However, effective cross-modal integration remains challenging due to fine-grained temporal misalignment across modalities and severe entanglement between shared and modality-specific semantics, which hinder localized interpretation and complementary reasoning. To address these issues, we propose MADI, a multi-modal LLM enhanced with fine-grained alignment and disentangled interaction, featuring (1) Patch-level Alignment, which enforces physically grounded fine-grained correspondence across heterogeneous modalities, (2) Discrete Disentangled Interaction, which separates modality-common semantics into compact discrete latents and adaptively synergizes the purified modality-unique information, and (3) Critical-token Highlighting, which emphasizes informative, query-relevant signals for robust reasoning. Experiments on synthetic and real-world benchmarks show that MADI consistently outperforms general-purpose LLMs and time-series-specialized MLLMs.

</details>


### [57] [SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation](https://arxiv.org/abs/2601.21452)
*Yu Xie,Xing Kai Ren,Ying Qi,Hu Yao*

Main category: cs.LG

TL;DR: 本文提出了一种新的优化框架SAGE，旨在解决现有推荐系统中大型语言模型依赖单独词汇表导致的高维护成本和差扩展性问题。此外，SAGE通过序列级信号解耦和非对称自适应动态机制解决了OneRec GBPO策略中的"对称保守主义"问题，从而改善了冷启动项目的表现并保持了推荐多样性。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型（LLM）的推荐系统如OneRec面临的问题包括：需要独立的词汇表，这限制了原生LLM词汇的有效利用，增加了维护成本且不利于扩展；以及OneRec采用的GBPO方法存在"对称保守主义"问题，无法有效处理冷启动物品同时在高噪声环境下难以维持推荐多样性。

Method: 提出了SAGE（Sequence-level Adaptive Gradient Evolution），一种针对列表生成式推荐设计的统一优化框架。SAGE包含两大创新点：1) 序列级信号解耦，通过结合几何平均重要性比率与解耦多目标优势来消除token级别的方差并解决"奖励崩溃"问题；2) 非对称自适应动力学，构建了一个动态梯度流形，在高潜力冷启动项上应用“增益因子”以实现超线性更新，并采用“熵感知惩罚”打破信息茧房效应。

Result: 理论分析和实证结果表明，SAGE能够有效地促进冷启动流量，并保持推荐系统的多样性，同时保留了GBPO的数值稳定性。

Conclusion: SAGE作为一种新的优化方法，不仅克服了现有技术对于额外词汇表的需求，还通过其独特的机制解决了冷启动挑战和推荐多样性下降的问题，为推荐系统提供了更加高效灵活的技术解决方案。

Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.

</details>


### [58] [HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing](https://arxiv.org/abs/2601.21459)
*Chengyu Du,Xintao Wang,Aili Chen,Weiyuan Li,Rui Xu,Junteng Liu,Zishan Huang,Rong Tian,Zijun Sun,Yuhao Li,Liheng Feng,Deming Ding,Pengyu Zhao,Yanghua Xiao*

Main category: cs.LG

TL;DR: 本文提出了一种名为HER的框架，用于认知层面的角色模拟。通过引入双层思维区分角色的第一人称思考与模型的第三人称思考，并利用增强推理的角色扮演数据及符合人类偏好的奖励模型进行训练，显著提高了Qwen3-32B在CoSER基准测试和Minimax角色扮演基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型（LLM）在模拟特定人物时能够很好地捕捉到角色的语气和知识，但在模拟其行为背后的内心想法方面仍面临挑战。此外，现有的努力主要存在两个不足：缺乏高质量的推理轨迹数据以及与人类偏好一致的可靠奖励信号。

Method: 提出了HER框架，该框架引入了双层思维机制来区分角色的第一人称思考与模型的第三人称思考；通过逆向工程策划带有增强推理的角色扮演数据集；构建与人类偏好对齐的原则和奖励模型；基于上述资源，采用监督学习和强化学习方法训练基于Qwen3-32B的模型。

Result: 实验结果表明，所提方法相比Qwen3-32B基线模型，在CoSER基准上实现了30.26%的改进，在Minimax角色扮演基准上获得了14.97%的增益。

Conclusion: 本研究开发了一个统一的框架HER，旨在改善LLM在角色扮演中的认知模拟能力。通过创建新的数据集、原则和奖励模型，该方法不仅有效提升了模型性能，还为未来的研究提供了宝贵的资源。

Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

</details>


### [59] [L$^3$: Large Lookup Layers](https://arxiv.org/abs/2601.21461)
*Albert Tseng,Christopher De Sa*

Main category: cs.LG

TL;DR: 本文提出了一种新的稀疏性方法——大型查找层（L$^3$），通过静态基于令牌的路由以依赖于上下文的方式聚合每个令牌的一组学习嵌入，从而在内存和计算之间高效平衡。L$^3$不仅提供了一个对系统友好的架构，还引入了信息论嵌入分配算法来平衡速度和质量，并且在语言建模和下游任务中表现优于密集模型和等稀疏度的MoE模型。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏语言模型主要依靠Mixture-of-Experts (MoE) 层实现动态令牌路由到密集MLP'专家'上，但这种方法存在硬件效率低以及需要辅助损失才能稳定训练等问题。相比之下，本研究旨在通过引入一种新形式的稀疏性解决这些问题，即通过开发一个既可利用稀疏性优势又避免了MoE层缺点的新组件。

Method: 本文介绍了一种称为Large Lookup Layer (L$^3$) 的新型层，它通过将嵌入表泛化至模型解码器层来开启一个新的稀疏轴。L$^3$层使用静态基于令牌的路由技术，在考虑上下文的情况下为每个令牌聚合一组学习过的嵌入。此外，L$^3$包含两个关键组成部分：一个是便于快速训练并支持CPU卸载推理而无额外开销的系统友好型架构；另一个是有效平衡速度与质量的信息论嵌入分配算法。

Result: 实验结果表明，装备有L$^3$层的变压器模型最多能够激活2.6B参数，并且在语言建模及下游任务中的表现显著优于同等规模的密集模型和具有相同稀疏度的MoE模型。

Conclusion: Large Lookup Layer (L$^3$) 提供了一种新颖的方法来增强语言模型的稀疏性，同时解决了传统MoE层存在的问题。通过结合高效的系统设计和优化的嵌入分配策略，L$^3$能够在不牺牲性能的前提下提高模型效率。

Abstract: Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.

</details>


### [60] [Partial Feedback Online Learning](https://arxiv.org/abs/2601.21462)
*Shihao Shao,Cong Fang,Zhouchen Lin,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文研究了部分反馈在线学习，其中每个实例都有多个正确标签集，但学习者每轮只能观察到一个正确标签。文章对确定性和随机学习者的最小最大遗憾进行了近乎完整的描述，并引入了部分反馈Littlestone维度（PFLdim）和部分反馈测度破碎维度（PMSdim），以精确控制可学习性和最小最大遗憾。此外，还讨论了在集合可实现性之外问题可能变得信息论上难以处理的情况，指出需要新的噪声敏感复杂度量来表征超越集合可实现性的可学习性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索一种更贴近实际应用情况的学习模型——部分反馈在线学习，在这种模型中，尽管存在多个可能正确的答案，但学习者每次仅能获得其中一个作为反馈。这种设置适用于语言生成等领域，其中可能存在多种合理的响应，但数据只提供单一参考。通过研究该模型下的最优学习策略及其限制条件，试图填补现有理论框架与实际应用场景之间的差距。

Method: 作者首先定义了部分反馈Littlestone维度（PFLdim）来衡量确定性学习者的性能，并展示了它如何准确地决定可学习性及最小最大遗憾；接着提出部分反馈测度破碎维度（PMSdim）用于分析随机学习者的边界。此外，还探讨了确保确定性和随机学习者之间不可分离性的广泛条件，并将这些发现扩展到了集合值在线学习领域。

Result: 研究表明，在集合可实现的条件下，可以通过新提出的PFLdim和PMSdim有效地刻画不同类型的在线学习算法的性能极限。同时，也揭示了当超出集合可实现性时，即面对更加现实或不确定的场景时，当前的方法可能会遭遇线性级别的遗憾，这表明需要开发出能够应对更多噪声的新复杂度指标。

Conclusion: 本研究为理解部分反馈在线学习提供了重要的理论基础，不仅提出了新的度量标准来评估这类问题中的学习算法表现，而且还强调了在处理非理想化设定下进一步研究的需求。

Abstract: We study partial-feedback online learning, where each instance admits a set of correct labels, but the learner only observes one correct label per round; any prediction within the correct set is counted as correct. This model captures settings such as language generation, where multiple responses may be valid but data provide only a single reference. We give a near-complete characterization of minimax regret for both deterministic and randomized learners in the set-realizable regime, i.e., in the regime where sublinear regret is generally attainable. For deterministic learners, we introduce the Partial-Feedback Littlestone dimension (PFLdim) and show it precisely governs learnability and minimax regret; technically, PFLdim cannot be defined via the standard version space, requiring a new collection version space viewpoint and an auxiliary dimension used only in the proof. We further develop the Partial-Feedback Measure Shattering dimension (PMSdim) to obtain tight bounds for randomized learners. We identify broad conditions ensuring inseparability between deterministic and randomized learnability (e.g., finite Helly number or nested-inclusion label structure), and extend the argument to set-valued online learning, resolving an open question of Raman et al. [2024b]. Finally, we show a sharp separation from weaker realistic and agnostic variants: outside set realizability, the problem can become information-theoretically intractable, with linear regret possible even for $|H|=2$. This highlights the need for fundamentally new, noise-sensitive complexity measures to meaningfully characterize learnability beyond set realizability.

</details>


### [61] [A block-coordinate descent framework for non-convex composite optimization. Application to sparse precision matrix estimation](https://arxiv.org/abs/2601.21467)
*Guillaume Lauga*

Main category: cs.LG

TL;DR: 本文提出了一种新的块坐标下降(BCD)框架，用于解决非凸复合优化问题，确保目标函数的减少和收敛到解。该框架足够通用，可以包含变量度量近端梯度更新、近端牛顿更新以及交替最小化更新，并且在稀疏精度矩阵估计问题中展现了其价值，提供了收敛性保证，并大幅减少了达到最先进估计质量所需的迭代次数。


<details>
  <summary>Details</summary>
Motivation: 尽管块坐标下降（BCD）是解决众多大规模优化问题的首选方法，但其在非凸优化理论研究方面得到的关注较少。为了解决这一不足，本文提出了一个新的框架来处理非凸复合优化问题。

Method: 文章介绍了一个新提出的块坐标下降（BCD）框架，它能够处理非凸复合优化问题，保证了目标函数的降低并能收敛至一个解决方案。此框架具有足够的灵活性，可以容纳变量度量近端梯度更新、近端牛顿更新及交替最小化更新等多种方法。

Result: 通过本框架的应用，在非凸稀疏精度矩阵估计问题上获得了显著的效果，不仅提供了收敛性的保障，还实现了高达100倍的迭代次数减少，以达到业界领先的估计质量。

Conclusion: 所提出的新BCD框架对于解决非凸复合优化问题非常有效，特别是当应用于如Graphical Lasso这样的实际问题时，能够极大地提高效率同时保证结果的质量。

Abstract: Block-coordinate descent (BCD) is the method of choice to solve numerous large scale optimization problems, however their theoretical study for non-convex optimization, has received less attention. In this paper, we present a new block-coordinate descent (BCD) framework to tackle non-convex composite optimization problems, ensuring decrease of the objective function and convergence to a solution. This framework is general enough to include variable metric proximal gradient updates, proximal Newton updates, and alternated minimization updates. This generality allows to encompass three versions of the most used solvers in the sparse precision matrix estimation problem, deemed Graphical Lasso: graphical ISTA, Primal GLasso, and QUIC. We demonstrate the value of this new framework on non-convex sparse precision matrix estimation problems, providing convergence guarantees and up to a $100$-fold reduction in the number of iterations required to reach state-of-the-art estimation quality.

</details>


### [62] [PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization](https://arxiv.org/abs/2601.21470)
*Ruicheng Ao,Hongyu Chen,Haoyang Liu,David Simchi-Levi,Will Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一种结合PPI和SVRG的半监督随机优化方法PPI-SVRG，该方法在标签稀缺但可利用预训练模型预测的情况下有效。理论分析与实验结果表明，PPI-SVRG能够在标签稀缺条件下显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决当标记数据稀缺而预训练模型预测可用时的半监督随机优化问题。目标是通过结合两种减少方差的方法（PPI使用预测值，SVRG使用参考梯度）来提高优化效率与准确性。

Method: 提出了PPI-SVRG算法，该算法综合了PPI与SVRG的优点。通过数学证明两者本质上等价，并在此基础上开发出新方法。PPI-SVRG的收敛性界限分解为标准SVRG速率加上由于预测不确定性带来的误差底限。

Result: 理论分析显示，即使预测质量下降，PPI-SVRG也能保持稳定收敛，只是达到较大的邻域。实验验证，在标签稀缺情况下，PPI-SVRG能够显著降低均值估计基准上的MSE达43-52%，并在仅10%标注数据的情况下将MNIST测试准确率提高2.7-2.9个百分点。

Conclusion: PPI-SVRG作为一种新的半监督随机优化方法，在处理标签稀缺问题上表现优异，提供了一种有效利用预训练模型预测提升学习效果的途径。

Abstract: We study semi-supervised stochastic optimization when labeled data is scarce but predictions from pre-trained models are available. PPI and SVRG both reduce variance through control variates -- PPI uses predictions, SVRG uses reference gradients. We show they are mathematically equivalent and develop PPI-SVRG, which combines both. Our convergence bound decomposes into the standard SVRG rate plus an error floor from prediction uncertainty. The rate depends only on loss geometry; predictions affect only the neighborhood size. When predictions are perfect, we recover SVRG exactly. When predictions degrade, convergence remains stable but reaches a larger neighborhood. Experiments confirm the theory: PPI-SVRG reduces MSE by 43--52\% under label scarcity on mean estimation benchmarks and improves test accuracy by 2.7--2.9 percentage points on MNIST with only 10\% labeled data.

</details>


### [63] [ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment](https://arxiv.org/abs/2601.21484)
*Xiuyu Li,Jinkai Zhang,Mingyang Yi,Yu Li,Longqiang Wang,Yue Wang,Ju Fan*

Main category: cs.LG

TL;DR: 提出了一种无需训练的推理方法ETS，通过在线蒙特卡洛估计关键能量项，并利用现代加速框架和定制的重要性采样估计器来减少推理延迟而不影响采样质量。实验表明ETS在多个基准测试中持续提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）后训练对语言模型的有效性虽然被认可，但其复杂的训练过程导致成本高昂且不稳定。为了解决这个问题，研究者们寻求一种无需额外训练就能直接从最优RL策略中采样的方法。

Method: 本文提出的方法是Energy-Guided Test-Time Scaling (ETS)，它通过在线蒙特卡洛方法估计关键的能量项，并结合了参考策略模型与能量项用于Masked Language Modeling (MLM)中的转换概率计算。此外，为了提高实际效率，ETS还利用了现代化加速框架以及特定设计的重要性采样估计器，从而显著降低了推理延迟同时保证了采样质量。

Result: ETS在包括自回归模型和扩散语言模型在内的多种掩码语言建模任务上进行了测试，涵盖了推理、编码及科学等多个基准。结果一致显示，ETS能够有效提升生成文本的质量。

Conclusion: ETS作为一种新的无需训练的推理方法，在提高生成质量方面表现出色，适用于不同类型的掩码语言模型。该方法不仅减少了传统RL方法带来的成本问题，同时也保持了较高的采样效率和质量，为后续相关研究提供了新思路。

Abstract: Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.

</details>


### [64] [Task-Awareness Improves LLM Generations and Uncertainty](https://arxiv.org/abs/2601.21500)
*Tim Tomov,Dominik Fuchsgruber,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过在任务依赖的潜在结构中直接建模大语言模型（LLM）的输出，并配备一个不相似性度量来计算贝叶斯最优响应。这种方法不仅优于传统的解码方法，如束搜索，还能够通过诱导的贝叶斯风险量化不确定性，从而更好地与输出质量和正确性保持一致。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）解码和不确定性估计方法通常只在语言空间内操作，而忽略了结构信息的重要性。为了改善这一情况，文章旨在开发一种能够在考虑底层结构的情况下优化LLM输出的方法。

Method: 研究者们通过在特定于任务的潜在结构中直接对LLM输出进行建模，并在此结构上定义了一个不相似性度量标准。基于此，他们能够计算出贝叶斯最优响应，这些响应不是从采样生成中选择出来的，而是通过在潜在空间中结合个体响应新合成的。

Result: 实验结果表明，在不同的任务中，贝叶斯最优响应始终优于像束搜索这样的标准解码方法。此外，利用所提出的贝叶斯风险评估不确定性，能够更准确地反映根据潜在结构变化的输出质量与准确性。

Conclusion: 本研究提供了一个决策理论框架，适用于任何具有潜在响应结构的问题，并且使得基于LLM的预测更加可靠、与任务相关。

Abstract: In many applications of LLMs, natural language responses often have an underlying structure such as representing discrete labels, numerical values, or graphs. Yet, existing decoding and uncertainty estimation methods operate only in language space and largely disregard structural information. We address this by modeling LLM outputs directly in a task-dependent latent structure. By equipping this structure with a dissimilarity measure, we can compute Bayes-optimal responses. These are not selected from sampled generations but are newly synthesized by combining individual responses in the latent space. Across different tasks, Bayes-optimal responses consistently outperform standard decoding methods like beam search. Moreover, quantifying uncertainty via the induced Bayesian risk captures variations in terms of the latent structure and improves alignment with output quality and correctness. Our decision-theoretic framework is applicable to any problem that admits a latent response structure and enables reliable task-aware LLM predictions.

</details>


### [65] [Cascaded Transfer: Learning Many Tasks under Budget Constraints](https://arxiv.org/abs/2601.21513)
*Eloi Campagne,Yvenn Amara-Ouali,Yannig Goude,Mathilde Mougeot,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 本文提出了一种新的多任务迁移学习范式——级联迁移学习，通过最小生成树结构将任务连接起来，并根据合适的距离度量和可用训练预算进行分配。实验表明，该方法在大规模任务集合上比其他方法更准确且成本效益更高。


<details>
  <summary>Details</summary>
Motivation: 为了解决大量相关任务需要被学习的问题，特别是在任务之间的具体关系未知的情况下，提出了级联迁移学习的新模式。

Method: 设计了一种基于最小生成树结构的级联迁移机制，该机制按照适当的度量标准连接任务，并沿其分支分配可用的训练预算。级联组织成一个有根树，定义了任务的学习和细化顺序。

Result: 合成和真实多任务设置下的实验表明，所提出的方法能够比替代方案更准确地适应大规模任务集合，并且更具成本效益。

Conclusion: Cascaded Transfer Learning提供了一种有效的手段来处理许多任务的学习问题，在确保给定预算限制的同时促进了模型参数等信息在任务间的层次传递。

Abstract: Many-Task Learning refers to the setting where a large number of related tasks need to be learned, the exact relationships between tasks are not known. We introduce the Cascaded Transfer Learning, a novel many-task transfer learning paradigm where information (e.g. model parameters) cascades hierarchically through tasks that are learned by individual models of the same class, while respecting given budget constraints. The cascade is organized as a rooted tree that specifies the order in which tasks are learned and refined. We design a cascaded transfer mechanism deployed over a minimum spanning tree structure that connects the tasks according to a suitable distance measure, and allocates the available training budget along its branches. Experiments on synthetic and real many-task settings show that the resulting method enables more accurate and cost effective adaptation across large task collections compared to alternative approaches.

</details>


### [66] [A Unified SPD Token Transformer Framework for EEG Classification: Systematic Comparison of Geometric Embeddings](https://arxiv.org/abs/2601.21521)
*Chi-Sheng Chen,En-Jui Kuo,Guan-Ying Chen,Xinyu Zhang,Fan Zhang*

Main category: cs.LG

TL;DR: 该论文探讨了EEG信号的空间协方差矩阵作为对称正定（SPD）矩阵在黎曼流形上的嵌入几何与优化动力学之间的理论联系。研究结果表明：(1)BWSPD方法在高维输入时提供更好的梯度调节；(2)嵌入空间批量归一化近似于黎曼归一化，并且在多通道ERP数据上提高了准确性；(3)bi-Lipschitz边界证明了BWSPD保持了流形距离，其失真仅由条件数比决定。通过统一的Transformer框架对比三种嵌入方式，Log-Euclidean Transformer在所有数据集上实现了最先进的性能，而BWSPD则提供了具有竞争力的准确率和相似的训练时间。


<details>
  <summary>Details</summary>
Motivation: 探索EEG信号的SPD矩阵在黎曼流形上的嵌入几何与优化动力学间的理论联系，以提高基于这些矩阵的机器学习模型性能。

Method: 分析了不同嵌入选择对于梯度调节及数值稳定性的影响，提出了三个理论发现，并通过一个统一的Transformer框架在多个EEG范式上对比了BWSPD、Log-Euclidean和Euclidean嵌入的表现。

Result: 结果显示，在高维度输入下，BWSPD相比Log-Euclidean提供了更优的梯度调节；嵌入空间批量归一化技术在多通道ERP数据中显著提升了模型准确性；同时，研究表明BWSPD能够以较小的失真保持流形距离。Log-Euclidean Transformer在所有测试的数据集上表现最佳，而BWSPD也表现出色，尤其是在训练时间方面。

Conclusion: 这项工作揭示了SPD矩阵嵌入选择如何影响基于EEG信号的学习模型性能，为未来的研究提供了理论基础和技术指导。特别是Log-Euclidean和BWSPD两种方法展现出了在处理EEG相关任务时的优势。

Abstract: Spatial covariance matrices of EEG signals are Symmetric Positive Definite (SPD) and lie on a Riemannian manifold, yet the theoretical connection between embedding geometry and optimization dynamics remains unexplored. We provide a formal analysis linking embedding choice to gradient conditioning and numerical stability for SPD manifolds, establishing three theoretical results: (1) BWSPD's $\sqrtκ$ gradient conditioning (vs $κ$ for Log-Euclidean) via Daleckii-Kreĭn matrices provides better gradient conditioning on high-dimensional inputs ($d \geq 22$), with this advantage reducing on low-dimensional inputs ($d \leq 8$) where eigendecomposition overhead dominates; (2) Embedding-Space Batch Normalization (BN-Embed) approximates Riemannian normalization up to $O(\varepsilon^2)$ error, yielding $+26\%$ accuracy on 56-channel ERP data but negligible effect on 8-channel SSVEP data, matching the channel-count-dependent prediction; (3) bi-Lipschitz bounds prove BWSPD tokens preserve manifold distances with distortion governed solely by the condition ratio $κ$. We validate these predictions via a unified Transformer framework comparing BWSPD, Log-Euclidean, and Euclidean embeddings within identical architecture across 1,500+ runs on three EEG paradigms (motor imagery, ERP, SSVEP; 36 subjects). Our Log-Euclidean Transformer achieves state-of-the-art performance on all datasets, substantially outperforming classical Riemannian classifiers and recent SPD baselines, while BWSPD offers competitive accuracy with similar training time.

</details>


### [67] [More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)](https://arxiv.org/abs/2601.21522)
*Sagi Meir,Tommer D. Keidar,Noam Levi,Shlomi Reuveni,Barak Hirshberg*

Main category: cs.LG

TL;DR: 本文提出了一种新的查询方法——Reset-and-Discard (ReD)，旨在提高大语言模型在固定预算下完成任务的覆盖率（coverage@cost），并展示了这种方法如何减少达到所需覆盖所需的尝试次数、令牌数和成本。


<details>
  <summary>Details</summary>
Motivation: 传统的评估大型语言模型性能的方法是通过pass@k来衡量，但在固定预算的情况下，更合适的度量标准应该是coverage@cost，即在总尝试次数内平均能回答的独特问题数量。为了改善随着尝试次数增加而带来的边际效益递减的问题，提出了一个新的解决方案。

Method: 引入了Reset-and-Discard (ReD)作为大型语言模型的一种新查询方法，该方法能够在给定预算下提升coverage@cost，不论pass@k的形式如何。此外，如果已知pass@k，则可以定量预测使用ReD时总的尝试次数节省情况；当未知pass@k时，ReD还能推断出幂律指数。

Result: 实验结果表明，在三个大型语言模型上应用ReD后，显著减少了达到特定覆盖率所需要的尝试次数、消耗的令牌数量以及美元成本。同时，ReD还提供了一种有效测量推理幂律关系的方法。

Conclusion: Reset-and-Discard (ReD) 方法被证明能够有效地提高大型语言模型在处理验证性任务时的效率，不仅减少了成本开销，也提供了对模型能力的新理解方式。

Abstract: The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.

</details>


### [68] [Multi-Modal Time Series Prediction via Mixture of Modulated Experts](https://arxiv.org/abs/2601.21547)
*Lige Zhang,Ali Maatouk,Jialin Chen,Leandros Tassiulas,Rex Ying*

Main category: cs.LG

TL;DR: 提出了一种名为Expert Modulation的新方法，用于多模态时间序列预测，通过文本信号调节路由和专家计算，从而实现对专家行为的直接且高效的跨模态控制。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的时间序列表现出复杂且不断演变的动态特性，这使得准确预测极其困难。虽然最近的多模态预测方法利用了如新闻报道等文本信息来提高预测准确性，但大多数依赖于令牌级别的融合，这种做法在高质量的时间-文本配对稀缺以及时间序列在规模和特征上表现出显著变化时并不合适。

Method: 提出了Expert Modulation，这是一种新的多模态时间序列预测范式，它基于文本信号条件化路由和专家计算，允许对专家行为进行直接有效的跨模态控制。

Result: 通过全面的理论分析与实验表明，所提方法在多模态时间序列预测中取得了显著改进。

Conclusion: Expert Modulation为解决多模态时间序列预测中的挑战提供了一个新方向，并展示了其相对于现有方法的有效性。

Abstract: Real-world time series exhibit complex and evolving dynamics, making accurate forecasting extremely challenging. Recent multi-modal forecasting methods leverage textual information such as news reports to improve prediction, but most rely on token-level fusion that mixes temporal patches with language tokens in a shared embedding space. However, such fusion can be ill-suited when high-quality time-text pairs are scarce and when time series exhibit substantial variation in scale and characteristics, thus complicating cross-modal alignment. In parallel, Mixture-of-Experts (MoE) architectures have proven effective for both time series modeling and multi-modal learning, yet many existing MoE-based modality integration methods still depend on token-level fusion. To address this, we propose Expert Modulation, a new paradigm for multi-modal time series prediction that conditions both routing and expert computation on textual signals, enabling direct and efficient cross-modal control over expert behavior. Through comprehensive theoretical analysis and experiments, our proposed method demonstrates substantial improvements in multi-modal time series prediction. The current code is available at https://github.com/BruceZhangReve/MoME

</details>


### [69] [HistoPrism: Unlocking Functional Pathway Analysis from Pan-Cancer Histology via Gene Expression Prediction](https://arxiv.org/abs/2601.21560)
*Susu Hu,Qinghe Zeng,Nithya Bhasker,Jakob Nicolas Kather,Stefanie Speidel*

Main category: cs.LG

TL;DR: 本研究提出了一种新的基于transformer的架构HistoPrism，用于从组织学图像预测跨癌症类型的基因表达。通过引入路径级别的基准评估，HistoPrism不仅在高变基因上超越了现有模型，还在生物学功能路径层面取得了显著进步，展示了其恢复生物一致性转录组模式的能力。该模型具有强大的泛化能力和改进的效率，为常规可用组织学的临床相关转录组建模设定了新标准。


<details>
  <summary>Details</summary>
Motivation: 从H&E组织学预测空间基因表达为测序提供了一个可扩展且临床可及的替代方案，但要实现临床影响，需要开发能够跨癌症类型通用并捕捉生物一致性信号的模型。先前的工作通常局限于特定癌症环境和基于变异性的评估，而对功能性相关性的探索不足。

Method: 研究者们提出了HistoPrism，这是一种高效的基于transformer的架构，用于跨癌症类型从组织学中预测基因表达。为了评估生物学意义，他们引入了一个路径级别的基准测试，将评估从孤立的基因级别变异性转移到连贯的功能路径上。

Result: HistoPrism不仅在高变基因上超越了之前最先进的模型，更重要的是，在路径级别预测上也取得了显著的进步，显示了它恢复生物一致性转录组模式的能力。此外，HistoPrism还展现了强大的跨癌症类型泛化能力和提高了效率。

Conclusion: HistoPrism为从常规可用的组织学图像进行临床相关转录组建模设定了一个新的标准，特别是在泛化能力以及捕获生物相干性信号方面。

Abstract: Predicting spatial gene expression from H&E histology offers a scalable and clinically accessible alternative to sequencing, but realizing clinical impact requires models that generalize across cancer types and capture biologically coherent signals. Prior work is often limited to per-cancer settings and variance-based evaluation, leaving functional relevance underexplored. We introduce HistoPrism, an efficient transformer-based architecture for pan-cancer prediction of gene expression from histology. To evaluate biological meaning, we introduce a pathway-level benchmark, shifting assessment from isolated gene-level variance to coherent functional pathways. HistoPrism not only surpasses prior state-of-the-art models on highly variable genes , but also more importantly, achieves substantial gains on pathway-level prediction, demonstrating its ability to recover biologically coherent transcriptomic patterns. With strong pan-cancer generalization and improved efficiency, HistoPrism establishes a new standard for clinically relevant transcriptomic modeling from routinely available histology.

</details>


### [70] [Representation Unlearning: Forgetting through Information Compression](https://arxiv.org/abs/2601.21564)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 该论文提出了一种新的机器遗忘框架——表示遗忘(Representation Unlearning)，直接在模型的表示空间中执行数据影响移除，而不是修改模型参数。通过学习一个对表示施加信息瓶颈的转换来实现这一目标，旨在最大化与保留数据之间的互信息同时抑制需要遗忘的数据信息。此方法在多个基准测试中展示了比基于参数的方法更可靠的遗忘效果、更好的实用性保持以及更高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法规和鲁棒性考虑的需求增加，人们希望有一种能够从模型中移除特定训练数据影响的方法。现有方法通常通过修改模型参数来达到目的，但这样做可能会导致更新不稳定、计算成本高，并且受限于局部近似。

Method: 提出了“表示遗忘”框架，它直接在模型的表示空间而非参数上进行操作。通过学习一种表示变换，该变换作为一个信息瓶颈，旨在最大化与保留在模型中的数据间的互信息，同时减少对于应被遗忘数据的信息量。为了使这个目标变得可行，作者推导出了变分替代方案，并展示了如何在两种实际情况下应用：当保留和遗忘的数据都可用时，以及仅能访问遗忘数据的零样本设置下。

Result: 实验表明，在多个基准测试中，与以参数为中心的基线方法相比，表示遗忘实现了更可靠的遗忘效果、更好的实用性保留以及更高的计算效率。

Conclusion: 表示遗忘为解决机器遗忘问题提供了一个新颖且有效的途径，不仅提高了遗忘过程的稳定性和效率，还增强了模型处理后续任务的能力。

Abstract: Machine unlearning seeks to remove the influence of specific training data from a model, a need driven by privacy regulations and robustness concerns. Existing approaches typically modify model parameters, but such updates can be unstable, computationally costly, and limited by local approximations. We introduce Representation Unlearning, a framework that performs unlearning directly in the model's representation space. Instead of modifying model parameters, we learn a transformation over representations that imposes an information bottleneck: maximizing mutual information with retained data while suppressing information about data to be forgotten. We derive variational surrogates that make this objective tractable and show how they can be instantiated in two practical regimes: when both retain and forget data are available, and in a zero-shot setting where only forget data can be accessed. Experiments across several benchmarks demonstrate that Representation Unlearning achieves more reliable forgetting, better utility retention, and greater computational efficiency than parameter-centric baselines.

</details>


### [71] [FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions](https://arxiv.org/abs/2601.21567)
*Yutao Jin,Yuang Tao,Junyong Zhai*

Main category: cs.LG

TL;DR: 提出了一种新的因果解缠表示学习框架FlexCausal，该框架基于块对角协方差VAE，并采用基于流的因子化先验来模拟外生噪声的复杂密度。通过结合监督对齐目标和反事实一致性约束，确保了学习到的潜在子空间与真实因果关系之间精确的结构对应。此外，引入了一种流形感知相对干预策略以保证高质量生成。实验结果表明FlexCausal在合成数据集和真实数据集上均优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有的解缠方法依赖于标准平均场近似（具有对角后验协方差），这会使得所有潜在维度去相关；同时，这些方法通常假设外生噪声为各向同性的高斯先验，无法捕捉现实中因果因素所具有的复杂、非高斯统计特性。

Method: 提出了名为FlexCausal的新CDRL框架，该框架基于块对角协方差变分自编码器(VAE)，并利用基于流的因子化先验来实际地建模外生噪声的复杂密度。通过整合监督对齐目标与反事实一致性限制条件，确保学到的潜变量子空间与真实因果关系间存在精准的结构性对应。还介绍了一种流形意识的相对干预策略来保障高保真度生成。

Result: 实验结果显示，在合成数据集以及真实世界数据集中，FlexCausal的表现显著优于其他现有方法。

Conclusion: FlexCausal提供了一种有效的方法来学习和解缠观测中的低维表示及其底层因果结构，解决了现有技术在处理复杂、非高斯统计属性时遇到的问题。

Abstract: Causal Disentangled Representation Learning(CDRL) aims to learn and disentangle low dimensional representations and their underlying causal structure from observations. However, existing disentanglement methods rely on a standard mean-field approximation with a diagonal posterior covariance, which decorrelates all latent dimensions. Additionally, these methods often assume isotropic Gaussian priors for exogenous noise, failing to capture the complex, non-Gaussian statistical properties prevalent in real-world causal factors. Therefore, we propose FlexCausal, a novel CDRL framework based on a block-diagonal covariance VAE. FlexCausal utilizes a Factorized Flow-based Prior to realistically model the complex densities of exogenous noise, effectively decoupling the learning of causal mechanisms from distributional statistics. By integrating supervised alignment objectives with counterfactual consistency constraints, our framework ensures a precise structural correspondence between the learned latent subspaces and the ground-truth causal relations. Finally, we introduce a manifold-aware relative intervention strategy to ensure high-fidelity generation. Experimental results on both synthetic and real-world datasets demonstrate that FlexCausal significantly outperforms other methods.

</details>


### [72] [Bridging Functional and Representational Similarity via Usable Information](https://arxiv.org/abs/2601.21568)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 提出了一种基于可用信息来量化表示相似性的统一框架，涵盖功能相似性、表示相似性三个关键维度，并证明了表示相似性是功能相似性的充分但不必要条件。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过可用信息的概念提供一个理论和实证的综合框架，以更严谨地衡量不同表示之间的相似度。

Method: 通过建立缝合性能与条件互信息之间的正式联系，分析了功能性相似；证明了基于重构的度量标准在特定条件下作为可用信息估计器的作用；展示了表示相似性对于功能性相似性而言是充分而非必要的，并通过任务细粒度层次结构统一了这些概念。

Result: 揭示了缝合本质上是不对称的，强调了双向分析的重要性；表明了表征相似性依赖于预测家族的能力；确立了复杂任务上的相似性确保了任何较粗略衍生任务上的相似性。

Conclusion: 该研究为理解表示学习中的相似性提供了新的视角，指出了评估表示间关系时需要考虑的关键因素。

Abstract: We present a unified framework for quantifying the similarity between representations through the lens of \textit{usable information}, offering a rigorous theoretical and empirical synthesis across three key dimensions. First, addressing functional similarity, we establish a formal link between stitching performance and conditional mutual information. We further reveal that stitching is inherently asymmetric, demonstrating that robust functional comparison necessitates a bidirectional analysis rather than a unidirectional mapping. Second, concerning representational similarity, we prove that reconstruction-based metrics and standard tools (e.g., CKA, RSA) act as estimators of usable information under specific constraints. Crucially, we show that similarity is relative to the capacity of the predictive family: representations that appear distinct to a rigid observer may be identical to a more expressive one. Third, we demonstrate that representational similarity is sufficient but not necessary for functional similarity. We unify these concepts through a task-granularity hierarchy: similarity on a complex task guarantees similarity on any coarser derivative, establishing representational similarity as the limit of maximum granularity: input reconstruction.

</details>


### [73] [Shaping capabilities with token-level data filtering](https://arxiv.org/abs/2601.21571)
*Neil Rathi,Alec Radford*

Main category: cs.LG

TL;DR: 该论文提出了一种在预训练过程中通过过滤预训练数据来减少语言模型中不期望能力的方法，特别是针对移除医疗相关能力的任务。研究发现，相比于文档级过滤，词元级过滤更有效且成本更低，并且随着模型规模的增加，这种过滤方法的效果会更好。此外，还介绍了一种使用稀疏自动编码器为词元打标签并提炼高质量分类器的方法论。


<details>
  <summary>Details</summary>
Motivation: 现有减少语言模型中不期望能力的方法多为事后处理，容易被绕过。因此，探索一种在预训练阶段就控制这些能力的有效方法变得尤为重要。

Method: 采用过滤预训练数据的方式，在预训练过程中移除非目标领域（如医疗）的知识。进一步地，对比了文档级与词元级两种过滤策略的效果，并提出了基于稀疏自动编码器对词元进行标记及提炼高效分类器的新方法。

Result: 实验表明，词元级过滤比文档级过滤更有效率；随着模型大小的增长，过滤技术对于限制特定领域知识更加有效；即使应用了词元过滤，模型仍能在非目标领域保持良好表现。

Conclusion: 通过对预训练数据实施精细过滤，可以在不影响语言模型整体性能的前提下显著降低其在特定领域的专业知识水平。此方法不仅效果显著，而且具有良好的鲁棒性和经济性。

Abstract: Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.

</details>


### [74] [Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity](https://arxiv.org/abs/2601.21577)
*Mutian Yang,Zisen Zhan,Yutong Chen,Haolin Li,Kaiwen Wang,Kaili Zheng,Yuguang Wang,Qi Wang,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 研究提出了一种基于梯度理论框架来解释大语言模型（LLMs）在持续学习过程中出现的灾难性遗忘现象，并据此开发了协作神经学习（CNL）方法，通过冻结冲突神经元仅更新协作神经元的方式，在不同设置下显著减少了遗忘。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在知识注入过程中遭遇的灾难性遗忘问题，当前解决方案缺乏坚实的理论基础。

Method: 建立了一个基于梯度的理论框架，证明了强负梯度相似性是遗忘的根本原因；利用梯度相似性区分出引发遗忘的冲突神经元和减少遗忘的协作神经元；提出了协作神经学习（CNL）方法，该方法通过冻结冲突神经元、只更新协作神经元来理论上消除灾难性遗忘。

Result: 在五个大语言模型、四个数据集以及四种优化器上的实验表明，CNL 方法在已掌握集合内实现了零遗忘，在未掌握集合外减少了59.1%-81.7%的遗忘。

Conclusion: 本研究表明，通过特定地选择并更新有助于保持记忆的神经元，可以有效减轻甚至避免大型语言模型中的灾难性遗忘问题。

Abstract: Catastrophic forgetting during knowledge injection severely undermines the continual learning capability of large language models (LLMs). Although existing methods attempt to mitigate this issue, they often lack a foundational theoretical explanation. We establish a gradient-based theoretical framework to explain catastrophic forgetting. We first prove that strongly negative gradient similarity is a fundamental cause of forgetting. We then use gradient similarity to identify two types of neurons: conflicting neurons that induce forgetting and account for 50%-75% of neurons, and collaborative neurons that mitigate forgetting and account for 25%-50%. Based on this analysis, we propose a knowledge injection method, Collaborative Neural Learning (CNL). By freezing conflicting neurons and updating only collaborative neurons, CNL theoretically eliminates catastrophic forgetting under an infinitesimal learning rate eta and an exactly known mastered set. Experiments on five LLMs, four datasets, and four optimizers show that CNL achieves zero forgetting in in-set settings and reduces forgetting by 59.1%-81.7% in out-of-set settings.

</details>


### [75] [Evaluating Prediction Uncertainty Estimates from BatchEnsemble](https://arxiv.org/abs/2601.21581)
*Morten Blørstad,Herman Jangsett Mostein,Nello Blaser,Pekka Parviainen*

Main category: cs.LG

TL;DR: 本文研究了BatchEnsemble作为一种通用且可扩展的方法来估计表格和时间序列任务中的不确定性，并引入了一种新的BatchEnsemble GRU单元——GRUBE。实验结果表明，BatchEnsemble在不确定性估计上与深度集成模型性能相当，优于蒙特卡洛dropout方法，同时参数更少，训练和推理时间也更短。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在不确定性估计方面存在困难，许多方法要么计算上不可行，要么低估了不确定性。因此，需要一种既能在不同任务中通用又可扩展的不确定性估计方法。

Method: 提出了BatchEnsemble作为跨表格和时间序列任务的不确定性估计方法；针对序列建模，引入了一种新型的BatchEnsemble GRU单元（GRUBE）；将BatchEnsemble与蒙特卡洛dropout及深度集成模型进行了比较。

Result: BatchEnsemble在不确定性估计上的表现与深度集成模型相匹配，明显优于蒙特卡洛dropout；GRUBE在预测和不确定性估计方面达到了相似或更好的性能；BatchEnsemble和GRUBE相较于传统集成方法使用更少的参数，减少了训练和推理的时间。

Conclusion: BatchEnsemble和GRUBE为不确定性估计提供了有效解决方案，它们在保持高性能的同时降低了计算成本。

Abstract: Deep learning models struggle with uncertainty estimation. Many approaches are either computationally infeasible or underestimate uncertainty. We investigate \textit{BatchEnsemble} as a general and scalable method for uncertainty estimation across both tabular and time series tasks. To extend BatchEnsemble to sequential modeling, we introduce GRUBE, a novel BatchEnsemble GRU cell. We compare the BatchEnsemble to Monte Carlo dropout and deep ensemble models. Our results show that BatchEnsemble matches the uncertainty estimation performance of deep ensembles, and clearly outperforms Monte Carlo dropout. GRUBE achieves similar or better performance in both prediction and uncertainty estimation. These findings show that BatchEnsemble and GRUBE achieve similar performance with fewer parameters and reduced training and inference time compared to traditional ensembles.

</details>


### [76] [CORDS: Continuous Representations of Discrete Structures](https://arxiv.org/abs/2601.21583)
*Tin Hadži Veljković,Erik Bekkers,Michael Tiemann,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 提出了一种名为CORDS的新策略，通过将不同大小集合的预测转换为连续推理问题来解决对象数量事先未知的学习问题。该方法能够处理未知集合大小，并在多个任务中表现出具有竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 许多学习问题需要在不知道对象数量的情况下预测对象集，现有方法往往依赖于填充表示或必须明确推断集合大小，这带来了挑战。

Method: CORDS（离散结构的连续表示）提供了一个可逆映射，将空间对象集转换为两个连续场：一个编码对象位置和计数的密度场，以及一个在其相同支持上携带属性的特征场。模型完全在场空间中操作，同时可以精确解码回离散集。

Result: CORDS被评估应用于分子生成与回归、对象检测、基于仿真的推理以及涉及局部最大值恢复的数学任务，证明了其对于未知集合大小的鲁棒处理能力，并保持了竞争性的准确性。

Conclusion: CORDS方法成功地解决了变大小集合预测的问题，为那些需要处理事先未知数量对象的任务提供了有效解决方案。

Abstract: Many learning problems require predicting sets of objects when the number of objects is not known beforehand. Examples include object detection, molecular modeling, and scientific inference tasks such as astrophysical source detection. Existing methods often rely on padded representations or must explicitly infer the set size, which often poses challenges. We present a novel strategy for addressing this challenge by casting prediction of variable-sized sets as a continuous inference problem. Our approach, CORDS (Continuous Representations of Discrete Structures), provides an invertible mapping that transforms a set of spatial objects into continuous fields: a density field that encodes object locations and count, and a feature field that carries their attributes over the same support. Because the mapping is invertible, models operate entirely in field space while remaining exactly decodable to discrete sets. We evaluate CORDS across molecular generation and regression, object detection, simulation-based inference, and a mathematical task involving recovery of local maxima, demonstrating robust handling of unknown set sizes with competitive accuracy.

</details>


### [77] [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 本文提出了一种无需迭代MCMC的理论基础方法，通过自回归地锐化基础模型的生成分布来提高大型语言模型（LLMs）的推理性能。实验证明，该方法在数学、问答和代码任务上与一次性GRPO相匹配或超越，并且与基于MCMC的采样相比，推理延迟减少了10倍以上。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）后期训练是提高大型语言模型（LLMs）推理性能的主要方法，但越来越多的证据表明，其收益主要来自于分布锐化而不是新能力的获得。虽然使用马尔可夫链蒙特卡洛（MCMC）从LLMs的能力分布中采样可以不依赖外部奖励就恢复到与RL后期训练相当的表现，但MCMC的高计算成本使其难以广泛采用。因此，需要开发一种新的方法，在不依赖于外部奖励的情况下提升LLMs的表现，同时减少计算成本。

Method: 作者推导出一个新的公式，表明全局能力分布可以通过一个token级别的低温缩放分布来近似，其中缩放因子捕捉了未来轨迹的质量。利用这一见解，他们引入了一种无需训练和验证器的算法，该算法能够自回归地锐化基础模型的生成分布。

Result: 通过对四个LLMs上的数学、问答和代码任务进行评估，证明了该方法在没有依赖任何外部奖励的情况下，与一次性GRPO相匹配或超越，同时相比于基于MCMC的采样，推理延迟减少了超过10倍。

Conclusion: 这项工作展示了一种创新的方法，可以在不增加计算负担的情况下有效提高LLMs的推理表现，为未来的研究提供了一个有前景的方向。

Abstract: Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.

</details>


### [78] [Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking](https://arxiv.org/abs/2601.21619)
*Yiming Wang,Zhuosheng Zhang,Rui Wang*

Main category: cs.LG

TL;DR: 本文探讨了并行思考在大规模语言模型推理中的应用，并提出了T2方法来解决过扩展问题，该方法通过利用潜在表示来估计每个样本的最佳并行度，从而在保持性能的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 由于样本异质性，使用全局并行度N处理所有样本会导致某些样本的预算冗余，形成系统级效率与样本级效率之间的不兼容性，即过扩展问题。

Method: 提出了一种轻量级的方法T2，该方法能够利用潜在表示预测解码前每个样本所需的最佳并行级别。

Result: 实验表明，T2方法能够在保持相当性能的前提下大幅度减少成本，实现了更高效的并行思考。

Conclusion: T2方法有效解决了LLM推理过程中遇到的过扩展诅咒问题，提高了并行思考的效率。

Abstract: Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.

</details>


### [79] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: 本文提出了一种自适应策略，通过选择性地提高部分计算精度来优化Transformer推理过程中的混合精度计算问题，从而在不大幅增加计算成本的情况下显著提升了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI的发展，尤其是大型语言模型的进步，如何实现高效且可本地部署的解决方案变得尤为重要。本文旨在解决这一挑战，特别是针对Transformer推理过程中由于使用混合精度计算而产生的舍入误差问题。

Method: 基于对复合函数$f(g(\mathrm{x}))$的舍入误差分析，本文提出了一种自适应策略，该策略能够识别并选择性地提高$g(\mathrm{x})$中一小部分组件的计算精度，同时允许其他计算以较低精度执行。此外，文章还探讨了如何将此策略应用于Transformer的不同组成部分，并展示了其对整个Transformer推理过程的影响。

Result: 通过在GPT-2模型上进行数值研究，证明了即使是非常低的重新计算率也能使准确性提高高达两个数量级。

Conclusion: 研究表明，所提出的自适应策略能够在保持较高效率的同时，显著改善使用混合精度进行计算的Transformer模型的准确性。

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [80] [Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation](https://arxiv.org/abs/2601.21636)
*Jan Schuchardt,Nikita Kalinin*

Main category: cs.LG

TL;DR: 本文研究了基于随机分配的差分隐私模型训练中的隐私放大问题，提出了基于Rényi散度和条件组合的无需采样的边界方法，相比现有的基于采样的Monte Carlo方法，该方法在小ε值下提供了更强的隐私保证，并且适用于任意带状和非带状矩阵。


<details>
  <summary>Details</summary>
Motivation: 针对Choquette-Choo等人提出的基于采样的Monte Carlo方法在计算隐私放大参数时存在的局限性，如高概率保证或机制随机弃权的要求，以及确保(ε,δ)-DP所需样本数与δ成反比的问题，本文旨在开发一种不依赖于采样的方法来解决这些问题。

Method: 通过引入Rényi散度和条件组合的概念，结合动态规划公式有效计算边界，提出了一种新的框架用于处理基于随机分配（球入箱模型）下的差分隐私模型训练隐私放大问题。

Result: 所提方法不仅能够提供更强大的隐私保护，尤其是在ε取较小值的情况下，而且对于广泛使用的矩阵机制均展现出有效性。

Conclusion: 本研究为差分隐私模型训练中的隐私放大问题提供了新的解决方案，通过采用Rényi散度和条件组合的方法克服了现有技术的一些限制，增强了隐私保护力度。

Abstract: We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.

</details>


### [81] [Generative Design of Ship Propellers using Conditional Flow Matching](https://arxiv.org/abs/2601.21637)
*Patrick Kruger,Rafael Diaz,Simon Hauschulz,Stefan Harries,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 本文探讨了生成式人工智能（GenAI）在船用螺旋桨设计中的应用，通过条件流匹配方法建立设计参数与基于性能标签调节的模拟噪声之间的双向映射，实现了从相同性能目标生成多个有效设计方案的目标。研究还讨论了模型准确性与可用数据量之间的权衡，并提出了一种使用伪标签的数据增强方法来提高模型的整体表现。最终展示了具有几乎相同性能特性的不同螺旋桨几何形状示例，证明了GenAI在工程设计中的灵活性和潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习模型根据给定的设计参数预测机械组件的性能，而本文旨在探索生成式人工智能（GenAI）如何生成达到特定性能目标的设计方案。特别是针对船舶螺旋桨设计领域，尝试利用GenAI技术创造满足相同性能指标的多种设计方案。

Method: 采用条件流匹配方法，在设计参数和基于性能标签调节的模拟噪声之间建立双向映射；使用涡格法进行数值模拟以支持模型训练；分析模型精度与可用数据量之间的关系；提出利用前向替代模型产生的伪标签进行数据扩充，以期改善整体模型性能。

Result: 研究表明，通过调整噪声向量可以为同一组性能目标生成多个有效的螺旋桨设计。此外，即使是在数据较少的情况下，通过引入伪标签进行数据增强也能显著提升模型的表现。最后，实验结果表明，确实存在几何结构各异但性能特征极其相似的不同螺旋桨设计实例，展示了GenAI在促进创新设计方面的巨大潜力。

Conclusion: 本研究证实了生成式人工智能技术在船用螺旋桨设计中生成多样化且符合特定性能要求设计方案的能力。这不仅突出了GenAI于工程设计领域的潜在价值，也开启了未来进一步探索更广泛应用场景的可能性。

Abstract: In this paper, we explore the use of generative artificial intelligence (GenAI) for ship propeller design. While traditional forward machine learning models predict the performance of mechanical components based on given design parameters, GenAI models aim to generate designs that achieve specified performance targets. In particular, we employ conditional flow matching to establish a bidirectional mapping between design parameters and simulated noise that is conditioned on performance labels. This approach enables the generation of multiple valid designs corresponding to the same performance targets by sampling over the noise vector.
  To support model training, we generate data using a vortex lattice method for numerical simulation and analyze the trade-off between model accuracy and the amount of available data. We further propose data augmentation using pseudo-labels derived from less data-intensive forward surrogate models, which can often improve overall model performance. Finally, we present examples of distinct propeller geometries that exhibit nearly identical performance characteristics, illustrating the versatility and potential of GenAI in engineering design.

</details>


### [82] [Seg-MoE: Multi-Resolution Segment-wise Mixture-of-Experts for Time Series Forecasting Transformers](https://arxiv.org/abs/2601.21641)
*Evandro S. Ortigossa,Eran Segal*

Main category: cs.LG

TL;DR: 本文提出了一种新的稀疏Mixture-of-Experts（MoE）设计，称为Seg-MoE，它通过处理连续的时间步长段而不是单独的令牌决策来改进时间序列预测。该方法在多个多变量长期预测基准上取得了最先进的预测准确性，并且消融研究证实了段级路由是推动这些改进的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer模型虽然在时间序列预测方面取得了显著进展，但在捕捉长期时间动态的同时有效扩展仍然存在困难。而现有针对时间序列预测的Mixture-of-Experts (MoE) 方法依赖于令牌级路由机制，这可能无法充分利用时间数据固有的局部性和连续性。

Method: 提出了Seg-MoE，这是一种将连续时间段而非单个标记作为单元进行路由和处理的稀疏MoE设计方案。每个专家可以直接建模段内的交互作用，自然地与内在的时间模式对齐。Seg-MoE层被集成到了一个时间序列Transformer中，并在多个多变量长期预测基准上进行了评估。

Result: Seg-MoE几乎在所有预测范围内都达到了最先进的时间序列预测准确性，优于密集型Transformers和先前的令牌级MoE模型。全面的消融研究表明，段级路由是推动这些增益的关键因素。

Conclusion: 通过调整MoE路由粒度以匹配时间序列的内在结构提供了一种强大的归纳偏置，为条件稀疏架构在序列数据建模中开辟了新途径。

Abstract: Transformer-based models have recently made significant advances in accurate time-series forecasting, but even these architectures struggle to scale efficiently while capturing long-term temporal dynamics. Mixture-of-Experts (MoE) layers are a proven solution to scaling problems in natural language processing. However, existing MoE approaches for time-series forecasting rely on token-wise routing mechanisms, which may fail to exploit the natural locality and continuity of temporal data. In this work, we introduce Seg-MoE, a sparse MoE design that routes and processes contiguous time-step segments rather than making independent expert decisions. Token segments allow each expert to model intra-segment interactions directly, naturally aligning with inherent temporal patterns. We integrate Seg-MoE layers into a time-series Transformer and evaluate it on multiple multivariate long-term forecasting benchmarks. Seg-MoE consistently achieves state-of-the-art forecasting accuracy across almost all prediction horizons, outperforming both dense Transformers and prior token-wise MoE models. Comprehensive ablation studies confirm that segment-level routing is the key factor driving these gains. Our results show that aligning the MoE routing granularity with the inherent structure of time series provides a powerful, yet previously underexplored, inductive bias, opening new avenues for conditionally sparse architectures in sequential data modeling.

</details>


### [83] [SENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework](https://arxiv.org/abs/2601.21664)
*Xingyue Zhang,Yuxuan Bao,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: SENDAI框架通过结合模拟导出的先验与学习到的差异校正，从极稀疏的传感器观测中重建完整的空间状态。在六个全球分布的站点上对MODIS植被指数场进行重建时表现出色，特别是在处理具有尖锐边界和亚季节动态的地景时，能够有效保留诊断相关的结构。该方法比传统基线及最近的高频方法有显著改进，适用于物理基础推断、资源受限部署和实时监控控制。


<details>
  <summary>Details</summary>
Motivation: 解决数据丰富训练环境与观测稀少实际部署条件之间的差距，特别是在目标域存在分布偏移、异构结构和多尺度动态的情况下，这些特征在可用训练数据中是缺失的。

Method: 提出了SENDAI，一个分层稀疏测量、高效数据同化框架，它通过结合模拟导出的先验知识与学习到的差异校正，从超稀疏传感器观测中重建全空间状态。

Result: SENDAI在六个全球分布站点上的MODIS植被指数场重建中表现优异，相较于传统基线最大SSIM提高了185%，相比近期基于高频率的方法也有36%的提升。对于具有尖锐边界和亚季节动态的地景，其性能尤为突出，并且能够有效地保持诊断相关结构。

Conclusion: SENDAI提供了一个轻量级且操作可行的稀疏测量重建框架，适合用于物理基础推断、资源限制下的部署以及实时监测和控制。

Abstract: Bridging the gap between data-rich training regimes and observation-sparse deployment conditions remains a central challenge in spatiotemporal field reconstruction, particularly when target domains exhibit distributional shifts, heterogeneous structure, and multi-scale dynamics absent from available training data. We present SENDAI, a hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework that reconstructs full spatial states from hyper sparse sensor observations by combining simulation-derived priors with learned discrepancy corrections. We demonstrate the performance on satellite remote sensing, reconstructing MODIS (Moderate Resolution Imaging Spectroradiometer) derived vegetation index fields across six globally distributed sites. Using seasonal periods as a proxy for domain shift, the framework consistently outperforms established baselines that require substantially denser observations -- SENDAI achieves a maximum SSIM improvement of 185% over traditional baselines and a 36% improvement over recent high-frequency-based methods. These gains are particularly pronounced for landscapes with sharp boundaries and sub-seasonal dynamics; more importantly, the framework effectively preserves diagnostically relevant structures -- such as field topologies, land cover discontinuities, and spatial gradients. By yielding corrections that are more structurally and spectrally separable, the reconstructed fields are better suited for downstream inference of indirectly observed variables. The results therefore highlight a lightweight and operationally viable framework for sparse-measurement reconstruction that is applicable to physically grounded inference, resource-limited deployment, and real-time monitor and control.

</details>


### [84] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文指出，在强化学习中，策略往往倾向于收敛到少数几个结果上，而这种现象实际上是期望回报最大化目标的结构性后果。为此，作者提出了一种名为逆概率缩放的新方法来修正这一问题，并通过Group Relative Policy Optimization (GRPO) 的修改版本IPS-GRPO证明了其有效性。实验表明，IPS-GRPO在减少结果层面模式崩溃的同时还能保持或超越基线性能。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，即使在存在多个高质量解决方案的情况下，标准强化学习算法训练出的策略也常常只集中在少数几种结果上。传统观点认为这是由于探索不足或正则化力度不够导致的。然而，本文揭示了这种情况实际上是由期望回报最大化目标本身结构上的缺陷所引起的。

Method: 文章首先从理论上分析了为何基于预期回报最大化的学习会导致结果层面的模式崩溃，并提出了一个称为逆概率缩放（inverse probability scaling, IPS）的方法来解决这个问题。该方法通过对学习信号中的概率乘数进行调整，从而改变学习动态，确保终端分布与奖励成比例。此外，研究人员还开发了一个名为IPS-GRPO的具体实现，作为对现有GRPO框架的一个简单改进。

Result: 实验结果显示，在不同的推理和分子生成任务中，IPS-GRPO能够有效地减少结果层面的模式崩溃，同时保持甚至超过基准模型的表现。这表明，通过修正目标函数而不是单纯增加探索机制，对于可靠地优化多模态策略至关重要。

Conclusion: 本文的研究为理解强化学习中的模式崩溃提供了新的视角，并提出了一种有效的解决方案——逆概率缩放。实验证明，这种方法不仅有助于提高策略多样性，还能维持或提升整体性能。

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [85] [LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics](https://arxiv.org/abs/2601.21681)
*Qisong Xiao,Xinhai Chen,Qinglin Wang,Xiaowei Guo,Binglin Wang,Weifeng Chen,Zhichao Wang,Yunfei Liu,Rui Xia,Hang Zou,Gencheng Liu,Shuai Li,Jie Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型的时空预测框架LLM4Fluid，用于流体动力学建模。该框架通过物理信息解缠机制增强的降阶建模将高维流场压缩至紧凑的潜在空间，并使用预训练的大语言模型作为时间处理器自回归地预测物理序列的动力学。实验表明，LLM4Fluid在无需重新训练的情况下展现出强大的泛化能力和零样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在处理未见过的流动条件时泛化能力有限，且通常需要针对新场景进行再训练。为了克服这些问题，研究者开发了LLM4Fluid，一种利用大语言模型作为通用神经求解器的时空预测框架，旨在提高对不同流体动力学场景的适应性和泛化性能。

Method: 1. 采用结合物理信息解缠机制的降阶建模技术，将复杂的高维流场映射到一个低维、紧凑的表示空间。
2. 利用预训练的大语言模型作为时间处理器，通过时间序列提示来自动回归地预测物理序列的发展过程。
3. 提出了一种专门的模态对齐策略，以解决提示与物理序列之间存在的表征不匹配问题，从而稳定长期预测结果。

Result: 广泛的实验结果表明，在多种不同的流动场景下，LLM4Fluid能够作为一个鲁棒且可泛化的神经求解器工作，无需额外训练即可达到最先进的准确率，并展示了出色的零样本和上下文内学习能力。

Conclusion: LLM4Fluid证明了使用大语言模型作为通用神经求解器对于流体动力学中时空预测的有效性。它不仅提高了现有方法的泛化能力，还展示了显著的零样本及上下文学习潜力，为未来相关领域研究提供了新的方向。

Abstract: Deep learning has emerged as a promising paradigm for spatio-temporal modeling of fluid dynamics. However, existing approaches often suffer from limited generalization to unseen flow conditions and typically require retraining when applied to new scenarios. In this paper, we present LLM4Fluid, a spatio-temporal prediction framework that leverages Large Language Models (LLMs) as generalizable neural solvers for fluid dynamics. The framework first compresses high-dimensional flow fields into a compact latent space via reduced-order modeling enhanced with a physics-informed disentanglement mechanism, effectively mitigating spatial feature entanglement while preserving essential flow structures. A pretrained LLM then serves as a temporal processor, autoregressively predicting the dynamics of physical sequences with time series prompts. To bridge the modality gap between prompts and physical sequences, which can otherwise degrade prediction accuracy, we propose a dedicated modality alignment strategy that resolves representational mismatch and stabilizes long-term prediction. Extensive experiments across diverse flow scenarios demonstrate that LLM4Fluid functions as a robust and generalizable neural solver without retraining, achieving state-of-the-art accuracy while exhibiting powerful zero-shot and in-context learning capabilities. Code and datasets are publicly available at https://github.com/qisongxiao/LLM4Fluid.

</details>


### [86] [XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision](https://arxiv.org/abs/2601.21688)
*Alexandre Myara,Nicolas Bourriez,Thomas Boyer,Thomas Lemercier,Ihab Bendidi,Auguste Genovesio*

Main category: cs.LG

TL;DR: 本文提出了一种弱监督的VAE框架XFactors，通过对比学习和KL正则化实现对选定因素的解耦表示，并在多个数据集上取得了领先的解耦得分。


<details>
  <summary>Details</summary>
Motivation: 完全无监督的方法虽然在合成数据上可以成功应用，但在没有强归纳偏置的情况下无法从真实数据中恢复语义因素；而有监督方法由于依赖对抗性目标或辅助分类器，在大规模属性集上不稳定且难以扩展。

Method: XFactors基于解缠信息瓶颈视角，将表示分解为目标特定子空间和残差子空间。每个目标因素通过对比监督（InfoNCE损失）在其指定子空间内编码，同时KL正则化对所有子空间施加高斯结构，以组织几何结构并避免对抗训练与分类器的需求。

Result: 在多个数据集上，XFactors使用恒定超参数达到了最先进的解纠缠分数，并且能够支持通过潜变量替换来控制因素交换。此外，该方法还展示了随着潜在容量增加时的良好扩展性，并在真实世界数据集CelebA上进行了评估。

Conclusion: XFactors作为一种新的弱监督VAE框架，有效地解决了从真实数据中学习解纠缠表示的问题，同时保持了良好的可扩展性和可控性。

Abstract: Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers.
  We introduce \textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\mathcal{S}$ and factor-specific subspaces $\mathcal{T}_1,\ldots,\mathcal{T}_K$ and a residual subspace $\mathcal{S}$. Each target factor is encoded in its assigned $\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers.
  Across multiple datasets, with constant hyperparameters, \textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.

</details>


### [87] [Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts](https://arxiv.org/abs/2601.21690)
*Qinglun Li,Anke Tang,Miao Zhang,Mengzhu Wang,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 本文通过L2稳定性理论分析了在不同超参数环境下模型合并的有效性，提出了一个统一的理论框架来解释现有合并算法，并为实践者提供了策略性微调建议，实验结果验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对不同微调超参数（如学习率、批量大小）下模型合并有效性的统一理论，同时开源微调模型中缺少超参数透明度，使得难以预测合并后模型的表现，给实际操作带来了困难。

Method: 利用L2稳定性理论，在异质超参数环境下分析合并模型x_avg的泛化能力。

Result: 开发了一个统一的理论框架来解释现有的合并算法，并且基于此框架向从业者提出了可执行的建议，帮助他们更有效地微调专家模型以构建友好的合并模型。广泛的实验结果证实了不同超参数对于x_avg泛化能力的影响与理论预测相符。

Conclusion: 该研究不仅填补了理论空白，还为如何创建更适合合并的模型提供了实用指南。

Abstract: Model merging efficiently aggregates capabilities from multiple fine-tuned models into a single one, operating purely in parameter space without original data or expensive re-computation. Despite empirical successes, a unified theory for its effectiveness under heterogeneous finetuning hyperparameters (e.g., varying learning rates, batch sizes) remains missing. Moreover, the lack of hyperparameter transparency in open-source fine-tuned models makes it difficult to predict merged-model performance, leaving practitioners without guidance on how to fine-tune merge-friendly experts. To address those two challenges, we employ $L_2$-Stability theory under heterogeneous hyperparameter environments to analyze the generalization of the merged model $\boldsymbol{x}_{avg}$. This pioneering analysis yields two key contributions: (i) \textit{A unified theoretical framework} is provided to explain existing merging algorithms, revealing how they optimize specific terms in our bound, thus offering a strong theoretical foundation for empirical observations. (ii) \textit{Actionable recommendations} are proposed for practitioners to strategically fine-tune expert models, enabling the construction of merge-friendly models within the pretraining-to-finetuning pipeline. Extensive experiments on the ResNet/Vit family across 20/8 visual classification tasks, involving thousands of finetuning models, robustly confirm the impact of different hyperparameters on the generalization of $\boldsymbol{x}_{avg}$ predicted by our theoretical results.

</details>


### [88] [Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics](https://arxiv.org/abs/2601.21698)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.LG

TL;DR: 本研究通过训练不同参数规模的Pythia模型（从14M到410M），在三种语言动机课程设置下探究了课程学习对预训练数据顺序的影响。结果表明，尽管不同的排序方式主要影响的是阶段内的数据暴露，而非创造新的学习轨迹，但它们确实有助于稳定优化过程。特别是在较小模型中，随机排序表现出更高的梯度噪声和更强的后期训练输出头谱饱和现象，并且最终准确率较低；而课程安排可以在相同计算资源下减少这些效应。随着模型规模增大，饱和差异减小，课程带来的收益也随之减少。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索课程学习是否改变了学习轨迹，还是仅仅重新排列了一个固定轨迹上的数据暴露顺序。

Method: 通过在三种基于语言学动机设计的课程（获得年龄、词频和动词变体）以及随机排序下训练Pythia模型来进行对比分析。

Result: 发现所有排序方式下训练都遵循一个共享的潜在阶段序列，课程主要改变的是阶段内部的数据暴露情况。对于小型模型而言，随机排序会导致更高水平的梯度噪声及更显著的后期训练输出头部光谱饱和现象，并且最终准确性较低；而在相同计算量条件下，采用课程可以减轻这两种效应。当模型规模增加时，饱和度差异变得不那么明显，同时课程带来的增益也有所下降。

Conclusion: 课程学习主要是通过稳定每个阶段内的优化过程来发挥作用，而不是通过创建新的学习阶段。

Abstract: Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.

</details>


### [89] [Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities](https://arxiv.org/abs/2601.21702)
*Tien Dang,The-Hai Nguyen,Dinh Mai Phuong,Nguyen Minh Phuong,Hoang Thanh-Tung,Le-Minh Nguyen,Naoya Inoue*

Main category: cs.LG

TL;DR: 本文探讨了表示误导（RM）这种大语言模型的遗忘方法，通过操纵待遗忘样本的潜在表示来实现遗忘。研究发现，除了能够实现遗忘外，该方法还能引发可控的副作用和增强的能力，这些效果与高层概念相关。


<details>
  <summary>Details</summary>
Motivation: 尽管表示误导在大语言模型中作为一种遗忘手段很重要，但其使用的目标向量的作用尚未得到充分探索。

Method: 从线性表示假设的角度重新审视表示误导方法，认为如果可以识别出对应于某个高层概念的一维表示，则能够在遗忘表示空间内对该概念向量进行线性操作。

Result: 实验证明了机器遗忘不仅可以达成目标上的遗忘，还能够激发出与高层概念相关的可控行为以及更强的能力，比如控制未学习模型的真实性、情感倾向及拒绝态度，以及提高未学习模型的情境学习能力等。

Conclusion: 表示误导现象既可能成为一种潜在风险，也可能作为一种机制被利用起来以开发出具有更强能力和可控行为的模型。

Abstract: We consider representation misdirection (RM), a class of LLM unlearning methods that achieves forgetting by manipulating the forget-representations, that is, latent representations of forget samples. Despite being important, the roles of target vectors used in RM, however, remain underexplored. Here, we approach and revisit RM through the lens of the linear representation hypothesis. Specifically, if one can somehow identify a one-dimensional representation corresponding to a high-level concept, the linear representation hypothesis enables linear operations on this concept vector within the forget-representation space. Under this view, we hypothesize that, beyond forgetting, machine unlearning elicits controllable side behaviors and stronger side capabilities corresponding to the high-level concept. Our hypothesis is empirically validated across a wide range of tasks, including behavioral control (e.g., controlling unlearned models' truth, sentiment, and refusal) and capability enhancement (e.g., improving unlearned models' in-context learning capability). Our findings reveal that this fairly attractive phenomenon could be either a hidden risk if misused or a mechanism that can be harnessed for developing models that require stronger capabilities and controllable behaviors.

</details>


### [90] [When does predictive inverse dynamics outperform behavior cloning?](https://arxiv.org/abs/2601.21718)
*Lukas Schäfer,Pallavi Choudhury,Abdelhak Lemkhenter,Chris Lovett,Somjit Nath,Luis França,Matheus Ribeiro Furtado de Mendonça,Alex Lamb,Riashat Islam,Siddhartha Sen,John Langford,Katja Hofmann,Sergio Valcarcel Macua*

Main category: cs.LG

TL;DR: 本文解释了为何预测性逆向动力学模型（PIDM）在行为克隆（BC）中表现更优，尤其是在专家演示有限的情况下。通过理论分析和实验验证，发现PIDM通过引入偏差-方差权衡减少了方差，并且在二维导航任务和复杂三维环境中的样本效率高于BC。


<details>
  <summary>Details</summary>
Motivation: 行为克隆(BC)是一种实用的离线模仿学习方法，但在专家演示有限时经常失败。尽管最近提出的预测性逆向动力学模型(PIDM)结合未来状态预测器与逆向动力学模型能够优于BC，但其背后的原因尚不清楚。

Method: 文章从理论上解释了PIDM如何通过引入偏差-方差权衡来减少方差；设定了状态预测器偏差条件下PIDM达到比BC更低预测误差及更高样本效率所需满足的条件；并通过二维导航任务以及具有高维视觉输入和随机转换的现代视频游戏中的复杂3D环境进行了实证研究。

Result: 在二维导航任务中，BC要达到与PIDM相当的表现需要多至五倍（平均三倍）的演示次数；而在复杂的3D环境中，BC则需超过66%以上的样本来匹配PIDM的效果。

Conclusion: 研究表明，PIDM通过降低方差显著提高了样本效率，特别是在额外数据源可用时优势更加明显，这为理解PIDM相对于传统BC方法的优势提供了坚实的理论基础。

Abstract: Behavior cloning (BC) is a practical offline imitation learning method, but it often fails when expert demonstrations are limited. Recent works have introduced a class of architectures named predictive inverse dynamics models (PIDM) that combine a future state predictor with an inverse dynamics model (IDM). While PIDM often outperforms BC, the reasons behind its benefits remain unclear. In this paper, we provide a theoretical explanation: PIDM introduces a bias-variance tradeoff. While predicting the future state introduces bias, conditioning the IDM on the prediction can significantly reduce variance. We establish conditions on the state predictor bias for PIDM to achieve lower prediction error and higher sample efficiency than BC, with the gap widening when additional data sources are available. We validate the theoretical insights empirically in 2D navigation tasks, where BC requires up to five times (three times on average) more demonstrations than PIDM to reach comparable performance; and in a complex 3D environment in a modern video game with high-dimensional visual inputs and stochastic transitions, where BC requires over 66\% more samples than PIDM.

</details>


### [91] [LoRA and Privacy: When Random Projections Help (and When They Don't)](https://arxiv.org/abs/2601.21719)
*Yaxi Hu,Johanna Düngler,Bernhard Schölkopf,Amartya Sanyal*

Main category: cs.LG

TL;DR: 本文介绍了Wishart投影机制，一种形式为$S \mapsto M f(S)$的随机映射，并研究了其差分隐私属性。对于向量值查询$f$，仅使用Wishart随机性即可保证非渐近DP，无需任何加性噪声。但对于矩阵值查询，在无噪声环境下该机制不提供DP保护，并且容易受到成员推断攻击。通过引入噪声变体和利用随机性和低秩投影，可以放大隐私保护。此外，研究表明LoRA风格更新属于矩阵值机制的一个实例，表明即使具有内置随机性，LoRA本质上也不具备隐私保护，但低秩微调在相同噪声水平下可能比全微调更私密。


<details>
  <summary>Details</summary>
Motivation: 研究者旨在探索基于Wishart分布的随机映射是否能在不添加额外噪声的情况下为数据查询提供足够的差分隐私保护。同时，他们也对这种机制在处理不同类型的查询（如向量值与矩阵值）时的表现感兴趣，特别是它如何影响隐私保护的有效性以及面对潜在攻击时的安全性。

Method: 研究者首先定义并分析了Wishart投影机制，这是一种特定形式的随机映射，其中$M$遵循给定参数的Wishart分布。对于向量值查询，他们证明了仅使用此机制即能满足差分隐私要求；而对于矩阵值查询，则展示了当没有外部噪声时该机制存在隐私泄露风险，并通过实现一个接近完美的成员推断攻击来证实这一点。随后，研究人员考察了一个带有噪声版本的机制，并展示了由于随机性和低秩投影带来的隐私增强效果。

Result: 结果表明，对于向量值查询，Wishart投影机制能够在没有添加额外噪声的情况下提供良好的差分隐私保护。然而，对于矩阵值查询，若无外部噪声加入，则该机制易受成员推断攻击。通过引入噪声及利用随机性和低秩投影技术，可以在一定程度上改善隐私保护性能。此外，还发现LoRA风格更新实质上是矩阵值机制的一种应用案例，这暗示着尽管LoRA自带随机性，但并不天然具备隐私保护功能；不过，相较于全面微调，在相同的噪声水平下，采用低秩微调可能会带来更好的隐私保障。

Conclusion: 结论指出，虽然Wishart投影机制能够为向量值查询提供有效的差分隐私保护而不需额外噪声，但在处理矩阵值查询时却存在局限性。通过增加适当的噪声和利用低秩投影等技术手段，可以显著提升该机制在保护隐私方面的表现。此外，研究强调了即使是设计中含有随机性的方法（如LoRA），也可能需要额外措施才能确保充分的隐私保护。

Abstract: We introduce the (Wishart) projection mechanism, a randomized map of the form $S \mapsto M f(S)$ with $M \sim W_d(1/r I_d, r)$ and study its differential privacy properties. For vector-valued queries $f$, we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP, and we demonstrate its vulnerability by implementing a near perfect membership inference attack (AUC $> 0.99$). We then analyze a noisy variant and prove privacy amplification due to randomness and low rank projection, in both large- and small-rank regimes, yielding stronger privacy guarantees than additive noise alone. Finally, we show that LoRA-style updates are an instance of the matrix-valued mechanism, implying that LoRA is not inherently private despite its built-in randomness, but that low-rank fine-tuning can be more private than full fine-tuning at the same noise level. Preliminary experiments suggest that tighter accounting enables lower noise and improved accuracy in practice.

</details>


### [92] [Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators](https://arxiv.org/abs/2601.21737)
*Rebecca Pelke,Joel Klein,Jose Cubero-Cascante,Nils Bosbach,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.LG

TL;DR: 提出了一种针对计算内存（CIM）架构的混合精度训练和编译框架，通过强化学习策略寻找合适的量化配置，在最佳情况下可实现高达2.48倍的速度提升，同时精度损失仅为0.086%。


<details>
  <summary>Details</summary>
Motivation: 现有的CIM加速器虽然能够直接在内存中执行矩阵向量乘法(MVM)，但由于大多数CIM编译器不支持低于8比特的量化，导致单个MVM需要很多计算周期，并且权重无法有效存储于单一交叉开关单元内。为了解决这一问题，提出了新的解决方案。

Method: 开发了一个混合精度训练与编译框架，特别是引入了基于强化学习的方法来搜索合适的量化参数配置，旨在平衡延迟与准确性之间的关系。

Result: 该方法在最优情况下相较于现有最先进解决方案实现了最高达2.48倍的速度提升，而准确度仅下降了0.086%。

Conclusion: 本研究展示了一种有效提高CIM架构性能的新途径，通过采用混合精度训练及基于强化学习的量化策略，显著减少了计算所需时间，同时保持了较高的模型精度。

Abstract: Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.

</details>


### [93] [Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle](https://arxiv.org/abs/2601.21739)
*Alberto Fernández-Hernández,Cristian Pérez-Corral,Jose I. Mestre,Manuel F. Dolz,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 本文探讨了当Adam优化器的动量参数$β_{1}$和$β_{2}$相等时，训练过程中的验证分数和定性行为得到改善的现象。通过引入并形式化'梯度尺度不变性'的概念，证明了只有当$β_{1}=β_{2}$时Adam才具有一阶梯度尺度不变性，并且这种设置与最新的强调尺度鲁棒更新的优化器设计理念一致。


<details>
  <summary>Details</summary>
Motivation: 研究者们注意到，在使用Adam优化器进行大规模训练时，如果将动量参数设置为$β_{1}=β_{2}$，则模型的验证得分及训练过程的质量都会有所提高。然而，对于这一现象背后的原因尚无明确解释。本研究旨在揭示这一选择背后的原理。

Method: 文章首先提出了“梯度尺度不变性”这一概念，并对其进行了数学上的定义。接着，通过理论分析证明了仅当$β_{1}=β_{2}$时Adam优化器能够实现一阶梯度尺度不变性。此外，还通过跨越视觉和语言任务以及不同架构家族的实验来验证该理论的有效性。

Result: 实验证明，在多种类型的机器学习任务中，当$β_{1}=β_{2}$时，改变梯度大小对更新的影响更为平滑。这不仅支持了理论部分提出的观点，也进一步证实了在Adam中采用相同值作为两个动量参数可以带来更好的性能表现。

Conclusion: 综上所述，本研究为理解Adam优化器的一个长期未解之谜提供了清晰的答案，并提出了一种新的设计原则——保持梯度尺度不变性，以指导未来优化算法的发展。

Abstract: Adam has been at the core of large-scale training for almost a decade, yet a simple empirical fact remains unaccounted for: both validation scores and the qualitative behaviour of the training runs improve when the momentum parameters satisfy $β_{1}=β_{2}$. Some recent studies have reported this pattern, but there is still no explanation for why this choice helps. We show that this choice is closely tied to a structural property that we refer to as \textit{gradient scale invariance}. We formalize this notion and prove that Adam becomes gradient scale invariant of first order if and only if $β_{1}=β_{2}$. This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-robust updates. The theory is supported by experiments across vision and language tasks, and across different architectural families, in which rescaling the gradient has a markedly smoother effect on the update when $β_{1}=β_{2}$. Overall, our results offer a coherent explanation for an open question in the behavior of Adam and provide a simple principle that helps guide the design of future optimizers.

</details>


### [94] [Temporal Sepsis Modeling: a Fully Interpretable Relational Way](https://arxiv.org/abs/2601.21747)
*Vincent Lemaire,Nédra Meloulli,Pierre Jaquet*

Main category: cs.LG

TL;DR: 该研究提出了一种基于关系方法的机器学习框架，用于脓毒症的早期预测。通过将电子病历中的时间数据表示为多变量患者日志，并应用命题化技术构建可解释特征，最后使用选择性朴素贝叶斯分类器进行分类。实验验证表明，该方法不仅有效且具有极高的可解释性（包括单变量、全局、局部和反事实解释）。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是一种复杂的综合症，在重症监护中表现出多种生理轨迹和对治疗的不同反应。尽管深度学习模型在脓毒症早期预测方面表现良好，但它们通常缺乏可解释性并且忽略了潜在的患者亚型。

Method: 本研究采用了一种新的方法来解决上述问题：一种关系方法。首先，将来自电子医疗记录的时间数据视为多变量患者日志，并以关系数据模式表示。接着，运用基于经典聚合/选择函数的命题化技术来构造可解释特征，实现数据的“扁平化”。最后，利用选择性朴素贝叶斯分类器对扁平化的数据进行分类。

Result: 实验验证了所提方法的相关性和极高可解释性。这种可解释性体现在四个方面：单变量、全局、局部以及反事实层面。

Conclusion: 提出的关系方法框架在脓毒症早期预测上提供了有效的解决方案，同时显著提高了模型的可解释性，有助于更好地理解不同患者群体之间的差异及个体化治疗方案的选择。

Abstract: Sepsis remains one of the most complex and heterogeneous syndromes in intensive care, characterized by diverse physiological trajectories and variable responses to treatment. While deep learning models perform well in the early prediction of sepsis, they often lack interpretability and ignore latent patient sub-phenotypes. In this work, we propose a machine learning framework by opening up a new avenue for addressing this issue: a relational approach. Temporal data from electronic medical records (EMRs) are viewed as multivariate patient logs and represented in a relational data schema. Then, a propositionalisation technique (based on classic aggregation/selection functions from the field of relational data) is applied to construct interpretable features to "flatten" the data. Finally, the flattened data is classified using a selective naive Bayesian classifier. Experimental validation demonstrates the relevance of the suggested approach as well as its extreme interpretability. The interpretation is fourfold: univariate, global, local, and counterfactual.

</details>


### [95] [ECSEL: Explainable Classification via Signomial Equation Learning](https://arxiv.org/abs/2601.21789)
*Adia Lumadjeng,Ilker Birbil,Erman Acar*

Main category: cs.LG

TL;DR: 本文提出了一种名为ECSEL的可解释分类方法，通过学习形式为符号方程的正式表达式来实现。该方法在标准符号回归基准测试中比其他最先进方法恢复了更大比例的目标方程，并且所需计算量大大减少。ECSEL不仅在分类准确率上与已建立的机器学习模型竞争，还保持了可解释性，并且在实际案例研究中展示了其能够揭示数据集偏差、支持反事实推理并提供可操作见解的能力。


<details>
  <summary>Details</summary>
Motivation: 作者观察到许多符号回归基准具有紧凑的符号结构，因此提出了ECSEL方法，旨在开发一种既能作为分类器又能作为解释工具的结构化闭式表达式。

Method: ECSEL采用直接构建符号方程式的形式，以此作为分类与解释的基础。它能够在标准符号回归基准上以更少的计算资源恢复更多目标方程。

Result: ECSEL在多个基准数据集以及电商和欺诈检测两个真实世界案例研究中表现良好，不仅能有效分类而且保持了高度的可解释性，同时还能揭示数据集中的偏见、支持反事实分析及提供实用洞见。

Conclusion: ECSEL作为一种新的可解释分类方法，在保持高分类准确性的同时提供了良好的模型解释能力，适用于需要理解模型决策过程的应用场景。

Abstract: We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.

</details>


### [96] [NetMamba+: A Framework of Pre-trained Models for Efficient and Accurate Network Traffic Classification](https://arxiv.org/abs/2601.21792)
*Tongze Wang,Xiaohui Xie,Wenduo Wang,Chuyi Wang,Jinzhou Liu,Boyan Huang,Yannan Hu,Youjian Zhao,Yong Cui*

Main category: cs.LG

TL;DR: NetMamba+ is a novel framework for efficient and accurate encrypted network traffic classification, addressing key challenges through innovative architecture, multimodal representation, and fine-tuning strategy. It outperforms existing methods in terms of F1 score, inference throughput, and few-shot learning, with demonstrated real-world application.


<details>
  <summary>Details</summary>
Motivation: The motivation behind NetMamba+ stems from the need to overcome three main challenges in current machine learning and deep learning approaches for traffic classification: inefficient computation by Transformer models, suboptimal traffic data representation that loses important details while keeping biases, and difficulty in managing long-tail distributions within actual data sets.

Method: NetMamba+ introduces an efficient architecture that combines Mamba and Flash Attention mechanisms, a multimodal traffic representation approach that keeps vital traffic information without the biases, and a label distribution-aware fine-tuning method. The system's performance was evaluated on large datasets across four primary classification tasks, showing significant improvements over state-of-the-art solutions.

Result: Evaluation results highlight NetMamba+'s enhanced classification performance, with up to 6.44% improvement in F1 score over leading baselines. It also achieves 1.7 times higher inference throughput with low memory usage, excels in few-shot learning, and demonstrates practical effectiveness in a real-world online traffic classification system with a throughput of 261.87 Mb/s.

Conclusion: NetMamba+ represents a pioneering effort in adapting the Mamba architecture for network traffic classification, offering a more efficient and precise solution compared to existing methods. Its innovations in architecture, representation, and fine-tuning not only boost its performance but also pave the way for better analysis in complex network scenarios.

Abstract: With the rapid growth of encrypted network traffic, effective traffic classification has become essential for network security and quality of service management. Current machine learning and deep learning approaches for traffic classification face three critical challenges: computational inefficiency of Transformer architectures, inadequate traffic representations with loss of crucial byte-level features while retaining detrimental biases, and poor handling of long-tail distributions in real-world data. We propose NetMamba+, a framework that addresses these challenges through three key innovations: (1) an efficient architecture considering Mamba and Flash Attention mechanisms, (2) a multimodal traffic representation scheme that preserves essential traffic information while eliminating biases, and (3) a label distribution-aware fine-tuning strategy. Evaluation experiments on massive datasets encompassing four main classification tasks showcase NetMamba+'s superior classification performance compared to state-of-the-art baselines, with improvements of up to 6.44\% in F1 score. Moreover, NetMamba+ demonstrates excellent efficiency, achieving 1.7x higher inference throughput than the best baseline while maintaining comparably low memory usage. Furthermore, NetMamba+ exhibits superior few-shot learning abilities, achieving better classification performance with fewer labeled data. Additionally, we implement an online traffic classification system that demonstrates robust real-world performance with a throughput of 261.87 Mb/s. As the first framework to adapt Mamba architecture for network traffic classification, NetMamba+ opens new possibilities for efficient and accurate traffic analysis in complex network environments.

</details>


### [97] [Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models](https://arxiv.org/abs/2601.21794)
*Yejin Kim,Dongjun Hwang,Sungmin Cha,Junsuk Choe*

Main category: cs.LG

TL;DR: 提出了一种无需训练的知识向量弱化(KVW)方法，用于从大规模视觉-语言模型中移除特定数据的影响，实验表明KVW在保证遗忘-保留平衡的同时显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 针对大型视觉-语言模型存在的隐私泄露和有害内容生成问题，现有的基于梯度优化的机器遗忘方法对于大规模模型来说计算成本过高。

Method: 提出了知识向量弱化(KVW)，一种不需要训练也不需要梯度计算的方法，直接作用于完整模型，通过识别并逐步削弱在遗忘集上激活的知识向量来阻止模型利用不希望的知识。

Result: 实验结果表明，KVW不仅实现了稳定的遗忘-保留权衡，还在计算效率方面明显优于基于梯度和LoRA的方法。

Conclusion: KVW作为一种高效的无训练机器遗忘方案，在处理LVLMs中的隐私及内容安全问题时展现出了巨大潜力。

Abstract: Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.

</details>


### [98] [Effective LoRA Adapter Routing using Task Representations](https://arxiv.org/abs/2601.21795)
*Akash Dhasade,Anne-Marie Kermarrec,Igor Pavlovic,Diana Petrescu,Rafael Pires,Mathis Randl,Martijn de Vos*

Main category: cs.LG

TL;DR: 本文提出了一种新的路由框架LORAUTER，它通过任务表示而不是适配器特性来选择和组合LoRA适配器。实验表明，LORAUTER在多个任务上优于基线路由方法，并且能够很好地处理大型、嘈杂的适配器池。


<details>
  <summary>Details</summary>
Motivation: 随着专为各种任务设计的公共适配器池快速增长，如何有效地使用这些适配器成为了一个挑战。现有的方法直接将查询映射到适配器，而没有考虑到任务级别的适应性。

Method: 提出了LORAUTER，一个基于任务嵌入而非适配器特性的新路由框架。该框架利用小规模验证集派生的任务嵌入来进行查询路由，无需适配器训练数据，并能够在任务级别实现高效路由。

Result: 实验结果表明，在存在与任务对齐的适配器时，LORAUTER能够匹配Oracle性能（101.2%），并且对于未见过的任务也能取得最先进的结果（+5.2点）。此外，LORAUTER还展示了其在扩展至超过1500个适配器时对非常大且嘈杂的适配器池的强大鲁棒性。

Conclusion: LORAUTER提供了一种更有效的途径来选择和组合LoRA适配器以完成特定任务，相比传统方法显示出显著优势。

Abstract: Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate adapters for a query. We introduce LORAUTER, a novel routing framework that selects and composes LoRA adapters using task representations rather than adapter characteristics. Unlike existing approaches that map queries directly to adapters, LORAUTER routes queries via task embeddings derived from small validation sets and does not require adapter training data. By operating at the task level, LORAUTER achieves efficient routing that scales with the number of tasks rather than the number of adapters. Experiments across multiple tasks show that LORAUTER consistently outperforms baseline routing approaches, matching Oracle performance (101.2%) when task-aligned adapters exist and achieving state-of-the-art results on unseen tasks (+5.2 points). We further demonstrate the robustness of LORAUTER to very large, noisy adapter pools by scaling it to over 1500 adapters.

</details>


### [99] [Nonparametric LLM Evaluation from Preference Data](https://arxiv.org/abs/2601.21816)
*Dennis Frauen,Athiya Deviyani,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出了一种非参数统计框架DMLEval，利用去偏机器学习从偏好数据中比较和排名大型语言模型（LLMs），并引入了广义平均排名分数（GARS）。该方法具有统计效率高、支持黑盒机器学习方法集成、可与预训练的LLM评估器结合以及在预算限制下收集偏好数据的最佳策略等优点。


<details>
  <summary>Details</summary>
Motivation: 现有的评价大型语言模型性能的方法要么依赖于严格的参数假设，要么在使用灵活的机器学习方法时缺乏有效的不确定性量化。因此，需要一种新的方法来更好地从人类偏好数据中评估这些模型。

Method: 开发了一个名为DMLEval的非参数统计框架，该框架通过去偏机器学习技术基于偏好数据来比较和排名LLMs，并且引入了能够处理复杂人类响应（如平局）的广义平均排名分数（GARS）。

Result: DMLEval能够提供统计上高效的GARS排名评分估计；自然地允许集成黑盒机器学习方法用于估计；可以与预训练的LLM评估者结合；并且推荐了在预算约束下的最佳偏好数据收集策略。这些优势已在理论分析及合成和真实世界偏好数据集上的实证研究中得到验证。

Conclusion: DMLEval为从业者提供了先进而强大的方法，以用来比较或排序大型语言模型，从而改善了现有评价体系中的局限性。

Abstract: Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. In this paper, we propose a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, we introduce generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. We demonstrate these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, our framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs.

</details>


### [100] [Goal-Driven Adaptive Sampling Strategies for Machine Learning Models Predicting Fields](https://arxiv.org/abs/2601.21832)
*Jigar Parekh,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 本文提出了一种用于预测场的机器学习模型的主动学习策略，该策略不依赖于模型架构本身。通过结合一个成熟的高斯过程模型来处理标量参考值，并同时减少认知模型误差和标量与场预测之间的差异。结果表明，在NASA通用研究模型上进行不确定性传播任务时，相较于没有采用主动学习的方法，该方法能够在显著降低计算成本的同时保持较高的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管在多个领域中都需要能够对场进行预测的主动学习策略，但目前此类策略要么缺失，要么仅限于非常特定的情景或模型类型。为了解决这一问题，作者提出了一个适用于能预测场的机器学习模型的新主动学习策略。

Method: 本研究提出的方法是开发一种新的主动学习策略，它将一个已建立的高斯过程模型应用于标量参考值，并且同时致力于减少认知模型误差以及标量与场预测间的差异。文中还介绍了这种方法的不同具体形式，并将其与仅基于标量值的信息填充方法进行了比较。

Result: 实验结果表明，所提出的主动学习策略在NASA通用研究模型上执行不确定性传播任务时表现出色，不仅实现了高水平的准确性，而且相对于未使用主动学习的方法来说，计算成本显著降低。

Conclusion: 这项工作成功地开发出了一种针对场预测的主动学习策略，该策略对于模型架构是不可知的，并且在提高精度的同时有效降低了计算成本。

Abstract: Machine learning models are widely regarded as a way forward to tackle multi-query challenges that arise once expensive black-box simulations such as computational fluid dynamics are investigated. However, ensuring the desired level of accuracy for a certain task at minimal computational cost, e.g. as few black-box samples as possible, remains a challenges. Active learning strategies are used for scalar quantities to overcome this challenges and different so-called infill criteria exists and are commonly employed in several scenarios. Even though needed in various field an extension of active learning strategies towards field predictions is still lacking or limited to very specific scenarios and/or model types. In this paper we propose an active learning strategy for machine learning models that are capable if predicting field which is agnostic to the model architecture itself. For doing so, we combine a well-established Gaussian process model for a scalar reference value and simultaneously aim at reducing the epistemic model error and the difference between scalar and field predictions. Different specific forms of the above-mentioned approach are introduced and compared to each other as well as only scalar-valued based infill. Results are presented for the NASA common research model for an uncertainty propagation task showcasing high level of accuracy at significantly smaller cost compared to an approach without active learning.

</details>


### [101] [Constrained Meta Reinforcement Learning with Provable Test-Time Safety](https://arxiv.org/abs/2601.21845)
*Tingting Ni,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 提出了一种在保证安全性的前提下，减少样本复杂度并加快学习最优策略的约束元强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 尽管元强化学习（RL）能够提高测试任务上的样本效率，但在机器人和医疗保健等实际应用中，测试期间存在安全性约束。为了解决如何在确保真实世界测试任务中策略的安全性的同时降低样本复杂度的问题。

Method: 提出了一种新算法，该算法改进了训练期间学习到的策略，并且对于在测试任务上学得接近最优策略提供了可证明的安全性和样本复杂度保障。此外还推导了一个匹配的下界，表明这种样本复杂度是紧致的。

Result: 所提出的算法能够在保证安全性的前提下有效地减少了学习最优策略所需的样本数量。

Conclusion: 本研究为在满足安全性要求的情况下通过约束元强化学习来加速学习过程提供了一种有效方法。

Abstract: Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.

</details>


### [102] [READY: Reward Discovery for Meta-Black-Box Optimization](https://arxiv.org/abs/2601.21847)
*Zechuan Huang,Zhiguang Cao,Hongshu Guo,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.LG

TL;DR: 本文提出了一种使用大型语言模型（LLM）作为元黑盒优化（MetaBBO）自动奖励发现工具的方法，通过迭代的程序搜索过程和多任务进化架构来同时提高优化的有效性和效率。实验结果表明，这种方法可以提升现有的MetaBBO工作表现。


<details>
  <summary>Details</summary>
Motivation: 当前元黑盒优化（MetaBBO）领域的奖励函数设计依赖于人类专家，这可能引入设计偏见并存在奖励被破解的风险。因此，作者们探索了利用大型语言模型（LLM）来自动生成奖励函数的可能性，旨在减少人为因素的影响同时提高优化性能。

Method: 采用大型语言模型作为核心工具，在效果方面借鉴启发式演化的思想，实现基于LLM的程序搜索过程中定制化演化范式；在效率方面，则是引入了多任务演化架构以支持多样化的MetaBBO方法平行地进行奖励发现。该架构还促进了跨任务的知识共享，有助于加速收敛速度。

Result: 实验证明，通过本研究方法发现的奖励函数能够有效促进现有MetaBBO工作的性能提升，强调了在MetaBBO中精心设计奖励的重要性。

Conclusion: 通过将大型语言模型应用于元黑盒优化中的奖励发现过程，并结合特定的演化策略与多任务处理机制，可以显著改善MetaBBO算法的表现。这不仅为解决传统上依赖人工设计奖励函数所面临的问题提供了新的思路，同时也为进一步优化相关技术开辟了道路。

Abstract: Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.

</details>


### [103] [Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models](https://arxiv.org/abs/2601.21851)
*Sidney Bender,Marco Morik*

Main category: cs.LG

TL;DR: 提出了一种新的框架Visual Disentangled Diffusion Autoencoders (DiDAE)，结合了冻结的基础模型和解缠字典学习，用于高效生成反事实样本，以对抗基础模型中的虚假相关性和'聪明汉斯'策略。通过与反事实知识蒸馏相结合，DiDAE-CFKD在减少捷径学习和提高不平衡数据集下游表现方面达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的缓解方法要么依赖于难以获得的组标签，要么需要计算成本高昂的基于梯度的对抗优化。为了解决这些问题，研究提出了一个新框架来更有效地生成反事实样例，从而减轻基础模型对错误关联的学习。

Method: 研究者们引入了Visual Disentangled Diffusion Autoencoders (DiDAE)这一新颖架构，它将冻结的基础模型与解缠字典学习结合起来，实现无需梯度计算的反事实样例生成。该方法首先在解缠字典所定义的方向上编辑基础模型嵌入，然后通过扩散自编码器进行解码，快速产生多样化的解缠反事实样例。

Result: DiDAE能够比现有基线更快地为每个实际案例生成多个不同且解缠的反事实样例。当与Counterfactual Knowledge Distillation结合使用时，DiDAE-CFKD在减少捷径学习以及提升不平衡数据集上的下游任务表现方面展现出了领先水平的表现。

Conclusion: DiDAE提供了一个有效且高效的解决方案，用于生成反事实样例并改进基础模型对于虚假关联的鲁棒性，特别是在处理具有挑战性的不平衡数据集时表现出色。

Abstract: Foundation models, despite their robust zero-shot capabilities, remain vulnerable to spurious correlations and 'Clever Hans' strategies. Existing mitigation methods often rely on unavailable group labels or computationally expensive gradient-based adversarial optimization. To address these limitations, we propose Visual Disentangled Diffusion Autoencoders (DiDAE), a novel framework integrating frozen foundation models with disentangled dictionary learning for efficient, gradient-free counterfactual generation directly for the foundation model. DiDAE first edits foundation model embeddings in interpretable disentangled directions of the disentangled dictionary and then decodes them via a diffusion autoencoder. This allows the generation of multiple diverse, disentangled counterfactuals for each factual, much faster than existing baselines, which generate single entangled counterfactuals. When paired with Counterfactual Knowledge Distillation, DiDAE-CFKD achieves state-of-the-art performance in mitigating shortcut learning, improving downstream performance on unbalanced datasets.

</details>


### [104] [Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions](https://arxiv.org/abs/2601.21873)
*Jinhang Chai,Xuyuan Liu,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 本文提出了一种适用于结构化矩阵估计的迁移学习框架，该框架能够在目标任务维度和内在表示同时增长的情况下，通过保持源任务子空间并仅估计低维创新和稀疏修改来实现知识迁移。


<details>
  <summary>Details</summary>
Motivation: 在许多学习系统中，随着特征或潜在表示随时间扩展，早期表示会被嵌入到具有有限新潜在结构的更高维度空间中。本文旨在研究在这种情况下如何有效地进行迁移学习，特别是在源任务良好估计的基础上将其作为子空间嵌入到更高维度的目标任务中时。

Method: 提出了一种通用的迁移学习框架，在此框架下，目标任务参数分解为嵌入的源组件、低维低秩创新以及稀疏编辑，并开发了锚定交替投影估计器，能够保持转移的子空间同时只估计低维创新和稀疏修改。

Result: 确定性误差界限成功地分离了目标噪声、表示增长及源估计误差，当秩与稀疏增量较小时可获得严格改进的速率。此外，还展示了该框架应用于马尔科夫转移矩阵估计和结构化协方差估计两个典型问题上的有效性。

Conclusion: 本研究表明，所提出的迁移学习方法对于解决在任务维度增加的同时有效利用先前学习的知识具有重要作用，尤其是在新增加的部分表现为低秩或稀疏变化时效果尤为显著。

Abstract: Learning systems often expand their ambient features or latent representations over time, embedding earlier representations into larger spaces with limited new latent structure. We study transfer learning for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task.
  We propose a general transfer framework in which the target parameter decomposes into an embedded source component, low-dimensional low-rank innovations, and sparse edits, and develop an anchored alternating projection estimator that preserves transferred subspaces while estimating only low-dimensional innovations and sparse modifications. We establish deterministic error bounds that separate target noise, representation growth, and source estimation error, yielding strictly improved rates when rank and sparsity increments are small.
  We demonstrate the generality of the framework by applying it to two canonical problems. For Markov transition matrix estimation from a single trajectory, we derive end-to-end theoretical guarantees under dependent noise. For structured covariance estimation under enlarged dimensions, we provide complementary theoretical analysis in the appendix and empirically validate consistent transfer gains.

</details>


### [105] [Managing Solution Stability in Decision-Focused Learning with Cost Regularization](https://arxiv.org/abs/2601.21883)
*Victor Spitzer,Francois Sanson*

Main category: cs.LG

TL;DR: 本研究关注组合优化问题中目标函数系数的估计，通过引入对估计成本向量的正则化来解决扰动强度波动导致的学习效率低下问题，从而提高学习过程的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 决策聚焦学习将预测建模与组合优化相结合，通过训练模型直接提升决策质量而不仅仅是预测准确性。本文旨在解决因学习阶段扰动强度波动引起的学习效率低下的问题，并通过理论联系到组合优化中的解稳定性概念加以论证。

Method: 提出了一种针对估计成本向量进行正则化的方法，以应对由于扰动强度变化而导致的学习过程中出现的问题。

Result: 广泛的数值实验证明了所提方法能够有效增强学习过程中的鲁棒性和可靠性。

Conclusion: 通过对估计成本向量施加适当的正则化处理，可以显著改善决策聚焦学习框架下遇到的学习效率低下问题，进而提高了整个过程的稳定性和结果的质量。

Abstract: Decision-focused learning integrates predictive modeling and combinatorial optimization by training models to directly improve decision quality rather than prediction accuracy alone. Differentiating through combinatorial optimization problems represents a central challenge, and recent approaches tackle this difficulty by introducing perturbation-based approximations. In this work, we focus on estimating the objective function coefficients of a combinatorial optimization problem. Our study demonstrates that fluctuations in perturbation intensity occurring during the learning phase can lead to ineffective training, by establishing a theoretical link to the notion of solution stability in combinatorial optimization. We propose addressing this issue by introducing a regularization of the estimated cost vectors which improves the robustness and reliability of the learning process, as demonstrated by extensive numerical experiments.

</details>


### [106] [Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning](https://arxiv.org/abs/2601.21894)
*Lukas Twist,Shu Yang,Hanqi Yan,Jingzhi Gong,Di Wang,Helen Yannakoudakis,Jie M. Zhang*

Main category: cs.LG

TL;DR: 研究发现代码的结构复杂性对于提高大型语言模型（LLMs）的推理能力至关重要。通过控制代码的结构复杂性进行微调，可以显著优于使用结构多样性代码训练的效果，在83%的实验中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究表明接触代码能够增强LLM的推理技能，但对于何种属性的代码能真正促进这种改进仍不清楚。本研究旨在探索代码的结构性质，特别是其控制流和组合结构如何影响模型在微调过程中吸收多步推理的能力。

Method: 研究人员基于圈复杂度和逻辑行数构建了受控微调数据集，并在多种开放权重的LLM上评估了它们在不同推理基准上的表现。研究设计考虑了两种场景：解决方案驱动的复杂性和问题驱动的复杂性。

Result: 结果显示，当对用于微调的数据集施加特定结构复杂性限制时，相比于使用结构更加多样化的代码训练，这种方法在83%的情况下取得了更好的效果。这表明，除了简单地增加规模外，还存在一种以数据为中心的方法来进一步改善推理性能。

Conclusion: 该研究表明，为了有效提升LLM的推理能力，应该关注代码的具体结构性质而非仅仅将其视为一般性的训练信号。选择具有适当结构复杂性的代码作为训练材料可能是未来研究的一个重要方向。

Abstract: Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.

</details>


### [107] [Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting](https://arxiv.org/abs/2601.21899)
*Zhiqing Cui,Siru Zhong,Ming Jin,Shirui Pan,Qingsong Wen,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了OmniAir框架，用于全球空气质量预测，通过编码物理环境属性和动态构建自适应稀疏拓扑结构来捕捉长距离非欧几里得相关性和物理扩散模式。基于覆盖全球超过7800个站点的WorldAir数据集，OmniAir在效率、可扩展性以及填补数据稀疏区域监测空白方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 全球空气质量预报面临极端空间异质性和现有转导模型对未见区域泛化能力差的问题。

Method: 提出了一种名为OmniAir的语义拓扑学习框架，该框架通过对不变的物理环境属性进行编码生成可泛化的站点身份，并且能够动态地构建适应性强的稀疏拓扑结构。

Result: OmniAir相比18个基准模型实现了最先进的性能，在保持高效率和可扩展性的同时，处理速度几乎是现有模型的10倍之快，并且有效解决了数据稀缺地区监测不足的问题。

Conclusion: OmniAir提供了一个有效的解决方案来应对全球范围内空气质量预测所面临的挑战，特别是在提高预测准确性、增强模型泛化能力和加速计算过程方面。

Abstract: Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.

</details>


### [108] [Hardware-Triggered Backdoors](https://arxiv.org/abs/2601.21902)
*Jonas Möller,Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.LG

TL;DR: 本研究展示了机器学习模型在不同硬件上运行时，由于硬件设计差异导致的微小数值变化可以被利用来创建后门。通过调整模型决策函数，可以使同一输入在不同硬件上产生不同的预测结果，从而实现基于硬件触发的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管通常预期各种计算硬件能够为机器学习模型提供一致的结果，但硬件设计上的差异可能导致推理过程中出现细微的数值变化。这些变化可能会被恶意利用，在机器学习模型中植入后门，这对使用第三方模型构成了新的威胁。

Method: 研究人员通过局部移动决策边界靠近目标输入，并进一步调整数值偏差以改变选定硬件上的预测结果，以此方法来形成模型的决策函数。该过程旨在让相同输入根据执行硬件的不同而得到不一样的预测输出。

Result: 实验证明了这种基于硬件触发的后门可以在常见的GPU加速器之间稳定地创建。此外，还探讨了几种防御措施来对抗这一新型威胁。

Conclusion: 这项工作揭示了一种新的攻击向量，它影响了第三方模型的使用安全。通过展示如何利用硬件间的细微差别来创建针对特定硬件有效的后门，强调了对于来自不可信来源的模型需要采取额外的安全考量。

Abstract: Machine learning models are routinely deployed on a wide range of computing hardware. Although such hardware is typically expected to produce identical results, differences in its design can lead to small numerical variations during inference. In this work, we show that these variations can be exploited to create backdoors in machine learning models. The core idea is to shape the model's decision function such that it yields different predictions for the same input when executed on different hardware. This effect is achieved by locally moving the decision boundary close to a target input and then refining numerical deviations to flip the prediction on selected hardware. We empirically demonstrate that these hardware-triggered backdoors can be created reliably across common GPU accelerators. Our findings reveal a novel attack vector affecting the use of third-party models, and we investigate different defenses to counter this threat.

</details>


### [109] [LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution](https://arxiv.org/abs/2601.21929)
*Shuangqi Li,Hieu Le,Jingyi Xu,Mathieu Salzmann*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法LoRIF，通过利用梯度的低秩结构来解决训练数据归因（TDA）中的可扩展性挑战。相比现有方法，LoRIF在存储需求和查询时间上都有显著减少，同时保持或提高了归因质量。


<details>
  <summary>Details</summary>
Motivation: 当前最佳的训练数据归因(TDA)方法依赖于梯度定义影响函数，但面临计算梯度时的可扩展性难题。尽管随机投影技术（如TRAK, LoGRA）试图缓解这一问题，但在处理大规模训练集和高质量归因时仍然存在两个瓶颈：(i) 存储与加载所有N个训练样本的投影每样本梯度；(ii) 形成D×D逆Hessian近似，这需要O(D^2)内存。随着投影维度D增加以提高归因质量，这两个瓶颈变得更加严重。

Method: 提出了LoRIF（低秩影响函数），该方法利用了梯度的低秩特性来解决上述瓶颈。(1) 通过存储每个样本梯度的rank-c因子而非完整矩阵，将存储与查询时间I/O从O(D)降低到O(c√D)。(2) 使用截断SVD加上Woodbury恒等式，在r维子空间内近似Hessian项，从而将内存需求从O(D^2)降至O(Dr)。

Result: 在参数量从0.1B至70B不等、训练数据集包含数百万示例的模型上，相较于LoGRA，LoRIF实现了高达20倍的存储减少和查询时间加速，并且其归因质量相匹配甚至更优。

Conclusion: LoRIF通过有效利用梯度的低秩性质，极大地改善了基于梯度的TDA方法在大规模应用中的可行性和效率，为前沿规模下的TDA实践提供了新途径。

Abstract: Training data attribution (TDA) identifies which training examples most influenced a model's prediction. The best-performing TDA methods exploits gradients to define an influence function. To overcome the scalability challenge arising from gradient computation, the most popular strategy is random projection (e.g., TRAK, LoGRA). However, this still faces two bottlenecks when scaling to large training sets and high-quality attribution: \emph{(i)} storing and loading projected per-example gradients for all $N$ training examples, where query latency is dominated by I/O; and \emph{(ii)} forming the $D \times D$ inverse Hessian approximation, which costs $O(D^2)$ memory. Both bottlenecks scale with the projection dimension $D$, yet increasing $D$ is necessary for attribution quality -- creating a quality--scalability tradeoff. We introduce \textbf{LoRIF (Low-Rank Influence Functions)}, which exploits low-rank structures of gradient to address both bottlenecks. First, we store rank-$c$ factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from $O(D)$ to $O(c\sqrt{D})$ per layer per sample. Second, we use truncated SVD with the Woodbury identity to approximate the Hessian term in an $r$-dimensional subspace, reducing memory from $O(D^2)$ to $O(Dr)$. On models from 0.1B to 70B parameters trained on datasets with millions of examples, LoRIF achieves up to 20$\times$ storage reduction and query-time speedup compared to LoGRA, while matching or exceeding its attribution quality. LoRIF makes gradient-based TDA practical at frontier scale.

</details>


### [110] [Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models](https://arxiv.org/abs/2601.21943)
*Ahmad Aghapour,Erhan Bayraktar,Ziqing Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的信息论方法来分析扩散生成模型的收敛性，该方法不依赖于目标分布的几何假设。通过这种方法，作者将目标分布与生成分布之间的KL散度限定在O(H^2/K)（忽略端点因素），其中H是香农熵，K是采样步骤数。此外，基于对KL散度的新表述，提出了损失自适应调度(LAS)策略，用于反向SDE的有效离散化，并且LAS在实践中优于常见的启发式调度方案。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型收敛性分析通常至少线性依赖于环境维度，而更精确的速度往往依赖于内在维度假设或目标分布的其他几何限制。因此，需要一种新的、不依赖于这些几何假设的方法来改善收敛性分析以及采样过程。

Method: 采用信息论的方法开发了一个无需任何几何假设的维度无关收敛性框架。通过设定温和的目标分布假设，研究人员能够使用KL散度来衡量目标与生成分布之间的差异。此外，还引入了损失自适应调度(LAS)策略，这是一种轻量级的方法，仅依赖训练损失进行操作，无需额外繁重的计算。

Result: 研究表明，在给定条件下，目标与生成分布间的KL散度可以被限制在O(H^2/K)（不考虑端点因子）。同时，提出的LAS策略在实验中展现出比常见启发式调度更好的采样质量。

Conclusion: 本研究提供了一种新颖的信息论视角来理解扩散生成模型中的收敛行为，不仅避免了传统方法中对数据几何结构的要求，而且通过引入LAS策略提升了实际应用中的采样效率。

Abstract: Diffusion generative models synthesize samples by discretizing reverse-time dynamics driven by a learned score (or denoiser). Existing convergence analyses of diffusion models typically scale at least linearly with the ambient dimension, and sharper rates often depend on intrinsic-dimension assumptions or other geometric restrictions on the target distribution. We develop an alternative, information-theoretic approach to dimension-free convergence that avoids any geometric assumptions. Under mild assumptions on the target distribution, we bound KL divergence between the target and generated distributions by $O(H^2/K)$ (up to endpoint factors), where $H$ is the Shannon entropy and $K$ is the number of sampling steps. Moreover, using a reformulation of the KL divergence, we propose a Loss-Adaptive Schedule (LAS) for efficient discretization of reverse SDE which is lightweight and relies only on the training loss, requiring no post-training heavy computation. Empirically, LAS improves sampling quality over common heuristic schedules.

</details>


### [111] [Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models](https://arxiv.org/abs/2601.21944)
*Konstantinos P. Panousis,Diego Marcos*

Main category: cs.LG

TL;DR: 本文研究了概念瓶颈模型中不同建模决策对形成表示的影响，提出了清晰度的概念来衡量下游性能与概念表示的稀疏性和精确性之间的关系，并探讨了三种不同的稀疏诱导策略。实验揭示了灵活性和可解释性之间的一个重要权衡。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型（VLMs）在各个领域的广泛应用，关于这些模型内部决策过程的可解释性问题日益受到关注。尽管已经有许多后验和先验的可解释性方法被提出，但对于学习到的表征的系统性和客观评估仍然有限，特别是对于那些被认为可以“诱导可解释性”的稀疏感知方法。

Method: 本文聚焦于概念瓶颈模型，探索了不同建模决策如何影响所形成的表征。引入了‘清晰度’这一概念，它捕捉了下游性能与概念表征的稀疏性和准确性之间的相互作用，并提出了一种使用具有真实概念注释的数据集来进行可解释性评估的框架。考虑了基于VLM和属性预测器的CBMs，以及三种不同的稀疏诱导策略：每个样本的$\ell_1$、$\ell_0$和伯努利基础公式。

Result: 实验表明，在灵活性与可解释性之间存在着一个关键的权衡。即使是在性能相当的情况下，给定的方法也可能表现出显著不同的行为。

Conclusion: 本研究表明，选择适当的稀疏诱导策略对于平衡模型的灵活性和可解释性至关重要。通过提出的清晰度指标及评估框架，为理解不同方法下概念瓶颈模型的表现提供了新的视角。

Abstract: The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to "induce interpretability". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.

</details>


### [112] [Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities](https://arxiv.org/abs/2601.21950)
*Linxiao Gong,Yang Liu,Lianlong Sun,Yulai Bi,Jing Liu,Xiaoguang Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，即Aleatoric Uncertainty Modeling (AUM)，用于处理医学多模态学习中的缺失模态问题。通过将每个单模态表示建模为多元高斯分布，并利用不确定性感知聚合机制在二分患者-模态图中进行动态消息传递，该框架能够自然地适应缺失模态的情况，同时更加强调可用模态中的可靠信息。实验结果显示，在MIMIC-IV死亡率预测和eICU数据集上，该方法相比现有技术有显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 医学多模态学习面临的一个主要挑战是临床实践中普遍存在的模态缺失问题。现有的方法通常假设每种模态对结果的贡献相等且缺失模式随机，忽略了医学数据获取过程中固有的不确定性。因此，需要一种能够量化单模态aleatoric不确定性的方法来更好地处理缺失模态的问题。

Method: 提出了Aleatoric Uncertainty Modeling (AUM) 方法，它将每个单模态表示视为一个多元高斯分布，以捕捉aleatoric不确定性并实现模态可靠性量化。此外，还开发了一种基于不确定性意识聚合机制的动态消息传递机制，用于在一个二分的患者-模态图内自适应地整合捕获的信息。

Result: AUM框架在MIMIC-IV死亡率预测任务上实现了2.26% AUC-ROC的改进，在eICU数据集上获得了2.17%的增益，超过了当前最先进的方法。

Conclusion: 通过引入AUM框架，研究证明了在面对医学多模态数据中常见的缺失模态时，采用能够显式量化单模态aleatoric不确定性的策略可以有效提高模型性能。

Abstract: Medical multimodal learning faces significant challenges with missing modalities prevalent in clinical practice. Existing approaches assume equal contribution of modality and random missing patterns, neglecting inherent uncertainty in medical data acquisition. In this regard, we propose the Aleatoric Uncertainty Modeling (AUM) that explicitly quantifies unimodal aleatoric uncertainty to address missing modalities. Specifically, AUM models each unimodal representation as a multivariate Gaussian distribution to capture aleatoric uncertainty and enable principled modality reliability quantification. To adaptively aggregate captured information, we develop a dynamic message-passing mechanism within a bipartite patient-modality graph using uncertainty-aware aggregation mechanism. Through this process, missing modalities are naturally accommodated, while more reliable information from available modalities is dynamically emphasized to guide representation generation. Our AUM framework achieves an improvement of 2.26% AUC-ROC on MIMIC-IV mortality prediction and 2.17% gain on eICU, outperforming existing state-of-the-art approaches.

</details>


### [113] [Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fréchet Inception Distance](https://arxiv.org/abs/2601.21979)
*Ciaran Bench,Vivek Desai,Carlijn Roozemond,Ruben van Engen,Spencer A. Thomas*

Main category: cs.LG

TL;DR: 该论文通过使用蒙特卡洛dropout计算FID的预测方差以及特征嵌入模型潜在表示的预测方差估计，来评估FID度量在医疗图像应用中的有效性。研究发现预测方差大小与测试输入相对于训练数据偏离分布的程度之间存在不同程度的相关性，为FID作为可信度指标的有效性提供了见解。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练模型获得的特征嵌入广泛应用于深度学习的医学领域，特别是用于评估合成医学图像的质量，但常用的FID质量度量基于InceptionV3模型（在自然图像上预训练）可能不适用于医学图像。本文旨在探索FID在多大程度上未能捕捉到图像特征的有意义差异，并考察其作为医学图像质量评价指标的有效性。

Method: 采用蒙特卡洛dropout技术计算FID分数的预测方差，并补充估算特征嵌入模型潜在表示的预测方差。通过分析这些预测方差与测试输入（包括不同强度增强后的ImageNet1K验证集及其他外部数据集）相对于训练数据的偏离分布程度之间的相关性，来探讨FID及其背后特征编码对于不同类型图像数据的适用性。

Result: 研究表明，所考虑的预测方差幅度与测试输入相对于其训练数据偏离分布的程度表现出不同程度的相关性，这为理解FID作为衡量合成图像质量及其它特征表示方法可靠性指示器的有效性提供了新的视角。

Conclusion: 通过本研究提供的证据表明，FID及其依赖的特征编码方法在处理非自然图像尤其是医学图像时可能不够可靠。预测方差分析为进一步了解FID度量局限性提供了一种手段，并暗示了在特定应用场景下选择或开发更合适的图像质量评估方法的重要性。

Abstract: Feature embeddings acquired from pretrained models are widely used in medical applications of deep learning to assess the characteristics of datasets; e.g. to determine the quality of synthetic, generated medical images. The Fréchet Inception Distance (FID) is one popular synthetic image quality metric that relies on the assumption that the characteristic features of the data can be detected and encoded by an InceptionV3 model pretrained on ImageNet1K (natural images). While it is widely known that this makes it less effective for applications involving medical images, the extent to which the metric fails to capture meaningful differences in image characteristics is not obviously known. Here, we use Monte Carlo dropout to compute the predictive variance in the FID as well as a supplemental estimate of the predictive variance in the feature embedding model's latent representations. We show that the magnitudes of the predictive variances considered exhibit varying degrees of correlation with the extent to which test inputs (ImageNet1K validation set augmented at various strengths, and other external datasets) are out-of-distribution relative to its training data, providing some insight into the effectiveness of their use as indicators of the trustworthiness of the FID.

</details>


### [114] [PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters](https://arxiv.org/abs/2601.21984)
*Jian Gao,Yiwei Zou,Abhishek Pradhan,Wenhao Huang,Yumin Su,Kaiyuan Yang,Xuan Zhang*

Main category: cs.LG

TL;DR: PowerGenie, a new framework for the automated discovery of high-performance reconfigurable power converters, uses an analytical approach to evaluate converter performance and an evolutionary fine-tuning method to improve design. It outperforms existing methods in terms of validity, novelty, and efficiency, discovering a new 8-mode reconfigurable converter with significantly higher FoM and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current AI methods in discovering superior circuit topologies, which are either confined to choosing from pre-existing templates or can only generate new designs at a small scale without comprehensive validation. The goal is to explore large-scale, performance-driven discovery of reconfigurable power converters.

Method: PowerGenie utilizes an automated analytical framework to assess the functionality and theoretical performance limits of power converters without the need for component sizing or SPICE simulation. Additionally, it employs an evolutionary fine-tuning method that co-evolves a generative model alongside its training data distribution through a process of fitness-based selection and uniqueness verification.

Result: The results show that PowerGenie achieves better syntax validity, function validity, novelty rate, and figure-of-merit (FoM) compared to other methods. A novel 8-mode reconfigurable converter was discovered with a 23% higher FoM than the best topology in the training set, and SPICE simulations confirmed average absolute efficiency gains of 10% across all modes, with up to 17% improvement in a single mode.

Conclusion: PowerGenie demonstrates significant advancements in the automated discovery of reconfigurable power converters, offering a scalable solution with improved performance metrics. The discovered converter showcases the potential for substantial improvements in efficiency and FoM, indicating a promising direction for future research and development.

Abstract: Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at https://github.com/xz-group/PowerGenie.

</details>


### [115] [Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields](https://arxiv.org/abs/2601.21985)
*Yunyang Li,Lin Huang,Luojia Xia,Wenhe Zhang,Mark Gerstein*

Main category: cs.LG

TL;DR: Elign是一个后训练框架，通过使用预训练的机器学习力场替代昂贵的DFT评估，并将物理引导移至训练阶段来减少计算成本。该方法采用FED-GRPO算法优化去噪策略，在保持生成速度的同时提高了分子构象的能量稳定性和结构稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的E(3)-等变扩散模型在生成三维分子构象时往往重现半经验训练数据中的偏差，而不是捕捉高保真哈密顿量的平衡分布。虽然基于物理的方法可以纠正这一点，但面临着量子化学评估成本高昂以及每一步采样都需要重复查询的问题。

Method: 提出了Elign框架，首先用预训练的基础机器学习力场（MLFF）替换昂贵的DFT评估以提供物理信号；其次，通过将物理导向转移到训练阶段消除了运行时重复查询的需求。为了实现第二个摊销，研究者将反向扩散形式化为强化学习问题，并引入了Force-Energy Disentangled Group Relative Policy Optimization (FED-GRPO) 来微调去噪策略。

Result: 实验表明，与未引导的采样相比，Elign能够生成具有更低标准DFT能量和力值的构象，同时改善了稳定性。重要的是，在生成过程中不需要进行能量评估，因此推断速度保持不变。

Conclusion: Elign通过结合预训练的机器学习力场和创新性的FED-GRPO算法，在不牺牲生成效率的前提下有效提升了3D分子构象生成的质量。

Abstract: Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.

</details>


### [116] [Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains](https://arxiv.org/abs/2601.21999)
*Meng Cao,Jiexi Liu,Songcan Chen*

Main category: cs.LG

TL;DR: 本文针对不平衡领域泛化（IDG）问题，提出了一种新的负主导对比学习方法（NDCL），通过增强跨领域的后验一致性、加强类间决策边界分离和鼓励类内紧凑性来提高模型在异构长尾分布下的泛化能力。实验结果表明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 不平衡领域泛化（IDG）旨在减轻域迁移和标签偏移的影响，这两个因素从根本上决定了模型的决策边界，特别是在跨域存在异构长尾分布的情况下尤为重要。尽管其实际意义重大，但因技术复杂性和理论基础缺乏而未得到充分探索。

Method: 论文首先从理论上建立了IDG的一般化界限，强调了后验差异与决策边际的作用，并据此提出了一个新的名为负主导对比学习（NDCL）的方法，以直接引导决策边界的调整。该方法通过加大负面样本的重要性来增强类间决策边界分离，同时采用重新加权交叉熵策略促进类内紧凑性及跨域预测中心对齐策略保证后验一致性。

Result: 通过在基准测试上进行严格且具有挑战性的实验验证了所提NDCL方法的有效性。

Conclusion: 本研究为解决不平衡领域泛化问题提供了新思路，即通过直接操控决策边界而非传统手段，并证明了所提出的NDCL方法能够有效提升模型在面对异构长尾分布时的表现。

Abstract: Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.

</details>


### [117] [Rate-Distortion Optimization for Transformer Inference](https://arxiv.org/abs/2601.22002)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 本文提出了一种基于率失真理论的框架，用于Transformer模型在推理过程中的中间表示压缩，实现了比特率和准确度之间的权衡。实验表明该方法可以在某些情况下提高准确性的同时大幅节省资源，并且超越了更复杂的基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于Transformers在许多任务上虽然表现优异，但在推理过程中需要大量的计算和内存资源。为了提高效率，可以通过跨多个设备分割处理来实现这一点，这又要求压缩其间的表示。

Method: 引入了一个基于率-失真原理的框架，该框架学习紧凑的编码方式，以明确地在比特率和准确性之间进行权衡。

Result: 实验显示，在语言基准测试中，所提出的编解码器达到了显著的节省效果，有时甚至提高了准确性，优于其他更复杂的基础方法。此外，研究还表征并分析了transformers的率-失真性能，为理解表示编码中的性能提供了一个统一视角。

Conclusion: 通过扩展信息论概念定义了比特率与熵之间的差距，并推导出了一些界限。进一步开发了可能近似正确（PAC）风格的界限来估计这个差距。对于不同的架构和任务，实验证明它们的比率受到这些界限的影响，增加了公式的可解释性。

Abstract: Transformers achieve superior performance on many tasks, but impose heavy compute and memory requirements during inference. This inference can be made more efficient by partitioning the process across multiple devices, which, in turn, requires compressing its intermediate representations. In this work, we introduce a principled rate-distortion-based framework for lossy compression that learns compact encodings that explicitly trade off bitrate against accuracy. Experiments on language benchmarks show that the proposed codec achieves substantial savings with improved accuracy in some cases, outperforming more complex baseline methods. We characterize and analyze the rate-distortion performance of transformers, offering a unified lens for understanding performance in representation coding. This formulation extends information-theoretic concepts to define the gap between rate and entropy, and derive some of its bounds. We further develop probably approximately correct (PAC)-style bounds for estimating this gap. For different architectures and tasks, we empirically demonstrate that their rates are driven by these bounds, adding to the explainability of the formulation.

</details>


### [118] [Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering](https://arxiv.org/abs/2601.22010)
*Dongxuan Zhu,Ly Tran Ho Khanh,Andy Yat-Ming Cheung,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种名为STARS的新方法，该方法在不牺牲生成质量的前提下，通过优化Stiefel流形上的激活向量来增加语言模型生成路径的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型往往生成同质化的输出，并且容易陷入模式崩溃。尽管基于采样的策略可以引入随机性，但它们仍难以保证多次并发生成运行之间的多样性。

Method: STARS是一种无需训练、仅在推理时进行干预的方法，它将激活引导转化为探索引擎。在每个标记处，STARS收集并发生成运行的隐藏激活，并在Stiefel流形上共同优化多个加法引导方向。通过最大化被引导激活的几何体积并诱导引导干预的正交性，STARS促进了并发生成运行之间发散的激活向量和生成轨迹。此外，还设计了一个轻量级单步更新机制以确保低延迟。

Result: 在测试用例生成和科学发现基准测试中，STARS持续优于标准采样方法，在不牺牲定性性能的情况下实现了更大的多样性。

Conclusion: STARS提供了一种有效的方法来提高语言模型输出的多样性，同时保持良好的生成质量。

Abstract: Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.

</details>


### [119] [TBDFiltering: Sample-Efficient Tree-Based Data Filtering](https://arxiv.org/abs/2601.22016)
*Robert Istvan Busa-Fekete,Julian Zimmert,Anne Xiangyi Zheng,Claudio Gentile,Andras Gyorgy*

Main category: cs.LG

TL;DR: 本文提出了一种基于文本嵌入的分层聚类方法，用于自适应选择由大型语言模型评估质量的文档。该方法在查询效率方面表现优异，并且在实验研究中证明了其相对于其他基于分类器的过滤方法的优势。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏廉价可靠的品质度量标准，为大型语言模型挑选高质量、多样化的训练集是一个难题。虽然可以通过询问现有的大型语言模型来获取文档的质量信息，但这对于训练中使用的大量（数十亿）文档来说并不实用。

Method: 提出了一种基于文本嵌入的分层聚类方法，该方法可以自适应地选择需要被大型语言模型评价质量的文档，从而估计簇的质量。

Result: 理论分析表明，在假设分层聚类包含一个子树使得每个叶簇足够纯净的情况下，该方法能够以高概率正确预测每个文档的质量，而只需查询少量文档。此外，通过全面的实验研究表明，本算法相较于其他基于分类器的筛选方法具有优势。

Conclusion: 提出的方法不仅在理论上证明了其查询效率，而且实验证明了它在实际应用中的有效性，为大规模文档集合的质量评估提供了一个新的解决方案。

Abstract: The quality of machine learning models depends heavily on their training data. Selecting high-quality, diverse training sets for large language models (LLMs) is a difficult task, due to the lack of cheap and reliable quality metrics. While querying existing LLMs for document quality is common, this is not scalable to the large number (billions) of documents used in training. Instead, practitioners often use classifiers trained on sparse quality signals. In this paper, we propose a text-embedding-based hierarchical clustering approach that adaptively selects the documents to be evaluated by the LLM to estimate cluster quality. We prove that our method is query efficient: under the assumption that the hierarchical clustering contains a subtree such that each leaf cluster in the tree is pure enough (i.e., it mostly contains either only good or only bad documents), with high probability, the method can correctly predict the quality of each document after querying a small number of documents. The number of such documents is proportional to the size of the smallest subtree with (almost) pure leaves, without the algorithm knowing this subtree in advance. Furthermore, in a comprehensive experimental study, we demonstrate the benefits of our algorithm compared to other classifier-based filtering methods.

</details>


### [120] [Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2601.22020)
*Chengyi Cai,Zesheng Ye,Peike Li,Bo Han,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 本文提出了一种针对多模态大型语言模型(MLLMs)的遗忘方法，称为视觉引导的关键令牌正则化(ViKeR)，通过利用无关的视觉输入预测理想的后遗忘令牌级分布，并使用这些分布来规范遗忘过程，从而优先考虑关键令牌。实验表明，该方法在执行遗忘的同时减轻了遗忘并保持了响应的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs遗忘方法主要采用为LLMs开发的方法，它们将所有答案令牌统一处理，忽略了它们在遗忘过程中重要性的差异，并且仅关注语言模式而忽略了指示答案中关键令牌的视觉线索。

Method: 提出了视觉引导的关键令牌正则化(ViKeR)方法，利用不相关的视觉输入预测理想的后遗忘令牌级别分布，并用这些分布来规范遗忘过程，同时通过信息熵定义遗忘中的关键令牌，并通过令牌级别的梯度重加权放大关键令牌上的更新。

Result: 在MLLMU和CLEAR基准测试上的实验表明，所提出的方法能够有效地执行遗忘，同时缓解了遗忘现象并保持了响应的一致性。

Conclusion: ViKeR方法为多模态大型语言模型提供了一种有效执行遗忘的新途径，它不仅考虑到了视觉信息的重要性，还通过对关键令牌进行特别处理提升了整体性能。

Abstract: Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.

</details>


### [121] [From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning](https://arxiv.org/abs/2601.22028)
*Haoran Tang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 本文提出了一种名为CLReg的对比表示正则化方法，该方法可以在最小化对保留特征影响的同时减少遗忘-保留特征之间的纠缠，从而改善大语言模型中的遗忘内容移除问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数大语言模型（LLM）的遗忘方法通过在预测空间中定义的目标来近似从头开始重新训练的行为，以尽量减少分布偏移。然而这种方法可能只是抑制了被遗忘的概念生成，而这些概念仍然存在于表示中并与保留的知识纠缠在一起。

Method: 引入了CLReg，一种对比表示正则化器，它能够识别出需要遗忘的特征，并将它们与需要保留的特征分开，显式地减少了遗忘-保留干扰，同时对于保留特征的影响降到最低。

Result: 在不同的遗忘基准测试和不同规模的语言模型上，CLReg有效地降低了遗忘-保留表征纠缠，支持主流遗忘方法而不增加额外隐私风险。

Conclusion: CLReg为通过重塑表征空间去除遗忘概念提供了新的思路，且不会带来额外的隐私风险，这为未来研究如何更有效地处理LLM中的遗忘问题指明了方向。

Abstract: Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.

</details>


### [122] [The Ensemble Inverse Problem: Applications and Methods](https://arxiv.org/abs/2601.22029)
*Zhengyan Huan,Camila Pazos,Martin Klassen,Vincent Croft,Pierre-Hugues Beauchemin,Shuchin Aeron*

Main category: cs.LG

TL;DR: 本文提出了一种新的多元统计问题，即集合逆问题（EIP），并开发了基于新型条件生成模型的非迭代推理时间方法来解决这一问题。通过在训练过程中使用多组真值-观测对，这些方法避免了在推理时显式和迭代地使用前向模型，并且能够利用观测集中的集合信息来进行后验推断，进而推广到未见先验。该方法在合成数据集、真实数据集以及不同领域的应用中进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 为了解决高能物理（HEP）中的展开问题，以及全波形反演（FWI）和未知先验下的逆成像等场景中存在的集合逆问题（EIP）。现有方法通常需要在推理阶段明确及反复运用前向模型，而作者希望提出一种新方法可以避免这一点。

Method: 提出了基于一类新的条件生成模型——集合逆生成模型的非迭代推理时间方法。这类模型不仅使用单次测量的信息，还额外利用了观测集中包含的集合信息来进行后验建模。通过跨多个与相同前向模型一致但来自广泛范围先验的真实-观测配对集进行训练，从而隐式编码似然模型。

Result: 实验结果表明，所提出的方法能够在逆成像、HEP和FWI等多个领域的人造和真实数据集上有效工作。此外，使用集合信息有助于后验推断，并允许方法泛化至之前未遇到过的先验情况。

Conclusion: 本研究成功地引入了集合逆问题的概念，并开发了相应的解决方案，该方案不依赖于传统意义上在推理阶段直接重复使用前向模型的方式。这为处理HEP、FWI及其他领域内的类似挑战提供了新的视角和技术手段。

Abstract: We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at https://github.com/ZhengyanHuan/The-Ensemble-Inverse-Problem--Applications-and-Methods.

</details>


### [123] [Per-parameter Task Arithmetic for Unlearning in Large Language Models](https://arxiv.org/abs/2601.22030)
*Chengyi Cai,Zesheng Ye,Jiangchao Yao,Jianzhong Qi,Bo Han,Xiaolu Zhang,Feng Liu,Jun Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于参数的任务算术（PerTA）机制，用于调整任务向量（TV），以在移除大型语言模型中的私有信息时减少过度遗忘的问题。通过使用梯度或对角Fisher信息近似来量化每个参数对于遗忘和保留的重要性，从而实现更有效的信息移除同时保持模型的实用性。实验结果表明，与标准任务算术和其他训练基删除方法相比，PerTA在遗忘效果和模型整体效用上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型的信息删除过程中，传统的方法如任务算术可能引起过度遗忘，影响到模型中其他重要信息的保留。因此，作者们受到启发，认为每个参数对于遗忘和保留的重要性不同，提出了一个新的解决方案以解决这个问题。

Method: 提出了一种称为PerTA的新方法，该方法通过对任务向量进行逐参数调整来改善信息删除过程。调整权重是基于每个参数对于遗忘与保留相对重要性的评估，这可以通过梯度(PerTA-grad)或者对角Fisher信息近似(PerTA-fisher)来完成。

Result: 广泛的实验证明了PerTA相较于传统的任务算术方法以及广泛使用的基于训练的删除方法，在遗忘效率和保持模型整体功能方面均有所改进。

Conclusion: PerTA提供了一个既原则又实用的大规模语言模型信息删除框架，它不仅保留了任务算术方法的效率优势，还有效缓解了过度遗忘问题。

Abstract: In large language model (LLM) unlearning, private information is required to be removed. Task arithmetic unlearns by subtracting a specific task vector (TV)--defined as the parameter difference between a privacy-information-tuned model and the original model. While efficient, it can cause over-forgetting by disrupting parameters essential for retaining other information. Motivated by the observation that each parameter exhibits different importance for forgetting versus retention, we propose a per-parameter task arithmetic (PerTA) mechanism to rescale the TV, allowing per-parameter adjustment. These weights quantify the relative importance of each parameter for forgetting versus retention, estimated via gradients (i.e., PerTA-grad) or the diagonal Fisher information approximation (i.e., PerTA-fisher). Moreover, we discuss the effectiveness of PerTA, extend it to a more general form, and provide further analysis. Extensive experiments demonstrate that PerTA consistently improves upon standard TV, and in many cases surpasses widely used training-based unlearning methods in both forgetting effectiveness and overall model utility. By retaining the efficiency of task arithmetic while mitigating over-forgetting, PerTA offers a principled and practical framework for LLM unlearning.

</details>


### [124] [Holographic generative flows with AdS/CFT](https://arxiv.org/abs/2601.22033)
*Ehsan Mirafzali,Sanjit Shashi,Sanya Murdeshwar,Edgar Shaghoulian,Daniele Venturi,Razvan Marinescu*

Main category: cs.LG

TL;DR: 该论文提出了一种生成机器学习框架，利用反德西特/共形场论(AdS/CFT)对应关系来表示从基分布到学习分布的数据流。通过在玩具数据集和MNIST上的实验表明，与没有物理背景的流匹配模型相比，该模型实现了更快且更高质量的收敛。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过结合量子引力的全息原理（具体来说是反德西特/共形场论(AdS/CFT)对应）与深度学习及传输理论的技术，开发出一种新的生成式建模方法。

Method: 使用了AdS空间中的标量场从内部到边界的映射来模拟数据从一个基本分布流向学习得到的新分布的过程，并将此过程与流匹配算法相结合。

Result: 实验结果显示，在处理像棋盘格玩具数据集和MNIST这样的数据时，所提出的方法比那些不包含物理信息的传统流匹配模型具有更快以及更好的收敛性能。

Conclusion: 这项工作不仅提供了一个物理上可解释版本的流匹配技术，而且证明了AdS物理学及其几何结构在开发新型生成模型范例方面的实用性。

Abstract: We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.

</details>


### [125] [Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space](https://arxiv.org/abs/2601.22036)
*Xiaolong Zhang,Jianwei Zhang,Xubo Song*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量方法——交叉融合距离(CFD)，用于量化表示空间中数据组之间的融合程度和可分离性。CFD能够区分影响融合的几何位移与不影响融合的因素如全局缩放等，具有线性计算复杂度，并在理论和实验上证明了其不变性和敏感性属性。


<details>
  <summary>Details</summary>
Motivation: 现有分布距离度量混淆了改变融合程度的因素（如表示组间的几何位移）与保持融合状态的因素（例如全局缩放和采样引起的布局变化），导致这些度量无法准确反映数据组之间真实的融合程度。

Method: 引入了名为交叉融合距离(CFD)的新度量标准，它旨在隔离影响融合的几何因素同时对那些不改变融合状态的变化保持鲁棒性，并且具备线性的计算复杂度。

Result: 通过理论分析和合成实验验证了CFD的不变性和敏感性特性；实际应用显示，在存在域偏移的真实世界数据集上，相比于常用替代方案，CFD更紧密地反映了下游泛化性能下降的情况。

Conclusion: CFD为表征学习提供了一个有理论基础且易于解释的距离度量方法。

Abstract: Quantifying degrees of fusion and separability between data groups in representation space is a fundamental problem in representation learning, particularly under domain shift. A meaningful metric should capture fusion-altering factors like geometric displacement between representation groups, whose variations change the extent of fusion, while remaining invariant to fusion-preserving factors such as global scaling and sampling-induced layout changes, whose variations do not. Existing distributional distance metrics conflate these factors, leading to measures that are not informative of the true extent of fusion between data groups. We introduce Cross-Fusion Distance (CFD), a principled measure that isolates fusion-altering geometry while remaining robust to fusion-preserving variations, with linear computational complexity. We characterize the invariance and sensitivity properties of CFD theoretically and validate them in controlled synthetic experiments. For practical utility on real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. Overall, CFD provides a theoretically grounded and interpretable distance measure for representation learning.

</details>


### [126] [Latent Adversarial Regularization for Offline Preference Optimization](https://arxiv.org/abs/2601.22083)
*Enyi Jiang,Yibo Jacky Zhang,Yinglun Xu,Andreas Haupt,Nancy Amato,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出了一种名为GANPO的方法，通过在语言模型偏好优化中引入潜空间正则化来提高模型的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的从人类反馈中学习的方法依赖于偏好优化，并通过令牌级别的正则化来限制策略更新。然而，对于语言模型来说，令牌空间的相似性并不意味着语义或行为上的相似性，这使得偏好优化特别具有挑战性。

Method: 提出了GANPO方法，该方法通过对策略模型和参考模型内部表示之间的差异进行惩罚来实现潜空间正则化。由于潜表示没有明确的概率密度关联，研究者采用了受GAN启发的对抗方法来最小化潜空间差异。GANPO作为正则项被整合进现有的离线偏好优化目标中。

Result: 跨多种模型架构和任务的实验表明，潜空间正则化带来了持续的改进。与令牌级别正则化相比，在分布偏移和噪声条件下，GANPO提供了更稳健的结构反馈，同时保持了可比的下游性能，且仅有轻微的计算开销增加。

Conclusion: 研究表明，通过采用GANPO这样的潜空间正则化技术可以有效地增强语言模型对偏好优化的学习效果，尤其是在面对数据分布变化时表现出更好的鲁棒性。

Abstract: Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.

</details>


### [127] [Boosting CVaR Policy Optimization with Quantile Gradients](https://arxiv.org/abs/2601.22100)
*Yudong Luo,Erick Delage*

Main category: cs.LG

TL;DR: 本文提出了一种通过增加预期分位数项来优化CVaR-PG的方法，以解决其样本效率低下的问题。新方法利用了所有采样数据，并在具有可验证风险规避行为的领域中表现优于现有的CVaR-PG和其他方法。


<details>
  <summary>Details</summary>
Motivation: 条件风险价值（CVaR）使用策略梯度进行优化时面临样本效率低的问题，因为该方法只关注尾部性能而忽略了大量样本轨迹。

Method: 研究者通过向CVaR添加一个预期分位数项来解决这个问题。由于分位数优化允许动态规划公式化，这使得能够利用所有的采样数据，从而提高样本效率。

Result: 实证结果显示，在需要表现出明显风险规避行为的应用场景下，所提出的算法在马尔科夫决策过程中显著优于CVaR-PG，并且持续超越其他现存方法。

Conclusion: 通过引入额外的预期分位数优化到CVaR目标中，可以有效提升学习过程中的样本利用率而不改变原有CVaR目标。这种方法对于需要处理极端情况但又希望保持良好平均性能的任务特别有用。

Abstract: Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. Empirical results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.

</details>


### [128] [Prior-Informed Flow Matching for Graph Reconstruction](https://arxiv.org/abs/2601.22107)
*Harvey Chen,Nicolas Zilberstein,Santiago Segarra*

Main category: cs.LG

TL;DR: 本文提出了一种用于图重建的条件流模型Prior-Informed Flow Matching (PIFM)，该方法通过结合基于嵌入的先验知识与连续时间流匹配，解决了从部分观察中重建图时面临的挑战。实验表明，PIFM在重建准确性上优于经典嵌入方法和最先进的生成基线。


<details>
  <summary>Details</summary>
Motivation: 从部分观测数据中重建图形是一个主要挑战；传统的嵌入方法往往缺乏全局一致性，而现代生成模型难以融入结构先验信息。

Method: 引入了Prior-Informed Flow Matching (PIFM)，一种将基于嵌入的先验（如graphons或GraphSAGE/node2vec）与连续时间流匹配相结合的方法。首先利用先验形成邻接矩阵的初始估计，然后应用修正后的流匹配来优化这一估计，使其趋向于干净图的真实分布，并学习到一个全局耦合。

Result: 不同数据集上的实验显示，PIFM能够一致地增强经典嵌入方法的效果，在重建准确性方面超越了这些方法以及当前最先进的生成模型基准。

Conclusion: 通过融合基于嵌入的先验知识与连续时间流匹配技术，PIFM为图重建提供了一个有效的新途径，显著提高了从部分观察中恢复图结构的能力。

Abstract: We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.

</details>


### [129] [Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics](https://arxiv.org/abs/2601.22123)
*Winfried Ripken,Michael Plainer,Gregor Lied,Thorben Frank,Oliver T. Unke,Stefan Chmiela,Frank Noé,Klaus Robert Müller*

Main category: cs.LG

TL;DR: 本文提出了一种通过预测选定时间跨度内的平均相空间演化来学习哈密顿流映射的方法，从而允许使用更大的积分时间步长进行稳定更新。


<details>
  <summary>Details</summary>
Motivation: 模拟哈密顿系统的长时间演化受到数值积分所需小时间步长的限制。为了解决这个问题，作者们引入了一个新的框架，该框架能够支持远超传统积分器稳定性限制的大时间步长更新。

Method: 通过施加一个针对时间平均哈密顿动力学的均值流一致性条件，使得训练过程可以基于独立的相空间样本而无需访问未来状态，避免了昂贵的轨迹生成过程。

Result: 在不同的哈密顿系统中验证了方法的有效性，并且特别地改进了利用机器学习力场（MLFF）的分子动力学模拟。提出的模型保持了相当的训练和推理成本，但支持在直接使用广泛可用的无轨迹MLFF数据集上训练时使用显著更大的积分时间步长。

Conclusion: 所提出的学习哈密顿流映射的方法不仅提高了分子动力学模拟效率，还降低了对于特定类型数据集的需求，展示了其在更广泛应用中的潜力。

Abstract: Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.

</details>


### [130] [Discovering Hidden Gems in Model Repositories](https://arxiv.org/abs/2601.22157)
*Jonathan Kahana,Eliahu Horwitz,Yedid Hoshen*

Main category: cs.LG

TL;DR: 研究发现公共仓库中存在大量未被充分利用但性能优越的微调模型，通过将模型发现过程建模为多臂赌博机问题并优化搜索算法，显著提高了发现这些优秀模型的效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索为何社区使用集中在少数基础检查点上，并试图揭示是否还有更优秀的模型未被充分发掘。

Method: 通过评估超过2000个模型来识别出性能优于流行模型的‘隐藏宝石’，并将模型发现过程建模为一个多臂赌博机问题，采用改进的顺序减半搜索算法加速模型发现。

Result: 在Llama-3.1-8B系列中找到了能够大幅提升数学表现至96.0%的不受欢迎模型；提出的方法只需对每个候选者进行50次查询就能找到顶级模型，速度提升了50倍以上。

Conclusion: 研究表明，通过有效策略可以发现大量未被充分利用但性能极佳的微调模型，这为提高现有模型库的价值提供了新途径。

Abstract: Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [131] [Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems](https://arxiv.org/abs/2601.21146)
*Francesco Paladino,Shulu Li,Edward A. Lee*

Main category: cs.DC

TL;DR: 本文提出了一种名为maxwait的简单协调机制，它可以在分布式时间敏感系统中明确并可配置地平衡时间要求和一致性。该机制不仅涵盖了经典的分布式系统方法，还能实现多种常用的分布式系统模式，并且在单一语义框架内提供了更好的时间控制、有界时间故障检测以及更高的确定性。


<details>
  <summary>Details</summary>
Motivation: 在通信延迟和同步不确定性存在的情况下，分布式时间敏感系统必须平衡时间需求（可用性）与一致性。为了应对这一挑战，需要一种能够明确表达这些权衡关系并且可配置的解决方案。

Method: 提出了maxwait机制，这是一种通用的协调方案，它统一了多种经典分布式系统方法如PTIDES、Chandy-and-Mrisa算法及其变体、Jefferson的时间扭曲理论及Lamport基于时间的错误检测技术等。此外，maxwait还支持逻辑执行时间(LET)、发布订阅模式、参与者模型、无冲突复制数据类型(CRDTs)以及带有未来值的远程过程调用等多种常用分布式系统模式。

Result: 通过扩展Lingua Franca协调语言实现了maxwait机制。当通信延迟受到限制时，maxwait确保逻辑时间的一致性；当边界被违反时，则提供结构化的故障处理方式。结果表明，这种机制不仅能够继承现有方法的优点，而且为它们添加了更优的时间控制能力、有限时间内的故障检测功能以及提高其确定性的选项。

Conclusion: maxwait作为一种新颖而通用的协调机制，在保持或改进已有分布式系统技术性能的同时，为解决时间敏感型应用中的可用性和一致性问题提供了新的可能性。

Abstract: Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

</details>


### [132] [Ira: Efficient Transaction Replay for Distributed Systems](https://arxiv.org/abs/2601.21286)
*Adithya Bhat,Harshal Bhadreshkumar Shah,Mohsen Minaei*

Main category: cs.DC

TL;DR: 提出了Ira框架，通过在事务批处理中传输紧凑的提示来加速备份重放，特别针对以太坊块执行进行了优化。实验表明，与现有的以太坊客户端reth相比，Ira-L能够显著提高备份节点的块重放速度。


<details>
  <summary>Details</summary>
Motivation: 在主备复制中，共识延迟受到备份节点重放由主节点提议的事务所需时间的限制。本文旨在通过利用主节点已知的信息来减少这种延迟，特别是通过提供关于未来访问模式的提示来优化备份节点上的缓存管理。

Method: 开发了名为Ira的框架，该框架允许主节点在发送给备份节点的事务批处理中包含简洁的提示信息。具体地，在研究案例中使用了以太坊，并基于此框架提出了一个具体的协议Ira-L，用于改善以太坊区块执行过程中的缓存管理。这些提示包含了每个区块内使用的键的工作集以及指示从哪个表读取数据的一字节元数据。

Result: Ira-L相对于最先进的以太坊客户端reth在两周内的以太坊主网活动（包含超过2400万笔交易的100,800个区块）上进行了评估。结果显示，尽管对主节点造成了约28.6%的额外时间开销（其中直接因生成提示而产生的成本占执行时间的10.9%，这部分可以通过流水线和并行化来缓解），但Ira-L使得备份节点实现了每区块平均25倍的速度提升。当使用16个预取线程时，总的重放时间从6.5小时减少到了16分钟。

Conclusion: Ira框架及其特定实现Ira-L证明了通过为主节点提供有关未来访问模式的知识可以有效加快备份节点上事务的重放速度，从而减少了主备复制中的共识延迟。此外，实验结果展示了即使增加了一定的主节点开销，也能显著提高系统整体性能。

Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).

</details>


### [133] [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)
*Yuchen Yang,Yaru Zhao,Pu Yang,Shaowei Wang,Zhi-Hua Zhou*

Main category: cs.DC

TL;DR: 提出了ZipMoE，一种高效的、无语义损失的设备上Mixture-of-Experts（MoE）服务系统，通过缓存-调度协同设计利用边缘设备硬件特性和MoE参数中的统计冗余，从而显著降低推理延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 虽然Mixture-of-Experts（MoE）架构大大增强了大型语言模型的表现力，但其巨大的内存占用严重阻碍了在资源受限的边缘设备上的实际部署，尤其是在不依赖有损量化的情况下保持模型行为时。

Method: 通过提出ZipMoE，一个高效且无语义损失的设备上MoE服务系统，它利用边缘设备硬件属性与MoE参数内在统计冗余之间的协同效应，并采用具有性能保证的缓存-调度协同设计来解决上述问题。

Result: 实验结果表明，相比于现有最先进系统，ZipMoE能够实现高达72.77%的推理延迟减少以及最高达6.76倍的吞吐量提升。

Conclusion: ZipMoE为在资源受限的边缘设备上部署大规模MoE模型提供了一种有效的方法，同时保持了良好的性能指标如低延迟和高吞吐量。

Abstract: While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\%$ inference latency reduction and up to $6.76\times$ higher throughput than the state-of-the-art systems.

</details>


### [134] [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)
*Bronislav Sidik,Chaya Levi,Joseph Kampeas*

Main category: cs.DC

TL;DR: 本研究提出了一种名为EWSJF的自适应请求级调度器，用于在混合工作负载下服务大型语言模型，能够同时提高公平性和吞吐量。与传统的先来先服务（FCFS）策略相比，EWSJF通过实时学习工作负载结构显著提升了处理效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 当前，在处理混合工作负载时——包括短小且对延迟敏感的交互查询以及长时间运行、更注重吞吐量的批量请求——采用标准的先来先服务(FCFS)策略存在严重的问题，如头部阻塞导致高尾部延迟和硬件利用率低下。因此，需要一种新的方法来解决这些问题，同时提高系统的公平性和吞吐能力。

Method: EWSJF是一种基于有效工作负载的最短作业优先调度机制，它包含四个主要组成部分：1. 精炼与修剪算法，一种无监督分区技术，用来识别性能相似的请求组；2. 动态队列路由，根据请求特性将其分配至合适的组内；3. 密度加权评分机制，这是一种上下文感知的优先级函数，旨在平衡紧迫性与公平性；4. 贝叶斯元优化，持续根据实时性能反馈调整评分和分区参数。

Result: 实验证明，当应用于vLLM框架时，EWSJF相较于FCFS提高了超过30%的整体吞吐量，并且对于较短请求而言，其平均首字节时间缩短了最多达4倍。这些成果表明，基于学习的自适应请求调度是实现高效及响应式大型语言模型服务的关键所在。

Conclusion: 这项研究表明，通过引入EWSJF这样的自适应学习型调度方案，可以显著改善混合工作负载场景下大型语言模型的服务效率和响应速度，从而解决了传统FCFS策略中存在的问题。

Abstract: Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.

</details>


### [135] [Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs](https://arxiv.org/abs/2601.21935)
*Tom Yates,Yuzhou Cheng,Ignacio Alzugaray,Danyal Akarca,Pedro A. M. Mediano,Andrew J. Davison*

Main category: cs.DC

TL;DR: 该论文为在高度非高斯、稀疏连接的因子图中使用BP时，高斯近似何时有效提供了理论保证，并通过实验验证了变量信念在几次BP迭代后变得越来越高斯。


<details>
  <summary>Details</summary>
Motivation: 尽管BP算法强大，但其实用性受限于计算和内存预算。GBP作为一种高效的非参数形式BP被广泛使用，即使在处理非高斯问题时也表现出色。本文旨在为GBP在非高斯条件下提供理论支持。

Method: 利用中心极限定理（CLT），证明了满足4个关键假设下的复杂环状因子图中的变量信念在BP作用下趋向于高斯分布。此外，通过立体深度估计任务实验证实了这一结论。

Result: 数学上证明了在特定条件下BP过程中变量信念趋向高斯分布，并通过实验证明，在立体深度估计任务中，仅需几次BP迭代后变量信念就变得更加高斯。

Conclusion: 研究结果表明，在适当条件下，即便面对本质上非高斯的问题，采用GBP方法也能获得接近高斯的结果，从而合理化了GBP在多种应用中的有效性。

Abstract: Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [136] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 本文提出了SteerEval框架，旨在评估基于自然语言的推荐系统在用户偏好调整方面的响应能力。通过多种干预手段（从电影类型到内容警告）来测试系统的可操控性，并探讨了不同配置文件和推荐干预对指导效果的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的基于自然语言的用户档案提高了推荐系统的可解释性和可控性潜力，但目前尚不清楚这些方法是否能够很好地响应用户的直接编辑命令以反映其偏好。此外，已有评估标准未能充分捕捉到推动可操控推荐背后更丰富形式的用户控制。

Method: 设计了一个名为SteerEval的新评价框架，该框架利用范围广泛的干预措施来衡量更加细致多样的可操控性。研究者们还评估了一组预训练的自然语言推荐器对于相对小众话题进行指导的能力与局限性。

Result: 研究发现表明，现有自然语言推荐系统在处理某些特定类型或主题上的用户偏好调整时存在局限；同时，不同类型的配置文件及推荐干预措施对提高系统响应度有着不同程度的影响。

Conclusion: 通过本研究提出的SteerEval框架及其应用结果，为未来可操控推荐系统的设计提供了实用建议，并指出了需要进一步探索的方向。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [137] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架，利用多视角的链式思维（MPCoT）生成多样化的解释，并结合监督微调(SFT)与直接偏好优化(DPO)，构建更加稳健的推理模型。同时引入了潜在推理知识蒸馏(LRKD)方法，在保持低延迟的同时有效地将大型语言模型(LLMs)复杂的推理能力内化到学生模型中。通过离线实验和在线A/B测试证明，该方法在商业表现和用户体验方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的电商搜索相关性建模方法存在两大局限：一是主要依赖单一视角的链式思维推理，无法全面捕捉电商相关性的多面性；二是尽管增强链式思维的大规模语言模型提供了丰富的推理能力，但其高推理延迟需要进行知识蒸馏才能实现实时部署，而当前的知识蒸馏方法在推理过程中舍弃了链式思维的结构，仅将其作为临时辅助信号使用，从而丧失了其推理价值。

Method: 为了解决上述问题，作者们提出了一种新框架，其中教师模型采用多视角链式思维(MPCoT)生成多种解释，并结合监督细调(SFT)和直接偏好优化(DPO)来建立一个更加强大的推理器。此外，还提出了潜藏推理知识蒸馏(LRKD)，给学生模型配备了一个轻量级的推理时潜藏推理提取器，使得能够高效且低延迟地吸收LLM复杂的推理能力。

Result: 所提方法经过每日服务数千万用户的电商搜索广告平台上的离线实验及在线A/B测试验证，显示出了明显的离线收益，在商业性能与用户体验两方面均展现出清晰的优势。

Conclusion: 这项研究成功地提高了电商搜索中的相关性建模效果，通过创新的方法解决了现有技术中存在的问题，不仅提升了模型的准确性和可解释性，同时也保证了其实用性，为实际应用奠定了坚实基础。

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [138] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: 提出了一种名为Inf-DDS的新颖强化学习驱动采样框架，该框架通过基于影响的奖励信号自适应地重新加权训练数据集，从而在多种文本检索任务中提高了模型性能，并且相比现有的基于梯度的采样方法，GPU计算成本降低了1.5到4倍。


<details>
  <summary>Details</summary>
Motivation: 通用开放领域密集检索系统通常使用大量多样化的语料库和搜索任务进行训练。如何对这些不同的语料库和任务进行采样以用于训练是一个问题。传统的做法是均匀采样或根据实例数量比例采样，或者依赖于专家级的人工监督。但这种训练数据采样策略对模型性能有很大影响，因此寻找最优策略变得尤为重要。

Method: 提出了一个名为Inf-DDS的新框架，它利用强化学习来指导训练数据集的自适应重加权。该技术通过迭代优化采样策略，优先考虑能够最大化目标开发集上模型性能的数据集。

Result: 在广泛的文本检索任务上评估了所提出的采样策略的有效性，展示了与现有基于梯度的采样方法相比，在检索性能和更好适应性方面的显著改进。同时，该策略在GPU计算成本方面也更加经济，比其他方法便宜1.5至4倍。特别地，当使用bge-m3多语言模型时，NDCG@10绝对值提高了5.03；而使用all-MiniLM-L6-v2模型时，即使从大量训练数据集中专家分配权重开始，NDCG@10也有0.94的绝对提升。

Conclusion: Inf-DDS框架提供了一种有效的方法来优化不同语料库和任务中的训练数据采样策略，不仅提高了检索性能，还降低了GPU计算成本。

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [139] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: OneMall, an innovative end-to-end generative recommendation framework for e-commerce at Kuaishou, unifies multiple item distribution scenarios. It integrates a semantic tokenizer, a Transformer-based architecture, and a reinforcement learning pipeline, achieving significant performance improvements in various e-commerce contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation behind OneMall is to create a unified and effective generative recommendation system for the diverse e-commerce environment at Kuaishou, aiming to enhance user experience and business outcomes across different item distribution channels like product cards, short videos, and live streaming.

Method: OneMall adopts a three-pronged approach: 1) An E-commerce Semantic Tokenizer that captures real-world semantics and specific item relations; 2) A Transformer-based Architecture that includes Query-Former, Cross-Attention, and Sparse MoE for handling long sequences, multi-behavior fusion, and scalable generation respectively; 3) A Reinforcement Learning Pipeline that connects retrieval and ranking models, with the ranking model serving as a reward signal for optimizing the retrieval model.

Result: Experiments show that OneMall leads to notable increases in key metrics: +13.01% GMV in product-card, +15.32% Orders in Short-Video, and +2.78% Orders in Live-Streaming. The framework has been successfully deployed, supporting over 400 million daily active users on the Kuaishou platform.

Conclusion: OneMall represents a significant advancement in generative recommendation systems for e-commerce, demonstrating its effectiveness through substantial performance gains and wide-scale deployment. This work highlights the potential of combining advanced NLP techniques, deep learning architectures, and reinforcement learning in building robust recommendation solutions.

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [140] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 本文探讨了跨域推荐（CDR）中出现的公平性问题，识别出两个关键挑战，并提出了一个名为跨域公平性增强（CDFA）的框架来解决这些问题。实验表明，该框架在不牺牲整体推荐性能的情况下显著减少了不公平现象。


<details>
  <summary>Details</summary>
Motivation: 尽管跨域推荐能够通过利用源领域的辅助信号提高目标领域的推荐质量，但有证据显示这可能会无意中加剧群体层面的不公平性。为了解决这个问题，作者进行了全面的理论和实证分析以揭示公平性问题产生的原因。

Method: 提出了一种跨域公平性增强（CDFA）框架，包括两个主要组成部分：一是通过自适应整合未标记数据来平衡不同组间训练信号的信息量，减少跨域差异转移；二是采用信息论方法重新分配跨域信息增益，确保各组间利益均衡分配。

Result: 在多个数据集上的广泛实验表明，提出的CDFA框架不仅显著降低了跨域推荐中的不公平性，而且还在一定程度上提高了整体推荐性能。

Conclusion: 本研究成功地解决了跨域推荐中存在的不公平性问题，证明了所提CDFA框架的有效性和实用性，为未来的研究提供了新的视角。

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [141] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: 提出了一种新的基于频谱的转换器适配器SpecTran，旨在解决现有方法在处理序列推荐模型中文本信息时遇到的问题。通过在频谱域中操作并关注整个频谱来选择和聚合信息成分，同时引入可学习的频谱位置编码以提高嵌入维度多样性。实验显示，在四个真实数据集上该方法相比强基线平均提高了9.17%。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型通常忽视了项目标题或描述等文本信息。尽管最近的研究开始尝试将高维语义嵌入集成到SR模型中，但当前的方法（如基于适配器的方法和基于SVD的方法）存在显著缺陷：前者容易发生维度崩溃，后者过于僵化且手动，只考虑少数主要光谱成分而忽略剩余光谱中的丰富信息。

Method: 提出了名为SpecTran的新方法，这是一种频谱感知的基于变压器的适配器，它在频谱域内工作，能够选择和聚合信息丰富的组件。通过使用可学习的频谱-位置编码注入奇异值线索作为归纳偏置，指导注意力朝向突出的频谱组件，并促进跨嵌入维度的多样性。

Result: 实验结果表明，SpecTran在四个现实世界的数据集以及三个不同的SR骨干模型上均优于强大的基线模型，平均性能提升达到9.17%。

Conclusion: SpecTran提供了一种有效利用项目文本信息的方法，解决了之前方法中存在的问题，比如维度崩溃和信息丢失等。其通过在频谱域执行操作并采用可学习的位置编码增强了模型表现。

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [142] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的重排序方法LANCER，旨在提高长篇检索增强生成中的信息覆盖度。通过预测应答子问题及文档对这些子问题的回答情况来重新排列文档顺序，以尽可能多地覆盖信息点。实验结果显示，LANCR在信息覆盖度指标上优于其他基于大语言模型的重排序方法，并且子问题生成在其中扮演了关键角色。


<details>
  <summary>Details</summary>
Motivation: 现有的检索方法主要针对相关性排名进行优化，而不是信息覆盖面。对于需要提供广泛相关信息的长篇检索增强生成任务（如自动化报告生成），现有方法存在不足。为了克服这一局限，提出了LANCER方法，专注于提高检索结果的信息覆盖范围。

Method: LANCER是一个基于大语言模型的重排序方法，它首先预测为满足特定信息需求所需回答的子问题，然后确定哪些文档能够回答这些子问题，最后根据这些信息对文档进行重新排序，目的是最大化覆盖的信息点数量。

Result: 实验表明，与其它基于大语言模型的重排序方法相比，LANCER在α-nDCG和信息覆盖方面表现更佳。此外，通过对理想情况下的分析发现，子问题生成步骤对于整体性能至关重要。

Conclusion: LANCER通过引入子问题生成机制有效地提高了长篇检索增强生成任务中检索结果的信息覆盖度，展示了其在提升检索质量方面的潜力。

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [143] [HADUA: Hierarchical Attention and Dynamic Uniform Alignment for Robust Cross-Subject Emotion Recognition](https://arxiv.org/abs/2601.21488)
*Jiahao Tang,Youjun Li,Yangxuan Zheng,Xiangting Fan,Siyuan Lu,Nuo Zhang,Zi-Gang Huang*

Main category: cs.MM

TL;DR: 提出了一种新的自适应学习框架HADUA，用于解决跨被试情感识别中的模态异质性和被试间分布偏移问题。该方法通过分层注意力模块和动态统一对齐损失来提高特征融合的判别性和语义一致性，并通过高斯加权方案处理伪标签噪声，实验表明其在准确性和鲁棒性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨被试从多模态生理信号中进行鲁棒的情感识别是一个具有挑战性的问题，主要是因为模态异质性和被试间的分布差异。

Method: 提出了一个名为Hierarchical Attention and Dynamic Uniform Alignment (HADUA)的新自适应学习框架，它结合了多模态表示学习与领域适应。首先设计了一个分层注意力模块以明确建模模内时间动态和模间语义交互（例如EEG与眼动之间），生成具有辨识度且语义连贯的融合特征；其次，引入置信度感知高斯加权方案减少目标域样本监督中的不确定性实例的影响；最后，使用统一对齐损失来规范类间伪标签的分布，从而缓解不平衡并稳定条件分布匹配。

Result: 在多个跨被试情绪识别基准上的广泛实验表明，HADUA在准确性和鲁棒性方面始终优于现有的最先进方法，验证了其在处理模态差距、噪声伪标签和类别不平衡方面的有效性。

Conclusion: 这些贡献共同为构建鲁棒的跨被试情感计算系统提供了实用且可泛化的解决方案。

Abstract: Robust cross-subject emotion recognition from multimodal physiological signals remains a challenging problem, primarily due to modality heterogeneity and inter-subject distribution shift. To tackle these challenges, we propose a novel adaptive learning framework named Hierarchical Attention and Dynamic Uniform Alignment (HADUA). Our approach unifies the learning of multimodal representations with domain adaptation. First, we design a hierarchical attention module that explicitly models intra-modal temporal dynamics and inter-modal semantic interactions (e.g., between electroencephalogram(EEG) and eye movement(EM)), yielding discriminative and semantically coherent fused features. Second, to overcome the noise inherent in pseudo-labels during adaptation, we introduce a confidence-aware Gaussian weighting scheme that smooths the supervision from target-domain samples by down-weighting uncertain instances. Third, a uniform alignment loss is employed to regularize the distribution of pseudo-labels across classes, thereby mitigating imbalance and stabilizing conditional distribution matching. Extensive experiments on multiple cross-subject emotion recognition benchmarks show that HADUA consistently surpasses existing state-of-the-art methods in both accuracy and robustness, validating its effectiveness in handling modality gaps, noisy pseudo-labels, and class imbalance. Taken together, these contributions offer a practical and generalizable solution for building robust cross-subject affective computing systems.

</details>


### [144] [MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding](https://arxiv.org/abs/2601.21740)
*Meng Yang,Jon McCormack,Maria Teresa Llano,Wanchao Su,Chao Lei*

Main category: cs.MM

TL;DR: 介绍了MIDI-LLaMA，首个用于符号音乐理解的多模态大语言模型（MLLM），通过两阶段流程将MIDI编码器MusicBERT与Llama-3-8B对齐，并创建了一个MIDI-文本数据集。相比基线模型，MIDI-LLaMA在字幕生成和语义对齐的问题回答中表现更优，且人类评估也证实了其在音乐理解、情感识别、创造力及总体偏好上的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管近期在音频音乐领域的多模态大语言模型显示出强大的音乐理解能力，但作为音乐结构基本表示形式的符号音乐尚未被探索。

Method: 提出了MIDI-LLaMA，一个指令跟随型MLLM，专门针对符号音乐的理解。该方法包括两个主要步骤：特征对齐与指令调优，并设计了一个可扩展的注释流程来为GiantMIDI-Piano添加细粒度元数据，从而构建了一个MIDI-文本数据集。

Result: MIDI-LLaMA在字幕生成和基于问题回答的语义对齐方面显著优于将MIDI转换为ABC记谱法训练得到的基线模型。此外，人工评估显示MIDI-LLaMA在音乐理解、情绪识别、创造性以及整体偏好上具有明显优势。

Conclusion: 研究结果表明，在大型语言模型中融入符号音乐可以提高它们的音乐理解能力。

Abstract: Recent advances in multimodal large language models (MLLM) for audio music have demonstrated strong capabilities in music understanding, yet symbolic music, a fundamental representation of musical structure, remains unexplored. In this work, we introduce MIDI-LLaMA, the first instruction-following MLLM for symbolic music understanding. Our approach aligns the MIDI encoder MusicBERT and Llama-3-8B via a two-stage pipeline comprising feature alignment and instruction tuning. To support training, we design a scalable annotation pipeline that annotates GiantMIDI-Piano with fine-grained metadata, resulting in a MIDI-text dataset. Compared with the baseline trained on converting MIDI into ABC notation under the same instruction-tuning procedure, MIDI-LLaMA substantially outperforms in captioning and semantic alignment in question answering. Human evaluation further confirms the advantages of MIDI-LLaMA in music understanding, emotion recognition, creativity, and overall preference. These findings demonstrate that incorporating symbolic music into large language models enhances their capacity for musical understanding.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [145] [A Survey on Large Language Model Impact on Software Evolvability and Maintainability: the Good, the Bad, the Ugly, and the Remedy](https://arxiv.org/abs/2601.20879)
*Bruno Claudino Matias,Savio Freire,Juliana Freitas,Felipe Fronchetti,Kostadin Damevski,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 本研究通过系统文献回顾，分析了大型语言模型（LLMs）对软件系统可维护性和可进化性的影响。研究发现，尽管LLMs在提高代码理解、测试能力及自动化修复等方面提供了诸多好处，但同时也引入了包括错误输出、上下文脆弱性等问题，对长期可持续性构成威胁。负责任地采用LLMs需要采取保护措施、严格的评估以及结构化的人工监督。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被嵌入到软件工程工作流程中执行代码生成、总结、修复和测试等任务，虽然已有实证研究表明其提高了生产力并减少了认知负荷，但对于这些混合效应如何影响软件的长期可维护性和可进化性尚不清楚。此外，关于幻觉、不稳定输出、方法论局限性以及新兴技术债务形式的问题仍然存在争议。

Method: 采用了系统文献综述的方法。通过对ACM数字图书馆、IEEE Xplore和Scopus数据库从2020年至2024年的搜索，共获得87项初步研究。通过校准后的多研究人员过程提取定性证据。质量属性进行了描述性分析，而影响、风险、弱点及缓解策略则通过结合LLM辅助分析工具与人工验证的混合主题方法进行综合处理。

Result: 结果显示，LLMs能够提供诸如提高可分析性、可测试性、代码理解力、调试支持及自动化修复等益处；然而，它们也带来了如产生虚假或不正确输出、对上下文敏感度差、领域推理能力有限、性能不稳定以及当前评估中的缺陷等问题，这些问题都威胁到了软件的长期可进化性。

Conclusion: 结论指出，虽然LLMs可以在一定程度上加强软件系统的可维护性和可进化性，但同时它们也给长期可持续性带来了不容忽视的风险。因此，在采用LLMs时需要实施保护措施、进行严格评估，并且确保有结构化的人员监督。

Abstract: Context. Large Language Models (LLMs) are increasingly embedded in software engineering workflows for tasks including code generation, summarization, repair, and testing. Empirical studies report productivity gains, improved comprehension, and reduced cognitive load. However, evidence remains fragmented, and concerns persist about hallucinations, unstable outputs, methodological limitations, and emerging forms of technical debt. How these mixed effects shape long-term software maintainability and evolvability remains unclear. Objectives. This study systematically examines how LLMs influence the maintainability and evolvability of software systems. We identify which quality attributes are addressed in existing research, the positive impacts LLMs provide, the risks and weaknesses they introduce, and the mitigation strategies proposed in the literature. Method. We conducted a systematic literature review. Searches across ACM DL, IEEE Xplore, and Scopus (2020 to 2024) yielded 87 primary studies. Qualitative evidence was extracted through a calibrated multi-researcher process. Attributes were analyzed descriptively, while impacts, risks, weaknesses, and mitigation strategies were synthesized using a hybrid thematic approach supported by an LLM-assisted analysis tool with human-in-the-loop validation. Results. LLMs provide benefits such as improved analyzability, testability, code comprehension, debugging support, and automated repair. However, they also introduce risks, including hallucinated or incorrect outputs, brittleness to context, limited domain reasoning, unstable performance, and flaws in current evaluations, which threaten long-term evolvability. Conclusion. LLMs can strengthen maintainability and evolvability, but they also pose nontrivial risks to long-term sustainability. Responsible adoption requires safeguards, rigorous evaluation, and structured human oversight.

</details>


### [146] [Another Systematic Review? A Critical Analysis of Systematic Literature Reviews on Agile Effort and Cost Estimation](https://arxiv.org/abs/2601.20893)
*Henry Edison,Nauman Ali*

Main category: cs.SE

TL;DR: 研究旨在理解作者如何证明在已有主题上进行额外的系统文献综述(SLR)的合理性，通过分析敏捷软件开发中工作量估算这一狭窄但研究充分的主题上的18篇已发表SLR。常见理由包括填补覆盖空白、先前研究的方法学限制、之前SLR的时间过时性或技术与方法学快速进步需要更新综合。


<details>
  <summary>Details</summary>
Motivation: 鉴于设计、执行和报告一个SLR非常耗费精力，而当前存在大量重复甚至经常是重复性的SLR表明研究人员并没有广泛检查某一主题下是否已经存在SLR。本研究旨在理解作者如何证明在已有主题上进行额外SLR的合理性，并提出改进建议以解决这个问题。

Method: 选取了敏捷软件开发中工作量估算这样一个足够狭窄但被充分研究的主题，通过对该主题下的18篇已发表SLR进行定性内容分析来识别常见的正当化模式。此外，在解释结果时还考虑了引用数据、出版年份、出版场所及SLR的质量等因素。

Result: 发现的常见正当化模式包括：声称存在覆盖缺口、早期研究中的方法论局限性、以往SLR因时间久远而过时，以及技术和方法学的迅速发展需要最新的综合分析。

Conclusion: 对一个相对狭窄主题内SLR的深入分析为软件工程领域内的SLR提供了见解。强调识别现有SLR的重要性以及进一步开展SLR时提供合理依据的需求，无论是从设计还是评审指南的角度出发，或是作为会议和期刊的一项政策，都可以减少重复劳动的可能性并加快领域内的进展速度。

Abstract: Background: Systematic literature reviews (SLRs) have become prevalent in software engineering research. Several researchers may conduct SLRs on similar topics without a prospective register for SLR protocols. However, even ignoring these unavoidable duplications of effort in the simultaneous conduct of SLRs, the proliferation of overlapping and often repetitive SLRs indicates that researchers are not extensively checking for existing SLRs on a topic. Given how effort-intensive it is to design, conduct, and report an SLR, the situation is less than ideal for software engineering research. Aim: To understand how authors justify additional SLRs on a topic. Method: To illustrate the issue and develop suggestions for improvement to address this issue, we have intentionally picked a sufficiently narrow but well-researched topic, i.e., effort estimation in Agile software development. We identify common justification patterns through a qualitative content analysis of 18 published SLRs. We further consider the citation data, publication years, publication venues, and the quality of the SLRs when interpreting the results. Results: The common justification patterns include authors claiming gaps in coverage, methodological limitations in prior studies, temporal obsolescence of previous SLRs, or rapid technological and methodological advancements necessitating updated syntheses. Conclusion: Our in-depth analysis of SLRs on a fairly narrow topic provides insights into SLRs in software engineering in general. By emphasizing the need for identifying existing SLRs and for justifying the undertaking of further SLRs, both in design and review guidelines and as a policy of conferences and journals, we can reduce the likelihood of duplication of effort and increase the rate of progress in the field.

</details>


### [147] [Infusion of Blockchain to Establish Trustworthiness in AI Supported Software Evolution: A Systematic Literature Review](https://arxiv.org/abs/2601.20918)
*Mohammad Naserameri,Juergen Rilling*

Main category: cs.SE

TL;DR: 本研究通过系统文献回顾（SLR）分析了区块链技术如何增强AI驱动的软件工程工具和过程中的信任度。尽管大多数研究集中在AI与软件工程的整合上，但只有31%的研究明确讨论了信任问题。六项近期研究展示了区块链在提高AI辅助软件工程任务的可靠性、透明性和责任性方面的潜力。未来工作需要开发可测量且可复现的信任框架以促进可靠、安全并符合规范的人工智能驱动软件生态系统的发展。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术和人工智能越来越多地被探索用于提升软件工程中的可信度，特别是支持软件演进任务时，理解这两者如何共同作用于增强软件开发过程中的信任变得尤为重要。

Method: 采用预定义协议和清晰的纳入标准进行系统文献回顾（SLR），旨在确保透明性、可重复性和最小化偏见的同时，综合关于区块链支持下AI驱动软件工程工具及流程中信任的研究成果。

Result: 发现大部分研究集中于将AI应用于软件工程领域，而直接关注信任议题的比例仅为31%。此外，有六篇最新研究报告探讨了利用区块链技术来加强AI辅助软件工程项目中的可靠性、透明度以及责任归属等方面的可能性。

Conclusion: 虽然区块链能够通过保证数据不可篡改性、模型透明度以及生命周期内的责任追踪等方式显著提高AI驱动软件工程的信任水平，但目前仍面临信任定义不一致及实际应用测试不足等挑战。未来的研究方向应致力于构建可量化、可复制的信任框架，从而推动形成更加可靠、安全并且合规的人工智能驱动型软件生态环境。

Abstract: Context: Blockchain and AI are increasingly explored to enhance trustworthiness in software engineering (SE), particularly in supporting software evolution tasks. Method: We conducted a systematic literature review (SLR) using a predefined protocol with clear eligibility criteria to ensure transparency, reproducibility, and minimized bias, synthesizing research on blockchain-enabled trust in AI-driven SE tools and processes. Results: Most studies focus on integrating AI in SE, with only 31% explicitly addressing trustworthiness. Our review highlights six recent studies exploring blockchain-based approaches to reinforce reliability, transparency, and accountability in AI-assisted SE tasks. Conclusion: Blockchain enhances trust by ensuring data immutability, model transparency, and lifecycle accountability, including federated learning with blockchain consensus and private data verification. However, inconsistent definitions of trust and limited real-world testing remain major challenges. Future work must develop measurable, reproducible trust frameworks to enable reliable, secure, and compliant AI-driven SE ecosystems, including applications involving large language models.

</details>


### [148] [Operationalizing Research Software for Supply Chain Security](https://arxiv.org/abs/2601.20980)
*Kelechi G. Kalu,Soham Rattan,Taylor R. Schorlemmer,George K. Thiruvathukal,Jeffrey C. Carver,James C. Davis*

Main category: cs.SE

TL;DR: 研究提出了一种面向研究软件供应链（RSSC）的分类法，以解决现有文献中对"研究软件"定义不一致的问题。通过对最近的代码仓库挖掘和数据集构建研究进行有针对性的范围审查，并将这些研究综合成一个统一的分类法和映射表，该研究还使用了Research Software Encyclopedia的数据来标注数据集并创建可重复的标注流程。最后，通过OpenSSF Scorecard的安全分析展示了按照分类法定义的集群间安全信号的差异性，强调了在解释RSSC安全度量时考虑分类的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于文献中对于"研究软件"的定义不一致，导致难以比较不同的实证研究。受研究软件供应链(RSSC)及其安全风险的启发，本研究旨在引入一种新的面向RSSC的分类法，明确界定实证研究软件安全研究的范围和操作边界。

Method: 首先进行了近期有关代码库挖掘与数据集建设研究的目标范围综述，从中提取每个工作的定义、纳入标准、分析单位及识别启发式方法。然后将这些信息整合为一个统一的分类法和映射关系，用于将先前的方法转换为共享的分类维度。接着，基于研究软件百科全书提供的大型社区策划语料库实现了对该分类法的操作化，生成了一个注释数据集、一份编码手册以及一个可再现的标记流程。最后，运用OpenSSF记分卡作为初步安全分析工具，展示根据分类法定义的不同群组之间存储库中心安全信号的差异，并说明为何需要分类意识下的分层处理来正确解读RSSC安全度量结果。

Result: 研究成功地提出了一个针对研究软件供应链的研究软件分类框架，并通过实际应用展示了不同分类下安全特性之间的显著差异。此外，还开发出了一套完整的从数据收集到安全评估的流程体系，包括但不限于：一个被注释过的数据集、一本标签编码指南以及一条能够重复使用的标记流水线。

Conclusion: 本文提出的RSSC导向分类法有助于未来研究更准确地界定研究对象，同时提供了一种可以更好地理解不同类型研究软件间安全特性的方法论。这不仅促进了该领域内研究的一致性和可比性，也为后续围绕研究软件供应链开展的安全分析奠定了基础。

Abstract: Empirical studies of research software are hard to compare because the literature operationalizes ``research software'' inconsistently. Motivated by the research software supply chain (RSSC) and its security risks, we introduce an RSSC-oriented taxonomy that makes scope and operational boundaries explicit for empirical research software security studies.
  We conduct a targeted scoping review of recent repository mining and dataset construction studies, extracting each work's definition, inclusion criteria, unit of analysis, and identification heuristics. We synthesize these into a harmonized taxonomy and a mapping that translates prior approaches into shared taxonomy dimensions. We operationalize the taxonomy on a large community-curated corpus from the Research Software Encyclopedia (RSE), producing an annotated dataset, a labeling codebook, and a reproducible labeling pipeline. Finally, we apply OpenSSF Scorecard as a preliminary security analysis to show how repository-centric security signals differ across taxonomy-defined clusters and why taxonomy-aware stratification is necessary for interpreting RSSC security measurements.

</details>


### [149] [Towards Comprehensive Benchmarking Infrastructure for LLMs In Software Engineering](https://arxiv.org/abs/2601.21070)
*Daniel Rodriguez-Cardenas,Xiaochang Li,Marcos Macedo,Antonio Mastropaolo,Dipin Khati,Yuan Tian,Huajie Shao,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文指出现有代码大型语言模型评估基准的不足，包括任务狭窄、指标单一等问题，并提出了BEHELM，一个结合软件场景规范与多指标评价的整体基准框架，旨在提供一种结构化的方法来跨任务、语言和质量维度评估模型。


<details>
  <summary>Details</summary>
Motivation: 现有代码大型语言模型的评估方法落后于模型本身的发展速度，存在任务过于狭窄、只关注单一指标等问题，导致在鲁棒性、可解释性、公平性、效率及实际应用等方面的关键缺陷被忽视。此外，还存在数据工程实践不一致、软件工程背景有限以及普遍存在的污染问题。

Method: 通过深入调查现有的基准测试并结合专门社区研讨会收集到的见解，识别出可靠评估面临的三大障碍：缺乏富含软件工程的数据集、过度依赖机器学习中心指标以及缺乏标准化、可重复的数据管道。基于这些发现，引入了BEHELM这一全面的基准测试基础设施，它将软件场景规范与多指标评估相结合。

Result: BEHELM提供了一种结构化的方式，能够根据任务、编程语言、输入输出粒度以及关键质量维度来评估模型。

Conclusion: BEHELM的目标是减少构建基准所需的工作量，同时使得对软件工程中大型语言模型进行公平、现实且面向未来的评估成为可能。

Abstract: Large language models for code are advancing fast, yet our ability to evaluate them lags behind. Current benchmarks focus on narrow tasks and single metrics, which hide critical gaps in robustness, interpretability, fairness, efficiency, and real-world usability. They also suffer from inconsistent data engineering practices, limited software engineering context, and widespread contamination issues. To understand these problems and chart a path forward, we combined an in-depth survey of existing benchmarks with insights gathered from a dedicated community workshop. We identified three core barriers to reliable evaluation: the absence of software-engineering-rich datasets, overreliance on ML-centric metrics, and the lack of standardized, reproducible data pipelines. Building on these findings, we introduce BEHELM, a holistic benchmarking infrastructure that unifies software-scenario specification with multi-metric evaluation. BEHELM provides a structured way to assess models across tasks, languages, input and output granularities, and key quality dimensions. Our goal is to reduce the overhead currently required to construct benchmarks while enabling a fair, realistic, and future-proof assessment of LLMs in software engineering.

</details>


### [150] [From Logic to Toolchains: An Empirical Study of Bugs in the TypeScript Ecosystem](https://arxiv.org/abs/2601.21186)
*TianYi Tang,Saba Alimadadi,Nick Sumner*

Main category: cs.SE

TL;DR: 本文对TypeScript项目中的错误进行了大规模实证研究，发现主要的错误类型是工具和配置错误、API误用以及异步错误处理问题。这些错误与构建复杂性和依赖多样性高度相关，并且随着语言设计和生态系统的发展，脆弱性从算法逻辑转向了构建系统和工具链。


<details>
  <summary>Details</summary>
Motivation: TypeScript在现代Web开发中变得越来越流行，但其对软件故障的影响仍不清楚。因此，本研究旨在通过分析真实世界TypeScript项目的错误报告来填补这一知识空白。

Method: 研究人员分析了来自16个受欢迎开源仓库的633份错误报告，构建了一个错误类型的分类体系，并量化了它们的普遍性及其与项目特征（如规模、领域、依赖组成）的关系。

Result: 研究结果显示，错误的主要类型并非逻辑或语法错误，而是工具和配置错误、API误用及异步错误处理问题。这些问题与构建复杂度及依赖多样性有很强的相关性。

Conclusion: 虽然TypeScript的静态类型减少了传统运行时和类型错误，但它将脆弱性转移到了构建系统和工具链上。研究结果为理解语言设计及生态系统演化如何重塑大规模软件系统的故障模式提供了新的见解。

Abstract: TypeScript has rapidly become a popular language for modern web development, yet its effect on software faults remains poorly understood. This paper presents the first large-scale empirical study of bugs in real-world TypeScript projects. We analyze 633 bug reports from 16 popular open-source repositories to construct a taxonomy of fault types, quantify their prevalence, and relate them to project characteristics such as size, domain, and dependency composition. Our results reveal a fault landscape dominated not by logic or syntax errors but by tooling and configuration faults, API misuses, and asynchronous error-handling issues. We show that these categories correlate strongly with build complexity and dependency heterogeneity, indicating that modern failures often arise at integration and orchestration boundaries rather than within algorithmic logic. A longitudinal comparison with JavaScript studies shows that while static typing in TypeScript has reduced traditional runtime and type errors, it has shifted fragility toward build systems and toolchains. These findings offer new insight into how language design and ecosystem evolution reshape the fault profiles of large-scale software systems.

</details>


### [151] [The Role of Social Identity in Shaping Biases Against Minorities in Software Organizations](https://arxiv.org/abs/2601.21259)
*Sayma Sultana,London Cavaletto,Bianca Trinkenreich,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 本研究通过应用社会身份理论（SIT）探讨了软件工程师面临的四种特定形式的偏见：缺乏职业发展机会、刻板任务分配、不友好环境以及身份攻击。结果显示，职业发展和任务选择偏见是最常见的两种形式，女性比男性更容易遭受这些偏见及不友好的工作环境；而来自边缘化种族背景的人则更频繁地成为身份攻击的目标。此外，年龄、工作经验年限、组织规模和地区等因素也被发现是偏见受害的重要预测因素。


<details>
  <summary>Details</summary>
Motivation: 尽管系统性职场偏见在非计算机领域已有充分记录，但其对软件工程师的具体影响尚未得到充分理解。本研究旨在填补这一空白，特别关注软件工程师群体内存在的职场偏见问题。

Method: 本研究采用了基于情景调查的方法来量化不同类型的偏见发生率，确定受影响最大的人口统计学特征，评估这些偏见带来的后果，并探索导致偏见行为背后的动机。

Result: 研究表明，职业发展与任务选择方面的偏见最为普遍，超过三分之二的受害者多次遭遇此类偏见。女性遭遇职业发展偏见、任务选择偏见以及不友好环境的概率是男性的三倍多。同时，边缘化种族背景的人群受到身份攻击的比例更高。分析还表明，除了性别和种族外，年龄、工作经验、公司规模和地理位置也是偏见经历的重要预测因子。

Conclusion: 软件工程师同样面临多种职场偏见问题，其中职业发展机会不足与不公平的任务分配尤为突出。性别、种族以及其他个人特征如年龄等都可能加剧个体遭受偏见的风险。

Abstract: While systemic workplace bias is well-documented in non-computing fields, its specific impact on software engineers remains poorly understood. This study addresses that gap by applying Social Identity Theory (SIT) to investigate four distinct forms of bias: lack of career development, stereotyped task selection, unwelcoming environments, and identity attacks. Using a vignette-based survey, we quantified the prevalence of these biases, identified the demographics most affected, assessed their consequences, and explored the motivations behind biased actions. Our results show that career development and task selection biases are the most prevalent forms, with over two-thirds of victims experiencing them multiple times. Women were more than three times as likely as men to face career development bias, task selection bias, and an unwelcoming environment. In parallel, individuals from marginalized ethnic backgrounds were disproportionately targeted by identity attacks. Our analysis also confirms that, beyond gender and race, factors such as age, years of experience, organization size, and geographic location are significant predictors of bias victimization.

</details>


### [152] [Detecting Multiple Semantic Concerns in Tangled Code Commits](https://arxiv.org/abs/2601.21298)
*Beomsu Koh,Neil Walkinshaw,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文将纠缠提交中的多关注点检测视为多标签分类问题，通过构建基于真实数据的人工纠缠提交受控数据集，并使用小型语言模型（SLMs）进行实证研究。结果显示，经过微调的140亿参数SLM在单关注点提交上与最先进的大型语言模型（LLMs）竞争，并且对于最多三个关注点仍然可用。此外，包含提交信息可以提高检测准确性高达44%，同时延迟开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 代码提交应该专注于单一目标，但在实践中开发人员经常将多个关注点捆绑到纠缠提交中，这掩盖了意图并使维护复杂化。尽管最近的研究表明小型语言模型（SLMs）能够接近大型语言模型（LLMs）的表现，但它们没有解决涉及多个关注点的纠缠提交问题，留下了利用语言模型进行多关注点检测可行性的问题未解。

Method: 作者将纠缠提交中的多关注点检测框架为一个多标签分类问题，并基于真实世界的数据构建了一个控制数据集。接着，他们使用小型语言模型（SLMs）进行了一个实证研究，考察了微调、关注点数量、提交消息包含以及标题保留截断在实际令牌预算限制下的影响。

Result: 研究结果表明，一个经过微调的140亿参数的小型语言模型在处理单个关注点提交时能够与最先进的大型语言模型相媲美，并且当涉及到最多三个不同关注点时仍保持实用性。特别地，加入提交消息作为输入可以显著提升至多44%的检测精度（以汉明损失衡量），而带来的延迟几乎可以忽略。

Conclusion: 本研究表明，小型语言模型在处理纠缠提交中的多关注点检测方面具有潜力，特别是在考虑提交消息的情况下，其性能有了显著提升。这意味着未来可能更加依赖于这些效率更高同时也保护隐私的小型模型来帮助软件开发过程中更好地理解和管理代码变更。

Abstract: Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.

</details>


### [153] [Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools](https://arxiv.org/abs/2601.21305)
*Mark Looi,Julianne Quinn*

Main category: cs.SE

TL;DR: 该研究探讨了147名专业开发人员使用生成式AI工具的模式及其对生产力和代码质量感知的影响，揭示了一个良性采用循环，并确定了三种开发者原型：热情者、实用主义者和谨慎者。结果表明，频繁和广泛地使用AI工具有助于提高感知到的生产力与代码质量，同时指出了在AI测试工具采纳上存在的差距以及安全顾虑是影响未来采纳意愿的重要因素。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能迅速进入软件开发领域，本研究旨在实证调查这些技术对实际工作的影响，包括开发者的使用习惯、AI工具使用与生产率及代码质量间的关系，以及开发者对于新兴AI增强型开发方法的态度。

Method: 通过对147位专业开发者的调查研究，分析了AI工具使用的频率、广度等因素与感知生产力（PP）、代码质量之间的关联性；同时考察了开发者对未来采用AI工具的意愿及其背后原因。

Result: 研究发现，AI工具的频繁使用和广泛应用范围是提高感知生产力（PP）和代码质量的关键因素。此外，当前高水平的使用、应用范围广泛、频繁利用AI进行测试以及易用性均强烈正相关于未来的采纳意图。然而，安全性担忧仍然构成一个重要的障碍。值得注意的是，在AI测试工具的采用方面落后于编码工具，形成了所谓的‘测试缺口’。

Conclusion: 组织层面的人工智能工具采用遵循一种创新扩散过程，其中由热情者推动工具的应用，并通过组织成功案例转化实用主义者。而谨慎群体则因缺乏早期采用者实例而停滞不前，未能进入良性采用循环或积累足够的使用频率来驱动其采用意向。政策本身并不直接影响个人增加使用的意图，但作为成熟度的一个标志，它反映了热情者成功推广后的正式化过程，同时也为谨慎群体设定了一个尚未达到的门槛。

Abstract: The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.

</details>


### [154] [Predicting Developer Acceptance of AI-Generated Code Suggestions](https://arxiv.org/abs/2601.21379)
*Jing Jiang,Liehao Li,Jinyun Hou,Xin Tan,Li Zhang*

Main category: cs.SE

TL;DR: 本研究通过分析一家大型科技公司66,329次开发者与AI的互动数据，找出了被接受和拒绝代码建议之间的显著特征差异，并基于这些发现提出了CSAP（代码建议接受预测）模型。该模型在不平衡和平衡数据集上分别达到了0.973和0.922的准确率，相较于现有的基线模型有了显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助编程工具得到广泛应用，但不合适的建议会打断开发者的流程并引起挫败感。现有研究多为定性探讨开发者与AI交互情况，而关于开发者对AI生成代码建议接受度的量化分析存在不足，部分原因是所需详细交互数据通常属于专有信息。

Method: 通过对一家大型技术公司的66,329个实际开发者-AI交互案例进行实证研究，分析了被接受和被拒绝代码建议之间存在显著差异的特点。基于研究结果，引入了CSAP (Code Suggestion Acceptance Prediction) 模型来预测开发者在接受显示前是否会采纳代码建议。

Result: 研究发现，被接受的建议具有更高的历史接受次数和比率、更长的生成间隔、项目中较短的前置代码上下文以及较旧的IDE版本等特点。CSAP模型在不平衡和平衡数据集上的准确率分别为0.973和0.922，相比大规模语言模型基线和工业生产过滤器，在不同数据集上准确率分别提高了12.6%至140.1%。

Conclusion: 针对性个性化是过滤掉预期会被拒绝代码建议的有效方法，能够减少开发者被打断的情况。这是首次基于大规模工业数据对代码建议接受度进行量化研究的工作，也为AI辅助编程的重要研究方向提供了启示。

Abstract: AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\% and 69.5\% on imbalanced dataset, and improves the accuracy by 87.0\% and 140.1\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.

</details>


### [155] [Chasing Elusive Memory Bugs in GPU Programs](https://arxiv.org/abs/2601.21552)
*Anubhab Ghosh,Ajay Nayak,Dhananjay Rao Thallikar Shyam,Arkaprava Basu*

Main category: cs.SE

TL;DR: Researchers have developed SCuBA, a compile-time tool that can detect input-dependent and intra-allocation out-of-bound (OOB) errors in GPU programs, which are not detected by existing runtime tools. SCuBA uses a SAT solver to analyze CPU and GPU code for potential OOBs under any input, effectively identifying 45 elusive memory bugs across 20 programs with no false alarms.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of detecting input-dependent and intra-allocation out-of-bound (OOB) memory safety bugs in GPU programs, which current runtime detection tools fail to identify since such bugs only manifest under specific conditions or logical partitioning within memory allocations.

Method: SCuBA, a novel compile-time technique, is introduced. It captures semantic relations between program variables from both CPU and GPU code, using these relations as constraints for a SAT solver to determine if an OOB access is possible. Additionally, it tracks the logical partitioning of memory to detect intra-allocation OOBs.

Result: SCuBA was able to successfully identify 45 memory bugs across 20 different programs, including those that are input-dependent and intra-allocation OOBs, without producing any false positives. This is in contrast to NVIDIA's Compute Sanitizer, which failed to detect these 45 bugs.

Conclusion: The paper concludes that SCuBA is effective in detecting a class of OOB bugs in GPU programs that are undetected by traditional runtime methods, thereby improving the security and reliability of GPU-accelerated software.

Abstract: Memory safety bugs, such as out-of-bound accesses (OOB) in GPU programs, can compromise the security and reliability of GPU-accelerated software. We report the existence of input-dependent OOBs in the wild that manifest only under specific inputs. All existing tools to detect OOBs in GPU programs rely on runtime techniques that require an OOB to manifest for detection. Thus, input-dependent OOBs elude them. We also discover intra-allocation OOBs that arise in the presence of logical partitioning of a memory allocation into multiple data structures. Existing techniques are oblivious to the possibility of such OOBs.
  We make a key observation that the presence (or absence) of semantic relations among program variables, which determines the size of allocations (CPU code) and those calculating offsets into memory allocations (GPU code), helps identify the absence (or presence) of OOBs. We build SCuBA, a first-of-its-kind compile-time technique that analyzes CPU and GPU code to capture such semantic relations (if present). It uses a SAT solver to check if an OOB access is possible under any input, given the captured relations expressed as constraints. It further analyzes GPU code to track logical partitioning of memory allocations for detecting intra-allocation OOB. Compared to NVIDIA's Compute Sanitizer that misses 45 elusive memory bugs across 20 programs, SCuBA misses none with no false alarms.

</details>


### [156] [Multi-objective Integer Linear Programming approach for Automatic Software Cognitive Complexity Reduction](https://arxiv.org/abs/2601.21565)
*Adriana Novoa-Hurtado,Rubén Saborido,Francisco Chicano,Manuel Giménez-Medina*

Main category: cs.SE

TL;DR: 本文提出了一种多目标整数线性规划模型，旨在通过平衡代码行数和认知复杂度来减少给定代码片段的认知复杂度。此外，开发了多种算法并集成到一个工具中，以参数化解决软件认知复杂度降低的问题。


<details>
  <summary>Details</summary>
Motivation: 为了保证软件的可维护性，清晰简洁的代码是必要的。通过提取方法重构作为主要过程来增强软件而不改变其功能，可以减少理解代码所需的努力。使用SonarSource定义的认知复杂度度量，将提取问题建模为组合优化问题，并且由于存在不同的评估解决方案标准，需要将其表述为一个多目标优化问题。

Method: 提出了一个多目标整数线性规划模型，用于获得一组能够减少给定代码片段认知复杂度的解。此外，开发了若干算法来验证该模型的有效性，并将这些算法集成到了一个工具里，以便于参数化解决降低软件认知复杂度的问题。

Result: 所提出的模型与算法能够在一定程度上帮助减少软件的认知复杂度，通过平衡代码行数和认知复杂度提供了有效的解集。而且，开发出的工具也证明了对于实际应用中的软件认知复杂度降低问题具有良好的处理能力。

Conclusion: 通过采用多目标整数线性规划模型及相关算法，可以有效地减少软件的认知复杂度，从而提高代码的可读性和可维护性。所开发的工具为实践者提供了一种实用的方法来参数化地解决此类问题。

Abstract: Clear and concise code is necessary to ensure maintainability, so it is crucial that the software is as simple as possible to understand, to avoid bugs and, above all, vulnerabilities. There are many ways to enhance software without changing its functionality, considering the extract method refactoring the primary process to reduce the effort required for code comprehension. The cognitive complexity measure employed in this work is the one defined by SonarSource, which is a company that develops well-known applications for static code analysis. This extraction problem can be modeled as a combinatorial optimization problem. The main difficulty arises from the existence of different criteria for evaluating the solutions obtained, requiring the formulation of the code extraction problem as a multi-objective optimization problem using alternative methods. We propose a multi-objective integer linear programming model to obtain a set of solutions that reduce the cognitive complexity of a given piece of code, such as balancing the number of lines of code and its cognitive complexity. In addition, several algorithms have been developed to validate the model. These algorithms have been integrated into a tool that enables the parameterised resolution of the problem of reducing software cognitive complexity.

</details>


### [157] [Is My RPC Response Reliable? Detecting RPC Bugs in Ethereum Blockchain Client under Context](https://arxiv.org/abs/2601.21593)
*Zhijie Zhong,Yuhong Nan,Mingxi Ye,Qing Xue,Jiashui Wang,Xinlei Ying,Long Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出了一种名为EthCRAFT的工具，该工具能够通过生成适当的上下文来触发依赖于上下文的RPC错误，并且在检测实际的以太坊客户端实现中的RPC错误方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的区块链RPC错误检测研究主要集中在为测试区块链客户端生成RPC方法调用上，但大量报告的RPC错误是在各种区块链上下文中被触发的。很少有研究关注如何生成能触发这些依赖上下文的RPC错误的适当上下文。

Method: EthCRAFT首先探索了区块链客户端的状态转换程序空间，并生成多种交易来构建上下文。然后设计了一个上下文感知的RPC方法调用生成方法，向区块链客户端发送RPC调用。使用5种不同客户端实现的响应作为交叉引用预言机来检测RPC错误。

Result: 实验结果表明，EthCRAFT相比现有客户端RPC检测器能够发现更多的RPC错误。此外，EthCRAFT在主要的以太坊客户端中发现了六个新的漏洞，并已向开发者报告。其中一个修复已被写入客户端更新的重大更改中。三个漏洞报告得到了以太坊基金会提供的漏洞赏金。

Conclusion: EthCRAFT是一个有效的工具，用于检测依赖上下文的RPC错误，并在真实世界的应用中已经证明了其有效性。

Abstract: Blockchain clients are fundamental software for running blockchain nodes. They provide users with various RPC (Remote Procedure Call) interfaces to interact with the blockchain. These RPC methods are expected to follow the same specification across different blockchain nodes, providing users with seamless interaction. However, there have been continuous reports on various RPC bugs that can cause unexpected responses or even Denial of Service weakness. Existing studies on blockchain RPC bug detection mainly focus on generating the RPC method calls for testing blockchain clients. However, a wide range of the reported RPC bugs are triggered in various blockchain contexts. To the best of our knowledge, little attention is paid to generating proper contexts that can trigger these context-dependent RPC bugs.
  In this work, we propose EthCRAFT, a Context-aware RPC Analysis and Fuzzing Tool for client RPC bug detection. EthCRAFT first proposes to explore the state transition program space of blockchain clients and generate various transactions to construct the context. EthCRAFT then designs a context-aware RPC method call generation method to send RPC calls to the blockchain clients. The responses of 5 different client implementations are used as cross-referring oracles to detect the RPC bugs. We evaluate EthCRAFT on real-world RPC bugs collected from the GitHub issues of Ethereum client implementations. Experiment results show that EthCRAFT outperforms existing client RPC detectors by detecting more RPC bugs. Moreover, EthCRAFT has found six new bugs in major Ethereum clients and reported them to the developers. One of the bug fixes has been written into breaking changes in the client's updates. Three of our bug reports have been offered a vulnerability bounty by the Ethereum Foundation.

</details>


### [158] [Age Matters: Analyzing Age-Related Discussions in App Reviews](https://arxiv.org/abs/2601.21605)
*Shashiwadana Nirmania,Garima Sharma,Hourieh Khalajzadeh,Mojtaba Shahin*

Main category: cs.SE

TL;DR: 本研究通过分析Google Play商店中的4,163条应用评论，探讨了不同年龄段用户在使用移动应用时面临的问题。研究发现年龄相关讨论的六个主要主题，并使用多种机器学习模型自动检测这些讨论，其中RoBERTa模型表现最佳，准确率达到92.46%。


<details>
  <summary>Details</summary>
Motivation: 尽管移动应用在过去几年里极大地改变了人们的生活方式，但在满足不同年龄层用户需求方面仍存在差距。不同年龄阶段的用户在与移动应用程序互动时会遇到不同的挑战，而开发者可能因缺乏对这些问题的理解而难以提供有效的解决方案。

Method: 研究人员从Google Play商店收集并手动整理了一个包含4,163条评论的数据集，并将这些评论分为年龄相关（1,429条）和非年龄相关（2,734条）。接着采用八种不同的机器学习、深度学习及大型语言模型来自动识别年龄相关的讨论内容；随后还进行了定性分析以揭示用户关注的核心问题。

Result: 研究结果显示，RoBERTa模型在自动检测年龄相关讨论方面表现最好，其精确度达到了92.46%。此外，通过对1,429条与年龄有关的评论进行定性分析，确定了六个主要反映用户担忧的主题。

Conclusion: 该研究表明，通过分析应用商店里的用户反馈可以有效了解不同年龄段用户的需求和挑战。利用先进的自然语言处理技术如RoBERTa可以帮助自动识别这类信息，为开发更加包容性的移动应用提供了有价值的见解。

Abstract: In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people's daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.

</details>


### [159] [Migrating Esope to Fortran 2008 using model transformations](https://arxiv.org/abs/2601.21755)
*Younoussa Sow,Nicolas Anquetil,Léandre Brault,Stéphane Ducasse*

Main category: cs.SE

TL;DR: 本文介绍了一种将带有专有扩展Esope的FORTRAN 77代码自动迁移到Fortran 2008的方法，并提供了一个转换工具来生成易于阅读且保持原有抽象层次的新代码。


<details>
  <summary>Details</summary>
Motivation: 维护和现代化如FORTRAN 77这样的遗留编程语言很具挑战性，尤其是在向更新的标准比如Fortran 2008迁移时。这种挑战在存在专有扩展的情况下更加明显，因为它们的语义通常基于旧的上下文。因此需要一种方法来自动化这一迁移过程。

Method: 采用模型驱动的工程技巧，通过转换生成目标模型，然后从该模型导出可读性强的Fortran 2008源代码。同时，确保转换后的代码保持Esope所提供的抽象层次。

Result: 开发出了一个能够将Esope源代码转换为Fortran 2008的工具，并讨论了该方法的优势、局限性以及可维护性方面的考虑。还提供了关于其可扩展性和适应不断变化需求的看法。

Conclusion: 提出的方法和工具为带有专有扩展的遗留FORTRAN 77代码向现代Fortran 2008标准迁移提供了一条可行路径，同时也关注到了生成代码的可读性和抽象层次的一致性问题。

Abstract: Legacy programming languages such as FORTRAN 77 still play a vital role in many industrial applications. Maintaining and modernizing these languages is challenging, especially when migrating to newer standards such as Fortran 2008. This is exacerbated in the presence of legacy proprietary extensions on such legacy languages, because their semantics are often based on old context (limits of legacy language, domain logic,...). This paper presents an approach for automatically migrating FORTRAN 77 with a proprietary extension, named Esope, to Fortran 2008. We introduce a tool that converts Esope source code to Fortran 2008. While supporting readability of the generated code, we want to maintain the level of abstraction provided by Esope. Our method uses model-driven engineering techniques, with transformations to generate a target model from which we export easy-to-read Fortran 2008 source code. We discuss the advantages, limitations, and maintainability considerations of our approach and provide insights into its scalability and adaptability to evolving requirements.

</details>


### [160] [Towards A Sustainable Future for Peer Review in Software Engineering](https://arxiv.org/abs/2601.21761)
*Esteban Parra,Sonia Haiduc,Preetha Chatterjee,Ramtin Ehsani,Polina Iaremchuk*

Main category: cs.SE

TL;DR: 该论文提出了一种未来软件工程研究领域的愿景，涉及一种更可扩展、包容和有弹性的同行评审过程，包括吸引和培训新成员成为高质量审稿人、激励更多社区成员担任同行评审员以及谨慎地整合AI工具以支持高质量的评审过程。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程领域论文提交数量的快速增长超过了合格审稿人的可用性，导致了一个日益增长的不平衡状态，这可能会限制并负面影响软件工程研究社区的长期发展。

Method: 论文并没有详细描述具体的方法论，而是提出了一个愿景，该愿景建议通过1) 吸引和培训新人作为高质量的审稿人，2) 激励更多的社区成员担任同行评审者，以及3) 谨慎地整合人工智能工具来支持高质量的评审流程，从而构建一个更加可扩展、包容性和抗压能力更强的同行评审机制。

Result: 文中没有提供具体的实验结果，但提出了对于改进软件工程研究领域同行评审过程的构想。

Conclusion: 作者们的愿景是创建一个更加可扩展、更具包容性且能够抵御外界压力的同行评审系统，以应对当前软件工程研究领域面临的挑战，并促进其长远健康发展。

Abstract: Peer review is the main mechanism by which the software engineering community assesses the quality of scientific results. However, the rapid growth of paper submissions in software engineering venues has outpaced the availability of qualified reviewers, creating a growing imbalance that risks constraining and negatively impacting the long-term growth of the Software Engineering (SE) research community. Our vision of the Future of the SE research landscape involves a more scalable, inclusive, and resilient peer review process that incorporates additional mechanisms for: 1) attracting and training newcomers to serve as high-quality reviewers, 2) incentivizing more community members to serve as peer reviewers, and 3) cautiously integrating AI tools to support a high-quality review process.

</details>


### [161] [Assessing the Business Process Modeling Competences of Large Language Models](https://arxiv.org/abs/2601.21787)
*Chantale Lauer,Peter Pfeiffer,Alexander Rombach,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: 本研究提出了一种新的评估框架BEF4LLM，用于系统性地评价大型语言模型（LLMs）生成的BPMN模型的质量。通过与人类建模专家的表现对比，研究发现LLMs在语法和实用质量方面表现出色，但在语义质量和有效性方面略逊一筹。这些发现指出了使用LLMs进行BPMN建模时的优势与局限，并为未来模型的发展提供了指导。


<details>
  <summary>Details</summary>
Motivation: 虽然最新的大型语言模型（LLMs）极大地扩展了直接从自然语言生成BPMN模型的可能性，但缺乏对这些模型产生的过程模型进行系统性评估。当前的努力要么采用LLM作为评判方法，要么没有考虑到既定的模型质量维度。

Method: 引入了名为BEF4LLM的新评估框架，该框架由四个视角组成：句法质量、实用质量、语义质量和有效性。利用这一框架，研究人员对开源LLMs进行了全面分析，并将其表现与人类建模专家进行了基准测试。

Result: 结果表明，LLMs在句法和实用质量方面表现出色，而人类则在语义方面表现更佳；不过，得分差异相对较小，这突显了尽管存在有效性和语义质量方面的挑战，LLMs仍具有竞争力。

Conclusion: 研究揭示了目前使用LLMs进行BPMN建模的优势与局限，并为未来的模型开发与微调提供了指导。解决这些领域的问题对于促进LLMs在业务流程建模中的实际部署至关重要。

Abstract: The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task requiring both domain knowledge and proficiency in modeling conventions. Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language, building upon earlier text-to-process methods with enhanced capabilities in handling complex descriptions. However, there is a lack of systematic evaluations of LLM-generated process models. Current efforts either use LLM-as-a-judge approaches or do not consider established dimensions of model quality. To this end, we introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity. Using BEF4LLM, we conduct a comprehensive analysis of open-source LLMs and benchmark their performance against human modeling experts. Results indicate that LLMs excel in syntactic and pragmatic quality, while humans outperform in semantic aspects; however, the differences in scores are relatively modest, highlighting LLMs' competitive potential despite challenges in validity and semantic quality. The insights highlight current strengths and limitations of using LLMs for BPMN modeling and guide future model development and fine-tuning. Addressing these areas is essential for advancing the practical deployment of LLMs in business process modeling.

</details>


### [162] [Folklore in Software Engineering: A Definition and Conceptual Foundations](https://arxiv.org/abs/2601.21814)
*Eduard Enoiu,Jean Malm,Gregory Gay*

Main category: cs.SE

TL;DR: 本文探讨了软件工程中的民俗概念，定义并描述了在软件开发社区中流传的叙述、神话、仪式、幽默和非正式知识。通过文献综述和主题分析，整理了典型的民俗项目，并分析了它们的叙事形式、象征意义、职业相关性以及与软件工程知识领域的联系。通过对瑞典12位行业从业者的半结构化访谈，研究了这些叙述如何在其日常工作中被认可或传播以及它们如何影响工作。基于这些结果，提出了一个关于软件工程民俗的工作定义，即在职业民间群体内非正式传递的传统和新兴叙述及启发式方法，塑造身份、价值观和集体知识。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过引入民俗学的概念来更好地理解软件开发社区中的文化现象，如信念、传说和技术债务等，从而为后续的民族志和民俗学研究提供基础，并促进反思实践以保留有效的上下文启发法同时挑战无益的民俗。

Method: 本研究采用文献回顾与主题分析相结合的方法，搜集并分析了软件工程领域内的典型民俗事例。此外，还进行了半结构化的深度访谈，对象是瑞典的12名工业从业者，以此来探索这些民俗故事是如何在日常工作环境中被识别和传播的，以及它们对实际工作的具体影响。

Result: 研究表明，软件工程中的民俗可以被定义为一种非正式传播的故事和经验法则，这些内容由特定的职业团体创造和发展，对于形成该团体的身份感、价值观念以及共享的知识体系具有重要意义。

Conclusion: 将软件工程民俗作为一个明确的概念提出来，不仅有助于未来开展更加深入的人类学和社会学研究，而且还能鼓励业界人士进行自我反省，以甄别出那些真正有益于特定情境下的实践指南，并且勇于质疑那些可能带来负面影响的传统说法。

Abstract: We explore the concept of folklore within software engineering, drawing from folklore studies to define and characterize narratives, myths, rituals, humor, and informal knowledge that circulate within software development communities. Using a literature review and thematic analysis, we curated exemplar folklore items (e.g., beliefs about where defects occur, the 10x developer legend, and technical debt). We analyzed their narrative form, symbolic meaning, occupational relevance, and links to knowledge areas in software engineering. To ground these concepts in practice, we conducted semi-structured interviews with 12 industrial practitioners in Sweden to explore how such narratives are recognized or transmitted within their daily work and how they affect it. Synthesizing these results, we propose a working definition of software engineering folklore as informally transmitted, traditional, and emergent narratives and heuristics enacted within occupational folk groups that shape identity, values, and collective knowledge. We argue that making the concept of software engineering folklore explicit provides a foundation for subsequent ethnography and folklore studies and for reflective practice that can preserve context-effective heuristics while challenging unhelpful folklore.

</details>
