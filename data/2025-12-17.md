<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.LG](#cs.LG) [Total: 45]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study](https://arxiv.org/abs/2512.13830)
*Chaima Boufaied,Thanh Nguyen,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过26次半结构化访谈，探讨了来自23个国家不同背景的从业者对AI/ML软件公平性的认识及其在软件开发生命周期中的应用。尽管受访者认识到AI公平性的重要性，但实践过程中存在不一致性和知识空白，公平性常常被忽视。研究强调了需要与相关利益相关者就明确、适合情境的公平性定义达成一致，并建立相应的评估指标和正式流程，以更好地将公平性整合到AI/ML项目中。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（特别是机器学习和大型语言模型）在各个领域的广泛应用，其潜在的不公平问题日益受到关注。为了解决这一问题，除了传统的注重AI模型的有效性外，还增加了对AI软件公平性的重视。本文旨在从软件工程的角度出发，探究AI软件开发中公平性的要求及其实施情况。

Method: 采用定性研究方法，通过对23个国家内不同应用领域及背景的从业者进行26次半结构化访谈来收集数据。研究重点在于评估参与者对于AI/ML软件公平性的意识，以及这些概念如何在软件开发生命周期的不同阶段得到体现或处理，包括需求转换、早期评估等环节。此外，还考察了公平性与其他优先事项如功能完整性和交付时间之间的权衡关系。

Result: 研究发现表明，虽然受访者普遍意识到AI公平性的几个关键方面，但在实践中却表现出较大的差异性和不确定性。公平性经常被排在较低优先级，且存在显著的知识缺口。这表明有必要与相关方协商确定清晰而具体的公平性定义、相应的评价标准以及正式化的过程，以便更有效地将公平性原则融入AI/ML项目的开发过程之中。

Conclusion: 该研究表明，在AI/ML软件开发过程中，虽然人们已经认识到公平性的重要性，但在实际操作中仍面临诸多挑战，包括缺乏统一的公平性定义、评估指标不足以及正式化流程缺失等问题。因此，未来工作应致力于加强跨学科合作，制定更加具体可行的指导方针，促进公平性成为AI项目不可或缺的一部分。

Abstract: Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.

</details>


### [2] [Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors](https://arxiv.org/abs/2512.13860)
*Henger Li,Shuangjie You,Flavio Di Palo,Yiyue Qian,Ayush Jain*

Main category: cs.SE

TL;DR: 提出了一种名为VGCO的框架，该框架利用大型语言模型作为编辑器来自动优化工具相关的文档和知识库上下文。通过两阶段工作流程，包括评估和优化，VGCO能够识别并修正工具与上下文之间的不匹配问题，从而提高准确性、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的工具文档和知识库通常面向人类用户编写，对于大型语言模型来说可能存在信息解读上的偏差。尤其是在工业环境中，由于工具数量众多且功能重叠，导致了可扩展性、变化性和模糊性方面的挑战。

Method: 提出了一个名为Verification-Guided Context Optimization (VGCO)的框架，该框架分为两个阶段：首先是通过收集实际失败案例来评估工具与上下文之间的不匹配；其次是通过离线学习执行层次化的编辑任务，采用结构感知的上下文优化方法。

Result: VGCO能够在单次调用工具的大规模场景下显著提升准确性、鲁棒性和泛化性能。此外，它还支持成本效益高的子任务专业化处理。

Conclusion: 本研究介绍了一种新的方法用于改进大型语言模型在使用外部工具时的表现，通过自动化地优化相关文档及背景资料，解决了现有材料与模型需求之间存在的错位问题。

Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.

</details>


### [3] [Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming](https://arxiv.org/abs/2512.13914)
*Bhargav Chickmagalur Nanjundappa,Spandan Maaheshwari*

Main category: cs.SE

TL;DR: ContextBranch, a conversation management system, improves the performance of Large Language Models (LLMs) in multi-turn and exploratory programming tasks by applying version control semantics to LLM interactions. It provides features like checkpoint, branch, switch, and inject, which help users to manage conversation state, explore alternatives, and merge insights, leading to higher response quality and better context awareness.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the significant performance drop (39% on average) that Large Language Models (LLMs) experience during multi-turn conversations, where they tend to make premature assumptions and fail to adjust their course. This is especially problematic for exploratory programming, where developers need to consider multiple approaches without being constrained by a single line of inquiry. The paper aims to provide a solution that allows users to avoid the dilemma of either continuing in a conversation with polluted context or starting over and losing all prior context.

Method: The method introduced in this paper is ContextBranch, a conversation management system designed to apply version control concepts to interactions with LLMs. It offers four key operations: checkpoint, branch, switch, and inject, which enable users to save conversation states, create separate branches for exploring different lines of thought, switch between these branches, and integrate information from one branch into another. The system was tested through a controlled experiment involving 30 software engineering scenarios, which were crafted to include intentionally confusing elements to simulate real-world complexity.

Result: The results of the evaluation show that branched conversations, facilitated by ContextBranch, led to a marked improvement in the quality of responses from LLMs compared to linear, unbranched conversations. There were notable enhancements in focus and context awareness, particularly in complex scenarios with conceptually distant explorations. Additionally, branching reduced the size of the context by 58.1%, from an average of 31.0 to 13.0 messages, effectively eliminating unnecessary exploratory content.

Conclusion: The conclusion of the paper establishes conversation branching as a critical tool for AI-assisted exploratory work. By allowing for the isolation of different lines of exploration, ContextBranch prevents the pollution of context, thereby enhancing the effectiveness of LLMs in supporting complex, multi-turn conversations. This approach is shown to be particularly beneficial in handling the intricacies of exploratory programming tasks.

Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.

</details>


### [4] [PerfCoder: Large Language Models for Interpretable Code Performance Optimization](https://arxiv.org/abs/2512.14018)
*Jiuding Yang,Shengyao Lu,Hongxuan Liu,Shayan Shirahmad Gale Bagi,Zahra Fazel,Tomasz Czajkowski,Di Niu*

Main category: cs.SE

TL;DR: 本文提出了一种名为PerfCoder的大型语言模型，专门用于通过可解释的定制优化生成性能增强代码。该模型在经过精心挑选的真实世界优化轨迹上进行微调，并通过运行时测量进行偏好对齐强化微调，从而能够直接提出并应用针对输入的改进策略。实验表明，PerfCoder在运行时间加速和有效优化率方面均优于现有模型，证明了性能优化不仅需要规模，还需要优化策略意识。此外，它还能生成关于源代码的可解释反馈，进一步提高代码优化效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自动代码生成领域取得了显著进步，但它们生成高性能代码的能力仍然有限，这在实际软件系统中是一个关键需求。目前的LLMs由于数据稀缺以及缺乏指导可解释和有效性能改进的监督而面临挑战。

Method: 作者们提出了PerfCoder，这是一种专为从源代码生成性能增强代码设计的语言模型系列，通过可解释的、定制化的优化来实现。PerfCoder基于一个精选的真实世界优化路径集进行微调，这些路径带有易于人类理解的注释，并且通过使用运行时度量进行偏好一致性强化微调，使其能够直接建议并实施特定于输入的改进措施而不依赖于迭代完善过程。

Result: 在PIE代码性能基准测试中，PerfCoder在运行速度提升和有效优化比率两方面都超过了所有现有的模型，显示出单靠扩大规模无法达到性能优化的目的，必须具备优化策略意识。另外，当把PerfCoder生成的关于源代码的可解释性反馈作为更大规模LLM的输入时，在规划者与优化者协作工作流程下，可以进一步改善结果，特别地，对于32B规模模型及GPT-5而言，其表现被显著提升至新的水平。

Conclusion: 研究证明，为了产生高性能代码，除了增加模型规模外，还需要注重优化策略的认识。PerfCoder提供了一种新方法，通过可解释的定制化优化来生成性能增强代码，同时也能给出源代码的具体改进建议，促进了更高效的工作流程。

Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

</details>


### [5] [PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design](https://arxiv.org/abs/2512.14233)
*Ruozhao Yang,Mingfei Cheng,Gelei Deng,Tianwei Zhang,Junjie Wang,Xiaofei Xie*

Main category: cs.SE

TL;DR: 本文提出了PentestEval，这是一个全面的基准测试工具，用于评估大型语言模型（LLMs）在六个分解的渗透测试阶段中的表现。通过346个任务和12个现实易受攻击场景的自动化评估管道，揭示了现有LLMs在渗透测试工作流程各阶段表现普遍较弱的问题，并强调了模块化增强每个单独阶段以提高整体性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的渗透测试工作流程高度依赖于人工且难以扩展，尽管大型语言模型（LLMs）为自动化提供了机会，但目前的应用缺乏任务分解或领域适应性，导致模型行为不可靠且对渗透测试各阶段能力了解有限。

Method: 开发了一个名为PentestEval的综合基准，它将专家标注的真实数据与全自动评估流水线相结合，涵盖了从信息收集到漏洞利用生成和修订等六个分解的渗透测试阶段。

Result: 九种广泛使用的LLMs在各个渗透测试阶段的表现普遍较差；端到端流水线的成功率仅为31%；基于现有LLM的系统如PentestGPT、PentestAgent和VulnBot也显示出类似的局限性。

Conclusion: 自主渗透测试需要更强的结构化推理能力，而模块化方法可以加强每一个独立阶段并改善总体表现。PentestEval为未来针对细粒度、阶段级别的评估研究奠定了基础，促进了更可靠的基于LLM自动化的进展。

Abstract: Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.

</details>


### [6] [Aligning Security Compliance and DevOps: A Longitudinal Study](https://arxiv.org/abs/2512.14453)
*Fabiola Moyón,Florian Angermeir,Daniel Mendez,Tony Gorschek,Markus Voggenreiter,Pierre-Louis Bonvin*

Main category: cs.SE

TL;DR: 本文介绍了RefA框架，这是一种基于IEC 62443-4-1标准的安全合规DevOps生命周期模型。它旨在帮助企业向DevOps转型的同时符合安全规范，并支持将安全合规知识转移给产品开发团队，确保跨职能团队拥有交付合规产品的所有必要技能。


<details>
  <summary>Details</summary>
Motivation: 公司采用敏捷方法论和DevOps来促进软件密集型产品的高效开发与部署，但同时也带来了如何在更灵活的工作流程中遵守安全标准的挑战，特别是对于关键基础设施相关的产品和服务工程来说。为了帮助公司在转向DevOps的过程中克服这些难题，提出了一个根据安全法规和标准调整后的DevOps方案。

Method: 研究者们通过在西门子公司的长期案例研究，包括几个单独的子研究，来设计、验证并初步采用了基于RefA（一种描述性的、遵循IEC 62443-4-1标准的安全合规DevOps生命周期模型）的整体框架。

Result: 研究表明，RefA能够有效地将安全合规的知识转移到产品开发团队中，从而支持敏捷性目标，即确保跨功能团队具备交付符合安全规范的产品所需的所有技能。

Conclusion: 所提出的基于RefA的框架为专业人士提供了一种实用的方法，在实施DevOps流程时保持与安全规范的一致性，不仅限于安全专家使用。

Abstract: Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.

</details>


### [7] [Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests](https://arxiv.org/abs/2512.14475)
*Johann Glock,Clemens Bauer,Martin Pinzger*

Main category: cs.SE

TL;DR: 本文提出了一种基于语义的方法，通过单路径符号分析从实现中提取规范，自动将单元测试转换为属性测试，并开发了原型工具Teralizer。实验结果表明，Teralizer在某些情况下可以提高变异分数，但在实际应用中仍面临类型支持和静态分析限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的单元测试仅验证单一输入-输出对，使得执行路径中的大多数输入未被测试到。而属性基础测试虽然可以通过生成满足特定属性的多个输入来解决这个问题，但定义这些属性及其约束需要大量的人工努力。

Method: 提出一种基于语义的方法，利用单路径符号分析技术自动从已有实现中抽取规范，并据此将单元测试转换为属性基础测试。为此，作者们开发了一个名为Teralizer的原型工具，该工具能够将Java语言编写的JUnit测试转换成jqwik属性测试。

Result: 通过对三个难度递增的数据集进行评估发现，在处理由EvoSuite生成的针对EqBench及Apache Commons实用程序的测试时，Teralizer能将突变率提高1至4个百分点；但对于经验丰富的开发者所编写且已相当完善的测试用例，则仅提高了0.05至0.07个百分点。此外，分析了来自RepoReapers的632个真实世界Java项目后指出，由于符号分析过程中存在的类型支持局限性以及当前原型工具的静态分析能力有限，只有大约1.7%的项目成功完成了整个泛化流程。

Conclusion: 研究展示了如何通过自动化手段改善现有测试的有效性，同时也指出了未来工作需关注的研究与工程难题，包括增强符号分析的类型支持范围、改进静态分析技术等，以推动测试泛化领域的进步。

Abstract: Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.
  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.
  Artifacts available at: https://doi.org/10.5281/zenodo.17950381

</details>


### [8] [MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development](https://arxiv.org/abs/2512.14613)
*Cristiano Welter,Kleinner Farias*

Main category: cs.SE

TL;DR: 本研究提出了一种基于模型的方法——万物模型（MoT），该方法结合了低代码原则，旨在简化云物联网（CoT）应用的开发。通过案例研究和技术接受模型（TAM）问卷调查验证了MoT的有效性，结果显示它能够降低技术门槛、提高开发效率，并且对于IoT经验有限的用户来说也非常易于使用。


<details>
  <summary>Details</summary>
Motivation: 尽管云计算与物联网（IoT）的融合对于构建可扩展、智能系统至关重要，但开发云物联网应用仍然面临诸多挑战，包括需要深厚的技术专长以及缺乏标准化、模型驱动的方法论等问题。当前解决方案未能充分保证互操作性、自动化水平及效率。

Method: 提出了一个名为Model of Things (MoT) 的基于模型的方法，该方法采用了低代码原则来简化云物联网(CoT)应用程序的开发过程。MoT通过提供专门为物联网和云服务设计的定制UML配置文件来减少技术障碍。为了评估MoT的效果，进行了案例研究并发放了技术接受模型(TAM)问卷。

Result: 结果证实了MoT的可行性，表明它可以简化CoT应用程序的开发和部署流程。即使是对IoT不太熟悉的用户也觉得MoT非常容易上手，反馈显示其具有很高的易用性和实用性。质性反馈进一步强调了MoT在减少复杂性和加快开发速度方面的优势。

Conclusion: MoT为CoT应用程序开发提供了一个有前景的模型驱动解决方案。通过降低入门门槛和促进自动化，它提高了效率和灵活性。这项研究标志着朝着更加用户友好的框架迈进了一步，有助于更广泛地采用CoT技术。

Abstract: The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.

</details>


### [9] [Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI](https://arxiv.org/abs/2512.14673)
*Ronnie de Souza Santos,Cleyton Magalhães,Italo Santos*

Main category: cs.SE

TL;DR: 该论文讨论了基于大语言模型的聊天机器人在技术、教育和分析领域的中心作用，同时指出了当前对这些系统环境影响的评估主要集中在模型架构、硬件效率和部署基础设施上，而忽视了用户交互行为对能源消耗的影响。文章从四个维度探讨了这一问题，并提出需要重新思考聊天机器人的设计方式，以实现更可持续的使用。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型（LLM）的聊天机器人在多个领域中的应用日益广泛，关于其可持续性的担忧也在增加。尽管目前对于减少这类系统能耗的努力多集中于优化模型架构和技术基础设施方面，但较少有人关注到用户与系统交互的方式本身也是影响能耗的关键因素之一。

Method: 本文采用了一种视角性方法来探讨用户交互如何影响LLM系统的能耗情况。通过分析四种不同维度下的具体表现——包括对话长度导致的计算成本上升、即时响应需求限制了节能调度的可能性、日常使用习惯带来的累积运营负担以及上下文积累对内存要求的影响——作者提出了新的见解。

Result: 研究表明，用户的交互模式显著地影响着基于LLM的系统的能源消耗。例如，长时间的对话增加了推理过程中的计算开销；用户期待快速反馈的做法减少了实施节能措施的机会；长期形成的使用习惯造成了难以量化的总能量需求增长；并且随着对话过程中信息量的增长，系统保持高效运行变得更加困难。

Conclusion: 为了构建更加环保的聊天机器人服务，有必要重新考虑并调整用户与LLM系统之间的交流方式。这不仅涉及技术层面的改进，还需要从社会文化角度出发，鼓励形成有利于节约资源的新型对话规范。

Abstract: LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 研究发现，当对大型语言模型进行专门任务（如数学推理）的微调时，会出现灾难性遗忘的问题。通过混合训练策略，即在训练过程中交错使用数学和自然语言推理的例子，可以完全消除这种遗忘现象，同时保持数学性能不变。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在特定任务（比如数学推理）微调时出现的灾难性遗忘问题进行了探讨，旨在寻找一种方法来避免模型在学习新技能的同时丧失已有的能力。

Method: 研究人员通过对Flan-T5-Base模型使用DeepMind Mathematics数据集进行微调，并且在训练过程中采用了不同的混合训练策略，包括将数学与自然语言推理(NLI)例子按照不同比例混合的方式来进行实验。

Result: 结果表明，采用平衡的1:1比率进行混合训练不仅能够达到与仅用数学训练相同水平的数学准确性(12.0%)，而且还几乎保持了原有的NLI准确性(86.2%)。此外，即使是非常少量的NLI样例混入也能有效防止遗忘发生。

Conclusion: 本研究表明，通过适当的混合训练方法可以让模型在专业化训练的过程中不必牺牲其通用能力，这对于更大规模模型的应用具有重要意义。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [11] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 介绍了一个名为Cornserve的系统，该系统能够高效地为多模态Any-to-Any模型提供在线服务。通过让模型开发者描述通用Any-to-Any模型的计算图，并自动找到优化部署方案，Cornserve在执行过程中有效地处理了异构性问题。评估显示，与现有解决方案相比，Cornserve可以将吞吐量提高至3.81倍，尾部延迟降低至5.79倍。


<details>
  <summary>Details</summary>
Motivation: 针对新兴的一类称为Any-to-Any的多模态模型，这些模型接受文本和多模态数据（如图像、视频、音频）作为输入，并生成组合的数据作为输出。这引入了请求类型、计算路径以及计算扩展上的异质性，需要一个能有效处理这种异质性的在线服务系统。

Method: 设计并实现了Cornserve系统，它允许模型开发者定义通用Any-to-Any模型的计算图，包括多模态编码器、自回归模型（例如大型语言模型LLMs）、多模态生成器等不同组件。Cornserve的规划器根据模型和工作负载特性自动找到最佳部署计划，包括是否及如何基于这些特征分解模型为更小的部分。其分布式运行时则按照计划执行模型，以高效管理在线服务过程中的Any-to-Any模型异质性。

Result: 实验结果表明，相比于现有的解决方案，Cornserve能够为多种不同的Any-to-Any模型和服务负载提供支持，显著提高了服务效率，最高可达到3.81倍的吞吐量改进以及高达5.79倍的尾部延迟减少。

Conclusion: Cornserve是一个高效的在线服务系统，特别适用于处理具有异质性的多模态Any-to-Any模型。通过自动化的模型部署策略和优化的分布式执行机制，它极大地提升了这类模型的服务性能。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [12] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 本研究结合合成孔径雷达（SAR）图像与环境和水文数据，对肯尼亚西部Nyando河流域的洪水易感性进行了建模。通过比较四种机器学习模型的表现，发现随机森林（RF）在预测性能上最佳，并据此绘制了洪水易感性地图，为灾害风险降低、土地利用规划及早期预警系统开发提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，对生态系统、基础设施以及人类生计构成严重威胁。这项研究旨在通过结合SAR影像与多种环境因素来提高洪水易发地区的识别精度，特别是在数据有限的情况下。

Method: 研究使用2024年5月洪水事件期间获取的Sentinel-1双极化SAR数据生成二进制洪水库存作为训练数据，并选择了六个条件因子（坡度、海拔、朝向、土地利用/覆盖类型、土壤类型及距溪流距离）与SAR导出的洪水库存一起用于训练四种监督分类器：逻辑回归（LR）、分类回归树（CART）、支持向量机（SVM）和随机森林（RF）。

Result: 结果表明，随机森林（RF）达到了最高的预测性能（准确率=0.762；Kappa系数=0.480），优于其他三种方法。基于RF的易感性地图显示，靠近维多利亚湖的低洼Kano平原地区具有最高的洪水脆弱性，这与历史洪水记录及2024年5月事件的影响相符。

Conclusion: 研究表明，将SAR数据与集成机器学习方法相结合，在数据有限区域进行洪水易感性制图是非常有价值的。所得地图为减少灾害风险、土地利用规划及早期预警系统的开发提供了重要信息。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [13] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hessian Reassignment的方法，用于文档分类器中的类级别遗忘学习。该方法通过两步模型无关的解决方案，在保持非删除类准确率的同时，显著降低了执行时间，并且减少了被移除类别的成员推理优势。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘学习的目标是在不完全重新训练的情况下有效移除特定训练数据对模型的影响。尽管在大型语言模型（LLMs）的遗忘学习方面取得了很大进展，但针对文档分类模型的研究相对较少。本文旨在为文档分类器开发一种有效的类级别遗忘学习方法。

Method: 提出了Hessian Reassignment，这是一种两步、模型无关的解决方案。第一步是进行单次影响式更新，通过使用共轭梯度解决Hessian-向量系统来减去目标类别所有训练点的贡献；第二步与随机重分类删除类样本的常见基准不同，本方法通过Top-1分类实施决策空间保证。

Result: 在标准文本基准测试中，Hessian Reassignment达到了接近于完全无类重训练的保留类准确率，同时运行速度提高了几个数量级。此外，它还持续降低了基于池化的多影子攻击测量到的被移除类别的成员推理优势。

Conclusion: 这些结果表明，Hessian Reassignment提供了一条实用且有原则的道路，以实现文档分类中高效地进行类级别遗忘学习。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [14] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 本研究开发了一种机器学习框架，通过整合废水监测、气象和空气质量数据来预测美国与RSV相关的住院情况。废水中的RSV水平是最重要的预测因子，其次是温度、臭氧水平和特定湿度等环境变量。研究还发现美洲原住民和阿拉斯加原住民的RSV相关住院率显著较高，并且高海拔州的RSV相关住院率也持续较高。为了便于访问和实用，研究人员开发了一个交互式的R Shiny仪表板，允许用户探索不同州的RSV相关住院风险等级，可视化关键预测因素的影响，并生成RSV爆发预测。


<details>
  <summary>Details</summary>
Motivation: 鉴于呼吸道合胞病毒（RSV）是幼儿住院的主要原因之一，其暴发受环境条件强烈影响，本研究旨在通过结合多种数据源建立模型来预测与RSV相关的住院情况，以期为公共卫生干预措施提供及时支持。

Method: 本研究采用CART、随机森林和提升算法等多种分类模型，基于综合了每周住院率、废水RSV水平、每日气象测量值及空气污染物浓度的数据集进行训练。

Result: 废水中的RSV水平被确定为最强预测因子，同时温度、臭氧水平以及特定湿度等气象和空气质量变量也被证明具有重要影响。研究还揭示了美洲原住民与阿拉斯加原住民之间存在显著更高的RSV相关住院率。此外，位于高海拔地区的州显示出持续较高的RSV相关住院率。

Conclusion: 研究表明，结合环境与社区监控数据能够有效预测RSV暴发，从而帮助更及时地采取公共卫生干预措施并合理分配资源。为此，研究团队开发了一个互动式R Shiny应用程序，方便用户探索各州的RSV相关住院风险级别、查看关键预测因素的影响，并生成预测结果。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [15] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 提出了一种两阶段联邦少样本学习(FFSL)框架，用于个性化的基于EEG的癫痫检测。该方法在不集中EEG记录的情况下，通过联邦学习实现了跨站点的知识共享和个人适应。实验结果表明，在实际的数据可用性和隐私限制条件下，FFSL能够支持有效的患者自适应癫痫检测。


<details>
  <summary>Details</summary>
Motivation: 当前基于EEG的癫痫检测深度学习方法大多依赖于大型集中标注数据集，但临床实践中EEG数据稀缺、分布分散且受严格隐私法规约束，这使得开发实用的AI癫痫检测模型变得困难。

Method: 设计了一个两阶段的联邦少样本学习(FFSL)架构。第一阶段使用联邦学习对预训练的生物信号变换器(BIOT)进行微调以实现跨非IID模拟医院站点的共享表示学习；第二阶段则通过联邦少样本个性化仅用五个标记EEG片段来为每位患者调整分类器。

Result: 联邦微调阶段达到了0.43的平衡准确率（集中式处理下为0.52），Cohen's kappa系数为0.42（集中式处理下为0.49），加权F1得分为0.69（集中式处理下为0.74）。在FFSL阶段，针对不同事件分布的四个站点，客户端特定模型平均达到0.77的平衡准确率、0.62的Cohen's kappa系数以及0.73的加权F1得分。

Conclusion: 研究表明，所提出的FFSL框架能够在考虑到真实世界中的数据可用性和隐私保护要求的同时，有效地促进个体化癫痫检测。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [16] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 本文探讨了在用户时间有限的情况下，如何通过强化学习算法同时学习用户偏好和时间预算，以提高电子商务场景中的推荐参与度。研究提出了一个统一的时间约束推荐模型，并通过实验证明该方法在紧张的时间预算下比传统的基于上下文强盗的方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统推荐任务没有考虑到用户时间有限这一关键资源限制，这要求推荐系统必须平衡物品的相关性和评估成本。本研究旨在解决这一问题，通过强化学习算法来同时理解用户的偏好及他们的时间预算，从而在资源受限条件下制定出更具吸引力的推荐方案。

Method: 采用阿里巴巴的个性化重排序数据集，在电子商务环境下支持版面优化的研究中，探索使用强化学习为用户推荐商品。提出了一种将时间约束推荐建模为具有预算意识效用的马尔可夫决策过程(MDPs)的统一公式；构建了一个模拟框架来研究重新排序数据上的策略行为。

Result: 实验结果表明，在时间预算紧张的情况下，无论是在线还是离线控制方法都能够比传统的基于上下文强盗的方法提供更好的性能表现。

Conclusion: 通过引入考虑用户时间预算因素的强化学习算法，可以有效提升推荐系统的参与度。本研究不仅为处理时间敏感型推荐问题提供了新的视角，还展示了MDP与预算意识效用相结合的优势。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [17] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的复合无分类器引导（CCFG）方法，用于改进扩散模型在风数据超分辨率任务中的性能。通过这种方法，WindDM模型在工业规模的风动态重建中实现了深度学习模型中的最先进重构质量，并且成本比传统方法降低了高达1000倍。


<details>
  <summary>Details</summary>
Motivation: 高分辨率和高精度的风数据对于天气建模问题非常重要，但获取这样的数据既困难又昂贵。现有的解决方案要么经济实惠但不够准确，要么准确性高但成本高昂。此外，风数据与自然图像不同，通常需要更多的输入通道。为了利用扩散模型处理大量条件变量的优势，作者开发了一种改进的方法。

Method: 文章介绍了一种对多个条件输入进行概括化的无分类器引导(CFG)技术，称为复合无分类器引导(CCFG)。该技术可以应用于任何使用标准CFG dropout训练过的预训练扩散模型。基于此技术，研究人员开发了WindDM，一个专为工业规模风动力学重建设计的扩散模型。

Result: 实验表明，在风数据超分辨率任务上，CCFG生成的结果比传统的CFG具有更高的保真度。WindDM模型在深度学习模型中达到了最先进的重建质量，并且其成本相较于经典方法减少了最多1000倍。

Conclusion: 通过引入CCFG以及开发WindDM模型，研究者们成功地提高了风数据超分辨率的质量同时大幅降低了成本，为相关领域提供了更高效、更经济的解决方案。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [18] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的物理反演求解器（PIS），它能够从任意观测集进行反演，尤其在观测数据极其稀疏的情况下也能保持稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习和算子学习模型在处理稀疏、不规则且受限于实际传感器布置的有限间接测量时表现不佳，特别是在流体力学、地震反演和结构健康监测等领域中，这些模型往往无法提供可靠的反演结果，并且缺乏不确定性量化。

Method: PIS采用基于集合变换器的编码器来处理任意数量或几何形状的测量值，并通过余弦退火稀疏性课程提高鲁棒性。此外，还进行了信息论分析以揭示不同物理系统下观测熵的变化，从而了解在极端稀疏情况下的反演极限。

Result: PIS在三个具有挑战性的偏微分方程反问题上进行了评估：达西流、波场反演（亥姆霍兹方程）以及结构健康监测（胡克定律）。实验表明，在所有任务及稀疏度条件下，包括观察率仅为0.29%的极端情况下，PIS相较于现有方法能够显著减少反演误差并可靠地生成校准后的后验样本。

Conclusion: PIS作为一种强大的通用解决方案，在面对任意且严重欠采样的观测数据时表现出独特的稀疏性韧性，能够准确反映数据稀缺性和内在物理不确定性。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [19] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: 本文提出了一种名为Learning to Low-Rank Compress (LLRC)的新方法，该方法通过直接学习选择奇异值的掩码权重来压缩大型语言模型，而无需微调。实验表明，在不同压缩率下，LLRC在常识推理和开放领域问答任务上优于其他不需要后压缩微调的排名选择方法，并且与需要微调的方法相比也具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前用于压缩大型语言模型的方法要么依赖于可能由于搜索空间有限而导致次优结果的启发式方法，要么是基于梯度但没有微调时表现不如启发式方法。因此，本文旨在开发一种新的方法，能够有效选择每层的最佳秩，同时优化压缩率和下游任务准确性。

Method: 提出了一种称为Learning to Low-Rank Compress (LLRC)的方法，它是一种基于梯度的方法，能够在不进行微调的情况下直接学习选定奇异值的掩码权重。使用校准数据集训练这些掩码权重，以减少所选奇异值数量的同时最小化中间激活与原始模型之间的差异。

Result: LLRC方法在各种压缩率下的常识推理和开放领域问答任务上超越了其他不需要后压缩微调的竞争排名选择方法。例如，在Llama-2-13B上实现20%压缩率时，LLRC在MMLU、BoolQ和OpenbookQA上的表现分别比STRS高出12%、3.5%和4.4%。此外，LLRC还持续优于SVD-LLM和LLM-Pruner无微调版本的表现，并且与LLM-Pruner的微调版本相比也具有竞争性。

Conclusion: LLRC提供了一种有效的解决方案，能够在不牺牲太多性能的前提下显著减小大型语言模型的大小。这种方法不仅在不需要额外微调的情况下表现出色，而且对于不同的数据集和压缩比率都保持了良好的通用性和竞争力。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [20] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出了一种基于参数高效微调（PEFT）的联邦推荐（FR）训练框架，旨在减少需要传输的嵌入参数量，从而降低通信开销并提高准确性。该框架采用轻量级、插件式设计，可与现有FR方法无缝集成，并引入了包括LoRA、哈希编码以及一种新的RQ-VAE策略在内的多种PEFT技术。实验表明，此方法在不同FR模型和数据集上均能有效减少通信负担且提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着云边协同的发展，推荐服务越来越多地在分布式环境中进行训练。联邦推荐(FR)允许多端协作训练同时通过共享模型参数而非原始数据来保护隐私。然而，由于大量项目嵌入导致的庞大参数数量极大地阻碍了通信效率。尽管现有研究主要集中在提高FR模型的效率上，但它们很大程度上忽略了嵌入参数开销的问题。

Method: 提出了一种基于参数高效微调(PEFT)的联邦推荐训练框架，该框架设计用于减少需要传输的嵌入参数量。方法中采用了轻量级、插件式的解决方案，能够轻松集成到现有的FR方法中。此外，在框架内不仅整合了常见的PEFT技术如LoRA和基于哈希的编码，还探索了使用残差量化变分自编码器(RQ-VAE)作为一种新颖的PEFT策略。

Result: 广泛的实验表明，所提出的框架能够在不同的FR模型骨干和数据集上显著减少通信开销，同时还能提高推荐准确性。

Conclusion: 本研究为解决联邦推荐系统中的大体积嵌入参数传输问题提供了一个有效的解决方案，通过引入参数高效微调技术，实现了通信效率的大幅提升及推荐质量的改善。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [21] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为DARTs的鲁棒长短期双路径框架，该框架具有窗口感知时空软融合机制，旨在解决高维噪声时间序列中长时间空间依赖性捕捉的问题。通过多视图稀疏图学习器、扩散多关系图单元和多尺度时空图构造器三个互补组件，实现了对复杂异常模式的有效识别和定位。实验结果表明了DARTs在主流数据集上的优越性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法难以在从高维噪声时间序列中稳健地捕捉长时间空间依赖性的问题，研究者们提出了DARTs框架，旨在提高大规模工业控制系统中多元时间序列异常检测的准确性和鲁棒性。

Method: DARTs框架由三个互补部分组成：短期路径中的多视图稀疏图学习器与扩散多关系图单元合作自适应地捕捉高噪声时间序列中的层次区分性短时空间模式；长期路径中的多尺度时空图构造器用于建模高维表示空间内的显著长期动态；以及一个窗口感知的时空软融合机制，用以过滤残余噪声并平滑整合异常模式。

Result: 广泛的定性和定量实验结果展示了DARTs相比其他方法在主流数据集上的优越性和鲁棒性。此外，一系列消融研究还探索了所提议组件的关键设计因素。

Conclusion: 本研究提出的DARTs框架有效地解决了高维噪声时间序列中长时间空间依赖性的捕捉问题，并且在多元时间序列异常检测方面表现出了优越性能。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [22] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出了一种用于检测重度抑郁症(MDD)的时间-频率融合和多域交叉损失(TF-MCL)模型，该模型通过融合映射头(FMH)生成时间-频率混合表示，并通过优化多域交叉损失函数来提高模型获取融合表示的能力。在公开数据集MODMA和PRED+CT上的实验表明，TF-MCL模型的准确率显著优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 监督学习方法在MDD检测中过度依赖标签，而现有的对比学习方法未能有效表征EEG信号的时间-频率分布，且其获取低语义数据表示的能力不足。为解决这些问题，提出了TF-MCL模型以改进MDD检测任务中的表现。

Method: 提出一种名为TF-MCL的新模型，它利用融合映射头(FMH)生成时间-频率混合表示，并通过多域交叉损失函数来重构这些表示在时频域及融合域中的分布，进而增强模型合成时间-频率信息的能力。

Result: 在两个公开可用的数据集（MODMA和PRED+CT）上测试了TF-MCL模型，结果显示与当前最先进的方法相比，分别提高了5.87%和9.96%的准确性。

Conclusion: TF-MCL模型通过有效地结合时间-频率信息并优化跨域损失函数，在MDD检测任务中表现出色，显著提高了识别精度。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [23] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 本文提出了层流假设，通过一种新的零样本度量——层间余弦速度的方差，来检测大型语言模型在面对对抗性提示时的语义湍流现象。实验结果表明，这种语义湍流不仅可以作为轻量级实时越狱检测器，还可以用作对黑盒模型底层安全架构进行分类的非侵入式诊断工具。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）变得越来越普遍，保护这些模型免受对抗性“越狱”攻击的需求也日益增加。当前的防御策略通常依赖于计算成本高昂的外部分类器或脆弱的词汇过滤器，忽视了模型推理过程中内在动态的重要性。

Method: 提出了一种名为层流假设的新理论，该理论认为良性输入会在LLM的高维潜在空间中引起平滑、渐进的转换，而对抗性提示则会由于安全对齐与指令遵循目标之间的内部冲突引发混乱且高方差的轨迹。这一现象通过一个新的零样本度量——层间余弦速度的变化率被形式化。

Result: 实验评估显示，对于不同规模的小型语言模型，所提出的度量方法展现了显著的诊断能力。特别是经过RLHF调整的Qwen2-1.5B模型，在受到攻击时其湍流程度增加了75.4%；而Gemma-2B模型则表现出22.0%的湍流减少，这表明前者存在明显的内部冲突机制，后者则是低熵的‘反射式’拒绝机制。

Conclusion: 研究表明，语义湍流不仅能够作为一个轻量级、实时的越狱探测器发挥作用，而且还可以作为一种非侵入式的诊断工具，用于揭示和分类黑盒模型的安全架构类型。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [24] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 本研究对比了在资源受限环境下，基于词嵌入的方法（如Word2Vec、GloVe和句子转换器）结合梯度提升技术对财经新闻情绪分类的效果。实验结果显示，在数据量不足的情况下，预训练的词嵌入表现不佳，且小规模验证集容易导致过拟合问题。研究建议，在标注样本稀缺时，可以考虑使用少样本学习、数据增强或混合方法等替代方案。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析有助于更好地理解市场动态，但传统的自然语言处理方法在应用于小数据集时面临很大挑战。本研究旨在评估资源受限条件下不同词嵌入方法对于财经新闻情感分类的有效性，并探索解决数据稀缺问题的新途径。

Method: 采用Word2Vec、GloVe以及句子转换器三种不同的词嵌入表示方法与梯度提升算法相结合，对人工标注的财经新闻标题进行情感分类实验。通过对验证集与测试集性能差异的研究来评估模型泛化能力及过拟合情况。

Result: 实验发现，尽管部分模型在验证阶段表现出色，但在测试阶段其性能远低于简单基线水平；预训练词向量在数据量达到一定阈值前收益递减；此外，较小规模的验证集会导致模型选择过程中出现过拟合现象。

Conclusion: 单纯依靠提高词嵌入质量无法根本解决情感分类中的数据稀缺问题。对于那些可用资源有限的从业者来说，当面临标注样本不足的情况时，应当考虑采取少量样本学习、数据扩充或者基于词典的混合方法等策略。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [25] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于采样的权重空间投影方法SCPO，该方法能够在无需约束函数梯度的情况下直接在参数空间中实施安全策略学习。通过结合轨迹展开与平滑性界限来构建局部安全区域，并使用凸二阶锥规划（SOCP）进行每一步的梯度更新投影，从而确保从任何安全初始化开始的所有中间策略都保持安全。此外，在具有稳定备份策略的受限控制场景中，该方法还能保证闭环稳定性并支持超出保守备份的安全适应。实验结果表明，该方法能够一致地拒绝不安全更新、在整个训练过程中保持可行性，并且实现了有意义的主要目标改进。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的学习任务中，需要开发出既能提高性能又不会离开安全操作范围的策略。本文针对模型参数必须满足未知且基于执行过程的安全约束条件下的策略学习问题进行了研究。

Method: 提出了SCPO，一种基于采样权重空间投影的方法，它不需要访问约束函数的梯度就能直接在参数空间中强制执行安全性。该方法通过将轨迹滚动与平滑性边界结合起来构造局部安全区域，并利用凸二阶锥规划(SOCP)对每次梯度更新进行投影，以产生一个安全的一阶梯度步骤。

Result: 实验显示，无论是在有有害监督的回归任务还是存在恶意专家的受限制双积分器任务上，所提方法都能持续拒绝不安全更新，在整个训练期间维持可行性，并取得显著的主要目标改进。

Conclusion: SCPO方法提供了一种有效的解决方案，能够在参数空间内直接处理安全约束，而无需依赖于约束函数的具体形式或其梯度信息。这为安全关键型学习任务中的策略优化开辟了新途径。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [26] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 本文提出了一种名为Cross-Trace Verification Protocol (CTVP)的新AI控制框架，用于验证不信任的代码生成模型。通过分析这些预测轨迹中的一致性模式来检测表明后门的行为异常，并引入了对抗鲁棒性商(ARQ)的概念，证明了语义轨道分析为代码生成任务提供了可扩展且理论基础扎实的AI控制方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地在最少的人类监督下生成代码，这引发了关于后门注入和恶意行为的关键担忧。

Method: 提出了Cross-Trace Verification Protocol (CTVP)，一种新的AI控制框架，该框架通过对语义等价程序转换中的执行跟踪进行预测分析来验证不受信任的代码生成模型。

Result: 研究表明，语义轨道分析提供了一种可扩展、理论上站得住脚的方法来进行AI控制以应对代码生成任务。同时，引入了Adversarial Robustness Quotient (ARQ)这一概念，用来量化相对于基线生成的验证计算成本，显示了随轨道大小呈指数增长的关系。

Conclusion: 本研究证明了语义轨道分析为代码生成任务提供了一种可扩展且具有理论基础的方法来进行AI控制，且由于空间复杂度的基本限制，对手无法通过训练改善情况。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [27] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种通过纠正导致不满意响应的原因来改进强化学习从人类反馈中学习（RLHF）的方法。方法分为两部分：后验解释方法和非学习方法，旨在识别导致不满意响应的训练数据，并通过取消学习这些数据来改善响应质量，同时不显著降低对其他提示的满意响应。实验结果表明该算法可以有效改进RLHF。


<details>
  <summary>Details</summary>
Motivation: 观察到通过RLHF调整的语言模型仍然可能产生不满意的响应，因此研究如何将人类通过寻找并纠正原因来改进不满结果这一策略应用于语言模型的对齐上。

Method: 提出了一个包含两个部分的方法：1) 后验解释方法，用于确定哪些训练数据导致了特定的不满意响应，通过构建一个约束组合优化问题解决；2) 非学习方法，旨在通过移除引起不满意响应的训练数据来改善响应，同时保持对其他提示的满意水平不受显著影响。

Result: 实验证明所提算法能够有效地提高RLHF的表现。

Conclusion: 本文介绍的方法为提高基于RLHF的语言模型性能提供了一个新的视角，通过直接处理导致不满意输出的具体原因而非仅仅依赖于进一步的训练或调整奖励函数。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [28] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 该论文为二元分类器的$L_1$校准误差估计问题提供了两个贡献：首先，对于校准函数具有有界变差的任何分类器提供了一个上界。其次，提出了一种修改任意分类器的方法，使得其校准误差可以被有效地给出上界，同时不会显著影响分类器性能，并且没有做出任何限制性假设。所有结果都是非渐近的和分布无关的。文章最后提供了如何在实践中测量校准误差的实际建议。


<details>
  <summary>Details</summary>
Motivation: 为了提高二元分类器在校准误差估计上的准确性与实用性，特别是从有限数据集中进行估计时。

Method: 通过理论分析给定校准函数有界变差情况下的误差上界，并提出一种修改分类器的方法来有效控制校准误差上界。

Result: 得出了非渐近性和无分布假设条件下校准误差的有效估计方法，以及在不影响分类器性能的前提下管理校准误差的新方法。

Conclusion: 研究结果为实际应用中准确估计及控制二元分类器的校准误差提供了新途径。

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [29] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 该论文提出了一种端到端的婴儿哭声分析流程，结合了去噪自动编码器、卷积分词器和通过高效联合学习训练的Transformer编码器。系统执行设备上的降噪、自适应分割、事后校准以及基于能量的异常值拒绝。在保护隐私的同时，对噪声具有鲁棒性，并且通信效率高，适用于联邦部署。


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声分类可以辅助早期评估婴儿的需求，但实际应用受限于音频数据的隐私问题、背景噪音敏感性以及录音环境之间的领域偏移。

Method: 研究者开发了一个端到终的婴儿哭声分析流程，集成了去噪自动编码器（DAE）、卷积分词器和使用通信高效联合学习（FL）训练的Transformer编码器。该系统能进行设备端降噪、自适应分割、后处理校准及基于能量的分布外（OOD）拒绝。联合训练采用了带正则化控制变量更新的8位适配器增量方法，在安全聚合下进行。

Result: 利用Baby Chillanto和Donate-a-Cry数据集加上ESC-50噪声叠加，模型达到了0.938的宏观F1分数、0.962的AUC值以及0.032的预期校准误差（ECE），同时将每轮客户端上传量从大约36到42MB减少到了3.3MB。在NVIDIA Jetson Nano (4GB, TensorRT FP16)上实现实时边缘推理，对于每个一秒长的频谱图帧仅需96毫秒。

Conclusion: 这些结果展示了一条实现隐私保护、抗噪声干扰以及通信高效的婴儿哭声分类实用路径，适合联邦部署。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [30] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: 介绍了一种名为OPTIMA的单次后训练剪枝方法，该方法通过解行级二次规划问题来平衡准确性和可扩展性，同时在不进行微调的情况下提高多个大型语言模型家族和稀疏度体制下的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练模型剪枝技术面临一个权衡：简单的启发式权重置零方法速度快但会降低准确性；而基于原则的联合优化方法虽然可以恢复准确性但在现代规模上计算不可行。因此，需要一种既能够保持较高准确性又具有实际操作性的剪枝方法。

Method: 提出了一种新的单次后训练剪枝方法——OPTIMA，它将层间权重重建视为独立的行级二次规划（QP）问题，共享同一个层Hessian矩阵。通过求解这些QP问题得到给定估计Hessian矩阵下每个行向量全局最优更新。此外，还实现了一个加速器友好的QP求解器，使得可以在单个加速器上高效地执行大规模剪枝任务。

Result: 实验结果表明，OPTIMA与现有掩码选择器集成后，在不同大型语言模型家族及稀疏度设置下持续提高了零样本性能，最高可达3.97%的绝对准确率提升。对于80亿参数的Transformer模型，在NVIDIA H100上使用OPTIMA完成端到端剪枝仅需40小时，并且峰值内存占用为60GB。

Conclusion: OPTIMA作为一种实用的单次后训练剪枝方案，成功地在准确性和效率之间找到了新的平衡点，为未来的研究提供了强有力的支持。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [31] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 该研究发现，对于长上下文的大型语言模型（LLM），当前基于推理时间的策略如生成思考标记，存在快速递减的回报，并且在长上下文中失败。提出了一种通过给定上下文的目标梯度更新来克服静态自注意力限制的方法，这种方法在多个模型和长上下文基准测试中表现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管训练和架构策略的进步使得拥有数百万个令牌上下文长度的大型语言模型成为可能，但实证证据表明这些模型能够消耗的文本量远超它们可以可靠利用的范围。此外，在处理涉及多步推理的挑战性任务时，使用推理时计算来扩展LLM的表现也显示出局限性。因此，研究人员旨在探索如何更有效地利用推理时计算以提高长上下文任务下的模型性能。

Method: 研究者们首先通过控制实验对沙箱长上下文任务进行了考察，发现现有推理时策略存在迅速减少的收益并在长上下文中失效的现象。随后，他们提出了一种新方法，通过对给定上下文进行针对性梯度更新，证明可以克服静态自我注意力机制带来的限制。

Result: 所提出的方法相较于传统增加思考标记等手段，在多个长上下文场景下都取得了显著更好的结果。特别是对于Qwen3-4B模型，在LongBench-v2和ZeroScrolls基准测试子集上平均分别实现了12.6和14.1个百分点的改进。

Conclusion: 对于长上下文处理来说，少量针对特定上下文的训练比目前流行的、依靠生成更多思考标记等方式来扩大推理时计算规模的做法更为有效。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [32] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 研究通过使用神经ODE模型来学习两体约化密度矩阵（2RDM）的动态，该模型在无需明确三体信息的情况下可以重现2RDM的动态，但仅限于两体和三体累积量之间皮尔森相关性高的参数区域。对于反相关或不相关的区域，表明没有简单的时域局部函数能够捕捉演化过程。研究结果强调了开发记忆依赖核的重要性，并将神经ODE定位为一种与模型无关的诊断工具，有助于指导非局域闭合方案的发展。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统中的快速关联建立是许多新兴现象的基础。精确波函数方法随着粒子数增加呈指数增长；而更简单的平均场方法则忽略了必要的两体关联。时间依赖的两体约化密度矩阵（TD2RDM）形式提供了一个中间地带，它通过传播2RDM并利用三体累积量重构来关闭BBGKY层级。然而，在不同动力学状态下忽略记忆效应的时间局部重构泛函的有效性和存在性仍不清楚。

Method: 采用神经ODE模型，基于准确的2RDM数据训练（无降维处理），以探究是否能在没有任何显式三体信息的情况下再现2RDM的动力学行为。

Result: 研究表明，在两体与三体累积量间皮尔逊相关性较高的参数区域内，经过训练的神经ODE模型能很好地复制2RDM的动态变化，而不需直接使用三体信息。但在反相关或不相关区域，该模型表现不佳，说明不存在能够简单地基于瞬时两体累积量的时间局部功能来准确描述演变过程。时间平均三体关联积累的程度似乎是最主要的成功预测指标：当这种积累适中时，无论是神经ODE预测还是现有的TD2RDM重构都相当准确；但如果积累较强，则会导致系统性的失效。

Conclusion: 本研究揭示了在特定条件下利用神经ODE模型学习高维度RDM动力学的可能性，并指出为了准确描述某些强关联构建情况下的演化过程，需要引入记忆依赖型内核到三体累积量重构中。此外，还提出神经ODE可作为模型无关的诊断工具，用于映射累积展开法的应用范围，并指导非局部闭合方案的发展。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [33] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 本文介绍了一种用于线性递归的分层分解框架，称为滑动窗口递归（SWR），并基于此开发了Phalanx层。在多混合模型中，Phalanx相比优化后的Transformer在不同上下文长度下实现了10-40%的速度提升，同时保持了困惑度一致。


<details>
  <summary>Details</summary>
Motivation: 由于更好的质量和性能，多混合架构有望成为语言建模的主流。为了进一步提高这些模型在GPU上的执行效率，特别是减少线程间通信成本，提出了新的方法。

Method: 作者提出了一种针对线性递归的层次分解框架，能够与GPU内存层次结构对齐，从而形成滑动窗口递归（Sliding Window Recurrences, SWR）。特别地，通过将递归截断至硬件对齐的窗口来自然地形成锯齿状窗口，减少了昂贵的跨线程组通信。基于SWR，开发了可以作为窗口注意力或线性递归直接替换选项的Phalanx层。

Result: 在拥有10亿参数的多混合模型实验中，Phalanx层相较于经过优化的Transformer，在从4K到32K的不同上下文长度上实现了超过10%到40%的速度提升，同时保持了相当的困惑度水平。

Conclusion: Phalanx层作为一种有效的替代方案，在保持模型性能的同时显著提高了处理速度，为未来高效的语言模型设计提供了新思路。

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [34] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: 本文提出了一种基于模式引导的扩散模型(PGDM)，通过利用时间数据中的内在模式来预测未来的时间步。该方法首先使用原型分析提取模式，并估计序列中的下一个最可能模式，从而做出更符合已知模式的真实预测。此外，还引入了一种新的不确定性量化技术，并根据模式估计的不确定性动态调整指导水平。在两个预测应用中，PGDM相对于基线显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在多变量时间序列数据预测方面表现出潜力，但很少有方法考虑到数据中重复出现的结构或模式。为了解决这一问题，作者提出了Pattern-Guided Diffusion Models (PGDM) 来利用时间数据中的内在模式进行更准确的预测。

Method: PGDM 通过原型分析识别并提取时间序列数据中的固有模式，然后估计序列中最可能出现的下一个模式。基于此模式估计，PGDM 能够生成更加贴合已知模式的预测结果。同时，该研究还介绍了一种基于原型分析的新颖不确定性量化技术，并依据模式估计的不确定性程度动态调整指导级别。

Result: 当应用于视觉场测量和运动捕捉帧预测这两个具体场景时，PGDM 在性能上（以MAE/CRPS衡量）分别提升了40.67%/56.26% 和 14.12%/14.10%。与基准方法相比，PGDM 的表现更是高出多达 65.58%/84.83% 和 93.64%/92.55%。

Conclusion: 本研究表明，通过将模式信息纳入预测过程中，PGDM能够显著提高对未来时间步骤的预测准确性。这不仅证明了考虑时间序列内生模式的重要性，也展示了PGDM作为有效预测工具的潜力。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [35] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 本研究构建了一个精心整理的数据集，用于解决MHC-II抗原表位在免疫治疗中面临的挑战，并提出了三个主要的机器学习任务来捕捉MHC-II抗原呈递途径中的生物过程。通过多尺度评估框架对现有模型进行了基准测试，并为未来基于机器学习的表位发现和免疫反应预测建模提供了基础。


<details>
  <summary>Details</summary>
Motivation: 与计算免疫治疗中更广泛研究的MHC-I相比，MHC-II抗原表位的研究面临更多挑战，主要是由于其复杂的结合特异性和模糊的基序模式。现有的MHC-II相互作用数据集比MHC-I的小且标准化程度低。

Method: 从IEDB及其他公开来源获取并整理一个高质量的数据集；定义了三个主要的机器学习任务：肽结合、肽呈现及抗原呈现；采用多尺度评价框架对不同模型设计进行全面分析。

Result: 成功构建了一个扩展并标准化了现有的肽-MHC-II数据集的新数据集，并引入了具有更丰富生物背景的新抗原-MHC-II数据集；通过对现有模型的基准测试和多种建模设计方案的全面分析，为理解MHC-II抗原呈递途径提供了支持。

Conclusion: 这项工作作为推动计算免疫治疗的重要资源，为未来基于机器学习指导的表位发现以及免疫应答预测模型的发展奠定了基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [36] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5 is a pathology foundation model that integrates histologic, genomic, epigenetic, and transcriptomic data to provide a more comprehensive reflection of tumor biology. It features multimodal SigLIP loss, F-RoPE for preserving spatial structure in whole-slide images (WSIs), and domain-specialized internal models for WSI and RNA-seq. The model shows high efficiency and adaptability, outperforming or matching state-of-the-art models on the Patho-Bench benchmark and an internal clinical dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation behind EXAONE Path 2.5 is to address the limitations of image-only models in capturing the full biological landscape of cancer progression, which involves interactions across multiple layers such as molecular and morphological. By integrating various types of patient data, the model aims to offer a more complete representation of tumor biology, supporting the development of next-generation precision oncology.

Method: EXAONE Path 2.5 utilizes a multimodal approach with three key components: (1) multimodal SigLIP loss for all-pairwise contrastive learning across different data types, (2) a fragment-aware rotary positional encoding (F-RoPE) to maintain spatial and topological information in WSIs, and (3) specialized internal models for both WSIs and RNA sequencing, providing embeddings that facilitate robust alignment of the multimodal data.

Result: EXAONE Path 2.5 demonstrates high data and parameter efficiency, performing comparably to leading pathology foundation models on the Patho-Bench benchmark and showing superior adaptability in a real-world internal clinical setting. This performance supports the effectiveness of its biologically informed design in capturing complex tumor biology.

Conclusion: The study concludes that EXAONE Path 2.5, with its integrated multimodal approach, offers a significant advancement in modeling tumor biology, highlighting the potential of genotype-to-phenotype integration for advancing precision oncology. The model's strong performance on both established benchmarks and in practical clinical settings underlines its value for future research and applications.

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [37] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: 提出了FusAD，一个统一的时序分析框架，适用于多种任务。它通过自适应时频融合机制和去噪机制有效地捕捉全局-局部及多尺度动态特征，并在主流时序基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在时序分析领域取得了显著进展，但构建一个高效、支持多任务且通用的统一框架仍具挑战性。现有方法往往针对单一任务或特定数据类型设计，难以同时处理多任务建模并有效整合不同类型的时间序列信息。此外，现实世界的数据常受到噪声、复杂频率成分以及多尺度动态模式的影响，这使得鲁棒特征提取与分析变得更加困难。

Method: FusAD采用了一种自适应时频融合机制，结合傅里叶变换和小波变换来有效捕捉全局-局部和多尺度动态特征；配备有自适应去噪机制以自动感知并过滤各种类型的噪声；集成了通用的信息融合与解码结构，并结合掩码预训练促进多粒度表示的有效学习与迁移。

Result: 广泛的实验表明，FusAD在主流时间序列基准上对于分类、预测和异常检测任务持续优于最先进的模型，同时保持了高效率和可扩展性。

Conclusion: FusAD作为一个创新性的统一时间序列分析框架，在解决多任务兼容性和鲁棒特征提取方面展现了强大的能力，为处理复杂环境下的时间序列提供了新的解决方案。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [38] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: 本文提出了一种名为SonicMoE的新方法，旨在解决细粒度和稀疏的Mixture of Experts (MoE)模型在计算效率和内存占用上的问题。通过减少激活缓存、设计新的GPU内核以及引入一种新的“token rounding”方法，该方法能够显著降低激活内存使用量，并提高计算吞吐量。实验结果显示，在Hopper GPU上相比ScatterMoE实现了更高的训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着Mixture of Experts (MoE) 模型逐渐成为扩展语言模型而不大幅增加计算成本的事实标准架构，出现了向更高专家颗粒度（即每个专家更小的中间维度）及更高稀疏性发展的趋势。然而，这种发展趋势导致了激活内存占用增加、硬件效率下降以及因Grouped GEMM内核中的填充而导致的计算浪费等问题。为了解决这些问题并进一步优化MoE模型的表现，提出了本研究。

Method: 针对上述挑战，本文首先提出了一种高效的算法来最小化反向传播过程中所需的激活缓存量；其次，开发了能够在执行计算的同时重叠内存I/O操作的GPU内核；最后，创新性地提出了“token rounding”方法以减少由Grouped GEMM内核中填充引起的无用计算。

Result: 实验结果表明，SonicMoE方法能够将激活内存使用量减少45%，并且在Hopper GPU上与ScatterMoE的BF16 MoE内核相比，对于一个细粒度的7B MoE模型，实现了1.86倍的计算吞吐量提升。此外，在高MoE稀疏性设置下，提出的tile-aware token rounding算法相较于传统的top-$K$路由方法，在保持相似下游性能的同时，还提供了额外1.16倍的内核执行时间加速。

Conclusion: 综上所述，SonicMoE不仅有效解决了现有MoE模型中存在的内存效率低下和计算资源浪费问题，而且通过其实验验证了其在实际应用中的优越性能。这为未来更大规模的语言模型训练提供了有力支持。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [39] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于逻辑相似性的奖励机制（S-GRPO），以替代传统的奖励建模方法，通过结合监督组件、生成项、KL散度正则化和基于标签的目标来优化模型训练。实验表明S-GRPO在性能和鲁棒性上均优于标准监督微调(SFT)，并为对齐训练提供了更灵活的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习从人类反馈中学习的方法（如PPO）依赖于奖励模型来指导大语言模型(LLMs)符合人类价值观与偏好，但训练出的奖励模型的质量和稳定性很大程度上决定了最终的对齐效果。为了改进这一现状，作者们提出了一个新的基于逻辑相似性的奖励机制，旨在提高模型与人类偏好的一致性。

Method: 引入了S-GRPO，一种GRPO框架下的监督变体，该方法不仅依靠形式逻辑一致性来引导模型对齐人类偏好，还加入了额外的监督部分，并且在训练过程中联合优化生成项、KL散度正则化以及基于标签的目标函数。

Result: 实验结果表明，S-GRPO相比标准监督微调(SFT)在表现和鲁棒性方面都有所提升，并且扩展了现有的偏好学习框架如GRPO和DPO，提供了一种更加灵活且适应任务需求的对齐训练方法。

Conclusion: S-GRPO作为一种新的基于逻辑相似性的奖励机制，在增强大语言模型与人类偏好一致性方面显示出了显著优势，为未来的研究提供了有价值的参考。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [40] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 本文提出了一种新的生成模型框架，利用随机桥（即在固定时间点取目标分布的随机过程）作为两个概率分布之间的随机传输。基于高斯随机桥的实验结果表明，该方法能够以较少的步骤生成高质量样本，并且在Frechet inception距离得分上具有竞争力，同时计算成本低廉，适用于高速生成任务。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索随机桥在生成建模领域中的应用潜力，通过将其作为两个概率分布间的随机传输手段，提供一种灵活多变的方法来连接不同的概率分布。

Method: 首先从一般概率陈述出发，然后发展出特定表示形式用于学习和模拟算法的信息处理。具体来说，本研究采用了高斯随机桥作为基础进行实验。

Result: 实验结果显示，与传统方法相比，在显著减少步骤的情况下生成了高质量样本，并且达到了有竞争力的Frechet inception距离分数。

Conclusion: 提出的基于随机桥的框架被证明是计算成本低且适合于高速生成任务的有效方法。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [41] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出了一种新的估计问题难度的方法——LLM compare，该方法通过让大型语言模型进行成对难度比较，并基于比较结果计算Bradley-Terry得分。此方法克服了现有方法在处理未见过的问题时的局限性，如不具有可扩展性、耗时及依赖于真实数据等。


<details>
  <summary>Details</summary>
Motivation: 当前用于估计问题难度的方法，例如人工校准或基于性能的评分，在处理超出分布的问题（即目前人类和大型语言模型都无法解决的问题）时存在不足，因为这些方法不够灵活、耗费时间且依赖于已知答案。因此，研究者们旨在开发一种更加通用有效的新方法来衡量问题难度。

Method: 本研究引入了一种称为“LLM compare”的新方法，其中大型语言模型会执行成对的问题难度比较任务，之后根据这些比较的结果使用Bradley-Terry模型来计算得分。此外，研究还提出了一个概念框架来定位现有评估方法，并确定一个好的度量标准应该具备哪些特性以适用于未见过的问题。

Result: 实验验证显示，LLM compare与人类标注之间表现出高度一致性（皮尔逊相关系数r≥0.80），并且对于10%噪声注入的情况，其皮尔逊相关性的下降幅度小于6%，证明了该方法对于幻觉现象也具有较强的鲁棒性。

Conclusion: 这项工作为替代耗时的人工标注过程以及合成数据生成提供了可能，同时将对课程设计、模型评价以及AI辅助的研究构思产生重要影响。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [42] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 本文提出了一种从数据中学习反应扩散系统的方法，同时确保所学模型的物理一致性和适定性。通过修改参数化的反应项以满足质量守恒和准正性，保证了学习到的RD系统的非负性和物理原理的一致性，并且在施加额外的规律性和增长条件下保证了所得PDE的适定性。此外，文章还扩展了基于正则化模型学习的理论结果至这些物理上一致的反应项的RD系统。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决从数据中学习反应-扩散（RD）系统时保持物理一致性和模型适定性的挑战。通过将关键物理特性直接融入学习过程中，使得学习到的RD系统不仅能够保留非负性，而且遵循物理原则。

Method: 采用基于正则化的框架来构建结构化的模型学习，专注于学习参数化的反应项，并研究如何直接将诸如质量守恒和准正性等关键物理属性纳入学习过程。提出了系统地修改给定类别的参数化反应项的技术，以确保满足质量和准正性。

Result: 证明了即使强制执行守恒定律和准正性，学习问题的解也会收敛于极限系统的唯一、正则化最小化解。此外，还为构造物理上一致的参数化提供了关于准正函数的近似结果。

Conclusion: 这些成果促进了与基本物理定律相一致的可解释和可靠的数据驱动模型的发展。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [43] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 介绍了FLAME，一种轻量级且功能强大的时间序列基础模型，支持通过生成概率建模进行确定性和概率性预测，使用Legendre Memory增强了泛化能力，并采用基于Normalization Flow的预测头提高了预测准确性。实验表明FLAME在多种预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一个既轻量又高效的时间序列预测模型，能够在保持效率的同时提供准确的确定性和概率性预测。

Method: 提出了一种名为FLAME的模型，该模型利用了Legendre Memory（包括其变体LegT和LegS）来增强数据中的归纳偏差捕捉能力，并采用了基于Normalization Flow的预测头来提高对复杂分布的建模能力。

Result: 在TSFM-Bench和ProbTS等公认基准上的全面实验显示，FLAME在零样本情况下于确定性和概率性预测任务上均展现出一致的最佳性能。

Conclusion: FLAME作为一种新型的时间序列基础模型，在保证轻量级的同时，能够有效地处理确定性和概率性时间序列预测问题，为实际应用提供了强有力的支持。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [44] [Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization](https://arxiv.org/abs/2512.14263)
*Nick Leenders,Thomas Quadt,Boris Cule,Roy Lindelauf,Herman Monsuur,Joost van Oijen,Mark Voskuijl*

Main category: cs.LG

TL;DR: 本文提出了一种基于决策树的可解释替代模型，该模型能够处理分类和连续数据，并且可以扩展到大型数据集。实验表明，在尖峰函数上该模型优于基于高斯过程的方法，并且在非尖峰函数上的表现也仅略逊一筹。此外，该模型被应用于真实世界的寿司数据集，展示了学习个人寿司偏好的能力。还探讨了使用历史偏好数据来加速新用户的优化过程。


<details>
  <summary>Details</summary>
Motivation: 当前的偏好贝叶斯优化方法依赖于高斯过程（GPs）作为替代模型，但这些模型难以解释、处理分类数据困难且计算复杂，限制了它们在现实世界中的实用性。

Method: 提出了一种基于决策树的内在可解释替代模型，该模型能够同时处理分类和连续数据，并且可以很好地扩展到大规模数据集上。

Result: 通过八个越来越尖锐的优化函数进行了广泛的数值实验，证明了所提出的模型在处理尖锐函数方面优于基于GP的方法，并且对于非尖锐函数仅有轻微的性能下降。此外，该模型成功应用于真实世界的寿司数据集，展示了学习个人对寿司偏好的能力。初步工作还表明，利用历史偏好数据可以加快针对新用户群体的优化过程。

Conclusion: 本研究开发了一种新的基于决策树的优选贝叶斯优化方法，它克服了传统GP方法的一些局限性，如难以解释性和处理分类数据的能力有限等。实验结果证实了该方法的有效性及其潜在的应用价值。

Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.

</details>


### [45] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: The study explores how classical Hopfield networks can learn graph isomorphism classes from a small sample of group-structured data, revealing that such learning benefits from a bias towards norm-efficient solutions, which leads to the emergence of approximate invariance and generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how symmetries and invariances can emerge implicitly in neural network models, particularly Hopfield networks, when trained on data with group structure, and to investigate the mechanisms behind the ability to generalize from limited samples.

Method: The method involves theoretical analysis of Hopfield networks, specifically focusing on the inference of graph isomorphism classes. The researchers use gradient descent to minimize energy flow (MEF) and observe the convergence of parameters toward an invariant subspace as the sample size increases.

Result: Results show that (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) minimizing energy flow has an implicit bias toward norm-efficient solutions, leading to a polynomial sample complexity for learning, and (iii) parameters converge to the invariant subspace across different learning rules as more samples are provided.

Conclusion: The findings suggest that a unifying mechanism for generalization in Hopfield networks is the bias toward norm efficiency, which drives the emergence of approximate invariance under group-structured data, allowing the networks to infer graph isomorphism classes effectively.

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [46] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于提升量子差分隐私的黑盒隐私审计框架，用于检测和量化量子机器学习模型中的记忆化和隐私泄露。该框架通过量子金丝雀（策略性偏移编码的量子状态）来检测训练过程中的记忆化，并提供实际隐私损失的测量方法，从而在理论保证与实际隐私验证之间架起桥梁。


<details>
  <summary>Details</summary>
Motivation: 尽管量子差分隐私机制提供了理论上的最坏情况保证，但在部署的模型中缺乏经验验证工具。因此，存在对能够检测记忆化并准确量化量子机器学习(QML)模型训练期间隐私泄露的解决方案的需求。

Method: 作者引入了首个基于提升量子差分隐私的QML黑盒隐私审计框架，利用量子金丝雀（策略性偏移编码的量子态）来探测记忆化现象，并精确衡量隐私泄漏程度。此外，还建立了canary偏移与迹距离界限之间的严格数学联系，为隐私预算消耗提供了实证下限。

Result: 综合评估表明，在模拟和物理量子硬件上，本框架都能够有效地度量QML模型的实际隐私损失，这使得在QML系统中实现稳健的隐私验证成为可能。

Conclusion: 这项工作填补了量子机器学习领域从理论保障到实践隐私验证的关键空白，通过引入一种新的隐私审计方法，增强了对于使用敏感数据训练的模型的隐私保护能力。

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [47] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: 本文提出了一种新的机制RePo，通过重新定位上下文来降低大型语言模型中的额外认知负荷，从而提高在噪音、结构化数据和长上下文任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型（LLMs）采用固定的线性或常数位置索引，这增加了额外的认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。基于认知负荷理论（CLT），文章旨在通过改善这一现状来提升模型性能。

Method: 引入了名为RePo的新机制，它利用一个可微模块$f_φ$为捕捉到上下文依赖关系的token分配位置，而非依赖预定义的整数范围。通过对OLMo-2 1B骨干进行持续预训练实现。

Result: 实验表明，RePo在涉及噪音环境、结构化数据以及更长上下文长度的任务上显著提高了性能，同时在一般短上下文任务中保持竞争力。进一步分析显示，RePo能够成功地将更多注意力分配给远距离但相关的信息，并以密集且非线性的方式分配位置，捕捉输入上下文的内在结构。

Conclusion: RePo作为一种创新的方法，有效减少了额外的认知负荷，提升了大型语言模型处理复杂上下文的能力。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [48] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: 本文提出了GRAFT模型，该模型改进了STanHOP方法，以更好地支持电网感知预测和多源文本干预。通过将每日汇总的新闻、社交媒体和政策文本与半小时负荷严格对齐，并采用跨注意力机制在训练和滚动预测中实现文本导向的融合。此外，还提供了一个即插即用的外部存储接口来适应现实部署中的不同信息源。实验结果表明，GRAFT在多个区域和预测范围内显著优于强大的基线，并且在事件驱动场景中表现出稳健性，能够通过注意力读出实现文本到负荷效应的时间定位和来源级解释。


<details>
  <summary>Details</summary>
Motivation: 电力负荷受到天气、日历节奏、突发事件和政策等外生因素的影响，这些影响作用于多个时间尺度上。因此，需要一种能够有效整合这些多源信息的方法来提高电力负荷预测的准确性。

Method: 开发了一种名为GRAFT的新模型，该模型基于STanHOP进行了修改和改进，旨在更好地支持电网感知预测及多源文本干预。GRAFT能够将每天聚合的新闻、社交媒体以及政策文本与每半小时的负荷数据紧密对接，并通过交叉注意力机制在训练和滚动预测阶段实现特定时间点上的文本指导融合。另外，GRAFT还配备有一个可插拔式的外部记忆接口，便于实际部署时接入不同类型的信息源。

Result: 实验结果显示，GRAFT不仅明显优于几个强有力的基准模型，在多个地区和预测时间跨度上达到了或超过了最先进水平；而且在由事件触发的情况下也显示出了良好的稳定性。更重要的是，通过注意力建模技术还可以清晰地识别出哪些文本信息对负荷变化产生了影响及其具体发生时间。

Conclusion: 提出并验证了GRAFT这一新框架的有效性，它能有效地结合多源文本信息进行电网负荷预测，并且在不同的地理区域和时间尺度上都表现优异。此外，该研究还公开了基准测试集、预处理脚本以及预测结果，促进了该领域内标准化实证评估的发展。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [49] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 研究引入了ClimaX-LETKF，这是首个纯数据驱动的基于机器学习的集合天气预报系统，通过同化实际观测数据来独立于数值天气预报模型稳定运行多年。研究表明，与RTPS相比，使用RTPP时该系统的稳定性和准确性更高，并揭示了MLWP模型在恢复大气场至其吸引子方面的能力较NWP模型为弱。


<details>
  <summary>Details</summary>
Motivation: 尽管基于机器学习的天气预测（MLWP）取得了显著进展，但在将实际观测或集合预报同化到MLWP模型中的研究仍然有限。

Method: 研究人员开发了ClimaX-LETKF，一个纯数据驱动的基于机器学习的集合天气预报系统，它通过同化NCEP ADP全球高空和地表天气观测数据独立于数值天气预报（NWP）模型稳定运行多年。研究中比较了两种方法：放松到先验扰动（RTPP）和放松到先验扩散（RTPS）。

Result: ClimaX-LETKF系统在采用RTPP时表现出比使用RTPS更高的稳定性和准确性；然而，NWP模型在使用RTPS时更稳定。此外，实验表明，相比于NWP模型，MLWP模型在将大气场恢复到其吸引子方面的能力较差。

Conclusion: 这项工作为改进基于机器学习的集合天气预报系统提供了宝贵的见解，并代表了向其实用化迈出的重要一步。

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [50] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 本研究通过使用变分自编码器(VAE)生成合成的多通道心房电图(EGMs)，以解决用于估计体表电位(BSPMs)中的心内心电图的配对BSPM-EGM数据集不足的问题。提出了两种模型：特定窦性心律VAE（VAE-S）和基于窦性心律及房颤信号训练的类别条件VAE（VAE-C）。生成的EGMs在形态、频谱和分布相似性指标上进行了评估，并且证明了通过这些生成的数据增强可以改善下游非侵入性EGM重建任务的表现。


<details>
  <summary>Details</summary>
Motivation: 房颤(AF)是临床中最常见的持续性心律失常，对其准确诊断需要精确的心房电活动特征化。尽管非侵入性心电成像(ECGI)结合深度学习方法估计体表电位(BSPMs)中的心内心电图(EGMs)显示出了潜力，但这种进展受到了配对BSPM-EGM数据集有限可用性的阻碍。为了解决这一局限性，研究旨在探索利用变分自编码器(VAEs)来生成合成多通道心房EGMs的可能性。

Method: 研究中提出并对比了两种变分自编码器(VAE)模型：一种是专为窦性心律设计的VAE-S，另一种是可以处理窦性心律和房颤信号的类别条件VAE-C。通过形态学、频谱以及分布相似性等度量标准对生成的心房电图(EGMs)进行评估。此外，还探讨了将生成的EGMs作为数据扩增手段应用于后续非侵入性EGM重建任务中的效果。

Result: 实验结果显示，VAE-S模型对于计算机模拟EGMs的保真度更高；而VAE-C模型虽然能够实现特定节律的生成，但在窦性心律重构质量方面有所下降。作为概念验证，当把生成的EGMs用于增加训练数据时，在一定程度上提高了非侵入性EGM重建任务的性能。

Conclusion: 研究表明，基于VAE的生成建模具有缓解数据稀缺问题的潜力，并能够增强基于深度学习的心电成像(ECGI)流程。这表明，通过生成合成数据来扩充现有数据集是一种有效的方法，可以提高ECGI技术的准确性与可靠性。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [51] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 本文指出当前用于时间序列分类的反事实技术在临床推荐环境中不足，因为它们假设静态数据且仅关注最小输入扰动以改变模型预测。作者提倡一种转换到反映持续、目标导向干预的反事实方法，这些方法与临床推理和患者特定动态相一致，并呼吁开发出可行性和可操作性更强的方法及评估框架。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于解决现有针对时间序列分类的反事实解释方法未能充分考虑临床场景下干预措施需具备的时间连贯性和因果合理性的问题。

Method: 通过分析几种最先进的时序数据处理方法对随机噪声的高度敏感性来支持其观点。

Result: 结果显示，所生成的反事实对于随机噪声极其敏感，这表明它们在存在不可避免的小测量变异的真实世界临床环境中的可靠性有限。

Conclusion: 结论是需要超越单纯改变预测而不考虑可行性或可操作性的方法和评估框架，强调了开发符合实际应用背景的、目的驱动型干预措施的重要性。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [52] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于模型的算法QR-MAX，用于处理离散非马尔可夫奖励决策过程（NMRDPs），并能通过奖励机将马尔科夫转移学习与非马尔科夫奖励处理分解开来。该方法是首个在离散动作NMRDPs上利用这种分解来实现PAC收敛至ε-最优策略且具有多项式样本复杂度的RL算法。此外，还提出了Bucket-QR-MAX以适应连续状态空间，并在实验中展示了比现有技术更高的样本效率和寻找最优策略时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对现实世界中的决策问题往往依赖于整个系统历史而非达到一个具有期望属性的状态这一特点，传统的马尔科夫强化学习方法并不适用。而现有的处理非马尔科夫奖励决策过程的方法缺乏对近似最优性和样本效率的形式化保证。

Method: 开发了名为QR-MAX的新算法，专为离散NMRDP设计，通过奖励机机制分离马尔科夫转移学习和非马尔科夫奖励管理；进一步地，为了应对连续状态空间的问题，引入了Bucket-QR-MAX，它基于SimHash进行离散化处理，保持了相同的分解结构，同时无需手动网格划分或函数逼近即可实现快速稳定的学习。

Result: 实验结果表明，所提方法相较于当前最先进的基于模型的强化学习方法，在越来越复杂的环境中展现了显著提高的样本效率以及增强的找到最优策略的能力。

Conclusion: 通过提出的QR-MAX及其变体Bucket-QR-MAX，解决了长期以来存在于非马尔科夫奖励决策过程中关于最优性及样本效率的问题，为处理时间依赖型任务提供了有效手段。

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [53] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: 本文介绍了一个名为gridfm-datakit-v1的Python库，用于生成多样且真实的电力流（PF）和最优电力流（OPF）数据集以训练机器学习求解器。它解决了现有数据集和库面临的三个主要挑战：不现实的数据多样性、仅限于可操作点的数据集以及固定的发电机成本函数限制，并展示了其在大规模电网上的高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有的电力流（PF）与最优电力流（OPF）数据集及库面临的主要问题包括缺乏实际随机负荷和拓扑扰动、PF数据集局限于OPF可行点导致机器学习求解器泛化能力受限、以及OPF数据集使用固定发电成本函数从而限制了对不同成本情况下的泛化能力。为了解决这些问题，研究者开发了gridfm-datakit-v1。

Method: 通过结合来自真实世界概况的整体负荷缩放与局部噪声添加，支持任意N-k拓扑变化来创建多样化而现实的数据集；生成超出运行限制的PF样本；并生产具有变动发电机成本的OPF数据。此外，该工具还能够有效扩展至大型电网（高达10,000个节点）。

Result: gridfm-datakit-v1成功地克服了上述提到的三个关键挑战，不仅提供了更加丰富多样的数据样本，而且允许生成违反操作限制的情况，同时还能处理变化的发电机成本。这使得基于此库训练出的机器学习模型能够更好地适应各种复杂的实际情况。

Conclusion: gridfm-datakit-v1作为一个开源Python库，在GitHub上可用，并通过pip安装。它为研究人员提供了一个强大的工具，用以生成更贴近实际应用需求的PF和OPF数据集，进而促进了针对复杂电网环境设计的机器学习算法的发展。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [54] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 本文介绍了一种多模态机器学习框架——早期预警指数（EWI），用于预测重症监护病房（ICU）入院、紧急响应团队派遣和死亡的综合风险。该系统通过结合临床医生设定警戒阈值与解释模型输出，并使用SHAP方法提供可解释性结果，帮助识别驱动每位患者风险的具体因素。在一家大型美国医院部署后，该系统自动从结构化和非结构化电子健康记录中提取特征，达到了0.796的C统计量，有助于医护人员更有效地管理高风险患者并优化资源配置。


<details>
  <summary>Details</summary>
Motivation: 面对日益增长且多样化的临床及运营数据，医院缺乏能够有效预测关键事件的自动化系统。准确地提前识别出有恶化风险的患者对于提高护理质量和医生管理至关重要。然而，将不同格式的数据转换为既准确又易于理解的风险评估是一项挑战。

Method: 开发了一个名为早期预警指数(EWI)的多模态机器学习框架，旨在预测包括ICU入院、紧急响应小组派遣以及死亡在内的综合风险。该框架采用人机交互设计，允许临床医生参与设定警报阈值并对模型输出进行解读；同时利用SHAP方法增强模型输出的可解释性，明确指出影响每位患者风险水平的关键临床与运营因素。

Result: 基于一家大型美国医院18,633名独特患者的数据库，EWI能够自动从结构化与非结构化电子健康记录中抽取特征，并达到0.796的C统计量表现。此系统作为分诊工具已被实际应用于主动管理高危患者，通过自动对不同风险级别的患者进行分类，使医生能专注于病人护理而非繁琐的数据筛选工作。

Conclusion: 提出的EWI方法不仅提高了风险预测的准确性，还通过突出显示特定风险驱动因素来支持基于数据的决策制定，从而帮助改善护理人员调度和关键资源分配。最终，这使得临床医生和管理者能够避免后续可能出现的并发症，如昂贵的治疗过程或较高的再入院率，进而提升整体患者流动效率。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [55] [Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing](https://arxiv.org/abs/2512.13904)
*Amirkia Rafiei Oskooei,Eren Caglar,Ibrahim Sahin,Ayse Kayabay,Mehmet S. Aktas*

Main category: cs.MM

TL;DR: 本文提出了一种系统级框架，旨在解决多用户视频会议应用中生成式AI流水线部署时遇到的计算复杂度和推理延迟问题。通过引入轮流机制和分段处理协议来降低计算复杂度并管理推理延迟，从而实现感知上的实时体验。实验结果表明该系统在现代硬件上达到了实时吞吐量，并且用户研究表明为了获得流畅不间断的播放体验，用户可以接受可预测的初始处理延迟。


<details>
  <summary>Details</summary>
Motivation: 实时部署用于视频翻译等应用的级联生成式AI流水线受到系统层面挑战的限制，包括顺序模型推理累积延迟以及多用户视频会议应用程序中从二次（$\mathcal{O}(N^2)$）计算复杂度导致的不可扩展性。

Method: 提出了一种实用的系统级框架，其中包括一个轮流机制以将多用户场景下的计算复杂度从二次减少到线性，以及一种分段处理协议来控制推理延迟，从而提供感知上的实时体验。此外，还实现了一个概念验证流水线，并在一个多层次硬件设置上进行了严格的性能分析。

Result: 客观评估显示，该系统在现代硬件上实现了实时吞吐量（$τ< 1.0$）。主观用户研究进一步验证了这种方法的有效性，表明用户愿意接受一个可预见的、最初的处理延迟，以换取顺畅无中断的播放体验。

Conclusion: 这项工作展示了一个经过验证的端到端系统设计，为在多语言通信平台中部署可扩展、实时的生成式AI应用程序提供了实际可行的道路图。

Abstract: The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($τ< 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.

</details>


### [56] [End-to-End Learning-based Video Streaming Enhancement Pipeline: A Generative AI Approach](https://arxiv.org/abs/2512.14185)
*Emanuele Artioli,Farzad Tashtarian,Christian Timmerer*

Main category: cs.MM

TL;DR: 本文介绍了一种名为ELVIS的端到端架构，它结合了服务器端编码优化和客户端生成性修复技术，以去除并重建冗余视频数据。实验结果显示，在不增加带宽需求的情况下，与基线相比可以实现高达11点VMAF的质量提升。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器无法利用上下文信息，必须对整个视频进行编码并向客户端传输所有数据。这导致了在追求高画质的同时难以保证播放流畅性的挑战。

Method: 提出了ELVIS架构，该架构通过结合服务器端的编码优化与客户端基于生成模型的修补技术来去除和重建视频中的冗余信息。此外，ELVIS采用模块化设计，允许集成不同的编解码器、修补模型以及质量评估指标。

Result: 研究结果表明，使用当前技术可以在不超过基准测试的情况下提高最多11个VMAF点的质量评分。然而，由于计算需求较高，实时应用仍面临一定挑战。

Conclusion: ELVIS作为将生成式AI融入视频流媒体管道的基础步骤，为不增加带宽要求的前提下提供更高质量的观看体验铺平了道路。

Abstract: The primary challenge of video streaming is to balance high video quality with smooth playback. Traditional codecs are well tuned for this trade-off, yet their inability to use context means they must encode the entire video data and transmit it to the client. This paper introduces ELVIS (End-to-end Learning-based VIdeo Streaming Enhancement Pipeline), an end-to-end architecture that combines server-side encoding optimizations with client-side generative in-painting to remove and reconstruct redundant video data. Its modular design allows ELVIS to integrate different codecs, inpainting models, and quality metrics, making it adaptable to future innovations. Our results show that current technologies achieve improvements of up to 11 VMAF points over baseline benchmarks, though challenges remain for real-time applications due to computational demands. ELVIS represents a foundational step toward incorporating generative AI into video streaming pipelines, enabling higher quality experiences without increased bandwidth requirements.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [57] [BiCoRec: Bias-Mitigated Context-Aware Sequential Recommendation Model](https://arxiv.org/abs/2512.13848)
*Mufhumudzi Muthivhi,Terence L van Zyl,Hairong Wang*

Main category: cs.IR

TL;DR: 提出了一种新的框架BiCoRec，通过共注意力机制和新颖的训练方案来适应用户对热门和小众商品不断变化的偏好，特别提高了对喜欢小众商品用户的推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的顺序推荐模型存在固有的流行度偏差问题，本研究旨在开发一种能够自适应地适应用户对热门和小众商品偏好变化的新框架。

Method: 开发了名为BiCoRec的新框架，该框架利用共注意力机制获取加权后的用户序列表示，并引入了一种新的训练方案，该方案采用一致性损失函数从未来偏好中学习。

Result: 对于偏好小众物品的用户群体，BiCoRec在NDCG@10指标上相比最先进基线平均提升了26.00%。针对不同数据集，在将相关项目与整个集合进行排名时，BiCoRec达到了特定的NDCG@10分数。

Conclusion: BiCoRec有效改善了对偏好小众商品用户群体的推荐表现，展示了其在处理用户偏好变化及减少流行度偏差方面的潜力。

Abstract: Sequential recommendation models aim to learn from users evolving preferences. However, current state-of-the-art models suffer from an inherent popularity bias. This study developed a novel framework, BiCoRec, that adaptively accommodates users changing preferences for popular and niche items. Our approach leverages a co-attention mechanism to obtain a popularity-weighted user sequence representation, facilitating more accurate predictions. We then present a new training scheme that learns from future preferences using a consistency loss function. BiCoRec aimed to improve the recommendation performance of users who preferred niche items. For these users, BiCoRec achieves a 26.00% average improvement in NDCG@10 over state-of-the-art baselines. When ranking the relevant item against the entire collection, BiCoRec achieves NDCG@10 scores of 0.0102, 0.0047, 0.0021, and 0.0005 for the Movies, Fashion, Games and Music datasets.

</details>


### [58] [Intent-Guided Reasoning for Sequential Recommendation](https://arxiv.org/abs/2512.14034)
*Yifan Shao,Peilin Zhou*

Main category: cs.IR

TL;DR: 本文提出了一种基于意图引导的推理框架IGR-SR，用于序列推荐系统。通过引入显式提取的高层次意图来稳定推理过程，并通过三个主要组件（潜在意图提炼器LID、意图感知审慎推理器IDR和意图一致性正则化ICR）解决了现有方法中存在的推理不稳定性和表面层次推理问题。实验结果表明，该方法在三个公开数据集上相比最新的基准平均提高了7.13%，并且在行为噪声条件下表现出更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的序列推荐系统中的推理增强方法存在两个关键问题：对近期行为过于敏感导致的推理不稳定性，以及模型仅记忆项目之间的转换而非理解内在行为模式的表面层次推理。为了解决这些问题，作者提出了一个基于意图引导的推理框架。

Method: 提出的IGR-SR框架包含三个核心部分：(1) 潜在意图提炼器(LID)，利用冻结编码器与可学习令牌有效提取多方面意图；(2) 意图感知审慎推理器(IDR)，通过双重注意力架构将推理分解为意图审议和决策制定；(3) 意图一致性正则化(ICR)，确保不同意图视图间表示的一致性以提高鲁棒性。

Result: 在三个公共数据集上的广泛实验显示，IGR-SR相较于最先进基线实现了平均7.13%的改进。尤其值得注意的是，在存在20%行为噪音的情况下，IGR-SR的表现下降了仅10.4%，而其他竞争方法则分别下降了16.2%和18.6%。

Conclusion: IGR-SR通过引入显式意图指导的方法成功缓解了序列推荐中常见的推理不稳定性和表面层次推理问题，同时展现出优越的效果和鲁棒性。

Abstract: Sequential recommendation systems aim to capture users' evolving preferences from their interaction histories. Recent reasoningenhanced methods have shown promise by introducing deliberate, chain-of-thought-like processes with intermediate reasoning steps. However, these methods rely solely on the next target item as supervision, leading to two critical issues: (1) reasoning instability--the process becomes overly sensitive to recent behaviors and spurious interactions like accidental clicks, and (2) surface-level reasoning--the model memorizes item-to-item transitions rather than understanding intrinsic behavior patterns. To address these challenges, we propose IGR-SR, an Intent-Guided Reasoning framework for Sequential Recommendation that anchors the reasoning process to explicitly extracted high-level intents. Our framework comprises three key components: (1) a Latent Intent Distiller (LID) that efficiently extracts multi-faceted intents using a frozen encoder with learnable tokens, (2) an Intent-aware Deliberative Reasoner (IDR) that decouples reasoning into intent deliberation and decision-making via a dual-attention architecture, and (3) an Intent Consistency Regularization (ICR) that ensures robustness by enforcing consistent representations across different intent views. Extensive experiments on three public datasets demonstrate that IGR-SR achieves an average 7.13% improvement over state-of-the-art baselines. Critically, under 20% behavioral noise, IGR-SR degrades only 10.4% compared to 16.2% and 18.6% for competing methods, validating the effectiveness and robustness of intent-guided reasoning.

</details>


### [59] [DTRec: Learning Dynamic Reasoning Trajectories for Sequential Recommendation](https://arxiv.org/abs/2512.14036)
*Yifan Shao,Peilin Zhou,Shoujin Wang,Weizhi Zhang,Xu Cai,Sunghun Kim*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架DTRec，用于探索序列推荐中的动态推理轨迹。通过引入层次过程监督（HPS）来指导推理方向，并采用自适应推理停止（ARH）机制优化推理深度，该方法在提高性能的同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的进步，增强推理的顺序推荐虽然解锁了捕捉用户偏好的更大潜力，但其静态推理轨迹不适合用户行为的多样性复杂性。主要问题在于静态的推理方向和固定的推理深度，导致性能不佳和显著的计算浪费。

Method: 提出了DTRec框架，它沿方向和深度探索动态推理轨迹。为指导方向，开发了提供粗到细监督信号的层次过程监督（HPS），模仿人类认知过程的自然逐步细化；为优化深度，引入了自适应推理停止(ARH)机制，通过共同监测三个指标动态调整推理步骤数量。

Result: 在三个真实世界数据集上的广泛实验表明，与强大的基线相比，所提方法实现了高达24.5%的性能提升，同时将计算成本减少了多达41.6%。

Conclusion: 通过引入动态推理轨迹探索，包括方向上的层次过程监督和深度上的自适应推理停止机制，DTRec能够有效克服现有方法的局限性，在提高推荐系统性能的同时也降低了计算开销。

Abstract: Inspired by advances in LLMs, reasoning-enhanced sequential recommendation performs multi-step deliberation before making final predictions, unlocking greater potential for capturing user preferences. However, current methods are constrained by static reasoning trajectories that are ill-suited for the diverse complexity of user behaviors. They suffer from two key limitations: (1) a static reasoning direction, which uses flat supervision signals misaligned with human-like hierarchical reasoning, and (2) a fixed reasoning depth, which inefficiently applies the same computational effort to all users, regardless of pattern complexity. These rigidity lead to suboptimal performance and significant computational waste. To overcome these challenges, we propose DTRec, a novel and effective framework that explores the Dynamic reasoning Trajectory for Sequential Recommendation along both direction and depth. To guide the direction, we develop Hierarchical Process Supervision (HPS), which provides coarse-to-fine supervisory signals to emulate the natural, progressive refinement of human cognitive processes. To optimize the depth, we introduce the Adaptive Reasoning Halting (ARH) mechanism that dynamically adjusts the number of reasoning steps by jointly monitoring three indicators. Extensive experiments on three real-world datasets demonstrate the superiority of our approach, achieving up to a 24.5% performance improvement over strong baselines while simultaneously reducing computational cost by up to 41.6%.

</details>


### [60] [From Feature Interaction to Feature Generation: A Generative Paradigm of CTR Prediction Models](https://arxiv.org/abs/2512.14041)
*Mingjia Yin,Junwei Pan,Hao Wang,Ximei Wang,Shangyu Zhang,Jie Jiang,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: 本文提出了一种新的监督特征生成（SFG）框架，旨在解决CTR预测任务中现有模型的嵌入维度崩溃和信息冗余问题。通过从判别式的“特征交互”转向生成式的“特征生成”，SFG框架利用监督损失来提高性能，并且可以与大多数现有的CTR模型无缝集成。实验结果表明，该方法在多个数据集和基准模型上显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 当前点击率(CTR)预测模型主要依赖于原始ID嵌入之间的显式交互，导致了嵌入维度崩溃和信息冗余的问题。这些问题源于对原始ID嵌入特征交互的过度依赖。

Method: 提出了一个名为Supervised Feature Generation (SFG)的新框架，它包含两个关键组件：编码器用于为每个特征构建隐藏嵌入，解码器负责从这些隐藏表示中重新生成所有特征的嵌入。不同于采用自监督损失的现有生成方法，SFG引入了监督损失以利用CTR预测任务中的监督信号（即是否点击）。

Result: 广泛的实验表明，SFG能够一致地缓解嵌入崩溃并减少信息冗余，同时在各种数据集和基础模型上实现了显著的性能提升。

Conclusion: SFG框架提供了一种有效的解决方案，解决了CTR预测中常见的嵌入维度崩溃和信息冗余问题，并且证明了其具有良好的泛化能力，可以被广泛应用于不同的CTR模型之中。

Abstract: Click-Through Rate (CTR) prediction, a core task in recommendation systems, aims to estimate the probability of users clicking on items. Existing models predominantly follow a discriminative paradigm, which relies heavily on explicit interactions between raw ID embeddings. However, this paradigm inherently renders them susceptible to two critical issues: embedding dimensional collapse and information redundancy, stemming from the over-reliance on feature interactions \emph{over raw ID embeddings}. To address these limitations, we propose a novel \emph{Supervised Feature Generation (SFG)} framework, \emph{shifting the paradigm from discriminative ``feature interaction" to generative ``feature generation"}. Specifically, SFG comprises two key components: an \emph{Encoder} that constructs hidden embeddings for each feature, and a \emph{Decoder} tasked with regenerating the feature embeddings of all features from these hidden representations. Unlike existing generative approaches that adopt self-supervised losses, we introduce a supervised loss to utilize the supervised signal, \ie, click or not, in the CTR prediction task. This framework exhibits strong generalizability: it can be seamlessly integrated with most existing CTR models, reformulating them under the generative paradigm. Extensive experiments demonstrate that SFG consistently mitigates embedding collapse and reduces information redundancy, while yielding substantial performance gains across various datasets and base models. The code is available at https://github.com/USTC-StarTeam/GE4Rec.

</details>


### [61] [AsarRec: Adaptive Sequential Augmentation for Robust Self-supervised Sequential Recommendation](https://arxiv.org/abs/2512.14047)
*Kaike Zhang,Qi Cao,Fei Sun,Xinran Liu*

Main category: cs.IR

TL;DR: 提出了一种自适应增强框架AsarRec，通过学习生成转换矩阵来提高顺序推荐系统的鲁棒性，并通过联合优化多样性、语义不变性和信息量这三个目标来确保所学增强对下游性能有益。实验表明该方法在不同噪声水平下具有优越的鲁棒性和一致性改进。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序推荐系统虽然能够建模用户动态偏好和捕捉项目过渡模式，但真实世界中的用户行为往往因为人为错误、不确定性以及行为模糊性等因素而显得嘈杂，这会导致推荐性能下降。最近的方法广泛采用自监督学习（特别是对比学习）来改善模型的鲁棒性，但是这些方法依赖于预定义的静态增强策略，限制了SSL的有效性。

Method: 提出了一个名为AsarRec（Adaptive Sequential Augmentation for Robust Sequential Recommendation）的自适应序列增强框架，它首先将现有的基本增强操作统一成一个基于结构化变换矩阵的公式表达；然后，通过将用户序列编码为概率转移矩阵并利用可微分Semi-Sinkhorn算法将其投影到硬半双随机矩阵中来学习生成变换矩阵。为了保证所学到的增强有利于下游任务的表现，同时优化了三个目标：多样性、语义不变性及信息量。

Result: 在三个基准数据集上进行了广泛的实验，结果表明AsarRec不仅展示了出色的鲁棒性，而且在不同噪声水平条件下也实现了持续的性能提升。

Conclusion: AsarRec通过引入一种新颖的自适应序列增强机制解决了传统方法中存在的问题，从而提高了顺序推荐系统在面对噪声数据时的鲁棒性和准确性。

Abstract: Sequential recommender systems have demonstrated strong capabilities in modeling users' dynamic preferences and capturing item transition patterns. However, real-world user behaviors are often noisy due to factors such as human errors, uncertainty, and behavioral ambiguity, which can lead to degraded recommendation performance. To address this issue, recent approaches widely adopt self-supervised learning (SSL), particularly contrastive learning, by generating perturbed views of user interaction sequences and maximizing their mutual information to improve model robustness. However, these methods heavily rely on their pre-defined static augmentation strategies~(where the augmentation type remains fixed once chosen) to construct augmented views, leading to two critical challenges: (1) the optimal augmentation type can vary significantly across different scenarios; (2) inappropriate augmentations may even degrade recommendation performance, limiting the effectiveness of SSL. To overcome these limitations, we propose an adaptive augmentation framework. We first unify existing basic augmentation operations into a unified formulation via structured transformation matrices. Building on this, we introduce AsarRec (Adaptive Sequential Augmentation for Robust Sequential Recommendation), which learns to generate transformation matrices by encoding user sequences into probabilistic transition matrices and projecting them into hard semi-doubly stochastic matrices via a differentiable Semi-Sinkhorn algorithm. To ensure that the learned augmentations benefit downstream performance, we jointly optimize three objectives: diversity, semantic invariance, and informativeness. Extensive experiments on three benchmark datasets under varying noise levels validate the effectiveness of AsarRec, demonstrating its superior robustness and consistent improvements.

</details>


### [62] [SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions](https://arxiv.org/abs/2512.14277)
*Panayiotis Smeros,Vincent Emonet,Ruijie Wang,Ana-Claudia Sima,Tarcisio Mendes de Farias*

Main category: cs.IR

TL;DR: 本文介绍并评估了SPARQL-LLM，一种开源的、与三元存储无关的方法，它通过轻量级元数据从自然语言文本生成SPARQL查询。实验结果显示，在最新的挑战中F1分数提高了24%，能够适应资源丰富的语言，并能快速低成本地形成复杂的生物信息学查询。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成结构化查询方面展现出潜力，但现有方法主要关注单一来源的响应准确性，忽视了跨分布式数据存储执行联邦查询的能力以及生成SPARQL查询的时间和成本等问题。这导致这些方法往往不适用于生产环境或难以部署于（可能是联合的）知识图谱上。

Method: 提出了一种名为SPARQL-LLM的新方法，该方法基于轻量级元数据，旨在从自然语言文本生成SPARQL查询。其架构包括用于元数据索引、提示构建及查询生成与执行的专门组件。

Result: 实验结果表明，相较于其他系统，SPARQL-LLM在最新挑战中的F1得分提升了24%，并且对于英语和西班牙语等高资源语言表现出良好的适应性；同时，它还展示了构造复杂且联合的生物信息学查询的能力。此外，SPARQL-LLM的速度最高可达其他系统的36倍，每条查询的成本不超过0.01美元，非常适合实时、低成本的文字到SPARQL应用。

Conclusion: 通过引入SPARQL-LLM，研究者们不仅提高了从自然语言到SPARQL查询转换的准确性，而且增强了该过程的效率和经济性，使之更加适合实际应用场景，特别是在需要处理分布式的知识图谱时。

Abstract: The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.

</details>


### [63] [Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias](https://arxiv.org/abs/2512.14313)
*Malika Iratni,Mohand Boughanem,Taoufiq Dkaki*

Main category: cs.IR

TL;DR: 本文研究了在检索增强生成（RAG）系统中，干扰信息和相关段落位置对生成质量的影响，并提出了一种基于查询特定信息需求动态预测最佳检索文档数量的上下文大小分类器，以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准RAG系统通常依赖于固定的前k个检索策略，这可能会遗漏相关信息或引入语义无关的段落（即干扰信息），从而降低输出质量。此外，检索到的段落在输入上下文中的位置也会影响模型注意力及生成结果，存在“中间被忽略”的现象。

Method: 系统地分析了干扰信息对生成质量的影响，并量化了不同条件下的效果；探讨了相关段落在上下文窗口内位置变化对其生成影响的作用；提出并实现了一个能够根据查询具体信息需求动态预测应检索的最佳文档数量的上下文大小分类器，并将其整合进完整的RAG流程中。

Result: 所提出的上下文大小分类器能够有效改善固定k基线的表现。

Conclusion: 通过减少干扰信息的影响以及优化相关段落的位置布局，可以显著提升RAG系统的生成质量。

Abstract: Retrieval Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieved from large corpora, which makes it highly suitable for tasks such as open domain question answering. Standard RAG systems typically rely on a fixed top k retrieval strategy, which can either miss relevant information or introduce semantically irrelevant passages, known as distractors, that degrade output quality. Additionally, the positioning of retrieved passages within the input context can influence the model attention and generation outcomes. Context placed in the middle tends to be overlooked, which is an issue known as the "lost in the middle" phenomenon. In this work, we systematically analyze the impact of distractors on generation quality, and quantify their effects under varying conditions. We also investigate how the position of relevant passages within the context window affects their influence on generation. Building on these insights, we propose a context-size classifier that dynamically predicts the optimal number of documents to retrieve based on query-specific informational needs. We integrate this approach into a full RAG pipeline, and demonstrate improved performance over fixed k baselines.

</details>


### [64] [PushGen: Push Notifications Generation with LLM](https://arxiv.org/abs/2512.14490)
*Shifu Bie,Jiangxia Cao,Zixiao Luo,Yichuan Zou,Lei Liang,Lu Zhang,Linxun Chen,Zhaojie Liu,Xuanping Li,Guorui Zhou,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: 介绍了PushGen框架，该框架通过可控类别提示技术和奖励模型来生成高质量的推送通知，并已在大规模工业应用中部署。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的兴起，利用LLM进行推送内容生成的兴趣日益增长，但保持风格控制和可靠的质量评估仍然是一个挑战。

Method: PushGen结合了两个关键组件：1）一种可控类别提示技术，用于引导LLM输出达到期望风格；2）一个奖励模型，对生成的内容进行排名和选择。

Result: 广泛的离线和在线实验表明了其有效性，并已在服务数亿用户的大型工业应用中部署。

Conclusion: PushGen为生成高质量推送通知提供了一个自动化解决方案，同时解决了风格控制与质量评估的问题。

Abstract: We present PushGen, an automated framework for generating high-quality push notifications comparable to human-crafted content. With the rise of generative models, there is growing interest in leveraging LLMs for push content generation. Although LLMs make content generation straightforward and cost-effective, maintaining stylistic control and reliable quality assessment remains challenging, as both directly impact user engagement. To address these issues, PushGen combines two key components: (1) a controllable category prompt technique to guide LLM outputs toward desired styles, and (2) a reward model that ranks and selects generated candidates. Extensive offline and online experiments demonstrate its effectiveness, which has been deployed in large-scale industrial applications, serving hundreds of millions of users daily.

</details>


### [65] [RecGPT-V2 Technical Report](https://arxiv.org/abs/2512.14503)
*Chao Yi,Dian Chen,Gaoyang Guo,Jiakai Tang,Jian Wu,Jing Yu,Mao Zhang,Wen Chen,Wenjun Yang,Yujie Luo,Yuning Jiang,Zhujin Gao,Bo Zheng,Binbin Cao,Changfa Wu,Dixuan Wang,Han Wu,Haoyi Hu,Kewei Zhu,Lang Tian,Lin Yang,Qiqi Huang,Siqi Yang,Wenbo Su,Xiaoxiao He,Xin Tong,Xu Chen,Xunke Xi,Xiaowei Huang,Yaxuan Wu,Yeqiu Yang,Yi Hu,Yujin Yuan,Yuliang Yan,Zile Zhou*

Main category: cs.IR

TL;DR: RecGPT-V2 addresses the limitations of RecGPT-V1 in recommender systems, such as inefficiency and lack of diversity, by introducing a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, and constrained reinforcement learning, resulting in significant improvements in efficiency, diversity, and user engagement metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the four key limitations of RecGPT-V1, which are computational inefficiency, insufficient explanation diversity, limited generalization, and an outcome-focused evaluation that does not align with human standards. The goal is to enhance the performance and practicality of LLM-powered intent reasoning in recommender systems.

Method: RecGPT-V2 introduces four main innovations: (1) A Hierarchical Multi-Agent System for restructuring intent reasoning, (2) Hybrid Representation Inference for compressing user-behavior contexts, (3) A Meta-Prompting framework for generating diverse explanations, and (4) Constrained reinforcement learning to resolve multi-reward conflicts. Additionally, an Agent-as-a-Judge framework is used to better align with human preferences during assessment.

Result: The results show that RecGPT-V2 reduces GPU consumption by 60%, improves exclusive recall from 9.39% to 10.99%, increases explanation diversity by 7.3%, and enhances tag prediction and explanation acceptance by 24.1% and 13.0% respectively. Online A/B testing on Taobao also demonstrated positive changes in CTR, IPV, TV, and NER, indicating both technical feasibility and commercial potential.

Conclusion: RecGPT-V2 successfully overcomes the limitations of its predecessor, significantly improving the efficiency, diversity, and accuracy of recommendations, while also achieving better alignment with human preferences. This showcases the potential for large-scale deployment of LLM-powered intent reasoning in real-world applications, bridging the gap between theoretical research and industrial utility.

Abstract: Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.
  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.

</details>


### [66] [Pairwise Comparison for Bias Identification and Quantification](https://arxiv.org/abs/2512.14565)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 该论文提出了一种基于成对比较的标注方法来减少语言偏见标注中的工作量，并通过模拟环境评估了不同评分技术和成本敏感替代方案的效果。研究结果支持使用成对比较作为量化主观语言特征的实际基础，包括优化对比和匹配组件、端到端评估以及为大规模注释提供实现蓝图。


<details>
  <summary>Details</summary>
Motivation: 在线新闻和社会媒体中普遍存在语言偏见，但难以衡量。由于主观性、上下文依赖性和高质量黄金标签数据集稀缺等问题，识别和量化这种偏见非常困难。本文旨在通过利用成对比较进行偏见标注来减少标注工作量。

Method: 研究者们在模拟环境中探索了不同评分技术的影响及三种成本敏感替代方案的参数，以寻找更高效的成对比较评价实现方式。此外，还通过应用此方法于人工标注的偏见基准数据集上，评估最有前景的方法，并与大型语言模型直接评估的结果及未经修改的成对比较标签基线进行了对比。

Result: 研究发现支持将成对比较作为量化主观语言特征（如偏见）的一个实用基础，使得偏见分析可以被重复执行。研究贡献包括比较和匹配组件的优化、包含仿真与实际数据应用在内的端到端评估，以及一个面向成本意识的大规模标注实现蓝图。

Conclusion: 这项工作为创建高质量基准数据集及量化偏见和其他主观语言方面奠定了基础，适用于人类和大型语言模型的标注。

Abstract: Linguistic bias in online news and social media is widespread but difficult to measure. Yet, its identification and quantification remain difficult due to subjectivity, context dependence, and the scarcity of high-quality gold-label datasets. We aim to reduce annotation effort by leveraging pairwise comparison for bias annotation. To overcome the costliness of the approach, we evaluate more efficient implementations of pairwise comparison-based rating. We achieve this by investigating the effects of various rating techniques and the parameters of three cost-aware alternatives in a simulation environment. Since the approach can in principle be applied to both human and large language model annotation, our work provides a basis for creating high-quality benchmark datasets and for quantifying biases and other subjective linguistic aspects.
  The controlled simulations include latent severity distributions, distance-calibrated noise, and synthetic annotator bias to probe robustness and cost-quality trade-offs. In applying the approach to human-labeled bias benchmark datasets, we then evaluate the most promising setups and compare them to direct assessment by large language models and unmodified pairwise comparison labels as baselines. Our findings support the use of pairwise comparison as a practical foundation for quantifying subjective linguistic aspects, enabling reproducible bias analysis. We contribute an optimization of comparison and matchmaking components, an end-to-end evaluation including simulation and real-data application, and an implementation blueprint for cost-aware large-scale annotation

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [67] [A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing](https://arxiv.org/abs/2512.14290)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了一种新的自适应扩展算法，用于满足SLA约束的边缘计算应用。该方法结合了基于机器学习的主动自适应扩展算法和考虑当前资源利用率及SLA限制的反应式自动扩展器。实验结果表明，所提出的混合解决方案在保证不同应用程序稳定符合SLA方面优于现有方案，将SLA违规率从最高23%降低到了6%。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算的发展及其在医疗保健、农业等领域的物联网应用中带来的新机遇，为满足严格的性能、可靠性和可用性指标等服务水平协议（SLA），需要一种有效的资源管理策略。虽然云提供了处理高峰需求所需的大规模数据存储与计算能力，但仅依靠云或现有的自动扩展算法无法充分保证SLA的一致性，因为存在资源配置延迟等问题。

Method: 研究者开发了一种新颖的自适应扩展算法，旨在改善边缘计算场景下的SLA遵守情况。该算法融合了能够预测即将到来的资源请求以预测需求的基于机器学习的前瞻性自适应扩展技术，以及一个根据当前资源使用情况和SLA约束条件进行即时调整的响应型自动缩放器。

Result: 通过在一个真实的边缘环境中对实际应用程序进行了广泛的测试后发现，相较于其他方法，所提议的方法可以显著降低SLA违反率，从最差情况下23%降至6%，证明了其在确保跨多种应用场景下SLA一致性方面的有效性。

Conclusion: 这项研究表明，通过采用结合了机器学习预测能力和即时响应机制的新型自适应扩展算法，可以在很大程度上解决边缘计算领域面临的SLA合规挑战。这种混合解决方案不仅提高了服务的质量，还简化了配置过程，展示了其在未来边缘计算环境中的巨大潜力。

Abstract: Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.

</details>


### [68] [Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs](https://arxiv.org/abs/2512.14445)
*Brenton Walker,Markus Fidler*

Main category: cs.DC

TL;DR: 本文研究了由于同步约束（如Apache Spark的Barrier Execution Mode）导致的并行计算任务中的稳定性和性能损失问题，分析了$(s,k,l)$障碍系统稳定性以及混合障碍系统的性能界限，并通过模拟验证了一个针对真实系统中调度开销的模型。


<details>
  <summary>Details</summary>
Motivation: 在一些并行计算模型中，工作任务被分解成更小的任务并且可以完全异步执行；但在其他情况下，并行任务之间存在需要同步开始甚至结束时间的限制条件。这种限制常见于许多并行化机器学习工作负载中，且Apache Spark最近添加了对Barrier Execution Mode的支持，允许用户为其作业添加此类屏障。然而，这些屏障会导致部分工作人员出现空闲期，从而相较于没有屏障的工作负载降低了其稳定性和性能。

Method: 文章首先分析了具有同步约束的工作负载所带来的稳定性影响，特别是对于$(s,k,l)$类型的障碍系统进行了深入探讨。接着，研究人员推导并评估了服务于不同类型工作负载（包括有无屏障及不同并行度）的混合障碍系统的性能上限。此外，还开发了一个用于描述真实系统中调度机制引发额外开销的模型，并通过与实际Spark系统的对比测试来验证该模型的有效性。

Result: 研究表明，在纯1-障碍情况下，理论边界与模拟结果同独立Spark系统基准数据相吻合。通过对真实系统中观察到的额外开销分布进行分析，将其归因于双事件和轮询驱动机制下障碍模式作业调度方式。所提出的模型能够很好地解释这一现象，并通过模拟得到了进一步验证。

Conclusion: 通过分析和建模，本文揭示了同步约束对并行处理效率的影响，并为理解及优化包含障碍模式的并行计算框架提供了新的视角。

Abstract: In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.
  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.

</details>


### [69] [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)
*Alireza Olama,Andreas Lundell,Izzat El Hajj,Johan Lilius,Jerker Björkqvist*

Main category: cs.DC

TL;DR: PruneX, a new system for distributed training, reduces inter-node communication in multi-GPU setups by 60% and achieves better scaling speedup compared to existing methods through its innovative H-SADMM algorithm and optimized synchronization model.


<details>
  <summary>Details</summary>
Motivation: The motivation behind PruneX is to address the increasing constraint of inter-node communication bandwidth on large-scale distributed training across multiple GPUs. Traditional pruning-aware systems do not effectively reduce this overhead due to inefficiencies with unstructured sparsity and dense collective operations.

Method: PruneX introduces Hierarchical Structured ADMM (H-SADMM) that enforces structured sparsity at the node level before synchronization, allowing for dynamic buffer compaction. It uses a leader-follower execution model, separating intra-node and inter-node process groups, which enables efficient use of dense collectives over compacted tensors and limits full synchronization to high-bandwidth intra-node links.

Result: Evaluation on ResNet architectures using 64 GPUs showed that PruneX can decrease inter-node communication volume by about 60%, achieving a 6.75x strong scaling speedup. This outperforms both the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer.

Conclusion: PruneX significantly improves upon current methods for reducing communication overhead in distributed training, demonstrating superior performance and scalability on large-scale GPU clusters.

Abstract: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [70] [Time and Relations into Focus: Ontological Foundations of Object-Centric Event Data](https://arxiv.org/abs/2512.14425)
*Hosna Hooshyar,Mattia Fumagalli,Marco Montali,Giancarlo Guizzardi*

Main category: cs.DB

TL;DR: 本文提出了一种新的方法来解决对象中心过程挖掘中的模糊性和表达性限制问题，通过基于轻量级的UFO-B版本gUFO增强OCED核心模型，形成名为gOCED的新元模型，该模型保持了现有模型的简洁性同时扩展了必要的特征以克服文献中报告的问题。


<details>
  <summary>Details</summary>
Motivation: 对象中心的过程挖掘是过程挖掘的一个新分支，其中事件与多个对象相关联，并且对象之间的交互对于理解过程动态至关重要。传统的事件数据模型（也称为案例中心）无法处理这种更复杂的关系所带来的复杂性。尽管已经提出了几种从案例中心转向对象中心事件数据(OCED)的模型，但这些模型仍然存在内在的模糊性，并缺乏对时间维度和（动态）关系的全面支持。

Method: 文章首先概述了当前OCED元模型在文献中报道的关键开放问题，并通过文献中提供的说明性和代表性示例展示了它们的模糊性和表现力局限。接着，考虑了作为定义新的面向对象事件数据标准基础而提出的OCED核心模型，并通过将其基于一种轻量级版本的UFO-B（称为gUFO）进行增强，形成了一个新元模型gOCED。

Result: gOCED元模型不仅覆盖了现有元模型的功能并保持了其简洁性，还扩展了所需的关键特性以克服文献中提到的模糊性和表现力问题。

Conclusion: 这项工作填补了对象中心事件数据领域内的空白，提供了一个更为稳健、明确和富有表现力的元模型gOCED，为未来的研究奠定了坚实的本体论基础。

Abstract: Object-centric process mining is a new branch of process mining where events are associated with multiple objects, and where object-to-object interactions are essential to understand the process dynamics. Traditional event data models, also called case-centric, are unable to cope with the complexity introduced by these more refined relationships. Several models have been made to move from case-centric to Object-Centric Event Data (OCED), trying to retain simplicity as much as possible. Still, these suffer from inherent ambiguities, and lack a comprehensive support of essential dimensions related to time and (dynamic) relations. In this work, we propose to fill this gap by leveraging a well-founded ontology of events and bringing ontological foundations to OCED, with a three-step approach. First, we start from key open issues reported in the literature regarding current OCED metamodels, and witness their ambiguity and expressiveness limitations on illustrative and representative examples proposed therein. Second, we consider the OCED Core Model, currently proposed as the basis for defining a new standard for object-centric event data, and we enhance it by grounding it on a lightweight version of UFO-B called gUFO, a well-known foundational ontology tailored to the representation of objects, events, time, and their (dynamic) relations. This results in a new metamodel, which we call gOCED. The third contribution then shows how gOCED at once covers the features of existing metamodels preserving their simplicity, and extends them with the essential features needed to overcome the ambiguity and expressiveness issues reported in the literature.

</details>
