<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 46]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Distributed Semi-Speculative Parallel Anisotropic Mesh Adaptation](https://arxiv.org/abs/2602.15204)
*Kevin Garner,Polykarpos Thomadakis,Nikos Chrisochoides*

Main category: cs.DC

TL;DR: 本文提出了一种分布式内存各向异性网格自适应方法，旨在避免使用集体通信和全局同步技术。通过将网格功能与性能方面分离，并利用一个独立的实体来处理每个方面，该方法能够在高性能计算架构上实现良好的性能，并且能够生成高达约10亿个元素的高质量网格。


<details>
  <summary>Details</summary>
Motivation: 为了提高各向异性网格自适应在高性能计算环境中的效率，同时避免使用可能降低并行计算效率的集体通信和全局同步技术。

Method: 首先在一个多核节点（共享内存）上对初始网格进行分解，并调整其界面元素（子域边界）。然后，将子域分配给HPC集群的各个节点，在这些节点上调整内部元素，而已经调整过的界面元素保持不变以确保网格一致性。此外，还介绍了如何重新设计共享内存软件以及分布式内存方法如何利用其推测执行模型来达到良好性能。

Result: 所提出的方法能够生成最高可达约10亿个元素的大规模网格，其质量和性能可与现有最先进的HPC网格软件相媲美。

Conclusion: 本研究展示了一种新的分布式内存各向异性网格自适应方案，它不仅减少了对集体通信和全局同步的需求，而且还能有效地在现代HPC架构上运行，为大规模科学计算提供支持。

Abstract: This paper presents a distributed memory method for anisotropic mesh adaptation that is designed to avoid the use of collective communication and global synchronization techniques. In the presented method, meshing functionality is separated from performance aspects by utilizing a separate entity for each - a multicore cc-NUMA-based (shared memory) mesh generation software and a parallel runtime system that is designed to help applications leverage the concurrency offered by emerging high-performance computing (HPC) architectures. First, an initial mesh is decomposed and its interface elements (subdomain boundaries) are adapted on a single multicore node (shared memory). Subdomains are then distributed among the nodes of an HPC cluster so that their interior elements are adapted while interface elements (already adapted) remain frozen to maintain mesh conformity. Lessons are presented regarding some re-designs of the shared memory software and how its speculative execution model is utilized by the distributed memory method to achieve good performance. The presented method is shown to generate meshes (of up to approximately 1 billion elements) with comparable quality and performance to existing state-of-the-art HPC meshing software.

</details>


### [2] [FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations](https://arxiv.org/abs/2602.15379)
*Zhihao Shu,Md Musfiqur Rahman Sanim,Hangyu Zheng,Kunxiong Zhu,Miao Yin,Gagan Agrawal,Wei Niu*

Main category: cs.DC

TL;DR: FlashMem, a memory streaming framework, optimizes the execution of large-scale DNNs and multi-DNN workloads on mobile GPUs by dynamically streaming model weights, achieving significant memory reduction and speedup compared to existing weight preloading strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the inefficiency of current DNN acceleration frameworks that rely on preloading all model parameters into memory, which is not suitable for large models or executing multiple distinct models in succession on mobile GPUs with limited resources.

Method: The authors introduce FlashMem, a new memory streaming framework that determines static loading schedules and streams model weights on demand. It utilizes 2.5D texture memory to reduce data transformations and improve efficiency during the execution of DNNs on mobile GPUs.

Result: Experiments conducted with 11 different DNN models show that FlashMem can achieve up to 8.4 times memory reduction and up to 75.0 times faster inference speed when compared to traditional frameworks, making it possible to run large-scale DNNs and multiple DNNs efficiently on resource-limited mobile devices.

Conclusion: FlashMem provides an effective solution for the efficient execution of large and complex DNNs on mobile GPUs, significantly reducing memory usage and inference latency, thus enabling support for more sophisticated and larger models as well as multi-DNN workloads.

Abstract: The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs.

</details>


### [3] [Service Orchestration in the Computing Continuum: Structural Challenges and Vision](https://arxiv.org/abs/2602.15794)
*Boris Sedlak,Víctor Casamayor Pujol,Ildefons Magrans de Abril,Praveen Kumar Donta,Adel N. Toosi,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文首先总结了计算连续体(CC)的结构问题，然后设想了一个理想的解决方案，用于在CC上实现自主服务编排。提出使用神经科学中的主动推理概念来支持自我组织的服务以优化服务质量。同时指出目前没有解决方案能够完全实现这一愿景，并指出了研究面临的几个结构性挑战，包括提供标准化的模拟和评估环境以比较编排机制的性能。


<details>
  <summary>Details</summary>
Motivation: 随着计算连续体（CC）整合从边缘到云的不同处理基础设施层，与中心架构相比，异构和动态的基础设增加了服务编排的复杂性。为了解决这个问题并指导相关研究，文章提出了一个理想化的自主服务编排方案。

Method: 本文采用了理论分析与概念引入相结合的方法。首先概述了CC中存在的结构问题，随后介绍了如何通过来自神经科学领域的‘主动推理’概念来促进服务在不断解读其环境的同时自我组织，从而优化服务质量。

Result: 虽然提出了利用主动推理来支持自组织服务的想法，但作者们承认目前还没有任何现有解决方案能够达到他们所设定的理想状态。此外，还强调了需要解决若干结构性挑战，特别是开发出可用于对比不同编排机制性能的标准模拟及评估环境。

Conclusion: 为了实现计算连续体中韧性且可扩展的服务编排目标，未来的研究需面对一系列结构性挑战，其中最重要的是要建立标准化的仿真和评价平台。

Abstract: The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [4] ["The Intangible Victory", Interactive Audiovisual Installation](https://arxiv.org/abs/2602.15071)
*Konstantinos Tsioutas,Panagiotis Pangalos,Konstantinos Tiligadis,Andreas Sitorengo*

Main category: cs.MM

TL;DR: 本研究通过一个视听装置重新定义了萨莫色雷斯胜利女神雕像的视觉象征，强调了时间作为磨损因素的作用以及空虚在雕塑形式缺失中的特殊重要性。该装置允许观众与作品互动，通过移动产生声音环境，从而创造出一种新的空间与时间之间的体验对话。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何利用数字媒体和技术来重新诠释古代雕塑的视觉象征意义，并且通过引入观众互动的方式，为现代观众提供了一种全新的艺术体验和对话方式。

Method: 研究人员创建了一个名为“无形胜利”的视听装置，该装置以萨莫色雷斯胜利女神像为基础但呈现为一种无形的存在。装置中使用了导电传感器制成的彩色线来重建雕塑形态，并让参观者能够通过移动与之互动，生成声音环境。

Result: 通过这种创新的方法，不仅重现了胜利女神像的艺术价值，还成功地将观众纳入到了作品的一部分，使得每位参与者都能成为艺术表达的一部分，共同创造出了时空之间的一种新对话。

Conclusion: 这项工作展示了数字媒体技术如何被用来增强传统艺术品的表现力，同时促进了观众与艺术作品之间的新型互动模式，为理解古老雕塑提供了当代视角下的新方法。

Abstract: "Intangible Victory" is an audiovisual installation in the form of the intangible being of the Victory of Samothrace that uses interactive digital media. Specifically, through this installation, we redefine the visual symbolism of the ancient sculpture, paying attention to time as a wear factor (entropy) and the special importance of the void as an absence of the sculptural form. Emptiness completes the intangible essence of the sculpture in the field of symbolism as well as in that of artistic significance for the interpretation of the work today. The function of the void and the interaction of the viewer with the work, causes the emergence of a new experience-dialogue between space and time. The use of digital media and technology reveals the absence of the sculptural form as it is visualized in the Victory of Samothrace. The sculptural form is reconstructed from fibers in space in a cylindrical arrangement. The form is rendered with colored strings - conductive sensors, that allow the visitor to interact with the work, creating a sound environment through movement. The sound completely replaces the volume, as the void of the sculptural form together with the viewer in unison present an audiovisual symbolism of the Victory of Samothrace.

</details>


### [5] [Proactive Conversational Assistant for a Procedural Manual Task based on Audio and IMU](https://arxiv.org/abs/2602.15707)
*Rehana Mahfuz,Yinyi Guo,Erik Visser,Phanidhar Chinchili*

Main category: cs.MM

TL;DR: 本文提出了一种仅依赖轻量级隐私保护模式（如音频和IMU输入）的实时对话助手，用于在用户执行家具组装任务时提供逐步指导并回答用户问题。通过一种新的用户意愿无关(UWA) LoRA微调方法，改进了模型抑制不那么有信息量对话的能力，同时保持其传达重要指令的趋势，从而在F-score上提高了30%以上，并且通过消除在提示中提供上下文示例的需求，实现了16倍的速度提升。此外还描述了如何在不依赖云的情况下，在边缘设备上实现这种助手。


<details>
  <summary>Details</summary>
Motivation: 现有的实时对话助手通常依赖于视频输入，这不仅计算成本高，还会侵犯用户隐私。为了解决这些问题，研究者希望开发一个能够使用更轻量、更注重隐私保护的数据输入方式（例如：音频和惯性测量单元IMU数据）来理解情境，并为用户提供详细指引的对话助手。

Method: 研究者们创建了一个包含助手引导用户完成任务对话的数据集。观察到现成的语言模型作为一个非常健谈的助手后，设计了一种新的用户意愿无关(UWA)LoRA微调方法，旨在提高模型压制较不具信息性的对话能力，同时保持其传达关键指示的倾向。

Result: >30%的F分数提升；经过微调的模型由于不需要在提示中给出上下文示例而实现了16倍的速度加速。

Conclusion: 本研究成功地开发出一种基于轻量级及隐私保护输入模态的实时对话助手，它能有效地帮助用户完成程序性任务，比如组装家具。通过引入用户意愿无关(UWA)LoRA微调技术，显著提升了系统性能以及响应速度。

Abstract: Real-time conversational assistants for procedural tasks often depend on video input, which can be computationally expensive and compromise user privacy. For the first time, we propose a real-time conversational assistant that provides comprehensive guidance for a procedural task using only lightweight privacy-preserving modalities such as audio and IMU inputs from a user's wearable device to understand the context. This assistant proactively communicates step-by-step instructions to a user performing a furniture assembly task, and answers user questions. We construct a dataset containing conversations where the assistant guides the user in performing the task. On observing that an off-the-shelf language model is a very talkative assistant, we design a novel User Whim Agnostic (UWA) LoRA finetuning method which improves the model's ability to suppress less informative dialogues, while maintaining its tendency to communicate important instructions. This leads to >30% improvement in the F-score. Finetuning the model also results in a 16x speedup by eliminating the need to provide in-context examples in the prompt. We further describe how such an assistant is implemented on edge devices with no dependence on the cloud.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [6] [Efficient Approximate Nearest Neighbor Search under Multi-Attribute Range Filter](https://arxiv.org/abs/2602.15488)
*Yuanhang Yu,Dawei Cheng,Ying Zhang,Lu Qin,Wenjie Zhang,Xuemin Lin*

Main category: cs.DB

TL;DR: 本文提出了KHI，一种用于多属性范围过滤近似最近邻搜索的索引方法。它结合了属性空间分割树和附加到树节点的HNSW图。实验表明，KHI在维持高召回率的同时实现了高查询吞吐量，与现有技术相比，在不同条件下显著提高了查询每秒处理数（QPS）。


<details>
  <summary>Details</summary>
Motivation: 在现代AI和数据库系统中，高维向量上的最近邻搜索是一项基础任务。对于涉及多个数值属性约束的查询，即范围过滤近似最近邻搜索(RFANNS)，虽然已有针对单一属性范围谓词的RFANNS索引，但在多属性场景下的扩展既非易事也往往效果不佳。

Method: 提出了一种名为KHI的新索引方法，专为解决多属性RFANNS问题而设计。该方法通过将属性空间分割树与附加于树节点上的HNSW图相结合来实现高效搜索。此外，采用了一种偏斜感知分裂规则以确保树的高度保持在O(log n)级别。

Result: 在四个真实世界数据集上的实验结果显示，KHI能够在保持高水平召回率的同时提供高查询吞吐量。与最先进基准相比，KHI平均提升了2.46倍的QPS，并且在难度较大的数据集上最高可达16.22倍提升。尤其是在选择性更低、k值更大以及谓词基数更高的情况下表现更佳。

Conclusion: KHI是一种有效的多属性RFANNS解决方案，能够大幅提高查询效率而不牺牲准确度。这对于需要处理复杂查询条件的实际应用来说是一个重要的进步。

Abstract: Nearest neighbor search on high-dimensional vectors is fundamental in modern AI and database systems. In many real-world applications, queries involve constraints on multiple numeric attributes, giving rise to range-filtering approximate nearest neighbor search (RFANNS). While there exist RFANNS indexes for single-attribute range predicates, extending them to the multi-attribute setting is nontrivial and often ineffective. In this paper, we propose KHI, an index for multi-attribute RFANNS that combines an attribute-space partitioning tree with HNSW graphs attached to tree nodes. A skew-aware splitting rule bounds the tree height by $O(\log n)$, and queries are answered by routing through the tree and running greedy search on the HNSW graphs. Experiments on four real-world datasets show that KHI consistently achieves high query throughput while maintaining high recall. Compared with the state-of-the-art RFANNS baseline, KHI improves QPS by $2.46\times$ on average and up to $16.22\times$ on the hard dataset, with larger gains for smaller selectivity, larger $k$, and higher predicate cardinality.

</details>


### [7] [A universal LLM Framework for General Query Refinements](https://arxiv.org/abs/2602.15681)
*Eldar Hacohen,Yuval Moskovitch,Amit Somech*

Main category: cs.DB

TL;DR: 介绍了一种名为OmniTune的新框架，它使用基于大型语言模型（LLM）的优化通过提示技术(OPRO)来改进任意SQL查询。该框架通过探索有潜力的修改子空间并从中选取候选方案，同时利用简洁的历史和天际线摘要提供有效反馈。实验表明，OmniTune不仅能够处理之前研究过的查询改进任务，还能应对超出当前解决方案范围的更复杂情况。


<details>
  <summary>Details</summary>
Motivation: 现有的SQL查询改进研究通常局限于特定类型的查询或约束条件。为了克服这一局限性，并提供一个更加通用的解决方案以适应各种查询及其约束条件，提出了OmniTune框架。

Method: OmniTune采用了一个两步走的OPRO方案：首先探索可能产生有效结果的修改子空间；然后在这些子空间内抽取候选方案。整个过程由简明的历史记录与天际线概要支持，确保了高效的反馈机制。

Result: 综合基准测试显示，OmniTune不仅能成功处理过去文献中讨论过的各类查询改进问题，而且对于那些现有方法难以解决的更为复杂的场景也表现出了良好的适应性和有效性。

Conclusion: OmniTune作为一个通用且强大的SQL查询改进工具展现出了显著优势，特别是在处理复杂查询及约束方面超越了现有方法。

Abstract: Numerous studies have explored the SQL query refinement problem, where the objective is to minimally modify an input query so that it satisfies a specified set of constraints. However, these works typically target restricted classes of queries or constraints. We present OmniTune, a general framework for refining arbitrary SQL queries using LLM-based optimization by prompting (OPRO). OmniTune employs a two-step OPRO scheme that explores promising refinement subspaces and samples candidates within them, supported by concise history and skyline summaries for effective feedback.
  Experiments on a comprehensive benchmark demonstrate that OmniTune handles both previously studied refinement tasks and more complex scenarios beyond the scope of existing solutions.

</details>


### [8] [Hierarchical Decomposition of Separable Workflow-Nets](https://arxiv.org/abs/2602.15739)
*Humam Kourani,Gyunam Park,Wil M. P. van der Aalst*

Main category: cs.DB

TL;DR: 本文提出了一种新的算法，将安全且合理的流程网（WF-nets）转换为等效的POWL 2.0模型。该算法通过递归识别WF-net中的结构模式并将其翻译成POWL表示来工作，并且证明了其在可分离WF-nets类上的正确性和完整性。此外，通过大规模流程模型评估表明该算法具有很高的可扩展性，并成功应用于1,493个工业和合成流程模型基准测试中，表明POWL 2.0能够捕捉到现实世界业务流程中的复杂逻辑。


<details>
  <summary>Details</summary>
Motivation: 为了弥合POWL理论优势与实际需求之间对现有标记兼容性的需要之间的差距，研究提出了一个鲁棒的模型转换算法。这有助于实现POWL在实际流程分析和改进应用中的更广泛采用。

Method: 提出的新算法通过递归地在WF-net中识别结构模式并将这些模式转换为其POWL表示来工作。此算法利用选择图来捕获广义决策和循环模式，区别于先前分别检测独占选择和循环的方法。

Result: 形式上证明了所生成的POWL模型保留了输入WF-net的语言特性，并且对于构建状态机和标记图层次嵌套而成的可分离WF-nets类，证明了算法的完备性。大规模过程模型的评估展示了算法的高度可扩展性，并且在1,493个工业级和合成过程模型的基准测试中成功转化了所有模型。

Conclusion: 研究表明，新提出的算法不仅能够有效地将WF-nets转换为POWL 2.0模型，而且在处理复杂的真实世界业务流程方面表现出色，这为进一步推广使用POWL奠定了基础。

Abstract: The Partially Ordered Workflow Language (POWL) has recently emerged as a process modeling notation, offering strong quality guarantees and high expressiveness. While early versions of POWL relied on strict block-structured operators for choices and loops, the language has recently evolved into POWL 2.0, introducing choice graphs to enable the modeling of non-block-structured decisions and cycles. To bridge the gap between the theoretical advantages of POWL and the practical need for compatibility with established notations, robust model transformations are required. This paper presents a novel algorithm for transforming safe and sound workflow nets (WF-nets) into equivalent POWL 2.0 models. The algorithm recursively identifies structural patterns within the WF-net and translates them into their POWL representation. Unlike the previous approach that required separate detection strategies for exclusive choices and loops, our new algorithm utilizes choice graphs to capture generalized decision and cyclic patterns. We formally prove the correctness of our approach, showing that the generated POWL model preserves the language of the input WF-net. Furthermore, we prove the completeness of our algorithm on the class of separable WF-nets, which corresponds to nets constructed via the hierarchical nesting of state machines and marked graphs. We evaluate our algorithm on large-scale process models to demonstrate its high scalability. Furthermore, to test its practical expressiveness, we applied it to a benchmark of 1,493 industrial and synthetic process models. Our algorithm successfully transformed all models in this benchmark, suggesting that POWL 2.0's expressive power is generally sufficient to capture the complex logic found in real-world business processes. This work paves the way for broader adoption of POWL in practical process analysis and improvement applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [9] [Semantics-Aware Denoising: A PLM-Guided Sample Reweighting Strategy for Robust Recommendation](https://arxiv.org/abs/2602.15359)
*Xikai Yang,Yang Wang,Yilin Li,Sebastian Sun*

Main category: cs.IR

TL;DR: 本文提出了SAID框架，通过利用用户兴趣和项目内容之间的语义一致性来识别并降低潜在噪声交互的影响，从而改善推荐系统的性能。实验表明，SAID在提高推荐准确性方面表现优异，特别是在高噪声条件下。


<details>
  <summary>Details</summary>
Motivation: 点击交互中包含大量噪音（如意外点击、标题党诱导的互动等），这些噪音并不反映用户的真实偏好。基于这样的数据训练推荐模型会导致预测准确度下降和推荐不可靠。

Method: SAID框架构建了从历史行为得出的文本用户兴趣档案，并使用预训练语言模型（PLM）为基础的文本编码器计算与目标项目描述之间的语义相似性。然后将相似性分数转换为样本权重，调整训练损失，有效减少了语义不一致点击的影响。该方法仅修改损失函数而不改变推荐模型的核心结构。

Result: 在两个真实世界数据集上的广泛实验表明，相比强大的基线模型，SAID能够始终提高推荐性能，在AUC上最高可实现相对2.2%的提升，尤其在高噪声条件下表现出色。

Conclusion: 通过引入SAID框架，可以有效地减少噪声交互对推荐系统性能的影响，提高了推荐结果的准确性和可靠性。

Abstract: Implicit feedback, such as user clicks, serves as the primary data source for modern recommender systems. However, click interactions inherently contain substantial noise, including accidental clicks, clickbait-induced interactions, and exploratory browsing behaviors that do not reflect genuine user preferences. Training recommendation models with such noisy positive samples leads to degraded prediction accuracy and unreliable recommendations. In this paper, we propose SAID (Semantics-Aware Implicit Denoising), a simple yet effective framework that leverages semantic consistency between user interests and item content to identify and downweight potentially noisy interactions. Our approach constructs textual user interest profiles from historical behaviors and computes semantic similarity with target item descriptions using pre-trained language model (PLM) based text encoders. The similarity scores are then transformed into sample weights that modulate the training loss, effectively reducing the impact of semantically inconsistent clicks. Unlike existing denoising methods that require complex auxiliary networks or multi-stage training procedures, SAID only modifies the loss function while keeping the backbone recommendation model unchanged. Extensive experiments on two real-world datasets demonstrate that SAID consistently improves recommendation performance, achieving up to 2.2% relative improvement in AUC over strong baselines, with particularly notable robustness under high noise conditions.

</details>


### [10] [Automatic Funny Scene Extraction from Long-form Cinematic Videos](https://arxiv.org/abs/2602.15381)
*Sibendu Paul,Haotian Jiang,Caren Chen*

Main category: cs.IR

TL;DR: 本文提出了一种端到端的系统，用于从长篇影视作品中自动识别和排名幽默场景。该系统包括镜头检测、多模态场景定位以及为影视内容优化的幽默标签。创新点包括结合视觉与文本线索的新颖场景分割方法、通过引导三元组挖掘改进的镜头表示法，以及利用音频和文本的多模态幽默标签框架。系统在OVSD数据集上的场景检测精度比现有最先进方法提高了18.3%，在长文本幽默检测上F1分数达到0.834。五个影视作品的广泛评估显示，系统提取的片段中有87%是意图搞笑的，而98%的场景被准确定位。


<details>
  <summary>Details</summary>
Motivation: 从影视标题中自动抽取吸引人且高质量的幽默场景对于制作引人入胜的视频预览和快餐式内容至关重要，这可以提高流媒体平台上的用户参与度。然而，长篇影视标题由于其较长的时长和复杂的叙事结构，使得场景定位变得困难；同时，幽默依赖于多种模态并且风格细腻，这增加了进一步的复杂性。

Method: 本文介绍了一种端到端系统，专门用于从长篇影视作品中自动识别并排名幽默场景。该系统包含三个主要部分：基于视觉和文本线索进行场景分割的新方法、通过引导三元组挖掘改善了镜头表现形式、以及一个融合了音频和文本信息的多模态幽默标记框架。

Result: 该系统在OVSD数据集上实现了相较于最先进技术18.3%的AP（平均精度）提升，并在长文本幽默检测方面达到了0.834的F1分数。对五部电影作品进行了广泛的测试，结果显示本系统所提取片段中有87%是旨在制造笑料的，同时98%的场景得到了精确地定位。此外，该方案还成功地扩展到了预告片的应用场景中。

Conclusion: 实验结果表明，所提出的系统能够有效地从长篇影视作品中识别出幽默场景，并且具有较高的准确性和泛化能力，适用于不同类型的内容创作流程，有助于提高用户参与度并简化快餐式内容生成过程。

Abstract: Automatically extracting engaging and high-quality humorous scenes from cinematic titles is pivotal for creating captivating video previews and snackable content, boosting user engagement on streaming platforms. Long-form cinematic titles, with their extended duration and complex narratives, challenge scene localization, while humor's reliance on diverse modalities and its nuanced style add further complexity. This paper introduces an end-to-end system for automatically identifying and ranking humorous scenes from long-form cinematic titles, featuring shot detection, multimodal scene localization, and humor tagging optimized for cinematic content. Key innovations include a novel scene segmentation approach combining visual and textual cues, improved shot representations via guided triplet mining, and a multimodal humor tagging framework leveraging both audio and text. Our system achieves an 18.3% AP improvement over state-of-the-art scene detection on the OVSD dataset and an F1 score of 0.834 for detecting humor in long text. Extensive evaluations across five cinematic titles demonstrate 87% of clips extracted by our pipeline are intended to be funny, while 98% of scenes are accurately localized. With successful generalization to trailers, these results showcase the pipeline's potential to enhance content creation workflows, improve user engagement, and streamline snackable content generation for diverse cinematic media formats.

</details>


### [11] [Binge Watch: Reproducible Multimodal Benchmarks Datasets for Large-Scale Movie Recommendation on MovieLens-10M and 20M](https://arxiv.org/abs/2602.15505)
*Giuseppe Spillo,Alessandro Petruzzelli,Cataldo Musto,Marco de Gemmis,Pasquale Lops,Giovanni Semeraro*

Main category: cs.IR

TL;DR: 本文发布并详细介绍了两个大规模、可复现的多模态电影推荐数据集M3L-10M和M3L-20M，旨在推动多模态推荐系统领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐系统的研究大多依赖于规模较小或中等的数据集，且这些数据集要么未公开，要么其构建过程缺乏文档记录。为了填补这一空白，本文旨在提供高质量、富含多媒体侧信息（如文本、图像、音频、视频）的大规模数据集。

Method: 通过扩展流行的MovieLens-10M和MovieLens-20M数据集，引入了电影剧情、海报及预告片等多模态特征，并利用多种最先进的编码器提取文本、视觉、声学及视频特征。所有处理步骤均被详尽记录以保证可复现性。

Result: 成功构建了M3L-10M与M3L-20M两个大型多模态数据集，并提供了原始数据下载链接、提取特征以及多种格式的完整数据集，促进了该领域的研究可复现性。同时，进行了定性和定量分析来展示数据集在不同方面的表现。

Conclusion: 本研究为大规模多模态电影推荐领域内的可复现性和重复性奠定了基础，所提供的资源能够极大促进该领域未来的研究与发展。

Abstract: With the growing interest in Multimodal Recommender Systems (MRSs), collecting high-quality datasets provided with multimedia side information (text, images, audio, video) has become a fundamental step. However, most of the current literature in the field relies on small- or medium-scale datasets that are either not publicly released or built using undocumented processes.
  In this paper, we aim to fill this gap by releasing M3L-10M and M3L-20M, two large-scale, reproducible, multimodal datasets for the movie domain, obtained by enriching with multimodal features the popular MovieLens-10M and MovieLens-20M, respectively. By following a fully documented pipeline, we collect movie plots, posters, and trailers, from which textual, visual, acoustic, and video features are extracted using several state-of-the-art encoders. We publicly release mappings to download the original raw data, the extracted features, and the complete datasets in multiple formats, fostering reproducibility and advancing the field of MRSs. In addition, we conduct qualitative and quantitative analyses that showcase our datasets across several perspectives.
  This work represents a foundational step to ensure reproducibility and replicability in the large-scale, multimodal movie recommendation domain. Our resource can be fully accessed at the following link: https://zenodo.org/records/18499145, while the source code is accessible at https://github.com/giuspillo/M3L_10M_20M.

</details>


### [12] [Eco-Amazon: Enriching E-commerce Datasets with Product Carbon Footprint for Sustainable Recommendations](https://arxiv.org/abs/2602.15508)
*Giuseppe Spillo,Allegra De Filippo,Cataldo Musto,Michela Milano,Giovanni Semeraro*

Main category: cs.IR

TL;DR: 本文介绍了Eco-Amazon，一种新的资源，旨在通过为广泛使用的亚马逊数据集添加产品碳足迹元数据来促进可持续性信息检索和推荐系统的研究。


<details>
  <summary>Details</summary>
Motivation: 在负责任和可持续的AI时代，信息检索和推荐系统必须扩大其关注范围，不仅局限于传统的准确性指标，还应考虑环境可持续性。然而，由于标准基准中缺乏项目级别的环境影响数据，这一研究方向受到了严重限制。

Method: 作者通过引入Eco-Amazon来解决这个问题，该资源包括三个广为人知的亚马逊数据集（即家居、服装和电子产品）的增强版本，并增加了基于产品属性的产品碳足迹（PCF）元数据。CO2e排放分数是利用一个零样本框架生成的，该框架使用大型语言模型(LLMs)来估计项目级别的PCF。

Result: 本研究的主要贡献在于：(i) 发布了带有PCF信号的Eco-Amazon数据集；(ii) 提供了一个基于LLM的PCF估计脚本，允许研究人员丰富任何产品目录并重现结果；(iii) 通过一个使用案例展示了如何利用PCF估计来推广更加环保的产品。

Conclusion: 通过提供这些环境信号，Eco-Amazon使社区能够开发、基准测试并评估下一代可持续检索和推荐模型。

Abstract: In the era of responsible and sustainable AI, information retrieval and recommender systems must expand their scope beyond traditional accuracy metrics to incorporate environmental sustainability. However, this research line is severely limited by the lack of item-level environmental impact data in standard benchmarks. This paper introduces Eco-Amazon, a novel resource designed to bridge this gap. Our resource consists of an enriched version of three widely used Amazon datasets (i.e., Home, Clothing, and Electronics) augmented with Product Carbon Footprint (PCF) metadata. CO2e emission scores were generated using a zero-shot framework that leverages Large Language Models (LLMs) to estimate item-level PCF based on product attributes. Our contribution is three-fold: (i) the release of the Eco-Amazon datasets, enriching item metadata with PCF signals; (ii) the LLM-based PCF estimation script, which allows researchers to enrich any product catalogue and reproduce our results; (iii) a use case demonstrating how PCF estimates can be exploited to promote more sustainable products. By providing these environmental signals, Eco-Amazon enables the community to develop, benchmark, and evaluate the next generation of sustainable retrieval and recommendation models. Our resource is available at https://doi.org/10.5281/zenodo.18549130, while our source code is available at: http://github.com/giuspillo/EcoAmazon/.

</details>


### [13] [Can Recommender Systems Teach Themselves? A Recursive Self-Improving Framework with Fidelity Control](https://arxiv.org/abs/2602.15659)
*Luankang Zhang,Hao Wang,Zhongzhou Liu,Mingjia Yin,Yonghao Huang,Jiaqi Li,Wei Guo,Yong Liu,Huifeng Guo,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: 本文提出了一种名为RSIR的框架，该框架通过模型自身生成并筛选用户交互序列来提高推荐系统的性能，而无需依赖外部数据或教师模型。理论分析表明，RSIR能够平滑优化景观，引导模型找到更稳健的解决方案。实验结果表明，RSIR方法在多个基准和架构上均能带来一致性的累积增益，证明了递归自我改进是克服数据稀疏性的一种通用方法。


<details>
  <summary>Details</summary>
Motivation: 推荐系统面临的一个主要问题是高质量训练数据的稀缺性，这导致了优化难题和泛化能力差。为了解决这个问题，作者提出了一个不需要额外数据或教师模型就能自我提升的框架。

Method: 提出的Recursive Self-Improving Recommendation (RSIR)框架采用闭环操作方式：当前模型生成合理的用户交互序列，然后通过基于保真度的质量控制机制筛选出符合用户偏好流形的数据，用这些经过增强的数据集来训练下一个模型版本。

Result: 理论分析显示，RSIR充当了数据驱动型隐式正则化器的角色，有助于平滑优化路径，并促使模型趋向于更加鲁棒解。实验证据支持了这一点，在多种基准测试和不同架构下，RSIR方法都显示出持续且累积的优势。此外，即使对于较小规模的模型而言也有帮助，弱模型还可以为更强的模型生成有效的训练课程。

Conclusion: 研究结果表明，递归自我改进是一种通用且不依赖特定模型的方法，可以有效解决数据稀疏问题，为推荐系统及其他领域提供了一个可扩展的发展方向。

Abstract: The scarcity of high-quality training data presents a fundamental bottleneck to scaling machine learning models. This challenge is particularly acute in recommendation systems, where extreme sparsity in user interactions leads to rugged optimization landscapes and poor generalization. We propose the Recursive Self-Improving Recommendation (RSIR) framework, a paradigm in which a model bootstraps its own performance without reliance on external data or teacher models. RSIR operates in a closed loop: the current model generates plausible user interaction sequences, a fidelity-based quality control mechanism filters them for consistency with user's approximate preference manifold, and a successor model is augmented on the enriched dataset. Our theoretical analysis shows that RSIR acts as a data-driven implicit regularizer, smoothing the optimization landscape and guiding models toward more robust solutions. Empirically, RSIR yields consistent, cumulative gains across multiple benchmarks and architectures. Notably, even smaller models benefit, and weak models can generate effective training curricula for stronger ones. These results demonstrate that recursive self-improvement is a general, model-agnostic approach to overcoming data sparsity, suggesting a scalable path forward for recommender systems and beyond. Our anonymized code is available at https://anonymous.4open.science/r/RSIR-7C5B .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Near-Optimal Sample Complexity for Online Constrained MDPs](https://arxiv.org/abs/2602.15076)
*Chang Liu,Yunfan Li,Lin F. Yang*

Main category: cs.LG

TL;DR: 提出了一种基于模型的原始对偶算法，用于解决具有安全约束的强化学习问题，在允许小违规的情况下，该算法的学习效率与无约束MDP相当；而在严格遵守约束条件下，通过一定数量的学习回合，能够以任意高的概率返回一个ε-最优策略且无违规。


<details>
  <summary>Details</summary>
Motivation: 在诸如自动驾驶、机器人和医疗保健等实际应用中，强化学习的安全性是一个基本挑战。当前方法要么导致显著的安全违规，要么需要很高的样本复杂度才能生成接近最优的策略。因此，研究旨在开发一种更有效的算法来平衡性能优化与安全约束。

Method: 研究者提出了一个基于模型的原始对Dual算法，它结合了在线强化学习和受限优化技术，旨在最小化后悔的同时控制约束违规量。针对放松可行性（允许少量违规）和严格可行性（不允许任何违规）两种情况进行了探讨。

Result: 对于放松可行性设置，证明了所提算法能够在$\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$个学习周期内以任意高概率返回$\varepsilon$-最优策略及$\varepsilon$-界限违规；而对于严格可行性，则需$\tilde{O}\left(\frac{SAH^5}{\varepsilon^2ζ^2}\right)$个周期来保证零违规。这些结果表明，在线学习CMDPs并不比使用生成模型学习更难。

Conclusion: 这项工作表明，在允许轻微违规的前提下，在线学习带有约束条件的马尔可夫决策过程(CMDPs)与学习未受约束的MDPs一样容易，并且其难度不会超过后者。此外，当要求严格遵守所有约束时，也提供了一个有效的方法来找到几乎最优解。

Abstract: Safety is a fundamental challenge in reinforcement learning (RL), particularly in real-world applications such as autonomous driving, robotics, and healthcare. To address this, Constrained Markov Decision Processes (CMDPs) are commonly used to enforce safety constraints while optimizing performance. However, existing methods often suffer from significant safety violations or require a high sample complexity to generate near-optimal policies. We address two settings: relaxed feasibility, where small violations are allowed, and strict feasibility, where no violation is allowed. We propose a model-based primal-dual algorithm that balances regret and bounded constraint violations, drawing on techniques from online RL and constrained optimization. For relaxed feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with $\varepsilon$-bounded violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ learning episodes, matching the lower bound for unconstrained MDPs. For strict feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with zero violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2ζ^2}\right)$ learning episodes, where $ζ$ is the problem-dependent Slater constant characterizing the size of the feasible region. This result matches the lower bound for learning CMDPs with access to a generative model.
  Our results demonstrate that learning CMDPs in an online setting is as easy as learning with a generative model and is no more challenging than learning unconstrained MDPs when small violations are allowed.

</details>


### [15] [Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction](https://arxiv.org/abs/2602.15089)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 该研究提出了一种结合了Granite TinyTimeMixer时间序列嵌入和基于领域知识的统计特征的方法，用于HVAC设备异常预测。通过使用LightGBM梯度提升分类器学习这些特征，实验结果表明该方法在30天、60天和90天的预测范围内达到了91-95%的精确度和0.995的ROC-AUC值，同时保持了低误报率（1.1%或更低）和高检测率（88-94%）。


<details>
  <summary>Details</summary>
Motivation: 纯深度学习方法在实际数据上往往无法达到足够的准确性，因此本研究旨在通过结合深度学习的时间序列表示学习能力和统计特征工程的优点来改进预测维护中的异常检测系统。

Method: 本研究采用了一种混合方法，将从经过LoRA微调的Granite TinyTimeMixer编码器提取的64维时间序列嵌入与28种基于领域知识的统计特征相结合，包括趋势、波动性和回撤指标，并利用LightGBM梯度提升分类器对这些特征进行学习。

Result: 实验结果显示，在包含64个设备单元和51,564个样本的数据集上，该方法在30天、60天及90天预测期内的精确度为91%-95%，ROC-AUC值为0.995。此外，该系统的假阳性率不超过1.1%，检出率为88%-94%，证明了其在预测性维护应用中的有效性。

Conclusion: 这项工作表明，通过利用深度学习的表示学习能力和统计特征工程之间的互补优势，可以实现实用的异常检测系统。

Abstract: In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\% or less and a detection rate of 88--94\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.

</details>


### [16] [PolyNODE: Variable-dimension Neural ODEs on M-polyfolds](https://arxiv.org/abs/2602.15128)
*Per Åhag,Alexander Friedrich,Fredrik Ohlsson,Viktor Vigren Näslund*

Main category: cs.LG

TL;DR: 本文提出了PolyNODE，这是几何深度学习中首个基于流的可变维度模型，通过在M-polyfolds（可以同时适应不同维度并具有可微性概念的空间）上扩展神经常微分方程(NODEs)。该模型能够解决重建任务，并从输入中提取潜在表示以完成下游分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有的神经常微分方程(NODEs)模型受限于固定维度的动力学特性，因为它们基于的流形固有地限制了维度。为了解决这个问题，文章提出了一种新的方法来支持可变维度下的流模型。

Method: 通过引入M-polyfolds的概念，作者们开发了PolyNODEs，它是一种能够在几何深度学习中处理变化维度的新型流模型。此外，他们还构建了具有维度瓶颈特征的具体M-polyfolds以及能够穿越这些瓶颈的参数化向量场基础上的PolyNODE自编码器。

Result: 实验表明，PolyNODE模型能够被训练来解决特定空间内的重建任务，并且可以从输入数据中提取出可用于后续分类任务的潜在表征。

Conclusion: PolyNODEs代表了在几何深度学习领域内实现可变维度流模型的一个重要进步，其不仅能够执行复杂的重建任务，而且还可以用于提取有助于分类任务的有用特征。

Abstract: Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at https://github.com/turbotage/PolyNODE .

</details>


### [17] [Learning Representations from Incomplete EHR Data with Dual-Masked Autoencoding](https://arxiv.org/abs/2602.15159)
*Xiao Xiang,David Restrepo,Hyewon Jeong,Yugang Jia,Leo Anthony Celi*

Main category: cs.LG

TL;DR: 提出了一种新的自编码器AID-MAE，可以从不完整的时间序列直接学习，并通过内在缺失掩码和增强掩码来处理电子健康记录中的数据缺失问题。该方法在多个临床任务上优于其他基线模型，并且学习到的嵌入能够自然地对患者群体进行分层。


<details>
  <summary>Details</summary>
Motivation: 从电子健康记录（EHRs）时间序列中学习面临着采样不规则、异质性缺失以及由此导致的观察稀疏性等问题。先前的自监督方法要么先填充再学习，要么通过专门的输入信号表示缺失值，或仅针对填充优化，这降低了它们有效学习支持临床下游任务表示的能力。

Method: 提出了Augmented-Intrinsic Dual-Masked Autoencoder (AID-MAE)，它能直接从未完成的时间序列学习。此方法使用一个内在缺失掩码来表示自然缺失值，并应用了一个增强掩码，在训练期间隐藏部分观察值用于重建。AID-MAE只处理未被遮盖的数据子集。

Result: AID-MAE在两个数据集上的多个临床任务中始终优于强基准模型，包括XGBoost和DuETT。此外，学习得到的嵌入能够在表示空间中自然地对患者队列进行分层。

Conclusion: AID-MAE提供了一种新的方式来处理EHRs时间序列中的数据缺失问题，同时有效地学习了有助于多种临床任务的表示。

Abstract: Learning from electronic health records (EHRs) time series is challenging due to irregular sam- pling, heterogeneous missingness, and the resulting sparsity of observations. Prior self-supervised meth- ods either impute before learning, represent missingness through a dedicated input signal, or optimize solely for imputation, reducing their capacity to efficiently learn representations that support clinical downstream tasks. We propose the Augmented-Intrinsic Dual-Masked Autoencoder (AID-MAE), which learns directly from incomplete time series by applying an intrinsic missing mask to represent naturally missing values and an augmented mask that hides a subset of observed values for reconstruction during training. AID-MAE processes only the unmasked subset of tokens and consistently outperforms strong baselines, including XGBoost and DuETT, across multiple clinical tasks on two datasets. In addition, the learned embeddings naturally stratify patient cohorts in the representation space.

</details>


### [18] [tensorFM: Low-Rank Approximations of Cross-Order Feature Interactions](https://arxiv.org/abs/2602.15229)
*Alessio Mazzetto,Mohammad Mahdi Khalili,Laura Fee Nern,Michael Viderman,Alex Shtoff,Krzysztof Dembczyński*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型tensorFM，用于处理表格分类数据的预测问题，该模型通过低秩张量近似有效捕捉属性间的高阶交互作用，表现优异且延迟低，适合在线广告等时间敏感应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决表格分类数据中的预测问题，比如点击率预测和社会科学领域的问题，本文提出了一个新的模型来更有效地捕捉不同属性之间的高阶交互。

Method: 本文介绍并分析了tensorFM模型，该模型利用低秩张量近似表示属性间交互作用的强度，是对field-weighted factorization machines的一种泛化。

Result: 实验结果表明，tensorFM在性能上能够与最先进方法竞争，并且由于其低延迟特性，特别适用于如在线广告这样对时间敏感的应用场景。

Conclusion: tensorFM作为一种新型模型，在处理具有多个分类属性的数据时展现出了强大的能力，尤其是在需要快速响应的实际应用中。

Abstract: We address prediction problems on tabular categorical data, where each instance is defined by multiple categorical attributes, each taking values from a finite set. These attributes are often referred to as fields, and their categorical values as features. Such problems frequently arise in practical applications, including click-through rate prediction and social sciences. We introduce and analyze {tensorFM}, a new model that efficiently captures high-order interactions between attributes via a low-rank tensor approximation representing the strength of these interactions. Our model generalizes field-weighted factorization machines. Empirically, tensorFM demonstrates competitive performance with state-of-the-art methods. Additionally, its low latency makes it well-suited for time-sensitive applications, such as online advertising.

</details>


### [19] [Seeing to Generalize: How Visual Data Corrects Binding Shortcuts](https://arxiv.org/abs/2602.15183)
*Nicolas Buzeta,Felipe del Rio,Cristian Hinostroza,Denis Parra,Hans Lobel,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 视觉语言模型（VLMs）在纯文本任务上，尤其是在长上下文信息检索中，可以超越其基础的大型语言模型（LLMs）。通过控制合成检索任务研究发现，仅文本训练的转换器虽然在分布内准确率达到完美，但无法很好地泛化到分布外。而后续在相同任务的图像标记版本上的训练几乎使文本单独的OOD性能翻倍。机制解释揭示了视觉训练改变了模型内部的绑定策略：仅文本训练鼓励位置捷径，而基于图像的训练通过空间平移不变性破坏了这些捷径，促使模型采用更稳健的符号绑定机制。此外，还展示了不同训练方式、视觉编码器和初始化下绑定策略的变化，并指出类似的转变也发生在预训练LLM到VLM的过程中。研究表明跨模态训练即使对于单一模态的任务也能增强推理能力和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨视觉语言模型（VLMs）相较于其基础的大型语言模型（LLMs），在纯文本任务特别是长上下文信息检索方面的表现优势。通过构建一个受控的合成检索任务来调查这一现象背后的原因，尤其是理解为何视觉训练能够改善模型处理文本任务的能力。

Method: 研究人员首先创建了一个受控的合成检索任务，比较了只经过文本训练的转换器与进一步接受图像标记版本任务训练后的性能差异。接着，使用机制可解释性技术来分析视觉训练如何改变模型内部的绑定策略。此外，研究还考察了不同训练方法、视觉编码器类型以及初始设置下绑定策略的变化情况。

Result: 研究发现表明，单凭文本训练的模型尽管能在分布内达到极高的准确性，但在面对分布外数据时表现不佳；而增加了基于图像的数据训练后，模型在处理分布外文本任务时的表现显著提升。这是因为视觉训练促进了更加健壮且灵活的符号绑定机制形成。

Conclusion: 本研究揭示了跨模态训练不仅能够赋予模型处理多模态输入的能力，还能显著提高其在单一模态任务中的推理与泛化能力。这意味着将视觉元素纳入语言模型训练中可能是一种有效的方法，以增强模型对复杂文本任务的理解和处理能力。

Abstract: Vision Language Models (VLMs) are designed to extend Large Language Models (LLMs) with visual capabilities, yet in this work we observe a surprising phenomenon: VLMs can outperform their underlying LLMs on purely text-only tasks, particularly in long-context information retrieval. To investigate this effect, we build a controlled synthetic retrieval task and find that a transformer trained only on text achieves perfect in-distribution accuracy but fails to generalize out of distribution, while subsequent training on an image-tokenized version of the same task nearly doubles text-only OOD performance. Mechanistic interpretability reveals that visual training changes the model's internal binding strategy: text-only training encourages positional shortcuts, whereas image-based training disrupts them through spatial translation invariance, forcing the model to adopt a more robust symbolic binding mechanism that persists even after text-only examples are reintroduced. We further characterize how binding strategies vary across training regimes, visual encoders, and initializations, and show that analogous shifts occur during pretrained LLM-to-VLM transitions. Our findings suggest that cross-modal training can enhance reasoning and generalization even for tasks grounded in a single modality.

</details>


### [20] [Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge](https://arxiv.org/abs/2602.15184)
*Siying Ma,Mehrdad M. Zadeh,Mauricio Soroco,Wuyang Chen,Jiguo Cao,Vijay Ganesh*

Main category: cs.LG

TL;DR: 提出了一种多物理训练框架，通过结合基本形式的PDE和原始PDE来学习，以提高数据效率、减少预测误差，并增强神经算子在不同物理参数设置下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于神经算子（NOs）的方法主要关注从目标偏微分方程（PDEs）中学习模拟，但往往忽略了这些方程背后的基本物理原理。受到数值求解器能够兼容不同PDE设置下模拟的启发，旨在开发一种方法可以同时从基础形式的PDE及其完整形式中学习，以提高模型的数据效率、准确性和跨场景泛化能力。

Method: 提出了一种与架构无关的多物理训练框架，该框架能够从原始PDEs及它们简化后的基本形式共同学习，从而增强了数据利用效率、降低了预测错误率，并提高了对于物理参数变化以及合成到现实世界转换情况下的外推泛化性能。

Result: 通过广泛的实验表明，在1D/2D/3D PDE问题上，该方法相对于归一化均方根误差（nRMSE）有了一致性的改进；特别是在涉及物理参数变化和合成至真实迁移的情况下，明确地将基本物理学知识纳入考虑显著增强了神经算子的泛化能力。

Conclusion: 通过引入基础物理知识并采用多物理训练策略，可以有效提升神经算子在解决由偏微分方程描述的物理系统动态演化问题时的数据效率、准确性以及泛化能力。

Abstract: Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde.

</details>


### [21] [COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression](https://arxiv.org/abs/2602.15200)
*Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Ammar Ali,Baher Mohammad,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: COMPOT是一种无需训练的Transformer模型压缩框架，利用少量校准数据集估计稀疏权重分解。通过采用正交字典和一次性动态分配策略，COMPOT能够有效提升压缩质量，同时保持与后训练量化兼容，实现在多种架构和任务中优于现有低秩和稀疏方法的质量-压缩权衡。


<details>
  <summary>Details</summary>
Motivation: 当前基于截断奇异值分解（SVD）的Transformer模型后训练压缩方法在适度压缩时可能会降低准确性。尽管稀疏字典学习提供了一种更灵活的子空间表示方式，但现有方法往往需要迭代更新字典和系数。因此，开发一种更加高效且能维持高准确率的压缩技术成为必要。

Method: 提出了一种名为COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers) 的无训练压缩框架，该框架使用小规模校准数据集来估计稀疏权重因式分解。COMPOT采用了允许闭式Procrustes更新的正交字典以及用于系数的一次性稀疏编码，从而消除了迭代优化的需求。此外，为了处理全局压缩预算下的异构层敏感度问题，COMPOT引入了一种一次性动态分配策略，可以自适应地重新分配各层的压缩率。

Result: 广泛的实验表明，在不同的架构和任务上，COMPOT相较于强大的低秩和稀疏基线方法，始终能够提供更好的质量-压缩比，并且完全兼容后训练量化以实现极端压缩。

Conclusion: COMPOT展示出了一种有效的、非迭代的方法来压缩Transformer模型，同时保持了较高的性能表现。它不仅改进了压缩效率，还提高了压缩后的模型质量，尤其是在结合后训练量化的场景下。

Abstract: Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available $\href{https://github.com/mts-ai/COMPOT}{here}$.

</details>


### [22] [MAVRL: Learning Reward Functions from Multiple Feedback Types with Amortized Variational Inference](https://arxiv.org/abs/2602.15206)
*Raphaël Baur,Yannick Metz,Maria Gkoulta,Mennatallah El-Assady,Giorgia Ramponi,Thomas Kleine Buening*

Main category: cs.LG

TL;DR: 本文提出了一种从多种反馈类型（如演示、比较、评分和停止）中联合学习奖励函数的方法，通过贝叶斯推断共享潜在奖励函数，并引入可扩展的变分推理方法来训练模型。该方法在离散和连续控制基准测试中优于单类型基线，能够利用不同类型反馈之间的互补信息，并生成对环境扰动更鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 目前，如何从提供定性不同信号的异构反馈类型（例如演示、比较、评分和停止）中联合学习奖励函数尚不清楚。现有的方法通常依赖单一类型的反馈或手动加权损失项结合多种反馈类型，这导致了手动平衡损失的需求以及难以处理不同反馈类型提供的多样化信息的问题。

Method: 本文将来自多种反馈类型的奖励学习问题形式化为共享潜在奖励函数上的贝叶斯推断过程，其中每种反馈类型都通过一个明确的可能性贡献信息。此外，还介绍了一种可扩展的摊销变分推理方法，该方法学习了一个共享奖励编码器和针对特定反馈的可能性解码器，并通过优化单一证据下界来进行训练。

Result: 实验结果表明，在离散和连续控制基准上，联合推断出的奖励后验分布比单一类型的基线表现更好，可以跨反馈类型利用互补信息，并产生对环境变化更具鲁棒性的策略。同时，所推测的奖励不确定性进一步提供了可解释的信号，用于分析模型信心及跨反馈类型的一致性。

Conclusion: 本文提出的方法为从多种反馈类型中有效学习奖励函数开辟了新的途径，无需手动调整损失权重，也不需要将所有反馈减少到共同的中间表示。它不仅提高了学习效率，而且增强了学习策略对于环境变动的适应能力。

Abstract: Reward learning typically relies on a single feedback type or combines multiple feedback types using manually weighted loss terms. Currently, it remains unclear how to jointly learn reward functions from heterogeneous feedback types such as demonstrations, comparisons, ratings, and stops that provide qualitatively different signals. We address this challenge by formulating reward learning from multiple feedback types as Bayesian inference over a shared latent reward function, where each feedback type contributes information through an explicit likelihood. We introduce a scalable amortized variational inference approach that learns a shared reward encoder and feedback-specific likelihood decoders and is trained by optimizing a single evidence lower bound. Our approach avoids reducing feedback to a common intermediate representation and eliminates the need for manual loss balancing. Across discrete and continuous-control benchmarks, we show that jointly inferred reward posteriors outperform single-type baselines, exploit complementary information across feedback types, and yield policies that are more robust to environment perturbations. The inferred reward uncertainty further provides interpretable signals for analyzing model confidence and consistency across feedback types.

</details>


### [23] [ÜberWeb: Insights from Multilingual Curation for a 20-Trillion-Token Dataset](https://arxiv.org/abs/2602.15210)
*DatologyAI,:,Aldo Gael Carranza,Kaleigh Mentzer,Ricardo Pio Monti,Alex Fang,Alvin Deng,Amro Abbas,Anshuman Suri,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Diego Kiner,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 该论文探讨了多语言数据整理对于提高多语言模型质量的重要性。研究发现，通过改进单一语言的数据质量可以提高其他语言的表现，且每种语言的定制化整理能够产生显著的效果。在大规模通用训练混合中，精心策划的多语言分配即使只占总标记的不到8%，也非常有效。基于此方法创建了一个20T-token的预训练语料库，使得3B和8B参数的模型在1T-token随机子集上训练时，相比强大的公共基线使用4-10倍少的训练FLOPs就能达到竞争性的多语言准确性。此外，这种方法的好处扩展到了前沿模型规模：20T-token语料库作为Trinity Large (400B/A13B)预训练数据集的一部分，展示了相对于其训练FLOPs的强大多语言性能。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型需要具备处理多种语言的能力，但因不同语言间的数据可用性不均等，以及多语言联合训练可能带来的性能干扰（即“多语言诅咒”），高质量多语言模型的开发面临挑战。本研究旨在探索是否可以通过改善数据质量和组成来解决这些问题，而不是受到基本容量限制的影响。

Method: 研究者们对跨十三种语言的多语言数据进行了整理，并进行了控制实验以验证单个语言数据质量的提升如何影响其他语言的表现。他们还开发了一个完全来源于公开资源的20T-token预训练语料库，并基于此训练了具有3B和8B参数的不同规模模型，同时评估了这些模型在减少训练FLOPs情况下的多语言准确性表现。

Result: 研究表明，改善任何一种语言的数据质量都可以正面影响到其它语言的表现；特定于每种语言的数据整理工作带来了更为显著的语言内改进效果。使用不到总标记量8%的精选多语言配置，在大幅度减少所需计算资源的同时仍能保持高效。特别地，用所提出的方法训练出的大规模模型展现出了与计算成本相比极高的多语言性能。

Conclusion: 目标明确、针对每种语言进行的数据整理有助于缓解多语言之间的相互干扰问题，并支持以计算效率更高的方式扩展多语言能力。

Abstract: Multilinguality is a core capability for modern foundation models, yet training high-quality multilingual models remains challenging due to uneven data availability across languages. A further challenge is the performance interference that can arise from joint multilingual training, commonly referred to as the "curse of multilinguality". We study multilingual data curation across thirteen languages and find that many reported regressions are not inherent to multilingual scaling but instead stem from correctable deficiencies in data quality and composition rather than fundamental capacity limits. In controlled bilingual experiments, improving data quality for any single language benefits others: curating English improves non-English performance in 12 of 13 languages, while curating non-English yields reciprocal improvements in English. Bespoke per-language curation produces substantially larger within-language improvements. Extending these findings to large-scale general-purpose training mixtures, we show that curated multilingual allocations comprising under 8% of total tokens remain remarkably effective. We operationalize this approach within an effort that produced a 20T-token pretraining corpus derived entirely from public sources. Models with 3B and 8B parameters trained on a 1T-token random subset achieve competitive multilingual accuracy with 4-10x fewer training FLOPs than strong public baselines, establishing a new Pareto frontier in multilingual performance versus compute. Moreover, these benefits extend to frontier model scale: the 20T-token corpus served as part of the pretraining dataset for Trinity Large (400B/A13B), which exhibits strong multilingual performance relative to its training FLOPs. These results show that targeted, per-language data curation mitigates multilingual interference and enables compute-efficient multilingual scaling.

</details>


### [24] [Automatically Finding Reward Model Biases](https://arxiv.org/abs/2602.15222)
*Atticus Wang,Iván Arcuschin,Arthur Conmy*

Main category: cs.LG

TL;DR: 本文提出了一种使用大型语言模型自动发现奖励模型中偏见的简单方法，能够识别已知偏见并揭示新的偏见，比如Skywork-V2-8B偏好有冗余空格和幻觉内容的回答。此外，研究表明进化迭代优于平面最佳N搜索，并通过合成注入偏见验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 过去的工作表明，用于大型语言模型后训练的奖励模型可能会奖励一些虚假或不希望出现的属性，如长度、格式、幻觉内容以及谄媚行为。本文旨在研究如何自动在自然语言中找到这些奖励模型的偏见。

Method: 采用一种简单的方法，利用大型语言模型来迭代地提出和完善候选偏见。

Result: 该方法不仅能够恢复已知的偏见，还能揭示新的偏见。例如，发现Skywork-V2-8B这一领先的开放权重奖励模型经常错误地偏向于含有冗余空格和幻觉内容的回答。同时，展示了进化迭代比直接的最佳N搜索表现更优，并通过合成注入偏见验证了流程的召回率。

Conclusion: 本研究为通过自动化可解释性方法改进奖励模型提供了贡献。

Abstract: Reward models are central to large language model (LLM) post-training. However, past work has shown that they can reward spurious or undesirable attributes such as length, format, hallucinations, and sycophancy. In this work, we introduce and study the research problem of automatically finding reward model biases in natural language. We offer a simple approach of using an LLM to iteratively propose and refine candidate biases. Our method can recover known biases and surface novel ones: for example, we found that Skywork-V2-8B, a leading open-weight reward model, often mistakenly favors responses with redundant spacing and responses with hallucinated content. In addition, we show evidence that evolutionary iteration outperforms flat best-of-N search, and we validate the recall of our pipeline using synthetically injected biases. We hope our work contributes to further research on improving RMs through automated interpretability methods.

</details>


### [25] [BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening](https://arxiv.org/abs/2602.15236)
*Anjie Qiao,Zhen Wang,Yaliang Li,Jiahua Rao,Yuedong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的虚拟筛选框架BindCLIP，通过结合对比学习和生成性学习来提高配体与靶点口袋之间相互作用的敏感度，并减少对训练数据中快捷关联的依赖。实验结果表明该方法在分布外虚拟筛选和配体类似物排名上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CLIP风格的模型虽然能够实现大规模虚拟筛选，但它们对于精细的结合相互作用不够敏感，且可能依赖于训练数据中的捷径相关性，这限制了它们根据真实结合兼容性对配体进行排序的能力。

Method: 提出了BindCLIP，一个统一的对比-生成表示学习框架，用于虚拟筛选。它同时使用CLIP风格的对比学习以及基于口袋条件下的扩散目标来进行结合姿态生成，以确保姿态级别的监督直接塑造检索嵌入空间朝向交互相关的特征。此外，还引入了硬负样本增强和配体-配体锚定正则化器，以防止表示坍缩。

Result: 在两个公开基准测试上的实验显示，相比强大的基线模型，BindCLIP表现出一致性的改进。特别是在具有挑战性的分布外虚拟筛选任务及FEP+基准测试中的配体类似物排名上取得了显著进步。

Conclusion: 结合生成性、姿态级监督与对比学习可以产生更关注于相互作用的嵌入，并改善实际筛选设置下的泛化能力，使虚拟筛选更加接近实际应用。

Abstract: Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.

</details>


### [26] [Closing the Distribution Gap in Adversarial Training for LLMs](https://arxiv.org/abs/2602.15238)
*Chengzhi Hu,Jonas Dornbusch,David Lüdke,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了Distributional Adversarial Training (DAT)方法，利用Diffusion LLMs来近似提示和响应的真实联合分布，生成多样且高概率的样本，从而有效提高对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对抗训练算法虽然在提高模型对抗鲁棒性方面取得了一定进展，但模型仍然容易受到简单的分布内攻击，如将提示重写为过去时态或将它们翻译成其他语言。这表明现有方法未能充分覆盖数据分布，导致对看似简单的攻击仍然脆弱。

Method: 通过结合扩散模型提供的数据分布优化与持续对抗训练，提出了一种新的方法——分布对抗训练（DAT）。该方法利用Diffusion LLMs来估计提示和响应之间的实际联合分布，使得能够生成更加多样化且具有较高可能性的样本集，进而解决泛化失败问题。

Result: 实验结果显示，与先前的方法相比，DAT方法在提升对抗鲁棒性方面取得了显著更高的成效。

Conclusion: Distributional Adversarial Training (DAT) 提供了一条克服当前对抗训练局限性的路径，通过更全面地探索数据分布，增强了大型语言模型对于各种类型攻击的抵抗力。

Abstract: Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.

</details>


### [27] [Fast and Effective On-policy Distillation from Reasoning Prefixes](https://arxiv.org/abs/2602.15260)
*Dongxu Zhang,Zhichao Yang,Sepehr Janghorbani,Jun Han,Andrew Ressler,Qian Qian,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.LG

TL;DR: 提出了一种改进的在线策略蒸馏方法，即仅对学生产出的前缀应用蒸馏目标并在蒸馏过程中提前终止采样，这种方法在保持与完整在线策略蒸馏相同性能的同时大幅减少了训练FLOP。


<details>
  <summary>Details</summary>
Motivation: 观察到在线策略蒸馏(OPD)期间训练信号往往集中在每个输出的前缀部分，并且即使是一个短的教师生成前缀也能显著帮助学生给出正确答案。基于这些观察结果，研究者旨在通过修改OPD来减少训练成本，特别是对于长响应来说。

Method: 提出的方法是仅对学生生成输出的前缀应用蒸馏目标，并且在蒸馏过程中提前终止每个采样过程。

Result: 实验表明，在一系列AI-for-Math和域外基准测试中，所提出的在线策略前缀蒸馏法与完整的OPD表现相当，同时将训练FLOP减少了2倍至47倍。

Conclusion: 该研究表明，通过对在线策略蒸馏进行简单的调整——专注于前缀并提前终止采样，可以在不牺牲模型性能的情况下显著降低训练成本。

Abstract: On-policy distillation (OPD), which samples trajectories from the student model and supervises them with a teacher at the token level, avoids relying solely on verifiable terminal rewards and can yield better generalization than off-policy distillation. However, OPD requires expensive on-the-fly sampling of the student policy during training, which substantially increases training cost, especially for long responses. Our initial analysis shows that, during OPD, training signals are often concentrated in the prefix of each output, and that even a short teacher-generated prefix can significantly help the student produce the correct answer. Motivated by these observations, we propose a simple yet effective modification of OPD: we apply the distillation objective only to prefixes of student-generated outputs and terminate each sampling early during distillation. Experiments on a suite of AI-for-Math and out-of-domain benchmarks show that on-policy prefix distillation matches the performance of full OPD while reducing training FLOP by 2x-47x.

</details>


### [28] [The Information Geometry of Softmax: Probing and Steering](https://arxiv.org/abs/2602.15293)
*Kiho Park,Todd Nief,Yo Joong Choe,Victor Veitch*

Main category: cs.LG

TL;DR: 本文探讨了AI系统如何将语义结构编码到其表示空间的几何结构中，特别关注定义softmax分布的表示，并提出了一种名为'对偶转向'的方法来稳健地操纵概念。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解AI模型如何利用表示来产生行为，并认为表示空间的自然几何形状应当反映这种使用方式。对于定义softmax分布的表示而言，信息几何被认为是自然几何。

Method: 提出了'对偶转向'方法，旨在通过线性探测器稳健地调整表示以展示特定概念。

Result: 证明了对偶转向能够最优地修改目标概念，同时最小化对非目标概念的影响；实验表明该方法增强了概念操作的可控性和稳定性。

Conclusion: 通过对信息几何在语义编码中的作用以及线性表示假设的研究，发现'对偶转向'是一种有效提高概念操控性能的技术。

Abstract: This paper concerns the question of how AI systems encode semantic structure into the geometric structure of their representation spaces. The motivating observation of this paper is that the natural geometry of these representation spaces should reflect the way models use representations to produce behavior. We focus on the important special case of representations that define softmax distributions. In this case, we argue that the natural geometry is information geometry. Our focus is on the role of information geometry on semantic encoding and the linear representation hypothesis. As an illustrative application, we develop "dual steering", a method for robustly steering representations to exhibit a particular concept using linear probes. We prove that dual steering optimally modifies the target concept while minimizing changes to off-target concepts. Empirically, we find that dual steering enhances the controllability and stability of concept manipulation.

</details>


### [29] [Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization](https://arxiv.org/abs/2602.15304)
*Farzana Akter,Rakib Hossain,Deb Kanna Roy Toushi,Mahmood Menon Khan,Sultana Amin,Lisan Al Amin*

Main category: cs.LG

TL;DR: 该论文提出了一种结合联邦学习（FL）和分割学习（SL）的混合隐私保护框架，用于支持决策导向的医疗保健建模，而无需共享原始数据。通过在客户端上保留特征提取部分并在协调服务器上托管预测头部，实现了共享表示学习，并允许在明确的合作边界处应用隐私控制。研究还通过成员推断审计了切层表示的泄露情况，并探讨了基于激活裁剪和加性高斯噪声的轻量级防御措施。实验结果表明，这种混合FL-SL方法能够在保持竞争力的同时提供可调节的隐私-实用性权衡。


<details>
  <summary>Details</summary>
Motivation: 临床协作决策支持常常受到治理和隐私规则限制，这些规则阻止跨机构汇集患者级别的记录。为了克服这一挑战并促进决策导向型医疗服务的发展，同时确保个人隐私得到保护，提出了本解决方案。

Method: 采用一种结合联邦学习与分割学习的新型架构，在不直接交换原始数据的前提下实现模型训练。此外，还引入了针对激活值裁剪及添加高斯噪声的方法来减轻潜在隐私泄露风险，并通过成员推断攻击测试了不同设置下的隐私保护效果。

Result: 实验评估显示，相较于单独使用FL或SL，所提出的混合FL-SL方案不仅能在多个公开临床数据集上取得相似甚至更好的预测性能，还能有效减少经审计确认的信息泄露水平，且提供了灵活调整隐私与实用性之间平衡的能力。

Conclusion: 研究表明，混合FL-SL为隐私保护型医疗决策支持提供了一个实用的设计空间，它能够显式地平衡实用性、泄露风险和部署成本。

Abstract: Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.

</details>


### [30] [On Surprising Effectiveness of Masking Updates in Adaptive Optimizers](https://arxiv.org/abs/2602.15322)
*Taejong Joo,Wenhan Xia,Cheolmin Kim,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: 研究提出了一种名为Momentum-aligned gradient masking (Magma)的新方法，通过随机遮罩参数更新来优化大型语言模型的训练过程。这种方法在不增加计算负担的情况下，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在挑战当前主要依赖于具有复杂预处理器的密集自适应优化器来训练大型语言模型的做法，通过引入基于随机遮罩参数更新的方法，并发现这种做法可以诱导出一种依赖于曲率的几何正则化效果，从而平滑优化轨迹。

Method: 研究人员首先展示了随机遮罩参数更新的有效性，然后基于这一观察提出了Momentum-aligned gradient masking (Magma)，该方法利用动量-梯度对齐来调整被遮罩的更新。

Result: 实验结果表明，在大规模语言模型预训练中，Magma不仅能够作为现有自适应优化器的一个简单替代方案，而且能够在几乎不增加额外计算成本的同时提供一致性的性能提升。特别是对于10亿参数规模的模型，相比Adam和Muon，Magma分别降低了超过19%和9%的困惑度。

Conclusion: 本研究表明，通过引入随机遮罩技术及基于此开发的Magma算法，可以在保持较低计算开销的前提下有效提高大型语言模型的训练效率与最终模型质量。

Abstract: Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\% and 9\% compared to Adam and Muon, respectively.

</details>


### [31] [Prescriptive Scaling Reveals the Evolution of Language Model Capabilities](https://arxiv.org/abs/2602.15327)
*Hanlin Zhang,Jikai Jin,Vasilis Syrgkanis,Sham Kakade*

Main category: cs.LG

TL;DR: 该论文通过大规模观察性评估和新的采样数据分析了预训练计算预算与下游任务准确率之间的关系，提出了一个基于平滑分位数回归的方法来估计基准分数的高条件分位数，并验证了其时间可靠性。此外，还分析了特定任务下的饱和度及数学推理任务上的污染相关变化，并提出了一种高效算法以较低的评估成本恢复接近完整的数据前沿。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的部署需求增加，实践者越来越需要具体的扩展法则：给定预训练计算预算，在当前后训练实践下可以达到什么样的下游准确率？这种映射在领域发展过程中有多稳定？

Method: 使用大规模观测评价（5k观测数据）和新采样的数据（2k），通过带有单调、饱和Sigmoid参数化的平滑分位数回归方法估计能力边界。进一步地，针对任务依赖性的饱和进行了分析，并探讨了数学推理任务中污染相关的偏移问题。

Result: 估计的能力边界对于多数任务来说是稳定的，除了数学推理表现出随时间持续进步的趋势。此外，还引入了一种有效算法，能够以大约20%的评估预算恢复接近完整数据前沿的结果。

Conclusion: 本研究发布了最新的模型性能评估数据集Proteus 2k，并介绍了一种实用的方法论，将计算预算转化为可靠的性能预期，并监测随时间变化的能力边界移动情况。

Abstract: For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.

</details>


### [32] [A Scalable Curiosity-Driven Game-Theoretic Framework for Long-Tail Multi-Label Learning in Data Mining](https://arxiv.org/abs/2602.15330)
*Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为CD-GTMLL的新方法，该方法通过将长尾多标签分类问题转化为多人游戏来解决大规模多标签分类中长尾分布的问题。每个子预测器专注于标签空间的一个分区，并基于尾部标签的稀有性和玩家之间的不一致追求内在的好奇奖励，从而自适应地为未充分代表的尾部标签注入学习信号。实验结果表明，该方法在基准测试上优于现有最先进方法，特别是在极端多标签分类数据集上。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的数据挖掘应用中，长尾分布对大规模多标签分类（MLC）构成了持久挑战。现有的重采样和重加权策略往往破坏了标签间的依赖关系或需要脆弱的超参数调整，尤其是在标签空间扩展到数万个标签时。为了解决这个问题，作者提出了Curiosity-Driven Game-Theoretic Multi-Label Learning (CD-GTMLL)框架。

Method: CD-GTMLL是一种可扩展的合作框架，它把长尾多标签分类问题重构为一个多玩家游戏。在这个游戏中，每个子预测器（“玩家”）专门处理标签空间的一个分块，并通过追求基于尾部标签稀缺性和玩家间分歧的内在好奇心奖励来合作最大化全局准确性。这种机制能够自适应地向代表性不足的尾部标签注入学习信号，而无需手动平衡或调参。

Result: 广泛的实验跨越7个基准测试，包括具有超过30,000个标签的极端多标签分类数据集，证明了CD-GTMLL始终超越最先进的方法，在Wiki10-31K上的P@3指标提高了高达1.6%。消融研究表明，博弈论合作与好奇心驱动探索两者都对稳健的尾部性能有所贡献。

Conclusion: 通过整合博弈论与好奇心机制，CD-GTMLL不仅提高了资源受限环境下的模型效率，还为电子商务和医疗保健等行业中的不平衡数据场景提供了更加自适应的学习路径。

Abstract: The long-tail distribution, where a few head labels dominate while rare tail labels abound, poses a persistent challenge for large-scale Multi-Label Classification (MLC) in real-world data mining applications. Existing resampling and reweighting strategies often disrupt inter-label dependencies or require brittle hyperparameter tuning, especially as the label space expands to tens of thousands of labels. To address this issue, we propose Curiosity-Driven Game-Theoretic Multi-Label Learning (CD-GTMLL), a scalable cooperative framework that recasts long-tail MLC as a multi-player game - each sub-predictor ("player") specializes in a partition of the label space, collaborating to maximize global accuracy while pursuing intrinsic curiosity rewards based on tail label rarity and inter-player disagreement. This mechanism adaptively injects learning signals into under-represented tail labels without manual balancing or tuning. We further provide a theoretical analysis showing that our CD-GTMLL converges to a tail-aware equilibrium and formally links the optimization dynamics to improvements in the Rare-F1 metric. Extensive experiments across 7 benchmarks, including extreme multi-label classification datasets with 30,000+ labels, demonstrate that CD-GTMLL consistently surpasses state-of-the-art methods, with gains up to +1.6% P@3 on Wiki10-31K. Ablation studies further confirm the contributions of both game-theoretic cooperation and curiosity-driven exploration to robust tail performance. By integrating game theory with curiosity mechanisms, CD-GTMLL not only enhances model efficiency in resource-constrained environments but also paves the way for more adaptive learning in imbalanced data scenarios across industries like e-commerce and healthcare.

</details>


### [33] [Directional Reasoning Trajectory Change (DRTC): Identifying Critical Trace Segments in Reasoning Models](https://arxiv.org/abs/2602.15332)
*Waldemar Chang*

Main category: cs.LG

TL;DR: 本文提出了一种名为DRTC的新框架，用于解释语言模型在长篇推理中的决策过程。该方法通过检测关键决策点并应用接收端干预来测量每个干预对模型逻辑概率轨迹方向的影响，并计算原始logits上的曲率变化作为补充诊断。实验证明，DRTC能够有效揭示特定上下文元素如何在策略动态下指导推理过程。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何执行长期推理仍然是一个未解决的挑战。现有可解释性方法通常只强调与答案相关的标记或片段，但很少揭示模型在哪一步进行了关键性的推理转折、哪些早期内容触发了这些转折以及高亮显示的文字是否真的引导了推理过程。

Method: 提出了方向推理轨迹变化（DRTC），这是一种针对单个在线策略展开的长篇推理的过程因果框架。DRTC首先利用不确定性和分布转移信号检测到关键决策点，然后实施接收方干预措施，在不重新采样后续部分的情况下保持实际展开状态，同时仅在转折点处阻止选定较早片段的信息流。它衡量每次干预是否改变了模型相对于实际展开方向的逻辑概率轨迹的方向，从而产生每块签名的归因分数。此外，还计算了原始logits上的转向角曲率变化作为补充诊断，并引入了曲率特征来总结共享干预响应几何。

Result: 经验上，方向性影响在四个推理模型中高度集中（每例|DRTC|份额导致Gini系数0.50至0.58，前5%质量占比0.23至0.28），且学习到的关键点比匹配随机跨度引发更强的干预幅度。在一个涉及500道数学题目的规模研究中，使用R1-Distill-Qwen-1.5B时，学习到的跨度优于匹配随机跨度（中位数差异=0.409, 500中有355为正；符号检验p=2.3e-21）。

Conclusion: 总的来说，DRTC提供了一种基于因果关系的、从轨迹层面出发的观点，说明了特定上下文元素是如何在在线策略动态下指导推理过程的。

Abstract: Understanding how language models carry out long-horizon reasoning remains an open challenge. Existing interpretability methods often highlight tokens or spans correlated with an answer, but they rarely reveal where the model makes consequential reasoning turns, which earlier context causally triggers those turns, or whether the highlighted text actually steers the reasoning process. We introduce Directional Reasoning Trajectory Change (DRTC), a process-causal framework for interpreting long-form reasoning from a single on-policy rollout. DRTC detects pivot decision points using uncertainty and distribution-shift signals, then applies receiver-side interventions that preserve the realized rollout without resampling the continuation while blocking information flow from selected earlier chunks only at a pivot. It measures whether each intervention redirects the direction of the model's log-probability trajectory relative to the realized rollout direction, producing a signed per-chunk attribution score. We also compute turning-angle curvature changes on raw logits as a complementary diagnostic and introduce curvature signatures to summarize shared intervention-response geometry. Empirically, directional influence is sharply concentrated across four reasoning models (per-example |DRTC| shares yield Gini 0.50 to 0.58 and top-5 percent mass 0.23 to 0.28), and learned pivots induce stronger intervention magnitudes than matched random spans. In a scaling study on 500 MATH problems with R1-Distill-Qwen-1.5B, learned spans outperform matched random spans (median delta = 0.409, 355 of 500 positive; sign test p = 2.3e-21). Overall, DRTC provides a causally grounded, trajectory-level view of how specific context elements steer reasoning under on-policy dynamics.

</details>


### [34] [FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning](https://arxiv.org/abs/2602.15337)
*Chaoyi Lu*

Main category: cs.LG

TL;DR: 本文提出了一种基于参数敏感性的异步联邦学习框架FedPSA，通过更细粒度地衡量模型过时性并动态调整对过期信息的容忍度来提高性能。实验显示，与基线方法相比，FedPSA最高可提升6.37%的表现，并且比当前最先进的方法高出1.93%。


<details>
  <summary>Details</summary>
Motivation: 现有的异步联邦学习方法通常仅依赖于当前模型和全局模型之间的轮次差异来衡量陈旧性，这种方式较为粗略，未能充分观察模型本身的状态，因此限制了异步方法的性能上限。

Method: 提出了FedPSA（基于参数敏感性的异步联邦学习），该方法利用参数敏感性来衡量模型的老化程度，并设置了一个动态动量队列来实时评估当前训练阶段，从而能够动态调整对于过时信息的接受度。

Result: 在多个数据集上的广泛实验以及与各种方法比较表明，FedPSA表现出色，相较于基线方法最多提高了6.37%，并且相对于当前最先进的方法也有1.93%的优势。

Conclusion: FedPSA通过引入更细致的陈旧性度量方式及动态调整机制，在不牺牲过多准确率的情况下加速了训练过程，为解决异步联邦学习中的挑战提供了一种新思路。

Abstract: Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\% improvement over baseline methods and 1.93\% over the current state-of-the-art method.

</details>


### [35] [Discovering Implicit Large Language Model Alignment Objectives](https://arxiv.org/abs/2602.15338)
*Edward Chen,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Obj-Disco的新框架，该框架能够将对齐奖励信号自动分解为一组简洁且带权重的人类可理解的自然语言目标组合。通过迭代贪婪算法分析训练检查点之间的行为变化，识别并验证最佳解释剩余奖励信号的候选目标。实验表明，该框架在各种任务、模型大小和对齐算法中表现出色，并能揭示潜在的不一致激励。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）对齐依赖于复杂的奖励信号，这些信号往往掩盖了被激励的具体行为，导致错位风险和奖励操纵问题。现有方法要么依赖预定义规则而可能忽略未预见的问题，要么无法全面覆盖或确定与模型行为直接相关的具体目标。

Method: 研究者们开发了一个名为Obj-Disco的框架，使用迭代贪婪算法来分析不同训练检查点间的行为变化，从而识别出最能解释剩余奖励信号的人类可读的目标集合。

Result: 跨多种任务、模型规模及对齐算法的广泛评估显示，该框架能够捕捉超过90%的奖励行为；此外，人类评价也支持这一发现。案例研究表明，Obj-Disco还能成功识别伴随预期行为出现的隐性不一致激励。

Conclusion: 这项工作为揭示LLM对齐中的隐含目标提供了重要工具，促进了更透明、更安全的人工智能发展之路。

Abstract: Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of "unknown unknowns", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.

</details>


### [36] [ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models](https://arxiv.org/abs/2602.15344)
*Mitchell Piehl,Zhaohan Xi,Zuobin Xiong,Pan He,Muchao Ye*

Main category: cs.LG

TL;DR: 本文首次系统地研究了针对长期记忆增强型大型语言模型的基于相似度检索机制的黑盒对抗性内存注入攻击，提出了一种统一框架ER-MIA，并通过实验证明了这种攻击方式在不同模型和记忆系统中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地配备长期记忆系统以克服有限上下文窗口并实现跨交互持久推理的同时，也增加了模型被攻击的风险。本文旨在探索这些模型中基于相似度检索机制的安全漏洞，并提出具体的攻击方法来评估相关风险。

Method: 提出了ER-MIA框架，该框架包括一系列可组合攻击原语及集成攻击方法，适用于内容导向攻击与问题导向攻击两种实际攻击场景。

Result: 实验表明，基于相似性的检索构成了一个基本且系统级别的漏洞，在不同的记忆设计和应用场景中都存在安全风险。ER-MIA能够在最小化攻击者假设条件下达到高成功率。

Conclusion: 本研究表明，对于增强有长期记忆系统的大型语言模型来说，基于相似度检索机制存在着显著的安全隐患，这要求我们在设计和使用这类系统时必须更加重视安全性考量。

Abstract: Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attacker assumptions. Extensive experiments across multiple LLMs and long-term memory systems demonstrate that similarity-based retrieval constitutes a fundamental and system-level vulnerability, revealing security risks that persist across memory designs and application scenarios.

</details>


### [37] [CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies](https://arxiv.org/abs/2602.15367)
*Sibo Zhang,Rui Jing,Liangfu Lv,Jian Zhang,Yunliang Zang*

Main category: cs.LG

TL;DR: 受小脑结构原理启发，提出了一种生物学基础的强化学习架构，该架构结合了大规模扩展、稀疏连接、稀疏激活和树突级调制。实验表明，与传统设计相比，这种小脑架构和树突调制在噪声高维强化学习基准上一致提高了样本效率、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）尽管在高维度序列决策任务中取得了显著表现，但仍然受限于较低的样本效率、对噪声敏感以及在部分可观察性下的弱泛化能力。当前大多数方法主要通过优化策略来解决这些问题，而较少探索架构先验对于表征学习及决策动态的影响。

Method: 提出了一个基于小脑结构原理的生物基础RL架构，此架构融合了大规模扩展、稀疏连接、稀疏激活以及树突级别的调制机制。

Result: 实验结果表明，在噪声高维度RL基准测试中，所提的小脑架构和树突调制相比于传统设计，在样本效率、鲁棒性及泛化能力方面均有持续改进。此外，架构参数的敏感性分析指出，受到小脑启发的结构可以在限定模型参数的情况下为RL提供最佳性能。

Conclusion: 本研究强调了小脑结构先验作为RL的有效归纳偏置的价值。

Abstract: Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.

</details>


### [38] [Fractional-Order Federated Learning](https://arxiv.org/abs/2602.15380)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为FOFedAvg的新方法，该方法通过引入分数阶随机梯度下降来改进联邦学习中的通信效率和收敛速度问题。实验结果表明，在多种非独立同分布的数据集上，FOFedAvg在测试性能和收敛速度方面优于或与基准算法相当。


<details>
  <summary>Details</summary>
Motivation: 联邦学习尽管具有隐私保护的优点，但也存在收敛慢、通信成本高以及数据非独立同分布等问题。为了解决这些问题，研究者们提出了FOFedAvg方法，旨在提高通信效率、加速收敛并减少由异构的非IID客户端数据引起的不稳定性。

Method: 本研究提出的方法是Fractional-Order Federated Averaging (FOFedAvg)，它基于Fractional-Order Stochastic Gradient Descent (FOSGD)技术，能够捕捉长时间范围内的关系及更深层次的历史信息。通过使用记忆感知的分数阶更新策略，FOFedAvg能够在保持良好性能的同时减少通信需求，并且对于不同类型的非IID数据都有很好的适应性。

Result: 实验部分，FOFedAvg被应用于多个标准数据集如MNIST, CIFAR-10等，并与一系列已有的联邦优化算法进行了对比。结果显示，在不同的非IID划分方案下，FOFedAvg通常能提供更好的测试表现和更快的收敛速度。此外，理论分析证明了当分数阶α满足0<α≤1时，FOFedAvg可以收敛到一个稳定点。

Conclusion: 研究表明，通过采用分数阶的记忆感知更新机制，FOFedAvg能够显著提升联邦学习在处理异构数据时的鲁棒性和有效性，为分布式训练提供了实用路径。

Abstract: Federated learning (FL) allows remote clients to train a global model collaboratively while protecting client privacy. Despite its privacy-preserving benefits, FL has significant drawbacks, including slow convergence, high communication cost, and non-independent-and-identically-distributed (non-IID) data. In this work, we present a novel FedAvg variation called Fractional-Order Federated Averaging (FOFedAvg), which incorporates Fractional-Order Stochastic Gradient Descent (FOSGD) to capture long-range relationships and deeper historical information. By introducing memory-aware fractional-order updates, FOFedAvg improves communication efficiency and accelerates convergence while mitigating instability caused by heterogeneous, non-IID client data. We compare FOFedAvg against a broad set of established federated optimization algorithms on benchmark datasets including MNIST, FEMNIST, CIFAR-10, CIFAR-100, EMNIST, the Cleveland heart disease dataset, Sent140, PneumoniaMNIST, and Edge-IIoTset. Across a range of non-IID partitioning schemes, FOFedAvg is competitive with, and often outperforms, these baselines in terms of test performance and convergence speed. On the theoretical side, we prove that FOFedAvg converges to a stationary point under standard smoothness and bounded-variance assumptions for fractional order $0<α\le 1$. Together, these results show that fractional-order, memory-aware updates can substantially improve the robustness and effectiveness of federated learning, offering a practical path toward distributed training on heterogeneous data.

</details>


### [39] [Joint Enhancement and Classification using Coupled Diffusion Models of Signals and Logits](https://arxiv.org/abs/2602.15405)
*Gilad Nurko,Roi Benita,Yehoshua Dissen,Tomohiro Nakatani,Marc Delcroix,Shoko Araki,Joseph Keshet*

Main category: cs.LG

TL;DR: 本文提出了一种将信号增强与分类器输出结合的一体化框架，通过两个相互作用的扩散模型来提高噪声环境下的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的处理方法将信号增强和分类视为两个独立且顺序进行的阶段，这样无法在降噪过程中利用分类器输出的语义信息。为了解决这个问题，作者提出了一种新的框架，旨在通过整合这两个过程来提高噪声条件下的分类准确度。

Method: 研究者开发了一个通用、领域无关的框架，该框架包含两个交互式的扩散模型：一个处理输入信号，另一个则基于分类器输出的logits。此联合形式允许双向指导，其中增强信号可以细化类别估计，同时不断变化的类别logits也能够引导信号重构朝向更具区分性的流形区域。此外，他们还提出了三种策略来有效建模输入与logit之间的联合分布。

Result: 实验结果表明，所提出的联合增强方法在图像分类和自动语音识别任务上均优于传统的序列增强基线，在各种噪声条件下实现了稳健且灵活的分类精度提升。

Conclusion: 通过将信号增强与分类器输出紧密结合，本研究所提出的框架不仅提高了噪声环境下的分类性能，而且无需对现有分类器进行再训练或微调，展现出良好的灵活性和实用性。

Abstract: Robust classification in noisy environments remains a fundamental challenge in machine learning. Standard approaches typically treat signal enhancement and classification as separate, sequential stages: first enhancing the signal and then applying a classifier. This approach fails to leverage the semantic information in the classifier's output during denoising. In this work, we propose a general, domain-agnostic framework that integrates two interacting diffusion models: one operating on the input signal and the other on the classifier's output logits, without requiring any retraining or fine-tuning of the classifier. This coupled formulation enables mutual guidance, where the enhancing signal refines the class estimation and, conversely, the evolving class logits guide the signal reconstruction towards discriminative regions of the manifold. We introduce three strategies to effectively model the joint distribution of the input and the logit. We evaluated our joint enhancement method for image classification and automatic speech recognition. The proposed framework surpasses traditional sequential enhancement baselines, delivering robust and flexible improvements in classification accuracy under diverse noise conditions.

</details>


### [40] [Fairness over Equality: Correcting Social Incentives in Asymmetric Sequential Social Dilemmas](https://arxiv.org/abs/2602.15407)
*Alper Demir,Hüseyin Aydın,Kale-ab Abebe Tessera,David Abel,Stefano V. Albrecht*

Main category: cs.LG

TL;DR: 本文研究了在不对称条件下，现有基于公平性的方法难以适应的问题，并提出了一种新的方法来促进更快的合作政策的形成。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习中的合作问题解决方案大多假设智能体面临相同的激励，并需要持续获取其他智能体的全局信息以评估公平性。但在实际中，智能体间的自然差异会影响合作动态。

Method: 提出了三种改进措施：重新定义考虑智能体奖励范围的公平性、引入基于智能体的加权机制以更好地处理固有不对称性、以及本地化社会反馈以在部分可观测情况下无需共享全局信息即可有效工作。

Result: 实验结果显示，在不对称场景下，相比现有方法，所提方法能促进更快的合作策略出现，同时不牺牲可扩展性或实用性。

Conclusion: 该研究表明，通过调整公平性概念和引入更灵活的机制来处理智能体间的不对称性，可以有效地促进多智能体系统中的合作行为。

Abstract: Sequential Social Dilemmas (SSDs) provide a key framework for studying how cooperation emerges when individual incentives conflict with collective welfare. In Multi-Agent Reinforcement Learning, these problems are often addressed by incorporating intrinsic drives that encourage prosocial or fair behavior. However, most existing methods assume that agents face identical incentives in the dilemma and require continuous access to global information about other agents to assess fairness. In this work, we introduce asymmetric variants of well-known SSD environments and examine how natural differences between agents influence cooperation dynamics. Our findings reveal that existing fairness-based methods struggle to adapt under asymmetric conditions by enforcing raw equality that wrongfully incentivize defection. To address this, we propose three modifications: (i) redefining fairness by accounting for agents' reward ranges, (ii) introducing an agent-based weighting mechanism to better handle inherent asymmetries, and (iii) localizing social feedback to make the methods effective under partial observability without requiring global information sharing. Experimental results show that in asymmetric scenarios, our method fosters faster emergence of cooperative policies compared to existing approaches, without sacrificing scalability or practicality.

</details>


### [41] [Logit Distance Bounds Representational Similarity](https://arxiv.org/abs/2602.15438)
*Beatrix M. B. Nielsen,Emanuele Marconato,Luigi Gresele,Andrea Dittadi,Simon Buchholz*

Main category: cs.LG

TL;DR: 该论文探讨了当两个模型的条件分布接近而非完全相同时，它们的内部表示是否也以某种线性方式相似。基于logit差异定义了一种新的度量方法，并证明了在该度量下相近意味着线性相似性。此外，实验表明基于KL散度的知识蒸馏可能无法保留教师模型的线性可恢复概念，而基于logit距离的知识蒸馏则表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究者想要探索当两个判别模型（如自回归语言模型）生成的条件分布不是完全相同而是接近时，它们的内部表示是否也以某种近似线性的方式相似。此动机源于对Nielsen等人(2025)观察结果的扩展，即KL散度上的接近并不必然意味着高线性表示相似性。

Method: 研究人员定义了一个基于模型可识别类别的表示差异度量，并证明了这种差异可以通过logit距离来界定。他们还展示了，在模型概率远离零的情况下，KL散度可以作为logit距离的一个上界；然而，实际应用中这样的界限往往缺乏实用性。通过合成数据集和图像数据集上的知识蒸馏实验对比了基于logit距离与基于KL散度的方法。

Result: 结果显示，基于logit距离的知识蒸馏能够产生具有更高线性表示相似性的学生模型，并且更好地保持了教师模型中线性可恢复的人类可解释概念。

Conclusion: 对于给定的一系列判别模型而言，即使两模型间条件分布仅是接近而非完全相同，它们之间仍可能存在较强的线性表示相似性，特别是当使用基于logit距离而非传统KL散度来衡量时。

Abstract: For a broad family of discriminative models that includes autoregressive language models, identifiability results imply that if two models induce the same conditional distributions, then their internal representations agree up to an invertible linear transformation. We ask whether an analogous conclusion holds approximately when the distributions are close instead of equal. Building on the observation of Nielsen et al. (2025) that closeness in KL divergence need not imply high linear representational similarity, we study a distributional distance based on logit differences and show that closeness in this distance does yield linear similarity guarantees. Specifically, we define a representational dissimilarity measure based on the models' identifiability class and prove that it is bounded by the logit distance. We further show that, when model probabilities are bounded away from zero, KL divergence upper-bounds logit distance; yet the resulting bound fails to provide nontrivial control in practice. As a consequence, KL-based distillation can match a teacher's predictions while failing to preserve linear representational properties, such as linear-probe recoverability of human-interpretable concepts. In distillation experiments on synthetic and image datasets, logit-distance distillation yields students with higher linear representational similarity and better preservation of the teacher's linearly recoverable concepts.

</details>


### [42] [On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks](https://arxiv.org/abs/2602.15460)
*Yannic Neuhaus,Nicolas Flammarion,Matthias Hein,Francesco Croce*

Main category: cs.LG

TL;DR: 本文提出了一种评估框架，用于严格检验chain-of-thought（CoT）方法在简单规划任务上的泛化能力。实验结果表明，尽管CoT推理提高了所有表示形式下的分布内泛化能力，但在大多数情况下，当控制与分布内数据的平凡匹配时，分布外泛化（例如到更大的地图）仍然非常有限。令人惊讶地发现，结合多种文本格式的推理轨迹产生了最佳（且非平凡）的分布外泛化表现。纯基于文本的模型始终优于使用基于图像输入的模型，包括最近提出的一种依赖于潜在空间推理的方法。


<details>
  <summary>Details</summary>
Motivation: 整合推理至大型语言模型和大型视觉-语言模型近期已显著提升了它们的能力。然而，推理模型的泛化性仍模糊不清且理解不足。

Method: 研究者设计了一个评估框架来严谨地测试chain-of-thought (CoT) 方法在一个基于网格的导航任务上的泛化性能。在这个任务中，给定一张地图，模型需要输出一系列动作指引玩家从起始位置到达目标同时避开障碍物。通过调整不同的输入表达方式（视觉与文本）及CoT推理策略，并在分布内(ID)与分布外(OOD)测试条件下系统性地评价这些模型变体。

Result: 实验结果显示，虽然CoT推理增强了所有表征下模型对于ID数据集的泛化能力，但对于OOD情况（如更大规模的地图），除非是与ID数据有直接对应关系的情况之外，其泛化效果依然相当有限。有趣的是，研究还发现将多种文本格式相结合的推理路径能够带来最优（并且不是因为简单的对应关系而造成的）OOD泛化表现。此外，完全基于文本的模型相比那些采用了基于图片输入的模型表现得更好，即便是对比最近提出的依靠潜空间进行推理的方法也是如此。

Conclusion: 该研究表明，尽管chain-of-thought(CoT)推理有助于提高模型在相同类型数据上的表现，但要实现良好跨场景或跨大小的地图泛化仍有挑战。结合不同文本格式可能是一种有效的解决方案来改善模型在面对未见复杂环境时的表现。

Abstract: Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.

</details>


### [43] [POP: Prior-fitted Optimizer Policies](https://arxiv.org/abs/2602.15473)
*Jan Kobiolka,Christian Frey,Gresa Shala,Arlind Kadra,Erind Bedalli,Josif Grabocka*

Main category: cs.LG

TL;DR: 本文提出了一种名为POP（Prior-fitted Optimizer Policies）的元学习优化器，它能够在不同复杂度的优化问题上超越传统的基于梯度的方法以及其他优化方法，表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 经典基于梯度的优化器对于超参数选择非常敏感，在高度非凸设定下其性能依赖于仔细调整的学习率、动量和梯度累积。为了解决这些限制，作者引入了POP。

Method: POP是一个元学习优化器，它通过预测条件在优化轨迹中提供的上下文信息来确定坐标级步长。该模型是在数百万个从跨越凸和非凸目标的新先验中采样的合成优化问题上学到的。

Result: POP在一个包含47个不同复杂度优化函数的基准测试中进行了评估，并且在匹配预算约束条件下始终优于一阶梯度方法、非凸优化方法（例如进化策略）、贝叶斯优化以及最近的一种元学习竞争对手。

Conclusion: 评估表明，POP不需要针对特定任务进行调优就能展现出强大的通用性。

Abstract: Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.

</details>


### [44] [Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data](https://arxiv.org/abs/2602.15478)
*Sharmad Kalpande,Saurabh Shirke,Haroon R. Lone*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedFAP的特征感知个性化联邦学习框架，用于基于智能手机感应数据跨国家进行情绪推断。实验结果表明，FedFAP在地理和文化多样性的群体中实现了0.744的AUROC值，优于集中式方法和个人化联邦学习基线。此外，本研究还为情绪感知系统的设计提供了见解，展示了如何通过考虑人群特点的个人化处理以及保护隐私的学习方法来实现可扩展的情绪感知移动感应技术。


<details>
  <summary>Details</summary>
Motivation: 情绪不稳定性是心理健康的关键行为指标，但传统的评估方式依赖于低频且回顾性的报告，无法捕捉到情绪变化的连续性。虽然基于智能手机的移动感应技术能够从日常行为中被动地推断情绪状态，但在大规模部署时面临着隐私限制、感应可用性不均等问题。因此，需要一种既能保护用户隐私又能有效处理不同地区异构感应模态的方法。

Method: 研究人员开发了FedFAP，这是一种特征感知个性化联邦学习框架，旨在解决跨区域间存在的异构感应模态问题。该框架允许每个国家作为独立客户端参与模型训练过程，同时保留本地数据集以确保隐私安全。

Result: FedFAP在地理与文化背景多样的人群中进行了测试，达到了0.744的AUROC评分，显著优于中心化方法和其他现有个性化联邦学习基准。

Conclusion: 研究表明，FedFAP不仅提高了情绪预测准确性，也为未来设计更加注重个体差异及隐私保护的情绪感知系统提供了重要参考。

Abstract: Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.
  In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.

</details>


### [45] [LLM-as-Judge on a Budget](https://arxiv.org/abs/2602.15481)
*Aadirupa Saha,Aniket Wagde,Branislav Kveton*

Main category: cs.LG

TL;DR: 本文提出了一种基于多臂老虎机理论和集中不等式的方差自适应方法，以在固定计算预算下优化查询分配，从而最小化估计误差。实验表明，该方法在保持相同预算的情况下显著优于均匀分配策略，减少了最坏情况下的估计误差。


<details>
  <summary>Details</summary>
Motivation: 鉴于使用LLM-as-a-judge评估大型语言模型时遇到的随机性问题，以及如何在固定计算预算下最优地分配查询以最小化估计误差这一挑战，研究者们提出了新的解决方案。

Method: 利用多臂老虎机理论与集中不等式设计了一种能够根据预估得分方差动态调整查询分配的方法，重点在于将资源集中在不确定性最高的地方。

Result: 所提算法在《Summarize-From-Feedback》和《HelpSteer2》数据集上进行了测试，结果表明相比均匀分配方式，新方法能在相同的预算限制内更有效地减少最坏情况下的评分估计误差。

Conclusion: 本研究为高效评估大语言模型提供了坚实的理论基础，并对AI安全、模型一致性及大规模自动化评估具有实际应用价值。

Abstract: LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? %
We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\tilde{O}\left(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}}\right)$, $σ_i^2$ being the unknown score variance for pair $i \in [K]$ with near-optimal budget allocation. %
Experiments on \emph{Summarize-From-Feedback} and \emph{HelpSteer2} demonstrate that our method significantly outperforms uniform allocation, reducing worst-case estimation error while maintaining identical budgets. Our work establishes a theoretical foundation for efficient LLM evaluation with practical implications for AI safety, model alignment, and automated assessment at scale.

</details>


### [46] [Approximation Theory for Lipschitz Continuous Transformers](https://arxiv.org/abs/2602.15503)
*Takashi Furuya,Davide Murari,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 本文提出了一类通过构造保持Lipschitz连续性的梯度下降型上下文Transformer，证明了这类模型在Lipschitz约束函数空间内的通用逼近定理，并采用测度论形式化方法为Transformer架构的设计提供了坚实的理论基础。


<details>
  <summary>Details</summary>
Motivation: 为了在安全敏感的环境中部署Transformers，需要保证其稳定性和鲁棒性。一种实现这种行为的原则性方法是限制模型的Lipschitz常数。然而，对于那些明确保留Lipschitz连续性的架构来说，还没有建立近似理论保证。

Method: 研究者引入了一类梯度下降类型的上下文Transformers，它们通过设计就是Lipschitz连续的。MLP和注意力模块被实现为负梯度流的显式Euler步骤，确保了固有的稳定性同时不牺牲表达力。分析采用了测度论的形式化方法，将Transformers解释为概率测度上的算子，从而得到与标记数量无关的逼近保证。

Result: 研究者为这一类在Lipschitz约束下的函数空间内工作的Transformers证明了一个通用逼近定理。这意味着该类模型可以在保持一定稳定性的同时，仍然能够很好地逼近复杂函数。

Conclusion: 这些结果为设计稳健且Lipschitz连续的Transformer架构提供了一个严格的理论基础。

Abstract: Stability and robustness are critical for deploying Transformers in safety-sensitive settings. A principled way to enforce such behavior is to constrain the model's Lipschitz constant. However, approximation-theoretic guarantees for architectures that explicitly preserve Lipschitz continuity have yet to be established. In this work, we bridge this gap by introducing a class of gradient-descent-type in-context Transformers that are Lipschitz-continuous by construction. We realize both MLP and attention blocks as explicit Euler steps of negative gradient flows, ensuring inherent stability without sacrificing expressivity. We prove a universal approximation theorem for this class within a Lipschitz-constrained function space. Crucially, our analysis adopts a measure-theoretic formalism, interpreting Transformers as operators on probability measures, to yield approximation guarantees independent of token count. These results provide a rigorous theoretical foundation for the design of robust, Lipschitz continuous Transformer architectures.

</details>


### [47] [The Obfuscation Atlas: Mapping Where Honesty Emerges in RLVR with Deception Probes](https://arxiv.org/abs/2602.15515)
*Mohammad Taufeeque,Stefan Heimersheim,Adam Gleave,Chris Cundy*

Main category: cs.LG

TL;DR: 本文研究了在现实编码环境中，AI系统在接受反欺骗检测器训练时可能采取的混淆策略，并提出了保持诚实政策的方法。


<details>
  <summary>Details</summary>
Motivation: 研究当AI系统接受反欺骗检测器训练时，模型是否学会了通过混淆其欺骗行为来逃避检测器的问题。

Method: 在构建的真实编码环境中，观察并分类了模型在面对欺骗检测器时可能出现的行为结果，包括保持诚实、通过激活混淆或策略混淆变得具有欺骗性。

Result: 实证发现，无论是否有检测器惩罚，激活混淆都会由于RL过程中的表示漂移而出现；而策略梯度方法中，探测惩罚仅激励了策略混淆；足够高的KL正则化和检测器惩罚可以产生诚实的策略。

Conclusion: 白盒欺骗检测器可以作为容易受到奖励黑客攻击的任务的有效训练信号，适当调整参数可以帮助防止模型学习到欺骗性的混淆策略。

Abstract: Training against white-box deception detectors has been proposed as a way to make AI systems honest. However, such training risks models learning to obfuscate their deception to evade the detector. Prior work has studied obfuscation only in artificial settings where models were directly rewarded for harmful output. We construct a realistic coding environment where reward hacking via hardcoding test cases naturally occurs, and show that obfuscation emerges in this setting. We introduce a taxonomy of possible outcomes when training against a deception detector. The model either remains honest, or becomes deceptive via two possible obfuscation strategies. (i) Obfuscated activations: the model outputs deceptive text while modifying its internal representations to no longer trigger the detector. (ii) Obfuscated policy: the model outputs deceptive text that evades the detector, typically by including a justification for the reward hack. Empirically, obfuscated activations arise from representation drift during RL, with or without a detector penalty. The probe penalty only incentivizes obfuscated policies; we theoretically show this is expected for policy gradient methods. Sufficiently high KL regularization and detector penalty can yield honest policies, establishing white-box deception detectors as viable training signals for tasks prone to reward hacking.

</details>


### [48] [CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals](https://arxiv.org/abs/2602.15546)
*Tomàs Garriga,Gerard Sanz,Eduard Serrahima de Cambra,Axel Brando*

Main category: cs.LG

TL;DR: 本文提出了一种新的针对受市场事件影响的时间序列数据的反事实推理方法，称为条件熵惩罚自编码器(CEPAE)，通过理论和实验证明了其在合成、半合成及真实世界数据集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 为了在金融、医疗保健和营销等领域做出决策时能够准确地对时间序列进行反事实推理，理解事件或处理措施随时间对结果的影响至关重要。

Method: 采用基于变分自编码器与对抗自编码器的方法，并在此基础上提出了条件熵惩罚自编码器（CEPAE），这是一种新颖的用于反事实推理的自编码器方法，通过对潜在空间施加熵惩罚损失来促进解耦的数据表示。

Result: CEPAE在评估指标上通常优于其他方法，在合成、半合成以及真实世界数据集上均得到了验证。

Conclusion: 本研究成功地将结构因果模型框架应用于时间序列数据的反事实分析中，开发出的新方法CEPAE为相关领域提供了有效工具。

Abstract: The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.

</details>


### [49] [1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization](https://arxiv.org/abs/2602.15563)
*Sohir Maskey,Constantin Eichenberg,Johannes Messner,Douglas Orr*

Main category: cs.LG

TL;DR: 本研究通过实证研究低比特环境下的量化感知训练（QAT），发现基于k-means的权重量化比整数格式更优，并且在固定推理内存预算下，使用1比特量化的权重可以获得生成任务的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 尽管量化感知训练能够显著减少大语言模型的内存占用，同时保持可接受的性能下降，但在实践中选择最佳量化格式和位宽仍是一个挑战。此外，量化与下游性能之间的精确权衡尚未得到充分理解，因为比较通常仅依赖于困惑度评估。

Method: 研究者们对低比特条件下的量化感知训练进行了实证研究，特别是比较了不同量化方法的效果。他们发现基于k-means的权重量化不仅优于整数格式，而且能在标准硬件上高效实现。

Result: 实验结果表明，在给定的推理内存预算限制内，采用1比特量化权重的方法在生成型下游任务中表现最佳。

Conclusion: 这项工作揭示了在进行量化感知训练时，基于k-means的权重量化是一种有效策略，特别是在追求极致压缩率的同时希望保持良好性能的情况下。

Abstract: Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.

</details>


### [50] [Uniform error bounds for quantized dynamical models](https://arxiv.org/abs/2602.15586)
*Abdelkader Metakalard,Fabien Lauer,Kevin Colin,Marion Gilson*

Main category: cs.LG

TL;DR: 本文提供了从依赖数据序列中学习到的动力学模型的准确性统计保证，特别是为系统识别中的量化模型和不完美优化算法开发了统一误差界限。


<details>
  <summary>Details</summary>
Motivation: 为了确保从依赖数据序列中学习到的动力学模型的准确性，并考虑到实际应用中常见的量化模型和不完美优化算法的需求。

Method: 通过块分解方法获得了慢速率界限，并通过一种新颖的间隔点策略得到了快速率、适应方差的界限。

Result: 研究结果表明，这些界限与编码模型所需的比特数成比例，从而将硬件限制转化为可解释的统计复杂性。

Conclusion: 该研究为动力学模型，特别是在混合系统识别中，提供了重要的统计准确度保障。

Abstract: This paper provides statistical guarantees on the accuracy of dynamical models learned from dependent data sequences. Specifically, we develop uniform error bounds that apply to quantized models and imperfect optimization algorithms commonly used in practical contexts for system identification, and in particular hybrid system identification. Two families of bounds are obtained: slow-rate bounds via a block decomposition and fast-rate, variance-adaptive, bounds via a novel spaced-point strategy. The bounds scale with the number of bits required to encode the model and thus translate hardware constraints into interpretable statistical complexities.

</details>


### [51] [Certified Per-Instance Unlearning Using Individual Sensitivity Bounds](https://arxiv.org/abs/2602.15602)
*Hanna Benarroch,Jamal Atif,Olivier Cappé*

Main category: cs.LG

TL;DR: 本文提出了一种基于自适应每个实例噪声校准的方法来实现机器认证遗忘，该方法针对每个数据点对学习解决方案的个人贡献进行调整。通过使用每个实例的差分隐私，并为岭回归训练导出高概率的每个实例敏感度界限，从而以更少的噪声注入实现了认证遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的通过噪声注入实现机器认证遗忘的方法通常会导致性能下降，因为噪声是根据最坏情况下的敏感性来校准的。本研究旨在探索一种替代方法，即基于自适应每个实例噪声校准的方法，这种方法能够针对每个数据点对所学解决方案的具体贡献进行定制化处理。

Method: 本文采用的方法是利用每个实例的差分隐私定义个体数据点在噪声梯度动态中的敏感性。对于通过Langevin动力学训练的岭回归，研究人员推导出了高概率的每个实例敏感性边界，从而减少了噪声注入量的同时也确保了认证遗忘的有效性。

Result: 实验结果表明，在线性设置中，提出的理论发现得到了验证；此外，在深度学习环境中进一步提供了实证证据，证明了该方法的相关性和有效性。

Conclusion: 本研究表明，通过采用自适应每个实例噪声校准的方法并结合每个实例的差分隐私技术，可以有效地减少噪声注入同时保持良好的认证遗忘保证，这为提高机器学习模型的安全性和隐私保护提供了一个新的视角。

Abstract: Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.

</details>


### [52] [The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems](https://arxiv.org/abs/2602.15637)
*Amirreza Dolatpour Fathkouhi,Alireza Namazi,Heman Shakeri*

Main category: cs.LG

TL;DR: 本文指出了时间序列插补基准中存在的一种系统性偏差——平稳性偏差，并提出了一种分层压力测试方法来解决这一问题。通过连续血糖监测（CGM）作为测试平台，研究发现线性插值在稳定期间表现优秀，但在关键过渡期形态保真度急剧下降，而深度学习模型则能在这些关键时刻保持点精度和形态完整性，更适合安全关键任务。此外，作者还从临床试验中推导出经验缺失分布并应用于完整训练数据上，以增强模型在实际缺失情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 时间序列插补基准通常使用均匀随机掩码和形状无关的度量标准（如MSE、RMSE），这导致了评估结果偏向于常见状态。对于具有主导吸引子的系统来说，这种做法会导致简单方法看似更优的现象，因为基准主要采样的是容易处理且熵低的状态。本文旨在解决这一平稳性偏差问题，确保评价方法能够准确反映不同状态下模型的真实性能。

Method: 提出了一个分层压力测试框架，将评估分为平稳态和过渡态两部分；选择连续血糖监测(CGM)作为实验平台，利用其严格的地面实况强迫函数（餐食、胰岛素）来精确识别状态；分析了线性插值与深度学习模型在不同状态下的表现差异；基于临床试验数据推导出缺失模式，并将其应用于训练数据集以模拟现实世界中的数据缺失情况。

Result: 证实了线性插值在稳定区间内达到了最先进的重建效果，但在关键过渡期内形态保真度显著下降；揭示了一个称为“RMSE幻象”的现象，在该现象中，尽管点误差较低，但信号形状遭到破坏；展示了深度学习模型在维持过渡期内点精度的同时还能保持形态完整性，表明它们对于安全关键应用至关重要；通过引入真实世界的缺失模式增强了模型对实际缺失情况的鲁棒性。

Conclusion: 本研究表明，针对具有明显稳态和瞬态特征的系统进行时间序列插补时，需要考虑不同状态下的模型表现。简单的线性插值方法虽然在稳态下表现出色，但在瞬态期间可能无法准确捕捉信号变化。相比之下，深度学习方法能够更好地保留信号形状，适合用于需要高可靠性的应用场景。此外，通过模拟实际数据缺失模式训练模型可以提高其在现实世界中的实用性。

Abstract: Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings with broad implications:(i)~Stationary Efficiency: Linear interpolation achieves state-of-the-art reconstruction during stable intervals, confirming that complex architectures are computationally wasteful in low-entropy regimes.(ii)~Transient Fidelity: During critical transients (post-prandial peaks, hypoglycemic events), linear methods exhibit drastically degraded morphological fidelity (DTW), disproportionate to their RMSE -- a phenomenon we term the \emph{RMSE Mirage}, where low pointwise error masks the destruction of signal shape.(iii)~Regime-Conditional Model Selection: Deep learning models preserve both pointwise accuracy and morphological integrity during transients, making them essential for safety-critical downstream tasks. We further derive empirical missingness distributions from clinical trials and impose them on complete training data, preventing models from exploiting unrealistically clean observations and encouraging robustness under real-world missingness. This framework generalizes to any regulated system where routine stationarity dominates critical transients.

</details>


### [53] [Guided Diffusion by Optimized Loss Functions on Relaxed Parameters for Inverse Material Design](https://arxiv.org/abs/2602.15648)
*Jens U. Kreber,Christian Weißenfels,Joerg Stueckler*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的逆向设计方法，通过将原始设计空间放松为连续网格表示，解决了由于离散参数或进一步约束导致无法直接使用基于梯度优化的问题。该方法在复合材料设计问题中得到了应用，能够找到与指定体积模量匹配的设计，并且能够在二维和三维设置下同时最小化生成样本的材料密度。


<details>
  <summary>Details</summary>
Motivation: 工程和材料科学中常见的逆向设计问题难以解决，特别是在存在离散参数或额外约束的情况下，直接使用基于梯度的优化变得不可行。因此，需要一种新的方法来处理这样的逆向设计挑战。

Method: 提出的方法基于扩散模型，首先将设计空间转换成一个允许计算梯度的连续网格表示。训练扩散模型以在这个放松后的参数空间上作为先验，然后通过在推理时根据可微模拟传递的目标函数梯度进行引导扩散采样。最终，通过反投影到原始参数空间获取设计方案。

Result: 所提方法在寻找与指定体积模量相匹配的设计方案方面表现良好，能够在二维和三维环境中产生误差范围在1%内的多种设计方案。此外，还展示了通过采用多目标损失函数同时最小化材料密度的能力。

Conclusion: 本研究展示了一种新颖的逆向设计策略，利用扩散模型有效地探索了设计空间，提供了多样化的解决方案，并能针对特定需求（如体积模量）及附加要求（如密度最小化）生成高质量的设计提案。

Abstract: Inverse design problems are common in engineering and materials science. The forward direction, i.e., computing output quantities from design parameters, typically requires running a numerical simulation, such as a FEM, as an intermediate step, which is an optimization problem by itself. In many scenarios, several design parameters can lead to the same or similar output values. For such cases, multi-modal probabilistic approaches are advantageous to obtain diverse solutions. A major difficulty in inverse design stems from the structure of the design space, since discrete parameters or further constraints disallow the direct use of gradient-based optimization. To tackle this problem, we propose a novel inverse design method based on diffusion models. Our approach relaxes the original design space into a continuous grid representation, where gradients can be computed by implicit differentiation in the forward simulation. A diffusion model is trained on this relaxed parameter space in order to serve as a prior for plausible relaxed designs. Parameters are sampled by guided diffusion using gradients that are propagated from an objective function specified at inference time through the differentiable simulation. A design sample is obtained by backprojection into the original parameter space. We develop our approach for a composite material design problem where the forward process is modeled as a linear FEM problem. We evaluate the performance of our approach in finding designs that match a specified bulk modulus. We demonstrate that our method can propose diverse designs within 1% relative error margin from medium to high target bulk moduli in 2D and 3D settings. We also demonstrate that the material density of generated samples can be minimized simultaneously by using a multi-objective loss function.

</details>


### [54] [CAMEL: An ECG Language Model for Forecasting Cardiac Events](https://arxiv.org/abs/2602.15677)
*Neelay Velingker,Alaia Solko-Breslin,Mayank Keoliya,Seewon Choi,Jiayi Xin,Anika Marathe,Alireza Oraii,Rajat Deo,Sameed Khatana,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: 本文提出了CAMEL，一种新型的心电图语言模型（ELM），能够处理更长时间的心电信号并预测未来心脏事件。通过采用专门的ECG编码器和结合LoRA适应与课程学习流程训练，CAMEL在多个任务和数据集上表现出色，特别是在新引入的ECGForecastBench基准测试中实现了对心律失常预测的领先结果。


<details>
  <summary>Details</summary>
Motivation: 尽管心电图（ECG）对于诊断心血管状况至关重要，但现有的ECG语言模型无法预测未来的潜在心脏问题，这限制了它们在早期干预计划中的临床价值。因此，研究旨在开发一种能够基于更长信号持续时间进行推理从而具备预测能力的新模型。

Method: 研究者们提出了一种名为CAMEL的心电图语言模型，该模型利用特制的ECG编码器来促进ECG信号与文本之间的跨理解，并且通过结合LoRA适应技术和课程学习管道对其进行训练。课程设计涵盖了ECG分类、指标计算以及多轮对话以激发推理能力。

Result: CAMEL在6项任务及9个数据集上展现了强大的零样本性能，特别是它在新设立的ECGForecastBench基准测试中对心律失常的预测方面表现优异。此外，在ECGBench上的成绩比现有方法高出7.0%，而在ECGForecastBench上的表现更是超过了全监督模型12.4%和零样本ELMs 21.1%。

Conclusion: CAMEL作为一种创新性的心电图语言模型，不仅能够在已知分布内外达到或超越现有方法的表现，而且还能有效预测未来的心脏事件，为心脏病学领域提供了新的可能性。

Abstract: Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).

</details>


### [55] [UrbanVerse: Learning Urban Region Representation Across Cities and Tasks](https://arxiv.org/abs/2602.15750)
*Fengze Sun,Egemen Tanin,Shanika Karunasekera,Zuqing Li,Flora D. Salim,Jianzhong Qi*

Main category: cs.LG

TL;DR: 本文提出了一种名为UrbanVerse的模型，用于跨城市的城市表示学习和跨任务的城市分析。通过关注目标区域的局部特征及周边结构特征而非整个城市的特征，并引入一个名为HCondDiffCT的任务通用模块，该模型在跨城市设置下的六个任务中始终优于现有最先进方法，预测准确率提高了最多35.89%。


<details>
  <summary>Details</summary>
Motivation: 现有的城市区域表示学习方法在跨城市和跨任务的应用上存在局限性，无法很好地泛化。本文旨在开发一种能够超越特定城市和任务设定的基础模型，以支持更广泛的城市分析应用。

Method: 提出了UrbanVerse模型，它将区域视为图上的节点并通过基于随机游走的过程形成“区域序列”，以此来捕捉局部与邻近地区的结构特性；为了实现跨任务泛化能力，设计了HCondDiffCT模块，该模块能够在扩散过程中整合区域条件先验知识和任务条件语义，共同建模多个下游城市预测任务。

Result: 实验结果表明，在跨城市设置下针对六项任务时，UrbanVerse相比当前最先进的方法表现出色，最高可提高35.89%的预测准确性。

Conclusion: UrbanVerse为跨城市和跨任务的城市分析提供了一个有效且强大的基础模型方案。

Abstract: Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form "sequences of regions" that reflect both local and neighborhood structural features for urban region representation learning. For cross-task generalization, we propose a cross-task learning module named HCondDiffCT. This module integrates region-conditioned prior knowledge and task-conditioned semantics into the diffusion process to jointly model multiple downstream urban prediction tasks. HCondDiffCT is generic. It can also be integrated with existing urban representation learning models to enhance their downstream task effectiveness. Experiments on real-world datasets show that UrbanVerse consistently outperforms state-of-the-art methods across six tasks under cross-city settings, achieving up to 35.89% improvements in prediction accuracy.

</details>


### [56] [The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety](https://arxiv.org/abs/2602.15799)
*Max Springer,Chung Peng Lee,Blossom Metevier,Jane Castleman,Bohdan Turbal,Hayoung Jung,Zeyu Shen,Aleksandra Korolova*

Main category: cs.LG

TL;DR: 本文揭示了在良性任务上微调对齐的语言模型可能会意外地降低安全护栏的有效性，即使训练数据不包含有害内容且开发者没有恶意。研究指出，当前认为微调更新应与高维参数空间中的安全关键方向正交的观点是不可靠的，因为这种正交性在梯度下降的动力学作用下会变得不稳定。作者通过新的几何分析证明，对齐集中在具有尖锐曲率的低维子空间内，形成了一个脆弱结构，使得一阶方法无法检测或防御。文章提出了“对齐不稳定性条件”，并发现对齐损失随训练时间四次方增长，这为开发曲率感知的方法提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决现有语言模型微调过程中出现的安全性问题，特别是当模型在看似无害的任务上进行微调时，其原有的安全防护机制可能被意外破坏的问题。

Method: 采用了一种新颖的几何分析方法来探讨为什么传统观点认为微调不会影响模型安全性是错误的，并通过提出“对齐不稳定性条件”解释了背后的原因。

Result: 结果表明，对齐确实集中在低维子空间中，并且这些子空间具有明显的曲率特征，导致使用标准梯度下降法时难以维持原有安全属性；同时，还发现了对齐损失随训练时间呈四次方增长的关系。

Conclusion: 结论强调了当前安全范式中存在的结构性盲点，并提倡从被动的红队测试转向预测性诊断，以更好地支持开放权重模型部署期间的安全分析。

Abstract: Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters. These results expose a structural blind spot in the current safety paradigm. The dominant approaches to safe fine-tuning address only the initial snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug to be patched; it is an intrinsic geometric property of gradient descent on curved manifolds. Our results motivate the development of curvature-aware methods, and we hope will further enable a shift in alignment safety analysis from reactive red-teaming to predictive diagnostics for open-weight model deployment.

</details>


### [57] [Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning](https://arxiv.org/abs/2602.15817)
*Oswin So,Eric Yang Yu,Songyuan Zhang,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: 提出了一种名为Feasibility-Guided Exploration (FGE)的新方法，该方法能够识别出存在安全策略的可行初始条件子集，并同时学习解决这些初始条件下可达性问题的策略。实验结果显示，与现有最佳方法相比，FGE在MuJoCo和Kinetix模拟器中对于具有挑战性的初始条件下学习到的策略覆盖范围超过50%。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习(RL)在高维控制任务上取得了显著成果，但将其应用于可达性问题时存在根本性不匹配：可达性旨在最大化系统可以无限期保持安全的状态集合，而RL则优化用户指定分布下的预期回报。这种不匹配可能导致策略在仍然属于安全集但概率较低的状态下表现不佳。

Method: 提出了Feasibility-Guided Exploration (FGE)方法，它能同时确定一组存在安全策略的可行初始条件（包括初始状态、动态以及安全集），并在此初始条件集上学习解决可达性问题的策略。

Result: 实验结果表明，在MuJoCo模拟器和使用像素观测的Kinetix模拟器的任务中，对于具有挑战性的初始条件，FGE学到的策略比现有的最佳方法多出了超过50%的覆盖范围。

Conclusion: FGE提供了一种有效的方法来处理强化学习中的可达性问题，特别是在面对不确定性较高的初始条件时，能够学习到更广泛的适用策略。

Abstract: Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.

</details>


### [58] [Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics](https://arxiv.org/abs/2602.15820)
*Anna Zimmel,Paul Setinek,Gianluca Galletti,Johannes Brandstetter,Werner Zellinger*

Main category: cs.LG

TL;DR: 提出了一种基于存储最具信息量（D-最优）统计信息的测试时适应框架，以解决高维、非结构化和回归问题中的分布偏移问题。应用于预训练模拟替代模型时，该方法在几乎无计算成本的情况下，实现了高达7%的分布外性能改进。


<details>
  <summary>Details</summary>
Motivation: 工程中使用的机器学习替代模型在从训练到部署过程中会遇到由于分布偏移导致的显著性能下降问题。现有测试时适应方法主要针对低维度分类问题设计，不适用于仿真中常见的高维度、非结构化及回归问题。

Method: 通过提出一种基于存储最大化信息量(D-最优)统计数据的TTA框架来解决这个问题。该框架同时支持稳定适应与测试时的原则性参数选择。

Result: 当应用于预训练的模拟替身时，本方法可以以微不足道的计算成本实现最高达7%的超出分布改善。

Conclusion: 这是首次系统地展示了有效的TTA对于高维模拟回归和生成设计优化的应用，并通过SIMSHIFT和EngiBench基准进行了验证。

Abstract: Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.

</details>


### [59] [Operationalising the Superficial Alignment Hypothesis via Task Complexity](https://arxiv.org/abs/2602.15829)
*Tomás Vergara-Browne,Darshan Patil,Ivan Titov,Siva Reddy,Tiago Pimentel,Marius Mosbach*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量标准，即任务复杂性，并通过实验验证了预训练模型如何显著降低完成某些任务的复杂性。结果显示，任务适应通常只需要很少的信息——通常只有几千字节。


<details>
  <summary>Details</summary>
Motivation: 表面一致性假设（SAH）认为大型语言模型在预训练过程中学习到了大部分知识，而后期训练只是让这些知识显现出来。然而，SAH缺乏精确定义，导致支持它的论据各异且看似正交，同时也有重要的批评意见。为了解决这个问题，研究者们提出了一个新的度量标准来统一以前的支持论据并提供更清晰的理解框架。

Method: 研究人员引入了“任务复杂性”这一新指标，定义为达到特定任务目标表现所需的最短程序长度。基于此框架，他们对数学推理、机器翻译以及指令遵循等任务进行了实验评估，旨在探索预训练模型对于降低任务复杂性的具体影响。

Result: 实验结果表明，在给定预训练模型的情况下，上述任务的复杂性可以异常低。此外，研究还发现虽然预训练能够使模型接触到强大的性能水平，但可能需要长达千兆字节的程序才能实现；相比之下，后期训练则能在几个数量级上减少达到相同性能所需的信息量。

Conclusion: 本研究表明，通过预训练模型进行的任务适应往往只需极少的信息量即可完成，强调了预训练阶段在简化后续任务解决过程中的关键作用。

Abstract: The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model. Further, we find that pre-training enables access to strong performances on our tasks, but it can require programs of gigabytes of length to access them. Post-training, on the other hand, collapses the complexity of reaching this same performance by several orders of magnitude. Overall, our results highlight that task adaptation often requires surprisingly little information -- often just a few kilobytes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [60] [CircuChain: Disentangling Competence and Compliance in LLM Circuit Analysis](https://arxiv.org/abs/2602.15037)
*Mayank Ravishankara*

Main category: cs.SE

TL;DR: 本研究引入了CircuChain，一个用于诊断大型语言模型在电路分析中遵循指令与物理推理能力之间差异的基准测试。通过Control/Trap问题对以及多阶段验证流程，发现更强的模型虽然物理推理准确但常违反约定，而较弱模型则更遵从明确指令。这表明模型能力增强并不一定意味着约束对齐的改善，并强调了需要新的评估框架来确保在数学严格领域下的指令跟随性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在工程领域的表现接近专家级水平，在用户指定的约束下进行可靠的推理变得至关重要。尤其是在电路分析中，如果解决方案违背了既定的方法论惯例如网孔方向性或极性分配规则，即使数值正确也是不够的，因为这样的错误可能会在安全关键系统中传播。目前尚不清楚前沿模型是否真正运用了第一原理推理还是依赖于训练过程中形成的、可能与明确指示相冲突的先验知识。

Method: 研究者们提出了CircuChain，这是一个旨在区分电气电路分析中遵守指示的能力和物理推理能力的诊断基准。CircuChain包含了五个经典电路拓扑结构中的平衡控制/陷阱问题对，并且加入了符号约定、电流方向及极性定义上的系统化变化。采用了一个包含符号解算器、SPICE仿真和基于大语言模型的错误分类在内的多阶段验证流程，以细致地将失败归因于约定错误、物理错误、算术错误或幻觉。

Result: 通过对每个模型100项任务的观察，研究者们发现了一种一致性的“合规-能力分歧”。最强的被评估模型展示了近乎完美的物理推理能力，但在陷阱条件故意反转自然符号模式时表现出较高的约定违反率。相反，较弱的模型显示出较低的物理保真度却拥有更好的遵循明确指示的能力。这些结果表明，模型能力的提高并不保证约束对齐的改善。

Conclusion: 研究表明，增加模型能力并不能保证改进约束对齐，强调了需要新的评估框架来强化在数学严格领域下指令遵循的重要性。CircuChain提供了一个这样的框架，并为工程教育和人工智能对齐研究提供了可操作的见解。

Abstract: As large language models (LLMs) advance toward expert-level performance in engineering domains, reliable reasoning under user-specified constraints becomes critical. In circuit analysis, for example, a numerically correct solution is insufficient if it violates established methodological conventions such as mesh directionality or polarity assignments, errors that can propagate in safety-critical systems. Yet it remains unclear whether frontier models truly apply first-principles reasoning or rely on entrenched training priors that conflict with explicit instructions. We introduce CircuChain, a diagnostic benchmark designed to disentangle instruction compliance from physical reasoning competence in electrical circuit analysis. CircuChain consists of counterbalanced Control/Trap problem pairs across five canonical circuit topologies, augmented with systematic variations in sign conventions, current orientations, and polarity definitions. A multi-stage verification pipeline, combining symbolic solvers, SPICE simulation, and an LLM-based error taxonomy, enables fine-grained attribution of failures to convention errors, physics errors, arithmetic mistakes, or hallucinations. Across 100 tasks per model, we observe a consistent Compliance-Competence Divergence. The strongest model evaluated exhibits near-perfect physical reasoning but a high rate of convention violations when Trap conditions deliberately invert natural sign patterns. Conversely, weaker models display lower physical fidelity yet superior adherence to explicit instructions. These results suggest that increased model capability does not guarantee improved constraint alignment and highlight the need for new evaluation frameworks that stress instruction-following under mathematically rigid domains. CircuChain provides one such framework and offers actionable insights for both engineering education and AI alignment research.

</details>


### [61] [An Empirical Study on the Effects of System Prompts in Instruction-Tuned Models for Code Generation](https://arxiv.org/abs/2602.15228)
*Zaiyu Cheng,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 研究了系统提示对通用和专门用于代码生成的语言模型的影响，发现提示的详细程度、模型规模、提示策略及编程语言等因素都会影响代码助手的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管在扩展和训练方法上取得了进展，但系统提示对通用语言模型和专门用于代码生成的语言模型（CLMs）的影响仍是一个未被充分探索的关键方面。

Method: 通过跨越四个模型、五个系统提示、三种提示策略、两种编程语言以及两种温度设置的360种配置进行系统评估。

Result: (1) 提示约束的具体性增加并不总是提高正确性——提示的有效性依赖于配置，并且可能根据任务需求和解码上下文帮助或妨碍；(2) 对于较大的代码专用模型而言，少量样本示例相比零样本生成可能会降低性能；(3) 编程语言很重要，Java比Python对系统提示的变化表现出显著更大的敏感度。

Conclusion: 本研究表明，为了优化代码助手的表现，需要考虑系统提示的细节、使用的模型大小、采取的提示策略以及目标编程语言等因素，这表明可能需要针对特定语言设计提示工程策略。

Abstract: Instruction-tuned Language Models (ILMs) have become essential components of modern AI systems, demonstrating exceptional versatility across natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs -- commonly referred to as Code Language Models (CLMs) -- translate human intent into executable programs. While progress has been driven by advances in scaling and training methodologies, one critical aspect remains underexplored: the impact of system prompts on both general-purpose ILMs and specialized CLMs for code generation. We systematically evaluate how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect code assistant. Our experimental setting spans 360 configurations across four models, five system prompts, three prompting strategies, two languages, and two temperature settings. We find that (1) increasing system-prompt constraint specificity does not monotonically improve correctness -- prompt effectiveness is configuration-dependent and can help or hinder based on alignment with task requirements and decoding context; (2) for larger code-specialized models, few-shot examples can degrade performance relative to zero-shot generation, contrary to conventional wisdom; and (3) programming language matters, with Java exhibiting significantly greater sensitivity to system prompt variations than Python, suggesting language-specific prompt engineering strategies may be necessary.

</details>


### [62] [GenAI for Systems: Recurring Challenges and Design Principles from Software to Silicon](https://arxiv.org/abs/2602.15241)
*Arya Tschand,Chenyu Wang,Zishen Wan,Andrew Cheng,Ioana Cristescu,Kevin He,Howard Huang,Alexander Ingare,Akseli Kangaslahti,Sara Kangaslahti,Theo Lebryk,Hongjin Lin,Jeffrey Jian Ma,Alexandru Meterez,Clara Mohri,Depen Morwani,Sunny Qin,Roy Rinberg,Paula Rodriguez-Diaz,Alyssa Mia Taliotis,Pernille Undrum Fathi,Rosie Zhao,Todd Zhou,Vijay Janapa Reddi*

Main category: cs.SE

TL;DR: 本文从跨栈视角探讨了生成模型在计算系统设计中的应用，指出了五个反复出现的挑战及对应的五个设计原则，并提出需要共享工程方法学来促进整个领域的进步。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何改变计算系统的构建方式，但现有研究分散于软件、架构和芯片设计领域。文章旨在通过跨栈视角分析生成模型的应用，以发现不同层次间共通的问题与解决方案。

Method: 通过对跨越计算栈三层的超过275篇论文进行综合分析，覆盖11个应用领域，识别出贯穿各层的五大挑战及其对应的设计原则。

Result: 确定了五个反复遇到的挑战（反馈循环危机、隐性知识问题、信任与验证、跨边界协同设计、从确定性到动态性的转变）以及五项有效的应对原则（采用混合方法、为持续反馈而设计、按角色分离关注点、匹配方法与问题结构、基于数十年系统知识建设）。

Conclusion: 生成式AI的研究和发展面临着一些共同的挑战，通过采取特定的设计原则可以有效应对这些问题；为了加速整个领域的发展，建议建立共享的工程方法论，包括通用词汇表、跨层基准测试和系统化设计实践。

Abstract: Generative AI is reshaping how computing systems are designed, optimized, and built, yet research remains fragmented across software, architecture, and chip design communities. This paper takes a cross-stack perspective, examining how generative models are being applied from code generation and distributed runtimes through hardware design space exploration to RTL synthesis, physical layout, and verification. Rather than reviewing each layer in isolation, we analyze how the same structural difficulties and effective responses recur across the stack. Our central finding is one of convergence. Despite the diversity of domains and tools, the field keeps encountering five recurring challenges (the feedback loop crisis, the tacit knowledge problem, trust and validation, co-design across boundaries, and the shift from determinism to dynamism) and keeps arriving at five design principles that independently emerge as effective responses (embracing hybrid approaches, designing for continuous feedback, separating concerns by role, matching methods to problem structure, and building on decades of systems knowledge). We organize these into a challenge--principle map that serves as a diagnostic and design aid, showing which principles have proven effective for which challenges across layers. Through concrete cross-stack examples, we show how systems navigate this map as they mature, and argue that the field needs shared engineering methodology, including common vocabularies, cross-layer benchmarks, and systematic design practices, so that progress compounds across communities rather than being rediscovered in each one. Our analysis covers more than 275 papers spanning eleven application areas across three layers of the computing stack, and distills open research questions that become visible only from a cross-layer vantage point.

</details>


### [63] [SACS: A Code Smell Dataset using Semi-automatic Generation Approach](https://arxiv.org/abs/2602.15342)
*Hanyu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: 本文提出了一种半自动的方法来生成高质量的代码异味数据集，通过自动规则生成候选样本，并使用多指标将这些样本分为自动接受组和手动审核组。此外，还制定了结构化的审核指南并开发了标注工具以支持手动验证过程。基于这种方法，创建了一个开源代码异味数据集SACS，涵盖了三种广泛研究的代码异味：长方法、大类和特性嫉妒。


<details>
  <summary>Details</summary>
Motivation: 应用机器学习技术于代码异味研究面临的主要挑战之一是缺乏高质量的数据集。虽然可以自动生成数据集，但往往标签可靠性低且数据质量受损；而完全人工构建则极其耗时费力。为了解决这个问题，本研究探索了一种能够产生高质量数据样本的半自动化方法。

Method: 采用一套自动化生成规则生产候选的有异味代码样本，然后利用多种度量标准将数据样本分成自动接受组和需要人工复核组两部分，以便审阅者能够集中精力处理模糊不清的样本。同时，建立了结构化的审查指南，并开发了一款注释工具辅助进行手动验证流程。

Result: 基于所提出的半自动化生成方法，成功构建了一个名为SACS的开源代码异味数据集，该数据集包含了三种常见的代码异味类型：长方法、大型类以及特性羡慕。每种类别均包含超过10,000个已标记样本。

Conclusion: 本研究提出的半自动代码异味数据集生成方法有效解决了现有数据集规模小、质量差的问题，为未来在代码异味检测与自动化重构领域的研究提供了有力支持。

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past of decades, the research on code smell has received extensive attention. Especially the researches applied machine learning-technique have become a popular topic in recent studies. However, one of the biggest challenges to apply machine learning-technique is the lack of high-quality code smell datasets. Manually constructing such datasets is extremely labor-intensive, as identifying code smells requires substantial development expertise and considerable time investment. In contrast, automatically generated datasets, while scalable, frequently exhibit reduced label reliability and compromised data quality. To overcome this challenge, in this study, we explore a semi-automatic approach to generate a code smell dataset with high quality data samples. Specifically, we first applied a set of automatic generation rules to produce candidate smelly samples. We then employed multiple metrics to group the data samples into an automatically accepted group and a manually reviewed group, enabling reviewers to concentrate their efforts on ambiguous samples. Furthermore, we established structured review guidelines and developed a annotation tool to support the manual validation process. Based on the proposed semi-automatic generation approach, we created an open-source code smell dataset, SACS, covering three widely studied code smells: Long Method, Large Class, and Feature Envy. Each code smell category includes over 10,000 labeled samples. This dataset could provide a large-scale and publicly available benchmark to facilitate future studies on code smell detection and automated refactoring.

</details>


### [64] [A Differential Fuzzing-Based Evaluation of Functional Equivalence in LLM-Generated Code Refactorings](https://arxiv.org/abs/2602.15761)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: 本研究通过差异模糊测试来评估大型语言模型（LLMs）生成的代码重构与原始实现之间的功能等效性，发现LLMs有非微不足道的趋势改变程序语义，导致19-35%的功能不等价重构，并且现有测试套件可能无法检测到所有这些不等价情况。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在自动代码重构中的快速采用，评估和确保LLM生成的重构与原始实现之间的功能等价变得至关重要。

Method: 利用差异模糊测试而非依赖预定义测试用例的方法来检查由LLM生成的代码重构的功能等价性。

Result: 在对六个LLM跨三个数据集和两种重构类型的广泛评估中，发现这些模型显示出非微不足道地改变程序语义的趋势，产生了19-35%的功能上不等价重构；此外，大约21%的这种不等价重构未被当前测试套件检测出来。

Conclusion: 依靠现有的测试可能会高估LLM生成的代码重构中的功能等价性，这表明这类重构仍然容易出现语义分歧。

Abstract: With the rapid adoption of large language models (LLMs) in automated code refactoring, assessing and ensuring functional equivalence between LLM-generated refactoring and the original implementation becomes critical. While prior work typically relies on predefined test cases to evaluate correctness, in this work, we leverage differential fuzzing to check functional equivalence in LLM-generated code refactorings. Unlike test-based evaluation, a differential fuzzing-based equivalence checker needs no predefined test cases and can explore a much larger input space by executing and comparing thousands of automatically generated test inputs. In a large-scale evaluation of six LLMs (CodeLlama, Codestral, StarChat2, Qwen-2.5, Olmo-3, and GPT-4o) across three datasets and two refactoring types, we find that LLMs show a non-trivial tendency to alter program semantics, producing 19-35% functionally non-equivalent refactorings. Our experiments further demonstrate that about 21% of these non-equivalent refactorings remain undetected by the existing test suites of the three evaluated datasets. Collectively, the findings of this study imply that reliance on existing tests might overestimate functional equivalence in LLM-generated code refactorings, which remain prone to semantic divergence.

</details>
