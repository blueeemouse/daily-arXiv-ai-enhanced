<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication](https://arxiv.org/abs/2601.09114)
*Yufan Xia,Marco De La Pierre,Amanda S. Barnard,Giuseppe Maria Junior Barca*

Main category: cs.DC

TL;DR: 本文提出了一种基于机器学习的方法来优化GEMM任务的运行时间性能，通过自动选择最佳线程数，在不同HPC节点架构上实现了25%到40%的速度提升。


<details>
  <summary>Details</summary>
Motivation: 由于现代多核共享内存系统的复杂性，确定最小化多线程GEMM运行时间的线程数量非常具有挑战性。因此，作者们旨在开发一种能够根据给定任务自适应地选择最优线程数的方法以优化GEMM的执行效率。

Method: 本文介绍了一种名为ADSALA（Architecture and Data-Structure Aware Linear Algebra）的软件库概念验证方法，该方法利用机器学习技术来优化BLAS例程的运行时性能。特别是，该方法采用机器学习模型即时自动选择对于给定GEMM任务而言最优的线程数目。

Result: 在基于双插槽Intel Cascade Lake和双插槽AMD Zen 3两种不同的HPC节点架构上进行测试后，发现当GEMM内存使用量在100MB以内时，与传统BLAS中的GEMM实现相比，本方法可以实现25%至40%的速度提升。

Conclusion: 通过引入ADSALA库并运用机器学习策略来自适应调整线程数目，可以显著提高特定条件下GEMM操作的计算效率。

Abstract: The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB.

</details>


### [2] [Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems](https://arxiv.org/abs/2601.09146)
*Lingkang Shangguan*

Main category: cs.DC

TL;DR: The paper introduces PDCC, a transaction-driven dynamic reconfiguration protocol for modern payment systems, which enhances performance by avoiding global transaction ordering and allowing smooth reconfigurations without affecting system performance.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of modern payment systems and enable seamless reconfiguration processes, the authors were motivated to develop a new protocol that does not rely on global transaction ordering, thus reducing the overhead and increasing efficiency.

Method: The method involves designing PDCC, a protocol that utilizes Byzantine Consistent Broadcast for transactions and incorporates user nonce-based transaction ordering with periodic consensus mechanisms. This approach allows for dynamic configuration changes while maintaining high system performance.

Result: The proposed PDCC protocol successfully achieves high performance in modern payment systems by enabling efficient reconfiguration and avoiding the need for global transaction ordering, ensuring that the reconfiguration process is smooth and has no negative impact on the original system's performance.

Conclusion: PDCC represents an effective solution for enhancing the performance of modern payment systems through a novel dynamic reconfiguration protocol that supports smooth transitions and maintains operational efficiency.

Abstract: We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance.

</details>


### [3] [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)
*Du Yin,Jiayi Ren,Xiayu Sun,Tianyao Zhou,Haizhu Zhou,Ruiyan Ma,Danyang Zhang*

Main category: cs.DC

TL;DR: 介绍了一种名为LatencyPrism的系统，该系统能在不修改代码或重启服务的情况下，实现跨平台的推理延迟分析，并在毫秒级触发警报以确保满足SLO要求。


<details>
  <summary>Details</summary>
Motivation: 现有的AI性能分析方法由于设计上的侵入性和硬件绑定性，在实时生产环境中分析分布式推理环境中的延迟问题存在不足。需要一种能够在不中断服务的情况下精确分析并预警延迟异常的技术解决方案。

Method: 开发了LatencyPrigm系统，它采用零侵入式设计，能够跨不同软件框架和XPU架构进行工作，对推理过程中的延迟进行细分，同时提供实时监测与异常警告功能。

Result: LatencyPrism被部署到了数千个XPU上超过六个月的时间，实现了低开销的实时监控，并且能够准确地区分由工作负载引起的变化和指示潜在问题的异常情况，F1分数达到0.98。

Conclusion: 通过广泛的实验和根因分析研究证明了LatencyPrism的有效性，表明其能够在保持高性能的同时帮助维护服务等级目标（SLO）。

Abstract: LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.
  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: 本文介绍了一种名为LAUDE的框架，它结合了设计源代码的语义理解和大型语言模型（LLMs）的思维链（CoT）推理能力，以生成单元测试并调试硬件设计。通过使用闭源和开源LLMs对来自VerilogEval数据集的大量有错误的硬件设计代码进行应用，生成的单元测试能够检测组合与顺序设计中几乎所有的错误，并成功调试了其中大部分设计。


<details>
  <summary>Details</summary>
Motivation: 在硬件设计生命周期中，单元测试对于确保组件设计模块的功能正确性和符合规格至关重要。开发针对不同设计特性的单元测试需要深刻理解设计功能及创新性。当一个或多个单元测试揭示了设计失败时，调试工程师需要诊断、定位并修复故障以确保设计的正确性，这通常是一个耗时且复杂的过程。

Method: 提出了LAUDE，一种统一的单元测试生成与调试框架，该框架利用设计源代码的语义理解与基础大型语言模型（LLMs）的思维链（CoT）推理能力之间的交叉融合。LAUDE通过集成提示工程和设计执行信息来提高其单元测试生成精度以及代码可调试性。

Result: 将LAUDE应用于从VerilogEval数据集中提取的大规模含错误硬件设计代码上时，所生成的单元测试能够检测到高达100%的组合逻辑设计错误和93%的时序逻辑设计错误；同时，在调试方面，分别成功处理了组合逻辑设计中的93%以及时序逻辑设计中的84%案例。

Conclusion: 研究结果表明，LAUDE框架在提升硬件设计过程中单元测试生成的质量及增强代码调试效率方面具有显著优势。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [5] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（如ChatGPT和GitHub Copilot）如何改变软件工程的核心能力，并提出了一种针对整合这些工具的教育设计模型。特别关注土耳其计算机工程课程所面临的挑战，建议从构建转向批判、验证以及人机协作，并指出需要转变传统的抄袭为中心的诚信机制至过程透明模型。此外，文章强调了进行长期实证研究以评估这些干预措施及其对学习长期影响的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被集成到专业工作流程中，改变了代码生成、解释和测试的成本，同时为日常开发任务引入了新的自动化形式。然而，大多数软件工程和计算机工程课程仍然紧密遵循将手动语法生产等同于技术能力的教学模式。这种日益增长的错位引发了关于评估有效性、学习成果及基础技能发展的担忧。

Method: 采用概念性研究方法，提出了一个理论框架来分析生成式AI如何改变核心软件工程能力，并介绍了一个针对LLM整合教育的教育设计模型。特别注意到了土耳其计算机工程项目中由于集中监管、大班授课和考试导向的评估实践而加剧的挑战。

Result: 该框架描述了问题分析、设计、实现与测试越来越从构建转向批判、验证及人机共管的趋势。论文还指出，传统的以抄袭为中心的诚信机制正变得不足，推动向过程透明度模型转变的需求。

Conclusion: 尽管本研究提供了一个结构化的课程适应提案，但仍然是理论上的贡献；文章最后概述了需要通过纵向实证研究来评估这些干预措施及其对学习的长期影响。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [6] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 本文提出了一种系统性的提示优化方法，通过遗传-帕累托（GEPA）框架迭代改进提示，以提高OpenACC指令生成的质量。该方法显著提高了由大型语言模型（特别是nano规模的模型）生成的代码的编译成功率，并且在功能上实现了GPU相对于CPU基线的加速。


<details>
  <summary>Details</summary>
Motivation: 尽管OpenACC简化了GPU卸载的过程，但编写高性能的pragma仍然复杂，需要深厚的专业知识。大型语言模型为自动并行代码生成提供了潜在解决方案，但简单的提示往往导致语法错误的指令、无法编译的代码或性能不佳。因此，有必要开发一种有效的方法来优化提示，从而改善LLM生成的OpenACC pragma质量。

Method: 采用遗传-帕累托（GEPA）框架，通过交叉和变异操作对指令进行迭代演化，同时利用专家策划的黄金示例及基于子句和子句参数级别不匹配情况的结构化反馈作为指导。

Result: 使用优化后的提示标注的程序，在PolyBench套件上的编译成功率明显提高，尤其是对于nano级别的模型如GPT-4.1 Nano和GPT-5 Nano，其编译成功率分别从66.7%升至93.3%，以及从86.7%达到100%。此外，优化后的提示还使得实现功能性GPU加速超过CPU基线的程序数量增加了21%。

Conclusion: 研究结果表明，通过提示优化可以有效释放小型廉价LLM在编写稳定有效的GPU卸载指令方面的潜力，为HPC工作流程中的自动化指令式并行化提供了一条成本效益高的路径。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [7] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 本文探讨了多大型语言模型生态系统中的自适应信任度量，提出了一种在受监管约束下量化和提高模型可靠性的框架，并通过金融合规性和医疗诊断的案例研究展示了其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地应用于医疗、金融和法律等敏感领域，对其信任、责任和可靠性提出了迫切的关注。文章旨在探索一种能够在这些受监管行业中促进安全且可扩展的人工智能采用的自适应信任度量方法。

Method: 通过分析系统行为、评估多个LLM之间的不确定性以及实施动态监控管道来实现对模型可靠性的量化与提升。

Result: 研究表明，通过使用自适应信任度量，在实际应用如金融合规及医疗诊断中可以有效地提高操作可信度。

Conclusion: 自适应信任度量被定位为受监管行业内安全和可扩展AI采纳的基础推动者。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [8] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: EZInput is a Python library that simplifies the use of computational algorithms by automatically generating user-friendly graphical interfaces, supporting cross-environment compatibility, and enhancing reproducibility through parameter persistence.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge faced by researchers in using computational algorithms, which often require programming skills for parameter configuration, suffer from interface inconsistencies across different environments, and lack session persistence. These issues lead to inefficiencies, such as repetitive input, slowed exploration, and difficulty in maintaining reproducibility.

Method: The authors introduce EZInput, a Python library designed to overcome these challenges. It allows developers to specify input requirements and validation constraints declaratively, then automatically generates graphical user interfaces (GUIs) that are compatible with various runtime environments, including Jupyter notebooks, Google Colab, and terminal environments. The system ensures 'write once, run anywhere' functionality, enabling seamless transition between prototyping and batch execution on remote systems. Additionally, it incorporates a mechanism for saving and restoring user configurations via YAML files, inspired by ImageJ/FIJI's approach, to support reproducibility and reduce redundant input.

Result: The implementation of EZInput demonstrates its effectiveness in providing an accessible solution for non-programmers to interact with computational tools, while also ensuring data integrity and offering clear feedback to users. This tool supports a wide range of input types necessary for scientific computing and enhances the overall user experience and research reproducibility.

Conclusion: In conclusion, EZInput significantly improves accessibility and usability of computational algorithms for end-users without requiring programming knowledge, facilitates consistent and efficient parameter management across different computing environments, and promotes better reproducibility in research through its innovative approach to parameter persistence.

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [9] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: 本文提出了一种名为开发阶段隔离的新策略，通过将构建自动化视为程序代码来强化构建系统以防止中毒。为此，作者开发了一个名为Foreman的工具原型，能够检测并警告如XZ Utils攻击中涉及的中毒测试文件，并计划未来自动检查开发阶段隔离以防范管道中毒。


<details>
  <summary>Details</summary>
Motivation: 现有的技术在尝试通过验证软件依赖关系来强化软件供应链安全时忽略了构建系统本身的安全性，而被污染的构建系统可以轻易绕过用于检测程序代码漏洞的工具。鉴于此，需要一种新的方法来专门针对构建系统的安全性进行保护。

Method: 提出了开发阶段隔离策略，将构建系统的行为和信息权限建模为程序代码，以此来加强构建系统对抗潜在攻击的能力。并且基于该策略开发了名为Foreman的工具，用以检测并提醒关于构建系统中毒的风险。

Result: Foreman工具成功识别并警告了XZ Utils攻击案例中的中毒测试文件。此外，还制定了未来计划，旨在通过自动检查开发阶段隔离进一步防护管道中毒的问题。

Conclusion: 通过对构建系统实施类似程序代码的安全检查机制，可以有效提高软件供应链的整体安全性。展望未来，希望构建系统安全检查器能像程序代码检查器一样普及。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [10] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本研究探讨了由大型语言模型（LLLMs）生成的测试用例在四个关系数据库管理系统中的脆弱性问题。结果显示，与现有测试相比，生成的测试用例中脆弱测试的比例略高，主要原因是测试依赖于未保证的特定顺序（即“无序集合”）。此外，发现脆弱性从现有测试转移到新生成测试的现象在闭源系统中更为普遍。


<details>
  <summary>Details</summary>
Motivation: 由于软件测试中存在不稳定测试的问题，这些测试在对同一代码多次执行时会产生不一致的结果，从而挑战了测试失败指示软件缺陷的假设。最近基于LLM的测试生成工作已经识别出生成测试的潜在脆弱性问题，但其普遍性和根本原因尚不清楚。

Method: 研究者们针对SAP HANA、DuckDB、MySQL和SQLite这四个关系数据库管理系统，使用GPT-4o和Mistral-Large-Instruct-2407两个LLMs放大测试套件来评估生成测试用例的脆弱性，并通过手动检查确定脆弱性的最常见根本原因。

Result: 结果表明，相比于现有的测试用例，由LLMs生成的测试用例具有稍微更高比例的脆弱性问题，其中大约63%的脆弱性测试是由于测试依赖于某个未被保证的特定顺序。此外，还观察到无论是哪种LLM，都倾向于将现有测试的脆弱性通过提供的提示上下文转移给新生成的测试，而且这种现象在像SAP HANA这样的闭源系统中比开源系统更加明显。

Conclusion: 这项研究表明了开发者可以预期从LLM生成的测试中遇到什么样的脆弱性类型，并强调了在使用LLM进行测试生成时提供定制化上下文的重要性。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [11] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: 提出了SafePlanner，一种针对自动驾驶系统规划模型的安全性测试框架。通过结构分析和引导模糊测试来生成测试场景并检测危险行为。在百度Apollo上评估显示，它能够高效地发现漏洞，并且在函数和决策覆盖率上超过了基准方法。


<details>
  <summary>Details</summary>
Motivation: 为了识别自动驾驶系统（ADS）的规划模型中存在的安全关键缺陷，特别是解决生成有意义的测试场景和检测危险规划行为两大核心挑战。

Method: 开发了名为SafePlanner的测试框架，该框架通过对规划模型实现进行结构分析（包括场景转换逻辑和层次控制流），从代码中提取可行的场景转换，并结合非玩家车辆的行为组合成测试场景；然后应用引导式模糊测试探索这些场景下的行为空间。

Result: 在百度Apollo平台上，SafePlanner生成了20635个测试用例，检测到了520种危险行为，并通过手动分析归类为15个根本原因。对于其中四个问题进行了修复，没有观察到明显的副作用。SafePlanner达到了83.63%的函数覆盖度和63.22%的决策覆盖度，在错误发现与效率方面优于基线方法。

Conclusion: SafePlanner证明了其在提高自动驾驶系统规划模型安全性方面的有效性，特别是在生成高质量测试案例以及识别潜在危险行为方面表现优异。

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [12] [DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries](https://arxiv.org/abs/2601.09440)
*Yi Gao,Xing Hu,Tongtong Xu,Jiali Zhao,Xiaohu Yang,Xin Xia*

Main category: cs.SE

TL;DR: DepRadar is a framework for analyzing defects and their impacts in deep learning (DL) library updates. It uses four agents to identify, synthesize, and check the impact of defects on downstream programs. Evaluation shows it achieves high precision and recall in defect identification and impact analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge faced by users of deep learning libraries, such as Transformers and Megatron, in assessing whether their own programs are affected by defects introduced through library updates. These defects can range from subtle performance issues to serious computation errors, and understanding their impact requires complex analysis involving various factors like configuration flags, runtime environments, and indirect API usage.

Method: The method proposed in the paper is DepRadar, an agent coordination framework designed for fine-grained defect and impact analysis in DL library updates. DepRadar operates through three steps with the involvement of four specialized agents: 1. PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests; 2. Orchestrator Agent synthesizes these signals into a unified defect pattern along with trigger conditions; 3. Impact Analyzer evaluates downstream programs to determine if the defect can be triggered. Additionally, DepRadar combines static analysis with domain-specific rules for enhanced accuracy and explainability.

Result: Evaluation results show that DepRadar performs well in both defect identification and impact analysis. On 157 pull requests and 70 commits across two representative DL libraries, DepRadar achieved 90% precision in identifying defects and produced high-quality structured fields. When applied to 122 client programs, it identified affected cases with 90% recall and 80% precision, significantly outperforming other baseline methods.

Conclusion: In conclusion, DepRadar effectively addresses the need for accurate and comprehensive defect and impact analysis in the context of deep learning library updates. By integrating advanced techniques such as static analysis and domain-specific rule-based reasoning, DepRadar not only improves the precision and recall of defect detection but also enhances the explainability of its findings, making it a valuable tool for developers and researchers working with DL libraries.

Abstract: Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.

</details>


### [13] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 本文开发并测试了一个针对能源研究软件的元数据模式，旨在提高研究软件的可发现性和可重用性，并符合FAIR4RS原则。


<details>
  <summary>Details</summary>
Motivation: 许多领域，包括能源研究，缺乏既定的元数据模式，这对提高研究软件的可查找性和可重用性至关重要。为了填补这一空白，作者们着手为能源研究软件开发一个特定领域的元数据模式。

Method: 通过需求分析来开发能源研究软件的元数据模式，并通过用户测试对其进行评估。

Result: 该模式平衡了形式化和互操作性的需求，同时也满足了能源研究人员的具体需要。此外，良好的信息展示对于帮助研究人员创建所需的元数据非常关键。

Conclusion: 论文提供了设计能源研究软件元数据模式所面临的挑战与机遇的见解。

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [14] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 该研究通过分析25,173个问题和拉取请求，探讨了用户在开发和维护nf-core流程时面临的挑战。研究使用了BERTopic模型识别出13个关键挑战，并指出89.38%的问题最终得到解决。标签的存在（大效应，δ=0.94）和代码片段（中等效应，δ=0.50）显著提高了问题解决的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管Nextflow及其社区nf-core被广泛采用，但对于用户在开发与维护这些流程过程中遇到的具体挑战知之甚少。本研究旨在揭示这些挑战、管理实践及感知到的困难，以提供有关如何改进这些流程可用性、可持续性和可重复性的见解。

Method: 本研究基于对来自nf-core项目的25,173个问题和拉取请求的数据集进行实证分析。采用了BERTopic建模来识别主要挑战类别，并通过统计方法分析了问题解决动态及影响因素。

Result: 研究发现了13个关键挑战领域，包括流程开发与集成、错误修复、基因组数据整合、CI配置管理以及处理版本更新等。约89.38%的问题或请求能够最终关闭，其中一半在三天内得到解决。标签和代码示例的存在被证明可以显著提高解决问题的可能性。工具开发与仓库维护是最大的挑战，其次是测试流程与CI配置，以及调试容器化流程。

Conclusion: 这项研究为理解nf-core流程的合作开发与维护提供了有价值的见解，强调了增强其可用性、可持续性和可重复性的机会。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [15] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: SysPro is an innovative method for reproducing system-level concurrency bugs by automatically extracting necessary system call information and generating input data, which is shown to be effective and efficient in real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of reproducing non-deterministic system-level concurrency bugs, where existing tools fail due to insufficient handling of specific system call interleavings and lack of detailed information in bug reports.

Method: SysPro, the proposed approach, combines information retrieval, regular expression matching, and the category-partition method to extract relevant system call names from bug reports and identify their locations in the source code. It then generates input data and uses dynamic source code instrumentation to reproduce the bugs with the extracted information.

Result: Through an empirical study on real-world benchmarks, SysPro has been demonstrated to effectively and efficiently localize and reproduce system-level concurrency bugs from bug reports.

Conclusion: The conclusion drawn from this work is that SysPro offers a viable solution for automating the reproduction of system-level concurrency bugs, improving over current methods by addressing key challenges in bug report analysis and data generation.

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [16] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 本研究复制了四种基于最新大语言模型（LLM）的测试生成工具，并评估了它们相对于仅使用LLM进行测试生成方法的有效性和效率。结果表明，单独使用LLM的方法在所有测试有效性指标上都优于现有技术，且通过先针对程序类再针对未覆盖方法来减少LLM请求次数，可以实现几乎相同甚至稍高的有效性同时降低约20%的成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）快速发展及其对软件工程领域的影响日益增加，自动单元测试生成的研究也逐渐增多。然而，直接使用LLM而不经过后处理产生的测试往往无法编译或达到高覆盖率。尽管已有多种技术试图解决这些问题并报告了改进成果，但这些技术通常与较弱的基线对比，这可能夸大了其性能贡献。因此，需要重新评估当前更强版本LLM下这些技术的实际增益。

Method: 选取HITS、SymPrompt、TestSpark和CoverUp四种包含工程组件以指导测试生成过程的最先进LLM测试生成工具进行复制，并将最新的LLM集成到所有方法中。随后，在393个类和3,657个方法上执行实验，比较这些方法与仅使用LLM直接生成测试之间的相对有效性和效率。此外，还探索了改变LLM应用粒度对成本的影响。

Result: 研究发现，单独使用LLM的方法在行覆盖率(高出17.72%)、分支覆盖率(高出19.80%)以及变异分数(高出20.92%)等所有测试有效性度量上均优于现有技术，并且在成本（LLM查询次数）方面具有可比性。进一步观察到，通过首先针对程序类然后针对未覆盖方法的方式，可以在保持相近甚至略高的有效性的同时，减少约20%的LLM请求次数。

Conclusion: 研究表明，当采用最新的LLM时，直接利用LLM生成测试代码不仅在有效性上超越了之前的技术，在效率上也显示出竞争力。通过调整LLM的应用策略，还可以进一步优化资源使用，为未来自动化测试生成技术的发展提供了新的视角。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [17] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: 提出了一种名为ShortCoder的知识注入框架，通过语法级简化规则、混合数据合成管道以及简洁性意识的微调策略来优化代码生成效率，同时保持语义等价性和可读性。实验结果表明，与现有方法相比，ShortCoder在HumanEval上实现了18.1%-37.8%的生成效率提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）极大地推动了代码生成技术的发展，但其效率仍然受到某些固有架构限制的影响，如每次生成token都需要进行一次完整的推理过程，这要求持续地在内存中保留上下文信息，并增加了资源消耗。现有的研究更多关注于推理阶段的优化，而对生成阶段的研究相对较少。

Method: 提出了一个名为ShortCoder的知识注入框架，该框架包含：(1)针对Python开发的十个语法级简化规则，这些规则基于AST保持转换，能够实现18.1%的token减少而不影响功能；(2)一种结合基于规则重写和LLM指导改进的混合数据合成管道，用于创建包含原始代码及其简化版本且保持语义一致性的数据集ShorterCodeBench；(3)向基础LLMs注入简洁性意识的微调策略。

Result: 广泛的实验结果显示，ShortCoder在HumanEval基准测试中始终优于最先进方法，在保证代码生成功能的前提下，相较于先前的方法提升了18.1%-37.8%的生成效率。

Conclusion: 本文介绍了一种新的知识注入框架ShortCoder，它通过一系列创新方法有效提高了代码生成任务中的效率问题，同时确保了输出代码的质量。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [18] [Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas](https://arxiv.org/abs/2601.08901)
*Yuexi Shen,Minqian Liu,Dawei Zhou,Lifu Huang*

Main category: cs.IR

TL;DR: 本文提出了一种新的科学知识结构化表示方法——Ideation Space，它将科学知识分解为研究问题、方法论和核心发现三个不同维度，并通过对比训练来学习每个维度。基于此表示，作者还提出了一个层次子空间检索框架用于高效的文献检索，以及一种分解的新颖性评估算法，可以识别出想法中的新颖方面。实验结果显示了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 当前科学文献数量快速增长，如何从大量文献中识别出概念上相关的工作并评估新想法与现有研究的区别成为一个关键挑战。现有的嵌入方法通常将不同的概念方面混为单一表示，无法支持细粒度的文献检索；同时，基于大型语言模型的评估者容易受到奉承偏见的影响，无法提供有区分度的新颖性评估。

Method: 引入了Ideation Space，这是一种将科学知识分解为研究问题、方法论和核心发现三个独立维度的结构化表示方法。每个维度都是通过对比训练来学习的。基于这种表示法，提出了一个层次子空间检索框架以实现高效的目标文献检索，及一种能够确定想法哪些方面是新颖的分解式新颖性评估算法。

Result: 实验结果表明，在召回率@30达到了0.329（比基线高出16.7%），在ideation转换检索上的命中率@30达到了0.643，而新颖性评估与专家判断之间的相关性达到0.37。

Conclusion: 这项工作为加速和评估科学研究提供了一个有前景的范例。

Abstract: Scientific discovery is a cumulative process and requires new ideas to be situated within an ever-expanding landscape of existing knowledge. An emerging and critical challenge is how to identify conceptually relevant prior work from rapidly growing literature, and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations and cannot support fine-grained literature retrieval; meanwhile, LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. To tackle these challenges, we introduce the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions, i.e., research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance between ideas, and modeling of ideation transitions that capture the logical connections within a proposed idea. Building upon this representation, we propose a Hierarchical Sub-Space Retrieval framework for efficient, targeted literature retrieval, and a Decomposed Novelty Assessment algorithm that identifies which aspects of an idea are novel. Extensive experiments demonstrate substantial improvements, where our approach achieves Recall@30 of 0.329 (16.7% over baselines), our ideation transition retrieval reaches Hit Rate@30 of 0.643, and novelty assessment attains 0.37 correlation with expert judgments. In summary, our work provides a promising paradigm for future research on accelerating and evaluating scientific discovery.

</details>


### [19] [Fine Grained Evaluation of LLMs-as-Judges](https://arxiv.org/abs/2601.08919)
*Sourav Saha,Mandar Mitra*

Main category: cs.IR

TL;DR: 本研究扩展了先前关于大型语言模型（LLMs）作为信息检索相关性评估者效能的研究，通过使用基于维基百科的测试集让LLMs不仅判断文档的相关性/非相关性，还标记出文档中被认为有用的段落。结果表明，在人类监督下，LLMs作为评判者的性能最佳。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）作为信息检索中标准即席任务相关性评估者的有效性，并进一步研究LLMs在文档级别之外、具体到段落级别的评估准确性。

Method: 使用由INEX倡议创建的一个基于维基百科的测试集，要求LLMs不仅要判断文档是否与查询表达的信息需求相关，还要像人类评估员一样标记出文档内所有响应查询需求的段落。

Result: 发现当受到人类监督时，LLMs作为评判者的性能达到最优。

Conclusion: 为了提高LLMs作为信息检索领域相关性评估者的准确性和可靠性，建议在应用过程中引入一定程度的人类监督。

Abstract: A good deal of recent research has focused on how Large Language Models
  (LLMs) may be used as `judges' in place of humans to evaluate the quality
  of the output produced by various text / image processing systems. Within
  this broader context, a number of studies have investigated the specific
  question of how effectively LLMs can be used as relevance assessors for
  the standard ad hoc task in Information Retrieval (IR). We extend these
  studies by looking at additional questions. Most importantly, we use a
  Wikipedia based test collection created by the INEX initiative, and
  prompt LLMs to not only judge whether documents are relevant /
  non-relevant, but to highlight relevant passages in documents that it
  regards as useful. The human relevance assessors involved in creating
  this collection were given analogous instructions, i.e., they were asked
  to highlight all passages within a document that respond to the
  information need expressed in a query. This enables us to evaluate the
  quality of LLMs as judges not only at the document level, but to also
  quantify how often these `judges' are right for the right reasons.
  Our findings suggest that LLMs-as-judges work best under human
  supervision.

</details>


### [20] [LLMs Meet Isolation Kernel: Lightweight, Learning-free Binary Embeddings for Fast Retrieval](https://arxiv.org/abs/2601.09159)
*Zhibo Zhang,Yang Xu,Kai Ming Ting,Cam-Tu Nguyen*

Main category: cs.IR

TL;DR: 本文提出了一种无需学习的方法——隔离核嵌入（IKE），该方法利用隔离核将大型语言模型的高维嵌入转换为二进制嵌入，从而显著降低存储和检索开销，并且在多个文本检索数据集上的实验表明，IKE相比其他压缩方法能够更好地平衡检索效率与效果。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的方法如Matryoshka表示学习（MRL）和对比稀疏表示（CSR）一定程度上缓解了大型语言模型（LLMs）高维嵌入带来的存储与检索开销问题，但它们仍然面临检索准确度下降的问题。为此，需要一种新方法来进一步减少存储和计算需求的同时保持甚至提高检索精度。

Method: 提出了隔离核嵌入（Isolation Kernel Embedding, IKE）方法，这是一种无需训练的学习方法，通过使用隔离核将大型语言模型的高维嵌入转化为二进制形式。IKE基于随机分区的集合构建，这有助于在LLM嵌入空间中稳健地估计理想的核函数，随着集合规模增加而减少检索准确性损失。此外，由于采用了轻量级的二进制编码方式，该方法具有较小的内存占用量以及快速位运算特性，从而降低了检索延迟。

Result: 在多个文本检索数据集上进行的实验显示，相比于原始的大规模语言模型嵌入，IKE可以实现高达16.7倍的检索速度提升及1/16的内存使用量减少，同时保持相当或更好的检索准确性。与其他压缩技术如CSR等相比，IKE始终能够在检索效率与有效性之间取得最佳平衡。

Conclusion: 提出的隔离核嵌入（IKE）方法成功解决了大型语言模型高维嵌入所带来的挑战，不仅极大地减少了存储需求和提高了检索速度，而且在保证检索性能方面也表现优异。

Abstract: Large language models (LLMs) have recently enabled remarkable progress in text representation. However, their embeddings are typically high-dimensional, leading to substantial storage and retrieval overhead. Although recent approaches such as Matryoshka Representation Learning (MRL) and Contrastive Sparse Representation (CSR) alleviate these issues to some extent, they still suffer from retrieval accuracy degradation. This paper proposes \emph{Isolation Kernel Embedding} or IKE, a learning-free method that transforms an LLM embedding into a binary embedding using Isolation Kernel (IK). IKE is an ensemble of diverse (random) partitions, enabling robust estimation of ideal kernel in the LLM embedding space, thus reducing retrieval accuracy loss as the ensemble grows. Lightweight and based on binary encoding, it offers low memory footprint and fast bitwise computation, lowering retrieval latency. Experiments on multiple text retrieval datasets demonstrate that IKE offers up to 16.7x faster retrieval and 16x lower memory usage than LLM embeddings, while maintaining comparable or better accuracy. Compared to CSR and other compression methods, IKE consistently achieves the best balance between retrieval efficiency and effectiveness.

</details>


### [21] [Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models](https://arxiv.org/abs/2601.09286)
*Hanze Guo,Jianxun Lian,Xiao Zhou*

Main category: cs.IR

TL;DR: 提出了一种名为SaD的框架，该框架结合了密集嵌入的语义表达能力和稀疏交互模式的结构可靠性，通过双向对齐机制解决了在建模不受欢迎项目时信号噪声比（SNR）下降的问题。实验表明，即使简单的矩阵分解风格的密集模型也能达到最先进的性能，并且SaD可以无缝应用于现有的推荐模型中。


<details>
  <summary>Details</summary>
Motivation: 当前基于密集嵌入的方法在处理不受欢迎项目时遇到了信号噪声比（SNR）的理论限制，特别是在数据极度稀疏的情况下。为了克服这一瓶颈，研究提出了SaD框架。

Method: SaD框架采用了一个轻量级的双向对齐机制，其中密集视图通过注入语义相关性来丰富稀疏视图，而稀疏视图则通过显式结构信号来规范化密集模型。

Result: 广泛的实验表明，在这种双重视角对齐下，即使是简单的矩阵分解样式的密集模型也能实现最先进的性能。进一步的真实世界基准测试显示，SaD始终优于强大的基线，在BarsMatch排行榜上排名第一。

Conclusion: SaD框架展示了当从双重视角利用协同过滤的力量时，其持久的价值。它不仅提高了对不受欢迎项目的建模能力，还能够被广泛地应用到各种现有推荐系统模型中。

Abstract: Collaborative Filtering (CF) remains the cornerstone of modern recommender systems, with dense embedding--based methods dominating current practice. However, these approaches suffer from a critical limitation: our theoretical analysis reveals a fundamental signal-to-noise ratio (SNR) ceiling when modeling unpopular items, where parameter-based dense models experience diminishing SNR under severe data sparsity. To overcome this bottleneck, we propose SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. We theoretically show that aligning these dual views yields a strictly superior global SNR. Concretely, SaD introduces a lightweight bidirectional alignment mechanism: the dense view enriches the sparse view by injecting semantic correlations, while the sparse view regularizes the dense model through explicit structural signals. Extensive experiments demonstrate that, under this dual-view alignment, even a simple matrix factorization--style dense model can achieve state-of-the-art performance. Moreover, SaD is plug-and-play and can be seamlessly applied to a wide range of existing recommender models, highlighting the enduring power of collaborative filtering when leveraged from dual perspectives. Further evaluations on real-world benchmarks show that SaD consistently outperforms strong baselines, ranking first on the BarsMatch leaderboard. The code is publicly available at https://github.com/harris26-G/SaD.

</details>


### [22] [On-Device Large Language Models for Sequential Recommendation](https://arxiv.org/abs/2601.09306)
*Xin Xia,Hongzhi Yin,Shane Culpepper*

Main category: cs.IR

TL;DR: 本文提出了OD-LLM，一种专为设备上部署大型语言模型（LLMs）以进行序列推荐任务而设计的任务自适应压缩框架。通过结合低秩结构压缩算法和新颖的标记标准化技术，以及采用渐进对齐算法来最小化压缩比提高时的性能下降，实验证明当模型大小减半时，OD-LLM仍能保持原推荐模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限设备上的实时推荐需求，特别是那些对于执行延迟、用户隐私和在互联网连接不稳定或不可用情况下的功能稳定性有要求的应用场景，现有的大型语言模型由于其巨大的内存占用量和计算开销难以直接应用。

Method: OD-LLM框架整合了两种互补的压缩策略：利用奇异值分解(SVD)显著减少模型参数冗余的低秩结构压缩算法，以及更有效地辅助低秩分解过程的新颖标记标准化技术。同时，为了减少使用更高压缩比率时可能出现的性能损失，还引入了一种新的渐进对齐算法来迭代地优化目标模型所需的层间参数。

Result: 实验评估表明，在序列推荐基准测试中，即使将部署模型大小减半，OD-LLM也能保持与原始推荐模型相当的效果。这证明了OD-LLM方案的有效性和可扩展性。

Conclusion: OD-LLM作为一种新颖且实用的解决方案，为寻求替代成本高昂且需远程执行的大型语言模型提供了可能，特别适用于需要实现实时、设备端处理的场景。

Abstract: On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.

</details>


### [23] [Dissecting Judicial Reasoning in U.S. Copyright Damage Awards](https://arxiv.org/abs/2601.09459)
*Pei-Chi Lo,Thomas Y. Lu*

Main category: cs.IR

TL;DR: 本研究提出了一种基于话语的大型语言模型方法，该方法结合了修辞结构理论和能动工作流程，从司法意见中提取并量化先前不透明的推理模式。通过构建数据集、话语分析和能带动态特征提取三个阶段的管道，识别出推理组件，并提取具有相应话语子树的特征标签。在分析版权损害裁决时，发现这种增强话语的LLM分析优于传统方法，同时揭示了不同巡回法院之间因素权重未量化的差异。


<details>
  <summary>Details</summary>
Motivation: 联邦法院在版权损害赔偿判决中的解释和因素权重存在广泛差异，导致诉讼当事人面临不确定性，同时也模糊了法律决策的经验基础。

Method: 本研究采用一种新颖的话语型大型语言模型（LLM）方法论，将修辞结构理论(RST)与能动性工作流程相结合，旨在从司法意见书中提取并量化之前难以理解的推理模式。整个框架分为三步：数据集构建、话语分析以及能动特征提取。

Result: 研究表明，在分析版权损害裁决时，增强型话语LLM分析比传统方法表现更佳，同时揭示了不同巡回法庭间因子权重上未曾被量化的变动情况。

Conclusion: 这些发现不仅为计算法学分析提供了方法上的进步，还为司法推理提供了实用见解，对寻求预测工具的法律从业者、研究法律原则应用的学者以及面对版权法不一致性的政策制定者都有重要意义。

Abstract: Judicial reasoning in copyright damage awards poses a core challenge for computational legal analysis. Although federal courts follow the 1976 Copyright Act, their interpretations and factor weightings vary widely across jurisdictions. This inconsistency creates unpredictability for litigants and obscures the empirical basis of legal decisions. This research introduces a novel discourse-based Large Language Model (LLM) methodology that integrates Rhetorical Structure Theory (RST) with an agentic workflow to extract and quantify previously opaque reasoning patterns from judicial opinions. Our framework addresses a major gap in empirical legal scholarship by parsing opinions into hierarchical discourse structures and using a three-stage pipeline, i.e., Dataset Construction, Discourse Analysis, and Agentic Feature Extraction. This pipeline identifies reasoning components and extract feature labels with corresponding discourse subtrees. In analyzing copyright damage rulings, we show that discourse-augmented LLM analysis outperforms traditional methods while uncovering unquantified variations in factor weighting across circuits. These findings offer both methodological advances in computational legal analysis and practical insights into judicial reasoning, with implications for legal practitioners seeking predictive tools, scholars studying legal principle application, and policymakers confronting inconsistencies in copyright law.

</details>


### [24] [Bridging Semantic Understanding and Popularity Bias with LLMs](https://arxiv.org/abs/2601.09478)
*Renqiang Luo,Dong Zhang,Yupeng Gao,Wen Shi,Mingliang Hou,Jiaying Liu,Zhe Wang,Shuo Yu*

Main category: cs.IR

TL;DR: 提出了FairLRM框架，通过分解流行度偏差并使用结构化指令提示来增强大型语言模型对全局项目分布和用户偏好的理解，从而提高推荐系统的公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的去偏差方法主要集中在多样性的增强或长尾覆盖上，未能深入理解造成偏差的语义层面，这限制了它们在去偏差效果和推荐准确性上的表现。

Method: FairLRM框架将流行度偏差分为物品端和用户端两部分，并利用基于结构化指令的提示来提升模型对于全局物品分布和个人用户喜好的理解能力。

Result: 实验结果表明，FairLRM显著提高了推荐系统的公平性与准确性，提供了一种更加语义感知且值得信赖的方法以增强对流行度偏差的理解。

Conclusion: FairLRM为解决推荐系统中的流行度偏差问题提供了新的思路，通过加强模型对潜在偏差的语义解释能力，实现了更好的推荐效果与公平性。

Abstract: Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as "diversity" or "debiasing", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM.

</details>


### [25] [Unifying Search and Recommendation in LLMs via Gradient Multi-Subspace Tuning](https://arxiv.org/abs/2601.09496)
*Jujia Zhao,Zihan Wang,Shuaiqun Pan,Suzan Verberne,Zhaochun Ren*

Main category: cs.IR

TL;DR: 该论文提出了一种名为GEMS的新框架，通过多子空间分解和零空间投影来统一搜索与推荐任务，并有效减少梯度冲突和保持通用领域知识。实验表明GEMS在搜索和推荐任务上均优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 搜索和推荐是在线平台的核心功能，分别通过查询处理显式意图并从行为中建模隐含意图。虽然已有研究尝试统一这两个过程，但现有的方法要么依赖于共享编码器加任务特定头部的设计，要么完全微调模型，这不仅计算成本高还限制了可扩展性。参数高效微调(PEFT)提供了一个更实用的选择，但在统一搜索与推荐时面临梯度冲突及用户意图理解偏移的问题。

Method: 提出了Gradient Multi-Subspace Tuning (GEMS)，一种新的框架，旨在利用LLMs（大型语言模型）统一搜索与推荐的同时解决梯度冲突问题并保存通用领域知识。GEMS包含两个关键组件：1. **多子空间分解**，将共享和任务特定的优化信号分离到互补的低秩子空间中，以此减少破坏性的梯度干扰；2. **零空间投影**，约束参数更新至一个与通用领域知识空间正交的子空间内，从而减轻对用户意图理解的偏移。

Result: 广泛的基准数据集上的实验结果显示，GEMS在搜索和推荐两项任务上都持续优于当前最先进的基线方法，表现出更高的有效性。

Conclusion: GEMS作为一种创新的方法，在利用大型语言模型(LLMs)实现搜索与推荐任务统一的过程中展现了显著的优势。它成功地解决了梯度冲突问题，并且能够有效地保持对通用领域知识的理解，为未来的研究提供了新的方向。

Abstract: Search and recommendation (S&R) are core to online platforms, addressing explicit intent through queries and modeling implicit intent from behaviors, respectively. Their complementary roles motivate a unified modeling paradigm. Early studies to unify S&R adopt shared encoders with task-specific heads, while recent efforts reframe item ranking in both S&R as conditional generation. The latter holds particular promise, enabling end-to-end optimization and leveraging the semantic understanding of LLMs. However, existing methods rely on full fine-tuning, which is computationally expensive and limits scalability. Parameter-efficient fine-tuning (PEFT) offers a more practical alternative but faces two critical challenges in unifying S&R: (1) gradient conflicts across tasks due to divergent optimization objectives, and (2) shifts in user intent understanding caused by overfitting to fine-tuning data, which distort general-domain knowledge and weaken LLM reasoning. To address the above issues, we propose Gradient Multi-Subspace Tuning (GEMS), a novel framework that unifies S&R with LLMs while alleviating gradient conflicts and preserving general-domain knowledge. GEMS introduces (1) \textbf{Multi-Subspace Decomposition}, which disentangles shared and task-specific optimization signals into complementary low-rank subspaces, thereby reducing destructive gradient interference, and (2) \textbf{Null-Space Projection}, which constrains parameter updates to a subspace orthogonal to the general-domain knowledge space, mitigating shifts in user intent understanding. Extensive experiments on benchmark datasets show that GEMS consistently outperforms the state-of-the-art baselines across both search and recommendation tasks, achieving superior effectiveness.

</details>


### [26] [TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09523)
*Abdelrahman Abdallah,Mohammed Ali,Muhammad Abdul-Mageed,Adam Jatowt*

Main category: cs.IR

TL;DR: 介绍了TEMPO，这是首个结合了时间推理与密集推理检索的基准测试，旨在解决现实世界中需要跨时间段合成证据和进行时间演变推理的信息需求。


<details>
  <summary>Details</summary>
Motivation: 现有的时间QA基准主要关注新闻语料中的简单事实查询，而密集推理检索基准则缺乏时间基础。但现实世界的信息需求往往要求对时间演变进行推理，并且需要跨时间段综合证据。

Method: 开发了一个名为TEMPO的新基准，它包含了1,730个复杂查询，这些查询需要深入的时间推理；设计了逐步检索计划，包括3,976个分解步骤以及每个步骤对应的金标准文档，以便于多跳评估；提出了新的时间度量指标如Temporal Coverage@k和Temporal Precision@k来衡量结果是否覆盖了所需的时间段。

Result: 通过12种检索系统的评估显示了显著挑战：最佳模型（DiVeR）仅达到32.0 NDCG@10和71.4% Temporal Coverage@10的成绩，表明在检索完整的时间证据方面存在困难。

Conclusion: TEMPO为提高检索和RAG系统中的时间推理能力提供了一个具有挑战性的基准。

Abstract: Existing temporal QA benchmarks focus on simple fact-seeking queries from news corpora, while reasoning-intensive retrieval benchmarks lack temporal grounding. However, real-world information needs often require reasoning about temporal evolution and synthesizing evidence across time periods. We introduce TEMPO, the first benchmark combining temporal reasoning with reasoning-intensive retrieval across 13 domains. TEMPO features: (1) 1,730 complex queries requiring deep temporal reasoning such as tracking changes, identifying trends, or comparing cross-period evidence; (2) step-wise retrieval planning with 3,976 decomposed steps and gold documents mapped to each step for multi-hop evaluation; and (3) novel temporal metrics including Temporal Coverage@k and Temporal Precision@k measuring whether results span required time periods. Evaluation of 12 retrieval systems reveals substantial challenges: the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4\% Temporal Coverage@10, demonstrating difficulty in retrieving temporally complete evidence. We believe TEMPO provides a challenging benchmark for improving temporal reasoning in retrieval and RAG systems. Our code and data are available at https://github.com/tempo-bench/Tempo. See also our official website: https://tempo-bench.github.io/.

</details>


### [27] [SpatCode: Rotary-based Unified Encoding Framework for Efficient Spatiotemporal Vector Retrieval](https://arxiv.org/abs/2601.09530)
*Bingde Hu,Enhao Pan,Wanjing Zhou,Yang Gao,Zunlei Feng,Hao Zhong*

Main category: cs.IR

TL;DR: 提出了一种统一的时空向量检索框架，该框架通过旋转统一编码方法、循环增量更新机制和基于加权兴趣的检索算法，实现了对时间和空间数据的有效处理，并在多个真实世界数据集上表现出优越的检索准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的时空检索方法通常依赖于外部过滤器或专门的索引以整合时间与空间约束，导致效率低下、架构复杂且在处理异构模态时灵活性有限。为了解决这些问题，本文提出了一个能够同时整合时间、空间和语义线索的统一时空向量检索框架。

Method: 1. 旋转统一编码方法：将时间和位置嵌入到旋转位置向量中，用于一致的时空表示；2. 循环增量更新机制：支持高效的滑动窗口更新而无需全局重新编码或重建索引；3. 基于加权兴趣的检索算法：根据上下文自适应地平衡模态权重，实现个性化检索。

Result: 广泛的实验表明，本框架在检索准确性和效率方面显著优于最先进基准模型，同时在动态数据演变下保持了鲁棒性。

Conclusion: 所提出的统一时空向量检索框架不仅有效提升了检索性能，而且证明了其在智能系统中进行可扩展时空信息检索方面的实用价值。

Abstract: Spatiotemporal vector retrieval has emerged as a critical paradigm in modern information retrieval, enabling efficient access to massive, heterogeneous data that evolve over both time and space. However, existing spatiotemporal retrieval methods are often extensions of conventional vector search systems that rely on external filters or specialized indices to incorporate temporal and spatial constraints, leading to inefficiency, architectural complexity, and limited flexibility in handling heterogeneous modalities. To overcome these challenges, we present a unified spatiotemporal vector retrieval framework that integrates temporal, spatial, and semantic cues within a coherent similarity space while maintaining scalability and adaptability to continuous data streams. Specifically, we propose (1) a Rotary-based Unified Encoding Method that embeds time and location into rotational position vectors for consistent spatiotemporal representation; (2) a Circular Incremental Update Mechanism that supports efficient sliding-window updates without global re-encoding or index reconstruction; and (3) a Weighted Interest-based Retrieval Algorithm that adaptively balances modality weights for context-aware and personalized retrieval. Extensive experiments across multiple real-world datasets demonstrate that our framework substantially outperforms state-of-the-art baselines in both retrieval accuracy and efficiency, while maintaining robustness under dynamic data evolution. These results highlight the effectiveness and practicality of the proposed approach for scalable spatiotemporal information retrieval in intelligent systems.

</details>


### [28] [Examining DOM Coordinate Effectiveness For Page Segmentation](https://arxiv.org/abs/2601.09543)
*Jason Carpenter,Faaiq Bilal,Eman Ramadan,Zhi-Li Zhang*

Main category: cs.IR

TL;DR: 该研究探讨了DOM坐标在网页分割中的影响，发现没有一种通用的向量适合所有情况，并且视觉坐标的表现不如DOM坐标。简单向量比复杂向量表现更好。通过正确匹配向量、聚类算法和页面，可以显著提高分割准确性。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的搜索和学习系统的兴起，网页成为宝贵的数据来源。然而，数据规模庞大且格式非结构化，这对自动化提取和检索机制提出了更高要求。现有工作通常从DOM（文档对象模型）中提取坐标来构建聚类向量，但这些向量的具体构成及其价值往往未被深入研究。

Method: 本研究提出并详细考察了DOM坐标对网页分割的影响，比较了不同类型坐标（如视觉坐标与DOM坐标）以及不同复杂度向量的表现，并探索了如何通过恰当匹配向量、聚类算法及页面以优化分割效果。

Result: 研究结果表明，视觉坐标的性能平均比DOM坐标低20-30%；简单向量比复杂向量更有效，占据了所分析页面中表现最好的向量的68.2%；当向量、聚类算法和页面得到适当匹配时，整体分割准确率可达74%，比直接应用向量提高了20%。

Conclusion: 研究结论挑战了当前关于分割向量创建的传统观念，指出了通过基于DOM坐标的聚类来优化页面分割的可能性，并强调了寻找最佳方法以实现网页高效分割的重要性。

Abstract: Web pages form a cornerstone of available data for daily human consumption and with the rise of LLM-based search and learning systems a treasure trove of valuable data. The scale of this data and its unstructured format still continue to grow requiring ever more robust automated extraction and retrieval mechanisms. Existing work, leveraging the web pages Document Object Model (DOM), often derives clustering vectors from coordinates informed by the DOM such as visual placement or tree structure. The construction and component value of these vectors often go unexamined. Our work proposes and examines DOM coordinates in a detail to understand their impact on web page segmentation. Our work finds that there is no one-size-fits-all vector, and that visual coordinates under-perform compared to DOM coordinates by about 20-30% on average. This challenges the necessity of including visual coordinates in clustering vectors. Further, our work finds that simple vectors, comprised of single coordinates, fare better than complex vectors constituting 68.2% of the top performing vectors of the pages examined. Finally, we find that if a vector, clustering algorithm, and page are properly matched, one can achieve overall high segmentation accuracy at 74%. This constitutes a 20% improvement over a naive application of vectors. Conclusively, our results challenge the current orthodoxy for segmentation vector creation, opens up the possibility to optimize page segmentation via clustering on DOM coordinates, and highlights the importance of finding mechanisms to match the best approach for web page segmentation.

</details>


### [29] [MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09562)
*Abdelrahman Abdallah,Mohamed Darwish Mounis,Mahmoud Abdalla,Mahmoud SalahEldin Kasem,Mostafa Farouk Senussi,Mohamed Mahmoud,Mohammed Ali,Adam Jatowt,Hyun-Soo Kang*

Main category: cs.IR

TL;DR: 本文介绍了一个名为MM-BRIGHT的新基准，用于评估需要密集推理的多模态检索任务。该数据集包含2,803个来自29个不同技术领域的实际查询，并设置了四个逐渐增加复杂度的任务。当前最先进的模型在所有任务上都表现不佳，显示出改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的检索基准主要基于文本查询，而许多现实世界中的查询包含了图像等多模态元素，这些元素往往需要更复杂的推理来确定相关文档。为了解决这一差距，作者引入了MM-BRIGHT，这是第一个针对需要密集推理的多模态检索的基准。

Method: 创建了一个包含2,803个跨29个不同技术领域的真实世界查询的数据集，并定义了四个复杂度递增的任务：文本到文本、多模态到文本、多模态到图像以及多模态到多模态检索。通过这个新基准对现有最先进模型进行了全面评估。

Result: 实验结果表明，无论是传统的BM25还是最新的多模态模型Nomic-Vision，在处理MM-BRIGHT中提出的任务时都面临挑战，尤其是后者在多模态到文本检索上的表现甚至不如某些纯文本模型。

Conclusion: MM-BRIGHT揭示了当前多模态检索模型存在的局限性，并为开发能够更好地整合视觉推理能力的新一代检索系统提供了测试平台。

Abstract: Existing retrieval benchmarks primarily consist of text-based queries where keyword or semantic matching is usually sufficient. Many real-world queries contain multimodal elements, particularly, images such as diagrams, charts, and screenshots that require intensive reasoning to identify relevant documents. To address this gap, we introduce MM-BRIGHT, the first multimodal benchmark for reasoning-intensive retrieval. Our dataset consists of 2,803 real-world queries spanning 29 diverse technical domains, with four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. Extensive evaluation reveals that state-of-the-art models struggle across all tasks: BM25 achieves only 8.5 nDCG@10 on text-only retrieval, while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval actually underperforming the best text-only model (DiVeR: 32.2). These results highlight substantial headroom and position MM-BRIGHT as a testbed for next-generation retrieval models that better integrate visual reasoning. Our code and data are available at https://github.com/mm-bright/MM-BRIGHT. See also our official website: https://mm-bright.github.io/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [30] [TiInsight: A SQL-based Automated Exploratory Data Analysis System through Large Language Models](https://arxiv.org/abs/2601.09404)
*Jun-Peng Zhu,Boyan Niu,Peng Cai,Zheming Ni,Kai Xu,Jiajun Huang,Shengbo Ma,Bing Wang,Xuan Zhou,Guanglei Bao,Donghui Zhang,Liu Tang,Qi Liu*

Main category: cs.DB

TL;DR: 介绍了TiInsight，一个基于SQL的自动化跨领域探索性数据分析系统，它通过自然语言查询、强大的跨领域分析流程以及数据可视化等功能，实现了从手动到自动的数据探索转变。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常缺乏跨领域分析的能力，并且对大型语言模型（LLMs）能力的探索还不充分。

Method: TiInsight提供了一个用户友好的图形界面来使用自然语言查询探索数据；同时，它还提供了一个稳健的跨领域探索性数据分析流程，包括分层数据上下文生成、问题澄清与分解、文本转SQL（即TiSQL），以及数据可视化（即TiChart）。

Result: 已经在PingCAP的生产环境中实现并部署了TiInsight，并使用代表性数据集展示了其功能。

Conclusion: TiInsight代表了一种新的自动化跨领域探索性数据分析解决方案，它结合了自然语言处理和SQL技术来提高数据探索过程的效率。

Abstract: The SQL-based exploratory data analysis has garnered significant attention within the data analysis community. The emergence of large language models (LLMs) has facilitated the paradigm shift from manual to automated data exploration. However, existing methods generally lack the ability for cross-domain analysis, and the exploration of LLMs capabilities remains insufficient. This paper presents TiInsight, an SQL-based automated cross-domain exploratory data analysis system. First, TiInsight offers a user-friendly GUI enabling users to explore data using natural language queries. Second, TiInsight offers a robust cross-domain exploratory data analysis pipeline: hierarchical data context (i.e., HDC) generation, question clarification and decomposition, text-to-SQL (i.e., TiSQL), and data visualization (i.e., TiChart). Third, we have implemented and deployed TiInsight in the production environment of PingCAP and demonstrated its capabilities using representative datasets. The demo video is available at https://youtu.be/JzYFyYd-emI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models](https://arxiv.org/abs/2601.08893)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 本文提出了一种名为谱生成流模型(SGFMs)的新型方法，作为基于变压器的大语言模型的替代方案。SGFMs通过在多尺度小波基中的约束随机动力学来处理文本或视频的生成，而不是将其视为由注意力机制处理的离散令牌序列。该方法提供了三个关键创新点：一个将文本和视频统一为随机偏微分方程轨迹的场论本体、提供稀疏性与计算效率的小波域表示以及确保稳定性和一致性的约束随机流。


<details>
  <summary>Details</summary>
Motivation: 研究者旨在开发一种新的生成模型架构，它能够克服现有基于变压器的大语言模型的一些局限性，特别是通过引入物理结构化的归纳偏差来实现长距离一致性、多模态通用性和物理结构化。

Method: 提出了谱生成流模型（SGFM），这是一种受到物理学启发的方法，用于代替传统的基于变压器的语言模型。SGFM利用连续场的概念，并通过多尺度小波基上的约束随机动力学来描述生成过程。

Result: SGFMs展示了在生成过程中引入物理原理如局部操作符、频谱投影及类似Navier-Stokes流动的好处，从而提供了一种新的生成机制，该机制根植于连续性、几何学和物理结构之中。

Conclusion: SGFMs作为一种新颖的生成模型框架，从根本上区别于自回归建模和基于扩散的方法，为下一代生成模型带来了长距离连贯性、多模式通用性和物理结构化的归纳偏差。

Abstract: We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure.
  Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.

</details>


### [32] [XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation](https://arxiv.org/abs/2601.08896)
*Sahaj Raj Malla,Shreeyash Kayastha,Rumi Suwal,Harish Chandra Bhandari,Rajendra Adhikari*

Main category: cs.LG

TL;DR: 本研究开发了一种使用XGBoost回归器预测尼泊尔股市指数（NEPSE）日对数收益率的稳健机器学习框架。通过综合特征集，包括滞后对数收益率和技术指标，并采用Optuna进行超参数优化。在多种滞后配置下，通过向前滚动验证评估了模型性能，最优配置为20个滞后的扩展窗口，其表现优于ARIMA和Ridge回归基准模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种能够有效预测新兴市场中波动性较强的时间序列非线性动态特性的机器学习方法。特别地，它关注于提高尼泊尔股市指数（NEPSE）的日对数收益率预测准确性。

Method: 构建了一个包含滞后对数收益率以及技术指标如短期与中期滚动波动率度量、14周期相对强弱指数等在内的全面特征集；利用Optuna结合时间序列交叉验证执行超参数调优；并通过固定长度滚动窗口和扩展窗口方案下的向前滚动验证来严格评估模型的外样本性能。

Result: 实验结果表明，在所有测试条件下，具有20个滞后项的扩展窗口配置表现最佳，其对数收益率RMSE为0.013450，MAE为0.009814，方向准确率为65.15%，优于经过调整的ARIMA和Ridge回归基线模型。

Conclusion: 梯度提升集成方法在处理新兴市场的高波动性时间序列数据时展现出有效性，为NEPSE指数预测提供了一个可重复使用的基准。尽管R平方值较低，但该模型在减少相对误差和方向性预测方面表现良好。

Abstract: This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling window schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R-squared), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration, an expanding window with 20 lags, outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R-squared remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.

</details>


### [33] [DriftGuard: A Hierarchical Framework for Concept Drift Detection and Remediation in Supply Chain Forecasting](https://arxiv.org/abs/2601.08928)
*Shahnawaz Alam,Mohammed Abdul Rahman,Bareera Sadeqa*

Main category: cs.LG

TL;DR: DriftGuard is a comprehensive system designed to address the issue of concept drift in supply chain forecasting models, offering early detection, root cause analysis, and automatic correction. It combines multiple detection methods with hierarchical propagation analysis, uses SHAP for diagnosis, and applies a cost-aware retraining strategy. The system demonstrates high detection recall and significant return on investment.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper stems from the challenge that supply chain forecasting models face due to concept drift, which leads to their silent degradation over time. This degradation results in stockouts or excess inventory without setting off any system alerts. Traditional industry practices are not efficient, as they either waste resources during stable periods or fail to catch rapid changes. Moreover, existing academic solutions only focus on detecting drift but do not provide a full solution including diagnosis and correction, nor do they consider the hierarchical structure of supply chain data. Retailers require an all-inclusive system capable of early drift detection, root cause explanation, and automatic model adjustments.

Method: To tackle the aforementioned issues, the authors propose DriftGuard, a five-module framework. This system integrates four different detection approaches - error-based monitoring, statistical tests, autoencoder anomaly detection, and CUSUM change-point analysis - to identify drifts across product lines through hierarchical propagation analysis. After detecting drift, it employs SHAP (Shapley Additive Explanations) to diagnose the underlying causes. A cost-sensitive retraining approach is then applied to selectively update only those models most impacted by the drift, ensuring efficient use of computational resources.

Result: Evaluations conducted on more than 30,000 time series extracted from the M5 retail dataset show that DriftGuard can achieve a 97.8% detection recall rate within just 4.2 days. Additionally, by implementing targeted remediation, the system can generate up to a 417 times return on investment, highlighting its effectiveness and economic value in managing concept drift in supply chain forecasting.

Conclusion: In conclusion, DriftGuard represents a significant advancement in dealing with concept drift in supply chain forecasting. By providing a complete solution that includes early detection, root cause analysis, and automated corrective actions, it addresses the limitations of both current industry practices and previous academic research. The system's ability to deliver high detection accuracy coupled with substantial financial benefits makes it a promising tool for retailers looking to improve their supply chain management.

Abstract: Supply chain forecasting models degrade over time as real-world conditions change. Promotions shift, consumer preferences evolve, and supply disruptions alter demand patterns, causing what is known as concept drift. This silent degradation leads to stockouts or excess inventory without triggering any system warnings. Current industry practice relies on manual monitoring and scheduled retraining every 3-6 months, which wastes computational resources during stable periods while missing rapid drift events. Existing academic methods focus narrowly on drift detection without addressing diagnosis or remediation, and they ignore the hierarchical structure inherent in supply chain data. What retailers need is an end-to-end system that detects drift early, explains its root causes, and automatically corrects affected models. We propose DriftGuard, a five-module framework that addresses the complete drift lifecycle. The system combines an ensemble of four complementary detection methods, namely error-based monitoring, statistical tests, autoencoder anomaly detection, and Cumulative Sum (CUSUM) change-point analysis, with hierarchical propagation analysis to identify exactly where drift occurs across product lines. Once detected, Shapley Additive Explanations (SHAP) analysis diagnoses the root causes, and a cost-aware retraining strategy selectively updates only the most affected models. Evaluated on over 30,000 time series from the M5 retail dataset, DriftGuard achieves 97.8% detection recall within 4.2 days and delivers up to 417 return on investment through targeted remediation.

</details>


### [34] [DP-FEDSOFIM: Differentially Private Federated Stochastic Optimization using Regularized Fisher Information Matrix](https://arxiv.org/abs/2601.09166)
*Sidhant R. Nair,Tanmay Sen,Mrinmay Sen*

Main category: cs.LG

TL;DR: 提出了一种名为DP-FedSOFIM的服务器端二阶优化框架，它利用Fisher信息矩阵作为自然梯度预处理器，在每个客户端仅需O(d)内存，通过CIFAR-10数据集实证评估表明，与一阶基线相比，DP-FedSOFIM在多种隐私条件下达到了更高的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 差分私有的联邦学习(DP-FL)在严格的隐私预算下由于引入了大量的噪声以保护隐私而导致收敛缓慢。虽然自适应优化器可以加速收敛，但现有的如DP-FedNew这样的二阶方法要求每个客户端保持O(d^2)内存来维护本地特征协方差矩阵，这使得它们对于高维模型来说是不切实际的。

Method: 提出了DP-FedSOFIM，一种服务器端的二阶优化框架，使用Fisher信息矩阵(FIM)作为自然梯度预处理条件，并且只需要每个客户端O(d)内存。通过应用Sherman-Morrison公式进行有效的矩阵求逆，DP-FedSOFIM每轮实现了O(d)计算复杂度的同时保持了二阶方法的收敛优势。分析证明了服务器端预处理通过后处理定理保持(epsilon, delta)-差分隐私。

Result: 基于CIFAR-10数据集的实证评估显示，在多个隐私制度下，DP-FedSOFIM相较于一阶基线方法实现了更好的测试准确率。

Conclusion: DP-FedSOFIM为解决差分私有联邦学习中收敛速度慢的问题提供了一个有效的方法，尤其适用于高维模型场景，同时保持了良好的隐私保护性能。

Abstract: Differentially private federated learning (DP-FL) suffers from slow convergence under tight privacy budgets due to the overwhelming noise introduced to preserve privacy. While adaptive optimizers can accelerate convergence, existing second-order methods such as DP-FedNew require O(d^2) memory at each client to maintain local feature covariance matrices, making them impractical for high-dimensional models. We propose DP-FedSOFIM, a server-side second-order optimization framework that leverages the Fisher Information Matrix (FIM) as a natural gradient preconditioner while requiring only O(d) memory per client. By employing the Sherman-Morrison formula for efficient matrix inversion, DP-FedSOFIM achieves O(d) computational complexity per round while maintaining the convergence benefits of second-order methods. Our analysis proves that the server-side preconditioning preserves (epsilon, delta)-differential privacy through the post-processing theorem. Empirical evaluation on CIFAR-10 demonstrates that DP-FedSOFIM achieves superior test accuracy compared to first-order baselines across multiple privacy regimes.

</details>


### [35] [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)
*Adrita Das,Peiran Jiang,Dantong Zhu,Barnabas Poczos,Jose Lugo-Martinez*

Main category: cs.LG

TL;DR: 本文通过Reverse Transition Kernel框架对直接去噪扩散模型(DDDM)进行了理论解释，阐明了确定性去噪过程如何实现高效推理，并解决了分子扩散中的多个瓶颈问题。实验结果表明，基于RTK的确定性去噪方法比随机扩散模型收敛更快、结构保真度更高，同时保持了化学有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在分子设计中表现出色，但其应用受到长采样轨迹、反向过程中随机变化以及去噪动力学中有限的结构感知能力限制。直接去噪扩散模型(DDDM)通过用确定性的去噪步骤代替随机反向MCMC更新来缓解这些问题，显著减少了推理时间。然而，这种确定性更新背后的理论基础尚未清晰。

Method: 本文采用了Reverse Transition Kernel (RTK)框架重新解读了DDDM，将确定性和随机扩散统一在一个共享的概率形式下。通过将DDDM反向过程表示为近似的核操作符，展示了直接去噪过程实际上是在优化噪声样本与干净样本之间的一种结构化传输映射。此外，RTK视角保证了数值稳定性、提高了样本一致性并支持可扩展且保持SE(3)等变性的去噪器。

Result: 实验证明，在GEOM-DRUGS数据集上，基于RTK指导下的确定性去噪方法相较于随机扩散模型能够达到更快的收敛速度和更高的结构保真度，同时还保持了化学有效性。

Conclusion: 通过RTK框架对DDDM进行的理论重解释不仅提供了更清晰的理解，还解决了分子扩散领域内的一些长期存在的瓶颈问题。这种方法不仅提高了效率和准确性，而且对于促进扩散模型在分子设计领域的应用具有重要意义。

Abstract: Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.

</details>


### [36] [Continuous Fairness On Data Streams](https://arxiv.org/abs/2601.08976)
*Subhodeep Ghosh,Zhihui Du,Angela Bonifati,Manish Kumar,David Bader,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: 本文提出了一种新的公平性模型，用于在数据流的滑动窗口中实现更细粒度（称为块）的组公平性，并设计了基于草图的数据结构来实现实时监控，同时开发了具有理论保证的有效重排序算法。实验表明，该方法处理时间达到毫秒级，每秒查询处理量约为30,000次，在某些情况下提高了高达95%的块级别组公平性。


<details>
  <summary>Details</summary>
Motivation: 针对数据流中的连续组公平性问题，特别是当窗口尺寸较大时，在整个窗口内维持一种更加细致级别的公平性显得尤为重要。

Method: 提出了一个新颖的公平模型，确保每个滑动窗口内的细粒度层次（即块）上的群体公平性；设计了基于概要的数据结构以最小开销维护属性分布；开发了最优且高效的重新排序算法，并有严格的理论保证支持。

Result: 在四个真实世界的数据流场景上进行了评估，展示了其实用效果。实现了毫秒级处理速度和平均约30,000次/秒的吞吐量。流重组算法在某些情况下可将块级群组公平性提高至多95%，平均而言跨数据集提升了50-60%。

Conclusion: 通过引入块级公平的概念及相应的监测与调整机制，本研究为数据流环境下实现持续性的组公平提供了有效解决方案。

Abstract: We study the problem of enforcing continuous group fairness over windows in data streams. We propose a novel fairness model that ensures group fairness at a finer granularity level (referred to as block) within each sliding window. This formulation is particularly useful when the window size is large, making it desirable to enforce fairness at a finer granularity. Within this framework, we address two key challenges: efficiently monitoring whether each sliding window satisfies block-level group fairness, and reordering the current window as effectively as possible when fairness is violated. To enable real-time monitoring, we design sketch-based data structures that maintain attribute distributions with minimal overhead. We also develop optimal, efficient algorithms for the reordering task, supported by rigorous theoretical guarantees. Our evaluation on four real-world streaming scenarios demonstrates the practical effectiveness of our approach. We achieve millisecond-level processing and a throughput of approximately 30,000 queries per second on average, depending on system parameters. The stream reordering algorithm improves block-level group fairness by up to 95% in certain cases, and by 50-60% on average across datasets. A qualitative study further highlights the advantages of block-level fairness compared to window-level fairness.

</details>


### [37] [Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction](https://arxiv.org/abs/2601.08999)
*Pranjal Patil,Anli Ji,Berkay Aydin*

Main category: cs.LG

TL;DR: 本文提出了一种物理引导的反事实解释框架，用于生成时间序列分类任务中的反事实解释，这些解释不仅与基本物理原理保持一致，而且在应用于太阳高能粒子预测时，能够显著减少动态时间扭曲距离、提高稀疏性，并几乎将运行时间减少50%。


<details>
  <summary>Details</summary>
Motivation: 准确预测太阳高能粒子事件对于保护卫星、宇航员和基于空间的基础设施至关重要。虽然机器学习模型在处理来自如GOES等来源的大规模高频多变量时间序列数据方面表现出强大的预测能力，但大多数现有方法忽略了领域特定的可行性约束条件，同时现有的反事实解释方法很少考虑物理合理性。

Method: 本文介绍了一种物理引导的反事实解释框架，这是一种新颖的方法，旨在为时间序列分类任务生成与基础物理原理相一致的反事实解释。

Result: 当应用于太阳高能粒子（SEP）预测时，该框架实现了超过80%的动态时间扭曲（DTW）距离减少，提高了接近度；生成了更高稀疏性的反事实解释，并且相比最先进的基线如DiCE，运行时间减少了近50%。

Conclusion: 除了数值上的改进外，该框架确保生成的反事实解释在科学领域内是物理上合理的并且可执行的，同时也为大数据环境下的可扩展反事实生成奠定了基础。

Abstract: Accurate prediction of solar energetic particle events is vital for safeguarding satellites, astronauts, and space-based infrastructure. Modern space weather monitoring generates massive volumes of high-frequency, multivariate time series (MVTS) data from sources such as the Geostationary perational Environmental Satellites (GOES). Machine learning (ML) models trained on this data show strong predictive power, but most existing methods overlook domain-specific feasibility constraints. Counterfactual explanations have emerged as a key tool for improving model interpretability, yet existing approaches rarely enforce physical plausibility. This work introduces a Physics-Guided Counterfactual Explanation framework, a novel method for generating counterfactual explanations in time series classification tasks that remain consistent with underlying physical principles. Applied to solar energetic particles (SEP) forecasting, this framework achieves over 80% reduction in Dynamic Time Warping (DTW) distance increasing the proximity, produces counterfactual explanations with higher sparsity, and reduces runtime by nearly 50% compared to state-of-the-art baselines such as DiCE. Beyond numerical improvements, this framework ensures that generated counterfactual explanations are physically plausible and actionable in scientific domains. In summary, the framework generates counterfactual explanations that are both valid and physically consistent, while laying the foundation for scalable counterfactual generation in big data environments.

</details>


### [38] [Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers](https://arxiv.org/abs/2601.09000)
*Annalisa Belloni,Lorenzo Noci,Antonio Orvieto*

Main category: cs.LG

TL;DR: 研究了Warmup Stable Decay (WSD)学习率调度器在不同模型上的表现，发现其对于旧的和新的非凸问题损失景观几何特性上存在一致性。


<details>
  <summary>Details</summary>
Motivation: 探讨Warmup Stable Decay (WSD)学习率调度器是否仅对基于transformer的语言模型表现出色，还是也能为理解其他类型模型（如CNN）训练动态提供见解。

Method: 比较了Adam优化器使用WSD学习率调度器在类似Pythia的语言模型与小型CNN（用于CIFAR10图像分类）上的训练路径、优化器路径特征及锐度动态。

Result: 观察到在这两种架构中，大多数训练信号、优化器路径特征以及锐度动态在定性上是相似的。

Conclusion: 这种一致性表明，无论是传统的还是新兴的非凸问题，在损失景观几何特性方面存在着共同点，并提示未来围绕高维优化问题几何学的研究方向。

Abstract: The Warmup Stable Decay (WSD) learning rate scheduler has recently become popular, largely due to its good performance and flexibility when training large language models. It remains an open question whether the remarkable performance of WSD - using a decaying learning rate for only a fraction of training compared to cosine decay - is a phenomenon specific to transformer-based language models that can potentially offer new theoretical insights into their training dynamics. Inspired by the usage of learning rate schedulers as a new lens into understanding landscape geometry (e.g., river valley, connected minima, progressive sharpening), in this work we compare the WSD path of the Adam optimizer on a Pythia-like language model to that of a small CNN trained to classify CIFAR10 images. We observe most training signals, optimizer path features, and sharpness dynamics to be qualitatively similar in such architectures. This consistency points to shared geometric characteristics of the loss landscapes of old and new nonconvex problems, and hints to future research questions around the geometry of high dimensional optimization problems.

</details>


### [39] [Meta-learning to Address Data Shift in Time Series Classification](https://arxiv.org/abs/2601.09018)
*Samuel Myren,Nidhi Parikh,Natalie Klein*

Main category: cs.LG

TL;DR: 本文系统地比较了传统深度学习（TDL）与基于元学习的算法在处理时间序列分类中的数据偏移问题时的表现。通过引入SeisTask基准测试，研究表明元学习方法通常能够更快、更稳定地适应新数据，并且在数据稀缺和模型架构较小的情况下表现更好。随着数据可用性和模型容量增加，TDL结合微调的表现可与之媲美。此外，研究还发现训练和测试分布的一致性是推动性能提升的关键因素。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，由于数据偏移现象的存在，传统深度学习模型在遇到分布变化的数据时表现不佳，需要频繁重新标记和训练。元学习作为一种快速适应少量新样本的方法，为解决这些问题提供了可能。

Method: 作者通过创建一个专门针对地震数据的时间序列分类任务基准(SeisTask)，并在此基础上对比了传统深度学习（包括微调）与基于优化的元学习算法应对数据偏移的能力。

Result: 实验结果表明，在数据量较少或模型较小时，元学习方法能够实现更快且更稳定的适应过程，同时减少过拟合现象；但当有足够的数据和更大的模型时，经过微调的传统深度学习方法也能达到相似的效果。此外，研究指出任务多样性对元学习的影响在于训练与测试间分布一致性的重要性。

Conclusion: 本研究为理解何时以及为什么元学习能在数据偏移情况下优于传统深度学习提供了系统的评估，并提出了SeisTask作为促进时间序列领域自适应学习研究的新基准。

Abstract: Across engineering and scientific domains, traditional deep learning (TDL) models perform well when training and test data share the same distribution. However, the dynamic nature of real-world data, broadly termed \textit{data shift}, renders TDL models prone to rapid performance degradation, requiring costly relabeling and inefficient retraining. Meta-learning, which enables models to adapt quickly to new data with few examples, offers a promising alternative for mitigating these challenges. Here, we systematically compare TDL with fine-tuning and optimization-based meta-learning algorithms to assess their ability to address data shift in time-series classification. We introduce a controlled, task-oriented seismic benchmark (SeisTask) and show that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. As data availability and model capacity increase, its advantages diminish, with TDL with fine-tuning performing comparably. Finally, we examine how task diversity influences meta-learning and find that alignment between training and test distributions, rather than diversity alone, drives performance gains. Overall, this work provides a systematic evaluation of when and why meta-learning outperforms TDL under data shift and contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.

</details>


### [40] [SCaLE: Switching Cost aware Learning and Exploration](https://arxiv.org/abs/2601.09042)
*Neelkamal Bhuyan,Debankur Mukherjee,Adam Wierman*

Main category: cs.LG

TL;DR: 本文针对在线凸优化中无界度量移动成本的问题，提出了首个无需了解命中成本结构即可实现次线性动态遗憾的算法SCaLE，并通过数值实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在线凸优化中存在的无界度量移动成本问题，特别是在高维度动态二次命中成本和噪声反馈模型中的L2范数切换成本。

Method: 提出了一种名为SCaLE的新算法，该算法能够在不依赖于具体分布信息的情况下处理未知的命中成本结构，并引入了一种新的谱后悔分析方法来量化特征值误差驱动的遗憾和特征基扰动驱动的遗憾。

Result: SCaLE算法在随机环境中实现了次线性的动态遗憾，且不需要事先知道命中成本的具体结构。此外，通过广泛的数值实验与在线学习基准对比，证明了算法的有效性和统计一致性。

Conclusion: 本研究为在线凸优化领域提供了处理未知成本结构下移动成本问题的新方法，并通过理论分析和实验证明了所提算法的有效性。

Abstract: This work addresses the fundamental problem of unbounded metric movement costs in bandit online convex optimization, by considering high-dimensional dynamic quadratic hitting costs and $\ell_2$-norm switching costs in a noisy bandit feedback model. For a general class of stochastic environments, we provide the first algorithm SCaLE that provably achieves a distribution-agnostic sub-linear dynamic regret, without the knowledge of hitting cost structure. En-route, we present a novel spectral regret analysis that separately quantifies eigenvalue-error driven regret and eigenbasis-perturbation driven regret. Extensive numerical experiments, against online-learning baselines, corroborate our claims, and highlight statistical consistency of our algorithm.

</details>


### [41] [MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting](https://arxiv.org/abs/2601.09085)
*Kangda Wei,Ruihong Huang*

Main category: cs.LG

TL;DR: 本文提出了MMR-GRPO，一种结合最大边际相关性来基于完成多样性重新分配奖励的方法。通过减少语义冗余的完成带来的有限边际学习信号，并优先考虑多样化的解决方案，该方法能够提供更具有信息量的更新并加速收敛过程。实验表明，MMR-GRPO在达到类似峰值性能的同时，平均需要少47.9%的训练步骤和节省70.2%的实际训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法虽然减少了达到峰值性能所需的训练步骤数量，但由于每步成本增加，导致整体实际训练时间没有减少甚至增加。为了解决这个问题，作者提出了一种新的方法，旨在通过优化奖励机制来提高训练效率。

Method: MMR-GRPO利用了最大边际相关性(Maximal Marginal Relevance)的概念来重新评估每个完成(completion)的重要性。这种方法假设语义上重复的完成对模型学习的帮助较小，因此应该给那些能提供更多样化答案的完成更高的权重。这样做的目的是为了产生更具信息价值的学习更新，从而加快整个训练过程的收敛速度。

Result: 通过对三个不同规模（1.5B, 7B, 8B）的模型、三种GRPO变体以及五个数学推理基准测试进行广泛评估后发现，MMR-GRPO不仅能够达到与传统方法相当的最佳性能水平，而且平均而言只需要较少47.9%的训练步骤，并且实际所需的时间减少了70.2%。这种改进的效果对于不同的模型、方法和评测标准都是一致的。

Conclusion: 研究结果表明，通过引入MMR-GRPO方法，可以显著降低训练数学推理模型的成本而不牺牲最终性能。这为未来开发更高效的人工智能训练策略提供了新的思路。

Abstract: Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.

</details>


### [42] [Deep Incomplete Multi-View Clustering via Hierarchical Imputation and Alignment](https://arxiv.org/abs/2601.09051)
*Yiming Du,Ziyu Wang,Jian Li,Rui Ning,Lusi Li*

Main category: cs.LG

TL;DR: 提出了一种新的深度不完全多视图聚类框架DIMVC-HIA，通过整合层次插补与对齐等四个关键组件解决了缺失视图准确插补而不引入偏差的问题，同时保持了视图间的语义一致性和簇内的紧凑性。


<details>
  <summary>Details</summary>
Motivation: 不完全多视图聚类（IMVC）旨在从未完全观测的多视图数据中发现共享的簇结构。其核心挑战在于准确地填补缺失视图而不引入偏见，同时保持跨视图的语义一致性以及簇内紧凑性。

Method: 提出了DIMVC-HIA框架，它包含四个主要部分：1) 视图特定自动编码器用于潜在特征提取，并结合视图共享聚类预测器生成软聚类分配；2) 层次插补模块首先基于跨视图对比相似性估计缺失的聚类分配，然后使用视图内、簇内统计重构缺失特征；3) 基于能量的语义对齐模块通过最小化低能量簇锚点周围的能量方差来促进簇内紧凑性；4) 对比分配对齐模块增强跨视图一致性并鼓励自信且分离良好的聚类预测。

Result: 在基准测试中的实验表明，该框架在不同程度的缺失情况下均实现了优越的表现。

Conclusion: DIMVC-HIA框架有效地解决了不完全多视图数据下的聚类问题，通过创新性的层次插补与对齐机制不仅提高了对于缺失值处理的准确性，而且增强了跨视图间的一致性和簇内的紧凑性，从而在多种条件下都展现出了优秀的性能。

Abstract: Incomplete multi-view clustering (IMVC) aims to discover shared cluster structures from multi-view data with partial observations. The core challenges lie in accurately imputing missing views without introducing bias, while maintaining semantic consistency across views and compactness within clusters. To address these challenges, we propose DIMVC-HIA, a novel deep IMVC framework that integrates hierarchical imputation and alignment with four key components: (1) view-specific autoencoders for latent feature extraction, coupled with a view-shared clustering predictor to produce soft cluster assignments; (2) a hierarchical imputation module that first estimates missing cluster assignments based on cross-view contrastive similarity, and then reconstructs missing features using intra-view, intra-cluster statistics; (3) an energy-based semantic alignment module, which promotes intra-cluster compactness by minimizing energy variance around low-energy cluster anchors; and (4) a contrastive assignment alignment module, which enhances cross-view consistency and encourages confident, well-separated cluster predictions. Experiments on benchmarks demonstrate that our framework achieves superior performance under varying levels of missingness.

</details>


### [43] [Resolving Predictive Multiplicity for the Rashomon Set](https://arxiv.org/abs/2601.09071)
*Parian Haghighat,Hadis Anahideh,Cynthia Rudin*

Main category: cs.LG

TL;DR: 本文提出三种方法来减少Rashomon集合中模型预测的一致性问题，包括异常值修正、局部修补和成对调解。实验表明这些方法在保持竞争力的准确性的同时减少了分歧指标。


<details>
  <summary>Details</summary>
Motivation: 在给定的预测任务中存在多个同样准确的模型会导致预测多样性的问题，即一组‘罗生门’模型虽然达到相似的准确性但它们的个体预测结果不同。这种不一致性在高风险应用中削弱了我们对一致预测的信任。

Method: 1. 异常值修正：识别并修正那些好模型都无法正确预测其标签的数据点，以降低局部区域内的预测方差。
2. 局部修补：检测并修正模型在测试点附近区域内因偏见而产生的分歧，通过使用验证集来实现。
3. 成对调解：找到在测试点周围区域意见不一致的模型对，并调整那些不一致的预测，使之变得不那么有偏见。

Result: 实验显示，所提出的方法能够有效降低不同模型之间的分歧度量，同时保持了具有竞争力的预测准确性。

Conclusion: 结合或单独使用这三种方法可以有效地减少预测多样性问题，最终可以将调和后的预测整合为一个可解释性强的单一模型，适用于实际部署。

Abstract: The existence of multiple, equally accurate models for a given predictive task leads to predictive multiplicity, where a ``Rashomon set'' of models achieve similar accuracy but diverges in their individual predictions. This inconsistency undermines trust in high-stakes applications where we want consistent predictions. We propose three approaches to reduce inconsistency among predictions for the members of the Rashomon set. The first approach is \textbf{outlier correction}. An outlier has a label that none of the good models are capable of predicting correctly. Outliers can cause the Rashomon set to have high variance predictions in a local area, so fixing them can lower variance. Our second approach is local patching. In a local region around a test point, models may disagree with each other because some of them are biased. We can detect and fix such biases using a validation set, which also reduces multiplicity. Our third approach is pairwise reconciliation, where we find pairs of models that disagree on a region around the test point. We modify predictions that disagree, making them less biased. These three approaches can be used together or separately, and they each have distinct advantages. The reconciled predictions can then be distilled into a single interpretable model for real-world deployment. In experiments across multiple datasets, our methods reduce disagreement metrics while maintaining competitive accuracy.

</details>


### [44] [SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache](https://arxiv.org/abs/2601.09083)
*Chi-Chih Chang,Siqi Zhu,Zhichen Zeng,Haibin Lin,Jiaxuan You,Mohamed S. Abdelfattah,Ziheng Jiang,Xuehai Qian*

Main category: cs.LG

TL;DR: 提出了一种名为SRT的方法，通过利用树结构缓存来加速语言模型的在线策略强化学习过程，而不会影响分布正确性。该方法能够减少生成和步长延迟，并降低每个令牌的推理成本，实现高达2.08倍的实际时间加速。


<details>
  <summary>Details</summary>
Motivation: 为了在不牺牲分布正确性的前提下加速语言模型的在线策略强化学习（RL）。

Method: 采用了一种无模型的方法——推测性展开与树结构缓存（SRT），通过为同一提示的不同训练步骤间存储之前生成的延续内容到一个基于提示的树形缓存中。使用当前策略根据这个树作为草稿模型执行推测解码。此外，SRT还会在线更新这些树以保持缓存的新鲜度，并在GPU空闲时主动进行提前生成。

Result: SRT可以持续减少生成时间和步长延迟，同时降低每个令牌的推理成本，在展开过程中实现了最高达2.08倍的实际时间提速。

Conclusion: SRT是一种简单有效的方法，用于加速语言模型上的在线策略强化学习，能够在不影响分布准确性的前提下显著提高效率。

Abstract: We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.

</details>


### [45] [Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning](https://arxiv.org/abs/2601.09088)
*Shaotian Yan,Kaiyuan Liu,Chen Shen,Bing Wang,Sinan Fan,Jun Zhang,Yue Wu,Zheng Wang,Jieping Ye*

Main category: cs.LG

TL;DR: 本文介绍了一个轻量级但功能强大的开源推理模型DASD-4B-Thinking，它在数学、科学推理和代码生成等具有挑战性的基准测试中达到了同类规模开源模型中的最优表现。文章重新审视了社区内广泛采用的蒸馏范式，并针对现有做法的三个关键局限性提出了方法创新，形成了一个增强的序列级蒸馏训练流程。值得注意的是，该模型仅使用了448K个训练样本就取得了竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进当前序列级蒸馏实践中存在的问题，包括教师模型序列级分布表示不足、教师输出分布与学生学习能力不匹配以及由教师强制训练与自回归推理引起的暴露偏差等问题。这些问题是由于缺乏明确的师生互动而造成的，导致蒸馏的本质未被充分利用。

Method: 为了解决这些问题，作者提出了一系列方法学上的创新，共同构成了一个增强型序列级蒸馏训练流水线。具体来说，通过改善教师模型序列级分布的表示、更好地对齐教师与学生的模型能力、以及减少训练与推理间差异引起的问题来实现改进。

Result: DASD-4B-Thinking模型不仅在多个具有挑战性的基准上表现出色，甚至超过了部分更大规模的模型。更重要的是，它仅用了相对较少数量（44.8万）的训练样本来达到这一成就。

Conclusion: 本研究表明，通过针对性地优化序列级知识蒸馏过程中的几个关键环节，可以显著提高小规模模型的能力。此外，公开发布的模型和训练数据集将有助于促进社区内的进一步研究。

Abstract: In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.

</details>


### [46] [Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.09093)
*Zhixiang Liang,Beichen Huang,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的剪枝框架STEP，通过评估推理步骤并动态剪枝来减少大型语言模型在生成多个推理路径时的计算量和延迟，同时提高推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的加速方法依赖于基于相似性或置信度的剪枝，但这些信号并不能可靠地指示路径质量。为了解决这个问题，并降低生成多个推理路径时带来的大量计算和高延迟问题。

Method: 设计了STEP：一个基于步级评估和剪枝的新框架，使用隐藏状态评估推理步骤，并在生成过程中动态剪枝不具前景的路径。训练了一个轻量级的步评分器来估计路径质量，并开发了一种GPU内存感知剪枝策略，在GPU内存因KV缓存饱和时触发剪枝以减少端到端延迟。

Result: 实验显示，与自一致性方法相比，STEP平均减少了45%-70%的端到端推理延迟，同时还提高了推理准确性。

Conclusion: STEP提供了一种有效的方法来加速大型语言模型的推理过程，同时保持甚至提高其推理性能。

Abstract: Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: https://github.com/Supercomputing-System-AI-Lab/STEP

</details>


### [47] [Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion](https://arxiv.org/abs/2601.09103)
*Haijian Shao,Wei Liu,Xing Deng,Daze Lu*

Main category: cs.LG

TL;DR: 本文提出了一种基于小波变换的特征融合方法，有效解决了ECG数据分类中的类别不平衡和噪声问题，显著提高了CPSC 2018数据集中ECG信号的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 不平衡的心电图（ECG）数据降低了基于深度学习的ECG分类算法的有效性和鲁棒性，尤其是对于罕见的心脏状况而言，这些状况在数据集中代表性不足。此外，ECG采集过程中的噪声也增加了处理难度。

Method: 本文提出的方法包括：1. 使用基于小波变换的特征融合技术来生成训练特征库和测试集特征库；2. 将原始训练与测试数据同其相应的特征数据库合并，以创建更加平衡的数据集。

Result: 所提出的ECG模型对正常、房颤、I度房室传导阻滞等不同类别的识别准确率分别达到了99%、98%、97%等高水平，并且平均识别准确率介于92%至98%之间。

Conclusion: 提出的数据融合方法不仅能够解决ECG数据分析中遇到的类别不平衡和噪声相关挑战，而且在CPSC 2018数据集上实现了超越现有算法的ECG分类准确度。

Abstract: Imbalanced electrocardiogram (ECG) data hampers the efficacy and resilience of algorithms in the automated processing and interpretation of cardiovascular diagnostic information, which in turn impedes deep learning-based ECG classification. Notably, certain cardiac conditions that are infrequently encountered are disproportionately underrepresented in these datasets. Although algorithmic generation and oversampling of specific ECG signal types can mitigate class skew, there is a lack of consensus regarding the effectiveness of such techniques in ECG classification. Furthermore, the methodologies and scenarios of ECG acquisition introduce noise, further complicating the processing of ECG data. This paper presents a significantly enhanced ECG classifier that simultaneously addresses both class imbalance and noise-related challenges in ECG analysis, as observed in the CPSC 2018 dataset. Specifically, we propose the application of feature fusion based on the wavelet transform, with a focus on wavelet transform-based interclass fusion, to generate the training feature library and the test set feature library. Subsequently, the original training and test data are amalgamated with their respective feature databases, resulting in more balanced training and test datasets. Employing this approach, our ECG model achieves recognition accuracies of up to 99%, 98%, 97%, 98%, 96%, 92%, and 93% for Normal, AF, I-AVB, LBBB, RBBB, PAC, PVC, STD, and STE, respectively. Furthermore, the average recognition accuracy for these categories ranges between 92\% and 98\%. Notably, our proposed data fusion methodology surpasses any known algorithms in terms of ECG classification accuracy in the CPSC 2018 dataset.

</details>


### [48] [EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge](https://arxiv.org/abs/2601.09142)
*Shijian Ma,Yan Lin,Yi Yang*

Main category: cs.LG

TL;DR: 本文介绍了一个名为EvasionBench的基准，用于检测财报电话会议中的回避性回答。通过利用多个先进语言模型之间的分歧来标注数据，提高了模型在识别困难样本上的表现。最终训练出的模型Eva-4B在准确率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 检测财报电话会议中的回避性回答对于提高财务透明度至关重要，但缺乏大规模的基准数据集阻碍了这一领域的发展。

Method: 作者提出了EvasionBench，一个包含30,000个训练样本和1,000个人工标注测试样本的数据集，并引入了一种多模型注释框架，该框架基于一种核心见解：前沿大型语言模型之间存在分歧时，往往指向那些对训练最有价值的难例。

Result: 采用此方法比单模型蒸馏方法提高了2.4个百分点的表现，尽管在训练损失上有所增加（0.421对比0.393），表明分歧挖掘起到了隐式正则化的作用。开发的模型Eva-4B达到了81.3%的准确率，比其基础模型高出25个百分点。

Conclusion: 研究表明，通过利用不同高级语言模型间的不一致来挑选并解决标签问题，可以有效增强模型识别复杂情况的能力，同时减少了推理成本。

Abstract: Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.

</details>


### [49] [Discrete Solution Operator Learning for Geometry-Dependent PDEs](https://arxiv.org/abs/2601.09143)
*Jinshuai Bai,Haolin Li,Zahra Sharif Khodaei,M. H. Aliabadi,YuanTong Gu,Xi-Qiao Feng*

Main category: cs.LG

TL;DR: 本文提出了离散解算子学习（DiSOL），一种新的范式，用于学习离散解算过程而非连续函数空间算子。DiSOL能够处理由几何变化引起的离散结构变化，并在多种几何相关问题上表现出色，包括对分布内外的几何形状均能产生稳定和准确的预测。


<details>
  <summary>Details</summary>
Motivation: 神经算子学习通过将算子近似为连续函数空间之间的映射来加速PDE求解。但在许多工程场景中，变化的几何形状会引起离散结构的变化，如拓扑变化、边界条件或类型突变以及有效计算域的变化等，这些都打破了平滑变化的前提。为此，需要一种新的方法来处理这种由于几何变化导致的离散结构变化问题。

Method: 提出了一种名为Discrete Solution Operator Learning (DiSOL)的新范式，该方法学习的是离散解算程序而不是连续函数空间算子。DiSOL将解算器分解成可学习阶段，这些阶段反映了经典离散化：局部贡献编码、多尺度组装以及嵌入网格上的隐式解重建，从而保持了程序级一致性的同时适应了几何相关的离散结构。

Result: 对于依赖于几何形状的泊松方程、对流扩散、线性弹性及时空热传导等问题，DiSOL不仅能够在分布内而且还能在强烈超出分布外的情况下提供稳定且准确的预测结果，包括不连续边界与拓扑变化的情形。

Conclusion: 研究结果强调了在以几何为主导的情境下采用程序化算子表示的重要性，并将离散解算子学习定位为科学机器学习领域内一个独特而互补的方向。

Abstract: Neural operator learning accelerates PDE solution by approximating operators as mappings between continuous function spaces. Yet in many engineering settings, varying geometry induces discrete structural changes, including topological changes, abrupt changes in boundary conditions or boundary types, and changes in the effective computational domain, which break the smooth-variation premise. Here we introduce Discrete Solution Operator Learning (DiSOL), a complementary paradigm that learns discrete solution procedures rather than continuous function-space operators. DiSOL factorizes the solver into learnable stages that mirror classical discretizations: local contribution encoding, multiscale assembly, and implicit solution reconstruction on an embedded grid, thereby preserving procedure-level consistency while adapting to geometry-dependent discrete structures. Across geometry-dependent Poisson, advection-diffusion, linear elasticity, as well as spatiotemporal heat-conduction problems, DiSOL produces stable and accurate predictions under both in-distribution and strongly out-of-distribution geometries, including discontinuous boundaries and topological changes. These results highlight the need for procedural operator representations in geometry-dominated regimes and position discrete solution operator learning as a distinct, complementary direction in scientific machine learning.

</details>


### [50] [KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education](https://arxiv.org/abs/2601.09156)
*Woojin Kim,Changkwon Lee,Hyeoncheol Kim*

Main category: cs.LG

TL;DR: 本研究提出了一种针对知识追踪(KT)的反事实解释生成方法KTCF，该方法考虑了知识概念之间的关系，并通过后处理方案将反事实解释转化为一系列教育指令。实验表明，KTCF方法在大规模教育数据集上表现出色且稳健，其性能优于现有方法。此外，研究还展示了由此产生的教育指示有助于减轻学生的学习负担，强调了反事实对于促进AI在教育领域负责任及实用应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了提高教学与学习效果，利用人工智能技术增强教育的适应性和可扩展性。考虑到知识追踪(KT)在学生建模任务中的优越表现和应用潜力，本文探索了从可解释AI(XAI)到KT再到教育领域的反事实解释作为连接桥梁的可能性。反事实解释不仅提供了可操作的补救措施、具有内在因果性和局部性，而且易于非专业背景的教育相关方理解。

Method: 提出了KTCF，一种为KT设计的反事实解释生成方法，它考虑到了知识概念间的关系；开发了一个后处理方案，能够把反事实解释转变成一系列易于执行的教学指南。

Result: 在大型教育数据集上的实验显示，相较于现有方法，KTCF方法在多个指标上实现了5.7%至34%不等的改进。另外，对后处理方案的定性评估证明，生成的教学指导有助于缓解沉重的学习压力。

Conclusion: 反事实解释作为一种连接XAI与教育的方式，在促进AI于教育领域内负责任地实践方面展现出巨大潜力。未来的研究可以从更贴近教育实际的角度出发，发展以利益相关者为中心的方法论。

Abstract: Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.

</details>


### [51] [Efficient Clustering in Stochastic Bandits](https://arxiv.org/abs/2601.09162)
*G Dhinesh Chandran,Kota Srinivas Reddy,Srikrishna Bhashyam*

Main category: cs.LG

TL;DR: 本文研究了固定置信度设置下的Bandit聚类(BC)问题，提出了一种高效的Bandit聚类算法（EBC）及其启发式变体EBC-H，旨在通过减少每步的计算复杂度同时保持渐近最优性来改进现有的BC算法。通过合成数据集和真实世界数据集上的模拟验证了EBC与EBC-H相比现有方法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决传统Bandit聚类算法在每次采样时需要解决优化问题导致计算成本高的问题，特别是在处理一类满足温和正则条件的向量参数分布时。目标是开发一种既有效率又能保持渐近最优性的新算法。

Method: 提出了两种新的算法：Efficient Bandit Clustering (EBC)，它通过朝最优值单步迭代而非求解完整的优化问题来降低计算复杂度；以及EBC的一个启发式版本EBC-H，进一步简化了采样规则。

Result: EBC和EBC-H相较于现有算法，在每个样本运行时间上表现出了显著的计算效率优势。通过合成及真实世界数据集上的仿真测试，证实了这两种算法不仅维持了渐近最优性，并且在性能上有明显提升。

Conclusion: 提出的EBC和EBC-H算法能够有效地解决固定置信度设定下Bandit聚类问题中的计算效率挑战，同时保持解决方案的质量。这些成果为处理具有不同分布特征的数据序列提供了新的高效途径。

Abstract: We study the Bandit Clustering (BC) problem under the fixed confidence setting, where the objective is to group a collection of data sequences (arms) into clusters through sequential sampling from adaptively selected arms at each time step while ensuring a fixed error probability at the stopping time. We consider a setting where arms in a cluster may have different distributions. Unlike existing results in this setting, which assume Gaussian-distributed arms, we study a broader class of vector-parametric distributions that satisfy mild regularity conditions. Existing asymptotically optimal BC algorithms require solving an optimization problem as part of their sampling rule at each step, which is computationally costly. We propose an Efficient Bandit Clustering algorithm (EBC), which, instead of solving the full optimization problem, takes a single step toward the optimal value at each time step, making it computationally efficient while remaining asymptotically optimal. We also propose a heuristic variant of EBC, called EBC-H, which further simplifies the sampling rule, with arm selection based on quantities computed as part of the stopping rule. We highlight the computational efficiency of EBC and EBC-H by comparing their per-sample run time with that of existing algorithms. The asymptotic optimality of EBC is supported through simulations on the synthetic datasets. Through simulations on both synthetic and real-world datasets, we show the performance gain of EBC and EBC-H over existing approaches.

</details>


### [52] [Multi-Teacher Ensemble Distillation: A Mathematical Framework for Probability-Domain Knowledge Aggregation](https://arxiv.org/abs/2601.09165)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文基于Sparse-KD的概率域蒸馏框架，开发了一个公理化的、算子理论框架用于多教师集成知识蒸馏。定义了五个核心公理来管理有效的知识聚合算子，并证明了满足这些公理的算子家族的存在性和非唯一性。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过构建一个多教师集成知识蒸馏的公理化框架来超越现有的特定聚合公式限制，从而为从不同前沿模型中进行多教师蒸馏提供理论基础。

Method: 通过定义五条核心公理（包括凸性、正性、连续性、权重单调性和温度一致性）来治理有效的知识聚合算子，并证明了符合这些原则的不同聚合机制的存在与多样性。

Result: 研究表明，多教师聚合可以减少随机方差和系统性的监督偏差，并且对于线性于教师权重的聚合算子，在标准独立假设下建立了经典的集成方差减少结果及其在相关误差制度下的扩展。

Conclusion: 该框架不仅为多教师知识蒸馏提供了理论支持，还允许多种有效的实现策略。

Abstract: Building on the probability-domain distillation framework of Sparse-KD, we develop an axiomatic, operator-theoretic framework for multi-teacher ensemble knowledge distillation. Rather than prescribing a specific aggregation formula, we define five core axioms governing valid knowledge aggregation operators, encompassing convexity, positivity, continuity, weight monotonicity, and temperature coherence. We prove the existence and non-uniqueness of operator families satisfying these axioms, establishing that multiple distinct aggregation mechanisms conform to the same foundational principles.
  Within this framework, we establish operator-agnostic guarantees showing that multi-teacher aggregation reduces both stochastic variance and systematic supervisory bias under heterogeneous teachers, while providing Jensen-type bounds, log-loss guarantees, and safety attenuation properties. For aggregation operators linear in teacher weights, we further establish classical ensemble variance-reduction results under standard independence assumptions, with extensions to correlated-error regimes. The framework provides theoretical grounding for multi-teacher distillation from diverse frontier models while admitting multiple valid implementation strategies.

</details>


### [53] [BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning](https://arxiv.org/abs/2601.09172)
*Pengyang Shao,Naixin Zhai,Lei Chen,Yonghui Yang,Fengbin Zhu,Xun Yang,Meng Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架BalDRO，用于解决大型语言模型（LLMs）中目标信息移除时遇到的样本不平衡问题。通过将遗忘过程定义为一个最小化-上界过程，BalDRO能够更有效地平衡不同难度样本间的遗忘程度，从而提高遗忘质量和模型实用性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地影响在线内容，从训练好的模型中删除特定信息变得至关重要。现有方法面临的主要挑战之一是忘记集内部样本间存在显著不平衡性，导致某些知识被过度遗忘而另一些则未充分清除。

Method: BalDRO将信息移除过程视为一个min-sup过程：内层步骤识别出强调难以忘记样本的最坏情况数据分布；外层步骤则基于此分布更新模型参数。此外，还提出了两种高效变体：一种是基于离散GroupDRO近似关注高损失子集的BalDRO-G，另一种是利用连续Donsker-Varadhan对偶方法实现标准训练流程内平滑自适应加权的BalDRO-DV。

Result: 实验结果表明，在TOFU和MUSE基准测试上，与现有方法相比，BalDRO在提高遗忘质量的同时也保持了较好的模型实用性。

Conclusion: BalDRO作为一种新颖且高效的框架，在处理大型语言模型的信息移除任务时展现了优越性能，不仅改善了遗忘效果，还提升了模型的整体效用。

Abstract: As Large Language Models (LLMs) increasingly shape online content, removing targeted information from well-trained LLMs (also known as LLM unlearning) has become critical for web governance. A key challenge lies in sample-wise imbalance within the forget set: different samples exhibit widely varying unlearning difficulty, leading to asynchronous forgetting where some knowledge remains insufficiently erased while others become over-forgotten. To address this, we propose BalDRO, a novel and efficient framework for balanced LLM unlearning. BalDRO formulates unlearning as a min-sup process: an inner step identifies a worst-case data distribution that emphasizes hard-to-unlearn samples, while an outer step updates model parameters under this distribution. We instantiate BalDRO via two efficient variants: BalDRO-G, a discrete GroupDRO-based approximation focusing on high-loss subsets, and BalDRO-DV, a continuous Donsker-Varadhan dual method enabling smooth adaptive weighting within standard training pipelines. Experiments on TOFU and MUSE show that BalDRO significantly improves both forgetting quality and model utility over existing methods, and we release code for reproducibility.

</details>


### [54] [Geometric Stability: The Missing Axis of Representations](https://arxiv.org/abs/2601.09173)
*Prashant C. Raju*

Main category: cs.LG

TL;DR: 本文介绍了一种新的度量方法——几何稳定性，用以评估表示几何在受到扰动时的可靠性。通过名为Shesha的框架，研究者们发现稳定性和相似性在实证上是不相关的，并且在安全监控、可控性、模型选择等方面提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的对学习到的表示进行分析的方法主要集中在相似性上，但这种方法只能揭示被表示的内容，而不能衡量该结构是否稳健。因此，作者引入了“几何稳定性”这一概念，旨在量化表示几何在扰动下的可靠性。

Method: 提出了一个名为Shesha的新框架来测量表示的几何稳定性。通过对七个领域中的2,463种配置进行实验，展示了稳定性和相似性之间的低相关性（ρ≈0.01），并探讨了几何稳定性在不同应用场景下的优势。

Result: 研究结果表明，几何稳定性与传统的相似性度量方式存在显著差异，并且在安全性监测、可控制性预测以及模型选择等领域中展现出独特的优势。此外，它还能够为生物和计算系统中的表示提供审计所需的重要补充信息。

Conclusion: 几何稳定性作为一种新的度量标准，对于评估表示的稳健性至关重要。它可以补充相似性度量所缺失的信息，帮助我们更好地理解生物及计算系统内表示的特性。

Abstract: Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($ρ\approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($ρ= 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.

</details>


### [55] [$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness](https://arxiv.org/abs/2601.09176)
*Lang Xiong,Ning Liu,Ao Ren,Yuheng Bai,Haining Fang,BinYan Zhang,Zhe Jiang,Yujuan Tan,Duo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的剪枝方法$D^2Prune$，通过双重泰勒展开和注意力感知动态更新策略解决了现有方法在激活分布偏移和注意力模块长尾分布特性上的不足。实验表明，该方法在多种大型语言模型上优于现有最先进方法，并且在基于ViT的视觉模型上也表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型由于其巨大的计算需求，在部署时面临重大挑战。虽然剪枝提供了一种有前景的压缩解决方案，但现有方法存在两个关键限制：一是忽视了校准数据与测试数据之间的激活分布偏移，导致错误估计不准确；二是忽略了注意力模块中激活值的长尾分布特征。为了解决这些问题，提出了一个新的剪枝方法$D^2Prune$。

Method: $D^2Prune$方法包括两部分：首先，采用基于双重泰勒展开的方法同时建模权重和激活扰动，以实现精确的误差估计，从而有助于在剪枝过程中选择精确的剪枝掩码及权重更新；其次，提出了一种注意力感知的动态更新策略，通过最小化注意力分布间的KL散度与重建误差来保留长尾注意力模式。

Result: 广泛的实验证明，$D^2Prune$在各种大型语言模型（如OPT-125M、LLaMA2/3和Qwen3）上始终优于现有的最先进方法。此外，这种动态注意力更新机制也能很好地推广到像DeiT这样的基于ViT的视觉模型上，在ImageNet-1K上实现了更高的准确性。

Conclusion: 综上所述，本文提出的$D^2Prune$方法有效地克服了当前剪枝技术中存在的问题，不仅提高了大型语言模型的效率而不牺牲性能，而且对于视觉任务也有很好的适用性。

Abstract: Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.

</details>


### [56] [From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences](https://arxiv.org/abs/2601.09220)
*Xinzi Tan,Kejian Zhang,Junhan Yu,Doudou Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制——Hawkes Attention，它基于多变量Hawkes过程理论，旨在更好地捕捉不同类型事件之间的时间效应和内容交互。这种新方法在实验中表现优于基线，并且可以轻松应用于特定的时序结构，如时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的方法主要通过位置编码来注入时间信息，这依赖于共享或参数化的衰减结构，限制了它们捕捉异质性和类型特异性时间效应的能力。受到这一点的启发，研究者们旨在开发一种能够统一处理事件时间和内容交互的新方法。

Method: 从多变量Hawkes过程理论出发，为标记时间点过程（MTPP）设计了一种名为Hawkes Attention的新注意力操作符。这种方法使用可学习的每种类型的神经核来调节查询、键和值投影，从而取代传统注意力中的相应部分。

Result: 实验结果显示，与基准方法相比，所提方法取得了更好的性能。此外，除了适用于一般MTPP外，该注意力机制还能容易地应用到特定的时序结构中，例如时间序列预测。

Conclusion: 通过引入Hawkes Attention，研究成功提高了模型对时间相关行为及类型特异性激发模式的学习能力，展示了其在多种应用场景下的潜力。

Abstract: Marked Temporal Point Processes (MTPPs) arise naturally in medical, social, commercial, and financial domains. However, existing Transformer-based methods mostly inject temporal information only via positional encodings, relying on shared or parametric decay structures, which limits their ability to capture heterogeneous and type-specific temporal effects. Inspired by this observation, we derive a novel attention operator called Hawkes Attention from the multivariate Hawkes process theory for MTPP, using learnable per-type neural kernels to modulate query, key and value projections, thereby replacing the corresponding parts in the traditional attention. Benefited from the design, Hawkes Attention unifies event timing and content interaction, learning both the time-relevant behavior and type-specific excitation patterns from the data. The experimental results show that our method achieves better performance compared to the baselines. In addition to the general MTPP, our attention mechanism can also be easily applied to specific temporal structures, such as time series forecasting.

</details>


### [57] [GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization](https://arxiv.org/abs/2601.09233)
*Zhengyang Zhao,Lu Ma,Yizhen Jiang,Xiaochen Ma,Zimo Meng,Chengyu Shen,Lexiang Tang,Haoze Sun,Peng Pei,Wentao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的后训练框架GIFT，通过在有限温度下引入监督作为能量势，解决了传统SFT方法中由于分布坍缩导致的探索空间不足问题，从而为后续强化学习提供了更优的初始化。实验表明，GIFT相比标准SFT和其他基线方法，在用于RL初始化时表现显著更好。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRM）采用的后训练范式——即先进行监督微调(SFT)再接强化学习(RL)——存在内在优化不匹配的问题：SFT过程中严格的监督会导致分布坍缩，进而限制了接下来RL所需的探索空间。

Method: 作者们重新定义了SFT，并提出了吉布斯有限温度初始化(GIFT)的方法。该方法将监督视为有限温度下的能量势，而不是像传统的SFT那样完全抑制基础先验，以此建立了一个保证整个后训练流程目标一致性的分布桥梁。

Result: 实验结果显示，当使用GIFT作为RL初始化方案时，其性能明显优于标准SFT及其他竞争基线方法，为实现后训练过程中的全局最优提供了一条数学原理上的路径。

Conclusion: GIFT通过解决传统SFT导致的分布坍缩问题，为后续的强化学习阶段提供了更加开放和多样化的探索起点，从而能够达到更好的整体性能。此外，它还指出了通往全局最优解的一条理论支撑的道路。

Abstract: The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.

</details>


### [58] [Reward Learning through Ranking Mean Squared Error](https://arxiv.org/abs/2601.09236)
*Chaitanya Kharyal,Calarina Muslimani,Matthew E. Taylor*

Main category: cs.LG

TL;DR: 本文提出了一种基于评分的强化学习方法R4，通过使用排序均方误差损失函数从人类反馈中学习奖励函数。实验表明，该方法在多个机器人运动基准测试中表现优于现有方法，并且需要的人类反馈更少。


<details>
  <summary>Details</summary>
Motivation: 奖励设计是将强化学习应用于现实世界问题的一个重要瓶颈。为了解决这个问题，研究者们开始探索从人类反馈中自动学习奖励函数的方法，而非手动指定。最近的研究提出了从人类提供的评分而非传统的二元偏好中学习奖励函数的新思路，这使得监督变得更加丰富并且可能对认知的要求更低。

Method: 本文介绍了一种名为Ranked Return Regression for RL (R4)的新方法，它利用一种新颖的排序均方误差（rMSE）损失来处理教师给出的评分作为序数目标。R4能够从一系列轨迹-评分对的数据集中学习，其中每个轨迹都附有一个离散评分标签。训练过程中，随机选取一组轨迹预测其回报值，并使用可微分排序操作符对其进行排名。然后，优化这些软排名与教师评分之间的均方误差损失。

Result: 实验结果证明，采用模拟的人类反馈时，R4在OpenAI Gym和DeepMind Control Suite中的机器人移动基准测试上始终能够匹配甚至超越现有的基于评分和偏好的RL方法，同时所需反馈量显著减少。

Conclusion: R4方法不仅提供了形式上的保证——即在温和假设下解集是最小且完备的，而且在实践中也展示了良好的性能优势。

Abstract: Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., "bad," "neutral," "good"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.

</details>


### [59] [XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs](https://arxiv.org/abs/2601.09237)
*Xinyang Chen,Huidong Jin,Yu Huang,Zaiwen Feng*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的时间序列预测模型XLinear，该模型基于多层感知器（MLP），通过利用内生变量的全局标记与外生变量交互，并使用具有Sigmoid激活函数的MLP提取时间模式和变量间依赖性。实验表明，XLinear在准确性和效率方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用中经常表现出不对称的因果关系和不同的数据获取成本，而现有的长期时间序列预测模型通常假设所有变量的重要性是均匀的。此外，基于Transformer的模型虽然能够捕捉长距离依赖关系，但计算成本高且存在排列不变性问题；基于块的方法虽然提高了效率，但可能错过局部时间模式。因此，需要一种能有效利用时间维度和相关外生变量信息的新方法。

Method: 提出了XLinear模型，它是一种基于多层感知器(MLP)的轻量级时间序列预测模型。XLinear采用从内生变量导出的全局token作为关键枢纽，与其他外生变量进行交互，并利用带有Sigmoid激活函数的MLP来提取时间模式以及变量间的依赖关系。预测头部分整合这些信号以预测内生序列。

Result: 在七个标准基准测试集和五个包含外生输入的真实世界数据集上对XLinear进行了评估。结果表明，无论是对于多变量预测还是受外生输入影响的单变量预测，XLinear都能提供比最先进模型更优的准确度和效率。

Conclusion: XLinear作为一种新的轻量级时间序列预测模型，在处理具有外生输入的情况下表现出了卓越的性能，特别是在准确率和计算效率方面超越了当前最先进的模型。

Abstract: Despite the prevalent assumption of uniform variable importance in long-term time series forecasting models, real world applications often exhibit asymmetric causal relationships and varying data acquisition costs. Specifically, cost-effective exogenous data (e.g., local weather) can unilaterally influence dynamics of endogenous variables, such as lake surface temperature. Exploiting these links enables more effective forecasts when exogenous inputs are readily available. Transformer-based models capture long-range dependencies but incur high computation and suffer from permutation invariance. Patch-based variants improve efficiency yet can miss local temporal patterns. To efficiently exploit informative signals across both the temporal dimension and relevant exogenous variables, this study proposes XLinear, a lightweight time series forecasting model built upon MultiLayer Perceptrons (MLPs). XLinear uses a global token derived from an endogenous variable as a pivotal hub for interacting with exogenous variables, and employs MLPs with sigmoid activation to extract both temporal patterns and variate-wise dependencies. Its prediction head then integrates these signals to forecast the endogenous series. We evaluate XLinear on seven standard benchmarks and five real-world datasets with exogenous inputs. Compared with state-of-the-art models, XLinear delivers superior accuracy and efficiency for both multivariate forecasts and univariate forecasts influenced by exogenous inputs.

</details>


### [60] [RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning](https://arxiv.org/abs/2601.09253)
*Zehua Liu,Shuqi Liu,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架——奖励信息微调（RIFT），该框架能够利用所有自生成样本，通过标量奖励重新加权损失来从正负轨迹中学习，以解决监督微调（SFT）和拒绝采样微调（RFT）在大型语言模型对齐过程中存在的数据效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前的监督微调（SFT）和拒绝采样微调（RFT）方法存在依赖昂贵专家数据或丢弃有价值负面样本的问题，导致数据使用效率低下。为了解决这个问题，提出了Reward Informed Fine-Tuning (RIFT)框架，旨在提高数据使用效率。

Method: RIFT框架通过对模型输出中的正面和负面轨迹进行学习，利用所有自生成样本，并通过标量奖励重新加权损失函数来避免直接采用硬阈值处理方式。此外，为解决直接将奖励与损失相乘可能导致训练崩溃的问题，研究者们引入了一种稳定的损失公式化方法，确保数值稳健性和优化效率。

Result: 在不同基础模型上的数学基准测试中，RIFT相比RFT表现出了持续的优势。结果表明，RIFT是一种鲁棒且数据高效的对齐方案，适用于混合质量的自生成数据。

Conclusion: RIFT作为一种新颖而有效的框架，能够充分利用包括正面和负面在内的所有自生成样本，提供了一种比传统SFT和RFT更高效的数据使用方法，尤其适合于利用大规模语言模型产生的混合质量数据来进行模型对齐。

Abstract: While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.

</details>


### [61] [Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability](https://arxiv.org/abs/2601.09261)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为元认知调节（metacognitive regulation）的方法，用于处理不可观察反馈可靠性下的学习问题。通过引入自我诊断机制，维持一个缓慢变化的经验信任变量来软性调整学习更新，从而在没有外部可靠性标签或明确的损坏模型的情况下改善了认识可识别性。


<details>
  <summary>Details</summary>
Motivation: 在不可观察反馈可靠性的环境下进行学习时，系统不仅需要决定如何稳定地学习，还需要判断是否从某个经验中学习。标准的鲁棒学习方法可能会形成高置信度但系统性错误的信念。因此，研究提出了元认知调节作为解决方案。

Method: 研究者提出了一种称为Monitor-Trust-Regulator (MTR)的模块化分解方法，并通过自我诊断机制实现。该机制维护了一个根据内部动态信息推断出的经验可信度变量，用以温和地调节学习过程中的更新步骤。

Result: 实验表明，在EIUR条件下，自我诊断与提高的认识可识别性相关联。在强化学习中，它允许校准怀疑并在奖励被系统性篡改时恢复；而在监督学习中，则揭示了性能恢复并不意味着认知恢复的现象。

Conclusion: MTR框架和自我诊断提供了一个组织抽象及具体设计方案，用于解决在不可见可靠性条件下的自主学习过程中内在可靠性评估的问题。

Abstract: Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.

</details>


### [62] [Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction](https://arxiv.org/abs/2601.09285)
*Mianzhi Pan,JianFei Li,Peishuo Liu,Botian Wang,Yawen Ouyang,Yiming Rong,Hao Zhou,Jianbing Zhang*

Main category: cs.LG

TL;DR: 本文介绍了一种名为MOF-LLM的新框架，它通过结合空间感知的持续预训练、结构监督微调和基于匹配的强化学习，专门用于预测金属有机框架（MOFs）的3D结构。实验表明，该方法在准确性和采样效率上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 金属有机框架（MOFs）由于其高原子复杂性，在准确预测其3D结构方面面临挑战。尽管大型语言模型（LLMs）在生成晶体方面表现出潜力，但直接应用于MOFs时遇到了障碍。

Method: 提出了MOF-LLM，这是首个特别为块级MOF结构预测设计的LLM框架。采用空间感知的持续预训练(CPT)、结构监督微调(SFT)以及基于匹配的强化学习(RL)，并通过软自适应策略优化(SAPO)来增强Qwen-3 8B模型的空间推理能力。

Result: 综合实验证明，MOF-LLM在准确度和采样效率上超过了现有的去噪基础和LLM基础的方法。

Conclusion: MOF-LLM提供了一个有效的解决方案，用于提高对MOFs这种具有高度复杂性的材料进行三维结构预测的能力。

Abstract: Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.

</details>


### [63] [GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR](https://arxiv.org/abs/2601.09361)
*Jiaying Zhang,Lei Shi,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 本文提出了一种名为GeoRA的方法，旨在解决现有参数高效方法在强化学习可验证奖励（RLVR）中的优化不稳定性和几何结构不匹配问题。通过利用RL更新子空间的各向异性和可压缩性，GeoRA能够在保持预训练几何结构的同时提高GPU计算效率。实验表明，GeoRA在关键数学基准测试中优于现有的低秩基线，并且在领域外任务上表现出更好的泛化能力和抗灾难性遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 现有的针对监督微调设计的参数高效方法（如PiSSA和MiLoRA）直接应用于强化学习可验证奖励（RLVR）时，由于未能考虑RLVR独特的优化动态和几何结构特性，导致光谱塌陷和优化不稳定性，严重限制了模型性能。同时，依赖稀疏更新的方法在现代硬件上遇到效率瓶颈。为了解决这些问题，提出了GeoRA方法。

Method: GeoRA通过奇异值分解（SVD）在几何约束子空间内提取主方向来初始化适配器，同时冻结剩余组件。这种方法保留了预训练模型的几何结构，并通过密集运算符实现了高效的GPU计算。

Result: 实验结果表明，GeoRA能够缓解由几何错位引起的优化瓶颈，在Qwen和Llama上的关键数学基准测试中始终优于已建立的低秩基线，达到了最先进的结果。此外，GeoRA在处理领域外任务时展现出优越的泛化能力和对灾难性遗忘的抵抗力。

Conclusion: 综上所述，GeoRA提供了一个有效的方法来克服现有技术在强化学习可验证奖励场景下的局限性，不仅提高了模型性能还增强了其在不同任务间的适应能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.

</details>


### [64] [Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps](https://arxiv.org/abs/2601.09428)
*Siyi Li,Joseph G. Lambourne,Longfei Zhang,Pradeep Kumar Jayaraman,Karl. D. D. Willis*

Main category: cs.LG

TL;DR: 该论文提出了一种通过一系列简单的几何构造生成CAD轮廓的新方法，这些构造包括曲线偏移、旋转和相交。在设计师提供的几何图形基础上逐步构建最终轮廓的点和曲线。引入构造步骤可以提高生成质量，并且通过强化学习优化构造序列可以在多个指标上进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了改进计算机辅助设计（CAD）中轮廓生成的质量，本文提出了一种基于简单几何操作如曲线偏移、旋转及相交等步骤来逐步建立最终设计轮廓的方法。目的是减少形状建模中的自由度至一小部分可调参数值，同时允许设计师进行参数化编辑。

Method: 研究者们开发了一个新流程，从设计师给定的基础几何开始，通过执行一系列几何变换（例如：曲线偏移、旋转与交点计算）逐步形成最终的设计轮廓。此外，还探索了如何利用强化学习技术优化这一系列构造步骤以进一步增强结果。

Result: 实验证明，在设计师提供的初始几何体与最终轮廓之间添加构造步骤确实能够提高生成轮廓的质量。而且，当应用强化学习于构造序列时，即使对于未被直接优化的一些评估标准而言，也能观察到显著改善。

Conclusion: 本研究表明，通过一系列可控的几何构造步骤来生成CAD轮廓是一种有效的方法，它不仅支持高精度的参数化编辑，还能通过结合强化学习技术达到更优的设计成果。

Abstract: We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.

</details>


### [65] [On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI](https://arxiv.org/abs/2601.09455)
*André Artelt,Martin Olsen,Kevin Tierney*

Main category: cs.LG

TL;DR: 本文探讨了生成反事实和半事实解释的计算复杂性，指出在很多情况下，生成这些解释是计算上困难的，并且在某些假设下也难以近似。这些发现对XAI社区及希望监管AI解释的政策制定者具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 为了使机器学习模型能够被部署于关键应用中，提供清晰的决策解释至关重要。反事实与半事实解释作为两种为用户提供模型输出洞察的方法而受到关注。然而，关于生成这类解释的计算复杂性研究显示，这通常是一个计算难题。

Method: 通过文献综述的方式总结了有关生成反事实和半事实解释的计算复杂性研究成果；同时，作者还贡献了自己的不可近似性结果来进一步证明不仅生成解释难，而且在特定条件下甚至难以近似。

Result: 研究发现，在许多情况下生成反事实或半事实解释都是计算上困难的任务，并且基于某些假设前提下，找到接近最优解也非常困难。

Conclusion: 生成反事实和半事实解释面临的计算挑战表明，对于XAI领域以及试图规范AI系统解释性的政策制定者来说，需要考虑这些复杂性问题。

Abstract: Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. Counterfactual and semi-factual explanations have emerged as two mechanisms for providing users with insights into the outputs of their models. We provide an overview of the computational complexity results in the literature for generating these explanations, finding that in many cases, generating explanations is computationally hard. We strengthen the argument for this considerably by further contributing our own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate. We discuss the implications of these complexity results for the XAI community and for policymakers seeking to regulate explanations in AI.

</details>


### [66] [Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting](https://arxiv.org/abs/2601.09467)
*Tianye Li,Qi Liu,Hao Li,Lei Chen,Wencong Cheng,Fei Zheng,Xiangao Xia,Ya Wang,Gang Huang,Weiwei Wang,Xuan Tong,Ziqing Zu,Yi Fang,Shenming Fu,Jiang Jiang,Haochen Li,Mingxing Li,Jiangjiang Xia*

Main category: cs.LG

TL;DR: 提出了一种新的天气预报模型YanTian，该模型基于Shifted Earth Transformer架构和Relay Autoregressive微调策略，实现了比现有高分辨率预报更高的准确性和更长的有效预报时间，同时大大降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的天气预报模型大多采用以视觉为中心的架构，忽略了地球的球形几何和纬向周期性；此外，传统的自回归训练方法由于误差累积导致计算成本高且限制了预报范围。

Method: 提出了Shifted Earth Transformer（Searth Transformer），这是一种物理信息架构，将纬向周期性和经向边界纳入基于窗口的自注意力机制中，以实现物理一致的全球信息交换。还引入了接力式自回归（RAR）微调策略，在有限的内存和计算预算下学习长期大气演变。

Result: YanTian模型在精度上超过了欧洲中期天气预报中心的高分辨率预报，并在一度分辨率下与最先进的AI模型竞争，同时相比标准自回归微调所需的计算成本低约200倍。对于Z500指标，YanTian达到了10.3天的有效预报提前期，优于HRES的9天。

Conclusion: 除了提高天气预报能力外，这项工作还为复杂全球尺度地球物理循环系统的预测建模提供了强有力的算法基础，为地球系统科学研究开辟了新路径。

Abstract: Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.

</details>


### [67] [SimMerge: Learning to Select Merge Operators from Similarity Signals](https://arxiv.org/abs/2601.09473)
*Oliver Bolton,Aakanksha,Arash Ahmadian,Sara Hooker,Marzieh Fadaee,Beyza Ermis*

Main category: cs.LG

TL;DR: 本文提出了一种名为SimMerge的方法，通过使用廉价的任务无关相似性信号来预测和选择最佳模型合并方式，从而避免了昂贵的合并-评估循环。SimMerge不仅在70亿参数的大规模语言模型的两两合并中超越了标准合并操作符的表现，而且无需重新训练即可推广到多路合并及1110亿参数的语言模型合并。此外，还介绍了一个支持动态添加新任务、模型和操作符的变体。


<details>
  <summary>Details</summary>
Motivation: 尽管模型合并是大语言模型开发中的一个重要工具，但随着规模扩大，如何选择合适的合并操作符、模型以及正确的合并顺序成为一大挑战。传统的合并-评估方法成本高昂，因此需要一种更加高效且经济的方法来优化这一过程。

Method: 提出了SimMerge，一种基于预测的选择合并方法。它利用少量未标记探针计算功能性和结构性特征，并据此预测给定二元合并的表现。基于这些预测，SimMerge能够自动选定最佳合并操作符、待合并模型子集及其合并顺序。

Result: SimMerge在70亿参数模型的两两合并上表现优于常规合并操作符；该方法也能直接应用于更大型（如1110亿参数）或更复杂的多路合并场景而无需额外训练；引入了适应性强的bandit版本以支持即时增加新的任务、模型和操作符。

Conclusion: 学习如何有效地进行模型合并为解决大规模模型组合问题提供了一条实用路径，尤其是在面对庞大检查点目录与有限评估预算时。

Abstract: Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \simmerge{}, \emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.

</details>


### [68] [Terminally constrained flow-based generative models from an optimal control perspective](https://arxiv.org/abs/2601.09474)
*Weiguo Gao,Ming Li,Qianxiao Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为TOCFlow的方法，通过最优控制公式来解决使用预训练流模型从终端约束分布中采样的问题。该方法在保持参考模型生成质量的同时，提高了对各种高维科学任务的约束满足度。


<details>
  <summary>Details</summary>
Motivation: 为了解决使用预训练流模型从具有终端约束的分布中进行采样的难题，并且希望能够开发出一种能够提高约束满意度同时保持良好生成效果的方法。

Method: 理论方面，通过对值函数进行特征化并推导出最优反馈控制；算法方面，引入了TOCFlow方法，这是一种针对预训练流模型设计的、具备几何感知能力的采样时间指导法。

Result: 实验结果表明，在包括Darcy流、受约束轨迹规划以及符合Kolmogorov谱尺度的湍流快照生成等在内的三个高维度科学任务上，TOCFlow相较于欧几里得引导和投影基线方法显著提升了约束条件下的表现。

Conclusion: TOCFlow提供了一种有效的方法来改善预训练流模型在处理具有终端约束的任务时的表现，同时不牺牲生成样本的质量。

Abstract: We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.

</details>


### [69] [Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels](https://arxiv.org/abs/2601.09579)
*Fiona Murphy,Alessio Benavoli*

Main category: cs.LG

TL;DR: 本文在Granger因果关系的背景下统一了两种先进的基于核的方法，并提出了一种新的高斯过程评分模型以及一个同时因果识别算法，展示了这些方法在非线性时间序列因果发现中的改进性能。


<details>
  <summary>Details</summary>
Motivation: 为了识别时间序列变量之间的非线性因果关系，文章旨在通过理论上的统一来提高基于核的Granger因果方法的效果，并引入新方法以改进现有的因果发现技术。

Method: 论文将两种最新的基于核的Granger因果方法统一到了核主成分回归(KPCR)框架下，并提出了一个新的基于平滑信息准则惩罚边际似然性的高斯过程评分模型（$GP_{SIC}$），此外还提出了一种完全基于Granger因果的同时因果识别算法。

Result: 所提出的基于$GP_{SIC}$评分模型的方法相比现有最先进的非线性时间序列因果发现方法表现更优；同时，基于GC的同时因果识别算法与当前最优的同时时间序列因果发现算法相比也显示出了良好的性能。

Conclusion: 研究结果表明，通过理论统一和引入的新方法可以改善时间序列数据中非线性因果关系的识别。

Abstract: Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based $GP_{SIC}$ method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.

</details>


### [70] [Energy-Entropy Regularization: The True Power of Minimal Looped Transformers](https://arxiv.org/abs/2601.09588)
*Wai-Lun Lam*

Main category: cs.LG

TL;DR: 提出了一种新的训练框架，利用Tsallis熵和哈密顿动力学来改变损失景观的几何形状，成功地训练了一个单头循环Transformer模型解决了输入序列长度为1000个标记的归纳头任务。


<details>
  <summary>Details</summary>
Motivation: 当前方法在基准任务上训练单头循环架构时经常失败或表现不佳，因为损失景观非常非凸且不规则。优化过程往往停滞于损失景观中的不良局部极小值和鞍点，阻碍了模型发现全局最小点。此外，这些单头循环Transformer模型的内部机制尚未被充分理解，从零开始训练它们仍然是一个重大挑战。

Method: 提出了一个新的训练框架，该框架利用Tsallis熵和哈密顿动力学来变换损失景观的几何形状，并将参数更新视为物理流进行处理。

Result: 成功地训练了一个具有模型维度$d=8$的单头循环Transformer解决输入序列长度达1000个标记的归纳头任务。

Conclusion: 新提出的训练框架揭示了单头循环Transformer卓越推理能力背后的内部机制，并展示了其在解决长序列任务方面的潜力。

Abstract: Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.

</details>


### [71] [Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric](https://arxiv.org/abs/2601.09624)
*Jiali Cheng,Ziheng Chen,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 本文提出了一种基于模型电路的度量方法——电路引导的遗忘难度（CUD），该方法在数据被删除前就能根据电路级信号为每个样本分配一个连续的难度分数。实验表明，CUD能够可靠地区分内在容易与难于遗忘的样本，并且对于不同的遗忘方法保持稳定。研究揭示了容易遗忘的样本与较短、较浅的交互相关，而难以遗忘的样本则依赖于接近后期计算的更长更深路径。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘对于构建可信赖和合规的语言模型变得至关重要，但不同样本之间的遗忘效果差异显著。本文认为这种差异不仅反映了数据层面的现象，还体现了模型内部机制如何编码和保护记忆信息的问题。

Method: 从机制角度出发，基于模型电路——即决定预测形成的结构化交互路径来研究这个问题。提出了电路引导的遗忘难度(CUD)作为预遗忘指标，利用电路级别的信号为每个样本分配连续的难度得分。

Result: 广泛的实验证明了CUD能够有效区分本质上易于或难以遗忘的样本，并且对于不同的遗忘方法具有稳定性。关键电路模式揭示了难度的机制特征：易于遗忘的样本与早期到中期部分的模型中较短、较浅的交互有关；而难以遗忘的样本则依赖于接近后期计算的更长更深路径。

Conclusion: 相比于现有的定性研究，CUD朝向建立一个有原则、细粒度且可解释的遗忘难度分析迈出了第一步，并激发了基于模型机制开发遗忘方法的研究。

Abstract: Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.

</details>


### [72] [Exploring Fine-Tuning for Tabular Foundation Models](https://arxiv.org/abs/2601.09654)
*Aditya Tanna,Pratinav Seth,Mohamed Bouadi,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 研究了表格基础模型（TFM）在零样本和微调情况下的性能，发现零样本TFM表现已很强，而微调的增益取决于模型和数据特性。全面对比了零样本、元学习、全监督微调（SFT）及参数高效微调（PEFT）方法，在不同数据集特性如不平衡性、大小与维度下效果各异，并提供了关于何时微调最有利及其局限性的实用指南。


<details>
  <summary>Details</summary>
Motivation: 探索表格基础模型（TFM）在结构化数据上的零样本学习能力以及微调策略的效果，特别是当面对不同类型的数据特征时，哪种方法能够提供最佳性能。

Method: 通过TALENT、OpenML-CC18和TabZilla等基准测试对TFMs进行评估，比较了零样本、元学习、监督式微调（SFT）以及参数高效微调（PEFT）四种方法的表现。分析了数据不平衡性、数据集大小及维度等因素如何影响最终结果。

Result: 研究表明，零样本TFMs已经表现出色；元学习和PEFT在特定条件下可以带来适度提升；然而，完全监督式的微调常常会降低准确性或校准质量。此外，研究还探讨了不同方法对于模型性能、校准度和公平性的影响。

Conclusion: 选择是否对TFMs进行微调应基于具体的应用场景和数据特性来决定。本研究为实践中判断何时采用微调提供了指导，并指出了现有方法的一些局限性。

Abstract: Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.

</details>


### [73] [Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection](https://arxiv.org/abs/2601.09684)
*Ziyu Yang,Guibin Chen,Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Ortho-LoRA的方法，通过动态地将冲突的任务梯度投影到彼此的正交补空间内来解决多任务学习中使用低秩适应（LoRA）时遇到的任务干扰问题。实验表明该方法能够有效缓解任务间的干扰，并且几乎不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 多任务学习(MTL)结合低秩适应(LoRA)虽然减少了大型语言模型(LLMs)部署时的存储开销，但会导致由于不同任务间梯度更新冲突而引起的表现下降问题。特别是对于LoRA来说，其低秩约束限制了优化空间对多样化任务需求的适应能力。

Method: 提出了Ortho-LoRA方法，这是一种专门针对LoRA的双部分结构设计的梯度投影技术。它能够在LoRA固有的子空间内动态地把相互冲突的任务梯度投射到对方的正交补空间上。

Result: 在GLUE基准测试上的广泛实验显示，与标准联合训练相比，Ortho-LoRA能更有效地减轻任务之间的干扰，并且恢复了多任务和单任务基线之间95%的表现差距，同时几乎没有引入额外的计算负担。

Conclusion: Ortho-LoRA为采用LoRA进行多任务学习提供了一个有效的解决方案，能够显著减少跨任务负面迁移的影响，同时保持较低的计算成本。

Abstract: Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.

</details>


### [74] [Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design](https://arxiv.org/abs/2601.09693)
*Lisa Schneckenreiter,Sohvi Luukkonen,Lukas Friedrich,Daniel Kuhn,Günter Klambauer*

Main category: cs.LG

TL;DR: 介绍了一种新的计算药物设计方法ConGLUDe，该方法通过对比几何学习统一了基于结构和基于配体的训练方式，不需要预先定义结合口袋，并且在虚拟筛选、靶点识别以及配体条件下的口袋选择方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统上，基于结构和基于配体的计算药物设计依赖于不同的数据源和建模假设，这限制了它们的大规模联合应用。为了克服这一局限性，提出了ConGLUDe模型，旨在开发一种能够同时处理两种类型信息的方法，以提高药物发现过程中的效率和准确性。

Method: ConGLUDe利用对比几何学习技术，将一个能够产生整个蛋白质表示及预测结合位点隐含嵌入的几何蛋白编码器与快速配体编码器相结合，从而无需预设结合口袋的位置。通过对比学习使配体与全局蛋白质表示及多个候选结合位点相匹配，支持除虚拟筛选和靶点识别外还具备配体条件下口袋预测的能力。

Result: ConGLUDe在多种基准测试中展示了最先进的零样本虚拟筛选性能，在没有提供结合口袋信息的情况下尤为突出；在具有挑战性的靶点识别任务上显著优于现有方法；并且在配体条件下的口袋选择方面也表现得相当有竞争力。

Conclusion: 这些结果表明了统一结构-配体训练的优势，并为药物发现领域朝着通用基础模型的方向迈进奠定了基础。

Abstract: Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.

</details>
