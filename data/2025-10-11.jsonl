{"id": "2510.07484", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07484", "abs": "https://arxiv.org/abs/2510.07484", "authors": ["Haoyu Han", "Kai Guo", "Harry Shomer", "Yu Wang", "Yucheng Chu", "Hang Li", "Li Ma", "Jiliang Tang"], "title": "Reasoning by Exploration: A Unified Approach to Retrieval and Generation over Graphs", "comment": null, "summary": "Reasoning over structured graphs remains a fundamental challenge for Large\nLanguage Models (LLMs), particularly when scaling to large graphs. Existing\napproaches typically follow the retrieval-augmented generation (RAG) paradigm:\nfirst retrieving subgraphs relevant to the query and then generating answers\nconditioned on the retrieved subgraphs. However, such two-phase pipelines often\nstruggle to faithfully incorporate graph structure, since the generation\nprocess is ultimately constrained by the quality and completeness of the\nretrieved subgraph. Although many advanced retrievers have been proposed\nrecently to mitigate this issue, they are usually tailored to the training\ngraphs and generalize poorly to unseen graphs, which limits their practical\napplicability. In this work, we propose Reasoning by Exploration (RoE), a novel\napproach that unifies retrieval and generation by framing reasoning over graphs\nas a process of graph exploration. At each step, the LLM selects candidate\nnodes and edges to explore, gradually constructing reasoning paths and\ngenerating answers along the way. To enable effective exploration, RoE is\ntrained in two stages: supervised fine-tuning (SFT) on gold reasoning paths,\nfollowed by reinforcement learning (RL) to enhance exploration effectiveness\nand generalization. Experiments on benchmark datasets demonstrate that RoE\nachieves substantial overall improvements over baselines, while also\ngeneralizing effectively to unseen graphs."}
{"id": "2510.07621", "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07621", "abs": "https://arxiv.org/abs/2510.07621", "authors": ["Saeideh Bakhshi", "Phuong Mai Nguyen", "Robert Schiller", "Tiantian Xu", "Pawan Kodandapani", "Andrew Levine", "Cayman Simpson", "Qifan Wang"], "title": "Retentive Relevance: Capturing Long-Term User Value in Recommendation Systems", "comment": null, "summary": "Recommendation systems have traditionally relied on short-term engagement\nsignals, such as clicks and likes, to personalize content. However, these\nsignals are often noisy, sparse, and insufficient for capturing long-term user\nsatisfaction and retention. We introduce Retentive Relevance, a novel\ncontent-level survey-based feedback measure that directly assesses users'\nintent to return to the platform for similar content. Unlike other survey\nmeasures that focus on immediate satisfaction, Retentive Relevance targets\nforward-looking behavioral intentions, capturing longer term user intentions\nand providing a stronger predictor of retention. We validate Retentive\nRelevance using psychometric methods, establishing its convergent,\ndiscriminant, and behavioral validity. Through large-scale offline modeling, we\nshow that Retentive Relevance significantly outperforms both engagement signals\nand other survey measures in predicting next-day retention, especially for\nusers with limited historical engagement. We develop a production-ready proxy\nmodel that integrates Retentive Relevance into the final stage of a multi-stage\nranking system on a social media platform. Calibrated score adjustments based\non this model yield substantial improvements in engagement, and retention,\nwhile reducing exposure to low-quality content, as demonstrated by large-scale\nA/B experiments. This work provides the first empirically validated framework\nlinking content-level user perceptions to retention outcomes in production\nsystems. We offer a scalable, user-centered solution that advances both\nplatform growth and user experience. Our work has broad implications for\nresponsible AI development."}
{"id": "2510.07644", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07644", "abs": "https://arxiv.org/abs/2510.07644", "authors": ["Shuoqi Sun", "Danula Hettiachchi", "Damiano Spina"], "title": "ISMIE: A Framework to Characterize Information Seeking in Modern Information Environments", "comment": "This paper has been accepted to SIGIR-AP 2025", "summary": "The modern information environment (MIE) is increasingly complex, shaped by a\nwide range of techniques designed to satisfy users' information needs.\nInformation seeking (IS) models are effective mechanisms for characterizing\nuser-system interactions. However, conceptualizing a model that fully captures\nthe MIE landscape poses a challenge. We argue: Does such a model exist? To\naddress this, we propose the Information Seeking in Modern Information\nEnvironments (ISMIE) framework as a fundamental step. ISMIE conceptualizes the\ninformation seeking process (ISP) via three key concepts: Components (e.g.,\nInformation Seeker), Intervening Variables (e.g., Interactive Variables), and\nActivities (e.g., Acquiring). Using ISMIE's concepts and employing a case study\nbased on a common scenario - misinformation dissemination - we analyze six\nexisting IS and information retrieval (IR) models to illustrate their\nlimitations and the necessity of ISMIE. We then show how ISMIE serves as an\nactionable framework for both characterization and experimental design. We\ncharacterize three pressing issues and then outline two research blueprints: a\nuser-centric, industry-driven experimental design for the authenticity and\ntrust crisis to AI-generated content and a system-oriented, academic-driven\ndesign for tackling dopamine-driven content consumption. Our framework offers a\nfoundation for developing IS and IR models to advance knowledge on\nunderstanding human interactions and system design in MIEs."}
{"id": "2510.07720", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07720", "abs": "https://arxiv.org/abs/2510.07720", "authors": ["Peyang Liu", "Xi Wang", "Ziqiang Cui", "Wei Ye"], "title": "Queries Are Not Alone: Clustering Text Embeddings for Video Search", "comment": "Accepted by International ACM SIGIR Conference on Research and\n  Development in Information Retrieval 2025", "summary": "The rapid proliferation of video content across various platforms has\nhighlighted the urgent need for advanced video retrieval systems. Traditional\nmethods, which primarily depend on directly matching textual queries with video\nmetadata, often fail to bridge the semantic gap between text descriptions and\nthe multifaceted nature of video content. This paper introduces a novel\nframework, the Video-Text Cluster (VTC), which enhances video retrieval by\nclustering text queries to capture a broader semantic scope. We propose a\nunique clustering mechanism that groups related queries, enabling our system to\nconsider multiple interpretations and nuances of each query. This clustering is\nfurther refined by our innovative Sweeper module, which identifies and\nmitigates noise within these clusters. Additionally, we introduce the\nVideo-Text Cluster-Attention (VTC-Att) mechanism, which dynamically adjusts\nfocus within the clusters based on the video content, ensuring that the\nretrieval process emphasizes the most relevant textual features. Further\nexperiments have demonstrated that our proposed model surpasses existing\nstate-of-the-art models on five public datasets."}
{"id": "2510.07728", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07728", "abs": "https://arxiv.org/abs/2510.07728", "authors": ["Peiyang Liu", "Ziqiang Cui", "Di Liang", "Wei Ye"], "title": "Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by\nmitigating hallucinations and outdated information issues, yet simultaneously\nfacilitates unauthorized data appropriation at scale. This paper addresses this\nchallenge through two key contributions. First, we introduce RPD, a novel\ndataset specifically designed for RAG plagiarism detection that encompasses\ndiverse professional domains and writing styles, overcoming limitations in\nexisting resources. Second, we develop a dual-layered watermarking system that\nembeds protection at both semantic and lexical levels, complemented by an\ninterrogator-detective framework that employs statistical hypothesis testing on\naccumulated evidence. Extensive experimentation demonstrates our approach's\neffectiveness across varying query volumes, defense prompts, and retrieval\nparameters, while maintaining resilience against adversarial evasion\ntechniques. This work establishes a foundational framework for intellectual\nproperty protection in retrieval-augmented AI systems."}
{"id": "2510.07784", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07784", "abs": "https://arxiv.org/abs/2510.07784", "authors": ["Ruining He", "Lukasz Heldt", "Lichan Hong", "Raghunandan Keshavan", "Shifan Mao", "Nikhil Mehta", "Zhengyang Su", "Alicia Tsai", "Yueqi Wang", "Shao-Chuan Wang", "Xinyang Yi", "Lexi Baugher", "Baykal Cakici", "Ed Chi", "Cristos Goodrow", "Ningren Han", "He Ma", "Romer Rosales", "Abby Van Soest", "Devansh Tandon", "Su-Lin Wu", "Weilong Yang", "Yilin Zheng"], "title": "PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations", "comment": "11 pages, 6 figures", "summary": "Large Language Models (LLMs) pose a new paradigm of modeling and computation\nfor information tasks. Recommendation systems are a critical application domain\npoised to benefit significantly from the sequence modeling capabilities and\nworld knowledge inherent in these large models. In this paper, we introduce\nPLUM, a framework designed to adapt pre-trained LLMs for industry-scale\nrecommendation tasks. PLUM consists of item tokenization using Semantic IDs,\ncontinued pre-training (CPT) on domain-specific data, and task-specific\nfine-tuning for recommendation objectives. For fine-tuning, we focus\nparticularly on generative retrieval, where the model is directly trained to\ngenerate Semantic IDs of recommended items based on user context. We conduct\ncomprehensive experiments on large-scale internal video recommendation\ndatasets. Our results demonstrate that PLUM achieves substantial improvements\nfor retrieval compared to a heavily-optimized production model built with large\nembedding tables. We also present a scaling study for the model's retrieval\nperformance, our learnings about CPT, a few enhancements to Semantic IDs, along\nwith an overview of the training and inference methods that enable launching\nthis framework to billions of users in YouTube."}
{"id": "2510.07885", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07885", "abs": "https://arxiv.org/abs/2510.07885", "authors": ["Madoka Hagiri", "Kazushi Okamoto", "Koki Karube", "Kei Harada", "Atsushi Shibata"], "title": "Generation and annotation of item usage scenarios in e-commerce using large language models", "comment": null, "summary": "Complementary recommendations suggest combinations of useful items that play\nimportant roles in e-commerce. However, complementary relationships are often\nsubjective and vary among individuals, making them difficult to infer from\nhistorical data. Unlike conventional history-based methods that rely on\nstatistical co-occurrence, we focus on the underlying usage context that\nmotivates item combinations. We hypothesized that people select complementary\nitems by imagining specific usage scenarios and identifying the needs in such\nsituations. Based on this idea, we explored the use of large language models\n(LLMs) to generate item usage scenarios as a starting point for constructing\ncomplementary recommendation systems. First, we evaluated the plausibility of\nLLM-generated scenarios through manual annotation. The results demonstrated\nthat approximately 85% of the generated scenarios were determined to be\nplausible, suggesting that LLMs can effectively generate realistic item usage\nscenarios."}
{"id": "2510.08048", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08048", "abs": "https://arxiv.org/abs/2510.08048", "authors": ["Jianhui Yang", "Yiming Jin", "Pengkun Jiao", "Chenhe Dong", "Zerui Huang", "Shaowei Yao", "Xiaojiang Zhou", "Dan Ou", "Haihong Tang"], "title": "TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance", "comment": null, "summary": "Query-product relevance prediction is fundamental to e-commerce search and\nhas become even more critical in the era of AI-powered shopping, where semantic\nunderstanding and complex reasoning directly shape the user experience and\nbusiness conversion. Large Language Models (LLMs) enable generative,\nreasoning-based approaches, typically aligned via supervised fine-tuning (SFT)\nor preference optimization methods like Direct Preference Optimization (DPO).\nHowever, the increasing complexity of business rules and user queries exposes\nthe inability of existing methods to endow models with robust reasoning\ncapacity for long-tail and challenging cases. Efforts to address this via\nreinforcement learning strategies like Group Relative Policy Optimization\n(GRPO) often suffer from sparse terminal rewards, offering insufficient\nguidance for multi-step reasoning and slowing convergence. To address these\nchallenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning\nframework for LLM-based relevance prediction in Taobao Search Relevance.\nTaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which\ndecomposes the final relevance judgment into dense, structured rewards aligned\nwith domain-specific relevance criteria; and (2) Adaptive Guided Replay, which\nidentifies low-accuracy rollouts during training and injects targeted\nground-truth guidance to steer the policy away from stagnant, rule-violating\nreasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on\nlarge-scale real-world datasets and through online side-by-side human\nevaluations on Taobao Search. It consistently outperforms DPO and standard GRPO\nbaselines in offline experiments, improving relevance accuracy, rule adherence,\nand training stability. The model trained with TaoSR-AGRL has been successfully\ndeployed in the main search scenario on Taobao, serving hundreds of millions of\nusers."}
{"id": "2510.08109", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08109", "abs": "https://arxiv.org/abs/2510.08109", "authors": ["Daniel Huwiler", "Kurt Stockinger", "Jonathan Fürst"], "title": "VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems fail when documents evolve\nthrough versioning-a ubiquitous characteristic of technical documentation.\nExisting approaches achieve only 58-64% accuracy on version-sensitive\nquestions, retrieving semantically similar content without temporal validity\nchecks. We present VersionRAG, a version-aware RAG framework that explicitly\nmodels document evolution through a hierarchical graph structure capturing\nversion sequences, content boundaries, and changes between document states.\nDuring retrieval, VersionRAG routes queries through specialized paths based on\nintent classification, enabling precise version-aware filtering and change\ntracking. On our VersionQA benchmark-100 manually curated questions across 34\nversioned technical documents-VersionRAG achieves 90% accuracy, outperforming\nnaive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit\nchange detection where baselines fail (0-10%), demonstrating its ability to\ntrack undocumented modifications. Additionally, VersionRAG requires 97% fewer\ntokens during indexing than GraphRAG, making it practical for large-scale\ndeployment. Our work establishes versioned document QA as a distinct task and\nprovides both a solution and benchmark for future research."}
{"id": "2510.08252", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08252", "abs": "https://arxiv.org/abs/2510.08252", "authors": ["Jianlyu Chen", "Junwei Lan", "Chaofan Li", "Defu Lian", "Zheng Liu"], "title": "ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval", "comment": "17 pages, 3 figures", "summary": "In this paper, we introduce ReasonEmbed, a novel text embedding model\ndeveloped for reasoning-intensive document retrieval. Our work includes three\nkey technical contributions. First, we propose ReMixer, a new data synthesis\nmethod that overcomes the triviality problem prevalent in previous synthetic\ndatasets, enabling large-scale production of 82K high-quality training samples.\nSecond, we design Redapter, a self-adaptive learning algorithm that dynamically\nadjusts training each sample's weight based on its reasoning intensity. This\nallows the model to effectively capture the complex semantic relationships\nbetween queries and documents. Third, we implement ReasonEmbed across multiple\nbackbones of varying sizes, all of which achieve superior performance on\nreasoning-intensive retrieval tasks. Notably, our ReasonEmbed-Qwen3-8B model\noffers a record-high nDCG@10 score of 38.1 on the BRIGHT benchmark, which\nsignificantly outperforms existing text embedding models. We will fully\nopen-source our created resources in ReasonEmbed to push forward the research\nadvancement in this field."}
{"id": "2510.08281", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08281", "abs": "https://arxiv.org/abs/2510.08281", "authors": ["Tianwei Li", "Yu Zhao", "Yunze Li", "Sheng Li"], "title": "Mobile Gamer Lifetime Value Prediction via Objective Decomposition and Reconstruction", "comment": "6 pages, 6 figures", "summary": "For Internet platforms operating real-time bidding (RTB) advertising service,\na comprehensive understanding of user lifetime value (LTV) plays a pivotal role\nin optimizing advertisement allocation efficiency and maximizing the return on\ninvestment (ROI) for advertisement sponsors, thereby facilitating growth of\ncommercialization revenue for the platform. However, the inherent complexity of\nuser LTV distributions induces significant challenges in accurate LTV\nprediction. Existing state-of-the-art works, which primarily focus on directly\nlearning the LTV distributions through well-designed loss functions, achieve\nlimited success due to their vulnerability to outliers. In this paper, we\nproposed a novel LTV prediction method to address distribution challenges\nthrough an objective decomposition and reconstruction framework. Briefly\nspeaking, based on the in-app purchase characteristics of mobile gamers, our\nmodel was designed to first predict the number of transactions at specific\nprices and then calculate the total payment amount from these intermediate\npredictions. Our proposed model was evaluated through experiments on real-world\nindustrial dataset, and deployed on the TapTap RTB advertising system for\nonline A/B testing along with the state-of-the-art ZILN model."}
