{"id": "2511.02052", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02052", "abs": "https://arxiv.org/abs/2511.02052", "authors": ["Karol Radziszewski", "Michał Szpunar", "Piotr Ociepka", "Mateusz Buczyński"], "title": "Solving cold start in news recommendations: a RippleNet-based system for large scale media outlet", "comment": null, "summary": "We present a scalable recommender system implementation based on RippleNet,\ntailored for the media domain with a production deployment in Onet.pl, one of\nPoland's largest online media platforms. Our solution addresses the cold-start\nproblem for newly published content by integrating content-based item\nembeddings into the knowledge propagation mechanism of RippleNet, enabling\neffective scoring of previously unseen items. The system architecture leverages\nAmazon SageMaker for distributed training and inference, and Apache Airflow for\norchestrating data pipelines and model retraining workflows. To ensure\nhigh-quality training data, we constructed a comprehensive golden dataset\nconsisting of user and item features and a separate interaction table, all\nenabling flexible extensions and integration of new signals."}
{"id": "2511.02113", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02113", "abs": "https://arxiv.org/abs/2511.02113", "authors": ["Hai-Dang Kieu", "Min Xu", "Thanh Trung Huynh", "Dung D. Le"], "title": "Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion", "comment": null, "summary": "Recent advances in multimodal recommendation (MMR) have shown that\nincorporating rich content sources such as images and text can lead to\nsignificant gains representation quality. However, existing methods often rely\non coarse visual features and uncontrolled fusion, leading to redundant or\nmisaligned representations. As a result, visual encoders often fail to capture\nsalient, item-relevant semantics, limiting their contribution in multimodal\nfusion. From an information-theoretic perspective, effective fusion should\nbalance the unique, shared, and redundant information across modalities,\npreserving complementary cues while avoiding correlation bias. This paper\npresents VLIF, a vision-language and information-theoretic fusion framework\nthat enhances multimodal recommendation through two key components. (i) A\nVLM-based visual enrichment module generates fine-grained, title-guided\ndescriptions to transform product images into semantically aligned\nrepresentations. (ii) An information-aware fusion module, inspired by Partial\nInformation Decomposition (PID), disentangles redundant and synergistic signals\nacross modalities for controlled integration. Experiments on three Amazon\ndatasets demonstrate that VLIF consistently outperforms recent multimodal\nbaselines and substantially strengthens the contribution of visual features."}
{"id": "2511.02181", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02181", "abs": "https://arxiv.org/abs/2511.02181", "authors": ["Yuhan Wang", "Qing Xie", "Zhifeng Bao", "Mengzi Tang", "Lin Li", "Yongjian Liu"], "title": "KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping Cross-Domain Recommendation", "comment": "13 pages, 4 figures", "summary": "Knowledge Graphs (KGs), as structured knowledge bases that organize\nrelational information across diverse domains, provide a unified semantic\nfoundation for cross-domain recommendation (CDR). By integrating symbolic\nknowledge with user-item interactions, KGs enrich semantic representations,\nsupport reasoning, and enhance model interpretability. Despite this potential,\nexisting KG-based methods still face major challenges in CDR, particularly\nunder non-overlapping user scenarios. These challenges arise from: (C1)\nsensitivity to KG sparsity and popularity bias, (C2) dependence on overlapping\nusers for domain alignment and (C3) lack of explicit disentanglement between\ntransferable and domain-specific knowledge, which limit effective and stable\nknowledge transfer. To this end, we propose KGBridge, a knowledge-guided prompt\nlearning framework for cross-domain sequential recommendation under\nnon-overlapping user scenarios. KGBridge comprises two core components: a\nKG-enhanced Prompt Encoder, which models relation-level semantics as soft\nprompts to provide structured and dynamic priors for user sequence modeling\n(addressing C1), and a Two-stage Training Paradigm, which combines cross-domain\npretraining and privacy-preserving fine-tuning to enable knowledge transfer\nwithout user overlap (addressing C2). By combining relation-aware semantic\ncontrol with correspondence-driven disentanglement, KGBridge explicitly\nseparates and balances domain-shared and domain-specific semantics, thereby\nmaintaining complementarity and stabilizing adaptation during fine-tuning\n(addressing C3). Extensive experiments on benchmark datasets demonstrate that\nKGBridge consistently outperforms state-of-the-art baselines and remains robust\nunder varying KG sparsity, highlighting its effectiveness in mitigating\nstructural imbalance and semantic entanglement in KG-enhanced cross-domain\nrecommendation."}
{"id": "2511.02571", "categories": ["cs.IR", "math.PR", "Primary 60E05, 60C05, Secondary 62R07, 68T05"], "pdf": "https://arxiv.org/pdf/2511.02571", "abs": "https://arxiv.org/abs/2511.02571", "authors": ["Tetiana Manzhos", "Tetiana Ianevych", "Olga Melnyk"], "title": "Average Precision at Cutoff k under Random Rankings: Expectation and Variance", "comment": "17 pages, 2 tables, 2 figures", "summary": "Recommender systems and information retrieval platforms rely on ranking\nalgorithms to present the most relevant items to users, thereby improving\nengagement and satisfaction. Assessing the quality of these rankings requires\nreliable evaluation metrics. Among them, Mean Average Precision at cutoff k\n(MAP@k) is widely used, as it accounts for both the relevance of items and\ntheir positions in the list.\n  In this paper, the expectation and variance of Average Precision at k (AP@k)\nare derived since they can be used as biselines for MAP@k. Here, we covered two\nwidely used evaluation models: offline and online. The expectation establishes\nthe baseline, indicating the level of MAP@k that can be achieved by pure\nchance. The variance complements this baseline by quantifying the extent of\nrandom fluctuations, enabling a more reliable interpretation of observed\nscores."}
{"id": "2511.02234", "categories": ["cs.MM", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.02234", "abs": "https://arxiv.org/abs/2511.02234", "authors": ["Jiawei Liu", "Enis Berk Çoban", "Zarina Schevchenko", "Hao Tang", "Zhigang Zhu", "Michael I Mandel", "Johanna Devaney"], "title": "An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM", "comment": null, "summary": "Standard training for Multi-modal Large Language Models (MLLMs) involves\nconcatenating non-textual information, like vision or audio, with a text\nprompt. This approach may not encourage deep integration of modalities,\nlimiting the model's ability to leverage the core language model's reasoning\ncapabilities. This work examined the impact of interleaved instruction tuning\nin an audio MLLM, where audio tokens are interleaved within the prompt. Using\nthe Listen, Think, and Understand (LTU) model as a testbed, we conduct an\nexperiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our\nnewly created reasoning benchmark for audio-based semantic reasoning focusing\non synonym and hypernym recognition. Our findings show that while even\nzero-shot interleaved prompting improves performance on our reasoning tasks, a\nsmall amount of fine-tuning using interleaved training prompts improves the\nresults further, however, at the expense of the MLLM's audio labeling ability."}
{"id": "2511.02478", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02478", "abs": "https://arxiv.org/abs/2511.02478", "authors": ["Bingyan Xie", "Yongpeng Wu", "Yuxuan Shi", "Biqian Feng", "Wenjun Zhang", "Jihong Park", "Tony Quek"], "title": "Wireless Video Semantic Communication with Decoupled Diffusion Multi-frame Compensation", "comment": null, "summary": "Existing wireless video transmission schemes directly conduct video coding in\npixel level, while neglecting the inner semantics contained in videos. In this\npaper, we propose a wireless video semantic communication framework with\ndecoupled diffusion multi-frame compensation (DDMFC), abbreviated as WVSC-D,\nwhich integrates the idea of semantic communication into wireless video\ntransmission scenarios. WVSC-D first encodes original video frames as semantic\nframes and then conducts video coding based on such compact representations,\nenabling the video coding in semantic level rather than pixel level. Moreover,\nto further reduce the communication overhead, a reference semantic frame is\nintroduced to substitute motion vectors of each frame in common video coding\nmethods. At the receiver, DDMFC is proposed to generate compensated current\nsemantic frame by a two-stage conditional diffusion process. With both the\nreference frame transmission and DDMFC frame compensation, the bandwidth\nefficiency improves with satisfying video transmission performance.\nExperimental results verify the performance gain of WVSC-D over other DL-based\nmethods e.g. DVSC about 1.8 dB in terms of PSNR."}
