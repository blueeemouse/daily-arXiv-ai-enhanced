<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.SE](#cs.SE) [Total: 20]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Accelerating Large-Scale Cheminformatics Using a Byte-Offset Indexing Architecture for Terabyte-Scale Data Integration](https://arxiv.org/abs/2601.18921)
*Malikussaid,Septian Caesar Floresko,Sutiyo*

Main category: cs.DB

TL;DR: 本文研究了通过字节偏移索引方法整合大规模化学数据库（PubChem、ChEMBL和eMolecules）以构建用于分子属性预测的精选数据集。此方法显著提高了处理速度，从预计100天减少到3.2小时，并解决了InChIKey标识符中的哈希冲突问题。最终系统提取了435,413个验证过的化合物。


<details>
  <summary>Details</summary>
Motivation: 在现代化学信息学研究中，特别是对于需要高质量、多源验证数据集的机器学习应用而言，大规模化学数据库的集成是一个关键瓶颈。本研究旨在探索一种更高效的方法来克服这一挑战。

Method: 研究者采用字节偏移索引技术对PubChem、ChEMBL及eMolecules三个大型公共化学品库进行整合，用以创建一个适合于分子特性预测的数据集。此外，由于发现InChIKey分子标识符存在哈希碰撞问题，团队转而使用无碰撞风险的完整InChI字符串重构流程。

Result: 该方法使得原本估计耗时长达百日的暴力搜索算法得以大幅优化，仅需3.2小时即可完成任务，性能提升了约740倍。同时，通过对1.76亿条数据库记录进行全面校验，最终成功提取出435,413个经过验证的化合物。

Conclusion: 研究表明，字节偏移索引能够有效解决大规模科学数据集成中的可扩展性限制问题，在保证数据完整性的同时极大提升了处理效率。此外，当唯一性约束超出基于哈希的标识符能力时，这种方法展示了其在大型科学数据集成方面的一般适用原则。

Abstract: The integration of large-scale chemical databases represents a critical bottleneck in modern cheminformatics research, particularly for machine learning applications requiring high-quality, multi-source validated datasets. This paper presents a case study of integrating three major public chemical repositories: PubChem (176 million compounds), ChEMBL, and eMolecules, to construct a curated dataset for molecular property prediction. We investigate whether byte-offset indexing can practically overcome brute-force scalability limits while preserving data integrity at hundred-million scale. Our results document the progression from an intractable brute-force search algorithm with projected 100-day runtime to a byte-offset indexing architecture achieving 3.2-hour completion-a 740-fold performance improvement through algorithmic complexity reduction from O(NxM) to O(N+M). Systematic validation of 176 million database entries revealed hash collisions in InChIKey molecular identifiers, necessitating pipeline reconstruction using collision-free full InChI strings. We present performance benchmarks, quantify trade-offs between storage overhead and scientific rigor, and compare our approach with alternative large-scale integration strategies. The resulting system successfully extracted 435,413 validated compounds and demonstrates generalizable principles for large-scale scientific data integration where uniqueness constraints exceed hash-based identifier capabilities.

</details>


### [2] [Educational Database Prototype: the Simplest of All](https://arxiv.org/abs/2601.19165)
*Yi Lyu,Yiyin Shen,Takashi Matsuzawa*

Main category: cs.DB

TL;DR: EduDB is a simple, educational database prototype designed to help students gain a comprehensive understanding of database systems, offering a platform for practical optimization exercises.


<details>
  <summary>Details</summary>
Motivation: The current DBMS course at UW-Madison (CS564) focuses on implementing specific modules, such as the B+ tree, but this approach may lead to students spending too much time on handling corner cases rather than gaining a broad understanding of the internal design of databases. EduDB aims to provide a cleaner, more concise overview and a platform for practicing optimizations.

Method: The development of EduDB, a simplified database system, along with an integrative series of course projects, is proposed to offer students a better learning experience in database management. This approach emphasizes providing a high-level interface that allows for various kinds of optimizations while ensuring serializability.

Result: EduDB and its associated course projects are expected to give students a clearer and more comprehensive understanding of database systems, enabling them to apply various optimization techniques learned throughout their studies in a practical setting.

Conclusion: By introducing EduDB into the curriculum, the aim is to improve the teaching and learning process in database management, making it easier for students to grasp the core concepts and apply them through hands-on experience with real-world optimizations.

Abstract: Database Management System (DBMS) is designed to help store and process large collections of data, and is incredibly flexible to perform various kinds of optimizations as long as it achieves serializability with a high-level interface available. The current undergraduate level DBMS course in UW-Madison (i.e., CS564) involves implementing specific modules of DB architecture, including B+ tree, but students may end up spending numerous amounts of effort on corner cases and not gaining a more comprehensive understanding of the internal design. Thus, we present EduDB, a simple database prototype for educational purposes that provides students a clean, concise, and comprehensive overview of the database system. We also attempt to develop an integrative series of course projects based on EduDB, which offers a platform for students to perform any optimization learned during the semester.

</details>


### [3] [Create Benchmarks for Data Lakes](https://arxiv.org/abs/2601.19176)
*Yi Lyu,Pei-Chieh Lo,Natan Lidukhover*

Main category: cs.DB

TL;DR: 本文提出了一种新的数据湖基准测试框架，旨在提供不同数据湖实现的客观和比较评估。该基准测试涵盖了多种数据类型和工作负载模型，包括数据检索、聚合、查询和相似性搜索等，并且设计为可扩展和可重复使用，以在现实和多样的场景下生成数据集并评估数据湖系统。


<details>
  <summary>Details</summary>
Motivation: 现有的数据湖系统缺乏标准化和全面的基准测试来评估性能。当前的基准测试主要针对传统数据仓库，专注于结构化SQL工作负载，无法充分捕捉到数据湖中典型的各种工作负载和访问模式。

Method: 研究者们提出了一种新的数据湖基准测试框架，覆盖了不同类型的数据和工作负载模型，如数据检索、聚合、查询以及相似性搜索等。此框架还测量了关键性能指标，比如查询执行时间、元数据生成时间和元数据大小，并且能够在不同的规模因子下进行评测。

Result: 通过在CloudLab上进行实验，展示了如何利用提出的基准测试来比较商业和开源的数据湖平台。

Conclusion: 新提出的基准测试框架能够有效地对不同数据湖实现进行客观和比较性的评估，支持用户在实际和多样化的场景中生成数据集并对数据湖系统进行评价。

Abstract: Data lakes have emerged as a flexible and scalable solution for storing and analyzing large volumes of heterogeneous data, including structured, semi-structured, and unstructured formats. Despite their growing adoption in both industry and academia, there is a lack of standardized and comprehensive benchmarks for evaluating the performance of data lake systems. Existing benchmarks primarily target traditional data warehouses and focus on structured SQL workloads, making them insufficient for capturing the diverse workloads and access patterns typical of data lakes.
  In this work, we propose a new benchmarking framework for data lakes that aims to provide an objective and comparative evaluation of different data lake implementations. Our benchmark covers multiple data types and workload models, including data retrieval, aggregation, querying, and similarity search, which is a common yet underexplored operation in existing benchmarks. We measure key performance metrics such as query execution time, metadata generation time, and metadata size across different scale factors. The benchmark is designed to be extensible and reproducible, enabling users to generate datasets and evaluate data lake systems under realistic and diverse scenarios. We conduct our experiments on CloudLab and demonstrate how the proposed benchmark can be used to compare both commercial and open-source data lake platforms.

</details>


### [4] [Topology-Aware Subset Repair via Entropy-Guided Density and Graph Decomposition](https://arxiv.org/abs/2601.19671)
*Guoqi Zhao,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: 提出了一种基于联合密度-冲突惩罚模型的拓扑感知近似子集修复框架，该框架通过结合信息熵和CFD权重来动态调整属性重要性，并引入冲突程度度量以实现拓扑自适应惩罚机制。此外，还开发了两种算法：一种是可扩展的启发式算法PPIS，另一种是具有理论保证的混合整数规划方法MICO。实验结果表明，该方法提高了修复精度和鲁棒性，同时有效保留了高质量数据。


<details>
  <summary>Details</summary>
Motivation: 现有的基于密度的方法在处理数据清洗时存在由脏集群引起的密度偏差、高计算成本以及统一属性加权的问题。为了克服这些问题，本文旨在提出一种新的方法来提高数据清洗的有效性和效率。

Method: 提出一个包含三个关键组件的拓扑感知近似子集修复框架：1）两层冲突检测策略；2）EntroCFDensity密度度量，结合信息熵与CFD权重动态调节属性的重要性；3）定义冲突程度度量，以支持带有动态权重分配的拓扑自适应惩罚机制。另外，将冲突图分解为独立子图，使全局修复问题转化为局部子问题，并据此开发了两个算法：PPIS（可扩展的启发式算法）和MICO（带理论保证的混合整数规划方法）。

Result: 实验结果显示，所提出的方法相较于现有技术，在保持高质量数据的同时提升了修复准确性与鲁棒性。

Conclusion: 本研究提出的拓扑感知近似子集修复框架及其实现算法能够有效地解决数据清洗中存在的多个最小修复方案选择问题，特别是在处理含有不一致元组的数据集时表现出更高的准确率和鲁棒性。

Abstract: Subset repair is an important data cleaning technique that enforces integrity constraints by deleting a minimal number of conflicting tuples, yet multiple minimal repairs often exist. Density-based methods address this ambiguity by favoring repairs that preserve dense, high-quality data regions; however, their effectiveness is limited by density bias from dirty clusters, high computational cost, and uniform attribute weighting. We propose a topology-aware approximate subset repair framework based on a joint density-conflict penalty model. The framework integrates three key components. First, a two-layer conflict detection strategy combines attribute inverted indexes with CFD rule grouping to efficiently identify violations. Second, we introduce EntroCFDensity, a density metric that incorporates information entropy and CFD weights to dynamically adjust attribute importance and reduce homogeneity bias. Third, a conflict degree measure is defined to complement local density, enabling a topology-adaptive penalty mechanism with dynamic weight allocation guided by the coefficient of variation. The conflict graph is further decomposed into independent subgraphs, transforming global repair into tractable local subproblems. Based on this framework, we develop two algorithms: PPIS, a scalable heuristic, and MICO, a mixed-integer programming method with theoretical guarantees. Experimental results show that our approach improves repair accuracy and robustness while effectively preserving high-quality data.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [5] [XProvence: Zero-Cost Multilingual Context Pruning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.18886)
*Youssef Mohamed,Mohamed Elhoseiny,Thibault Formal,Nadezhda Chirkova*

Main category: cs.IR

TL;DR: 本文介绍了XProvence，一个支持超过100种语言的多语言零成本上下文剪枝模型，用于增强检索生成（RAG）。该模型在四个多语言问答基准测试中表现优异，几乎不会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统在不同语言中的使用日益增多，作者旨在将原本仅针对英语的Provence框架推广到更多语言上，以实现高效的零成本上下文剪枝。

Method: 通过训练一个名为XProvence的模型，在16种语言上进行学习，并利用有效的跨语言迁移来支持超过100种语言。该模型基于原始的Provence框架改进而来，后者首次实现了直接集成于重排序模型内的高效零成本上下文剪枝功能。

Result: XProvence能够在不显著影响甚至无损性能的前提下对RAG上下文进行剪枝处理，并且在四个多语言问答基准测试中超越了强大的基线模型。

Conclusion: XProvence证明了多语言环境下零成本上下文剪枝的有效性及其对于提高RAG系统效率的重要作用。

Abstract: This paper introduces XProvence, a multilingual zero-cost context pruning model for retrieval-augmented generation (RAG), trained on 16 languages and supporting 100+ languages through effective cross-lingual transfer. Motivated by the growing use of RAG systems across diverse languages, we explore several strategies to generalize the Provence framework-which first integrated efficient zero-cost context pruning directly into the re-ranking model-beyond English. Across four multilingual question answering benchmarks, we show how XProvence can prune RAG contexts with minimal-to-no performance degradation and outperforms strong baselines. Our model is available at https://huggingface.co/naver/xprovence-reranker-bgem3-v2.

</details>


### [6] [Recommending Composite Items Using Multi-Level Preference Information: A Joint Interaction Modeling Approach](https://arxiv.org/abs/2601.19005)
*Xuan Bi,Yaqiong Wang,Gediminas Adomavicius,Shawn Curley*

Main category: cs.IR

TL;DR: 提出了一种名为JIMA的联合交互建模方法，用于处理复合项目推荐问题。该方法能利用不同粒度级别的数据来学习用户偏好之间的复杂关系，并在多种实验设置下表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能技术的进步，推荐系统被广泛应用于各种平台以高效匹配用户与商品。面对日益多样化且复杂的使用场景，对更高级推荐技术的需求也在增加。例如，在时尚搭配这类复合项目推荐中，可能需要考虑多个层次的用户偏好信息。

Method: 提出了JIMA（Joint Interaction Modeling Approach），这是一种能够利用单一模型处理来自不同细粒度级别数据的方法，通过整合交互作用来学习低阶（原子项）与高阶（复合项）用户偏好之间以及领域知识（如风格匹配）间复杂关系的技术。

Result: 通过多次模拟研究及真实数据在线上线下环境中的测试表明，所提方法相较于其他先进基线具有持续优越的表现。

Conclusion: 本研究表明，JIMA方法在处理包含多层级用户偏好信息的复合项目推荐时展现出显著优势，为未来相关领域的研究提供了新的思路。

Abstract: With the advancement of machine learning and artificial intelligence technologies, recommender systems have been increasingly used across a vast variety of platforms to efficiently and effectively match users with items. As application contexts become more diverse and complex, there is a growing need for more sophisticated recommendation techniques. One example is the composite item (for example, fashion outfit) recommendation where multiple levels of user preference information might be available and relevant. In this study, we propose JIMA, a joint interaction modeling approach that uses a single model to take advantage of all data from different levels of granularity and incorporate interactions to learn the complex relationships among lower-order (atomic item) and higher-order (composite item) user preferences as well as domain expertise (e.g., on the stylistic fit). We comprehensively evaluate the proposed method and compare it with advanced baselines through multiple simulation studies as well as with real data in both offline and online settings. The results consistently demonstrate the superior performance of the proposed approach.

</details>


### [7] [Accelerating Generative Recommendation via Simple Categorical User Sequence Compression](https://arxiv.org/abs/2601.19158)
*Qijiong Liu,Lu Fan,Zhongzhou Liu,Xiaoyu Dong,Yuankai Luo,Guoyuan An,Nuo Chen,Wei Guo,Yong Liu,Xiao-Ming Wu*

Main category: cs.IR

TL;DR: 提出了一种通过利用项目类别特征来压缩长期用户历史记录的方法，从而在保持用户兴趣的同时提高效率。实验表明，与HSTU模型相比，该方法可以将计算成本降低6倍，并且在相似的成本下准确性提高了39%。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统尽管在处理较长序列时表现更好，但其实时部署受到高昂的计算成本限制。为解决这一问题，研究旨在找到一种既能保持用户兴趣又能提高效率的方法来压缩长用户历史。

Method: 通过利用固有的项目类别特征来压缩长期用户历史记录。

Result: 与著名的HSTU模型相比，本方法能够实现高达6倍的计算成本减少，在类似成本（即相似序列长度）的情况下，精度提高了39%。

Conclusion: 提出的方法不仅有效地降低了生成式推荐系统的计算负担，同时也在保持甚至提高推荐准确度方面展现出了显著优势。

Abstract: Although generative recommenders demonstrate improved performance with longer sequences, their real-time deployment is hindered by substantial computational costs. To address this challenge, we propose a simple yet effective method for compressing long-term user histories by leveraging inherent item categorical features, thereby preserving user interests while enhancing efficiency. Experiments on two large-scale datasets demonstrate that, compared to the influential HSTU model, our approach achieves up to a 6x reduction in computational cost and up to 39% higher accuracy at comparable cost (i.e., similar sequence length).

</details>


### [8] [HELM: A Human-Centered Evaluation Framework for LLM-Powered Recommender Systems](https://arxiv.org/abs/2601.19197)
*Sushant Mehta*

Main category: cs.IR

TL;DR: 本文提出了一种新的评估框架HELMM，旨在全面评估基于大型语言模型的推荐系统在五个以用户为中心的维度上的表现：意图对齐、解释质量、交互自然性、信任与透明度以及公平性与多样性。通过实验发现，尽管GPT-4在解释质量和交互自然性方面表现出色，但它显示出显著的流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法主要集中在传统准确度指标上，未能捕捉到决定实际用户体验的多方面的人文特质。因此，需要一个更加全面的评价体系来衡量基于大语言模型的推荐系统的性能。

Method: 设计了一个名为HELMM的综合评估框架，该框架围绕五个关键领域进行评估：意图一致性、解释的质量、互动的自然度、信任和透明度及公平性和多样性。研究者们利用三个最先进的基于大语言模型的推荐系统（GPT-4、LLaMA-3.1 和 P5）在电影、书籍和餐馆三个领域进行了广泛测试，并由12位领域专家根据847个推荐场景进行了严格评估。

Result: 结果显示，虽然GPT-4在提供高质量解释(得分为4.21/5.0)和保持自然对话(得分为4.35/5.0)方面优于其他系统，但其倾向于推荐更受欢迎的内容(Gini系数为0.73)，相比传统的协同过滤算法(Gini系数0.58)而言，这表明存在较大的流行度偏见问题。

Conclusion: 研究表明，所提出的HELMM框架能够揭示出传统度量标准无法显示的关键质量维度。此外，还公开发布了HELMM作为开源工具包，以促进推荐系统领域内以人为中心的评估实践发展。

Abstract: The integration of Large Language Models (LLMs) into recommendation systems has introduced unprecedented capabilities for natural language understanding, explanation generation, and conversational interactions. However, existing evaluation methodologies focus predominantly on traditional accuracy metrics, failing to capture the multifaceted human-centered qualities that determine the real-world user experience. We introduce \framework{} (\textbf{H}uman-centered \textbf{E}valuation for \textbf{L}LM-powered reco\textbf{M}menders), a comprehensive evaluation framework that systematically assesses LLM-powered recommender systems across five human-centered dimensions: \textit{Intent Alignment}, \textit{Explanation Quality}, \textit{Interaction Naturalness}, \textit{Trust \& Transparency}, and \textit{Fairness \& Diversity}. Through extensive experiments involving three state-of-the-art LLM-based recommenders (GPT-4, LLaMA-3.1, and P5) across three domains (movies, books, and restaurants), and rigorous evaluation by 12 domain experts using 847 recommendation scenarios, we demonstrate that \framework{} reveals critical quality dimensions invisible to traditional metrics. Our results show that while GPT-4 achieves superior explanation quality (4.21/5.0) and interaction naturalness (4.35/5.0), it exhibits a significant popularity bias (Gini coefficient 0.73) compared to traditional collaborative filtering (0.58). We release \framework{} as an open-source toolkit to advance human-centered evaluation practices in the recommender systems community.

</details>


### [9] [Propagating Similarity, Mitigating Uncertainty: Similarity Propagation-enhanced Uncertainty for Multimodal Recommendation](https://arxiv.org/abs/2601.19198)
*Xinzhuo Wu,Hongbo Wang,Yuan Lin,Kan Xu,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架SPUMR，通过构建模态相似性图和协作相似性图来明确建模并减少不确定性，从而改进多模态推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推荐系统通常受到模态特征中固有噪声和不确定性的阻碍，并且未能充分利用用户与项目间的丰富相似性模式来精炼表示及其相应的不确定性估计。

Method: 提出了一种名为SPUMR的新框架，该框架首先构建模态相似性图和协作相似性图以从内容和行为两个角度精炼表示，然后使用不确定性感知偏好聚合模块自适应地融合精炼后的多模态特征，为更可靠的模态分配更大的权重。

Result: 在三个基准数据集上的广泛实验表明，SPUMR相比现有领先方法取得了显著改进。

Conclusion: 通过有效处理模态特定的不确定性和利用用户-项目间丰富的相似性模式，SPUMR为多模态推荐提供了一个有效的解决方案。

Abstract: Multimodal Recommendation (MMR) systems are crucial for modern platforms but are often hampered by inherent noise and uncertainty in modal features, such as blurry images, diverse visual appearances, or ambiguous text. Existing methods often overlook this modality-specific uncertainty, leading to ineffective feature fusion. Furthermore, they fail to leverage rich similarity patterns among users and items to refine representations and their corresponding uncertainty estimates. To address these challenges, we propose a novel framework, Similarity Propagation-enhanced Uncertainty for Multimodal Recommendation (SPUMR). SPUMR explicitly models and mitigates uncertainty by first constructing the Modality Similarity Graph and the Collaborative Similarity Graph to refine representations from both content and behavioral perspectives. The Uncertainty-aware Preference Aggregation module then adaptively fuses the refined multimodal features, assigning greater weight to more reliable modalities. Extensive experiments on three benchmark datasets demonstrate that SPUMR achieves significant improvements over existing leading methods.

</details>


### [10] [Physics-Informed Neuro-Symbolic Recommender System: A Dual-Physics Approach for Personalized Nutrition](https://arxiv.org/abs/2601.19244)
*Chayan Banerjee*

Main category: cs.IR

TL;DR: 本文提出了一种结合营养科学的物理信息神经符号推荐系统，通过双层架构确保推荐的商品组合符合用户的蛋白质和热量目标。


<details>
  <summary>Details</summary>
Motivation: 传统的电子商务推荐系统主要优化用户参与度和购买可能性，往往忽略了人体健康所需的严格生理限制。标准的协同过滤算法在结构上对这些硬性限制视而不见，经常建议不符合特定每日总能量消耗和宏量营养素平衡要求的商品组合。

Method: 该论文介绍了一种物理信息神经符号推荐系统，通过双层架构将营养科学直接整合到推荐流程中。首先使用句子级编码器构建语义知识图谱，将商业产品与权威营养数据严格对齐；然后在训练阶段应用隐式物理正则化器，利用可微热力学损失函数保证学习到的潜在嵌入反映营养学上的合理性而非单纯的受欢迎程度；最后，在推理阶段采用显式物理优化器，利用模拟退火和弹性数量优化生成严格遵守用户蛋白质和卡路里目标的离散杂货组合。

Result: 所提出的系统能够生成既满足用户偏好又符合其营养需求的商品组合，从而填补了传统推荐系统在考虑人体健康生理限制方面的空白。

Conclusion: 本研究开发的物理信息神经符号推荐系统为电子商务领域提供了一个新的方向，它不仅考虑了用户的兴趣偏好，还融入了重要的营养科学原理，以促进更健康的消费选择。

Abstract: Traditional e-commerce recommender systems primarily optimize for user engagement and purchase likelihood, often neglecting the rigid physiological constraints required for human health. Standard collaborative filtering algorithms are structurally blind to these hard limits, frequently suggesting bundles that fail to meet specific total daily energy expenditure and macronutrient balance requirements. To address this disconnect, this paper introduces a Physics-Informed Neuro-Symbolic Recommender System that integrates nutritional science directly into the recommendation pipeline via a dual-layer architecture. The framework begins by constructing a semantic knowledge graph using sentence-level encoders to strictly align commercial products with authoritative nutritional data. During the training phase, an implicit physics regularizer applies a differentiable thermodynamic loss function, ensuring that learned latent embeddings reflect nutritional plausibility rather than simple popularity. Subsequently, during the inference phase, an explicit physics optimizer employs simulated annealing and elastic quantity optimization to generate discrete grocery bundles that strictly adhere to the user's protein and caloric targets.

</details>


### [11] [Talos: Optimizing Top-$K$ Accuracy in Recommender Systems](https://arxiv.org/abs/2601.19276)
*Shengjia Zhang,Weiqin Yang,Jiawei Chen,Peng Wu,Yuegang Sun,Gang Wang,Qihao Shi,Can Wang*

Main category: cs.IR

TL;DR: 本文提出了一种名为Talos的损失函数，旨在优化推荐系统的Top-K准确性。Talos通过使用分位数技术简化了排名相关操作，并引入了基于采样的回归算法来高效准确地估计阈值。同时，还加入了约束项以防止分数膨胀，并采用了定制的替代函数来处理不连续性并增强对分布偏移的鲁棒性。理论分析和实验证明了Talos的有效性、效率、收敛性和抗分布变化的能力。


<details>
  <summary>Details</summary>
Motivation: 推荐系统主要关注Top-K结果的质量，但估计Top-K准确性需要确定项目的排名位置，这会带来大量的计算开销并对优化构成挑战。此外，用户偏好或数据偏差导致的分布偏移也增加了任务难度。

Method: 提出了Talos，一种特别设计用于优化推荐准确性的损失函数。该方法利用分位数技术将复杂的排名依赖操作转换为预测分数与学习到的分数阈值之间的简单比较；开发了一种基于采样的回归算法以实现高效的阈值估计；引入了一个约束项来保持优化稳定性，防止分数膨胀；采用了一个量身定制的替代函数来解决不连续性问题并提高对分布变化的鲁棒性。

Result: 综合的理论分析和实验表明，Talos在有效性、效率、收敛性以及对抗分布变化方面表现出色。

Conclusion: Talos提供了一种有效的方法来优化推荐系统的Top-K准确性，同时解决了计算开销大和优化困难的问题，增强了对数据分布变化的鲁棒性。

Abstract: Recommender systems (RS) aim to retrieve a small set of items that best match individual user preferences. Naturally, RS place primary emphasis on the quality of the Top-$K$ results rather than performance across the entire item set. However, estimating Top-$K$ accuracy (e.g., Precision@$K$, Recall@$K$) requires determining the ranking positions of items, which imposes substantial computational overhead and poses significant challenges for optimization. In addition, RS often suffer from distribution shifts due to evolving user preferences or data biases, further complicating the task.
  To address these issues, we propose Talos, a loss function that is specifically designed to optimize the Talos recommendation accuracy. Talos leverages a quantile technique that replaces the complex ranking-dependent operations into simpler comparisons between predicted scores and learned score thresholds. We further develop a sampling-based regression algorithm for efficient and accurate threshold estimation, and introduce a constraint term to maintain optimization stability by preventing score inflation. Additionally, we incorporate a tailored surrogate function to address discontinuity and enhance robustness against distribution shifts. Comprehensive theoretical analyzes and empirical experiments are conducted to demonstrate the effectiveness, efficiency, convergence, and distributional robustness of Talos. The code is available at https://github.com/cynthia-shengjia/WWW-2026-Talos.

</details>


### [12] [UniRec: Unified Multimodal Encoding for LLM-Based Recommendations](https://arxiv.org/abs/2601.19423)
*Zijie Lei,Tao Feng,Zhigang Hua,Yan Xie,Guanyu Lin,Shuang Yang,Ge Liu,Jiaxuan You*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的多模态推荐系统UniRec，该系统能够处理文本、图像、类别特征和数值属性四种模ality的数据，并通过特定编码器生成一致的嵌入，采用三元表示法来区分模式与原始输入，并保持语义差异。实验表明，UniRec在多个实际基准测试中比现有最先进系统高出最多15%的表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的推荐信号远不止文本和图像两种模态，还包括类别特征和数值属性等。这些不同类型的信息给大语言模型理解多模态信息带来了独特的挑战，包括跨模态和同模态内的挑战。此外，用户历史记录中存在的嵌套结构也增加了复杂性。为了解决这些问题，提出了UniRec这一解决方案。

Method: UniRec首先使用针对不同模态设计的编码器生成一致性的嵌入表示；接着采用一种由属性名、类型和值组成的三元组表示方法，以区分数据模式与原始输入并保留语义上的区别；最后，利用层次化的Q-Former模型来捕捉用户交互行为中的嵌套结构，同时保持其层级组织。

Result: 在多个真实世界的基准测试中，UniRec相较于最先进的多模态及基于大语言模型的推荐系统性能提升了高达15%。广泛的消融研究进一步验证了每个组成部分的有效性。

Conclusion: UniRec作为一种统一的多模态编码器，在处理异构信号方面表现出色，能够有效应对多模态推荐中存在的独特挑战。其在提升推荐准确性方面的显著效果证明了该方法的有效性和实用性。

Abstract: Large language models have recently shown promise for multimodal recommendation, particularly with text and image inputs. Yet real-world recommendation signals extend far beyond these modalities. To reflect this, we formalize recommendation features into four modalities: text, images, categorical features, and numerical attributes, and highlight the unique challenges this heterogeneity poses for LLMs in understanding multimodal information. In particular, these challenges arise not only across modalities but also within them, as attributes such as price, rating, and time may all be numeric yet carry distinct semantic meanings. Beyond this intra-modality ambiguity, another major challenge is the nested structure of recommendation signals, where user histories are sequences of items, each associated with multiple attributes. To address these challenges, we propose UniRec, a unified multimodal encoder for LLM-based recommendation. UniRec first employs modality-specific encoders to produce consistent embeddings across heterogeneous signals. It then adopts a triplet representation, comprising attribute name, type, and value, to separate schema from raw inputs and preserve semantic distinctions. Finally, a hierarchical Q-Former models the nested structure of user interactions while maintaining their layered organization. Across multiple real-world benchmarks, UniRec outperforms state-of-the-art multimodal and LLM-based recommenders by up to 15%, and extensive ablation studies further validate the contributions of each component.

</details>


### [13] [Masked Diffusion Generative Recommendation](https://arxiv.org/abs/2601.19501)
*Lingyu Mu,Hao Deng,Haibo Xing,Jinxin Hu,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种名为MDGR的Masked Diffusion Generative Recommendation框架，旨在解决现有生成推荐方法中存在的三个关键限制：全局依赖性捕捉困难、用户对项目属性的关注顺序假设过于统一以及推理时效率低下。实验表明MDGR在多个公开和工业规模数据集上优于最新的十种基准方法，并且在一个大规模在线广告平台上实现了收入1.20%的增长。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐系统虽然已经在推荐性能方面表现出色，但直接从语言模型继承自回归解码范式仍然存在三个主要问题：难以同时捕捉SID不同位置相关联的多维特征之间的全局依赖关系；使用统一固定的解码路径假定所有用户以相同顺序关注物品属性；自回归解码在推理时间效率低下，难以满足实时需求。

Method: 提出了MDGR框架，该框架通过三个方面重塑了生成式推荐流程：采用并行码本为基于扩散的生成式推荐提供结构基础；训练期间沿时间和样本维度自适应构建掩码监督信号；推理阶段开发了一种基于预热的两阶段并行解码策略来高效生成SIDs。

Result: 在多个公共和工业规模的数据集上的广泛实验表明，MDGR比十个最先进的基线方法最多提高了10.78%。此外，在一个大型在线广告平台部署MDGR后，实现了收入1.20%的增长。

Conclusion: MDGR通过引入新的框架设计解决了传统生成式推荐方法中的局限性，不仅在理论上提高了模型性能，还在实际应用中证明了其商业价值。

Abstract: Generative recommendation (GR) typically first quantizes continuous item embeddings into multi-level semantic IDs (SIDs), and then generates the next item via autoregressive decoding. Although existing methods are already competitive in terms of recommendation performance, directly inheriting the autoregressive decoding paradigm from language models still suffers from three key limitations: (1) autoregressive decoding struggles to jointly capture global dependencies among the multi-dimensional features associated with different positions of SID; (2) using a unified, fixed decoding path for the same item implicitly assumes that all users attend to item attributes in the same order; (3) autoregressive decoding is inefficient at inference time and struggles to meet real-time requirements. To tackle these challenges, we propose MDGR, a Masked Diffusion Generative Recommendation framework that reshapes the GR pipeline from three perspectives: codebook, training, and inference. (1) We adopt a parallel codebook to provide a structural foundation for diffusion-based GR. (2) During training, we adaptively construct masking supervision signals along both the temporal and sample dimensions. (3) During inference, we develop a warm-up-based two-stage parallel decoding strategy for efficient generation of SIDs. Extensive experiments on multiple public and industrial-scale datasets show that MDGR outperforms ten state-of-the-art baselines by up to 10.78%. Furthermore, by deploying MDGR on a large-scale online advertising platform, we achieve a 1.20% increase in revenue, demonstrating its practical value. The code will be released upon acceptance.

</details>


### [14] [Enhancing Academic Paper Recommendations Using Fine-Grained Knowledge Entities and Multifaceted Document Embeddings](https://arxiv.org/abs/2601.19513)
*Haixu Xi,Heng Zhang,Chengzhi Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种新的学术论文推荐方法，通过整合细粒度的知识实体、文档标题与摘要以及引用数据等多维信息来生成推荐。实验结果表明，该方法在STM-KG数据集上的前50个推荐的平均准确率为27.3%，比现有方法提高了6.7%。


<details>
  <summary>Details</summary>
Motivation: 当前学术论文推荐系统主要基于一般主题或领域相似性提供宽泛且粗粒度的建议，难以满足学者们更具体和细粒度的需求，如寻找使用特定研究方法的论文或在同一主题下处理不同研究任务的文章。为了应对这一挑战，并提高研究效率及激发创新思维，提出了本研究。

Method: 提出的方法包括了结合细粒度知识实体（如特定的研究方法）、文档标题与摘要、以及引用数据等多元信息进行嵌入，然后通过计算组合后的论文向量之间的相似度来进行推荐。

Result: 采用覆盖十个不同领域的STM-KG数据集对该推荐方法进行了评估。实验结果显示，在前50个推荐中，该方法达到了27.3%的平均准确率，相较于现有的方法提升了6.7%。

Conclusion: 本文介绍的新推荐方法能够更好地满足学者们在研究过程中对文献多样化和特定需求的要求，显示出相比传统方法更好的性能。

Abstract: In the era of explosive growth in academic literature, the burden of literature review on scholars are increasing. Proactively recommending academic papers that align with scholars' literature needs in the research process has become one of the crucial pathways to enhance research efficiency and stimulate innovative thinking. Current academic paper recommendation systems primarily focus on broad and coarse-grained suggestions based on general topic or field similarities. While these systems effectively identify related literature, they fall short in addressing scholars' more specific and fine-grained needs, such as locating papers that utilize particular research methods, or tackle distinct research tasks within the same topic. To meet the diverse and specific literature needs of scholars in the research process, this paper proposes a novel academic paper recommendation method. This approach embeds multidimensional information by integrating new types of fine-grained knowledge entities, title and abstract of document, and citation data. Recommendations are then generated by calculating the similarity between combined paper vectors. The proposed recommendation method was evaluated using the STM-KG dataset, a knowledge graph that incorporates scientific concepts derived from papers across ten distinct domains. The experimental results indicate that our method outperforms baseline models, achieving an average precision of 27.3% among the top 50 recommendations. This represents an improvement of 6.7% over existing approaches.

</details>


### [15] [LURE-RAG: Lightweight Utility-driven Reranking for Efficient RAG](https://arxiv.org/abs/2601.19535)
*Manish Chandra,Debasis Ganguly,Iadh Ounis*

Main category: cs.IR

TL;DR: 提出了一种新的检索增强生成（RAG）框架LURE-RAG，通过使用LambdaMART重排序器优化文档顺序，以提高生成文本的质量。该方法在保持训练和推理效率的同时，在标准数据集上表现出接近最先进的密集神经基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于相关性的检索增强生成（RAG）管道通常不能很好地满足下游任务（如问答或基于查询的摘要）的实际需求，即检索到的段落是否真正提高了生成文本的质量。此外，当前效用驱动的检索方法资源消耗大且缺乏列表级排序损失训练。

Method: 提出了轻量级效用驱动重排序高效RAG (LURE-RAG) 框架，它为任意黑盒检索器添加了一个高效的基于LambdaMART的重排序器，并采用由大型语言模型（LLM）指导的列表级排序损失来直接优化检索文档的顺序。

Result: 实验表明，LURE-RAG 在两个标准数据集上的表现具有竞争力，达到了最先进的密集神经基线性能的97-98%，同时保持了训练和推断过程中的效率。其密集版本UR-RAG相比现有最佳基线提升了多达3%。

Conclusion: LURE-RAG提供了一种有效的方法来改善检索增强生成中检索到的信息与最终生成质量之间的匹配度，同时减少了计算资源的需求。

Abstract: Most conventional Retrieval-Augmented Generation (RAG) pipelines rely on relevance-based retrieval, which often misaligns with utility -- that is, whether the retrieved passages actually improve the quality of the generated text specific to a downstream task such as question answering or query-based summarization. The limitations of existing utility-driven retrieval approaches for RAG are that, firstly, they are resource-intensive typically requiring query encoding, and that secondly, they do not involve listwise ranking loss during training. The latter limitation is particularly critical, as the relative order between documents directly affects generation in RAG. To address this gap, we propose Lightweight Utility-driven Reranking for Efficient RAG (LURE-RAG), a framework that augments any black-box retriever with an efficient LambdaMART-based reranker. Unlike prior methods, LURE-RAG trains the reranker with a listwise ranking loss guided by LLM utility, thereby directly optimizing the ordering of retrieved documents. Experiments on two standard datasets demonstrate that LURE-RAG achieves competitive performance, reaching 97-98% of the state-of-the-art dense neural baseline, while remaining efficient in both training and inference. Moreover, its dense variant, UR-RAG, significantly outperforms the best existing baseline by up to 3%.

</details>


### [16] [Comparing how Large Language Models perform against keyword-based searches for social science research data discovery](https://arxiv.org/abs/2601.19559)
*Mark Green,Maura Halstead,Caroline Jay,Richard Kingston,Alex Singleton,David Topping*

Main category: cs.IR

TL;DR: 本研究通过比较基于大型语言模型（LLM）的语义搜索工具与传统的基于关键词的搜索在数据发现上的表现，发现语义搜索能返回更多结果，尤其擅长处理地点相关、拼写错误、晦涩或复杂的查询。尽管语义搜索与关键词搜索的具体重叠度较低，但返回的数据集在语义上高度相似。两种工具对最相关结果的排名差异显著，反映出不同的优先级策略。案例研究表明，基于LLM的工具能够有效应对拼写错误、解读地理和上下文相关性，并支持自然语言查询。总体而言，LLM驱动的语义搜索为数据发现提供了实质性的改进，是对传统关键词搜索方法的有效补充。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估基于大型语言模型（LLM）的语义搜索工具相较于传统关键词搜索在数据发现方面的能力。随着数据量的增长，寻找更高效准确的数据检索方式变得越来越重要，而语义搜索被认为是一种潜在的解决方案，因为它能够理解查询背后的意图并提供更加相关的搜索结果。

Method: 研究采用来自英国研究与创新机构（UKRI）数据服务的真实搜索行为作为样本，选取了从2023年12月至2024年10月期间CDRC搜索日志中使用频率最高的131个搜索词进行分析。通过对这些搜索词使用定制化的语义搜索系统与现有的关键词搜索系统对比，利用描述性统计、定性检查及包括精确数据集重叠率、Jaccard相似性和BERT嵌入衍生的余弦相似性在内的定量相似度测量来评估两者之间的差异。

Result: 结果显示，对于大多数情况下，特别是针对基于地点、错别字、不常见或复杂查询时，语义搜索比关键词搜索能够返回更多的结果。虽然语义搜索未能捕捉到所有关键词搜索的结果，但是返回的数据集在很大程度上是语义相似的，这体现在即使直接匹配度不高，它们之间也存在着较高的余弦相似性分数。此外，两种工具对于最相关结果的排序存在明显区别，说明了各自采取的不同优先级策略。

Conclusion: 总的来说，研究发现表明基于LLM的语义搜索极大地提高了数据发现效率，尤其是在处理复杂查询时表现尤为出色。它不仅能够很好地理解和解释用户的查询意图，还能有效地处理拼写错误等问题。因此，建议将语义搜索作为现有关键词搜索方法的一个有力补充，而非完全取代之。

Abstract: This paper evaluates the performance of a large language model (LLM) based semantic search tool relative to a traditional keyword-based search for data discovery. Using real-world search behaviour, we compare outputs from a bespoke semantic search system applied to UKRI data services with the Consumer Data Research Centre (CDRC) keyword search. Analysis is based on 131 of the most frequently used search terms extracted from CDRC search logs between December 2023 and October 2024. We assess differences in the volume, overlap, ranking, and relevance of returned datasets using descriptive statistics, qualitative inspection, and quantitative similarity measures, including exact dataset overlap, Jaccard similarity, and cosine similarity derived from BERT embeddings. Results show that the semantic search consistently returns a larger number of results than the keyword search and performs particularly well for place based, misspelled, obscure, or complex queries. While the semantic search does not capture all keyword based results, the datasets returned are overwhelmingly semantically similar, with high cosine similarity scores despite lower exact overlap. Rankings of the most relevant results differ substantially between tools, reflecting contrasting prioritisation strategies. Case studies demonstrate that the LLM based tool is robust to spelling errors, interprets geographic and contextual relevance effectively, and supports natural-language queries that keyword search fails to resolve. Overall, the findings suggest that LLM driven semantic search offers a substantial improvement for data discovery, complementing rather than fully replacing traditional keyword-based approaches.

</details>


### [17] [LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation](https://arxiv.org/abs/2601.19585)
*Chongjun Xia,Yanchun Peng,Xianzhi Wang*

Main category: cs.IR

TL;DR: 本文提出了一种名为LLM-Enhanced Reinforcement Learning (LERL)的新型层次化推荐框架，结合了大语言模型(LLM)的语义规划能力和强化学习(RL)的精细化适应性，旨在解决交互式推荐系统中存在的内容同质化和过滤气泡效应问题。通过高层的基于LLM的规划器选择语义多样性的内容类别，并由低层RL策略在选定的语义空间内推荐个性化项目，实验表明LERL相比现有技术能显著提高用户的长期满意度。


<details>
  <summary>Details</summary>
Motivation: 交互式推荐系统虽然能够动态地适应用户反馈，但往往会因为过度拟合短期用户偏好而导致内容同质化和过滤气泡效应。尽管已有研究试图改善内容多样性，但主要集中在静态或一次性设定下，忽视了用户兴趣的长期演变。此外，尽管强化学习为优化长期用户满意度提供了一个有原则的框架，但在推荐场景下的应用受到稀疏、长尾分布的用户-物品互动以及有限的语义规划能力限制。

Method: 提出了一个名为LLM-Enhanced Reinforcement Learning (LERL)的新框架，该框架整合了大语言模型（LLM）提供的强大语义规划功能与强化学习（RL）对细节的高度适应性。具体来说，LERL包含两部分：一是位于较高层级的基于LLM的规划者，负责挑选出语义上具有多样性的内容分类；二是较低层级的RL策略，在所选的语义范围内向用户推荐个性化的条目。这种分层设计缩小了动作范围，提高了规划效率，并减少了对重复内容的过度暴露。

Result: 在真实世界数据集上的广泛实验显示，与最先进的基线相比，LERL显著提升了用户的长期满意度。

Conclusion: 通过结合大语言模型的语义规划能力与强化学习的自适应推荐机制，LERL框架有效解决了交互式推荐系统中内容同质化及过滤气泡的问题，为用户提供更加多样化且符合其长期兴趣的内容推荐方案。

Abstract: Interactive recommender systems can dynamically adapt to user feedback, but often suffer from content homogeneity and filter bubble effects due to overfitting short-term user preferences. While recent efforts aim to improve content diversity, they predominantly operate in static or one-shot settings, neglecting the long-term evolution of user interests. Reinforcement learning provides a principled framework for optimizing long-term user satisfaction by modeling sequential decision-making processes. However, its application in recommendation is hindered by sparse, long-tailed user-item interactions and limited semantic planning capabilities. In this work, we propose LLM-Enhanced Reinforcement Learning (LERL), a novel hierarchical recommendation framework that integrates the semantic planning power of LLM with the fine-grained adaptability of RL. LERL consists of a high-level LLM-based planner that selects semantically diverse content categories, and a low-level RL policy that recommends personalized items within the selected semantic space. This hierarchical design narrows the action space, enhances planning efficiency, and mitigates overexposure to redundant content. Extensive experiments on real-world datasets demonstrate that LERL significantly improves long-term user satisfaction when compared with state-of-the-art baselines. The implementation of LERL is available at https://anonymous.4open.science/r/code3-18D3/.

</details>


### [18] [Differentiable Semantic ID for Generative Recommendation](https://arxiv.org/abs/2601.19711)
*Junchen Fu,Xuri Ge,Alexandros Karatzoglou,Ioannis Arapakis,Suzan Verberne,Joemon M. Jose,Zhaochun Ren*

Main category: cs.IR

TL;DR: 本文提出了DIGER，一种用于生成推荐的可微语义ID方法。通过引入Gumbel噪声促进早期代码探索，并设计了两种不确定性衰减策略来平衡探索与收敛，从而解决代码本崩溃问题并提高推荐准确性。实验表明该方法在多个公开数据集上表现出一致改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将SID视为预定义项，在静态索引下训练推荐系统，但SID优化仅针对内容重建而非推荐准确性，导致目标不匹配。直接让推荐梯度影响SID学习会导致代码本崩溃的问题。

Method: 提出了一种名为DIGER的方法，通过向SID学习过程中添加Gumbel噪声以鼓励早期阶段对代码进行探索，同时设计了两种不确定性衰减策略来逐渐减少Gumbel噪声，确保从探索平稳过渡到利用已学得的SID。

Result: 在多个公开数据集上的广泛实验显示，使用可微语义ID能够带来一致性的提升，证实了通过可微SID对齐索引和推荐目标的有效性。

Conclusion: 研究表明，通过采用DIGER提出的可微语义索引技术，可以有效解决代码本崩溃问题，并提高SID在生成式推荐中的利用率和推荐准确性，为未来研究指出了一个有前景的方向。

Abstract: Generative recommendation provides a novel paradigm in which each item is represented by a discrete semantic ID (SID) learned from rich content. Most existing methods treat SIDs as predefined and train recommenders under static indexing. In practice, SIDs are typically optimized only for content reconstruction rather than recommendation accuracy. This leads to an objective mismatch: the system optimizes an indexing loss to learn the SID and a recommendation loss for interaction prediction, but because the tokenizer is trained independently, the recommendation loss cannot update it. A natural approach is to make semantic indexing differentiable so that recommendation gradients can directly influence SID learning, but this often causes codebook collapse, where only a few codes are used. We attribute this issue to early deterministic assignments that limit codebook exploration, resulting in imbalance and unstable optimization.
  In this paper, we propose DIGER (Differentiable Semantic ID for Generative Recommendation), a first step toward effective differentiable semantic IDs for generative recommendation. DIGER introduces Gumbel noise to explicitly encourage early-stage exploration over codes, mitigating codebook collapse and improving code utilization. To balance exploration and convergence, we further design two uncertainty decay strategies that gradually reduce the Gumbel noise, enabling a smooth transition from early exploration to exploitation of learned SIDs. Extensive experiments on multiple public datasets demonstrate consistent improvements from differentiable semantic IDs. These results confirm the effectiveness of aligning indexing and recommendation objectives through differentiable SIDs and highlight differentiable semantic indexing as a promising research direction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [NavFormer: IGRF Forecasting in Moving Coordinate Frames](https://arxiv.org/abs/2601.18800)
*Yoontae Hwang,Dongwoo Lee,Minseok Choi,Yong Sup Ihn,Daham Kim,Deok-Young Lee*

Main category: cs.LG

TL;DR: NavFormer利用旋转不变标量特征和规范SPD模块来预测IGRF总强度目标，即使传感器姿态变化也能保持稳定。实验表明，在标准训练、少量样本训练和零样本迁移中，该方法的误差低于强大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决在传感器姿态变化时三轴磁力计组件变化的问题，同时保持IGRF总强度目标不变。

Method: 通过使用旋转不变的标量特征与一个可以稳定三轴窗口级二阶矩谱且无符号间断的Canonical SPD模块。该模块从每个窗口的Gram矩阵构建规范框架，并在原始坐标系中应用状态依赖的谱缩放。

Result: 在五个飞行测试中的结果显示，NavFormer在标准训练、少量样本学习以及零样本迁移任务上的错误率均低于强大的基准对比方法。

Conclusion: Nav Fiormer提供了一种有效的方法来处理因传感器姿态改变导致的磁场测量不稳定问题，对于自主导航系统具有潜在的应用价值。

Abstract: Triad magnetometer components change with sensor attitude even when the IGRF total intensity target stays invariant. NavFormer forecasts this invariant target with rotation invariant scalar features and a Canonical SPD module that stabilizes the spectrum of window level second moments of the triads without sign discontinuities. The module builds a canonical frame from a Gram matrix per window and applies state dependent spectral scaling in the original coordinates. Experiments across five flights show lower error than strong baselines in standard training, few shot training, and zero shot transfer. The code is available at: https://anonymous.4open.science/r/NavFormer-Robust-IGRF-Forecasting-for-Autonomous-Navigators-0765

</details>


### [20] [VAE with Hyperspherical Coordinates: Improving Anomaly Detection from Hypervolume-Compressed Latent Space](https://arxiv.org/abs/2601.18823)
*Alejandro Ascarate,Leo Lebrat,Rodrigo Santa Cruz,Clinton Fookes,Olivier Salvado*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过使用超球坐标来表示变分自编码器（VAE）的潜在变量，以解决高维潜在空间中异常检测的问题。这种方法可以将潜在向量压缩到超球面上的一个给定方向，从而提高近似后验的表达能力，并在完全无监督和OOD（out-of-distribution）异常检测方面表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在高维潜在空间中，标准VAE生成的潜在向量分布于超球体的“赤道”区域，这为异常检测带来了挑战。随着维度增加，超体积呈指数增长，严重影响了VAE的生成能力。为了克服这些问题，研究者希望找到一种改进的方法，以增强VAE在异常检测上的表现。

Method: 提出采用超球坐标表示VAE的潜在变量，使得能够沿着超球面某一特定方向压缩潜在向量，进而获得更具表达力的近似后验。

Result: 该方法不仅提高了VAE在完全无监督环境下对火星探测器相机捕捉到的独特景观及地面影像中的特殊星系等复杂真实世界数据集上异常现象的识别能力，也在Cifar10与ImageNet子集作为ID类别的标准基准测试中展现出色的OOD异常检测性能。

Conclusion: 通过使用超球坐标来调整VAE的潜在变量表示形式，能有效改善模型处理高维数据时的异常检测能力，在多种场景下均超越了现有技术水平。

Abstract: Variational autoencoders (VAE) encode data into lower-dimensional latent vectors before decoding those vectors back to data. Once trained, one can hope to detect out-of-distribution (abnormal) latent vectors, but several issues arise when the latent space is high dimensional. This includes an exponential growth of the hypervolume with the dimension, which severely affects the generative capacity of the VAE. In this paper, we draw insights from high dimensional statistics: in these regimes, the latent vectors of a standard VAE are distributed on the `equators' of a hypersphere, challenging the detection of anomalies. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows compressing the latent vectors towards a given direction on the hypersphere, thereby allowing for a more expressive approximate posterior. We show that this improves both the fully unsupervised and OOD anomaly detection ability of the VAE, achieving the best performance on the datasets we considered, outperforming existing methods. For the unsupervised and OOD modalities, respectively, these are: i) detecting unusual landscape from the Mars Rover camera and unusual Galaxies from ground based imagery (complex, real world datasets); ii) standard benchmarks like Cifar10 and subsets of ImageNet as the in-distribution (ID) class.

</details>


### [21] [IPBC: An Interactive Projection-Based Framework for Human-in-the-Loop Semi-Supervised Clustering of High-Dimensional Data](https://arxiv.org/abs/2601.18828)
*Mohammad Zare*

Main category: cs.LG

TL;DR: 提出了一种交互式基于项目的聚类（IPBC）框架，该框架通过非线性投影模块和用户反馈循环，允许用户调整视角并提供简单的约束条件来改进高维数据的聚类。随着用户的互动，2D布局得到优化，常规聚类算法可以更可靠地识别不同组别，并且解释组件可以帮助理解每个聚类的特征。实验表明少量的交互精炼步骤就能显著提高聚类质量。


<details>
  <summary>Details</summary>
Motivation: 针对高维数据集难以有效聚类的问题，传统降维技术生成静态的2D或3D嵌入，这限制了可解释性并且在探索过程中无法利用分析师的直觉。为了填补这一空白，本文提出了一个将聚类视为迭代的人机协作可视化分析过程的新框架。

Method: 引入了Interactive Project-Based Clustering (IPBC)框架，它结合了一个非线性投影模块和一个反馈回路，让用户能够通过调整观看角度以及指定如必须链接或不能链接等简单约束来修改嵌入。这些约束改变了投影模型的目标函数，逐渐使得语义相关的点靠得更近而无关的点则被推远。随着用户交互使投影变得更加结构化和富有表现力，在优化后的2D布局上运行的传统聚类算法可以更准确地识别出不同的群体。

Result: 在各种基准数据集上的实验表明，只需少数几次交互式的细化步骤就能够大幅改善聚类的质量。此外，通过额外的可解释性组件，可以将发现的每个聚类映射回原始特征空间，产生可解释的规则或特征排名，突出显示每个聚类的不同之处。

Conclusion: IPBC将聚类转变成一个人机协作的发现过程，在这个过程中机器表示与人类洞察互相加强。

Abstract: High-dimensional datasets are increasingly common across scientific and industrial domains, yet they remain difficult to cluster effectively due to the diminishing usefulness of distance metrics and the tendency of clusters to collapse or overlap when projected into lower dimensions. Traditional dimensionality reduction techniques generate static 2D or 3D embeddings that provide limited interpretability and do not offer a mechanism to leverage the analyst's intuition during exploration. To address this gap, we propose Interactive Project-Based Clustering (IPBC), a framework that reframes clustering as an iterative human-guided visual analysis process. IPBC integrates a nonlinear projection module with a feedback loop that allows users to modify the embedding by adjusting viewing angles and supplying simple constraints such as must-link or cannot-link relationships. These constraints reshape the objective of the projection model, gradually pulling semantically related points closer together and pushing unrelated points further apart. As the projection becomes more structured and expressive through user interaction, a conventional clustering algorithm operating on the optimized 2D layout can more reliably identify distinct groups. An additional explainability component then maps each discovered cluster back to the original feature space, producing interpretable rules or feature rankings that highlight what distinguishes each cluster. Experiments on various benchmark datasets show that only a small number of interactive refinement steps can substantially improve cluster quality. Overall, IPBC turns clustering into a collaborative discovery process in which machine representation and human insight reinforce one another.

</details>


### [22] [Native LLM and MLLM Inference at Scale on Apple Silicon](https://arxiv.org/abs/2601.19139)
*Wayner Barrios*

Main category: cs.LG

TL;DR: vllm-mlx是一个为苹果芯片上的大语言模型和多模态大语言模型推理设计的高效框架，通过内容哈希技术减少重复图像编码，显著提升了文本及多模态模型的吞吐量，并在开源社区发布。


<details>
  <summary>Details</summary>
Motivation: 随着苹果芯片在机器学习开发中的日益普及，对于能够利用其独特统一内存架构进行高效推理解决方案的需求也随之增加。然而，现有的工具要么缺乏原生优化（如PyTorch MPS），要么仅专注于文本模型（如llama.cpp），导致多模态工作负载未能得到充分支持。

Method: 提出了vllm-mlx，一个专为苹果芯片上大语言模型（LLM）和多模态大语言模型（MLLM）推理而构建的框架。针对文本模型，实现了比llama.cpp高出21%至87%的吞吐量；对于多模态模型，则引入了基于内容的前缀缓存机制，通过内容哈希识别相同图像来消除冗余视觉编码。

Result: 在苹果M4 Max上的评估显示，文本模型的吞吐量达到了每秒高达525个token，在重复图像查询时速度提高了28倍，将多模态延迟从21.7秒降低到不到1秒。视频分析中使用最多64帧时可实现24.7倍的缓存加速。

Conclusion: vllm-mlx展示了在苹果芯片上执行高效推理的巨大潜力，特别是对于需要处理大量文本与图像数据的应用场景。该项目已经开源，旨在促进消费级苹果设备上的高效推理实践。

Abstract: The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models (llama.cpp), leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21% to 87% higher throughput than llama.cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.

</details>


### [23] [CP Loss: Channel-wise Perceptual Loss for Time Series Forecasting](https://arxiv.org/abs/2601.18829)
*Yaohua Zha,Chunlin Fan,Peiyuan Liu,Yong Jiang,Tao Dai,Hai Wu,Shu-Tao Xia*

Main category: cs.LG

TL;DR: 本文提出了一种针对多通道时间序列数据的Channel-wise Perceptual Loss（CP损失），通过学习每个通道的独特感知空间来捕捉通道特定动态，从而优化预测模型。


<details>
  <summary>Details</summary>
Motivation: 现有的预测模型通常使用像MSE这样的通道无关损失函数，这导致无法很好地捕捉到各个通道特有的动态特性，如急剧波动或趋势变化。

Method: 设计了一个可学习的通道级滤波器，将原始信号分解为解缠的多尺度表示，并基于这些表示构建感知空间。滤波器与主预测模型一起优化，确保所学得的感知空间明确地面向预测任务。最后，在这些感知空间内计算损失以优化模型。

Result: 实验结果表明，相较于传统的通道无关损失函数方法，采用CP Loss的方法在处理具有显著异质性的多通道时间序列数据时能够更好地捕捉到通道特有动态，提高预测准确性。

Conclusion: Channel-wise Perceptual Loss提供了一种有效的方式，通过为每个通道创建适应其特征的独特感知空间来改善多通道时间序列数据的预测性能。

Abstract: Multi-channel time-series data, prevalent across diverse applications, is characterized by significant heterogeneity in its different channels. However, existing forecasting models are typically guided by channel-agnostic loss functions like MSE, which apply a uniform metric across all channels. This often leads to fail to capture channel-specific dynamics such as sharp fluctuations or trend shifts. To address this, we propose a Channel-wise Perceptual Loss (CP Loss). Its core idea is to learn a unique perceptual space for each channel that is adapted to its characteristics, and to compute the loss within this space. Specifically, we first design a learnable channel-wise filter that decomposes the raw signal into disentangled multi-scale representations, which form the basis of our perceptual space. Crucially, the filter is optimized jointly with the main forecasting model, ensuring that the learned perceptual space is explicitly oriented towards the prediction task. Finally, losses are calculated within these perception spaces to optimize the model. Code is available at https://github.com/zyh16143998882/CP_Loss.

</details>


### [24] [Knowledge-Aware Evolution for Streaming Federated Continual Learning with Category Overlap and without Task Identifiers](https://arxiv.org/abs/2601.19788)
*Sixing Tan,Xianmin Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedKACE的流式联邦持续学习方法，通过自适应推理模型切换机制、自适应梯度平衡重放方案和核谱边界缓冲区维护来解决类别重叠场景下的知识混淆问题，并在多个实验中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于批次的联邦持续学习（FCL）方法在处理具有类别重叠且缺乏任务标识符的数据流时表现不佳，导致旧知识与新知识难以区分及任务分配不确定的问题。为了解决这些挑战，提出了新的流式联邦持续学习框架以及FedKACE方法。

Method: 1. 引入了流式联邦持续学习设置，其中客户端每轮次处理可能类别重叠但样本不重复的数据流。
2. FedKACE包括三个主要部分：
   - 自适应推理模型切换机制，支持从本地模型到全局模型的单向切换，以平衡个性化与泛化。
   - 自适应梯度平衡重放方案，用于在类别重叠情况下调和新旧知识的学习。
   - 核谱边界缓冲区维护，保存高信息量和高边界影响样本以优化跨轮次的知识保留。

Result: 实验结果表明FedKACE能够在多种场景下有效减少遗憾并提高性能，证明了所提方法的有效性。

Conclusion: FedKACE为应对数据流中的类别重叠情况提供了一个有效的解决方案，通过增强对旧有知识的记忆同时促进新知识获取之间的平衡，从而改善了联邦持续学习的表现。

Abstract: Federated Continual Learning (FCL) leverages inter-client collaboration to balance new knowledge acquisition and prior knowledge retention in non-stationary data. However, existing batch-based FCL methods lack adaptability to streaming scenarios featuring category overlap between old and new data and absent task identifiers, leading to indistinguishability of old and new knowledge, uncertain task assignments for samples, and knowledge confusion.To address this, we propose streaming federated continual learning setting: per federated learning (FL) round, clients process streaming data with disjoint samples and potentially overlapping categories without task identifiers, necessitating sustained inference capability for all prior categories after each FL round.Next, we introduce FedKACE: 1) an adaptive inference model switching mechanism that enables unidirectional switching from local model to global model to achieve a trade-off between personalization and generalization; 2) a adaptive gradient-balanced replay scheme that reconciles new knowledge learning and old knowledge retention under overlapping-class scenarios; 3) a kernel spectral boundary buffer maintenance that preserves high-information and high-boundary-influence samples to optimize cross-round knowledge retention. Experiments across multiple scenarios and regret analysis demonstrate the effectiveness of FedKACE.

</details>


### [25] [The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning](https://arxiv.org/abs/2601.18832)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的框架——几何推理器(TGR)，它在严格的内存限制下执行基于流形信息的潜在前瞻性搜索，以解决现有方法在计算成本和覆盖质量之间的权衡问题。TGR通过轻量级的前瞻估计结合软几何正则化来评分候选潜在锚点，鼓励平滑轨迹和多样化探索。实验表明，TGR能够在几乎不增加额外开销的情况下显著提高难题与代码基准测试中的稳健轨迹覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩展测试时计算以增强长链思维（CoT）推理的方法面临着计算成本与覆盖质量之间的基本权衡：要么导致高昂的训练费用，要么产生冗余的轨迹。为了解决这个问题，作者们提出了一个不需要训练的新框架。

Method: 提出了几何推理器（TGR），这是一种无需训练即可工作的框架，其特点是能够执行基于流形信息的潜在前瞻性搜索，并且保持了严格的内存限制。该方法通过对候选潜在锚点进行评分，利用轻量级的前瞻估计加上软几何正则化来促进平滑轨迹及多样化探索；同时，通过分块KV缓存重置技术维持线性的内存消耗增长。

Result: 实验结果表明，在具有挑战性的数学和代码基准上，TGR相比Qwen3-8B模型提高了高达13个百分点的Pass@$k$曲线下的面积(AUC)指标，而仅引入了大约1.1到1.3倍的微小额外开销。

Conclusion: 几何推理器（TGR）提供了一个有效的方法来改善长链思维推理任务中遇到的问题，即在保持低额外开销的同时大幅提升了复杂问题解决过程中轨迹的质量。

Abstract: Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times.

</details>


### [26] [Analysis of Control Bellman Residual Minimization for Markov Decision Problem](https://arxiv.org/abs/2601.18840)
*Donghwan Lee,Hyukjun Yang*

Main category: cs.LG

TL;DR: 本文探讨了贝尔曼残差最小化方法在策略优化中的应用，建立了控制贝尔曼残差最小化的基础理论结果。


<details>
  <summary>Details</summary>
Motivation: 虽然贝尔曼残差最小化方法在策略评估中已有广泛研究，但在策略优化（控制任务）中的应用却很少被探索。考虑到贝尔曼残差最小化具有若干优点，如在价值函数逼近时收敛更稳定等，值得进一步研究。

Method: 本文通过建立控制贝尔曼残差最小化用于策略优化的基础理论结果来进行研究。

Result: 确立了控制贝尔曼残差最小化在策略优化上下文中的基础结果。

Conclusion: 该研究为贝尔曼残差最小化方法应用于更广泛的场景提供了理论依据，特别是对于那些需要利用价值函数逼近来实现策略优化的任务而言。

Abstract: Markov decision problems are most commonly solved via dynamic programming. Another approach is Bellman residual minimization, which directly minimizes the squared Bellman residual objective function. However, compared to dynamic programming, this approach has received relatively less attention, mainly because it is often less efficient in practice and can be more difficult to extend to model-free settings such as reinforcement learning. Nonetheless, Bellman residual minimization has several advantages that make it worth investigating, such as more stable convergence with function approximation for value functions. While Bellman residual methods for policy evaluation have been widely studied, methods for policy optimization (control tasks) have been scarcely explored. In this paper, we establish foundational results for the control Bellman residual minimization for policy optimization.

</details>


### [27] [GraIP: A Benchmarking Framework For Neural Graph Inverse Problems](https://arxiv.org/abs/2601.18917)
*Semih Cantürk,Andrei Manolache,Arman Mielke,Chendi Qian,Antoine Siraudin,Christopher Morris,Mathias Niepert,Guy Wolf*

Main category: cs.LG

TL;DR: 本文提出了神经图逆问题（GraIP）概念框架，将一大类图学习任务形式化并重新定义为逆问题，旨在通过观察数据恢复底层图结构。此框架适用于重连、因果发现和神经关系推理等任务，并提供了基准数据集和指标来评估现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前解决图结构推断等图学习任务的方法通常是孤立且特定于任务的，缺乏统一的理论基础。

Method: 引入了神经图逆问题（GraIP）的概念框架，该框架将许多图学习任务作为逆问题进行处理，目的是基于观察到的数据通过逆转产生观测输出的过程来恢复底层图结构。

Result: 展示了GraIP在多种图学习任务中的通用性，包括重连、因果发现和神经关系推理等；提出了针对每种考虑的GraIP领域的基准数据集和度量标准，并对现有的基线方法进行了表征和实证评价。

Conclusion: 提供了一个统一的观点，连接了看似不同的应用领域，并为约束和组合设置下的结构学习提供了原则性的方法，同时促进了现有方法在图逆问题之间的交叉融合。

Abstract: A wide range of graph learning tasks, such as structure discovery, temporal graph analysis, and combinatorial optimization, focus on inferring graph structures from data, rather than making predictions on given graphs. However, the respective methods to solve such problems are often developed in an isolated, task-specific manner and thus lack a unifying theoretical foundation. Here, we provide a stepping stone towards the formation of such a foundation and further development by introducing the Neural Graph Inverse Problem (GraIP) conceptual framework, which formalizes and reframes a broad class of graph learning tasks as inverse problems. Unlike discriminative approaches that directly predict target variables from given graph inputs, the GraIP paradigm addresses inverse problems, i.e., it relies on observational data and aims to recover the underlying graph structure by reversing the forward process, such as message passing or network dynamics, that produced the observed outputs. We demonstrate the versatility of GraIP across various graph learning tasks, including rewiring, causal discovery, and neural relational inference. We also propose benchmark datasets and metrics for each GraIP domain considered, and characterize and empirically evaluate existing baseline methods used to solve them. Overall, our unifying perspective bridges seemingly disparate applications and provides a principled approach to structural learning in constrained and combinatorial settings while encouraging cross-pollination of existing methods across graph inverse problems.

</details>


### [28] [One Global Model, Many Behaviors: Stockout-Aware Feature Engineering and Dynamic Scaling for Multi-Horizon Retail Demand Forecasting with a Cost-Aware Ordering Policy (VN2 Winner Report)](https://arxiv.org/abs/2601.18919)
*Bartosz Szabłowski*

Main category: cs.LG

TL;DR: 本文介绍了一种两阶段的预测-优化方法，用于解决零售连锁店的库存计划问题。该方法结合了全局多时间点预测模型与成本感知的订购策略，在VN2 Inventory Planning Challenge中取得了第一名的成绩。


<details>
  <summary>Details</summary>
Motivation: 零售连锁店在进行库存规划时需要将需求预测转化为订购决策，同时考虑缺货和持有成本。为此，提出了一个能够处理这种场景的有效解决方案。

Method: 提出的方法包括两个阶段：首先使用梯度提升决策树（GBDT）模型进行全局多时间点需求预测，该模型通过CatBoost实现，并采用了缺货意识特征工程、序列缩放及基于时间的观测权重等技术；其次，在决策阶段，将库存预测到交付周开始时，并计算出一个权衡短缺成本和持有成本的目标库存水平。

Result: 在官方竞赛模拟的六轮评估中，所提方案结合了强大的全局预测模型与轻量级的成本感知策略，获得了第一名。

Conclusion: 虽然该方法是为VN2设置开发的，但其可以扩展至实际应用和其他运营约束条件。

Abstract: Inventory planning for retail chains requires translating demand forecasts into ordering decisions, including asymmetric shortages and holding costs. The VN2 Inventory Planning Challenge formalizes this setting as a weekly decision-making cycle with a two-week product delivery lead time, where the total cost is defined as the shortage cost plus the holding cost. This report presents the winning VN2 solution: a two-stage predict-then-optimize pipeline that combines a single global multi-horizon forecasting model with a cost-aware ordering policy. The forecasting model is trained in a global paradigm, jointly using all available time series. A gradient-boosted decision tree (GBDT) model implemented in CatBoost is used as the base learner. The model incorporates stockout-aware feature engineering to address censored demand during out-of-stock periods, per-series scaling to focus learning on time-series patterns rather than absolute levels, and time-based observation weights to reflect shifts in demand patterns. In the decision stage, inventory is projected to the start of the delivery week, and a target stock level is calculated that explicitly trades off shortage and holding costs. Evaluated by the official competition simulation in six rounds, the solution achieved first place by combining a strong global forecasting model with a lightweight cost-aware policy. Although developed for the VN2 setting, the proposed approach can be extended to real-world applications and additional operational constraints.

</details>


### [29] [A Scalable Inter-edge Correlation Modeling in CopulaGNN for Link Sign Prediction](https://arxiv.org/abs/2601.19175)
*Jinkyu Sung,Myunggeum Jee,Joonseok Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过使用Gaussian copula和相关矩阵来直接建模边之间的潜在统计依赖性，并通过将相关矩阵表示为边嵌入的Gramian以及重构条件概率分布来解决计算上的不可行性问题。该方法在理论上被证明具有线性收敛性，并且实验表明其比基线模型收敛得更快，同时保持了与最先进模型相竞争的预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于负边的存在违反了图同质性的假设，即相邻节点是相似的，因此常规的图方法无法直接应用于有符号图上而不借助辅助结构处理负边。本文旨在直接利用Gaussian copula及其对应的相关矩阵来建模边之间潜在的统计依赖关系，从而扩展CopulaGNN的方法。

Method: 1) 通过将相关矩阵表示为边嵌入的Gramian矩阵，大幅减少参数数量；2) 重新定义条件概率分布以极大降低推理成本。

Result: 理论分析验证了所提方法的可扩展性，证明了其线性收敛性。广泛的实验显示，相较于基线方法，本方法实现了显著更快的收敛速度，同时保持了与当前最优模型相当的预测性能。

Conclusion: 提出的方法不仅解决了有符号图中链接符号预测的问题，而且在保证预测性能的同时提高了计算效率，为处理大规模图数据提供了可行方案。

Abstract: Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable without auxiliary structures to handle them. We aim to directly model the latent statistical dependency among edges with the Gaussian copula and its corresponding correlation matrix, extending CopulaGNN. However, a naive modeling of edge-edge relations is computationally intractable even for a graph with moderate scale. To address this, we propose to 1) represent the correlation matrix as a Gramian of edge embeddings, significantly reducing the number of parameters, and 2) reformulate the conditional probability distribution to dramatically reduce the inference cost. We theoretically verify scalability of our method by proving its linear convergence. Also, our extensive experiments demonstrate that it achieves significantly faster convergence than baselines, maintaining competitive prediction performance to the state-of-the-art models.

</details>


### [30] [FSD-CAP: Fractional Subgraph Diffusion with Class-Aware Propagation for Graph Feature Imputation](https://arxiv.org/abs/2601.18938)
*Xin Qiao,Shijie Sun,Anqi Dong,Cong Hua,Xia Zhao,Longfei Zhang,Guangming Zhu,Liang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FSD-CAP的两阶段框架，用于在极端稀疏条件下提高图中缺失节点特征的填充质量。通过图距离导向的子图扩展和分数扩散算子来定位扩散过程，并使用类感知传播进一步优化填充特征。实验结果表明，在五个基准数据集上，即使99.5%的特征缺失，FSD-CAP依然能够达到接近于拥有完整特征的标准GCN模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于潜在表示或全局扩散的方法在处理高缺失率下的图节点特征填充时往往表现不佳，容易导致错误在整个图中传播。因此，需要一种新的方法能够在极端稀疏条件下更准确地估计并填充缺失的节点特征。

Method: 提出了FSD-CAP，一个旨在改善极端稀疏性下填充质量的两阶段框架。第一阶段利用图距离指导的子图扩展来局部化扩散过程，并采用分数扩散算子根据局部结构调整传播锐度；第二阶段则通过引入伪标签与邻域熵来进行类感知传播，以促进一致性的同时对初步填充的特征进行精炼。

Result: 在五个基准数据集上，当99.5%的特征缺失时，FSD-CAP在节点分类任务上的平均准确率达到80.06%（结构性）和81.01%（均匀性），接近完全特征情况下标准GCN模型的81.31%。对于链路预测任务，在相同设置下，它达到了91.65%（结构性）和92.41%（均匀性）的AUC分数，而完全观察到的情况下为95.06%。此外，FSD-CAP在大规模和异质性数据集上也表现出优于其他模型的性能。

Conclusion: FSD-CAP提供了一个有效应对极高缺失率下图节点特征填充问题的新途径，其不仅能够在节点分类与链路预测任务中实现接近完整的特征表现，而且相较于现有方法展现了更好的鲁棒性和准确性。

Abstract: Imputing missing node features in graphs is challenging, particularly under high missing rates. Existing methods based on latent representations or global diffusion often fail to produce reliable estimates, and may propagate errors across the graph. We propose FSD-CAP, a two-stage framework designed to improve imputation quality under extreme sparsity. In the first stage, a graph-distance-guided subgraph expansion localizes the diffusion process. A fractional diffusion operator adjusts propagation sharpness based on local structure. In the second stage, imputed features are refined using class-aware propagation, which incorporates pseudo-labels and neighborhood entropy to promote consistency. We evaluated FSD-CAP on multiple datasets. With $99.5\%$ of features missing across five benchmark datasets, FSD-CAP achieves average accuracies of $80.06\%$ (structural) and $81.01\%$ (uniform) in node classification, close to the $81.31\%$ achieved by a standard GCN with full features. For link prediction under the same setting, it reaches AUC scores of $91.65\%$ (structural) and $92.41\%$ (uniform), compared to $95.06\%$ for the fully observed case. Furthermore, FSD-CAP demonstrates superior performance on both large-scale and heterophily datasets when compared to other models.

</details>


### [31] [A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy](https://arxiv.org/abs/2601.18939)
*Claire O'Brien,Jessica Seto,Dristi Roy,Aditya Dwivedi,Sunishchal Dev,Kevin Zhu,Sean O'Brien,Ashwinee Panda,Ryan Lagasse*

Main category: cs.LG

TL;DR: 本文提出了一种针对大型语言模型（LLMs）的行为对齐方法，该方法通过识别并仅更新负责特定行为的神经元来减少数据需求，同时保持或超越了当前最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的广泛微调方法在实现大型语言模型的行为对齐时，可能会引起分布偏移和低解释性等问题。因此，研究者希望开发一种更精准、需要较少训练数据的方法。

Method: 利用稀疏自动编码器(SAEs)与线性探测器定位出对于目标行为最具预测性的3% MLP神经元，并将这些选定的神经元解码到残差空间中进行单独微调，此过程采用了梯度掩蔽技术。

Result: 在减少谄媚行为的任务上，这种方法使用Gemma-2-2B和9B模型达到了或超过了四个基准测试(Syco-Bench, NLP, POLI, PHIL)上的最先进表现。

Conclusion: 研究表明，在全模型微调之外，稀疏的神经元级别更新提供了一个可扩展且精确的选择，即使是在数据量有限的情况下也能有效。

Abstract: Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a target behavior, decode them into residual space, and fine-tune only those neurons using gradient masking. We demonstrate this approach on the task of reducing sycophantic behavior, where our method matches or exceeds state-of-the-art performance on four benchmarks (Syco-Bench, NLP, POLI, PHIL) using Gemma-2-2B and 9B models. Our results show that sparse, neuron-level updates offer a scalable and precise alternative to full-model fine-tuning, remaining effective even in situations when little data is available

</details>


### [32] [Vector-Valued Distributional Reinforcement Learning Policy Evaluation: A Hilbert Space Embedding Approach](https://arxiv.org/abs/2601.18952)
*Mehrdad Mohammadi,Qi Zheng,Ruoqing Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种多维分布强化学习框架(KE-DRL)，利用Hilbert空间映射估计目标策略下的多维价值分布的核平均嵌入。该方法用积分概率度量替代Wasserstein度量，适用于连续和多维状态-动作空间，并提供了理论保证及实证结果。


<details>
  <summary>Details</summary>
Motivation: 针对多维且连续的状态-动作变量，在直接计算Wasserstein距离存在计算难题的情况下，旨在开发一种有效的方法来评估离策略并恢复核平均嵌入。

Method: 通过将概率测度映射到再生核希尔伯特空间（RKHS）中，使用核平均嵌入代替Wasserstein度量，从而提出了一个基于Matérn核族的新度量方法。

Result: 理论分析显示了所提出的度量下分布贝尔曼算子具有收缩性质，并提供了均匀收敛保证。模拟与实验结果展示了在温和假设条件下（即核函数的Lipschitz连续性和有界性），离策略评估的有效性和核平均嵌入的成功恢复。

Conclusion: 基于嵌入的方法在复杂现实世界决策场景和风险评估中展现出了潜力，为处理多维连续状态-动作空间提供了一个有效的解决方案。

Abstract: We propose an (offline) multi-dimensional distributional reinforcement learning framework (KE-DRL) that leverages Hilbert space mappings to estimate the kernel mean embedding of the multi-dimensional value distribution under a proposed target policy. In our setting, the state-action variables are multi-dimensional and continuous. By mapping probability measures into a reproducing kernel Hilbert space via kernel mean embeddings, our method replaces Wasserstein metrics with an integral probability metric. This enables efficient estimation in multi-dimensional state-action spaces and reward settings, where direct computation of Wasserstein distances is computationally challenging. Theoretically, we establish contraction properties of the distributional Bellman operator under our proposed metric involving the Matern family of kernels and provide uniform convergence guarantees. Simulations and empirical results demonstrate robust off-policy evaluation and recovery of the kernel mean embedding under mild assumptions, namely, Lipschitz continuity and boundedness of the kernels, highlighting the potential of embedding-based approaches in complex real-world decision-making scenarios and risk evaluation.

</details>


### [33] [Towards Self-Optimizing Electron Microscope: Robust Tuning of Aberration Coefficients via Physics-Aware Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2601.18972)
*Utkarsh Pratiush,Austin Houston,Richard Liu,Gerd Duscher,Sergei Kalinin*

Main category: cs.LG

TL;DR: 本文介绍了一种多目标贝叶斯优化框架，用于实现高效的数据驱动的像差校正。该方法通过高斯过程回归主动选择最有信息量的透镜设置进行评估，相较于传统算法更加稳健，并能有效调整焦点、散光和高阶像差，从而在实验过程中动态维持最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的STEM自动对准程序效率低下且难以同时校正多个相互作用参数；而深度学习方法虽然快速但缺乏适应不同样本条件的灵活性。因此，需要一种新的方法来提高像差校正的速度和效率，同时保持足够的灵活性以应对不同的实验需求。

Method: 提出了一种基于多目标贝叶斯优化（MOBO）的方法，利用高斯过程回归模型来概率性地描绘像差景观，并允许用户定义物理动机的奖励公式。通过帕累托前沿揭示了竞争实验优先级之间的权衡关系。

Result: 实验表明，所提出的主动学习循环比传统的优化算法更健壮，并能够有效地调节焦点、散光以及更高阶的像差问题。

Conclusion: 这种多目标贝叶斯优化框架为实现‘自优化’显微术提供了可能，能够在实验过程中动态地保持最优性能，提高了数据利用效率并增强了对复杂样本条件的适应能力。

Abstract: Realizing high-throughput aberration-corrected Scanning Transmission Electron Microscopy (STEM) exploration of atomic structures requires rapid tuning of multipole probe correctors while compensating for the inevitable drift of the optical column. While automated alignment routines exist, conventional approaches rely on serial, gradient-free searches (e.g., Nelder-Mead) that are sample-inefficient and struggle to correct multiple interacting parameters simultaneously. Conversely, emerging deep learning methods offer speed but often lack the flexibility to adapt to varying sample conditions without extensive retraining. Here, we introduce a Multi-Objective Bayesian Optimization (MOBO) framework for rapid, data-efficient aberration correction. Importantly, this framework does not prescribe a single notion of image quality; instead, it enables user-defined, physically motivated reward formulations (e.g., symmetry-induced objectives) and uses Pareto fronts to expose the resulting trade-offs between competing experimental priorities. By using Gaussian Process regression to model the aberration landscape probabilistically, our workflow actively selects the most informative lens settings to evaluate next, rather than performing an exhaustive blind search. We demonstrate that this active learning loop is more robust than traditional optimization algorithms and effectively tunes focus, astigmatism, and higher-order aberrations. By balancing competing objectives, this approach enables "self-optimizing" microscopy by dynamically sustaining optimal performance during experiments.

</details>


### [34] [When Does Adaptation Win? Scaling Laws for Meta-Learning in Quantum Control](https://arxiv.org/abs/2601.18973)
*Nima Leclerc,Chris Miller,Nicholas Brawand*

Main category: cs.LG

TL;DR: 本文研究了量子硬件中设备异质性和环境漂移问题，通过推导元学习的缩放定律下界来量化任务特定梯度步骤带来的预期保真度改进。实验验证表明，在极端分布外条件下，双量子比特门的保真度可提高超过40%，这对减少云量子处理器上的每个设备校准时间具有重要意义。此外，经典线性二次控制的进一步验证证实了这些规律源于一般优化几何而非量子物理特有现象。


<details>
  <summary>Details</summary>
Motivation: 由于量子硬件存在固有的设备异质性和环境变化，研究人员必须在次优的非自适应控制器和成本高昂的每设备重新校准之间做出选择。该文旨在通过提供一个关于何时适应性调整可以证明其开销合理的定量标准，来解决这一挑战。

Method: 作者们首先推导了一个用于元学习的缩放定律下界，展示了从任务特定梯度步骤获得的适应增益如何随着梯度步骤数量呈指数饱和，并且与任务方差成线性关系。然后，他们通过量子门校准以及经典线性-二次控制问题进行了方法的有效性验证。

Result: 研究表明，在低方差任务上使用自适应控制器带来的好处微乎其微；然而，在极端超出分布条件（训练噪声的10倍）下，双量子比特门的保真度提高了超过40%。这表明所提出的方法有助于显著减少云量子处理器上每台设备所需的校准时间。

Conclusion: 这项工作为自适应控制中的决策制定提供了一个可转移框架，并强调了在高变异环境下采用适应性策略的重要性。它不仅对量子计算领域有价值，也为其他需要考虑设备间差异性的控制问题提供了见解。

Abstract: Quantum hardware suffers from intrinsic device heterogeneity and environmental drift, forcing practitioners to choose between suboptimal non-adaptive controllers or costly per-device recalibration. We derive a scaling law lower bound for meta-learning showing that the adaptation gain (expected fidelity improvement from task-specific gradient steps) saturates exponentially with gradient steps and scales linearly with task variance, providing a quantitative criterion for when adaptation justifies its overhead. Validation on quantum gate calibration shows negligible benefits for low-variance tasks but $>40\%$ fidelity gains on two-qubit gates under extreme out-of-distribution conditions (10$\times$ the training noise), with implications for reducing per-device calibration time on cloud quantum processors. Further validation on classical linear-quadratic control confirms these laws emerge from general optimization geometry rather than quantum-specific physics. Together, these results offer a transferable framework for decision-making in adaptive control.

</details>


### [35] [Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning](https://arxiv.org/abs/2601.18984)
*Haolin Liu,Dian Yu,Sidi Lu,Yujun Zhou,Rui Liu,Zhenwen Liang,Haitao Mi,Chen-Yu Wei,Dong Yu*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习方法VPPO，通过仅使用过程奖励模型（PRM）来定位错误发生的第一步，并基于此对正确的前缀给予奖励而对错误的后缀施加惩罚，从而在多个推理基准测试中优于稀疏奖励RL和先前基于PRM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法依赖于稀疏的结果奖励，未能有效认可部分成功解决方案中的正确中间步骤；虽然过程奖励模型提供了更细粒度的步骤级监督，但其评分往往带有噪声且难以评估。因此，需要一种能够更好地利用过程奖励模型进行信用分配的新方法。

Method: 设计了可验证前缀策略优化（Verifiable Prefix Policy Optimization, VPPO），该方法利用过程奖励模型确定推理路径中首次出现错误的位置，并将轨迹划分为一个被验证为正确的前缀和一个错误的后缀，对前者予以奖励，同时只针对检测到的错误之后的部分施加针对性的惩罚。

Result: 实验表明，在多个推理基准上，与依赖稀疏奖励的传统强化学习方法以及之前基于过程奖励模型的方法相比，VPPO在Pass@1和Pass@K指标上均表现出色。

Conclusion: VPPO通过改进奖励机制实现了更稳定、可解释的学习信号生成方式，有效提高了大型语言模型推理能力训练过程中对正确行为的认可度。

Abstract: Reinforcement learning (RL) has emerged as a powerful framework for improving the reasoning capabilities of large language models (LLMs). However, most existing RL approaches rely on sparse outcome rewards, which fail to credit correct intermediate steps in partially successful solutions. Process reward models (PRMs) offer fine-grained step-level supervision, but their scores are often noisy and difficult to evaluate. As a result, recent PRM benchmarks focus on a more objective capability: detecting the first incorrect step in a reasoning path. However, this evaluation target is misaligned with how PRMs are typically used in RL, where their step-wise scores are treated as raw rewards to maximize. To bridge this gap, we propose Verifiable Prefix Policy Optimization (VPPO), which uses PRMs only to localize the first error during RL. Given an incorrect rollout, VPPO partitions the trajectory into a verified correct prefix and an erroneous suffix based on the first error, rewarding the former while applying targeted penalties only after the detected mistake. This design yields stable, interpretable learning signals and improves credit assignment. Across multiple reasoning benchmarks, VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on both Pass@1 and Pass@K.

</details>


### [36] [Randomization Boosts KV Caching, Learning Balances Query Load: A Joint Perspective](https://arxiv.org/abs/2601.18999)
*Fangzhou Wu,Sandeep Silwal,Qiuyi,Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的数学模型，用于解决在多语言模型服务场景下KV缓存淘汰和查询路由之间的核心权衡问题。通过结合具有竞争力的随机KV缓存淘汰算法与基于学习的方法自适应地路由查询，从而平衡查询负载和缓存命中率。实验表明，相比现有方法，新方法在缓存命中率、延迟减少、首次出词时间减少以及吞吐量提升方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前的最少最近使用（LRU）淘汰算法难以有效处理动态在线查询到达的问题，特别是在需要跨多个工作器平衡查询负载并同时最大化每个工作器缓存命中率的多语言模型服务场景中表现不佳。这促使研究者开发一种能够更好地应对这些问题的新方法。

Method: 作者首先构建了一个统一的数学模型来捕捉KV缓存淘汰与查询路由之间的重要权衡。基于此模型分析了现有方法的理论局限性，并提出了将可证明竞争性的随机KV缓存淘汰策略与基于学习的查询路由技术相结合的新算法。

Result: 广泛实验结果表明，在四个基准测试和三种前缀共享设置下，所提方法相较于最先进技术水平实现了高达6.92倍的缓存命中率提高、11.96倍的延迟降低、14.06倍的时间到第一个令牌减少以及77.4%的吞吐量增长。

Conclusion: 这项研究表明，通过采用更先进的KV缓存管理和查询路由策略，可以显著改善大型语言模型推理过程中的性能指标。提出的解决方案不仅在理论上被验证为有效，而且通过实验证明了其实际应用价值。

Abstract: KV caching is a fundamental technique for accelerating Large Language Model (LLM) inference by reusing key-value (KV) pairs from previous queries, but its effectiveness under limited memory is highly sensitive to the eviction policy. The default Least Recently Used (LRU) eviction algorithm struggles with dynamic online query arrivals, especially in multi-LLM serving scenarios, where balancing query load across workers and maximizing cache hit rate of each worker are inherently conflicting objectives. We give the first unified mathematical model that captures the core trade-offs between KV cache eviction and query routing. Our analysis reveals the theoretical limitations of existing methods and leads to principled algorithms that integrate provably competitive randomized KV cache eviction with learning-based methods to adaptively route queries with evolving patterns, thus balancing query load and cache hit rate. Our theoretical results are validated by extensive experiments across 4 benchmarks and 3 prefix-sharing settings, demonstrating improvements of up to 6.92$\times$ in cache hit rate, 11.96$\times$ reduction in latency, 14.06$\times$ reduction in time-to-first-token (TTFT), and 77.4% increase in throughput over the state-of-the-art methods. Our code is available at https://github.com/fzwark/KVRouting.

</details>


### [37] [Accelerated training of Gaussian processes using banded square exponential covariances](https://arxiv.org/abs/2601.19007)
*Emily C. Ehrhardt,Felipe Tobar*

Main category: cs.LG

TL;DR: 本文提出了一种新的计算高效GP训练方法，通过消除接近于零的非对角线元素来构造带状矩阵近似原始协方差矩阵，从而降低计算逆和行列式的成本，并在理论分析和实验验证中证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于解决高斯过程（GP）训练中的计算效率问题，特别是针对平方指数（SE）核产生的协方差矩阵中存在大量几乎为零的非对角线元素的情况。

Method: 作者们基于观察到SE核生成的协方差矩阵内含有许多非常接近于零的非对角线值这一点，开发出一套有原则的过程来去除这些项，进而产生一个带状矩阵作为原协方差矩阵的近似版本。这样做的好处是可以以较低的计算代价完成对于这个近似矩阵求逆以及行列式的操作。

Result: 研究者们提供了理论分析支持，在一维场景下使用SE核时，所提方法能够保持原有协方差结构的特点；同时，通过与变分自由能法进行稀疏GP处理的方式对比，证实了其计算效率的优势。

Conclusion: 结论表明，所提出的带状矩阵近似方法不仅有效降低了高斯过程中关键运算的成本，而且在保持模型性能的同时显著提高了计算效率。

Abstract: We propose a novel approach to computationally efficient GP training based on the observation that square-exponential (SE) covariance matrices contain several off-diagonal entries extremely close to zero. We construct a principled procedure to eliminate those entries to produce a \emph{banded}-matrix approximation to the original covariance, whose inverse and determinant can be computed at a reduced computational cost, thus contributing to an efficient approximation to the likelihood function. We provide a theoretical analysis of the proposed method to preserve the structure of the original covariance in the 1D setting with SE kernel, and validate its computational efficiency against the variational free energy approach to sparse GPs.

</details>


### [38] [EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting](https://arxiv.org/abs/2601.19022)
*Antanas Zilinskas,Robert N. Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: 提出了一种名为EVEREST的基于Transformer的架构，用于概率性稀有事件预测，集成了注意力瓶颈、证据头、极值头和前驱头四个组件，通过复合损失函数联合优化，在空间天气数据上实现了领先的真实技能统计。


<details>
  <summary>Details</summary>
Motivation: 由于严重的类别不平衡、长距离依赖性和分布不确定性，对多变量时间序列数据中的稀有事件进行预测非常具有挑战性。为了解决这些问题，提出了EVEREST架构来改善稀有事件预测的准确性以及提供校准后的预测和尾部风险估计。

Method: 开发了一个名为EVEREST的基于变压器的架构，该架构包括：(i)一个可学习的注意力瓶颈，用于软聚合时间动态；(ii)一个证据头，通过正态-逆伽玛分布估计随机和认知不确定性；(iii)一个使用广义帕累托分布建模尾部风险的极值头；以及(iv)一个轻量级的前驱头，用于早期事件检测。这些模块使用复合损失（焦点损失、证据负对数似然和尾部敏感的EVT惩罚）在训练时共同优化。

Result: 在十年的空间天气数据上，对于C类耀斑，EVEREST在24/48/72小时的时间范围内达到了0.973/0.970/0.966的最佳真实技能统计。模型紧凑，在商用硬件上易于训练，并且适用于诸如工业监控、天气预报及卫星诊断等高风险领域。

Conclusion: EVEREST是一种有效的解决方案，能够处理多变量时间序列数据中稀有事件的预测问题，并且在实际应用中表现出了卓越性能。然而，它也存在一些限制，比如依赖于固定长度输入并且不支持基于图像的模态，这提示了未来研究可以朝向流式传输和多模态预测的方向发展。

Abstract: Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting.

</details>


### [39] [A Unifying View of Coverage in Linear Off-Policy Evaluation](https://arxiv.org/abs/2601.19030)
*Philip Amortila,Audrey Huang,Akshay Krishnamurthy,Nan Jiang*

Main category: cs.LG

TL;DR: 本文针对线性离策略评估（OPE）场景，提出了一种新的有限样本分析方法，该方法基于特征-动态覆盖参数来定义，为理解线性OPE中的覆盖提供了一个统一的观点。


<details>
  <summary>Details</summary>
Motivation: 在仅目标价值函数在线性特征中可实现的最小设置下，对于统计速率的精确刻画缺乏清晰的理解，尤其是合适的覆盖概念尚不清楚。现有分析中的候选定义具有不理想的属性，并且与文献中更标准的定义脱节。

Method: 通过采用工具变量视角，对LSTDQ算法进行了新颖的有限样本分析，开发了依赖于新提出的特征-动态覆盖参数的误差界。

Result: 引入了特征-动态覆盖的新概念，这一概念可以解释为特征演化所诱导的动力系统中的线性覆盖。此外，在进一步假设如Bellman完备性的情况下，成功恢复了专门针对这些设定的覆盖参数，从而为线性OPE中的覆盖提供了统一的理解。

Conclusion: 这项工作提出了一个关键的特征-动态覆盖参数，为理解和评估线性离策略评估问题提供了一个更为统一和全面的框架。

Abstract: Off-policy evaluation (OPE) is a fundamental task in reinforcement learning (RL). In the classic setting of linear OPE, finite-sample guarantees often take the form $$ \textrm{Evaluation error} \le \textrm{poly}(C^π, d, 1/n,\log(1/δ)), $$ where $d$ is the dimension of the features and $C^π$ is a coverage parameter that characterizes the degree to which the visited features lie in the span of the data distribution. While such guarantees are well-understood for several popular algorithms under stronger assumptions (e.g. Bellman completeness), the understanding is lacking and fragmented in the minimal setting where only the target value function is linearly realizable in the features. Despite recent interest in tight characterizations of the statistical rate in this setting, the right notion of coverage remains unclear, and candidate definitions from prior analyses have undesirable properties and are starkly disconnected from more standard definitions in the literature.
  We provide a novel finite-sample analysis of a canonical algorithm for this setting, LSTDQ. Inspired by an instrumental-variable view, we develop error bounds that depend on a novel coverage parameter, the feature-dynamics coverage, which can be interpreted as linear coverage in an induced dynamical system for feature evolution. With further assumptions -- such as Bellman-completeness -- our definition successfully recovers the coverage parameters specialized to those settings, finally yielding a unified understanding for coverage in linear OPE.

</details>


### [40] [ProToken: Token-Level Attribution for Federated Large Language Models](https://arxiv.org/abs/2601.19672)
*Waris Gill,Ahmad Humayun,Ali Anwar,Muhammad Ali Gulzar*

Main category: cs.LG

TL;DR: 本文提出了一种名为ProToken的新方法，用于联邦大语言模型中令牌级别的来源追踪。该方法能够在保证隐私的前提下准确地定位生成文本中的特定响应是由哪些客户端贡献的，从而解决了调试、恶意客户端识别等问题。实验结果表明，ProToken在多种模型架构和领域上均能实现高达98%的归因准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习使得大型语言模型可以在分布式数据源上进行协作训练的同时保护隐私。然而，在关键应用部署联邦大语言模型时，尚不清楚哪些客户端对特定生成的回答做出了贡献，这阻碍了调试、恶意客户端识别、公平奖励分配以及信任验证的过程。

Method: ProToken是一种针对联邦大语言模型设计的令牌级别来源追踪方法。它基于两个主要见解来实现每个令牌的来源：（1）转换器架构在后期块中集中任务特定信号，允许为计算可行性选择策略层；（2）基于梯度的相关性加权过滤掉无关的神经激活，将归因集中在直接影响令牌生成的神经元上。

Result: 通过对覆盖四个大语言模型架构（Gemma, Llama, Qwen, SmolLM）及四个领域（医疗、金融、数学、编码）的16种配置进行评估，ProToken达到了平均98%的归因准确率，即使在扩展客户端数量的情况下也能保持高准确性。

Conclusion: ProToken展示了其在现实世界部署环境中实际可行的能力，能够有效解决联邦学习框架下大语言模型面临的挑战，包括但不限于调试困难、恶意用户识别问题等，同时保持了较高的准确性和良好的扩展性。

Abstract: Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.

</details>


### [41] [Unravelling the (In)compatibility of Statistical-Parity and Equalized-Odds](https://arxiv.org/abs/2601.19035)
*Mortaza S. Bargh,Sunil Choenni,Floris ter Braak*

Main category: cs.LG

TL;DR: 本文研究了统计公平性度量中的统计均等（Statistical-Parity）和平等机会（Equalized-Odds）之间的关系，指出基本比率不平衡会导致两者不兼容，并建议在依赖或强制执行统计均等标准前应检查基本比率平衡情况。


<details>
  <summary>Details</summary>
Motivation: 数据和算法驱动系统中确保公平正义原则面临挑战。统计公平性度量是检测数据与算法中公平问题的重要技术手段。尽管统计均等被广泛采纳用于评估算法公平性，但其与平等机会度量之间存在基于敏感群体基础比率的潜在不兼容性。

Method: 通过分析统计均等和平等机会两种度量方式之间的关系，特别是考察不同敏感群体的基础比率如何影响这两种度量的一致性。

Result: 研究表明，当敏感群体间的基础比率不平衡时，统计均等和平等机会度量可能变得不兼容。这为实践中如何权衡这两种度量提供了见解。

Conclusion: 在采用统计均等作为公平性标准之前，必须先审查基础比率平衡状况及其可能导致的不兼容性。这些发现可能会促进现有实践或法律框架的改进。

Abstract: A key challenge in employing data, algorithms and data-driven systems is to adhere to the principle of fairness and justice. Statistical fairness measures belong to an important category of technical/formal mechanisms for detecting fairness issues in data and algorithms. In this contribution we study the relations between two types of statistical fairness measures namely Statistical-Parity and Equalized-Odds. The Statistical-Parity measure does not rely on having ground truth, i.e., (objectively) labeled target attributes. This makes Statistical-Parity a suitable measure in practice for assessing fairness in data and data classification algorithms. Therefore, Statistical-Parity is adopted in many legal and professional frameworks for assessing algorithmic fairness. The Equalized-Odds measure, on the contrary, relies on having (reliable) ground-truth, which is not always feasible in practice. Nevertheless, there are several situations where the Equalized-Odds definition should be satisfied to enforce false prediction parity among sensitive social groups. We present a novel analyze of the relation between Statistical-Parity and Equalized-Odds, depending on the base-rates of sensitive groups. The analysis intuitively shows how and when base-rate imbalance causes incompatibility between Statistical-Parity and Equalized-Odds measures. As such, our approach provides insight in (how to make design) trade-offs between these measures in practice. Further, based on our results, we plea for examining base-rate (im)balance and investigating the possibility of such an incompatibility before enforcing or relying on the Statistical-Parity criterion. The insights provided, we foresee, may trigger initiatives to improve or adjust the current practice and/or the existing legal frameworks.

</details>


### [42] [OATS: Online Data Augmentation for Time Series Foundation Models](https://arxiv.org/abs/2601.19040)
*Junwei Deng,Chang Xu,Jiaqi W. Ma,Ming Jin,Chenghao Liu,Jiang Bian*

Main category: cs.LG

TL;DR: 提出了一种名为OATS（在线数据增强）的新方法，专为时间序列基础模型设计。该方法根据训练的不同阶段动态生成合成数据，以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据增强方法通常依赖于启发式和静态范式，而动态数据优化表明样本在不同训练阶段的贡献是不同的。基于此，提出了OATS来解决这个问题。

Method: OATS利用有价值的训练样本作为指导信号，并基于这些信号动态生成高质量的合成数据。此外，还设计了一个基于扩散的框架来生成真实的时间序列，并引入了探索-利用机制来平衡效率和效果。

Result: 实验显示，在六个验证数据集和两种TSFM架构上，OATS相比常规训练以及静态数据增强基线显著提高了性能。

Conclusion: OATS是一种有效的策略，能够通过动态生成合成数据来改善时间序列基础模型的训练过程和最终表现。

Abstract: Time Series Foundation Models (TSFMs) are a powerful paradigm for time series analysis and are often enhanced by synthetic data augmentation to improve the training data quality. Existing augmentation methods, however, typically rely on heuristics and static paradigms. Motivated by dynamic data optimization, which shows that the contribution of samples varies across training stages, we propose OATS (Online Data Augmentation for Time Series Foundation Models), a principled strategy that generates synthetic data tailored to different training steps. OATS leverages valuable training samples as principled guiding signals and dynamically generates high-quality synthetic data conditioned on them. We further design a diffusion-based framework to produce realistic time series and introduce an explore-exploit mechanism to balance efficiency and effectiveness. Experiments on TSFMs demonstrate that OATS consistently outperforms regular training and yields substantial performance gains over static data augmentation baselines across six validation datasets and two TSFM architectures. The code is available at the link https://github.com/microsoft/TimeCraft.

</details>


### [43] [Speed is Confidence](https://arxiv.org/abs/2601.19085)
*Joshua V. Dillon*

Main category: cs.LG

TL;DR: 该论文提出了一种基于最早停止计算的Tiny Recursive Models (TRM)集成方法，用于提高Sudoku-Extreme谜题的准确率。通过仅依据首个完成计算的模型而不是平均预测结果来做出预测，实现了在减少10倍计算量的同时达到97.2%的准确性。此外，通过在训练时保持K=4个并行潜在状态但仅通过损失最小的'胜者'反向传播，单个模型能够以一次前向传递匹配测试时增强（TTA）的表现，达到96.9%的准确度。


<details>
  <summary>Details</summary>
Motivation: 受到生物神经系统的启发，研究者们试图找到一种快速且节能的方法来处理复杂的推理任务。传统方法如测试时数据增强虽然有效，但需要大量的计算资源。因此，动机在于开发一种更高效、计算成本更低的解决方案。

Method: 采用Tiny Recursive Models (TRM)组成的集合，并基于“先到先得”的原则，即根据哪个模型最先完成计算来决定最终预测结果。此外，在训练过程中维持多个(K=4)并行的状态，但仅对表现最好的一个进行反向传播更新。为了实现这一目标，还引入了一种改进版的SwiGLU激活函数——Muon，以支持这种方法的有效性。

Result: 与使用测试时增强技术相比，所提出的方法能够在显著降低计算需求的情况下，达到相似甚至更高的解题准确率。特别地，在Sudoku-Extreme问题上达到了97.2%的准确率，而基准方法仅有86.1%。当只使用单个模型并通过特殊训练技巧时，也能够接近或达到测试时增强的效果。

Conclusion: 这项研究表明，通过模仿自然界的效率策略，可以设计出更加高效的人工智能算法。即使是在有限资源条件下，也能通过创新的方法和优化的技术（如改进后的SwiGLU）取得优异的结果。

Abstract: Biological neural systems must be fast but are energy-constrained. Evolution's solution: act on the first signal. Winner-take-all circuits and time-to-first-spike coding implicitly treat when a neuron fires as an expression of confidence. We apply this principle to ensembles of Tiny Recursive Models (TRM). By basing the ensemble prediction solely on the first to halt rather than averaging predictions, we achieve 97.2% puzzle accuracy on Sudoku-Extreme while using 10x less compute than test-time augmentation (the baseline achieves 86.1% single-pass, 97.3% with TTA). Inference speed is an implicit indication of confidence. But can this capability be manifested as a training-only cost? Evidently yes: by maintaining K = 4 parallel latent states during training but backpropping only through the lowest-loss "winner," a single model achieves 96.9% +/- 0.6% puzzle accuracy with a single forward pass-matching TTA performance without any test-time augmentation. As in nature, this work was also resource constrained: all experimentation used a single RTX 5090. This necessitated efficiency and compelled our invention of a modified SwiGLU which made Muon viable. With Muon and K = 1 training, we exceed TRM baseline performance in 7k steps (40 min). Higher accuracy requires 36k steps: 1.5 hours for K = 1, 6 hours for K = 4.

</details>


### [44] [EPAS: Efficient Training with Progressive Activation Sharing](https://arxiv.org/abs/2601.19089)
*Rezaul Karim,Maryam Dialameh,Yang Liu,Boxing Chen,Walid Ahmed*

Main category: cs.LG

TL;DR: 提出了一种新的高效训练方法EPAS，通过逐步增加共享激活区域来减少计算量，从而提高训练和推理吞吐量。实验表明该方法在保持模型性能的同时，可以显著提升LLaMA模型的训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决Transformer深层中冗余QK（或KV）激活的问题，并提高训练与推理效率，提出了EPAS方法。

Method: EPAS方法通过渐进地将解码层切换到激活共享模式来增长共享区域，利用了模型深层中的冗余现象以减少计算需求。

Result: 实验证明，在125M至7B参数范围内的LLaMA模型上应用EPAS可使训练吞吐量最多提高11.1%，推理吞吐量最多提高29%，同时保持与基线模型相似的损失曲线；此外，使用EPAS进行持续预训练转换TinyLLaMA模型为注意力共享模型时，平均准确率比现有最佳方法提高了10%。

Conclusion: EPAS提供了一种有效的方法来利用跨层激活共享模型中的冗余性，通过渐进式训练策略实现了训练及推理效率的显著改进。

Abstract: We present a novel method for Efficient training with Progressive Activation Sharing (EPAS). This method bridges progressive training paradigm with the phenomenon of redundant QK (or KV ) activations across deeper layers of transformers. EPAS gradually grows a sharing region during training by switching decoder layers to activation sharing mode. This results in throughput increase due to reduced compute. To utilize deeper layer redundancy, the sharing region starts from the deep end of the model and grows towards the shallow end. The EPAS trained models allow for variable region lengths of activation sharing for different compute budgets during inference. Empirical evaluations with QK activation sharing in LLaMA models ranging from 125M to 7B parameters show up to an 11.1% improvement in training throughput and up to a 29% improvement in inference throughput while maintaining similar loss curve to the baseline models. Furthermore, applying EPAS in continual pretraining to transform TinyLLaMA into an attention-sharing model yields up to a 10% improvement in average accuracy over state-of-the-art methods, emphasizing the significance of progressive training in cross layer activation sharing models.

</details>


### [45] [Privacy-Preserving Model Transcription with Differentially Private Synthetic Distillation](https://arxiv.org/abs/2601.19090)
*Bochao Liu,Shiming Ge,Pengju Wang,Shikun Li,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为差异私有合成蒸馏的协同-竞争学习方法，通过可训练生成器将预训练模型（教师）转换为保护隐私的对应模型（学生），无需访问私人数据。该方法在统一框架中交替优化三个参与者：生成器、教师和学生。最终转录的学生模型具有良好的性能和隐私保护，并且生成器可以为下游任务生成私有的合成数据。


<details>
  <summary>Details</summary>
Motivation: 许多基于私有数据集训练的深度学习模型在实际任务中的部署可能会导致隐私泄露的风险，因为攻击者可以从模型中恢复出有用的数据或标签知识。为了促进模型在保证隐私的情况下进行部署，本研究提出了隐私保护模型转录技术。

Method: 提出了一种名为“差异私有合成蒸馏”的协同-竞争学习方法，通过一个可训练生成器来实现从预训练模型（教师）到其隐私保护版本（学生）的转换过程，此过程中不使用任何原始私有数据。整个学习过程在一个统一框架内由三部分交替优化完成：1) 学习生成合成数据的生成器；2) 教师与学生接受这些合成数据并计算带有噪声扰动的差分隐私标签；3) 使用带噪声的标签更新学生模型，同时以学生作为判别器对生成器进行对抗性训练。

Result: 理论证明了所提方法能够确保差分隐私及算法收敛。实验结果表明，转录后得到的学生模型不仅保持了良好的性能还提供了有效的隐私保护措施，与此同时，该方法还能产生可用于其他任务的私有合成数据。此外，大量实验证明本方法优于26种现有最先进技术。

Conclusion: 通过引入差异私有合成蒸馏方法，本研究成功地实现了模型间的转换，在不依赖于原始私有数据的前提下保障了模型的隐私安全。这不仅为模型的安全部署提供了解决方案，也为后续利用私有数据生成的合成数据开展更多应用开辟了道路。

Abstract: While many deep learning models trained on private datasets have been deployed in various practical tasks, they may pose a privacy leakage risk as attackers could recover informative data or label knowledge from models. In this work, we present \emph{privacy-preserving model transcription}, a data-free model-to-model conversion solution to facilitate model deployment with a privacy guarantee. To this end, we propose a cooperative-competitive learning approach termed \emph{differentially private synthetic distillation} that learns to convert a pretrained model (teacher) into its privacy-preserving counterpart (student) via a trainable generator without access to private data. The learning collaborates with three players in a unified framework and performs alternate optimization: i)~the generator is learned to generate synthetic data, ii)~the teacher and student accept the synthetic data and compute differential private labels by flexible data or label noisy perturbation, and iii)~the student is updated with noisy labels and the generator is updated by taking the student as a discriminator for adversarial training. We theoretically prove that our approach can guarantee differential privacy and convergence. The transcribed student has good performance and privacy protection, while the resulting generator can generate private synthetic data for downstream tasks. Extensive experiments clearly demonstrate that our approach outperforms 26 state-of-the-arts.

</details>


### [46] [Out-of-Distribution Generalization for Neural Physics Solvers](https://arxiv.org/abs/2601.19091)
*Zhao Wei,Chin Chun Ooi,Jian Cheng Wong,Abhishek Gupta,Pao-Hsiung Chiu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 本文介绍了一种新的神经物理求解器NOVA，它能够快速准确地解决超出训练范围的物理问题，包括在偏微分方程参数、几何形状和初始条件上存在分布变化的情况下。NOVA通过从少量场景中学习与物理对齐的表示，显著降低了复杂非线性问题中的分布外误差，并展示了其在稳定长时间动态预测和改进生成设计方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的神经物理求解器在探索新颖设计和长时间范围预测时遇到了泛化能力不足的问题。为了解决这一限制并促进科学发现领域内新假设空间的探索，研究者们提出了NOVA方法。

Method: NOVA通过从一组稀疏的场景开始学习与物理一致的表达方式，这种方法允许即使面对偏微分方程参数、几何结构以及初始条件的变化也能提供快速而精确的解决方案。

Result: 实验结果表明，在处理复杂的非线性问题如热传递、扩散-反应及流体流动时，相比于基于数据的方法，NOVA能将分布外错误降低1-2个数量级。此外，NOVA还被应用于非线性图灵系统模拟和流体芯片优化中，进一步证明了其对于稳定长时间动力学预测和增强生成设计的有效性。

Conclusion: NOVA代表了一种向通用型神经物理求解器迈进的重要步骤，它不仅能够在已知条件下表现出色，而且能够在未知情况下可靠地进行外推，这对于需要探索新奇假设空间的科学研究至关重要。

Abstract: Neural physics solvers are increasingly used in scientific discovery, given their potential for rapid in silico insights into physical, materials, or biological systems and their long-time evolution. However, poor generalization beyond their training support limits exploration of novel designs and long-time horizon predictions. We introduce NOVA, a route to generalizable neural physics solvers that can provide rapid, accurate solutions to scenarios even under distributional shifts in partial differential equation parameters, geometries and initial conditions. By learning physics-aligned representations from an initial sparse set of scenarios, NOVA consistently achieves 1-2 orders of magnitude lower out-of-distribution errors than data-driven baselines across complex, nonlinear problems including heat transfer, diffusion-reaction and fluid flow. We further showcase NOVA's dual impact on stabilizing long-time dynamical rollouts and improving generative design through application to the simulation of nonlinear Turing systems and fluidic chip optimization. Unlike neural physics solvers that are constrained to retrieval and/or emulation within an a priori space, NOVA enables reliable extrapolation beyond known regimes, a key capability given the need for exploration of novel hypothesis spaces in scientific discovery

</details>


### [47] [OWLEYE: Zero-Shot Learner for Cross-Domain Graph Data Anomaly Detection](https://arxiv.org/abs/2601.19102)
*Lecheng Zheng,Dongqi Fu,Zihao Li,Jingrui He*

Main category: cs.LG

TL;DR: 提出了一种名为OWLEYE的新型零样本图异常检测框架，通过跨域特征对齐模块、多域多模式字典学习和基于截断注意力的重建模块来实现无标签数据下未见过图结构数据的异常检测。


<details>
  <summary>Details</summary>
Motivation: 面对大量多领域的图数据，新兴的努力试图开发出能够无需重新训练即可在未见过的图中检测异常的基础通用模型。然而，跨域图数据的不同特征语义和维度严重阻碍了图基础模型的发展，使得进一步深入的持续学习和推理能力成为一个相当开放的问题。

Method: OWLEYE提出了一个跨域特征对齐模块来协调特征分布，并保持了领域特定的语义；设计了多域多模式字典学习来编码共享结构和基于属性的模式；开发了一个基于截断注意力的重建模块以稳健地检测异常，而不需要为未见过的图结构数据提供标记数据。

Result: 在真实世界数据集上的广泛实验表明，OWLEYE与最先进基线相比实现了更优的表现和泛化能力，为可扩展且标签高效的异常检测奠定了坚实基础。

Conclusion: OWLEYE通过其独特的三重贡献——跨域特征对齐、多域多模式字典学习以及基于截断注意力的重建——有效地解决了跨领域图数据中的异常检测问题，展示了优越的性能和泛化性。

Abstract: Graph data is informative to represent complex relationships such as transactions between accounts, communications between devices, and dependencies among machines or processes. Correspondingly, graph anomaly detection (GAD) plays a critical role in identifying anomalies across various domains, including finance, cybersecurity, manufacturing, etc. Facing the large-volume and multi-domain graph data, nascent efforts attempt to develop foundational generalist models capable of detecting anomalies in unseen graphs without retraining. To the best of our knowledge, the different feature semantics and dimensions of cross-domain graph data heavily hinder the development of the graph foundation model, leaving further in-depth continual learning and inference capabilities a quite open problem. Hence, we propose OWLEYE, a novel zero-shot GAD framework that learns transferable patterns of normal behavior from multiple graphs, with a threefold contribution. First, OWLEYE proposes a cross-domain feature alignment module to harmonize feature distributions, which preserves domain-specific semantics during alignment. Second, with aligned features, to enable continuous learning capabilities, OWLEYE designs the multi-domain multi-pattern dictionary learning to encode shared structural and attribute-based patterns. Third, for achieving the in-context learning ability, OWLEYE develops a truncated attention-based reconstruction module to robustly detect anomalies without requiring labeled data for unseen graph-structured data. Extensive experiments on real-world datasets demonstrate that OWLEYE achieves superior performance and generalizability compared to state-of-the-art baselines, establishing a strong foundation for scalable and label-efficient anomaly detection.

</details>


### [48] [Learning Ordered Representations in Latent Space for Intrinsic Dimension Estimation via Principal Component Autoencoder](https://arxiv.org/abs/2601.19179)
*Qipeng Zhan,Zhuoping Zhou,Zexuan Wang,Li Shen*

Main category: cs.LG

TL;DR: 本文提出了一种新的自编码器框架，该框架结合了非均匀方差正则化和等距约束，从而自然地推广了PCA，保留了有序表示和方差保留等关键优势，同时对非线性降维任务有效。


<details>
  <summary>Details</summary>
Motivation: 之前的方法在处理非线性问题时，无法独立于非线性映射恰当地捕捉剩余的方差。为解决这一不足，并保持PCA的优点如有序表示和方差保留，同时使模型适用于非线性降维任务，作者提出了新的方法。

Method: 提出的新自编码器框架集成了非均匀方差正则化与一个等距约束条件，作为PCA的一种自然泛化形式。

Result: 所提出的框架不仅能够维持PCA的关键优点，还能够在非线性降维任务中表现良好。

Conclusion: 通过引入非均匀方差正则化及等距约束，新设计的自编码器框架成功地将PCA的优点扩展到了非线性场景下。

Abstract: Autoencoders have long been considered a nonlinear extension of Principal Component Analysis (PCA). Prior studies have demonstrated that linear autoencoders (LAEs) can recover the ordered, axis-aligned principal components of PCA by incorporating non-uniform $\ell_2$ regularization or by adjusting the loss function. However, these approaches become insufficient in the nonlinear setting, as the remaining variance cannot be properly captured independently of the nonlinear mapping. In this work, we propose a novel autoencoder framework that integrates non-uniform variance regularization with an isometric constraint. This design serves as a natural generalization of PCA, enabling the model to preserve key advantages, such as ordered representations and variance retention, while remaining effective for nonlinear dimensionality reduction tasks.

</details>


### [49] [Foresight Learning for SEC Risk Prediction](https://arxiv.org/abs/2601.19189)
*Benjamin Turtel,Paul Wilczewski,Danny Franklin,Kris Skotheim*

Main category: cs.LG

TL;DR: 本研究提出了一种全自动的数据生成流程，将SEC文件中的定性风险披露转换为基于时间的监督数据，并使用这些数据训练了一个紧凑的大语言模型来预测特定时间内披露的风险实现的概率。该模型不仅在概率准确性和校准方面优于预训练和通用模型（包括GPT-5），而且展示了仅使用原始领域内文本即可规模化、全自动化地训练领域特定专家模型的可能性。


<details>
  <summary>Details</summary>
Motivation: 当前SEC文件中的风险披露多为描述性质，缺乏对潜在不利事件发生概率的具体量化，这限制了它们在概率分析中的应用价值。此外，缺乏大规模、针对具体风险层面的监管数据来关联已披露的风险与实际结果也是主要障碍之一。

Method: 开发了一套全自动数据生成流程，能够从SEC文件的风险因素部分生成特定于公司的、有时限的风险查询，并通过自动解析后续披露的信息来标记这些查询的结果。利用由此产生的风险查询及其结果数据集，训练了一个专门用于估计指定时间段内披露风险实现概率的小型大语言模型。

Result: 尽管模型规模较小，但在概率准确性和校准方面显著优于预训练和基准方法，并且在性能上超越了包括GPT-5在内的前沿通用模型。更重要的是，这项工作证明了仅依靠原始、按时间顺序排列的领域内文本文档就能实现领域特定专家模型的规模化和全自动化训练。

Conclusion: 本研究表明，通过前瞻学习的方法可以利用自然发生的公司文档学习到经过校准、对决策有用的信号，而无需依赖专有数据、外部语料库或人工标注。同时，所得到的模型能够在单一GPU上部署并达到前沿水平的表现。

Abstract: Risk disclosures in SEC filings describe potential adverse events but rarely quantify their likelihood, limiting their usefulness for probabilistic analysis. A central obstacle is the absence of large-scale, risk-level supervision linking disclosed risks to realized outcomes.
  We introduce a fully automated data generation pipeline that converts qualitative SEC risk disclosures into temporally grounded supervision using only public data. For each filing, the pipeline generates firm-specific, time-bounded risk queries from the Risk Factors section and labels them by automatically resolving outcomes against subsequent disclosures.
  Using this dataset of risk queries and outcomes grounded in SEC filings, we train a compact large language model to estimate the probability that a disclosed risk will materialize within a specified horizon. Despite its modest size, the resulting model substantially improves over pretrained and heuristic baselines, and outperforms frontier general-purpose models, including GPT-5, on probabilistic accuracy and calibration.
  More broadly, this work demonstrates that Foresight Learning enables scalable and fully automated training of domain-specific expert models using only raw, chronological, in-domain text -- without proprietary data, external corpora, or manual annotation. The resulting models achieve frontier-level performance while remaining deployable on a single GPU. This result suggests a general pathway for learning calibrated, decision-relevant signals from naturally occurring enterprise documents.
  To support transparency and reproducibility, we open-source the evaluation dataset used in this study.
  Evaluation Data: https://huggingface.co/datasets/LightningRodLabs/sec_risk_questions_test_set
  Data Generation Platform: https://lightningrod.ai/
  SDK: https://github.com/lightning-rod-labs/lightningrod-python-sdk

</details>


### [50] [Accelerated Multiple Wasserstein Gradient Flows for Multi-objective Distributional Optimization](https://arxiv.org/abs/2601.19220)
*Dai Hai Nguyen,Duc Dung Nguyen,Atsuyoshi Nakamura,Hiroshi Mamitsuka*

Main category: cs.LG

TL;DR: 本文提出了一种加速的多目标Wasserstein梯度下降算法A-MWGraD，理论分析表明其在几何凸情况下收敛速度达到O(1/t^2)，在强几何凸情况下达到O(e^{-\sqrt{\beta}t})。此外，通过数值实验验证了该方法在收敛速度和采样效率上优于MWGraD。


<details>
  <summary>Details</summary>
Motivation: 基于最近Nguyen等人（2025）提出的利用Wasserstein空间几何结构进行多目标联合优化的方法，作者们旨在开发一种加速版本，以提高在概率分布上的多目标优化效率。

Method: 提出了A-MWGraD算法，该算法受到Nesterov加速技术的启发，并且对于此算法进行了连续时间动力学分析，证明了它能够收敛到概率空间中的弱帕累托最优解。另外，还为A-MWGraD引入了一个实用的基于核的离散化方案。

Result: 理论结果显示，对于几何凸目标函数，A-MWGraD的收敛率为O(1/t^2)；对于β-强几何凸目标函数，收敛率达到了O(e^{-\sqrt{\beta}t})。实验证明，在多目标采样任务中，A-MWGraD比MWGraD具有更快的收敛速度和更高的采样效率。

Conclusion: A-MWGraD不仅在理论上提高了多目标优化问题的收敛速度，而且在实际应用中也表现出色，特别是在处理多目标采样任务时。

Abstract: We study multi-objective optimization over probability distributions in Wasserstein space. Recently, Nguyen et al. (2025) introduced Multiple Wasserstein Gradient Descent (MWGraD) algorithm, which exploits the geometric structure of Wasserstein space to jointly optimize multiple objectives. Building on this approach, we propose an accelerated variant, A-MWGraD, inspired by Nesterov's acceleration. We analyze the continuous-time dynamics and establish convergence to weakly Pareto optimal points in probability space. Our theoretical results show that A-MWGraD achieves a convergence rate of O(1/t^2) for geodesically convex objectives and O(e^{-\sqrtβt}) for $β$-strongly geodesically convex objectives, improving upon the O(1/t) rate of MWGraD in the geodesically convex setting. We further introduce a practical kernel-based discretization for A-MWGraD and demonstrate through numerical experiments that it consistently outperforms MWGraD in convergence speed and sampling efficiency on multi-target sampling tasks.

</details>


### [51] [Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model](https://arxiv.org/abs/2601.19232)
*Qi Si,Xuyang Liu,Penglei Wang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种结合潜扩散模型(LDM)和强化学习(RL)的新框架SOLD，用于RNA逆折叠问题。通过单步噪声优化而非完整扩散轨迹采样，该方法在处理非可微结构目标时表现出色，实验结果表明其在所有指标上均优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前的RNA逆折叠方法主要集中在序列恢复上，在处理如二级结构一致性(SS)、最小自由能(MFE)及局部距离差异测试(LDDT)等结构目标时表现不佳，导致结构准确性不足。为解决这一问题，作者提出了新的解决方案。

Method: 提出的解决方案是一种名为SOLD（Step-wise Optimization of Latent Diffusion Model）的新RL框架，它利用预训练的RNA-FM嵌入来增强LDM，并通过策略驱动的奖励优化来有效应对复杂的非梯度型目标，而不需要对整个扩散路径进行采样。

Result: 实验结果显示，SOLD不仅在其LDM基线方面取得了超越，而且还在所有相关度量上超过了最先进的方法，证明了其在提高RNA逆折叠任务中的结构准确性和效率方面的有效性。

Conclusion: SOLD框架提供了一个强大的工具，用于改善RNA逆折叠过程中的多个结构性目标，对于生物技术和治疗应用具有深远的意义。

Abstract: RNA inverse folding, designing sequences to form specific 3D structures, is critical for therapeutics, gene regulation, and synthetic biology. Current methods, focused on sequence recovery, struggle to address structural objectives like secondary structure consistency (SS), minimum free energy (MFE), and local distance difference test (LDDT), leading to suboptimal structural accuracy. To tackle this, we propose a reinforcement learning (RL) framework integrated with a latent diffusion model (LDM). Drawing inspiration from the success of diffusion models in RNA inverse folding, which adeptly model complex sequence-structure interactions, we develop an LDM incorporating pre-trained RNA-FM embeddings from a large-scale RNA model. These embeddings capture co-evolutionary patterns, markedly improving sequence recovery accuracy. However, existing approaches, including diffusion-based methods, cannot effectively handle non-differentiable structural objectives. By contrast, RL excels in this task by using policy-driven reward optimization to navigate complex, non-gradient-based objectives, offering a significant advantage over traditional methods. In summary, we propose the Step-wise Optimization of Latent Diffusion Model (SOLD), a novel RL framework that optimizes single-step noise without sampling the full diffusion trajectory, achieving efficient refinement of multiple structural objectives. Experimental results demonstrate SOLD surpasses its LDM baseline and state-of-the-art methods across all metrics, establishing a robust framework for RNA inverse folding with profound implications for biotechnological and therapeutic applications.

</details>


### [52] [LLM-Assisted Logic Rule Learning: Scaling Human Expertise for Time Series Anomaly Detection](https://arxiv.org/abs/2601.19255)
*Haoting Zhang,Shekhar Jain*

Main category: cs.LG

TL;DR: 本文提出了一种利用大型语言模型（LLMs）系统地将人类专业知识编码为可解释的、基于逻辑的规则，以检测供应链时间序列数据中的异常模式的框架。该方法通过LLM指导的数据标注、自动生成并迭代优化符号规则以及通过LLM支持增加业务相关异常类别来增强可解释性三个阶段操作。实验结果显示，相比无监督学习方法，本方法在检测准确性和可解释性方面表现更优，并且与直接部署LLM进行时间序列异常检测相比，提供了稳定、确定的结果，同时具有低计算延迟和成本，更适合生产环境部署。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测对于供应链管理采取主动措施至关重要，但面临两个挑战：基于数据模式探索的经典无监督异常检测往往产生与业务需求和领域知识不一致的结果；而手动专家分析无法扩展到供应链中数百万的产品。因此，需要一种既能规模化又能结合专家知识的方法来进行异常检测。

Method: 提出了一个三阶段框架：1) 由领域知识指导的LLM基础训练数据标注；2) 通过LLM驱动的优化过程自动生成功能性和可迭代改进的符号规则；3) 利用LLM支持添加与业务相关的异常类别以提高可解释性。

Result: 实验结果表明，所提出的方法在检测准确度和可解释性方面优于无监督学习方法。此外，相较于直接使用LLM进行时间序列异常检测，该方法能够提供一致性更强、确定性更高的结果，同时保持较低的计算延迟和成本，非常适合实际生产环境中的部署。

Conclusion: 提出的框架展示了如何利用LLM来填补可扩展自动化与专家驱动决策之间的空白，在运营环境中实现两者的优势结合。

Abstract: Time series anomaly detection is critical for supply chain management to take proactive operations, but faces challenges: classical unsupervised anomaly detection based on exploiting data patterns often yields results misaligned with business requirements and domain knowledge, while manual expert analysis cannot scale to millions of products in the supply chain. We propose a framework that leverages large language models (LLMs) to systematically encode human expertise into interpretable, logic-based rules for detecting anomaly patterns in supply chain time series data. Our approach operates in three stages: 1) LLM-based labeling of training data instructed by domain knowledge, 2) automated generation and iterative improvements of symbolic rules through LLM-driven optimization, and 3) rule augmentation with business-relevant anomaly categories supported by LLMs to enhance interpretability. The experiment results showcase that our approach outperforms the unsupervised learning methods in both detection accuracy and interpretability. Furthermore, compared to direct LLM deployment for time series anomaly detection, our approach provides consistent, deterministic results with low computational latency and cost, making it ideal for production deployment. The proposed framework thus demonstrates how LLMs can bridge the gap between scalable automation and expert-driven decision-making in operational settings.

</details>


### [53] [E-QRGMM: Efficient Generative Metamodeling for Covariate-Dependent Uncertainty Quantification](https://arxiv.org/abs/2601.19256)
*Zhiyang Liang,Qingkai Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为E-QRGMM的新框架，通过结合三次Hermite插值与梯度估计来加速基于分位数回归的生成元模型方法，有效解决了协变量依赖不确定性量化的问题，同时在计算效率和分布准确性上取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如共形预测和经典自助法在处理协变量特定条件时存在局限性，这使得基于模拟推理中的协变量依赖不确定性量化变得困难，尤其是在高风险决策中尤为重要。

Method: Efficient Quantile-Regression-Based Generative Metamodeling (E-QRGMM) 结合了三次Hermite插值与梯度估计技术，以减少网格复杂度并提高计算效率。

Result: E-QRGMM不仅保持了原QRGMM的收敛速度，还将大多数分位数水平下的网格复杂度从O(n^1/2)降低到了O(n^1/5)，显著提升了计算效率，并且在合成数据集和实际数据集中相比其他先进深度生成模型展现出了更优的分布准确性和训练速度之间的权衡。

Conclusion: E-QRGMM为协变量依赖不确定性量化提供了一个实用解决方案，能够针对任意感兴趣的估计量构建基于自助法的信心区间。

Abstract: Covariate-dependent uncertainty quantification in simulation-based inference is crucial for high-stakes decision-making but remains challenging due to the limitations of existing methods such as conformal prediction and classical bootstrap, which struggle with covariate-specific conditioning. We propose Efficient Quantile-Regression-Based Generative Metamodeling (E-QRGMM), a novel framework that accelerates the quantile-regression-based generative metamodeling (QRGMM) approach by integrating cubic Hermite interpolation with gradient estimation. Theoretically, we show that E-QRGMM preserves the convergence rate of the original QRGMM while reducing grid complexity from $O(n^{1/2})$ to $O(n^{1/5})$ for the majority of quantile levels, thereby substantially improving computational efficiency. Empirically, E-QRGMM achieves a superior trade-off between distributional accuracy and training speed compared to both QRGMM and other advanced deep generative models on synthetic and practical datasets. Moreover, by enabling bootstrap-based construction of confidence intervals for arbitrary estimands of interest, E-QRGMM provides a practical solution for covariate-dependent uncertainty quantification.

</details>


### [54] [Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2601.19280)
*Kishan Panaganti,Zhenwen Liang,Wenhao Yu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 本文提出了一种名为多对手组分布鲁棒优化（GDRO）的框架，通过动态调整训练分布来改进大型语言模型的推理能力。该方法包括在线难度分类器以及两种独立的GDRO游戏：Prompt-GDRO和Rollout-GDRO，分别用于提高难问题上的梯度方差减少和计算效率。实验结果表明，与GRPO基线相比，在1.7B、4B和8B规模上，Prompt-GDRO和Rollout-GDRO在pass@8准确性方面平均相对提高了10.6%和10.1%。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）推理的进步很大程度上受到后训练损失函数和对齐策略改进的驱动。然而，标准的强化学习（RL）范式如群组相对策略优化（GRPO），由于采用统一提示采样和每条提示固定数量的rollouts，对于异构且重尾分布的推理数据来说，这导致了结构性低效，即过多计算资源被浪费在已经解决的问题模式上，而难以解决的问题却训练不足。

Method: 研究者提出了一个多对手组分布鲁棒优化（GDRO）框架，旨在超越均匀推理模型，通过动态适应训练分布以提高效率。该框架引入了一个在线难度分类器，将提示划分成不同难度等级，并基于此提出了两种独立的GDRO游戏：Prompt-GDRO使用EMA去偏移乘法权重带宽采样器针对高难度边缘并增加持续困难群体的权重；Rollout-GDRO则利用影子价格控制器重新分配各组之间的rollouts，以最大化在固定均值预算下对难题任务的梯度方差减少。

Result: 通过对DAPO 14.1k数据集上的Qwen3-Base模型进行验证，发现与GRPO基准相比，Prompt-GDRO和Rollout-GDRO分别实现了平均相对增益+10.6%和+10.1%，在1.7B、4B和8B规模下的pass@8准确性得到了显著提升。此外，定性分析显示出现了一种新兴课程：对手们将资源转移到不断变化的推理前沿，从而增强了推理模型的表现。

Conclusion: 本研究表明，通过采用GDRO框架可以有效改善大型语言模型在处理复杂推理任务时的表现，特别是在处理那些传统方法难以覆盖到的‘长尾’难题时展现出明显优势。

Abstract: Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.
  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.

</details>


### [55] [Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework](https://arxiv.org/abs/2601.19285)
*Xinyu Zhou,Jiawei Zhang,Stephen J. Wright*

Main category: cs.LG

TL;DR: 本文探讨了扩散模型中的记忆现象，通过理论框架解释了这一问题的成因，并提出两种新方法——噪声去条件化和温度平滑——来提高泛化能力。实验表明这些方法在保持高生成质量的同时有效提高了模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决扩散模型中由于训练样本主导得分函数而导致的记忆问题，这限制了模型的泛化能力。

Method: 1. 建立了一个理论框架来解释记忆现象，指出经验得分函数可视为高斯分布得分函数的加权和。
2. 提出两种新方法：噪声去条件化（让每个训练样本自适应地确定其得分函数权重）与温度平滑（引入一个显式参数控制平滑度），以增强模型的泛化能力。

Result: 实验结果验证了理论分析的有效性，并展示了所提方法能够显著改善模型的泛化性能同时保持良好的生成质量。

Conclusion: 通过理解并调整扩散模型中得分函数的行为，可以有效地缓解记忆效应，从而提高模型的泛化能力和生成质量。

Abstract: Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.

</details>


### [56] [Queue Length Regret Bounds for Contextual Queueing Bandits](https://arxiv.org/abs/2601.19300)
*Seoungbin Bae,Garyeong Kang,Dabeen Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的上下文感知框架——上下文排队bandits，用于在学习未知服务速率的同时进行调度。通过引入策略切换队列和复杂的耦合论证，解决了不同策略下队列状态差异的问题，并提出了两种算法CQB-$\varepsilon$和CQB-Opt，分别实现了$\widetilde{\mathcal{O}}(T^{-1/4})$和$\mathcal{O}(\log^2 T)$的遗憾上界。实验结果验证了理论发现的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决在同时学习未知服务率的情况下如何根据个体任务携带的异构上下文特征选择任务并匹配服务器以最大化离开率的问题。为此，提出了一种新的上下文感知框架来优化队列长度遗憾。

Method: 作者们提出了上下文排队bandits框架，其中服务/离开率由基于上下文特征的逻辑模型决定，该模型具有未知的服务器特定参数。为了解决分析中的主要挑战，即不同策略下队列中剩余作业特征列表可能不同的问题，他们提出了策略切换队列的概念，并使用复杂的耦合论点来处理这个问题。此外，还开发了两种算法：CQB-$\varepsilon$ 和 CQB-Opt 来实现对遗憾的有效控制。

Result: 研究显示，CQB-$\varepsilon$ 算法达到了 $\widetilde{\mathcal{O}}(T^{-1/4})$ 的遗憾上界；而针对对抗性选择上下文的情况，CQB-Opt 算法则达到了 $\mathcal{O}(\log^2 T)$ 的遗憾上界。实验证明了这些理论发现。

Conclusion: 本文提出的上下文排队bandits框架及其相关算法（CQB-$\varepsilon$ 和 CQB-Opt）成功地解决了在学习未知服务率的同时进行有效调度的问题，通过创新的方法分析了短期与长期决策对队列状态的影响，并得到了良好的理论和实验结果支持。

Abstract: We introduce contextual queueing bandits, a new context-aware framework for scheduling while simultaneously learning unknown service rates. Individual jobs carry heterogeneous contextual features, based on which the agent chooses a job and matches it with a server to maximize the departure rate. The service/departure rate is governed by a logistic model of the contextual feature with an unknown server-specific parameter. To evaluate the performance of a policy, we consider queue length regret, defined as the difference in queue length between the policy and the optimal policy. The main challenge in the analysis is that the lists of remaining job features in the queue may differ under our policy versus the optimal policy for a given time step, since they may process jobs in different orders. To address this, we propose the idea of policy-switching queues equipped with a sophisticated coupling argument. This leads to a novel queue length regret decomposition framework, allowing us to understand the short-term effect of choosing a suboptimal job-server pair and its long-term effect on queue state differences. We show that our algorithm, CQB-$\varepsilon$, achieves a regret upper bound of $\widetilde{\mathcal{O}}(T^{-1/4})$. We also consider the setting of adversarially chosen contexts, for which our second algorithm, CQB-Opt, achieves a regret upper bound of $\mathcal{O}(\log^2 T)$. Lastly, we provide experimental results that validate our theoretical findings.

</details>


### [57] [LightSBB-M: Bridging Schrödinger and Bass for Generative Diffusion Modeling](https://arxiv.org/abs/2601.19312)
*Alexandre Alouadi,Pierre Henry-Labordère,Grégoire Loeper,Othmane Mazhar,Huyên Pham,Nizar Touzi*

Main category: cs.LG

TL;DR: 提出了一种名为LightSBB-M的新算法，该算法能够高效地计算出最优的Schrodinger Bridge and Bass (SBB)传输计划，并且在合成数据集上相比现有方法表现出色。此外，还展示了该框架在非配对图像到图像转换任务中的生成能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高Schrodinger Bridge (SB)模型的性能，特别是当同时控制漂移和波动率时，作者们开发了LightSBB-M算法。这个算法旨在通过较少的迭代次数来计算出最优SBB传输方案，同时提供一个可调节参数β以平衡漂移与波动率的影响。

Method: LightSBB-M算法基于SBB目标函数的对偶表示法，推导出最优漂移和波动率的解析表达式。该方法引入了一个大于零的可调参数β，允许用户根据需要在纯漂移（即Schrodinger桥）与纯波动率（Bass鞅传输）之间进行插值调整。

Result: 实验结果表明，在合成数据集上，LightSBB-M相较于当前先进的SB及扩散基线方法，可以实现高达32%的2-Wasserstein距离改进。此外，该研究还展示了LightSBB-M在成人至儿童脸部图像转换等实际生成任务中的应用潜力。

Conclusion: LightSBB-M为解决SBB问题提供了高效且高质量的解决方案，其表现优于现有的SB及扩散方法，不仅在合成数据集上有所体现，在真实世界生成任务中也展现出了优越性。

Abstract: The Schrodinger Bridge and Bass (SBB) formulation, which jointly controls drift and volatility, is an established extension of the classical Schrodinger Bridge (SB). Building on this framework, we introduce LightSBB-M, an algorithm that computes the optimal SBB transport plan in only a few iterations. The method exploits a dual representation of the SBB objective to obtain analytic expressions for the optimal drift and volatility, and it incorporates a tunable parameter beta greater than zero that interpolates between pure drift (the Schrodinger Bridge) and pure volatility (Bass martingale transport). We show that LightSBB-M achieves the lowest 2-Wasserstein distance on synthetic datasets against state-of-the-art SB and diffusion baselines with up to 32 percent improvement. We also illustrate the generative capability of the framework on an unpaired image-to-image translation task (adult to child faces in FFHQ). These findings demonstrate that LightSBB-M provides a scalable, high-fidelity SBB solver that outperforms existing SB and diffusion baselines across both synthetic and real-world generative tasks. The code is available at https://github.com/alexouadi/LightSBB-M.

</details>


### [58] [StableQAT: Stable Quantization-Aware Training at Ultra-Low Bitwidths](https://arxiv.org/abs/2601.19320)
*Tianyi Chen,Sihan Chen,Xiaoyi Qu,Dan Zhao,Ruomei Yan,Jongwoo Ko,Luming Liang,Pashmina Cameron*

Main category: cs.LG

TL;DR: 提出了一种名为StableQAT的新框架，通过基于离散傅里叶分析的替代方法优化了极低比特设置下的量化感知训练（QAT），改善了训练稳定性、鲁棒性，并且在2-4比特范围内表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 在严格的内存和延迟限制下部署大型模型时，量化感知训练（QAT）是必不可少的，但在极低比特宽度下实现稳定和鲁棒的优化仍然具有挑战性。现有基于直通估计器（STE）或软量化器的方法通常会遇到梯度不匹配、不稳定或高计算开销等问题。

Method: 提出了StableQAT，这是一种统一且高效的QAT框架，通过一种新颖、轻量级且有理论基础的反向传播替代方案来稳定极低比特设置下的训练，该方案是从对舍入操作符进行离散傅里叶分析得出的。

Result: 实验表明，在2到4比特范围内的设定下，StableQAT展示了更加稳定和高效的QAT，与标准QAT技术相比，它提高了训练稳定性、鲁棒性和性能，同时几乎不增加训练开销。

Conclusion: StableQAT提供了一个优于传统方法的解决方案，特别是在极低比特量化场景中，能够有效提高量化模型的训练质量和最终表现。

Abstract: Quantization-aware training (QAT) is essential for deploying large models under strict memory and latency constraints, yet achieving stable and robust optimization at ultra-low bitwidths remains challenging. Common approaches based on the straight-through estimator (STE) or soft quantizers often suffer from gradient mismatch, instability, or high computational overhead. As such, we propose StableQAT, a unified and efficient QAT framework that stabilizes training in ultra low-bit settings via a novel, lightweight, and theoretically grounded surrogate for backpropagation derived from a discrete Fourier analysis of the rounding operator. StableQAT strictly generalizes STE as the latter arises as a special case of our more expressive surrogate family, yielding smooth, bounded, and inexpensive gradients that improve QAT training performance and stability across various hyperparameter choices. In experiments, StableQAT exhibits stable and efficient QAT at 2-4 bit regimes, demonstrating improved training stability, robustness, and superior performance with negligible training overhead against standard QAT techniques. Our code is available at https://github.com/microsoft/StableQAT.

</details>


### [59] [Metric $k$-clustering using only Weak Comparison Oracles](https://arxiv.org/abs/2601.19333)
*Rahul Raychaudhury,Aryan Esmailpour,Sainyam Galhotra,Stavros Sintos*

Main category: cs.LG

TL;DR: 本研究提出了一种基于噪声四元组预言机的随机算法，用于在R模型下进行k-聚类。该方法在任意度量空间中实现了O(n·k·polylog(n))的查询复杂度，在具有有界加倍维度的度量空间中进一步优化到O((n+k^2)·polylog(n))。此外，当度量空间具有有界加倍维度时，通过保持相同的渐近查询复杂度，可将近似比从常数改进至1+ε。


<details>
  <summary>Details</summary>
Motivation: 传统的k-聚类算法（如k-中位数和k-均值）假设能够访问确切的成对距离，这在许多现代应用场景中是不现实的。因此，研究者们探索了在仅能获取相对距离比较的R模型下的聚类问题，其中的距离信息完全由一个四元组预言机提供。这种预言机可以代表学习模型或人类反馈，并且通常带有噪声并涉及访问成本。

Method: 设计了一套使用噪声四元组预言机的随机算法来确定中心点集合并映射输入项到这些中心，使得最终得到的聚类成本不超过最优k-聚类成本的常数倍。对于不同的度量空间类型，算法展示了不同的性能表现，包括在具有有界加倍维度的空间内达到更优的近似比。

Result: 对于任意度量空间，所提算法达到了O(n·k·polylog(n))的查询复杂度；而对于具有有界加倍维度的度量空间，则进一步改善到了O((n+k^2)·polylog(n))。此外，在后一种情况下，还能将近似比率提升至1+ε（ε为任意小正数），同时保持相同的渐近查询复杂度。

Conclusion: 这项工作证明了如何系统地将来自大型语言模型等来源的噪声低代价预言机融入可扩展的聚类算法之中，从而在实际应用中实现高效准确的聚类。

Abstract: Clustering is a fundamental primitive in unsupervised learning. However, classical algorithms for $k$-clustering (such as $k$-median and $k$-means) assume access to exact pairwise distances -- an unrealistic requirement in many modern applications. We study clustering in the \emph{Rank-model (R-model)}, where access to distances is entirely replaced by a \emph{quadruplet oracle} that provides only relative distance comparisons. In practice, such an oracle can represent learned models or human feedback, and is expected to be noisy and entail an access cost.
  Given a metric space with $n$ input items, we design randomized algorithms that, using only a noisy quadruplet oracle, compute a set of $O(k \cdot \mathsf{polylog}(n))$ centers along with a mapping from the input items to the centers such that the clustering cost of the mapping is at most constant times the optimum $k$-clustering cost. Our method achieves a query complexity of $O(n\cdot k \cdot \mathsf{polylog}(n))$ for arbitrary metric spaces and improves to $O((n+k^2) \cdot \mathsf{polylog}(n))$ when the underlying metric has bounded doubling dimension. When the metric has bounded doubling dimension we can further improve the approximation from constant to $1+\varepsilon$, for any arbitrarily small constant $\varepsilon\in(0,1)$, while preserving the same asymptotic query complexity. Our framework demonstrates how noisy, low-cost oracles, such as those derived from large language models, can be systematically integrated into scalable clustering algorithms.

</details>


### [60] [From Observations to Events: Event-Aware World Model for Reinforcement Learning](https://arxiv.org/abs/2601.19336)
*Zhao-Han Peng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.LG

TL;DR: 该论文提出了一种事件感知世界模型（EAWM），通过自动事件生成器和通用事件分割器来学习不依赖于手工标签的事件意识表示，从而简化策略学习。实验表明，EAWM在多个基准测试中显著提高了基于模型的强化学习基线性能10%-45%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的强化学习方法难以在结构相似场景间泛化，并且容易受到纹理或颜色变化等无关紧要的变化影响。受认知科学启发，人类能够将连续的感觉流分割成离散事件并以此为基础进行决策，因此提出了事件感知的世界模型框架以提高策略学习效率。

Method: EAWM框架利用自动事件生成器从原始观察中提取事件，并引入了通用事件分割器（GES）来确定事件边界。通过事件预测，塑造表达空间以捕捉有意义的空间-时间转换。此外，作者还提供了一个统一的形式化描述，将看似不同的世界模型架构联系起来，展示了所提方法的广泛适用性。

Result: 在Atari 100K、Craftax 1M、DeepMind Control 500K以及DMC-GB2 500K等多个基准测试上的实验显示，相比于强大的MBRL基线，EAWM能稳定地提升性能达10%-45%，并在这些基准上设定了新的最先进结果。

Conclusion: EAWM作为一种新颖的方法，不仅有效解决了现有MBRL方法面临的挑战，而且在不需要人工标注的情况下成功地促进了更加高效和鲁棒的策略学习过程。

Abstract: While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.

</details>


### [61] [GraphSB: Boosting Imbalanced Node Classification on Graphs through Structural Balance](https://arxiv.org/abs/2601.19352)
*Zhixiao Wang,Chaofan Zhu,Qihan Feng,Jian Zhang,Xiaobin Rui,Philip S Yu*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架GraphSB，通过结构平衡策略解决图学习中固有的不平衡图结构问题，从而有效提高少数类别的学习效果。实验表明，GraphSB显著优于现有方法，并且可以作为即插即用模块轻松集成到现有方法中，平均准确率提升4.57%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GNN的方法在处理节点分类任务时面临着类别不平衡的问题，这主要由于基础的图结构不平衡导致多数类别占据主导地位和少数类别被同化。当前的数据级与算法级解决方案未能直接应对这一根本性问题。

Method: 提出了GraphSB（图结构平衡）框架，该框架引入了结构平衡作为关键策略来优化图结构。具体来说，它包括两阶段的结构优化：一是通过双视角分析挖掘决策边界附近的难例并增强少数类别的连接性；二是通过关系扩散传播增强后的少数类上下文同时捕捉高阶结构依赖。

Result: 广泛的实验证明了GraphSB相较于最先进方法的有效性。此外，所提出的结构平衡技术能够以简单插件形式无缝整合进现有技术中，使得这些方法的准确性平均提高了4.57%。

Conclusion: GraphSB提供了一个有效的途径来解决GNN中由图结构不平衡引起的问题，不仅提升了少数类的学习表现，还能够轻易地增强现有方法的效果。

Abstract: Imbalanced node classification is a critical challenge in graph learning, where most existing methods typically utilize Graph Neural Networks (GNNs) to learn node representations. These methods can be broadly categorized into the data-level and the algorithm-level. The former aims to synthesize minority-class nodes to mitigate quantity imbalance, while the latter tries to optimize the learning process to highlight minority classes. However, neither of them addresses the inherently imbalanced graph structure, which is a fundamental factor that incurs majority-class dominance and minority-class assimilation in GNNs. Our theoretical analysis further supports this critical insight. Therefore, we propose GraphSB (Graph Structural Balance), a novel framework that incorporates Structural Balance as a key strategy to address the underlying imbalanced graph structure before node synthesis. Structural Balance performs a two-stage structure optimization: Structure Enhancement that mines hard samples near decision boundaries through dual-view analysis and enhances connectivity for minority classes through adaptive augmentation, and Relation Diffusion that propagates the enhanced minority context while simultaneously capturing higher-order structural dependencies. Thus, GraphSB balances structural distribution before node synthesis, enabling more effective learning in GNNs. Extensive experiments demonstrate that GraphSB significantly outperforms the state-of-the-art methods. More importantly, the proposed Structural Balance can be seamlessly integrated into state-of-the-art methods as a simple plug-and-play module, increasing their accuracy by an average of 4.57%.

</details>


### [62] [Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection](https://arxiv.org/abs/2601.19375)
*Quy-Anh Dang,Chris Ngo*

Main category: cs.LG

TL;DR: 本文提出了一种名为Selective Steering的新方法，通过保持范数不变的旋转公式和区分层选择来改进大型语言模型对抗攻击中的行为控制问题。实验表明，该方法在提高攻击成功率的同时，保持了零困惑度违规和大约100%的能力保留。


<details>
  <summary>Details</summary>
Motivation: 尽管在对齐方面取得了显著进展，但大型语言模型（LLMs）仍然容易受到引出有害行为的对抗性攻击。现有激活转向技术要么需要仔细调整系数且对层特定范数变化敏感，要么仅提供二进制控制；最近的工作虽引入了连续控制但实际实现中违反了范数保持原则，导致分布偏移和生成崩溃，特别是在参数量低于7B的模型中更为明显。

Method: 提出了Selective Steering方法，包含两个关键创新点：(1) 数学上严格的保持范数不变的旋转公式，以维持激活分布完整性；(2) 区分层选择策略，仅在特征表示显示出相反符号类对齐的地方应用转向。(3) 通过这些改进，在九个不同模型上的实验验证了其有效性。

Result: 实验结果显示，与先前的方法相比，Selective Steering能够达到5.5倍高的攻击成功率，同时保持零困惑度违规，并且在标准基准测试上大约100%的能力得以保留。

Conclusion: Selective Steering为可控和稳定的LLM行为修改提供了一个有原则、高效的框架。

Abstract: Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering

</details>


### [63] [DSP-Reg: Domain-Sensitive Parameter Regularization for Robust Domain Generalization](https://arxiv.org/abs/2601.19394)
*Xudong Han,Senkang Hu,Yihang Tao,Yu Guo,Philip Birch,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，称为Domain-Sensitive Parameter Regularization (DSP-Reg)，通过参数敏感性分析框架量化模型中每个参数对领域变化的敏感度，并利用软正则化技术鼓励模型更多依赖于领域不变参数，从而提高了模型对于未见领域的鲁棒性和泛化能力。实验表明该方法在多个基准测试上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的领域泛化方法主要集中在学习领域不变特征上，但忽略了参数层面的深入分析，导致难以明确区分对领域变化敏感和不敏感的参数，影响了模型的整体泛化能力。

Method: 首先构建了一个基于协方差的参数敏感性分析框架来量化模型中各参数对领域变化的敏感程度；然后提出了Domain-Sensitive Parameter Regularization (DSP-Reg)框架，通过软正则化技术促进模型更多地依赖于那些对领域变化不太敏感（即领域不变）的参数，同时抑制领域特定参数的影响。

Result: 在PACS、VLCS、OfficeHome及DomainNet等基准数据集上的广泛实验表明，所提DSP-Reg方法达到了66.7%的平均准确率，超越了所有基线方法。

Conclusion: 通过引入参数级敏感性分析以及针对性的正则化策略，本研究为提高模型跨领域泛化性能提供了一条新途径，实验结果证明了其有效性和优越性。

Abstract: Domain Generalization (DG) is a critical area that focuses on developing models capable of performing well on data from unseen distributions, which is essential for real-world applications. Existing approaches primarily concentrate on learning domain-invariant features, which assume that a model robust to variations in the source domains will generalize well to unseen target domains. However, these approaches neglect a deeper analysis at the parameter level, which makes the model hard to explicitly differentiate between parameters sensitive to domain shifts and those robust, potentially hindering its overall ability to generalize. In order to address these limitations, we first build a covariance-based parameter sensitivity analysis framework to quantify the sensitivity of each parameter in a model to domain shifts. By computing the covariance of parameter gradients across multiple source domains, we can identify parameters that are more susceptible to domain variations, which serves as our theoretical foundation. Based on this, we propose Domain-Sensitive Parameter Regularization (DSP-Reg), a principled framework that guides model optimization by a soft regularization technique that encourages the model to rely more on domain-invariant parameters while suppressing those that are domain-specific. This approach provides a more granular control over the model's learning process, leading to improved robustness and generalization to unseen domains. Extensive experiments on benchmarks, such as PACS, VLCS, OfficeHome, and DomainNet, demonstrate that DSP-Reg outperforms state-of-the-art approaches, achieving an average accuracy of 66.7\% and surpassing all baselines.

</details>


### [64] [SEAFormer: A Spatial Proximity and Edge-Aware Transformer for Real-World Vehicle Routing Problems](https://arxiv.org/abs/2601.19395)
*Saeed Nasehi Basharzad,Farhana Choudhury,Egemen Tanin*

Main category: cs.LG

TL;DR: 本文提出了一种新的变换器SEAFormer，通过利用节点级和边级信息来解决实际车辆路径问题（RWVRPs），并展示了其在处理大规模实例时的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经方法在解决实际车辆路径问题（RWVRPs）上表现不佳，因为它们忽略了顺序依赖性，并且未能充分利用边级别的信息。

Method: 提出了SEAFormer，一种结合了节点级和边级信息的新变换器模型。SEAFormer通过两个关键创新实现了这一点：首先，使用基于局部感知聚类的集群邻近注意力（CPA）将注意力复杂度从O(n^2)降低到O(n)，同时保持全局视角；其次，轻量级边缘感知模块通过残差融合捕捉成对特征，从而有效整合基于边缘的信息并加速收敛。

Result: 广泛的实验表明，SEAFormer在四种不同规模的RWVRP变体上优于现有最先进方法的表现。特别地，SEAFormer是首个能够有效解决超过1,000个节点的RWVRPs的神经方法，同时也表现出色于经典VRPs。

Conclusion: SEAFormer为解决实际世界中的车辆路径问题提供了一个既适用于研究基准测试又适用于现实应用的多功能解决方案。

Abstract: Real-world Vehicle Routing Problems (RWVRPs) require solving complex, sequence-dependent challenges at scale with constraints such as delivery time window, replenishment or recharging stops, asymmetric travel cost, etc. While recent neural methods achieve strong results on large-scale classical VRP benchmarks, they struggle to address RWVRPs because their strategies overlook sequence dependencies and underutilize edge-level information, which are precisely the characteristics that define the complexity of RWVRPs. We present SEAFormer, a novel transformer that incorporates both node-level and edge-level information in decision-making through two key innovations. First, our Clustered Proximity Attention (CPA) exploits locality-aware clustering to reduce the complexity of attention from $O(n^2)$ to $O(n)$ while preserving global perspective, allowing SEAFormer to efficiently train on large instances. Second, our lightweight edge-aware module captures pairwise features through residual fusion, enabling effective incorporation of edge-based information and faster convergence. Extensive experiments across four RWVRP variants with various scales demonstrate that SEAFormer achieves superior results over state-of-the-art methods. Notably, SEAFormer is the first neural method to solve 1,000+ node RWVRPs effectively, while also achieving superior performance on classic VRPs, making it a versatile solution for both research benchmarks and real-world applications.

</details>


### [65] [OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation](https://arxiv.org/abs/2601.19439)
*Giuseppe Chiari,Michele Piccoli,Davide Zoni*

Main category: cs.LG

TL;DR: 提出了OSIRIS，一个用于模拟集成电路设计的可扩展数据集生成流程，以促进基于机器学习的研究，并附带了一个包含87,100种电路变体的数据集以及一种基于强化学习的设计优化基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动化模拟集成电路设计面临长期挑战，主要是因为物理布局、寄生效应和电路级性能之间的复杂相互依赖关系难以使用传统设计方法准确捕捉和优化。尽管机器学习在自动化特定阶段显示出潜力，但开发整合这些阶段并利用后布局、寄生感知性能反馈迭代改进布局的整体框架仍处于早期阶段。此外，针对模拟领域的高质量开放数据集有限，限制了基于ML技术的基准测试和泛化能力。

Method: 引入了OSIRIS，这是一个为模拟IC设计创建的可扩展数据集生成流程，能够系统地探索模拟电路设计空间，并提供全面的性能指标和元数据。同时，发布了一个由OSIRIS生成的包含87,100种电路变体的数据集，并提出了一种基于强化学习（RL）的方法作为基线，该方法利用OSIRIS进行模拟设计优化。

Result: 通过OSIRIS生成的数据集包含了大量电路变体及其性能指标，这有助于促进电子设计自动化(EDA)领域内基于机器学习的研究。此外，提出的基于RL的基线方法展示了如何有效利用所构建的数据集来进行模拟电路设计优化。

Conclusion: OSIRIS及随附的数据集为推动基于机器学习的模拟集成电路设计研究提供了重要资源，有助于克服当前存在的挑战，如复杂的设计约束和缺乏高质量训练数据等。

Abstract: The automation of analog integrated circuit (IC) design remains a longstanding challenge, primarily due to the intricate interdependencies among physical layout, parasitic effects, and circuit-level performance. These interactions impose complex constraints that are difficult to accurately capture and optimize using conventional design methodologies. Although recent advances in machine learning (ML) have shown promise in automating specific stages of the analog design flow, the development of holistic, end-to-end frameworks that integrate these stages and iteratively refine layouts using post-layout, parasitic-aware performance feedback is still in its early stages. Furthermore, progress in this direction is hindered by the limited availability of open, high-quality datasets tailored to the analog domain, restricting both the benchmarking and the generalizability of ML-based techniques. To address these limitations, we present OSIRIS, a scalable dataset generation pipeline for analog IC design. OSIRIS systematically explores the design space of analog circuits while producing comprehensive performance metrics and metadata, thereby enabling ML-driven research in electronic design automation (EDA). In addition, we release a dataset consisting of 87,100 circuit variations generated with OSIRIS, accompanied by a reinforcement learning (RL)-based baseline method that exploits OSIRIS for analog design optimization.

</details>


### [66] [APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition](https://arxiv.org/abs/2601.19452)
*Finn Rietz,Pedro Zuidberg dos Martires,Johannes Andreas Stork*

Main category: cs.LG

TL;DR: 提出了一种新的方法APC，通过自适应组合多个数据驱动的归一化流先验来加速强化学习，同时在面对稀疏、次优或不一致的演示时保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的将演示数据融入强化学习的方法往往假设演示是最佳且完全符合目标任务的，但在实践中，演示通常是稀疏的、次优的或与目标不完全对齐的，这可能会导致性能下降。

Method: 提出了Adaptive Policy Composition (APC)，这是一种分层模型，能够自适应地组合多个基于数据的归一化流（NF）先验。APC不是强制严格遵循这些先验，而是估计每个先验对于目标任务的相关性，并利用它们进行探索。此外，APC可以改进有用的先验或者在必要时避开不一致的先验以优化后续奖励。

Result: 在多种基准测试中，当演示与目标对齐时，APC能够加速学习过程；即使在严重不对齐的情况下也能保持鲁棒性；并且能够利用次优演示来促进探索，同时避免了因过于严格遵守次优演示而导致的性能下降。

Conclusion: APC提供了一种有效的方法，能够在存在稀疏、次优或不一致演示的情况下增强强化学习算法的学习效率和鲁棒性。

Abstract: Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior's applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.

</details>


### [67] [LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment](https://arxiv.org/abs/2601.19487)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Wenhai Wang*

Main category: cs.LG

TL;DR: 提出了一种新的方法LLM-VA，通过调整向量方向而非幅度来改善语言模型的安全性对齐问题，同时减少越狱和过度拒绝现象。该方法无需微调或架构更改，在12个语言模型上的实验表明其在保持高实用性的同时显著提高了安全性。


<details>
  <summary>Details</summary>
Motivation: 当前安全对齐的语言模型存在越狱（响应有害输入）和过度拒绝（拒绝良性查询）的问题。现有的矢量引导方法通过调整答案向量的大小来解决这个问题，但这种方法造成了一个根本性的权衡——减少越狱会增加过度拒绝，反之亦然。

Method: 提出了LLM-VA方法，它通过封闭形式的权重更新使回答意愿($v_a$)与输入安全性判断($v_b$)的方向一致，从而使模型的回答意愿依赖于其安全评估过程。此方法使用SVM识别每层中的相关向量，选择与安全相关的层，并通过最小范数权重修改迭代地对齐向量。整个过程中不需要进行微调或改变模型结构。

Result: 在12个大型语言模型上进行的实验显示，相比最佳基线方法，LLM-VA能够实现高出11.45%的F1分数，同时保留了95.92%的功能性。此外，该方法能够自动适应每个模型的安全偏置而无需手动调整。

Conclusion: LLM-VA提供了一个有效的方法来改进语言模型的安全性对齐问题，能够在不牺牲太多功能性的情况下提高模型处理潜在有害输入的能力。

Abstract: Safety-aligned LLMs suffer from two failure modes: jailbreak (answering harmful inputs) and over-refusal (declining benign queries). Existing vector steering methods adjust the magnitude of answer vectors, but this creates a fundamental trade-off -- reducing jailbreak increases over-refusal and vice versa. We identify the root cause: LLMs encode the decision to answer (answer vector $v_a$) and the judgment of input safety (benign vector $v_b$) as nearly orthogonal directions, treating them as independent processes. We propose LLM-VA, which aligns $v_a$ with $v_b$ through closed-form weight updates, making the model's willingness to answer causally dependent on its safety assessment -- without fine-tuning or architectural changes. Our method identifies vectors at each layer using SVMs, selects safety-relevant layers, and iteratively aligns vectors via minimum-norm weight modifications. Experiments on 12 LLMs demonstrate that LLM-VA achieves 11.45% higher F1 than the best baseline while preserving 95.92% utility, and automatically adapts to each model's safety bias without manual tuning. Code and models are available at https://hotbento.github.io/LLM-VA-Web/.

</details>


### [68] [GenCP: Towards Generative Modeling Paradigm of Coupled Physics](https://arxiv.org/abs/2601.19541)
*Tianrun Gao,Haoren Zheng,Wenhao Deng,Haodong Feng,Tao Zhang,Ruiqi Feng,Qianyi Chen,Tailin Wu*

Main category: cs.LG

TL;DR: 提出了一种新的生成范式GenCP，用于处理多物理场耦合仿真问题，通过将多物理场建模视为概率建模问题，并结合概率密度演化和迭代多物理场耦合，使得模型能够从解耦的数据中学习并在采样时推断出耦合的物理现象。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物理系统通常涉及多种物理现象的耦合，这使得它们的模拟既非常重要又极具挑战性。现有的主流方法在处理解耦数据时面临困难，并且在强耦合时空物理系统中效率低、保真度差。

Method: GenCP通过将耦合物理建模转化为概率建模问题来工作，其中的关键创新是将概率密度演化的生成建模与迭代多物理场耦合相结合，从而能够在来自解耦仿真的数据上进行训练，并在采样过程中推断出耦合的物理现象。此外，利用算子分裂理论在概率演化空间中为这种'条件到联合'采样方案建立误差可控性保证。

Result: 在合成设置及三个具有挑战性的多物理场景中评估了该范式，展示了GenCP在原理洞察力和应用性能上的优越表现。

Conclusion: GenCP作为一种新颖而优雅的生成范式，为解决复杂耦合多物理场系统的模拟提供了一个有效的方法，并在多个实验场景中展示了其出色的应用潜力。

Abstract: Real-world physical systems are inherently complex, often involving the coupling of multiple physics, making their simulation both highly valuable and challenging. Many mainstream approaches face challenges when dealing with decoupled data. Besides, they also suffer from low efficiency and fidelity in strongly coupled spatio-temporal physical systems. Here we propose GenCP, a novel and elegant generative paradigm for coupled multiphysics simulation. By formulating coupled-physics modeling as a probability modeling problem, our key innovation is to integrate probability density evolution in generative modeling with iterative multiphysics coupling, thereby enabling training on data from decoupled simulation and inferring coupled physics during sampling. We also utilize operator-splitting theory in the space of probability evolution to establish error controllability guarantees for this "conditional-to-joint" sampling scheme. We evaluate our paradigm on a synthetic setting and three challenging multi-physics scenarios to demonstrate both principled insight and superior application performance of GenCP. Code is available at this repo: github.com/AI4Science-WestlakeU/GenCP.

</details>


### [69] [Scale-Consistent State-Space Dynamics via Fractal of Stationary Transformations](https://arxiv.org/abs/2601.19551)
*Geunhyeok Yu,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FROST的新方法，它通过分形归纳偏置强制执行自相似表示流形，从而在深度学习模型中保证了状态空间模型跨迭代细化的尺度一致性潜在动力学。基于这种几何结构，中间状态对应于共享表示的不同分辨率，并且自然地支持一种基于内在特征质量而非外在目标的排序式终止机制。ImageNet-100上的控制实验验证了预测的尺度一致行为，表明自适应效率源于对齐的潜在几何结构。


<details>
  <summary>Details</summary>
Motivation: 最近的深度学习模型越来越依赖于深度，但缺乏对中间表征有效性的结构保证，这使得早期停止和自适应计算变得不合适。为了解决这一局限性，研究者们提出了一个对于状态空间模型在迭代细化过程中尺度一致的潜在动力学的结构要求。

Method: 研究者们开发出了Fractal of Stationary Transformations (FROST)，该方法通过对分形归纳偏置的应用来强制实现一个自相似的表示流形。根据这种几何结构，不同迭代过程中的中间状态被解释为同一共享表示的不同分辨率版本。

Result: 在ImageNet-100数据集上进行的受控实验证明了所提出的FROST方法确实能够表现出预期的尺度一致性行为。这意味着从对齐的潜在几何结构中可以产生出自适应效率。

Conclusion: 通过引入FROST方法，研究成功地在深度学习模型中实现了状态空间模型跨迭代细化时的尺度一致性潜在动力学。这种方法不仅提供了关于中间表示的有效性保障，还促进了基于内在特征质量的自然终止机制的发展。

Abstract: Recent deep learning models increasingly rely on depth without structural guarantees on the validity of intermediate representations, rendering early stopping and adaptive computation ill-posed. We address this limitation by formulating a structural requirement for state-space model's scale-consistent latent dynamics across iterative refinement, and derive Fractal of Stationary Transformations (FROST), which enforces a self-similar representation manifold through a fractal inductive bias. Under this geometry, intermediate states correspond to different resolutions of a shared representation, and we provide a geometric analysis establishing contraction and stable convergence across iterations. As a consequence of this scale-consistent structure, halting naturally admits a ranking-based formulation driven by intrinsic feature quality rather than extrinsic objectives. Controlled experiments on ImageNet-100 empirically verify the predicted scale-consistent behavior, showing that adaptive efficiency emerges from the aligned latent geometry.

</details>


### [70] [AROMMA: Unifying Olfactory Embeddings for Single Molecules and Mixtures](https://arxiv.org/abs/2601.19561)
*Dayoung Kang,JongWon Kim,Jiho Park,Keonseock Lee,Ji-Woong Choi,Jinhyun So*

Main category: cs.LG

TL;DR: 本文提出了一种名为AROMMA的框架，该框架可以学习单分子和双分子混合物的统一嵌入空间。通过化学基础模型编码每个分子，并使用基于注意力的聚合器组合混合物，确保排列不变性和不对称分子相互作用。此外，通过知识蒸馏和类别感知伪标记来对齐气味描述集以丰富缺失的混合注释。AROMMA在单分子和分子对数据集中都达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前公开的嗅觉数据集较小且分散于单一分子与混合物之间，这限制了可泛化气味表示的学习。最近的工作要么学习单分子嵌入，要么通过相似性或成对标签预测来处理混合物，导致表示分离且未对齐。

Method: 提出了AROMMA框架，它能够为单个分子和两分子混合物学习一个统一的嵌入空间。每种分子由化学基础模型编码，而混合物则通过基于注意力机制的聚合器进行组合，从而保证排列不变性以及非对称分子间交互。同时，利用知识蒸馏和类别敏感型伪标签技术对气味描述集合进行对齐，以补充缺少的混合物标注信息。

Result: AROMMA在单分子及分子对数据集上均取得了业界领先的性能表现，相较于现有方法，在AUROC指标上最高提升了19.1%。

Conclusion: AROMMA提供了一个强大的框架，能够在两个领域内实现稳健的一般化，对于单分子及其混合物的数据集来说，这是一种非常有效的解决方案。

Abstract: Public olfaction datasets are small and fragmented across single molecules and mixtures, limiting learning of generalizable odor representations. Recent works either learn single-molecule embeddings or address mixtures via similarity or pairwise label prediction, leaving representations separate and unaligned. In this work, we propose AROMMA, a framework that learns a unified embedding space for single molecules and two-molecule mixtures. Each molecule is encoded by a chemical foundation model and the mixtures are composed by an attention-based aggregator, ensuring both permutation invariance and asymmetric molecular interactions. We further align odor descriptor sets using knowledge distillation and class-aware pseudo-labeling to enrich missing mixture annotations. AROMMA achieves state-of-the-art performance in both single-molecule and molecule-pair datasets, with up to 19.1% AUROC improvement, demonstrating a robust generalization in two domains.

</details>


### [71] [From Atoms to Chains: Divergence-Guided Reasoning Curriculum for Unlabeled LLM Domain Adaptation](https://arxiv.org/abs/2601.19588)
*Yongqi Wang,Xiaofeng Ji,Jie Wang,Qingbin Li,Xiao Xiong,Zheming Yang,Jian Xu,Minghui Qiu,Xinxiao Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于分歧引导的推理课程（DGRC）方法，通过利用大型语言模型在原子子问题上的高保真度来解决专业领域适应性的问题。当学生和教师产生冲突结果时，该方法会指导教师进行诊断分析，形成针对分歧点的原子查询，并自我回答这些查询以创建高置信度的原子问答对，从而为学生提供修正知识缺口的原子课程以及作为事实标准过滤教师原始推理链，生成一个验证过的CoT课程。实验表明，在医学和法律领域的不同规模的学生模型上，此方法均有效，特别是对于医学领域1.5B规模的学生模型，相对于强大的无标签基线实现了7.76%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 目前，将大型语言模型（LLMs）适配到特定领域而不使用人工标注数据是一项重要但具有挑战性的任务。广泛采用的知识蒸馏方法往往导致粗粒度模仿，学生模型无法高效地针对自身弱点，并可能继承教师模型的推理缺陷。这引发了一个关键的教学困境：如何在教师本身并非绝对专家的情况下设计出可靠的课程体系。

Method: 提出了Divergence-Guided Reasoning Curriculum (DGRC) 方法，它能够从原子级知识到推理链条构建学习路径。当学生与教师之间的推理路径存在差异时，DGRC 促使教师执行诊断分析，识别并聚焦于具体的分歧点，进而生成针对这些分歧点的原子级问题-答案对。这些问答对不仅用于填补学生的知识空白，还被用来筛选出经过验证的推理链，教会学生如何将分散的知识整合成完整的推理过程。

Result: 实验结果表明，DGRC框架在医学和法律等多个专业领域内对学生模型的有效性得到了验证。特别是在医疗领域中，对于1.5B参数规模的学生模型而言，相较于强无标签基线，本方法达到了7.76%的相对提升。

Conclusion: 通过引入一种新的基于分歧点引导的推理训练策略——DGRC，这项研究成功解决了没有完美专家情况下自动生成高质量教学材料的问题。该方法不仅提高了学生模型处理复杂推理任务的能力，而且在多个实际应用领域展示了其优越性。

Abstract: Adapting Large Language Models (LLMs) to specialized domains without human-annotated data is a crucial yet formidable challenge. Widely adopted knowledge distillation methods often devolve into coarse-grained mimicry, where the student model inefficiently targets its own weaknesses and risks inheriting the teacher's reasoning flaws. This exposes a critical pedagogical dilemma: how to devise a reliable curriculum when the teacher itself is not an infallible expert. Our work resolves this by capitalizing on a key insight: while LLMs may exhibit fallibility in complex, holistic reasoning, they often exhibit high fidelity on focused, atomic sub-problems. Based on this, we propose Divergence-Guided Reasoning Curriculum (DGRC), which constructs a learning path from atomic knowledge to reasoning chains by dynamically deriving two complementary curricula from disagreements in reasoning pathways. When a student and teacher produce conflicting results, DGRC directs the teacher to perform a diagnostic analysis: it analyzes both reasoning paths to formulate atomic queries that target the specific points of divergence, and then self-answers these queries to create high-confidence atomic question-answer pairs. These pairs then serve a dual purpose: (1) providing an atomic curriculum to rectify the student's knowledge gaps, and (2) serving as factual criteria to filter the teacher's original reasoning chains, yielding a verified CoT curriculum that teaches the student how to integrate atomic knowledge into complete reasoning paths. Experiments across the medical and legal domains on student models of various sizes demonstrate the effectiveness of our DGRC framework. Notably, our method achieves a 7.76% relative improvement for the 1.5B student model in the medical domain over strong unlabeled baseline.

</details>


### [72] [Intersectional Fairness via Mixed-Integer Optimization](https://arxiv.org/abs/2601.19595)
*Jiří Němeček,Mark Kozdoba,Illia Kryvoviaz,Tomáš Pevný,Jakub Mareček*

Main category: cs.LG

TL;DR: 本文提出了一种利用混合整数优化（MIO）训练交集公平且内在可解释分类器的统一框架，旨在解决高风险领域中AI模型的公平性和透明性问题。


<details>
  <summary>Details</summary>
Motivation: 在金融和医疗等高风险领域部署人工智能时，需要确保模型既公平又透明。尽管包括欧盟AI法案在内的监管框架要求减少偏见，但对于偏见的定义却故意模糊。研究认为真正的公平需要解决受保护群体交叉点上的偏见问题。

Method: 提出一个基于混合整数优化（MIO）的框架来训练具有交集公平性和内在可解释性的分类器。证明了两种衡量交集公平性的指标（MSD和SPSF）在识别最不公平子群体方面是等价的，并通过实证展示其算法在发现偏见方面的性能提升。

Result: 开发出高性能且易于理解的分类器，能够将交集偏见控制在一个可接受范围内，为受监管行业及其他领域提供了一个强有力的解决方案。

Conclusion: 该研究为实现AI系统的交集公平性提供了有效方法，同时保证了模型的透明度与解释能力，满足了高风险应用领域对AI模型的基本要求。

Abstract: The deployment of Artificial Intelligence in high-risk domains, such as finance and healthcare, necessitates models that are both fair and transparent. While regulatory frameworks, including the EU's AI Act, mandate bias mitigation, they are deliberately vague about the definition of bias. In line with existing research, we argue that true fairness requires addressing bias at the intersections of protected groups. We propose a unified framework that leverages Mixed-Integer Optimization (MIO) to train intersectionally fair and intrinsically interpretable classifiers. We prove the equivalence of two measures of intersectional fairness (MSD and SPSF) in detecting the most unfair subgroup and empirically demonstrate that our MIO-based algorithm improves performance in finding bias. We train high-performing, interpretable classifiers that bound intersectional bias below an acceptable threshold, offering a robust solution for regulated industries and beyond.

</details>


### [73] [The Geometric Mechanics of Contrastive Representation Learning: Alignment Potentials, Entropic Dispersion, and Cross-Modal Divergence](https://arxiv.org/abs/2601.19597)
*Yichao Cai,Zhen Zhang,Yuhang Liu,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 本文提出了一种基于测度论的框架，用以表征对比学习中InfoNCE目标函数在固定嵌入流形上的几何机制。通过建立大批量限制下的值和梯度一致性，揭示了单峰与多峰状态之间基本的几何分岔现象，并指出在多峰设置下存在一个持续的负对称散度项，该现象促进了屏障驱动的协同适应，从而作为结构几何必要性而非初始化人工产物强制执行群体级别的模态差异。


<details>
  <summary>Details</summary>
Motivation: 虽然InfoNCE推动了现代对比学习的发展，但其背后的几何机制除了经典的对齐-均匀性分解之外仍不明确。本文旨在更深入地理解这些机制，特别是它们如何影响表示学习过程中的分布特性。

Method: 研究者们开发了一个新的理论框架，使用测度论来建模学习过程，即将表示视为固定嵌入流形上的测度演化。通过对大批量极限下的值和梯度一致性的探讨，将随机目标函数与显式的确定性能量景观联系起来，进而探索不同模式（单峰与多峰）下固有的景观特性。

Result: 研究表明，在单峰情况下，内在景观是严格凸的且具有唯一的吉布斯平衡点；而在多峰设定中，即使经过核锐化处理后，依然存在一个持久的负对称散度项，这导致了由障碍驱动的共适应现象，形成了群体层面的模态间隙。

Conclusion: 这项工作提供了一种从点对点区分到群体几何的新分析视角，为诊断和控制分布错位提供了理论基础。它强调了在多峰场景下，保持一定模态差距的重要性，并非仅仅是初始化的结果而是由于底层几何结构所决定的。

Abstract: While InfoNCE powers modern contrastive learning, its geometric mechanisms remain under-characterized beyond the canonical alignment--uniformity decomposition. We present a measure-theoretic framework that models learning as the evolution of representation measures on a fixed embedding manifold. By establishing value and gradient consistency in the large-batch limit, we bridge the stochastic objective to explicit deterministic energy landscapes, uncovering a fundamental geometric bifurcation between the unimodal and multimodal regimes. In the unimodal setting, the intrinsic landscape is strictly convex with a unique Gibbs equilibrium; here, entropy acts merely as a tie-breaker, clarifying "uniformity" as a constrained expansion within the alignment basin. In contrast, the symmetric multimodal objective contains a persistent negative symmetric divergence term that remains even after kernel sharpening. We show that this term induces barrier-driven co-adaptation, enforcing a population-level modality gap as a structural geometric necessity rather than an initialization artifact. Our results shift the analytical lens from pointwise discrimination to population geometry, offering a principled basis for diagnosing and controlling distributional misalignment.

</details>


### [74] [Explicit Multi-head Attention for Inter-head Interaction in Large Language Models](https://arxiv.org/abs/2601.19611)
*Runyu Peng,Yunhua Zhou,Demin Song,Kai Lv,Bo Wang,Qipeng Guo,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出了一种名为多头显式注意力（MEA）的新方法，通过在注意力机制中引入头级别的线性组合模块和组归一化层来增强跨头交互。这种方法不仅提高了模型的鲁棒性和训练效率，还通过减少实际注意力头的数量并使用低秩“虚拟头”重构它们，实现了一种有效的键值缓存压缩策略，在不影响性能的情况下显著减少了内存占用。


<details>
  <summary>Details</summary>
Motivation: 受到近期研究显示跨头交互能够提高注意力表现的启发，本文旨在设计一种简单有效的方法来显式地建模这种交互，以期进一步提升基于Transformer架构的大规模语言模型的性能。

Method: 提出了多头显式注意力(MEA)机制，它包括两个主要部分：头级别线性组合(HLC)模块，该模块对不同头之间的键和值向量应用可学习的线性组合；以及一个用于调整重组后头部统计属性的头级别组归一化层。此外，通过降低注意力头的实际数量并利用HLC创建低秩'虚拟头'，实现了参数效率的探索及KV-cache的有效压缩。

Result: MEA展示了强大的预训练鲁棒性，允许使用更大的学习率从而加快收敛速度，并最终降低了验证损失同时提升了多个任务上的表现。对于知识密集型和科学推理任务而言，即使将KV-cache内存使用量减少了50%，其性能损失也几乎可以忽略不计；而对于奥林匹克水平的数学基准测试来说，准确率仅下降了3.59%。

Conclusion: 本研究表明，通过显式建模跨头交互作用，可以有效改进大型语言模型的注意机制，不仅提高了训练效率和模型性能，而且为减少内存消耗提供了新的途径。

Abstract: In large language models built upon the Transformer architecture, recent studies have shown that inter-head interaction can enhance attention performance. Motivated by this, we propose Multi-head Explicit Attention (MEA), a simple yet effective attention variant that explicitly models cross-head interaction. MEA consists of two key components: a Head-level Linear Composition (HLC) module that separately applies learnable linear combinations to the key and value vectors across heads, thereby enabling rich inter-head communication; and a head-level Group Normalization layer that aligns the statistical properties of the recombined heads. MEA shows strong robustness in pretraining, which allows the use of larger learning rates that lead to faster convergence, ultimately resulting in lower validation loss and improved performance across a range of tasks. Furthermore, we explore the parameter efficiency of MEA by reducing the number of attention heads and leveraging HLC to reconstruct them using low-rank "virtual heads". This enables a practical key-value cache compression strategy that reduces KV-cache memory usage by 50% with negligible performance loss on knowledge-intensive and scientific reasoning tasks, and only a 3.59% accuracy drop for Olympiad-level mathematical benchmarks.

</details>


### [75] [R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning](https://arxiv.org/abs/2601.19620)
*Zhizheng Jiang,Kang Zhao,Weikai Xu,Xinkui Lin,Wei Liu,Jian Luan,Shuo Shang,Peng Han*

Main category: cs.LG

TL;DR: 提出了一种名为R^3的强化学习机制，通过跨上下文重放、上下文自我反思和结构熵排名奖励三种方式来解决大型推理模型在复杂任务下的训练脆弱性和低效率问题。实验表明该方法在数学基准上达到了最先进的性能，并且比基础模型使用更少的推理令牌。


<details>
  <summary>Details</summary>
Motivation: 现有的基于组策略优化的方法虽然可以实现稳定的优势估计，但依赖于同一批次内高质量样本带来的优势差异，这导致在处理具有挑战性的任务时组内优势崩溃，使得训练过程变得不稳定且效率低下。

Method: 研究者提出了一个名为R^3的强化学习机制，它包含三个主要组成部分：（1）跨上下文重放策略，通过回忆同一查询历史轨迹中的有价值示例保持组内优势；（2）上下文自我反思机制，让模型能够利用过去的失败经验改进输出结果；（3）结构熵排名奖励，通过对响应进行基于令牌级别熵模式的排名为截断或失败样本分配相对奖励，同时捕捉局部探索与全局稳定性。

Result: 本研究将所提方法应用于Deepseek-R1-Distill-Qwen-1.5B，并在DeepscaleR-40k数据集上的数学领域进行了训练。实验结果显示，该方法在多个数学基准测试中取得了最先进水平的表现，并且相较于基础模型减少了推理令牌的数量。

Conclusion: R^3机制通过其独特的设计有效地解决了LRMs在执行复杂任务时遇到的训练脆弱性和效率问题，特别是在数学领域的应用展示了显著优于现有技术水平的结果。

Abstract: Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.

</details>


### [76] [Cross-Domain Offshore Wind Power Forecasting: Transfer Learning Through Meteorological Clusters](https://arxiv.org/abs/2601.19674)
*Dominic Weisser,Chloé Hashimoto-Cullen,Benjamin Guedj*

Main category: cs.LG

TL;DR: 提出了一种新的迁移学习框架，用于解决新海上风电场由于缺乏特定地点数据而难以进行准确电力预测的问题。该框架通过根据气象特征聚类电力输出，并使用专门针对不同天气模式训练的专家模型集合来进行预测。这种方法不需要一年的本地测量数据即可实现准确的跨领域预测，有助于加速项目开发并降低风险。


<details>
  <summary>Details</summary>
Motivation: 新建设的海上风电场需要从一开始就提供准确的电力预测以确保电网稳定、良好的储备管理和有效的能源交易。然而，现有的机器学习模型通常需要大量的特定地点数据才能表现出色，这对于新建风电场来说是一个挑战，因为它们还没有积累足够的数据。为了解决这一问题，提出了一个新颖的迁移学习框架来减少对大量现场数据的需求。

Method: 本研究提出了一种基于迁移学习的新框架，它首先根据协变量气象特征将电力输出进行聚类；然后，不是训练一个通用模型，而是采用一组专家模型进行预测，每个模型都针对某一特定集群进行了训练。这些预先训练好的模型各自专注于不同的天气模式，因此能够高效地适应新站点，并捕捉到可转移的、依赖于气候的动力学特性。

Result: 通过对八个海上风电场的全面评估，证明了即使在只有不到五个月的特定地点数据的情况下，也能实现准确的跨领域预测。实验结果表明平均绝对误差（MAE）达到了3.52%，这为可靠的预测并不总是需要完整的一年周期提供了实证支持。

Conclusion: 这项工作不仅介绍了一种新的、气候感知型的迁移学习方法来改善海上风电场的电力预测准确性，还展示了如何通过减少数据需求来加速项目开发并有效管理相关风险。此外，该方法也为早期风资源评估等其他海上风电应用开辟了新的可能性。

Abstract: Ambitious decarbonisation targets are catalysing growth in orders of new offshore wind farms. For these newly commissioned plants to run, accurate power forecasts are needed from the onset. These allow grid stability, good reserve management and efficient energy trading. Despite machine learning models having strong performances, they tend to require large volumes of site-specific data that new farms do not yet have. To overcome this data scarcity, we propose a novel transfer learning framework that clusters power output according to covariate meteorological features. Rather than training a single, general-purpose model, we thus forecast with an ensemble of expert models, each trained on a cluster. As these pre-trained models each specialise in a distinct weather pattern, they adapt efficiently to new sites and capture transferable, climate-dependent dynamics. Through the expert models' built-in calibration to seasonal and meteorological variability, we remove the industry-standard requirement of local measurements over a year. Our contributions are two-fold - we propose this novel framework and comprehensively evaluate it on eight offshore wind farms, achieving accurate cross-domain forecasting with under five months of site-specific data. Our experiments achieve a MAE of 3.52\%, providing empirical verification that reliable forecasts do not require a full annual cycle. Beyond power forecasting, this climate-aware transfer learning method opens new opportunities for offshore wind applications such as early-stage wind resource assessment, where reducing data requirements can significantly accelerate project development whilst effectively mitigating its inherent risks.

</details>


### [77] [LoPRo: Enhancing Low-Rank Quantization via Permuted Block-Wise Rotation](https://arxiv.org/abs/2601.19675)
*Hongyaoxing Gu,Lijuan Hu,Liye Yu,Haowei Li,Fangfang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种无需微调的后训练量化算法LoPRo，通过块级置换和Walsh-Hadamard变换提高残差矩阵量化质量，并引入基于秩-1草图的混合精度快速低秩分解进一步减少量化成本。实验表明，LoPRo在2比特和3比特量化下优于现有的无需微调PTQ方法，特别是在LLaMA系列模型上达到了最先进的量化精度，同时显著提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前的权重仅后训练量化（PTQ）方法主要集中在具有挑战性的次3比特范围内，在此范围内通常需要微调才能达到竞争性性能。本研究旨在解决低秩近似下量化残差矩阵的难题，开发一种不需微调且能保持高准确率的PTQ方法。

Method: 提出了名为LoPRo的新颖PTQ算法，该算法通过应用块级置换及Walsh-Hadamard转换来旋转相似重要性的列，同时显式地保留最显著列块的量化准确性。此外，还介绍了一种基于秩-1草图(R1SVD)的混合精度快速低秩分解技术以进一步降低量化成本。

Result: 实验显示，LoPRo在2比特与3比特量化水平上超越了现有的无须微调PTQ方法，特别是在LLaMA-2和LLaMA-3系列模型中达到了顶尖的量化精度，同时也实现了高达4倍的速度提升。对于MoE模型Mixtral-8x7B，LoPRo能够在2.5小时内完成量化，同时减少了0.4的困惑度并提高了8%的准确性。

Conclusion: LoPRo是一种有效的无须微调PTQ解决方案，它不仅在多个基准测试中表现出色，而且还能显著提高推理效率，显示出在实际应用中的巨大潜力。

Abstract: Post-training quantization (PTQ) enables effective model compression while preserving relatively high accuracy. Current weight-only PTQ methods primarily focus on the challenging sub-3-bit regime, where approaches often suffer significant accuracy degradation, typically requiring fine-tuning to achieve competitive performance. In this work, we revisit the fundamental characteristics of weight quantization and analyze the challenges in quantizing the residual matrix under low-rank approximation. We propose LoPRo, a novel fine-tuning-free PTQ algorithm that enhances residual matrix quantization by applying block-wise permutation and Walsh-Hadamard transformations to rotate columns of similar importance, while explicitly preserving the quantization accuracy of the most salient column blocks. Furthermore, we introduce a mixed-precision fast low-rank decomposition based on rank-1 sketch (R1SVD) to further minimize quantization costs. Experiments demonstrate that LoPRo outperforms existing fine-tuning-free PTQ methods at both 2-bit and 3-bit quantization, achieving accuracy comparable to fine-tuning baselines. Specifically, LoPRo achieves state-of-the-art quantization accuracy on LLaMA-2 and LLaMA-3 series models while delivering up to a 4$\times$ speedup. In the MoE model Mixtral-8x7B, LoPRo completes quantization within 2.5 hours, simultaneously reducing perplexity by 0.4$\downarrow$ and improving accuracy by 8\%$\uparrow$. Moreover, compared to other low-rank quantization methods, LoPRo achieves superior accuracy with a significantly lower rank, while maintaining high inference efficiency and minimal additional latency.

</details>


### [78] [Out-of-Distribution Generalization via Invariant Trajectories for Multimodal Large Language Model Editing](https://arxiv.org/abs/2601.19700)
*Jiajie Su,Haoyuan Wang,Xiaohua Feng,Yunshan Ma,Xiaobo Xia,Yuyuan Li,Xiaolin Zheng,Jianmao Xiao,Chaochao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为ODEdit的框架，用于解决多模态大语言模型(MLLM)的知识编辑问题。通过将MLLM编辑重新定义为一个分布外(OOD)泛化问题，该方法旨在识别不变的因果轨迹，同时抑制虚假关联，从而在多种跨模odal提示中实现鲁棒编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的单模态LLM编辑方法依赖于刚性的参数到输出映射，在处理多模态LLLM时会导致因果欠拟合和过拟合的问题。因此，需要一种新的方法来准确区分语义变化与事实变化，并确保在不同形式的跨模态提示下都能稳健地进行知识编辑。

Method: 提出了ODEdit框架，该框架基于即插即用的不变性学习，优化了三部分OOD风险目标，以同时提高编辑的可靠性、局部性和通用性。此外，还引入了一种编辑轨迹不变性学习方法，通过在风险最小化目标中加入总变异惩罚来稳定环境变化下的编辑轨迹。

Result: 理论分析和广泛的实验表明，ODEdit能够有效地解决多模态大语言模型中的知识编辑挑战，提高了编辑过程中的可靠性和适应性。

Conclusion: 通过将多模态大语言模型的知识编辑视作一个OOD泛化问题并采用ODEdit框架，可以更准确地识别不变因果路径、减少假相关性的影响，进而实现在多样化跨模态情境下的鲁棒知识编辑。

Abstract: Knowledge editing emerges as a crucial technique for efficiently correcting incorrect or outdated knowledge in large language models (LLM). Existing editing methods for unimodal LLM rely on a rigid parameter-to-output mapping, which causes causal-underfit and causal-overfit in cascaded reasoning for Multimodal LLM (MLLM). In this paper, we reformulate MLLM editing as an out-of-distribution (OOD) generalization problem, where the goal is to discern semantic shift with factual shift and thus achieve robust editing among diverse cross-modal prompting. The key challenge of this OOD problem lies in identifying invariant causal trajectories that generalize accurately while suppressing spurious correlations. To address it, we propose ODEdit, a plug-and-play invariant learning based framework that optimizes the tripartite OOD risk objective to simultaneously enhance editing reliability, locality, and generality.We further introduce an edit trajectory invariant learning method, which integrates a total variation penalty into the risk minimization objective to stabilize edit trajectories against environmental variations. Theoretical analysis and extensive experiments demonstrate the effectiveness of ODEdit.

</details>


### [79] [Rethinking Divisive Hierarchical Clustering from a Distributional Perspective](https://arxiv.org/abs/2601.19718)
*Kaifeng Zhang,Kai Ming Ting,Tianrun Liang,Qiuran Zhao*

Main category: cs.LG

TL;DR: 本文揭示了当前基于目标的分裂层次聚类方法存在的问题，并提出了一种新的分布导向的目标函数，以最大化所有簇的总相似度(TSC)。通过理论分析和实证评估，证明了所提方法在人工数据集和空间转录组学数据集上的有效性，特别是在生物区域一致性方面优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于目标的分裂层次聚类（DHC）方法生成的树状图缺乏三个期望属性：无过度分割、将相似集群归为同一子集、与真实情况相对应。这些问题的根本原因在于使用了集合导向的二分评估标准。

Method: 作者提出采用分布核来替代原有的集合导向标准，从而解决上述缺陷。新方法旨在实现一个分布导向的新目标——最大化所有集群的总相似性（TSC）。理论分析表明，由此产生的树状图能够保证TSC的一个下界。

Result: 实验评价显示，在人工数据集以及空间转录组学（生物信息学领域）的数据集中，所提出的方法是有效的。特别是，该方法成功创建了一个与空间转录组学数据集中的生物区域相一致的树状图，而其他竞争者则未能做到这一点。

Conclusion: 通过引入分布核作为评估标准，代替传统的集合导向方法，可以有效改善分裂层次聚类的结果，使其更符合实际需求。此外，这种方法在处理特定类型的数据如空间转录组学数据时表现出色。

Abstract: We uncover that current objective-based Divisive Hierarchical Clustering (DHC) methods produce a dendrogram that does not have three desired properties i.e., no unwarranted splitting, group similar clusters into a same subset, ground-truth correspondence. This shortcoming has their root cause in using a set-oriented bisecting assessment criterion. We show that this shortcoming can be addressed by using a distributional kernel, instead of the set-oriented criterion; and the resultant clusters achieve a new distribution-oriented objective to maximize the total similarity of all clusters (TSC). Our theoretical analysis shows that the resultant dendrogram guarantees a lower bound of TSC. The empirical evaluation shows the effectiveness of our proposed method on artificial and Spatial Transcriptomics (bioinformatics) datasets. Our proposed method successfully creates a dendrogram that is consistent with the biological regions in a Spatial Transcriptomics dataset, whereas other contenders fail.

</details>


### [80] [Stability and Generalization of Nonconvex Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2601.19730)
*Hongxu Chen,Ke Wei,Xiaoming Yuan,Luo Luo*

Main category: cs.LG

TL;DR: 本文开发了一个通用框架，用于在重尾噪声下建立泛化界，并对几种流行的随机算法进行了稳定性与泛化分析。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数研究集中在优化误差的收敛性上，而对重尾梯度噪声下的泛化界分析仍然有限。因此，有必要为这种类型的噪声开发新的泛化界理论。

Method: 通过引入截断参数，在假设中心矩有界的条件下基于算法稳定性实现了泛化误差界。然后，利用这个框架进一步提供了几种流行随机算法（如裁剪和归一化的随机梯度下降及其小批量和动量变体）在重尾噪声下的稳定性和泛化分析。

Result: 成功构建了针对重尾噪声情况下的泛化界理论，并对多种特定算法进行了具体的稳定性及泛化性能分析。

Conclusion: 本研究表明，在重尾梯度噪声的情况下，可以使用所提出的框架来有效地分析和理解机器学习模型训练过程中不同随机优化算法的泛化能力。

Abstract: The empirical evidence indicates that stochastic optimization with heavy-tailed gradient noise is more appropriate to characterize the training of machine learning models than that with standard bounded gradient variance noise. Most existing works on this phenomenon focus on the convergence of optimization errors, while the analysis for generalization bounds under the heavy-tailed gradient noise remains limited. In this paper, we develop a general framework for establishing generalization bounds under heavy-tailed noise. Specifically, we introduce a truncation argument to achieve the generalization error bound based on the algorithmic stability under the assumption of bounded $p$th centered moment with $p\in(1,2]$. Building on this framework, we further provide the stability and generalization analysis for several popular stochastic algorithms under heavy-tailed noise, including clipped and normalized stochastic gradient descent, as well as their mini-batch and momentum variants.

</details>


### [81] [A Multi-directional Meta-Learning Framework for Class-Generalizable Anomaly Detection](https://arxiv.org/abs/2601.19833)
*Padmaksha Roy,Lamine Mili,Almuatazbellah Boker*

Main category: cs.LG

TL;DR: 本文提出了一种多方向元学习算法，旨在通过主要学习可用的正常数据和少量异常数据来开发一个统一的模型，以检测完全未见过的异常（OOD类）。该算法在内部级别学习正常数据的流形，在外部级别利用少量异常样本来最大化正常样本和异常样本之间的softmax置信度边界，从而实现对未见异常类更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决类别可泛化的异常检测问题，即在仅有少量标记的异常数据情况下，开发能够识别完全未见过的异常（OOD类）的统一模型。

Method: 提出了一种多方向元学习算法，该算法分为两个层次：内层专注于学习正常数据的表示；外层则通过少数异常样本调整模型，以优化正常与异常样本间的决策边界，通过多次迭代训练，增强模型对未知异常类别的泛化能力。

Result: 通过这种两层优化加上多方向训练的方法，实现了对未见异常类更强大的泛化性能。

Conclusion: 所提出的多方向元学习框架能够有效地从有限的异常样本中学习，并且对于检测全新的、未见过的异常具有很好的泛化能力。

Abstract: In this paper, we address the problem of class-generalizable anomaly detection, where the objective is to develop a unified model by focusing our learning on the available normal data and a small amount of anomaly data in order to detect the completely unseen anomalies, also referred to as the out-of-distribution (OOD) classes. Adding to this challenge is the fact that the anomaly data is rare and costly to label. To achieve this, we propose a multidirectional meta-learning algorithm -- at the inner level, the model aims to learn the manifold of the normal data (representation); at the outer level, the model is meta-tuned with a few anomaly samples to maximize the softmax confidence margin between the normal and anomaly samples (decision surface calibration), treating normals as in-distribution (ID) and anomalies as out-of-distribution (OOD). By iteratively repeating this process over multiple episodes of predominantly normal and a small number of anomaly samples, we realize a multidirectional meta-learning framework. This two-level optimization, enhanced by multidirectional training, enables stronger generalization to unseen anomaly classes.

</details>


### [82] [Calibration without Ground Truth](https://arxiv.org/abs/2601.19862)
*Yuqing Kong,Mingyu Song,Yizhou Wang,Yifan Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种无需标签的后处理框架，通过利用一个虽然较弱但校准更好的参考模型来改进一个强大但校准不佳的模型。实验证明这种方法能够显著降低适当的损失和校准误差。


<details>
  <summary>Details</summary>
Motivation: 随着公开可用的人类文本预计在未来十年内耗尽，提高没有真实标签访问权限的模型变得越来越重要。

Method: 提出了一种基于当强模型与参考模型之间不互相校准时可以实现严格改进的条件下的无标签后处理框架，并开发了一个高效的Bregman投影算法来保证最坏情况下的损失减少而无需标签。

Result: 实验表明，所提出的方法在不同的LLM规模上都能够显著地降低适当的损失和校准错误，达到了与监督基线相竞争的表现。

Conclusion: 本研究提出的无标签后处理框架提供了一种有效途径，在没有真实标签的情况下改善了模型表现。

Abstract: Villalobos et al. [2024] predict that publicly available human text will be exhausted within the next decade. Thus, improving models without access to ground-truth labels becomes increasingly important. We propose a label-free post-processing framework that improves a strong but miscalibrated model using a weaker yet better-calibrated reference. Our framework guarantees a strict performance improvement under any proper loss. Our approach is based on a characterization of when strict improvement is possible: when the strong and reference models are not mutually calibrated. We formalize this condition, connect it to arbitrage and no-trade results from economics, and develop an efficient Bregman projection algorithm that guarantees worst-case loss reduction without labels. Experiments on representative LLMs across varying scales demonstrate that our label-free method significantly reduces proper losses and calibration errors, achieving performance competitive with supervised baselines.

</details>


### [83] [Bandits in Flux: Adversarial Constraints in Dynamic Environments](https://arxiv.org/abs/2601.19867)
*Tareq Si Salem*

Main category: cs.LG

TL;DR: 本文研究了在时变约束下运行的对抗性多臂老虎机问题，提出了一种新的原始-对偶算法，该算法扩展了在线镜像下降方法，并通过适当的梯度估计器和有效的约束处理来解决这个问题。理论分析证明了所提策略具有次线性的动态遗憾和次线性约束违反。实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机来源于现实世界中存在大量需要在时变约束条件下运作的应用场景，如资源分配、在线广告等。这些应用场景促使研究者探索如何有效地解决对抗性多臂老虎机问题。

Method: 为了解决这一复杂设定下的问题，作者们提出了一种新的原始-对偶算法，它基于在线镜像下降框架但引入了适合的梯度估计器以及有效的约束管理机制。

Result: 理论分析表明，所提出的策略能够保证次线性的动态遗憾增长速度与次线性的约束违背量；实验结果进一步证实了该方法相较于现有技术的优势。

Conclusion: 本研究针对时变约束条件下的对抗性多臂老虎机问题提供了一个有效解决方案，不仅从理论上证明了其优越性，而且也通过实验证实了其实用价值。

Abstract: We investigate the challenging problem of adversarial multi-armed bandits operating under time-varying constraints, a scenario motivated by numerous real-world applications. To address this complex setting, we propose a novel primal-dual algorithm that extends online mirror descent through the incorporation of suitable gradient estimators and effective constraint handling. We provide theoretical guarantees establishing sublinear dynamic regret and sublinear constraint violation for our proposed policy. Our algorithm achieves state-of-the-art performance in terms of both regret and constraint violation. Empirical evaluations demonstrate the superiority of our approach.

</details>


### [84] [RHSIA: Real-time Hemodynamics Surrogation for Non-idealized Intracranial Aneurysms](https://arxiv.org/abs/2601.19876)
*Yiying Sheng,Wenhao Ding,Dylan Roi,Leonard Leong Litt Yeo,Hwa Liang Leo,Choon Hwai Yap*

Main category: cs.LG

TL;DR: 研究人员开发了一种结合时间信息的图转换模型，该模型能够从颅内动脉瘤表面网格准确预测心脏周期中的壁面剪切应力（WSS），并可通过增加稳态CFD数据来提高小样本情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管流体机械标记物可以指示颅内动脉瘤（IAs）的疾病进展风险，但计算流体力学（CFD）需要专业知识且耗时长、通量低，这阻碍了其临床应用。

Method: 采用了一种整合了时间信息的图转换模型，并通过大量的CFD数据进行监督学习，以实现从IA表面网格到生物力学标记物如WSS的映射。此外，还引入了大量的低成本稳态CFD数据作为增强手段，以改善在脉动CFD数据样本量较小时模型的表现。

Result: 该模型能有效捕捉WSS模式的时间变化，达到了高达0.981的结构相似性指数(SSIM)以及基于最大值的相对L2误差为2.8%的结果。消融研究和与最先进方法的比较进一步证实了此方法的有效性。

Conclusion: 这项研究表明，即使是在脉动CFD数据样本量较小的情况下，也能利用深度学习模型从几何网格实时计算心血管流动参数的时间序列。此方法可能也适用于其他心血管场景。

Abstract: Extensive studies suggested that fluid mechanical markers of intracranial aneurysms (IAs) derived from Computational Fluid Dynamics (CFD) can indicate disease progression risks, but to date this has not been translated clinically. This is because CFD requires specialized expertise and is time-consuming and low throughput, making it difficult to support clinical trials. A deep learning model that maps IA morphology to biomechanical markers can address this, enabling physicians to obtain these markers in real time without performing CFD. Here, we show that a Graph Transformer model that incorporates temporal information, which is supervised by large CFD data, can accurately predict Wall Shear Stress (WSS) across the cardiac cycle from IA surface meshes. The model effectively captures the temporal variations of the WSS pattern, achieving a Structural Similarity Index (SSIM) of up to 0.981 and a maximum-based relative L2 error of 2.8%. Ablation studies and SOTA comparison confirmed its optimality. Further, as pulsatile CFD data is computationally expensive to generate and sample sizes are limited, we engaged a strategy of injecting a large amount of steady-state CFD data, which are extremely low-cost to generate, as augmentation. This approach enhances network performance substantially when pulsatile CFD data sample size is small. Our study provides a proof of concept that temporal sequences cardiovascular fluid mechanical parameters can be computed in real time using a deep learning model from the geometric mesh, and this is achievable even with small pulsatile CFD sample size. Our approach is likely applicable to other cardiovascular scenarios.

</details>


### [85] [Self-Distillation Enables Continual Learning](https://arxiv.org/abs/2601.19897)
*Idan Shenfeld,Mehul Damani,Jonas Hübotter,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 本文提出了一种名为Self-Distillation Fine-Tuning (SDFT)的方法，通过使用演示条件模型作为自身的教师来直接从演示中进行在线学习，这种方法在技能学习和知识获取任务中优于传统的监督微调(SFT)，同时显著减少了灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 持续学习对于基础模型来说是一个基本挑战，尽管在线强化学习可以减少遗忘现象，但通常需要明确的奖励函数，而这些奖励函数往往是不可得的。从专家演示中学习的主要替代方法是监督微调（SFT），这是一种本质上离线的学习方式。为了解决这个问题，作者们引入了Self-Distillation Fine-Tuning (SDFT)方法。

Method: Self-Distillation Fine-Tuning (SDFT)利用上下文中的学习，通过使用一个基于演示条件下的模型作为自己的教师，从而生成在线训练信号，这有助于保持先前的能力同时习得新技能。

Result: 在技能学习和知识获取任务上，SDFT的表现始终优于SFT，实现了更高的新任务准确性的同时大幅度减少了灾难性遗忘。此外，在连续学习实验中，SDFT让单一模型能够随着时间累积多种技能而不出现性能退化。

Conclusion: SDFT为从演示中持续学习提供了一条实用路径，它证明了在线提炼作为一种有效手段可以帮助模型在不损害现有能力的情况下获得新的技能与知识。

Abstract: Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [86] [Trustworthy Scheduling for Big Data Applications](https://arxiv.org/abs/2601.18983)
*Dimitrios Tomaras,Vana Kalogeraki,Dimitrios Gunopulos*

Main category: cs.DC

TL;DR: 本文提出了一种名为X-Sched的中间件，它利用可解释性技术为资源配置生成可操作指导，以在容器化环境中满足服务级别目标(SLOs)，并通过与随机森林等高级机器学习模型集成来识别最优配置。实验结果表明了该方法的有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的调度器虽然致力于优化任务执行时间和资源利用率等性能指标，但它们对于决策过程以及开发者需要采取的具体措施缺乏透明度，这使得满足服务级别目标(SLOs)变得困难。

Method: X-Sched结合了反事实解释与如随机森林这样的高级机器学习模型，用来高效地确定能够使任务执行符合性能目标的最佳资源配置。

Result: 通过来自真实执行环境的数据验证，实验证明了所提方法不仅有效而且实用性强。

Conclusion: X-Sched提供了一种新颖的方法，通过提高对调度决策背后原因的理解，帮助用户更好地适应容器化执行环境下的资源和时间限制，同时确保达到预期的服务水平目标。

Abstract: Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach.

</details>


### [87] [Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers](https://arxiv.org/abs/2601.19092)
*Bohan Hou,Hongyi Jin,Guanjie Wang,Jinqi Chen,Yaxing Cai,Lijie Yang,Zihao Ye,Yaoyao Ding,Ruihang Lai,Tianqi Chen*

Main category: cs.DC

TL;DR: 本文介绍了一种名为Axe Layout的硬件感知抽象方法，该方法通过命名轴将逻辑张量坐标映射到多轴物理空间，统一了跨设备分布和设备布局中的分块、分片、复制和偏移，从而在最新的GPU设备和多设备环境中接近手工优化内核的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现代深度学习工作负载对数据和计算在设备网格、内存层次结构和异构加速器上的协调放置的需求，提出了一种新的硬件感知抽象方法。

Method: 提出了Axe Layout，这是一种硬件感知抽象，它通过命名轴将逻辑张量坐标映射到一个多轴物理空间，并且统一处理分块、分片、复制以及偏移等问题。基于Axe Layout，设计了一个多粒度、分布感知的DSL（领域特定语言）与编译器，能够在一个内核中组合线程局部控制与集体操作。

Result: 实验结果表明，所提出的统一方法可以在最新的GPU设备和多设备环境中实现接近手工调整内核的性能。

Conclusion: Axe Layout提供了一种有效的方法来解决深度学习工作负载下的数据和计算资源分配问题，在不同类型的硬件上都能达到接近最佳的手动优化效果。

Abstract: Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends.

</details>


### [88] [KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing](https://arxiv.org/abs/2601.19160)
*Sheng Qi,Zhiquan Zhang,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: KUBEDIRECT, a Kubernetes-based cluster manager for FaaS, bypasses the API Server to perform direct message passing, improving efficiency while maintaining compatibility with Kubernetes. It uses a novel state management scheme to ensure consistency, reducing serving latency by 26.7x over Knative.


<details>
  <summary>Details</summary>
Motivation: To address the bottleneck in scaling out FaaS instances on Kubernetes, where extensive state exchange through the API Server hinders performance, without resorting to a complete redesign that would compromise compatibility and require significant engineering effort.

Method: KUBEDIRECT identifies a common 'narrow waist' across FaaS platforms, allowing it to bypass the API Server for more efficient direct message passing. It introduces a new state management approach, using the narrow waist as a hierarchical write-back cache, to maintain consistency and achieve the desired state despite the lack of centralized coordination.

Result: KUBEDIRECT successfully integrates with Kubernetes with minimal code changes (~150 LoC per controller) and significantly reduces serving latency (26.7x improvement over Knative), matching the performance of leading clean-slate solutions like Dirigent.

Conclusion: KUBEDIRECT offers an effective solution for enhancing the scalability and performance of FaaS on Kubernetes, achieving high efficiency and maintaining full compatibility with the existing Kubernetes ecosystem, making it a viable alternative to both traditional and clean-slate approaches.

Abstract: FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.
  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent.

</details>


### [89] [Revisiting Parameter Server in LLM Post-Training](https://arxiv.org/abs/2601.19362)
*Xinyi Wan,Penghui Qi,Guangxing Huang,Chaoyi Ruan,Min Lin,Jialin Li*

Main category: cs.DC

TL;DR: 本文提出了一种名为按需通信(ODC)的方法，该方法通过直接点对点通信替换集体收集和减少散射，从而适应全分片数据并行(FSDP)，以解决大规模语言模型(LLM)后训练中由于序列长度方差高导致的工作负载不平衡问题。相较于FSDP，ODC减少了同步障碍，并解耦了每个设备上的工作负载，使得更快的工作者不会被阻塞。此外，它还实现了更简单有效的迷你批次级别的负载平衡。


<details>
  <summary>Details</summary>
Motivation: 现代数据并行(DP)训练在平衡工作负载下倾向于使用集体通信而非参数服务器(PS)，但对于大型语言模型(LLM)的后训练阶段，由于序列长度差异大导致工作负载不平衡，集体通信会创建同步障碍，造成处理较小工作负载的设备利用率不足。为了解决这一问题，研究者重新审视了对于这种不平衡具有鲁棒性的PS范式。

Method: 提出了按需通信(ODC)方法，通过直接点对点通信来替代完全分片数据并行(FSDP)中的集体all-gather和reduce-scatter操作。

Result: 与FSDP相比，ODC将同步障碍从每层一次降低到每小批量一次，并且解除了各设备间的工作负荷关联，使快速工作的设备不被拖慢。同时，它也能够在小批量级别上实现更加简便而高效的负载均衡。在多种LLM后训练任务中，ODC持续提高了设备利用率和训练吞吐量，相比于标准FSDP最多可提速36%。

Conclusion: 结果表明，ODC是应对LLM后训练中普遍存在工作负载不平衡情况下的更好选择。

Abstract: Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [90] [Encoder-Free ECG-Language Models](https://arxiv.org/abs/2601.18798)
*William Han,Tony Chen,Chaojing Duan,Xiaoyu Song,Yihang Yao,Yuzhe Yang,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.MM

TL;DR: 本文提出了一种无编码器的ECG语言模型(ELF)，通过单一投影层与LLM联合训练，简化了模型架构和训练流程。在五个数据集上的实验表明，ELF的表现可以匹敌甚至超越使用更复杂编码器的现有模型。此外，研究还发现当前ECG语言模型可能更多依赖于基准测试中的特定特征和语言先验知识而非ECG本身的信息，揭示了当前评估实践和模型设计中存在的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的ECG语言模型（ELMs）通常基于视觉-语言模型设计，并依赖预训练的ECG编码器，这增加了架构和训练的复杂性。为了解决这些问题，受到无需编码器的视觉-语言模型启发，研究者旨在开发一种新的、更简单的ECG语言模型。

Method: 提出了名为ELF的新方法，它是一种不需要专门ECG编码器的ECG语言模型。该模型利用一个简单的投影层代替传统编码器，并且这个投影层与大型语言模型一同进行训练。为了验证这种方法的有效性，研究者还在多个数据集上进行了广泛的测试。

Result: 实验结果显示，在五个不同的数据集上，ELF不仅能够匹配而且有时还能超越那些采用了更为复杂的编码器和训练过程的最新ELM表现。进一步地，当尝试向ELF添加结构偏置时，发现即使只使用单一的线性投影也足以保持竞争力。

Conclusion: ELF作为首个无编码器的ECG语言模型，展示了其在简化模型架构的同时不牺牲性能的能力。这项工作同时也指出了当前ECG语言模型及其评估方法存在的问题，强调了对于更准确反映ECG信息处理能力的评测标准的需求。

Abstract: ECG-Language Models (ELMs) extend recent progress in Multimodal Large Language Models (MLLMs) to automated ECG interpretation. However, most ELMs follow Vision-Language Model (VLM) designs and depend on pretrained ECG encoders, adding architectural and training complexity. Inspired by encoder-free VLMs, we introduce ELF, an encoder-free ELM that replaces the ECG encoder with a single projection layer trained jointly with the LLM. Across five datasets, ELF matches or exceeds state-of-the-art ELMs that use far more complex encoders and training pipelines. We also test whether adding architectural biases to ELF improves performance and find that the single linear projection remains competitive. Finally, we show that ELF, and potentially other ELMs, often rely more on benchmark artifacts and language priors than ECG-derived information, highlighting limitations in current evaluation practices and ELM design. All data and code is available at https://github.com/willxxy/ECG-Bench.

</details>


### [91] [Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues](https://arxiv.org/abs/2601.19750)
*Junchen Fu,Wenhao Deng,Kaiwen Zheng,Alexandros Karatzoglou,Ioannis Arapakis,Yu Ye,Yongxin Ni,Joemon M. Jose,Xuri Ge*

Main category: cs.MM

TL;DR: 本文探讨了多模态大语言模型（MLLMs）在电商场景中生成缺失模态信息的能力，提出了一个基准测试MMPCBench，并评估了几种先进模型的表现。实验结果揭示了这些模型在细粒度对齐上的不足以及性能随产品类别和模型规模变化的情况。此外，研究还尝试了通过组相对策略优化（GRPO）来改进模型表现，但仅在图像到文本的任务上有改善。


<details>
  <summary>Details</summary>
Motivation: 电商平台上经常出现由于标注错误或元数据不完整导致的产品信息缺失问题，这不仅影响了产品的展示效果，也影响了如推荐系统等下游应用的效果。受到最近多模态大语言模型生成能力的启发，本研究旨在探索这些模型能否有效填补电商场景中的信息空白。

Method: 构建了一个名为MMPCBench的基准测试框架，该框架包括内容质量完成基准与推荐基准两部分。利用Qwen2.5-VL和Gemma-3系列中的六个多模态大语言模型，在九个真实世界电商类别上进行了图像到文本及文本到图像任务的评测。同时，研究引入了组相对策略优化方法以尝试提升模型在此类任务中的表现。

Result: 实验表明，虽然多模态大语言模型能够捕捉高层次语义信息，但在单词级别、像素或块级别的精细对齐方面存在困难；不同产品类别间及不同规模模型间的性能差异显著；模型大小与性能之间没有明显的直接关系；使用组相对策略优化后，图像到文本任务有所改进，但对于文本到图像任务则未见明显提升。

Conclusion: 当前多模态大语言模型在实际跨模态生成任务中仍面临一定局限性，特别是在处理细节层面的信息时。尽管如此，这项研究为解决电商场景下缺失模态产品补全问题迈出了重要一步。

Abstract: Missing-modality information on e-commerce platforms, such as absent product images or textual descriptions, often arises from annotation errors or incomplete metadata, impairing both product presentation and downstream applications such as recommendation systems. Motivated by the multimodal generative capabilities of recent Multimodal Large Language Models (MLLMs), this work investigates a fundamental yet underexplored question: can MLLMs generate missing modalities for products in e-commerce scenarios? We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.
  We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks. Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment. In addition, performance varies substantially across product categories and model scales, and we observe no trivial correlation between model size and performance, in contrast to trends commonly reported in mainstream benchmarks. We also explore Group Relative Policy Optimization (GRPO) to better align MLLMs with this task. GRPO improves image-to-text completion but does not yield gains for text-to-image completion. Overall, these findings expose the limitations of current MLLMs in real-world cross-modal generation and represent an early step toward more effective missing-modality product completion.

</details>


### [92] [Subjective Evaluation of Frame Rate in Bitrate-Constrained Live Streaming](https://arxiv.org/abs/2601.19776)
*Jiaqi He,Zhengfang Duanmu,Kede Ma*

Main category: cs.MM

TL;DR: 本研究创建了一个高帧率直播视频数据集（HFR-LS），通过改变压缩强度和帧率来探索在带宽限制条件下，视频编码如何平衡压缩强度与帧率对感知质量的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨了在直播流媒体中，由于带宽限制，视频编码需要权衡压缩强度与帧率，但这种折衷对感知质量的具体影响尚未得到充分研究。

Method: 构建了HFR-LS数据集，包括384个以不同目标比特率编码的1080p视频片段，这些视频通过系统地调整压缩强度和帧率生成；进行了一项单刺激隐藏参考主观测试，以评估帧率对于感知质量的影响及其与比特率、源内容之间的相互作用。

Result: 研究发现，帧率显著影响着感知到的质量，并且它与比特率及源内容之间存在交互效应。

Conclusion: HFR-LS数据集为研究带宽受限条件下的直播流媒体提供了宝贵的资源。

Abstract: Bandwidth constraints in live streaming require video codecs to balance compression strength and frame rate, yet the perceptual consequences of this trade-off remain underexplored. We present the high frame rate live streaming (HFR-LS) dataset, comprising 384 subject-rated 1080p videos encoded at multiple target bitrates by systematically varying compression strength and frame rate. A single-stimulus, hidden-reference subjective study shows that frame rate has a noticeable effect on perceived quality, and interacts with both bitrate and source content. The HFR-LS dataset is available at https://github.com/real-hjq/HFR-LS to facilitate research on bitrate-constrained live streaming.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [93] [Reducing False Positives in Static Bug Detection with LLMs: An Empirical Study in Industry](https://arxiv.org/abs/2601.18844)
*Xueying Du,Jiayi Feng,Yi Zou,Wei Xu,Jie Ma,Wei Zhang,Sisi Liu,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLMs）在实际企业环境中减少静态分析工具（SATs）误报的有效性，特别是在腾讯这样的大规模IT公司中。结果显示，结合LLM和静态分析的混合技术可以消除94-98%的误报，并且成本远低于人工审查。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具被广泛用于提高软件质量，但其使用常因高误报率而受阻，特别是在大规模企业系统中。这些误报需要大量的人工检查，导致工业代码审查效率低下。虽然已有研究表明大型语言模型（LLMs）在开源基准测试上减少误报的潜力，但在它们在真实世界企业环境中的有效性尚不清楚。

Method: 通过腾讯企业定制的SAT在其大规模广告与营销服务软件上的数据，构建了一个包含433个警报的数据集（其中328个为误报，105个为真阳性），涵盖了三种常见错误类型。通过对开发者的访谈及数据分析，评估了多种基于LLM的误报减少技术的效果。

Result: 结果表明，误报消耗了大量的手动检查时间（例如每个警报需要10-20分钟）。同时发现，基于LLM的技术在减少工业设置下的误报方面具有巨大潜力（例如，LLM与静态分析相结合的技术能以高召回率消除94-98%的误报）。此外，基于LLM的技术成本效益显著，每条警报的成本低至2.1-109.5秒和$0.0011-$0.12，比手动审查节省了多个数量级的成本。

Conclusion: 基于LLM的方法在减少实际企业环境中静态分析工具产生的误报方面表现出色，能够大幅降低人力成本并提高审查效率。不过也指出了该方法的一些局限性。

Abstract: Static analysis tools (SATs) are widely adopted in both academia and industry for improving software quality, yet their practical use is often hindered by high false positive rates, especially in large-scale enterprise systems. These false alarms demand substantial manual inspection, creating severe inefficiencies in industrial code review. While recent work has demonstrated the potential of large language models (LLMs) for false alarm reduction on open-source benchmarks, their effectiveness in real-world enterprise settings remains unclear. To bridge this gap, we conduct the first comprehensive empirical study of diverse LLM-based false alarm reduction techniques in an industrial context at Tencent, one of the largest IT companies in China. Using data from Tencent's enterprise-customized SAT on its large-scale Advertising and Marketing Services software, we construct a dataset of 433 alarms (328 false positives, 105 true positives) covering three common bug types. Through interviewing developers and analyzing the data, our results highlight the prevalence of false positives, which wastes substantial manual effort (e.g., 10-20 minutes of manual inspection per alarm). Meanwhile, our results show the huge potential of LLMs for reducing false alarms in industrial settings (e.g., hybrid techniques of LLM and static analysis eliminate 94-98% of false positives with high recall). Furthermore, LLM-based techniques are cost-effective, with per-alarm costs as low as 2.1-109.5 seconds and $0.0011-$0.12, representing orders-of-magnitude savings compared to manual review. Finally, our case analysis further identifies key limitations of LLM-based false alarm reduction in industrial settings.

</details>


### [94] [Towards Safety-Compliant Transformer Architectures for Automotive Systems](https://arxiv.org/abs/2601.18850)
*Sven Kirchner,Nils Purschke,Chengdong Wu,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一个概念框架，将Transformer架构从安全角度整合到汽车系统中，通过多模态基础模型利用传感器多样性和冗余性来提高容错能力和鲁棒性，并展示了不同输入模态如何融合以保持一致的场景理解。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的架构在视觉和语言任务中表现出色，但在关键安全应用中面临独特挑战。为了克服这些挑战并确保自动驾驶系统的安全性，需要开发一种新的方法来结合现代深度学习与既定的功能安全实践。

Method: 提出了一种架构，该架构结合了多个独立的特定模态编码器，这些编码器将其表示融合进一个共享的潜在空间内，支持当某一模态性能下降时的操作。此外，还探讨了如何融合不同的输入模态以维持对场景的一致理解。

Result: 通过在表示层面上结构性地嵌入冗余性和多样性，这种方法为自动驾驶领域内可认证的人工智能系统铺平了道路。

Conclusion: 本文介绍的方法为将Transformer集成到汽车系统提供了一个新的视角，它不仅提高了系统的容错能力与鲁棒性，同时也为未来实现更安全可靠的自动驾驶技术奠定了基础。

Abstract: Transformer-based architectures have shown remarkable performance in vision and language tasks but pose unique challenges for safety-critical applications. This paper presents a conceptual framework for integrating Transformers into automotive systems from a safety perspective. We outline how multimodal Foundation Models can leverage sensor diversity and redundancy to improve fault tolerance and robustness. Our proposed architecture combines multiple independent modality-specific encoders that fuse their representations into a shared latent space, supporting fail-operational behavior if one modality degrades. We demonstrate how different input modalities could be fused in order to maintain consistent scene understanding. By structurally embedding redundancy and diversity at the representational level, this approach bridges the gap between modern deep learning and established functional safety practices, paving the way for certifiable AI systems in autonomous driving.

</details>


### [95] [Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions](https://arxiv.org/abs/2601.18949)
*Cole Granger,Dipin Khati,Daniel Rodriguez-Cardenas,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文介绍了Tricky$^2$数据集，该数据集结合了人为编写的缺陷与GPT-5和OpenAI-oss-20b在C++、Python和Java程序中引入的错误，旨在研究这两类错误之间的相互作用，并通过初步评估展示了其在分类、定位及修复任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在软件开发流程中引入的逻辑或数据误用错误与人为错误之间的相互作用。

Method: 构建了一个名为Tricky$^2$的数据集，该数据集基于已有的包含人为编写缺陷的TrickyBugs语料库，并添加了由GPT-5和OpenAI-oss-20b生成的错误。采用了一种分类指导下的提示框架来产生机器来源的bug，同时保持原有的人类缺陷和程序结构不变。

Result: 创建了一个包括仅人为错误、仅LLM错误以及混合错误的数据集，为分析不同来源错误的行为、多bug修复鲁棒性及人机混合代码可靠性提供了基础。此外，还通过小规模基准测试展示了该数据集在分类、定位和修复任务中的使用情况。

Conclusion: Tricky$^2$数据集为理解及处理由人类和大型语言模型共同导致的复杂编程错误提供了宝贵的资源。

Abstract: Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.

</details>


### [96] [The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability](https://arxiv.org/abs/2601.19065)
*Antonios Saravanos,John Pazarzis,Stavros Zervoudakis,Dongnanzi Zheng*

Main category: cs.SE

TL;DR: 本文探讨了Python中如何通过使用不透明委托（类似C++中的PIMPL惯用法）来保持稳定公共API的同时允许内部实现的演变，讨论了这种模式在现有代码库中的应用方式及其优缺点。


<details>
  <summary>Details</summary>
Motivation: Python库需要保持稳定的公共API，即使其内部实现发生变化、增加新后端或依赖于重型可选库。由于Python语言特性，用户可能依赖于非公开的内部对象，这使得重构变得有风险，并且减慢了长期维护的速度。

Method: 重新解读C++中的PIMPL惯用法为Python风格的不透明委托模式：一个小型公共对象（或模块），它将行为委托给被视为内部的另一个实现对象。将此模式置于Python封装技术更广泛的分类体系内，并将其与现有的做法如模块级间接、外观对象和后端调度联系起来。

Result: 展示了如何在现有代码库中利用Python风格的PIMPL来隔离重型依赖项、支持延迟导入以及在不改变公共API的情况下启用运行时选择替代后端。此外，还讨论了这种方法的优点和权衡之处，并提供了关于何时适用该模式及如何在大型长期存在的Python库中应用它的实际指导。

Conclusion: 采用Python风格的PIMPL模式有助于维护稳定公共API的同时促进内部演进，但开发者需考虑相关的设计复杂度增加等潜在成本。

Abstract: Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on "reachable internals" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries.

</details>


### [97] [HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation](https://arxiv.org/abs/2601.19072)
*Kla Tantithamthavorn,Hong Yi Lin,Patanamon Thongtanunam,Wachiraphan Charoenwet,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 本文提出了一种名为HalluJudge的方法，用于检测大型语言模型生成的代码审查评论中的幻觉问题。通过采用直接评估到结构化多分支推理等多种策略，该方法在Atlassian的企业级软件项目中进行了全面评估，并显示出成本效益和高F1分数（0.85），平均成本为$0.009。此外，大约67%的HalluJudge评估结果与实际生产环境中开发者对AI生成审查评论的偏好一致，表明这种方法能够作为实用的保障措施来减少开发者遇到幻觉评论的风险，从而促进对AI辅助代码审查的信任。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动化代码审查方面展现出了强大的能力，比如生成审查评论；然而，这些模型有时会生成脱离实际代码基础的评论（即出现幻觉），这给LLMs在代码审查流程中的应用带来了挑战。

Method: 设计了名为HalluJudge的系统，它基于上下文一致性来评估生成的审查评论是否基于实际代码。HalluJudge采用了从直接评估到树状思维等结构化多分支推理在内的四种关键策略。

Result: 实验结果显示，HalluJudge在幻觉检测上具有成本效益，F1得分为0.85，平均每次评估成本仅为$0.009。同时，在线生产环境中约67%的HalluJudge判断与开发者的偏好相匹配。

Conclusion: HalluJudge可以作为一种实用的保护措施，帮助减少开发者接触到由大型语言模型生成的幻觉评论的机会，从而增强对AI辅助代码审查的信心。

Abstract: Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.

</details>


### [98] [Hybrid Fault-Driven Mutation Testing for Python](https://arxiv.org/abs/2601.19088)
*Saba Alimadadi,Golnaz Gharachorlu*

Main category: cs.SE

TL;DR: 本文提出了一种新的针对Python程序的变异测试技术，通过结合静态和动态分析来生成补充现有通用工具的变异体，以发现高覆盖率测试套件中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的变异测试技术在捕捉像Python这样的动态类型语言中常见的许多类型故障方面存在不足。

Method: 设计了七种新的变异算子，这些算子受到Python程序中普遍存在的反模式启发，并实现了一个名为PyTation的工具，该工具使用静态与动态分析的混合方法来基于这些算子对Python程序进行变异，同时尽量减少等价变异体的数量。

Result: 评估结果显示PyTation生成的变异体能够补充通用工具产生的变异体，在测试执行过程中表现出不同的行为，揭示了高覆盖率测试套件中的缺陷。此外，相对于基线工具而言，PyTation产生了较高比例的独特变异体、较低的交叉杀伤率以及较低的测试重叠比率，且由于动态分析启发式算法的帮助，它还减少了等价变异体的数量。

Conclusion: 提出的PyTation工具通过引入专门针对Python特性的新型变异算子，成功地扩展了模拟故障的范围，提高了测试套件的有效性检测能力。

Abstract: Mutation testing is an effective technique for assessing the effectiveness of test suites by systematically injecting artificial faults into programs. However, existing mutation testing techniques fall short in capturing many types of common faults in dynamically typed languages like Python. In this paper, we introduce a novel set of seven mutation operators that are inspired by prevalent anti-patterns in Python programs, designed to complement the existing general-purpose operators and broaden the spectrum of simulated faults. We propose a mutation testing technique that utilizes a hybrid of static and dynamic analyses to mutate Python programs based on these operators while minimizing equivalent mutants. We implement our approach in a tool called PyTation and evaluate it on 13 open-source Python applications. Our results show that PyTation generates mutants that complement those from general-purpose tools, exhibiting distinct behaviour under test execution and uncovering inadequacies in high-coverage test suites. We further demonstrate that PyTation produces a high proportion of unique mutants, a low cross-kill rate, and a low test overlap ratio relative to baseline tools, highlighting its novel fault model. PyTation also incurs few equivalent mutants, aided by dynamic analysis heuristics.

</details>


### [99] [Detecting and Correcting Hallucinations in LLM-Generated Code via Deterministic AST Analysis](https://arxiv.org/abs/2601.19106)
*Dipin Khati,Daniel Rodriguez-Cardenas,Paul Pantzer,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该论文提出了一种确定性的静态分析框架，用于检测并自动修正由大型语言模型生成的代码中的知识冲突性幻觉（KCHs）。实验结果表明，该方法在检测KCHs上具有100%的精确度和87.6%的召回率，并成功修正了所识别幻觉中的77.0%，证明了其作为可靠代码生成解决方案的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成中虽然提高了生产力，但经常引入知识冲突性幻觉(KCHs)，即细微的语义错误如不存在的API参数等，这些问题能够绕过代码检查工具导致运行时失败。现有的缓解措施往往不可靠。

Method: 作者们设计了一个后处理框架，将生成的代码解析为抽象语法树(AST)，并与通过库内省动态构建的知识库(KB)进行对比验证。这个非执行的方法利用确定性规则来发现并修复API级与标识符级别的冲突。

Result: 在包含200个Python代码片段的手动整理数据集上测试显示，该框架以100％的准确率和87.6％的召回率（F1分数为0.934）检测到了KCHs，并且成功地自动更正了所有被识别出的幻象中的77.0％。

Conclusion: 研究表明，这种基于确定性后处理的方法是概率性修复的一种可行且可靠的替代方案，为实现可信赖的代码生成指明了一条清晰的道路。

Abstract: Large Language Models (LLMs) for code generation boost productivity but frequently introduce Knowledge Conflicting Hallucinations (KCHs), subtle, semantic errors, such as non-existent API parameters, that evade linters and cause runtime failures. Existing mitigations like constrained decoding or non-deterministic LLM-in-the-loop repair are often unreliable for these errors. This paper investigates whether a deterministic, static-analysis framework can reliably detect \textit{and} auto-correct KCHs. We propose a post-processing framework that parses generated code into an Abstract Syntax Tree (AST) and validates it against a dynamically-generated Knowledge Base (KB) built via library introspection. This non-executing approach uses deterministic rules to find and fix both API and identifier-level conflicts. On a manually-curated dataset of 200 Python snippets, our framework detected KCHs with 100\% precision and 87.6\% recall (0.934 F1-score), and successfully auto-corrected 77.0\% of all identified hallucinations. Our findings demonstrate that this deterministic post-processing approach is a viable and reliable alternative to probabilistic repair, offering a clear path toward trustworthy code generation.

</details>


### [100] [The Promise and Reality of Continuous Integration Caching: An Empirical Study of Travis CI Builds](https://arxiv.org/abs/2601.19146)
*Taher A. Ghaleb,Daniel Alencar da Costa,Ying Zou*

Main category: cs.SE

TL;DR: 本研究通过分析Travis CI中的513,384次构建来自1,279个GitHub项目的大型实证研究，探讨了持续集成（CI）缓存在实际应用中的采用情况及其面临的挑战。结果发现只有30%的项目采用了CI缓存，且早期采用者通常与项目成熟度相关。此外，许多项目未采用缓存的原因主要是对CI缓存支持的认识不足。尽管三分之一的项目在启用缓存后看到了显著的构建时间减少，但维护缓存仍面临诸如上传频率高、存在过时缓存等问题。


<details>
  <summary>Details</summary>
Motivation: 持续集成（CI）通过自动化软件构建提供早期反馈，但较长的构建时间可能会影响开发者的生产力。CI服务提供了缓存机制以加快构建速度，通过重用不经常变化的工件来实现。然而，对于缓存如何被实际采纳以及它所带来的挑战知之甚少。

Method: 进行了大规模的经验研究，分析了来自1,279个GitHub项目的513,384次构建数据，并向未采用缓存的项目提交了开启缓存功能的拉取请求，以了解开发者对CI缓存的态度和接受程度。

Result: 研究发现仅有30%的项目使用了CI缓存，且这些项目的特征往往包括更多依赖项、更多提交记录及更长的CI生命周期等成熟度指标。大约一半针对非采用缓存项目的拉取请求得到了接受或合并。虽然部分项目从启用缓存中获得了显著的构建时间缩短，但同时也面临着需要频繁上传缓存文件以及处理过期缓存的问题。

Conclusion: CI缓存并不适用于所有项目，并且其在实践中比很多开发者预期的要复杂得多，需要持续性的维护工作。开发者主要遇到的问题是损坏或过时的缓存，或者希望获得更广泛的缓存特性支持。

Abstract: Continuous Integration (CI) provides early feedback by automatically building software, but long build durations can hinder developer productivity. CI services offer caching mechanisms to speed up builds by reusing infrequently changing artifacts, yet little is known about how caching is adopted in practice and what challenges it entails. In this paper, we conduct a large-scale empirical study of CI caching in Travis CI, analyzing 513,384 builds from 1,279 GitHub projects. We find that only 30% of projects adopt CI caching, and early adoption is strongly associated with project maturity, such as more dependencies, more commits, and longer CI lifespans. To understand why many projects do not adopt caching, we submitted pull requests enabling caching in non-adopting projects, and nearly half were accepted or merged. Developer feedback suggests that non- or late adoption mainly stems from limited awareness of CI caching support. We also examine cache maintenance and identify five common activities, performed by 24% of cache-enabled projects. Although one-third of projects see substantial build-time reductions, cache uploads occur in 97% of builds, and 33% of projects contain stale cached artifacts. Finally, our analysis of reported caching issues shows developers mainly struggle with corrupted or outdated caches or request broader caching features. Overall, CI caching does not help all projects, needs ongoing maintenance, and is more complex in practice than many developers expect.

</details>


### [101] [LLM-based Vulnerability Detection at Project Scale: An Empirical Study](https://arxiv.org/abs/2601.19239)
*Fengjie Li,Jiajun Jiang,Dongchi Chen,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本研究对基于LLM的检测器与传统静态分析工具进行了首次全面的经验性比较，揭示了基于LLM的方法在项目规模上检测漏洞时存在的局限性，包括召回率低、误报率高以及计算成本高昂等问题。


<details>
  <summary>Details</summary>
Motivation: 探索并对比基于大型语言模型（LLM）的检测器与传统静态分析工具在项目规模上检测软件漏洞的有效性和实用性。

Method: 通过内部基准测试222个已知真实世界中的漏洞（C/C++和Java），以及对24个活跃开源项目的385条警告进行人工检查来评估五种最新的基于LLM的方法和两种传统工具的表现。

Result: 发现基于LLM的检测器虽然召回率较低，但在内部基准上识别出更多独特的漏洞；然而，在实际项目中所有工具都面临极高的误报率问题；此外，基于LLM的方法还存在浅层过程间推理错误、源/汇点配对误判等特有失败原因，并且计算成本非常高。

Conclusion: 当前基于LLM的检测器在鲁棒性、可靠性和可扩展性方面存在关键限制，需要进一步研究以实现更有效和实用的大规模漏洞检测。

Abstract: In this paper, we present the first comprehensive empirical study of specialized LLM-based detectors and compare them with traditional static analyzers at the project scale. Specifically, our study evaluates five latest and representative LLM-based methods and two traditional tools using: 1) an in-house benchmark of 222 known real-world vulnerabilities (C/C++ and Java) to assess detection capability, and 2) 24 active open-source projects, where we manually inspected 385 warnings to assess their practical usability and underlying root causes of failures. Our evaluation yields three key findings: First, while LLM-based detectors exhibit low recall on the in-house benchmark, they still uncover more unique vulnerabilities than traditional tools. Second, in open-source projects, both LLM-based and traditional tools generate substantial warnings but suffer from very high false discovery rates, hindering practical use. Our manual analysis further reveals shallow interprocedural reasoning and misidentified source/sink pairs as primary failure causes, with LLM-based tools exhibiting additional unique failures. Finally, LLM-based methods incurs substantial computational costs-hundreds of thousands to hundreds of millions of tokens and multi-hour to multi-day runtimes. Overall, our findings underscore critical limitations in the robustness, reliability, and scalability of current LLM-based detectors. We ultimately summarize a set of implications for future research toward more effective and practical project-scale vulnerability detection.

</details>


### [102] ["ENERGY STAR" LLM-Enabled Software Engineering Tools](https://arxiv.org/abs/2601.19260)
*Himon Thakur,Armin Moin*

Main category: cs.SE

TL;DR: 本文研究了AI增强的软件工程工具（如CASE工具和IDE）在能源效率方面的影响，特别是大型语言模型（LLMs）提供的高级机器学习能力。提出了一种结合检索增强生成（RAG）与提示工程技术（PETs）的方法，旨在提高基于LLM代码生成的质量与能效。通过一个全面的框架衡量不同参数规模模型下的实时能耗和推理时间，为未来深入分析提供了概念验证。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术越来越多地融入到支持软件工程过程的工具中，这些系统对于整个软件开发生命周期中的能源消耗模式产生了重要影响。特别是在默认激活AI功能的情况下，如何提高这类工具尤其是大型语言模型（LLMs）驱动的代码生成质量同时保持较高的能源效率成为了一个值得关注的问题。

Method: 作者提出了一种新的方法，该方法将检索增强生成(RAG)与提示工程技术(PETs)相结合，以优化基于大型语言模型的代码生成过程。此外，还设计了一个能够测量从1.25亿到70亿参数范围内不同类型模型架构下实时能量消耗及推理时间的综合框架。

Result: 研究表明，通过采用RAG与PETs相结合的技术可以有效提升基于LLM的代码生成任务的质量，并且在一定程度上降低了能耗。实验结果表明，在不同规模的模型之间存在显著差异，但总体趋势表明所提方案有助于实现更高效、更环保的AI应用。

Conclusion: 本研究证明了通过结合RAG与PETs来改善基于LLM的代码生成功能不仅能够提高输出质量，而且还能促进能源效率的提升。这为开发更加可持续发展的AI赋能软件工程工具开辟了新路径。

Abstract: The discussion around AI-Engineering, that is, Software Engineering (SE) for AI-enabled Systems, cannot ignore a crucial class of software systems that are increasingly becoming AI-enhanced: Those used to enable or support the SE process, such as Computer-Aided SE (CASE) tools and Integrated Development Environments (IDEs). In this paper, we study the energy efficiency of these systems. As AI becomes seamlessly available in these tools and, in many cases, is active by default, we are entering a new era with significant implications for energy consumption patterns throughout the Software Development Lifecycle (SDLC). We focus on advanced Machine Learning (ML) capabilities provided by Large Language Models (LLMs). Our proposed approach combines Retrieval-Augmented Generation (RAG) with Prompt Engineering Techniques (PETs) to enhance both the quality and energy efficiency of LLM-based code generation. We present a comprehensive framework that measures real-time energy consumption and inference time across diverse model architectures ranging from 125M to 7B parameters, including GPT-2, CodeLlama, Qwen 2.5, and DeepSeek Coder. These LLMs, chosen for practical reasons, are sufficient to validate the core ideas and provide a proof of concept for more in-depth future analysis.

</details>


### [103] [Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code](https://arxiv.org/abs/2601.19264)
*Syed Mehedi Hasan Nirob,Shamim Ehsan,Moqsadur Rahman,Summit Haque*

Main category: cs.SE

TL;DR: 本研究探讨了区分人类编写和机器生成代码的问题，通过比较基于特征的检测器和基于嵌入的检测器两种互补方法。基于特征的模型在性能上表现出色，而基于CodeBERT嵌入的模型也非常有竞争力。分析显示，与缩进和空白相关的特征提供了特别具有区分性的线索，而嵌入则捕捉到更深层次的语义模式并提供稍高的精确度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）使得从自然语言提示合成合理的源代码变得异常容易，加速了软件开发和支持学习的同时也带来了新的风险，如学术诚信、作者归属以及负责任的人工智能使用问题。因此，需要有效的方法来区分人写代码与机器生成代码。

Method: 研究采用了两种互补的方法进行对比：一种是基于轻量级、可解释的代码风格和结构属性构建的特征检测器；另一种是利用预训练代码编码器的嵌入式检测器。实验数据集包含了60万个人类编写和AI生成的代码样本。

Result: 结果表明，基于特征的模型在ROC-AUC (0.995)、PR-AUC (0.995) 和 F1分数 (0.971) 上表现优异；而基于CodeBERT嵌入的模型也展现了极强的竞争性(ROC-AUC 0.994, PR-AUC 0.994, F1 0.965)。进一步分析发现，与缩进和空白有关的特征为辨别提供了特别有用的线索，而嵌入能够捕捉更深一层的语义模式，并且在精度上略高一筹。

Conclusion: 该研究表明，在识别代码来源方面，基于特征的方法和基于嵌入的方法各有优势，其中前者在解释性和性能之间取得平衡，后者则在泛化能力上略有优势。这些发现为在学术界和工业界部署稳健的代码起源检测提供了实用指导。

Abstract: Large language models (LLMs) have made it remarkably easy to synthesize plausible source code from natural language prompts. While this accelerates software development and supports learning, it also raises new risks for academic integrity, authorship attribution, and responsible AI use. This paper investigates the problem of distinguishing human-written from machine-generated code by comparing two complementary approaches: feature-based detectors built from lightweight, interpretable stylometric and structural properties of code, and embedding-based detectors leveraging pretrained code encoders. Using a recent large-scale benchmark dataset of 600k human-written and AI-generated code samples, we find that feature-based models achieve strong performance (ROC-AUC 0.995, PR-AUC 0.995, F1 0.971), while embedding-based models with CodeBERT embeddings are also very competitive (ROC-AUC 0.994, PR-AUC 0.994, F1 0.965). Analysis shows that features tied to indentation and whitespace provide particularly discriminative cues, whereas embeddings capture deeper semantic patterns and yield slightly higher precision. These findings underscore the trade-offs between interpretability and generalization, offering practical guidance for deploying robust code-origin detection in academic and industrial contexts.

</details>


### [104] [Modeling Sampling Workflows for Code Repositories](https://arxiv.org/abs/2601.19316)
*Romain Lefeuvre,Maïwenn Le Goasteller,Jessie Galasso,Benoit Combemale,Quentin Perez,Houari Sahraoui*

Main category: cs.SE

TL;DR: 本文提出了一种领域特定语言（DSL）来明确描述通过可组合采样操作符实现的复杂采样策略，以解决软件工程研究中采样设计和代表性以及采样决策对普遍性影响推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 在实证软件工程研究中，数据集抽样策略的设计与评估至关重要，因为它们直接影响研究结果的普遍性。然而，抽样仍然是软件工程研究中被低估的一个方面。主要存在两个挑战：(1) 抽样方法的设计及其代表性；(2) 对抽样决定如何影响结果普遍性的理解能力。

Method: 为了解决上述问题，作者们提出了一种领域特定语言（DSL），它能够通过可组合的抽样操作符来明确定义复杂的抽样策略。该形式化方法不仅支持抽样策略的具体化，还帮助基于所应用的抽样策略进行关于结果普遍性的推理。此外，他们还开发了一个基于Python的流畅API来实现这个DSL，并展示了如何使用从抽样工作流中提取的统计指标来促进代表性的推理。

Result: 通过一个涉及代码仓库抽样的MSR论文案例研究验证了他们的方法。结果表明，提出的DSL能够建模近期文献中报告的抽样策略。

Conclusion: 该研究提供了一种新的方法来改进软件工程研究中的抽样实践，通过采用一种领域特定语言来更好地定义、执行及推理抽样策略，从而提高研究发现的可信度。

Abstract: Empirical software engineering research often depends on datasets of code repository artifacts, where sampling strategies are employed to enable large-scale analyses. The design and evaluation of these strategies are critical, as they directly influence the generalizability of research findings. However, sampling remains an underestimated aspect in software engineering research: we identify two main challenges related to (1) the design and representativeness of sampling approaches, and (2) the ability to reason about the implications of sampling decisions on generalizability. To address these challenges, we propose a Domain-Specific Language (DSL) to explicitly describe complex sampling strategies through composable sampling operators. This formalism supports both the specification and the reasoning about the generalizability of results based on the applied sampling strategies. We implement the DSL as a Python-based fluent API, and demonstrate how it facilitates representativeness reasoning using statistical indicators extracted from sampling workflows. We validate our approach through a case study of MSR papers involving code repository sampling. Our results show that the DSL can model the sampling strategies reported in recent literature.

</details>


### [105] [High-quality data augmentation for code comment classification](https://arxiv.org/abs/2601.19383)
*Thomas Borsani,Andrea Rosani,Giuseppe Di Fatta*

Main category: cs.SE

TL;DR: 研究提出了一种新的合成过采样和增强技术(Q-SYNTH)，以解决现有代码注释分类数据集规模小和类别不平衡的问题，从而提高了基线分类器的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的代码注释分类数据集存在规模限制和类别不平衡问题，这些问题源于依赖人工标注且可能无法准确反映实际代码库中注释的分布情况。

Method: 研究者们引入了基于高质量数据生成的新合成过采样及增强技术（Q-SYNTH），旨在改善NLBSE'26挑战赛的数据集。

Result: 通过采用Q-SYNTH方法，基线分类器的表现得到了$2.56\%$的提升。

Conclusion: 这项研究表明，通过使用合成过采样与增强技术可以有效缓解当前代码注释分类任务面临的数据集局限性，并有助于提高机器学习模型在理解自然语言代码注释上的性能。

Abstract: Code comments serve a crucial role in software development for documenting functionality, clarifying design choices, and assisting with issue tracking. They capture developers' insights about the surrounding source code, serving as an essential resource for both human comprehension and automated analysis. Nevertheless, since comments are in natural language, they present challenges for machine-based code understanding. To address this, recent studies have applied natural language processing (NLP) and deep learning techniques to classify comments according to developers' intentions. However, existing datasets for this task suffer from size limitations and class imbalance, as they rely on manual annotations and may not accurately represent the distribution of comments in real-world codebases. To overcome this issue, we introduce new synthetic oversampling and augmentation techniques based on high-quality data generation to enhance the NLBSE'26 challenge datasets. Our Synthetic Quality Oversampling Technique and Augmentation Technique (Q-SYNTH) yield promising results, improving the base classifier by $2.56\%$.

</details>


### [106] [Bridging the Socio-Emotional Gap: The Functional Dimension of Human-AI Collaboration for Software Engineering](https://arxiv.org/abs/2601.19387)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 本研究探讨了软件从业者如何看待人与AI在协作中的社会情感差距，以及他们期望AI系统具备哪些能力以实现有效协作。结果表明，从业者目前将AI视为智力队友而非社交伙伴，并且认为有效的AI协作可能更多地依赖于功能设计而非复制人类的社会情感智能特质。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型被用于支持软件工程师及其开发团队，理解有效的人机协作变得越来越重要。社会情感智能（SEI）可以增强人类团队成员之间的协作，但在人机协作中的作用尚不明确。当前的AI系统缺乏人类在团队工作中带来的SEI能力，这可能会导致协作动态中出现潜在差距。

Method: 通过与10位从业者的半结构化访谈，研究者考察了他们对于与人类队友相比如何考虑与AI队友合作的看法，特别关注他们对SEI的期待及所设想的AI能力。

Result: 研究发现，从业者目前把AI模型看作是智力上的队友而不是社交伙伴，并且对他们期望的社会情感属性比对人类队友少。然而，他们认为社会情感差距不是由于AI未能展示出SEI特征，而是因为协作能力的功能性差距——即AI无法协商责任、适应情境或维持持续的合作关系。

Conclusion: 提出了功能等效物的概念：技术能力（内部认知、情境智能、适应性学习和协作智能），这些能够达到与人类SEI属性相比较的协作效果。研究建议，对于软件工程任务而言，与AI的有效协作或许能从功能性设计中受益，而不是简单地模仿人类的SEI特质，从而重新定义了协作作为功能性的一致。

Abstract: As GenAI models are adopted to support software engineers and their development teams, understanding effective human-AI collaboration (HAIC) is increasingly important. Socio-emotional intelligence (SEI) enhances collaboration among human teammates, but its role in HAIC remains unclear. Current AI systems lack SEI capabilities that humans bring to teamwork, creating a potential gap in collaborative dynamics. In this study, we investigate how software practitioners perceive the socio-emotional gap in HAIC and what capabilities AI systems require for effective collaboration. Through semi-structured interviews with 10 practitioners, we examine how they think about collaborating with human versus AI teammates, focusing on their SEI expectations and the AI capabilities they envision. Results indicate that practitioners currently view AI models as intellectual teammates rather than social partners and expect fewer SEI attributes from them than from human teammates. However, they see the socio-emotional gap not as AIs failure to exhibit SEI traits, but as a functional gap in collaborative capabilities (AIs inability to negotiate responsibilities, adapt contextually, or maintain sustained partnerships). We introduce the concept of functional equivalents: technical capabilities (internal cognition, contextual intelligence, adaptive learning, and collaborative intelligence) that achieve collaborative outcomes comparable to human SEI attributes. Our findings suggest that effective collaboration with AI for SE tasks may benefit from functional design rather than replicating human SEI traits for SE tasks, thereby redefining collaboration as functional alignment.

</details>


### [107] [From Scattered to Structured: A Vision for Automating Architectural Knowledge Management](https://arxiv.org/abs/2601.19548)
*Jan Keim,Angelika Kaplan*

Main category: cs.SE

TL;DR: 提出了一种自动化流程，用于从多种软件工件中系统地提取架构知识、链接这些知识、识别并解决不一致性，并将这些知识整合到一个结构化的知识库中。该知识库支持架构一致性检查、变更影响分析以及自然语言问答，以改进对架构知识的访问。


<details>
  <summary>Details</summary>
Motivation: 软件架构本质上是以知识为中心的，而这些知识分布在需求文档、设计图、代码和文档等多种异构软件工件中，这使得开发者难以有效地获取和利用这些知识。随着系统的演变，这些工件之间经常出现不一致，导致架构侵蚀并阻碍了维护活动。

Method: 计划开发专门针对不同类型工件的抽取器，设计统一的知识表示模式，实现一致性检查机制，并集成检索增强生成技术来支持对话式知识访问。

Result: 通过本研究提出的自动化流程和技术手段，期望能够有效解决软件架构知识分散及不一致的问题，提高开发者对于架构知识的访问效率和质量。

Conclusion: 为了解决软件架构知识分散及随时间推移产生的不一致性问题，本文提出了一种新的方法论，旨在通过自动化工具链促进架构知识的有效管理和利用。

Abstract: Software architecture is inherently knowledge-centric. The architectural knowledge is distributed across heterogeneous software artifacts such as requirements documents, design diagrams, code, and documentation, making it difficult for developers to access and utilize this knowledge effectively. Moreover, as systems evolve, inconsistencies frequently emerge between these artifacts, leading to architectural erosion and impeding maintenance activities. We envision an automated pipeline that systematically extracts architectural knowledge from diverse artifacts, links them, identifies and resolves inconsistencies, and consolidates this knowledge into a structured knowledge base. This knowledge base enables critical activities such as architecture conformance checking and change impact analysis, while supporting natural language question-answering to improve access to architectural knowledge. To realize this vision, we plan to develop specialized extractors for different artifact types, design a unified knowledge representation schema, implement consistency checking mechanisms, and integrate retrieval-augmented generation techniques for conversational knowledge access.

</details>


### [108] [The Competence Crisis: A Design Fiction on AI-Assisted Research in Software Engineering](https://arxiv.org/abs/2601.19628)
*Mairieli Wessel,Daniel Feitosa,Sangeeth Kochanthara*

Main category: cs.SE

TL;DR: 本文通过设计虚构的方法探讨了在日益增长的发表压力和生成式AI工具常规使用背景下，软件工程研究领域可能出现的问题，如技能退化、责任归属及学术产出的信任度等。


<details>
  <summary>Details</summary>
Motivation: 随着发表压力的增加以及生成式AI工具的普遍应用，软件工程研究的方式、评估标准及教学方法正在发生变化。虽然这些变化带来了效率上的提升，但同时也引发了对于技能退化、责任分配以及对学术作品信任度下降等方面的担忧。

Method: 本文采用了一种名为“设计虚构”的方法论视角来审视如果当前实践继续下去，上述担忧将如何具体化。基于最近一次社区调查所报告的主题，作者构建了一个设定在未来不久的研究环境中的推测性人工制品。这里提到的设计虚构被用作一种分析手段而非预测工具，旨在促进对自动化辅助可能如何阻碍领域知识能力、验证过程以及指导实践等方面的反思。

Result: 通过展示一个故意让人感到不安的情景，文章促使读者思考未来软件工程研究界将如何定义熟练度、划分责任和支持学习。

Conclusion: 该文呼吁对未来软件工程研究领域内专业技能的界定、责任分配机制以及支持学习的方式进行深入讨论。

Abstract: Rising publication pressure and the routine use of generative AI tools are reshaping how software engineering research is produced, assessed, and taught. While these developments promise efficiency, they also raise concerns about skill degradation, responsibility, and trust in scholarly outputs. This vision paper employs Design Fiction as a methodological lens to examine how such concerns might materialise if current practices persist. Drawing on themes reported in a recent community survey, we construct a speculative artifact situated in a near future research setting. The fiction is used as an analytical device rather than a forecast, enabling reflection on how automated assistance might impede domain knowledge competence, verification, and mentoring practices. By presenting an intentionally unsettling scenario, the paper invites discussion on how the software engineering research community in the future will define proficiency, allocate responsibility, and support learning.

</details>


### [109] [Using LLMs to Evaluate Architecture Documents: Results from a Digital Marketplace Environment](https://arxiv.org/abs/2601.19693)
*Frank Elberzhager,Matthias Gerbershagen,Joshua Ginkel*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLMs）在评估软件架构文档中的应用，发现文档质量越高，LLM的评估结果与人类专家越一致。尽管LLM在这一任务中展现出潜力，但其表现的一致性仍需进一步分析。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI特别是大型语言模型，在软件工程活动如软件架构文件评估中的实际效用，并希望通过比较LLM和人类专家对架构文档质量的评价来提高这些工件的质量。

Method: 在一个开发数字市场的研究项目背景下，使用不同的LLM来分析架构文档的质量，并将结果与软件架构师的人工评估进行对比。

Result: 研究表明，架构文档本身的质量显著影响了LLM的表现；高质量的文档使得LLM的评价更接近于人类专家的意见。然而，也观察到了一些不一致性，表明需要进一步的研究以全面理解LLM在这种场景下的适用性。

Conclusion: 虽然LLM支持的架构文档评估显示出一定的前景，但在将其广泛应用于实践之前，还需要解决一些关于准确性和一致性的问题。

Abstract: Generative AI plays an increasing role during software engineering activities to make them, e.g., more efficient or provide better quality. However, it is often unclear how much benefit LLMs really provide. We concentrate on software architects and investigated how an LLM-supported evaluation of architecture documents can support software architects to improve such artefacts. In the context of a research project where a digital marketplace is developed and digital solutions should be analyzed, we used different LLMs to analyze the quality of architecture documents and compared the results with evaluations from software architects. We found out that the quality of the artifact has a strong influence on the quality of the LLM, i.e., the better the quality of the architecture document was, the more consistent were the LLM-based evaluation and the human expert evaluation. While using LLMs in this architecture task is promising, our results showed inconsistencies that need further analyses before generalizing them.

</details>


### [110] [AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion](https://arxiv.org/abs/2601.19697)
*Tianyue Jiang,Yanli Wang,Yanlin Wang,Daya Guo,Ensheng Shi,Yuchi Ma,Jiachi Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了一种名为AlignCoder的代码补全框架，该框架通过查询增强机制和基于强化学习的检索器训练方法来提高代码补全的准确性。在CrossCodeEval基准测试中相比基线提高了18.1%的EM分数，并展示了对于不同代码LLMs及编程语言的高度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代码大型语言模型（code LLMs）在理解特定存储库上下文和领域知识方面存在局限性，导致在仓库级别上的代码补全任务上面临挑战。虽然检索增强生成(RAG)方法通过检索相关代码片段作为跨文件上下文展现出了潜力，但它们面临着查询与目标代码之间不匹配以及现有检索方法无法有效利用推理信息的问题。

Method: 提出了AlignCoder，一个仓库级别的代码补全框架，它引入了一个查询增强机制和基于强化学习的检索器训练方法。首先，通过生成多个候选完成项来构建一个增强了的查询，以缩小初始查询与目标代码之间的语义差距；其次，使用强化学习训练一个称为AlignRetriever的组件，使其能够学会如何利用增强查询中的推理信息来进行更准确的检索。

Result: 在两个广泛使用的基准测试(CrossCodeEval和RepoEval)上对五种不同的基础代码LLM进行了评估。结果表明，在CrossCodeEval基准测试中，相比基线方法，AlignCoder框架在EM得分上提升了18.1%，并且对于多种代码LLM和编程语言都表现出了优秀的性能和高度的泛化能力。

Conclusion: AlignCoder提供了一种有效的解决方案来克服当前代码LLM在仓库级代码补全任务上的限制，特别是在处理需要深入理解特定上下文或领域知识的情况下。此外，该研究还证明了通过结合查询增强技术和基于强化学习的检索策略可以显著提高代码补全的质量。

Abstract: Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.

</details>


### [111] [Future of Software Engineering Research: The SIGSOFT Perspective](https://arxiv.org/abs/2601.19731)
*Massimiliano Di Penta,Kelly Blincoe,Marsha Chechik,Claire Le Goues,David Lo,Emerson Murphy-Hill,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 软件工程会议规模的增长导致成本上升和形式过时，为许多研究人员参与设置了障碍。基于调查数据，文章提出了ACM SIGSOFT可以采取的具体行动来解决这些问题，包括提高会议资金透明度、尝试混合海报展示以及扩大对代表性不足地区的外展活动，以此保证社区的可访问性和包容性。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程会议规模的增大，不断攀升的成本及陈旧的举办方式正成为众多研究者参会的阻碍。这些障碍威胁到了软件工程（SE）领域一直以来所依赖的成功因素之一——包容性和全球多样性。

Method: 通过收集并分析调查数据，确定了若干具体措施，旨在建议ACM SIGSOFT如何应对当前挑战。

Result: 提出了一系列改善措施，比如增强会议财务透明度、探索线上线下结合的海报展示模式以及加强对不发达地区的联系与支持等。

Conclusion: 如果能够实施上述变革，SIGSOFT将有助于确保软件工程领域的开放性和友好氛围，使得更多人能够无障碍地参与到这个大家庭中来。

Abstract: As software engineering conferences grow in size, rising costs and outdated formats are creating barriers to participation for many researchers. These barriers threaten the inclusivity and global diversity that have contributed to the success of the SE community. Based on survey data, we identify concrete actions the ACM Special Interest Group on Software Engineering (SIGSOFT) can take to address these challenges, including improving transparency around conference funding, experimenting with hybrid poster presentations, and expanding outreach to underrepresented regions. By implementing these changes, SIGSOFT can help ensure the software engineering community remains accessible and welcoming.

</details>


### [112] [Assessing Task-based Chatbots: Snapshot and Curated Datasets for Dialogflow](https://arxiv.org/abs/2601.19787)
*Elena Masserini,Diego Clerissi,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本文介绍了两个数据集TOFU-D和COD，旨在为聊天机器人的质量与安全研究提供基础。初步评估显示，许多聊天机器人存在测试覆盖率不足及安全性漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模的精选数据集限制了对聊天机器人质量和可靠性的研究，尽管它们在不同领域得到了广泛应用。

Method: 构建了包含1,788个Dialogflow聊天机器人的TOFU-D数据集及其子集COD（包含185个验证过的聊天机器人），并使用Botium测试框架和Bandit静态分析器进行了初步评估。

Result: 结果表明，多个聊天机器人存在测试覆盖率低以及常见的安全漏洞问题。

Conclusion: 需要进行系统性、跨平台的研究来提高聊天机器人的质量和安全性。

Abstract: In recent years, chatbots have gained widespread adoption thanks to their ability to assist users at any time and across diverse domains. However, the lack of large-scale curated datasets limits research on their quality and reliability. This paper presents TOFU-D, a snapshot of 1,788 Dialogflow chatbots from GitHub, and COD, a curated subset of TOFU-D including 185 validated chatbots. The two datasets capture a wide range of domains, languages, and implementation patterns, offering a sound basis for empirical studies on chatbot quality and security. A preliminary assessment using the Botium testing framework and the Bandit static analyzer revealed gaps in test coverage and frequent security vulnerabilities in several chatbots, highlighting the need for systematic, multi-Platform research on chatbot quality and security.

</details>
