<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Time-Aware Adaptive Side Information Fusion for Sequential Recommendation](https://arxiv.org/abs/2512.24246)
*Jie Luo,Wenyu Zhang,Xinming Zhang,Yuan Fang*

Main category: cs.IR

TL;DR: 提出了TASIF框架，通过时间感知的自适应侧信息融合、简单的插件式时间跨度划分机制和自适应频率滤波器来解决序列推荐中存在的三个关键挑战。实验表明，TASIF在四个公开数据集上显著优于现有技术，同时保持了良好的训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前模型在将商品侧信息（如类别和品牌）融入序列推荐时面临三大挑战：忽视时间戳中的细粒度时间动态、对用户交互序列中的噪音敏感以及依赖于计算成本高昂的融合架构。

Method: 提出了一种名为TASIF（Time-Aware Adaptive Side Information Fusion）的框架，该框架结合了三个协同工作的组件：1) 一种简单的时间跨度分割机制，用于捕捉全局时间模式；2) 一个自适应频率过滤器，利用可学习门控机制来自适应地去噪特征序列；3) 一种高效的自适应侧信息融合层，采用“引导而非混合”的架构，使属性能够指导注意力机制而不与内容表示项嵌入相混，确保深层互动的同时保证计算效率。

Result: 在四个公共数据集上的广泛实验表明，TASIF不仅显著超越了最先进的基线方法，而且在训练过程中保持了极高的效率。

Conclusion: TASIF框架有效地解决了序列推荐中涉及的三个主要问题，并且相比于现有技术展示出了更优的表现与更高的计算效率。

Abstract: Incorporating item-side information, such as category and brand, into sequential recommendation is a well-established and effective approach for improving performance. However, despite significant advancements, current models are generally limited by three key challenges: they often overlook the fine-grained temporal dynamics inherent in timestamps, exhibit vulnerability to noise in user interaction sequences, and rely on computationally expensive fusion architectures. To systematically address these challenges, we propose the Time-Aware Adaptive Side Information Fusion (TASIF) framework. TASIF integrates three synergistic components: (1) a simple, plug-and-play time span partitioning mechanism to capture global temporal patterns; (2) an adaptive frequency filter that leverages a learnable gate to denoise feature sequences adaptively, thereby providing higher-quality inputs for subsequent fusion modules; and (3) an efficient adaptive side information fusion layer, this layer employs a "guide-not-mix" architecture, where attributes guide the attention mechanism without being mixed into the content-representing item embeddings, ensuring deep interaction while ensuring computational efficiency. Extensive experiments on four public datasets demonstrate that TASIF significantly outperforms state-of-the-art baselines while maintaining excellent efficiency in training. Our source code is available at https://github.com/jluo00/TASIF.

</details>


### [2] [RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2512.24268)
*Pankayaraj Pathmanathan,Michael-Andrei Panaitescu-Liess,Cho-Yu Jason Chiang,Furong Huang*

Main category: cs.IR

TL;DR: 本文针对检索增强生成(RAG)管道中的语料库中毒问题，提出了两种互补的检索阶段防御措施：RAGPart和RAGMask。这些方法旨在减轻恶意文档对模型输出的影响，同时保持良性条件下的实用性。


<details>
  <summary>Details</summary>
Motivation: 由于检索增强生成（RAG）在提高大型语言模型性能方面展现出潜力，但同时也暴露出面对语料库中毒攻击时的脆弱性，作者提出了一种新的解决方案来对抗这种威胁，旨在减少攻击成功率的同时保持系统在正常情况下的有效性。

Method: 提出了两种直接作用于检索器上的轻量级防御机制：RAGPart通过利用密集检索器训练动态，并采用文档分区来缓解中毒点影响；而RAGMask则通过识别基于目标令牌遮蔽下显著相似度变化的可疑令牌来进行防御。

Result: 实验结果表明，在两个基准测试、四种中毒策略以及四种最先进的检索器上，所提出的防御措施能够一致地降低攻击成功率，同时在非恶意条件下维持了良好的性能。

Conclusion: 研究揭示了检索阶段防御措施的有效性和局限性，为实现更加稳健的RAG部署提供了实用见解。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.

</details>


### [3] [On the Factual Consistency of Text-based Explainable Recommendation Models](https://arxiv.org/abs/2512.24366)
*Ben Kabongo,Vincent Guigue*

Main category: cs.IR

TL;DR: 本文提出了一种评估基于文本的可解释推荐系统中事实一致性的综合框架，通过设计一种基于提示的方法从评论中提取原子解释语句来构建基准，并提出了结合大语言模型和自然语言推理方法的对齐度量标准以评估生成解释的事实一致性和相关性。实验结果揭示了现有模型在保持高语义相似度的同时，在事实准确性上表现不佳的问题，强调了在可解释推荐中考虑事实准确性的必要性。


<details>
  <summary>Details</summary>
Motivation: 虽然最近的进步利用了大型语言模型（LLMs）来生成流畅的输出，但一个关键问题仍然未被充分探索：这些解释是否与可用证据在事实上保持一致？

Method: 引入了一个全面的框架来评估基于文本的可解释推荐者的事实一致性；设计了一种基于提示的方法，使用LLMs从评论中提取原子解释语句，从而构建了一个专注于它们事实内容的真实基准；提出了结合LLM-和NLI-基础方法的陈述级别对齐指标，以评估生成解释的事实一致性和相关性。

Result: 通过对六种最先进的可解释推荐模型进行广泛的实验，发现尽管模型达到了高的语义相似度分数(BERTScore F1: 0.81-0.90)，所有事实性指标都显示出令人担忧的低性能(LLM-based statement-level precision: 4.38%-32.88%)。

Conclusion: 研究结果强调了在可解释推荐中进行事实意识评估的需求，并为开发更值得信赖的解释系统提供了基础。

Abstract: Text-based explainable recommendation aims to generate natural-language explanations that justify item recommendations, to improve user trust and system transparency. Although recent advances leverage LLMs to produce fluent outputs, a critical question remains underexplored: are these explanations factually consistent with the available evidence? We introduce a comprehensive framework for evaluating the factual consistency of text-based explainable recommenders. We design a prompting-based pipeline that uses LLMs to extract atomic explanatory statements from reviews, thereby constructing a ground truth that isolates and focuses on their factual content. Applying this pipeline to five categories from the Amazon Reviews dataset, we create augmented benchmarks for fine-grained evaluation of explanation quality. We further propose statement-level alignment metrics that combine LLM- and NLI-based approaches to assess both factual consistency and relevance of generated explanations. Across extensive experiments on six state-of-the-art explainable recommendation models, we uncover a critical gap: while models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90), all our factuality metrics reveal alarmingly low performance (LLM-based statement-level precision: 4.38%-32.88%). These findings underscore the need for factuality-aware evaluation in explainable recommendation and provide a foundation for developing more trustworthy explanation systems.

</details>


### [4] [MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints](https://arxiv.org/abs/2512.24711)
*Kangyang Luo,Shuzheng Si,Yuzhuo Bai,Cheng Gao,Zhitong Wang,Cheng Huang,Yingli Shen,Yufeng Han,Wenhao Li,Cunliang Kong,Maosong Sun*

Main category: cs.IR

TL;DR: 提出了一种名为MEIC-DT的新方法，该方法是一种基于轻量级Transformer的双阈值、内存高效的增量聚类方法，旨在解决共指解析中效率与性能之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型时代，监督神经方法仍然是共指解析的最先进方法，但它们在增量聚类方面的潜力尚未被充分探索，特别是在处理长文本时如何平衡效率与性能成为一个关键挑战。

Method: MEIC-DT通过引入一种双阈值约束机制来精确控制给定内存预算内的Transformer输入规模，并结合统计感知驱逐策略（SAES）进行智能缓存管理。此外，还提出了内部正则化策略（IRP），通过对最具代表性的提及进行选择性压缩，从而保持语义完整性。

Result: 在常见基准测试上的广泛实验表明，MEIC-DT在严格的内存限制条件下实现了非常有竞争力的共指解析表现。

Conclusion: MEIC-DT提供了一种有效的方法，在保证高效率的同时也达到了优秀的共指解析效果，为处理长文本时遇到的内存限制问题提供了新的解决方案。

Abstract: In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints.

</details>


### [5] [MDiffFR: Modality-Guided Diffusion Generation for Cold-start Items in Federated Recommendation](https://arxiv.org/abs/2512.24715)
*Kang Fu,Honglei Zhang,Xuechao Zou,Yidong Li*

Main category: cs.IR

TL;DR: 本文提出了一种名为MDiffFR的新方法，通过在联邦推荐系统中采用基于生成的模态引导扩散模型来解决冷启动问题。该方法利用预训练的模态编码器提取特征作为条件信号指导反向去噪过程，以实现项目语义的一致性。实验结果表明，与现有方法相比，MDiffFR能够提供更强的隐私保护，并且在四个真实数据集上均优于所有基线方法。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统因能保留用户隐私而受到广泛关注，但其严格的隐私限制导致难以学习全局有效的表示形式，尤其是对于新（冷启动）项目而言挑战更大。现有的解决方案通常无法很好地建模不同的数据分布，容易造成嵌入不一致的问题。

Method: 提出了MDiffFR，一种针对联邦推荐系统中冷启动项目的新型生成式模态引导扩散方法。该方法在服务器端使用定制化的扩散模型为新项目生成嵌入，并将这些嵌入分发给客户端进行冷启动推理。此外，还部署了预训练的模态编码器来提取模态特征作为条件信号，以指导反向去噪过程。

Result: 理论分析验证了所提方法相较于现有的基于映射的方法具有更强的隐私保障。在四个实际数据集上的广泛实验表明，该方法在联邦推荐场景下始终优于所有基准方法。

Conclusion: MDiffFR通过引入生成式的模态引导扩散机制有效解决了联邦推荐系统中的冷启动问题，同时提供了更好的隐私保护性能。

Abstract: Federated recommendations (FRs) provide personalized services while preserving user privacy by keeping user data on local clients, which has attracted significant attention in recent years. However, due to the strict privacy constraints inherent in FRs, access to user-item interaction data and user profiles across clients is highly restricted, making it difficult to learn globally effective representations for new (cold-start) items. Consequently, the item cold-start problem becomes even more challenging in FRs. Existing solutions typically predict embeddings for new items through the attribute-to-embedding mapping paradigm, which establishes a fixed one-to-one correspondence between item attributes and their embeddings. However, this one-to-one mapping paradigm often fails to model varying data distributions and tends to cause embedding misalignment, as verified by our empirical studies. To this end, we propose MDiffFR, a novel generation-based modality-guided diffusion method for cold-start items in FRs. In this framework, we employ a tailored diffusion model on the server to generate embeddings for new items, which are then distributed to clients for cold-start inference. To align item semantics, we deploy a pre-trained modality encoder to extract modality features as conditional signals to guide the reverse denoising process. Furthermore, our theoretical analysis verifies that the proposed method achieves stronger privacy guarantees compared to existing mapping-based approaches. Extensive experiments on four real datasets demonstrate that our method consistently outperforms all baselines in FRs.

</details>


### [6] [OpenOneRec Technical Report](https://arxiv.org/abs/2512.24762)
*Guorui Zhou,Honghui Bao,Jiaming Huang,Jiaxin Deng,Jinghao Zhang,Junda She,Kuo Cai,Lejian Ren,Lu Ren,Qiang Luo,Qianqian Wang,Qigen Hu,Rongzhou Zhang,Ruiming Tang,Shiyao Wang,Wuchao Li,Xiangyu Wu,Xinchen Luo,Xingmei Wang,Yifei Hu,Yunfan Wu,Zhanyu Liu,Zhiyang Zhang,Zixing Zhang,Bo Chen,Bin Wen,Chaoyi Ma,Chengru Song,Chenglong Chu,Defu Lian,Fan Yang,Feng Jiang,Hongtao Cheng,Huanjie Wang,Kun Gai,Pengfei Zheng,Qiang Wang,Rui Huang,Siyang Mao,Tingting Gao,Wei Yuan,Yan Wang,Yang Zhou,Yi Su,Zexuan Cheng,Zhixin Ling,Ziming Li*

Main category: cs.IR

TL;DR: 本文提出了RecIF-Bench，一个全面的基准测试，覆盖了从基础预测到复杂推理的8个多样化任务，并开源了一个大规模训练数据集。同时，作者还开源了完整的训练流程，并发布了OneRec Foundation模型系列，在所有RecIF-Bench任务上均取得了最新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统虽然在模式匹配方面表现出色，但缺乏世界知识、推理能力和指令执行能力，且没有一个全面的基准来评估这些综合能力。

Method: 1) 提出RecIF-Bench，一个涵盖8种不同任务的综合性基准。
2) 开源包含9600万次互动的大规模训练数据集。
3) 开源整个训练流程，包括数据处理、协同预训练和后训练。
4) 发布OneRec Foundation（1.7B和8B）模型系列。

Result: OneRec Foundation模型在RecIF-Bench的所有任务中建立了新的最先进成果，并在转移到Amazon基准时，相比最强基线平均提高了26.8%的Recall@10。

Conclusion: 这项工作标志着朝着构建真正智能的推荐系统迈出了重要一步，尽管实现这一愿景仍面临重大技术和理论挑战。

Abstract: While the OneRec series has successfully unified the fragmented recommendation pipeline into an end-to-end generative framework, a significant gap remains between recommendation systems and general intelligence. Constrained by isolated data, they operate as domain specialists-proficient in pattern matching but lacking world knowledge, reasoning capabilities, and instruction following. This limitation is further compounded by the lack of a holistic benchmark to evaluate such integrated capabilities. To address this, our contributions are: 1) RecIF Bench & Open Data: We propose RecIF-Bench, a holistic benchmark covering 8 diverse tasks that thoroughly evaluate capabilities from fundamental prediction to complex reasoning. Concurrently, we release a massive training dataset comprising 96 million interactions from 160,000 users to facilitate reproducible research. 2) Framework & Scaling: To ensure full reproducibility, we open-source our comprehensive training pipeline, encompassing data processing, co-pretraining, and post-training. Leveraging this framework, we demonstrate that recommendation capabilities can scale predictably while mitigating catastrophic forgetting of general knowledge. 3) OneRec-Foundation: We release OneRec Foundation (1.7B and 8B), a family of models establishing new state-of-the-art (SOTA) results across all tasks in RecIF-Bench. Furthermore, when transferred to the Amazon benchmark, our models surpass the strongest baselines with an average 26.8% improvement in Recall@10 across 10 diverse datasets (Figure 1). This work marks a step towards building truly intelligent recommender systems. Nonetheless, realizing this vision presents significant technical and theoretical challenges, highlighting the need for broader research engagement in this promising direction.

</details>


### [7] [HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment](https://arxiv.org/abs/2512.24787)
*Yunsheng Pang,Zijian Liu,Yudong Li,Shaojie Zhu,Zijian Luo,Chenyun Yu,Sikai Wu,Shichen Shen,Cong Xu,Bin Wang,Kai Jiang,Hongyong Yu,Chengxiang Zhuo,Zang Li*

Main category: cs.IR

TL;DR: 本文提出了一种新的生成式板推荐框架HiGR，该框架结合了层次规划和列表偏好对齐，以解决现有自回归方法在语义项标记化和序列解码上的不足。实验表明，HiGR不仅在线下评估中比现有最先进方法提高了超过10%的推荐质量，并且推理速度提高了5倍，而且在线A/B测试中平均观看时间和平均视频浏览量分别增加了1.22%和1.73%。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归方法在处理板推荐时存在语义项标记化纠缠以及缺乏整体板规划的问题，导致推荐效率和质量受限。

Method: 提出了HiGR框架，通过引入残差量化与对比约束的自动编码器来实现可控生成；将生成过程分解为列表级规划阶段和项目级解码阶段；并通过列表偏好对齐目标直接利用隐式用户反馈优化板的质量。

Result: HiGR在大规模商业媒体平台上的实验显示，其相比最先进的方法在离线推荐质量上提高了超过10%，同时推理速度提升了5倍；在线A/B测试中，平均观看时间提高了1.22%，平均视频浏览次数提高了1.73%。

Conclusion: HiGR提供了一种更有效的生成式板推荐解决方案，能够在保持高质量推荐的同时显著提高系统效率。

Abstract: Slate recommendation, where users are presented with a ranked list of items simultaneously, is widely adopted in online platforms. Recent advances in generative models have shown promise in slate recommendation by modeling sequences of discrete semantic IDs autoregressively. However, existing autoregressive approaches suffer from semantically entangled item tokenization and inefficient sequential decoding that lacks holistic slate planning. To address these limitations, we propose HiGR, an efficient generative slate recommendation framework that integrates hierarchical planning with listwise preference alignment. First, we propose an auto-encoder utilizing residual quantization and contrastive constraints to tokenize items into semantically structured IDs for controllable generation. Second, HiGR decouples generation into a list-level planning stage for global slate intent, followed by an item-level decoding stage for specific item selection. Third, we introduce a listwise preference alignment objective to directly optimize slate quality using implicit user feedback. Experiments on our large-scale commercial media platform demonstrate that HiGR delivers consistent improvements in both offline evaluations and online deployment. Specifically, it outperforms state-of-the-art methods by over 10% in offline recommendation quality with a 5x inference speedup, while further achieving a 1.22% and 1.73% increase in Average Watch Time and Average Video Views in online A/B tests.

</details>


### [8] [RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment](https://arxiv.org/abs/2512.24943)
*Chenji Lu,Zhuo Chen,Hui Zhao,Zhenyi Wang,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: 本文提出了一个名为RAIR的中文数据集，旨在为搜索相关性评估提供标准化框架。该数据集基于真实场景构建，并分为三个子集来全面评估模型性能。实验表明，即使是最先进的GPT-5在RAIR上也面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估搜索相关性的基准缺乏足够的复杂度来进行全面的模型评价，导致行业内缺乏统一的相关性评价指标。为解决这一问题，作者们提出了一种新的基准测试方法。

Method: 开发了一个称为Rule-Aware benchmark with Image for Relevance assessment (RAIR)的数据集，它由三部分组成：一般子集、长尾难题子集以及视觉显著性子集，分别用来评估基本能力、极限性能和多模态理解能力。

Result: 通过使用14个开源及闭源模型对RAIR进行测试后发现，即使是表现最好的GPT-5也在该数据集上遇到了相当大的挑战。

Conclusion: RAIR不仅为搜索相关性提供了标准化的评估框架，还促进了通用语言模型和视觉语言模型评估的新见解。

Abstract: Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [A Comprehensive Study of Deep Learning Model Fixing Approaches](https://arxiv.org/abs/2512.23745)
*Hanmo You,Zan Wang,Zishuo Dong,Luanqi Mo,Jianjun Zhao,Junjie Chen*

Main category: cs.LG

TL;DR: 本文对16种最前沿的深度学习模型修复方法进行了大规模实证研究，涵盖了模型级、层级和神经元级三类，并全面评估了它们在修复效果、鲁棒性、公平性和向后兼容性等方面的表现。研究表明，模型级方法的修复效果优于其他级别，但没有一种方法能够在提高准确性的同时保持所有其他属性不受影响，这为未来的研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 鉴于深度学习系统在多个工业领域的广泛应用及其潜在故障可能给用户带来的重大风险，文章旨在通过综合评估当前主流的深度学习模型修复方法来理解其有效性及对其他关键属性的影响。

Method: 采用大规模实证研究的方式，选取了16种代表性的深度学习模型修复方法进行分析，这些方法被分为模型级、层级和神经元级三大类。为了确保评价的全面性和公正性，研究使用了多样化的数据集、模型架构以及应用场景，在统一的实验设置下开展测试。

Result: 研究发现，模型级的方法在修复效果上表现更佳；然而，不存在能够同时达到最佳修复性能、提升准确率并维持其余所有特性不变的理想方案。

Conclusion: 该研究揭示了现有深度学习模型修复技术的优势与局限，强调了学术界需要进一步探索如何减少这些副作用的重要性，同时也为未来相关领域的发展提供了有价值的参考。

Abstract: Deep Learning (DL) has been widely adopted in diverse industrial domains, including autonomous driving, intelligent healthcare, and aided programming. Like traditional software, DL systems are also prone to faults, whose malfunctioning may expose users to significant risks. Consequently, numerous approaches have been proposed to address these issues. In this paper, we conduct a large-scale empirical study on 16 state-of-the-art DL model fixing approaches, spanning model-level, layer-level, and neuron-level categories, to comprehensively evaluate their performance. We assess not only their fixing effectiveness (their primary purpose) but also their impact on other critical properties, such as robustness, fairness, and backward compatibility. To ensure comprehensive and fair evaluation, we employ a diverse set of datasets, model architectures, and application domains within a uniform experimental setup for experimentation. We summarize several key findings with implications for both industry and academia. For example, model-level approaches demonstrate superior fixing effectiveness compared to others. No single approach can achieve the best fixing performance while improving accuracy and maintaining all other properties. Thus, academia should prioritize research on mitigating these side effects. These insights highlight promising directions for future exploration in this field.

</details>


### [10] [A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios](https://arxiv.org/abs/2512.23748)
*Haley Rosso,Talea Mayo*

Main category: cs.LG

TL;DR: 本文综述了基于扩散模型的模拟基础推理（SBI）方法，从基本原理到实际应用。文章首先回顾了扩散模型的数学基础，并解释了如何通过条件分数实现无似然后验抽样。接着分析了扩散模型在处理科学数据常见问题（如模型错配、非结构化或无限维观测以及缺失数据）时相较于标准化流的优势与权衡。此外，还讨论了该领域未来的研究方向，特别是不确定性量化在概率地质物理模型中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 对于复杂的仿真问题，由于似然函数难以处理，通常无法使用经典的基于似然的方法来推断感兴趣的科学参数。基于仿真的推理(SBI)方法通过直接利用来自模拟器的样本来学习给定观测数据$\mathbf{x}_{\text{o}}$下的参数$\mathbfθ$的后验分布，从而避免了显式似然的需求。最近的工作引起了对扩散模型的关注——一种基于分数匹配和反向时间随机动力学的生成模型——作为执行SBI任务的一个灵活框架。

Method: 本文首先回顾了扩散建模的数学基础，包括前向噪声添加、反向时间SDE/ODE、概率流及去噪分数匹配，并解释了条件分数如何使无似然后验抽样成为可能。随后探讨了扩散模型如何解决神经后验/似然估计中标准化流存在的痛点，同时引入新的权衡（例如迭代采样成本）。重点在于基于扩散模型的SBI在科学数据常见的非理想条件下（如模型错配、非结构化或无限维度观察、数据缺失等）的表现稳健性。

Result: 文章综合了从薛定谔桥公式出发的基础方法、条件和顺序后验抽样器、适用于非结构化数据的摊销架构以及推理时先验适应等技术。在整个过程中采用了统一的符号体系，并强调了准确后验所需的条件与注意事项。

Conclusion: 本综述以讨论未解决问题结尾，特别关注于那些可能受益于基于扩散模型的SBI的概率地质物理模型中的不确定性量化应用。

Abstract: For complex simulation problems, inferring parameters of scientific interest often precludes the use of classical likelihood-based techniques due to intractable likelihood functions. Simulation-based inference (SBI) methods forego the need for explicit likelihoods by directly utilizing samples from the simulator to learn posterior distributions over parameters $\mathbfθ$ given observed data $\mathbf{x}_{\text{o}}$. Recent work has brought attention to diffusion models -- a type of generative model rooted in score matching and reverse-time stochastic dynamics -- as a flexible framework SBI tasks. This article reviews diffusion-based SBI from first principles to applications in practice. We first recall the mathematical foundations of diffusion modeling (forward noising, reverse-time SDE/ODE, probability flow, and denoising score matching) and explain how conditional scores enable likelihood-free posterior sampling. We then examine where diffusion models address pain points of normalizing flows in neural posterior/likelihood estimation and where they introduce new trade-offs (e.g., iterative sampling costs). The key theme of this review is robustness of diffusion-based SBI in non-ideal conditions common to scientific data: misspecification (mismatch between simulated training data and reality), unstructured or infinite-dimensional observations, and missingness. We synthesize methods spanning foundations drawing from Schrodinger-bridge formulations, conditional and sequential posterior samplers, amortized architectures for unstructured data, and inference-time prior adaptation. Throughout, we adopt consistent notation and emphasize conditions and caveats required for accurate posteriors. The review closes with a discussion of open problems with an eye toward applications of uncertainty quantification for probabilistic geophysical models that may benefit from diffusion-based SBI.

</details>


### [11] [Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents](https://arxiv.org/abs/2512.23749)
*Amin Sadri,M Maruf Hossain*

Main category: cs.LG

TL;DR: 本文提出了一种名为坐标矩阵机（CM$^2$）的小型模型，该模型通过学习文档结构并利用这些信息进行文档分类来增强人类智能。与需要大量预训练和强大GPU支持的现代'红色AI'趋势不同，CM$^2$作为一种绿色AI解决方案，仅需每个类别一个样本即可实现人类级别的概念学习，并且在准确性、环保性、计算效率等方面具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 作者指出，人类能够从单个例子中学习新概念，而机器学习算法通常需要数百个样本来学习单一概念。受此启发，研究旨在开发一种能够模仿人类这种高效学习方式的新模型，同时减少对大规模数据集和高性能计算资源的需求。

Method: 提出了坐标矩阵机（CM$^2$），这是一种专门设计用于识别文档结构特征的小型模型。它专注于提取人类可能认为重要的结构性质，而不是依赖于详尽的语义向量或复杂的深度学习架构。

Result: 实验表明，CM$^2$不仅能够在非常少的数据下达到高精度（单样本学习），而且提供了几何和结构智能、环境可持续性、专为仅CPU环境优化、内在可解释性、快速计算及低延迟等优点。此外，该模型还表现出对抗不平衡类别的鲁棒性以及经济可行性。

Conclusion: 坐标矩阵机（CM$^2$）提供了一种新的方法来解决机器学习中的数据饥饿问题，特别是对于文档分类任务而言。作为绿色AI的一个实例，CM$^2$展示了如何在不牺牲性能的前提下实现更加环保和成本效益的解决方案。

Abstract: Human-level concept learning argues that humans typically learn new concepts from a single example, whereas machine learning algorithms typically require hundreds of samples to learn a single concept. Our brain subconsciously identifies important features and learns more effectively. \vspace*{6pt}
  Contribution: In this paper, we present the Coordinate Matrix Machine (CM$^2$). This purpose-built small model augments human intelligence by learning document structures and using this information to classify documents. While modern "Red AI" trends rely on massive pre-training and energy-intensive GPU infrastructure, CM$^2$ is designed as a Green AI solution. It achieves human-level concept learning by identifying only the structural "important features" a human would consider, allowing it to classify very similar documents using only one sample per class.
  Advantage: Our algorithm outperforms traditional vectorizers and complex deep learning models that require larger datasets and significant compute. By focusing on structural coordinates rather than exhaustive semantic vectors, CM$^2$ offers: 1. High accuracy with minimal data (one-shot learning) 2. Geometric and structural intelligence 3. Green AI and environmental sustainability 4. Optimized for CPU-only environments 5. Inherent explainability (glass-box model) 6. Faster computation and low latency 7. Robustness against unbalanced classes 8. Economic viability 9. Generic, expandable, and extendable

</details>


### [12] [Geometric Scaling of Bayesian Inference in LLMs](https://arxiv.org/abs/2512.23752)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: cs.LG

TL;DR: 研究发现，现代生产级语言模型（如Pythia、Phi-2、Llama-3和Mistral系列）保留了在受控环境下训练的小型变压器所展示的贝叶斯推理所需的几何基底特性。这些模型的最后一层值表示沿着与预测熵高度相关的单一主导轴组织，并且领域受限提示将这种结构简化为合成设置中观察到的低维流形。通过针对Pythia-410M中的熵对齐轴进行干预实验，进一步表明该几何结构是不确定性的一种特权读出方式，而非唯一的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探索在受控条件下小型变压器所展现的确切贝叶斯推理及几何基底特征是否同样存在于实际应用级别的语言模型中。

Method: 通过对Pythia、Phi-2、Llama-3以及Mistral家族的语言模型进行分析，特别是研究它们最后一层值表示如何根据预测熵来组织；同时，在Pythia-410M上执行特定干预以探索熵对齐轴的作用。

Result: 发现现代语言模型确实保留了支持贝叶斯推理的几何基底，并且它们会沿此基底组织近似贝叶斯更新。此外，虽然针对熵对齐轴的干预能够破坏局部不确定性几何，但并未导致相应的行为退化，表明该几何结构更多地作为不确定性的特殊指示器而非关键计算障碍。

Conclusion: 这项研究表明，即使是在复杂的应用场景下，现代语言模型也能够保持一种有助于实现贝叶斯推断的内部几何结构。

Abstract: Recent work has shown that small transformers trained in controlled "wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings.
  To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate.

</details>


### [13] [A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit](https://arxiv.org/abs/2512.23766)
*Karim Salta,Michael Kirby,Chris Peterson*

Main category: cs.LG

TL;DR: 本研究提出了一种新的子空间聚类算法，该算法使用可训练的原型（称为最佳拟合舒伯特变体SVBF）代替子空间均值，从而在合成数据、图像、光谱和视频动作数据上提高了聚类纯度，同时保持了下游分析所需的数学结构。


<details>
  <summary>Details</summary>
Motivation: 在许多分类和聚类任务中，为数据集或一个簇计算几何代表（如均值或中位数）是很有用的。当数据集由子空间表示时，这些代表就成为格拉斯曼流形或标志流形上的点，并且距离通常通过主角度来诱导。研究者希望改进现有的子空间聚类方法，以提高聚类纯度并保留数学结构。

Method: 研究人员引入了一种新的子空间聚类算法，它利用一个可训练的原型——最佳拟合舒伯特变体(SVBF)，这是一种尽可能接近于与每个簇成员至少在一个固定方向相交的子空间。此SVBF-LBG方案被集成到Linde-Buzo-Grey (LBG)流程中。

Result: 实验结果表明，SVBF-LBG方案在合成数据、图像、光谱和视频动作数据等不同类型的资料上都获得了更高的聚类纯度。

Conclusion: 提出的基于最佳拟合舒伯特变体的子空间聚类算法不仅提高了聚类纯度，还维持了对后续分析至关重要的数学结构。

Abstract: In many classification and clustering tasks, it is useful to compute a geometric representative for a dataset or a cluster, such as a mean or median. When datasets are represented by subspaces, these representatives become points on the Grassmann or flag manifold, with distances induced by their geometry, often via principal angles. We introduce a subspace clustering algorithm that replaces subspace means with a trainable prototype defined as a Schubert Variety of Best Fit (SVBF) - a subspace that comes as close as possible to intersecting each cluster member in at least one fixed direction. Integrated in the Linde-Buzo-Grey (LBG) pipeline, this SVBF-LBG scheme yields improved cluster purity on synthetic, image, spectral, and video action data, while retaining the mathematical structure required for downstream analysis.

</details>


### [14] [HINTS: Extraction of Human Insights from Time-Series Without External Sources](https://arxiv.org/abs/2512.23755)
*Sheo Yon Jhin,Noseong Park*

Main category: cs.LG

TL;DR: 本文提出了一种名为HINTS的自监督学习框架，该框架能够从时间序列残差中内生地提取潜在的人类因素，而无需外部数据。实验结果表明HINTS能持续提高预测准确性，并通过多个案例研究和消融研究验证了其可解释性及实用性。


<details>
  <summary>Details</summary>
Motivation: 当前许多时间序列预测模型依赖于新闻、社交媒体等外部来源来捕捉人类因素，但这种方法在财务、计算以及实际应用方面带来了高昂的数据依赖成本。为解决这一问题，作者们旨在开发一种不需要外部数据就能从时间序列内部提取这些复杂人类因素的新方法。

Method: 提出了HINTS，一个利用Friedkin-Johnsen意见动态模型作为结构归纳偏置来自监督学习框架，用以从时间序列残差中提取随时间变化的社会影响、记忆与偏差模式等潜在人类因素。然后将提取到的因素以注意力图的形式集成到最先进的基础模型中。

Result: 通过对九个真实世界和基准数据集进行测试，HINTS被证明可以一致地改善预测准确性。此外，通过几个案例研究和消融研究表明，所提取因素与现实世界事件之间存在强语义对齐，进一步证实了HINTS的有效性和实用性。

Conclusion: HINTS提供了一种有效的方法，能够在不增加额外数据依赖成本的情况下，从时间序列数据本身中提取并整合人类因素，从而提高了预测性能。它不仅增强了模型的预测能力，还提供了良好的可解释性。

Abstract: Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.

</details>


### [15] [Neural Optimal Design of Experiment for Inverse Problems](https://arxiv.org/abs/2512.23763)
*John E. Darges,Babak Maboudi Afkham,Matthias Chung*

Main category: cs.LG

TL;DR: 提出了一种基于学习的实验最优设计框架（NODE），该框架在单个优化循环中联合训练神经重建模型和表示传感器位置等的连续设计变量，从而直接优化测量位置，避免了经典双层优化和间接稀疏正则化。


<details>
  <summary>Details</summary>
Motivation: 为了解决反问题中的最优实验设计问题，同时避免经典的双层优化和间接稀疏性正则化带来的挑战。

Method: 通过引入Neural Optimal Design of Experiments (NODE)，在一个优化循环内共同训练神经重建模型与代表如传感器位置、采样时间或测量角度的一组固定预算下的连续设计变量，直接针对测量位置进行优化。

Result: NODE方法在可解析指数增长基准测试、MNIST图像采样以及真实世界稀疏视角X射线CT示例上均优于基线方法，显示出改进的重建准确性和任务特定性能。

Conclusion: NODE提供了一个有效且高效的解决方案，适用于需要最佳实验设计的各种逆问题场景，并且在减少计算复杂度的同时提高了重建精度。

Abstract: We introduce Neural Optimal Design of Experiments, a learning-based framework for optimal experimental design in inverse problems that avoids classical bilevel optimization and indirect sparsity regularization. NODE jointly trains a neural reconstruction model and a fixed-budget set of continuous design variables representing sensor locations, sampling times, or measurement angles, within a single optimization loop. By optimizing measurement locations directly rather than weighting a dense grid of candidates, the proposed approach enforces sparsity by design, eliminates the need for l1 tuning, and substantially reduces computational complexity. We validate NODE on an analytically tractable exponential growth benchmark, on MNIST image sampling, and illustrate its effectiveness on a real world sparse view X ray CT example. In all cases, NODE outperforms baseline approaches, demonstrating improved reconstruction accuracy and task-specific performance.

</details>


### [16] [FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading](https://arxiv.org/abs/2512.23773)
*Molei Qin,Xinyu Cai,Yewen Li,Haochong Xia,Chuqiao Zong,Shuo Sun,Xinrun Wang,Bo An*

Main category: cs.LG

TL;DR: A novel reinforcement learning framework, FineFT, is introduced for futures trading, which addresses the challenges of high leverage and market unpredictability. Through a three-stage process, it ensures stable training, proper risk management, and improved profitability, outperforming existing methods in several key financial metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to address the challenges of applying reinforcement learning (RL) to the futures market, which is characterized by high leverage and liquidity. The two main issues are the amplified reward fluctuations due to high leverage, leading to unstable training, and the lack of self-awareness regarding the model's capability limits, which can result in significant losses during unexpected market events.

Method: The proposed method, FineFT, is a three-stage ensemble RL framework. In the first stage, it uses an ensemble of Q learners selectively updated by ensemble TD errors for better convergence. In the second stage, it filters these Q-learners based on their performance and trains VAEs to recognize the boundaries of the learners' capabilities. In the third stage, it selects from the filtered ensemble or a conservative policy, guided by the trained VAEs, to ensure both profitability and risk management under new market conditions.

Result: Experimental results show that FineFT excels in a high-frequency trading environment with 5x leverage, surpassing 12 state-of-the-art (SOTA) baselines across six financial metrics. It reduces risk by more than 40% compared to the next best performer while also achieving higher profitability. The study also demonstrates that the selective updating of agents and the use of VAEs for identifying learner capabilities contribute to the reduction of maximum drawdown and the improvement of overall performance.

Conclusion: FineFT, a novel three-stage ensemble RL framework, outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by over 40% while maintaining superior profitability. The selective update mechanism and VAEs-based routing are key to its success.

Abstract: Futures are contracts obligating the exchange of an asset at a predetermined date and price, notable for their high leverage and liquidity and, therefore, thrive in the Crypto market. RL has been widely applied in various quantitative tasks. However, most methods focus on the spot and could not be directly applied to the futures market with high leverage because of 2 challenges. First, high leverage amplifies reward fluctuations, making training stochastic and difficult to converge. Second, prior works lacked self-awareness of capability boundaries, exposing them to the risk of significant loss when encountering new market state (e.g.,a black swan event like COVID-19). To tackle these challenges, we propose the Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading (FineFT), a novel three-stage ensemble RL framework with stable training and proper risk management. In stage I, ensemble Q learners are selectively updated by ensemble TD errors to improve convergence. In stage II, we filter the Q-learners based on their profitabilities and train VAEs on market states to identify the capability boundaries of the learners. In stage III, we choose from the filtered ensemble and a conservative policy, guided by trained VAEs, to maintain profitability and mitigate risk with new market states. Through extensive experiments on crypto futures in a high-frequency trading environment with high fidelity and 5x leverage, we demonstrate that FineFT outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by more than 40% while achieving superior profitability compared to the runner-up. Visualization of the selective update mechanism shows that different agents specialize in distinct market dynamics, and ablation studies certify routing with VAEs reduces maximum drawdown effectively, and selective update improves convergence and performance.

</details>


### [17] [Improved Bounds for Private and Robust Alignment](https://arxiv.org/abs/2512.23816)
*Wenqian Weng,Yi He,Xingyu Zhou*

Main category: cs.LG

TL;DR: 本文从理论角度研究了语言模型在隐私和鲁棒性条件下的对齐问题，提出了在线和离线设置中次优差距的上限。对于仅考虑隐私的情况，表明使用MLE风格算法的日志损失接近最优率；对于同时考虑隐私和破坏的情况，证明现有离线算法能够提供更强保证，并为私有且鲁棒的在线对齐提供了首组结果。


<details>
  <summary>Details</summary>
Motivation: 作者旨在探索在存在隐私限制和/或对抗性破坏的情况下，如何实现语言模型的私密性和鲁棒性对齐，特别是在日志损失与平方损失下新的统一收敛保证。

Method: 通过建立在线和离线环境下的次优差距上界来分析两种不同场景：首先考虑隐私然后是破坏（privacy-first and corruption-first）。研究了针对仅隐私、仅破坏以及两者结合情况下的处理方法。

Result: 发现对于仅隐私设定，采用MLE样式的算法可以达到几乎最优的结果；而对于隐私加破坏的联合设定，则展示了现有离线算法比之前认为的更强大，不仅关于破坏水平也关于隐私参数。此外，还首次提出了针对私有和鲁棒在线对齐的结果。

Conclusion: 本研究通过引入新的统一收敛保障，为学习理论和统计学中的私有及鲁棒性对齐提供了有价值的见解，尤其是在面对数据隐私约束和潜在敌手干扰时。

Abstract: In this paper, we study the private and robust alignment of language models from a theoretical perspective by establishing upper bounds on the suboptimality gap in both offline and online settings. We consider preference labels subject to privacy constraints and/or adversarial corruption, and analyze two distinct interplays between them: privacy-first and corruption-first. For the privacy-only setting, we show that log loss with an MLE-style algorithm achieves near-optimal rates, in contrast to conventional wisdom. For the joint privacy-and-corruption setting, we first demonstrate that existing offline algorithms in fact provide stronger guarantees -- simultaneously in terms of corruption level and privacy parameters -- than previously known, which further yields improved bounds in the corruption-only regime. In addition, we also present the first set of results for private and robust online alignment. Our results are enabled by new uniform convergence guarantees for log loss and square loss under privacy and corruption, which we believe have broad applicability across learning theory and statistics.

</details>


### [18] [Exploiting the Prior of Generative Time Series Imputation](https://arxiv.org/abs/2512.23832)
*YuYang Miao,Chang Li,Zehua Chen*

Main category: cs.LG

TL;DR: 提出了Bridge-TS，一种新的时间序列填补方法，通过引入专家先验和组合先验来提高生成模型的准确性。实验表明，该方法在多个基准数据集上达到了更高的填补精度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于生成模型的时间序列填补方法由于其先验信息不足以反映真实目标，导致生成过程负担增加且填补精度有限。

Method: 设计了名为Bridge-TS的方法，其中包括利用预训练的基于Transformer模块作为专家来确定性地估计缺失值（专家先验）以及结合多个预训练模型提供的不同估计结果以实现从组合先验到目标的数据生成过程（组合先验）。

Result: 在ETT、Exchange和Weather等几个基准数据集上的实验显示，Bridge-TS在均方误差(MSE)和平均绝对误差(MAE)方面达到了新的填补准确度记录。

Conclusion: 通过改进用于生成时间序列填补的先验，Bridge-TS证明了其在提升填补准确性方面的优越性。

Abstract: Time series imputation, i.e., filling the missing values of a time recording, finds various applications in electricity, finance, and weather modelling. Previous methods have introduced generative models such as diffusion probabilistic models and Schrodinger bridge models to conditionally generate the missing values from Gaussian noise or directly from linear interpolation results. However, as their prior is not informative to the ground-truth target, their generation process inevitably suffer increased burden and limited imputation accuracy. In this work, we present Bridge-TS, building a data-to-data generation process for generative time series imputation and exploiting the design of prior with two novel designs. Firstly, we propose expert prior, leveraging a pretrained transformer-based module as an expert to fill the missing values with a deterministic estimation, and then taking the results as the prior of ground truth target. Secondly, we explore compositional priors, utilizing several pretrained models to provide different estimation results, and then combining them in the data-to-data generation process to achieve a compositional priors-to-target imputation process. Experiments conducted on several benchmark datasets such as ETT, Exchange, and Weather show that Bridge-TS reaches a new record of imputation accuracy in terms of mean square error and mean absolute error, demonstrating the superiority of improving prior for generative time series imputation.

</details>


### [19] [Trellis: Learning to Compress Key-Value Memory in Attention Models](https://arxiv.org/abs/2512.23852)
*Mahdi Karami,Ali Behrouz,Praneeth Kacham,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer架构Trellis，该架构具有固定内存并能在测试时动态压缩其键值内存。通过在线梯度下降过程和遗忘门机制，Trellis能够递归更新压缩后的内存，并在处理长序列任务时表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer模型面临计算复杂度高及注意力机制中键值缓存不断增长的问题。为了解决这些问题，提出了Trellis架构，旨在通过使用固定大小的内存并学习如何在测试时动态压缩键值内存来改善效率与性能。

Method: Trellis架构用固定大小的内存替换了标准的键值缓存，并训练了一个两阶段的循环压缩机制以将新键值存储到内存中。该方法利用带有遗忘门的在线梯度下降程序，使得压缩后的内存可以递归地更新，同时学会保留来自输入标记的重要上下文信息。

Result: 广泛的实验表明，Trellis架构在语言建模、常识推理、召回密集型任务以及时间序列分析等多个领域超越了强大的基线模型。特别是当序列长度增加时，其性能提升更加显著，突显出其在长上下文应用中的潜力。

Conclusion: Trellis作为一种创新的Transformer架构，成功解决了传统模型中存在的问题，不仅提高了计算效率，还增强了模型处理长序列的能力，为未来的研究提供了新的方向。

Abstract: Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.

</details>


### [20] [Flow Matching Neural Processes](https://arxiv.org/abs/2512.23853)
*Hussen Abu Hamad,Dan Rosenbaum*

Main category: cs.LG

TL;DR: 本文介绍了一种基于流匹配的新型神经过程模型，该模型易于实现，并且可以通过ODE求解器从条件分布中采样。此外，通过调整ODE求解器中的步数，模型还能在准确性和运行时间之间提供可控的权衡。实验表明，该模型在合成的一维高斯过程数据、二维图像以及真实天气数据等多个基准测试上优于现有的最先进方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有神经过程模型在实施复杂度和采样效率上的局限性，作者提出了一种新的基于流匹配的神经过程模型。这种方法不仅简化了模型的实现过程，还允许使用ODE求解器直接进行条件分布采样，无需额外的条件化手段。同时，新模型能够根据实际需要灵活调整精确度与计算成本之间的平衡点。

Method: 本文提出的神经过程模型采用了流匹配这一生成建模范式。该模型遵循NP训练框架，能够对任意数据点上的条件分布做出预估。特别地，它利用ODE（常微分方程）求解技术来执行条件分布采样任务，这使得用户可以通过调节ODE求解步骤数目来控制预测精度与计算时间之间的权衡。

Result: 实验结果表明，所提出的模型在多个基准数据集上均取得了优于现有最先进神经过程方法的表现，包括合成的一维高斯过程数据、二维图像处理及真实世界中的天气数据分析任务。

Conclusion: 基于流匹配的新颖神经过程模型因其简洁的设计理念、高效的采样机制以及良好的性能表现，在不同应用场景下展现了其作为新一代神经过程解决方案的巨大潜力。

Abstract: Neural processes (NPs) are a class of models that learn stochastic processes directly from data and can be used for inference, sampling and conditional sampling. We introduce a new NP model based on flow matching, a generative modeling paradigm that has demonstrated strong performance on various data modalities. Following the NP training framework, the model provides amortized predictions of conditional distributions over any arbitrary points in the data. Compared to previous NP models, our model is simple to implement and can be used to sample from conditional distributions using an ODE solver, without requiring auxiliary conditioning methods. In addition, the model provides a controllable tradeoff between accuracy and running time via the number of steps in the ODE solver. We show that our model outperforms previous state-of-the-art neural process methods on various benchmarks including synthetic 1D Gaussian processes data, 2D images, and real-world weather data.

</details>


### [21] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 本研究对比了在资源受限环境下，使用Word2Vec、GloVe和句子转换器等基于嵌入的方法进行金融新闻情感分类的效果。实验结果显示，尽管验证指标表现良好，但在测试集上的性能却低于简单的基线模型，并且预训练的嵌入只有在达到一定数据充足阈值时才能产生较好效果。对于数据稀缺的情况，建议考虑少样本学习、数据增强或词典增强的混合方法。


<details>
  <summary>Details</summary>
Motivation: 由于标准自然语言处理方法在应用于小规模数据集时面临重大挑战，本研究旨在评估基于嵌入的方法在资源受限条件下对金融新闻情感分类的有效性。

Method: 采用了Word2Vec、GloVe以及句子转换器作为文本表示方法，并结合梯度提升算法来对人工标注的新闻标题进行分析。

Result: 观察到验证与测试性能之间存在显著差距，即使是在强大的验证指标下模型的表现也差于简单基线；预训练的嵌入仅在超过特定的数据充分性阈值时才显示出价值；小型验证集导致模型选择过程中出现过拟合现象。

Conclusion: 单靠提高嵌入质量无法解决情感分类中根本的数据稀缺问题。对于资源有限的操作者来说，在标记样本稀缺的情况下应该探索如少样本学习、数据扩增或词典增强的混合方法等替代方案。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [22] [Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding](https://arxiv.org/abs/2512.23858)
*Yue Guan,Changming Yu,Shihan Fang,Weiming Hu,Zaifeng Pan,Zheng Wang,Zihan Liu,Yangjie Zhou,Yufei Ding,Minyi Guo,Jingwen Leng*

Main category: cs.LG

TL;DR: 介绍了Yggdrasil系统，它通过上下文感知的树形草稿和对编译器友好的执行来实现延迟最优的推测解码，从而提高了大型语言模型（LLM）推理过程中的推测解码性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法由于动态推测与静态运行时假设之间的不匹配而导致性能不佳。

Method: 提出了Yggdrasil，一个共同设计的系统，该系统利用了等增长树结构、延迟感知优化目标以及基于阶段的调度策略，以支持未修改的语言模型并减少开销。

Result: Yggdrasil在多种硬件设置下实现了比现有最先进基线高达3.98倍的速度提升。

Conclusion: Yggdrasil展示了通过专门设计的方法解决现有推测解码问题的有效性，并显著提高了大型语言模型推理的速度。

Abstract: Speculative decoding improves LLM inference by generating and verifying multiple tokens in parallel, but existing systems suffer from suboptimal performance due to a mismatch between dynamic speculation and static runtime assumptions. We present Yggdrasil, a co-designed system that enables latency-optimal speculative decoding through context-aware tree drafting and compiler-friendly execution. Yggdrasil introduces an equal-growth tree structure for static graph compatibility, a latency-aware optimization objective for draft selection, and stage-based scheduling to reduce overhead. Yggdrasil supports unmodified LLMs and achieves up to $3.98\times$ speedup over state-of-the-art baselines across multiple hardware setups.

</details>


### [23] [Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining](https://arxiv.org/abs/2512.23862)
*Ruizhe Huang,Kexuan Zhang,Yihao Fang,Baifeng Yu*

Main category: cs.LG

TL;DR: 本研究探索了小型语言模型（SLM）的小规模预训练，以实现有限数据和计算资源的有效利用、提高低资源环境下的可访问性并降低成本。通过使用Infini-attention机制，即使在长上下文检索中性能有所下降，该模型相比基线仍能表现出高达31%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过小规模预训练技术来提高小型语言模型（SLMs）的效率，特别是在数据和算力受限的情况下，并且通过引入Infini-attention改善模型处理长上下文的能力。

Method: 采用3亿参数的LLaMA模型作为基础，结合Infini-attention进行预训练。此方法通过对过去段落构建压缩记忆同时保持局部注意力，以增强紧凑模型中的长上下文外推能力。

Result: 实验表明，所提出的模型不仅展示了训练稳定性，而且在长上下文检索任务上超越了基线模型。尽管随着序列长度增加导致多次记忆压缩后检索准确性降低，但总体上Infini-attention显著提升了SLM的表现。

Conclusion: 研究表明，在SLMs中实现强大的长上下文能力可以从类似Infini-attention这样的架构记忆中受益。

Abstract: This study investigates small-scale pretraining for Small Language Models (SLMs) to enable efficient use of limited data and compute, improve accessibility in low-resource settings and reduce costs. To enhance long-context extrapolation in compact models, we focus on Infini-attention, which builds a compressed memory from past segments while preserving local attention. In our work, we conduct an empirical study using 300M-parameter LLaMA models pretrained with Infini-attention. The model demonstrates training stability and outperforms the baseline in long-context retrieval. We identify the balance factor as a key part of the model performance, and we found that retrieval accuracy drops with repeated memory compressions over long sequences. Even so, Infini-attention still effectively compensates for the SLM's limited parameters. Particularly, despite performance degradation at a 16,384-token context, the Infini-attention model achieves up to 31% higher accuracy than the baseline. Our findings suggest that achieving robust long-context capability in SLMs benefits from architectural memory like Infini-attention.

</details>


### [24] [Max-Entropy Reinforcement Learning with Flow Matching and A Case Study on LQR](https://arxiv.org/abs/2512.23870)
*Yuyang Zhang,Yang Hu,Bo Dai,Na Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于流模型参数化的SAC算法变体，通过即时变量变换技术评估策略，并使用一种在线变体的流匹配方法更新策略，该方法仅需要从用户指定的采样分布中获取样本。研究还对ISFM进行了理论分析，并在最大熵线性二次调节器问题上进行了案例研究，证明了所提算法能够学习到最优动作分布。


<details>
  <summary>Details</summary>
Motivation: 现有的软演员-评论家（SAC）算法在实践中为了效率而采用简单的策略类来近似能量基策略，这牺牲了表达能力和鲁棒性。

Method: 提出了SAC算法的一个变体，该变体利用流模型丰富的表达能力来参数化策略。算法中，通过瞬时变量变换技术来评估基于流的策略，并且用本论文开发的一种称为重要性采样流匹配(ISFM)的在线流匹配变体来更新策略。

Result: 发展了针对ISFM的理论分析，描述了不同选择的采样分布如何影响学习效率。此外，在最大熵线性二次调节器问题上的案例研究表明，所提出的算法可以学习到最优动作分布。

Conclusion: 通过将策略参数化为基于流的模型并引入ISFM方法，本文所提出的SAC算法变体提高了策略的表达力和鲁棒性，同时保持了学习过程的有效性。

Abstract: Soft actor-critic (SAC) is a popular algorithm for max-entropy reinforcement learning. In practice, the energy-based policies in SAC are often approximated using simple policy classes for efficiency, sacrificing the expressiveness and robustness. In this paper, we propose a variant of the SAC algorithm that parameterizes the policy with flow-based models, leveraging their rich expressiveness. In the algorithm, we evaluate the flow-based policy utilizing the instantaneous change-of-variable technique and update the policy with an online variant of flow matching developed in this paper. This online variant, termed importance sampling flow matching (ISFM), enables policy update with only samples from a user-specified sampling distribution rather than the unknown target distribution. We develop a theoretical analysis of ISFM, characterizing how different choices of sampling distributions affect the learning efficiency. Finally, we conduct a case study of our algorithm on the max-entropy linear quadratic regulator problems, demonstrating that the proposed algorithm learns the optimal action distribution.

</details>


### [25] [Rethinking Dense Linear Transformations: Stagewise Pairwise Mixing (SPM) for Near-Linear Training in Neural Networks](https://arxiv.org/abs/2512.23905)
*Peter Farag*

Main category: cs.LG

TL;DR: 本文介绍了一种名为Stagewise Pairwise Mixers (SPM)的结构化线性运算符，它能够以较低的时间复杂度和参数量替代密集型线性层，并且具有显式的组合归纳偏置，有助于提高模型在特定任务上的泛化能力。实验证明了该方法在减少计算成本的同时还能保持甚至提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决现代机器学习模型中由于使用密集型线性层而导致的高计算与参数开销问题。这些层不仅具有二次复杂度，而且往往与其所学表示的组成结构不匹配。

Method: 提出了一种新的结构化线性算子——Stagewise Pairwise Mixers (SPPM)，通过一系列稀疏成对混合阶段来代替传统的密集矩阵。SPM可以在O(nL)时间内实现全局线性变换，其中L通常是常数或log_2n，并支持精确闭式前向和后向计算。

Result: 实验表明，SPM能够在保证甚至改善模型准确性的同时大幅降低实际运行时间成本。此外，SPM还展示了在结构化学习问题上优于传统方法的表现。

Conclusion: SPM作为一种有效的替代方案，可以显著降低计算和参数成本，同时通过其层次结构引入明确的组合归纳偏差，从而提高了模型在与任务结构相匹配时的泛化能力。

Abstract: Dense linear layers are a dominant source of computational and parametric cost in modern machine learning models, despite their quadratic complexity and often being misaligned with the compositional structure of learned representations. We introduce Stagewise Pairwise Mixers (SPM), a structured linear operator that replaces dense matrices with a composition of sparse pairwise-mixing stages. An SPM layer implements a global linear transformation in $O(nL)$ time with $O(nL)$ parameters, where $L$ is typically constant or $log_2n$, and admits exact closed-form forward and backward computations. SPM is designed as a drop-in replacement for dense linear layers in feedforward networks, recurrent architectures, attention mechanisms, etc. We derive complete forward and backward expressions for two parameterizations: an orthogonal norm-preserving rotation-based variant and a fully general $2 \times 2$ mixing variant. Beyond computational savings, the stagewise structure of SPM induces an explicit compositional inductive bias that constrains model capacity and improves generalization when aligned with task structure. We present proof-of-concept experiments demonstrating substantial reductions in wall-clock cost and improved accuracy on structured learning problems, while retaining competitive performance on real-world benchmarks.

</details>


### [26] [Interactive Machine Learning: From Theory to Scale](https://arxiv.org/abs/2512.23924)
*Yinglun Zhu*

Main category: cs.LG

TL;DR: 该论文研究了交互式机器学习，开发了新的算法原则，并为有噪声数据和丰富模型类别的主动学习、具有大动作空间的序列决策制定以及在部分反馈下的模型选择建立了基本限制。这些成果包括首次实现了无需低噪声假设即可达到指数级标签节省的计算高效主动学习算法；首个保证与动作空间大小无关的有效通用上下文强盗算法；以及首次对序列决策中模型选择的基本成本进行了严格表征。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在许多应用领域取得了显著的成功，但其最有效的方法往往依赖于大量的标记数据或广泛的在线互动。然而，在实践中，获取高质量标签及通过试错做决定可能是昂贵、耗时或有风险的，特别是在大规模或高风险环境中。

Method: 研究了三个维度上的交互式学习：处理含噪声数据和复杂模型类别的主动学习、具有大动作空间的顺序决策制定，以及在部分反馈条件下的模型选择。

Result: 开发出的新算法原则不仅在统计上是最佳的，而且计算效率也高，并且还提供了将交互式学习方法部署到大规模现实世界环境中的原则性指导。

Conclusion: 本论文通过提出统计最优且计算高效的算法，推进了交互式学习的理论基础，同时为在大规模实际场景中部署交互式学习方法提供了原则性的指导。

Abstract: Machine learning has achieved remarkable success across a wide range of applications, yet many of its most effective methods rely on access to large amounts of labeled data or extensive online interaction. In practice, acquiring high-quality labels and making decisions through trial-and-error can be expensive, time-consuming, or risky, particularly in large-scale or high-stakes settings. This dissertation studies interactive machine learning, in which the learner actively influences how information is collected or which actions are taken, using past observations to guide future interactions. We develop new algorithmic principles and establish fundamental limits for interactive learning along three dimensions: active learning with noisy data and rich model classes, sequential decision making with large action spaces, and model selection under partial feedback. Our results include the first computationally efficient active learning algorithms achieving exponential label savings without low-noise assumptions; the first efficient, general-purpose contextual bandit algorithms whose guarantees are independent of the size of the action space; and the first tight characterizations of the fundamental cost of model selection in sequential decision making. Overall, this dissertation advances the theoretical foundations of interactive learning by developing algorithms that are statistically optimal and computationally efficient, while also providing principled guidance for deploying interactive learning methods in large-scale, real-world settings.

</details>


### [27] [Improved Balanced Classification with Theoretically Grounded Loss Functions](https://arxiv.org/abs/2512.23947)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 该论文引入并研究了两种先进的替代损失函数：广义对数调整(GLA)损失函数和广义类别感知加权(GCA)损失，以解决多类分类中的类别不平衡问题。GLA损失函数对于完整假设集是H-一致的，但其一致性界限依赖于最小类别概率；而GCA损失函数对于有界或完整的假设集都是一致的，并且在非常不平衡的情况下提供更强的理论保证。实验结果表明，两者均优于简单的类别加权损失，在常见基准测试中GLA表现略好，而在高度不平衡情况下GCA则表现出微弱优势。


<details>
  <summary>Details</summary>
Motivation: 为了解决在类别不平衡情况下的多类分类问题，直接最小化平衡分类损失通常是难以处理的问题，因此需要设计有效的替代损失函数。

Method: 本文提出了两类新的替代损失函数：广义对数调整（GLA）损失函数与广义类别感知加权（GCA）损失函数。GLA损失扩展了基于类别先验调整logits的方法至更广泛的交叉熵损失家族；GCA损失通过引入类别依赖置信边界改进了标准的类别权重损失方法，并同样扩展到了一般交叉熵损失家族。

Result: 理论分析表明，GLA损失仅在完整假设集上是H-一致的，其一致性边界反比于最小类别概率；相比之下，GCA损失对于任何有界或完整假设集都是H-一致的，且其一致性边界随$1/\sqrt{p_{min}}$缩放，提供了在不平衡设置下更强有力的理论保障。实验结果显示，GLA损失在通用基准测试中通常表现稍好，而GCA损失在极端不平衡条件下显示出轻微优势。

Conclusion: 通过引入GLA和GCA两种新型替代损失函数，本研究为处理类别不平衡问题提供了新的途径。特别是GCA损失，在高度不平衡的数据集上展现出了更好的性能及理论支持。

Abstract: The balanced loss is a widely adopted objective for multi-class classification under class imbalance. By assigning equal importance to all classes, regardless of their frequency, it promotes fairness and ensures that minority classes are not overlooked. However, directly minimizing the balanced classification loss is typically intractable, which makes the design of effective surrogate losses a central question. This paper introduces and studies two advanced surrogate loss families: Generalized Logit-Adjusted (GLA) loss functions and Generalized Class-Aware weighted (GCA) losses. GLA losses generalize Logit-Adjusted losses, which shift logits based on class priors, to the broader general cross-entropy loss family. GCA loss functions extend the standard class-weighted losses, which scale losses inversely by class frequency, by incorporating class-dependent confidence margins and extending them to the general cross-entropy family. We present a comprehensive theoretical analysis of consistency for both loss families. We show that GLA losses are Bayes-consistent, but only $H$-consistent for complete (i.e., unbounded) hypothesis sets. Moreover, their $H$-consistency bounds depend inversely on the minimum class probability, scaling at least as $1/\mathsf p_{\min}$. In contrast, GCA losses are $H$-consistent for any hypothesis set that is bounded or complete, with $H$-consistency bounds that scale more favorably as $1/\sqrt{\mathsf p_{\min}}$, offering significantly stronger theoretical guarantees in imbalanced settings. We report the results of experiments demonstrating that, empirically, both the GCA losses with calibrated class-dependent confidence margins and GLA losses can greatly outperform straightforward class-weighted losses as well as the LA losses. GLA generally performs slightly better in common benchmarks, whereas GCA exhibits a slight edge in highly imbalanced settings.

</details>


### [28] [Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing](https://arxiv.org/abs/2512.23977)
*Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: DataFlow 是一个用于构建、测试和部署高性能机器学习系统的计算框架，特别适用于无限时间序列数据。它通过基于有向无环图（DAG）的统一执行模型解决了从批量处理到流式生产系统迁移时遇到的问题，如因果关系违反、批处理边界问题以及实时故障再现性差等问题，并支持灵活的时间和特征维度切片配置，以适应不同应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统的数据科学工作流程假设有限的数据集，在从批量原型转移到流式生产系统时需要大量的重新实现。这种差距导致了因果关系违反、批处理边界问题以及实时故障再现性差等问题。

Method: DataFlow 通过基于有向无环图（DAGs）的统一执行模型解决这些问题，确保在任何时间点 t 的输出仅依赖于 t 前的一个固定长度上下文窗口。该框架自动追踪所有转换过程中的知识时间，从而强制严格的因果关系。此外，DataFlow 支持跨时间和特征维度的灵活分块，并与 Python 数据科学栈原生集成，提供在线学习的拟合/预测语义、缓存与增量计算以及基于 DAG 的调度自动并行化。

Result: DataFlow 能够保证开发于批处理模式下的模型无需代码修改即可在流式生产环境中相同地执行，同时支持相同的模型通过配置操作在不同的频率和内存配置下运行。其有效性已在金融交易、物联网、欺诈检测和实时分析等多个领域得到验证。

Conclusion: DataFlow 提供了一种新的方法来桥接传统批处理和流式处理间的差距，使得机器学习系统能够更高效且准确地处理无限时间序列数据，提高了模型从研发到生产的可移植性和可靠性。

Abstract: We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.
  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.
  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics.

</details>


### [29] [Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems](https://arxiv.org/abs/2512.23978)
*Tinglong Dai,David Simchi-Levi,Michelle Xiao Wu,Yao Xie*

Main category: cs.LG

TL;DR: 本文探讨了生成式人工智能（GenAI）从对话助手向自主决策系统转变过程中所面临的自治悖论，并提出了一种基于运筹学的概念框架，该框架通过流式生成模型和对抗鲁棒性的视角来确保在安全关键及可靠性敏感操作领域中的自主性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能（GenAI）逐渐转向能够感知、决策并执行的自主系统，这带来了新的挑战：即如何在赋予这些系统更大运作自主权的同时，保证其结构更加正式、约束更加明确以及尾部风险控制更强。文章旨在解决这一自治悖论，并为安全关键与可靠性敏感的操作领域提供一种保障自主性的方法。

Method: 提出了一个基于运筹学（OR）概念上的保障自主性框架，包含两方面互补的方法：一是利用流式生成模型将生成过程视为由常微分方程描述的确定性传输，从而支持可审计性、约束意识生成等；二是从对抗鲁棒性的角度构建运营安全性，考虑最坏情况下的扰动以将未建模的风险纳入设计之中。

Result: 该框架不仅阐明了随着自主性增强，运筹学角色如何从单纯的问题解决者转变为护栏乃至系统架构师的过程，同时也定义了一个研究议程，旨在为安全关键及可靠性敏感的操作领域提供有保障的自主性解决方案。

Conclusion: 为了应对生成式AI系统在获得更高操作自主权时可能遇到的脆弱性问题，需要结合运筹学原理开发出能够确保可行性验证、抗分布偏移能力及高后果情景下压力测试的新机制。

Abstract: Generative artificial intelligence (GenAI) is shifting from conversational assistants toward agentic systems -- autonomous decision-making systems that sense, decide, and act within operational workflows. This shift creates an autonomy paradox: as GenAI systems are granted greater operational autonomy, they should, by design, embody more formal structure, more explicit constraints, and stronger tail-risk discipline. We argue stochastic generative models can be fragile in operational domains unless paired with mechanisms that provide verifiable feasibility, robustness to distribution shift, and stress testing under high-consequence scenarios. To address this challenge, we develop a conceptual framework for assured autonomy grounded in operations research (OR), built on two complementary approaches. First, flow-based generative models frame generation as deterministic transport characterized by an ordinary differential equation, enabling auditability, constraint-aware generation, and connections to optimal transport, robust optimization, and sequential decision control. Second, operational safety is formulated through an adversarial robustness lens: decision rules are evaluated against worst-case perturbations within uncertainty or ambiguity sets, making unmodeled risks part of the design. This framework clarifies how increasing autonomy shifts OR's role from solver to guardrail to system architect, with responsibility for control logic, incentive protocols, monitoring regimes, and safety boundaries. These elements define a research agenda for assured autonomy in safety-critical, reliability-sensitive operational domains.

</details>


### [30] [Information-Theoretic Quality Metric of Low-Dimensional Embeddings](https://arxiv.org/abs/2512.23981)
*Sebastián Gutiérrez-Bernal,Hector Medel Cobaxin,Abiel Galindo González*

Main category: cs.LG

TL;DR: 本文从信息论的角度出发，研究了低维嵌入的质量。提出了一种新的度量标准——熵秩保持度量（ERPM），它基于邻域矩阵奇异值谱的香农熵和稳定秩来量化原始表示与其降维投影之间的不确定性变化。通过与基于距离的标准（如平均相对秩误差MRRE）及基于几何属性的标准（如局部普罗克鲁斯特分析）进行对比，发现ERPM能够补充现有度量方法，识别出信息严重丢失的邻域，从而为嵌入效果提供更全面的评估，特别是在构建预警指标等对信息敏感的应用中。


<details>
  <summary>Details</summary>
Motivation: 现有的低维嵌入质量评价指标主要集中在距离失真或局部几何结构的变化上，并不能直接衡量从高维空间到低维空间映射过程中保留了多少信息。因此，需要一种新的度量方式来直接评估信息保存情况。

Method: 提出了熵秩保持度量(ERPM)，这是一种基于邻域矩阵奇异值谱的香农熵与稳定秩的局部度量方法，用于量化数据从原始高维表示到低维投影过程中的不确定性变化。

Result: 通过将ERPM的结果与基于距离的平均相对秩误差(MRRE)以及基于几何特性的局部普罗克鲁斯特方法相比较，发现在金融时间序列和文献中常研究的流形上，基于距离的度量与几何和谱度量之间显示出非常低的相关性；而ERPM与局部普罗克鲁斯特方法则表现出强平均相关性，但在局部区域存在显著差异。

Conclusion: ERPM作为一种补充性的度量工具，能够有效识别信息损失严重的邻域，为低维嵌入的质量提供了更为全面的评估手段，特别适用于信息敏感领域内的应用。

Abstract: In this work we study the quality of low-dimensional embeddings from an explicitly information-theoretic perspective. We begin by noting that classical evaluation metrics such as stress, rank-based neighborhood criteria, or Local Procrustes quantify distortions in distances or in local geometries, but do not directly assess how much information is preserved when projecting high-dimensional data onto a lower-dimensional space. To address this limitation, we introduce the Entropy Rank Preservation Measure (ERPM), a local metric based on the Shannon entropy of the singular-value spectrum of neighborhood matrices and on the stable rank, which quantifies changes in uncertainty between the original representation and its reduced projection, providing neighborhood-level indicators and a global summary statistic. To validate the results of the metric, we compare its outcomes with the Mean Relative Rank Error (MRRE), which is distance-based, and with Local Procrustes, which is based on geometric properties, using a financial time series and a manifold commonly studied in the literature. We observe that distance-based criteria exhibit very low correlation with geometric and spectral measures, while ERPM and Local Procrustes show strong average correlation but display significant discrepancies in local regimes, leading to the conclusion that ERPM complements existing metrics by identifying neighborhoods with severe information loss, thereby enabling a more comprehensive assessment of embeddings, particularly in information-sensitive applications such as the construction of early-warning indicators.

</details>


### [31] [Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective](https://arxiv.org/abs/2512.24002)
*Tan Pan,Yixuan Sun,Chen Jiang,Qiong Gao,Rui Sun,Xingmeng Zhang,Zhenqi Yang,Limei Han,Yixiu Liang,Yuan Cheng,Kaiyu Guo*

Main category: cs.LG

TL;DR: 提出了一种名为CLEAR-HUG的两阶段框架，用于捕捉心电图导联间心脏传导的细微变化，并遵循ECCG诊断指南。该框架首先通过Conduction-LEAd Reconstructor (CLEAR)模型捕捉心跳间的特异性和共性，然后使用Hierarchical lead-Unified Group head (HUG)进行疾病诊断。实验表明，在六个任务上平均提高了6.84%的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的心电图自监督学习方法忽视了心脏跳动之间固有的差异以及这些差异所携带的独特生理特征。此外，目前的方法在应用于下游任务时往往忽略了从单个心跳到单个导联再到导联组合这一逐步递进的心电图诊断逻辑。为了解决这些问题，提出了新的框架。

Method: 设计了一个名为CLEAR-HUG的两阶段框架。第一阶段，开发了Conduction-LEAd Reconstructor (CLEAR)，它将每个心跳视为独立实体，并采用稀疏注意力机制来重建信号而不受其他心跳干扰；第二阶段，实现了Hierarchical lead-Unified Group head (HUG)，以模仿临床工作流程来进行疾病诊断。

Result: 实验结果表明，该方法在六个不同的任务中平均提升了6.84%的性能，证明了CLEAR-HUG能够有效地增强对心脏传导表现的学习，并且与专家诊断指南相匹配。

Conclusion: CLEAR-HUG框架不仅能够捕捉到心电图中跨导联的心脏传导过程中的细微变异，而且还能确保这种模式识别符合临床实践中的诊断指南要求。

Abstract: The multi-lead electrocardiogram (ECG) stands as a cornerstone of cardiac diagnosis. Recent strides in electrocardiogram self-supervised learning (eSSL) have brightened prospects for enhancing representation learning without relying on high-quality annotations. Yet earlier eSSL methods suffer a key limitation: they focus on consistent patterns across leads and beats, overlooking the inherent differences in heartbeats rooted in cardiac conduction processes, while subtle but significant variations carry unique physiological signatures. Moreover, representation learning for ECG analysis should align with ECG diagnostic guidelines, which progress from individual heartbeats to single leads and ultimately to lead combinations. This sequential logic, however, is often neglected when applying pre-trained models to downstream tasks. To address these gaps, we propose CLEAR-HUG, a two-stage framework designed to capture subtle variations in cardiac conduction across leads while adhering to ECG diagnostic guidelines. In the first stage, we introduce an eSSL model termed Conduction-LEAd Reconstructor (CLEAR), which captures both specific variations and general commonalities across heartbeats. Treating each heartbeat as a distinct entity, CLEAR employs a simple yet effective sparse attention mechanism to reconstruct signals without interference from other heartbeats. In the second stage, we implement a Hierarchical lead-Unified Group head (HUG) for disease diagnosis, mirroring clinical workflow. Experimental results across six tasks show a 6.84% improvement, validating the effectiveness of CLEAR-HUG. This highlights its ability to enhance representations of cardiac conduction and align patterns with expert diagnostic guidelines.

</details>


### [32] [Hyperspherical Graph Representation Learning via Adaptive Neighbor-Mean Alignment and Uniformity](https://arxiv.org/abs/2512.24062)
*Rui Chen,Junjun Guo,Hongbin Wang,Yan Xiang,Yantuan Xian,Zhengtao Yu*

Main category: cs.LG

TL;DR: 本文提出了一种新的图表示学习框架HyperGRL，通过自适应邻居均值对齐和无采样均匀性在超球面上进行节点嵌入。该方法使用两个对抗耦合目标来稳定训练并提高模型性能，在节点分类、节点聚类和链接预测任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的图表示学习（GRL）方法通常依赖于替代对比目标或互信息最大化策略，这需要复杂的架构、负采样策略以及敏感的超参数调整。这些设计选择可能导致过平滑、过压缩和训练不稳定的问题。

Method: HyperGRL通过两个对抗耦合的目标将节点嵌入单位超球面：邻居均值对齐和无采样均匀性。对齐目标利用每个节点局部邻域的平均表示构建语义基础的稳定目标，捕捉共享结构和特征模式；均匀性目标通过基于L2范数的超球面正则化促进全局均匀分布同时保留区分信息。此外，引入了熵引导自适应平衡机制以进一步稳定训练过程，无需手动调节即可动态调节对齐与均匀性之间的相互作用。

Result: 广泛的实验表明，HyperGRL在节点分类、节点聚类和链接预测任务中提供了优于现有最强方法的表现，分别实现了1.49%、0.86%和0.74%的平均改进。

Conclusion: 结果证明了几何基础且无需采样的对比目标对于图表示学习的有效性。

Abstract: Graph representation learning (GRL) aims to encode structural and semantic dependencies of graph-structured data into low-dimensional embeddings. However, existing GRL methods often rely on surrogate contrastive objectives or mutual information maximization, which typically demand complex architectures, negative sampling strategies, and sensitive hyperparameter tuning. These design choices may induce over-smoothing, over-squashing, and training instability. In this work, we propose HyperGRL, a unified framework for hyperspherical graph representation learning via adaptive neighbor-mean alignment and sampling-free uniformity. HyperGRL embeds nodes on a unit hypersphere through two adversarially coupled objectives: neighbor-mean alignment and sampling-free uniformity. The alignment objective uses the mean representation of each node's local neighborhood to construct semantically grounded, stable targets that capture shared structural and feature patterns. The uniformity objective formulates dispersion via an L2-based hyperspherical regularization, encouraging globally uniform embedding distributions while preserving discriminative information. To further stabilize training, we introduce an entropy-guided adaptive balancing mechanism that dynamically regulates the interplay between alignment and uniformity without requiring manual tuning. Extensive experiments on node classification, node clustering, and link prediction demonstrate that HyperGRL delivers superior representation quality and generalization across diverse graph structures, achieving average improvements of 1.49%, 0.86%, and 0.74% over the strongest existing methods, respectively. These findings highlight the effectiveness of geometrically grounded, sampling-free contrastive objectives for graph representation learning.

</details>


### [33] [How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns](https://arxiv.org/abs/2512.24063)
*Haoyue Bai,Yiyou Sun,Wenjie Hu,Shi Qiu,Maggie Ziyu Huan,Peiyang Song,Robert Nowak,Dawn Song*

Main category: cs.LG

TL;DR: 本研究通过引入一种新的基准来分解大语言模型中的推理能力为基本核心技能，如计算、事实检索等，并结合低级统计模式分析，详细探讨了监督微调（SFT）与强化学习调整（RL）下模型泛化行为的差异。结果表明，RL调优模型在保持稳定的行为特征和抵抗推理技能退化方面表现更佳，而SFT模型则倾向于过度拟合表面模式。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要依赖于粗略的准确性指标来评估大语言模型的能力变化，但未能深入解释监督微调导致能力缩小及强化学习调整能够保持能力的原因。为了填补这一空白，研究人员希望通过创建一个可以将推理能力分解成更具体的核心技能的新基准，来更好地理解LLMs中推理的本质及其在不同训练方法下的演变情况。

Method: 研究人员设计了一个新基准，该基准将推理任务细分为多个原子化的基础技能，并通过此框架追踪模型在不同训练阶段的行为表现。此外，他们还利用对底层统计模式（包括分布差异性和参数统计数据）的分析，以更细致地研究两种训练方式——监督微调(SFT)与强化学习(RL)调优——如何影响数学、科学推理及其他非推理任务上的泛化性能。

Result: 研究发现，采用强化学习方式进行调优的大语言模型相较于监督微调模型而言，在维持稳定的行为特征以及防止推理技能衰退方面展现出了更强的优势；相反，监督微调模型更容易出现显著的行为漂移，并且可能过度适应表面上的数据模式而非真正理解问题背后的逻辑。

Conclusion: 这项工作揭示了大语言模型中推理特性的本质，并为开发旨在促进广泛且稳健泛化能力的训练策略提供了新的见解。

Abstract: Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.

</details>


### [34] [Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework](https://arxiv.org/abs/2512.24075)
*Jiazhao Shi,Ziyu Wang,Yichen Lin,Shoufeng Lu*

Main category: cs.LG

TL;DR: 本文提出了一种结合深度时间表示和物理启发交互线索的TPI-AI框架，用于提高车道变换意图预测的准确性。实验结果显示，该方法在不同场景下的预测表现优于单独使用LightGBM或Bi-LSTM的方法。


<details>
  <summary>Details</summary>
Motivation: 在自然驾驶条件下，由于运动学噪声、严重的类别不平衡以及跨异构高速公路场景泛化能力有限等问题，车道变换意图预测对于自动驾驶和ADAS来说仍然是一个难题。

Method: 提出了Temporal Physics-Informed AI (TPI-AI)混合框架，该框架融合了深层时间序列表达与基于物理原理的交互线索。使用两层双向LSTM编码器从多步轨迹历史中学习紧凑嵌入，并将其与考虑运动学、安全性和交互性的特征（如车头时距、到达时间估计和安全间距指标）连接起来，训练LightGBM分类器实现三类意图识别（不换道、左换道、右换道）。为了改善少数类别的可靠性，应用了包括重采样/加权和按折校准阈值在内的不平衡意识优化技术。

Result: 通过两个大规模基于无人机的数据集highD（直线路段）和exiD（匝道丰富环境）进行实验验证，采用基于位置的数据分割方式并评估了T=1, 2, 3秒的预测范围。TPI-AI在所有测试条件下都超过了单独使用LightGBM或Bi-LSTM基线模型的表现，在highD上分别达到0.9562、0.9124、0.8345的宏观F1分数，在exiD上则为0.9247、0.8197、0.7605。

Conclusion: 结合基于物理原理的交互特征与学习到的时间序列嵌入可以产生鲁棒性更强的多场景车道变换意图预测结果。

Abstract: Lane-change intention prediction is safety-critical for autonomous driving and ADAS, but remains difficult in naturalistic traffic due to noisy kinematics, severe class imbalance, and limited generalization across heterogeneous highway scenarios. We propose Temporal Physics-Informed AI (TPI-AI), a hybrid framework that fuses deep temporal representations with physics-inspired interaction cues. A two-layer bidirectional LSTM (Bi-LSTM) encoder learns compact embeddings from multi-step trajectory histories; we concatenate these embeddings with kinematics-, safety-, and interaction-aware features (e.g., headway, TTC, and safe-gap indicators) and train a LightGBM classifier for three-class intention recognition (No-LC, Left-LC, Right-LC). To improve minority-class reliability, we apply imbalance-aware optimization including resampling/weighting and fold-wise threshold calibration. Experiments on two large-scale drone-based datasets, highD (straight highways) and exiD (ramp-rich environments), use location-based splits and evaluate prediction horizons T = 1, 2, 3 s. TPI-AI outperforms standalone LightGBM and Bi-LSTM baselines, achieving macro-F1 of 0.9562, 0.9124, 0.8345 on highD and 0.9247, 0.8197, 0.7605 on exiD at T = 1, 2, 3 s, respectively. These results show that combining physics-informed interaction features with learned temporal embeddings yields robust multi-scenario lane-change intention prediction.

</details>


### [35] [Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study](https://arxiv.org/abs/2512.24102)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 本文通过系统性的消融研究，探讨了GP-VAE模型中潜在自回归的作用。结果显示，在中等规模语料库和短训练上下文的条件下，潜在自回归有助于生成更符合高斯过程先验且长时稳定性更强的潜在轨迹。


<details>
  <summary>Details</summary>
Motivation: 基于之前引入GP-VAE架构的工作，本文旨在进一步理解在该类模型中潜变量自回归所起的作用，并与非自回归变体及标准的标记级自回归Transformer进行比较。

Method: 采用了三种对比方案：（i）具有自回归潜在动态的完整GP-VAE模型；（ii）潜变量相互独立的非自回归版本；（iii）标准的标记级自回归Transformer。通过对这些模型在中等规模数据集上的表现进行了系统的消融分析。

Result: 研究发现，保持潜在自回归可以产生与高斯过程先验更加一致的潜在轨迹，并表现出更好的长时间稳定性。相反地，去除自回归会导致潜在结构退化以及不稳定的长期行为。

Conclusion: 本研究表明，潜在自回归是组织长期结构的有效机制，它与标记级别的自回归建模相辅相成，但这项工作主要关注于表征结构的经验性分析而非提出新的架构。

Abstract: This paper provides an ablation-based analysis of latent autoregression in GP-VAE models, building upon our previous work introducing the architecture. Language models typically rely on an autoregressive factorization over tokens. In contrast, our prior work proposed shifting sequential structure to the latent space through a causal Gaussian process, while using a non-autoregressive decoder. Here, we conduct a systematic ablation study of the role played by latent autoregression. We compare (i) a full GP-VAE model with autoregressive latent dynamics, (ii) a non-autoregressive ablation in which latent variables are independent, and (iii) a standard token-level autoregressive Transformer. Our results show that, within the considered regime (medium-scale corpora and short training contexts), latent autoregression induces latent trajectories that are significantly more compatible with the Gaussian-process prior and exhibit greater long-horizon stability. In contrast, removing autoregression leads to degraded latent structure and unstable long-range behavior. These findings highlight the role of latent autoregression as an effective mechanism for organizing long-range structure, while remaining complementary to token-level autoregressive modeling. They should be interpreted as an empirical analysis of representational structure rather than as a proposal for a new architecture.

</details>


### [36] [Enhancing LLM Planning Capabilities through Intrinsic Self-Critique](https://arxiv.org/abs/2512.24103)
*Bernd Bohnet,Pierre-Alexandre Kamienny,Hanie Sedghi,Dilan Gorur,Pranjal Awasthi,Aaron Parisi,Kevin Swersky,Rosanne Liu,Azade Nova,Noah Fiedel*

Main category: cs.LG

TL;DR: 本文提出了一种方法，让大语言模型通过自我批判来改进其在规划任务中的表现，这种方法在Blocksworld、Logistics以及Mini-grid数据集上均取得了显著的性能提升，并且超越了强基线准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管早期研究对大语言模型利用自我批判方法的有效性表示怀疑，但本文旨在展示一种通过内在自我批判来增强大语言模型表现的方法，以期在不依赖外部验证的情况下，在规划基准测试中取得显著进步。

Method: 采用了少量学习技术并逐步扩展至大量学习方法作为基础方法，并通过迭代过程进行修正和细化，从而在这一已经具有竞争力的方法之上实现了显著改进。

Result: 在Blocksworld领域上的规划数据集、Logistics以及Mini-grid数据集上都显示出了显著的性能提升，并且超过了强大的基线准确性。实证结果为考虑中的模型类别树立了新的最先进标准。

Conclusion: 本文主要集中在方法本身，展示了无论具体模型版本如何，都能够应用的内在自我改进能力，并认为将该方法应用于更复杂的搜索技术和更强力的模型会带来更好的表现。

Abstract: We demonstrate an approach for LLMs to critique their \emph{own} answers with the goal of enhancing their performance that leads to significant improvements over established planning benchmarks. Despite the findings of earlier research that has cast doubt on the effectiveness of LLMs leveraging self critique methods, we show significant performance gains on planning datasets in the Blocksworld domain through intrinsic self-critique, without external source such as a verifier. We also demonstrate similar improvements on Logistics and Mini-grid datasets, exceeding strong baseline accuracies. We employ a few-shot learning technique and progressively extend it to a many-shot approach as our base method and demonstrate that it is possible to gain substantial improvement on top of this already competitive approach by employing an iterative process for correction and refinement. We illustrate how self-critique can significantly boost planning performance. Our empirical results present new state-of-the-art on the class of models considered, namely LLM model checkpoints from October 2024. Our primary focus lies on the method itself, demonstrating intrinsic self-improvement capabilities that are applicable regardless of the specific model version, and we believe that applying our method to more complex search techniques and more capable models will lead to even better performance.

</details>


### [37] [OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization](https://arxiv.org/abs/2512.24124)
*Advait Gadhikar,Riccardo Grazzi,James Hensman*

Main category: cs.LG

TL;DR: 提出了一种名为OptRot的方法，通过最小化旋转权重的四次幂来减少权重异常值，从而改善大型语言模型的量化。此外，还提出了一个数据依赖方法OptRot+，进一步提高了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）中的权重和激活存在异常值，这使得它们难以量化。为了解决这个问题，研究者们尝试使用旋转来减轻这些异常值的影响。

Method: 本文主要介绍了一种名为OptRot的新方法，该方法通过最小化旋转后权重的四次方来减少权重异常值。另外，还引入了一个考虑激活协方差的数据依赖方法OptRot+，以进一步提高性能。

Result: 实验表明，OptRot在权重量化方面优于Hadamard旋转和其他更昂贵的数据依赖方法如SpinQuant和OSTQuant，并且在W4A8设置下也改善了激活量化。然而，在W4A4设置中，OptRot与OptRot+的表现都较差，显示了权重与激活量化之间存在权衡。

Conclusion: 本研究表明，通过学习可融合旋转来减少权重异常值是有效的，特别是OptRot方法在多个场景下表现出色。同时，对于某些特定条件下的激活量化问题仍需进一步探索解决方案。

Abstract: The presence of outliers in Large Language Models (LLMs) weights and activations makes them difficult to quantize. Recent work has leveraged rotations to mitigate these outliers. In this work, we propose methods that learn fusible rotations by minimizing principled and cheap proxy objectives to the weight quantization error. We primarily focus on GPTQ as the quantization method. Our main method is OptRot, which reduces weight outliers simply by minimizing the element-wise fourth power of the rotated weights. We show that OptRot outperforms both Hadamard rotations and more expensive, data-dependent methods like SpinQuant and OSTQuant for weight quantization. It also improves activation quantization in the W4A8 setting. We also propose a data-dependent method, OptRot$^{+}$, that further improves performance by incorporating information on the activation covariance. In the W4A4 setting, we see that both OptRot and OptRot$^{+}$ perform worse, highlighting a trade-off between weight and activation quantization.

</details>


### [38] [Paired Seed Evaluation: Statistical Reliability for Learning-Based Simulators](https://arxiv.org/abs/2512.24145)
*Udit Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种配对种子评估设计，通过在相同的随机种子下评估竞争系统，来减少由于学习随机性和随机初始化导致的高方差问题。这种设计可以提高统计效能、获得更紧凑的置信区间，并且当存在正相关时，在实际应用中具有优势。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统虽然看似随机，但实际上是由确定性伪随机数生成器产生的，因此在不同执行过程中会产生相同的结果。然而，基于学习的模拟器在进行算法、设计选择和干预比较时，由于随机初始化和学习过程中的随机性，评估结果通常表现出较高的方差。作者分析了在这种动态下的比较评估的统计结构，发现标准独立评估设计未能利用跨选项共享的随机源。

Method: 研究者形式化了一个配对种子评估设计方案，其中竞争系统在同一组随机种子下被评估，这诱导了随机组件的匹配实现，并且只要结果在种子级别上是正相关的就能严格降低方差。

Result: 实验表明，种子级别的相关性通常是大而正的，能够产生数量级上的效率增益。配对种子评估方法在实践中是弱主导地位的，当存在相关性时能改善统计可靠性，而在没有相关性的情况下也不会失去有效性地退化为独立评估。

Conclusion: 配对种子评估设计提供了一种有效的方法来减少比较评估中的方差，从而得到更紧密的置信区间、更高的统计功效以及固定计算预算内的有效样本量增加。

Abstract: Machine learning systems appear stochastic but are deterministically random, as seeded pseudorandom number generators produce identical realisations across executions. Learning-based simulators are widely used to compare algorithms, design choices, and interventions under such dynamics, yet evaluation outcomes often exhibit high variance due to random initialisation and learning stochasticity. We analyse the statistical structure of comparative evaluation in these settings and show that standard independent evaluation designs fail to exploit shared sources of randomness across alternatives. We formalise a paired seed evaluation design in which competing systems are evaluated under identical random seeds, inducing matched realisations of stochastic components and strict variance reduction whenever outcomes are positively correlated at the seed level. This yields tighter confidence intervals, higher statistical power, and effective sample size gains at fixed computational budgets. Empirically, seed-level correlations are typically large and positive, producing order-of-magnitude efficiency gains. Paired seed evaluation is weakly dominant in practice, improving statistical reliability when correlation is present and reducing to independent evaluation without loss of validity when it is not.

</details>


### [39] [Early Prediction of Sepsis using Heart Rate Signals and Genetic Optimized LSTM Algorithm](https://arxiv.org/abs/2512.24253)
*Alireza Rafiei,Farshid Hajati,Alireza Rezaee,Amirhossien Panahi,Shahadat Uddin*

Main category: cs.LG

TL;DR: 研究介绍并评估了四种新的机器学习算法，这些算法通过分析心率数据来预测可穿戴设备上败血症的发作。模型架构通过遗传算法进行了优化，并且最初设计用于一小时的预测窗口，后来通过迁移学习扩展到四小时。


<details>
  <summary>Details</summary>
Motivation: 尽管已开发出许多针对重症监护病房（ICU）患者的模型，但在非病房环境中早期检测败血症的方法仍存在明显差距。及时预测败血症的发展对于通过早期干预减少不良结果至关重要。

Method: 本研究介绍了四种新的机器学习算法，旨在通过分析可穿戴设备上的心率数据来预测败血症的发生。使用遗传算法对这些模型的结构进行改进，以优化性能、计算复杂性和内存需求。

Result: 研究表明，这些模型在可穿戴设备上有实施的可能性，能够准确监测心率，从而为非ICU和病房环境下的早期败血症检测提供可能。

Conclusion: 该研究的结果表明，可穿戴技术有可能促进ICU和病房环境之外的早期败血症检测。

Abstract: Sepsis, characterized by a dysregulated immune response to infection, results in significant mortality, morbidity, and healthcare costs. The timely prediction of sepsis progression is crucial for reducing adverse outcomes through early intervention. Despite the development of numerous models for Intensive Care Unit (ICU) patients, there remains a notable gap in approaches for the early detection of sepsis in non-ward settings. This research introduces and evaluates four novel machine learning algorithms designed for predicting the onset of sepsis on wearable devices by analyzing heart rate data. The architecture of these models was refined through a genetic algorithm, optimizing for performance, computational complexity, and memory requirements. Performance metrics were subsequently extracted for each model to evaluate their feasibility for implementation on wearable devices capable of accurate heart rate monitoring. The models were initially tailored for a prediction window of one hour, later extended to four hours through transfer learning. The encouraging outcomes of this study suggest the potential for wearable technology to facilitate early sepsis detection outside ICU and ward environments.

</details>


### [40] [Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction](https://arxiv.org/abs/2512.24324)
*Haojin Li,Anbang Zhang,Chen Sun,Chenyuan Feng,Kaiqian Qu,Tony Q. S. Quek,Haijun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为SaM2B的语义感知多模态波束预测框架，该框架通过轻量级线索（如环境视觉、飞行姿态和地理空间数据）自适应地分配不同时间点上各模态的贡献，并利用跨模态对比学习来提高在模态噪声和分布变化下的判别能力和鲁棒性。实验证明，与基线方法相比，SaM2B在真实低空无人机数据集上取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 随着城市空中交通、物流无人机以及空中传感等领域的快速发展，无人飞行器通信中快速准确的波束预测对于实现可靠的连接至关重要。然而，现有的多模态方法大多采用固定或经验权重，假设任何给定时刻所有模态的可靠性相同，这忽略了不同运动场景下模态重要性的显著波动，静态加权甚至可能放大退化模态的负面影响。此外，模态不匹配和弱对齐问题进一步削弱了跨场景泛化能力。

Method: 提出了一个名为SaM2B的语义感知多模态波束预测框架，该框架能够基于环境视觉、飞行姿态和地理空间等轻量级线索，在不同时间点上自适应地调整各模态之间的贡献度；同时，通过跨模态对比学习将与特定波束信息相关的“多源表示波束语义”映射到共享语义空间，从而增强模型在存在模态噪声和分布偏移情况下的区分力和鲁棒性。

Result: 实验结果表明，相较于基准方法，SaM2B在处理实际低空无人机数据集时表现出色，实现了更令人满意的结果。

Conclusion: 研究证明了所提出的SaM2B框架的有效性，它不仅能够根据实际情况动态调整各模态的重要性，还通过跨模态对比学习提高了系统整体的稳定性和准确性，为低空经济领域中的可靠通信提供了新的解决方案。

Abstract: The low-altitude economy (LAE) is rapidly expanding driven by urban air mobility, logistics drones, and aerial sensing, while fast and accurate beam prediction in uncrewed aerial vehicles (UAVs) communications is crucial for achieving reliable connectivity. Current research is shifting from single-signal to multi-modal collaborative approaches. However, existing multi-modal methods mostly employ fixed or empirical weights, assuming equal reliability across modalities at any given moment. Indeed, the importance of different modalities fluctuates dramatically with UAV motion scenarios, and static weighting amplifies the negative impact of degraded modalities. Furthermore, modal mismatch and weak alignment further undermine cross-scenario generalization. To this end, we propose a reliability-aware dynamic weighting scheme applied to a semantic-aware multi-modal beam prediction framework, named SaM2B. Specifically, SaM2B leverages lightweight cues such as environmental visual, flight posture, and geospatial data to adaptively allocate contributions across modalities at different time points through reliability-aware dynamic weight updates. Moreover, by utilizing cross-modal contrastive learning, we align the "multi-source representation beam semantics" associated with specific beam information to a shared semantic space, thereby enhancing discriminative power and robustness under modal noise and distribution shifts. Experiments on real-world low-altitude UAV datasets show that SaM2B achieves more satisfactory results than baseline methods.

</details>


### [41] [Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning](https://arxiv.org/abs/2512.24404)
*Soham Pahari,M. Srinivas*

Main category: cs.LG

TL;DR: 本文探讨了多模态智能在视觉理解和高级推理方面的进展，提出了一个名为ViReLoc的新框架，该框架仅使用视觉表示来进行规划和定位。通过强化学习目标优化、对比学习和自适应特征交互，ViReLoc能够提高空间推理准确性和跨视图检索性能，为导航和定位任务提供了一个不依赖实时全球定位系统数据的强大补充方法。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态智能在视觉理解和高级推理方面取得了显著进步，但大多数推理系统仍然主要依赖文本信息进行推断，这限制了它们在视觉导航和地理定位等空间任务中的有效性。本文旨在探索这一领域的潜在范围，并提出一种新的视觉推理范式，以解决基于文本的推理难以理解的空间依赖性和几何关系问题。

Method: 提出了Geo-Consistent Visual Planning的概念以及Visual Reasoning for Localization (ViReLoc)框架，该框架完全基于视觉表示来执行规划与定位任务。ViReLoc通过在视觉领域内逐步编码推理过程，并利用基于强化的学习目标进行优化；同时结合对比学习和自适应特征交互技术，以校准不同视角间的差异并减少视角变化带来的影响。

Result: 实验表明，在各种导航和定位场景下，ViReLoc框架能持续提高空间推理准确性及跨视图检索表现。这些发现证实了视觉推理作为导航与定位任务中强有力补充手段的有效性，并指出这类任务可以无需依赖实时GPS数据即可完成，从而为更安全的导航解决方案铺平道路。

Conclusion: 本研究介绍了一种创新性的视觉推理框架ViReLoc，它通过纯视觉方式实现了高效的空间推理与定位能力，证明了即使没有实时GPS支持也能成功执行相关任务。研究表明，这种基于视觉的方法不仅提高了空间推理的准确性，还增强了跨视图检索的效果，为未来开发更加安全可靠的导航系统提供了新思路。

Abstract: Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.

</details>


### [42] [Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models](https://arxiv.org/abs/2512.24407)
*Lars van der Laan,Aurelien Bibaut,Nathan Kallus*

Main category: cs.LG

TL;DR: 本文提出了一种半参数框架，用于无偏逆强化学习，该框架可以在最大熵逆强化学习和Gumbel-shock DDC模型中对一大类依赖于奖励的功能进行统计有效推断。通过将行为策略的对数视为伪奖励，并基于这一特征构建自动去偏机器学习估计器，允许灵活地进行非参数估计，同时达到根号n一致性、渐近正态性和半参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有的逆强化学习方法通常依赖于机器学习技术但无法保证有效的统计推断；而传统的动态离散选择模型则施加了严格的参数设定要求并且经常需要重复执行动态规划。为了解决这些问题，作者们开发了一个新的半参数框架，旨在提供一个统一且计算上可行的方法来进行逆强化学习中的统计推断。

Method: 文章首先展示了行为策略的对数如何作为伪奖励来识别政策价值差异，并在简单归一化后识别奖励本身。接着，正式定义了这些目标（包括已知及反事实softmax政策下的政策价值以及归一化奖励的功能），并将其视为行为策略和转移核平滑功能的一部分。在此基础上，确立了路径可微性并导出了它们的有效影响函数。最后，基于上述特征构建了自动去偏机器学习估计器。

Result: 所提出的半参数框架能够实现对于奖励相关功能的统计高效推断，在采用灵活的非参数估计的同时保持根号n一致性、渐近正态性和半参数效率。此外，该框架还扩展了传统DDC模型对于非参数奖励和现代机器学习工具的应用范围。

Conclusion: 本文介绍了一种新的半参数方法，解决了逆强化学习领域内关于统计推断的关键问题。这种方法不仅提供了更广泛的灵活性，而且确保了统计上的有效性，从而为逆强化学习研究开辟了新途径。

Abstract: Inverse reinforcement learning (IRL) and dynamic discrete choice (DDC) models explain sequential decision-making by recovering reward functions that rationalize observed behavior. Flexible IRL methods typically rely on machine learning but provide no guarantees for valid inference, while classical DDC approaches impose restrictive parametric specifications and often require repeated dynamic programming. We develop a semiparametric framework for debiased inverse reinforcement learning that yields statistically efficient inference for a broad class of reward-dependent functionals in maximum entropy IRL and Gumbel-shock DDC models. We show that the log-behavior policy acts as a pseudo-reward that point-identifies policy value differences and, under a simple normalization, the reward itself. We then formalize these targets, including policy values under known and counterfactual softmax policies and functionals of the normalized reward, as smooth functionals of the behavior policy and transition kernel, establish pathwise differentiability, and derive their efficient influence functions. Building on this characterization, we construct automatic debiased machine-learning estimators that allow flexible nonparametric estimation of nuisance components while achieving $\sqrt{n}$-consistency, asymptotic normality, and semiparametric efficiency. Our framework extends classical inference for DDC models to nonparametric rewards and modern machine-learning tools, providing a unified and computationally tractable approach to statistical inference in IRL.

</details>


### [43] [Sparse classification with positive-confidence data in high dimensions](https://arxiv.org/abs/2512.24443)
*The Tien Mai,Mai Anh Nguyen,Trung Nghia Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种针对高维Positive-Confidence(Pconf)分类问题的新稀疏惩罚框架，通过使用Lasso和非凸(SCAD, MCP)惩罚来解决收缩偏差并改进特征恢复。理论分析表明，在受限强凸性条件下，L1正则化Pconf估计器几乎达到最小最优的稀疏恢复率。此外，还开发了有效的近端梯度算法来解决复合目标函数。实验结果表明，所提方法在预测性能和变量选择准确性上接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 高维学习问题中特征数量超过样本量时需要稀疏正则化来进行有效预测与变量选择，但现有针对弱监督环境如仅利用带置信度得分的正样本进行学习（Pconf）的方法并不适合处理高维情况。

Method: 提出了一个适用于高维Pconf分类的新稀疏惩罚框架，引入了采用凸（Lasso）及非凸（SCAD、MCP）惩罚项的估计器；为了解决由此产生的复合目标函数，设计了一个高效的近端梯度算法。

Result: 广泛的模拟研究表明，所提出的方法能够实现与全监督方法相媲美的预测表现和变量选择准确性。

Conclusion: 本研究成功地缩小了弱监督学习与高维统计之间的差距，为处理高维Pconf分类问题提供了新的解决方案。

Abstract: High-dimensional learning problems, where the number of features exceeds the sample size, often require sparse regularization for effective prediction and variable selection. While established for fully supervised data, these techniques remain underexplored in weak-supervision settings such as Positive-Confidence (Pconf) classification. Pconf learning utilizes only positive samples equipped with confidence scores, thereby avoiding the need for negative data. However, existing Pconf methods are ill-suited for high-dimensional regimes. This paper proposes a novel sparse-penalization framework for high-dimensional Pconf classification. We introduce estimators using convex (Lasso) and non-convex (SCAD, MCP) penalties to address shrinkage bias and improve feature recovery. Theoretically, we establish estimation and prediction error bounds for the L1-regularized Pconf estimator, proving it achieves near minimax-optimal sparse recovery rates under Restricted Strong Convexity condition. To solve the resulting composite objective, we develop an efficient proximal gradient algorithm. Extensive simulations demonstrate that our proposed methods achieve predictive performance and variable selection accuracy comparable to fully supervised approaches, effectively bridging the gap between weak supervision and high-dimensional statistics.

</details>


### [44] [Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics](https://arxiv.org/abs/2512.24445)
*Akash Samanta,Sheldon Williamson*

Main category: cs.LG

TL;DR: 本文提出了一种诊断驱动的自适应学习框架，该框架通过将误差分解为偏差、噪声和对齐三部分来显式建模误差演变。基于此框架，作者们开发了多种实例化方法，并在标准平滑假设下证明了其有效性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对非平稳和安全关键环境中的学习系统存在的不稳定性、收敛缓慢或脆弱适应等问题，现有方法主要关注梯度统计信息而忽略了误差信号的时间结构。

Method: 提出一种新的误差演变建模方法，将其分解成三个组成部分：偏差（持续漂移）、噪声（随机变异性）以及对齐（重复方向性激励导致的过冲）。这些诊断指标可以从损失或时间差分误差轨迹的轻量级统计中在线计算得出，且与模型架构或任务领域无关。基于这个框架，进一步推导出包括稳定监督优化器、诊断调节的演员-评论家方案以及诊断条件下的学习优化器在内的具体实现形式。

Result: 在标准平滑条件下，所有提出的算法都具有有界有效更新和稳定性属性。通过演员-评论家学习中的代表性诊断图示说明了所提议信号如何根据时间差分误差结构调整适应过程。

Conclusion: 这项工作将误差演变提升为自适应学习中的首要对象，并为动态环境下的可靠学习提供了可解释且轻量化的基础。

Abstract: Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Building on this framework, we derive diagnostic-driven instantiations including a stabilized supervised optimizer, a diagnostic-regulated actor-critic scheme, and a diagnostic-conditioned learned optimizer. Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to temporal-difference error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.

</details>


### [45] [Generative forecasting with joint probability models](https://arxiv.org/abs/2512.24446)
*Patrick Wyrod,Ashesh Chattopadhyay,Daniele Venturi*

Main category: cs.LG

TL;DR: 本文提出了一种新的完全生成式预测方法，通过学习系统状态滞后窗口上的联合概率分布，并利用边缘化来获得预测。该方法能够捕捉非线性时间依赖关系、表示多步轨迹段，并且产生与所学联合分布一致的下一步预测。此外，还引入了一个通用的训练和推理框架用于联合生成预测，并通过三个互补的不确定性量化指标评估了预测的鲁棒性和可靠性。在两个经典混沌动力系统上的实验表明，这种方法在短期预测准确性、保持吸引子几何形状以及实现远期统计行为方面优于传统的条件下一步模型。


<details>
  <summary>Details</summary>
Motivation: 混沌动力系统对初始条件高度敏感，并且通常包含未解决的多尺度过程，这使得确定性预测本质上受到限制。现有的大多数方法侧重于下一步条件预测，而不是底层动态结构的学习。因此，需要一种能够更好地捕捉系统动态本质并提供更可靠长期预测的方法。

Method: 作者将预测问题重新定义为一个完全生成的问题，通过学习短时间窗口内滞后系统状态的联合概率分布，并通过边缘化得到预测结果。同时，他们提出了一个与模型无关的训练和推理框架，用于联合生成预测，并使用三种补充性的不确定性量化度量（集成方差、短时自相关和累积Wasserstein漂移）来评估预测的稳健性和可靠性。

Result: 研究表明，在Lorenz-63系统和Kuramoto-Sivashinsky方程这两个典型的混沌动力系统上，提出的联合生成模型相较于传统条件下一步模型，在提高短期预测技巧、保持吸引子几何形状以及达到更准确的长范围统计行为方面表现更好。

Conclusion: 这项工作展示了一种基于联合生成模型的新颖预测方法，它不仅改善了混沌系统的短期预测性能，而且对于保持系统的长期统计特性也非常有效。通过引入新的训练和推理框架，该方法还能有效地评估预测结果的鲁棒性和可靠性。

Abstract: Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.

</details>


### [46] [HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors](https://arxiv.org/abs/2512.24478)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为HOLOGRAPH的框架，该框架通过层论将大型语言模型(LLMs)作为因果知识来源进行形式化处理。实验表明，HOLOGRAPH不仅提供了严格的数学基础，在50-100变量的因果发现任务中也表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的从观察数据中进行因果发现的方法受到可识别性约束的限制，而近期虽然探索了利用大型语言模型（LLMs）作为先验因果知识源，但缺乏理论依据。因此，作者旨在开发一种基于理论支持的方法来整合LLM提供的因果信息。

Method: 提出了HOLOGRAPH框架，它使用层论来表示局部因果信念，并通过代数潜在投影处理隐藏混淆因素，同时采用自然梯度下降法在信念流形上进行优化。

Result: HOLOGRAPH框架在合成和真实世界基准测试中展示了其竞争力，特别是在涉及50至100个变量的因果发现任务中。此外，通过层论分析揭示了在较大图中局部性公理失效的现象，提示潜在变量投影中的基本非局部耦合。

Conclusion: HOLOGRAPH为利用LLM进行因果发现提供了一个具备严格数学基础的新方法，并且在实际应用中证明了它的有效性。

Abstract: Causal discovery from observational data remains fundamentally limited by identifiability constraints. Recent work has explored leveraging Large Language Models (LLMs) as sources of prior causal knowledge, but existing approaches rely on heuristic integration that lacks theoretical grounding. We introduce HOLOGRAPH, a framework that formalizes LLM-guided causal discovery through sheaf theory--representing local causal beliefs as sections of a presheaf over variable subsets. Our key insight is that coherent global causal structure corresponds to the existence of a global section, while topological obstructions manifest as non-vanishing sheaf cohomology. We propose the Algebraic Latent Projection to handle hidden confounders and Natural Gradient Descent on the belief manifold for principled optimization. Experiments on synthetic and real-world benchmarks demonstrate that HOLOGRAPH provides rigorous mathematical foundations while achieving competitive performance on causal discovery tasks with 50-100 variables. Our sheaf-theoretic analysis reveals that while Identity, Transitivity, and Gluing axioms are satisfied to numerical precision (<10^{-6}), the Locality axiom fails for larger graphs, suggesting fundamental non-local coupling in latent variable projections. Code is available at [https://github.com/hyunjun1121/holograph](https://github.com/hyunjun1121/holograph).

</details>


### [47] [More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization](https://arxiv.org/abs/2512.24545)
*Yuma Ichikawa,Yoshihiko Fujisawa,Yudai Fujimoto,Akira Sakai,Katsuki Fujisawa*

Main category: cs.LG

TL;DR: 本文提出了一种改进的二进制因子分解方法——多包络DBF（MDBF），通过共享符号矩阵并在有限内存预算内提高幅度表现力，从而在保持相同部署友好推理原语的同时，提高了困惑度和零样本准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的双二进制因子分解(DBF)方法在处理大型语言模型极低比特量化时存在局限性，特别是其缩放参数过于限制，导致所有秩分量共享相同的幅度分布，进而引起性能饱和。

Method: 提出了多包络DBF (MDBF)，保留了共享的一对1位符号基但用一个秩-l包络替换单一包络。此外，还引入了闭式初始化和交替精炼方法来优化MDBF。

Result: 实验表明，在LLaMA和Qwen系列中，与之前的二进制格式相比，MDBF在每权重比特数匹配的情况下提高了困惑度和零样本准确性，并且保持了相同的易于部署的推理原语。

Conclusion: MDBF为极端低比特量化提供了一个有效解决方案，它不仅克服了传统DBF方法的局限性，还在不牺牲精度的前提下实现了高效的推理。

Abstract: For extreme low-bit quantization of large language models (LLMs), Double Binary Factorization (DBF) is attractive as it enables efficient inference without sacrificing accuracy. However, the scaling parameters of DBF are too restrictive; after factoring out signs, all rank components share the same magnitude profile, resulting in performance saturation. We propose Multi-envelope DBF (MDBF), which retains a shared pair of 1-bit sign bases but replaces the single envelope with a rank-$l$ envelope. By sharing sign matrices among envelope components, MDBF effectively maintains a binary carrier and utilizes the limited memory budget for magnitude expressiveness. We also introduce a closed-form initialization and an alternating refinement method to optimize MDBF. Across the LLaMA and Qwen families, MDBF enhances perplexity and zero-shot accuracy over previous binary formats at matched bits per weight while preserving the same deployment-friendly inference primitive.

</details>


### [48] [CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts](https://arxiv.org/abs/2512.24564)
*Shunbo Jia,Caizhi Liao*

Main category: cs.LG

TL;DR: 本研究提出了一种新的方法CPR，通过在因果解缠框架中引入生理结构先验来解决ECG诊断模型对对抗性扰动的脆弱性问题。实验结果表明，CPR不仅在SAP攻击下表现出色，而且保持了单次推理效率，实现了鲁棒性、效率和临床可解释性之间的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在心电图诊断方面取得了显著准确性，但它们容易受到模仿生物形态的平滑对抗扰动（SAP）的影响。现有的防御措施要么计算成本过高，要么导致显著的推理延迟，不适合实时临床监测。这种脆弱性源于模型依赖于非稳健的虚假相关性而非不变的病理特征。

Method: 提出了Causal Physiological Representation Learning (CPR)，它在一个因果解缠框架内整合了生理结构先验。通过使用结构性因果模型（SCM）模拟ECG生成过程，CPR强制执行一种结构干预，严格区分不变的病理形态（P-QRS-T复合体）与非因果的人工制品。

Result: 在PTB-XL数据集上的实证结果显示，面对SAP攻击时，CPR达到了0.632的F1分数，比中值平滑法高出9.1%。此外，CPR在保持单次推理效率的同时，匹配了随机平滑认证的鲁棒性。

Conclusion: CPR方法为提高ECG诊断模型对抗对抗性扰动的能力提供了有效解决方案，在保证鲁棒性和临床可解释性的同时，也维持了高效性。

Abstract: Deep learning models for Electrocardiogram (ECG) diagnosis have achieved remarkable accuracy but exhibit fragility against adversarial perturbations, particularly Smooth Adversarial Perturbations (SAP) that mimic biological morphology. Existing defenses face a critical dilemma: Adversarial Training (AT) provides robustness but incurs a prohibitive computational burden, while certified methods like Randomized Smoothing (RS) introduce significant inference latency, rendering them impractical for real-time clinical monitoring. We posit that this vulnerability stems from the models' reliance on non-robust spurious correlations rather than invariant pathological features. To address this, we propose Causal Physiological Representation Learning (CPR). Unlike standard denoising approaches that operate without semantic constraints, CPR incorporates a Physiological Structural Prior within a causal disentanglement framework. By modeling ECG generation via a Structural Causal Model (SCM), CPR enforces a structural intervention that strictly separates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts. Empirical results on PTB-XL demonstrate that CPR significantly outperforms standard clinical preprocessing methods. Specifically, under SAP attacks, CPR achieves an F1 score of 0.632, surpassing Median Smoothing (0.541 F1) by 9.1%. Crucially, CPR matches the certified robustness of Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off between robustness, efficiency, and clinical interpretability.

</details>


### [49] [Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space](https://arxiv.org/abs/2512.24617)
*Xingwei Qu,Shaowen Wang,Zihao Huang,Kai Hua,Fan Yin,Rui-Jie Zhu,Jundong Zhou,Qiyang Min,Zihao Wang,Yizhi Li,Tianyu Zhang,He Xing,Zheng Zhang,Yuxuan Song,Tianyu Zheng,Zhiyuan Zeng,Chenghua Lin,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: 提出了一种新的语言模型框架DLCM，该框架通过学习语义边界并从token空间压缩到概念空间来更高效地进行推理。引入了首个压缩感知缩放定律，并开发了解耦的$\mu$P参数化方法以支持超参数的零样本迁移。在实际设置下，DLCM能够将大约三分之一的推理计算重新分配给更高容量的推理主干，在匹配的推理FLOPs条件下实现了平均2.69%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型对所有token应用相同的计算方式，这导致了对于局部可预测部分过度使用计算能力，而对语义上关键转换部分则计算不足。

Method: 提出了动态大概念模型（DLCM），这是一种层次化的语言建模框架，能够从潜在表示中学习语义边界，并将计算从token空间转移到更加高效的压缩概念空间。此外，还介绍了首个压缩感知的缩放定律，以及一种解耦的$\mu$P参数化方法。

Result: DLCM能够在保持相同推理FLOPs的情况下，将约三分之一的推理计算资源重新分配给一个更高容量的推理主干，从而在12个零样本基准测试中获得了平均+2.69%的表现提升。

Conclusion: DLCM通过有效利用计算资源和优化推理过程中的信息密度处理，为提高语言模型效率提供了新途径。

Abstract: Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.

</details>


### [50] [AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt](https://arxiv.org/abs/2512.24625)
*Zijian Zhao,Yitong Shang,Sen Li*

Main category: cs.LG

TL;DR: 提出了一种名为AutoFed的新个性化联邦学习框架，用于交通预测，通过引入联合表示器和客户端对齐适配器来解决非独立同分布问题，无需手动调整超参数，在实际数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的交通预测方法由于隐私问题通常依赖于本地训练，导致数据孤岛现象；而标准的联邦学习在处理客户端间非独立同分布数据时存在困难。此外，当前的个性化联邦学习框架需要进一步适应交通预测任务，并且很多研究依赖于跨数据集的超参数优化，这在现实中难以实现。

Method: 开发了AutoFed框架，它利用启发自提示学习的方法，使用客户端对齐适配器将本地数据提炼成一个紧凑的、全局共享的提示矩阵。该提示矩阵为每个客户端提供了一个个性化的预测条件，从而允许它们从跨客户端的知识中受益同时保持本地特性。

Result: 在真实世界的数据集上进行的广泛实验表明，AutoFed能够在多种场景下持续地达到更好的性能表现。

Conclusion: AutoFed是一个创新的个性化联邦学习框架，专为交通预测设计，能够有效解决现有方法中的局限性，不需要手工调节超参数，展示了在不同情况下的优越性。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation Systems, including ride-hailing, urban road planning, and vehicle fleet management. However, due to significant privacy concerns surrounding traffic data, most existing methods rely on local training, resulting in data silos and limited knowledge sharing. Federated Learning (FL) offers an efficient solution through privacy-preserving collaborative training; however, standard FL struggles with the non-independent and identically distributed (non-IID) problem among clients. This challenge has led to the emergence of Personalized Federated Learning (PFL) as a promising paradigm. Nevertheless, current PFL frameworks require further adaptation for traffic prediction tasks, such as specialized graph feature engineering, data processing, and network architecture design. A notable limitation of many prior studies is their reliance on hyper-parameter optimization across datasets-information that is often unavailable in real-world scenarios-thus impeding practical deployment. To address this challenge, we propose AutoFed, a novel PFL framework for traffic prediction that eliminates the need for manual hyper-parameter tuning. Inspired by prompt learning, AutoFed introduces a federated representor that employs a client-aligned adapter to distill local data into a compact, globally shared prompt matrix. This prompt then conditions a personalized predictor, allowing each client to benefit from cross-client knowledge while maintaining local specificity. Extensive experiments on real-world datasets demonstrate that AutoFed consistently achieves superior performance across diverse scenarios. The code of this paper is provided at https://github.com/RS2002/AutoFed .

</details>


### [51] [Nested Learning: The Illusion of Deep Learning Architectures](https://arxiv.org/abs/2512.24695)
*Ali Behrouz,Meisam Razaviyayn,Peilin Zhong,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 本文提出了一种新的学习范式——嵌套学习（NL），通过一系列嵌套、多层次和/或并行的优化问题来表示机器学习模型，每个问题都有自己的上下文流。基于此，我们提出了三个核心贡献：表达性优化器、自修改学习模块和连续记忆系统，并展示了在多个任务中的良好结果。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在开发语言模型方面取得了进展，但关于此类模型如何持续学习/记忆、自我改进并找到有效解决方案仍存在根本性的挑战和未解之谜。

Method: 1. 提出了一种新的学习范式——嵌套学习（NL）；2. 发现已知基于梯度的优化器实际上是一种关联记忆模块，并基于此提出了更具有表现力的优化器；3. 利用对学习算法的新见解，设计了一个能够学习如何自我修改的序列模型；4. 提出了一个新形式的记忆系统，该系统概括了传统长/短期记忆的观点。

Result: 结合自修改序列模型与连续记忆系统，提出了一个持续学习模块Hope，在语言建模、知识融合、少量样本泛化任务、持续学习以及长上下文推理任务中显示了有希望的结果。

Conclusion: 嵌套学习为设计更具表现力的学习算法提供了哲学指导，促进了更高阶的情境内学习，并可能解锁有效的持续学习能力。

Abstract: Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.

</details>


### [52] [Causal Discovery with Mixed Latent Confounding via Precision Decomposition](https://arxiv.org/abs/2512.24696)
*Amir Asiaee,Samhita Pal,James O'quinn,James P. Long*

Main category: cs.LG

TL;DR: 本文提出了一种名为DCL-DECOR的新方法，用于从存在混合潜在混淆的线性高斯系统中进行因果发现。该方法通过将观测到的精度矩阵分解为结构化成分和低秩成分来隔离普遍存在的潜在影响，并使用相关噪声DAG学习器恢复有向边，同时建模剩余的结构化误差相关性。实验表明，这种方法在恢复有向边上比直接应用相关噪声DAG学习于混淆数据更有效。


<details>
  <summary>Details</summary>
Motivation: 当前因果发现的方法在处理混合潜在混淆（即一些未观察到的因素广泛作用于许多变量而其他因素只影响小部分变量）时面临挑战：可微分和基于分数的DAG学习者可能将全局潜在效应误解为因果边，而潜变量图形模型仅能恢复无向结构。因此需要一种能够准确区分这些影响并有效恢复真实因果结构的方法。

Method: DCL-DECOR方法首先通过分解观测精度矩阵来分离普遍性的潜在效应与局部依赖关系。接着，利用一个考虑了剩余结构化错误相关的相关噪声DAG学习算法对去混淆后的表示进行分析，以恢复有向边。最后，实施简单的调整步骤确保结果满足弓形自由条件。

Result: 合成实验显示，在改变普遍混淆强度和维度的情况下，DCL-DECOR方法在恢复有向边方面相较于直接对混淆数据应用相关噪声DAG学习具有持续改进的效果。此外，研究还提供了关于在混合混淆条件下可识别因果目标的理论结果。

Conclusion: DCL-DECOR提供了一个有效的框架，解决了因混合潜在混淆而导致的因果发现难题。通过分离普遍性和局部的影响，该方法能够更准确地恢复因果图中的有向边。

Abstract: We study causal discovery from observational data in linear Gaussian systems affected by \emph{mixed latent confounding}, where some unobserved factors act broadly across many variables while others influence only small subsets. This setting is common in practice and poses a challenge for existing methods: differentiable and score-based DAG learners can misinterpret global latent effects as causal edges, while latent-variable graphical models recover only undirected structure.
  We propose \textsc{DCL-DECOR}, a modular, precision-led pipeline that separates these roles. The method first isolates pervasive latent effects by decomposing the observed precision matrix into a structured component and a low-rank component. The structured component corresponds to the conditional distribution after accounting for pervasive confounders and retains only local dependence induced by the causal graph and localized confounding. A correlated-noise DAG learner is then applied to this deconfounded representation to recover directed edges while modeling remaining structured error correlations, followed by a simple reconciliation step to enforce bow-freeness.
  We provide identifiability results that characterize the recoverable causal target under mixed confounding and show how the overall problem reduces to well-studied subproblems with modular guarantees. Synthetic experiments that vary the strength and dimensionality of pervasive confounding demonstrate consistent improvements in directed edge recovery over applying correlated-noise DAG learning directly to the confounded data.

</details>


### [53] [FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference](https://arxiv.org/abs/2512.24713)
*Fen-Yu Hsieh,Yun-Chang Teng,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.LG

TL;DR: 本研究提出了一种自动化框架，结合了权重剪枝和低位量化技术，并通过硬件-软件协同设计方法在FPGA平台上生成加速器，以减少大型语言模型（LLM）的计算和内存需求。该方法实现了高达4倍的权重存储减少和1.71倍的矩阵乘法速度提升，从而使得与密集GPU基线相比端到端延迟降低了1.29倍。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在众多自然语言处理任务中表现出色，但其对计算资源和内存的需求极大，这限制了它们在资源受限环境中的部署。为了解决这一问题，研究者开发了一个新的自动化框架来降低LLM的资源消耗。

Method: 研究采用了N:M结构化剪枝和4位整数量化相结合的方法，旨在减少内存占用。随后，通过对去量化和矩阵乘法进行优化，进一步提高了多种硬件平台（包括CPU、具有Dense和2:4稀疏张量核心的NVIDIA GPU以及基于systolic array的自定义FPGA加速器）上的LLM推理效率。

Result: 实验结果表明，所提方法能够实现最多达4倍的权重存储减少和1.71倍的矩阵乘法加速，相较于使用密集GPU作为基准的情况，端到端延迟减少了1.29倍。此外，在LLaMA-7B模型上进行的规模分析显示，结构化稀疏性可使每令牌吞吐量提高1.36倍。

Conclusion: 这项工作展示了细粒度N:M稀疏性和量化技术相结合对于实现高效且易于部署的LLM推理的重要性。提出的FPGA加速器不仅支持更广泛的稀疏模式，还为未来研究提供了灵活的架构方向。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of language processing tasks. However, this success comes at the cost of substantial computation and memory requirements, which significantly impedes their deployment in resource-constrained environments. To address this challenge, this work introduces an automation framework that leverages weight pruning and low-bit quantization, and presents a hardware-software co-design method that generates accelerators on the Field-Programmable Gate Array (FPGA) platform. In particular, we implement a unified pipeline that applies N:M structured pruning and 4-bit integer quantization to reduce the memory footprint, followed by optimized dequantization and matrix multiplication to enhance LLM inference on several hardware platforms, including CPUs, NVIDIA GPUs with Dense and 2:4 Sparse Tensor Cores, and a custom systolic-array-based FPGA accelerator. Utilizing 2:4 sparsity combined with quantization on $4096 \times 4096$ matrices, our approach achieves a reduction of up to $4\times$ in weight storage and a $1.71\times$ speedup in matrix multiplication, yielding a $1.29\times$ end-to-end latency reduction compared to dense GPU baselines. Scaling analysis on the LLaMA-7B model further shows that structured sparsity enhances the throughput per token by $1.36\times$. These results demonstrate the synergy of fine-grained N:M sparsity and quantization for enabling efficient and deployable LLM inference, while the proposed FPGA accelerator offers a flexible architectural path for supporting a broader class of sparsity patterns beyond the fixed 2:4 hardware constraints.

</details>


### [54] [From Trial to Deployment: A SEM Analysis of Traveler Adoptions to Fully Operational Autonomous Taxis](https://arxiv.org/abs/2512.24767)
*Yutong Cai,Hua Wang*

Main category: cs.LG

TL;DR: 本研究基于中国武汉实际运营的百度Apollo Robotaxi服务，通过调查数据收集了336份有效反馈，利用结构方程模型识别出信任与政策支持、成本敏感性、性能、行为意图、生活方式和教育这六个潜在心理因素，并探讨了这些因素对用户采用自动驾驶出租车行为的影响。结果显示成本敏感性和行为意图是影响采用行为最强的正面预测因子。


<details>
  <summary>Details</summary>
Motivation: 虽然现有文献已经通过声明偏好实验和假设情景探索了用户对于自动驾驶出租车的接受度，但很少有研究基于实际运行的自动驾驶车辆服务来调查用户的真实行为。本研究旨在填补这一空白，提供关于用户真实行为的数据以及对自动驾驶出租车采纳行为背后因素的理解。

Method: 研究方法包括设计一个包含实际服务属性的现实调查问卷，并从中国武汉地区获取336位实际用户的有效回复。接着，使用结构方程建模技术分析了六个潜在的心理构建（信任与政策支持、成本敏感性、性能、行为意图、生活方式、教育）及其对选择频率作为采用行为指标的影响。

Result: 结果表明，在所有考察的因素中，成本敏感性和行为意图是对自动驾驶出租车采用行为最强有力的正向预测因素。其他潜在构造则在不同程度上发挥着作用。此外，所建立的模型在多个指标上表现出良好的拟合度。

Conclusion: 该研究为制定相关政策、票价设计及公众宣传策略提供了实证依据，以促进自动驾驶出租车在城市环境中的实际部署。

Abstract: Autonomous taxi services represent a transformative advancement in urban mobility, offering safety, efficiency, and round-the-clock operations. While existing literature has explored user acceptance of autonomous taxis through stated preference experiments and hypothetical scenarios, few studies have investigated actual user behavior based on operational AV services. This study addresses that gap by leveraging survey data from Wuhan, China, where Baidu's Apollo Robotaxi service operates at scale. We design a realistic survey incorporating actual service attributes and collect 336 valid responses from actual users. Using Structural Equation Modeling, we identify six latent psychological constructs, namely Trust \& Policy Support, Cost Sensitivity, Performance, Behavioral Intention, Lifestyle, and Education. Their influences on adoption behavior, measured by the selection frequency of autonomous taxis in ten scenarios, are examined and interpreted. Results show that Cost Sensitivity and Behavioral Intention are the strongest positive predictors of adoption, while other latent constructs play more nuanced roles. The model demonstrates strong goodness-of-fit across multiple indices. Our findings offer empirical evidence to support policymaking, fare design, and public outreach strategies for scaling autonomous taxis deployments in real-world urban settings.

</details>


### [55] [DTI-GP: Bayesian operations for drug-target interactions using deep kernel Gaussian processes](https://arxiv.org/abs/2512.24810)
*Bence Bolgár,András Millinghoffer,Péter Antal*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度核学习的高斯过程架构（DTI-GP），用于药物-靶点相互作用预测。该方法结合了化学化合物和蛋白质靶点的神经嵌入模块以及一个高斯过程模块，能够提供精确的概率信息，并支持贝叶斯分类、前K选择及排序等操作。


<details>
  <summary>Details</summary>
Motivation: 准确的概率信息对于理解药物-靶点相互作用预测的局限性并提高其预测性能至关重要。因此，需要一种能够整合最先进的DTI表示和贝叶斯推理的方法来实现这一点。

Method: 提出了一个基于深度核学习的高斯过程架构（DTI-GP）。该架构包括一个针对化学化合物和蛋白质靶点的组合神经嵌入模块以及一个高斯过程模块。通过从预测分布中采样来估计贝叶斯优先级矩阵，以进行快速而准确的选择和排名操作。

Result: DTI-GP优于现有最先进解决方案的表现，并且允许(1)构建贝叶斯准确性-置信度富集分数，(2)采用拒绝方案以改善富集效果，以及(3)估计并搜索具有高预期效用的前K选择与排名。

Conclusion: DTI-GP不仅在预测性能上超越了当前最先进的方法，还提供了新的功能如贝叶斯分类带拒绝、前K选择及有效排名等，这些都对药物发现领域有着重要的意义。

Abstract: Precise probabilistic information about drug-target interaction (DTI) predictions is vital for understanding limitations and boosting predictive performance. Gaussian processes (GP) offer a scalable framework to integrate state-of-the-art DTI representations and Bayesian inference, enabling novel operations, such as Bayesian classification with rejection, top-$K$ selection, and ranking. We propose a deep kernel learning-based GP architecture (DTI-GP), which incorporates a combined neural embedding module for chemical compounds and protein targets, and a GP module. The workflow continues with sampling from the predictive distribution to estimate a Bayesian precedence matrix, which is used in fast and accurate selection and ranking operations. DTI-GP outperforms state-of-the-art solutions, and it allows (1) the construction of a Bayesian accuracy-confidence enrichment score, (2) rejection schemes for improved enrichment, and (3) estimation and search for top-$K$ selections and ranking with high expected utility.

</details>


### [56] [Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback](https://arxiv.org/abs/2512.24818)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文提出了使用乐观乘性权重更新（OMWU）算法在从人类反馈中进行纳什学习（NLHF）时的首个收敛性保证，该方法在存在全支持纳什均衡的情况下能实现线性收敛，并且不依赖于纳什均衡唯一性的假设。实验结果证明了OMWU在表格型和神经策略类中的理论优势。


<details>
  <summary>Details</summary>
Motivation: 标准的偏好建模采用Bradley-Terry模型，但该模型假设了传递性，忽视了人群偏好内在的复杂性。为解决这一问题，研究者提出了通过将非传递性偏好视为双人零和博弈来处理的纳什学习法（NLHF），但是现有的算法通常依赖于正则化，在计算原博弈中的对偶间隙时会产生不可避免的偏差。

Method: 研究提出了一种乐观乘性权重更新($\mathtt{OMWU}$)算法用于NLHF，并提供了其在存在全支持纳什均衡情况下能够实现最后一次迭代线性收敛的首份保证。此外，还发现了一种新的边际收敛行为，即很少被选择的动作的概率会以指数方式增长，这使得与先前结果相比，对实例相关常数有更好的依赖性。

Result: 实验验证了$\mathtt{OMWU}$在表格型和神经策略类中的理论优势，展示了其在大规模语言模型应用方面的潜力。

Conclusion: 本研究表明，乐观乘性权重更新算法能够在不需要纳什均衡唯一性假设的前提下，有效地解决从人类反馈中学习纳什均衡的问题，并且对于实际应用具有很好的前景。

Abstract: Aligning large language models (LLMs) with human preferences has proven effective for enhancing model capabilities, yet standard preference modeling using the Bradley-Terry model assumes transitivity, overlooking the inherent complexity of human population preferences. Nash learning from human feedback (NLHF) addresses this by framing non-transitive preferences as a two-player zero-sum game, where alignment reduces to finding the Nash equilibrium (NE). However, existing algorithms typically rely on regularization, incurring unavoidable bias when computing the duality gap in the original game. In this work, we provide the first convergence guarantee for Optimistic Multiplicative Weights Update ($\mathtt{OMWU}$) in NLHF, showing that it achieves last-iterate linear convergence after a burn-in phase whenever an NE with full support exists, with an instance-dependent linear convergence rate to the original NE, measured by duality gaps. Compared to prior results in Wei et al. (2020), we do not require the assumption of NE uniqueness. Our analysis identifies a novel marginal convergence behavior, where the probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants than prior results. Experiments corroborate the theoretical strengths of $\mathtt{OMWU}$ in both tabular and neural policy classes, demonstrating its potential for LLM applications.

</details>


### [57] [Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics](https://arxiv.org/abs/2512.24827)
*Raul D. Steleac,Mohan Sridharan,David Abel*

Main category: cs.LG

TL;DR: 提出了一种新的多智能体选项发现方法，通过联合状态抽象压缩状态空间，并保持发现强协调行为所需的信息。该方法基于一种归纳偏置，即在没有明确目标的情况下，智能体状态之间的同步为协调提供了自然基础。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，随着智能体数量增加，联合状态空间呈指数增长，这使得协调行为变得尤为重要。然而，这种增长也给多智能体选项的设计带来了挑战。现有的多智能体选项发现方法往往牺牲了协调性，产生松散耦合或完全独立的行为。

Method: 1. 提出一个联合状态抽象方法来压缩状态空间，同时保留发现强协调行为所需的信息。
2. 通过近似一个与团队最大对齐的虚构状态（Fermat状态），并使用它定义一个衡量“分散度”的指标，捕捉团队层面在每个单独状态维度上的不一致。
3. 基于上述表示，采用神经图拉普拉斯估计器导出能够捕捉智能体之间状态同步模式的选项。

Result: 在两个多智能体领域中的多个场景下评估了所得到的选项，结果显示与替代选项发现方法相比，它们产生了更强的下游协调能力。

Conclusion: 本研究介绍的方法为多智能体系统中发现促进强协调行为的选项提供了一个有效途径，特别是当面临因智能体数目增加而导致的状态空间爆炸问题时。

Abstract: Temporally extended actions improve the ability to explore and plan in single-agent settings. In multi-agent settings, the exponential growth of the joint state space with the number of agents makes coordinated behaviours even more valuable. Yet, this same exponential growth renders the design of multi-agent options particularly challenging. Existing multi-agent option discovery methods often sacrifice coordination by producing loosely coupled or fully independent behaviours. Toward addressing these limitations, we describe a novel approach for multi-agent option discovery. Specifically, we propose a joint-state abstraction that compresses the state space while preserving the information necessary to discover strongly coordinated behaviours. Our approach builds on the inductive bias that synchronisation over agent states provides a natural foundation for coordination in the absence of explicit objectives. We first approximate a fictitious state of maximal alignment with the team, the \textit{Fermat} state, and use it to define a measure of \textit{spreadness}, capturing team-level misalignment on each individual state dimension. Building on this representation, we then employ a neural graph Laplacian estimator to derive options that capture state synchronisation patterns between agents. We evaluate the resulting options across multiple scenarios in two multi-agent domains, showing that they yield stronger downstream coordination capabilities compared to alternative option discovery methods.

</details>


### [58] [AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference](https://arxiv.org/abs/2512.24847)
*Linhao Fan,Hongqiang Fang,Jingyang Dai,Yong Jiang,Qixing Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散贝叶斯推理的概率重构框架AODDiff，用于气溶胶光学厚度（AOD）场的高质量重建。通过从自然不完整数据中学习时空AOD先验，并采用解耦退火后验采样策略，该框架能够在不同的重建任务之间灵活适应，同时提供不确定性量化。实验结果表明AODDiff在降尺度和图像修复任务上具有高效性和鲁棒性，特别是在保持高空间光谱保真度方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前模型受限于完整训练数据的稀缺性和缺乏不确定性量化的问题，影响了AOD场重建的质量。

Method: 提出了AODDiff框架，它基于扩散贝叶斯推理，利用从自然不完整数据中学到的时空AOD概率分布作为生成先验。采用了考虑数据损坏情况下的训练策略及解耦退火后验采样方法，以更有效地整合不同类型的观测作为约束条件指导生成过程。

Result: 通过再分析数据上的广泛实验验证了所提框架的有效性和鲁棒性，在降尺度与图像修复任务上表现优异，尤其在保持高空间光谱保真度方面。此外，作为生成模型，AODDiff能够通过多重采样自然地实现不确定性量化，为下游应用提供了重要的置信度指标。

Conclusion: AODDiff框架展示了在多种重建任务中的灵活性以及对异构观测数据的有效整合能力，同时其内在支持不确定性量化的特点使其成为大气监测领域的一项重要进展。

Abstract: High-quality reconstruction of Aerosol Optical Depth (AOD) fields is critical for Atmosphere monitoring, yet current models remain constrained by the scarcity of complete training data and a lack of uncertainty quantification.To address these limitations, we propose AODDiff, a probabilistic reconstruction framework based on diffusion-based Bayesian inference. By leveraging the learned spatiotemporal probability distribution of the AOD field as a generative prior, this framework can be flexibly adapted to various reconstruction tasks without requiring task-specific retraining. We first introduce a corruption-aware training strategy to learns a spatiotemporal AOD prior solely from naturally incomplete data. Subsequently, we employ a decoupled annealing posterior sampling strategy that enables the more effective and integration of heterogeneous observations as constraints to guide the generation process. We validate the proposed framework through extensive experiments on Reanalysis data. Results across downscaling and inpainting tasks confirm the efficacy and robustness of AODDiff, specifically demonstrating its advantage in maintaining high spatial spectral fidelity. Furthermore, as a generative model, AODDiff inherently enables uncertainty quantification via multiple sampling, offering critical confidence metrics for downstream applications.

</details>


### [59] [Characterization of Transfer Using Multi-task Learning Curves](https://arxiv.org/abs/2512.24866)
*András Millinghoffer,Bence Bolgár,Péter Antal*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法来量化多任务学习曲线，以更好地捕捉多任务学习中的迁移效应，并通过药物-靶点相互作用数据集进行了评估。


<details>
  <summary>Details</summary>
Motivation: 作者认为，通过增加样本量来扰动数据集，而非通过梯度更新来扰动模型，为迁移效应提供了一个更加基础和互补的表征方式。

Method: 研究者们开发了一种有效的方法来近似多任务学习曲线，这种方法类似于在训练过程中应用的任务亲和度分组方法。他们还比较了统计学与计算方法在处理迁移问题上的表现。

Result: 结果表明，使用学习曲线可以更有效地捕获多任务学习的影响，并且其扩展版本能够区分基础模型中的成对及上下文迁移效应。

Conclusion: 新提出的学习曲线方法不仅提高了迁移学习研究的能力，也展示了在不同任务间识别特定类型迁移效应的可能性。

Abstract: Transfer effects manifest themselves both during training using a fixed data set and in inductive inference using accumulating data. We hypothesize that perturbing the data set by including more samples, instead of perturbing the model by gradient updates, provides a complementary and more fundamental characterization of transfer effects. To capture this phenomenon, we quantitatively model transfer effects using multi-task learning curves approximating the inductive performance over varying sample sizes. We describe an efficient method to approximate multi-task learning curves analogous to the Task Affinity Grouping method applied during training. We compare the statistical and computational approaches to transfer, which indicates considerably higher compute costs for the previous but better power and broader applicability. Evaluations are performed using a benchmark drug-target interaction data set. Our results show that learning curves can better capture the effects of multi-task learning and their multi-task extensions can delineate pairwise and contextual transfer effects in foundation models.

</details>


### [60] [Frequent subgraph-based persistent homology for graph classification](https://arxiv.org/abs/2512.24917)
*Xinyang Chen,Amaël Broustet,Guoting Chen*

Main category: cs.LG

TL;DR: This paper introduces Frequent Subgraph Filtration (FSF), a new graph filtration method, to generate more informative and stable persistent homology (PH) features for graph classification. The authors also propose FPH-ML and FPH-GNNs models that integrate these features into machine learning and deep learning, respectively, showing significant improvements in performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of current PH methods on graphs, which often rely on simple filtrations like degree or weight, thereby missing out on richer topological information. By introducing FSF, the authors aim to capture more complex and recurring patterns across datasets, enhancing the expressive power and interpretability of PH-based models.

Method: The authors propose Frequent Subgraph Filtration (FSF) as a novel approach to generate frequency-based persistent homology (FPH) features. They analyze the theoretical properties of FSF and validate it through experiments. Additionally, they develop two models for graph classification: FPH-ML, which uses traditional machine learning techniques, and FPH-GNN, a hybrid model that integrates FPH with graph neural networks (GNNs).

Result: The experimental results demonstrate that the FPH-ML model achieves competitive or better accuracy compared to kernel-based and degree-based filtration methods. Furthermore, when FPH is integrated into GNNs, the FPH-GNN model shows relative performance gains ranging from 0.4% to 21%, with up to 8.2 percentage points improvement over GCN and GIN backbones across different benchmarks.

Conclusion: The conclusion of this work highlights the effectiveness of Frequent Subgraph Filtration (FSF) in generating stable and informative persistent homology (PH) features for graph classification. The integration of FPH into both traditional machine learning and deep learning models significantly enhances their performance, suggesting a promising direction for future research in topology-aware feature extraction and graph representation learning.

Abstract: Persistent homology (PH) has recently emerged as a powerful tool for extracting topological features. Integrating PH into machine learning and deep learning models enhances topology awareness and interpretability. However, most PH methods on graphs rely on a limited set of filtrations, such as degree-based or weight-based filtrations, which overlook richer features like recurring information across the dataset and thus restrict expressive power. In this work, we propose a novel graph filtration called Frequent Subgraph Filtration (FSF), which is derived from frequent subgraphs and produces stable and information-rich frequency-based persistent homology (FPH) features. We study the theoretical properties of FSF and provide both proofs and experimental validation. Beyond persistent homology itself, we introduce two approaches for graph classification: an FPH-based machine learning model (FPH-ML) and a hybrid framework that integrates FPH with graph neural networks (FPH-GNNs) to enhance topology-aware graph representation learning. Our frameworks bridge frequent subgraph mining and topological data analysis, offering a new perspective on topology-aware feature extraction. Experimental results show that FPH-ML achieves competitive or superior accuracy compared with kernel-based and degree-based filtration methods. When integrated into graph neural networks, FPH yields relative performance gains ranging from 0.4 to 21 percent, with improvements of up to 8.2 percentage points over GCN and GIN backbones across benchmarks.

</details>


### [61] [MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control](https://arxiv.org/abs/2512.24955)
*Yongwei Zhang,Yuanzhe Xing,Quan Quan,Zhikun She*

Main category: cs.LG

TL;DR: 本文提出了一种名为MSACL的框架，该框架结合了指数稳定性理论与最大熵强化学习，通过多步李雅普诺夫证书学习来实现模型无关强化学习中的可证明稳定性。MSACL利用非策略多步数据学习满足理论稳定性条件的李雅普诺夫证书，并通过引入指数稳定性标签(ESL)和$λ$-加权聚合机制来平衡多步学习中的偏差-方差折衷。实验表明MSACL在六个基准测试中优于当前基于李雅普诺夫的RL算法，展现了其在简单奖励下的指数稳定性和快速收敛性以及对不确定性的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在模型无关的强化学习(RL)中实现可证明的稳定性是一个挑战，尤其是在探索与严格安全之间取得平衡时。传统的做法依赖于复杂的奖励工程设计，而缺乏一种能够确保系统稳定同时促进高效学习的方法。

Method: 提出了MSACL框架，它将指数稳定性理论与最大熵RL相结合，并通过多步李雅普诺夫证书学习来达成这一目标。此方法利用了非策略多步数据来学习符合理论稳定性要求的李雅普诺夫证书。此外，还引入了指数稳定性标签(ESL)和$λ$-加权聚合机制以有效处理多步学习过程中的偏差-方差问题。

Result: MSACL在包括稳定化和非线性跟踪任务在内的六个基准上进行了评估，结果表明其性能优于最新的基于李雅普诺夫的RL算法。MSACL能够在使用简单的奖励函数情况下达到指数级稳定性并快速收敛，同时也显示出了对不确定性极强的鲁棒性和对于未见过轨迹的良好泛化能力。

Conclusion: 通过将李雅普诺夫理论与非策略演员-评论家框架联系起来，MSACL为基于学习的安全控制提供了验证基础。研究还发现多步范围$n=20$是跨不同系统的稳健默认值。

Abstract: Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.

</details>


### [62] [Attribution-Guided Distillation of Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2512.24975)
*Cristina P. Martin-Linares,Jonathan P. Ling*

Main category: cs.LG

TL;DR: 提出了一种名为DMSAEs的训练方法，通过迭代提炼过程来提取并重用一组一致有用的特征，从而提高稀疏自动编码器(SAE)的学习特征的可解释性和一致性。实验结果表明该方法可以提高SAE的一些性能指标，并且能够在不同稀疏度级别之间转移一组一致的潜在特征。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自动编码器学习到的特征往往存在冗余性，且在不同的训练运行和稀疏度水平下变化较大，这导致了特征难以被解读、转移及重用的问题。

Method: 研究者引入了Distilled Matryoshka Sparse Autoencoders (DMSAEs)，这是一种新的训练流程，旨在提炼出一组紧凑的核心特征，并能够重复利用这些核心特征来训练新的SAE模型。DMSAEs采用迭代式的提炼循环：首先训练一个具有共享核心的Matryoshka SAE，然后利用梯度乘以激活值的方法衡量每个特征对于最内层重构中下一个token损失的贡献，并仅保留能解释固定比例归因的最小特征子集。在每次循环中，仅有核心编码器权重向量被传递；而核心解码器及所有非核心潜变量则每次都重新初始化。

Result: 通过对Gemma-2-2B第12层残差流激活进行测试，在经过7轮提炼（处理5亿个tokens，宽度为65,000）后，得到了一个由197个特征组成的精炼核心，这些特征在多次选择过程中被反复挑选出来。使用这个精炼核心进行训练提高了多个SAEBench评价指标的表现，证明了可以跨不同稀疏度级别迁移一组一致性的潜在特征。

Conclusion: DMSAEs提供了一种有效的方法来提升稀疏自动编码器学习特征的一致性和可解释性，同时展示了如何在不同稀疏度设置间成功地转移一组有用且稳定的潜在特征。

Abstract: Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: train a Matryoshka SAE with a shared core, use gradient X activation to measure each feature's contribution to next-token loss in the most nested reconstruction, and keep only the smallest subset that explains a fixed fraction of the attribution. Only the core encoder weight vectors are transferred across cycles; the core decoder and all non-core latents are reinitialized each time. On Gemma-2-2B layer 12 residual stream activations, seven cycles of distillation (500M tokens, 65k width) yielded a distilled core of 197 features that were repeatedly selected. Training using this distilled core improves several SAEBench metrics and demonstrates that consistent sets of latent features can be transferred across sparsity levels

</details>


### [63] [Efficiently Estimating Data Efficiency for Language Model Fine-tuning](https://arxiv.org/abs/2512.24991)
*Gyung Hyun Je,Colin Raffel*

Main category: cs.LG

TL;DR: 该论文提出了一种基于少量标注样本的梯度余弦相似性方法来预测任务的数据效率，从而避免了增量注释和重新训练的成本。实验结果表明，此方法在多种不同数据效率的任务上能够达到8.6%的整体数据效率预测误差，并且通常能在每个任务上减少数百个不必要的注释。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在许多下游任务中表现出合理的零样本能力，但微调是提高其性能的一种常见做法。然而，一个任务的数据效率——即达到所需性能水平所需的微调样本数量——往往是未知的，这导致了代价高昂的增量注释与重新训练循环。研究者们发现，即使是在零样本情况下表现不佳的高性能LLMs，通过微调后也能获得更强的性能。这促使了寻找无需增量注释即可预测任务数据效率的方法的需求。

Method: 首先定义了一个具体度量来量化任务的数据效率。然后，提出了使用低置信度例子的梯度余弦相似性来基于少量标记样本预测数据效率的方法。

Result: 所提出的方法在一个包含不同数据效率的多样化任务集上进行了验证，达到了8.6%的数据效率预测总体误差，并且通常能消除每个任务上的数百次不必要的注释。

Conclusion: 本文介绍的方法能够有效地预测任务的数据效率而不需要进行额外的大量注释工作，这有助于减少相关成本并加速模型开发流程。

Abstract: While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.

</details>


### [64] [Diffusion Language Models are Provably Optimal Parallel Samplers](https://arxiv.org/abs/2512.25014)
*Haozhe Jiang,Nika Haghtalab,Lijie Chen*

Main category: cs.LG

TL;DR: 扩散语言模型（DLMs）通过并行生成标记，能够作为自回归模型的替代方案来加速推理。研究证明了当目标分布可以通过少量顺序步骤生成时，DLMs可以在相同数量的最佳顺序步骤内生成该分布。此外，允许修订或重新遮罩可以进一步优化空间复杂度，并且具有这些功能的DLMs比没有的更加表达能力强。


<details>
  <summary>Details</summary>
Motivation: 探讨扩散语言模型（DLMs）在并行生成标记方面相对于自回归模型的优势，以及如何通过允许修订或重新遮罩进一步提升其效率与表达能力。

Method: 通过形式化一个并行采样的模型来分析DLMs结合多项式长度的思维链（CoT）模拟任何并行采样算法的能力，并考察修订和重新遮罩对于优化空间复杂度的影响。

Result: 结果表明，当目标分布可以通过少量顺序步骤生成时，DLMs能以最优顺序步骤数生成；并且启用修订或重新遮罩可以让DLMs在保持最优空间复杂度的同时模拟任何并行采样算法。此外，还证实了具备修订或重新遮罩能力的DLMs在表达力上优于不具备这些特性的模型。

Conclusion: 本研究表明，DLMs不仅在理论上被证明为最有效的并行采样器，而且建议在实践中启用修订机制以增强其性能。

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.

</details>


### [65] [Generative Classifiers Avoid Shortcut Solutions](https://arxiv.org/abs/2512.25034)
*Alexander C. Li,Ananya Kumar,Deepak Pathak*

Main category: cs.LG

TL;DR: 生成式分类器通过建模所有特征而非仅依赖于虚假相关特征，能够在分布偏移的情况下避免判别式方法常见的失效问题，并在图像和文本基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 判别式分类方法往往学习到只在训练数据分布内有效的捷径，这些捷径基于与标签虚假相关的特征，在轻微的分布变化下容易失败。

Method: 采用基于类条件生成模型的生成式分类器，这种方法能够对核心特征和虚假特征都进行建模，而不需要特殊的增强、强正则化、额外的超参数调整或事先知道要避免的具体虚假相关性。

Result: 扩散式和自回归生成式分类器在五个标准图像和文本分布偏移基准上达到了最先进的表现，并且减少了实际应用（如医疗或卫星数据集）中虚假相关性的影响。

Conclusion: 生成式分类器通过处理所有类型的特征来减轻虚假相关性带来的负面影响，在面对分布偏移时表现出更强的鲁棒性。

Abstract: Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.

</details>


### [66] [On the geometry and topology of representations: the manifolds of modular addition](https://arxiv.org/abs/2512.25060)
*Gabriela Moisescu-Pareja,Gavin McCracken,Harley Wiltzer,Vincent Létourneau,Colin Daniels,Doina Precup,Jonathan Love*

Main category: cs.LG

TL;DR: 研究发现，无论是均匀注意力架构还是可学习注意力架构，在实现模块加法时都采用了拓扑和几何上等价的表示方法。通过将每个学习到的表示视为一个整体来研究，揭示了这些表示形成了可以利用拓扑工具进行分析的流形，从而证明了从常见深度学习范式中自然产生的学习模块加法电路之间的相似性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨不同的架构设计（即均匀注意力与可训练注意力）是否会导致在执行模块加法任务时产生根本不同的电路。基于先前提出的Clock和Pizza解释，作者希望验证这些假设，并进一步了解不同类型的注意力机制如何影响模型内部表示的学习方式。

Method: 采用了一种超越单个神经元及其权重解读的方法论，该方法首先识别出对应于每一个学到表示的所有神经元，然后将这些神经元群体作为一个整体来进行研究。通过这种方法，研究人员能够利用拓扑学中的工具来分析由大量神经元组成的流形，进而对数百个电路中学习到的表示进行统计分析。

Result: 结果显示，无论是在均匀注意力还是可训练注意力架构下，所学到的用于执行模块加法任务的表示都是拓扑和几何上等价的。这表明，两种看似不同的架构实际上实现了相同的算法，并且生成了非常相似的内部表示结构。

Conclusion: 本研究表明，对于模块加法这样的任务而言，均匀注意力与可训练注意力架构之间并没有本质上的区别；两者都能够以极其相似的方式实现相同的功能。这一发现挑战了之前关于特定架构选择可能带来完全不同结果的观点。

Abstract: The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition. In this work, we show that this is not the case, and that both uniform attention and trainable attention architectures implement the same algorithm via topologically and geometrically equivalent representations. Our methodology goes beyond the interpretation of individual neurons and weights. Instead, we identify all of the neurons corresponding to each learned representation and then study the collective group of neurons as one entity. This method reveals that each learned representation is a manifold that we can study utilizing tools from topology. Based on this insight, we can statistically analyze the learned representations across hundreds of circuits to demonstrate the similarity between learned modular addition circuits that arise naturally from common deep learning paradigms.

</details>


### [67] [Scaling Open-Ended Reasoning to Predict the Future](https://arxiv.org/abs/2512.25070)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.LG

TL;DR: 研究通过合成预测问题训练语言模型来进行高风险决策的未来预测，使用离线新闻语料库防止信息泄露，并且开发了OpenForecaster 8B模型，该模型在准确度、校准和一致性方面表现出色。所有模型、代码和数据都开源共享以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 为了提高对未来开放性预测问题进行推理的能力，特别是在高风险决策中，研究旨在通过自动化生成预测问题来扩大训练数据集规模，并确保在训练与评估过程中不会泄露未来信息。

Method: 研究采用了一种全自动精心策划的方法从日常新闻中的全球事件合成新的预测问题，利用Qwen3思维模型在创建的数据集OpenForesight上进行训练。为避免未来信息泄露，整个过程使用了离线新闻语料库作为数据生成与检索的基础。此外，还基于一个小验证集优化了检索机制及强化学习的奖励函数。

Result: 最终开发出的预测系统OpenForecaster 8B能够在保持较小规模的同时达到与更大专有模型相当的表现，在预测准确性、校准以及一致性方面均有显著提升。这些改进不仅限于特定基准测试中也得到了体现。

Conclusion: 本研究表明，通过精心设计的数据集和训练方法可以有效提升语言模型在未来预测任务上的表现，特别是对于需要处理不确定性信息的高风险决策场景而言。同时，公开研究成果有助于进一步推动该领域的发展。

Abstract: High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [68] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 本文提出了一种基于测量和容器的资源管理框架，用于优化单一边缘服务器上托管的多种异构应用程序的资源分配。通过建立一个非线性拟合模型来描述CPU/内存分配与处理延迟之间的关系，并据此构建了一个混合整数非线性规划问题以最小化系统延迟和能耗。该问题通过两阶段容器资源管理方案（CRMS）得到有效解决，模拟结果显示CRMS相比其他基线方法在减少延迟和提高能效方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 边缘计算允许对时延敏感的应用程序在靠近终端设备的地方处理数据，但任务异构性和有限资源给高效编排带来了挑战。为了解决这些问题，本研究旨在开发一种能够有效管理和优化单个边缘服务器上的多类型应用资源使用的框架。

Method: 1. 通过广泛的性能测试实验，建立了一个非线性拟合模型，用以刻画不同工作负载下CPU/内存分配与处理延迟间的关系。
2. 利用上述模型及基于排队论的延迟公式，构建了目标为同时最小化系统延迟与功耗的混合整数非线性规划问题。
3. 提出了一种两阶段容器资源管理方案(CRMS)，首先将原问题分解为可解的凸子问题，然后结合凸优化技术和贪婪细化算法求解。

Result: 仿真结果表明，相比于启发式和其他搜索为基础的方法，所提出的CRMS方案能够在降低超过14%的延迟的同时提升能源效率。

Conclusion: 提出的基于容器的资源管理方案CRMS提供了一种实用且可扩展的解决方案，适用于具有动态工作负载特性的异构边缘环境，在减少延迟和提高能效方面表现出色。

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [69] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: 本文提出了一种名为PackKV的高效KV缓存管理框架，专门针对长上下文生成进行了优化。通过引入专为KV缓存数据特点设计的有损压缩技术，实现了在保持高计算效率的同时显著减少内存占用，并且提高了执行吞吐量。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的大规模语言模型（LLM）在众多实际应用中展现出了巨大潜力，但在处理长上下文推理时面临重大挑战，特别是由于随着序列长度和批处理大小增加而迅速增长的关键-值(KV)缓存所需的大量内存。

Method: PackKV采用了一种新颖的、针对KV缓存数据特征定制的有损压缩技术，结合了压缩算法与系统架构的精心协同设计。该方法不仅兼容KV缓存动态增长的特点，还维持了较高的计算效率。

Result: 实验结果显示，在保证精度下降幅度与最先进的量化方法相同的情况下，PackKV对K缓存平均达到了153.2%更高的内存缩减率，对于V缓存则是179.6%。此外，PackKV大幅提升了执行吞吐量，特别是在A100和RTX Pro 6000 GPU上相对于cuBLAS矩阵向量乘法内核而言，K缓存的平均吞吐量提高了75.7%，V缓存则提升了171.7%，同时所需GPU内存带宽更少。

Conclusion: PackKV作为一种通用且高效的KV缓存管理解决方案，在支持长上下文生成任务方面表现出色，它不仅大幅度减少了内存使用，而且显著提高了计算性能。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [70] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 随着LLM和基础模型的扩展，检查点/恢复已成为训练和推理的关键模式。本文探讨了在3D并行性下使用liburing进行检查点操作的有效性，并通过微基准测试评估了聚合、对齐和I/O合并策略的效果。结果表明，与DataStates-LLM相比，该方法可实现高达3.9倍的写入吞吐量，与TorchSnapshot相比则高达7.6倍。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和基础模型的规模增长，检查点/恢复机制对于训练和推理变得至关重要。然而，在处理大量不同形状和大小的张量时，这种机制变成了一个大数据I/O问题。现有解决方案存在性能瓶颈，特别是在并发情况下。尽管像liburing这样的内核加速I/O库可能有助于缓解这些问题，但它们对于LLM检查点的实际效果仍需进一步研究。

Method: 研究人员开发了微基准测试来量化使用liburing时的各种权衡，特别是考察在缓冲和直接I/O条件下聚合、对齐以及I/O合并如何相互作用。

Result: 实验发现，未合并的小缓冲区操作会导致合成工作负载下的吞吐量减半；而文件系统感知的聚合能够恢复带宽并减少元数据开销。与当前最先进的LLM检查点引擎相比，所提出的方法分别实现了比DataStates-LLM高3.9倍、比TorchSnapshot高7.6倍的写入吞吐量。

Conclusion: 研究结果强调了需要采用与现代文件系统和I/O后端相适应的聚合和合并策略，以提高大规模模型的检查点/恢复效率。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [71] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出了首个资源自适应的分布式双层优化框架，采用无二阶超梯度估计器，使每个客户端能够根据可用资源优化子模型。理论分析证明了RABO和RAFBO方法能达到渐近最优收敛率，并通过实验验证了其有效性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，传统的分布式双层优化算法在资源有限的客户端上直接应用遇到了挑战，主要原因是同时优化下层和上层函数需要大量的计算。

Method: 开发了一种新的资源自适应分布式双层优化框架，该框架包括一个无需二阶导数的超梯度估计器，允许每个客户端根据自身资源情况来优化适合的子模型。针对内外参数部分耦合导致的问题，重新定义了内参数的误差界限。

Result: 通过可证明的定理表明，无论是RABO还是RAFBO方法都能达到渐近最优的收敛速度$O(1/\sqrt{C_x^{\ast}Q})$，这一结果受到外参数最小覆盖$C_x^{\ast}$的影响。此外，在两个不同任务上的广泛实验进一步证实了所提出方法的有效性和计算效率。

Conclusion: 本研究提出的资源自适应分布式双层优化方法不仅解决了资源受限环境下传统双层优化算法难以直接应用的问题，还通过实验证明了其在提高计算效率方面的有效性。

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [72] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 本文提出了一种基于AI的自适应资源优化框架，用于多集群云系统中的主动协调资源管理。通过分析跨集群遥测和历史执行模式，该框架动态调整资源配置以平衡性能、成本和可靠性目标。原型实现表明与传统反应式方法相比，本方法提高了资源效率，加快了工作负载波动期间的稳定速度，并减少了性能变化。


<details>
  <summary>Details</summary>
Motivation: 当前的资源管理方法主要为被动且以单个集群为中心，这限制了它们在动态工作负载下优化整个系统行为的能力。这样的局限性导致了分布式环境中资源利用率低下、适应延迟以及运营开销增加的问题。

Method: 提出了一个结合预测学习、策略感知决策制定及持续反馈机制的人工智能驱动框架，旨在实现跨集群的主动和协调资源管理。

Result: 实验结果表明，所提出的方案相较于传统的响应式方法，在资源使用效率方面有所提高，同时能够在工作负载变动时更快达到稳定状态，并降低了性能波动幅度。

Conclusion: 智能自适应基础设施管理作为实现可扩展性和弹性的云平台的关键推动因素，其有效性得到了证明。

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [73] [DEFT: Differentiable Automatic Test Pattern Generation](https://arxiv.org/abs/2512.23746)
*Wei Li,Yan Zou,Yixin Liang,José Moura,Shawn Blanton*

Main category: cs.SE

TL;DR: 本文提出了一种新的自动测试模式生成方法DEFT，通过将离散的ATPG问题重新表述为连续优化任务来提高对难以检测故障的测试效果。DEFT在两个工业基准测试中相比领先商业工具显著提高了难检故障的检测率，并且支持实际ATPG设置如部分分配模式生成，显示出作为现有启发式方法有价值的补充的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路复杂性导致测试模式增长，大多数模式针对一小部分难以检测（HTD）的故障。这促使开发新的ATPG算法以专门改善对HTD故障的测试效果。

Method: DEFT（可微分自动测试模式生成），通过数学上合理的重参数化将离散ATPG问题转换成连续优化任务，该方法引入了自定义CUDA内核以确保深度电路图上的可扩展性和稳定性，并应用梯度归一化来缓解消失梯度问题。

Result: 与领先的商业工具相比，在相同的模式预算和相当的运行时间下，DEFT在两个工业基准上平均提升了21.1%和48.9%的HTD故障检测率；此外，在支持实际ATPG设置如部分分配模式生成时，产生的模式0/1位减少了19.3%，同时仍能检测到更多的故障。

Conclusion: DEFT是一种有前途且有效的ATPG引擎，能够提供比现有启发式方法更优的结果，尤其是在处理难以检测的故障方面表现出色。

Abstract: Modern IC complexity drives test pattern growth, with the majority of patterns targeting a small set of hard-to-detect (HTD) faults. This motivates new ATPG algorithms to improve test effectiveness specifically for HTD faults. This paper presents DEFT (Differentiable Automatic Test Pattern Generation), a new ATPG approach that reformulates the discrete ATPG problem as a continuous optimization task. DEFT introduces a mathematically grounded reparameterization that aligns the expected continuous objective with discrete fault-detection semantics, enabling reliable gradient-based pattern generation. To ensure scalability and stability on deep circuit graphs, DEFT integrates a custom CUDA kernel for efficient forward-backward propagation and applies gradient normalization to mitigate vanishing gradients. Compared to a leading commercial tool on two industrial benchmarks, DEFT improves HTD fault detection by 21.1% and 48.9% on average under the same pattern budget and comparable runtime. DEFT also supports practical ATPG settings such as partial assignment pattern generation, producing patterns with 19.3% fewer 0/1 bits while still detecting 35% more faults. These results indicate DEFT is a promising and effective ATPG engine, offering a valuable complement to existing heuristic.

</details>


### [74] [State-of-the-art Small Language Coder Model: Mify-Coder](https://arxiv.org/abs/2512.23747)
*Abhinav Parmar,Abhisek Panigrahi,Abhishek Kumar Dwivedi,Abhishek Bhattacharya,Adarsh Ramachandra,Aditya Choudhary,Aditya Garg,Aditya Raj,Alankrit Bhatt,Alpesh Yadav,Anant Vishnu,Ananthu Pillai,Ankush Kumar,Aryan Patnaik,Aswatha Narayanan S,Avanish Raj Singh,Bhavya Shree Gadda,Brijesh Pankajbhai Kachhadiya,Buggala Jahnavi,Chidurala Nithin Krishna,Chintan Shah,Chunduru Akshaya,Debarshi Banerjee,Debrup Dey,Deepa R.,Deepika B G,Faiz ur Rahman,Gagan Gayari,Gudhi Jagadeesh Kumar Naidu,Gursimar Singh,Harshal Tyagi,Harshini K,James Mani Vathalloor,Jayarama Nettar,Jayashree Gajjam,Joe Walter Sugil George,Kamalakara Sri Krishna Tadepalli,Kamalkumar Rathinasamy,Karan Chaurasia,Karthikeyan S,Kashish Arora,Kaushal Desai,Khushboo Buwade,Kiran Manjrekar,Malikireddy Venkata Sai Likhitha,Manjunath A,Mitali Mahavir Bedmutha,Mohammed Rafee Tarafdar,Nikhil Tiwari,Nikitha K Gigi,Pavan Ravikumar,Pendyala Swarnanjali,Piyush Anand,Prakash Chandrasekar,Prasanna Bhalchandra Gawade,Prasanth Sivan,Preeti Khurana,Priyanshi Babbar,Rajab Ali Mondal,Rajesh Kumar Vissapragada,Rajeshwari Ganesan,Rajeswari Koppisetti,Ramjee R.,Ramkumar Thiruppathisamy,Rani G. S.,S Reka,Samarth Gupta,Sandeep Reddy Kothakota,Sarathy K,Sathyanarayana Sampath Kumar,Saurabh Kumar,Shashank Khasare,Shenbaga Devi Venkatesh Kumar,Shiva Rama Krishna Parvatham,Shoeb Shaikh,Shrishanmathi A,Shubham Pathak,Sree Samhita Koppaka,Sreenivasa Raghavan K S,Sreeram Venkatasubramanian,Suprabha Desai Bojja,Swetha R,Syed Ahmed,Chinmai Harshitha Thota,Tushar Yadav,Veeravelly Kusumitha,V V S S Prasanth Patnaik,Vidya Sri Sesetti,Vijayakeerthi K,Vikram Raj Bakshi,Vinay K K,Vinoth Kumar Loganathan,Vipin Tiwari,Vivek Kumar Shrivastav,V Venkata Sri Datta Charan,Wasim Akhtar Khan*

Main category: cs.SE

TL;DR: Mify-Coder, a 2.5B-parameter model, achieves high accuracy and safety on coding tasks, outperforming larger models through efficient training with high-quality data and LLM-based quality filtering. It can be deployed on standard desktops.


<details>
  <summary>Details</summary>
Motivation: The motivation is to demonstrate that a compact model, Mify-Coder, can match or even outperform much larger models in code generation and function-calling benchmarks while maintaining efficiency and safety compliance, making it accessible for deployment on standard computing environments.

Method: Mify-Coder was trained using a compute-optimal strategy on 4.2T tokens, based on the Mify-2.5B foundation. The process involved combining high-quality curated sources with synthetic data, iteratively refined using enterprise-grade evaluation datasets, and applying LLM-based quality filtering. The training also explored CPT-SFT objectives, data mixtures, and sampling dynamics to enhance performance.

Result: Mify-Coder showed comparable accuracy and safety to larger baseline models, while significantly outperforming them on coding and function-calling benchmarks. Additionally, quantized versions of Mify-Coder were successfully deployed on standard desktops without needing specialized hardware.

Conclusion: Through disciplined use of data and compute resources, smaller models like Mify-Coder can achieve competitive results in terms of accuracy, efficiency, and safety, making advanced code intelligence more widely accessible.

Abstract: We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.

</details>


### [75] [A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context](https://arxiv.org/abs/2512.23782)
*Kessia Nepomuceno,Fabio Petrillo*

Main category: cs.SE

TL;DR: 本文通过对95项研究的系统文献映射，探讨了软件工程中公平性解决方案的当前进展，揭示了研究主要集中在方法和算法上，并且多为后处理和群体公平性，缺乏早期干预、个体公平性和对偏见根源的理解。此外，公平性研究仍主要停留在学术层面，与工业界的合作较少，技术成熟度较低。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程领域近年来的发展，系统中的公平性问题日益受到重视。尽管已经提出了若干指导方针来解决公平性问题，但要全面理解确保软件系统公平性的研究解决方案仍然具有挑战性。

Method: 本文开发了一个分类框架，从新的角度组织关于软件公平性的研究，并将其应用于95项选定的研究，分析它们在工业采用方面的潜力。

Result: 研究结果表明，软件公平性研究正在扩展，但仍主要集中在方法和算法上，特别是后处理和群体公平性方面。对于早期阶段干预措施、个体公平性指标以及理解偏见的根本原因的关注较少。此外，公平性研究很大程度上仍然是学术性质的，与行业的合作有限，技术就绪水平（TRL）低到中等，表明向工业转移还有一定距离。

Conclusion: 结论强调了在整个软件开发生命周期中考虑公平性的重要性，并呼吁加强学术界与产业界的协作。这项分析为该领域提供了一个全面概览，为未来关于软件系统公平性的研究和实际应用奠定了基础。

Abstract: Context: Fairness in systems has emerged as a critical concern in software engineering, garnering increasing attention as the field has advanced in recent years. While several guidelines have been proposed to address fairness, achieving a comprehensive understanding of research solutions for ensuring fairness in software systems remains challenging. Objectives: This paper presents a systematic literature mapping to explore and categorize current advancements in fairness solutions within software engineering, focusing on three key dimensions: research trends, research focus, and viability in industrial contexts. Methods: We develop a classification framework to organize research on software fairness from a fresh perspective, applying it to 95 selected studies and analyzing their potential for industrial adoption. Results: Our findings reveal that software fairness research is expanding, yet it remains heavily focused on methods and algorithms. It primarily focuses on post-processing and group fairness, with less emphasis on early-stage interventions, individual fairness metrics, and understanding bias root causes. Additionally fairness research remains largely academic, with limited industry collaboration and low to medium Technology Readiness Level (TRL), indicating that industrial transferability remains distant. Conclusion: Our results underscore the need to incorporate fairness considerations across all stages of the software development life-cycle and to foster greater collaboration between academia and industry. This analysis provides a comprehensive overview of the field, offering a foundation to guide future research and practical applications of fairness in software systems.

</details>


### [76] [Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education](https://arxiv.org/abs/2512.23982)
*Hung-Fu Chang,MohammadShokrolah Shirazi,Lizhou Cao,Supannika Koolmanojwong Mobasser*

Main category: cs.SE

TL;DR: 本文通过分析57个YouTube视频，探讨了大型语言模型（LLMs）在专业软件开发中的应用、相关风险以及对开发工作流程的转变，并讨论了这些发现对计算机科学和软件工程教育的意义。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注个体层面或教育场景中基于AI的编码，而工业实践者的视角尚未得到充分探索。本文旨在填补这一空白，研究LLM编码工具在专业实践中的使用情况、相关担忧与风险，以及开发工作流的变化，特别关注对计算教育的影响。

Method: 本研究采用定性分析方法，筛选并评估了2024年末至2025年间发布的57个精选YouTube视频，这些视频包含了从业者分享的经验和个人见解。通过对选定来源进行分析，比较了基于LLM的编程与传统编程方式，识别新兴风险，并描绘出演变中的工作流程特征。

Result: 研究结果揭示了基于AI的编码实践定义、显著的生产力提升及入门门槛降低现象。同时，也指出了向代码审查环节转移的发展瓶颈问题，以及关于代码质量、可维护性、安全漏洞、道德议题、基础问题解决技能侵蚀和初级工程师准备不足等方面的担忧。

Conclusion: 基于以上洞察，文章讨论了对计算机科学和软件工程教育的影响，并提倡课程转向注重解决问题能力、架构思维、代码审查及早融入项目式学习的教学模式，以整合LLM工具。本研究为基于AI的编码提供了来自行业的视角，并就如何使教育实践与快速变化的职业现实保持一致提出了指导建议。

Abstract: Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.

</details>


### [77] [Developing controlled natural language for formal specification patterns using AI assistants](https://arxiv.org/abs/2512.24159)
*Natalia Garanina,Vladimir Zyubin,Igor Anureev*

Main category: cs.SE

TL;DR: 本研究提出了一种基于形式规范模式的系统化构建需求控制自然语言的方法，该方法通过三个阶段实现：1）编译一个利用形式规范模板所有属性的一般自然语言需求模式；2）使用AI助手根据部分评估属性生成一系列简化后的自然语言需求模式；3）通过对结果模式语法结构的分析来正式化控制自然语言的语法。此方法已在事件驱动的时间需求上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一种能够将复杂的形式规范转换为易于理解的控制自然语言需求描述的方法，以提高软件工程中需求规格说明的可读性和准确性。

Method: 采用三步法：首先创建一个通用自然语言需求模板，接着借助AI工具生成多个具体的自然语言表述样本，最后对这些样本文本进行语法分析并据此定义一套控制自然语言规则。

Result: 成功地为事件驱动型时间要求制定了控制自然语言，并且证实了所提方法在转换形式化需求至更易懂格式方面的有效性。

Conclusion: 研究表明，结合AI技术可以帮助有效地从形式化的需求模式中派生出用户友好的控制自然语言表达方式，这对于改善软件开发过程中的沟通效率具有重要意义。

Abstract: Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements.

</details>


### [78] ["Game Changer" or "Overenthusiastic Drunk Acquaintance"? Generative AI Use by Blind and Low Vision Software Professionals in the Workplace](https://arxiv.org/abs/2512.24462)
*Yoonha Cha,Victoria Jackson,Lauren Shu,Stacy Branham,André van der Hoek*

Main category: cs.SE

TL;DR: 研究探讨了盲人和低视力软件专业人员（BLVSPs）在软件开发中使用生成式AI（GenAI）的情况，发现尽管GenAI提高了他们的生产力和可访问性，但也带来了诸如幻觉问题等风险，并且有时组织政策限制了其使用。


<details>
  <summary>Details</summary>
Motivation: 考虑到盲人和低视力软件专业人士在软件开发领域面临的技术与合作障碍，以及他们对生成式AI的独特视角尚未被充分探讨，本研究旨在了解生成式AI对于这些专业人士的工作意味着什么。

Method: 通过与39名盲人及低视力软件专业人士进行半结构化访谈的方式，收集关于引入生成式AI后对他们工作影响的第一手资料。

Result: 结果显示，虽然生成式AI帮助提升了这部分群体的工作效率和可访问性，但同时也伴随着特定挑战，如更容易受到AI产生的错误信息影响；此外，一些组织内部的政策也构成了使用上的障碍。

Conclusion: 基于研究结果讨论了盲人和低视力软件专业人士在决定是否以及何时采用生成式AI工具时必须权衡的高风险与高回报。

Abstract: The software development workplace poses numerous technical and collaborative accessibility challenges for blind and low vision software professionals (BLVSPs). Though Generative AI (GenAI) is increasingly adopted within the software development industry and has been a rapidly growing topic of interest in research, to date, the unique perspectives of BLVSPs have yet to be consulted. We report on a qualitative study involving 39 semi-structured interviews with BLVSPs about what the introduction of GenAI has meant for their work. We found that BLVSPs used GenAI for many software development tasks, resulting in benefits such as increased productivity and accessibility. However, significant costs were also accompanied by GenAI use as they were more vulnerable to hallucinations than their sighted colleagues. Sometimes, organizational policies prevented use. Based on our findings, we discuss the higher-risks and higher-returns that BLVSPs had to carefully weigh when deciding whether and when to use GenAI tools for work.

</details>


### [79] [A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation](https://arxiv.org/abs/2512.24530)
*Nikolaos Mavrogeorgis,Christos Vasiladiotis,Pei Mu,Amir Khordadi,Björn Franke,Antonio Barbalace*

Main category: cs.SE

TL;DR: 本文介绍了一种名为Unifico的新型多指令集架构编译器，该编译器在不同架构上执行时能够保持相同的栈布局，从而避免了运行时栈转换的需求，并减少了与指令集架构迁移相关的开销。


<details>
  <summary>Details</summary>
Motivation: 异构ISA处理器设计需要显式的软件支持来桥接ISA异质性，但缺乏支持异构ISA目标的编译工具链阻碍了这一领域的研究。编译器在迁移时有效地处理状态转换至关重要，特别是程序栈从一种架构到另一种架构的运行时转换成本过高。

Method: 开发了一个新的多ISA编译器Unifico，它生成在任一架构上执行期间保持相同栈布局的二进制文件。Unifico使用LLVM编译器基础设施实现，目前针对x86-64和ARMv8 ISA。

Result: Unifico在一系列计算密集型NAS基准测试中进行了评估，显示对整体执行时间的影响极小，高端（低端）处理器平均引入的开销不到6%（10%）。与最先进的Popcorn编译器相比，Unifico将二进制大小开销从约200%减少到约10%，同时消除了ISA迁移期间的栈转换开销。

Conclusion: 通过避免运行时栈转换，Unifico成功地消除了与ISA迁移相关的开销，并且在保持性能的同时显著减小了二进制大小开销。

Abstract: Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, "getting right" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration.

</details>


### [80] [Localized Calibrated Uncertainty in Code Language Models](https://arxiv.org/abs/2512.24560)
*David Gros,Prem Devanbu*

Main category: cs.SE

TL;DR: 本文提出了用于定位大型语言模型生成代码中可能偏离用户意图部分的技术，并通过创建一个包含测试用例验证正确性的修复程序的数据集来衡量这些技术的有效性。研究发现，使用小型监督模型的探测方法可以有效地估计大型模型生成代码中需要编辑的部分。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能够根据自然语言提示生成复杂的源代码，但有时生成的结果会偏离用户的意图，需要监督和编辑。为了支持这一过程，作者提出了一种技术来定位代码生成中可能存在偏差的地方。

Method: 首先创建了一个包含'最小意图对齐补丁'的数据集，每个程序都利用测试用例来验证其正确性。然后，评估了多种技术在为代码中哪些部分将被编辑分配经过良好校准的概率方面的能力。比较了白盒探测（提出了一种高效任意跨度查询的技术）、黑盒反射以及基于自一致性方法的效果。

Result: 结果显示，使用小型监督模型进行探测的方法能够在估计由大得多的模型生成的代码中被编辑行数时达到较低的校准误差和约0.2的Brier技能分数。此外，还探讨了这些技术的泛化能力及其与AI监督和控制之间的联系，发现仅在代码上训练的探测器如果允许新的概率缩放，则显示出一定程度上泛化到自然语言错误的能力。

Conclusion: 这项工作表明，通过使用适当的技术和较小规模的监督模型，有可能有效地识别出大型语言模型生成代码中潜在的问题区域，从而提高人机协作效率。同时，也揭示了这些技术可能在更广泛的AI安全和可控性领域发挥作用。

Abstract: Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of "Minimal Intent Aligning Patches" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.

</details>


### [81] [On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study](https://arxiv.org/abs/2512.24570)
*Shiqi Kuang,Zhao Tian,Tao Xiao,Dong Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本研究首次大规模实证分析了五种广泛使用的训练数据优化技术及其两两组合对基于大型语言模型（LLM）代码生成的影响。结果表明，数据合成是最有效的技术，可以提高功能正确性并减少代码异味，但在代码可维护性方面不如数据重构、清理和选择。大多数技术组合虽然不能进一步提高功能正确性，但能有效提升代码质量。其中，数据合成与数据重构的组合表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管为提高数据质量提出了许多训练数据优化技术，但它们的整体有效性尚未得到系统评估。为了填补这一空白，需要进行一项大规模实证研究来考察这些技术及它们的组合对于基于LLM的代码生成的有效性。

Method: 研究选取了五种广泛应用的数据优化技术，并考虑了它们的所有两两组合形式，通过三个基准测试和四个不同的大型语言模型来评估这些方法在代码生成上的效果。

Result: 数据合成在改善功能正确性和降低代码异味方面最为有效；然而，在增强代码可维护性上，数据重构、清理和选择方法表现更好。多数技术组合能够有效改善代码质量，特别是数据合成与数据重构相结合时整体性能最强。

Conclusion: 这项工作是系统理解训练数据优化及组合策略的第一步，为未来基于LLM的代码生成研究和应用提供了实用指导。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.

</details>


### [82] [A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs](https://arxiv.org/abs/2512.24594)
*Zhongyi Wang,Tengjie Lin,Mingshuai Chen,Haokun Li,Mingqi Yang,Xiao Yi,Shengchao Qin,Yixing Luo,Xiaofeng Li,Bin Gu,Liqiang Lu,Jianwei Yin*

Main category: cs.SE

TL;DR: 本文提出了一种名为Preguss的框架，它通过结合静态分析和演绎验证来自动化生成和优化形式化规范。该方法显著提高了现有基于大语言模型（LLM）方法的表现，并大幅度减少了人工验证工作量。


<details>
  <summary>Details</summary>
Motivation: 全自动的大规模软件和硬件系统的验证是形式化方法追求的目标之一。虽然大语言模型展示了它们在提升形式验证自动化程度方面的潜力，但它们在处理长上下文推理及推断复杂、跨过程规范方面存在局限性。

Method: Preguss框架采用分而治之的方式，协同利用静态分析与演绎验证，具体包括：1) 以潜在运行时错误为导向构建并优先排序验证单元；2) 利用LLM辅助合成单元级别的跨过程规范。

Result: 实验表明，Preguss相比最先进的基于LLM的方法表现出色，特别是对于超过一千行代码的真实程序实现高度自动化的无运行时错误验证，同时将人工验证工作量减少了80.6%~88.9%。

Conclusion: Preguss为解决大规模软件系统中形式化验证的自动化问题提供了一个有效方案，特别擅长于减少对人工干预的需求以及提高处理复杂跨过程规范的能力。

Abstract: Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.

</details>


### [83] [DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information](https://arxiv.org/abs/2512.24635)
*Zhili Huang,Ling Xu,Chao Liu,Weifeng Sun,Xu Zhang,Yan Lei,Meng Yan,Hongyu Zhang*

Main category: cs.SE

TL;DR: 提出了一种新的自动程序修复方法DynaFix，该方法通过迭代利用运行时信息来改进修补过程。在Defects4J v1.2和v2.0基准测试中，DynaFix展示了其在修复复杂错误方面的有效性和效率，相比现有方法，不仅提高了修复成功率还减少了搜索空间。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的自动程序修复技术主要依赖静态分析，忽视了运行时行为，即便尝试整合动态信号也多限于训练或微调阶段，并且未充分利用执行级的细粒度信息。这种局限性导致这些模型难以模拟人类逐步调试的过程，从而限制了它们处理多步推理及修复复杂错误的能力。

Method: DynaFix是一种以执行级动态信息为驱动的自动程序修复方法，它通过每次修复循环捕获变量状态、控制流路径及调用堆栈等运行时信息，并将其转化为结构化的提示来指导大模型生成候选补丁。如果补丁验证失败，则重新执行修改后的程序以收集新一轮的信息用于下一次尝试。

Result: DynaFix在Defects4J v1.2和v2.0上进行了评估，成功修复了186个单函数错误，比最先进的基线提高了10%，其中包括以前未被修复的38个错误。此外，在最多35次尝试内找到正确的补丁，相较于现有方法减少了70%的补丁搜索空间。

Conclusion: DynaFix通过迭代利用执行级动态信息有效地提高了自动程序修复的成功率和效率，尤其是在处理复杂的软件错误方面展现出了显著优势。

Abstract: Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.
  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.

</details>


### [84] [Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study](https://arxiv.org/abs/2512.24656)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 本研究对123个开源量子存储库进行了首次生态系统规模的纵向分析，涵盖了从2012年到2024年的软件缺陷情况。结果表明，全栈库和编译器由于电路、门及转译相关问题而最容易出现缺陷，而模拟器主要受到测量和噪声建模错误的影响。经典bug主要影响可用性和互操作性，而特定于量子的bug则不成比例地降低了性能、可维护性和可靠性。通过自动化测试可以显著减少预期的缺陷发生率。


<details>
  <summary>Details</summary>
Motivation: 量子软件工程对于确保混合量子-经典系统的可靠性和可维护性至关重要，但关于bug如何在实际量子项目中产生及其对质量的影响的实际证据仍然有限。

Method: 采用结合了仓库挖掘、静态代码分析、问题元数据提取以及一个经过验证的基于规则的分类框架的混合方法来分析32,296份核实过的bug报告。

Result: 全栈库和编译器是最容易出现缺陷的类别；模拟器主要受测量和噪声建模错误影响；经典bug主要影响可用性和互操作性，而量子特定bug则特别损害性能、可维护性和可靠性。长期分析显示，缺陷密度在2017年至2021年间达到顶峰，之后下降。高严重性缺陷集中在密码学、实验计算和编译工具链中。使用自动化测试的存储库能更快检测并解决问题。负二项回归进一步表明，自动化测试与预期缺陷发生率大约降低60%有关。

Conclusion: 这项工作提供了首个大规模数据驱动的量子软件缺陷特征描述，并为改进量子软件工程中的测试、文档编制和可维护性实践提供了实证指导。

Abstract: Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.

</details>


### [85] [Feature Slice Matching for Precise Bug Detection](https://arxiv.org/abs/2512.24858)
*Ke Ma,Jianjun Huang,Wei You,Bin Liang,Jingzheng Wu,Yanjun Wu,Yuanjun Gong*

Main category: cs.SE

TL;DR: 本文提出了一种名为MATUS的方法，通过减少目标噪音来提高基于相似性度量的精确错误检测。该方法从有缺陷的查询和目标中提取特征片段以表示（潜在）错误逻辑的语义特征，并最终在实际项目中有效检测到未知错误。


<details>
  <summary>Details</summary>
Motivation: 现有的基于函数相似性测量进行错误检测的方法受到无关语句产生的噪声干扰，导致性能受到影响。现有工作在抑制噪声干扰方面效果不佳，特别是无法消除目标中的噪声。

Method: MATUS通过从有缺陷的查询和目标代码中抽取特征片段来表示潜在错误逻辑的语义特征。它利用了有缺陷代码的先验知识来指导目标切片过程，以端到端的方式确定目标中的切片标准。所有特征片段都被嵌入并通过向量相似性进行比较，从而审计可疑错误候选者确认目标中的未知错误。

Result: 实验表明，MATUS在真实世界项目的错误检测中具有优势，并且效率可接受。它总共发现了Linux内核中的31个未知错误，这些错误都得到了内核开发者的确认，并且其中11个被分配了CVE编号。

Conclusion: MATUS为解决基于相似性度量的错误检测中的噪声问题提供了一个有效的解决方案，有助于更准确地识别软件中的未知错误。

Abstract: Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [86] [Hojabr: Towards a Theory of Everything for AI and Data Analytics](https://arxiv.org/abs/2512.23925)
*Amir Shaikhha*

Main category: cs.DB

TL;DR: Hojabr是一种统一的声明性中间语言，它在一个高阶代数框架中整合了关系代数、张量代数和基于约束的推理，旨在解决数据分析流水线中跨范式、执行模型和研究社区的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数据处理系统在范式、执行模型以及研究社群之间存在分割，导致优化工作重复、互操作性受限以及逻辑抽象与物理执行策略之间的严格分离。

Method: 提出Hojabr作为统一的声明性中间语言，在一个单一的高阶代数框架内集成关系代数、张量代数及基于约束的推理，使得连接、聚合、张量收缩和递归计算能够以统一的方式表达。

Result: Hojabr通过明确语义、结构和代数属性，并支持编译栈中的可扩展性，实现了数据库系统、机器学习框架和编译器基础设施间的优化技术系统化推理与重用。

Conclusion: Hojabr作为一种创新的方法，通过提供一种通用的语言来描述不同类型的数据分析任务，有助于提高不同领域间的技术互通性和优化效率。

Abstract: Modern data analytics pipelines increasingly combine relational queries, graph processing, and tensor computation within a single application, but existing systems remain fragmented across paradigms, execution models, and research communities. This fragmentation results in repeated optimization efforts, limited interoperability, and strict separation between logical abstractions and physical execution strategies.
  We propose Hojabr as a unified declarative intermediate language to address this problem. Hojabr integrates relational algebra, tensor algebra, and constraint-based reasoning within a single higher-order algebraic framework, in which joins, aggregations, tensor contractions, and recursive computations are expressed uniformly. Physical choices, such as join algorithms, execution models, and sparse versus dense tensor representations, are handled as constraint-specialization decisions rather than as separate formalisms. Hojabr supports bidirectional translation with existing declarative languages, enabling programs to be both lowered into Hojabr for analysis and optimization and lifted back into their original declarative form. By making semantic, structural, and algebraic properties explicit, and by supporting extensibility across the compilation stack, Hojabr enables systematic reasoning and reuse of optimization techniques across database systems, machine learning frameworks, and compiler infrastructures.

</details>


### [87] [LMG Index: A Robust Learned Index for Multi-Dimensional Performance Balance](https://arxiv.org/abs/2512.24824)
*Yuzhen Chen,Bin Yao*

Main category: cs.DB

TL;DR: 本文提出了LMIndex，一种鲁棒的学习索引框架，通过高效的查询/更新顶层结构和最优误差阈值训练算法来优化。基于此开发的LMG变体采用新的间隙分配策略，以提高更新性能并在动态工作负载下保持稳定性。实验表明LMG在批量加载、点查询、范围查询、更新效率、稳定性和空间使用方面均表现出色，有效打破了现有方法中的多维性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型索引大多只针对有限的目标进行优化，例如查询延迟或空间使用，而忽略了其他实际评估维度如更新效率和稳定性。此外，许多学习型索引依赖于关于数据分布或工作负载的假设，在面对未知或不断变化的情况时缺乏理论保证，这限制了它们在现实世界系统中的通用性。

Method: 提出了一种名为LMIndex的学习索引框架，该框架利用高效的查询/更新顶层结构（当键类型固定时理论上为$O(1)$）和一个接近$O(1)$的实际操作效率的最佳误差阈值训练算法。进一步地，开发了LMG（带有间隙的LMIndex），这是一种采用新颖的间隙分配策略以增强更新性能并保持在动态工作负载下的稳定性的变体。

Result: 广泛的评估显示，LMG在多个方面实现了竞争或领先的性能：包括批量加载速度提高了最多8.25倍、点查询速度提升了至多1.49倍、范围查询比B+树快达4.02倍、读写工作负载下的更新速度快了高达1.5倍、稳定性上变异系数降低了最高82.59倍、以及空间使用减少了最多1.38倍。

Conclusion: LMG有效地解决了当前最先进方法中存在的多维度性能权衡问题，提供了一个平衡且多功能的框架。

Abstract: Index structures are fundamental for efficient query processing on large-scale datasets. Learned indexes model the indexing process as a prediction problem to overcome the inherent trade-offs of traditional indexes. However, most existing learned indexes optimize only for limited objectives like query latency or space usage, neglecting other practical evaluation dimensions such as update efficiency and stability. Moreover, many learned indexes rely on assumptions about data distributions or workloads, lacking theoretical guarantees when facing unknown or evolving scenarios, which limits their generality in real-world systems.
  In this paper, we propose LMIndex, a robust framework for learned indexing that leverages a efficient query/update top-layer structure (theoretically $O(1)$ when the key type is fixed) and a efficient optimal error threshold training algorithm (approach $O(1)$ in practice). Building upon this, we develop LMG (LMIndex with gaps), a variant employing a novel gap allocation strategy to enhance update performance and maintain stability under dynamic workloads. Extensive evaluations show that LMG achieves competitive or leading performance, including bulk loading (up to 8.25$\times$ faster), point queries (up to 1.49$\times$ faster), range queries (up to 4.02$\times$ faster than B+Tree), update (up to 1.5$\times$ faster on read-write workloads), stability (up to 82.59$\times$ lower coefficient of variation), and space usage (up to 1.38$\times$ smaller). These results demonstrate that LMG effectively breaks the multi-dimensional performance trade-offs inherent in state-of-the-art approaches, offering a balanced and versatile framework.

</details>
